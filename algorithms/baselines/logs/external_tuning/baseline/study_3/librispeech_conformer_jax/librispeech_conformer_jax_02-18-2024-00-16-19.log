python3 submission_runner.py --framework=jax --workload=librispeech_conformer --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification/study_3 --overwrite=true --save_checkpoints=false --num_tuning_trials=5 --rng_seed=4110531300 --max_global_steps=80000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_conformer_jax_02-18-2024-00-16-19.log
I0218 00:16:40.039838 140549388556096 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification/study_3/librispeech_conformer_jax.
I0218 00:16:41.079003 140549388556096 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host CUDA Interpreter
I0218 00:16:41.079803 140549388556096 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0218 00:16:41.079960 140549388556096 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0218 00:16:41.081028 140549388556096 submission_runner.py:542] Using RNG seed 4110531300
I0218 00:16:42.231828 140549388556096 submission_runner.py:551] --- Tuning run 1/5 ---
I0218 00:16:42.232028 140549388556096 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_3/librispeech_conformer_jax/trial_1.
I0218 00:16:42.232228 140549388556096 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_3/librispeech_conformer_jax/trial_1/hparams.json.
I0218 00:16:42.413619 140549388556096 submission_runner.py:206] Initializing dataset.
I0218 00:16:42.413843 140549388556096 submission_runner.py:213] Initializing model.
I0218 00:16:47.308886 140549388556096 submission_runner.py:255] Initializing optimizer.
I0218 00:16:48.542419 140549388556096 submission_runner.py:262] Initializing metrics bundle.
I0218 00:16:48.542624 140549388556096 submission_runner.py:280] Initializing checkpoint and logger.
I0218 00:16:48.543830 140549388556096 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_3/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I0218 00:16:48.543965 140549388556096 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_3/librispeech_conformer_jax/trial_1/meta_data_0.json.
I0218 00:16:48.544158 140549388556096 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0218 00:16:48.544232 140549388556096 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0218 00:16:48.850247 140549388556096 logger_utils.py:220] Unable to record git information. Continuing without it.
I0218 00:16:49.126403 140549388556096 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_3/librispeech_conformer_jax/trial_1/flags_0.json.
I0218 00:16:49.141668 140549388556096 submission_runner.py:314] Starting training loop.
I0218 00:16:49.432114 140549388556096 input_pipeline.py:20] Loading split = train-clean-100
I0218 00:16:49.469541 140549388556096 input_pipeline.py:20] Loading split = train-clean-360
I0218 00:16:49.883845 140549388556096 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0218 00:17:50.517647 140377088575232 logging_writer.py:48] [0] global_step=0, grad_norm=66.69013214111328, loss=31.5310001373291
I0218 00:17:50.552923 140549388556096 spec.py:321] Evaluating on the training split.
I0218 00:17:50.719369 140549388556096 input_pipeline.py:20] Loading split = train-clean-100
I0218 00:17:50.754416 140549388556096 input_pipeline.py:20] Loading split = train-clean-360
I0218 00:17:51.148066 140549388556096 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0218 00:19:04.570715 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 00:19:04.687631 140549388556096 input_pipeline.py:20] Loading split = dev-clean
I0218 00:19:04.693028 140549388556096 input_pipeline.py:20] Loading split = dev-other
I0218 00:20:05.724837 140549388556096 spec.py:349] Evaluating on the test split.
I0218 00:20:05.842958 140549388556096 input_pipeline.py:20] Loading split = test-clean
I0218 00:20:41.828311 140549388556096 submission_runner.py:408] Time since start: 232.68s, 	Step: 1, 	{'train/ctc_loss': Array(31.169794, dtype=float32), 'train/wer': 1.120843137663203, 'validation/ctc_loss': Array(30.090126, dtype=float32), 'validation/wer': 0.9587360128213792, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.214182, dtype=float32), 'test/wer': 0.9757885970791949, 'test/num_examples': 2472, 'score': 61.41118311882019, 'total_duration': 232.68418264389038, 'accumulated_submission_time': 61.41118311882019, 'accumulated_eval_time': 171.2729423046112, 'accumulated_logging_time': 0}
I0218 00:20:41.856282 140367617849088 logging_writer.py:48] [1] accumulated_eval_time=171.272942, accumulated_logging_time=0, accumulated_submission_time=61.411183, global_step=1, preemption_count=0, score=61.411183, test/ctc_loss=30.214181900024414, test/num_examples=2472, test/wer=0.975789, total_duration=232.684183, train/ctc_loss=31.1697940826416, train/wer=1.120843, validation/ctc_loss=30.090126037597656, validation/num_examples=5348, validation/wer=0.958736
I0218 00:22:20.236876 140379054606080 logging_writer.py:48] [100] global_step=100, grad_norm=9.565184593200684, loss=6.946313381195068
I0218 00:23:36.083833 140379062998784 logging_writer.py:48] [200] global_step=200, grad_norm=1.959326148033142, loss=6.087654113769531
I0218 00:24:52.002767 140379054606080 logging_writer.py:48] [300] global_step=300, grad_norm=0.6718751788139343, loss=5.8619585037231445
I0218 00:26:08.018671 140379062998784 logging_writer.py:48] [400] global_step=400, grad_norm=0.38509175181388855, loss=5.810033798217773
I0218 00:27:24.085919 140379054606080 logging_writer.py:48] [500] global_step=500, grad_norm=0.5616556406021118, loss=5.80573034286499
I0218 00:28:40.083334 140379062998784 logging_writer.py:48] [600] global_step=600, grad_norm=0.5529247522354126, loss=5.8009467124938965
I0218 00:29:56.188347 140379054606080 logging_writer.py:48] [700] global_step=700, grad_norm=0.4732825458049774, loss=5.803762435913086
I0218 00:31:12.197511 140379062998784 logging_writer.py:48] [800] global_step=800, grad_norm=0.27119767665863037, loss=5.795228958129883
I0218 00:32:28.310297 140379054606080 logging_writer.py:48] [900] global_step=900, grad_norm=0.2818555235862732, loss=5.816906929016113
I0218 00:33:44.444882 140379062998784 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.28820735216140747, loss=5.815548896789551
I0218 00:35:03.380149 140379146925824 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.31184425950050354, loss=5.807464599609375
I0218 00:36:19.755789 140379138533120 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.3146357536315918, loss=5.762359142303467
I0218 00:37:35.739593 140379146925824 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.42102736234664917, loss=5.743952751159668
I0218 00:38:51.846549 140379138533120 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.3553369343280792, loss=5.653533458709717
I0218 00:40:07.986350 140379146925824 logging_writer.py:48] [1500] global_step=1500, grad_norm=3.3306751251220703, loss=5.564675807952881
I0218 00:41:24.131093 140379138533120 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.0550106763839722, loss=5.431827545166016
I0218 00:42:40.161700 140379146925824 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.844876766204834, loss=5.177966594696045
I0218 00:43:56.181147 140379138533120 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.3177138566970825, loss=4.6562180519104
I0218 00:44:42.213027 140549388556096 spec.py:321] Evaluating on the training split.
I0218 00:45:18.973782 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 00:46:03.353156 140549388556096 spec.py:349] Evaluating on the test split.
I0218 00:46:25.180018 140549388556096 submission_runner.py:408] Time since start: 1776.03s, 	Step: 1862, 	{'train/ctc_loss': Array(5.747149, dtype=float32), 'train/wer': 0.9443883751107083, 'validation/ctc_loss': Array(5.8983974, dtype=float32), 'validation/wer': 0.8964635005841065, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.7778125, dtype=float32), 'test/wer': 0.8994576808238377, 'test/num_examples': 2472, 'score': 1501.6815497875214, 'total_duration': 1776.0325355529785, 'accumulated_submission_time': 1501.6815497875214, 'accumulated_eval_time': 274.23418140411377, 'accumulated_logging_time': 0.043993473052978516}
I0218 00:46:25.211533 140379146925824 logging_writer.py:48] [1862] accumulated_eval_time=274.234181, accumulated_logging_time=0.043993, accumulated_submission_time=1501.681550, global_step=1862, preemption_count=0, score=1501.681550, test/ctc_loss=5.777812480926514, test/num_examples=2472, test/wer=0.899458, total_duration=1776.032536, train/ctc_loss=5.7471489906311035, train/wer=0.944388, validation/ctc_loss=5.898397445678711, validation/num_examples=5348, validation/wer=0.896464
I0218 00:46:54.828295 140379138533120 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.8607090711593628, loss=4.160313606262207
I0218 00:48:10.657963 140379146925824 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.0348700284957886, loss=3.8494014739990234
I0218 00:49:29.832848 140379146925824 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.7096261978149414, loss=3.6005911827087402
I0218 00:50:45.971914 140379138533120 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.8949760794639587, loss=3.415109157562256
I0218 00:52:02.039143 140379146925824 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.3796262741088867, loss=3.2684035301208496
I0218 00:53:18.083993 140379138533120 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.1138763427734375, loss=3.143732786178589
I0218 00:54:34.064391 140379146925824 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.2749282121658325, loss=3.08941388130188
I0218 00:55:49.990936 140379138533120 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.2349799871444702, loss=2.973093032836914
I0218 00:57:05.957210 140379146925824 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.1549097299575806, loss=2.867025852203369
I0218 00:58:21.767694 140379138533120 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.9939264059066772, loss=2.849351167678833
I0218 00:59:37.603955 140379146925824 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.106295108795166, loss=2.7564051151275635
I0218 01:00:53.515002 140379138533120 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.1371177434921265, loss=2.662708282470703
I0218 01:02:12.550945 140379146925824 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.402054786682129, loss=2.587855100631714
I0218 01:03:28.419488 140379138533120 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.0981780290603638, loss=2.549677610397339
I0218 01:04:44.677683 140379146925824 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.0725196599960327, loss=2.522270441055298
I0218 01:06:00.564805 140379138533120 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.9957862496376038, loss=2.5107338428497314
I0218 01:07:16.433610 140379146925824 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.2155474424362183, loss=2.5608363151550293
I0218 01:08:32.277417 140379138533120 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.069151520729065, loss=2.4582953453063965
I0218 01:09:48.049118 140379146925824 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.9240093231201172, loss=2.4534668922424316
I0218 01:10:25.657151 140549388556096 spec.py:321] Evaluating on the training split.
I0218 01:11:16.366482 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 01:12:05.353617 140549388556096 spec.py:349] Evaluating on the test split.
I0218 01:12:30.098271 140549388556096 submission_runner.py:408] Time since start: 3340.95s, 	Step: 3751, 	{'train/ctc_loss': Array(2.6828344, dtype=float32), 'train/wer': 0.569682763764217, 'validation/ctc_loss': Array(3.0748646, dtype=float32), 'validation/wer': 0.6164013246183998, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.7431552, dtype=float32), 'test/wer': 0.5617167347104585, 'test/num_examples': 2472, 'score': 2942.039139032364, 'total_duration': 3340.9508938789368, 'accumulated_submission_time': 2942.039139032364, 'accumulated_eval_time': 398.6696357727051, 'accumulated_logging_time': 0.09018349647521973}
I0218 01:12:30.132668 140379146925824 logging_writer.py:48] [3751] accumulated_eval_time=398.669636, accumulated_logging_time=0.090183, accumulated_submission_time=2942.039139, global_step=3751, preemption_count=0, score=2942.039139, test/ctc_loss=2.7431552410125732, test/num_examples=2472, test/wer=0.561717, total_duration=3340.950894, train/ctc_loss=2.6828343868255615, train/wer=0.569683, validation/ctc_loss=3.074864625930786, validation/num_examples=5348, validation/wer=0.616401
I0218 01:13:07.975950 140379138533120 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.010082721710205, loss=2.408489942550659
I0218 01:14:23.693684 140379146925824 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.9416064023971558, loss=2.3917620182037354
I0218 01:15:39.477448 140379138533120 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.0081433057785034, loss=2.31838059425354
I0218 01:16:55.269104 140379146925824 logging_writer.py:48] [4100] global_step=4100, grad_norm=1.0225260257720947, loss=2.318294048309326
I0218 01:18:14.237218 140379146925824 logging_writer.py:48] [4200] global_step=4200, grad_norm=1.1654164791107178, loss=2.268728494644165
I0218 01:19:29.859917 140379138533120 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.854944109916687, loss=2.2446250915527344
I0218 01:20:45.779927 140379146925824 logging_writer.py:48] [4400] global_step=4400, grad_norm=1.103878378868103, loss=2.2270960807800293
I0218 01:22:01.468096 140379138533120 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.8524659872055054, loss=2.211982488632202
I0218 01:23:17.010385 140379146925824 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.9825778007507324, loss=2.116478443145752
I0218 01:24:32.677937 140379138533120 logging_writer.py:48] [4700] global_step=4700, grad_norm=1.0133792161941528, loss=2.159533977508545
I0218 01:25:48.353286 140379146925824 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.9873033165931702, loss=2.1141934394836426
I0218 01:27:04.116214 140379138533120 logging_writer.py:48] [4900] global_step=4900, grad_norm=1.0554084777832031, loss=2.0910184383392334
I0218 01:28:19.672773 140379146925824 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.9383173584938049, loss=2.0768144130706787
I0218 01:29:35.248733 140379138533120 logging_writer.py:48] [5100] global_step=5100, grad_norm=1.205795407295227, loss=2.0619051456451416
I0218 01:30:53.984891 140379146925824 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.09047269821167, loss=2.0259063243865967
I0218 01:32:09.653557 140379138533120 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.8407374024391174, loss=1.9956804513931274
I0218 01:33:25.626233 140379146925824 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.8009201288223267, loss=1.9859076738357544
I0218 01:34:41.406433 140379138533120 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.8035291433334351, loss=1.9585951566696167
I0218 01:35:57.051254 140379146925824 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.8593753576278687, loss=1.964231014251709
I0218 01:36:30.750457 140549388556096 spec.py:321] Evaluating on the training split.
I0218 01:37:23.463229 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 01:38:13.457735 140549388556096 spec.py:349] Evaluating on the test split.
I0218 01:38:38.587055 140549388556096 submission_runner.py:408] Time since start: 4909.44s, 	Step: 5646, 	{'train/ctc_loss': Array(0.7053174, dtype=float32), 'train/wer': 0.24406597403418884, 'validation/ctc_loss': Array(1.0583082, dtype=float32), 'validation/wer': 0.3103391679619993, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.7743883, dtype=float32), 'test/wer': 0.24930432839761948, 'test/num_examples': 2472, 'score': 4382.566341638565, 'total_duration': 4909.4394245147705, 'accumulated_submission_time': 4382.566341638565, 'accumulated_eval_time': 526.5003283023834, 'accumulated_logging_time': 0.14023447036743164}
I0218 01:38:38.620568 140379146925824 logging_writer.py:48] [5646] accumulated_eval_time=526.500328, accumulated_logging_time=0.140234, accumulated_submission_time=4382.566342, global_step=5646, preemption_count=0, score=4382.566342, test/ctc_loss=0.774388313293457, test/num_examples=2472, test/wer=0.249304, total_duration=4909.439425, train/ctc_loss=0.7053173780441284, train/wer=0.244066, validation/ctc_loss=1.0583082437515259, validation/num_examples=5348, validation/wer=0.310339
I0218 01:39:20.144607 140379138533120 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.8703577518463135, loss=1.9809304475784302
I0218 01:40:35.682429 140379146925824 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.787231981754303, loss=1.871193766593933
I0218 01:41:51.357220 140379138533120 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.7396848201751709, loss=1.902463436126709
I0218 01:43:07.078641 140379146925824 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.8591330647468567, loss=1.9162349700927734
I0218 01:44:22.743947 140379138533120 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.7719302773475647, loss=1.857641339302063
I0218 01:45:41.655779 140379146925824 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.9366943836212158, loss=1.826412558555603
I0218 01:46:57.157057 140379138533120 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.7761312127113342, loss=1.868302345275879
I0218 01:48:12.793815 140379146925824 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.8374708890914917, loss=1.8117624521255493
I0218 01:49:28.630643 140379138533120 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.8544617891311646, loss=1.8106727600097656
I0218 01:50:44.259356 140379146925824 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.7780470252037048, loss=1.808542251586914
I0218 01:51:59.895776 140379138533120 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.7668764591217041, loss=1.8266183137893677
I0218 01:53:15.500743 140379146925824 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.887675940990448, loss=1.7801663875579834
I0218 01:54:31.107764 140379138533120 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.7998024821281433, loss=1.799001693725586
I0218 01:55:46.636983 140379146925824 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.6995809078216553, loss=1.8089581727981567
I0218 01:57:02.150016 140379138533120 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.8091624975204468, loss=1.7821612358093262
I0218 01:58:18.809819 140379146925824 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.8105731010437012, loss=1.7243902683258057
I0218 01:59:37.446128 140379146925824 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.8160887360572815, loss=1.7461657524108887
I0218 02:00:53.076162 140379138533120 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.7126579880714417, loss=1.7411776781082153
I0218 02:02:08.770106 140379146925824 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.8408200740814209, loss=1.7457056045532227
I0218 02:02:38.881365 140549388556096 spec.py:321] Evaluating on the training split.
I0218 02:03:32.222490 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 02:04:21.911859 140549388556096 spec.py:349] Evaluating on the test split.
I0218 02:04:47.211642 140549388556096 submission_runner.py:408] Time since start: 6478.06s, 	Step: 7541, 	{'train/ctc_loss': Array(0.50713336, dtype=float32), 'train/wer': 0.17643393479877265, 'validation/ctc_loss': Array(0.8168226, dtype=float32), 'validation/wer': 0.2444944340925109, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5533391, dtype=float32), 'test/wer': 0.185343164137875, 'test/num_examples': 2472, 'score': 5822.735749721527, 'total_duration': 6478.064398050308, 'accumulated_submission_time': 5822.735749721527, 'accumulated_eval_time': 654.8250741958618, 'accumulated_logging_time': 0.19190430641174316}
I0218 02:04:47.247771 140379146925824 logging_writer.py:48] [7541] accumulated_eval_time=654.825074, accumulated_logging_time=0.191904, accumulated_submission_time=5822.735750, global_step=7541, preemption_count=0, score=5822.735750, test/ctc_loss=0.5533391237258911, test/num_examples=2472, test/wer=0.185343, total_duration=6478.064398, train/ctc_loss=0.5071333646774292, train/wer=0.176434, validation/ctc_loss=0.8168225884437561, validation/num_examples=5348, validation/wer=0.244494
I0218 02:05:32.455580 140379138533120 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.7488539218902588, loss=1.7029225826263428
I0218 02:06:47.992678 140379146925824 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.794895589351654, loss=1.6930450201034546
I0218 02:08:03.718257 140379138533120 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.9873561263084412, loss=1.7484092712402344
I0218 02:09:19.380853 140379146925824 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.7214880585670471, loss=1.7535886764526367
I0218 02:10:35.057290 140379138533120 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.755108654499054, loss=1.772928237915039
I0218 02:11:50.754400 140379146925824 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.7620207667350769, loss=1.6695291996002197
I0218 02:13:06.491187 140379138533120 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.7260572910308838, loss=1.7304232120513916
I0218 02:14:25.328097 140379146925824 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.824108898639679, loss=1.7542725801467896
I0218 02:15:40.772418 140379138533120 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.7110762596130371, loss=1.6714071035385132
I0218 02:16:56.329185 140379146925824 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.7212955355644226, loss=1.764975666999817
I0218 02:18:12.181197 140379138533120 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.7782078981399536, loss=1.7236199378967285
I0218 02:19:27.713750 140379146925824 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.8863672614097595, loss=1.7851587533950806
I0218 02:20:43.217432 140379138533120 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.8885135054588318, loss=1.6656229496002197
I0218 02:21:58.793795 140379146925824 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.8816512823104858, loss=1.645275592803955
I0218 02:23:15.613623 140379138533120 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.7635384798049927, loss=1.6836285591125488
I0218 02:24:36.106025 140379146925824 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.6708938479423523, loss=1.755757451057434
I0218 02:25:56.713095 140379138533120 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.7016149759292603, loss=1.6851545572280884
I0218 02:27:18.706090 140379146925824 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.7018463611602783, loss=1.664034366607666
I0218 02:28:34.025567 140379138533120 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.7046346068382263, loss=1.603853702545166
I0218 02:28:47.294274 140549388556096 spec.py:321] Evaluating on the training split.
I0218 02:29:39.894371 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 02:30:29.901265 140549388556096 spec.py:349] Evaluating on the test split.
I0218 02:30:54.989243 140549388556096 submission_runner.py:408] Time since start: 8045.84s, 	Step: 9419, 	{'train/ctc_loss': Array(0.42972216, dtype=float32), 'train/wer': 0.15163952249450954, 'validation/ctc_loss': Array(0.7233104, dtype=float32), 'validation/wer': 0.21718142058565126, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.47338814, dtype=float32), 'test/wer': 0.15836938638717932, 'test/num_examples': 2472, 'score': 7262.69296336174, 'total_duration': 8045.841583013535, 'accumulated_submission_time': 7262.69296336174, 'accumulated_eval_time': 782.5141160488129, 'accumulated_logging_time': 0.24313855171203613}
I0218 02:30:55.025733 140379146925824 logging_writer.py:48] [9419] accumulated_eval_time=782.514116, accumulated_logging_time=0.243139, accumulated_submission_time=7262.692963, global_step=9419, preemption_count=0, score=7262.692963, test/ctc_loss=0.473388135433197, test/num_examples=2472, test/wer=0.158369, total_duration=8045.841583, train/ctc_loss=0.4297221601009369, train/wer=0.151640, validation/ctc_loss=0.7233104109764099, validation/num_examples=5348, validation/wer=0.217181
I0218 02:31:56.670167 140379138533120 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.6998419761657715, loss=1.5961631536483765
I0218 02:33:12.204842 140379146925824 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.6722065210342407, loss=1.624401330947876
I0218 02:34:27.890806 140379138533120 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.7876576781272888, loss=1.6474019289016724
I0218 02:35:43.289883 140379146925824 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.6156360507011414, loss=1.5779343843460083
I0218 02:36:58.671370 140379138533120 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.6812938451766968, loss=1.592373013496399
I0218 02:38:14.269606 140379146925824 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.8358457088470459, loss=1.6155749559402466
I0218 02:39:29.770528 140379138533120 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.6789433360099792, loss=1.622249722480774
I0218 02:40:45.261190 140379146925824 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.7195215225219727, loss=1.5980627536773682
I0218 02:42:07.827837 140379146925824 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.6681296825408936, loss=1.495927095413208
I0218 02:43:23.642959 140379138533120 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.7610431909561157, loss=1.6392271518707275
I0218 02:44:39.430585 140379146925824 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.7508426904678345, loss=1.5749207735061646
I0218 02:45:55.187912 140379138533120 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.6808169484138489, loss=1.6716231107711792
I0218 02:47:10.910250 140379146925824 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.7355743646621704, loss=1.5796406269073486
I0218 02:48:26.902415 140379138533120 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.6123937368392944, loss=1.5382931232452393
I0218 02:49:42.707624 140379146925824 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.7164494395256042, loss=1.580747127532959
I0218 02:50:58.457977 140379138533120 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.779797375202179, loss=1.6372050046920776
I0218 02:52:16.110532 140379146925824 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.6089356541633606, loss=1.5938856601715088
I0218 02:53:37.065208 140379138533120 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.6442447900772095, loss=1.5255242586135864
I0218 02:54:55.352677 140549388556096 spec.py:321] Evaluating on the training split.
I0218 02:55:49.626392 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 02:56:39.230149 140549388556096 spec.py:349] Evaluating on the test split.
I0218 02:57:04.605941 140549388556096 submission_runner.py:408] Time since start: 9615.46s, 	Step: 11299, 	{'train/ctc_loss': Array(0.4041753, dtype=float32), 'train/wer': 0.14432038239326545, 'validation/ctc_loss': Array(0.6717132, dtype=float32), 'validation/wer': 0.20635855450534385, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4285467, dtype=float32), 'test/wer': 0.14736051022688035, 'test/num_examples': 2472, 'score': 8702.930635690689, 'total_duration': 9615.45825767517, 'accumulated_submission_time': 8702.930635690689, 'accumulated_eval_time': 911.7616634368896, 'accumulated_logging_time': 0.29471397399902344}
I0218 02:57:04.645094 140379146925824 logging_writer.py:48] [11299] accumulated_eval_time=911.761663, accumulated_logging_time=0.294714, accumulated_submission_time=8702.930636, global_step=11299, preemption_count=0, score=8702.930636, test/ctc_loss=0.4285466969013214, test/num_examples=2472, test/wer=0.147361, total_duration=9615.458258, train/ctc_loss=0.4041753113269806, train/wer=0.144320, validation/ctc_loss=0.6717131733894348, validation/num_examples=5348, validation/wer=0.206359
I0218 02:57:06.263641 140379138533120 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.7494103312492371, loss=1.5731817483901978
I0218 02:58:24.891902 140379146925824 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.6786395311355591, loss=1.5582081079483032
I0218 02:59:40.349664 140379138533120 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.6946113705635071, loss=1.5257340669631958
I0218 03:00:55.887438 140379146925824 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.7311601638793945, loss=1.523370623588562
I0218 03:02:11.549127 140379138533120 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.6046242713928223, loss=1.5662211179733276
I0218 03:03:27.300690 140379146925824 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.7530354261398315, loss=1.485422134399414
I0218 03:04:42.871950 140379138533120 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.6772758364677429, loss=1.5232136249542236
I0218 03:05:58.509622 140379146925824 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.7092727422714233, loss=1.624828815460205
I0218 03:07:17.329473 140379138533120 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.6983859539031982, loss=1.5822417736053467
I0218 03:08:39.124954 140379146925824 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.820141613483429, loss=1.519365906715393
I0218 03:09:59.630285 140379138533120 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.6776072382926941, loss=1.5809398889541626
I0218 03:11:20.406396 140379146925824 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.592535138130188, loss=1.492564082145691
I0218 03:12:35.833384 140379138533120 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.6529820561408997, loss=1.4876980781555176
I0218 03:13:51.302516 140379146925824 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.6504045724868774, loss=1.476325511932373
I0218 03:15:06.903302 140379138533120 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.6396257281303406, loss=1.524077296257019
I0218 03:16:22.351893 140379146925824 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.6330906748771667, loss=1.462428331375122
I0218 03:17:38.024388 140379138533120 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.7503719925880432, loss=1.5601164102554321
I0218 03:18:53.503792 140379146925824 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.8579195737838745, loss=1.5060462951660156
I0218 03:20:09.133040 140379138533120 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.8211451768875122, loss=1.5016975402832031
I0218 03:21:05.253755 140549388556096 spec.py:321] Evaluating on the training split.
I0218 03:21:59.965770 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 03:22:50.374960 140549388556096 spec.py:349] Evaluating on the test split.
I0218 03:23:16.165235 140549388556096 submission_runner.py:408] Time since start: 11187.02s, 	Step: 13172, 	{'train/ctc_loss': Array(0.34445938, dtype=float32), 'train/wer': 0.12448152638631868, 'validation/ctc_loss': Array(0.6240188, dtype=float32), 'validation/wer': 0.1905442327929946, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3915885, dtype=float32), 'test/wer': 0.13375175187374322, 'test/num_examples': 2472, 'score': 10143.447443723679, 'total_duration': 11187.017598867416, 'accumulated_submission_time': 10143.447443723679, 'accumulated_eval_time': 1042.6672360897064, 'accumulated_logging_time': 0.353374719619751}
I0218 03:23:16.199291 140379146925824 logging_writer.py:48] [13172] accumulated_eval_time=1042.667236, accumulated_logging_time=0.353375, accumulated_submission_time=10143.447444, global_step=13172, preemption_count=0, score=10143.447444, test/ctc_loss=0.3915885090827942, test/num_examples=2472, test/wer=0.133752, total_duration=11187.017599, train/ctc_loss=0.3444593846797943, train/wer=0.124482, validation/ctc_loss=0.6240187883377075, validation/num_examples=5348, validation/wer=0.190544
I0218 03:23:38.104703 140379138533120 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.6836460828781128, loss=1.4802883863449097
I0218 03:24:53.637333 140379146925824 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.715053379535675, loss=1.4017311334609985
I0218 03:26:12.660996 140379146925824 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.9778838753700256, loss=1.5150984525680542
I0218 03:27:28.223015 140379138533120 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.7607376575469971, loss=1.4785010814666748
I0218 03:28:43.748745 140379146925824 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.7991004586219788, loss=1.4654114246368408
I0218 03:29:59.275551 140379138533120 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.6437007188796997, loss=1.5482903718948364
I0218 03:31:14.724346 140379146925824 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.7863792777061462, loss=1.522589921951294
I0218 03:32:30.284349 140379138533120 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.6452538371086121, loss=1.472165822982788
I0218 03:33:46.150524 140379146925824 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.7133322358131409, loss=1.5211915969848633
I0218 03:35:02.243241 140379138533120 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.6687904000282288, loss=1.4728960990905762
I0218 03:36:21.656263 140379146925824 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.6915798187255859, loss=1.4989410638809204
I0218 03:37:42.218799 140379138533120 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.6186760663986206, loss=1.5304864645004272
I0218 03:39:03.021993 140379146925824 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.6615489721298218, loss=1.426361322402954
I0218 03:40:22.514454 140379146925824 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.653225839138031, loss=1.5005323886871338
I0218 03:41:37.959511 140379138533120 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.5709813833236694, loss=1.482542634010315
I0218 03:42:53.577849 140379146925824 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.6602782607078552, loss=1.482637643814087
I0218 03:44:09.221990 140379138533120 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.6502349972724915, loss=1.4859744310379028
I0218 03:45:24.816771 140379146925824 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.6738036274909973, loss=1.4553520679473877
I0218 03:46:40.751363 140379138533120 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.6356707811355591, loss=1.4286357164382935
I0218 03:47:16.660446 140549388556096 spec.py:321] Evaluating on the training split.
I0218 03:48:11.090153 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 03:49:01.224820 140549388556096 spec.py:349] Evaluating on the test split.
I0218 03:49:26.421228 140549388556096 submission_runner.py:408] Time since start: 12757.27s, 	Step: 15049, 	{'train/ctc_loss': Array(0.29312336, dtype=float32), 'train/wer': 0.10872877609922255, 'validation/ctc_loss': Array(0.5906545, dtype=float32), 'validation/wer': 0.1808702704268322, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36472577, dtype=float32), 'test/wer': 0.12491621473401987, 'test/num_examples': 2472, 'score': 11583.814504861832, 'total_duration': 12757.273002386093, 'accumulated_submission_time': 11583.814504861832, 'accumulated_eval_time': 1172.4215152263641, 'accumulated_logging_time': 0.40801548957824707}
I0218 03:49:26.454771 140379146925824 logging_writer.py:48] [15049] accumulated_eval_time=1172.421515, accumulated_logging_time=0.408015, accumulated_submission_time=11583.814505, global_step=15049, preemption_count=0, score=11583.814505, test/ctc_loss=0.3647257685661316, test/num_examples=2472, test/wer=0.124916, total_duration=12757.273002, train/ctc_loss=0.29312336444854736, train/wer=0.108729, validation/ctc_loss=0.5906544923782349, validation/num_examples=5348, validation/wer=0.180870
I0218 03:50:05.606336 140379138533120 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.7589359283447266, loss=1.4496738910675049
I0218 03:51:21.091898 140379146925824 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.6962511539459229, loss=1.5065051317214966
I0218 03:52:36.769746 140379138533120 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.6419395804405212, loss=1.4509568214416504
I0218 03:53:52.276180 140379146925824 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.6128853559494019, loss=1.5101491212844849
I0218 03:55:11.073732 140379146925824 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.6513035297393799, loss=1.4463104009628296
I0218 03:56:26.673388 140379138533120 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.7202708125114441, loss=1.3965708017349243
I0218 03:57:42.299074 140379146925824 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.6477341651916504, loss=1.4548717737197876
I0218 03:58:57.864513 140379138533120 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.6232461333274841, loss=1.4547744989395142
I0218 04:00:13.443941 140379146925824 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.7166273593902588, loss=1.4695019721984863
I0218 04:01:29.176464 140379138533120 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.7207353115081787, loss=1.5004574060440063
I0218 04:02:44.881874 140379146925824 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.7482610940933228, loss=1.423388957977295
I0218 04:04:02.992248 140379138533120 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.7343113422393799, loss=1.3828016519546509
I0218 04:05:23.681146 140379146925824 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.6341321468353271, loss=1.4752720594406128
I0218 04:06:44.136550 140379138533120 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.6826658248901367, loss=1.4657257795333862
I0218 04:08:06.411596 140379146925824 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.7023965120315552, loss=1.4155327081680298
I0218 04:09:21.785474 140379138533120 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.6183854341506958, loss=1.4071106910705566
I0218 04:10:37.317101 140379146925824 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.71665358543396, loss=1.3869900703430176
I0218 04:11:52.823712 140379138533120 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.6449740529060364, loss=1.428221344947815
I0218 04:13:08.304307 140379146925824 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.7409661412239075, loss=1.4672273397445679
I0218 04:13:26.886645 140549388556096 spec.py:321] Evaluating on the training split.
I0218 04:14:20.585251 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 04:15:10.776633 140549388556096 spec.py:349] Evaluating on the test split.
I0218 04:15:36.049401 140549388556096 submission_runner.py:408] Time since start: 14326.90s, 	Step: 16926, 	{'train/ctc_loss': Array(0.27578232, dtype=float32), 'train/wer': 0.10147949927913645, 'validation/ctc_loss': Array(0.57158834, dtype=float32), 'validation/wer': 0.17360031667262038, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3517897, dtype=float32), 'test/wer': 0.11868055978713464, 'test/num_examples': 2472, 'score': 13024.158386707306, 'total_duration': 14326.901668548584, 'accumulated_submission_time': 13024.158386707306, 'accumulated_eval_time': 1301.5782821178436, 'accumulated_logging_time': 0.4575080871582031}
I0218 04:15:36.087743 140379146925824 logging_writer.py:48] [16926] accumulated_eval_time=1301.578282, accumulated_logging_time=0.457508, accumulated_submission_time=13024.158387, global_step=16926, preemption_count=0, score=13024.158387, test/ctc_loss=0.3517897129058838, test/num_examples=2472, test/wer=0.118681, total_duration=14326.901669, train/ctc_loss=0.2757823169231415, train/wer=0.101479, validation/ctc_loss=0.5715883374214172, validation/num_examples=5348, validation/wer=0.173600
I0218 04:16:32.562472 140379138533120 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.7008281350135803, loss=1.4276974201202393
I0218 04:17:48.205645 140379146925824 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.7225885391235352, loss=1.4727057218551636
I0218 04:19:03.749578 140379138533120 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.8701343536376953, loss=1.4191175699234009
I0218 04:20:19.251410 140379146925824 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.7821441888809204, loss=1.4194188117980957
I0218 04:21:34.708755 140379138533120 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.7162795662879944, loss=1.4453129768371582
I0218 04:22:50.186125 140379146925824 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.646586537361145, loss=1.4999768733978271
I0218 04:24:08.858068 140379146925824 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.6609752774238586, loss=1.3685736656188965
I0218 04:25:24.342966 140379138533120 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.7525370121002197, loss=1.4245370626449585
I0218 04:26:39.842118 140379146925824 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.6180917620658875, loss=1.418491244316101
I0218 04:27:55.347140 140379138533120 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.6693434715270996, loss=1.4431759119033813
I0218 04:29:10.851289 140379146925824 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.7239468097686768, loss=1.4564582109451294
I0218 04:30:26.280361 140379138533120 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.7145687937736511, loss=1.3792908191680908
I0218 04:31:42.003895 140379146925824 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.7208502888679504, loss=1.4544304609298706
I0218 04:33:00.469612 140379138533120 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.6253582835197449, loss=1.3986997604370117
I0218 04:34:20.729013 140379146925824 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.6539284586906433, loss=1.3705211877822876
I0218 04:35:40.898293 140379138533120 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.6494299173355103, loss=1.4424316883087158
I0218 04:37:01.456530 140379146925824 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.6839800477027893, loss=1.4565250873565674
I0218 04:38:17.002997 140379138533120 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.7005792260169983, loss=1.3764288425445557
I0218 04:39:32.528308 140379146925824 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.63077312707901, loss=1.4195939302444458
I0218 04:39:36.757210 140549388556096 spec.py:321] Evaluating on the training split.
I0218 04:40:30.143729 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 04:41:20.349936 140549388556096 spec.py:349] Evaluating on the test split.
I0218 04:41:45.868526 140549388556096 submission_runner.py:408] Time since start: 15896.72s, 	Step: 18807, 	{'train/ctc_loss': Array(0.26276767, dtype=float32), 'train/wer': 0.09987826264164705, 'validation/ctc_loss': Array(0.5514074, dtype=float32), 'validation/wer': 0.16706411655097175, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3391219, dtype=float32), 'test/wer': 0.11443543964414113, 'test/num_examples': 2472, 'score': 14464.73980808258, 'total_duration': 15896.72179198265, 'accumulated_submission_time': 14464.73980808258, 'accumulated_eval_time': 1430.684591293335, 'accumulated_logging_time': 0.510749340057373}
I0218 04:41:45.903352 140379146925824 logging_writer.py:48] [18807] accumulated_eval_time=1430.684591, accumulated_logging_time=0.510749, accumulated_submission_time=14464.739808, global_step=18807, preemption_count=0, score=14464.739808, test/ctc_loss=0.33912190794944763, test/num_examples=2472, test/wer=0.114435, total_duration=15896.721792, train/ctc_loss=0.2627676725387573, train/wer=0.099878, validation/ctc_loss=0.5514073967933655, validation/num_examples=5348, validation/wer=0.167064
I0218 04:42:56.617255 140379138533120 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.7063847184181213, loss=1.3616958856582642
I0218 04:44:12.104430 140379146925824 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.6692885160446167, loss=1.4224073886871338
I0218 04:45:27.692950 140379138533120 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.6435986161231995, loss=1.3749803304672241
I0218 04:46:43.212448 140379146925824 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.6454998254776001, loss=1.3991154432296753
I0218 04:47:58.928886 140379138533120 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.6813215613365173, loss=1.4296200275421143
I0218 04:49:14.389633 140379146925824 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.6553459167480469, loss=1.3423587083816528
I0218 04:50:32.864534 140379138533120 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.7831166386604309, loss=1.4537203311920166
I0218 04:51:55.126182 140379146925824 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.6798178553581238, loss=1.316611647605896
I0218 04:53:10.626156 140379138533120 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.7127328515052795, loss=1.3784204721450806
I0218 04:54:26.114211 140379146925824 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.7684412002563477, loss=1.3825610876083374
I0218 04:55:41.578408 140379138533120 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.658329427242279, loss=1.351122260093689
I0218 04:56:57.109695 140379146925824 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.6702343821525574, loss=1.3254365921020508
I0218 04:58:12.578284 140379138533120 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.6448129415512085, loss=1.4119982719421387
I0218 04:59:28.203227 140379146925824 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.6818333268165588, loss=1.3235820531845093
I0218 05:00:43.899571 140379138533120 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.6752532720565796, loss=1.39909827709198
I0218 05:02:04.148319 140379146925824 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.7330119609832764, loss=1.3788683414459229
I0218 05:03:24.875218 140379138533120 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.7162824869155884, loss=1.4034550189971924
I0218 05:04:48.605996 140379146925824 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.5972771644592285, loss=1.3224483728408813
I0218 05:05:46.403506 140549388556096 spec.py:321] Evaluating on the training split.
I0218 05:06:41.104176 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 05:07:31.801901 140549388556096 spec.py:349] Evaluating on the test split.
I0218 05:07:57.150264 140549388556096 submission_runner.py:408] Time since start: 17468.00s, 	Step: 20678, 	{'train/ctc_loss': Array(0.2670035, dtype=float32), 'train/wer': 0.09698828631111346, 'validation/ctc_loss': Array(0.5288567, dtype=float32), 'validation/wer': 0.15960106973555904, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32171547, dtype=float32), 'test/wer': 0.10734669835273089, 'test/num_examples': 2472, 'score': 15905.15338897705, 'total_duration': 17468.002720832825, 'accumulated_submission_time': 15905.15338897705, 'accumulated_eval_time': 1561.4255406856537, 'accumulated_logging_time': 0.5602102279663086}
I0218 05:07:57.187960 140379146925824 logging_writer.py:48] [20678] accumulated_eval_time=1561.425541, accumulated_logging_time=0.560210, accumulated_submission_time=15905.153389, global_step=20678, preemption_count=0, score=15905.153389, test/ctc_loss=0.32171547412872314, test/num_examples=2472, test/wer=0.107347, total_duration=17468.002721, train/ctc_loss=0.26700350642204285, train/wer=0.096988, validation/ctc_loss=0.5288566946983337, validation/num_examples=5348, validation/wer=0.159601
I0218 05:08:14.495628 140379138533120 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.7029579877853394, loss=1.396764874458313
I0218 05:09:29.678977 140379146925824 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.6321653127670288, loss=1.3395670652389526
I0218 05:10:45.020104 140379138533120 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.64468914270401, loss=1.3645013570785522
I0218 05:12:00.365075 140379146925824 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.6490954160690308, loss=1.4327943325042725
I0218 05:13:15.821242 140379138533120 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.5970097780227661, loss=1.3582358360290527
I0218 05:14:31.310151 140379146925824 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.7332035303115845, loss=1.3746453523635864
I0218 05:15:46.812409 140379138533120 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.822960376739502, loss=1.334568977355957
I0218 05:17:02.530216 140379146925824 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.6149319410324097, loss=1.3764005899429321
I0218 05:18:17.993734 140379138533120 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.6235713958740234, loss=1.3499659299850464
I0218 05:19:38.127030 140379146925824 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.6245569586753845, loss=1.3202464580535889
I0218 05:20:57.987647 140379146925824 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.6829004883766174, loss=1.3464313745498657
I0218 05:22:13.443669 140379138533120 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.6268675327301025, loss=1.356795072555542
I0218 05:23:28.861243 140379146925824 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.7247318029403687, loss=1.3747655153274536
I0218 05:24:44.385214 140379138533120 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.8178211450576782, loss=1.3934625387191772
I0218 05:25:59.914144 140379146925824 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.7397034764289856, loss=1.3601990938186646
I0218 05:27:15.379966 140379138533120 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.9124345779418945, loss=1.373430609703064
I0218 05:28:30.852864 140379146925824 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.6823542714118958, loss=1.3674014806747437
I0218 05:29:49.526410 140379138533120 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.6923335790634155, loss=1.3765015602111816
I0218 05:31:10.454434 140379146925824 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.5623005628585815, loss=1.3508894443511963
I0218 05:31:57.580130 140549388556096 spec.py:321] Evaluating on the training split.
I0218 05:32:52.152176 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 05:33:43.036947 140549388556096 spec.py:349] Evaluating on the test split.
I0218 05:34:08.820133 140549388556096 submission_runner.py:408] Time since start: 19039.67s, 	Step: 22560, 	{'train/ctc_loss': Array(0.2625285, dtype=float32), 'train/wer': 0.09521196612762814, 'validation/ctc_loss': Array(0.5192188, dtype=float32), 'validation/wer': 0.15608677602170365, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31339175, dtype=float32), 'test/wer': 0.10649361200820588, 'test/num_examples': 2472, 'score': 17345.456032276154, 'total_duration': 19039.672203540802, 'accumulated_submission_time': 17345.456032276154, 'accumulated_eval_time': 1692.6593585014343, 'accumulated_logging_time': 0.6153922080993652}
I0218 05:34:08.857226 140379146925824 logging_writer.py:48] [22560] accumulated_eval_time=1692.659359, accumulated_logging_time=0.615392, accumulated_submission_time=17345.456032, global_step=22560, preemption_count=0, score=17345.456032, test/ctc_loss=0.3133917450904846, test/num_examples=2472, test/wer=0.106494, total_duration=19039.672204, train/ctc_loss=0.26252850890159607, train/wer=0.095212, validation/ctc_loss=0.5192188024520874, validation/num_examples=5348, validation/wer=0.156087
I0218 05:34:39.707949 140379138533120 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.5958206057548523, loss=1.3189533948898315
I0218 05:35:58.357797 140379146925824 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.7233641743659973, loss=1.3177967071533203
I0218 05:37:13.771182 140379138533120 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.6923137903213501, loss=1.3893942832946777
I0218 05:38:29.207873 140379146925824 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.6532291173934937, loss=1.3393391370773315
I0218 05:39:44.928314 140379138533120 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.7389248013496399, loss=1.3521432876586914
I0218 05:41:00.434067 140379146925824 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.6303170919418335, loss=1.338745355606079
I0218 05:42:16.061372 140379138533120 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.602461040019989, loss=1.3348571062088013
I0218 05:43:31.659144 140379146925824 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.797646701335907, loss=1.3648326396942139
I0218 05:44:49.584674 140379138533120 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.5956313610076904, loss=1.2854082584381104
I0218 05:46:10.097548 140379146925824 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.7187167406082153, loss=1.3358125686645508
I0218 05:47:30.898098 140379138533120 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.6832554936408997, loss=1.2874614000320435
I0218 05:48:54.103467 140379146925824 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.604241669178009, loss=1.3273208141326904
I0218 05:50:09.584951 140379138533120 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.699073076248169, loss=1.360958218574524
I0218 05:51:25.066963 140379146925824 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.6906914710998535, loss=1.3225162029266357
I0218 05:52:40.493666 140379138533120 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.715467095375061, loss=1.3676354885101318
I0218 05:53:55.938684 140379146925824 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.6264015436172485, loss=1.3678324222564697
I0218 05:55:11.451303 140379138533120 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.7433263063430786, loss=1.3733546733856201
I0218 05:56:26.921902 140379146925824 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.8229473233222961, loss=1.355655312538147
I0218 05:57:42.383267 140379138533120 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.647411584854126, loss=1.3471170663833618
I0218 05:58:09.056715 140549388556096 spec.py:321] Evaluating on the training split.
I0218 05:59:04.526961 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 05:59:54.548344 140549388556096 spec.py:349] Evaluating on the test split.
I0218 06:00:20.162790 140549388556096 submission_runner.py:408] Time since start: 20611.02s, 	Step: 24436, 	{'train/ctc_loss': Array(0.2446675, dtype=float32), 'train/wer': 0.08934561168844532, 'validation/ctc_loss': Array(0.50914836, dtype=float32), 'validation/wer': 0.15377931394035355, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30653575, dtype=float32), 'test/wer': 0.10352812138199988, 'test/num_examples': 2472, 'score': 18785.565538167953, 'total_duration': 20611.015005350113, 'accumulated_submission_time': 18785.565538167953, 'accumulated_eval_time': 1823.7593805789948, 'accumulated_logging_time': 0.6687924861907959}
I0218 06:00:20.203073 140379146925824 logging_writer.py:48] [24436] accumulated_eval_time=1823.759381, accumulated_logging_time=0.668792, accumulated_submission_time=18785.565538, global_step=24436, preemption_count=0, score=18785.565538, test/ctc_loss=0.3065357506275177, test/num_examples=2472, test/wer=0.103528, total_duration=20611.015005, train/ctc_loss=0.24466750025749207, train/wer=0.089346, validation/ctc_loss=0.509148359298706, validation/num_examples=5348, validation/wer=0.153779
I0218 06:01:09.081140 140379138533120 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.7262467741966248, loss=1.376729965209961
I0218 06:02:24.758070 140379146925824 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.6290490627288818, loss=1.329149603843689
I0218 06:03:40.190177 140379138533120 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.8196430802345276, loss=1.3524212837219238
I0218 06:04:59.048954 140379146925824 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.9686102867126465, loss=1.3068718910217285
I0218 06:06:14.555392 140379138533120 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.6828154921531677, loss=1.342126727104187
I0218 06:07:30.162770 140379146925824 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.6532573103904724, loss=1.328567624092102
I0218 06:08:45.753911 140379138533120 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.9241829514503479, loss=1.3734734058380127
I0218 06:10:01.226698 140379146925824 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.7054985165596008, loss=1.379382610321045
I0218 06:11:16.729458 140379138533120 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.694242537021637, loss=1.3595391511917114
I0218 06:12:33.497001 140379146925824 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.797736406326294, loss=1.310126543045044
I0218 06:13:54.235235 140379138533120 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.6032353639602661, loss=1.2829785346984863
I0218 06:15:15.448320 140379146925824 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.9372957348823547, loss=1.3531080484390259
I0218 06:16:35.532254 140379138533120 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.76719069480896, loss=1.2577407360076904
I0218 06:17:56.703351 140379146925824 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.5988412499427795, loss=1.292959213256836
I0218 06:19:12.155964 140379138533120 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.7023634910583496, loss=1.3273178339004517
I0218 06:20:27.643732 140379146925824 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.715675950050354, loss=1.350772738456726
I0218 06:21:43.099189 140379138533120 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.6628085374832153, loss=1.3350274562835693
I0218 06:22:58.569985 140379146925824 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.7802979946136475, loss=1.2936441898345947
I0218 06:24:14.078628 140379138533120 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.6641071438789368, loss=1.3151500225067139
I0218 06:24:20.601208 140549388556096 spec.py:321] Evaluating on the training split.
I0218 06:25:14.985674 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 06:26:05.150410 140549388556096 spec.py:349] Evaluating on the test split.
I0218 06:26:30.632197 140549388556096 submission_runner.py:408] Time since start: 22181.48s, 	Step: 26310, 	{'train/ctc_loss': Array(0.22402444, dtype=float32), 'train/wer': 0.08299375026392466, 'validation/ctc_loss': Array(0.4923003, dtype=float32), 'validation/wer': 0.14871062108383135, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29196113, dtype=float32), 'test/wer': 0.09853147279263909, 'test/num_examples': 2472, 'score': 20225.87024140358, 'total_duration': 22181.484354496002, 'accumulated_submission_time': 20225.87024140358, 'accumulated_eval_time': 1953.7842502593994, 'accumulated_logging_time': 0.7299957275390625}
I0218 06:26:30.674804 140379146925824 logging_writer.py:48] [26310] accumulated_eval_time=1953.784250, accumulated_logging_time=0.729996, accumulated_submission_time=20225.870241, global_step=26310, preemption_count=0, score=20225.870241, test/ctc_loss=0.291961133480072, test/num_examples=2472, test/wer=0.098531, total_duration=22181.484354, train/ctc_loss=0.2240244448184967, train/wer=0.082994, validation/ctc_loss=0.4923003017902374, validation/num_examples=5348, validation/wer=0.148711
I0218 06:27:39.166466 140379138533120 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.6773408651351929, loss=1.3854820728302002
I0218 06:28:54.673303 140379146925824 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.6686317324638367, loss=1.2869501113891602
I0218 06:30:10.296935 140379138533120 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.7153801918029785, loss=1.2742087841033936
I0218 06:31:26.039135 140379146925824 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.6343708634376526, loss=1.363330364227295
I0218 06:32:44.737469 140379146925824 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.5957148671150208, loss=1.2451916933059692
I0218 06:34:00.116034 140379138533120 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.7361804246902466, loss=1.340547800064087
I0218 06:35:15.593194 140379146925824 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.7243286967277527, loss=1.2255501747131348
I0218 06:36:31.074014 140379138533120 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.7840027809143066, loss=1.3063863515853882
I0218 06:37:46.674742 140379146925824 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.6893768310546875, loss=1.3105226755142212
I0218 06:39:02.395292 140379138533120 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.5899552702903748, loss=1.318520188331604
I0218 06:40:17.912274 140379146925824 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.6941704154014587, loss=1.3104608058929443
I0218 06:41:33.992690 140379138533120 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.5759943127632141, loss=1.2421982288360596
I0218 06:42:54.318396 140379146925824 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.7174438834190369, loss=1.359971523284912
I0218 06:44:14.966440 140379138533120 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.6705482602119446, loss=1.3024049997329712
I0218 06:45:35.232790 140379146925824 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.7467010021209717, loss=1.3378111124038696
I0218 06:46:54.529582 140379146925824 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.6854885816574097, loss=1.3194218873977661
I0218 06:48:10.093362 140379138533120 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.6843506693840027, loss=1.2662262916564941
I0218 06:49:25.624483 140379146925824 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.6753523945808411, loss=1.3129160404205322
I0218 06:50:30.941038 140549388556096 spec.py:321] Evaluating on the training split.
I0218 06:51:25.130435 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 06:52:15.498501 140549388556096 spec.py:349] Evaluating on the test split.
I0218 06:52:40.937312 140549388556096 submission_runner.py:408] Time since start: 23751.79s, 	Step: 28188, 	{'train/ctc_loss': Array(0.21112266, dtype=float32), 'train/wer': 0.0805446151438483, 'validation/ctc_loss': Array(0.47902232, dtype=float32), 'validation/wer': 0.14473290402309394, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28334022, dtype=float32), 'test/wer': 0.0969268580017468, 'test/num_examples': 2472, 'score': 21666.047757864, 'total_duration': 23751.79039502144, 'accumulated_submission_time': 21666.047757864, 'accumulated_eval_time': 2083.775384426117, 'accumulated_logging_time': 0.7895841598510742}
I0218 06:52:40.974506 140379146925824 logging_writer.py:48] [28188] accumulated_eval_time=2083.775384, accumulated_logging_time=0.789584, accumulated_submission_time=21666.047758, global_step=28188, preemption_count=0, score=21666.047758, test/ctc_loss=0.2833402156829834, test/num_examples=2472, test/wer=0.096927, total_duration=23751.790395, train/ctc_loss=0.21112266182899475, train/wer=0.080545, validation/ctc_loss=0.4790223240852356, validation/num_examples=5348, validation/wer=0.144733
I0218 06:52:50.808219 140379138533120 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.6503354907035828, loss=1.2333317995071411
I0218 06:54:06.118490 140379146925824 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.7585872411727905, loss=1.2729510068893433
I0218 06:55:21.562093 140379138533120 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.8013397455215454, loss=1.3181744813919067
I0218 06:56:37.073027 140379146925824 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.6705776453018188, loss=1.3017069101333618
I0218 06:57:52.529080 140379138533120 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.720703661441803, loss=1.3050785064697266
I0218 06:59:08.083271 140379146925824 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.6632137894630432, loss=1.2938523292541504
I0218 07:00:25.331091 140379138533120 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.6308820843696594, loss=1.2533658742904663
I0218 07:01:45.973304 140379146925824 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.7288371324539185, loss=1.3310132026672363
I0218 07:03:01.426328 140379138533120 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.7615819573402405, loss=1.3091564178466797
I0218 07:04:16.974332 140379146925824 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.6764912605285645, loss=1.2875531911849976
I0218 07:05:32.467832 140379138533120 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.785363495349884, loss=1.3057435750961304
I0218 07:06:47.993602 140379146925824 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.7698339223861694, loss=1.311556100845337
I0218 07:08:03.463329 140379138533120 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.6591325402259827, loss=1.290966510772705
I0218 07:09:18.863880 140379146925824 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.6390738487243652, loss=1.318769931793213
I0218 07:10:37.099653 140379138533120 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.6341791749000549, loss=1.3068530559539795
I0218 07:11:57.745382 140379146925824 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.72303706407547, loss=1.307348608970642
I0218 07:13:17.992239 140379138533120 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.7892058491706848, loss=1.2869257926940918
I0218 07:14:39.858036 140379146925824 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.7928069233894348, loss=1.2562153339385986
I0218 07:15:55.236366 140379138533120 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.8110248446464539, loss=1.2619985342025757
I0218 07:16:40.983708 140549388556096 spec.py:321] Evaluating on the training split.
I0218 07:17:35.751807 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 07:18:25.608759 140549388556096 spec.py:349] Evaluating on the test split.
I0218 07:18:51.104049 140549388556096 submission_runner.py:408] Time since start: 25321.96s, 	Step: 30062, 	{'train/ctc_loss': Array(0.21464497, dtype=float32), 'train/wer': 0.07797813257509896, 'validation/ctc_loss': Array(0.48762044, dtype=float32), 'validation/wer': 0.14551493092095735, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2829486, dtype=float32), 'test/wer': 0.09359575894217294, 'test/num_examples': 2472, 'score': 23105.966525793076, 'total_duration': 25321.955672502518, 'accumulated_submission_time': 23105.966525793076, 'accumulated_eval_time': 2213.889081478119, 'accumulated_logging_time': 0.8440456390380859}
I0218 07:18:51.142732 140379146925824 logging_writer.py:48] [30062] accumulated_eval_time=2213.889081, accumulated_logging_time=0.844046, accumulated_submission_time=23105.966526, global_step=30062, preemption_count=0, score=23105.966526, test/ctc_loss=0.2829486131668091, test/num_examples=2472, test/wer=0.093596, total_duration=25321.955673, train/ctc_loss=0.21464496850967407, train/wer=0.077978, validation/ctc_loss=0.48762044310569763, validation/num_examples=5348, validation/wer=0.145515
I0218 07:19:20.607393 140379138533120 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.8217154145240784, loss=1.3249669075012207
I0218 07:20:36.152831 140379146925824 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.5939515233039856, loss=1.2222051620483398
I0218 07:21:51.779048 140379138533120 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.8331717252731323, loss=1.2976866960525513
I0218 07:23:07.450560 140379146925824 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.787553608417511, loss=1.3215678930282593
I0218 07:24:23.129102 140379138533120 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.687900185585022, loss=1.2867555618286133
I0218 07:25:38.690071 140379146925824 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.7842300534248352, loss=1.3008512258529663
I0218 07:26:54.426651 140379138533120 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.6442821025848389, loss=1.2931352853775024
I0218 07:28:10.117114 140379146925824 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.613326370716095, loss=1.3258315324783325
I0218 07:29:31.669112 140379146925824 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.6752516627311707, loss=1.2486525774002075
I0218 07:30:47.243590 140379138533120 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.6505810618400574, loss=1.2808470726013184
I0218 07:32:02.663634 140379146925824 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.7943914532661438, loss=1.2385139465332031
I0218 07:33:18.124886 140379138533120 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.6060241460800171, loss=1.2879465818405151
I0218 07:34:33.564112 140379146925824 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.6795867681503296, loss=1.2335844039916992
I0218 07:35:49.065831 140379138533120 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.6559964418411255, loss=1.254549503326416
I0218 07:37:04.430981 140379146925824 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.689629852771759, loss=1.2549680471420288
I0218 07:38:20.025999 140379138533120 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.8308316469192505, loss=1.2597934007644653
I0218 07:39:39.926109 140379146925824 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.7544371485710144, loss=1.2751749753952026
I0218 07:41:00.147028 140379138533120 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.7066026329994202, loss=1.2777503728866577
I0218 07:42:20.857153 140379146925824 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.6506344676017761, loss=1.2734508514404297
I0218 07:42:51.735205 140549388556096 spec.py:321] Evaluating on the training split.
I0218 07:43:46.979695 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 07:44:37.151799 140549388556096 spec.py:349] Evaluating on the test split.
I0218 07:45:02.294600 140549388556096 submission_runner.py:408] Time since start: 26893.15s, 	Step: 31936, 	{'train/ctc_loss': Array(0.22176528, dtype=float32), 'train/wer': 0.08092823712948517, 'validation/ctc_loss': Array(0.46996224, dtype=float32), 'validation/wer': 0.13909458663602922, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27928066, dtype=float32), 'test/wer': 0.0920114557309122, 'test/num_examples': 2472, 'score': 24546.469081401825, 'total_duration': 26893.146836042404, 'accumulated_submission_time': 24546.469081401825, 'accumulated_eval_time': 2344.442668914795, 'accumulated_logging_time': 0.900031566619873}
I0218 07:45:02.338588 140379146925824 logging_writer.py:48] [31936] accumulated_eval_time=2344.442669, accumulated_logging_time=0.900032, accumulated_submission_time=24546.469081, global_step=31936, preemption_count=0, score=24546.469081, test/ctc_loss=0.2792806625366211, test/num_examples=2472, test/wer=0.092011, total_duration=26893.146836, train/ctc_loss=0.22176527976989746, train/wer=0.080928, validation/ctc_loss=0.4699622392654419, validation/num_examples=5348, validation/wer=0.139095
I0218 07:45:51.412080 140379138533120 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.6773237586021423, loss=1.2360806465148926
I0218 07:47:06.818538 140379146925824 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.7703465819358826, loss=1.2732231616973877
I0218 07:48:22.246312 140379138533120 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.8513933420181274, loss=1.2489053010940552
I0218 07:49:37.770247 140379146925824 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.7988112568855286, loss=1.2990429401397705
I0218 07:50:53.269232 140379138533120 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.7318137884140015, loss=1.2959147691726685
I0218 07:52:08.845106 140379146925824 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.5417265295982361, loss=1.2589526176452637
I0218 07:53:24.383116 140379138533120 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.6350395083427429, loss=1.1974016427993774
I0218 07:54:39.754888 140379146925824 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.7526317238807678, loss=1.2332414388656616
I0218 07:55:55.235550 140379138533120 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.6696928143501282, loss=1.2271063327789307
I0218 07:57:10.745712 140379146925824 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.6486510038375854, loss=1.3110672235488892
I0218 07:58:31.752460 140379146925824 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.6234920620918274, loss=1.22990882396698
I0218 07:59:47.335528 140379138533120 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.6345775723457336, loss=1.178922414779663
I0218 08:01:02.863378 140379146925824 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.8788072466850281, loss=1.3094450235366821
I0218 08:02:18.378526 140379138533120 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.7401940226554871, loss=1.2963300943374634
I0218 08:03:33.895221 140379146925824 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.6568101644515991, loss=1.2711914777755737
I0218 08:04:49.367610 140379138533120 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.6868495345115662, loss=1.2947641611099243
I0218 08:06:04.905218 140379146925824 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.6445831060409546, loss=1.2494730949401855
I0218 08:07:20.892061 140379138533120 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.7354524731636047, loss=1.2318631410598755
I0218 08:08:41.333451 140379146925824 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.7868837118148804, loss=1.2733513116836548
I0218 08:09:02.305280 140549388556096 spec.py:321] Evaluating on the training split.
I0218 08:09:56.294517 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 08:10:46.507294 140549388556096 spec.py:349] Evaluating on the test split.
I0218 08:11:12.054141 140549388556096 submission_runner.py:408] Time since start: 28462.91s, 	Step: 33828, 	{'train/ctc_loss': Array(0.20109509, dtype=float32), 'train/wer': 0.07282188183332519, 'validation/ctc_loss': Array(0.4561879, dtype=float32), 'validation/wer': 0.13637197447309732, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2653281, dtype=float32), 'test/wer': 0.08896471878618, 'test/num_examples': 2472, 'score': 25986.34561252594, 'total_duration': 28462.906080007553, 'accumulated_submission_time': 25986.34561252594, 'accumulated_eval_time': 2474.185190677643, 'accumulated_logging_time': 0.9610598087310791}
I0218 08:11:12.090514 140379146925824 logging_writer.py:48] [33828] accumulated_eval_time=2474.185191, accumulated_logging_time=0.961060, accumulated_submission_time=25986.345613, global_step=33828, preemption_count=0, score=25986.345613, test/ctc_loss=0.2653281092643738, test/num_examples=2472, test/wer=0.088965, total_duration=28462.906080, train/ctc_loss=0.2010950893163681, train/wer=0.072822, validation/ctc_loss=0.456187903881073, validation/num_examples=5348, validation/wer=0.136372
I0218 08:12:07.075423 140379138533120 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.806041419506073, loss=1.301080346107483
I0218 08:13:25.741647 140379146925824 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.6541802287101746, loss=1.281328797340393
I0218 08:14:41.436036 140379138533120 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.7703647613525391, loss=1.292404055595398
I0218 08:15:56.861617 140379146925824 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.6625988483428955, loss=1.239016056060791
I0218 08:17:12.306864 140379138533120 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.6960323452949524, loss=1.2127115726470947
I0218 08:18:27.841739 140379146925824 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.6089257597923279, loss=1.270660400390625
I0218 08:19:43.258233 140379138533120 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.6670946478843689, loss=1.234734058380127
I0218 08:20:58.793818 140379146925824 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.8850684762001038, loss=1.2656997442245483
I0218 08:22:15.502434 140379138533120 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.7372184991836548, loss=1.2612338066101074
I0218 08:23:35.852630 140379146925824 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.6865994930267334, loss=1.2398860454559326
I0218 08:24:56.768746 140379138533120 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.7666631937026978, loss=1.2474077939987183
I0218 08:26:17.139043 140379146925824 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.6782941818237305, loss=1.2442179918289185
I0218 08:27:36.446599 140379146925824 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.6411277055740356, loss=1.21280038356781
I0218 08:28:52.193782 140379138533120 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.8489034175872803, loss=1.207380771636963
I0218 08:30:07.621717 140379146925824 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.739206850528717, loss=1.2216567993164062
I0218 08:31:23.069939 140379138533120 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.8151676058769226, loss=1.2038111686706543
I0218 08:32:38.504081 140379146925824 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.8425366282463074, loss=1.1859413385391235
I0218 08:33:53.920031 140379138533120 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.6607286930084229, loss=1.238507866859436
I0218 08:35:09.425459 140379146925824 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.7215768098831177, loss=1.2451720237731934
I0218 08:35:12.139160 140549388556096 spec.py:321] Evaluating on the training split.
I0218 08:36:07.550204 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 08:36:58.361461 140549388556096 spec.py:349] Evaluating on the test split.
I0218 08:37:24.357812 140549388556096 submission_runner.py:408] Time since start: 30035.21s, 	Step: 35705, 	{'train/ctc_loss': Array(0.20980647, dtype=float32), 'train/wer': 0.07649394613420729, 'validation/ctc_loss': Array(0.45002848, dtype=float32), 'validation/wer': 0.13602440696293577, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26111528, dtype=float32), 'test/wer': 0.08673044502670972, 'test/num_examples': 2472, 'score': 27426.305513381958, 'total_duration': 30035.209615707397, 'accumulated_submission_time': 27426.305513381958, 'accumulated_eval_time': 2606.3973863124847, 'accumulated_logging_time': 1.0138370990753174}
I0218 08:37:24.395265 140379146925824 logging_writer.py:48] [35705] accumulated_eval_time=2606.397386, accumulated_logging_time=1.013837, accumulated_submission_time=27426.305513, global_step=35705, preemption_count=0, score=27426.305513, test/ctc_loss=0.26111528277397156, test/num_examples=2472, test/wer=0.086730, total_duration=30035.209616, train/ctc_loss=0.20980647206306458, train/wer=0.076494, validation/ctc_loss=0.4500284790992737, validation/num_examples=5348, validation/wer=0.136024
I0218 08:38:36.683206 140379138533120 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.6983329653739929, loss=1.3173654079437256
I0218 08:39:52.385784 140379146925824 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.7211272120475769, loss=1.2642008066177368
I0218 08:41:08.008013 140379138533120 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.7322747111320496, loss=1.2012633085250854
I0218 08:42:26.922404 140379146925824 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.7616519331932068, loss=1.1708101034164429
I0218 08:43:42.441855 140379138533120 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.7841300368309021, loss=1.2113240957260132
I0218 08:44:58.149013 140379146925824 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.7454650402069092, loss=1.2528008222579956
I0218 08:46:13.659305 140379138533120 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.6453592777252197, loss=1.1747024059295654
I0218 08:47:29.207239 140379146925824 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.7050313949584961, loss=1.2646465301513672
I0218 08:48:44.735313 140379138533120 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.6867943406105042, loss=1.2157154083251953
I0218 08:50:00.308213 140379146925824 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.7971532940864563, loss=1.258142113685608
I0218 08:51:16.681849 140379138533120 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.7490934729576111, loss=1.1783673763275146
I0218 08:52:37.178714 140379146925824 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.7949635982513428, loss=1.2659114599227905
I0218 08:53:56.870193 140379138533120 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.7730213403701782, loss=1.236794114112854
I0218 08:55:19.619968 140379146925824 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.7221242189407349, loss=1.1192363500595093
I0218 08:56:35.073253 140379138533120 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.815180242061615, loss=1.2206075191497803
I0218 08:57:50.768677 140379146925824 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.7382367849349976, loss=1.2048184871673584
I0218 08:59:06.275203 140379138533120 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.7471926212310791, loss=1.2252217531204224
I0218 09:00:21.736356 140379146925824 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.7086085677146912, loss=1.1917566061019897
I0218 09:01:24.728689 140549388556096 spec.py:321] Evaluating on the training split.
I0218 09:02:18.688463 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 09:03:09.234837 140549388556096 spec.py:349] Evaluating on the test split.
I0218 09:03:34.843637 140549388556096 submission_runner.py:408] Time since start: 31605.70s, 	Step: 37585, 	{'train/ctc_loss': Array(0.19981158, dtype=float32), 'train/wer': 0.07120325671110543, 'validation/ctc_loss': Array(0.4394024, dtype=float32), 'validation/wer': 0.1306178012493121, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2542946, dtype=float32), 'test/wer': 0.08490240285987041, 'test/num_examples': 2472, 'score': 28866.55081510544, 'total_duration': 31605.695909023285, 'accumulated_submission_time': 28866.55081510544, 'accumulated_eval_time': 2736.506334066391, 'accumulated_logging_time': 1.0663466453552246}
I0218 09:03:34.887230 140379146925824 logging_writer.py:48] [37585] accumulated_eval_time=2736.506334, accumulated_logging_time=1.066347, accumulated_submission_time=28866.550815, global_step=37585, preemption_count=0, score=28866.550815, test/ctc_loss=0.25429460406303406, test/num_examples=2472, test/wer=0.084902, total_duration=31605.695909, train/ctc_loss=0.19981157779693604, train/wer=0.071203, validation/ctc_loss=0.43940240144729614, validation/num_examples=5348, validation/wer=0.130618
I0218 09:03:46.956326 140379138533120 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.8211212754249573, loss=1.231911540031433
I0218 09:05:02.419733 140379146925824 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.8798284530639648, loss=1.2437909841537476
I0218 09:06:17.978750 140379138533120 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.7187614440917969, loss=1.2428150177001953
I0218 09:07:33.549612 140379146925824 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.7246658802032471, loss=1.2203688621520996
I0218 09:08:49.104209 140379138533120 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.7439278364181519, loss=1.1974236965179443
I0218 09:10:04.765468 140379146925824 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.7633817195892334, loss=1.2662363052368164
I0218 09:11:23.575685 140379146925824 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.7130911350250244, loss=1.2082571983337402
I0218 09:12:39.065968 140379138533120 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.8507603406906128, loss=1.1703581809997559
I0218 09:13:54.834372 140379146925824 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.6691697835922241, loss=1.2226563692092896
I0218 09:15:10.339367 140379138533120 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.6912786364555359, loss=1.245124340057373
I0218 09:16:25.895648 140379146925824 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.7753531336784363, loss=1.2198121547698975
I0218 09:17:41.410457 140379138533120 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.6967082023620605, loss=1.2037222385406494
I0218 09:18:56.945764 140379146925824 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.8224107027053833, loss=1.251448154449463
I0218 09:20:16.299627 140379138533120 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.7390425801277161, loss=1.2298537492752075
I0218 09:21:37.853719 140379146925824 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.7107122540473938, loss=1.2693521976470947
I0218 09:22:58.398784 140379138533120 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.738831102848053, loss=1.2390422821044922
I0218 09:24:18.554077 140379146925824 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.8147655725479126, loss=1.166930079460144
I0218 09:25:33.972432 140379138533120 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.7053177952766418, loss=1.212929606437683
I0218 09:26:49.469031 140379146925824 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.67415851354599, loss=1.1779204607009888
I0218 09:27:35.348455 140549388556096 spec.py:321] Evaluating on the training split.
I0218 09:28:30.065394 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 09:29:20.381904 140549388556096 spec.py:349] Evaluating on the test split.
I0218 09:29:45.878969 140549388556096 submission_runner.py:408] Time since start: 33176.73s, 	Step: 39462, 	{'train/ctc_loss': Array(0.15873039, dtype=float32), 'train/wer': 0.0605711181492984, 'validation/ctc_loss': Array(0.43230936, dtype=float32), 'validation/wer': 0.12975853712696833, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24808052, dtype=float32), 'test/wer': 0.08443523652834481, 'test/num_examples': 2472, 'score': 30306.92392349243, 'total_duration': 33176.732083559036, 'accumulated_submission_time': 30306.92392349243, 'accumulated_eval_time': 2867.0316922664642, 'accumulated_logging_time': 1.1251184940338135}
I0218 09:29:45.918408 140379146925824 logging_writer.py:48] [39462] accumulated_eval_time=2867.031692, accumulated_logging_time=1.125118, accumulated_submission_time=30306.923923, global_step=39462, preemption_count=0, score=30306.923923, test/ctc_loss=0.2480805218219757, test/num_examples=2472, test/wer=0.084435, total_duration=33176.732084, train/ctc_loss=0.1587303876876831, train/wer=0.060571, validation/ctc_loss=0.4323093593120575, validation/num_examples=5348, validation/wer=0.129759
I0218 09:30:15.300411 140379138533120 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.8190525770187378, loss=1.2161740064620972
I0218 09:31:30.714573 140379146925824 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.7456697225570679, loss=1.207375168800354
I0218 09:32:46.290678 140379138533120 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.7646483778953552, loss=1.2278363704681396
I0218 09:34:01.818517 140379146925824 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.7299668192863464, loss=1.1770625114440918
I0218 09:35:17.406176 140379138533120 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.7578430771827698, loss=1.1530585289001465
I0218 09:36:32.999863 140379146925824 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.7100439667701721, loss=1.2157386541366577
I0218 09:37:50.467544 140379138533120 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.7068571448326111, loss=1.223000407218933
I0218 09:39:12.242159 140379146925824 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.7694939970970154, loss=1.1540088653564453
I0218 09:40:27.627408 140379138533120 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.6981467008590698, loss=1.2281793355941772
I0218 09:41:43.122685 140379146925824 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.7372605204582214, loss=1.1607744693756104
I0218 09:42:58.823457 140379138533120 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.9676004648208618, loss=1.2410352230072021
I0218 09:44:14.321303 140379146925824 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.7123856544494629, loss=1.2104260921478271
I0218 09:45:29.803538 140379138533120 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.7432081699371338, loss=1.219163179397583
I0218 09:46:45.240931 140379146925824 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.7896260619163513, loss=1.174476146697998
I0218 09:48:03.483652 140379138533120 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.8473352193832397, loss=1.2067917585372925
I0218 09:49:23.697720 140379146925824 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.7104982733726501, loss=1.2066494226455688
I0218 09:50:43.099138 140379138533120 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.7161422967910767, loss=1.1652824878692627
I0218 09:52:06.197393 140379146925824 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.9554362893104553, loss=1.1706410646438599
I0218 09:53:21.578690 140379138533120 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.7109407186508179, loss=1.154792070388794
I0218 09:53:46.194839 140549388556096 spec.py:321] Evaluating on the training split.
I0218 09:54:40.874610 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 09:55:31.146706 140549388556096 spec.py:349] Evaluating on the test split.
I0218 09:55:56.907728 140549388556096 submission_runner.py:408] Time since start: 34747.76s, 	Step: 41334, 	{'train/ctc_loss': Array(0.17622687, dtype=float32), 'train/wer': 0.06372093514392516, 'validation/ctc_loss': Array(0.4257966, dtype=float32), 'validation/wer': 0.1274896936578584, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24189256, dtype=float32), 'test/wer': 0.08045416692056141, 'test/num_examples': 2472, 'score': 31747.11150074005, 'total_duration': 34747.760195970535, 'accumulated_submission_time': 31747.11150074005, 'accumulated_eval_time': 2997.738777399063, 'accumulated_logging_time': 1.1801607608795166}
I0218 09:55:56.945426 140379146925824 logging_writer.py:48] [41334] accumulated_eval_time=2997.738777, accumulated_logging_time=1.180161, accumulated_submission_time=31747.111501, global_step=41334, preemption_count=0, score=31747.111501, test/ctc_loss=0.24189256131649017, test/num_examples=2472, test/wer=0.080454, total_duration=34747.760196, train/ctc_loss=0.17622686922550201, train/wer=0.063721, validation/ctc_loss=0.42579659819602966, validation/num_examples=5348, validation/wer=0.127490
I0218 09:56:47.399476 140379138533120 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.7308322787284851, loss=1.1950795650482178
I0218 09:58:03.091459 140379146925824 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.8853949904441833, loss=1.1294260025024414
I0218 09:59:19.026676 140379138533120 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.7040755152702332, loss=1.247231364250183
I0218 10:00:34.578994 140379146925824 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.8929409980773926, loss=1.1874428987503052
I0218 10:01:50.165704 140379138533120 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.7488027811050415, loss=1.1835658550262451
I0218 10:03:05.768018 140379146925824 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.786866307258606, loss=1.1913373470306396
I0218 10:04:21.310084 140379138533120 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.8371959328651428, loss=1.2159866094589233
I0218 10:05:36.961554 140379146925824 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.680601954460144, loss=1.1844655275344849
I0218 10:06:55.225774 140379138533120 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.8116841316223145, loss=1.1548821926116943
I0218 10:08:15.560755 140379146925824 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.6631085276603699, loss=1.161157488822937
I0218 10:09:31.028445 140379138533120 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.7206838726997375, loss=1.1756722927093506
I0218 10:10:46.636581 140379146925824 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.787156343460083, loss=1.1543197631835938
I0218 10:12:02.449148 140379138533120 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.6672941446304321, loss=1.1414575576782227
I0218 10:13:18.026792 140379146925824 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.0059205293655396, loss=1.1345300674438477
I0218 10:14:33.558097 140379138533120 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.8593399524688721, loss=1.133815050125122
I0218 10:15:49.037202 140379146925824 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.7612028121948242, loss=1.1697856187820435
I0218 10:17:10.460338 140379138533120 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.7913566827774048, loss=1.1562209129333496
I0218 10:18:31.333625 140379146925824 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.716036319732666, loss=1.158827543258667
I0218 10:19:52.871904 140379138533120 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.7355809211730957, loss=1.1400320529937744
I0218 10:19:57.451761 140549388556096 spec.py:321] Evaluating on the training split.
I0218 10:20:51.113225 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 10:21:41.566623 140549388556096 spec.py:349] Evaluating on the test split.
I0218 10:22:07.007433 140549388556096 submission_runner.py:408] Time since start: 36317.86s, 	Step: 43207, 	{'train/ctc_loss': Array(0.21449152, dtype=float32), 'train/wer': 0.07816644337529764, 'validation/ctc_loss': Array(0.4130111, dtype=float32), 'validation/wer': 0.12294235206657848, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23688465, dtype=float32), 'test/wer': 0.07982450795198343, 'test/num_examples': 2472, 'score': 33187.53070783615, 'total_duration': 36317.85943412781, 'accumulated_submission_time': 33187.53070783615, 'accumulated_eval_time': 3127.2881696224213, 'accumulated_logging_time': 1.2326452732086182}
I0218 10:22:07.045361 140379146925824 logging_writer.py:48] [43207] accumulated_eval_time=3127.288170, accumulated_logging_time=1.232645, accumulated_submission_time=33187.530708, global_step=43207, preemption_count=0, score=33187.530708, test/ctc_loss=0.23688465356826782, test/num_examples=2472, test/wer=0.079825, total_duration=36317.859434, train/ctc_loss=0.21449151635169983, train/wer=0.078166, validation/ctc_loss=0.4130111038684845, validation/num_examples=5348, validation/wer=0.122942
I0218 10:23:21.049266 140379146925824 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.7897316813468933, loss=1.177758812904358
I0218 10:24:36.424245 140379138533120 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.7158004641532898, loss=1.1668564081192017
I0218 10:25:52.068778 140379146925824 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.8623806834220886, loss=1.1446186304092407
I0218 10:27:07.650693 140379138533120 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.7028494477272034, loss=1.1604282855987549
I0218 10:28:23.421482 140379146925824 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.6864967942237854, loss=1.1482666730880737
I0218 10:29:38.940342 140379138533120 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.8213303685188293, loss=1.2091307640075684
I0218 10:30:54.505313 140379146925824 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.8777452707290649, loss=1.17326021194458
I0218 10:32:15.676231 140379138533120 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.8317998647689819, loss=1.176867961883545
I0218 10:33:36.260279 140379146925824 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.9184483885765076, loss=1.1987736225128174
I0218 10:34:56.357087 140379138533120 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.679314136505127, loss=1.1996901035308838
I0218 10:36:18.748859 140379146925824 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.7394595146179199, loss=1.1420782804489136
I0218 10:37:34.098903 140379138533120 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.7554837465286255, loss=1.195652723312378
I0218 10:38:49.576800 140379146925824 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.7951688766479492, loss=1.1617851257324219
I0218 10:40:05.148519 140379138533120 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.7479013800621033, loss=1.1410025358200073
I0218 10:41:20.638118 140379146925824 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.7463511228561401, loss=1.1248687505722046
I0218 10:42:36.325187 140379138533120 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.7413245439529419, loss=1.1785717010498047
I0218 10:43:51.800470 140379146925824 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.7668622136116028, loss=1.1684008836746216
I0218 10:45:10.061454 140379138533120 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.7916334271430969, loss=1.1437580585479736
I0218 10:46:07.254109 140549388556096 spec.py:321] Evaluating on the training split.
I0218 10:47:00.273482 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 10:47:50.534433 140549388556096 spec.py:349] Evaluating on the test split.
I0218 10:48:16.400377 140549388556096 submission_runner.py:408] Time since start: 37887.25s, 	Step: 45073, 	{'train/ctc_loss': Array(0.21359752, dtype=float32), 'train/wer': 0.07842364422633884, 'validation/ctc_loss': Array(0.4034216, dtype=float32), 'validation/wer': 0.12121416916883092, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23037176, dtype=float32), 'test/wer': 0.07590437308309467, 'test/num_examples': 2472, 'score': 34627.65042424202, 'total_duration': 37887.25235772133, 'accumulated_submission_time': 34627.65042424202, 'accumulated_eval_time': 3256.428151845932, 'accumulated_logging_time': 1.2872276306152344}
I0218 10:48:16.440783 140379146925824 logging_writer.py:48] [45073] accumulated_eval_time=3256.428152, accumulated_logging_time=1.287228, accumulated_submission_time=34627.650424, global_step=45073, preemption_count=0, score=34627.650424, test/ctc_loss=0.23037175834178925, test/num_examples=2472, test/wer=0.075904, total_duration=37887.252358, train/ctc_loss=0.21359752118587494, train/wer=0.078424, validation/ctc_loss=0.4034216105937958, validation/num_examples=5348, validation/wer=0.121214
I0218 10:48:37.558676 140379138533120 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.7576583027839661, loss=1.1800024509429932
I0218 10:49:52.953838 140379146925824 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.041961669921875, loss=1.1629186868667603
I0218 10:51:08.395435 140379138533120 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.7547500133514404, loss=1.1406649351119995
I0218 10:52:27.274793 140379146925824 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.7067734003067017, loss=1.1528269052505493
I0218 10:53:42.765514 140379138533120 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.8128985166549683, loss=1.178247094154358
I0218 10:54:58.259004 140379146925824 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.9601899981498718, loss=1.199769377708435
I0218 10:56:13.793794 140379138533120 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.9454429745674133, loss=1.1424790620803833
I0218 10:57:29.562980 140379146925824 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.9207068681716919, loss=1.1661604642868042
I0218 10:58:45.176113 140379138533120 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.8869099020957947, loss=1.1449183225631714
I0218 11:00:02.644258 140379146925824 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.7932999730110168, loss=1.2165340185165405
I0218 11:01:23.678788 140379138533120 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.7633726596832275, loss=1.1325989961624146
I0218 11:02:44.577531 140379146925824 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.7436977028846741, loss=1.1384927034378052
I0218 11:04:05.825781 140379138533120 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.799335777759552, loss=1.1374189853668213
I0218 11:05:27.424647 140379146925824 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.8763625621795654, loss=1.1649909019470215
I0218 11:06:42.888827 140379138533120 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.746649980545044, loss=1.1237311363220215
I0218 11:07:58.452966 140379146925824 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.8273777365684509, loss=1.1671772003173828
I0218 11:09:14.058769 140379138533120 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.8708569407463074, loss=1.135325312614441
I0218 11:10:29.556963 140379146925824 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.9067149758338928, loss=1.1072065830230713
I0218 11:11:45.261293 140379138533120 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.8377021551132202, loss=1.1376463174819946
I0218 11:12:16.629373 140549388556096 spec.py:321] Evaluating on the training split.
I0218 11:13:08.522433 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 11:13:58.995017 140549388556096 spec.py:349] Evaluating on the test split.
I0218 11:14:24.464899 140549388556096 submission_runner.py:408] Time since start: 39455.32s, 	Step: 46943, 	{'train/ctc_loss': Array(0.25127593, dtype=float32), 'train/wer': 0.09323139379120332, 'validation/ctc_loss': Array(0.40328342, dtype=float32), 'validation/wer': 0.12057696206686813, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22683704, dtype=float32), 'test/wer': 0.07783397314809172, 'test/num_examples': 2472, 'score': 36067.74671292305, 'total_duration': 39455.31745886803, 'accumulated_submission_time': 36067.74671292305, 'accumulated_eval_time': 3384.257961034775, 'accumulated_logging_time': 1.3474400043487549}
I0218 11:14:24.508843 140379146925824 logging_writer.py:48] [46943] accumulated_eval_time=3384.257961, accumulated_logging_time=1.347440, accumulated_submission_time=36067.746713, global_step=46943, preemption_count=0, score=36067.746713, test/ctc_loss=0.22683703899383545, test/num_examples=2472, test/wer=0.077834, total_duration=39455.317459, train/ctc_loss=0.2512759268283844, train/wer=0.093231, validation/ctc_loss=0.40328341722488403, validation/num_examples=5348, validation/wer=0.120577
I0218 11:15:08.223866 140379138533120 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.8517696857452393, loss=1.1394935846328735
I0218 11:16:23.727417 140379146925824 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.8477513790130615, loss=1.1837764978408813
I0218 11:17:39.192722 140379138533120 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.8557897210121155, loss=1.1988072395324707
I0218 11:18:54.832615 140379146925824 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.7879022359848022, loss=1.0928596258163452
I0218 11:20:13.662034 140379146925824 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.9058787822723389, loss=1.1009571552276611
I0218 11:21:29.052454 140379138533120 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.8138946890830994, loss=1.1365160942077637
I0218 11:22:44.633656 140379146925824 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.844016432762146, loss=1.1019349098205566
I0218 11:24:00.088975 140379138533120 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.8387982249259949, loss=1.1507152318954468
I0218 11:25:15.695129 140379146925824 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.8029320240020752, loss=1.144057035446167
I0218 11:26:31.215115 140379138533120 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.6997640132904053, loss=1.1255162954330444
I0218 11:27:47.030091 140379146925824 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.8981885313987732, loss=1.080710768699646
I0218 11:29:03.773496 140379138533120 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.8876634240150452, loss=1.1086835861206055
I0218 11:30:24.351369 140379146925824 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.7497158050537109, loss=1.145945429801941
I0218 11:31:44.543483 140379138533120 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.8025434017181396, loss=1.1334969997406006
I0218 11:33:04.995668 140379146925824 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.7501646876335144, loss=1.1580241918563843
I0218 11:34:23.708115 140379146925824 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.7964972853660583, loss=1.0690207481384277
I0218 11:35:39.240321 140379138533120 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.8326216340065002, loss=1.0871490240097046
I0218 11:36:54.715806 140379146925824 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.7751343250274658, loss=1.0657403469085693
I0218 11:38:10.399446 140379138533120 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.932378888130188, loss=1.0984013080596924
I0218 11:38:25.204208 140549388556096 spec.py:321] Evaluating on the training split.
I0218 11:39:17.017836 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 11:40:07.594414 140549388556096 spec.py:349] Evaluating on the test split.
I0218 11:40:33.773470 140549388556096 submission_runner.py:408] Time since start: 41024.63s, 	Step: 48821, 	{'train/ctc_loss': Array(0.21432593, dtype=float32), 'train/wer': 0.07740404712421033, 'validation/ctc_loss': Array(0.39701816, dtype=float32), 'validation/wer': 0.11645442521023007, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22061086, dtype=float32), 'test/wer': 0.07182174557715354, 'test/num_examples': 2472, 'score': 37508.35221242905, 'total_duration': 41024.62561440468, 'accumulated_submission_time': 37508.35221242905, 'accumulated_eval_time': 3512.8210985660553, 'accumulated_logging_time': 1.4097182750701904}
I0218 11:40:33.814479 140379146925824 logging_writer.py:48] [48821] accumulated_eval_time=3512.821099, accumulated_logging_time=1.409718, accumulated_submission_time=37508.352212, global_step=48821, preemption_count=0, score=37508.352212, test/ctc_loss=0.2206108570098877, test/num_examples=2472, test/wer=0.071822, total_duration=41024.625614, train/ctc_loss=0.2143259346485138, train/wer=0.077404, validation/ctc_loss=0.397018164396286, validation/num_examples=5348, validation/wer=0.116454
I0218 11:41:34.121356 140379138533120 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.8998540639877319, loss=1.1184191703796387
I0218 11:42:49.856608 140379146925824 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.8865256309509277, loss=1.1727062463760376
I0218 11:44:05.492538 140379138533120 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.8018863201141357, loss=1.160443663597107
I0218 11:45:21.114773 140379146925824 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.765509843826294, loss=1.0986073017120361
I0218 11:46:36.714994 140379138533120 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.9082916378974915, loss=1.1114130020141602
I0218 11:47:56.096839 140379146925824 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.8716252446174622, loss=1.0960445404052734
I0218 11:49:16.767401 140379146925824 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.957697868347168, loss=1.1226266622543335
I0218 11:50:32.143856 140379138533120 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.7840523719787598, loss=1.068595290184021
I0218 11:51:47.782655 140379146925824 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.0193299055099487, loss=1.1054303646087646
I0218 11:53:03.243874 140379138533120 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.7797878384590149, loss=1.0838637351989746
I0218 11:54:18.670455 140379146925824 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.8880828022956848, loss=1.1523289680480957
I0218 11:55:34.117021 140379138533120 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.825967013835907, loss=1.0810134410858154
I0218 11:56:49.906123 140379146925824 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.9287582039833069, loss=1.1320852041244507
I0218 11:58:09.066397 140379138533120 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.9104329347610474, loss=1.0802156925201416
I0218 11:59:29.777201 140379146925824 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.8873804807662964, loss=1.0970354080200195
I0218 12:00:50.651031 140379138533120 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.9300928115844727, loss=1.132106900215149
I0218 12:02:13.298529 140379146925824 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.7754533886909485, loss=1.0898178815841675
I0218 12:03:28.694995 140379138533120 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.8312461972236633, loss=1.123321771621704
I0218 12:04:33.931035 140549388556096 spec.py:321] Evaluating on the training split.
I0218 12:05:26.390046 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 12:06:16.917752 140549388556096 spec.py:349] Evaluating on the test split.
I0218 12:06:42.376011 140549388556096 submission_runner.py:408] Time since start: 42593.23s, 	Step: 50688, 	{'train/ctc_loss': Array(0.19890381, dtype=float32), 'train/wer': 0.07448253593577767, 'validation/ctc_loss': Array(0.39558187, dtype=float32), 'validation/wer': 0.11495795398592351, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21984032, dtype=float32), 'test/wer': 0.07275607824020475, 'test/num_examples': 2472, 'score': 38948.3796274662, 'total_duration': 42593.2292907238, 'accumulated_submission_time': 38948.3796274662, 'accumulated_eval_time': 3641.26109457016, 'accumulated_logging_time': 1.4670865535736084}
I0218 12:06:42.417896 140379146925824 logging_writer.py:48] [50688] accumulated_eval_time=3641.261095, accumulated_logging_time=1.467087, accumulated_submission_time=38948.379627, global_step=50688, preemption_count=0, score=38948.379627, test/ctc_loss=0.21984031796455383, test/num_examples=2472, test/wer=0.072756, total_duration=42593.229291, train/ctc_loss=0.19890381395816803, train/wer=0.074483, validation/ctc_loss=0.3955818712711334, validation/num_examples=5348, validation/wer=0.114958
I0218 12:06:52.279810 140379138533120 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.9003249406814575, loss=1.0723304748535156
I0218 12:08:07.524035 140379146925824 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.7785595655441284, loss=1.0874853134155273
I0218 12:09:23.024876 140379138533120 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.8942855000495911, loss=1.0935730934143066
I0218 12:10:38.484489 140379146925824 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.7622735500335693, loss=1.0851942300796509
I0218 12:11:54.038354 140379138533120 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.7256316542625427, loss=1.0645807981491089
I0218 12:13:09.807927 140379146925824 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.0281734466552734, loss=1.0728763341903687
I0218 12:14:25.413008 140379138533120 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.9625863432884216, loss=1.193595290184021
I0218 12:15:46.062779 140379146925824 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.7692232131958008, loss=1.1235451698303223
I0218 12:17:11.301545 140379146925824 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.8243582844734192, loss=1.0456156730651855
I0218 12:18:26.747563 140379138533120 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.8967413306236267, loss=1.0564228296279907
I0218 12:19:42.269506 140379146925824 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.7846148610115051, loss=1.10664963722229
I0218 12:20:57.867210 140379138533120 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.7939017415046692, loss=1.1149952411651611
I0218 12:22:13.427067 140379146925824 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.910241961479187, loss=1.1064937114715576
I0218 12:23:29.024597 140379138533120 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.8234036564826965, loss=1.0844197273254395
I0218 12:24:44.622111 140379146925824 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.7165339589118958, loss=1.0735363960266113
I0218 12:26:03.738038 140379138533120 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.8360336422920227, loss=1.1010507345199585
I0218 12:27:25.899433 140379146925824 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.837935209274292, loss=1.0779399871826172
I0218 12:28:47.064917 140379138533120 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.7728855013847351, loss=1.114920973777771
I0218 12:30:07.699189 140379146925824 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.9524139761924744, loss=1.0510671138763428
I0218 12:30:42.497975 140549388556096 spec.py:321] Evaluating on the training split.
I0218 12:31:36.992705 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 12:32:27.800375 140549388556096 spec.py:349] Evaluating on the test split.
I0218 12:32:53.332048 140549388556096 submission_runner.py:408] Time since start: 44164.18s, 	Step: 52542, 	{'train/ctc_loss': Array(0.1622737, dtype=float32), 'train/wer': 0.06263798409988146, 'validation/ctc_loss': Array(0.37560198, dtype=float32), 'validation/wer': 0.11092230900682584, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21027523, dtype=float32), 'test/wer': 0.07037962342331364, 'test/num_examples': 2472, 'score': 40388.371878385544, 'total_duration': 44164.18422412872, 'accumulated_submission_time': 40388.371878385544, 'accumulated_eval_time': 3772.0892395973206, 'accumulated_logging_time': 1.5247292518615723}
I0218 12:32:53.374155 140379146925824 logging_writer.py:48] [52542] accumulated_eval_time=3772.089240, accumulated_logging_time=1.524729, accumulated_submission_time=40388.371878, global_step=52542, preemption_count=0, score=40388.371878, test/ctc_loss=0.21027523279190063, test/num_examples=2472, test/wer=0.070380, total_duration=44164.184224, train/ctc_loss=0.16227370500564575, train/wer=0.062638, validation/ctc_loss=0.37560197710990906, validation/num_examples=5348, validation/wer=0.110922
I0218 12:33:37.769301 140379138533120 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.914173424243927, loss=1.0613795518875122
I0218 12:34:53.164150 140379146925824 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.0910189151763916, loss=1.0818006992340088
I0218 12:36:08.756430 140379138533120 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.9424894452095032, loss=1.050187110900879
I0218 12:37:24.188951 140379146925824 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.9977384805679321, loss=1.062999963760376
I0218 12:38:39.792744 140379138533120 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.9992910623550415, loss=1.0706439018249512
I0218 12:39:55.302612 140379146925824 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.8410797715187073, loss=1.116001009941101
I0218 12:41:10.921126 140379138533120 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.7762035131454468, loss=1.0505889654159546
I0218 12:42:26.653691 140379146925824 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.9283744096755981, loss=1.0525606870651245
I0218 12:43:42.213640 140379138533120 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.0381700992584229, loss=1.0880805253982544
I0218 12:44:58.895555 140379146925824 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.9525474309921265, loss=1.1574846506118774
I0218 12:46:20.030171 140379146925824 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.9421182870864868, loss=1.0676008462905884
I0218 12:47:35.494263 140379138533120 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.8581137657165527, loss=1.0625863075256348
I0218 12:48:51.012473 140379146925824 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.0051677227020264, loss=1.0822999477386475
I0218 12:50:06.544818 140379138533120 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.8783190250396729, loss=1.0305073261260986
I0218 12:51:22.097770 140379146925824 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.8681572079658508, loss=1.0377111434936523
I0218 12:52:37.617729 140379138533120 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.1250663995742798, loss=1.02670419216156
I0218 12:53:53.150652 140379146925824 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.82000333070755, loss=1.0650426149368286
I0218 12:55:08.716428 140379138533120 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.8223943710327148, loss=1.1004350185394287
I0218 12:56:26.721760 140379146925824 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.9673150181770325, loss=1.0377448797225952
I0218 12:56:53.723648 140549388556096 spec.py:321] Evaluating on the training split.
I0218 12:57:47.858840 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 12:58:38.031317 140549388556096 spec.py:349] Evaluating on the test split.
I0218 12:59:03.156908 140549388556096 submission_runner.py:408] Time since start: 45734.01s, 	Step: 54436, 	{'train/ctc_loss': Array(0.17281756, dtype=float32), 'train/wer': 0.06613370119537058, 'validation/ctc_loss': Array(0.36489117, dtype=float32), 'validation/wer': 0.10767834557865164, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20149006, dtype=float32), 'test/wer': 0.06842971177868502, 'test/num_examples': 2472, 'score': 41828.6324532032, 'total_duration': 45734.00941133499, 'accumulated_submission_time': 41828.6324532032, 'accumulated_eval_time': 3901.5167338848114, 'accumulated_logging_time': 1.5817480087280273}
I0218 12:59:03.198023 140379146925824 logging_writer.py:48] [54436] accumulated_eval_time=3901.516734, accumulated_logging_time=1.581748, accumulated_submission_time=41828.632453, global_step=54436, preemption_count=0, score=41828.632453, test/ctc_loss=0.20149005949497223, test/num_examples=2472, test/wer=0.068430, total_duration=45734.009411, train/ctc_loss=0.17281755805015564, train/wer=0.066134, validation/ctc_loss=0.3648911714553833, validation/num_examples=5348, validation/wer=0.107678
I0218 12:59:52.129840 140379138533120 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.0499862432479858, loss=1.1032228469848633
I0218 13:01:10.898574 140379146925824 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.922968327999115, loss=1.1054246425628662
I0218 13:02:26.531740 140379138533120 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.8403334617614746, loss=1.033164620399475
I0218 13:03:42.131390 140379146925824 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.9060460925102234, loss=1.0659657716751099
I0218 13:04:57.828608 140379138533120 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.9293367862701416, loss=1.0883907079696655
I0218 13:06:13.495560 140379146925824 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.9544596672058105, loss=1.0604221820831299
I0218 13:07:28.962660 140379138533120 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.0056358575820923, loss=1.1108746528625488
I0218 13:08:44.463950 140379146925824 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.0659253597259521, loss=1.0902223587036133
I0218 13:10:00.021357 140379138533120 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.9856905937194824, loss=1.0652589797973633
I0218 13:11:18.283496 140379146925824 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.8390933871269226, loss=1.0756480693817139
I0218 13:12:37.588401 140379138533120 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.9486194849014282, loss=1.0463322401046753
I0218 13:13:58.130193 140379146925824 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.9327903985977173, loss=1.0526174306869507
I0218 13:15:17.164417 140379146925824 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.1440422534942627, loss=1.0321135520935059
I0218 13:16:32.665973 140379138533120 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.0839555263519287, loss=1.0091270208358765
I0218 13:17:48.199968 140379146925824 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.8861761093139648, loss=1.0180226564407349
I0218 13:19:03.700820 140379138533120 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.018406629562378, loss=0.978270947933197
I0218 13:20:19.357973 140379146925824 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.96019047498703, loss=1.0518410205841064
I0218 13:21:34.845618 140379138533120 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.9092106819152832, loss=1.0129824876785278
I0218 13:22:50.476869 140379146925824 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.9020677208900452, loss=0.9407454133033752
I0218 13:23:03.775542 140549388556096 spec.py:321] Evaluating on the training split.
I0218 13:23:58.576044 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 13:24:49.019764 140549388556096 spec.py:349] Evaluating on the test split.
I0218 13:25:14.371290 140549388556096 submission_runner.py:408] Time since start: 47305.22s, 	Step: 56319, 	{'train/ctc_loss': Array(0.14977348, dtype=float32), 'train/wer': 0.057254981961862225, 'validation/ctc_loss': Array(0.36254397, dtype=float32), 'validation/wer': 0.10628807553800554, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19897437, dtype=float32), 'test/wer': 0.06700790120447667, 'test/num_examples': 2472, 'score': 43269.12178993225, 'total_duration': 47305.22345614433, 'accumulated_submission_time': 43269.12178993225, 'accumulated_eval_time': 4032.1063737869263, 'accumulated_logging_time': 1.6375198364257812}
I0218 13:25:14.415964 140379146925824 logging_writer.py:48] [56319] accumulated_eval_time=4032.106374, accumulated_logging_time=1.637520, accumulated_submission_time=43269.121790, global_step=56319, preemption_count=0, score=43269.121790, test/ctc_loss=0.1989743709564209, test/num_examples=2472, test/wer=0.067008, total_duration=47305.223456, train/ctc_loss=0.1497734785079956, train/wer=0.057255, validation/ctc_loss=0.3625439703464508, validation/num_examples=5348, validation/wer=0.106288
I0218 13:26:16.137148 140379138533120 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.9442412853240967, loss=1.0694750547409058
I0218 13:27:31.953314 140379146925824 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.9525086879730225, loss=1.0767767429351807
I0218 13:28:47.462231 140379138533120 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.317370891571045, loss=1.016392469406128
I0218 13:30:06.171046 140379146925824 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.9249471426010132, loss=0.9722889065742493
I0218 13:31:21.605264 140379138533120 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.091056227684021, loss=1.0013736486434937
I0218 13:32:37.172506 140379146925824 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.9143975377082825, loss=0.9983739256858826
I0218 13:33:52.856543 140379138533120 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.0209918022155762, loss=1.0733144283294678
I0218 13:35:08.435474 140379146925824 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.0188812017440796, loss=1.048157811164856
I0218 13:36:23.896439 140379138533120 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.9966237545013428, loss=1.0850801467895508
I0218 13:37:39.435627 140379146925824 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.9617449045181274, loss=1.0784896612167358
I0218 13:38:56.240323 140379138533120 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.0807063579559326, loss=1.0380795001983643
I0218 13:40:16.723484 140379146925824 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.0610271692276, loss=1.0290724039077759
I0218 13:41:36.193173 140379138533120 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.9618768692016602, loss=1.0370310544967651
I0218 13:42:59.107265 140379146925824 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.8917208909988403, loss=1.0504682064056396
I0218 13:44:14.561189 140379138533120 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.8601059913635254, loss=0.9761584401130676
I0218 13:45:30.154292 140379146925824 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.9053347110748291, loss=1.0386130809783936
I0218 13:46:45.689649 140379138533120 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.938827395439148, loss=1.0225849151611328
I0218 13:48:01.200130 140379146925824 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.8081343173980713, loss=0.9880748391151428
I0218 13:49:14.845737 140549388556096 spec.py:321] Evaluating on the training split.
I0218 13:50:08.661081 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 13:50:59.155975 140549388556096 spec.py:349] Evaluating on the test split.
I0218 13:51:24.722481 140549388556096 submission_runner.py:408] Time since start: 48875.57s, 	Step: 58199, 	{'train/ctc_loss': Array(0.14436826, dtype=float32), 'train/wer': 0.056059406155415006, 'validation/ctc_loss': Array(0.3545192, dtype=float32), 'validation/wer': 0.10204968284464698, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18987672, dtype=float32), 'test/wer': 0.06467206954684866, 'test/num_examples': 2472, 'score': 44709.46357750893, 'total_duration': 48875.57385182381, 'accumulated_submission_time': 44709.46357750893, 'accumulated_eval_time': 4161.976230621338, 'accumulated_logging_time': 1.697356939315796}
I0218 13:51:24.762994 140379146925824 logging_writer.py:48] [58199] accumulated_eval_time=4161.976231, accumulated_logging_time=1.697357, accumulated_submission_time=44709.463578, global_step=58199, preemption_count=0, score=44709.463578, test/ctc_loss=0.1898767203092575, test/num_examples=2472, test/wer=0.064672, total_duration=48875.573852, train/ctc_loss=0.1443682610988617, train/wer=0.056059, validation/ctc_loss=0.35451918840408325, validation/num_examples=5348, validation/wer=0.102050
I0218 13:51:26.378072 140379138533120 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.9998106956481934, loss=1.0149163007736206
I0218 13:52:41.544057 140379146925824 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.9742702841758728, loss=0.9978371858596802
I0218 13:53:56.955351 140379138533120 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.9395387768745422, loss=1.0362882614135742
I0218 13:55:12.460902 140379146925824 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.9473320841789246, loss=1.0281639099121094
I0218 13:56:28.349066 140379138533120 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.997207760810852, loss=0.9562239646911621
I0218 13:57:43.898986 140379146925824 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.8907604217529297, loss=1.0010846853256226
I0218 13:59:02.489577 140379146925824 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.9138291478157043, loss=0.9846270680427551
I0218 14:00:18.084815 140379138533120 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.8985400199890137, loss=0.9635262489318848
I0218 14:01:33.742468 140379146925824 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.899488627910614, loss=1.0024632215499878
I0218 14:02:49.280398 140379138533120 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.9486128687858582, loss=0.9945254325866699
I0218 14:04:04.843261 140379146925824 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.983302891254425, loss=1.0094386339187622
I0218 14:05:20.442503 140379138533120 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.9969894886016846, loss=1.0102554559707642
I0218 14:06:36.039532 140379146925824 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.9806921482086182, loss=0.9969154596328735
I0218 14:07:53.001251 140379138533120 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.1363935470581055, loss=0.9897542595863342
I0218 14:09:13.286323 140379146925824 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.9931057095527649, loss=0.9779356718063354
I0218 14:10:33.796416 140379138533120 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.051608681678772, loss=0.9829285144805908
I0218 14:11:54.430311 140379146925824 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.9411194920539856, loss=0.9573025703430176
I0218 14:13:09.999584 140379138533120 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.1417158842086792, loss=1.0165965557098389
I0218 14:14:25.519835 140379146925824 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.2308762073516846, loss=0.9644724726676941
I0218 14:15:24.847300 140549388556096 spec.py:321] Evaluating on the training split.
I0218 14:16:18.387770 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 14:17:08.266336 140549388556096 spec.py:349] Evaluating on the test split.
I0218 14:17:33.721588 140549388556096 submission_runner.py:408] Time since start: 50444.58s, 	Step: 60080, 	{'train/ctc_loss': Array(0.1338687, dtype=float32), 'train/wer': 0.05099792960662526, 'validation/ctc_loss': Array(0.34373295, dtype=float32), 'validation/wer': 0.09889261129401315, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18780683, dtype=float32), 'test/wer': 0.062112810513273616, 'test/num_examples': 2472, 'score': 46149.45885229111, 'total_duration': 50444.575095653534, 'accumulated_submission_time': 46149.45885229111, 'accumulated_eval_time': 4290.845764160156, 'accumulated_logging_time': 1.7542970180511475}
I0218 14:17:33.760964 140379146925824 logging_writer.py:48] [60080] accumulated_eval_time=4290.845764, accumulated_logging_time=1.754297, accumulated_submission_time=46149.458852, global_step=60080, preemption_count=0, score=46149.458852, test/ctc_loss=0.18780682981014252, test/num_examples=2472, test/wer=0.062113, total_duration=50444.575096, train/ctc_loss=0.13386869430541992, train/wer=0.050998, validation/ctc_loss=0.34373295307159424, validation/num_examples=5348, validation/wer=0.098893
I0218 14:17:49.654209 140379138533120 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.0157400369644165, loss=0.9538330435752869
I0218 14:19:05.116019 140379146925824 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.0404350757598877, loss=1.022525668144226
I0218 14:20:20.601151 140379138533120 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.1051368713378906, loss=0.9779747128486633
I0218 14:21:36.064251 140379146925824 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.9521748423576355, loss=0.9919455051422119
I0218 14:22:51.644822 140379138533120 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.0505971908569336, loss=1.0185154676437378
I0218 14:24:07.144358 140379146925824 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.9375994801521301, loss=0.944370448589325
I0218 14:25:25.239533 140379138533120 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.9655963182449341, loss=0.979278564453125
I0218 14:26:48.039203 140379146925824 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.8932684659957886, loss=0.9396381378173828
I0218 14:28:03.542318 140379138533120 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.3696520328521729, loss=1.0029950141906738
I0218 14:29:19.033212 140379146925824 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.9607592225074768, loss=0.9446743130683899
I0218 14:30:34.554214 140379138533120 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.43400239944458, loss=0.96774822473526
I0218 14:31:50.121505 140379146925824 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.9899556636810303, loss=0.9781171083450317
I0218 14:33:05.647975 140379138533120 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.0294570922851562, loss=0.9781283736228943
I0218 14:34:21.161178 140379146925824 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.9739795923233032, loss=0.9765387177467346
I0218 14:35:39.537017 140379138533120 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.8752391338348389, loss=0.9827494025230408
I0218 14:37:00.539284 140379146925824 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.0671669244766235, loss=0.9415252804756165
I0218 14:38:21.617052 140379138533120 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.0225849151611328, loss=0.9501713514328003
I0218 14:39:44.897069 140379146925824 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.8899211287498474, loss=0.951564371585846
I0218 14:41:00.345852 140379138533120 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.2929071187973022, loss=0.9772336483001709
I0218 14:41:34.004440 140549388556096 spec.py:321] Evaluating on the training split.
I0218 14:42:26.357382 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 14:43:16.954151 140549388556096 spec.py:349] Evaluating on the test split.
I0218 14:43:42.547976 140549388556096 submission_runner.py:408] Time since start: 52013.40s, 	Step: 61946, 	{'train/ctc_loss': Array(0.13237758, dtype=float32), 'train/wer': 0.050976958208852716, 'validation/ctc_loss': Array(0.33442134, dtype=float32), 'validation/wer': 0.09731890284522626, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18222867, dtype=float32), 'test/wer': 0.06036601466496049, 'test/num_examples': 2472, 'score': 47589.61220765114, 'total_duration': 52013.4002263546, 'accumulated_submission_time': 47589.61220765114, 'accumulated_eval_time': 4419.383289575577, 'accumulated_logging_time': 1.8110344409942627}
I0218 14:43:42.593698 140379146925824 logging_writer.py:48] [61946] accumulated_eval_time=4419.383290, accumulated_logging_time=1.811034, accumulated_submission_time=47589.612208, global_step=61946, preemption_count=0, score=47589.612208, test/ctc_loss=0.1822286695241928, test/num_examples=2472, test/wer=0.060366, total_duration=52013.400226, train/ctc_loss=0.13237757980823517, train/wer=0.050977, validation/ctc_loss=0.3344213366508484, validation/num_examples=5348, validation/wer=0.097319
I0218 14:44:23.998427 140379138533120 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.0137516260147095, loss=0.9362521171569824
I0218 14:45:39.455039 140379146925824 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.0681737661361694, loss=0.9831461310386658
I0218 14:46:54.974407 140379138533120 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.1308265924453735, loss=0.983945906162262
I0218 14:48:10.604488 140379146925824 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.8965969085693359, loss=0.9161984920501709
I0218 14:49:26.126157 140379138533120 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.425140619277954, loss=0.9707505702972412
I0218 14:50:41.722763 140379146925824 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.1528080701828003, loss=1.0076725482940674
I0218 14:51:57.285608 140379138533120 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.0700393915176392, loss=0.97536301612854
I0218 14:53:15.500390 140379146925824 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.2674875259399414, loss=0.9736504554748535
I0218 14:54:35.600642 140379138533120 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.1513400077819824, loss=0.9601314663887024
I0218 14:55:55.833710 140379146925824 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.1014634370803833, loss=0.9517486691474915
I0218 14:57:11.653245 140379138533120 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.1131905317306519, loss=0.9289058446884155
I0218 14:58:27.511497 140379146925824 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.0960050821304321, loss=0.9802605509757996
I0218 14:59:43.319355 140379138533120 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.176203966140747, loss=0.9692952036857605
I0218 15:00:59.184463 140379146925824 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.0520505905151367, loss=0.9193968772888184
I0218 15:02:14.904371 140379138533120 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.050384521484375, loss=0.9238512516021729
I0218 15:03:30.677261 140379146925824 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.0847119092941284, loss=0.9678977131843567
I0218 15:04:50.500718 140379138533120 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.2198234796524048, loss=0.9265564680099487
I0218 15:06:11.701174 140379146925824 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.0100153684616089, loss=0.9169484376907349
I0218 15:07:32.542076 140379138533120 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.2514705657958984, loss=0.9486772418022156
I0218 15:07:42.575079 140549388556096 spec.py:321] Evaluating on the training split.
I0218 15:08:37.712407 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 15:09:28.325456 140549388556096 spec.py:349] Evaluating on the test split.
I0218 15:09:54.154156 140549388556096 submission_runner.py:408] Time since start: 53585.01s, 	Step: 63814, 	{'train/ctc_loss': Array(0.12851778, dtype=float32), 'train/wer': 0.049004018776070316, 'validation/ctc_loss': Array(0.32344717, dtype=float32), 'validation/wer': 0.09304189153962752, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17482771, dtype=float32), 'test/wer': 0.05762395141470152, 'test/num_examples': 2472, 'score': 49029.50516271591, 'total_duration': 53585.0060338974, 'accumulated_submission_time': 49029.50516271591, 'accumulated_eval_time': 4550.955982685089, 'accumulated_logging_time': 1.8721585273742676}
I0218 15:09:54.204589 140379146925824 logging_writer.py:48] [63814] accumulated_eval_time=4550.955983, accumulated_logging_time=1.872159, accumulated_submission_time=49029.505163, global_step=63814, preemption_count=0, score=49029.505163, test/ctc_loss=0.1748277097940445, test/num_examples=2472, test/wer=0.057624, total_duration=53585.006034, train/ctc_loss=0.1285177767276764, train/wer=0.049004, validation/ctc_loss=0.32344716787338257, validation/num_examples=5348, validation/wer=0.093042
I0218 15:11:03.192276 140379146925824 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.997426450252533, loss=0.9396070837974548
I0218 15:12:18.756508 140379138533120 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.9752634167671204, loss=0.9384480118751526
I0218 15:13:34.459139 140379146925824 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.0243661403656006, loss=0.9608588814735413
I0218 15:14:50.093240 140379138533120 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.993168830871582, loss=0.9329211115837097
I0218 15:16:05.712761 140379146925824 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.1590347290039062, loss=0.9375948905944824
I0218 15:17:21.314799 140379138533120 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.2903175354003906, loss=0.9342420697212219
I0218 15:18:37.160514 140379146925824 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.2880274057388306, loss=0.9571214318275452
I0218 15:19:56.619926 140379138533120 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.0391958951950073, loss=0.9369795322418213
I0218 15:21:17.173949 140379146925824 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.0773065090179443, loss=0.9295654296875
I0218 15:22:38.777954 140379138533120 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.1694637537002563, loss=0.939049482345581
I0218 15:24:02.746348 140379146925824 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.1420204639434814, loss=0.9009732604026794
I0218 15:25:18.421123 140379138533120 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.1124342679977417, loss=0.9264037609100342
I0218 15:26:33.944775 140379146925824 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.160512089729309, loss=0.9546754956245422
I0218 15:27:49.426396 140379138533120 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.0380222797393799, loss=0.9076551198959351
I0218 15:29:05.120477 140379146925824 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.0083661079406738, loss=0.9426214098930359
I0218 15:30:20.672801 140379138533120 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.106346845626831, loss=0.9895143508911133
I0218 15:31:36.223689 140379146925824 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.0591280460357666, loss=0.9396144151687622
I0218 15:32:51.841895 140379138533120 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.2417609691619873, loss=0.9918662905693054
I0218 15:33:54.686072 140549388556096 spec.py:321] Evaluating on the training split.
I0218 15:34:48.139595 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 15:35:39.171935 140549388556096 spec.py:349] Evaluating on the test split.
I0218 15:36:04.697428 140549388556096 submission_runner.py:408] Time since start: 55155.55s, 	Step: 65679, 	{'train/ctc_loss': Array(0.10282077, dtype=float32), 'train/wer': 0.040542964108772166, 'validation/ctc_loss': Array(0.31785193, dtype=float32), 'validation/wer': 0.09087924925417805, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17353387, dtype=float32), 'test/wer': 0.05595840188491459, 'test/num_examples': 2472, 'score': 50469.89919543266, 'total_duration': 55155.54966759682, 'accumulated_submission_time': 50469.89919543266, 'accumulated_eval_time': 4680.961303472519, 'accumulated_logging_time': 1.9368162155151367}
I0218 15:36:04.738298 140379146925824 logging_writer.py:48] [65679] accumulated_eval_time=4680.961303, accumulated_logging_time=1.936816, accumulated_submission_time=50469.899195, global_step=65679, preemption_count=0, score=50469.899195, test/ctc_loss=0.1735338717699051, test/num_examples=2472, test/wer=0.055958, total_duration=55155.549668, train/ctc_loss=0.10282076895236969, train/wer=0.040543, validation/ctc_loss=0.3178519308567047, validation/num_examples=5348, validation/wer=0.090879
I0218 15:36:21.335250 140379138533120 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.111462116241455, loss=0.934010922908783
I0218 15:37:36.733016 140379146925824 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.1314548254013062, loss=0.9402863383293152
I0218 15:38:52.300527 140379138533120 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.1224778890609741, loss=0.8898316025733948
I0218 15:40:11.038678 140379146925824 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.0949589014053345, loss=0.905552864074707
I0218 15:41:26.826676 140379138533120 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.1157515048980713, loss=0.9588455557823181
I0218 15:42:42.331715 140379146925824 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.0160484313964844, loss=0.9312779903411865
I0218 15:43:57.840981 140379138533120 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.2366400957107544, loss=0.9207984209060669
I0218 15:45:13.502639 140379146925824 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.0818188190460205, loss=0.9030473828315735
I0218 15:46:29.076076 140379138533120 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.103765606880188, loss=0.923018217086792
I0218 15:47:45.598623 140379146925824 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.131613850593567, loss=0.8833203911781311
I0218 15:49:06.334104 140379138533120 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.0054482221603394, loss=0.9185381531715393
I0218 15:50:26.878923 140379146925824 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.2506053447723389, loss=0.9482973217964172
I0218 15:51:47.339098 140379138533120 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.1541571617126465, loss=0.9428581595420837
I0218 15:53:08.485531 140379146925824 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.2588815689086914, loss=0.9031155705451965
I0218 15:54:24.176136 140379138533120 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.1175891160964966, loss=0.9230896234512329
I0218 15:55:39.630358 140379146925824 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.1474167108535767, loss=0.9406719207763672
I0218 15:56:55.172542 140379138533120 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.152048945426941, loss=0.927772581577301
I0218 15:58:10.681261 140379146925824 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.0952762365341187, loss=0.8913319706916809
I0218 15:59:26.256571 140379138533120 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.0990544557571411, loss=0.914775013923645
I0218 16:00:05.234595 140549388556096 spec.py:321] Evaluating on the training split.
I0218 16:00:58.329129 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 16:01:48.615875 140549388556096 spec.py:349] Evaluating on the test split.
I0218 16:02:14.155143 140549388556096 submission_runner.py:408] Time since start: 56725.01s, 	Step: 67553, 	{'train/ctc_loss': Array(0.09873752, dtype=float32), 'train/wer': 0.03779494317922505, 'validation/ctc_loss': Array(0.3111092, dtype=float32), 'validation/wer': 0.08922830358091081, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16759367, dtype=float32), 'test/wer': 0.05427254077549611, 'test/num_examples': 2472, 'score': 51910.30626130104, 'total_duration': 56725.00738286972, 'accumulated_submission_time': 51910.30626130104, 'accumulated_eval_time': 4809.875825405121, 'accumulated_logging_time': 1.9941620826721191}
I0218 16:02:14.197118 140379146925824 logging_writer.py:48] [67553] accumulated_eval_time=4809.875825, accumulated_logging_time=1.994162, accumulated_submission_time=51910.306261, global_step=67553, preemption_count=0, score=51910.306261, test/ctc_loss=0.16759367287158966, test/num_examples=2472, test/wer=0.054273, total_duration=56725.007383, train/ctc_loss=0.09873751550912857, train/wer=0.037795, validation/ctc_loss=0.31110918521881104, validation/num_examples=5348, validation/wer=0.089228
I0218 16:02:50.351031 140379138533120 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.2050312757492065, loss=0.9518657922744751
I0218 16:04:05.850241 140379146925824 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.0101664066314697, loss=0.8778136968612671
I0218 16:05:21.387758 140379138533120 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.1763802766799927, loss=0.9167954921722412
I0218 16:06:36.948170 140379146925824 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.3178271055221558, loss=0.9287607669830322
I0218 16:07:55.892767 140379146925824 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.1095026731491089, loss=0.925902247428894
I0218 16:09:11.308679 140379138533120 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.320581316947937, loss=0.8766138553619385
I0218 16:10:26.966438 140379146925824 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.1909178495407104, loss=0.8947389125823975
I0218 16:11:42.498470 140379138533120 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.3865656852722168, loss=0.9169108867645264
I0218 16:12:58.123280 140379146925824 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.0746514797210693, loss=0.8797255158424377
I0218 16:14:13.729982 140379138533120 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.1701276302337646, loss=0.9231076240539551
I0218 16:15:29.364612 140379146925824 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.1416898965835571, loss=0.9010260701179504
I0218 16:16:44.963743 140379138533120 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.1331552267074585, loss=0.8878821134567261
I0218 16:18:06.142710 140379146925824 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.1570688486099243, loss=0.8650234937667847
I0218 16:19:27.308805 140379138533120 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.1175963878631592, loss=0.8454088568687439
I0218 16:20:48.149292 140379146925824 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.036981225013733, loss=0.8680400848388672
I0218 16:22:07.012559 140379146925824 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.0437158346176147, loss=0.9049582481384277
I0218 16:23:22.529970 140379138533120 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.0410619974136353, loss=0.8576867580413818
I0218 16:24:38.231588 140379146925824 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.1768466234207153, loss=0.8836310505867004
I0218 16:25:53.792225 140379138533120 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.446479082107544, loss=0.8407605886459351
I0218 16:26:14.618130 140549388556096 spec.py:321] Evaluating on the training split.
I0218 16:27:07.198066 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 16:27:57.994075 140549388556096 spec.py:349] Evaluating on the test split.
I0218 16:28:23.184524 140549388556096 submission_runner.py:408] Time since start: 58294.04s, 	Step: 69429, 	{'train/ctc_loss': Array(0.08786926, dtype=float32), 'train/wer': 0.034247645404633306, 'validation/ctc_loss': Array(0.30810562, dtype=float32), 'validation/wer': 0.0879442347239252, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16458964, dtype=float32), 'test/wer': 0.05348038916986574, 'test/num_examples': 2472, 'score': 53350.63709568977, 'total_duration': 58294.03611445427, 'accumulated_submission_time': 53350.63709568977, 'accumulated_eval_time': 4938.435527801514, 'accumulated_logging_time': 2.0534727573394775}
I0218 16:28:23.228452 140379146925824 logging_writer.py:48] [69429] accumulated_eval_time=4938.435528, accumulated_logging_time=2.053473, accumulated_submission_time=53350.637096, global_step=69429, preemption_count=0, score=53350.637096, test/ctc_loss=0.16458964347839355, test/num_examples=2472, test/wer=0.053480, total_duration=58294.036114, train/ctc_loss=0.08786925673484802, train/wer=0.034248, validation/ctc_loss=0.30810561776161194, validation/num_examples=5348, validation/wer=0.087944
I0218 16:29:17.421128 140379138533120 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.2339527606964111, loss=0.8693789839744568
I0218 16:30:32.914020 140379146925824 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.1556861400604248, loss=0.8974502086639404
I0218 16:31:48.402232 140379138533120 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.1045094728469849, loss=0.8790682554244995
I0218 16:33:03.937350 140379146925824 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.3544234037399292, loss=0.8449192643165588
I0218 16:34:19.379184 140379138533120 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.4783709049224854, loss=0.8777886629104614
I0218 16:35:34.992830 140379146925824 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.2386243343353271, loss=0.9004697203636169
I0218 16:36:54.931819 140379146925824 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.1695728302001953, loss=0.9045742154121399
I0218 16:38:10.474663 140379138533120 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.3188093900680542, loss=0.8772760033607483
I0218 16:39:26.245663 140379146925824 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.2846680879592896, loss=0.8736333847045898
I0218 16:40:41.685358 140379138533120 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.9716013073921204, loss=0.8299609422683716
I0218 16:41:57.259450 140379146925824 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.1080843210220337, loss=0.8759157061576843
I0218 16:43:12.833651 140379138533120 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.1092307567596436, loss=0.8413245677947998
I0218 16:44:28.324057 140379146925824 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.0194878578186035, loss=0.8667481541633606
I0218 16:45:45.754868 140379138533120 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.366565465927124, loss=0.9261156916618347
I0218 16:47:06.910852 140379146925824 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.2143912315368652, loss=0.9092151522636414
I0218 16:48:27.322129 140379138533120 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.3263109922409058, loss=0.8847230076789856
I0218 16:49:49.443357 140379146925824 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.427167296409607, loss=0.8000816106796265
I0218 16:51:05.079598 140379138533120 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.1579363346099854, loss=0.8680912852287292
I0218 16:52:20.828340 140379146925824 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.158583641052246, loss=0.8628966212272644
I0218 16:52:23.578039 140549388556096 spec.py:321] Evaluating on the training split.
I0218 16:53:16.040144 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 16:54:06.855224 140549388556096 spec.py:349] Evaluating on the test split.
I0218 16:54:32.392447 140549388556096 submission_runner.py:408] Time since start: 59863.25s, 	Step: 71305, 	{'train/ctc_loss': Array(0.08843144, dtype=float32), 'train/wer': 0.03386530173027323, 'validation/ctc_loss': Array(0.30182338, dtype=float32), 'validation/wer': 0.08612915994863725, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16141044, dtype=float32), 'test/wer': 0.051753904901184165, 'test/num_examples': 2472, 'score': 54790.89835476875, 'total_duration': 59863.24539685249, 'accumulated_submission_time': 54790.89835476875, 'accumulated_eval_time': 5067.244613170624, 'accumulated_logging_time': 2.112919569015503}
I0218 16:54:32.440142 140379146925824 logging_writer.py:48] [71305] accumulated_eval_time=5067.244613, accumulated_logging_time=2.112920, accumulated_submission_time=54790.898355, global_step=71305, preemption_count=0, score=54790.898355, test/ctc_loss=0.16141043603420258, test/num_examples=2472, test/wer=0.051754, total_duration=59863.245397, train/ctc_loss=0.08843144029378891, train/wer=0.033865, validation/ctc_loss=0.30182337760925293, validation/num_examples=5348, validation/wer=0.086129
I0218 16:55:45.028865 140379138533120 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.211355209350586, loss=0.8719413876533508
I0218 16:57:00.675940 140379146925824 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.57442045211792, loss=0.8709533214569092
I0218 16:58:16.395698 140379138533120 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.198644757270813, loss=0.8605261445045471
I0218 16:59:32.067925 140379146925824 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.2228829860687256, loss=0.8578049540519714
I0218 17:00:47.680841 140379138533120 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.1967895030975342, loss=0.8596232533454895
I0218 17:02:03.297444 140379146925824 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.6004573106765747, loss=0.8975633978843689
I0218 17:03:19.132841 140379138533120 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.3058156967163086, loss=0.8411715626716614
I0218 17:04:43.386414 140379146925824 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.2561044692993164, loss=0.8326201438903809
I0218 17:05:59.027194 140379138533120 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.4827083349227905, loss=0.8168684244155884
I0218 17:07:14.926685 140379146925824 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.3448656797409058, loss=0.8727087378501892
I0218 17:08:30.733877 140379138533120 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.4035893678665161, loss=0.8833938241004944
I0218 17:09:46.780747 140379146925824 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.2378640174865723, loss=0.857934296131134
I0218 17:11:02.580583 140379138533120 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.1712459325790405, loss=0.8658624887466431
I0218 17:12:18.356944 140379146925824 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.1839065551757812, loss=0.8675623536109924
I0218 17:13:34.159880 140379138533120 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.1718531847000122, loss=0.8401504158973694
I0218 17:14:51.276732 140379146925824 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.33319890499115, loss=0.8527045845985413
I0218 17:16:11.062776 140379138533120 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.354349970817566, loss=0.8756176233291626
I0218 17:17:31.637829 140379146925824 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.319333553314209, loss=0.8731091022491455
I0218 17:18:32.833323 140549388556096 spec.py:321] Evaluating on the training split.
I0218 17:19:26.100301 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 17:20:16.765135 140549388556096 spec.py:349] Evaluating on the test split.
I0218 17:20:42.254466 140549388556096 submission_runner.py:408] Time since start: 61433.11s, 	Step: 73177, 	{'train/ctc_loss': Array(0.08011135, dtype=float32), 'train/wer': 0.030846805751213076, 'validation/ctc_loss': Array(0.30042586, dtype=float32), 'validation/wer': 0.08493198296919201, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16003639, dtype=float32), 'test/wer': 0.05112424593260618, 'test/num_examples': 2472, 'score': 56231.20172023773, 'total_duration': 61433.10589194298, 'accumulated_submission_time': 56231.20172023773, 'accumulated_eval_time': 5196.659123182297, 'accumulated_logging_time': 2.1780107021331787}
I0218 17:20:42.307370 140379146925824 logging_writer.py:48] [73177] accumulated_eval_time=5196.659123, accumulated_logging_time=2.178011, accumulated_submission_time=56231.201720, global_step=73177, preemption_count=0, score=56231.201720, test/ctc_loss=0.1600363850593567, test/num_examples=2472, test/wer=0.051124, total_duration=61433.105892, train/ctc_loss=0.08011134713888168, train/wer=0.030847, validation/ctc_loss=0.30042585730552673, validation/num_examples=5348, validation/wer=0.084932
I0218 17:21:00.410196 140379138533120 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.0572785139083862, loss=0.8545621633529663
I0218 17:22:15.748419 140379146925824 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.2566895484924316, loss=0.8681015372276306
I0218 17:23:31.239131 140379138533120 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.3762702941894531, loss=0.8801122903823853
I0218 17:24:47.008427 140379146925824 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.2107700109481812, loss=0.8945423364639282
I0218 17:26:02.589001 140379138533120 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.4480570554733276, loss=0.8595209121704102
I0218 17:27:18.112726 140379146925824 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.1036266088485718, loss=0.8177822232246399
I0218 17:28:33.699509 140379138533120 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.254418969154358, loss=0.8454362750053406
I0218 17:29:49.202662 140379146925824 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.1438546180725098, loss=0.8511195182800293
I0218 17:31:04.725065 140379138533120 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.381453514099121, loss=0.8437974452972412
I0218 17:32:21.945294 140379146925824 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.395973563194275, loss=0.8926815390586853
I0218 17:33:43.333562 140379146925824 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.4646286964416504, loss=0.8388921618461609
I0218 17:34:58.816439 140379138533120 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.1268656253814697, loss=0.835597038269043
I0218 17:36:14.249485 140379146925824 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.057088851928711, loss=0.8355075120925903
I0218 17:37:29.710847 140379138533120 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.9249658584594727, loss=0.8443030714988708
I0218 17:38:45.333339 140379146925824 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.2071094512939453, loss=0.8638964295387268
I0218 17:40:00.811101 140379138533120 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.5084692239761353, loss=0.869285523891449
I0218 17:41:16.265411 140379146925824 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.365169644355774, loss=0.8667751550674438
I0218 17:42:31.815577 140379138533120 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.412030816078186, loss=0.8543666005134583
I0218 17:43:51.807927 140379146925824 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.0439484119415283, loss=0.7954844832420349
I0218 17:44:43.066496 140549388556096 spec.py:321] Evaluating on the training split.
I0218 17:45:36.965137 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 17:46:27.461714 140549388556096 spec.py:349] Evaluating on the test split.
I0218 17:46:52.923338 140549388556096 submission_runner.py:408] Time since start: 63003.78s, 	Step: 75065, 	{'train/ctc_loss': Array(0.07875839, dtype=float32), 'train/wer': 0.029531895110855296, 'validation/ctc_loss': Array(0.2965718, dtype=float32), 'validation/wer': 0.08304932562248375, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15909529, dtype=float32), 'test/wer': 0.05025084800844962, 'test/num_examples': 2472, 'score': 57671.870633125305, 'total_duration': 63003.7757563591, 'accumulated_submission_time': 57671.870633125305, 'accumulated_eval_time': 5326.510108470917, 'accumulated_logging_time': 2.247305393218994}
I0218 17:46:52.969821 140379146925824 logging_writer.py:48] [75065] accumulated_eval_time=5326.510108, accumulated_logging_time=2.247305, accumulated_submission_time=57671.870633, global_step=75065, preemption_count=0, score=57671.870633, test/ctc_loss=0.15909528732299805, test/num_examples=2472, test/wer=0.050251, total_duration=63003.775756, train/ctc_loss=0.07875838875770569, train/wer=0.029532, validation/ctc_loss=0.2965717911720276, validation/num_examples=5348, validation/wer=0.083049
I0218 17:47:20.094108 140379138533120 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.7513093948364258, loss=0.8350453972816467
I0218 17:48:38.920463 140379146925824 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.2606221437454224, loss=0.8808839321136475
I0218 17:49:54.307554 140379138533120 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.3413219451904297, loss=0.8132797479629517
I0218 17:51:09.976545 140379146925824 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.1359190940856934, loss=0.8038904070854187
I0218 17:52:25.390305 140379138533120 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.2596313953399658, loss=0.8576257824897766
I0218 17:53:40.940500 140379146925824 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.339965581893921, loss=0.8414386510848999
I0218 17:54:56.710154 140379138533120 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.2004305124282837, loss=0.8039103150367737
I0218 17:56:12.272411 140379146925824 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.223645567893982, loss=0.8530585765838623
I0218 17:57:27.781540 140379138533120 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.2942875623703003, loss=0.8632519245147705
I0218 17:58:48.483484 140379146925824 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.4695545434951782, loss=0.8679482340812683
I0218 18:00:08.422129 140379138533120 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.399017095565796, loss=0.9084104895591736
I0218 18:01:27.811211 140379146925824 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.4568923711776733, loss=0.836511492729187
I0218 18:02:46.979005 140379146925824 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.2626994848251343, loss=0.8297349214553833
I0218 18:04:02.653836 140379138533120 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.5076814889907837, loss=0.8561256527900696
I0218 18:05:18.166370 140379146925824 logging_writer.py:48] [76500] global_step=76500, grad_norm=1.1772115230560303, loss=0.8349089622497559
I0218 18:06:33.655124 140379138533120 logging_writer.py:48] [76600] global_step=76600, grad_norm=1.1710295677185059, loss=0.8392590880393982
I0218 18:07:49.395719 140379146925824 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.2929476499557495, loss=0.8073832988739014
I0218 18:09:05.059410 140379138533120 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.2389276027679443, loss=0.8019421696662903
I0218 18:10:20.771761 140379146925824 logging_writer.py:48] [76900] global_step=76900, grad_norm=1.0642116069793701, loss=0.8570088744163513
I0218 18:10:53.732284 140549388556096 spec.py:321] Evaluating on the training split.
I0218 18:11:46.949619 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 18:12:38.362118 140549388556096 spec.py:349] Evaluating on the test split.
I0218 18:13:03.943714 140549388556096 submission_runner.py:408] Time since start: 64574.80s, 	Step: 76942, 	{'train/ctc_loss': Array(0.06596456, dtype=float32), 'train/wer': 0.025091449797312197, 'validation/ctc_loss': Array(0.2928794, dtype=float32), 'validation/wer': 0.08256659296948164, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15672454, dtype=float32), 'test/wer': 0.05053521012329129, 'test/num_examples': 2472, 'score': 59112.54464316368, 'total_duration': 64574.79559850693, 'accumulated_submission_time': 59112.54464316368, 'accumulated_eval_time': 5456.7151482105255, 'accumulated_logging_time': 2.3087172508239746}
I0218 18:13:03.990455 140379146925824 logging_writer.py:48] [76942] accumulated_eval_time=5456.715148, accumulated_logging_time=2.308717, accumulated_submission_time=59112.544643, global_step=76942, preemption_count=0, score=59112.544643, test/ctc_loss=0.15672454237937927, test/num_examples=2472, test/wer=0.050535, total_duration=64574.795599, train/ctc_loss=0.06596455723047256, train/wer=0.025091, validation/ctc_loss=0.2928794026374817, validation/num_examples=5348, validation/wer=0.082567
I0218 18:13:48.461345 140379138533120 logging_writer.py:48] [77000] global_step=77000, grad_norm=1.5020737648010254, loss=0.841995358467102
I0218 18:15:04.032799 140379146925824 logging_writer.py:48] [77100] global_step=77100, grad_norm=1.1585103273391724, loss=0.8657122254371643
I0218 18:16:19.587960 140379138533120 logging_writer.py:48] [77200] global_step=77200, grad_norm=1.2956209182739258, loss=0.8806416392326355
I0218 18:17:38.314832 140379146925824 logging_writer.py:48] [77300] global_step=77300, grad_norm=1.5075682401657104, loss=0.861769437789917
I0218 18:18:53.889023 140379138533120 logging_writer.py:48] [77400] global_step=77400, grad_norm=1.395099401473999, loss=0.8344616889953613
I0218 18:20:09.396504 140379146925824 logging_writer.py:48] [77500] global_step=77500, grad_norm=1.3902047872543335, loss=0.8233039379119873
I0218 18:21:24.983877 140379138533120 logging_writer.py:48] [77600] global_step=77600, grad_norm=1.5727750062942505, loss=0.8672876954078674
I0218 18:22:40.566934 140379146925824 logging_writer.py:48] [77700] global_step=77700, grad_norm=1.1671086549758911, loss=0.8975574374198914
I0218 18:23:56.351073 140379138533120 logging_writer.py:48] [77800] global_step=77800, grad_norm=1.2916375398635864, loss=0.7801176309585571
I0218 18:25:11.903344 140379146925824 logging_writer.py:48] [77900] global_step=77900, grad_norm=1.3506886959075928, loss=0.8561873435974121
I0218 18:26:30.132034 140379138533120 logging_writer.py:48] [78000] global_step=78000, grad_norm=1.6050375699996948, loss=0.8624452948570251
I0218 18:27:50.474679 140379146925824 logging_writer.py:48] [78100] global_step=78100, grad_norm=1.2973798513412476, loss=0.7978037595748901
I0218 18:29:11.061340 140379138533120 logging_writer.py:48] [78200] global_step=78200, grad_norm=1.2420088052749634, loss=0.8539520502090454
I0218 18:30:34.126571 140379146925824 logging_writer.py:48] [78300] global_step=78300, grad_norm=1.1392391920089722, loss=0.8376062512397766
I0218 18:31:49.641350 140379138533120 logging_writer.py:48] [78400] global_step=78400, grad_norm=1.362955927848816, loss=0.7981073260307312
I0218 18:33:05.161108 140379146925824 logging_writer.py:48] [78500] global_step=78500, grad_norm=1.156031608581543, loss=0.8309623599052429
I0218 18:34:20.652685 140379138533120 logging_writer.py:48] [78600] global_step=78600, grad_norm=1.385284185409546, loss=0.8108392357826233
I0218 18:35:36.223639 140379146925824 logging_writer.py:48] [78700] global_step=78700, grad_norm=1.3997142314910889, loss=0.8495641350746155
I0218 18:36:51.712407 140379138533120 logging_writer.py:48] [78800] global_step=78800, grad_norm=1.2410110235214233, loss=0.806736409664154
I0218 18:37:04.018222 140549388556096 spec.py:321] Evaluating on the training split.
I0218 18:37:55.876077 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 18:38:46.503878 140549388556096 spec.py:349] Evaluating on the test split.
I0218 18:39:12.134097 140549388556096 submission_runner.py:408] Time since start: 66142.99s, 	Step: 78817, 	{'train/ctc_loss': Array(0.06438663, dtype=float32), 'train/wer': 0.02406642764495647, 'validation/ctc_loss': Array(0.29217574, dtype=float32), 'validation/wer': 0.0821224789287197, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15655187, dtype=float32), 'test/wer': 0.05000710905287104, 'test/num_examples': 2472, 'score': 60552.484139442444, 'total_duration': 66142.98616504669, 'accumulated_submission_time': 60552.484139442444, 'accumulated_eval_time': 5584.824814081192, 'accumulated_logging_time': 2.3721067905426025}
I0218 18:39:12.184762 140379146925824 logging_writer.py:48] [78817] accumulated_eval_time=5584.824814, accumulated_logging_time=2.372107, accumulated_submission_time=60552.484139, global_step=78817, preemption_count=0, score=60552.484139, test/ctc_loss=0.15655186772346497, test/num_examples=2472, test/wer=0.050007, total_duration=66142.986165, train/ctc_loss=0.06438662856817245, train/wer=0.024066, validation/ctc_loss=0.2921757400035858, validation/num_examples=5348, validation/wer=0.082122
I0218 18:40:15.391486 140379138533120 logging_writer.py:48] [78900] global_step=78900, grad_norm=1.5362868309020996, loss=0.7830654382705688
I0218 18:41:31.018648 140379146925824 logging_writer.py:48] [79000] global_step=79000, grad_norm=1.0829719305038452, loss=0.813539445400238
I0218 18:42:46.647934 140379138533120 logging_writer.py:48] [79100] global_step=79100, grad_norm=1.4034619331359863, loss=0.817193865776062
I0218 18:44:02.192848 140379146925824 logging_writer.py:48] [79200] global_step=79200, grad_norm=1.3533320426940918, loss=0.8531072735786438
I0218 18:45:17.765459 140379138533120 logging_writer.py:48] [79300] global_step=79300, grad_norm=1.3268357515335083, loss=0.8619628548622131
I0218 18:46:36.513797 140379146925824 logging_writer.py:48] [79400] global_step=79400, grad_norm=1.4131543636322021, loss=0.8320515751838684
I0218 18:47:47.972845 140379138533120 logging_writer.py:48] [79496] global_step=79496, preemption_count=0, score=61068.208966
I0218 18:47:48.911611 140549388556096 checkpoints.py:490] Saving checkpoint at step: 79496
I0218 18:47:50.486788 140549388556096 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_3/librispeech_conformer_jax/trial_1/checkpoint_79496
I0218 18:47:50.523888 140549388556096 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_3/librispeech_conformer_jax/trial_1/checkpoint_79496.
I0218 18:47:54.079016 140549388556096 submission_runner.py:583] Tuning trial 1/5
I0218 18:47:54.079303 140549388556096 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0218 18:47:54.102923 140549388556096 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.169794, dtype=float32), 'train/wer': 1.120843137663203, 'validation/ctc_loss': Array(30.090126, dtype=float32), 'validation/wer': 0.9587360128213792, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.214182, dtype=float32), 'test/wer': 0.9757885970791949, 'test/num_examples': 2472, 'score': 61.41118311882019, 'total_duration': 232.68418264389038, 'accumulated_submission_time': 61.41118311882019, 'accumulated_eval_time': 171.2729423046112, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1862, {'train/ctc_loss': Array(5.747149, dtype=float32), 'train/wer': 0.9443883751107083, 'validation/ctc_loss': Array(5.8983974, dtype=float32), 'validation/wer': 0.8964635005841065, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.7778125, dtype=float32), 'test/wer': 0.8994576808238377, 'test/num_examples': 2472, 'score': 1501.6815497875214, 'total_duration': 1776.0325355529785, 'accumulated_submission_time': 1501.6815497875214, 'accumulated_eval_time': 274.23418140411377, 'accumulated_logging_time': 0.043993473052978516, 'global_step': 1862, 'preemption_count': 0}), (3751, {'train/ctc_loss': Array(2.6828344, dtype=float32), 'train/wer': 0.569682763764217, 'validation/ctc_loss': Array(3.0748646, dtype=float32), 'validation/wer': 0.6164013246183998, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.7431552, dtype=float32), 'test/wer': 0.5617167347104585, 'test/num_examples': 2472, 'score': 2942.039139032364, 'total_duration': 3340.9508938789368, 'accumulated_submission_time': 2942.039139032364, 'accumulated_eval_time': 398.6696357727051, 'accumulated_logging_time': 0.09018349647521973, 'global_step': 3751, 'preemption_count': 0}), (5646, {'train/ctc_loss': Array(0.7053174, dtype=float32), 'train/wer': 0.24406597403418884, 'validation/ctc_loss': Array(1.0583082, dtype=float32), 'validation/wer': 0.3103391679619993, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.7743883, dtype=float32), 'test/wer': 0.24930432839761948, 'test/num_examples': 2472, 'score': 4382.566341638565, 'total_duration': 4909.4394245147705, 'accumulated_submission_time': 4382.566341638565, 'accumulated_eval_time': 526.5003283023834, 'accumulated_logging_time': 0.14023447036743164, 'global_step': 5646, 'preemption_count': 0}), (7541, {'train/ctc_loss': Array(0.50713336, dtype=float32), 'train/wer': 0.17643393479877265, 'validation/ctc_loss': Array(0.8168226, dtype=float32), 'validation/wer': 0.2444944340925109, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5533391, dtype=float32), 'test/wer': 0.185343164137875, 'test/num_examples': 2472, 'score': 5822.735749721527, 'total_duration': 6478.064398050308, 'accumulated_submission_time': 5822.735749721527, 'accumulated_eval_time': 654.8250741958618, 'accumulated_logging_time': 0.19190430641174316, 'global_step': 7541, 'preemption_count': 0}), (9419, {'train/ctc_loss': Array(0.42972216, dtype=float32), 'train/wer': 0.15163952249450954, 'validation/ctc_loss': Array(0.7233104, dtype=float32), 'validation/wer': 0.21718142058565126, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.47338814, dtype=float32), 'test/wer': 0.15836938638717932, 'test/num_examples': 2472, 'score': 7262.69296336174, 'total_duration': 8045.841583013535, 'accumulated_submission_time': 7262.69296336174, 'accumulated_eval_time': 782.5141160488129, 'accumulated_logging_time': 0.24313855171203613, 'global_step': 9419, 'preemption_count': 0}), (11299, {'train/ctc_loss': Array(0.4041753, dtype=float32), 'train/wer': 0.14432038239326545, 'validation/ctc_loss': Array(0.6717132, dtype=float32), 'validation/wer': 0.20635855450534385, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4285467, dtype=float32), 'test/wer': 0.14736051022688035, 'test/num_examples': 2472, 'score': 8702.930635690689, 'total_duration': 9615.45825767517, 'accumulated_submission_time': 8702.930635690689, 'accumulated_eval_time': 911.7616634368896, 'accumulated_logging_time': 0.29471397399902344, 'global_step': 11299, 'preemption_count': 0}), (13172, {'train/ctc_loss': Array(0.34445938, dtype=float32), 'train/wer': 0.12448152638631868, 'validation/ctc_loss': Array(0.6240188, dtype=float32), 'validation/wer': 0.1905442327929946, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3915885, dtype=float32), 'test/wer': 0.13375175187374322, 'test/num_examples': 2472, 'score': 10143.447443723679, 'total_duration': 11187.017598867416, 'accumulated_submission_time': 10143.447443723679, 'accumulated_eval_time': 1042.6672360897064, 'accumulated_logging_time': 0.353374719619751, 'global_step': 13172, 'preemption_count': 0}), (15049, {'train/ctc_loss': Array(0.29312336, dtype=float32), 'train/wer': 0.10872877609922255, 'validation/ctc_loss': Array(0.5906545, dtype=float32), 'validation/wer': 0.1808702704268322, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36472577, dtype=float32), 'test/wer': 0.12491621473401987, 'test/num_examples': 2472, 'score': 11583.814504861832, 'total_duration': 12757.273002386093, 'accumulated_submission_time': 11583.814504861832, 'accumulated_eval_time': 1172.4215152263641, 'accumulated_logging_time': 0.40801548957824707, 'global_step': 15049, 'preemption_count': 0}), (16926, {'train/ctc_loss': Array(0.27578232, dtype=float32), 'train/wer': 0.10147949927913645, 'validation/ctc_loss': Array(0.57158834, dtype=float32), 'validation/wer': 0.17360031667262038, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3517897, dtype=float32), 'test/wer': 0.11868055978713464, 'test/num_examples': 2472, 'score': 13024.158386707306, 'total_duration': 14326.901668548584, 'accumulated_submission_time': 13024.158386707306, 'accumulated_eval_time': 1301.5782821178436, 'accumulated_logging_time': 0.4575080871582031, 'global_step': 16926, 'preemption_count': 0}), (18807, {'train/ctc_loss': Array(0.26276767, dtype=float32), 'train/wer': 0.09987826264164705, 'validation/ctc_loss': Array(0.5514074, dtype=float32), 'validation/wer': 0.16706411655097175, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3391219, dtype=float32), 'test/wer': 0.11443543964414113, 'test/num_examples': 2472, 'score': 14464.73980808258, 'total_duration': 15896.72179198265, 'accumulated_submission_time': 14464.73980808258, 'accumulated_eval_time': 1430.684591293335, 'accumulated_logging_time': 0.510749340057373, 'global_step': 18807, 'preemption_count': 0}), (20678, {'train/ctc_loss': Array(0.2670035, dtype=float32), 'train/wer': 0.09698828631111346, 'validation/ctc_loss': Array(0.5288567, dtype=float32), 'validation/wer': 0.15960106973555904, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32171547, dtype=float32), 'test/wer': 0.10734669835273089, 'test/num_examples': 2472, 'score': 15905.15338897705, 'total_duration': 17468.002720832825, 'accumulated_submission_time': 15905.15338897705, 'accumulated_eval_time': 1561.4255406856537, 'accumulated_logging_time': 0.5602102279663086, 'global_step': 20678, 'preemption_count': 0}), (22560, {'train/ctc_loss': Array(0.2625285, dtype=float32), 'train/wer': 0.09521196612762814, 'validation/ctc_loss': Array(0.5192188, dtype=float32), 'validation/wer': 0.15608677602170365, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31339175, dtype=float32), 'test/wer': 0.10649361200820588, 'test/num_examples': 2472, 'score': 17345.456032276154, 'total_duration': 19039.672203540802, 'accumulated_submission_time': 17345.456032276154, 'accumulated_eval_time': 1692.6593585014343, 'accumulated_logging_time': 0.6153922080993652, 'global_step': 22560, 'preemption_count': 0}), (24436, {'train/ctc_loss': Array(0.2446675, dtype=float32), 'train/wer': 0.08934561168844532, 'validation/ctc_loss': Array(0.50914836, dtype=float32), 'validation/wer': 0.15377931394035355, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30653575, dtype=float32), 'test/wer': 0.10352812138199988, 'test/num_examples': 2472, 'score': 18785.565538167953, 'total_duration': 20611.015005350113, 'accumulated_submission_time': 18785.565538167953, 'accumulated_eval_time': 1823.7593805789948, 'accumulated_logging_time': 0.6687924861907959, 'global_step': 24436, 'preemption_count': 0}), (26310, {'train/ctc_loss': Array(0.22402444, dtype=float32), 'train/wer': 0.08299375026392466, 'validation/ctc_loss': Array(0.4923003, dtype=float32), 'validation/wer': 0.14871062108383135, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29196113, dtype=float32), 'test/wer': 0.09853147279263909, 'test/num_examples': 2472, 'score': 20225.87024140358, 'total_duration': 22181.484354496002, 'accumulated_submission_time': 20225.87024140358, 'accumulated_eval_time': 1953.7842502593994, 'accumulated_logging_time': 0.7299957275390625, 'global_step': 26310, 'preemption_count': 0}), (28188, {'train/ctc_loss': Array(0.21112266, dtype=float32), 'train/wer': 0.0805446151438483, 'validation/ctc_loss': Array(0.47902232, dtype=float32), 'validation/wer': 0.14473290402309394, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28334022, dtype=float32), 'test/wer': 0.0969268580017468, 'test/num_examples': 2472, 'score': 21666.047757864, 'total_duration': 23751.79039502144, 'accumulated_submission_time': 21666.047757864, 'accumulated_eval_time': 2083.775384426117, 'accumulated_logging_time': 0.7895841598510742, 'global_step': 28188, 'preemption_count': 0}), (30062, {'train/ctc_loss': Array(0.21464497, dtype=float32), 'train/wer': 0.07797813257509896, 'validation/ctc_loss': Array(0.48762044, dtype=float32), 'validation/wer': 0.14551493092095735, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2829486, dtype=float32), 'test/wer': 0.09359575894217294, 'test/num_examples': 2472, 'score': 23105.966525793076, 'total_duration': 25321.955672502518, 'accumulated_submission_time': 23105.966525793076, 'accumulated_eval_time': 2213.889081478119, 'accumulated_logging_time': 0.8440456390380859, 'global_step': 30062, 'preemption_count': 0}), (31936, {'train/ctc_loss': Array(0.22176528, dtype=float32), 'train/wer': 0.08092823712948517, 'validation/ctc_loss': Array(0.46996224, dtype=float32), 'validation/wer': 0.13909458663602922, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27928066, dtype=float32), 'test/wer': 0.0920114557309122, 'test/num_examples': 2472, 'score': 24546.469081401825, 'total_duration': 26893.146836042404, 'accumulated_submission_time': 24546.469081401825, 'accumulated_eval_time': 2344.442668914795, 'accumulated_logging_time': 0.900031566619873, 'global_step': 31936, 'preemption_count': 0}), (33828, {'train/ctc_loss': Array(0.20109509, dtype=float32), 'train/wer': 0.07282188183332519, 'validation/ctc_loss': Array(0.4561879, dtype=float32), 'validation/wer': 0.13637197447309732, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2653281, dtype=float32), 'test/wer': 0.08896471878618, 'test/num_examples': 2472, 'score': 25986.34561252594, 'total_duration': 28462.906080007553, 'accumulated_submission_time': 25986.34561252594, 'accumulated_eval_time': 2474.185190677643, 'accumulated_logging_time': 0.9610598087310791, 'global_step': 33828, 'preemption_count': 0}), (35705, {'train/ctc_loss': Array(0.20980647, dtype=float32), 'train/wer': 0.07649394613420729, 'validation/ctc_loss': Array(0.45002848, dtype=float32), 'validation/wer': 0.13602440696293577, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26111528, dtype=float32), 'test/wer': 0.08673044502670972, 'test/num_examples': 2472, 'score': 27426.305513381958, 'total_duration': 30035.209615707397, 'accumulated_submission_time': 27426.305513381958, 'accumulated_eval_time': 2606.3973863124847, 'accumulated_logging_time': 1.0138370990753174, 'global_step': 35705, 'preemption_count': 0}), (37585, {'train/ctc_loss': Array(0.19981158, dtype=float32), 'train/wer': 0.07120325671110543, 'validation/ctc_loss': Array(0.4394024, dtype=float32), 'validation/wer': 0.1306178012493121, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2542946, dtype=float32), 'test/wer': 0.08490240285987041, 'test/num_examples': 2472, 'score': 28866.55081510544, 'total_duration': 31605.695909023285, 'accumulated_submission_time': 28866.55081510544, 'accumulated_eval_time': 2736.506334066391, 'accumulated_logging_time': 1.0663466453552246, 'global_step': 37585, 'preemption_count': 0}), (39462, {'train/ctc_loss': Array(0.15873039, dtype=float32), 'train/wer': 0.0605711181492984, 'validation/ctc_loss': Array(0.43230936, dtype=float32), 'validation/wer': 0.12975853712696833, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24808052, dtype=float32), 'test/wer': 0.08443523652834481, 'test/num_examples': 2472, 'score': 30306.92392349243, 'total_duration': 33176.732083559036, 'accumulated_submission_time': 30306.92392349243, 'accumulated_eval_time': 2867.0316922664642, 'accumulated_logging_time': 1.1251184940338135, 'global_step': 39462, 'preemption_count': 0}), (41334, {'train/ctc_loss': Array(0.17622687, dtype=float32), 'train/wer': 0.06372093514392516, 'validation/ctc_loss': Array(0.4257966, dtype=float32), 'validation/wer': 0.1274896936578584, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24189256, dtype=float32), 'test/wer': 0.08045416692056141, 'test/num_examples': 2472, 'score': 31747.11150074005, 'total_duration': 34747.760195970535, 'accumulated_submission_time': 31747.11150074005, 'accumulated_eval_time': 2997.738777399063, 'accumulated_logging_time': 1.1801607608795166, 'global_step': 41334, 'preemption_count': 0}), (43207, {'train/ctc_loss': Array(0.21449152, dtype=float32), 'train/wer': 0.07816644337529764, 'validation/ctc_loss': Array(0.4130111, dtype=float32), 'validation/wer': 0.12294235206657848, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23688465, dtype=float32), 'test/wer': 0.07982450795198343, 'test/num_examples': 2472, 'score': 33187.53070783615, 'total_duration': 36317.85943412781, 'accumulated_submission_time': 33187.53070783615, 'accumulated_eval_time': 3127.2881696224213, 'accumulated_logging_time': 1.2326452732086182, 'global_step': 43207, 'preemption_count': 0}), (45073, {'train/ctc_loss': Array(0.21359752, dtype=float32), 'train/wer': 0.07842364422633884, 'validation/ctc_loss': Array(0.4034216, dtype=float32), 'validation/wer': 0.12121416916883092, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23037176, dtype=float32), 'test/wer': 0.07590437308309467, 'test/num_examples': 2472, 'score': 34627.65042424202, 'total_duration': 37887.25235772133, 'accumulated_submission_time': 34627.65042424202, 'accumulated_eval_time': 3256.428151845932, 'accumulated_logging_time': 1.2872276306152344, 'global_step': 45073, 'preemption_count': 0}), (46943, {'train/ctc_loss': Array(0.25127593, dtype=float32), 'train/wer': 0.09323139379120332, 'validation/ctc_loss': Array(0.40328342, dtype=float32), 'validation/wer': 0.12057696206686813, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22683704, dtype=float32), 'test/wer': 0.07783397314809172, 'test/num_examples': 2472, 'score': 36067.74671292305, 'total_duration': 39455.31745886803, 'accumulated_submission_time': 36067.74671292305, 'accumulated_eval_time': 3384.257961034775, 'accumulated_logging_time': 1.3474400043487549, 'global_step': 46943, 'preemption_count': 0}), (48821, {'train/ctc_loss': Array(0.21432593, dtype=float32), 'train/wer': 0.07740404712421033, 'validation/ctc_loss': Array(0.39701816, dtype=float32), 'validation/wer': 0.11645442521023007, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22061086, dtype=float32), 'test/wer': 0.07182174557715354, 'test/num_examples': 2472, 'score': 37508.35221242905, 'total_duration': 41024.62561440468, 'accumulated_submission_time': 37508.35221242905, 'accumulated_eval_time': 3512.8210985660553, 'accumulated_logging_time': 1.4097182750701904, 'global_step': 48821, 'preemption_count': 0}), (50688, {'train/ctc_loss': Array(0.19890381, dtype=float32), 'train/wer': 0.07448253593577767, 'validation/ctc_loss': Array(0.39558187, dtype=float32), 'validation/wer': 0.11495795398592351, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21984032, dtype=float32), 'test/wer': 0.07275607824020475, 'test/num_examples': 2472, 'score': 38948.3796274662, 'total_duration': 42593.2292907238, 'accumulated_submission_time': 38948.3796274662, 'accumulated_eval_time': 3641.26109457016, 'accumulated_logging_time': 1.4670865535736084, 'global_step': 50688, 'preemption_count': 0}), (52542, {'train/ctc_loss': Array(0.1622737, dtype=float32), 'train/wer': 0.06263798409988146, 'validation/ctc_loss': Array(0.37560198, dtype=float32), 'validation/wer': 0.11092230900682584, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21027523, dtype=float32), 'test/wer': 0.07037962342331364, 'test/num_examples': 2472, 'score': 40388.371878385544, 'total_duration': 44164.18422412872, 'accumulated_submission_time': 40388.371878385544, 'accumulated_eval_time': 3772.0892395973206, 'accumulated_logging_time': 1.5247292518615723, 'global_step': 52542, 'preemption_count': 0}), (54436, {'train/ctc_loss': Array(0.17281756, dtype=float32), 'train/wer': 0.06613370119537058, 'validation/ctc_loss': Array(0.36489117, dtype=float32), 'validation/wer': 0.10767834557865164, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20149006, dtype=float32), 'test/wer': 0.06842971177868502, 'test/num_examples': 2472, 'score': 41828.6324532032, 'total_duration': 45734.00941133499, 'accumulated_submission_time': 41828.6324532032, 'accumulated_eval_time': 3901.5167338848114, 'accumulated_logging_time': 1.5817480087280273, 'global_step': 54436, 'preemption_count': 0}), (56319, {'train/ctc_loss': Array(0.14977348, dtype=float32), 'train/wer': 0.057254981961862225, 'validation/ctc_loss': Array(0.36254397, dtype=float32), 'validation/wer': 0.10628807553800554, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19897437, dtype=float32), 'test/wer': 0.06700790120447667, 'test/num_examples': 2472, 'score': 43269.12178993225, 'total_duration': 47305.22345614433, 'accumulated_submission_time': 43269.12178993225, 'accumulated_eval_time': 4032.1063737869263, 'accumulated_logging_time': 1.6375198364257812, 'global_step': 56319, 'preemption_count': 0}), (58199, {'train/ctc_loss': Array(0.14436826, dtype=float32), 'train/wer': 0.056059406155415006, 'validation/ctc_loss': Array(0.3545192, dtype=float32), 'validation/wer': 0.10204968284464698, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18987672, dtype=float32), 'test/wer': 0.06467206954684866, 'test/num_examples': 2472, 'score': 44709.46357750893, 'total_duration': 48875.57385182381, 'accumulated_submission_time': 44709.46357750893, 'accumulated_eval_time': 4161.976230621338, 'accumulated_logging_time': 1.697356939315796, 'global_step': 58199, 'preemption_count': 0}), (60080, {'train/ctc_loss': Array(0.1338687, dtype=float32), 'train/wer': 0.05099792960662526, 'validation/ctc_loss': Array(0.34373295, dtype=float32), 'validation/wer': 0.09889261129401315, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18780683, dtype=float32), 'test/wer': 0.062112810513273616, 'test/num_examples': 2472, 'score': 46149.45885229111, 'total_duration': 50444.575095653534, 'accumulated_submission_time': 46149.45885229111, 'accumulated_eval_time': 4290.845764160156, 'accumulated_logging_time': 1.7542970180511475, 'global_step': 60080, 'preemption_count': 0}), (61946, {'train/ctc_loss': Array(0.13237758, dtype=float32), 'train/wer': 0.050976958208852716, 'validation/ctc_loss': Array(0.33442134, dtype=float32), 'validation/wer': 0.09731890284522626, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18222867, dtype=float32), 'test/wer': 0.06036601466496049, 'test/num_examples': 2472, 'score': 47589.61220765114, 'total_duration': 52013.4002263546, 'accumulated_submission_time': 47589.61220765114, 'accumulated_eval_time': 4419.383289575577, 'accumulated_logging_time': 1.8110344409942627, 'global_step': 61946, 'preemption_count': 0}), (63814, {'train/ctc_loss': Array(0.12851778, dtype=float32), 'train/wer': 0.049004018776070316, 'validation/ctc_loss': Array(0.32344717, dtype=float32), 'validation/wer': 0.09304189153962752, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17482771, dtype=float32), 'test/wer': 0.05762395141470152, 'test/num_examples': 2472, 'score': 49029.50516271591, 'total_duration': 53585.0060338974, 'accumulated_submission_time': 49029.50516271591, 'accumulated_eval_time': 4550.955982685089, 'accumulated_logging_time': 1.8721585273742676, 'global_step': 63814, 'preemption_count': 0}), (65679, {'train/ctc_loss': Array(0.10282077, dtype=float32), 'train/wer': 0.040542964108772166, 'validation/ctc_loss': Array(0.31785193, dtype=float32), 'validation/wer': 0.09087924925417805, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17353387, dtype=float32), 'test/wer': 0.05595840188491459, 'test/num_examples': 2472, 'score': 50469.89919543266, 'total_duration': 55155.54966759682, 'accumulated_submission_time': 50469.89919543266, 'accumulated_eval_time': 4680.961303472519, 'accumulated_logging_time': 1.9368162155151367, 'global_step': 65679, 'preemption_count': 0}), (67553, {'train/ctc_loss': Array(0.09873752, dtype=float32), 'train/wer': 0.03779494317922505, 'validation/ctc_loss': Array(0.3111092, dtype=float32), 'validation/wer': 0.08922830358091081, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16759367, dtype=float32), 'test/wer': 0.05427254077549611, 'test/num_examples': 2472, 'score': 51910.30626130104, 'total_duration': 56725.00738286972, 'accumulated_submission_time': 51910.30626130104, 'accumulated_eval_time': 4809.875825405121, 'accumulated_logging_time': 1.9941620826721191, 'global_step': 67553, 'preemption_count': 0}), (69429, {'train/ctc_loss': Array(0.08786926, dtype=float32), 'train/wer': 0.034247645404633306, 'validation/ctc_loss': Array(0.30810562, dtype=float32), 'validation/wer': 0.0879442347239252, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16458964, dtype=float32), 'test/wer': 0.05348038916986574, 'test/num_examples': 2472, 'score': 53350.63709568977, 'total_duration': 58294.03611445427, 'accumulated_submission_time': 53350.63709568977, 'accumulated_eval_time': 4938.435527801514, 'accumulated_logging_time': 2.0534727573394775, 'global_step': 69429, 'preemption_count': 0}), (71305, {'train/ctc_loss': Array(0.08843144, dtype=float32), 'train/wer': 0.03386530173027323, 'validation/ctc_loss': Array(0.30182338, dtype=float32), 'validation/wer': 0.08612915994863725, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16141044, dtype=float32), 'test/wer': 0.051753904901184165, 'test/num_examples': 2472, 'score': 54790.89835476875, 'total_duration': 59863.24539685249, 'accumulated_submission_time': 54790.89835476875, 'accumulated_eval_time': 5067.244613170624, 'accumulated_logging_time': 2.112919569015503, 'global_step': 71305, 'preemption_count': 0}), (73177, {'train/ctc_loss': Array(0.08011135, dtype=float32), 'train/wer': 0.030846805751213076, 'validation/ctc_loss': Array(0.30042586, dtype=float32), 'validation/wer': 0.08493198296919201, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16003639, dtype=float32), 'test/wer': 0.05112424593260618, 'test/num_examples': 2472, 'score': 56231.20172023773, 'total_duration': 61433.10589194298, 'accumulated_submission_time': 56231.20172023773, 'accumulated_eval_time': 5196.659123182297, 'accumulated_logging_time': 2.1780107021331787, 'global_step': 73177, 'preemption_count': 0}), (75065, {'train/ctc_loss': Array(0.07875839, dtype=float32), 'train/wer': 0.029531895110855296, 'validation/ctc_loss': Array(0.2965718, dtype=float32), 'validation/wer': 0.08304932562248375, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15909529, dtype=float32), 'test/wer': 0.05025084800844962, 'test/num_examples': 2472, 'score': 57671.870633125305, 'total_duration': 63003.7757563591, 'accumulated_submission_time': 57671.870633125305, 'accumulated_eval_time': 5326.510108470917, 'accumulated_logging_time': 2.247305393218994, 'global_step': 75065, 'preemption_count': 0}), (76942, {'train/ctc_loss': Array(0.06596456, dtype=float32), 'train/wer': 0.025091449797312197, 'validation/ctc_loss': Array(0.2928794, dtype=float32), 'validation/wer': 0.08256659296948164, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15672454, dtype=float32), 'test/wer': 0.05053521012329129, 'test/num_examples': 2472, 'score': 59112.54464316368, 'total_duration': 64574.79559850693, 'accumulated_submission_time': 59112.54464316368, 'accumulated_eval_time': 5456.7151482105255, 'accumulated_logging_time': 2.3087172508239746, 'global_step': 76942, 'preemption_count': 0}), (78817, {'train/ctc_loss': Array(0.06438663, dtype=float32), 'train/wer': 0.02406642764495647, 'validation/ctc_loss': Array(0.29217574, dtype=float32), 'validation/wer': 0.0821224789287197, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15655187, dtype=float32), 'test/wer': 0.05000710905287104, 'test/num_examples': 2472, 'score': 60552.484139442444, 'total_duration': 66142.98616504669, 'accumulated_submission_time': 60552.484139442444, 'accumulated_eval_time': 5584.824814081192, 'accumulated_logging_time': 2.3721067905426025, 'global_step': 78817, 'preemption_count': 0})], 'global_step': 79496}
I0218 18:47:54.103262 140549388556096 submission_runner.py:586] Timing: 61068.20896577835
I0218 18:47:54.103341 140549388556096 submission_runner.py:588] Total number of evals: 43
I0218 18:47:54.103410 140549388556096 submission_runner.py:589] ====================
I0218 18:47:54.103483 140549388556096 submission_runner.py:542] Using RNG seed 4110531300
I0218 18:47:54.105888 140549388556096 submission_runner.py:551] --- Tuning run 2/5 ---
I0218 18:47:54.106029 140549388556096 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_3/librispeech_conformer_jax/trial_2.
I0218 18:47:54.107987 140549388556096 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_3/librispeech_conformer_jax/trial_2/hparams.json.
I0218 18:47:54.110477 140549388556096 submission_runner.py:206] Initializing dataset.
I0218 18:47:54.110638 140549388556096 submission_runner.py:213] Initializing model.
I0218 18:47:58.116981 140549388556096 submission_runner.py:255] Initializing optimizer.
I0218 18:47:58.629020 140549388556096 submission_runner.py:262] Initializing metrics bundle.
I0218 18:47:58.629259 140549388556096 submission_runner.py:280] Initializing checkpoint and logger.
I0218 18:47:58.734278 140549388556096 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_3/librispeech_conformer_jax/trial_2 with prefix checkpoint_
I0218 18:47:58.734465 140549388556096 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_3/librispeech_conformer_jax/trial_2/meta_data_0.json.
I0218 18:47:58.734728 140549388556096 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0218 18:47:58.734834 140549388556096 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0218 18:47:59.296497 140549388556096 logger_utils.py:220] Unable to record git information. Continuing without it.
I0218 18:47:59.755991 140549388556096 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_3/librispeech_conformer_jax/trial_2/flags_0.json.
I0218 18:47:59.879357 140549388556096 submission_runner.py:314] Starting training loop.
I0218 18:47:59.882487 140549388556096 input_pipeline.py:20] Loading split = train-clean-100
I0218 18:47:59.928904 140549388556096 input_pipeline.py:20] Loading split = train-clean-360
I0218 18:48:00.065098 140549388556096 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0218 18:48:34.936619 140367777228544 logging_writer.py:48] [0] global_step=0, grad_norm=69.10867309570312, loss=31.631193161010742
I0218 18:48:34.952638 140549388556096 spec.py:321] Evaluating on the training split.
I0218 18:49:28.581824 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 18:50:17.941860 140549388556096 spec.py:349] Evaluating on the test split.
I0218 18:50:43.209870 140549388556096 submission_runner.py:408] Time since start: 163.33s, 	Step: 1, 	{'train/ctc_loss': Array(31.51566, dtype=float32), 'train/wer': 1.1540354719622474, 'validation/ctc_loss': Array(30.090126, dtype=float32), 'validation/wer': 0.9587360128213792, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.214182, dtype=float32), 'test/wer': 0.9757885970791949, 'test/num_examples': 2472, 'score': 35.07322812080383, 'total_duration': 163.3276309967041, 'accumulated_submission_time': 35.07322812080383, 'accumulated_eval_time': 128.25435638427734, 'accumulated_logging_time': 0}
I0218 18:50:43.227442 140483498055424 logging_writer.py:48] [1] accumulated_eval_time=128.254356, accumulated_logging_time=0, accumulated_submission_time=35.073228, global_step=1, preemption_count=0, score=35.073228, test/ctc_loss=30.214181900024414, test/num_examples=2472, test/wer=0.975789, total_duration=163.327631, train/ctc_loss=31.51565933227539, train/wer=1.154035, validation/ctc_loss=30.090126037597656, validation/num_examples=5348, validation/wer=0.958736
I0218 18:52:24.486692 140378536228608 logging_writer.py:48] [100] global_step=100, grad_norm=2.0676281452178955, loss=6.562723636627197
I0218 18:53:40.512969 140378544621312 logging_writer.py:48] [200] global_step=200, grad_norm=1.075092077255249, loss=5.89063835144043
I0218 18:54:56.712239 140378536228608 logging_writer.py:48] [300] global_step=300, grad_norm=0.972876250743866, loss=5.812668323516846
I0218 18:56:12.934700 140378544621312 logging_writer.py:48] [400] global_step=400, grad_norm=0.44534242153167725, loss=5.820737838745117
I0218 18:57:29.306406 140378536228608 logging_writer.py:48] [500] global_step=500, grad_norm=2.6772305965423584, loss=5.811158657073975
I0218 18:58:45.575613 140378544621312 logging_writer.py:48] [600] global_step=600, grad_norm=0.3571988046169281, loss=5.782829761505127
I0218 19:00:01.862803 140378536228608 logging_writer.py:48] [700] global_step=700, grad_norm=2.2175068855285645, loss=5.805422306060791
I0218 19:01:18.121762 140378544621312 logging_writer.py:48] [800] global_step=800, grad_norm=0.29511672258377075, loss=5.802970886230469
I0218 19:02:34.350577 140378536228608 logging_writer.py:48] [900] global_step=900, grad_norm=2.636833429336548, loss=5.782876014709473
I0218 19:03:53.578759 140378544621312 logging_writer.py:48] [1000] global_step=1000, grad_norm=5.054660320281982, loss=5.78410530090332
I0218 19:05:13.967815 140483498055424 logging_writer.py:48] [1100] global_step=1100, grad_norm=2.536882162094116, loss=5.635854721069336
I0218 19:06:30.193976 140468535932672 logging_writer.py:48] [1200] global_step=1200, grad_norm=2.9234957695007324, loss=5.544885635375977
I0218 19:07:46.321548 140483498055424 logging_writer.py:48] [1300] global_step=1300, grad_norm=2.9649384021759033, loss=5.459657192230225
I0218 19:09:02.516197 140468535932672 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.6653542518615723, loss=5.1016716957092285
I0218 19:10:18.628208 140483498055424 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.9928048849105835, loss=4.479243755340576
I0218 19:11:35.018966 140468535932672 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.8602539896965027, loss=4.035604000091553
I0218 19:12:54.293911 140483498055424 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.4116456508636475, loss=3.6988446712493896
I0218 19:14:15.661059 140468535932672 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.4880776405334473, loss=3.5313773155212402
I0218 19:14:43.853204 140549388556096 spec.py:321] Evaluating on the training split.
I0218 19:15:22.556575 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 19:16:08.186046 140549388556096 spec.py:349] Evaluating on the test split.
I0218 19:16:31.354410 140549388556096 submission_runner.py:408] Time since start: 1711.47s, 	Step: 1836, 	{'train/ctc_loss': Array(5.6703877, dtype=float32), 'train/wer': 0.9337997724687145, 'validation/ctc_loss': Array(5.695042, dtype=float32), 'validation/wer': 0.8934609034824333, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.594893, dtype=float32), 'test/wer': 0.8955578575345805, 'test/num_examples': 2472, 'score': 1475.6158843040466, 'total_duration': 1711.4689011573792, 'accumulated_submission_time': 1475.6158843040466, 'accumulated_eval_time': 235.74946308135986, 'accumulated_logging_time': 0.030027151107788086}
I0218 19:16:31.387440 140483498055424 logging_writer.py:48] [1836] accumulated_eval_time=235.749463, accumulated_logging_time=0.030027, accumulated_submission_time=1475.615884, global_step=1836, preemption_count=0, score=1475.615884, test/ctc_loss=5.594892978668213, test/num_examples=2472, test/wer=0.895558, total_duration=1711.468901, train/ctc_loss=5.6703877449035645, train/wer=0.933800, validation/ctc_loss=5.695042133331299, validation/num_examples=5348, validation/wer=0.893461
I0218 19:17:20.707664 140468535932672 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.019018292427063, loss=3.2691454887390137
I0218 19:18:36.769081 140483498055424 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.4809658527374268, loss=3.264472484588623
I0218 19:19:56.241996 140483498055424 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.1331535577774048, loss=3.0879762172698975
I0218 19:21:12.142455 140468535932672 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.2416611909866333, loss=2.95446515083313
I0218 19:22:28.094959 140483498055424 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.2843912839889526, loss=2.9410321712493896
I0218 19:23:44.082179 140468535932672 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.1085623502731323, loss=2.8780975341796875
I0218 19:24:59.914201 140483498055424 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.379752278327942, loss=2.8050947189331055
I0218 19:26:15.768010 140468535932672 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.8140830993652344, loss=2.6867144107818604
I0218 19:27:31.924275 140483498055424 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.9543843269348145, loss=2.648937940597534
I0218 19:28:50.714974 140468535932672 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.9548366665840149, loss=2.607621669769287
I0218 19:30:11.547821 140483498055424 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.4176595211029053, loss=2.6492297649383545
I0218 19:31:33.600314 140468535932672 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.5673738718032837, loss=2.5495450496673584
I0218 19:32:57.540590 140483498055424 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.0833872556686401, loss=2.4427945613861084
I0218 19:34:13.319195 140468535932672 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.7965810298919678, loss=2.4236881732940674
I0218 19:35:29.067852 140483498055424 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.1604578495025635, loss=2.397693157196045
I0218 19:36:44.963764 140468535932672 logging_writer.py:48] [3400] global_step=3400, grad_norm=2.3731000423431396, loss=2.3877193927764893
I0218 19:38:00.694479 140483498055424 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.9131953120231628, loss=2.2819974422454834
I0218 19:39:16.573784 140468535932672 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.3873270750045776, loss=2.303510904312134
I0218 19:40:32.369176 140483498055424 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.9313752055168152, loss=2.184905529022217
I0218 19:40:32.378526 140549388556096 spec.py:321] Evaluating on the training split.
I0218 19:41:25.333197 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 19:42:15.613219 140549388556096 spec.py:349] Evaluating on the test split.
I0218 19:42:42.238444 140549388556096 submission_runner.py:408] Time since start: 3282.35s, 	Step: 3701, 	{'train/ctc_loss': Array(2.2839155, dtype=float32), 'train/wer': 0.5220066461751967, 'validation/ctc_loss': Array(2.2517276, dtype=float32), 'validation/wer': 0.5036446315301659, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.9130712, dtype=float32), 'test/wer': 0.46221030609550506, 'test/num_examples': 2472, 'score': 2916.513821363449, 'total_duration': 3282.3531198501587, 'accumulated_submission_time': 2916.513821363449, 'accumulated_eval_time': 365.60345458984375, 'accumulated_logging_time': 0.08441042900085449}
I0218 19:42:42.274672 140483498055424 logging_writer.py:48] [3701] accumulated_eval_time=365.603455, accumulated_logging_time=0.084410, accumulated_submission_time=2916.513821, global_step=3701, preemption_count=0, score=2916.513821, test/ctc_loss=1.9130711555480957, test/num_examples=2472, test/wer=0.462210, total_duration=3282.353120, train/ctc_loss=2.2839155197143555, train/wer=0.522007, validation/ctc_loss=2.25172758102417, validation/num_examples=5348, validation/wer=0.503645
I0218 19:43:57.945700 140468535932672 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.1462643146514893, loss=2.1946797370910645
I0218 19:45:13.890665 140483498055424 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.8015778660774231, loss=2.1856529712677
I0218 19:46:29.715011 140468535932672 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.1142054796218872, loss=2.1296024322509766
I0218 19:47:45.580258 140483498055424 logging_writer.py:48] [4100] global_step=4100, grad_norm=1.5926026105880737, loss=2.164165735244751
I0218 19:49:04.802252 140483498055424 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.7189061641693115, loss=2.072653293609619
I0218 19:50:20.477577 140468535932672 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.7903205156326294, loss=2.06666898727417
I0218 19:51:36.260965 140483498055424 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.9328932762145996, loss=2.021711587905884
I0218 19:52:51.940179 140468535932672 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.4069318771362305, loss=2.0031867027282715
I0218 19:54:07.675747 140483498055424 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.8447269201278687, loss=1.9359973669052124
I0218 19:55:23.361635 140468535932672 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.6855471134185791, loss=1.9151443243026733
I0218 19:56:39.042169 140483498055424 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.7919679284095764, loss=1.9284096956253052
I0218 19:57:59.986059 140468535932672 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.8081175088882446, loss=1.8806735277175903
I0218 19:59:20.481297 140483498055424 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.6801708936691284, loss=1.8699103593826294
I0218 20:00:41.506626 140468535932672 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.6996142268180847, loss=1.8970582485198975
I0218 20:02:02.828545 140483498055424 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.6199235320091248, loss=1.7959059476852417
I0218 20:03:18.450252 140468535932672 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.6319066882133484, loss=1.8514803647994995
I0218 20:04:34.119960 140483498055424 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.9503957629203796, loss=1.8411177396774292
I0218 20:05:49.798926 140468535932672 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.7394019961357117, loss=1.7870829105377197
I0218 20:06:42.427660 140549388556096 spec.py:321] Evaluating on the training split.
I0218 20:07:34.988339 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 20:08:26.124620 140549388556096 spec.py:349] Evaluating on the test split.
I0218 20:08:51.686456 140549388556096 submission_runner.py:408] Time since start: 4851.80s, 	Step: 5571, 	{'train/ctc_loss': Array(0.7571463, dtype=float32), 'train/wer': 0.24622433208213734, 'validation/ctc_loss': Array(0.86724573, dtype=float32), 'validation/wer': 0.26055977678442127, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6027771, dtype=float32), 'test/wer': 0.20080027623748298, 'test/num_examples': 2472, 'score': 4356.575798273087, 'total_duration': 4851.800904512405, 'accumulated_submission_time': 4356.575798273087, 'accumulated_eval_time': 494.8561522960663, 'accumulated_logging_time': 0.13894009590148926}
I0218 20:08:51.721572 140483498055424 logging_writer.py:48] [5571] accumulated_eval_time=494.856152, accumulated_logging_time=0.138940, accumulated_submission_time=4356.575798, global_step=5571, preemption_count=0, score=4356.575798, test/ctc_loss=0.6027771234512329, test/num_examples=2472, test/wer=0.200800, total_duration=4851.800905, train/ctc_loss=0.7571462988853455, train/wer=0.246224, validation/ctc_loss=0.8672457337379456, validation/num_examples=5348, validation/wer=0.260560
I0218 20:09:14.411121 140468535932672 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.6939507126808167, loss=1.7758681774139404
I0218 20:10:29.981697 140483498055424 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.6632827520370483, loss=1.7862656116485596
I0218 20:11:45.646534 140468535932672 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.6733450293540955, loss=1.7128641605377197
I0218 20:13:01.538040 140483498055424 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.7459981441497803, loss=1.7710161209106445
I0218 20:14:17.324493 140468535932672 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.8608480095863342, loss=1.787443995475769
I0218 20:15:33.024716 140483498055424 logging_writer.py:48] [6100] global_step=6100, grad_norm=1.384129285812378, loss=1.7558727264404297
I0218 20:16:54.937942 140483498055424 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.6387519836425781, loss=1.735870122909546
I0218 20:18:10.582549 140468535932672 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.6817148327827454, loss=1.6986110210418701
I0218 20:19:26.277285 140483498055424 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.7138116955757141, loss=1.7376426458358765
I0218 20:20:41.979573 140468535932672 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.8370855450630188, loss=1.682608723640442
I0218 20:21:57.800315 140483498055424 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.8475736975669861, loss=1.7048496007919312
I0218 20:23:13.589052 140468535932672 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.7400704026222229, loss=1.6913152933120728
I0218 20:24:29.421305 140483498055424 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.6626303195953369, loss=1.6625803709030151
I0218 20:25:45.345413 140468535932672 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.0164742469787598, loss=1.6736401319503784
I0218 20:27:06.192733 140483498055424 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.7767014503479004, loss=1.7127172946929932
I0218 20:28:27.245324 140468535932672 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.7689357995986938, loss=1.669119954109192
I0218 20:29:49.059171 140483498055424 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.6300318837165833, loss=1.647606372833252
I0218 20:31:08.136301 140483498055424 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.721485435962677, loss=1.6271347999572754
I0218 20:32:23.925503 140468535932672 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.6735358834266663, loss=1.5700064897537231
I0218 20:32:52.374735 140549388556096 spec.py:321] Evaluating on the training split.
I0218 20:33:44.006765 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 20:34:34.355284 140549388556096 spec.py:349] Evaluating on the test split.
I0218 20:35:00.151316 140549388556096 submission_runner.py:408] Time since start: 6420.27s, 	Step: 7439, 	{'train/ctc_loss': Array(0.57816416, dtype=float32), 'train/wer': 0.19906977816456972, 'validation/ctc_loss': Array(0.7265921, dtype=float32), 'validation/wer': 0.22083087944234725, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4682187, dtype=float32), 'test/wer': 0.15782097373712753, 'test/num_examples': 2472, 'score': 5797.141031265259, 'total_duration': 6420.266736745834, 'accumulated_submission_time': 5797.141031265259, 'accumulated_eval_time': 622.6275777816772, 'accumulated_logging_time': 0.18962836265563965}
I0218 20:35:00.187306 140483498055424 logging_writer.py:48] [7439] accumulated_eval_time=622.627578, accumulated_logging_time=0.189628, accumulated_submission_time=5797.141031, global_step=7439, preemption_count=0, score=5797.141031, test/ctc_loss=0.46821871399879456, test/num_examples=2472, test/wer=0.157821, total_duration=6420.266737, train/ctc_loss=0.5781641602516174, train/wer=0.199070, validation/ctc_loss=0.7265921235084534, validation/num_examples=5348, validation/wer=0.220831
I0218 20:35:46.917597 140468535932672 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.6231921911239624, loss=1.6455949544906616
I0218 20:37:02.541345 140483498055424 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.5842589735984802, loss=1.5976991653442383
I0218 20:38:18.305337 140468535932672 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.6142988204956055, loss=1.610550045967102
I0218 20:39:33.974450 140483498055424 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.7794962525367737, loss=1.610549807548523
I0218 20:40:49.588251 140468535932672 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.6535481810569763, loss=1.63588285446167
I0218 20:42:05.224066 140483498055424 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.6830284595489502, loss=1.6040033102035522
I0218 20:43:21.256343 140468535932672 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.6521095037460327, loss=1.5816422700881958
I0218 20:44:43.602228 140483498055424 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.6312467455863953, loss=1.5877130031585693
I0218 20:46:04.386797 140483498055424 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.0214729309082031, loss=1.6236387491226196
I0218 20:47:19.988218 140468535932672 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.6552232503890991, loss=1.5736969709396362
I0218 20:48:35.590976 140483498055424 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.7005499601364136, loss=1.5931273698806763
I0218 20:49:51.285161 140468535932672 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.6752774715423584, loss=1.616540789604187
I0218 20:51:06.874256 140483498055424 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.6923080086708069, loss=1.592482328414917
I0218 20:52:22.404771 140468535932672 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.6955091953277588, loss=1.6220005750656128
I0218 20:53:37.960927 140483498055424 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.6386377215385437, loss=1.5872471332550049
I0218 20:54:55.626997 140468535932672 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.7102784514427185, loss=1.4852831363677979
I0218 20:56:17.231734 140483498055424 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.585492730140686, loss=1.60018789768219
I0218 20:57:37.861841 140468535932672 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.6136077046394348, loss=1.5180556774139404
I0218 20:58:59.972985 140483498055424 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.6392835378646851, loss=1.544825792312622
I0218 20:59:00.462870 140549388556096 spec.py:321] Evaluating on the training split.
I0218 20:59:53.505732 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 21:00:44.029752 140549388556096 spec.py:349] Evaluating on the test split.
I0218 21:01:09.670297 140549388556096 submission_runner.py:408] Time since start: 7989.78s, 	Step: 9302, 	{'train/ctc_loss': Array(0.4748012, dtype=float32), 'train/wer': 0.16590620132425546, 'validation/ctc_loss': Array(0.6415892, dtype=float32), 'validation/wer': 0.19483089875165335, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40672186, dtype=float32), 'test/wer': 0.13864684256494628, 'test/num_examples': 2472, 'score': 7237.32617020607, 'total_duration': 7989.78463101387, 'accumulated_submission_time': 7237.32617020607, 'accumulated_eval_time': 751.8287582397461, 'accumulated_logging_time': 0.24376177787780762}
I0218 21:01:09.704529 140483498055424 logging_writer.py:48] [9302] accumulated_eval_time=751.828758, accumulated_logging_time=0.243762, accumulated_submission_time=7237.326170, global_step=9302, preemption_count=0, score=7237.326170, test/ctc_loss=0.4067218601703644, test/num_examples=2472, test/wer=0.138647, total_duration=7989.784631, train/ctc_loss=0.4748012125492096, train/wer=0.165906, validation/ctc_loss=0.6415892243385315, validation/num_examples=5348, validation/wer=0.194831
I0218 21:02:24.185669 140468535932672 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.6007760167121887, loss=1.5100256204605103
I0218 21:03:39.681995 140483498055424 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.7179705500602722, loss=1.5007795095443726
I0218 21:04:55.358849 140468535932672 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.6403092741966248, loss=1.4792115688323975
I0218 21:06:11.077155 140483498055424 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.6739798784255981, loss=1.471514344215393
I0218 21:07:26.708169 140468535932672 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.5979604125022888, loss=1.454030990600586
I0218 21:08:42.408845 140483498055424 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.7057018280029297, loss=1.5005707740783691
I0218 21:09:58.076098 140468535932672 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.7108340263366699, loss=1.4779688119888306
I0218 21:11:13.719272 140483498055424 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.6057991981506348, loss=1.5144814252853394
I0218 21:12:29.354888 140468535932672 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.6526473760604858, loss=1.5160187482833862
I0218 21:13:52.437135 140483498055424 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.557410717010498, loss=1.4287338256835938
I0218 21:15:08.054861 140468535932672 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.602067232131958, loss=1.498569369316101
I0218 21:16:23.645940 140483498055424 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.5804528594017029, loss=1.4449645280838013
I0218 21:17:39.127762 140468535932672 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.7211189866065979, loss=1.5572208166122437
I0218 21:18:54.780630 140483498055424 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.5867401361465454, loss=1.4741171598434448
I0218 21:20:10.487784 140468535932672 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.4961461126804352, loss=1.4724737405776978
I0218 21:21:25.967895 140483498055424 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.5977205038070679, loss=1.4834188222885132
I0218 21:22:41.599953 140468535932672 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.5933256149291992, loss=1.404930591583252
I0218 21:23:57.329409 140483498055424 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.6854897141456604, loss=1.4996206760406494
I0218 21:25:10.220430 140549388556096 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0218 21:26:28.684463 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 21:27:19.766398 140549388556096 spec.py:349] Evaluating on the test split.
I0218 21:27:45.254357 140549388556096 submission_runner.py:408] Time since start: 9585.37s, 	Step: 11198, 	{'train/ctc_loss': Array(0.29416263, dtype=float32), 'train/wer': 0.10913853936405374, 'validation/ctc_loss': Array(0.600491, dtype=float32), 'validation/wer': 0.18280120103884068, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37524578, dtype=float32), 'test/wer': 0.1282270022139622, 'test/num_examples': 2472, 'score': 8677.753060102463, 'total_duration': 9585.371998786926, 'accumulated_submission_time': 8677.753060102463, 'accumulated_eval_time': 906.8597481250763, 'accumulated_logging_time': 0.29329824447631836}
I0218 21:27:45.287620 140483498055424 logging_writer.py:48] [11198] accumulated_eval_time=906.859748, accumulated_logging_time=0.293298, accumulated_submission_time=8677.753060, global_step=11198, preemption_count=0, score=8677.753060, test/ctc_loss=0.3752457797527313, test/num_examples=2472, test/wer=0.128227, total_duration=9585.371999, train/ctc_loss=0.2941626310348511, train/wer=0.109139, validation/ctc_loss=0.6004909873008728, validation/num_examples=5348, validation/wer=0.182801
I0218 21:27:47.665345 140468535932672 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.5753903985023499, loss=1.4543653726577759
I0218 21:29:03.148353 140483498055424 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.6195182204246521, loss=1.419615387916565
I0218 21:30:21.944812 140483498055424 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.7147199511528015, loss=1.4490567445755005
I0218 21:31:37.586270 140468535932672 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.7746056318283081, loss=1.4309343099594116
I0218 21:32:53.266366 140483498055424 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.6330299973487854, loss=1.4807565212249756
I0218 21:34:08.920011 140468535932672 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.6142655611038208, loss=1.4662123918533325
I0218 21:35:24.621514 140483498055424 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.5796183943748474, loss=1.4256244897842407
I0218 21:36:40.331841 140468535932672 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.5394333004951477, loss=1.4050763845443726
I0218 21:37:56.020488 140483498055424 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.6056213974952698, loss=1.5014902353286743
I0218 21:39:11.776768 140468535932672 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.5881671905517578, loss=1.4504694938659668
I0218 21:40:31.172230 140483498055424 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.8151406645774841, loss=1.3990039825439453
I0218 21:41:52.423496 140468535932672 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.5151270627975464, loss=1.4273439645767212
I0218 21:43:13.723693 140483498055424 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.6009917259216309, loss=1.4241862297058105
I0218 21:44:29.227480 140468535932672 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.6581031084060669, loss=1.3979778289794922
I0218 21:45:44.807774 140483498055424 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.616142749786377, loss=1.4214982986450195
I0218 21:47:00.368289 140468535932672 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.5834814310073853, loss=1.3761181831359863
I0218 21:48:15.927328 140483498055424 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.6659201383590698, loss=1.411374807357788
I0218 21:49:31.549460 140468535932672 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.5566653609275818, loss=1.4089508056640625
I0218 21:50:47.074080 140483498055424 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.7792612910270691, loss=1.419331669807434
I0218 21:51:45.596430 140549388556096 spec.py:321] Evaluating on the training split.
I0218 21:52:40.598548 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 21:53:31.605400 140549388556096 spec.py:349] Evaluating on the test split.
I0218 21:53:56.860738 140549388556096 submission_runner.py:408] Time since start: 11156.98s, 	Step: 13079, 	{'train/ctc_loss': Array(0.26120993, dtype=float32), 'train/wer': 0.09492812725657557, 'validation/ctc_loss': Array(0.5636484, dtype=float32), 'validation/wer': 0.1702308427546656, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34251323, dtype=float32), 'test/wer': 0.11575569232019174, 'test/num_examples': 2472, 'score': 10117.966343402863, 'total_duration': 11156.975125789642, 'accumulated_submission_time': 10117.966343402863, 'accumulated_eval_time': 1038.117871761322, 'accumulated_logging_time': 0.349048376083374}
I0218 21:53:56.901766 140483498055424 logging_writer.py:48] [13079] accumulated_eval_time=1038.117872, accumulated_logging_time=0.349048, accumulated_submission_time=10117.966343, global_step=13079, preemption_count=0, score=10117.966343, test/ctc_loss=0.34251323342323303, test/num_examples=2472, test/wer=0.115756, total_duration=11156.975126, train/ctc_loss=0.2612099349498749, train/wer=0.094928, validation/ctc_loss=0.5636484026908875, validation/num_examples=5348, validation/wer=0.170231
I0218 21:54:13.533786 140468535932672 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.6524574756622314, loss=1.4590520858764648
I0218 21:55:29.083931 140483498055424 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.6493769884109497, loss=1.4188339710235596
I0218 21:56:44.672225 140468535932672 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.6445744037628174, loss=1.3573235273361206
I0218 21:58:04.038614 140483498055424 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.6341730952262878, loss=1.3813235759735107
I0218 21:59:19.613660 140468535932672 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.719792366027832, loss=1.3848066329956055
I0218 22:00:35.186618 140483498055424 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.5458401441574097, loss=1.3522056341171265
I0218 22:01:50.805791 140468535932672 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.588537335395813, loss=1.442772388458252
I0218 22:03:06.445608 140483498055424 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.5983838438987732, loss=1.4010405540466309
I0218 22:04:22.044795 140468535932672 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.7160955667495728, loss=1.411268949508667
I0218 22:05:37.688461 140483498055424 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.6184327602386475, loss=1.3727915287017822
I0218 22:06:55.639363 140468535932672 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.6527222990989685, loss=1.3891358375549316
I0218 22:08:17.086001 140483498055424 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.5202229022979736, loss=1.3756210803985596
I0218 22:09:37.247453 140468535932672 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.5942811369895935, loss=1.4055615663528442
I0218 22:10:58.444553 140483498055424 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.4889219403266907, loss=1.3586676120758057
I0218 22:12:18.165521 140483498055424 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.794655442237854, loss=1.3683243989944458
I0218 22:13:33.884241 140468535932672 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.6552520990371704, loss=1.386178731918335
I0218 22:14:49.419594 140483498055424 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.7342812418937683, loss=1.363195776939392
I0218 22:16:04.908295 140468535932672 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.6639528870582581, loss=1.3857636451721191
I0218 22:17:20.524153 140483498055424 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.7414374947547913, loss=1.354597568511963
I0218 22:17:57.161851 140549388556096 spec.py:321] Evaluating on the training split.
I0218 22:18:51.705548 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 22:19:42.359677 140549388556096 spec.py:349] Evaluating on the test split.
I0218 22:20:07.870052 140549388556096 submission_runner.py:408] Time since start: 12727.98s, 	Step: 14950, 	{'train/ctc_loss': Array(0.25069952, dtype=float32), 'train/wer': 0.09362826713179524, 'validation/ctc_loss': Array(0.53552085, dtype=float32), 'validation/wer': 0.16169612944958822, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31989244, dtype=float32), 'test/wer': 0.10931692157699104, 'test/num_examples': 2472, 'score': 11558.134120464325, 'total_duration': 12727.983196496964, 'accumulated_submission_time': 11558.134120464325, 'accumulated_eval_time': 1168.8186223506927, 'accumulated_logging_time': 0.4085848331451416}
I0218 22:20:07.909087 140483498055424 logging_writer.py:48] [14950] accumulated_eval_time=1168.818622, accumulated_logging_time=0.408585, accumulated_submission_time=11558.134120, global_step=14950, preemption_count=0, score=11558.134120, test/ctc_loss=0.31989243626594543, test/num_examples=2472, test/wer=0.109317, total_duration=12727.983196, train/ctc_loss=0.250699520111084, train/wer=0.093628, validation/ctc_loss=0.5355208516120911, validation/num_examples=5348, validation/wer=0.161696
I0218 22:20:46.344755 140468535932672 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.5512523055076599, loss=1.329697847366333
I0218 22:22:01.800179 140483498055424 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.620903491973877, loss=1.3461893796920776
I0218 22:23:17.289626 140468535932672 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.6490634679794312, loss=1.370133399963379
I0218 22:24:32.858532 140483498055424 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.6508611440658569, loss=1.3534821271896362
I0218 22:25:48.364168 140468535932672 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.632658064365387, loss=1.3447883129119873
I0218 22:27:07.259085 140483498055424 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.7144127488136292, loss=1.3672549724578857
I0218 22:28:23.009582 140468535932672 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.7785812020301819, loss=1.3187322616577148
I0218 22:29:38.531239 140483498055424 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.5565688014030457, loss=1.3471698760986328
I0218 22:30:54.046385 140468535932672 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.5434674024581909, loss=1.3542447090148926
I0218 22:32:09.660208 140483498055424 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.653322696685791, loss=1.3528220653533936
I0218 22:33:25.261200 140468535932672 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.5865239500999451, loss=1.3516058921813965
I0218 22:34:41.388421 140483498055424 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.6858121156692505, loss=1.3125534057617188
I0218 22:36:02.099877 140468535932672 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.6919950842857361, loss=1.329518437385559
I0218 22:37:24.462252 140483498055424 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.598892867565155, loss=1.3541908264160156
I0218 22:38:45.826398 140468535932672 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.706762969493866, loss=1.380418062210083
I0218 22:40:08.035450 140483498055424 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.6903741955757141, loss=1.3201850652694702
I0218 22:41:23.499380 140468535932672 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.6946579813957214, loss=1.3732738494873047
I0218 22:42:39.265558 140483498055424 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.6388883590698242, loss=1.281194806098938
I0218 22:43:54.807093 140468535932672 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.6255297660827637, loss=1.2668331861495972
I0218 22:44:08.132924 140549388556096 spec.py:321] Evaluating on the training split.
I0218 22:45:02.126845 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 22:45:51.977880 140549388556096 spec.py:349] Evaluating on the test split.
I0218 22:46:17.511619 140549388556096 submission_runner.py:408] Time since start: 14297.63s, 	Step: 16819, 	{'train/ctc_loss': Array(0.21822503, dtype=float32), 'train/wer': 0.08363072877535688, 'validation/ctc_loss': Array(0.5099662, dtype=float32), 'validation/wer': 0.1562991783890246, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30219615, dtype=float32), 'test/wer': 0.10397497613389393, 'test/num_examples': 2472, 'score': 12998.268486976624, 'total_duration': 14297.626657962799, 'accumulated_submission_time': 12998.268486976624, 'accumulated_eval_time': 1298.191771030426, 'accumulated_logging_time': 0.4646894931793213}
I0218 22:46:17.551030 140483498055424 logging_writer.py:48] [16819] accumulated_eval_time=1298.191771, accumulated_logging_time=0.464689, accumulated_submission_time=12998.268487, global_step=16819, preemption_count=0, score=12998.268487, test/ctc_loss=0.3021961450576782, test/num_examples=2472, test/wer=0.103975, total_duration=14297.626658, train/ctc_loss=0.21822503209114075, train/wer=0.083631, validation/ctc_loss=0.5099661946296692, validation/num_examples=5348, validation/wer=0.156299
I0218 22:47:19.280718 140468535932672 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.7314531207084656, loss=1.318036675453186
I0218 22:48:34.870270 140483498055424 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.7054039239883423, loss=1.3241877555847168
I0218 22:49:50.350126 140468535932672 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.5377458930015564, loss=1.3533354997634888
I0218 22:51:05.987384 140483498055424 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.6182100772857666, loss=1.3484916687011719
I0218 22:52:21.592576 140468535932672 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.5977745056152344, loss=1.3231505155563354
I0218 22:53:39.309433 140483498055424 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.6448391675949097, loss=1.3262510299682617
I0218 22:55:00.406306 140468535932672 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.6114677786827087, loss=1.3510205745697021
I0218 22:56:19.768574 140483498055424 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.718599796295166, loss=1.3128632307052612
I0218 22:57:35.419094 140468535932672 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.7496926784515381, loss=1.310315728187561
I0218 22:58:51.303829 140483498055424 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.5773831605911255, loss=1.3522469997406006
I0218 23:00:06.893691 140468535932672 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.8267813324928284, loss=1.3066874742507935
I0218 23:01:22.524411 140483498055424 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.5598885416984558, loss=1.3001677989959717
I0218 23:02:38.165388 140468535932672 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.71071457862854, loss=1.2632124423980713
I0218 23:04:00.196439 140483498055424 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.682029664516449, loss=1.2475523948669434
I0218 23:05:21.964284 140468535932672 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.7483675479888916, loss=1.3213474750518799
I0218 23:06:43.295302 140483498055424 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.5382100939750671, loss=1.2889595031738281
I0218 23:08:03.827741 140468535932672 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.6447996497154236, loss=1.346010446548462
I0218 23:09:24.523789 140483498055424 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.8559622168540955, loss=1.2962185144424438
I0218 23:10:17.903955 140549388556096 spec.py:321] Evaluating on the training split.
I0218 23:11:11.867628 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 23:12:02.516418 140549388556096 spec.py:349] Evaluating on the test split.
I0218 23:12:27.957059 140549388556096 submission_runner.py:408] Time since start: 15868.07s, 	Step: 18672, 	{'train/ctc_loss': Array(0.21849884, dtype=float32), 'train/wer': 0.08307254319697607, 'validation/ctc_loss': Array(0.4908321, dtype=float32), 'validation/wer': 0.14991745271633664, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29380992, dtype=float32), 'test/wer': 0.09956736335384803, 'test/num_examples': 2472, 'score': 14438.530722856522, 'total_duration': 15868.071051120758, 'accumulated_submission_time': 14438.530722856522, 'accumulated_eval_time': 1428.2382934093475, 'accumulated_logging_time': 0.5230772495269775}
I0218 23:12:27.991647 140483498055424 logging_writer.py:48] [18672] accumulated_eval_time=1428.238293, accumulated_logging_time=0.523077, accumulated_submission_time=14438.530723, global_step=18672, preemption_count=0, score=14438.530723, test/ctc_loss=0.2938099205493927, test/num_examples=2472, test/wer=0.099567, total_duration=15868.071051, train/ctc_loss=0.2184988409280777, train/wer=0.083073, validation/ctc_loss=0.4908320903778076, validation/num_examples=5348, validation/wer=0.149917
I0218 23:12:49.865955 140468535932672 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.7778182029724121, loss=1.2321406602859497
I0218 23:14:05.279657 140483498055424 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.5540009140968323, loss=1.2767702341079712
I0218 23:15:21.066466 140468535932672 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.4775822162628174, loss=1.2478817701339722
I0218 23:16:36.521833 140483498055424 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.5344517230987549, loss=1.278667688369751
I0218 23:17:52.153076 140468535932672 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.625317394733429, loss=1.2708572149276733
I0218 23:19:07.744975 140483498055424 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.6709148287773132, loss=1.265040636062622
I0218 23:20:23.401022 140468535932672 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.6507221460342407, loss=1.3458272218704224
I0218 23:21:40.810862 140483498055424 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.6072332859039307, loss=1.2994049787521362
I0218 23:23:02.349671 140468535932672 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.5897843837738037, loss=1.3432897329330444
I0218 23:24:25.071555 140483498055424 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.6126031279563904, loss=1.2378168106079102
I0218 23:25:40.657121 140468535932672 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.6841970682144165, loss=1.2457034587860107
I0218 23:26:56.280995 140483498055424 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.6239794492721558, loss=1.2732410430908203
I0218 23:28:12.021765 140468535932672 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.5785541534423828, loss=1.3254722356796265
I0218 23:29:27.707211 140483498055424 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.7269724607467651, loss=1.2668672800064087
I0218 23:30:43.640334 140468535932672 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.5661323070526123, loss=1.2328380346298218
I0218 23:31:59.358494 140483498055424 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.5137694478034973, loss=1.2618577480316162
I0218 23:33:19.284842 140468535932672 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.69285649061203, loss=1.2806392908096313
I0218 23:34:41.589567 140483498055424 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.7191993594169617, loss=1.3230730295181274
I0218 23:36:03.916287 140468535932672 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.6713152527809143, loss=1.3018078804016113
I0218 23:36:28.672286 140549388556096 spec.py:321] Evaluating on the training split.
I0218 23:37:23.423264 140549388556096 spec.py:333] Evaluating on the validation split.
I0218 23:38:14.099683 140549388556096 spec.py:349] Evaluating on the test split.
I0218 23:38:39.918196 140549388556096 submission_runner.py:408] Time since start: 17440.03s, 	Step: 20532, 	{'train/ctc_loss': Array(0.20132376, dtype=float32), 'train/wer': 0.07777789650271939, 'validation/ctc_loss': Array(0.49051324, dtype=float32), 'validation/wer': 0.1492223176960136, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28147247, dtype=float32), 'test/wer': 0.0954034895293807, 'test/num_examples': 2472, 'score': 15879.123154878616, 'total_duration': 17440.032745838165, 'accumulated_submission_time': 15879.123154878616, 'accumulated_eval_time': 1559.4781639575958, 'accumulated_logging_time': 0.5743563175201416}
I0218 23:38:39.953210 140483498055424 logging_writer.py:48] [20532] accumulated_eval_time=1559.478164, accumulated_logging_time=0.574356, accumulated_submission_time=15879.123155, global_step=20532, preemption_count=0, score=15879.123155, test/ctc_loss=0.28147247433662415, test/num_examples=2472, test/wer=0.095403, total_duration=17440.032746, train/ctc_loss=0.2013237625360489, train/wer=0.077778, validation/ctc_loss=0.49051323533058167, validation/num_examples=5348, validation/wer=0.149222
I0218 23:39:35.348275 140483498055424 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.6340347528457642, loss=1.2366067171096802
I0218 23:40:50.856940 140468535932672 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.5165615677833557, loss=1.2848016023635864
I0218 23:42:06.427769 140483498055424 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.6659228801727295, loss=1.283984899520874
I0218 23:43:21.940390 140468535932672 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.636511504650116, loss=1.209610939025879
I0218 23:44:37.448209 140483498055424 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.6476593613624573, loss=1.3093211650848389
I0218 23:45:53.115300 140468535932672 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.589683473110199, loss=1.2319526672363281
I0218 23:47:11.613780 140483498055424 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.5505127906799316, loss=1.2422581911087036
I0218 23:48:31.955760 140468535932672 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.7161283493041992, loss=1.2758548259735107
I0218 23:49:52.441150 140483498055424 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.6860754489898682, loss=1.2262098789215088
I0218 23:51:13.474632 140468535932672 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.5523895025253296, loss=1.3162448406219482
I0218 23:52:34.192865 140483498055424 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.6292209029197693, loss=1.2603700160980225
I0218 23:53:54.460736 140483498055424 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.7013105154037476, loss=1.2578186988830566
I0218 23:55:10.101386 140468535932672 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.7782271504402161, loss=1.2199288606643677
I0218 23:56:25.715311 140483498055424 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.6236191987991333, loss=1.2512143850326538
I0218 23:57:41.387498 140468535932672 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.6361104846000671, loss=1.2587436437606812
I0218 23:58:57.002430 140483498055424 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.5752243995666504, loss=1.2770518064498901
I0219 00:00:12.638765 140468535932672 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.6171092391014099, loss=1.2318298816680908
I0219 00:01:28.598905 140483498055424 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.533520519733429, loss=1.294290542602539
I0219 00:02:40.361011 140549388556096 spec.py:321] Evaluating on the training split.
I0219 00:03:34.248714 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 00:04:25.108677 140549388556096 spec.py:349] Evaluating on the test split.
I0219 00:04:50.900271 140549388556096 submission_runner.py:408] Time since start: 19011.01s, 	Step: 22391, 	{'train/ctc_loss': Array(0.2204811, dtype=float32), 'train/wer': 0.08096725040776032, 'validation/ctc_loss': Array(0.4734632, dtype=float32), 'validation/wer': 0.14523494598221612, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27528322, dtype=float32), 'test/wer': 0.09422541791075092, 'test/num_examples': 2472, 'score': 17319.44011616707, 'total_duration': 19011.014504671097, 'accumulated_submission_time': 17319.44011616707, 'accumulated_eval_time': 1690.0110762119293, 'accumulated_logging_time': 0.626962423324585}
I0219 00:04:50.934678 140483498055424 logging_writer.py:48] [22391] accumulated_eval_time=1690.011076, accumulated_logging_time=0.626962, accumulated_submission_time=17319.440116, global_step=22391, preemption_count=0, score=17319.440116, test/ctc_loss=0.27528321743011475, test/num_examples=2472, test/wer=0.094225, total_duration=19011.014505, train/ctc_loss=0.22048109769821167, train/wer=0.080967, validation/ctc_loss=0.4734632074832916, validation/num_examples=5348, validation/wer=0.145235
I0219 00:04:58.547639 140468535932672 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.635117769241333, loss=1.309648871421814
I0219 00:06:14.036724 140483498055424 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.7257136702537537, loss=1.3078221082687378
I0219 00:07:29.605569 140468535932672 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.6551075577735901, loss=1.258069634437561
I0219 00:08:48.594622 140483498055424 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.5619340538978577, loss=1.2769100666046143
I0219 00:10:04.325896 140468535932672 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.6002159714698792, loss=1.2248005867004395
I0219 00:11:20.052084 140483498055424 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.5097736716270447, loss=1.235946774482727
I0219 00:12:35.777550 140468535932672 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.6466180086135864, loss=1.2604097127914429
I0219 00:13:51.573254 140483498055424 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.5404117703437805, loss=1.1637455224990845
I0219 00:15:07.382061 140468535932672 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.5593215227127075, loss=1.2191497087478638
I0219 00:16:23.279590 140483498055424 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.5498461127281189, loss=1.2326350212097168
I0219 00:17:40.515352 140468535932672 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.6247619986534119, loss=1.1760375499725342
I0219 00:19:00.655356 140483498055424 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.5182060599327087, loss=1.2555700540542603
I0219 00:20:21.050771 140468535932672 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.682361364364624, loss=1.2072237730026245
I0219 00:21:44.789081 140483498055424 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.5286799669265747, loss=1.1875020265579224
I0219 00:23:00.203678 140468535932672 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.564670205116272, loss=1.1840604543685913
I0219 00:24:15.894380 140483498055424 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.8196541666984558, loss=1.2123833894729614
I0219 00:25:31.560523 140468535932672 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.6416138410568237, loss=1.2661890983581543
I0219 00:26:47.230515 140483498055424 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.5420106053352356, loss=1.2611626386642456
I0219 00:28:02.869370 140468535932672 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.6398548483848572, loss=1.1955995559692383
I0219 00:28:51.528985 140549388556096 spec.py:321] Evaluating on the training split.
I0219 00:29:45.220327 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 00:30:34.962428 140549388556096 spec.py:349] Evaluating on the test split.
I0219 00:31:00.123251 140549388556096 submission_runner.py:408] Time since start: 20580.24s, 	Step: 24266, 	{'train/ctc_loss': Array(0.19056775, dtype=float32), 'train/wer': 0.07419423240033927, 'validation/ctc_loss': Array(0.45724428, dtype=float32), 'validation/wer': 0.13879529239116792, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26455674, dtype=float32), 'test/wer': 0.09042715251965146, 'test/num_examples': 2472, 'score': 18759.946305513382, 'total_duration': 20580.23752808571, 'accumulated_submission_time': 18759.946305513382, 'accumulated_eval_time': 1818.5990698337555, 'accumulated_logging_time': 0.6770737171173096}
I0219 00:31:00.161350 140483498055424 logging_writer.py:48] [24266] accumulated_eval_time=1818.599070, accumulated_logging_time=0.677074, accumulated_submission_time=18759.946306, global_step=24266, preemption_count=0, score=18759.946306, test/ctc_loss=0.26455673575401306, test/num_examples=2472, test/wer=0.090427, total_duration=20580.237528, train/ctc_loss=0.190567746758461, train/wer=0.074194, validation/ctc_loss=0.45724427700042725, validation/num_examples=5348, validation/wer=0.138795
I0219 00:31:26.576321 140468535932672 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.6444247961044312, loss=1.2551223039627075
I0219 00:32:41.998774 140483498055424 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.7247024774551392, loss=1.2526451349258423
I0219 00:33:57.704918 140468535932672 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.5175719261169434, loss=1.2554019689559937
I0219 00:35:13.227977 140483498055424 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.5846990942955017, loss=1.2720983028411865
I0219 00:36:28.745023 140468535932672 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.6008186340332031, loss=1.2191191911697388
I0219 00:37:47.576303 140483498055424 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.6196189522743225, loss=1.1711914539337158
I0219 00:39:03.162407 140468535932672 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.608842134475708, loss=1.226828694343567
I0219 00:40:18.821751 140483498055424 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.5601248741149902, loss=1.1895323991775513
I0219 00:41:34.383075 140468535932672 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.6271106004714966, loss=1.2778786420822144
I0219 00:42:50.167987 140483498055424 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.6131153106689453, loss=1.2506026029586792
I0219 00:44:05.809917 140468535932672 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.7016141414642334, loss=1.2226394414901733
I0219 00:45:22.203693 140483498055424 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.9173985123634338, loss=1.2026129961013794
I0219 00:46:42.856089 140468535932672 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.5862050652503967, loss=1.2448676824569702
I0219 00:48:04.260758 140483498055424 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.4901486933231354, loss=1.164466381072998
I0219 00:49:24.914123 140468535932672 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.71112459897995, loss=1.1580486297607422
I0219 00:50:46.181812 140483498055424 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.6605669856071472, loss=1.2571171522140503
I0219 00:52:01.722962 140468535932672 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.5767943263053894, loss=1.188361644744873
I0219 00:53:17.337877 140483498055424 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.6022794842720032, loss=1.211754560470581
I0219 00:54:33.007088 140468535932672 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.5556023716926575, loss=1.2346293926239014
I0219 00:55:00.619200 140549388556096 spec.py:321] Evaluating on the training split.
I0219 00:55:55.855303 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 00:56:46.163734 140549388556096 spec.py:349] Evaluating on the test split.
I0219 00:57:11.618032 140549388556096 submission_runner.py:408] Time since start: 22151.73s, 	Step: 26138, 	{'train/ctc_loss': Array(0.15853791, dtype=float32), 'train/wer': 0.06276950062122177, 'validation/ctc_loss': Array(0.4487414, dtype=float32), 'validation/wer': 0.13519410679977215, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25610334, dtype=float32), 'test/wer': 0.08640545975260495, 'test/num_examples': 2472, 'score': 20200.31648516655, 'total_duration': 22151.732254505157, 'accumulated_submission_time': 20200.31648516655, 'accumulated_eval_time': 1949.5915653705597, 'accumulated_logging_time': 0.7306003570556641}
I0219 00:57:11.655078 140483498055424 logging_writer.py:48] [26138] accumulated_eval_time=1949.591565, accumulated_logging_time=0.730600, accumulated_submission_time=20200.316485, global_step=26138, preemption_count=0, score=20200.316485, test/ctc_loss=0.2561033368110657, test/num_examples=2472, test/wer=0.086405, total_duration=22151.732255, train/ctc_loss=0.15853790938854218, train/wer=0.062770, validation/ctc_loss=0.4487414062023163, validation/num_examples=5348, validation/wer=0.135194
I0219 00:57:59.189309 140468535932672 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.6261361837387085, loss=1.187451958656311
I0219 00:59:14.793943 140483498055424 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.5765482783317566, loss=1.1845495700836182
I0219 01:00:30.313485 140468535932672 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.6642241477966309, loss=1.2153863906860352
I0219 01:01:45.898306 140483498055424 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.6633856892585754, loss=1.192914605140686
I0219 01:03:01.510827 140468535932672 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.6929550170898438, loss=1.1778347492218018
I0219 01:04:17.203347 140483498055424 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.8906376957893372, loss=1.3141958713531494
I0219 01:05:36.236928 140483498055424 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.635933518409729, loss=1.2219568490982056
I0219 01:06:51.702520 140468535932672 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.6619148850440979, loss=1.20598566532135
I0219 01:08:07.237338 140483498055424 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.5629530549049377, loss=1.1299175024032593
I0219 01:09:22.726730 140468535932672 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.6230416297912598, loss=1.2373225688934326
I0219 01:10:38.258481 140483498055424 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.6732593178749084, loss=1.2280453443527222
I0219 01:11:53.848090 140468535932672 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.6039472818374634, loss=1.224320650100708
I0219 01:13:09.309380 140483498055424 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.5832143425941467, loss=1.157763123512268
I0219 01:14:24.862276 140468535932672 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.5375754833221436, loss=1.2018210887908936
I0219 01:15:44.870979 140483498055424 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.48134300112724304, loss=1.1941759586334229
I0219 01:17:05.537369 140468535932672 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.7251520752906799, loss=1.1881928443908691
I0219 01:18:26.239802 140483498055424 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.7374266982078552, loss=1.212303876876831
I0219 01:19:45.423230 140483498055424 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.6092973947525024, loss=1.2142244577407837
I0219 01:21:00.978231 140468535932672 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.5543356537818909, loss=1.1580321788787842
I0219 01:21:12.033979 140549388556096 spec.py:321] Evaluating on the training split.
I0219 01:22:05.471694 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 01:22:55.244661 140549388556096 spec.py:349] Evaluating on the test split.
I0219 01:23:20.964526 140549388556096 submission_runner.py:408] Time since start: 23721.08s, 	Step: 28016, 	{'train/ctc_loss': Array(0.15658772, dtype=float32), 'train/wer': 0.061639522721936436, 'validation/ctc_loss': Array(0.43760598, dtype=float32), 'validation/wer': 0.13245218533072015, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24930707, dtype=float32), 'test/wer': 0.08565393130623769, 'test/num_examples': 2472, 'score': 21640.60538506508, 'total_duration': 23721.0798664093, 'accumulated_submission_time': 21640.60538506508, 'accumulated_eval_time': 2078.5168731212616, 'accumulated_logging_time': 0.785571813583374}
I0219 01:23:21.000269 140483498055424 logging_writer.py:48] [28016] accumulated_eval_time=2078.516873, accumulated_logging_time=0.785572, accumulated_submission_time=21640.605385, global_step=28016, preemption_count=0, score=21640.605385, test/ctc_loss=0.2493070662021637, test/num_examples=2472, test/wer=0.085654, total_duration=23721.079866, train/ctc_loss=0.15658771991729736, train/wer=0.061640, validation/ctc_loss=0.43760597705841064, validation/num_examples=5348, validation/wer=0.132452
I0219 01:24:25.052533 140468535932672 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.553722620010376, loss=1.1763279438018799
I0219 01:25:40.515474 140483498055424 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.5482845306396484, loss=1.182844638824463
I0219 01:26:56.111267 140468535932672 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.712783932685852, loss=1.1748374700546265
I0219 01:28:11.806957 140483498055424 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.6139300465583801, loss=1.202514410018921
I0219 01:29:27.380768 140468535932672 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.6223560571670532, loss=1.175523042678833
I0219 01:30:43.003260 140483498055424 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.7112360000610352, loss=1.190687894821167
I0219 01:31:58.662576 140468535932672 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.7069138288497925, loss=1.1924266815185547
I0219 01:33:18.775630 140483498055424 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.6372255682945251, loss=1.183242678642273
I0219 01:34:39.791759 140483498055424 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.5506377816200256, loss=1.1811038255691528
I0219 01:35:55.596704 140468535932672 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.6561890244483948, loss=1.1959388256072998
I0219 01:37:11.203452 140483498055424 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.5592631101608276, loss=1.1653038263320923
I0219 01:38:26.867944 140468535932672 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.6186140179634094, loss=1.176798939704895
I0219 01:39:42.587639 140483498055424 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.6403250098228455, loss=1.169656753540039
I0219 01:40:58.293452 140468535932672 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.5942973494529724, loss=1.1952240467071533
I0219 01:42:13.952010 140483498055424 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.6164225935935974, loss=1.21541428565979
I0219 01:43:30.294435 140468535932672 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.567770779132843, loss=1.1599253416061401
I0219 01:44:50.600747 140483498055424 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.6197789311408997, loss=1.1845145225524902
I0219 01:46:10.995712 140468535932672 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.690244734287262, loss=1.2028473615646362
I0219 01:47:21.314468 140549388556096 spec.py:321] Evaluating on the training split.
I0219 01:48:15.278369 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 01:49:05.655482 140549388556096 spec.py:349] Evaluating on the test split.
I0219 01:49:31.277455 140549388556096 submission_runner.py:408] Time since start: 25291.39s, 	Step: 29887, 	{'train/ctc_loss': Array(0.1453827, dtype=float32), 'train/wer': 0.05890779836277467, 'validation/ctc_loss': Array(0.42563367, dtype=float32), 'validation/wer': 0.12759589484151887, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24524696, dtype=float32), 'test/wer': 0.08112444904840249, 'test/num_examples': 2472, 'score': 23080.832427740097, 'total_duration': 25291.391725063324, 'accumulated_submission_time': 23080.832427740097, 'accumulated_eval_time': 2208.4737632274628, 'accumulated_logging_time': 0.8361155986785889}
I0219 01:49:31.316991 140483498055424 logging_writer.py:48] [29887] accumulated_eval_time=2208.473763, accumulated_logging_time=0.836116, accumulated_submission_time=23080.832428, global_step=29887, preemption_count=0, score=23080.832428, test/ctc_loss=0.24524696171283722, test/num_examples=2472, test/wer=0.081124, total_duration=25291.391725, train/ctc_loss=0.14538270235061646, train/wer=0.058908, validation/ctc_loss=0.42563366889953613, validation/num_examples=5348, validation/wer=0.127596
I0219 01:49:41.898890 140468535932672 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.7273446917533875, loss=1.1858601570129395
I0219 01:50:57.290472 140483498055424 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.7012350559234619, loss=1.1231317520141602
I0219 01:52:13.080875 140468535932672 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.6546545028686523, loss=1.177728295326233
I0219 01:53:28.698823 140483498055424 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.6617906093597412, loss=1.1590943336486816
I0219 01:54:44.302863 140468535932672 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.7213175296783447, loss=1.1696879863739014
I0219 01:56:00.024923 140483498055424 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.5998405814170837, loss=1.180084228515625
I0219 01:57:15.772619 140468535932672 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.6045272350311279, loss=1.1387237310409546
I0219 01:58:31.452592 140483498055424 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.5558138489723206, loss=1.1717503070831299
I0219 01:59:47.074517 140468535932672 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.665838360786438, loss=1.147274136543274
I0219 02:01:02.676509 140483498055424 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.5118050575256348, loss=1.1958258152008057
I0219 02:02:24.805514 140483498055424 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.6015698313713074, loss=1.1110949516296387
I0219 02:03:40.414965 140468535932672 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.6648110151290894, loss=1.1936875581741333
I0219 02:04:55.993215 140483498055424 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.5644915699958801, loss=1.1553641557693481
I0219 02:06:11.824928 140468535932672 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.5475935935974121, loss=1.1471627950668335
I0219 02:07:27.392793 140483498055424 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.8560872077941895, loss=1.200124979019165
I0219 02:08:42.963364 140468535932672 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.6639246940612793, loss=1.1615171432495117
I0219 02:09:58.409136 140483498055424 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.6164317727088928, loss=1.1433161497116089
I0219 02:11:14.069851 140468535932672 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.561937689781189, loss=1.0750125646591187
I0219 02:12:31.659400 140483498055424 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.6672229170799255, loss=1.1097006797790527
I0219 02:13:31.783610 140549388556096 spec.py:321] Evaluating on the training split.
I0219 02:14:26.749173 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 02:15:17.156897 140549388556096 spec.py:349] Evaluating on the test split.
I0219 02:15:42.542795 140549388556096 submission_runner.py:408] Time since start: 26862.66s, 	Step: 31777, 	{'train/ctc_loss': Array(0.15269236, dtype=float32), 'train/wer': 0.05871874967352354, 'validation/ctc_loss': Array(0.4212963, dtype=float32), 'validation/wer': 0.12711316218851676, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2405563, dtype=float32), 'test/wer': 0.08043385534092987, 'test/num_examples': 2472, 'score': 24521.21066737175, 'total_duration': 26862.657521247864, 'accumulated_submission_time': 24521.21066737175, 'accumulated_eval_time': 2339.2270991802216, 'accumulated_logging_time': 0.8912203311920166}
I0219 02:15:42.582151 140483498055424 logging_writer.py:48] [31777] accumulated_eval_time=2339.227099, accumulated_logging_time=0.891220, accumulated_submission_time=24521.210667, global_step=31777, preemption_count=0, score=24521.210667, test/ctc_loss=0.24055629968643188, test/num_examples=2472, test/wer=0.080434, total_duration=26862.657521, train/ctc_loss=0.15269236266613007, train/wer=0.058719, validation/ctc_loss=0.42129629850387573, validation/num_examples=5348, validation/wer=0.127113
I0219 02:16:00.724832 140468535932672 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.5563028454780579, loss=1.129979133605957
I0219 02:17:16.235537 140483498055424 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.590376079082489, loss=1.1611855030059814
I0219 02:18:34.970086 140483498055424 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.6926220059394836, loss=1.1349170207977295
I0219 02:19:50.507774 140468535932672 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.7529870271682739, loss=1.1691551208496094
I0219 02:21:06.093463 140483498055424 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.6601967215538025, loss=1.093778133392334
I0219 02:22:21.899022 140468535932672 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.6321213245391846, loss=1.1613234281539917
I0219 02:23:37.484638 140483498055424 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.5748636722564697, loss=1.1602845191955566
I0219 02:24:53.100710 140468535932672 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.6208561062812805, loss=1.105520248413086
I0219 02:26:08.714947 140483498055424 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.6819772124290466, loss=1.111881136894226
I0219 02:27:27.473867 140468535932672 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.5480309724807739, loss=1.1205170154571533
I0219 02:28:47.233162 140483498055424 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.5370779037475586, loss=1.1516399383544922
I0219 02:30:06.805807 140468535932672 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.7488254308700562, loss=1.1895854473114014
I0219 02:31:28.108765 140483498055424 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.5506181716918945, loss=1.1292251348495483
I0219 02:32:43.564017 140468535932672 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.5994219183921814, loss=1.1138664484024048
I0219 02:33:59.102174 140483498055424 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.5497031211853027, loss=1.1421806812286377
I0219 02:35:14.682035 140468535932672 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.6427562832832336, loss=1.1986881494522095
I0219 02:36:30.183480 140483498055424 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.696229875087738, loss=1.1743922233581543
I0219 02:37:45.894798 140468535932672 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.6248801350593567, loss=1.14452064037323
I0219 02:39:01.367537 140483498055424 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.643031120300293, loss=1.1670894622802734
I0219 02:39:42.562542 140549388556096 spec.py:321] Evaluating on the training split.
I0219 02:40:37.014124 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 02:41:27.756160 140549388556096 spec.py:349] Evaluating on the test split.
I0219 02:41:53.047838 140549388556096 submission_runner.py:408] Time since start: 28433.16s, 	Step: 33656, 	{'train/ctc_loss': Array(0.14723137, dtype=float32), 'train/wer': 0.05695567709103195, 'validation/ctc_loss': Array(0.41512138, dtype=float32), 'validation/wer': 0.12382092549504234, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23121922, dtype=float32), 'test/wer': 0.0780777121036703, 'test/num_examples': 2472, 'score': 25961.102822065353, 'total_duration': 28433.16188645363, 'accumulated_submission_time': 25961.102822065353, 'accumulated_eval_time': 2469.7058634757996, 'accumulated_logging_time': 0.9466145038604736}
I0219 02:41:53.093014 140483498055424 logging_writer.py:48] [33656] accumulated_eval_time=2469.705863, accumulated_logging_time=0.946615, accumulated_submission_time=25961.102822, global_step=33656, preemption_count=0, score=25961.102822, test/ctc_loss=0.23121921718120575, test/num_examples=2472, test/wer=0.078078, total_duration=28433.161886, train/ctc_loss=0.14723137021064758, train/wer=0.056956, validation/ctc_loss=0.4151213765144348, validation/num_examples=5348, validation/wer=0.123821
I0219 02:42:26.981442 140468535932672 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.5294457674026489, loss=1.1110217571258545
I0219 02:43:42.387531 140483498055424 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.6054782867431641, loss=1.214235782623291
I0219 02:44:57.871089 140468535932672 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.5047363638877869, loss=1.1733086109161377
I0219 02:46:16.566771 140483498055424 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.6785057783126831, loss=1.1099541187286377
I0219 02:47:31.942570 140468535932672 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.5908769369125366, loss=1.1743760108947754
I0219 02:48:47.490500 140483498055424 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.5464151501655579, loss=1.0778647661209106
I0219 02:50:03.003974 140468535932672 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.5607597827911377, loss=1.0817806720733643
I0219 02:51:18.537222 140483498055424 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.607180655002594, loss=1.1048334836959839
I0219 02:52:34.054034 140468535932672 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.5269143581390381, loss=1.1051889657974243
I0219 02:53:49.844311 140483498055424 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.7260851263999939, loss=1.1357917785644531
I0219 02:55:06.483820 140468535932672 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.6655666828155518, loss=1.1787807941436768
I0219 02:56:27.026311 140483498055424 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.5778859853744507, loss=1.1189990043640137
I0219 02:57:48.167024 140468535932672 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.6260649561882019, loss=1.1322344541549683
I0219 02:59:08.540727 140483498055424 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.6644467115402222, loss=1.059471607208252
I0219 03:00:27.586994 140483498055424 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.7709732055664062, loss=1.0860579013824463
I0219 03:01:43.050135 140468535932672 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.5704927444458008, loss=1.1307458877563477
I0219 03:02:58.492432 140483498055424 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.785141110420227, loss=1.1066828966140747
I0219 03:04:13.940881 140468535932672 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.7799698710441589, loss=1.133208155632019
I0219 03:05:29.524084 140483498055424 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.7264087796211243, loss=1.126911997795105
I0219 03:05:53.360887 140549388556096 spec.py:321] Evaluating on the training split.
I0219 03:06:48.283225 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 03:07:38.727446 140549388556096 spec.py:349] Evaluating on the test split.
I0219 03:08:04.101400 140549388556096 submission_runner.py:408] Time since start: 30004.22s, 	Step: 35533, 	{'train/ctc_loss': Array(0.13383973, dtype=float32), 'train/wer': 0.052879318460432326, 'validation/ctc_loss': Array(0.4006253, dtype=float32), 'validation/wer': 0.1209148749239696, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22429307, dtype=float32), 'test/wer': 0.07698088680356671, 'test/num_examples': 2472, 'score': 27401.28202843666, 'total_duration': 30004.2161693573, 'accumulated_submission_time': 27401.28202843666, 'accumulated_eval_time': 2600.440567970276, 'accumulated_logging_time': 1.0080838203430176}
I0219 03:08:04.139630 140483498055424 logging_writer.py:48] [35533] accumulated_eval_time=2600.440568, accumulated_logging_time=1.008084, accumulated_submission_time=27401.282028, global_step=35533, preemption_count=0, score=27401.282028, test/ctc_loss=0.2242930680513382, test/num_examples=2472, test/wer=0.076981, total_duration=30004.216169, train/ctc_loss=0.13383972644805908, train/wer=0.052879, validation/ctc_loss=0.4006252884864807, validation/num_examples=5348, validation/wer=0.120915
I0219 03:08:55.253817 140468535932672 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.5581506490707397, loss=1.1427412033081055
I0219 03:10:10.961405 140483498055424 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.6646052002906799, loss=1.1066508293151855
I0219 03:11:26.506641 140468535932672 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.680508017539978, loss=1.1075505018234253
I0219 03:12:42.015986 140483498055424 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.1904288530349731, loss=1.1703453063964844
I0219 03:13:57.554665 140468535932672 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.7204616069793701, loss=1.1196306943893433
I0219 03:15:16.434841 140483498055424 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.7691035270690918, loss=1.0846046209335327
I0219 03:16:32.046833 140468535932672 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.5287156105041504, loss=1.0449942350387573
I0219 03:17:47.740819 140483498055424 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.6166959404945374, loss=1.1234309673309326
I0219 03:19:03.432162 140468535932672 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.8526399731636047, loss=1.1237976551055908
I0219 03:20:19.009018 140483498055424 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.6425753831863403, loss=1.12553071975708
I0219 03:21:34.667055 140468535932672 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.6258635520935059, loss=1.0764464139938354
I0219 03:22:50.254586 140483498055424 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.7217804193496704, loss=1.0818886756896973
I0219 03:24:09.670334 140468535932672 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.5949241518974304, loss=1.1284103393554688
I0219 03:25:30.493250 140483498055424 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.6208257675170898, loss=1.0724695920944214
I0219 03:26:51.662905 140468535932672 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.65514075756073, loss=1.1815063953399658
I0219 03:28:14.572992 140483498055424 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.506670355796814, loss=1.075654149055481
I0219 03:29:30.073503 140468535932672 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.6223847270011902, loss=1.134624719619751
I0219 03:30:45.567975 140483498055424 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.6046969890594482, loss=1.1370875835418701
I0219 03:32:01.081667 140468535932672 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.5659497380256653, loss=1.0573257207870483
I0219 03:32:04.583565 140549388556096 spec.py:321] Evaluating on the training split.
I0219 03:32:58.894870 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 03:33:49.164111 140549388556096 spec.py:349] Evaluating on the test split.
I0219 03:34:15.105753 140549388556096 submission_runner.py:408] Time since start: 31575.22s, 	Step: 37406, 	{'train/ctc_loss': Array(0.12914908, dtype=float32), 'train/wer': 0.04979800996588962, 'validation/ctc_loss': Array(0.39642468, dtype=float32), 'validation/wer': 0.11791227782229645, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21991388, dtype=float32), 'test/wer': 0.0740153961773607, 'test/num_examples': 2472, 'score': 28841.636016368866, 'total_duration': 31575.221014022827, 'accumulated_submission_time': 28841.636016368866, 'accumulated_eval_time': 2730.957435131073, 'accumulated_logging_time': 1.0638580322265625}
I0219 03:34:15.141541 140483498055424 logging_writer.py:48] [37406] accumulated_eval_time=2730.957435, accumulated_logging_time=1.063858, accumulated_submission_time=28841.636016, global_step=37406, preemption_count=0, score=28841.636016, test/ctc_loss=0.21991388499736786, test/num_examples=2472, test/wer=0.074015, total_duration=31575.221014, train/ctc_loss=0.12914907932281494, train/wer=0.049798, validation/ctc_loss=0.39642468094825745, validation/num_examples=5348, validation/wer=0.117912
I0219 03:35:26.799827 140468535932672 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.8005656003952026, loss=1.078548789024353
I0219 03:36:42.531337 140483498055424 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.7137715220451355, loss=1.0781842470169067
I0219 03:37:58.245113 140468535932672 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.1370892524719238, loss=1.124432921409607
I0219 03:39:13.936123 140483498055424 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.6250568628311157, loss=1.134844183921814
I0219 03:40:29.935588 140468535932672 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.716286301612854, loss=1.101563811302185
I0219 03:41:48.185964 140483498055424 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.6876813769340515, loss=1.0348024368286133
I0219 03:43:09.939604 140468535932672 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.584398090839386, loss=1.1599160432815552
I0219 03:44:28.982362 140483498055424 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.9379255771636963, loss=1.1048084497451782
I0219 03:45:44.670828 140468535932672 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.6246391534805298, loss=1.072450876235962
I0219 03:47:00.224091 140483498055424 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.7137694954872131, loss=1.0793284177780151
I0219 03:48:15.825603 140468535932672 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.5578771233558655, loss=1.0898629426956177
I0219 03:49:31.397464 140483498055424 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.6430792212486267, loss=1.1134850978851318
I0219 03:50:47.031234 140468535932672 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.6842512488365173, loss=1.0687427520751953
I0219 03:52:05.360143 140483498055424 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.6164067983627319, loss=1.0894724130630493
I0219 03:53:26.218164 140468535932672 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.577002763748169, loss=1.1203539371490479
I0219 03:54:47.087726 140483498055424 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.5827510356903076, loss=1.067699670791626
I0219 03:56:08.120406 140468535932672 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.5617722868919373, loss=1.1425117254257202
I0219 03:57:29.232031 140483498055424 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.6567295789718628, loss=1.0244332551956177
I0219 03:58:15.667389 140549388556096 spec.py:321] Evaluating on the training split.
I0219 03:59:09.974327 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 04:00:00.376788 140549388556096 spec.py:349] Evaluating on the test split.
I0219 04:00:25.969372 140549388556096 submission_runner.py:408] Time since start: 33146.08s, 	Step: 39263, 	{'train/ctc_loss': Array(0.11346611, dtype=float32), 'train/wer': 0.04597961733738948, 'validation/ctc_loss': Array(0.3963149, dtype=float32), 'validation/wer': 0.11630960541432944, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21920747, dtype=float32), 'test/wer': 0.07375134564215059, 'test/num_examples': 2472, 'score': 30282.072615385056, 'total_duration': 33146.08340334892, 'accumulated_submission_time': 30282.072615385056, 'accumulated_eval_time': 2861.2528777122498, 'accumulated_logging_time': 1.1172175407409668}
I0219 04:00:26.009790 140483498055424 logging_writer.py:48] [39263] accumulated_eval_time=2861.252878, accumulated_logging_time=1.117218, accumulated_submission_time=30282.072615, global_step=39263, preemption_count=0, score=30282.072615, test/ctc_loss=0.21920746564865112, test/num_examples=2472, test/wer=0.073751, total_duration=33146.083403, train/ctc_loss=0.11346611380577087, train/wer=0.045980, validation/ctc_loss=0.3963148891925812, validation/num_examples=5348, validation/wer=0.116310
I0219 04:00:54.653181 140468535932672 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.5805990099906921, loss=1.099123239517212
I0219 04:02:10.102444 140483498055424 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.5479868054389954, loss=1.0494104623794556
I0219 04:03:25.728477 140468535932672 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.635247528553009, loss=1.0595200061798096
I0219 04:04:41.236882 140483498055424 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.7104315757751465, loss=1.0999811887741089
I0219 04:05:56.915657 140468535932672 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.7693371772766113, loss=1.0602725744247437
I0219 04:07:12.414443 140483498055424 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.5153087973594666, loss=0.9973918795585632
I0219 04:08:28.023195 140468535932672 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.562718391418457, loss=1.0677638053894043
I0219 04:09:44.050866 140483498055424 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.6247046589851379, loss=1.0685862302780151
I0219 04:11:05.544780 140468535932672 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.6105622053146362, loss=1.0860912799835205
I0219 04:12:27.593641 140483498055424 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.6063826680183411, loss=1.045330286026001
I0219 04:13:43.011641 140468535932672 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.6987650394439697, loss=1.0710667371749878
I0219 04:14:58.565803 140483498055424 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.6773685216903687, loss=1.0206568241119385
I0219 04:16:14.110555 140468535932672 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.6289059519767761, loss=1.0880423784255981
I0219 04:17:29.541407 140483498055424 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.5272350907325745, loss=1.0197027921676636
I0219 04:18:45.097297 140468535932672 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.7234767079353333, loss=1.0510820150375366
I0219 04:20:00.705101 140483498055424 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.7061073184013367, loss=1.063759207725525
I0219 04:21:20.755253 140468535932672 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.7331964373588562, loss=1.1315209865570068
I0219 04:22:40.610172 140483498055424 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.6594271659851074, loss=1.0955320596694946
I0219 04:24:02.447462 140468535932672 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.9004421830177307, loss=1.0980360507965088
I0219 04:24:26.442955 140549388556096 spec.py:321] Evaluating on the training split.
I0219 04:25:21.532896 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 04:26:12.002539 140549388556096 spec.py:349] Evaluating on the test split.
I0219 04:26:37.337099 140549388556096 submission_runner.py:408] Time since start: 34717.45s, 	Step: 41131, 	{'train/ctc_loss': Array(0.11233612, dtype=float32), 'train/wer': 0.04404061671172253, 'validation/ctc_loss': Array(0.39019135, dtype=float32), 'validation/wer': 0.1171012869652529, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21722014, dtype=float32), 'test/wer': 0.07283732455873093, 'test/num_examples': 2472, 'score': 31722.417976379395, 'total_duration': 34717.45092463493, 'accumulated_submission_time': 31722.417976379395, 'accumulated_eval_time': 2992.140277147293, 'accumulated_logging_time': 1.1743803024291992}
I0219 04:26:37.375626 140483498055424 logging_writer.py:48] [41131] accumulated_eval_time=2992.140277, accumulated_logging_time=1.174380, accumulated_submission_time=31722.417976, global_step=41131, preemption_count=0, score=31722.417976, test/ctc_loss=0.21722014248371124, test/num_examples=2472, test/wer=0.072837, total_duration=34717.450925, train/ctc_loss=0.11233612149953842, train/wer=0.044041, validation/ctc_loss=0.39019134640693665, validation/num_examples=5348, validation/wer=0.117101
I0219 04:27:33.436730 140483498055424 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.7565639019012451, loss=1.1342220306396484
I0219 04:28:49.166700 140468535932672 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.6017784476280212, loss=1.0775668621063232
I0219 04:30:04.759577 140483498055424 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.5918511152267456, loss=1.0592564344406128
I0219 04:31:20.269233 140468535932672 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.6554118990898132, loss=1.0295567512512207
I0219 04:32:35.781479 140483498055424 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.702183187007904, loss=1.0425443649291992
I0219 04:33:51.434489 140468535932672 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.814155101776123, loss=1.0411080121994019
I0219 04:35:06.995552 140483498055424 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.6725631356239319, loss=1.0652114152908325
I0219 04:36:25.419845 140468535932672 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.697237491607666, loss=1.0397828817367554
I0219 04:37:46.401126 140483498055424 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.7453125715255737, loss=1.0984641313552856
I0219 04:39:08.289834 140468535932672 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.5588197112083435, loss=1.0411988496780396
I0219 04:40:29.215928 140483498055424 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.5975795388221741, loss=1.0656834840774536
I0219 04:41:48.959602 140483498055424 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.8137106895446777, loss=1.0248570442199707
I0219 04:43:04.814607 140468535932672 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.592443585395813, loss=1.049137830734253
I0219 04:44:20.404907 140483498055424 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.7948931455612183, loss=1.0203511714935303
I0219 04:45:36.016939 140468535932672 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.5771967768669128, loss=1.043937087059021
I0219 04:46:51.596928 140483498055424 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.8337766528129578, loss=0.9781122803688049
I0219 04:48:07.135223 140468535932672 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.6047690510749817, loss=1.0781623125076294
I0219 04:49:23.259552 140483498055424 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.6860078573226929, loss=1.0777108669281006
I0219 04:50:37.582225 140549388556096 spec.py:321] Evaluating on the training split.
I0219 04:51:31.369889 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 04:52:22.093761 140549388556096 spec.py:349] Evaluating on the test split.
I0219 04:52:47.540850 140549388556096 submission_runner.py:408] Time since start: 36287.66s, 	Step: 42993, 	{'train/ctc_loss': Array(0.11740988, dtype=float32), 'train/wer': 0.046672798095650295, 'validation/ctc_loss': Array(0.38365835, dtype=float32), 'validation/wer': 0.11446556667986135, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20735815, dtype=float32), 'test/wer': 0.07086710133447079, 'test/num_examples': 2472, 'score': 33162.53774404526, 'total_duration': 36287.6553106308, 'accumulated_submission_time': 33162.53774404526, 'accumulated_eval_time': 3122.092783689499, 'accumulated_logging_time': 1.228879451751709}
I0219 04:52:47.583133 140483498055424 logging_writer.py:48] [42993] accumulated_eval_time=3122.092784, accumulated_logging_time=1.228879, accumulated_submission_time=33162.537744, global_step=42993, preemption_count=0, score=33162.537744, test/ctc_loss=0.20735815167427063, test/num_examples=2472, test/wer=0.070867, total_duration=36287.655311, train/ctc_loss=0.11740988492965698, train/wer=0.046673, validation/ctc_loss=0.38365834951400757, validation/num_examples=5348, validation/wer=0.114466
I0219 04:52:53.714110 140468535932672 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.7146216034889221, loss=1.0971702337265015
I0219 04:54:09.091000 140483498055424 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.5726761221885681, loss=1.012096881866455
I0219 04:55:24.662864 140468535932672 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.6214450001716614, loss=1.0635219812393188
I0219 04:56:43.585763 140483498055424 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.7201655507087708, loss=1.0535497665405273
I0219 04:57:59.080638 140468535932672 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.5633575916290283, loss=0.9932821393013
I0219 04:59:14.984905 140483498055424 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.6503982543945312, loss=1.001234531402588
I0219 05:00:30.501270 140468535932672 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.6593695282936096, loss=1.0534100532531738
I0219 05:01:46.054729 140483498055424 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.7671330571174622, loss=1.0346226692199707
I0219 05:03:01.633969 140468535932672 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.5729719996452332, loss=1.0434234142303467
I0219 05:04:17.177961 140483498055424 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.725544810295105, loss=1.0083019733428955
I0219 05:05:37.736567 140468535932672 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.6402186155319214, loss=1.0609056949615479
I0219 05:06:58.566430 140483498055424 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.6205019354820251, loss=1.0619927644729614
I0219 05:08:19.959862 140468535932672 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.6383601427078247, loss=1.0834035873413086
I0219 05:09:43.687989 140483498055424 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.8356582522392273, loss=1.0649842023849487
I0219 05:10:59.173245 140468535932672 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.6822719573974609, loss=1.0495119094848633
I0219 05:12:14.841092 140483498055424 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.5717736482620239, loss=1.0097993612289429
I0219 05:13:30.719326 140468535932672 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.8306458592414856, loss=1.029370903968811
I0219 05:14:46.317173 140483498055424 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.6062799692153931, loss=1.0316452980041504
I0219 05:16:01.847934 140468535932672 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.6886175274848938, loss=1.0102590322494507
I0219 05:16:48.234543 140549388556096 spec.py:321] Evaluating on the training split.
I0219 05:17:41.976866 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 05:18:32.584626 140549388556096 spec.py:349] Evaluating on the test split.
I0219 05:18:58.015845 140549388556096 submission_runner.py:408] Time since start: 37858.13s, 	Step: 44863, 	{'train/ctc_loss': Array(0.10558901, dtype=float32), 'train/wer': 0.041014103534872066, 'validation/ctc_loss': Array(0.378038, dtype=float32), 'validation/wer': 0.11151124284348841, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20476188, dtype=float32), 'test/wer': 0.06828753072126419, 'test/num_examples': 2472, 'score': 34603.10031223297, 'total_duration': 37858.130298137665, 'accumulated_submission_time': 34603.10031223297, 'accumulated_eval_time': 3251.867955684662, 'accumulated_logging_time': 1.2873446941375732}
I0219 05:18:58.054310 140483498055424 logging_writer.py:48] [44863] accumulated_eval_time=3251.867956, accumulated_logging_time=1.287345, accumulated_submission_time=34603.100312, global_step=44863, preemption_count=0, score=34603.100312, test/ctc_loss=0.20476187765598297, test/num_examples=2472, test/wer=0.068288, total_duration=37858.130298, train/ctc_loss=0.10558900982141495, train/wer=0.041014, validation/ctc_loss=0.3780379891395569, validation/num_examples=5348, validation/wer=0.111511
I0219 05:19:26.647440 140468535932672 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.6031048893928528, loss=0.9952697157859802
I0219 05:20:42.117706 140483498055424 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.723343551158905, loss=1.0608797073364258
I0219 05:21:57.608230 140468535932672 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.6718505620956421, loss=1.0469588041305542
I0219 05:23:13.197985 140483498055424 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.7596476078033447, loss=1.018941879272461
I0219 05:24:28.749611 140468535932672 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.6281719207763672, loss=1.0558663606643677
I0219 05:25:47.390925 140483498055424 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.6845577955245972, loss=1.0157028436660767
I0219 05:27:02.892955 140468535932672 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.6743230819702148, loss=1.0607050657272339
I0219 05:28:18.303421 140483498055424 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.6733986139297485, loss=1.0546940565109253
I0219 05:29:33.947860 140468535932672 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.6335809826850891, loss=1.0389114618301392
I0219 05:30:49.309660 140483498055424 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.6285281777381897, loss=1.042046308517456
I0219 05:32:04.884614 140468535932672 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.6850503087043762, loss=1.0350693464279175
I0219 05:33:20.343299 140483498055424 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.6246245503425598, loss=1.0638606548309326
I0219 05:34:40.755133 140468535932672 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.6076955199241638, loss=1.021694302558899
I0219 05:36:01.967308 140483498055424 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.6389749646186829, loss=1.008652925491333
I0219 05:37:23.724517 140468535932672 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.8629966974258423, loss=1.0359268188476562
I0219 05:38:44.646615 140483498055424 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.7889159321784973, loss=0.9935405850410461
I0219 05:40:00.183239 140468535932672 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.6838359236717224, loss=1.0211349725723267
I0219 05:41:15.795319 140483498055424 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.6233028173446655, loss=1.0270787477493286
I0219 05:42:31.324967 140468535932672 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.6037288308143616, loss=1.0292420387268066
I0219 05:42:58.261495 140549388556096 spec.py:321] Evaluating on the training split.
I0219 05:43:52.894450 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 05:44:43.267772 140549388556096 spec.py:349] Evaluating on the test split.
I0219 05:45:09.081424 140549388556096 submission_runner.py:408] Time since start: 39429.20s, 	Step: 46737, 	{'train/ctc_loss': Array(0.09857526, dtype=float32), 'train/wer': 0.04018159433671899, 'validation/ctc_loss': Array(0.37337637, dtype=float32), 'validation/wer': 0.10832520733367447, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20145229, dtype=float32), 'test/wer': 0.06751569069526538, 'test/num_examples': 2472, 'score': 36043.22028398514, 'total_duration': 39429.1965405941, 'accumulated_submission_time': 36043.22028398514, 'accumulated_eval_time': 3382.6824221611023, 'accumulated_logging_time': 1.341782808303833}
I0219 05:45:09.125485 140483498055424 logging_writer.py:48] [46737] accumulated_eval_time=3382.682422, accumulated_logging_time=1.341783, accumulated_submission_time=36043.220284, global_step=46737, preemption_count=0, score=36043.220284, test/ctc_loss=0.20145228505134583, test/num_examples=2472, test/wer=0.067516, total_duration=39429.196541, train/ctc_loss=0.09857526421546936, train/wer=0.040182, validation/ctc_loss=0.37337636947631836, validation/num_examples=5348, validation/wer=0.108325
I0219 05:45:57.507524 140468535932672 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.633083164691925, loss=1.038865566253662
I0219 05:47:13.021903 140483498055424 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.676034152507782, loss=1.067583441734314
I0219 05:48:28.472582 140468535932672 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.7361451983451843, loss=1.0061849355697632
I0219 05:49:43.998739 140483498055424 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.6692269444465637, loss=1.0173602104187012
I0219 05:50:59.459944 140468535932672 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.828586220741272, loss=1.0461314916610718
I0219 05:52:15.087181 140483498055424 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.5908765196800232, loss=1.0143578052520752
I0219 05:53:37.972096 140483498055424 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.7073851823806763, loss=1.0264487266540527
I0219 05:54:53.424657 140468535932672 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.7269489765167236, loss=1.0039488077163696
I0219 05:56:08.939174 140483498055424 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.833550214767456, loss=0.9897351264953613
I0219 05:57:24.589475 140468535932672 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.5607064366340637, loss=0.9955881237983704
I0219 05:58:40.141962 140483498055424 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.607318639755249, loss=1.018984079360962
I0219 05:59:55.724179 140468535932672 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.6684320569038391, loss=1.0457772016525269
I0219 06:01:11.670616 140483498055424 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.6307109594345093, loss=0.973636269569397
I0219 06:02:28.291633 140468535932672 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.6541110277175903, loss=0.9928406476974487
I0219 06:03:48.536117 140483498055424 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.6179233193397522, loss=1.0173182487487793
I0219 06:05:08.559863 140468535932672 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.6205832958221436, loss=1.035123586654663
I0219 06:06:28.979861 140483498055424 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.8656330108642578, loss=1.0034475326538086
I0219 06:07:48.080129 140483498055424 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.7514780163764954, loss=1.0420469045639038
I0219 06:09:03.643801 140468535932672 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.640369176864624, loss=0.9786577820777893
I0219 06:09:09.416081 140549388556096 spec.py:321] Evaluating on the training split.
I0219 06:10:03.132006 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 06:10:53.655259 140549388556096 spec.py:349] Evaluating on the test split.
I0219 06:11:19.310379 140549388556096 submission_runner.py:408] Time since start: 40999.42s, 	Step: 48609, 	{'train/ctc_loss': Array(0.11590645, dtype=float32), 'train/wer': 0.04348681621408894, 'validation/ctc_loss': Array(0.36853057, dtype=float32), 'validation/wer': 0.10704113847668884, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19999944, dtype=float32), 'test/wer': 0.06763756017305465, 'test/num_examples': 2472, 'score': 37483.42413520813, 'total_duration': 40999.42491674423, 'accumulated_submission_time': 37483.42413520813, 'accumulated_eval_time': 3512.5706765651703, 'accumulated_logging_time': 1.40153169631958}
I0219 06:11:19.350572 140483498055424 logging_writer.py:48] [48609] accumulated_eval_time=3512.570677, accumulated_logging_time=1.401532, accumulated_submission_time=37483.424135, global_step=48609, preemption_count=0, score=37483.424135, test/ctc_loss=0.19999943673610687, test/num_examples=2472, test/wer=0.067638, total_duration=40999.424917, train/ctc_loss=0.11590644717216492, train/wer=0.043487, validation/ctc_loss=0.3685305714607239, validation/num_examples=5348, validation/wer=0.107041
I0219 06:12:28.619479 140468535932672 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.7759108543395996, loss=0.9912369847297668
I0219 06:13:44.123585 140483498055424 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.6492449045181274, loss=1.0394290685653687
I0219 06:14:59.657980 140468535932672 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.5674771666526794, loss=1.0113465785980225
I0219 06:16:15.179514 140483498055424 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.7738168239593506, loss=1.0199551582336426
I0219 06:17:30.942680 140468535932672 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.666612982749939, loss=0.9801123738288879
I0219 06:18:46.549512 140483498055424 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.9801753759384155, loss=0.9796320199966431
I0219 06:20:02.079207 140468535932672 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.7164918780326843, loss=1.0037635564804077
I0219 06:21:20.998037 140483498055424 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.6511285901069641, loss=0.9644109010696411
I0219 06:22:41.499081 140483498055424 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.6367321014404297, loss=1.0082299709320068
I0219 06:23:57.105477 140468535932672 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.6443748474121094, loss=0.9627354741096497
I0219 06:25:12.766512 140483498055424 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.7758772373199463, loss=1.013649582862854
I0219 06:26:28.422670 140468535932672 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.7210662364959717, loss=0.9954878687858582
I0219 06:27:43.993272 140483498055424 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.6547979712486267, loss=0.9854623079299927
I0219 06:28:59.665628 140468535932672 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.714806854724884, loss=0.9549590349197388
I0219 06:30:15.254061 140483498055424 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.7757061719894409, loss=1.0260367393493652
I0219 06:31:34.029048 140468535932672 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.8417877554893494, loss=0.980957567691803
I0219 06:32:53.808586 140483498055424 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.242056965827942, loss=1.0191744565963745
I0219 06:34:14.898480 140468535932672 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.8060582876205444, loss=1.011147379875183
I0219 06:35:19.331035 140549388556096 spec.py:321] Evaluating on the training split.
I0219 06:36:14.530490 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 06:37:04.567710 140549388556096 spec.py:349] Evaluating on the test split.
I0219 06:37:29.748979 140549388556096 submission_runner.py:408] Time since start: 42569.86s, 	Step: 50478, 	{'train/ctc_loss': Array(0.07502113, dtype=float32), 'train/wer': 0.030328978403101094, 'validation/ctc_loss': Array(0.35714394, dtype=float32), 'validation/wer': 0.10416405186479624, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19472961, dtype=float32), 'test/wer': 0.06446895375053317, 'test/num_examples': 2472, 'score': 38923.31358218193, 'total_duration': 42569.863436460495, 'accumulated_submission_time': 38923.31358218193, 'accumulated_eval_time': 3642.982671022415, 'accumulated_logging_time': 1.4595589637756348}
I0219 06:37:29.789279 140483498055424 logging_writer.py:48] [50478] accumulated_eval_time=3642.982671, accumulated_logging_time=1.459559, accumulated_submission_time=38923.313582, global_step=50478, preemption_count=0, score=38923.313582, test/ctc_loss=0.19472961127758026, test/num_examples=2472, test/wer=0.064469, total_duration=42569.863436, train/ctc_loss=0.07502113282680511, train/wer=0.030329, validation/ctc_loss=0.35714393854141235, validation/num_examples=5348, validation/wer=0.104164
I0219 06:37:47.209839 140468535932672 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.8938559293746948, loss=0.9589713215827942
I0219 06:39:02.640010 140483498055424 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.6295589208602905, loss=0.9561787843704224
I0219 06:40:18.127103 140468535932672 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.8248851299285889, loss=0.9601795077323914
I0219 06:41:33.674998 140483498055424 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.7058268785476685, loss=0.9613505601882935
I0219 06:42:49.127593 140468535932672 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.5885628461837769, loss=1.0015604496002197
I0219 06:44:04.820686 140483498055424 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.7654163837432861, loss=1.0004206895828247
I0219 06:45:20.436491 140468535932672 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.7557520866394043, loss=0.968580424785614
I0219 06:46:36.139456 140483498055424 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.8720887303352356, loss=0.9317926168441772
I0219 06:47:52.050398 140468535932672 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.636906623840332, loss=1.0116584300994873
I0219 06:49:07.648447 140483498055424 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.8406069278717041, loss=1.0203884840011597
I0219 06:50:30.613176 140483498055424 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.6150796413421631, loss=0.9322666525840759
I0219 06:51:46.027995 140468535932672 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.6823546290397644, loss=0.9382577538490295
I0219 06:53:01.579617 140483498055424 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.637122631072998, loss=0.9947022199630737
I0219 06:54:17.062155 140468535932672 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.7952682375907898, loss=0.9898340702056885
I0219 06:55:32.598256 140483498055424 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.6488491892814636, loss=1.0057880878448486
I0219 06:56:48.114259 140468535932672 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.7427778840065002, loss=1.0176422595977783
I0219 06:58:03.643821 140483498055424 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.6545567512512207, loss=1.0216809511184692
I0219 06:59:19.096560 140468535932672 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.6075763702392578, loss=1.0459225177764893
I0219 07:00:36.941280 140483498055424 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.6605396866798401, loss=0.9288102388381958
I0219 07:01:30.069550 140549388556096 spec.py:321] Evaluating on the training split.
I0219 07:02:24.838711 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 07:03:15.072191 140549388556096 spec.py:349] Evaluating on the test split.
I0219 07:03:40.413354 140549388556096 submission_runner.py:408] Time since start: 44140.53s, 	Step: 52367, 	{'train/ctc_loss': Array(0.08077259, dtype=float32), 'train/wer': 0.03261068446888706, 'validation/ctc_loss': Array(0.35393357, dtype=float32), 'validation/wer': 0.10222346659972774, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18940833, dtype=float32), 'test/wer': 0.06320963581337721, 'test/num_examples': 2472, 'score': 40363.50043487549, 'total_duration': 44140.527676820755, 'accumulated_submission_time': 40363.50043487549, 'accumulated_eval_time': 3773.3202497959137, 'accumulated_logging_time': 1.5202560424804688}
I0219 07:03:40.457349 140483498055424 logging_writer.py:48] [52367] accumulated_eval_time=3773.320250, accumulated_logging_time=1.520256, accumulated_submission_time=40363.500435, global_step=52367, preemption_count=0, score=40363.500435, test/ctc_loss=0.1894083321094513, test/num_examples=2472, test/wer=0.063210, total_duration=44140.527677, train/ctc_loss=0.08077258616685867, train/wer=0.032611, validation/ctc_loss=0.35393357276916504, validation/num_examples=5348, validation/wer=0.102223
I0219 07:04:06.441196 140468535932672 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.7543168067932129, loss=0.9767920970916748
I0219 07:05:21.938357 140483498055424 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.7508882880210876, loss=0.9428704380989075
I0219 07:06:40.888605 140483498055424 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.6654719710350037, loss=1.0042601823806763
I0219 07:07:56.370625 140468535932672 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.9996119141578674, loss=0.9053922891616821
I0219 07:09:11.944444 140483498055424 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.6869204044342041, loss=0.946308970451355
I0219 07:10:27.575860 140468535932672 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.2587083578109741, loss=0.9504032731056213
I0219 07:11:43.254251 140483498055424 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.878078281879425, loss=0.9660136103630066
I0219 07:12:58.905527 140468535932672 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.826775074005127, loss=0.995357096195221
I0219 07:14:17.001432 140483498055424 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.7933434844017029, loss=0.978950023651123
I0219 07:15:37.990673 140468535932672 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.6861526966094971, loss=1.0095576047897339
I0219 07:16:58.427674 140483498055424 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.7357063889503479, loss=0.9793124198913574
I0219 07:18:18.735626 140468535932672 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.6950414776802063, loss=0.9991055130958557
I0219 07:19:40.328764 140483498055424 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.8688787221908569, loss=0.9825998544692993
I0219 07:20:55.916386 140468535932672 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.7382124066352844, loss=0.9549976587295532
I0219 07:22:11.481133 140483498055424 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.6808087825775146, loss=0.9715975522994995
I0219 07:23:26.957365 140468535932672 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.6310648918151855, loss=1.019606590270996
I0219 07:24:42.466741 140483498055424 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.7274693250656128, loss=0.9938679337501526
I0219 07:25:58.142429 140468535932672 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.7173349857330322, loss=0.9722315073013306
I0219 07:27:14.121870 140483498055424 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.6612566113471985, loss=0.985651433467865
I0219 07:27:40.900279 140549388556096 spec.py:321] Evaluating on the training split.
I0219 07:28:34.387017 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 07:29:25.017359 140549388556096 spec.py:349] Evaluating on the test split.
I0219 07:29:50.512069 140549388556096 submission_runner.py:408] Time since start: 45710.63s, 	Step: 54234, 	{'train/ctc_loss': Array(0.09836104, dtype=float32), 'train/wer': 0.039445336030074524, 'validation/ctc_loss': Array(0.34822646, dtype=float32), 'validation/wer': 0.09976153006941696, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18704453, dtype=float32), 'test/wer': 0.06178782523916885, 'test/num_examples': 2472, 'score': 41803.85074186325, 'total_duration': 45710.626412153244, 'accumulated_submission_time': 41803.85074186325, 'accumulated_eval_time': 3902.9258086681366, 'accumulated_logging_time': 1.5857644081115723}
I0219 07:29:50.552259 140483498055424 logging_writer.py:48] [54234] accumulated_eval_time=3902.925809, accumulated_logging_time=1.585764, accumulated_submission_time=41803.850742, global_step=54234, preemption_count=0, score=41803.850742, test/ctc_loss=0.18704453110694885, test/num_examples=2472, test/wer=0.061788, total_duration=45710.626412, train/ctc_loss=0.09836103767156601, train/wer=0.039445, validation/ctc_loss=0.3482264578342438, validation/num_examples=5348, validation/wer=0.099762
I0219 07:30:40.988630 140468535932672 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.7439543008804321, loss=1.0008206367492676
I0219 07:31:56.453842 140483498055424 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.8557451963424683, loss=0.9582234025001526
I0219 07:33:11.891339 140468535932672 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.7073671221733093, loss=0.96441251039505
I0219 07:34:30.842457 140483498055424 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.9794889092445374, loss=0.9900553822517395
I0219 07:35:46.712221 140468535932672 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.3862255811691284, loss=0.9557681679725647
I0219 07:37:02.502624 140483498055424 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.6451785564422607, loss=0.9413464069366455
I0219 07:38:18.016139 140468535932672 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.8116510510444641, loss=0.9637983441352844
I0219 07:39:33.521398 140483498055424 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.9099481701850891, loss=0.9470382332801819
I0219 07:40:49.015899 140468535932672 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.7215682864189148, loss=0.9742575287818909
I0219 07:42:04.554881 140483498055424 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.6505151987075806, loss=0.9812690615653992
I0219 07:43:23.887766 140468535932672 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.6480821371078491, loss=0.9035005569458008
I0219 07:44:45.501346 140483498055424 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.8103138208389282, loss=0.9634934067726135
I0219 07:46:07.402133 140468535932672 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.7494915127754211, loss=0.983138918876648
I0219 07:47:28.081469 140483498055424 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.808929979801178, loss=0.9551178812980652
I0219 07:48:47.304323 140483498055424 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.7192844748497009, loss=0.9612284302711487
I0219 07:50:03.163419 140468535932672 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.6445956826210022, loss=0.9166698455810547
I0219 07:51:18.889368 140483498055424 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.7534987926483154, loss=0.9161202311515808
I0219 07:52:34.499426 140468535932672 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.699379026889801, loss=0.9356198906898499
I0219 07:53:50.024291 140483498055424 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.7766406536102295, loss=0.9123181700706482
I0219 07:53:51.259213 140549388556096 spec.py:321] Evaluating on the training split.
I0219 07:54:44.852638 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 07:55:35.267263 140549388556096 spec.py:349] Evaluating on the test split.
I0219 07:56:00.543539 140549388556096 submission_runner.py:408] Time since start: 47280.66s, 	Step: 56103, 	{'train/ctc_loss': Array(0.09685396, dtype=float32), 'train/wer': 0.03788420696577002, 'validation/ctc_loss': Array(0.34267035, dtype=float32), 'validation/wer': 0.0985836623960918, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18341146, dtype=float32), 'test/wer': 0.06073162309832836, 'test/num_examples': 2472, 'score': 43244.468445539474, 'total_duration': 47280.65885734558, 'accumulated_submission_time': 43244.468445539474, 'accumulated_eval_time': 4032.204874753952, 'accumulated_logging_time': 1.641676664352417}
I0219 07:56:00.588847 140483498055424 logging_writer.py:48] [56103] accumulated_eval_time=4032.204875, accumulated_logging_time=1.641677, accumulated_submission_time=43244.468446, global_step=56103, preemption_count=0, score=43244.468446, test/ctc_loss=0.18341146409511566, test/num_examples=2472, test/wer=0.060732, total_duration=47280.658857, train/ctc_loss=0.09685396403074265, train/wer=0.037884, validation/ctc_loss=0.34267035126686096, validation/num_examples=5348, validation/wer=0.098584
I0219 07:57:14.394647 140468535932672 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.7503607273101807, loss=0.930539608001709
I0219 07:58:30.002500 140483498055424 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.7080793380737305, loss=0.9056679606437683
I0219 07:59:45.543192 140468535932672 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.8546748161315918, loss=0.9993472695350647
I0219 08:01:01.166350 140483498055424 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.7567431330680847, loss=0.9440435767173767
I0219 08:02:16.832244 140468535932672 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.8286761045455933, loss=0.978944718837738
I0219 08:03:36.407701 140483498055424 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.7560531497001648, loss=0.9111842513084412
I0219 08:04:51.942299 140468535932672 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.969803512096405, loss=0.9154184460639954
I0219 08:06:07.849564 140483498055424 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.6912227272987366, loss=0.9195467829704285
I0219 08:07:23.311465 140468535932672 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.7729079127311707, loss=0.9742552638053894
I0219 08:08:38.938212 140483498055424 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.7610681653022766, loss=0.9396154284477234
I0219 08:09:54.481136 140468535932672 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.6919972896575928, loss=0.9563349485397339
I0219 08:11:10.931468 140483498055424 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.6818332672119141, loss=0.9389061331748962
I0219 08:12:31.550043 140468535932672 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.8150933384895325, loss=0.9566663503646851
I0219 08:13:52.644932 140483498055424 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.7095369696617126, loss=0.9416816234588623
I0219 08:15:13.880557 140468535932672 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.7196269035339355, loss=0.9139358401298523
I0219 08:16:36.972213 140483498055424 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.0793465375900269, loss=0.9393635988235474
I0219 08:17:52.534994 140468535932672 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.7648050785064697, loss=0.9608669877052307
I0219 08:19:08.250235 140483498055424 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.7671184539794922, loss=0.9701182246208191
I0219 08:20:00.836049 140549388556096 spec.py:321] Evaluating on the training split.
I0219 08:20:52.903249 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 08:21:43.836791 140549388556096 spec.py:349] Evaluating on the test split.
I0219 08:22:10.007540 140549388556096 submission_runner.py:408] Time since start: 48850.12s, 	Step: 57971, 	{'train/ctc_loss': Array(0.10674078, dtype=float32), 'train/wer': 0.043809170224614814, 'validation/ctc_loss': Array(0.34073088, dtype=float32), 'validation/wer': 0.09737683076358651, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18483536, dtype=float32), 'test/wer': 0.06006134097048727, 'test/num_examples': 2472, 'score': 44684.62251138687, 'total_duration': 48850.121492147446, 'accumulated_submission_time': 44684.62251138687, 'accumulated_eval_time': 4161.369755983353, 'accumulated_logging_time': 1.7073440551757812}
I0219 08:22:10.047386 140483498055424 logging_writer.py:48] [57971] accumulated_eval_time=4161.369756, accumulated_logging_time=1.707344, accumulated_submission_time=44684.622511, global_step=57971, preemption_count=0, score=44684.622511, test/ctc_loss=0.18483535945415497, test/num_examples=2472, test/wer=0.060061, total_duration=48850.121492, train/ctc_loss=0.10674078017473221, train/wer=0.043809, validation/ctc_loss=0.3407308757305145, validation/num_examples=5348, validation/wer=0.097377
I0219 08:22:32.906251 140468535932672 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.673140287399292, loss=0.9694316387176514
I0219 08:23:48.233196 140483498055424 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.6960839629173279, loss=0.9741702675819397
I0219 08:25:03.884248 140468535932672 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.7499657273292542, loss=0.952713131904602
I0219 08:26:19.439953 140483498055424 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.0295988321304321, loss=0.9237244725227356
I0219 08:27:35.012630 140468535932672 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.2965826988220215, loss=0.9744526743888855
I0219 08:28:50.595473 140483498055424 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.804977536201477, loss=0.9524155259132385
I0219 08:30:10.974442 140468535932672 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.7942700982093811, loss=0.9061334729194641
I0219 08:31:32.056481 140483498055424 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.2128709554672241, loss=0.9288796782493591
I0219 08:32:51.009398 140483498055424 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.877612292766571, loss=0.928751528263092
I0219 08:34:06.555864 140468535932672 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.8215034008026123, loss=0.8985016345977783
I0219 08:35:22.174676 140483498055424 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.8288219571113586, loss=0.9381489157676697
I0219 08:36:37.758697 140468535932672 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.6765127182006836, loss=0.9241058826446533
I0219 08:37:53.544107 140483498055424 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.6983932852745056, loss=0.9207245707511902
I0219 08:39:09.200054 140468535932672 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.7475842833518982, loss=0.9399772882461548
I0219 08:40:28.064681 140483498055424 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.6884370446205139, loss=0.9066171050071716
I0219 08:41:48.490167 140468535932672 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.6341783404350281, loss=0.906829833984375
I0219 08:43:09.582494 140483498055424 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.7769688367843628, loss=0.9351781606674194
I0219 08:44:30.721510 140468535932672 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.8089544177055359, loss=0.9301397204399109
I0219 08:45:51.298874 140483498055424 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.6088717579841614, loss=0.8858983516693115
I0219 08:46:10.676823 140549388556096 spec.py:321] Evaluating on the training split.
I0219 08:47:02.470168 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 08:47:53.446470 140549388556096 spec.py:349] Evaluating on the test split.
I0219 08:48:19.506191 140549388556096 submission_runner.py:408] Time since start: 50419.62s, 	Step: 59827, 	{'train/ctc_loss': Array(0.09002703, dtype=float32), 'train/wer': 0.03508040849865007, 'validation/ctc_loss': Array(0.3356909, dtype=float32), 'validation/wer': 0.0955038280699383, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1787121, dtype=float32), 'test/wer': 0.057441147198017586, 'test/num_examples': 2472, 'score': 46125.16544651985, 'total_duration': 50419.62053847313, 'accumulated_submission_time': 46125.16544651985, 'accumulated_eval_time': 4290.192884206772, 'accumulated_logging_time': 1.7623403072357178}
I0219 08:48:19.551076 140483498055424 logging_writer.py:48] [59827] accumulated_eval_time=4290.192884, accumulated_logging_time=1.762340, accumulated_submission_time=46125.165447, global_step=59827, preemption_count=0, score=46125.165447, test/ctc_loss=0.17871209979057312, test/num_examples=2472, test/wer=0.057441, total_duration=50419.620538, train/ctc_loss=0.09002703428268433, train/wer=0.035080, validation/ctc_loss=0.3356908857822418, validation/num_examples=5348, validation/wer=0.095504
I0219 08:49:15.285244 140468535932672 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.7393156290054321, loss=0.907978892326355
I0219 08:50:30.914154 140483498055424 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.6480785012245178, loss=0.8697024583816528
I0219 08:51:46.655969 140468535932672 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.3604588508605957, loss=0.9016868472099304
I0219 08:53:02.313836 140483498055424 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.8032457232475281, loss=0.9176822900772095
I0219 08:54:18.268197 140468535932672 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.8882415294647217, loss=0.9535337090492249
I0219 08:55:34.006922 140483498055424 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.0416587591171265, loss=0.9316408038139343
I0219 08:56:49.708646 140468535932672 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.7783153057098389, loss=0.9185786247253418
I0219 08:58:05.366219 140483498055424 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.9722962975502014, loss=0.9399656653404236
I0219 08:59:23.146614 140468535932672 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.6592346429824829, loss=0.9381850361824036
I0219 09:00:45.454572 140483498055424 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.8856174945831299, loss=0.9265459775924683
I0219 09:02:01.043150 140468535932672 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.8734527230262756, loss=0.9230993390083313
I0219 09:03:16.639874 140483498055424 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.8045925498008728, loss=0.8549333810806274
I0219 09:04:32.325916 140468535932672 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.8443982005119324, loss=0.9579933285713196
I0219 09:05:47.968639 140483498055424 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.74058997631073, loss=0.9055708646774292
I0219 09:07:03.529581 140468535932672 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.9378047585487366, loss=0.8976289629936218
I0219 09:08:19.489721 140483498055424 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.6834176778793335, loss=0.8861334919929504
I0219 09:09:38.118767 140468535932672 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.640920102596283, loss=0.8897935748100281
I0219 09:10:59.528060 140483498055424 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.7256481647491455, loss=0.8862168788909912
I0219 09:12:20.526226 140468535932672 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.9275851249694824, loss=0.8472260236740112
I0219 09:12:20.535807 140549388556096 spec.py:321] Evaluating on the training split.
I0219 09:13:13.555183 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 09:14:04.539746 140549388556096 spec.py:349] Evaluating on the test split.
I0219 09:14:30.373575 140549388556096 submission_runner.py:408] Time since start: 51990.49s, 	Step: 61701, 	{'train/ctc_loss': Array(0.08025563, dtype=float32), 'train/wer': 0.03238386369894189, 'validation/ctc_loss': Array(0.33256233, dtype=float32), 'validation/wer': 0.09461559998841441, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17447585, dtype=float32), 'test/wer': 0.056994292446123536, 'test/num_examples': 2472, 'score': 47566.06129670143, 'total_duration': 51990.48733663559, 'accumulated_submission_time': 47566.06129670143, 'accumulated_eval_time': 4420.0238037109375, 'accumulated_logging_time': 1.8231210708618164}
I0219 09:14:30.417226 140483498055424 logging_writer.py:48] [61701] accumulated_eval_time=4420.023804, accumulated_logging_time=1.823121, accumulated_submission_time=47566.061297, global_step=61701, preemption_count=0, score=47566.061297, test/ctc_loss=0.17447584867477417, test/num_examples=2472, test/wer=0.056994, total_duration=51990.487337, train/ctc_loss=0.08025562763214111, train/wer=0.032384, validation/ctc_loss=0.33256232738494873, validation/num_examples=5348, validation/wer=0.094616
I0219 09:15:48.923576 140483498055424 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.756742000579834, loss=0.8601365089416504
I0219 09:17:04.400255 140468535932672 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.7452801465988159, loss=0.924601674079895
I0219 09:18:19.925170 140483498055424 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.137120008468628, loss=0.8980111479759216
I0219 09:19:35.505106 140468535932672 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.8309042453765869, loss=0.919942319393158
I0219 09:20:51.115528 140483498055424 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.72745281457901, loss=0.9184566140174866
I0219 09:22:06.735127 140468535932672 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.769615113735199, loss=0.8960070610046387
I0219 09:23:22.378391 140483498055424 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.7900975942611694, loss=0.9276614785194397
I0219 09:24:42.910451 140468535932672 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.8394942283630371, loss=0.8954564332962036
I0219 09:26:04.255371 140483498055424 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.9623862504959106, loss=0.9512217044830322
I0219 09:27:24.517885 140468535932672 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.8870665431022644, loss=0.8906397819519043
I0219 09:28:45.552566 140483498055424 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.7883402705192566, loss=0.9283390045166016
I0219 09:30:05.764790 140483498055424 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.8856474757194519, loss=0.8851807713508606
I0219 09:31:21.370400 140468535932672 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.7560939192771912, loss=0.8526318073272705
I0219 09:32:37.008585 140483498055424 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.8547084331512451, loss=0.9241284132003784
I0219 09:33:52.570788 140468535932672 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.6625429391860962, loss=0.8831635117530823
I0219 09:35:08.162413 140483498055424 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.6656278967857361, loss=0.8467200994491577
I0219 09:36:23.823445 140468535932672 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.0362908840179443, loss=0.9117967486381531
I0219 09:37:42.411451 140483498055424 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.7862765192985535, loss=0.9066779613494873
I0219 09:38:31.578933 140549388556096 spec.py:321] Evaluating on the training split.
I0219 09:39:25.687695 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 09:40:16.084488 140549388556096 spec.py:349] Evaluating on the test split.
I0219 09:40:41.341666 140549388556096 submission_runner.py:408] Time since start: 53561.46s, 	Step: 63562, 	{'train/ctc_loss': Array(0.06656477, dtype=float32), 'train/wer': 0.026900180055476553, 'validation/ctc_loss': Array(0.3317119, dtype=float32), 'validation/wer': 0.09362117072323006, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17370649, dtype=float32), 'test/wer': 0.05662868401275567, 'test/num_examples': 2472, 'score': 49007.134853601456, 'total_duration': 53561.45655012131, 'accumulated_submission_time': 49007.134853601456, 'accumulated_eval_time': 4549.780838727951, 'accumulated_logging_time': 1.8821280002593994}
I0219 09:40:41.387924 140483498055424 logging_writer.py:48] [63562] accumulated_eval_time=4549.780839, accumulated_logging_time=1.882128, accumulated_submission_time=49007.134854, global_step=63562, preemption_count=0, score=49007.134854, test/ctc_loss=0.17370648682117462, test/num_examples=2472, test/wer=0.056629, total_duration=53561.456550, train/ctc_loss=0.06656476855278015, train/wer=0.026900, validation/ctc_loss=0.33171188831329346, validation/num_examples=5348, validation/wer=0.093621
I0219 09:41:10.833139 140468535932672 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.9143106937408447, loss=0.9042723774909973
I0219 09:42:26.258482 140483498055424 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.310275912284851, loss=0.905529797077179
I0219 09:43:41.759374 140468535932672 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.6937777400016785, loss=0.9124351143836975
I0219 09:45:00.637513 140483498055424 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.8190339803695679, loss=0.9338188171386719
I0219 09:46:16.373292 140468535932672 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.7649641633033752, loss=0.8902954459190369
I0219 09:47:32.101785 140483498055424 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.717210054397583, loss=0.8614500761032104
I0219 09:48:47.780687 140468535932672 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.2248860597610474, loss=0.8518570065498352
I0219 09:50:03.408675 140483498055424 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.971299946308136, loss=0.8893967866897583
I0219 09:51:19.082072 140468535932672 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.679038941860199, loss=0.913845419883728
I0219 09:52:34.819540 140483498055424 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.7831605076789856, loss=0.9113000631332397
I0219 09:53:54.767854 140468535932672 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.9659938216209412, loss=0.9030794501304626
I0219 09:55:17.545135 140483498055424 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.9526525735855103, loss=0.8897039890289307
I0219 09:56:38.899651 140468535932672 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.8265997171401978, loss=0.9035446643829346
I0219 09:58:02.061062 140483498055424 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.7100470066070557, loss=0.8417490720748901
I0219 09:59:17.608830 140468535932672 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.7117916941642761, loss=0.8508694767951965
I0219 10:00:33.192958 140483498055424 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.8171792030334473, loss=0.8904141783714294
I0219 10:01:48.761019 140468535932672 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.7201123237609863, loss=0.853238582611084
I0219 10:03:04.291425 140483498055424 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.951190710067749, loss=0.9472042322158813
I0219 10:04:19.791775 140468535932672 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.7505003213882446, loss=0.8943243622779846
I0219 10:04:41.393841 140549388556096 spec.py:321] Evaluating on the training split.
I0219 10:05:34.587777 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 10:06:25.607225 140549388556096 spec.py:349] Evaluating on the test split.
I0219 10:06:50.726707 140549388556096 submission_runner.py:408] Time since start: 55130.84s, 	Step: 65430, 	{'train/ctc_loss': Array(0.07390667, dtype=float32), 'train/wer': 0.029956807224973278, 'validation/ctc_loss': Array(0.32825038, dtype=float32), 'validation/wer': 0.09135232725412012, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1742499, dtype=float32), 'test/wer': 0.055592793451546725, 'test/num_examples': 2472, 'score': 50447.04723358154, 'total_duration': 55130.839566230774, 'accumulated_submission_time': 50447.04723358154, 'accumulated_eval_time': 4679.105979681015, 'accumulated_logging_time': 1.9498560428619385}
I0219 10:06:50.769675 140483498055424 logging_writer.py:48] [65430] accumulated_eval_time=4679.105980, accumulated_logging_time=1.949856, accumulated_submission_time=50447.047234, global_step=65430, preemption_count=0, score=50447.047234, test/ctc_loss=0.17424990236759186, test/num_examples=2472, test/wer=0.055593, total_duration=55130.839566, train/ctc_loss=0.07390667498111725, train/wer=0.029957, validation/ctc_loss=0.32825037837028503, validation/num_examples=5348, validation/wer=0.091352
I0219 10:07:44.195664 140468535932672 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.9966265559196472, loss=0.9110793471336365
I0219 10:08:59.623878 140483498055424 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.7647304534912109, loss=0.8813818097114563
I0219 10:10:15.145780 140468535932672 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.8231770396232605, loss=0.9186999797821045
I0219 10:11:30.895937 140483498055424 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.6716387867927551, loss=0.8530471324920654
I0219 10:12:46.423444 140468535932672 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.7872169017791748, loss=0.8951141238212585
I0219 10:14:05.217959 140483498055424 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.9808316230773926, loss=0.8701252341270447
I0219 10:15:20.856877 140468535932672 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.8000357747077942, loss=0.8672931790351868
I0219 10:16:36.506898 140483498055424 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.7868109345436096, loss=0.8720887303352356
I0219 10:17:52.045396 140468535932672 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.7013726234436035, loss=0.9195756316184998
I0219 10:19:07.666968 140483498055424 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.6400108933448792, loss=0.8320018649101257
I0219 10:20:23.234659 140468535932672 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.8658080697059631, loss=0.8925620317459106
I0219 10:21:40.390693 140483498055424 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.8211081027984619, loss=0.8557697534561157
I0219 10:23:01.313711 140468535932672 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.8574061989784241, loss=0.8842799067497253
I0219 10:24:22.690374 140483498055424 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.934295654296875, loss=0.8491096496582031
I0219 10:25:45.050140 140468535932672 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.6910435557365417, loss=0.8903616666793823
I0219 10:27:07.191863 140483498055424 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.7774249315261841, loss=0.8987876176834106
I0219 10:28:22.694313 140468535932672 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.9980822801589966, loss=0.8970909118652344
I0219 10:29:38.225019 140483498055424 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.7874944806098938, loss=0.8677446842193604
I0219 10:30:51.074809 140549388556096 spec.py:321] Evaluating on the training split.
I0219 10:31:45.834885 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 10:32:36.086802 140549388556096 spec.py:349] Evaluating on the test split.
I0219 10:33:01.840541 140549388556096 submission_runner.py:408] Time since start: 56701.96s, 	Step: 67298, 	{'train/ctc_loss': Array(0.06367131, dtype=float32), 'train/wer': 0.024762057533953587, 'validation/ctc_loss': Array(0.326362, dtype=float32), 'validation/wer': 0.09152611100920088, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17171825, dtype=float32), 'test/wer': 0.054353787094022304, 'test/num_examples': 2472, 'score': 51887.26133728027, 'total_duration': 56701.95521783829, 'accumulated_submission_time': 51887.26133728027, 'accumulated_eval_time': 4809.865817785263, 'accumulated_logging_time': 2.0128204822540283}
I0219 10:33:01.881330 140483498055424 logging_writer.py:48] [67298] accumulated_eval_time=4809.865818, accumulated_logging_time=2.012820, accumulated_submission_time=51887.261337, global_step=67298, preemption_count=0, score=51887.261337, test/ctc_loss=0.17171825468540192, test/num_examples=2472, test/wer=0.054354, total_duration=56701.955218, train/ctc_loss=0.0636713057756424, train/wer=0.024762, validation/ctc_loss=0.3263620138168335, validation/num_examples=5348, validation/wer=0.091526
I0219 10:33:04.269557 140468535932672 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.0310077667236328, loss=0.8458480834960938
I0219 10:34:19.676165 140483498055424 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.7639495134353638, loss=0.8696638941764832
I0219 10:35:35.296676 140468535932672 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.8844615817070007, loss=0.9167775511741638
I0219 10:36:50.973537 140483498055424 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.7115944623947144, loss=0.8592934608459473
I0219 10:38:06.563737 140468535932672 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.7934694886207581, loss=0.8600894808769226
I0219 10:39:22.184360 140483498055424 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.103933334350586, loss=0.8738859295845032
I0219 10:40:38.079617 140468535932672 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.0119335651397705, loss=0.8606805801391602
I0219 10:42:00.751497 140483498055424 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.7717711925506592, loss=0.8706276416778564
I0219 10:43:16.533576 140468535932672 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.7965117692947388, loss=0.8971051573753357
I0219 10:44:32.140411 140483498055424 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.8678168058395386, loss=0.9254857301712036
I0219 10:45:47.678037 140468535932672 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.8984098434448242, loss=0.9029121398925781
I0219 10:47:03.329355 140483498055424 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.8144197463989258, loss=0.8351809978485107
I0219 10:48:19.015827 140468535932672 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.9185640215873718, loss=0.8611850142478943
I0219 10:49:34.699109 140483498055424 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.9401257634162903, loss=0.8798534870147705
I0219 10:50:51.013151 140468535932672 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.9544031620025635, loss=0.8825994729995728
I0219 10:52:12.385123 140483498055424 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.8352147936820984, loss=0.8803528547286987
I0219 10:53:33.321694 140468535932672 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.9276267290115356, loss=0.8930402994155884
I0219 10:54:54.097147 140483498055424 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.7380892634391785, loss=0.861243486404419
I0219 10:56:13.186283 140483498055424 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.0491887331008911, loss=0.8638700842857361
I0219 10:57:02.149495 140549388556096 spec.py:321] Evaluating on the training split.
I0219 10:57:55.432907 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 10:58:45.897835 140549388556096 spec.py:349] Evaluating on the test split.
I0219 10:59:11.456935 140549388556096 submission_runner.py:408] Time since start: 58271.57s, 	Step: 69166, 	{'train/ctc_loss': Array(0.062219, dtype=float32), 'train/wer': 0.024897712352425213, 'validation/ctc_loss': Array(0.32358345, dtype=float32), 'validation/wer': 0.08950828851965205, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1701673, dtype=float32), 'test/wer': 0.054313163934759205, 'test/num_examples': 2472, 'score': 53327.44131612778, 'total_duration': 58271.57095623016, 'accumulated_submission_time': 53327.44131612778, 'accumulated_eval_time': 4939.1666939258575, 'accumulated_logging_time': 2.068631172180176}
I0219 10:59:11.499542 140483498055424 logging_writer.py:48] [69166] accumulated_eval_time=4939.166694, accumulated_logging_time=2.068631, accumulated_submission_time=53327.441316, global_step=69166, preemption_count=0, score=53327.441316, test/ctc_loss=0.17016729712486267, test/num_examples=2472, test/wer=0.054313, total_duration=58271.570956, train/ctc_loss=0.06221899762749672, train/wer=0.024898, validation/ctc_loss=0.3235834538936615, validation/num_examples=5348, validation/wer=0.089508
I0219 10:59:37.933058 140468535932672 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.9888999462127686, loss=0.8633691668510437
I0219 11:00:53.523018 140483498055424 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.69144606590271, loss=0.842113733291626
I0219 11:02:09.295986 140468535932672 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.7321113348007202, loss=0.8509950041770935
I0219 11:03:24.845270 140483498055424 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.9031828045845032, loss=0.8973962664604187
I0219 11:04:40.510344 140468535932672 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.1613174676895142, loss=0.8425213098526001
I0219 11:05:56.206878 140483498055424 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.825948178768158, loss=0.8576675057411194
I0219 11:07:11.888973 140468535932672 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.8990792632102966, loss=0.8359072208404541
I0219 11:08:27.710309 140483498055424 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.649133563041687, loss=0.8538317680358887
I0219 11:09:49.379032 140468535932672 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.7594521641731262, loss=0.8539878726005554
I0219 11:11:10.354104 140483498055424 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.145410418510437, loss=0.8791508078575134
I0219 11:12:26.001906 140468535932672 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.7590523958206177, loss=0.8648278117179871
I0219 11:13:41.765263 140483498055424 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.079826831817627, loss=0.8327545523643494
I0219 11:14:57.485832 140468535932672 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.8449806571006775, loss=0.8402107357978821
I0219 11:16:13.162085 140483498055424 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.7731345891952515, loss=0.8188000917434692
I0219 11:17:28.879735 140468535932672 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.8687009811401367, loss=0.8350317478179932
I0219 11:18:44.452445 140483498055424 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.7857221364974976, loss=0.8166436553001404
I0219 11:20:03.538051 140468535932672 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.89743572473526, loss=0.8585841655731201
I0219 11:21:24.606418 140483498055424 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.8616654872894287, loss=0.9034499526023865
I0219 11:22:45.326989 140468535932672 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.7688802480697632, loss=0.8641442060470581
I0219 11:23:11.736137 140549388556096 spec.py:321] Evaluating on the training split.
I0219 11:24:05.690017 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 11:24:56.000284 140549388556096 spec.py:349] Evaluating on the test split.
I0219 11:25:21.503569 140549388556096 submission_runner.py:408] Time since start: 59841.62s, 	Step: 71034, 	{'train/ctc_loss': Array(0.05749372, dtype=float32), 'train/wer': 0.02272054551234967, 'validation/ctc_loss': Array(0.32213494, dtype=float32), 'validation/wer': 0.0897496548461531, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17008592, dtype=float32), 'test/wer': 0.053703816545812764, 'test/num_examples': 2472, 'score': 54767.586223602295, 'total_duration': 59841.618315935135, 'accumulated_submission_time': 54767.586223602295, 'accumulated_eval_time': 5068.928290843964, 'accumulated_logging_time': 2.1295037269592285}
I0219 11:25:21.545173 140483498055424 logging_writer.py:48] [71034] accumulated_eval_time=5068.928291, accumulated_logging_time=2.129504, accumulated_submission_time=54767.586224, global_step=71034, preemption_count=0, score=54767.586224, test/ctc_loss=0.17008592188358307, test/num_examples=2472, test/wer=0.053704, total_duration=59841.618316, train/ctc_loss=0.05749372020363808, train/wer=0.022721, validation/ctc_loss=0.32213494181632996, validation/num_examples=5348, validation/wer=0.089750
I0219 11:26:15.363099 140483498055424 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.7635247707366943, loss=0.7980517148971558
I0219 11:27:30.848253 140468535932672 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.9673037528991699, loss=0.8711618781089783
I0219 11:28:46.385658 140483498055424 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.9990532398223877, loss=0.852938711643219
I0219 11:30:02.368086 140468535932672 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.6534630656242371, loss=0.837361752986908
I0219 11:31:17.917553 140483498055424 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.8430430889129639, loss=0.8818872570991516
I0219 11:32:33.600919 140468535932672 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.8576096892356873, loss=0.8678568601608276
I0219 11:33:52.753504 140483498055424 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.8246470093727112, loss=0.8234821557998657
I0219 11:35:14.108880 140468535932672 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.011304497718811, loss=0.8411730527877808
I0219 11:36:35.520733 140483498055424 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.8687710762023926, loss=0.8664270043373108
I0219 11:37:56.674676 140468535932672 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.7922708988189697, loss=0.8976156115531921
I0219 11:39:21.728700 140483498055424 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.8735290169715881, loss=0.7961631417274475
I0219 11:40:37.340931 140468535932672 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.7610035538673401, loss=0.8524426221847534
I0219 11:41:52.861456 140483498055424 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.935896635055542, loss=0.8303087949752808
I0219 11:43:08.490879 140468535932672 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.696564793586731, loss=0.8593397736549377
I0219 11:44:24.345745 140483498055424 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.2256784439086914, loss=0.8392099738121033
I0219 11:45:39.922107 140468535932672 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.9183141589164734, loss=0.8417373895645142
I0219 11:46:55.469578 140483498055424 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.927233874797821, loss=0.8200503587722778
I0219 11:48:14.066629 140468535932672 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.837963342666626, loss=0.8665874004364014
I0219 11:49:22.124585 140549388556096 spec.py:321] Evaluating on the training split.
I0219 11:50:14.749577 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 11:51:05.512624 140549388556096 spec.py:349] Evaluating on the test split.
I0219 11:51:31.117242 140549388556096 submission_runner.py:408] Time since start: 61411.23s, 	Step: 72886, 	{'train/ctc_loss': Array(0.06182419, dtype=float32), 'train/wer': 0.024071894111229147, 'validation/ctc_loss': Array(0.3198358, dtype=float32), 'validation/wer': 0.08821456500960638, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16816369, dtype=float32), 'test/wer': 0.05376475128470741, 'test/num_examples': 2472, 'score': 56208.07806992531, 'total_duration': 61411.231580495834, 'accumulated_submission_time': 56208.07806992531, 'accumulated_eval_time': 5197.914718389511, 'accumulated_logging_time': 2.186281204223633}
I0219 11:51:31.163788 140483498055424 logging_writer.py:48] [72886] accumulated_eval_time=5197.914718, accumulated_logging_time=2.186281, accumulated_submission_time=56208.078070, global_step=72886, preemption_count=0, score=56208.078070, test/ctc_loss=0.16816368699073792, test/num_examples=2472, test/wer=0.053765, total_duration=61411.231580, train/ctc_loss=0.061824191361665726, train/wer=0.024072, validation/ctc_loss=0.3198358118534088, validation/num_examples=5348, validation/wer=0.088215
I0219 11:51:42.506252 140468535932672 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.9004155397415161, loss=0.856415867805481
I0219 11:52:57.819016 140483498055424 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.5087553262710571, loss=0.8634387850761414
I0219 11:54:13.410867 140468535932672 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.7867093682289124, loss=0.8359456658363342
I0219 11:55:32.467885 140483498055424 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.8427748084068298, loss=0.8331930041313171
I0219 11:56:48.143432 140468535932672 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.6972904205322266, loss=0.8421100378036499
I0219 11:58:03.887564 140483498055424 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.7425252199172974, loss=0.872786819934845
I0219 11:59:19.691126 140468535932672 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.7142572402954102, loss=0.8596168756484985
I0219 12:00:35.485061 140483498055424 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.9232825040817261, loss=0.8360400795936584
I0219 12:01:51.503797 140468535932672 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.7323625683784485, loss=0.8272534012794495
I0219 12:03:08.051781 140483498055424 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.7286607027053833, loss=0.8416147828102112
I0219 12:04:28.994379 140468535932672 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.6754488348960876, loss=0.8596276640892029
I0219 12:05:50.569254 140483498055424 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.7593080401420593, loss=0.8154705166816711
I0219 12:07:11.101513 140468535932672 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.7249929904937744, loss=0.872361958026886
I0219 12:08:33.752945 140483498055424 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.8997166752815247, loss=0.8366978764533997
I0219 12:09:49.405970 140468535932672 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.8814333081245422, loss=0.8390634655952454
I0219 12:11:04.942995 140483498055424 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.9756923317909241, loss=0.8107295632362366
I0219 12:12:20.514249 140468535932672 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.7673435211181641, loss=0.8279705047607422
I0219 12:13:36.152293 140483498055424 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.7253273129463196, loss=0.8190277218818665
I0219 12:14:51.748023 140468535932672 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.2582170963287354, loss=0.868756115436554
I0219 12:15:31.789244 140549388556096 spec.py:321] Evaluating on the training split.
I0219 12:16:25.209258 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 12:17:15.357803 140549388556096 spec.py:349] Evaluating on the test split.
I0219 12:17:40.875790 140549388556096 submission_runner.py:408] Time since start: 62980.99s, 	Step: 74754, 	{'train/ctc_loss': Array(0.06552874, dtype=float32), 'train/wer': 0.02504612318860415, 'validation/ctc_loss': Array(0.31876612, dtype=float32), 'validation/wer': 0.08809870917288587, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16721477, dtype=float32), 'test/wer': 0.05232262913086751, 'test/num_examples': 2472, 'score': 57648.61340594292, 'total_duration': 62980.98971796036, 'accumulated_submission_time': 57648.61340594292, 'accumulated_eval_time': 5326.994605541229, 'accumulated_logging_time': 2.25032901763916}
I0219 12:17:40.920533 140483498055424 logging_writer.py:48] [74754] accumulated_eval_time=5326.994606, accumulated_logging_time=2.250329, accumulated_submission_time=57648.613406, global_step=74754, preemption_count=0, score=57648.613406, test/ctc_loss=0.1672147661447525, test/num_examples=2472, test/wer=0.052323, total_duration=62980.989718, train/ctc_loss=0.0655287355184555, train/wer=0.025046, validation/ctc_loss=0.31876611709594727, validation/num_examples=5348, validation/wer=0.088099
I0219 12:18:16.389471 140468535932672 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.1291815042495728, loss=0.8642853498458862
I0219 12:19:31.939170 140483498055424 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.8666520714759827, loss=0.8222008943557739
I0219 12:20:47.591465 140468535932672 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.7757140398025513, loss=0.8589307069778442
I0219 12:22:03.285912 140483498055424 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.9449467658996582, loss=0.8016506433486938
I0219 12:23:22.168375 140483498055424 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.1447653770446777, loss=0.8446274995803833
I0219 12:24:37.734133 140468535932672 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.9580909609794617, loss=0.8041318655014038
I0219 12:25:53.401342 140483498055424 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.939525842666626, loss=0.8089760541915894
I0219 12:27:08.965986 140468535932672 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.0038750171661377, loss=0.8552508354187012
I0219 12:28:24.604433 140483498055424 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.320369005203247, loss=0.8238394856452942
I0219 12:29:40.222169 140468535932672 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.914821207523346, loss=0.8061432242393494
I0219 12:30:57.073350 140483498055424 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.7789371609687805, loss=0.8286886215209961
I0219 12:32:18.060195 140468535932672 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.9549230933189392, loss=0.8771517872810364
I0219 12:33:39.393943 140483498055424 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.9008089900016785, loss=0.8723921775817871
I0219 12:35:01.165058 140468535932672 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.1373759508132935, loss=0.887562096118927
I0219 12:36:21.926566 140483498055424 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.3140599727630615, loss=0.8485483527183533
I0219 12:37:41.567088 140483498055424 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.7370189428329468, loss=0.8049978613853455
I0219 12:38:57.115874 140468535932672 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.8705506920814514, loss=0.8217105269432068
I0219 12:40:12.711213 140483498055424 logging_writer.py:48] [76500] global_step=76500, grad_norm=1.604681372642517, loss=0.8312239646911621
I0219 12:41:28.381099 140468535932672 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.7425134181976318, loss=0.8750540018081665
I0219 12:41:40.940983 140549388556096 spec.py:321] Evaluating on the training split.
I0219 12:42:34.228560 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 12:43:25.412501 140549388556096 spec.py:349] Evaluating on the test split.
I0219 12:43:50.770801 140549388556096 submission_runner.py:408] Time since start: 64550.89s, 	Step: 76618, 	{'train/ctc_loss': Array(0.05517682, dtype=float32), 'train/wer': 0.022161267122714882, 'validation/ctc_loss': Array(0.31730285, dtype=float32), 'validation/wer': 0.08726840900972224, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16727436, dtype=float32), 'test/wer': 0.05274917230313001, 'test/num_examples': 2472, 'score': 59088.544001579285, 'total_duration': 64550.88621211052, 'accumulated_submission_time': 59088.544001579285, 'accumulated_eval_time': 5456.8192529678345, 'accumulated_logging_time': 2.3125410079956055}
I0219 12:43:50.813206 140483498055424 logging_writer.py:48] [76618] accumulated_eval_time=5456.819253, accumulated_logging_time=2.312541, accumulated_submission_time=59088.544002, global_step=76618, preemption_count=0, score=59088.544002, test/ctc_loss=0.1672743558883667, test/num_examples=2472, test/wer=0.052749, total_duration=64550.886212, train/ctc_loss=0.05517682433128357, train/wer=0.022161, validation/ctc_loss=0.3173028528690338, validation/num_examples=5348, validation/wer=0.087268
I0219 12:44:53.242753 140468535932672 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.4755051136016846, loss=0.8625084757804871
I0219 12:46:08.780875 140483498055424 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.9090414643287659, loss=0.8334552049636841
I0219 12:47:24.412540 140468535932672 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.8384395241737366, loss=0.8403681516647339
I0219 12:48:40.421283 140483498055424 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.864021897315979, loss=0.8520224094390869
I0219 12:49:56.017989 140468535932672 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.8604058623313904, loss=0.8403785228729248
I0219 12:51:11.680962 140483498055424 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.9989398121833801, loss=0.8560781478881836
I0219 12:52:32.491814 140483498055424 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.9800114631652832, loss=0.8242131471633911
I0219 12:53:47.974277 140468535932672 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.757836639881134, loss=0.819945752620697
I0219 12:55:03.513376 140483498055424 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.8189811706542969, loss=0.8424785137176514
I0219 12:56:19.135339 140468535932672 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.9658822417259216, loss=0.8176765441894531
I0219 12:57:34.681651 140483498055424 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.7848970890045166, loss=0.8494663834571838
I0219 12:58:50.337121 140468535932672 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.9019635319709778, loss=0.8385225534439087
I0219 13:00:06.712030 140483498055424 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.7346612215042114, loss=0.8338818550109863
I0219 13:01:26.771532 140468535932672 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.9902219772338867, loss=0.8713487386703491
I0219 13:02:48.044617 140483498055424 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.7634766697883606, loss=0.8547526597976685
I0219 13:04:09.297242 140468535932672 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.8324568271636963, loss=0.8169766068458557
I0219 13:05:32.387718 140483498055424 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.8016517162322998, loss=0.8521074056625366
I0219 13:06:47.986784 140468535932672 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.8072596788406372, loss=0.7971828579902649
I0219 13:07:51.168224 140549388556096 spec.py:321] Evaluating on the training split.
I0219 13:08:42.739010 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 13:09:32.688068 140549388556096 spec.py:349] Evaluating on the test split.
I0219 13:09:58.271061 140549388556096 submission_runner.py:408] Time since start: 66118.39s, 	Step: 78485, 	{'train/ctc_loss': Array(0.05889492, dtype=float32), 'train/wer': 0.022904281303285867, 'validation/ctc_loss': Array(0.31750944, dtype=float32), 'validation/wer': 0.08736495554032266, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16668418, dtype=float32), 'test/wer': 0.052769483882761564, 'test/num_examples': 2472, 'score': 60528.807544231415, 'total_duration': 66118.38577461243, 'accumulated_submission_time': 60528.807544231415, 'accumulated_eval_time': 5583.916262388229, 'accumulated_logging_time': 2.374246597290039}
I0219 13:09:58.320473 140483498055424 logging_writer.py:48] [78485] accumulated_eval_time=5583.916262, accumulated_logging_time=2.374247, accumulated_submission_time=60528.807544, global_step=78485, preemption_count=0, score=60528.807544, test/ctc_loss=0.16668418049812317, test/num_examples=2472, test/wer=0.052769, total_duration=66118.385775, train/ctc_loss=0.058894917368888855, train/wer=0.022904, validation/ctc_loss=0.3175094425678253, validation/num_examples=5348, validation/wer=0.087365
I0219 13:10:10.445610 140468535932672 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.9674822688102722, loss=0.8166030645370483
I0219 13:11:25.852587 140483498055424 logging_writer.py:48] [78600] global_step=78600, grad_norm=1.034705638885498, loss=0.8430063128471375
I0219 13:12:41.371704 140468535932672 logging_writer.py:48] [78700] global_step=78700, grad_norm=1.1458981037139893, loss=0.8494377136230469
I0219 13:13:56.934743 140483498055424 logging_writer.py:48] [78800] global_step=78800, grad_norm=1.1072582006454468, loss=0.8325871825218201
I0219 13:15:12.580065 140468535932672 logging_writer.py:48] [78900] global_step=78900, grad_norm=1.2282017469406128, loss=0.8361513018608093
I0219 13:16:28.155825 140483498055424 logging_writer.py:48] [79000] global_step=79000, grad_norm=1.612215280532837, loss=0.8137289881706238
I0219 13:17:43.751703 140468535932672 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.9842281341552734, loss=0.812887966632843
I0219 13:18:57.927654 140483498055424 logging_writer.py:48] [79196] global_step=79196, preemption_count=0, score=61068.353629
I0219 13:18:58.840667 140549388556096 checkpoints.py:490] Saving checkpoint at step: 79196
I0219 13:19:00.429007 140549388556096 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_3/librispeech_conformer_jax/trial_2/checkpoint_79196
I0219 13:19:00.461522 140549388556096 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_3/librispeech_conformer_jax/trial_2/checkpoint_79196.
I0219 13:19:03.646865 140549388556096 submission_runner.py:583] Tuning trial 2/5
I0219 13:19:03.647109 140549388556096 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.2, learning_rate=0.0008445074561975979, one_minus_beta1=0.11042418465, beta2=0.9978504782314613, weight_decay=0.08135402759553023, warmup_factor=0.05)
I0219 13:19:03.670960 140549388556096 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.51566, dtype=float32), 'train/wer': 1.1540354719622474, 'validation/ctc_loss': Array(30.090126, dtype=float32), 'validation/wer': 0.9587360128213792, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.214182, dtype=float32), 'test/wer': 0.9757885970791949, 'test/num_examples': 2472, 'score': 35.07322812080383, 'total_duration': 163.3276309967041, 'accumulated_submission_time': 35.07322812080383, 'accumulated_eval_time': 128.25435638427734, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1836, {'train/ctc_loss': Array(5.6703877, dtype=float32), 'train/wer': 0.9337997724687145, 'validation/ctc_loss': Array(5.695042, dtype=float32), 'validation/wer': 0.8934609034824333, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.594893, dtype=float32), 'test/wer': 0.8955578575345805, 'test/num_examples': 2472, 'score': 1475.6158843040466, 'total_duration': 1711.4689011573792, 'accumulated_submission_time': 1475.6158843040466, 'accumulated_eval_time': 235.74946308135986, 'accumulated_logging_time': 0.030027151107788086, 'global_step': 1836, 'preemption_count': 0}), (3701, {'train/ctc_loss': Array(2.2839155, dtype=float32), 'train/wer': 0.5220066461751967, 'validation/ctc_loss': Array(2.2517276, dtype=float32), 'validation/wer': 0.5036446315301659, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.9130712, dtype=float32), 'test/wer': 0.46221030609550506, 'test/num_examples': 2472, 'score': 2916.513821363449, 'total_duration': 3282.3531198501587, 'accumulated_submission_time': 2916.513821363449, 'accumulated_eval_time': 365.60345458984375, 'accumulated_logging_time': 0.08441042900085449, 'global_step': 3701, 'preemption_count': 0}), (5571, {'train/ctc_loss': Array(0.7571463, dtype=float32), 'train/wer': 0.24622433208213734, 'validation/ctc_loss': Array(0.86724573, dtype=float32), 'validation/wer': 0.26055977678442127, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6027771, dtype=float32), 'test/wer': 0.20080027623748298, 'test/num_examples': 2472, 'score': 4356.575798273087, 'total_duration': 4851.800904512405, 'accumulated_submission_time': 4356.575798273087, 'accumulated_eval_time': 494.8561522960663, 'accumulated_logging_time': 0.13894009590148926, 'global_step': 5571, 'preemption_count': 0}), (7439, {'train/ctc_loss': Array(0.57816416, dtype=float32), 'train/wer': 0.19906977816456972, 'validation/ctc_loss': Array(0.7265921, dtype=float32), 'validation/wer': 0.22083087944234725, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4682187, dtype=float32), 'test/wer': 0.15782097373712753, 'test/num_examples': 2472, 'score': 5797.141031265259, 'total_duration': 6420.266736745834, 'accumulated_submission_time': 5797.141031265259, 'accumulated_eval_time': 622.6275777816772, 'accumulated_logging_time': 0.18962836265563965, 'global_step': 7439, 'preemption_count': 0}), (9302, {'train/ctc_loss': Array(0.4748012, dtype=float32), 'train/wer': 0.16590620132425546, 'validation/ctc_loss': Array(0.6415892, dtype=float32), 'validation/wer': 0.19483089875165335, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40672186, dtype=float32), 'test/wer': 0.13864684256494628, 'test/num_examples': 2472, 'score': 7237.32617020607, 'total_duration': 7989.78463101387, 'accumulated_submission_time': 7237.32617020607, 'accumulated_eval_time': 751.8287582397461, 'accumulated_logging_time': 0.24376177787780762, 'global_step': 9302, 'preemption_count': 0}), (11198, {'train/ctc_loss': Array(0.29416263, dtype=float32), 'train/wer': 0.10913853936405374, 'validation/ctc_loss': Array(0.600491, dtype=float32), 'validation/wer': 0.18280120103884068, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37524578, dtype=float32), 'test/wer': 0.1282270022139622, 'test/num_examples': 2472, 'score': 8677.753060102463, 'total_duration': 9585.371998786926, 'accumulated_submission_time': 8677.753060102463, 'accumulated_eval_time': 906.8597481250763, 'accumulated_logging_time': 0.29329824447631836, 'global_step': 11198, 'preemption_count': 0}), (13079, {'train/ctc_loss': Array(0.26120993, dtype=float32), 'train/wer': 0.09492812725657557, 'validation/ctc_loss': Array(0.5636484, dtype=float32), 'validation/wer': 0.1702308427546656, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34251323, dtype=float32), 'test/wer': 0.11575569232019174, 'test/num_examples': 2472, 'score': 10117.966343402863, 'total_duration': 11156.975125789642, 'accumulated_submission_time': 10117.966343402863, 'accumulated_eval_time': 1038.117871761322, 'accumulated_logging_time': 0.349048376083374, 'global_step': 13079, 'preemption_count': 0}), (14950, {'train/ctc_loss': Array(0.25069952, dtype=float32), 'train/wer': 0.09362826713179524, 'validation/ctc_loss': Array(0.53552085, dtype=float32), 'validation/wer': 0.16169612944958822, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31989244, dtype=float32), 'test/wer': 0.10931692157699104, 'test/num_examples': 2472, 'score': 11558.134120464325, 'total_duration': 12727.983196496964, 'accumulated_submission_time': 11558.134120464325, 'accumulated_eval_time': 1168.8186223506927, 'accumulated_logging_time': 0.4085848331451416, 'global_step': 14950, 'preemption_count': 0}), (16819, {'train/ctc_loss': Array(0.21822503, dtype=float32), 'train/wer': 0.08363072877535688, 'validation/ctc_loss': Array(0.5099662, dtype=float32), 'validation/wer': 0.1562991783890246, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30219615, dtype=float32), 'test/wer': 0.10397497613389393, 'test/num_examples': 2472, 'score': 12998.268486976624, 'total_duration': 14297.626657962799, 'accumulated_submission_time': 12998.268486976624, 'accumulated_eval_time': 1298.191771030426, 'accumulated_logging_time': 0.4646894931793213, 'global_step': 16819, 'preemption_count': 0}), (18672, {'train/ctc_loss': Array(0.21849884, dtype=float32), 'train/wer': 0.08307254319697607, 'validation/ctc_loss': Array(0.4908321, dtype=float32), 'validation/wer': 0.14991745271633664, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29380992, dtype=float32), 'test/wer': 0.09956736335384803, 'test/num_examples': 2472, 'score': 14438.530722856522, 'total_duration': 15868.071051120758, 'accumulated_submission_time': 14438.530722856522, 'accumulated_eval_time': 1428.2382934093475, 'accumulated_logging_time': 0.5230772495269775, 'global_step': 18672, 'preemption_count': 0}), (20532, {'train/ctc_loss': Array(0.20132376, dtype=float32), 'train/wer': 0.07777789650271939, 'validation/ctc_loss': Array(0.49051324, dtype=float32), 'validation/wer': 0.1492223176960136, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28147247, dtype=float32), 'test/wer': 0.0954034895293807, 'test/num_examples': 2472, 'score': 15879.123154878616, 'total_duration': 17440.032745838165, 'accumulated_submission_time': 15879.123154878616, 'accumulated_eval_time': 1559.4781639575958, 'accumulated_logging_time': 0.5743563175201416, 'global_step': 20532, 'preemption_count': 0}), (22391, {'train/ctc_loss': Array(0.2204811, dtype=float32), 'train/wer': 0.08096725040776032, 'validation/ctc_loss': Array(0.4734632, dtype=float32), 'validation/wer': 0.14523494598221612, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27528322, dtype=float32), 'test/wer': 0.09422541791075092, 'test/num_examples': 2472, 'score': 17319.44011616707, 'total_duration': 19011.014504671097, 'accumulated_submission_time': 17319.44011616707, 'accumulated_eval_time': 1690.0110762119293, 'accumulated_logging_time': 0.626962423324585, 'global_step': 22391, 'preemption_count': 0}), (24266, {'train/ctc_loss': Array(0.19056775, dtype=float32), 'train/wer': 0.07419423240033927, 'validation/ctc_loss': Array(0.45724428, dtype=float32), 'validation/wer': 0.13879529239116792, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26455674, dtype=float32), 'test/wer': 0.09042715251965146, 'test/num_examples': 2472, 'score': 18759.946305513382, 'total_duration': 20580.23752808571, 'accumulated_submission_time': 18759.946305513382, 'accumulated_eval_time': 1818.5990698337555, 'accumulated_logging_time': 0.6770737171173096, 'global_step': 24266, 'preemption_count': 0}), (26138, {'train/ctc_loss': Array(0.15853791, dtype=float32), 'train/wer': 0.06276950062122177, 'validation/ctc_loss': Array(0.4487414, dtype=float32), 'validation/wer': 0.13519410679977215, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25610334, dtype=float32), 'test/wer': 0.08640545975260495, 'test/num_examples': 2472, 'score': 20200.31648516655, 'total_duration': 22151.732254505157, 'accumulated_submission_time': 20200.31648516655, 'accumulated_eval_time': 1949.5915653705597, 'accumulated_logging_time': 0.7306003570556641, 'global_step': 26138, 'preemption_count': 0}), (28016, {'train/ctc_loss': Array(0.15658772, dtype=float32), 'train/wer': 0.061639522721936436, 'validation/ctc_loss': Array(0.43760598, dtype=float32), 'validation/wer': 0.13245218533072015, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24930707, dtype=float32), 'test/wer': 0.08565393130623769, 'test/num_examples': 2472, 'score': 21640.60538506508, 'total_duration': 23721.0798664093, 'accumulated_submission_time': 21640.60538506508, 'accumulated_eval_time': 2078.5168731212616, 'accumulated_logging_time': 0.785571813583374, 'global_step': 28016, 'preemption_count': 0}), (29887, {'train/ctc_loss': Array(0.1453827, dtype=float32), 'train/wer': 0.05890779836277467, 'validation/ctc_loss': Array(0.42563367, dtype=float32), 'validation/wer': 0.12759589484151887, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24524696, dtype=float32), 'test/wer': 0.08112444904840249, 'test/num_examples': 2472, 'score': 23080.832427740097, 'total_duration': 25291.391725063324, 'accumulated_submission_time': 23080.832427740097, 'accumulated_eval_time': 2208.4737632274628, 'accumulated_logging_time': 0.8361155986785889, 'global_step': 29887, 'preemption_count': 0}), (31777, {'train/ctc_loss': Array(0.15269236, dtype=float32), 'train/wer': 0.05871874967352354, 'validation/ctc_loss': Array(0.4212963, dtype=float32), 'validation/wer': 0.12711316218851676, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2405563, dtype=float32), 'test/wer': 0.08043385534092987, 'test/num_examples': 2472, 'score': 24521.21066737175, 'total_duration': 26862.657521247864, 'accumulated_submission_time': 24521.21066737175, 'accumulated_eval_time': 2339.2270991802216, 'accumulated_logging_time': 0.8912203311920166, 'global_step': 31777, 'preemption_count': 0}), (33656, {'train/ctc_loss': Array(0.14723137, dtype=float32), 'train/wer': 0.05695567709103195, 'validation/ctc_loss': Array(0.41512138, dtype=float32), 'validation/wer': 0.12382092549504234, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23121922, dtype=float32), 'test/wer': 0.0780777121036703, 'test/num_examples': 2472, 'score': 25961.102822065353, 'total_duration': 28433.16188645363, 'accumulated_submission_time': 25961.102822065353, 'accumulated_eval_time': 2469.7058634757996, 'accumulated_logging_time': 0.9466145038604736, 'global_step': 33656, 'preemption_count': 0}), (35533, {'train/ctc_loss': Array(0.13383973, dtype=float32), 'train/wer': 0.052879318460432326, 'validation/ctc_loss': Array(0.4006253, dtype=float32), 'validation/wer': 0.1209148749239696, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22429307, dtype=float32), 'test/wer': 0.07698088680356671, 'test/num_examples': 2472, 'score': 27401.28202843666, 'total_duration': 30004.2161693573, 'accumulated_submission_time': 27401.28202843666, 'accumulated_eval_time': 2600.440567970276, 'accumulated_logging_time': 1.0080838203430176, 'global_step': 35533, 'preemption_count': 0}), (37406, {'train/ctc_loss': Array(0.12914908, dtype=float32), 'train/wer': 0.04979800996588962, 'validation/ctc_loss': Array(0.39642468, dtype=float32), 'validation/wer': 0.11791227782229645, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21991388, dtype=float32), 'test/wer': 0.0740153961773607, 'test/num_examples': 2472, 'score': 28841.636016368866, 'total_duration': 31575.221014022827, 'accumulated_submission_time': 28841.636016368866, 'accumulated_eval_time': 2730.957435131073, 'accumulated_logging_time': 1.0638580322265625, 'global_step': 37406, 'preemption_count': 0}), (39263, {'train/ctc_loss': Array(0.11346611, dtype=float32), 'train/wer': 0.04597961733738948, 'validation/ctc_loss': Array(0.3963149, dtype=float32), 'validation/wer': 0.11630960541432944, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21920747, dtype=float32), 'test/wer': 0.07375134564215059, 'test/num_examples': 2472, 'score': 30282.072615385056, 'total_duration': 33146.08340334892, 'accumulated_submission_time': 30282.072615385056, 'accumulated_eval_time': 2861.2528777122498, 'accumulated_logging_time': 1.1172175407409668, 'global_step': 39263, 'preemption_count': 0}), (41131, {'train/ctc_loss': Array(0.11233612, dtype=float32), 'train/wer': 0.04404061671172253, 'validation/ctc_loss': Array(0.39019135, dtype=float32), 'validation/wer': 0.1171012869652529, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21722014, dtype=float32), 'test/wer': 0.07283732455873093, 'test/num_examples': 2472, 'score': 31722.417976379395, 'total_duration': 34717.45092463493, 'accumulated_submission_time': 31722.417976379395, 'accumulated_eval_time': 2992.140277147293, 'accumulated_logging_time': 1.1743803024291992, 'global_step': 41131, 'preemption_count': 0}), (42993, {'train/ctc_loss': Array(0.11740988, dtype=float32), 'train/wer': 0.046672798095650295, 'validation/ctc_loss': Array(0.38365835, dtype=float32), 'validation/wer': 0.11446556667986135, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20735815, dtype=float32), 'test/wer': 0.07086710133447079, 'test/num_examples': 2472, 'score': 33162.53774404526, 'total_duration': 36287.6553106308, 'accumulated_submission_time': 33162.53774404526, 'accumulated_eval_time': 3122.092783689499, 'accumulated_logging_time': 1.228879451751709, 'global_step': 42993, 'preemption_count': 0}), (44863, {'train/ctc_loss': Array(0.10558901, dtype=float32), 'train/wer': 0.041014103534872066, 'validation/ctc_loss': Array(0.378038, dtype=float32), 'validation/wer': 0.11151124284348841, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20476188, dtype=float32), 'test/wer': 0.06828753072126419, 'test/num_examples': 2472, 'score': 34603.10031223297, 'total_duration': 37858.130298137665, 'accumulated_submission_time': 34603.10031223297, 'accumulated_eval_time': 3251.867955684662, 'accumulated_logging_time': 1.2873446941375732, 'global_step': 44863, 'preemption_count': 0}), (46737, {'train/ctc_loss': Array(0.09857526, dtype=float32), 'train/wer': 0.04018159433671899, 'validation/ctc_loss': Array(0.37337637, dtype=float32), 'validation/wer': 0.10832520733367447, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20145229, dtype=float32), 'test/wer': 0.06751569069526538, 'test/num_examples': 2472, 'score': 36043.22028398514, 'total_duration': 39429.1965405941, 'accumulated_submission_time': 36043.22028398514, 'accumulated_eval_time': 3382.6824221611023, 'accumulated_logging_time': 1.341782808303833, 'global_step': 46737, 'preemption_count': 0}), (48609, {'train/ctc_loss': Array(0.11590645, dtype=float32), 'train/wer': 0.04348681621408894, 'validation/ctc_loss': Array(0.36853057, dtype=float32), 'validation/wer': 0.10704113847668884, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19999944, dtype=float32), 'test/wer': 0.06763756017305465, 'test/num_examples': 2472, 'score': 37483.42413520813, 'total_duration': 40999.42491674423, 'accumulated_submission_time': 37483.42413520813, 'accumulated_eval_time': 3512.5706765651703, 'accumulated_logging_time': 1.40153169631958, 'global_step': 48609, 'preemption_count': 0}), (50478, {'train/ctc_loss': Array(0.07502113, dtype=float32), 'train/wer': 0.030328978403101094, 'validation/ctc_loss': Array(0.35714394, dtype=float32), 'validation/wer': 0.10416405186479624, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19472961, dtype=float32), 'test/wer': 0.06446895375053317, 'test/num_examples': 2472, 'score': 38923.31358218193, 'total_duration': 42569.863436460495, 'accumulated_submission_time': 38923.31358218193, 'accumulated_eval_time': 3642.982671022415, 'accumulated_logging_time': 1.4595589637756348, 'global_step': 50478, 'preemption_count': 0}), (52367, {'train/ctc_loss': Array(0.08077259, dtype=float32), 'train/wer': 0.03261068446888706, 'validation/ctc_loss': Array(0.35393357, dtype=float32), 'validation/wer': 0.10222346659972774, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18940833, dtype=float32), 'test/wer': 0.06320963581337721, 'test/num_examples': 2472, 'score': 40363.50043487549, 'total_duration': 44140.527676820755, 'accumulated_submission_time': 40363.50043487549, 'accumulated_eval_time': 3773.3202497959137, 'accumulated_logging_time': 1.5202560424804688, 'global_step': 52367, 'preemption_count': 0}), (54234, {'train/ctc_loss': Array(0.09836104, dtype=float32), 'train/wer': 0.039445336030074524, 'validation/ctc_loss': Array(0.34822646, dtype=float32), 'validation/wer': 0.09976153006941696, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18704453, dtype=float32), 'test/wer': 0.06178782523916885, 'test/num_examples': 2472, 'score': 41803.85074186325, 'total_duration': 45710.626412153244, 'accumulated_submission_time': 41803.85074186325, 'accumulated_eval_time': 3902.9258086681366, 'accumulated_logging_time': 1.5857644081115723, 'global_step': 54234, 'preemption_count': 0}), (56103, {'train/ctc_loss': Array(0.09685396, dtype=float32), 'train/wer': 0.03788420696577002, 'validation/ctc_loss': Array(0.34267035, dtype=float32), 'validation/wer': 0.0985836623960918, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18341146, dtype=float32), 'test/wer': 0.06073162309832836, 'test/num_examples': 2472, 'score': 43244.468445539474, 'total_duration': 47280.65885734558, 'accumulated_submission_time': 43244.468445539474, 'accumulated_eval_time': 4032.204874753952, 'accumulated_logging_time': 1.641676664352417, 'global_step': 56103, 'preemption_count': 0}), (57971, {'train/ctc_loss': Array(0.10674078, dtype=float32), 'train/wer': 0.043809170224614814, 'validation/ctc_loss': Array(0.34073088, dtype=float32), 'validation/wer': 0.09737683076358651, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18483536, dtype=float32), 'test/wer': 0.06006134097048727, 'test/num_examples': 2472, 'score': 44684.62251138687, 'total_duration': 48850.121492147446, 'accumulated_submission_time': 44684.62251138687, 'accumulated_eval_time': 4161.369755983353, 'accumulated_logging_time': 1.7073440551757812, 'global_step': 57971, 'preemption_count': 0}), (59827, {'train/ctc_loss': Array(0.09002703, dtype=float32), 'train/wer': 0.03508040849865007, 'validation/ctc_loss': Array(0.3356909, dtype=float32), 'validation/wer': 0.0955038280699383, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1787121, dtype=float32), 'test/wer': 0.057441147198017586, 'test/num_examples': 2472, 'score': 46125.16544651985, 'total_duration': 50419.62053847313, 'accumulated_submission_time': 46125.16544651985, 'accumulated_eval_time': 4290.192884206772, 'accumulated_logging_time': 1.7623403072357178, 'global_step': 59827, 'preemption_count': 0}), (61701, {'train/ctc_loss': Array(0.08025563, dtype=float32), 'train/wer': 0.03238386369894189, 'validation/ctc_loss': Array(0.33256233, dtype=float32), 'validation/wer': 0.09461559998841441, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17447585, dtype=float32), 'test/wer': 0.056994292446123536, 'test/num_examples': 2472, 'score': 47566.06129670143, 'total_duration': 51990.48733663559, 'accumulated_submission_time': 47566.06129670143, 'accumulated_eval_time': 4420.0238037109375, 'accumulated_logging_time': 1.8231210708618164, 'global_step': 61701, 'preemption_count': 0}), (63562, {'train/ctc_loss': Array(0.06656477, dtype=float32), 'train/wer': 0.026900180055476553, 'validation/ctc_loss': Array(0.3317119, dtype=float32), 'validation/wer': 0.09362117072323006, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17370649, dtype=float32), 'test/wer': 0.05662868401275567, 'test/num_examples': 2472, 'score': 49007.134853601456, 'total_duration': 53561.45655012131, 'accumulated_submission_time': 49007.134853601456, 'accumulated_eval_time': 4549.780838727951, 'accumulated_logging_time': 1.8821280002593994, 'global_step': 63562, 'preemption_count': 0}), (65430, {'train/ctc_loss': Array(0.07390667, dtype=float32), 'train/wer': 0.029956807224973278, 'validation/ctc_loss': Array(0.32825038, dtype=float32), 'validation/wer': 0.09135232725412012, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1742499, dtype=float32), 'test/wer': 0.055592793451546725, 'test/num_examples': 2472, 'score': 50447.04723358154, 'total_duration': 55130.839566230774, 'accumulated_submission_time': 50447.04723358154, 'accumulated_eval_time': 4679.105979681015, 'accumulated_logging_time': 1.9498560428619385, 'global_step': 65430, 'preemption_count': 0}), (67298, {'train/ctc_loss': Array(0.06367131, dtype=float32), 'train/wer': 0.024762057533953587, 'validation/ctc_loss': Array(0.326362, dtype=float32), 'validation/wer': 0.09152611100920088, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17171825, dtype=float32), 'test/wer': 0.054353787094022304, 'test/num_examples': 2472, 'score': 51887.26133728027, 'total_duration': 56701.95521783829, 'accumulated_submission_time': 51887.26133728027, 'accumulated_eval_time': 4809.865817785263, 'accumulated_logging_time': 2.0128204822540283, 'global_step': 67298, 'preemption_count': 0}), (69166, {'train/ctc_loss': Array(0.062219, dtype=float32), 'train/wer': 0.024897712352425213, 'validation/ctc_loss': Array(0.32358345, dtype=float32), 'validation/wer': 0.08950828851965205, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1701673, dtype=float32), 'test/wer': 0.054313163934759205, 'test/num_examples': 2472, 'score': 53327.44131612778, 'total_duration': 58271.57095623016, 'accumulated_submission_time': 53327.44131612778, 'accumulated_eval_time': 4939.1666939258575, 'accumulated_logging_time': 2.068631172180176, 'global_step': 69166, 'preemption_count': 0}), (71034, {'train/ctc_loss': Array(0.05749372, dtype=float32), 'train/wer': 0.02272054551234967, 'validation/ctc_loss': Array(0.32213494, dtype=float32), 'validation/wer': 0.0897496548461531, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17008592, dtype=float32), 'test/wer': 0.053703816545812764, 'test/num_examples': 2472, 'score': 54767.586223602295, 'total_duration': 59841.618315935135, 'accumulated_submission_time': 54767.586223602295, 'accumulated_eval_time': 5068.928290843964, 'accumulated_logging_time': 2.1295037269592285, 'global_step': 71034, 'preemption_count': 0}), (72886, {'train/ctc_loss': Array(0.06182419, dtype=float32), 'train/wer': 0.024071894111229147, 'validation/ctc_loss': Array(0.3198358, dtype=float32), 'validation/wer': 0.08821456500960638, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16816369, dtype=float32), 'test/wer': 0.05376475128470741, 'test/num_examples': 2472, 'score': 56208.07806992531, 'total_duration': 61411.231580495834, 'accumulated_submission_time': 56208.07806992531, 'accumulated_eval_time': 5197.914718389511, 'accumulated_logging_time': 2.186281204223633, 'global_step': 72886, 'preemption_count': 0}), (74754, {'train/ctc_loss': Array(0.06552874, dtype=float32), 'train/wer': 0.02504612318860415, 'validation/ctc_loss': Array(0.31876612, dtype=float32), 'validation/wer': 0.08809870917288587, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16721477, dtype=float32), 'test/wer': 0.05232262913086751, 'test/num_examples': 2472, 'score': 57648.61340594292, 'total_duration': 62980.98971796036, 'accumulated_submission_time': 57648.61340594292, 'accumulated_eval_time': 5326.994605541229, 'accumulated_logging_time': 2.25032901763916, 'global_step': 74754, 'preemption_count': 0}), (76618, {'train/ctc_loss': Array(0.05517682, dtype=float32), 'train/wer': 0.022161267122714882, 'validation/ctc_loss': Array(0.31730285, dtype=float32), 'validation/wer': 0.08726840900972224, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16727436, dtype=float32), 'test/wer': 0.05274917230313001, 'test/num_examples': 2472, 'score': 59088.544001579285, 'total_duration': 64550.88621211052, 'accumulated_submission_time': 59088.544001579285, 'accumulated_eval_time': 5456.8192529678345, 'accumulated_logging_time': 2.3125410079956055, 'global_step': 76618, 'preemption_count': 0}), (78485, {'train/ctc_loss': Array(0.05889492, dtype=float32), 'train/wer': 0.022904281303285867, 'validation/ctc_loss': Array(0.31750944, dtype=float32), 'validation/wer': 0.08736495554032266, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16668418, dtype=float32), 'test/wer': 0.052769483882761564, 'test/num_examples': 2472, 'score': 60528.807544231415, 'total_duration': 66118.38577461243, 'accumulated_submission_time': 60528.807544231415, 'accumulated_eval_time': 5583.916262388229, 'accumulated_logging_time': 2.374246597290039, 'global_step': 78485, 'preemption_count': 0})], 'global_step': 79196}
I0219 13:19:03.671210 140549388556096 submission_runner.py:586] Timing: 61068.353628873825
I0219 13:19:03.671276 140549388556096 submission_runner.py:588] Total number of evals: 43
I0219 13:19:03.671336 140549388556096 submission_runner.py:589] ====================
I0219 13:19:03.671383 140549388556096 submission_runner.py:542] Using RNG seed 4110531300
I0219 13:19:03.673494 140549388556096 submission_runner.py:551] --- Tuning run 3/5 ---
I0219 13:19:03.673633 140549388556096 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_3/librispeech_conformer_jax/trial_3.
I0219 13:19:03.675729 140549388556096 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_3/librispeech_conformer_jax/trial_3/hparams.json.
I0219 13:19:03.676914 140549388556096 submission_runner.py:206] Initializing dataset.
I0219 13:19:03.677050 140549388556096 submission_runner.py:213] Initializing model.
I0219 13:19:07.144053 140549388556096 submission_runner.py:255] Initializing optimizer.
I0219 13:19:07.580142 140549388556096 submission_runner.py:262] Initializing metrics bundle.
I0219 13:19:07.580342 140549388556096 submission_runner.py:280] Initializing checkpoint and logger.
I0219 13:19:07.585243 140549388556096 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_3/librispeech_conformer_jax/trial_3 with prefix checkpoint_
I0219 13:19:07.585383 140549388556096 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_3/librispeech_conformer_jax/trial_3/meta_data_0.json.
I0219 13:19:07.585614 140549388556096 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0219 13:19:07.585688 140549388556096 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0219 13:19:08.150134 140549388556096 logger_utils.py:220] Unable to record git information. Continuing without it.
I0219 13:19:08.652745 140549388556096 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_3/librispeech_conformer_jax/trial_3/flags_0.json.
I0219 13:19:08.672522 140549388556096 submission_runner.py:314] Starting training loop.
I0219 13:19:08.675788 140549388556096 input_pipeline.py:20] Loading split = train-clean-100
I0219 13:19:08.721242 140549388556096 input_pipeline.py:20] Loading split = train-clean-360
I0219 13:19:09.217600 140549388556096 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0219 13:19:42.722295 140367735265024 logging_writer.py:48] [0] global_step=0, grad_norm=72.01214599609375, loss=32.303321838378906
I0219 13:19:42.742867 140549388556096 spec.py:321] Evaluating on the training split.
I0219 13:20:36.909807 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 13:21:26.448808 140549388556096 spec.py:349] Evaluating on the test split.
I0219 13:21:52.215282 140549388556096 submission_runner.py:408] Time since start: 163.54s, 	Step: 1, 	{'train/ctc_loss': Array(31.87367, dtype=float32), 'train/wer': 1.1416312330885194, 'validation/ctc_loss': Array(30.090126, dtype=float32), 'validation/wer': 0.9587456674744393, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.214182, dtype=float32), 'test/wer': 0.9757885970791949, 'test/num_examples': 2472, 'score': 34.07029867172241, 'total_duration': 163.53999519348145, 'accumulated_submission_time': 34.07029867172241, 'accumulated_eval_time': 129.469651222229, 'accumulated_logging_time': 0}
I0219 13:21:52.234222 140483498055424 logging_writer.py:48] [1] accumulated_eval_time=129.469651, accumulated_logging_time=0, accumulated_submission_time=34.070299, global_step=1, preemption_count=0, score=34.070299, test/ctc_loss=30.214181900024414, test/num_examples=2472, test/wer=0.975789, total_duration=163.539995, train/ctc_loss=31.87367057800293, train/wer=1.141631, validation/ctc_loss=30.090126037597656, validation/num_examples=5348, validation/wer=0.958746
I0219 13:23:32.875090 140378519443200 logging_writer.py:48] [100] global_step=100, grad_norm=9.809666633605957, loss=6.95017671585083
I0219 13:24:49.012729 140378536228608 logging_writer.py:48] [200] global_step=200, grad_norm=2.743786096572876, loss=6.12620735168457
I0219 13:26:05.165345 140378519443200 logging_writer.py:48] [300] global_step=300, grad_norm=0.5736270546913147, loss=5.840163707733154
I0219 13:27:21.170121 140378536228608 logging_writer.py:48] [400] global_step=400, grad_norm=0.4396912455558777, loss=5.846502304077148
I0219 13:28:37.299196 140378519443200 logging_writer.py:48] [500] global_step=500, grad_norm=0.29867804050445557, loss=5.81038761138916
I0219 13:29:53.292727 140378536228608 logging_writer.py:48] [600] global_step=600, grad_norm=0.31931325793266296, loss=5.79368782043457
I0219 13:31:09.450786 140378519443200 logging_writer.py:48] [700] global_step=700, grad_norm=0.28569281101226807, loss=5.816399097442627
I0219 13:32:25.552349 140378536228608 logging_writer.py:48] [800] global_step=800, grad_norm=0.9195380210876465, loss=5.795423984527588
I0219 13:33:46.338483 140378519443200 logging_writer.py:48] [900] global_step=900, grad_norm=0.3395422399044037, loss=5.785499095916748
I0219 13:35:08.557526 140378536228608 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.5988287329673767, loss=5.805814743041992
I0219 13:36:29.994214 140483498055424 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.48904016613960266, loss=5.7922563552856445
I0219 13:37:46.321408 140468535932672 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.3376001715660095, loss=5.783962249755859
I0219 13:39:02.576203 140483498055424 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.261164903640747, loss=5.7489542961120605
I0219 13:40:18.756754 140468535932672 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.7754452228546143, loss=5.62339973449707
I0219 13:41:35.087775 140483498055424 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.7870280742645264, loss=5.512491703033447
I0219 13:42:51.296167 140468535932672 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.1806167364120483, loss=5.429317474365234
I0219 13:44:09.754338 140483498055424 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.185824394226074, loss=5.139142036437988
I0219 13:45:31.486253 140468535932672 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.6435465216636658, loss=4.625363826751709
I0219 13:45:52.806262 140549388556096 spec.py:321] Evaluating on the training split.
I0219 13:46:31.446431 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 13:47:16.364488 140549388556096 spec.py:349] Evaluating on the test split.
I0219 13:47:39.363098 140549388556096 submission_runner.py:408] Time since start: 1710.69s, 	Step: 1828, 	{'train/ctc_loss': Array(5.8229604, dtype=float32), 'train/wer': 0.9391896477614642, 'validation/ctc_loss': Array(5.8899813, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.791992, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1474.5567479133606, 'total_duration': 1710.6854240894318, 'accumulated_submission_time': 1474.5567479133606, 'accumulated_eval_time': 236.02138757705688, 'accumulated_logging_time': 0.03240823745727539}
I0219 13:47:39.406792 140483498055424 logging_writer.py:48] [1828] accumulated_eval_time=236.021388, accumulated_logging_time=0.032408, accumulated_submission_time=1474.556748, global_step=1828, preemption_count=0, score=1474.556748, test/ctc_loss=5.7919921875, test/num_examples=2472, test/wer=0.899580, total_duration=1710.685424, train/ctc_loss=5.822960376739502, train/wer=0.939190, validation/ctc_loss=5.889981269836426, validation/num_examples=5348, validation/wer=0.896618
I0219 13:48:34.774903 140468535932672 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.8577592968940735, loss=4.149359226226807
I0219 13:49:50.917941 140483498055424 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.9398270845413208, loss=3.806675672531128
I0219 13:51:10.585867 140483498055424 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.3087928295135498, loss=3.5782277584075928
I0219 13:52:26.719451 140468535932672 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.048152208328247, loss=3.4404537677764893
I0219 13:53:42.898278 140483498055424 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.109189748764038, loss=3.2848081588745117
I0219 13:54:59.335861 140468535932672 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.012368083000183, loss=3.1541740894317627
I0219 13:56:15.422389 140483498055424 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.2067391872406006, loss=3.0947487354278564
I0219 13:57:31.423041 140468535932672 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.0866786241531372, loss=2.974601984024048
I0219 13:58:50.085767 140483498055424 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.5867801904678345, loss=2.8835055828094482
I0219 14:00:11.254772 140468535932672 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.0388190746307373, loss=2.8921124935150146
I0219 14:01:32.762435 140483498055424 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.1538296937942505, loss=2.7782599925994873
I0219 14:02:54.058182 140468535932672 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.1222498416900635, loss=2.7176833152770996
I0219 14:04:18.968973 140483498055424 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.4865154027938843, loss=2.7001357078552246
I0219 14:05:34.769156 140468535932672 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.1723493337631226, loss=2.6365153789520264
I0219 14:06:50.636520 140483498055424 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.3909896612167358, loss=2.627838134765625
I0219 14:08:06.517873 140468535932672 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.403603434562683, loss=2.4870049953460693
I0219 14:09:22.694365 140483498055424 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.2978460788726807, loss=2.535244941711426
I0219 14:10:38.506461 140468535932672 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.3758405447006226, loss=2.4741501808166504
I0219 14:11:39.530765 140549388556096 spec.py:321] Evaluating on the training split.
I0219 14:12:24.516580 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 14:13:12.685940 140549388556096 spec.py:349] Evaluating on the test split.
I0219 14:13:37.255285 140549388556096 submission_runner.py:408] Time since start: 3268.58s, 	Step: 3682, 	{'train/ctc_loss': Array(3.304899, dtype=float32), 'train/wer': 0.7357704402515723, 'validation/ctc_loss': Array(3.243913, dtype=float32), 'validation/wer': 0.7009953947304903, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.893342, dtype=float32), 'test/wer': 0.6339040887209798, 'test/num_examples': 2472, 'score': 2914.5891761779785, 'total_duration': 3268.5771906375885, 'accumulated_submission_time': 2914.5891761779785, 'accumulated_eval_time': 353.74039936065674, 'accumulated_logging_time': 0.09537768363952637}
I0219 14:13:37.288989 140483498055424 logging_writer.py:48] [3682] accumulated_eval_time=353.740399, accumulated_logging_time=0.095378, accumulated_submission_time=2914.589176, global_step=3682, preemption_count=0, score=2914.589176, test/ctc_loss=2.8933420181274414, test/num_examples=2472, test/wer=0.633904, total_duration=3268.577191, train/ctc_loss=3.304898977279663, train/wer=0.735770, validation/ctc_loss=3.243912935256958, validation/num_examples=5348, validation/wer=0.700995
I0219 14:13:51.701230 140468535932672 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.089834451675415, loss=2.4191744327545166
I0219 14:15:07.450145 140483498055424 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.0445345640182495, loss=2.389453887939453
I0219 14:16:23.243862 140468535932672 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.0538033246994019, loss=2.4672040939331055
I0219 14:17:38.996081 140483498055424 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.3673628568649292, loss=2.336024045944214
I0219 14:18:54.907716 140468535932672 logging_writer.py:48] [4100] global_step=4100, grad_norm=1.0546098947525024, loss=2.348256826400757
I0219 14:20:14.421643 140483498055424 logging_writer.py:48] [4200] global_step=4200, grad_norm=1.0024042129516602, loss=2.3098487854003906
I0219 14:21:30.148229 140468535932672 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.9187269806861877, loss=2.2530670166015625
I0219 14:22:45.947595 140483498055424 logging_writer.py:48] [4400] global_step=4400, grad_norm=1.1084023714065552, loss=2.2654378414154053
I0219 14:24:01.646526 140468535932672 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.9278073906898499, loss=2.233957290649414
I0219 14:25:17.468476 140483498055424 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.9011375904083252, loss=2.1609840393066406
I0219 14:26:33.561604 140468535932672 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.8794596791267395, loss=2.162526845932007
I0219 14:27:50.913715 140483498055424 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.9641215205192566, loss=2.0839436054229736
I0219 14:29:12.593438 140468535932672 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.8212953805923462, loss=2.0614094734191895
I0219 14:30:33.839668 140483498055424 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.8201158046722412, loss=2.0388762950897217
I0219 14:31:55.637266 140468535932672 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.9227120280265808, loss=2.018070697784424
I0219 14:33:17.357703 140483498055424 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.896522045135498, loss=2.0065951347351074
I0219 14:34:33.035065 140468535932672 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.8285133242607117, loss=2.011364698410034
I0219 14:35:48.764880 140483498055424 logging_writer.py:48] [5400] global_step=5400, grad_norm=1.189361333847046, loss=2.015467643737793
I0219 14:37:04.628336 140468535932672 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.8874415159225464, loss=2.0135743618011475
I0219 14:37:37.574276 140549388556096 spec.py:321] Evaluating on the training split.
I0219 14:38:30.650576 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 14:39:21.042280 140549388556096 spec.py:349] Evaluating on the test split.
I0219 14:39:46.476634 140549388556096 submission_runner.py:408] Time since start: 4837.80s, 	Step: 5545, 	{'train/ctc_loss': Array(1.0559078, dtype=float32), 'train/wer': 0.3306912298852493, 'validation/ctc_loss': Array(1.0954505, dtype=float32), 'validation/wer': 0.31886422661401664, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.80377287, dtype=float32), 'test/wer': 0.26098348668575955, 'test/num_examples': 2472, 'score': 4354.786093473434, 'total_duration': 4837.799213647842, 'accumulated_submission_time': 4354.786093473434, 'accumulated_eval_time': 482.6379177570343, 'accumulated_logging_time': 0.14456629753112793}
I0219 14:39:46.512556 140483498055424 logging_writer.py:48] [5545] accumulated_eval_time=482.637918, accumulated_logging_time=0.144566, accumulated_submission_time=4354.786093, global_step=5545, preemption_count=0, score=4354.786093, test/ctc_loss=0.8037728667259216, test/num_examples=2472, test/wer=0.260983, total_duration=4837.799214, train/ctc_loss=1.0559078454971313, train/wer=0.330691, validation/ctc_loss=1.095450520515442, validation/num_examples=5348, validation/wer=0.318864
I0219 14:40:28.792235 140468535932672 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.1251277923583984, loss=1.9978162050247192
I0219 14:41:44.512945 140483498055424 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.096309781074524, loss=2.0044102668762207
I0219 14:43:00.632952 140468535932672 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.7204158306121826, loss=1.881727695465088
I0219 14:44:16.454651 140483498055424 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.8705224990844727, loss=1.9017400741577148
I0219 14:45:32.217611 140468535932672 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.8124357461929321, loss=1.8792471885681152
I0219 14:46:48.796531 140483498055424 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.8309268355369568, loss=1.887128233909607
I0219 14:48:12.660412 140483498055424 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.8374708890914917, loss=1.792685866355896
I0219 14:49:28.302600 140468535932672 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.7380461692810059, loss=1.8324753046035767
I0219 14:50:43.973243 140483498055424 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.7162943482398987, loss=1.7869446277618408
I0219 14:51:59.771218 140468535932672 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.8780080080032349, loss=1.797249674797058
I0219 14:53:15.442108 140483498055424 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.7995650172233582, loss=1.8336193561553955
I0219 14:54:31.111306 140468535932672 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.8208613991737366, loss=1.8914375305175781
I0219 14:55:46.849615 140483498055424 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.827872097492218, loss=1.7661882638931274
I0219 14:57:05.093243 140468535932672 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.8420759439468384, loss=1.8266724348068237
I0219 14:58:26.596709 140483498055424 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.9059316515922546, loss=1.8151617050170898
I0219 14:59:48.186579 140468535932672 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.7713407278060913, loss=1.811475157737732
I0219 15:01:09.356886 140483498055424 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.7034707069396973, loss=1.7452518939971924
I0219 15:02:28.358672 140483498055424 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.903976321220398, loss=1.7706146240234375
I0219 15:03:44.098573 140468535932672 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.7591049075126648, loss=1.766182780265808
I0219 15:03:46.851971 140549388556096 spec.py:321] Evaluating on the training split.
I0219 15:04:39.047719 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 15:05:30.184660 140549388556096 spec.py:349] Evaluating on the test split.
I0219 15:05:56.013744 140549388556096 submission_runner.py:408] Time since start: 6407.34s, 	Step: 7405, 	{'train/ctc_loss': Array(0.68506795, dtype=float32), 'train/wer': 0.22820694141939862, 'validation/ctc_loss': Array(0.83268803, dtype=float32), 'validation/wer': 0.24793149058188593, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5571465, dtype=float32), 'test/wer': 0.18810553896776552, 'test/num_examples': 2472, 'score': 5795.037358760834, 'total_duration': 6407.335177659988, 'accumulated_submission_time': 5795.037358760834, 'accumulated_eval_time': 611.7937211990356, 'accumulated_logging_time': 0.1955733299255371}
I0219 15:05:56.049849 140483498055424 logging_writer.py:48] [7405] accumulated_eval_time=611.793721, accumulated_logging_time=0.195573, accumulated_submission_time=5795.037359, global_step=7405, preemption_count=0, score=5795.037359, test/ctc_loss=0.5571464896202087, test/num_examples=2472, test/wer=0.188106, total_duration=6407.335178, train/ctc_loss=0.6850679516792297, train/wer=0.228207, validation/ctc_loss=0.83268803358078, validation/num_examples=5348, validation/wer=0.247931
I0219 15:07:08.221897 140468535932672 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.8134472370147705, loss=1.7452678680419922
I0219 15:08:23.696073 140483498055424 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.8247685432434082, loss=1.7635266780853271
I0219 15:09:39.275722 140468535932672 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.6779192686080933, loss=1.747251272201538
I0219 15:10:54.844733 140483498055424 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.7345534563064575, loss=1.7341270446777344
I0219 15:12:10.331627 140468535932672 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.8077706098556519, loss=1.7147167921066284
I0219 15:13:25.741142 140483498055424 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.7196704745292664, loss=1.7243579626083374
I0219 15:14:44.786916 140468535932672 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.6630329489707947, loss=1.7500141859054565
I0219 15:16:05.599612 140483498055424 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.7669380903244019, loss=1.7281752824783325
I0219 15:17:26.835849 140483498055424 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.8032707571983337, loss=1.729397177696228
I0219 15:18:42.349641 140468535932672 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.7778053283691406, loss=1.6578631401062012
I0219 15:19:57.907777 140483498055424 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.6999326944351196, loss=1.6711221933364868
I0219 15:21:13.603214 140468535932672 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.6803020238876343, loss=1.741576075553894
I0219 15:22:29.192562 140483498055424 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.7875974178314209, loss=1.683026909828186
I0219 15:23:44.717106 140468535932672 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.6281376481056213, loss=1.671890377998352
I0219 15:25:02.506463 140483498055424 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.7290088534355164, loss=1.68195378780365
I0219 15:26:23.854645 140468535932672 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.7617154717445374, loss=1.615450143814087
I0219 15:27:45.722388 140483498055424 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.6404151916503906, loss=1.7041213512420654
I0219 15:29:06.512475 140468535932672 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.8114175200462341, loss=1.6373780965805054
I0219 15:29:56.308870 140549388556096 spec.py:321] Evaluating on the training split.
I0219 15:30:47.534108 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 15:31:37.813051 140549388556096 spec.py:349] Evaluating on the test split.
I0219 15:32:03.639488 140549388556096 submission_runner.py:408] Time since start: 7974.96s, 	Step: 9263, 	{'train/ctc_loss': Array(0.56729215, dtype=float32), 'train/wer': 0.1905387948281184, 'validation/ctc_loss': Array(0.7289868, dtype=float32), 'validation/wer': 0.2186778918099578, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.47376433, dtype=float32), 'test/wer': 0.15731318424633883, 'test/num_examples': 2472, 'score': 7235.207192897797, 'total_duration': 7974.961220741272, 'accumulated_submission_time': 7235.207192897797, 'accumulated_eval_time': 739.1186428070068, 'accumulated_logging_time': 0.24791836738586426}
I0219 15:32:03.675753 140483498055424 logging_writer.py:48] [9263] accumulated_eval_time=739.118643, accumulated_logging_time=0.247918, accumulated_submission_time=7235.207193, global_step=9263, preemption_count=0, score=7235.207193, test/ctc_loss=0.4737643301486969, test/num_examples=2472, test/wer=0.157313, total_duration=7974.961221, train/ctc_loss=0.5672921538352966, train/wer=0.190539, validation/ctc_loss=0.7289867997169495, validation/num_examples=5348, validation/wer=0.218678
I0219 15:32:35.912562 140483498055424 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.7440772652626038, loss=1.6382222175598145
I0219 15:33:51.383445 140468535932672 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.6195070147514343, loss=1.6330070495605469
I0219 15:35:06.905840 140483498055424 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.7460862994194031, loss=1.5707234144210815
I0219 15:36:22.603240 140468535932672 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.6997404098510742, loss=1.6084686517715454
I0219 15:37:38.158251 140483498055424 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.6466330885887146, loss=1.615564227104187
I0219 15:38:53.680097 140468535932672 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.647042453289032, loss=1.6407581567764282
I0219 15:40:09.362295 140483498055424 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.6486186981201172, loss=1.579003095626831
I0219 15:41:30.326663 140468535932672 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.7253236770629883, loss=1.595977783203125
I0219 15:42:52.165151 140483498055424 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.7705985903739929, loss=1.5723828077316284
I0219 15:44:13.566304 140468535932672 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.9401488900184631, loss=1.6130774021148682
I0219 15:45:38.540482 140483498055424 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.6009551882743835, loss=1.5824077129364014
I0219 15:46:54.476690 140468535932672 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.6840632557868958, loss=1.5753644704818726
I0219 15:48:09.923310 140483498055424 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.5880128145217896, loss=1.564932942390442
I0219 15:49:25.480656 140468535932672 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.7075085639953613, loss=1.6477769613265991
I0219 15:50:41.008428 140483498055424 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.6637449264526367, loss=1.5837937593460083
I0219 15:51:56.763708 140468535932672 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.6678656339645386, loss=1.5648283958435059
I0219 15:53:12.447866 140483498055424 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.7594677209854126, loss=1.5948526859283447
I0219 15:54:32.466760 140468535932672 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.6405015587806702, loss=1.5783097743988037
I0219 15:55:53.530950 140483498055424 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.7304794192314148, loss=1.5910005569458008
I0219 15:56:03.834316 140549388556096 spec.py:321] Evaluating on the training split.
I0219 15:56:56.606442 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 15:57:47.840274 140549388556096 spec.py:349] Evaluating on the test split.
I0219 15:58:13.719110 140549388556096 submission_runner.py:408] Time since start: 9545.04s, 	Step: 11114, 	{'train/ctc_loss': Array(0.55909514, dtype=float32), 'train/wer': 0.192576916815888, 'validation/ctc_loss': Array(0.6618509, dtype=float32), 'validation/wer': 0.20010233932243646, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42439848, dtype=float32), 'test/wer': 0.14524810594519935, 'test/num_examples': 2472, 'score': 8675.277812957764, 'total_duration': 9545.040509700775, 'accumulated_submission_time': 8675.277812957764, 'accumulated_eval_time': 868.9974067211151, 'accumulated_logging_time': 0.300107479095459}
I0219 15:58:13.756294 140483498055424 logging_writer.py:48] [11114] accumulated_eval_time=868.997407, accumulated_logging_time=0.300107, accumulated_submission_time=8675.277813, global_step=11114, preemption_count=0, score=8675.277813, test/ctc_loss=0.4243984818458557, test/num_examples=2472, test/wer=0.145248, total_duration=9545.040510, train/ctc_loss=0.5590951442718506, train/wer=0.192577, validation/ctc_loss=0.6618509292602539, validation/num_examples=5348, validation/wer=0.200102
I0219 15:59:19.267956 140468535932672 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.7270087599754333, loss=1.524795651435852
I0219 16:00:34.809997 140483498055424 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.6969079375267029, loss=1.537670612335205
I0219 16:01:53.703331 140483498055424 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.795366108417511, loss=1.6017214059829712
I0219 16:03:09.167535 140468535932672 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.5818172693252563, loss=1.5473027229309082
I0219 16:04:24.902827 140483498055424 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.5775193572044373, loss=1.568068027496338
I0219 16:05:40.357897 140468535932672 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.6447969675064087, loss=1.517701506614685
I0219 16:06:55.944974 140483498055424 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.6821705102920532, loss=1.5710904598236084
I0219 16:08:11.485608 140468535932672 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.6411266922950745, loss=1.5248286724090576
I0219 16:09:29.280658 140483498055424 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.6968660354614258, loss=1.5837026834487915
I0219 16:10:51.284880 140468535932672 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.5945247411727905, loss=1.5090954303741455
I0219 16:12:13.003530 140483498055424 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.6560055613517761, loss=1.580858826637268
I0219 16:13:34.526201 140468535932672 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.6448988318443298, loss=1.5396819114685059
I0219 16:14:55.722983 140483498055424 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.8238059878349304, loss=1.5368266105651855
I0219 16:16:11.220585 140468535932672 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.7785826921463013, loss=1.5132617950439453
I0219 16:17:26.758348 140483498055424 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.625694751739502, loss=1.5004314184188843
I0219 16:18:42.512254 140468535932672 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.7240542769432068, loss=1.5724492073059082
I0219 16:19:57.988811 140483498055424 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.7220385670661926, loss=1.5157475471496582
I0219 16:21:13.545334 140468535932672 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.5460721850395203, loss=1.4520397186279297
I0219 16:22:14.257172 140549388556096 spec.py:321] Evaluating on the training split.
I0219 16:23:07.774229 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 16:23:58.466405 140549388556096 spec.py:349] Evaluating on the test split.
I0219 16:24:24.577648 140549388556096 submission_runner.py:408] Time since start: 11115.90s, 	Step: 12982, 	{'train/ctc_loss': Array(0.47844568, dtype=float32), 'train/wer': 0.16600666360036348, 'validation/ctc_loss': Array(0.6252392, dtype=float32), 'validation/wer': 0.18973324193595104, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39323118, dtype=float32), 'test/wer': 0.13417829504600573, 'test/num_examples': 2472, 'score': 10115.687534570694, 'total_duration': 11115.899310827255, 'accumulated_submission_time': 10115.687534570694, 'accumulated_eval_time': 999.3121480941772, 'accumulated_logging_time': 0.35501646995544434}
I0219 16:24:24.615223 140483498055424 logging_writer.py:48] [12982] accumulated_eval_time=999.312148, accumulated_logging_time=0.355016, accumulated_submission_time=10115.687535, global_step=12982, preemption_count=0, score=10115.687535, test/ctc_loss=0.39323118329048157, test/num_examples=2472, test/wer=0.134178, total_duration=11115.899311, train/ctc_loss=0.4784456789493561, train/wer=0.166007, validation/ctc_loss=0.6252391934394836, validation/num_examples=5348, validation/wer=0.189733
I0219 16:24:39.028972 140468535932672 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.7651668787002563, loss=1.5150355100631714
I0219 16:25:54.614384 140483498055424 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.6928408741950989, loss=1.5166422128677368
I0219 16:27:10.168727 140468535932672 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.617739737033844, loss=1.50810706615448
I0219 16:28:25.645107 140483498055424 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.7433902621269226, loss=1.5030362606048584
I0219 16:29:44.689013 140483498055424 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.6768084168434143, loss=1.4525138139724731
I0219 16:31:00.202423 140468535932672 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.7745633721351624, loss=1.5296553373336792
I0219 16:32:15.778027 140483498055424 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.5796598196029663, loss=1.3883354663848877
I0219 16:33:31.402014 140468535932672 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.6497941017150879, loss=1.4904152154922485
I0219 16:34:47.090458 140483498055424 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.7261261940002441, loss=1.462965726852417
I0219 16:36:02.952523 140468535932672 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.6788715720176697, loss=1.5472264289855957
I0219 16:37:18.554422 140483498055424 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.6055801510810852, loss=1.5390205383300781
I0219 16:38:37.020627 140468535932672 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.8399860262870789, loss=1.5347685813903809
I0219 16:39:57.735954 140483498055424 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.7608591914176941, loss=1.5161758661270142
I0219 16:41:19.319277 140468535932672 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.6496385335922241, loss=1.500734806060791
I0219 16:42:40.774048 140483498055424 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.6553603410720825, loss=1.4629915952682495
I0219 16:44:00.546628 140483498055424 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.7108674645423889, loss=1.4664548635482788
I0219 16:45:16.102131 140468535932672 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.9613596200942993, loss=1.4667856693267822
I0219 16:46:31.640947 140483498055424 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.6520926356315613, loss=1.4490183591842651
I0219 16:47:47.135656 140468535932672 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.7705211639404297, loss=1.4955880641937256
I0219 16:48:24.576812 140549388556096 spec.py:321] Evaluating on the training split.
I0219 16:49:18.291992 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 16:50:09.220293 140549388556096 spec.py:349] Evaluating on the test split.
I0219 16:50:34.867768 140549388556096 submission_runner.py:408] Time since start: 12686.19s, 	Step: 14851, 	{'train/ctc_loss': Array(0.4646131, dtype=float32), 'train/wer': 0.15785684451186002, 'validation/ctc_loss': Array(0.59083736, dtype=float32), 'validation/wer': 0.18072545063093157, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36638296, dtype=float32), 'test/wer': 0.12546462738407166, 'test/num_examples': 2472, 'score': 11555.559470653534, 'total_duration': 12686.189148187637, 'accumulated_submission_time': 11555.559470653534, 'accumulated_eval_time': 1129.5970721244812, 'accumulated_logging_time': 0.4081838130950928}
I0219 16:50:34.909189 140483498055424 logging_writer.py:48] [14851] accumulated_eval_time=1129.597072, accumulated_logging_time=0.408184, accumulated_submission_time=11555.559471, global_step=14851, preemption_count=0, score=11555.559471, test/ctc_loss=0.3663829565048218, test/num_examples=2472, test/wer=0.125465, total_duration=12686.189148, train/ctc_loss=0.4646131098270416, train/wer=0.157857, validation/ctc_loss=0.5908373594284058, validation/num_examples=5348, validation/wer=0.180725
I0219 16:51:12.560668 140468535932672 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.6471896171569824, loss=1.481730341911316
I0219 16:52:28.376312 140483498055424 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.6228854060173035, loss=1.4453628063201904
I0219 16:53:43.962152 140468535932672 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.6927675008773804, loss=1.4514318704605103
I0219 16:54:59.475097 140483498055424 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.7348058819770813, loss=1.488809585571289
I0219 16:56:15.144276 140468535932672 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.8975051045417786, loss=1.4492151737213135
I0219 16:57:31.149987 140483498055424 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.6365984082221985, loss=1.441114902496338
I0219 16:58:52.245320 140483498055424 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.7602702379226685, loss=1.4568125009536743
I0219 17:00:07.694254 140468535932672 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.7515809535980225, loss=1.4333727359771729
I0219 17:01:23.182182 140483498055424 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.6969084739685059, loss=1.434805154800415
I0219 17:02:38.704251 140468535932672 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.6729128360748291, loss=1.3708816766738892
I0219 17:03:54.302324 140483498055424 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.6372665762901306, loss=1.4460673332214355
I0219 17:05:09.831091 140468535932672 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.6472867727279663, loss=1.465610384941101
I0219 17:06:28.681573 140483498055424 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.6185122728347778, loss=1.4239625930786133
I0219 17:07:48.736092 140468535932672 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.6847748756408691, loss=1.4509336948394775
I0219 17:09:10.104118 140483498055424 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.7252086997032166, loss=1.4762909412384033
I0219 17:10:31.433323 140468535932672 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.5883209109306335, loss=1.4422978162765503
I0219 17:11:54.614994 140483498055424 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.9296980500221252, loss=1.4404027462005615
I0219 17:13:09.974312 140468535932672 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.6418352723121643, loss=1.4160536527633667
I0219 17:14:25.373721 140483498055424 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.6155114769935608, loss=1.3866451978683472
I0219 17:14:34.888878 140549388556096 spec.py:321] Evaluating on the training split.
I0219 17:15:28.405500 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 17:16:19.003047 140549388556096 spec.py:349] Evaluating on the test split.
I0219 17:16:44.483739 140549388556096 submission_runner.py:408] Time since start: 14255.81s, 	Step: 16714, 	{'train/ctc_loss': Array(0.38969892, dtype=float32), 'train/wer': 0.13778405189697507, 'validation/ctc_loss': Array(0.5713257, dtype=float32), 'validation/wer': 0.17264450601967618, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3518366, dtype=float32), 'test/wer': 0.11969613876871205, 'test/num_examples': 2472, 'score': 12995.451155424118, 'total_duration': 14255.806090593338, 'accumulated_submission_time': 12995.451155424118, 'accumulated_eval_time': 1259.186912059784, 'accumulated_logging_time': 0.46500158309936523}
I0219 17:16:44.517872 140483498055424 logging_writer.py:48] [16714] accumulated_eval_time=1259.186912, accumulated_logging_time=0.465002, accumulated_submission_time=12995.451155, global_step=16714, preemption_count=0, score=12995.451155, test/ctc_loss=0.35183659195899963, test/num_examples=2472, test/wer=0.119696, total_duration=14255.806091, train/ctc_loss=0.38969892263412476, train/wer=0.137784, validation/ctc_loss=0.5713257193565369, validation/num_examples=5348, validation/wer=0.172645
I0219 17:17:49.965521 140468535932672 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.6695530414581299, loss=1.4228180646896362
I0219 17:19:05.455963 140483498055424 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.6286914944648743, loss=1.397450566291809
I0219 17:20:21.088983 140468535932672 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.7272533178329468, loss=1.455530047416687
I0219 17:21:36.416857 140483498055424 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.9084110260009766, loss=1.5032676458358765
I0219 17:22:51.916371 140468535932672 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.5735324621200562, loss=1.3807209730148315
I0219 17:24:07.796540 140483498055424 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.7334839701652527, loss=1.4657055139541626
I0219 17:25:25.558673 140468535932672 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.7541595697402954, loss=1.421567440032959
I0219 17:26:46.818570 140483498055424 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.6430674195289612, loss=1.4012641906738281
I0219 17:28:05.773745 140483498055424 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.6158573627471924, loss=1.423164963722229
I0219 17:29:21.427441 140468535932672 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.5622987747192383, loss=1.3809843063354492
I0219 17:30:36.927668 140483498055424 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.8767442107200623, loss=1.4065065383911133
I0219 17:31:52.490530 140468535932672 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.7769272327423096, loss=1.4525201320648193
I0219 17:33:08.126816 140483498055424 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.6503216028213501, loss=1.3771662712097168
I0219 17:34:23.650045 140468535932672 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.7163668274879456, loss=1.4301514625549316
I0219 17:35:39.899820 140483498055424 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.6484789252281189, loss=1.4207669496536255
I0219 17:37:01.056387 140468535932672 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.598707377910614, loss=1.3311553001403809
I0219 17:38:21.936576 140483498055424 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.657558023929596, loss=1.3833212852478027
I0219 17:39:42.819957 140468535932672 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.7868005633354187, loss=1.450903058052063
I0219 17:40:44.700903 140549388556096 spec.py:321] Evaluating on the training split.
I0219 17:41:36.335021 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 17:42:27.123908 140549388556096 spec.py:349] Evaluating on the test split.
I0219 17:42:52.941839 140549388556096 submission_runner.py:408] Time since start: 15824.26s, 	Step: 18576, 	{'train/ctc_loss': Array(0.39127594, dtype=float32), 'train/wer': 0.13950707273182844, 'validation/ctc_loss': Array(0.5567127, dtype=float32), 'validation/wer': 0.16816474699981657, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33858824, dtype=float32), 'test/wer': 0.11459793228119351, 'test/num_examples': 2472, 'score': 14435.541295289993, 'total_duration': 15824.262801408768, 'accumulated_submission_time': 14435.541295289993, 'accumulated_eval_time': 1387.4215536117554, 'accumulated_logging_time': 0.5193827152252197}
I0219 17:42:52.979340 140483498055424 logging_writer.py:48] [18576] accumulated_eval_time=1387.421554, accumulated_logging_time=0.519383, accumulated_submission_time=14435.541295, global_step=18576, preemption_count=0, score=14435.541295, test/ctc_loss=0.33858823776245117, test/num_examples=2472, test/wer=0.114598, total_duration=15824.262801, train/ctc_loss=0.39127594232559204, train/wer=0.139507, validation/ctc_loss=0.5567126870155334, validation/num_examples=5348, validation/wer=0.168165
I0219 17:43:11.858411 140468535932672 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.5893189311027527, loss=1.3806953430175781
I0219 17:44:27.261293 140483498055424 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.6813883781433105, loss=1.3597337007522583
I0219 17:45:42.897152 140468535932672 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.7376865744590759, loss=1.4345307350158691
I0219 17:46:58.455018 140483498055424 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.6485083103179932, loss=1.349610686302185
I0219 17:48:14.059528 140468535932672 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.5819305777549744, loss=1.3830904960632324
I0219 17:49:29.710915 140483498055424 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.6656156182289124, loss=1.399031162261963
I0219 17:50:45.271198 140468535932672 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.6372136473655701, loss=1.401855707168579
I0219 17:52:00.815438 140483498055424 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.7485472559928894, loss=1.4097932577133179
I0219 17:53:16.353659 140468535932672 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.6467977166175842, loss=1.3714828491210938
I0219 17:54:32.972898 140483498055424 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.6926297545433044, loss=1.4049259424209595
I0219 17:55:55.379961 140483498055424 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.6370728015899658, loss=1.404920220375061
I0219 17:57:10.810700 140468535932672 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.6078096628189087, loss=1.3764147758483887
I0219 17:58:26.378187 140483498055424 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.64017254114151, loss=1.3631691932678223
I0219 17:59:41.865268 140468535932672 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.8016101121902466, loss=1.4132040739059448
I0219 18:00:57.377502 140483498055424 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.6523280739784241, loss=1.3519951105117798
I0219 18:02:12.905138 140468535932672 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.8273738622665405, loss=1.4172706604003906
I0219 18:03:28.482217 140483498055424 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.6813380718231201, loss=1.3156918287277222
I0219 18:04:43.941989 140468535932672 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.7152096033096313, loss=1.3979405164718628
I0219 18:06:03.856465 140483498055424 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.7062028646469116, loss=1.434224247932434
I0219 18:06:53.639439 140549388556096 spec.py:321] Evaluating on the training split.
I0219 18:07:46.539422 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 18:08:37.295083 140549388556096 spec.py:349] Evaluating on the test split.
I0219 18:09:03.272460 140549388556096 submission_runner.py:408] Time since start: 17394.59s, 	Step: 20463, 	{'train/ctc_loss': Array(0.3756434, dtype=float32), 'train/wer': 0.1329070768267737, 'validation/ctc_loss': Array(0.53292954, dtype=float32), 'validation/wer': 0.16323121928613496, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32874158, dtype=float32), 'test/wer': 0.1117136879735137, 'test/num_examples': 2472, 'score': 15876.112209320068, 'total_duration': 17394.59325671196, 'accumulated_submission_time': 15876.112209320068, 'accumulated_eval_time': 1517.0479528903961, 'accumulated_logging_time': 0.5736007690429688}
I0219 18:09:03.308001 140483498055424 logging_writer.py:48] [20463] accumulated_eval_time=1517.047953, accumulated_logging_time=0.573601, accumulated_submission_time=15876.112209, global_step=20463, preemption_count=0, score=15876.112209, test/ctc_loss=0.32874158024787903, test/num_examples=2472, test/wer=0.111714, total_duration=17394.593257, train/ctc_loss=0.37564340233802795, train/wer=0.132907, validation/ctc_loss=0.532929539680481, validation/num_examples=5348, validation/wer=0.163231
I0219 18:09:31.910354 140468535932672 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.6296610832214355, loss=1.430443286895752
I0219 18:10:50.894136 140483498055424 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.6248167753219604, loss=1.3227667808532715
I0219 18:12:06.568840 140468535932672 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.8560991883277893, loss=1.379640817642212
I0219 18:13:22.107665 140483498055424 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.7209567427635193, loss=1.3923702239990234
I0219 18:14:37.675110 140468535932672 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.7826665639877319, loss=1.3703138828277588
I0219 18:15:53.423465 140483498055424 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.6638240814208984, loss=1.322706699371338
I0219 18:17:09.039612 140468535932672 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.7358852028846741, loss=1.3678925037384033
I0219 18:18:24.521919 140483498055424 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.6865049004554749, loss=1.3614940643310547
I0219 18:19:40.071080 140468535932672 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.7993406057357788, loss=1.3537346124649048
I0219 18:20:58.502758 140483498055424 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.6667875051498413, loss=1.3425700664520264
I0219 18:22:19.510587 140468535932672 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.833830714225769, loss=1.335452914237976
I0219 18:23:40.298016 140483498055424 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.9047735333442688, loss=1.3565937280654907
I0219 18:25:00.706303 140483498055424 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.6826668381690979, loss=1.3591580390930176
I0219 18:26:16.344752 140468535932672 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.6789307594299316, loss=1.3702205419540405
I0219 18:27:32.247932 140483498055424 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.7029296159744263, loss=1.3458251953125
I0219 18:28:47.923560 140468535932672 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.6256887316703796, loss=1.3657289743423462
I0219 18:30:03.661160 140483498055424 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.9662618637084961, loss=1.3739532232284546
I0219 18:31:19.374440 140468535932672 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.6326102614402771, loss=1.363608717918396
I0219 18:32:35.053303 140483498055424 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.5991352200508118, loss=1.3460458517074585
I0219 18:33:03.487803 140549388556096 spec.py:321] Evaluating on the training split.
I0219 18:34:07.332913 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 18:34:58.077417 140549388556096 spec.py:349] Evaluating on the test split.
I0219 18:35:24.276691 140549388556096 submission_runner.py:408] Time since start: 18975.60s, 	Step: 22339, 	{'train/ctc_loss': Array(0.25177395, dtype=float32), 'train/wer': 0.09277053612055351, 'validation/ctc_loss': Array(0.5271379, dtype=float32), 'validation/wer': 0.15752531932764996, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31686965, dtype=float32), 'test/wer': 0.10669672780452136, 'test/num_examples': 2472, 'score': 17316.20405101776, 'total_duration': 18975.59814763069, 'accumulated_submission_time': 17316.20405101776, 'accumulated_eval_time': 1657.830862045288, 'accumulated_logging_time': 0.6243786811828613}
I0219 18:35:24.313513 140483498055424 logging_writer.py:48] [22339] accumulated_eval_time=1657.830862, accumulated_logging_time=0.624379, accumulated_submission_time=17316.204051, global_step=22339, preemption_count=0, score=17316.204051, test/ctc_loss=0.3168696463108063, test/num_examples=2472, test/wer=0.106697, total_duration=18975.598148, train/ctc_loss=0.2517739534378052, train/wer=0.092771, validation/ctc_loss=0.5271378755569458, validation/num_examples=5348, validation/wer=0.157525
I0219 18:36:11.048389 140468535932672 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.7020128965377808, loss=1.4114031791687012
I0219 18:37:26.675597 140483498055424 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.7528754472732544, loss=1.328905701637268
I0219 18:38:42.222255 140468535932672 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.7124007940292358, loss=1.364530324935913
I0219 18:40:01.167116 140483498055424 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.6327884197235107, loss=1.3733162879943848
I0219 18:41:16.795698 140468535932672 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.8183611035346985, loss=1.3337161540985107
I0219 18:42:32.278610 140483498055424 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.8235949873924255, loss=1.380670428276062
I0219 18:43:48.169746 140468535932672 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.6511364579200745, loss=1.361342191696167
I0219 18:45:03.766140 140483498055424 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.5930566787719727, loss=1.2845429182052612
I0219 18:46:19.301783 140468535932672 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.6875972747802734, loss=1.3624485731124878
I0219 18:47:34.901288 140483498055424 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.6845019459724426, loss=1.3317387104034424
I0219 18:48:50.558751 140468535932672 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.6819580793380737, loss=1.2822484970092773
I0219 18:50:10.093343 140483498055424 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.6944609880447388, loss=1.383550763130188
I0219 18:51:31.065349 140468535932672 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.6908881664276123, loss=1.314721941947937
I0219 18:52:54.711388 140483498055424 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.6758525371551514, loss=1.3178153038024902
I0219 18:54:10.133093 140468535932672 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.671520471572876, loss=1.3311915397644043
I0219 18:55:25.568655 140483498055424 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.8739437460899353, loss=1.330783486366272
I0219 18:56:41.128381 140468535932672 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.6672960519790649, loss=1.3125419616699219
I0219 18:57:56.677337 140483498055424 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.577907145023346, loss=1.371109127998352
I0219 18:59:12.450032 140468535932672 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.6962863802909851, loss=1.3658270835876465
I0219 18:59:24.991412 140549388556096 spec.py:321] Evaluating on the training split.
I0219 19:00:19.913908 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 19:01:11.232252 140549388556096 spec.py:349] Evaluating on the test split.
I0219 19:01:36.669726 140549388556096 submission_runner.py:408] Time since start: 20547.99s, 	Step: 24218, 	{'train/ctc_loss': Array(0.23562592, dtype=float32), 'train/wer': 0.0862083217093911, 'validation/ctc_loss': Array(0.5088276, dtype=float32), 'validation/wer': 0.15359587553221274, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30003414, dtype=float32), 'test/wer': 0.10330469400605286, 'test/num_examples': 2472, 'score': 18756.792862415314, 'total_duration': 20547.990478992462, 'accumulated_submission_time': 18756.792862415314, 'accumulated_eval_time': 1789.5025107860565, 'accumulated_logging_time': 0.6770033836364746}
I0219 19:01:36.709673 140483498055424 logging_writer.py:48] [24218] accumulated_eval_time=1789.502511, accumulated_logging_time=0.677003, accumulated_submission_time=18756.792862, global_step=24218, preemption_count=0, score=18756.792862, test/ctc_loss=0.30003413558006287, test/num_examples=2472, test/wer=0.103305, total_duration=20547.990479, train/ctc_loss=0.23562592267990112, train/wer=0.086208, validation/ctc_loss=0.5088276267051697, validation/num_examples=5348, validation/wer=0.153596
I0219 19:02:39.296838 140468535932672 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.6498837471008301, loss=1.3523821830749512
I0219 19:03:54.823419 140483498055424 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.7131264805793762, loss=1.3608349561691284
I0219 19:05:10.452209 140468535932672 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.6394572854042053, loss=1.3009445667266846
I0219 19:06:26.086313 140483498055424 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.5959067344665527, loss=1.3814499378204346
I0219 19:07:41.625165 140468535932672 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.7306104898452759, loss=1.3261398077011108
I0219 19:09:00.454223 140483498055424 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.8090604543685913, loss=1.2574673891067505
I0219 19:10:16.021296 140468535932672 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.6369320750236511, loss=1.2857142686843872
I0219 19:11:31.484101 140483498055424 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.7427265644073486, loss=1.3226479291915894
I0219 19:12:46.997684 140468535932672 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.8841581344604492, loss=1.3584587574005127
I0219 19:14:02.460992 140483498055424 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.6878853440284729, loss=1.3804304599761963
I0219 19:15:18.190389 140468535932672 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.637575626373291, loss=1.2959753274917603
I0219 19:16:34.569845 140483498055424 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.7298969626426697, loss=1.318021297454834
I0219 19:17:54.531867 140468535932672 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.7296386361122131, loss=1.2873754501342773
I0219 19:19:15.338072 140483498055424 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.8284156918525696, loss=1.3060328960418701
I0219 19:20:34.219555 140468535932672 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.6253986358642578, loss=1.3191497325897217
I0219 19:21:55.311846 140483498055424 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.7557669281959534, loss=1.3146114349365234
I0219 19:23:10.849315 140468535932672 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.6974122524261475, loss=1.277724266052246
I0219 19:24:26.330497 140483498055424 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.7112331986427307, loss=1.3697205781936646
I0219 19:25:36.845737 140549388556096 spec.py:321] Evaluating on the training split.
I0219 19:26:30.582787 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 19:27:20.850095 140549388556096 spec.py:349] Evaluating on the test split.
I0219 19:27:46.384874 140549388556096 submission_runner.py:408] Time since start: 22117.71s, 	Step: 26095, 	{'train/ctc_loss': Array(0.23156746, dtype=float32), 'train/wer': 0.08548788246862167, 'validation/ctc_loss': Array(0.5032359, dtype=float32), 'validation/wer': 0.1530841789200305, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2946906, dtype=float32), 'test/wer': 0.09922206650011171, 'test/num_examples': 2472, 'score': 20196.840671777725, 'total_duration': 22117.707132577896, 'accumulated_submission_time': 20196.840671777725, 'accumulated_eval_time': 1919.0365262031555, 'accumulated_logging_time': 0.7321634292602539}
I0219 19:27:46.420129 140483498055424 logging_writer.py:48] [26095] accumulated_eval_time=1919.036526, accumulated_logging_time=0.732163, accumulated_submission_time=20196.840672, global_step=26095, preemption_count=0, score=20196.840672, test/ctc_loss=0.2946906089782715, test/num_examples=2472, test/wer=0.099222, total_duration=22117.707133, train/ctc_loss=0.23156745731830597, train/wer=0.085488, validation/ctc_loss=0.5032358765602112, validation/num_examples=5348, validation/wer=0.153084
I0219 19:27:51.034603 140468535932672 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.7305182814598083, loss=1.334985613822937
I0219 19:29:06.594843 140483498055424 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.8128963112831116, loss=1.3045803308486938
I0219 19:30:22.226362 140468535932672 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.7009803056716919, loss=1.313115119934082
I0219 19:31:37.811598 140483498055424 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.7160854935646057, loss=1.3183200359344482
I0219 19:32:53.648686 140468535932672 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.7028874754905701, loss=1.3201253414154053
I0219 19:34:09.323341 140483498055424 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.7219454646110535, loss=1.2727750539779663
I0219 19:35:25.494082 140468535932672 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.666786253452301, loss=1.353060007095337
I0219 19:36:48.046142 140483498055424 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.6959892511367798, loss=1.2654281854629517
I0219 19:38:03.428415 140468535932672 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.693143367767334, loss=1.3081159591674805
I0219 19:39:19.004839 140483498055424 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.636577308177948, loss=1.2083865404129028
I0219 19:40:34.411893 140468535932672 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.751674234867096, loss=1.345358967781067
I0219 19:41:49.912106 140483498055424 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.6659731268882751, loss=1.3013379573822021
I0219 19:43:05.359523 140468535932672 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.6458610892295837, loss=1.3630520105361938
I0219 19:44:20.876824 140483498055424 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.6979941129684448, loss=1.282362937927246
I0219 19:45:39.801988 140468535932672 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.7061266899108887, loss=1.2875168323516846
I0219 19:47:00.902760 140483498055424 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.630380392074585, loss=1.2549079656600952
I0219 19:48:21.375567 140468535932672 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.7238717675209045, loss=1.2921109199523926
I0219 19:49:41.890116 140483498055424 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.7415570616722107, loss=1.3193976879119873
I0219 19:51:00.910970 140483498055424 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.6436671018600464, loss=1.3120864629745483
I0219 19:51:46.529226 140549388556096 spec.py:321] Evaluating on the training split.
I0219 19:52:41.627868 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 19:53:31.767671 140549388556096 spec.py:349] Evaluating on the test split.
I0219 19:53:57.191767 140549388556096 submission_runner.py:408] Time since start: 23688.51s, 	Step: 27962, 	{'train/ctc_loss': Array(0.21804456, dtype=float32), 'train/wer': 0.08092597639462486, 'validation/ctc_loss': Array(0.48922175, dtype=float32), 'validation/wer': 0.14635488573718103, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2876642, dtype=float32), 'test/wer': 0.09544411268864379, 'test/num_examples': 2472, 'score': 21636.859580755234, 'total_duration': 23688.512843370438, 'accumulated_submission_time': 21636.859580755234, 'accumulated_eval_time': 2049.692736387253, 'accumulated_logging_time': 0.7841732501983643}
I0219 19:53:57.231452 140483498055424 logging_writer.py:48] [27962] accumulated_eval_time=2049.692736, accumulated_logging_time=0.784173, accumulated_submission_time=21636.859581, global_step=27962, preemption_count=0, score=21636.859581, test/ctc_loss=0.2876642048358917, test/num_examples=2472, test/wer=0.095444, total_duration=23688.512843, train/ctc_loss=0.21804456412792206, train/wer=0.080926, validation/ctc_loss=0.4892217516899109, validation/num_examples=5348, validation/wer=0.146355
I0219 19:54:26.629565 140468535932672 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.7464466094970703, loss=1.2991048097610474
I0219 19:55:42.223638 140483498055424 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.7000299692153931, loss=1.3233293294906616
I0219 19:56:57.751549 140468535932672 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.6501411199569702, loss=1.2871664762496948
I0219 19:58:13.426473 140483498055424 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.7081076502799988, loss=1.2894818782806396
I0219 19:59:29.056851 140468535932672 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.6913394331932068, loss=1.324413776397705
I0219 20:00:44.658087 140483498055424 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.6981843113899231, loss=1.2949488162994385
I0219 20:02:00.153944 140468535932672 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.6053864359855652, loss=1.2769696712493896
I0219 20:03:15.948256 140483498055424 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.6824321746826172, loss=1.3184465169906616
I0219 20:04:34.696123 140468535932672 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.6823951601982117, loss=1.2976016998291016
I0219 20:05:55.857874 140483498055424 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.730774998664856, loss=1.3370791673660278
I0219 20:07:11.399852 140468535932672 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.7228860259056091, loss=1.2790485620498657
I0219 20:08:26.909474 140483498055424 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.9161924123764038, loss=1.28605055809021
I0219 20:09:42.369649 140468535932672 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.6765564680099487, loss=1.3031877279281616
I0219 20:10:57.877640 140483498055424 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.7559666633605957, loss=1.2773910760879517
I0219 20:12:13.441759 140468535932672 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.7416356801986694, loss=1.2801707983016968
I0219 20:13:28.997000 140483498055424 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.654677152633667, loss=1.2570642232894897
I0219 20:14:45.512530 140468535932672 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.6354600191116333, loss=1.313161849975586
I0219 20:16:05.834565 140483498055424 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.6290775537490845, loss=1.2826881408691406
I0219 20:17:26.944938 140468535932672 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.7099347114562988, loss=1.3298232555389404
I0219 20:17:57.586631 140549388556096 spec.py:321] Evaluating on the training split.
I0219 20:18:51.151622 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 20:19:42.234937 140549388556096 spec.py:349] Evaluating on the test split.
I0219 20:20:08.914038 140549388556096 submission_runner.py:408] Time since start: 25260.24s, 	Step: 29839, 	{'train/ctc_loss': Array(0.21714135, dtype=float32), 'train/wer': 0.08038208140551374, 'validation/ctc_loss': Array(0.4858514, dtype=float32), 'validation/wer': 0.14604593683925968, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2838056, dtype=float32), 'test/wer': 0.09597221375906405, 'test/num_examples': 2472, 'score': 23077.1240503788, 'total_duration': 25260.23565888405, 'accumulated_submission_time': 23077.1240503788, 'accumulated_eval_time': 2181.0143551826477, 'accumulated_logging_time': 0.8413059711456299}
I0219 20:20:08.948894 140483498055424 logging_writer.py:48] [29839] accumulated_eval_time=2181.014355, accumulated_logging_time=0.841306, accumulated_submission_time=23077.124050, global_step=29839, preemption_count=0, score=23077.124050, test/ctc_loss=0.28380560874938965, test/num_examples=2472, test/wer=0.095972, total_duration=25260.235659, train/ctc_loss=0.21714134514331818, train/wer=0.080382, validation/ctc_loss=0.4858514070510864, validation/num_examples=5348, validation/wer=0.146046
I0219 20:20:59.127779 140483498055424 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.7896599769592285, loss=1.2996164560317993
I0219 20:22:14.539968 140468535932672 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.7146173715591431, loss=1.2699837684631348
I0219 20:23:30.173218 140483498055424 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.8207277059555054, loss=1.2475762367248535
I0219 20:24:45.727545 140468535932672 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.6159974336624146, loss=1.297179937362671
I0219 20:26:01.394793 140483498055424 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.7516118884086609, loss=1.239992380142212
I0219 20:27:16.971001 140468535932672 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.7376482486724854, loss=1.2832351922988892
I0219 20:28:32.632405 140483498055424 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.7059805393218994, loss=1.2318869829177856
I0219 20:29:52.139947 140468535932672 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.751704216003418, loss=1.3051191568374634
I0219 20:31:13.214816 140483498055424 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.619735836982727, loss=1.310526967048645
I0219 20:32:33.524530 140468535932672 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.6162142753601074, loss=1.306675910949707
I0219 20:33:56.979519 140483498055424 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.711955189704895, loss=1.2625269889831543
I0219 20:35:12.691055 140468535932672 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.6759856939315796, loss=1.300394892692566
I0219 20:36:28.163569 140483498055424 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.683463454246521, loss=1.2793387174606323
I0219 20:37:43.664707 140468535932672 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.7346124649047852, loss=1.3138614892959595
I0219 20:38:59.085678 140483498055424 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.8062736392021179, loss=1.3023853302001953
I0219 20:40:14.597984 140468535932672 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.6557076573371887, loss=1.2562897205352783
I0219 20:41:30.148666 140483498055424 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.6697028279304504, loss=1.2733280658721924
I0219 20:42:48.071838 140468535932672 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.7772340178489685, loss=1.264451265335083
I0219 20:44:08.517769 140483498055424 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.6453476548194885, loss=1.2641745805740356
I0219 20:44:08.996080 140549388556096 spec.py:321] Evaluating on the training split.
I0219 20:45:03.949682 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 20:45:54.425656 140549388556096 spec.py:349] Evaluating on the test split.
I0219 20:46:20.303937 140549388556096 submission_runner.py:408] Time since start: 26831.63s, 	Step: 31702, 	{'train/ctc_loss': Array(0.19798476, dtype=float32), 'train/wer': 0.07470515976992709, 'validation/ctc_loss': Array(0.46828082, dtype=float32), 'validation/wer': 0.14064898577869606, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26807746, dtype=float32), 'test/wer': 0.09170678203643898, 'test/num_examples': 2472, 'score': 24517.084721565247, 'total_duration': 26831.625014066696, 'accumulated_submission_time': 24517.084721565247, 'accumulated_eval_time': 2312.3158464431763, 'accumulated_logging_time': 0.8909256458282471}
I0219 20:46:20.343995 140483498055424 logging_writer.py:48] [31702] accumulated_eval_time=2312.315846, accumulated_logging_time=0.890926, accumulated_submission_time=24517.084722, global_step=31702, preemption_count=0, score=24517.084722, test/ctc_loss=0.26807746291160583, test/num_examples=2472, test/wer=0.091707, total_duration=26831.625014, train/ctc_loss=0.1979847550392151, train/wer=0.074705, validation/ctc_loss=0.4682808220386505, validation/num_examples=5348, validation/wer=0.140649
I0219 20:47:34.842445 140468535932672 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.6532748341560364, loss=1.3240315914154053
I0219 20:48:50.289632 140483498055424 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.7644696235656738, loss=1.319866418838501
I0219 20:50:09.248685 140483498055424 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.8894231915473938, loss=1.2694588899612427
I0219 20:51:24.843057 140468535932672 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.581169605255127, loss=1.276666522026062
I0219 20:52:40.772552 140483498055424 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.6957264542579651, loss=1.2552249431610107
I0219 20:53:56.323190 140468535932672 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.6927949786186218, loss=1.2437604665756226
I0219 20:55:11.892654 140483498055424 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.6769993901252747, loss=1.3030143976211548
I0219 20:56:27.369740 140468535932672 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.6106598377227783, loss=1.2246123552322388
I0219 20:57:44.999310 140483498055424 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.7135101556777954, loss=1.2241371870040894
I0219 20:59:05.874982 140468535932672 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.6532447934150696, loss=1.2904026508331299
I0219 21:00:26.200633 140483498055424 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.5936734676361084, loss=1.2473597526550293
I0219 21:01:46.221056 140468535932672 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.7596026062965393, loss=1.3099761009216309
I0219 21:03:07.986016 140483498055424 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.7412244081497192, loss=1.224764108657837
I0219 21:04:23.506275 140468535932672 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.6969168782234192, loss=1.214577555656433
I0219 21:05:38.946565 140483498055424 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.6766144633293152, loss=1.2400798797607422
I0219 21:06:54.833046 140468535932672 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.7840487360954285, loss=1.2804104089736938
I0219 21:08:10.349029 140483498055424 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.8359874486923218, loss=1.2345362901687622
I0219 21:09:25.900278 140468535932672 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.6179046630859375, loss=1.2282541990280151
I0219 21:10:20.635512 140549388556096 spec.py:321] Evaluating on the training split.
I0219 21:11:14.122204 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 21:12:04.844739 140549388556096 spec.py:349] Evaluating on the test split.
I0219 21:12:30.612607 140549388556096 submission_runner.py:408] Time since start: 28401.93s, 	Step: 33574, 	{'train/ctc_loss': Array(0.23919919, dtype=float32), 'train/wer': 0.08453729330585567, 'validation/ctc_loss': Array(0.46265563, dtype=float32), 'validation/wer': 0.1399828147175531, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26978704, dtype=float32), 'test/wer': 0.09255986838096399, 'test/num_examples': 2472, 'score': 25957.286108970642, 'total_duration': 28401.9341943264, 'accumulated_submission_time': 25957.286108970642, 'accumulated_eval_time': 2442.287088871002, 'accumulated_logging_time': 0.9477226734161377}
I0219 21:12:30.650377 140483498055424 logging_writer.py:48] [33574] accumulated_eval_time=2442.287089, accumulated_logging_time=0.947723, accumulated_submission_time=25957.286109, global_step=33574, preemption_count=0, score=25957.286109, test/ctc_loss=0.2697870433330536, test/num_examples=2472, test/wer=0.092560, total_duration=28401.934194, train/ctc_loss=0.2391991913318634, train/wer=0.084537, validation/ctc_loss=0.462655633687973, validation/num_examples=5348, validation/wer=0.139983
I0219 21:12:51.042041 140468535932672 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.7862154841423035, loss=1.2192052602767944
I0219 21:14:06.581005 140483498055424 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.7857326865196228, loss=1.2479922771453857
I0219 21:15:22.221069 140468535932672 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.7564413547515869, loss=1.251155972480774
I0219 21:16:37.896956 140483498055424 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.7922645211219788, loss=1.2569409608840942
I0219 21:17:56.974138 140483498055424 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.6894117593765259, loss=1.202802300453186
I0219 21:19:12.463844 140468535932672 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.7917224764823914, loss=1.2738761901855469
I0219 21:20:28.022805 140483498055424 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.68654465675354, loss=1.1885677576065063
I0219 21:21:43.552594 140468535932672 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.7118040323257446, loss=1.2365992069244385
I0219 21:22:59.099824 140483498055424 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.71363365650177, loss=1.2643473148345947
I0219 21:24:14.890503 140468535932672 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.725003182888031, loss=1.1801559925079346
I0219 21:25:30.455668 140483498055424 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.7285880446434021, loss=1.2652651071548462
I0219 21:26:47.369967 140468535932672 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.7756826877593994, loss=1.2630656957626343
I0219 21:28:08.328918 140483498055424 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.6843972206115723, loss=1.1935498714447021
I0219 21:29:28.539560 140468535932672 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.663427472114563, loss=1.2556145191192627
I0219 21:30:49.157694 140483498055424 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.6029120087623596, loss=1.1899440288543701
I0219 21:32:08.923612 140483498055424 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.7192015647888184, loss=1.3141813278198242
I0219 21:33:24.628167 140468535932672 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.6771759390830994, loss=1.2249746322631836
I0219 21:34:40.322085 140483498055424 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.7290199995040894, loss=1.2327663898468018
I0219 21:35:56.017166 140468535932672 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.7049540877342224, loss=1.2127454280853271
I0219 21:36:31.204388 140549388556096 spec.py:321] Evaluating on the training split.
I0219 21:37:25.945358 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 21:38:16.906063 140549388556096 spec.py:349] Evaluating on the test split.
I0219 21:38:42.539561 140549388556096 submission_runner.py:408] Time since start: 29973.86s, 	Step: 35448, 	{'train/ctc_loss': Array(0.19585215, dtype=float32), 'train/wer': 0.0732812986505813, 'validation/ctc_loss': Array(0.45211855, dtype=float32), 'validation/wer': 0.13498170443245122, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2617301, dtype=float32), 'test/wer': 0.08855848719354904, 'test/num_examples': 2472, 'score': 27397.751355171204, 'total_duration': 29973.861132621765, 'accumulated_submission_time': 27397.751355171204, 'accumulated_eval_time': 2573.6164152622223, 'accumulated_logging_time': 1.000833511352539}
I0219 21:38:42.577574 140483498055424 logging_writer.py:48] [35448] accumulated_eval_time=2573.616415, accumulated_logging_time=1.000834, accumulated_submission_time=27397.751355, global_step=35448, preemption_count=0, score=27397.751355, test/ctc_loss=0.2617301046848297, test/num_examples=2472, test/wer=0.088558, total_duration=29973.861133, train/ctc_loss=0.1958521455526352, train/wer=0.073281, validation/ctc_loss=0.45211854577064514, validation/num_examples=5348, validation/wer=0.134982
I0219 21:39:22.553867 140468535932672 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.6760159134864807, loss=1.2133114337921143
I0219 21:40:38.426576 140483498055424 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.7069339752197266, loss=1.2799538373947144
I0219 21:41:54.187550 140468535932672 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.6878892183303833, loss=1.2294172048568726
I0219 21:43:10.005780 140483498055424 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.7500647306442261, loss=1.28330659866333
I0219 21:44:25.561657 140468535932672 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.7643725872039795, loss=1.2347720861434937
I0219 21:45:41.163734 140483498055424 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.7579160928726196, loss=1.1436200141906738
I0219 21:47:00.143659 140483498055424 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.7650157809257507, loss=1.1886149644851685
I0219 21:48:15.784873 140468535932672 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.7591953277587891, loss=1.2018637657165527
I0219 21:49:31.194180 140483498055424 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.7225894927978516, loss=1.2193206548690796
I0219 21:50:46.770176 140468535932672 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.7297614216804504, loss=1.2483456134796143
I0219 21:52:02.369292 140483498055424 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.6961604952812195, loss=1.255609154701233
I0219 21:53:17.852940 140468535932672 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.9516153335571289, loss=1.224479079246521
I0219 21:54:33.271663 140483498055424 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.1366593837738037, loss=1.2142956256866455
I0219 21:55:51.235885 140468535932672 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.7618499994277954, loss=1.2316709756851196
I0219 21:57:11.333299 140483498055424 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.7573829889297485, loss=1.2390024662017822
I0219 21:58:31.394105 140468535932672 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.6820051074028015, loss=1.2658312320709229
I0219 21:59:54.500451 140483498055424 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.7619693875312805, loss=1.186936616897583
I0219 22:01:09.925783 140468535932672 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.7043473720550537, loss=1.2311275005340576
I0219 22:02:25.413160 140483498055424 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.7169492244720459, loss=1.2596044540405273
I0219 22:02:43.220789 140549388556096 spec.py:321] Evaluating on the training split.
I0219 22:03:37.092445 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 22:04:28.011476 140549388556096 spec.py:349] Evaluating on the test split.
I0219 22:04:53.515197 140549388556096 submission_runner.py:408] Time since start: 31544.84s, 	Step: 37325, 	{'train/ctc_loss': Array(0.17662221, dtype=float32), 'train/wer': 0.06877232740034307, 'validation/ctc_loss': Array(0.4406018, dtype=float32), 'validation/wer': 0.13279009818782161, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25678006, dtype=float32), 'test/wer': 0.08654764081002579, 'test/num_examples': 2472, 'score': 28838.30406999588, 'total_duration': 31544.836877584457, 'accumulated_submission_time': 28838.30406999588, 'accumulated_eval_time': 2703.9050781726837, 'accumulated_logging_time': 1.056443452835083}
I0219 22:04:53.552573 140483498055424 logging_writer.py:48] [37325] accumulated_eval_time=2703.905078, accumulated_logging_time=1.056443, accumulated_submission_time=28838.304070, global_step=37325, preemption_count=0, score=28838.304070, test/ctc_loss=0.25678005814552307, test/num_examples=2472, test/wer=0.086548, total_duration=31544.836878, train/ctc_loss=0.176622211933136, train/wer=0.068772, validation/ctc_loss=0.44060179591178894, validation/num_examples=5348, validation/wer=0.132790
I0219 22:05:50.693847 140468535932672 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.6315016746520996, loss=1.2190889120101929
I0219 22:07:06.275446 140483498055424 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.7030597925186157, loss=1.1726841926574707
I0219 22:08:21.820345 140468535932672 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.7063753604888916, loss=1.2307003736495972
I0219 22:09:37.424503 140483498055424 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.7791442275047302, loss=1.2404711246490479
I0219 22:10:53.050163 140468535932672 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.7638037800788879, loss=1.2216848134994507
I0219 22:12:08.889673 140483498055424 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.754817008972168, loss=1.2532192468643188
I0219 22:13:25.629464 140468535932672 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.7894505858421326, loss=1.2435113191604614
I0219 22:14:46.292047 140483498055424 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.7215794920921326, loss=1.2274264097213745
I0219 22:16:05.382203 140483498055424 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.8036486506462097, loss=1.2629886865615845
I0219 22:17:20.915207 140468535932672 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.7473581433296204, loss=1.1804468631744385
I0219 22:18:36.537058 140483498055424 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.8890504837036133, loss=1.1843574047088623
I0219 22:19:52.158543 140468535932672 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.7461983561515808, loss=1.1855741739273071
I0219 22:21:07.655136 140483498055424 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.7710806131362915, loss=1.2354674339294434
I0219 22:22:23.247761 140468535932672 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.7930903434753418, loss=1.2802236080169678
I0219 22:23:38.818283 140483498055424 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.7488219738006592, loss=1.204544186592102
I0219 22:24:58.883038 140468535932672 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.7031322121620178, loss=1.232696294784546
I0219 22:26:18.781016 140483498055424 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.8280509114265442, loss=1.209647297859192
I0219 22:27:39.596825 140468535932672 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.7073158621788025, loss=1.2449368238449097
I0219 22:28:54.036769 140549388556096 spec.py:321] Evaluating on the training split.
I0219 22:29:48.635156 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 22:30:39.420252 140549388556096 spec.py:349] Evaluating on the test split.
I0219 22:31:05.133554 140549388556096 submission_runner.py:408] Time since start: 33116.45s, 	Step: 39194, 	{'train/ctc_loss': Array(0.17059627, dtype=float32), 'train/wer': 0.0640816435006777, 'validation/ctc_loss': Array(0.43647674, dtype=float32), 'validation/wer': 0.1290440928005252, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.250659, dtype=float32), 'test/wer': 0.08421180915239779, 'test/num_examples': 2472, 'score': 30278.698662042618, 'total_duration': 33116.45455908775, 'accumulated_submission_time': 30278.698662042618, 'accumulated_eval_time': 2834.99561214447, 'accumulated_logging_time': 1.10945463180542}
I0219 22:31:05.173048 140483498055424 logging_writer.py:48] [39194] accumulated_eval_time=2834.995612, accumulated_logging_time=1.109455, accumulated_submission_time=30278.698662, global_step=39194, preemption_count=0, score=30278.698662, test/ctc_loss=0.2506589889526367, test/num_examples=2472, test/wer=0.084212, total_duration=33116.454559, train/ctc_loss=0.17059627175331116, train/wer=0.064082, validation/ctc_loss=0.4364767372608185, validation/num_examples=5348, validation/wer=0.129044
I0219 22:31:10.522372 140468535932672 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.7203894257545471, loss=1.2062046527862549
I0219 22:32:25.790324 140483498055424 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.7708855867385864, loss=1.179682970046997
I0219 22:33:41.192721 140468535932672 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.7321107983589172, loss=1.1472028493881226
I0219 22:34:56.719468 140483498055424 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.7232341170310974, loss=1.2175943851470947
I0219 22:36:12.247880 140468535932672 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.8444343209266663, loss=1.2011346817016602
I0219 22:37:27.778919 140483498055424 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.7403905987739563, loss=1.169589638710022
I0219 22:38:43.329499 140468535932672 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.6927232146263123, loss=1.196704626083374
I0219 22:39:58.884797 140483498055424 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.8605897426605225, loss=1.2128263711929321
I0219 22:41:16.424912 140468535932672 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.7315542697906494, loss=1.1853218078613281
I0219 22:42:36.533278 140483498055424 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.6378200650215149, loss=1.2524900436401367
I0219 22:43:58.302185 140483498055424 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.6800388693809509, loss=1.204840898513794
I0219 22:45:13.905959 140468535932672 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.6383183598518372, loss=1.1543421745300293
I0219 22:46:29.456069 140483498055424 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.7563657760620117, loss=1.1535741090774536
I0219 22:47:45.088137 140468535932672 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.768630862236023, loss=1.1288241147994995
I0219 22:49:00.729050 140483498055424 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.6871689558029175, loss=1.1933187246322632
I0219 22:50:16.358900 140468535932672 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.7827848196029663, loss=1.1818054914474487
I0219 22:51:31.945925 140483498055424 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.727925717830658, loss=1.1902366876602173
I0219 22:52:48.990024 140468535932672 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.8036652207374573, loss=1.2207831144332886
I0219 22:54:09.696755 140483498055424 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.7458382844924927, loss=1.188480019569397
I0219 22:55:05.287556 140549388556096 spec.py:321] Evaluating on the training split.
I0219 22:55:59.429921 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 22:56:50.496369 140549388556096 spec.py:349] Evaluating on the test split.
I0219 22:57:16.580382 140549388556096 submission_runner.py:408] Time since start: 34687.90s, 	Step: 41071, 	{'train/ctc_loss': Array(0.16845806, dtype=float32), 'train/wer': 0.06581072351421188, 'validation/ctc_loss': Array(0.4294395, dtype=float32), 'validation/wer': 0.12859032410670324, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24354605, dtype=float32), 'test/wer': 0.08370401966160908, 'test/num_examples': 2472, 'score': 31718.723798036575, 'total_duration': 34687.90142393112, 'accumulated_submission_time': 31718.723798036575, 'accumulated_eval_time': 2966.2820715904236, 'accumulated_logging_time': 1.1651394367218018}
I0219 22:57:16.619844 140483498055424 logging_writer.py:48] [41071] accumulated_eval_time=2966.282072, accumulated_logging_time=1.165139, accumulated_submission_time=31718.723798, global_step=41071, preemption_count=0, score=31718.723798, test/ctc_loss=0.24354605376720428, test/num_examples=2472, test/wer=0.083704, total_duration=34687.901424, train/ctc_loss=0.16845805943012238, train/wer=0.065811, validation/ctc_loss=0.429439514875412, validation/num_examples=5348, validation/wer=0.128590
I0219 22:57:39.271734 140468535932672 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.8479589819908142, loss=1.1563981771469116
I0219 22:58:58.412412 140483498055424 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.7424564957618713, loss=1.1800450086593628
I0219 23:00:14.131891 140468535932672 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.7751192450523376, loss=1.2108428478240967
I0219 23:01:29.636010 140483498055424 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.8293225169181824, loss=1.1838492155075073
I0219 23:02:45.174893 140468535932672 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.955231249332428, loss=1.1761401891708374
I0219 23:04:00.638033 140483498055424 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.9124417304992676, loss=1.2118951082229614
I0219 23:05:16.161565 140468535932672 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.7707203030586243, loss=1.1585170030593872
I0219 23:06:31.908707 140483498055424 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.7330936789512634, loss=1.1735056638717651
I0219 23:07:49.797934 140468535932672 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.685606837272644, loss=1.158259630203247
I0219 23:09:10.825783 140483498055424 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.8165775537490845, loss=1.1442490816116333
I0219 23:10:31.991136 140468535932672 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.9220210313796997, loss=1.2086735963821411
I0219 23:11:52.332243 140483498055424 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.6638689637184143, loss=1.1162810325622559
I0219 23:13:12.417083 140483498055424 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.689283549785614, loss=1.1584312915802002
I0219 23:14:27.861034 140468535932672 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.8161062598228455, loss=1.2177180051803589
I0219 23:15:43.580818 140483498055424 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.7778396010398865, loss=1.190533995628357
I0219 23:16:59.101464 140468535932672 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.9474076628684998, loss=1.1599280834197998
I0219 23:18:14.633360 140483498055424 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.7908629775047302, loss=1.1542917490005493
I0219 23:19:30.195521 140468535932672 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.7650203704833984, loss=1.2008588314056396
I0219 23:20:45.713221 140483498055424 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.6637027263641357, loss=1.167481780052185
I0219 23:21:17.319197 140549388556096 spec.py:321] Evaluating on the training split.
I0219 23:22:13.121669 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 23:23:04.643754 140549388556096 spec.py:349] Evaluating on the test split.
I0219 23:23:30.577728 140549388556096 submission_runner.py:408] Time since start: 36261.90s, 	Step: 42942, 	{'train/ctc_loss': Array(0.1750182, dtype=float32), 'train/wer': 0.06475229077741695, 'validation/ctc_loss': Array(0.421468, dtype=float32), 'validation/wer': 0.12547187116830957, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23960043, dtype=float32), 'test/wer': 0.07893079844819531, 'test/num_examples': 2472, 'score': 33159.33412575722, 'total_duration': 36261.89895486832, 'accumulated_submission_time': 33159.33412575722, 'accumulated_eval_time': 3099.5344285964966, 'accumulated_logging_time': 1.2207741737365723}
I0219 23:23:30.616309 140483498055424 logging_writer.py:48] [42942] accumulated_eval_time=3099.534429, accumulated_logging_time=1.220774, accumulated_submission_time=33159.334126, global_step=42942, preemption_count=0, score=33159.334126, test/ctc_loss=0.23960043489933014, test/num_examples=2472, test/wer=0.078931, total_duration=36261.898955, train/ctc_loss=0.17501820623874664, train/wer=0.064752, validation/ctc_loss=0.42146798968315125, validation/num_examples=5348, validation/wer=0.125472
I0219 23:24:15.075605 140468535932672 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.7282260060310364, loss=1.1539654731750488
I0219 23:25:30.603009 140483498055424 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.7334312200546265, loss=1.1409623622894287
I0219 23:26:46.229272 140468535932672 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.6654841899871826, loss=1.178551435470581
I0219 23:28:05.128350 140483498055424 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.8117637634277344, loss=1.0917519330978394
I0219 23:29:20.575237 140468535932672 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.7387149930000305, loss=1.1146986484527588
I0219 23:30:35.991448 140483498055424 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.8351041674613953, loss=1.1589041948318481
I0219 23:31:51.775344 140468535932672 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.7241296768188477, loss=1.153947114944458
I0219 23:33:07.422597 140483498055424 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.7630202770233154, loss=1.1693906784057617
I0219 23:34:22.924392 140468535932672 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.6478795409202576, loss=1.119238018989563
I0219 23:35:38.355473 140483498055424 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.7960302233695984, loss=1.2011024951934814
I0219 23:36:54.499255 140468535932672 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.8022448420524597, loss=1.1601173877716064
I0219 23:38:14.892099 140483498055424 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.8104888200759888, loss=1.1910717487335205
I0219 23:39:34.102428 140468535932672 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.8825820088386536, loss=1.1764240264892578
I0219 23:40:57.313096 140483498055424 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.7924962043762207, loss=1.1282001733779907
I0219 23:42:12.854740 140468535932672 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.8097977042198181, loss=1.1586698293685913
I0219 23:43:28.516120 140483498055424 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.7838220596313477, loss=1.1674489974975586
I0219 23:44:44.179806 140468535932672 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.7840177416801453, loss=1.1783446073532104
I0219 23:45:59.790854 140483498055424 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.8742468953132629, loss=1.1768097877502441
I0219 23:47:15.608125 140468535932672 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.7266768217086792, loss=1.103268027305603
I0219 23:47:31.175622 140549388556096 spec.py:321] Evaluating on the training split.
I0219 23:48:26.328592 140549388556096 spec.py:333] Evaluating on the validation split.
I0219 23:49:17.512129 140549388556096 spec.py:349] Evaluating on the test split.
I0219 23:49:43.622842 140549388556096 submission_runner.py:408] Time since start: 37834.94s, 	Step: 44822, 	{'train/ctc_loss': Array(0.1670249, dtype=float32), 'train/wer': 0.06235486775523529, 'validation/ctc_loss': Array(0.40870935, dtype=float32), 'validation/wer': 0.12115624125047067, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23448966, dtype=float32), 'test/wer': 0.07860581317409054, 'test/num_examples': 2472, 'score': 34599.805364370346, 'total_duration': 37834.94379091263, 'accumulated_submission_time': 34599.805364370346, 'accumulated_eval_time': 3231.975162744522, 'accumulated_logging_time': 1.27516770362854}
I0219 23:49:43.664185 140483498055424 logging_writer.py:48] [44822] accumulated_eval_time=3231.975163, accumulated_logging_time=1.275168, accumulated_submission_time=34599.805364, global_step=44822, preemption_count=0, score=34599.805364, test/ctc_loss=0.23448966443538666, test/num_examples=2472, test/wer=0.078606, total_duration=37834.943791, train/ctc_loss=0.1670248955488205, train/wer=0.062355, validation/ctc_loss=0.4087093472480774, validation/num_examples=5348, validation/wer=0.121156
I0219 23:50:43.128624 140468535932672 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.7219350337982178, loss=1.1184779405593872
I0219 23:51:58.546649 140483498055424 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.7208627462387085, loss=1.1378930807113647
I0219 23:53:14.118212 140468535932672 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.8872061371803284, loss=1.2116906642913818
I0219 23:54:29.708844 140483498055424 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.7980471253395081, loss=1.1574808359146118
I0219 23:55:45.292269 140468535932672 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.7094839215278625, loss=1.2023869752883911
I0219 23:57:04.650665 140483498055424 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.7262750864028931, loss=1.1482810974121094
I0219 23:58:20.321331 140468535932672 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.7607215642929077, loss=1.1072261333465576
I0219 23:59:35.975619 140483498055424 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.8245161175727844, loss=1.1623588800430298
I0220 00:00:51.614871 140468535932672 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.771967887878418, loss=1.1340594291687012
I0220 00:02:07.267412 140483498055424 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.7013545036315918, loss=1.1131582260131836
I0220 00:03:23.282255 140468535932672 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.9118769764900208, loss=1.1452546119689941
I0220 00:04:38.898394 140483498055424 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.7187868356704712, loss=1.1641945838928223
I0220 00:05:57.020658 140468535932672 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.8318958878517151, loss=1.1152105331420898
I0220 00:07:17.270970 140483498055424 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.7713646292686462, loss=1.1367053985595703
I0220 00:08:37.678199 140468535932672 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.842097818851471, loss=1.1427371501922607
I0220 00:09:59.380241 140483498055424 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.7530662417411804, loss=1.1399221420288086
I0220 00:11:14.897526 140468535932672 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.894786536693573, loss=1.203681468963623
I0220 00:12:30.420656 140483498055424 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.8249863386154175, loss=1.1467558145523071
I0220 00:13:43.916790 140549388556096 spec.py:321] Evaluating on the training split.
I0220 00:14:38.819310 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 00:15:29.901128 140549388556096 spec.py:349] Evaluating on the test split.
I0220 00:15:55.378188 140549388556096 submission_runner.py:408] Time since start: 39406.70s, 	Step: 46699, 	{'train/ctc_loss': Array(0.15492408, dtype=float32), 'train/wer': 0.05874819830562958, 'validation/ctc_loss': Array(0.40402222, dtype=float32), 'validation/wer': 0.12081832839336919, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2308128, dtype=float32), 'test/wer': 0.07746836471472386, 'test/num_examples': 2472, 'score': 36039.968794584274, 'total_duration': 39406.69979095459, 'accumulated_submission_time': 36039.968794584274, 'accumulated_eval_time': 3363.430745601654, 'accumulated_logging_time': 1.3329179286956787}
I0220 00:15:55.422818 140483498055424 logging_writer.py:48] [46699] accumulated_eval_time=3363.430746, accumulated_logging_time=1.332918, accumulated_submission_time=36039.968795, global_step=46699, preemption_count=0, score=36039.968795, test/ctc_loss=0.23081280291080475, test/num_examples=2472, test/wer=0.077468, total_duration=39406.699791, train/ctc_loss=0.15492407977581024, train/wer=0.058748, validation/ctc_loss=0.404022216796875, validation/num_examples=5348, validation/wer=0.120818
I0220 00:15:57.036208 140468535932672 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.777321457862854, loss=1.1110727787017822
I0220 00:17:12.224109 140483498055424 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.7548770308494568, loss=1.0877573490142822
I0220 00:18:27.648620 140468535932672 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.7771121263504028, loss=1.0993939638137817
I0220 00:19:43.048750 140483498055424 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.8007734417915344, loss=1.1472861766815186
I0220 00:20:58.806538 140468535932672 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.7871223092079163, loss=1.189665675163269
I0220 00:22:14.239366 140483498055424 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.7333071231842041, loss=1.1517267227172852
I0220 00:23:31.696659 140468535932672 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.7748425602912903, loss=1.1428622007369995
I0220 00:24:54.235698 140483498055424 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.737237811088562, loss=1.1251482963562012
I0220 00:26:09.542356 140468535932672 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.8427867889404297, loss=1.170125126838684
I0220 00:27:24.928674 140483498055424 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.7483215928077698, loss=1.0879099369049072
I0220 00:28:40.433675 140468535932672 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.01102876663208, loss=1.1490074396133423
I0220 00:29:55.939610 140483498055424 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.890212893486023, loss=1.1231070756912231
I0220 00:31:11.548816 140468535932672 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.7531632781028748, loss=1.117020606994629
I0220 00:32:27.033150 140483498055424 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.0049290657043457, loss=1.0742756128311157
I0220 00:33:45.506593 140468535932672 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.8539085388183594, loss=1.0587595701217651
I0220 00:35:06.018158 140483498055424 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.9472019672393799, loss=1.1850394010543823
I0220 00:36:25.835457 140468535932672 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.8175389766693115, loss=1.1448824405670166
I0220 00:37:45.547282 140483498055424 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.7314264178276062, loss=1.1252723932266235
I0220 00:39:04.667101 140483498055424 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.8516778945922852, loss=1.0907113552093506
I0220 00:39:55.603502 140549388556096 spec.py:321] Evaluating on the training split.
I0220 00:40:49.985881 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 00:41:40.314362 140549388556096 spec.py:349] Evaluating on the test split.
I0220 00:42:06.161901 140549388556096 submission_runner.py:408] Time since start: 40977.48s, 	Step: 48569, 	{'train/ctc_loss': Array(0.14281599, dtype=float32), 'train/wer': 0.05467450313684792, 'validation/ctc_loss': Array(0.3916889, dtype=float32), 'validation/wer': 0.11527655753690491, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22736883, dtype=float32), 'test/wer': 0.07564032254788455, 'test/num_examples': 2472, 'score': 37480.06201171875, 'total_duration': 40977.48327946663, 'accumulated_submission_time': 37480.06201171875, 'accumulated_eval_time': 3493.9831075668335, 'accumulated_logging_time': 1.3930652141571045}
I0220 00:42:06.202802 140483498055424 logging_writer.py:48] [48569] accumulated_eval_time=3493.983108, accumulated_logging_time=1.393065, accumulated_submission_time=37480.062012, global_step=48569, preemption_count=0, score=37480.062012, test/ctc_loss=0.22736883163452148, test/num_examples=2472, test/wer=0.075640, total_duration=40977.483279, train/ctc_loss=0.1428159922361374, train/wer=0.054675, validation/ctc_loss=0.39168891310691833, validation/num_examples=5348, validation/wer=0.115277
I0220 00:42:30.333425 140468535932672 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.7152833342552185, loss=1.0716078281402588
I0220 00:43:45.903352 140483498055424 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.6552494168281555, loss=1.07802414894104
I0220 00:45:01.490705 140468535932672 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.8807591199874878, loss=1.09141206741333
I0220 00:46:17.149973 140483498055424 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.8531669974327087, loss=1.1561219692230225
I0220 00:47:32.855703 140468535932672 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.8022732734680176, loss=1.1067465543746948
I0220 00:48:48.497935 140483498055424 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.8338615894317627, loss=1.0922342538833618
I0220 00:50:03.995448 140468535932672 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.7578126788139343, loss=1.121860384941101
I0220 00:51:19.962392 140483498055424 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.6986198425292969, loss=1.1032510995864868
I0220 00:52:39.234823 140468535932672 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.7535616755485535, loss=1.0619231462478638
I0220 00:53:59.648209 140483498055424 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.7093331813812256, loss=1.0727514028549194
I0220 00:55:15.183123 140468535932672 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.8715235590934753, loss=1.0765730142593384
I0220 00:56:30.659299 140483498055424 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.8760876059532166, loss=1.102016806602478
I0220 00:57:46.151843 140468535932672 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.7944216132164001, loss=1.1008330583572388
I0220 00:59:01.613593 140483498055424 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.7454066276550293, loss=1.0756174325942993
I0220 01:00:17.257091 140468535932672 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.8541813492774963, loss=1.0877772569656372
I0220 01:01:32.787264 140483498055424 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.9386515021324158, loss=1.0635226964950562
I0220 01:02:49.700944 140468535932672 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.9955353140830994, loss=1.1134380102157593
I0220 01:04:09.078848 140483498055424 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.7965379953384399, loss=1.069395661354065
I0220 01:05:30.271799 140468535932672 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.826542317867279, loss=1.0788975954055786
I0220 01:06:06.636270 140549388556096 spec.py:321] Evaluating on the training split.
I0220 01:07:02.309949 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 01:07:53.550689 140549388556096 spec.py:349] Evaluating on the test split.
I0220 01:08:19.051643 140549388556096 submission_runner.py:408] Time since start: 42550.37s, 	Step: 50447, 	{'train/ctc_loss': Array(0.12362996, dtype=float32), 'train/wer': 0.04713523789406981, 'validation/ctc_loss': Array(0.38162008, dtype=float32), 'validation/wer': 0.11288220357801443, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21504119, dtype=float32), 'test/wer': 0.07153738346231187, 'test/num_examples': 2472, 'score': 38920.40566134453, 'total_duration': 42550.372653484344, 'accumulated_submission_time': 38920.40566134453, 'accumulated_eval_time': 3626.3920748233795, 'accumulated_logging_time': 1.450796365737915}
I0220 01:08:19.097871 140483498055424 logging_writer.py:48] [50447] accumulated_eval_time=3626.392075, accumulated_logging_time=1.450796, accumulated_submission_time=38920.405661, global_step=50447, preemption_count=0, score=38920.405661, test/ctc_loss=0.21504119038581848, test/num_examples=2472, test/wer=0.071537, total_duration=42550.372653, train/ctc_loss=0.12362995743751526, train/wer=0.047135, validation/ctc_loss=0.3816200792789459, validation/num_examples=5348, validation/wer=0.112882
I0220 01:09:03.264675 140483498055424 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.8073696494102478, loss=1.1147276163101196
I0220 01:10:18.901091 140468535932672 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.9007037878036499, loss=1.0882914066314697
I0220 01:11:34.721876 140483498055424 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.8202153444290161, loss=1.0420653820037842
I0220 01:12:50.411783 140468535932672 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.846013605594635, loss=1.110628366470337
I0220 01:14:05.984585 140483498055424 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.7702325582504272, loss=1.0231735706329346
I0220 01:15:21.629097 140468535932672 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.7646862864494324, loss=1.0602891445159912
I0220 01:16:37.233452 140483498055424 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.7719753384590149, loss=1.085946798324585
I0220 01:17:53.603985 140468535932672 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.8456193804740906, loss=1.0853102207183838
I0220 01:19:13.785196 140483498055424 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.8116633296012878, loss=1.1401844024658203
I0220 01:20:33.681224 140468535932672 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.8413500189781189, loss=1.0886352062225342
I0220 01:21:56.868103 140483498055424 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.8077006936073303, loss=1.0682616233825684
I0220 01:23:12.702200 140468535932672 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.8552022576332092, loss=1.0728869438171387
I0220 01:24:28.156200 140483498055424 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.9311943650245667, loss=1.1056445837020874
I0220 01:25:43.697210 140468535932672 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.0631526708602905, loss=1.0730804204940796
I0220 01:26:59.220031 140483498055424 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.9342634677886963, loss=1.0852442979812622
I0220 01:28:14.853612 140468535932672 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.8994084000587463, loss=1.0742177963256836
I0220 01:29:30.374741 140483498055424 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.8146659731864929, loss=1.163367509841919
I0220 01:30:45.852550 140468535932672 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.8174383640289307, loss=1.0742650032043457
I0220 01:32:05.413168 140483498055424 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.8278018236160278, loss=1.0492123365402222
I0220 01:32:19.316832 140549388556096 spec.py:321] Evaluating on the training split.
I0220 01:33:15.461069 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 01:34:06.450609 140549388556096 spec.py:349] Evaluating on the test split.
I0220 01:34:32.186575 140549388556096 submission_runner.py:408] Time since start: 44123.51s, 	Step: 52319, 	{'train/ctc_loss': Array(0.12619652, dtype=float32), 'train/wer': 0.04762634466272334, 'validation/ctc_loss': Array(0.37294978, dtype=float32), 'validation/wer': 0.11031406586404317, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21084793, dtype=float32), 'test/wer': 0.06954684865842016, 'test/num_examples': 2472, 'score': 40360.535601854324, 'total_duration': 44123.50813269615, 'accumulated_submission_time': 40360.535601854324, 'accumulated_eval_time': 3759.255940914154, 'accumulated_logging_time': 1.513139009475708}
I0220 01:34:32.229404 140483498055424 logging_writer.py:48] [52319] accumulated_eval_time=3759.255941, accumulated_logging_time=1.513139, accumulated_submission_time=40360.535602, global_step=52319, preemption_count=0, score=40360.535602, test/ctc_loss=0.21084792912006378, test/num_examples=2472, test/wer=0.069547, total_duration=44123.508133, train/ctc_loss=0.12619651854038239, train/wer=0.047626, validation/ctc_loss=0.3729497790336609, validation/num_examples=5348, validation/wer=0.110314
I0220 01:35:34.153654 140468535932672 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.8248915672302246, loss=1.0945581197738647
I0220 01:36:49.841093 140483498055424 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.8546769022941589, loss=1.0673037767410278
I0220 01:38:08.627927 140483498055424 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.7833077907562256, loss=1.0602504014968872
I0220 01:39:24.141307 140468535932672 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.1055549383163452, loss=1.0427988767623901
I0220 01:40:39.944755 140483498055424 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.8107311129570007, loss=1.0516471862792969
I0220 01:41:55.429147 140468535932672 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.1049200296401978, loss=1.067043423652649
I0220 01:43:11.001743 140483498055424 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.8337163329124451, loss=1.0604894161224365
I0220 01:44:26.504614 140468535932672 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.9270147085189819, loss=1.0796879529953003
I0220 01:45:42.027817 140483498055424 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.0014569759368896, loss=1.0502091646194458
I0220 01:47:01.049932 140468535932672 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.7996373772621155, loss=1.1007620096206665
I0220 01:48:21.187644 140483498055424 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.826474666595459, loss=1.0740519762039185
I0220 01:49:40.643874 140468535932672 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.9314668774604797, loss=1.0536843538284302
I0220 01:51:01.365118 140483498055424 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.8298565745353699, loss=1.0558184385299683
I0220 01:52:16.841985 140468535932672 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.9977680444717407, loss=1.0154024362564087
I0220 01:53:32.341354 140483498055424 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.9192461371421814, loss=1.0741655826568604
I0220 01:54:48.140841 140468535932672 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.8598300218582153, loss=1.0548440217971802
I0220 01:56:03.698423 140483498055424 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.834236741065979, loss=1.0880615711212158
I0220 01:57:19.248515 140468535932672 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.9255672097206116, loss=1.0548129081726074
I0220 01:58:32.882153 140549388556096 spec.py:321] Evaluating on the training split.
I0220 01:59:26.793445 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 02:00:17.792643 140549388556096 spec.py:349] Evaluating on the test split.
I0220 02:00:43.730511 140549388556096 submission_runner.py:408] Time since start: 45695.05s, 	Step: 54199, 	{'train/ctc_loss': Array(0.12776229, dtype=float32), 'train/wer': 0.04950703654682478, 'validation/ctc_loss': Array(0.36549774, dtype=float32), 'validation/wer': 0.10876932137443641, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20148818, dtype=float32), 'test/wer': 0.06747506753600227, 'test/num_examples': 2472, 'score': 41801.09794139862, 'total_duration': 45695.05196380615, 'accumulated_submission_time': 41801.09794139862, 'accumulated_eval_time': 3890.098317861557, 'accumulated_logging_time': 1.5733115673065186}
I0220 02:00:43.771809 140483498055424 logging_writer.py:48] [54199] accumulated_eval_time=3890.098318, accumulated_logging_time=1.573312, accumulated_submission_time=41801.097941, global_step=54199, preemption_count=0, score=41801.097941, test/ctc_loss=0.2014881819486618, test/num_examples=2472, test/wer=0.067475, total_duration=45695.051964, train/ctc_loss=0.12776228785514832, train/wer=0.049507, validation/ctc_loss=0.36549773812294006, validation/num_examples=5348, validation/wer=0.108769
I0220 02:00:45.396025 140468535932672 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.8935755491256714, loss=1.0745227336883545
I0220 02:02:00.809399 140483498055424 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.9054384231567383, loss=1.0875208377838135
I0220 02:03:16.401990 140468535932672 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.8454583883285522, loss=1.030674934387207
I0220 02:04:31.871463 140483498055424 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.919043779373169, loss=1.1059989929199219
I0220 02:05:50.779436 140483498055424 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.8858987092971802, loss=1.0758345127105713
I0220 02:07:06.284114 140468535932672 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.8655704855918884, loss=1.0638535022735596
I0220 02:08:21.747718 140483498055424 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.8839422464370728, loss=1.0116949081420898
I0220 02:09:37.262321 140468535932672 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.8660638332366943, loss=1.0368741750717163
I0220 02:10:52.750303 140483498055424 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.868894100189209, loss=1.0579668283462524
I0220 02:12:08.577991 140468535932672 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.2493510246276855, loss=1.0465946197509766
I0220 02:13:24.159183 140483498055424 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.9881521463394165, loss=1.0637450218200684
I0220 02:14:39.763044 140468535932672 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.9168740510940552, loss=1.042782187461853
I0220 02:15:56.082714 140483498055424 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.8847724795341492, loss=1.0300898551940918
I0220 02:17:16.372236 140468535932672 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.9108845591545105, loss=1.0325098037719727
I0220 02:18:36.966803 140483498055424 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.2827426195144653, loss=1.0427168607711792
I0220 02:19:56.451376 140483498055424 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.1341710090637207, loss=1.0405441522598267
I0220 02:21:11.904622 140468535932672 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.8436266183853149, loss=0.995448648929596
I0220 02:22:27.433933 140483498055424 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.860154390335083, loss=0.9905738234519958
I0220 02:23:42.970769 140468535932672 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.8222178220748901, loss=1.0605823993682861
I0220 02:24:44.378332 140549388556096 spec.py:321] Evaluating on the training split.
I0220 02:25:39.396931 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 02:26:29.706994 140549388556096 spec.py:349] Evaluating on the test split.
I0220 02:26:55.005030 140549388556096 submission_runner.py:408] Time since start: 47266.33s, 	Step: 56083, 	{'train/ctc_loss': Array(0.11001102, dtype=float32), 'train/wer': 0.04222769847988913, 'validation/ctc_loss': Array(0.35788244, dtype=float32), 'validation/wer': 0.10450196472189772, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19801894, dtype=float32), 'test/wer': 0.06597201064326773, 'test/num_examples': 2472, 'score': 43241.61392068863, 'total_duration': 47266.32650756836, 'accumulated_submission_time': 43241.61392068863, 'accumulated_eval_time': 4020.7191219329834, 'accumulated_logging_time': 1.631901502609253}
I0220 02:26:55.048192 140483498055424 logging_writer.py:48] [56083] accumulated_eval_time=4020.719122, accumulated_logging_time=1.631902, accumulated_submission_time=43241.613921, global_step=56083, preemption_count=0, score=43241.613921, test/ctc_loss=0.19801893830299377, test/num_examples=2472, test/wer=0.065972, total_duration=47266.326508, train/ctc_loss=0.1100110188126564, train/wer=0.042228, validation/ctc_loss=0.35788244009017944, validation/num_examples=5348, validation/wer=0.104502
I0220 02:27:08.655200 140468535932672 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.9010579586029053, loss=1.0143288373947144
I0220 02:28:24.350529 140483498055424 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.9123908281326294, loss=1.0117430686950684
I0220 02:29:39.869367 140468535932672 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.9865582585334778, loss=1.0168561935424805
I0220 02:30:55.405791 140483498055424 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.8871651887893677, loss=1.0415021181106567
I0220 02:32:11.004467 140468535932672 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.9237552285194397, loss=1.0639005899429321
I0220 02:33:26.545458 140483498055424 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.9148719310760498, loss=1.0233694314956665
I0220 02:34:45.483371 140483498055424 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.9050842523574829, loss=0.9971785545349121
I0220 02:36:00.875873 140468535932672 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.095279335975647, loss=1.0081679821014404
I0220 02:37:16.342485 140483498055424 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.896676778793335, loss=1.0046508312225342
I0220 02:38:31.825402 140468535932672 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.9045429229736328, loss=1.0927480459213257
I0220 02:39:47.344216 140483498055424 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.8717668652534485, loss=1.0519295930862427
I0220 02:41:02.897457 140468535932672 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.838554859161377, loss=1.0464531183242798
I0220 02:42:18.345995 140483498055424 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.852814793586731, loss=1.0411131381988525
I0220 02:43:36.134694 140468535932672 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.8766781091690063, loss=1.011198878288269
I0220 02:44:55.181973 140483498055424 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.1604501008987427, loss=1.0623936653137207
I0220 02:46:15.337081 140468535932672 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.8957147598266602, loss=1.0532770156860352
I0220 02:47:37.291340 140483498055424 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.1160510778427124, loss=1.0265390872955322
I0220 02:48:52.713913 140468535932672 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.9434673190116882, loss=1.0071935653686523
I0220 02:50:08.164333 140483498055424 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.9845176339149475, loss=1.070839524269104
I0220 02:50:55.342657 140549388556096 spec.py:321] Evaluating on the training split.
I0220 02:51:50.209340 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 02:52:41.539520 140549388556096 spec.py:349] Evaluating on the test split.
I0220 02:53:07.698567 140549388556096 submission_runner.py:408] Time since start: 48839.02s, 	Step: 57964, 	{'train/ctc_loss': Array(0.10896836, dtype=float32), 'train/wer': 0.043260168554203726, 'validation/ctc_loss': Array(0.34203002, dtype=float32), 'validation/wer': 0.10161522345694507, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19171545, dtype=float32), 'test/wer': 0.06424552637458615, 'test/num_examples': 2472, 'score': 44681.81542778015, 'total_duration': 48839.02103638649, 'accumulated_submission_time': 44681.81542778015, 'accumulated_eval_time': 4153.070083618164, 'accumulated_logging_time': 1.6941826343536377}
I0220 02:53:07.738701 140483498055424 logging_writer.py:48] [57964] accumulated_eval_time=4153.070084, accumulated_logging_time=1.694183, accumulated_submission_time=44681.815428, global_step=57964, preemption_count=0, score=44681.815428, test/ctc_loss=0.19171544909477234, test/num_examples=2472, test/wer=0.064246, total_duration=48839.021036, train/ctc_loss=0.10896836221218109, train/wer=0.043260, validation/ctc_loss=0.34203001856803894, validation/num_examples=5348, validation/wer=0.101615
I0220 02:53:35.633682 140468535932672 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.9891640543937683, loss=1.0653334856033325
I0220 02:54:51.060348 140483498055424 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.9716928601264954, loss=1.0539547204971313
I0220 02:56:06.527566 140468535932672 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.085313081741333, loss=1.0165449380874634
I0220 02:57:22.052284 140483498055424 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.9144553542137146, loss=0.9559184908866882
I0220 02:58:37.647985 140468535932672 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.876973569393158, loss=1.0364067554473877
I0220 02:59:53.413777 140483498055424 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.9192247986793518, loss=1.0275061130523682
I0220 03:01:08.945708 140468535932672 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.8480316996574402, loss=0.9827011823654175
I0220 03:02:29.688970 140483498055424 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.0410006046295166, loss=0.988495409488678
I0220 03:03:48.584247 140483498055424 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.8358445167541504, loss=0.9838742613792419
I0220 03:05:04.107091 140468535932672 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.9480190873146057, loss=0.9759301543235779
I0220 03:06:19.714032 140483498055424 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.3629971742630005, loss=1.0033544301986694
I0220 03:07:35.334537 140468535932672 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.9567355513572693, loss=0.9866082668304443
I0220 03:08:50.863031 140483498055424 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.0257210731506348, loss=1.0181790590286255
I0220 03:10:06.453519 140468535932672 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.0655910968780518, loss=1.0154670476913452
I0220 03:11:22.167600 140483498055424 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.8406619429588318, loss=0.9577717781066895
I0220 03:12:40.828732 140468535932672 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.3953349590301514, loss=0.9916285872459412
I0220 03:14:01.157199 140483498055424 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.9738566875457764, loss=1.0101956129074097
I0220 03:15:21.253640 140468535932672 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.089495062828064, loss=1.0089104175567627
I0220 03:16:41.453217 140483498055424 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.9365063905715942, loss=0.9792962670326233
I0220 03:17:08.268634 140549388556096 spec.py:321] Evaluating on the training split.
I0220 03:18:03.107213 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 03:18:54.031777 140549388556096 spec.py:349] Evaluating on the test split.
I0220 03:19:19.819163 140549388556096 submission_runner.py:408] Time since start: 50411.14s, 	Step: 59837, 	{'train/ctc_loss': Array(0.12385153, dtype=float32), 'train/wer': 0.044139625672146666, 'validation/ctc_loss': Array(0.3396015, dtype=float32), 'validation/wer': 0.0995201637429159, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18553233, dtype=float32), 'test/wer': 0.06199094103548433, 'test/num_examples': 2472, 'score': 46122.25700163841, 'total_duration': 50411.139713048935, 'accumulated_submission_time': 46122.25700163841, 'accumulated_eval_time': 4284.613779306412, 'accumulated_logging_time': 1.749586582183838}
I0220 03:19:19.864272 140483498055424 logging_writer.py:48] [59837] accumulated_eval_time=4284.613779, accumulated_logging_time=1.749587, accumulated_submission_time=46122.257002, global_step=59837, preemption_count=0, score=46122.257002, test/ctc_loss=0.1855323314666748, test/num_examples=2472, test/wer=0.061991, total_duration=50411.139713, train/ctc_loss=0.12385153025388718, train/wer=0.044140, validation/ctc_loss=0.3396014869213104, validation/num_examples=5348, validation/wer=0.099520
I0220 03:20:08.071934 140468535932672 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.8940497636795044, loss=0.953842043876648
I0220 03:21:23.584654 140483498055424 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.9129683971405029, loss=0.9897152781486511
I0220 03:22:39.230487 140468535932672 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.0660909414291382, loss=0.9885877370834351
I0220 03:23:54.869973 140483498055424 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.1317024230957031, loss=1.0178805589675903
I0220 03:25:10.551715 140468535932672 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.0729830265045166, loss=1.0123311281204224
I0220 03:26:26.162564 140483498055424 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.0813312530517578, loss=1.0337485074996948
I0220 03:27:41.778208 140468535932672 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.1894471645355225, loss=1.020471453666687
I0220 03:28:57.381738 140483498055424 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.1339077949523926, loss=0.9708933234214783
I0220 03:30:14.093866 140468535932672 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.9798759818077087, loss=0.9800740480422974
I0220 03:31:35.819125 140483498055424 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.1079679727554321, loss=0.9486207962036133
I0220 03:32:51.224067 140468535932672 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.0905691385269165, loss=0.9949384927749634
I0220 03:34:06.790582 140483498055424 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.4129443168640137, loss=0.9242828488349915
I0220 03:35:22.355450 140468535932672 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.0172688961029053, loss=1.0153894424438477
I0220 03:36:37.915330 140483498055424 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.0977200269699097, loss=1.0181539058685303
I0220 03:37:53.417212 140468535932672 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.055228590965271, loss=0.9840348362922668
I0220 03:39:09.018559 140483498055424 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.061369776725769, loss=0.9542081952095032
I0220 03:40:24.549871 140468535932672 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.055186152458191, loss=0.9848999381065369
I0220 03:41:43.971532 140483498055424 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.9249290227890015, loss=0.9816864728927612
I0220 03:43:04.578916 140468535932672 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.14353346824646, loss=0.9742474555969238
I0220 03:43:20.211687 140549388556096 spec.py:321] Evaluating on the training split.
I0220 03:44:16.465883 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 03:45:07.346948 140549388556096 spec.py:349] Evaluating on the test split.
I0220 03:45:33.505246 140549388556096 submission_runner.py:408] Time since start: 51984.83s, 	Step: 61721, 	{'train/ctc_loss': Array(0.07916234, dtype=float32), 'train/wer': 0.03054289351566671, 'validation/ctc_loss': Array(0.33002672, dtype=float32), 'validation/wer': 0.09577415835561949, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17721963, dtype=float32), 'test/wer': 0.059289500944488455, 'test/num_examples': 2472, 'score': 47562.51353478432, 'total_duration': 51984.82677769661, 'accumulated_submission_time': 47562.51353478432, 'accumulated_eval_time': 4417.90145611763, 'accumulated_logging_time': 1.811532735824585}
I0220 03:45:33.549082 140483498055424 logging_writer.py:48] [61721] accumulated_eval_time=4417.901456, accumulated_logging_time=1.811533, accumulated_submission_time=47562.513535, global_step=61721, preemption_count=0, score=47562.513535, test/ctc_loss=0.17721962928771973, test/num_examples=2472, test/wer=0.059290, total_duration=51984.826778, train/ctc_loss=0.0791623443365097, train/wer=0.030543, validation/ctc_loss=0.3300267159938812, validation/num_examples=5348, validation/wer=0.095774
I0220 03:46:37.246737 140483498055424 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.9750800728797913, loss=0.9708847403526306
I0220 03:47:52.586076 140468535932672 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.0586782693862915, loss=0.9611768126487732
I0220 03:49:08.386962 140483498055424 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.2124792337417603, loss=0.9577158689498901
I0220 03:50:23.806948 140468535932672 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.0926686525344849, loss=0.919460654258728
I0220 03:51:39.319082 140483498055424 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.1079070568084717, loss=0.991212010383606
I0220 03:52:54.847868 140468535932672 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.1264982223510742, loss=0.9356202483177185
I0220 03:54:10.263882 140483498055424 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.031894326210022, loss=1.0090728998184204
I0220 03:55:25.769979 140468535932672 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.9008066058158875, loss=0.9387340545654297
I0220 03:56:46.530920 140483498055424 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.0833886861801147, loss=0.9467335939407349
I0220 03:58:06.973334 140468535932672 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.2923294305801392, loss=0.9695820212364197
I0220 03:59:26.821416 140483498055424 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.1009780168533325, loss=0.9911520481109619
I0220 04:00:46.408996 140483498055424 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.077893853187561, loss=0.9236734509468079
I0220 04:02:01.878169 140468535932672 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.0685003995895386, loss=0.8941503763198853
I0220 04:03:17.724360 140483498055424 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.0180916786193848, loss=0.9437546133995056
I0220 04:04:33.270910 140468535932672 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.1010624170303345, loss=1.008998990058899
I0220 04:05:48.966697 140483498055424 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.0099236965179443, loss=0.9338642954826355
I0220 04:07:04.537978 140468535932672 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.216041088104248, loss=0.9438636898994446
I0220 04:08:20.207722 140483498055424 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.147226095199585, loss=0.9881418347358704
I0220 04:09:33.782842 140549388556096 spec.py:321] Evaluating on the training split.
I0220 04:10:29.352317 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 04:11:20.168677 140549388556096 spec.py:349] Evaluating on the test split.
I0220 04:11:46.054318 140549388556096 submission_runner.py:408] Time since start: 53557.37s, 	Step: 63598, 	{'train/ctc_loss': Array(0.07950283, dtype=float32), 'train/wer': 0.030960009987099996, 'validation/ctc_loss': Array(0.33140823, dtype=float32), 'validation/wer': 0.09571623043725924, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17631762, dtype=float32), 'test/wer': 0.057827067211017, 'test/num_examples': 2472, 'score': 49002.657531023026, 'total_duration': 53557.37498831749, 'accumulated_submission_time': 49002.657531023026, 'accumulated_eval_time': 4550.166195392609, 'accumulated_logging_time': 1.8721532821655273}
I0220 04:11:46.098261 140483498055424 logging_writer.py:48] [63598] accumulated_eval_time=4550.166195, accumulated_logging_time=1.872153, accumulated_submission_time=49002.657531, global_step=63598, preemption_count=0, score=49002.657531, test/ctc_loss=0.17631761729717255, test/num_examples=2472, test/wer=0.057827, total_duration=53557.374988, train/ctc_loss=0.07950282841920853, train/wer=0.030960, validation/ctc_loss=0.33140823245048523, validation/num_examples=5348, validation/wer=0.095716
I0220 04:11:48.460451 140468535932672 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.0870822668075562, loss=0.9881041646003723
I0220 04:13:03.817604 140483498055424 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.3062058687210083, loss=0.9181706309318542
I0220 04:14:19.247700 140468535932672 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.1438732147216797, loss=0.8873940706253052
I0220 04:15:38.133193 140483498055424 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.0979474782943726, loss=0.9465365409851074
I0220 04:16:53.603001 140468535932672 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.9707361459732056, loss=0.920533299446106
I0220 04:18:09.223541 140483498055424 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.9851654171943665, loss=0.9454705119132996
I0220 04:19:24.994648 140468535932672 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.0469627380371094, loss=0.9251551032066345
I0220 04:20:40.489022 140483498055424 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.1470947265625, loss=0.9102030992507935
I0220 04:21:56.025325 140468535932672 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.998188316822052, loss=0.9635717868804932
I0220 04:23:11.493225 140483498055424 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.9741096496582031, loss=0.9263020753860474
I0220 04:24:28.185146 140468535932672 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.9692578911781311, loss=0.9276594519615173
I0220 04:25:48.513821 140483498055424 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.991315484046936, loss=0.9888295531272888
I0220 04:27:09.661777 140468535932672 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.1246147155761719, loss=0.945579469203949
I0220 04:28:32.419598 140483498055424 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.064003825187683, loss=0.9330835938453674
I0220 04:29:47.860794 140468535932672 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.3257677555084229, loss=0.9476014375686646
I0220 04:31:03.434053 140483498055424 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.2851821184158325, loss=0.8955631256103516
I0220 04:32:18.942094 140468535932672 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.9828770756721497, loss=0.887333333492279
I0220 04:33:34.535213 140483498055424 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.160784363746643, loss=0.9302439093589783
I0220 04:34:50.487568 140468535932672 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.205517053604126, loss=0.938815712928772
I0220 04:35:46.802409 140549388556096 spec.py:321] Evaluating on the training split.
I0220 04:36:40.035822 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 04:37:30.596894 140549388556096 spec.py:349] Evaluating on the test split.
I0220 04:37:56.229284 140549388556096 submission_runner.py:408] Time since start: 55127.55s, 	Step: 65476, 	{'train/ctc_loss': Array(0.09328667, dtype=float32), 'train/wer': 0.035991286841149876, 'validation/ctc_loss': Array(0.3145385, dtype=float32), 'validation/wer': 0.09162265753980131, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16963969, dtype=float32), 'test/wer': 0.05577559766823066, 'test/num_examples': 2472, 'score': 50443.269728422165, 'total_duration': 55127.55088472366, 'accumulated_submission_time': 50443.269728422165, 'accumulated_eval_time': 4679.587248086929, 'accumulated_logging_time': 1.935394525527954}
I0220 04:37:56.272162 140483498055424 logging_writer.py:48] [65476] accumulated_eval_time=4679.587248, accumulated_logging_time=1.935395, accumulated_submission_time=50443.269728, global_step=65476, preemption_count=0, score=50443.269728, test/ctc_loss=0.1696396917104721, test/num_examples=2472, test/wer=0.055776, total_duration=55127.550885, train/ctc_loss=0.0932866707444191, train/wer=0.035991, validation/ctc_loss=0.31453850865364075, validation/num_examples=5348, validation/wer=0.091623
I0220 04:38:15.171853 140468535932672 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.0863240957260132, loss=0.9404869675636292
I0220 04:39:30.505574 140483498055424 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.069233775138855, loss=0.9201942086219788
I0220 04:40:46.022092 140468535932672 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.1373491287231445, loss=0.9272991418838501
I0220 04:42:01.455403 140483498055424 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.9657993912696838, loss=0.8977434039115906
I0220 04:43:17.075749 140468535932672 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.1358838081359863, loss=0.9129579067230225
I0220 04:44:35.926705 140483498055424 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.0483726263046265, loss=0.8797727227210999
I0220 04:45:51.528522 140468535932672 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.0674983263015747, loss=0.9537416696548462
I0220 04:47:07.206377 140483498055424 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.9353005886077881, loss=0.9221627712249756
I0220 04:48:22.838899 140468535932672 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.1555578708648682, loss=0.9506283402442932
I0220 04:49:38.485480 140483498055424 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.2087883949279785, loss=0.9040298461914062
I0220 04:50:54.435973 140468535932672 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.0447721481323242, loss=0.9155009984970093
I0220 04:52:10.029904 140483498055424 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.1180946826934814, loss=0.9424228072166443
I0220 04:53:25.697568 140468535932672 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.2320735454559326, loss=0.9332449436187744
I0220 04:54:42.213558 140483498055424 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.0386358499526978, loss=0.8744581937789917
I0220 04:56:02.144443 140468535932672 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.0987483263015747, loss=0.9324383735656738
I0220 04:57:23.754489 140483498055424 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.3523306846618652, loss=0.9613427519798279
I0220 04:58:39.065498 140468535932672 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.9834256172180176, loss=0.9468661546707153
I0220 04:59:54.473190 140483498055424 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.2682429552078247, loss=0.9183900952339172
I0220 05:01:09.995754 140468535932672 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.1190911531448364, loss=0.886381983757019
I0220 05:01:56.338325 140549388556096 spec.py:321] Evaluating on the training split.
I0220 05:02:49.315677 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 05:03:39.747056 140549388556096 spec.py:349] Evaluating on the test split.
I0220 05:04:05.387577 140549388556096 submission_runner.py:408] Time since start: 56696.71s, 	Step: 67363, 	{'train/ctc_loss': Array(0.0919391, dtype=float32), 'train/wer': 0.03440438914599853, 'validation/ctc_loss': Array(0.31448692, dtype=float32), 'validation/wer': 0.08974000019309306, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17028022, dtype=float32), 'test/wer': 0.05447565657181159, 'test/num_examples': 2472, 'score': 51883.246060848236, 'total_duration': 56696.70898079872, 'accumulated_submission_time': 51883.246060848236, 'accumulated_eval_time': 4808.630482435226, 'accumulated_logging_time': 1.9940145015716553}
I0220 05:04:05.431177 140483498055424 logging_writer.py:48] [67363] accumulated_eval_time=4808.630482, accumulated_logging_time=1.994015, accumulated_submission_time=51883.246061, global_step=67363, preemption_count=0, score=51883.246061, test/ctc_loss=0.17028021812438965, test/num_examples=2472, test/wer=0.054476, total_duration=56696.708981, train/ctc_loss=0.09193909913301468, train/wer=0.034404, validation/ctc_loss=0.31448692083358765, validation/num_examples=5348, validation/wer=0.089740
I0220 05:04:34.144743 140468535932672 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.0084409713745117, loss=0.946606457233429
I0220 05:05:49.716892 140483498055424 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.3067257404327393, loss=0.9489188194274902
I0220 05:07:05.442276 140468535932672 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.3132351636886597, loss=0.9003158211708069
I0220 05:08:21.396737 140483498055424 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.9833045601844788, loss=0.869427502155304
I0220 05:09:37.180421 140468535932672 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.1139243841171265, loss=0.8537238240242004
I0220 05:10:52.908608 140483498055424 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.4026590585708618, loss=0.8932399153709412
I0220 05:12:11.936230 140483498055424 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.2245515584945679, loss=0.8824135065078735
I0220 05:13:27.401473 140468535932672 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.1734898090362549, loss=0.8933761119842529
I0220 05:14:42.936998 140483498055424 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.322992205619812, loss=0.9186854362487793
I0220 05:15:58.620714 140468535932672 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.2999097108840942, loss=0.8909642696380615
I0220 05:17:14.137557 140483498055424 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.2260382175445557, loss=0.8874136209487915
I0220 05:18:29.808591 140468535932672 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.0915846824645996, loss=0.9311483502388
I0220 05:19:45.388823 140483498055424 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.195662498474121, loss=0.8761508464813232
I0220 05:21:00.965134 140468535932672 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.0570032596588135, loss=0.9038219451904297
I0220 05:22:16.724851 140483498055424 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.2906692028045654, loss=0.8962176442146301
I0220 05:23:36.428579 140468535932672 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.2863112688064575, loss=0.8471354246139526
I0220 05:24:56.569074 140483498055424 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.1573668718338013, loss=0.8396444916725159
I0220 05:26:15.719455 140483498055424 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.1202166080474854, loss=0.8688267469406128
I0220 05:27:31.175589 140468535932672 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.0777474641799927, loss=0.8830960392951965
I0220 05:28:05.543173 140549388556096 spec.py:321] Evaluating on the training split.
I0220 05:28:57.561288 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 05:29:48.086162 140549388556096 spec.py:349] Evaluating on the test split.
I0220 05:30:13.909014 140549388556096 submission_runner.py:408] Time since start: 58265.23s, 	Step: 69247, 	{'train/ctc_loss': Array(0.10656315, dtype=float32), 'train/wer': 0.04128829226374341, 'validation/ctc_loss': Array(0.30797923, dtype=float32), 'validation/wer': 0.08775114166272435, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1640094, dtype=float32), 'test/wer': 0.05335851969207645, 'test/num_examples': 2472, 'score': 53323.26822352409, 'total_duration': 58265.23030591011, 'accumulated_submission_time': 53323.26822352409, 'accumulated_eval_time': 4936.9902057647705, 'accumulated_logging_time': 2.0550296306610107}
I0220 05:30:13.951263 140483498055424 logging_writer.py:48] [69247] accumulated_eval_time=4936.990206, accumulated_logging_time=2.055030, accumulated_submission_time=53323.268224, global_step=69247, preemption_count=0, score=53323.268224, test/ctc_loss=0.16400940716266632, test/num_examples=2472, test/wer=0.053359, total_duration=58265.230306, train/ctc_loss=0.10656315088272095, train/wer=0.041288, validation/ctc_loss=0.3079792261123657, validation/num_examples=5348, validation/wer=0.087751
I0220 05:30:54.625591 140468535932672 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.1773205995559692, loss=0.8856338262557983
I0220 05:32:10.121591 140483498055424 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.3936069011688232, loss=0.8719997406005859
I0220 05:33:25.561225 140468535932672 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.1015617847442627, loss=0.8650099635124207
I0220 05:34:41.194175 140483498055424 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.2146209478378296, loss=0.8951051235198975
I0220 05:35:56.817249 140468535932672 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.4658186435699463, loss=0.8606898188591003
I0220 05:37:12.350343 140483498055424 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.251718521118164, loss=0.8456193804740906
I0220 05:38:27.863630 140468535932672 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.0671495199203491, loss=0.8320817947387695
I0220 05:39:43.760257 140483498055424 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.5654302835464478, loss=0.8989439010620117
I0220 05:41:04.658540 140483498055424 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.1529189348220825, loss=0.8966407775878906
I0220 05:42:20.174089 140468535932672 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.162505030632019, loss=0.8517050743103027
I0220 05:43:35.717861 140483498055424 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.1409915685653687, loss=0.9235112071037292
I0220 05:44:51.293978 140468535932672 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.1140823364257812, loss=0.864734947681427
I0220 05:46:06.812824 140483498055424 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.102835774421692, loss=0.8539257049560547
I0220 05:47:22.319670 140468535932672 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.0565801858901978, loss=0.851642906665802
I0220 05:48:37.786823 140483498055424 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.2653529644012451, loss=0.8748522400856018
I0220 05:49:53.153063 140468535932672 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.0785226821899414, loss=0.9240527749061584
I0220 05:51:11.035238 140483498055424 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.2352287769317627, loss=0.8940100073814392
I0220 05:52:30.333998 140468535932672 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.4350014925003052, loss=0.892206072807312
I0220 05:53:52.346378 140483498055424 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.3168376684188843, loss=0.834148645401001
I0220 05:54:13.912078 140549388556096 spec.py:321] Evaluating on the training split.
I0220 05:55:05.697621 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 05:55:56.032841 140549388556096 spec.py:349] Evaluating on the test split.
I0220 05:56:21.572504 140549388556096 submission_runner.py:408] Time since start: 59832.89s, 	Step: 71130, 	{'train/ctc_loss': Array(0.08343864, dtype=float32), 'train/wer': 0.031037345368659822, 'validation/ctc_loss': Array(0.29869056, dtype=float32), 'validation/wer': 0.08500922019367234, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16000023, dtype=float32), 'test/wer': 0.05153047752523714, 'test/num_examples': 2472, 'score': 54763.13962602615, 'total_duration': 59832.894115924835, 'accumulated_submission_time': 54763.13962602615, 'accumulated_eval_time': 5064.64481139183, 'accumulated_logging_time': 2.114377737045288}
I0220 05:56:21.618097 140483498055424 logging_writer.py:48] [71130] accumulated_eval_time=5064.644811, accumulated_logging_time=2.114378, accumulated_submission_time=54763.139626, global_step=71130, preemption_count=0, score=54763.139626, test/ctc_loss=0.16000023484230042, test/num_examples=2472, test/wer=0.051530, total_duration=59832.894116, train/ctc_loss=0.08343864232301712, train/wer=0.031037, validation/ctc_loss=0.2986905574798584, validation/num_examples=5348, validation/wer=0.085009
I0220 05:57:15.077770 140468535932672 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.1094294786453247, loss=0.8780696392059326
I0220 05:58:30.646771 140483498055424 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.1885285377502441, loss=0.8894997239112854
I0220 05:59:46.190085 140468535932672 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.214849829673767, loss=0.8537527322769165
I0220 06:01:01.682247 140483498055424 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.2710820436477661, loss=0.901949942111969
I0220 06:02:17.333130 140468535932672 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.1611518859863281, loss=0.8763012290000916
I0220 06:03:33.100161 140483498055424 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.3226344585418701, loss=0.8485642075538635
I0220 06:04:48.754313 140468535932672 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.2358076572418213, loss=0.8754194378852844
I0220 06:06:04.331951 140483498055424 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.1194839477539062, loss=0.8819029331207275
I0220 06:07:19.920858 140468535932672 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.2705421447753906, loss=0.8768693208694458
I0220 06:08:41.114567 140483498055424 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.3843135833740234, loss=0.8537263870239258
I0220 06:09:56.601848 140468535932672 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.0188372135162354, loss=0.8305832743644714
I0220 06:11:12.418982 140483498055424 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.2808400392532349, loss=0.8869011998176575
I0220 06:12:27.954326 140468535932672 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.06919264793396, loss=0.842475414276123
I0220 06:13:43.413567 140483498055424 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.3386116027832031, loss=0.8678417205810547
I0220 06:14:59.075774 140468535932672 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.5603748559951782, loss=0.867925763130188
I0220 06:16:14.595788 140483498055424 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.1942147016525269, loss=0.9019320607185364
I0220 06:17:30.073022 140468535932672 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.2143144607543945, loss=0.8572781682014465
I0220 06:18:45.643505 140483498055424 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.243997573852539, loss=0.8485658168792725
I0220 06:20:05.698062 140468535932672 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.297455072402954, loss=0.8434847593307495
I0220 06:20:21.982947 140549388556096 spec.py:321] Evaluating on the training split.
I0220 06:21:14.936752 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 06:22:05.548847 140549388556096 spec.py:349] Evaluating on the test split.
I0220 06:22:31.050689 140549388556096 submission_runner.py:408] Time since start: 61402.37s, 	Step: 73022, 	{'train/ctc_loss': Array(0.08170432, dtype=float32), 'train/wer': 0.031414565348437805, 'validation/ctc_loss': Array(0.2942599, dtype=float32), 'validation/wer': 0.08340654778570532, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15819612, dtype=float32), 'test/wer': 0.05140860804744785, 'test/num_examples': 2472, 'score': 56203.4166662693, 'total_duration': 61402.37157559395, 'accumulated_submission_time': 56203.4166662693, 'accumulated_eval_time': 5193.7060170173645, 'accumulated_logging_time': 2.1751956939697266}
I0220 06:22:31.099272 140483498055424 logging_writer.py:48] [73022] accumulated_eval_time=5193.706017, accumulated_logging_time=2.175196, accumulated_submission_time=56203.416666, global_step=73022, preemption_count=0, score=56203.416666, test/ctc_loss=0.1581961214542389, test/num_examples=2472, test/wer=0.051409, total_duration=61402.371576, train/ctc_loss=0.08170431852340698, train/wer=0.031415, validation/ctc_loss=0.2942599058151245, validation/num_examples=5348, validation/wer=0.083407
I0220 06:23:30.580500 140468535932672 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.2609807252883911, loss=0.833795428276062
I0220 06:24:49.317821 140483498055424 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.383141279220581, loss=0.8888978362083435
I0220 06:26:04.848349 140468535932672 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.2027469873428345, loss=0.8213585615158081
I0220 06:27:20.593381 140483498055424 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.10663902759552, loss=0.8653912544250488
I0220 06:28:36.194085 140468535932672 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.2517759799957275, loss=0.867864191532135
I0220 06:29:51.692278 140483498055424 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.1811041831970215, loss=0.8804075717926025
I0220 06:31:07.264734 140468535932672 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.1967995166778564, loss=0.8696786761283875
I0220 06:32:23.039950 140483498055424 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.3122162818908691, loss=0.8825916051864624
I0220 06:33:42.166535 140468535932672 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.4478843212127686, loss=0.8594655990600586
I0220 06:35:01.452533 140483498055424 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.2035287618637085, loss=0.8564556837081909
I0220 06:36:22.384626 140468535932672 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.2514234781265259, loss=0.8885551691055298
I0220 06:37:43.202912 140483498055424 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.3201991319656372, loss=0.806421160697937
I0220 06:38:58.562433 140468535932672 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.4582055807113647, loss=0.8281592726707458
I0220 06:40:14.104513 140483498055424 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.0863627195358276, loss=0.815567672252655
I0220 06:41:29.642315 140468535932672 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.3312114477157593, loss=0.8546891808509827
I0220 06:42:45.419418 140483498055424 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.389460563659668, loss=0.8163259029388428
I0220 06:44:00.880954 140468535932672 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.4252657890319824, loss=0.832301676273346
I0220 06:45:16.454742 140483498055424 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.1668188571929932, loss=0.8401148319244385
I0220 06:46:31.133484 140549388556096 spec.py:321] Evaluating on the training split.
I0220 06:47:25.011879 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 06:48:15.922341 140549388556096 spec.py:349] Evaluating on the test split.
I0220 06:48:41.516398 140549388556096 submission_runner.py:408] Time since start: 62972.84s, 	Step: 74899, 	{'train/ctc_loss': Array(0.0615289, dtype=float32), 'train/wer': 0.0238534577830379, 'validation/ctc_loss': Array(0.29282883, dtype=float32), 'validation/wer': 0.08261486623478186, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1556986, dtype=float32), 'test/wer': 0.05051489854365974, 'test/num_examples': 2472, 'score': 57643.363176584244, 'total_duration': 62972.83807229996, 'accumulated_submission_time': 57643.363176584244, 'accumulated_eval_time': 5324.08319067955, 'accumulated_logging_time': 2.238858938217163}
I0220 06:48:41.560692 140483498055424 logging_writer.py:48] [74899] accumulated_eval_time=5324.083191, accumulated_logging_time=2.238859, accumulated_submission_time=57643.363177, global_step=74899, preemption_count=0, score=57643.363177, test/ctc_loss=0.15569859743118286, test/num_examples=2472, test/wer=0.050515, total_duration=62972.838072, train/ctc_loss=0.06152889505028725, train/wer=0.023853, validation/ctc_loss=0.29282882809638977, validation/num_examples=5348, validation/wer=0.082615
I0220 06:48:43.173058 140468535932672 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.363671064376831, loss=0.8512163758277893
I0220 06:49:58.375170 140483498055424 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.2776663303375244, loss=0.8653580546379089
I0220 06:51:13.897054 140468535932672 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.2522354125976562, loss=0.8425313234329224
I0220 06:52:32.804521 140483498055424 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.2582389116287231, loss=0.8667179942131042
I0220 06:53:48.303954 140468535932672 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.2293853759765625, loss=0.8672576546669006
I0220 06:55:03.904684 140483498055424 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.3587857484817505, loss=0.8291241526603699
I0220 06:56:19.470437 140468535932672 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.1396883726119995, loss=0.8596525192260742
I0220 06:57:34.978760 140483498055424 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.6399881839752197, loss=0.8766605257987976
I0220 06:58:50.820830 140468535932672 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.5607644319534302, loss=0.8522790670394897
I0220 07:00:06.343117 140483498055424 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.3331573009490967, loss=0.8063433170318604
I0220 07:01:21.923656 140468535932672 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.2133487462997437, loss=0.8908159136772156
I0220 07:02:39.855915 140483498055424 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.1989535093307495, loss=0.8511658906936646
I0220 07:04:00.009332 140468535932672 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.8879958391189575, loss=0.8959265351295471
I0220 07:05:19.025595 140483498055424 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.2928942441940308, loss=0.83546382188797
I0220 07:06:38.424323 140483498055424 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.2129660844802856, loss=0.8009944558143616
I0220 07:07:53.936186 140468535932672 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.5237265825271606, loss=0.8241428732872009
I0220 07:09:09.464598 140483498055424 logging_writer.py:48] [76500] global_step=76500, grad_norm=1.1231038570404053, loss=0.8199399709701538
I0220 07:10:24.918251 140468535932672 logging_writer.py:48] [76600] global_step=76600, grad_norm=1.1044039726257324, loss=0.8664894104003906
I0220 07:11:40.502621 140483498055424 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.2374008893966675, loss=0.8271490335464478
I0220 07:12:42.010917 140549388556096 spec.py:321] Evaluating on the training split.
I0220 07:13:35.875273 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 07:14:27.215546 140549388556096 spec.py:349] Evaluating on the test split.
I0220 07:14:53.143632 140549388556096 submission_runner.py:408] Time since start: 64544.47s, 	Step: 76783, 	{'train/ctc_loss': Array(0.07354782, dtype=float32), 'train/wer': 0.02850058566596241, 'validation/ctc_loss': Array(0.29150823, dtype=float32), 'validation/wer': 0.08157216370429729, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15388285, dtype=float32), 'test/wer': 0.04974305851766092, 'test/num_examples': 2472, 'score': 59083.7230618, 'total_duration': 64544.46607041359, 'accumulated_submission_time': 59083.7230618, 'accumulated_eval_time': 5455.210937261581, 'accumulated_logging_time': 2.299424171447754}
I0220 07:14:53.187584 140483498055424 logging_writer.py:48] [76783] accumulated_eval_time=5455.210937, accumulated_logging_time=2.299424, accumulated_submission_time=59083.723062, global_step=76783, preemption_count=0, score=59083.723062, test/ctc_loss=0.15388284623622894, test/num_examples=2472, test/wer=0.049743, total_duration=64544.466070, train/ctc_loss=0.07354781776666641, train/wer=0.028501, validation/ctc_loss=0.2915082275867462, validation/num_examples=5348, validation/wer=0.081572
I0220 07:15:06.856375 140468535932672 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.2053989171981812, loss=0.8392862677574158
I0220 07:16:22.631745 140483498055424 logging_writer.py:48] [76900] global_step=76900, grad_norm=1.1573529243469238, loss=0.824665904045105
I0220 07:17:38.304737 140468535932672 logging_writer.py:48] [77000] global_step=77000, grad_norm=1.282152533531189, loss=0.8488611578941345
I0220 07:18:53.993024 140483498055424 logging_writer.py:48] [77100] global_step=77100, grad_norm=1.1594769954681396, loss=0.8675885796546936
I0220 07:20:09.656208 140468535932672 logging_writer.py:48] [77200] global_step=77200, grad_norm=1.2849632501602173, loss=0.820935845375061
I0220 07:21:28.656321 140483498055424 logging_writer.py:48] [77300] global_step=77300, grad_norm=1.422325611114502, loss=0.8571776747703552
I0220 07:22:44.387281 140468535932672 logging_writer.py:48] [77400] global_step=77400, grad_norm=1.2959612607955933, loss=0.8104230165481567
I0220 07:24:00.289601 140483498055424 logging_writer.py:48] [77500] global_step=77500, grad_norm=1.285630702972412, loss=0.803377628326416
I0220 07:25:16.089954 140468535932672 logging_writer.py:48] [77600] global_step=77600, grad_norm=1.1864022016525269, loss=0.8279125690460205
I0220 07:26:31.878876 140483498055424 logging_writer.py:48] [77700] global_step=77700, grad_norm=1.6617459058761597, loss=0.8305044770240784
I0220 07:27:47.738195 140468535932672 logging_writer.py:48] [77800] global_step=77800, grad_norm=1.194972038269043, loss=0.8033202290534973
I0220 07:29:03.520629 140483498055424 logging_writer.py:48] [77900] global_step=77900, grad_norm=1.250728726387024, loss=0.8039539456367493
I0220 07:30:19.626326 140468535932672 logging_writer.py:48] [78000] global_step=78000, grad_norm=1.5658451318740845, loss=0.8221375942230225
I0220 07:31:38.800897 140483498055424 logging_writer.py:48] [78100] global_step=78100, grad_norm=1.259436011314392, loss=0.8068157434463501
I0220 07:32:58.707614 140468535932672 logging_writer.py:48] [78200] global_step=78200, grad_norm=1.3632155656814575, loss=0.8446021676063538
I0220 07:34:21.604674 140483498055424 logging_writer.py:48] [78300] global_step=78300, grad_norm=1.1574031114578247, loss=0.8405053019523621
I0220 07:35:37.064789 140468535932672 logging_writer.py:48] [78400] global_step=78400, grad_norm=1.189563274383545, loss=0.8493894934654236
I0220 07:36:52.657919 140483498055424 logging_writer.py:48] [78500] global_step=78500, grad_norm=1.8196202516555786, loss=0.8349224328994751
I0220 07:38:08.304948 140468535932672 logging_writer.py:48] [78600] global_step=78600, grad_norm=1.3274657726287842, loss=0.858611524105072
I0220 07:38:53.216798 140549388556096 spec.py:321] Evaluating on the training split.
I0220 07:39:47.016673 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 07:40:37.908737 140549388556096 spec.py:349] Evaluating on the test split.
I0220 07:41:03.680150 140549388556096 submission_runner.py:408] Time since start: 66115.00s, 	Step: 78661, 	{'train/ctc_loss': Array(0.06644049, dtype=float32), 'train/wer': 0.02468937875751503, 'validation/ctc_loss': Array(0.2898537, dtype=float32), 'validation/wer': 0.08116666827577551, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15340821, dtype=float32), 'test/wer': 0.049864927995450205, 'test/num_examples': 2472, 'score': 60523.66146183014, 'total_duration': 66115.00144767761, 'accumulated_submission_time': 60523.66146183014, 'accumulated_eval_time': 5585.668171644211, 'accumulated_logging_time': 2.3606507778167725}
I0220 07:41:03.725231 140483498055424 logging_writer.py:48] [78661] accumulated_eval_time=5585.668172, accumulated_logging_time=2.360651, accumulated_submission_time=60523.661462, global_step=78661, preemption_count=0, score=60523.661462, test/ctc_loss=0.1534082144498825, test/num_examples=2472, test/wer=0.049865, total_duration=66115.001448, train/ctc_loss=0.06644048541784286, train/wer=0.024689, validation/ctc_loss=0.28985369205474854, validation/num_examples=5348, validation/wer=0.081167
I0220 07:41:33.843434 140468535932672 logging_writer.py:48] [78700] global_step=78700, grad_norm=1.1697969436645508, loss=0.8059987425804138
I0220 07:42:49.218630 140483498055424 logging_writer.py:48] [78800] global_step=78800, grad_norm=1.2853753566741943, loss=0.8128319382667542
I0220 07:44:04.740953 140468535932672 logging_writer.py:48] [78900] global_step=78900, grad_norm=1.301113247871399, loss=0.8004137873649597
I0220 07:45:20.192398 140483498055424 logging_writer.py:48] [79000] global_step=79000, grad_norm=1.3444463014602661, loss=0.8032845854759216
I0220 07:46:35.734436 140468535932672 logging_writer.py:48] [79100] global_step=79100, grad_norm=1.303102731704712, loss=0.8091064691543579
I0220 07:47:51.479733 140483498055424 logging_writer.py:48] [79200] global_step=79200, grad_norm=1.1924582719802856, loss=0.8520039319992065
I0220 07:49:06.898836 140468535932672 logging_writer.py:48] [79300] global_step=79300, grad_norm=1.154205083847046, loss=0.832746684551239
I0220 07:50:08.836253 140483498055424 logging_writer.py:48] [79379] global_step=79379, preemption_count=0, score=61068.697877
I0220 07:50:09.743632 140549388556096 checkpoints.py:490] Saving checkpoint at step: 79379
I0220 07:50:11.264383 140549388556096 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_3/librispeech_conformer_jax/trial_3/checkpoint_79379
I0220 07:50:11.298634 140549388556096 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_3/librispeech_conformer_jax/trial_3/checkpoint_79379.
I0220 07:50:14.064656 140549388556096 submission_runner.py:583] Tuning trial 3/5
I0220 07:50:14.064930 140549388556096 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0220 07:50:14.091727 140549388556096 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.87367, dtype=float32), 'train/wer': 1.1416312330885194, 'validation/ctc_loss': Array(30.090126, dtype=float32), 'validation/wer': 0.9587456674744393, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.214182, dtype=float32), 'test/wer': 0.9757885970791949, 'test/num_examples': 2472, 'score': 34.07029867172241, 'total_duration': 163.53999519348145, 'accumulated_submission_time': 34.07029867172241, 'accumulated_eval_time': 129.469651222229, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1828, {'train/ctc_loss': Array(5.8229604, dtype=float32), 'train/wer': 0.9391896477614642, 'validation/ctc_loss': Array(5.8899813, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.791992, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1474.5567479133606, 'total_duration': 1710.6854240894318, 'accumulated_submission_time': 1474.5567479133606, 'accumulated_eval_time': 236.02138757705688, 'accumulated_logging_time': 0.03240823745727539, 'global_step': 1828, 'preemption_count': 0}), (3682, {'train/ctc_loss': Array(3.304899, dtype=float32), 'train/wer': 0.7357704402515723, 'validation/ctc_loss': Array(3.243913, dtype=float32), 'validation/wer': 0.7009953947304903, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.893342, dtype=float32), 'test/wer': 0.6339040887209798, 'test/num_examples': 2472, 'score': 2914.5891761779785, 'total_duration': 3268.5771906375885, 'accumulated_submission_time': 2914.5891761779785, 'accumulated_eval_time': 353.74039936065674, 'accumulated_logging_time': 0.09537768363952637, 'global_step': 3682, 'preemption_count': 0}), (5545, {'train/ctc_loss': Array(1.0559078, dtype=float32), 'train/wer': 0.3306912298852493, 'validation/ctc_loss': Array(1.0954505, dtype=float32), 'validation/wer': 0.31886422661401664, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.80377287, dtype=float32), 'test/wer': 0.26098348668575955, 'test/num_examples': 2472, 'score': 4354.786093473434, 'total_duration': 4837.799213647842, 'accumulated_submission_time': 4354.786093473434, 'accumulated_eval_time': 482.6379177570343, 'accumulated_logging_time': 0.14456629753112793, 'global_step': 5545, 'preemption_count': 0}), (7405, {'train/ctc_loss': Array(0.68506795, dtype=float32), 'train/wer': 0.22820694141939862, 'validation/ctc_loss': Array(0.83268803, dtype=float32), 'validation/wer': 0.24793149058188593, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5571465, dtype=float32), 'test/wer': 0.18810553896776552, 'test/num_examples': 2472, 'score': 5795.037358760834, 'total_duration': 6407.335177659988, 'accumulated_submission_time': 5795.037358760834, 'accumulated_eval_time': 611.7937211990356, 'accumulated_logging_time': 0.1955733299255371, 'global_step': 7405, 'preemption_count': 0}), (9263, {'train/ctc_loss': Array(0.56729215, dtype=float32), 'train/wer': 0.1905387948281184, 'validation/ctc_loss': Array(0.7289868, dtype=float32), 'validation/wer': 0.2186778918099578, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.47376433, dtype=float32), 'test/wer': 0.15731318424633883, 'test/num_examples': 2472, 'score': 7235.207192897797, 'total_duration': 7974.961220741272, 'accumulated_submission_time': 7235.207192897797, 'accumulated_eval_time': 739.1186428070068, 'accumulated_logging_time': 0.24791836738586426, 'global_step': 9263, 'preemption_count': 0}), (11114, {'train/ctc_loss': Array(0.55909514, dtype=float32), 'train/wer': 0.192576916815888, 'validation/ctc_loss': Array(0.6618509, dtype=float32), 'validation/wer': 0.20010233932243646, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42439848, dtype=float32), 'test/wer': 0.14524810594519935, 'test/num_examples': 2472, 'score': 8675.277812957764, 'total_duration': 9545.040509700775, 'accumulated_submission_time': 8675.277812957764, 'accumulated_eval_time': 868.9974067211151, 'accumulated_logging_time': 0.300107479095459, 'global_step': 11114, 'preemption_count': 0}), (12982, {'train/ctc_loss': Array(0.47844568, dtype=float32), 'train/wer': 0.16600666360036348, 'validation/ctc_loss': Array(0.6252392, dtype=float32), 'validation/wer': 0.18973324193595104, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39323118, dtype=float32), 'test/wer': 0.13417829504600573, 'test/num_examples': 2472, 'score': 10115.687534570694, 'total_duration': 11115.899310827255, 'accumulated_submission_time': 10115.687534570694, 'accumulated_eval_time': 999.3121480941772, 'accumulated_logging_time': 0.35501646995544434, 'global_step': 12982, 'preemption_count': 0}), (14851, {'train/ctc_loss': Array(0.4646131, dtype=float32), 'train/wer': 0.15785684451186002, 'validation/ctc_loss': Array(0.59083736, dtype=float32), 'validation/wer': 0.18072545063093157, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36638296, dtype=float32), 'test/wer': 0.12546462738407166, 'test/num_examples': 2472, 'score': 11555.559470653534, 'total_duration': 12686.189148187637, 'accumulated_submission_time': 11555.559470653534, 'accumulated_eval_time': 1129.5970721244812, 'accumulated_logging_time': 0.4081838130950928, 'global_step': 14851, 'preemption_count': 0}), (16714, {'train/ctc_loss': Array(0.38969892, dtype=float32), 'train/wer': 0.13778405189697507, 'validation/ctc_loss': Array(0.5713257, dtype=float32), 'validation/wer': 0.17264450601967618, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3518366, dtype=float32), 'test/wer': 0.11969613876871205, 'test/num_examples': 2472, 'score': 12995.451155424118, 'total_duration': 14255.806090593338, 'accumulated_submission_time': 12995.451155424118, 'accumulated_eval_time': 1259.186912059784, 'accumulated_logging_time': 0.46500158309936523, 'global_step': 16714, 'preemption_count': 0}), (18576, {'train/ctc_loss': Array(0.39127594, dtype=float32), 'train/wer': 0.13950707273182844, 'validation/ctc_loss': Array(0.5567127, dtype=float32), 'validation/wer': 0.16816474699981657, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33858824, dtype=float32), 'test/wer': 0.11459793228119351, 'test/num_examples': 2472, 'score': 14435.541295289993, 'total_duration': 15824.262801408768, 'accumulated_submission_time': 14435.541295289993, 'accumulated_eval_time': 1387.4215536117554, 'accumulated_logging_time': 0.5193827152252197, 'global_step': 18576, 'preemption_count': 0}), (20463, {'train/ctc_loss': Array(0.3756434, dtype=float32), 'train/wer': 0.1329070768267737, 'validation/ctc_loss': Array(0.53292954, dtype=float32), 'validation/wer': 0.16323121928613496, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32874158, dtype=float32), 'test/wer': 0.1117136879735137, 'test/num_examples': 2472, 'score': 15876.112209320068, 'total_duration': 17394.59325671196, 'accumulated_submission_time': 15876.112209320068, 'accumulated_eval_time': 1517.0479528903961, 'accumulated_logging_time': 0.5736007690429688, 'global_step': 20463, 'preemption_count': 0}), (22339, {'train/ctc_loss': Array(0.25177395, dtype=float32), 'train/wer': 0.09277053612055351, 'validation/ctc_loss': Array(0.5271379, dtype=float32), 'validation/wer': 0.15752531932764996, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31686965, dtype=float32), 'test/wer': 0.10669672780452136, 'test/num_examples': 2472, 'score': 17316.20405101776, 'total_duration': 18975.59814763069, 'accumulated_submission_time': 17316.20405101776, 'accumulated_eval_time': 1657.830862045288, 'accumulated_logging_time': 0.6243786811828613, 'global_step': 22339, 'preemption_count': 0}), (24218, {'train/ctc_loss': Array(0.23562592, dtype=float32), 'train/wer': 0.0862083217093911, 'validation/ctc_loss': Array(0.5088276, dtype=float32), 'validation/wer': 0.15359587553221274, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30003414, dtype=float32), 'test/wer': 0.10330469400605286, 'test/num_examples': 2472, 'score': 18756.792862415314, 'total_duration': 20547.990478992462, 'accumulated_submission_time': 18756.792862415314, 'accumulated_eval_time': 1789.5025107860565, 'accumulated_logging_time': 0.6770033836364746, 'global_step': 24218, 'preemption_count': 0}), (26095, {'train/ctc_loss': Array(0.23156746, dtype=float32), 'train/wer': 0.08548788246862167, 'validation/ctc_loss': Array(0.5032359, dtype=float32), 'validation/wer': 0.1530841789200305, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2946906, dtype=float32), 'test/wer': 0.09922206650011171, 'test/num_examples': 2472, 'score': 20196.840671777725, 'total_duration': 22117.707132577896, 'accumulated_submission_time': 20196.840671777725, 'accumulated_eval_time': 1919.0365262031555, 'accumulated_logging_time': 0.7321634292602539, 'global_step': 26095, 'preemption_count': 0}), (27962, {'train/ctc_loss': Array(0.21804456, dtype=float32), 'train/wer': 0.08092597639462486, 'validation/ctc_loss': Array(0.48922175, dtype=float32), 'validation/wer': 0.14635488573718103, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2876642, dtype=float32), 'test/wer': 0.09544411268864379, 'test/num_examples': 2472, 'score': 21636.859580755234, 'total_duration': 23688.512843370438, 'accumulated_submission_time': 21636.859580755234, 'accumulated_eval_time': 2049.692736387253, 'accumulated_logging_time': 0.7841732501983643, 'global_step': 27962, 'preemption_count': 0}), (29839, {'train/ctc_loss': Array(0.21714135, dtype=float32), 'train/wer': 0.08038208140551374, 'validation/ctc_loss': Array(0.4858514, dtype=float32), 'validation/wer': 0.14604593683925968, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2838056, dtype=float32), 'test/wer': 0.09597221375906405, 'test/num_examples': 2472, 'score': 23077.1240503788, 'total_duration': 25260.23565888405, 'accumulated_submission_time': 23077.1240503788, 'accumulated_eval_time': 2181.0143551826477, 'accumulated_logging_time': 0.8413059711456299, 'global_step': 29839, 'preemption_count': 0}), (31702, {'train/ctc_loss': Array(0.19798476, dtype=float32), 'train/wer': 0.07470515976992709, 'validation/ctc_loss': Array(0.46828082, dtype=float32), 'validation/wer': 0.14064898577869606, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26807746, dtype=float32), 'test/wer': 0.09170678203643898, 'test/num_examples': 2472, 'score': 24517.084721565247, 'total_duration': 26831.625014066696, 'accumulated_submission_time': 24517.084721565247, 'accumulated_eval_time': 2312.3158464431763, 'accumulated_logging_time': 0.8909256458282471, 'global_step': 31702, 'preemption_count': 0}), (33574, {'train/ctc_loss': Array(0.23919919, dtype=float32), 'train/wer': 0.08453729330585567, 'validation/ctc_loss': Array(0.46265563, dtype=float32), 'validation/wer': 0.1399828147175531, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26978704, dtype=float32), 'test/wer': 0.09255986838096399, 'test/num_examples': 2472, 'score': 25957.286108970642, 'total_duration': 28401.9341943264, 'accumulated_submission_time': 25957.286108970642, 'accumulated_eval_time': 2442.287088871002, 'accumulated_logging_time': 0.9477226734161377, 'global_step': 33574, 'preemption_count': 0}), (35448, {'train/ctc_loss': Array(0.19585215, dtype=float32), 'train/wer': 0.0732812986505813, 'validation/ctc_loss': Array(0.45211855, dtype=float32), 'validation/wer': 0.13498170443245122, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2617301, dtype=float32), 'test/wer': 0.08855848719354904, 'test/num_examples': 2472, 'score': 27397.751355171204, 'total_duration': 29973.861132621765, 'accumulated_submission_time': 27397.751355171204, 'accumulated_eval_time': 2573.6164152622223, 'accumulated_logging_time': 1.000833511352539, 'global_step': 35448, 'preemption_count': 0}), (37325, {'train/ctc_loss': Array(0.17662221, dtype=float32), 'train/wer': 0.06877232740034307, 'validation/ctc_loss': Array(0.4406018, dtype=float32), 'validation/wer': 0.13279009818782161, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25678006, dtype=float32), 'test/wer': 0.08654764081002579, 'test/num_examples': 2472, 'score': 28838.30406999588, 'total_duration': 31544.836877584457, 'accumulated_submission_time': 28838.30406999588, 'accumulated_eval_time': 2703.9050781726837, 'accumulated_logging_time': 1.056443452835083, 'global_step': 37325, 'preemption_count': 0}), (39194, {'train/ctc_loss': Array(0.17059627, dtype=float32), 'train/wer': 0.0640816435006777, 'validation/ctc_loss': Array(0.43647674, dtype=float32), 'validation/wer': 0.1290440928005252, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.250659, dtype=float32), 'test/wer': 0.08421180915239779, 'test/num_examples': 2472, 'score': 30278.698662042618, 'total_duration': 33116.45455908775, 'accumulated_submission_time': 30278.698662042618, 'accumulated_eval_time': 2834.99561214447, 'accumulated_logging_time': 1.10945463180542, 'global_step': 39194, 'preemption_count': 0}), (41071, {'train/ctc_loss': Array(0.16845806, dtype=float32), 'train/wer': 0.06581072351421188, 'validation/ctc_loss': Array(0.4294395, dtype=float32), 'validation/wer': 0.12859032410670324, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24354605, dtype=float32), 'test/wer': 0.08370401966160908, 'test/num_examples': 2472, 'score': 31718.723798036575, 'total_duration': 34687.90142393112, 'accumulated_submission_time': 31718.723798036575, 'accumulated_eval_time': 2966.2820715904236, 'accumulated_logging_time': 1.1651394367218018, 'global_step': 41071, 'preemption_count': 0}), (42942, {'train/ctc_loss': Array(0.1750182, dtype=float32), 'train/wer': 0.06475229077741695, 'validation/ctc_loss': Array(0.421468, dtype=float32), 'validation/wer': 0.12547187116830957, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23960043, dtype=float32), 'test/wer': 0.07893079844819531, 'test/num_examples': 2472, 'score': 33159.33412575722, 'total_duration': 36261.89895486832, 'accumulated_submission_time': 33159.33412575722, 'accumulated_eval_time': 3099.5344285964966, 'accumulated_logging_time': 1.2207741737365723, 'global_step': 42942, 'preemption_count': 0}), (44822, {'train/ctc_loss': Array(0.1670249, dtype=float32), 'train/wer': 0.06235486775523529, 'validation/ctc_loss': Array(0.40870935, dtype=float32), 'validation/wer': 0.12115624125047067, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23448966, dtype=float32), 'test/wer': 0.07860581317409054, 'test/num_examples': 2472, 'score': 34599.805364370346, 'total_duration': 37834.94379091263, 'accumulated_submission_time': 34599.805364370346, 'accumulated_eval_time': 3231.975162744522, 'accumulated_logging_time': 1.27516770362854, 'global_step': 44822, 'preemption_count': 0}), (46699, {'train/ctc_loss': Array(0.15492408, dtype=float32), 'train/wer': 0.05874819830562958, 'validation/ctc_loss': Array(0.40402222, dtype=float32), 'validation/wer': 0.12081832839336919, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2308128, dtype=float32), 'test/wer': 0.07746836471472386, 'test/num_examples': 2472, 'score': 36039.968794584274, 'total_duration': 39406.69979095459, 'accumulated_submission_time': 36039.968794584274, 'accumulated_eval_time': 3363.430745601654, 'accumulated_logging_time': 1.3329179286956787, 'global_step': 46699, 'preemption_count': 0}), (48569, {'train/ctc_loss': Array(0.14281599, dtype=float32), 'train/wer': 0.05467450313684792, 'validation/ctc_loss': Array(0.3916889, dtype=float32), 'validation/wer': 0.11527655753690491, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22736883, dtype=float32), 'test/wer': 0.07564032254788455, 'test/num_examples': 2472, 'score': 37480.06201171875, 'total_duration': 40977.48327946663, 'accumulated_submission_time': 37480.06201171875, 'accumulated_eval_time': 3493.9831075668335, 'accumulated_logging_time': 1.3930652141571045, 'global_step': 48569, 'preemption_count': 0}), (50447, {'train/ctc_loss': Array(0.12362996, dtype=float32), 'train/wer': 0.04713523789406981, 'validation/ctc_loss': Array(0.38162008, dtype=float32), 'validation/wer': 0.11288220357801443, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21504119, dtype=float32), 'test/wer': 0.07153738346231187, 'test/num_examples': 2472, 'score': 38920.40566134453, 'total_duration': 42550.372653484344, 'accumulated_submission_time': 38920.40566134453, 'accumulated_eval_time': 3626.3920748233795, 'accumulated_logging_time': 1.450796365737915, 'global_step': 50447, 'preemption_count': 0}), (52319, {'train/ctc_loss': Array(0.12619652, dtype=float32), 'train/wer': 0.04762634466272334, 'validation/ctc_loss': Array(0.37294978, dtype=float32), 'validation/wer': 0.11031406586404317, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21084793, dtype=float32), 'test/wer': 0.06954684865842016, 'test/num_examples': 2472, 'score': 40360.535601854324, 'total_duration': 44123.50813269615, 'accumulated_submission_time': 40360.535601854324, 'accumulated_eval_time': 3759.255940914154, 'accumulated_logging_time': 1.513139009475708, 'global_step': 52319, 'preemption_count': 0}), (54199, {'train/ctc_loss': Array(0.12776229, dtype=float32), 'train/wer': 0.04950703654682478, 'validation/ctc_loss': Array(0.36549774, dtype=float32), 'validation/wer': 0.10876932137443641, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20148818, dtype=float32), 'test/wer': 0.06747506753600227, 'test/num_examples': 2472, 'score': 41801.09794139862, 'total_duration': 45695.05196380615, 'accumulated_submission_time': 41801.09794139862, 'accumulated_eval_time': 3890.098317861557, 'accumulated_logging_time': 1.5733115673065186, 'global_step': 54199, 'preemption_count': 0}), (56083, {'train/ctc_loss': Array(0.11001102, dtype=float32), 'train/wer': 0.04222769847988913, 'validation/ctc_loss': Array(0.35788244, dtype=float32), 'validation/wer': 0.10450196472189772, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19801894, dtype=float32), 'test/wer': 0.06597201064326773, 'test/num_examples': 2472, 'score': 43241.61392068863, 'total_duration': 47266.32650756836, 'accumulated_submission_time': 43241.61392068863, 'accumulated_eval_time': 4020.7191219329834, 'accumulated_logging_time': 1.631901502609253, 'global_step': 56083, 'preemption_count': 0}), (57964, {'train/ctc_loss': Array(0.10896836, dtype=float32), 'train/wer': 0.043260168554203726, 'validation/ctc_loss': Array(0.34203002, dtype=float32), 'validation/wer': 0.10161522345694507, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19171545, dtype=float32), 'test/wer': 0.06424552637458615, 'test/num_examples': 2472, 'score': 44681.81542778015, 'total_duration': 48839.02103638649, 'accumulated_submission_time': 44681.81542778015, 'accumulated_eval_time': 4153.070083618164, 'accumulated_logging_time': 1.6941826343536377, 'global_step': 57964, 'preemption_count': 0}), (59837, {'train/ctc_loss': Array(0.12385153, dtype=float32), 'train/wer': 0.044139625672146666, 'validation/ctc_loss': Array(0.3396015, dtype=float32), 'validation/wer': 0.0995201637429159, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18553233, dtype=float32), 'test/wer': 0.06199094103548433, 'test/num_examples': 2472, 'score': 46122.25700163841, 'total_duration': 50411.139713048935, 'accumulated_submission_time': 46122.25700163841, 'accumulated_eval_time': 4284.613779306412, 'accumulated_logging_time': 1.749586582183838, 'global_step': 59837, 'preemption_count': 0}), (61721, {'train/ctc_loss': Array(0.07916234, dtype=float32), 'train/wer': 0.03054289351566671, 'validation/ctc_loss': Array(0.33002672, dtype=float32), 'validation/wer': 0.09577415835561949, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17721963, dtype=float32), 'test/wer': 0.059289500944488455, 'test/num_examples': 2472, 'score': 47562.51353478432, 'total_duration': 51984.82677769661, 'accumulated_submission_time': 47562.51353478432, 'accumulated_eval_time': 4417.90145611763, 'accumulated_logging_time': 1.811532735824585, 'global_step': 61721, 'preemption_count': 0}), (63598, {'train/ctc_loss': Array(0.07950283, dtype=float32), 'train/wer': 0.030960009987099996, 'validation/ctc_loss': Array(0.33140823, dtype=float32), 'validation/wer': 0.09571623043725924, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17631762, dtype=float32), 'test/wer': 0.057827067211017, 'test/num_examples': 2472, 'score': 49002.657531023026, 'total_duration': 53557.37498831749, 'accumulated_submission_time': 49002.657531023026, 'accumulated_eval_time': 4550.166195392609, 'accumulated_logging_time': 1.8721532821655273, 'global_step': 63598, 'preemption_count': 0}), (65476, {'train/ctc_loss': Array(0.09328667, dtype=float32), 'train/wer': 0.035991286841149876, 'validation/ctc_loss': Array(0.3145385, dtype=float32), 'validation/wer': 0.09162265753980131, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16963969, dtype=float32), 'test/wer': 0.05577559766823066, 'test/num_examples': 2472, 'score': 50443.269728422165, 'total_duration': 55127.55088472366, 'accumulated_submission_time': 50443.269728422165, 'accumulated_eval_time': 4679.587248086929, 'accumulated_logging_time': 1.935394525527954, 'global_step': 65476, 'preemption_count': 0}), (67363, {'train/ctc_loss': Array(0.0919391, dtype=float32), 'train/wer': 0.03440438914599853, 'validation/ctc_loss': Array(0.31448692, dtype=float32), 'validation/wer': 0.08974000019309306, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17028022, dtype=float32), 'test/wer': 0.05447565657181159, 'test/num_examples': 2472, 'score': 51883.246060848236, 'total_duration': 56696.70898079872, 'accumulated_submission_time': 51883.246060848236, 'accumulated_eval_time': 4808.630482435226, 'accumulated_logging_time': 1.9940145015716553, 'global_step': 67363, 'preemption_count': 0}), (69247, {'train/ctc_loss': Array(0.10656315, dtype=float32), 'train/wer': 0.04128829226374341, 'validation/ctc_loss': Array(0.30797923, dtype=float32), 'validation/wer': 0.08775114166272435, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1640094, dtype=float32), 'test/wer': 0.05335851969207645, 'test/num_examples': 2472, 'score': 53323.26822352409, 'total_duration': 58265.23030591011, 'accumulated_submission_time': 53323.26822352409, 'accumulated_eval_time': 4936.9902057647705, 'accumulated_logging_time': 2.0550296306610107, 'global_step': 69247, 'preemption_count': 0}), (71130, {'train/ctc_loss': Array(0.08343864, dtype=float32), 'train/wer': 0.031037345368659822, 'validation/ctc_loss': Array(0.29869056, dtype=float32), 'validation/wer': 0.08500922019367234, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16000023, dtype=float32), 'test/wer': 0.05153047752523714, 'test/num_examples': 2472, 'score': 54763.13962602615, 'total_duration': 59832.894115924835, 'accumulated_submission_time': 54763.13962602615, 'accumulated_eval_time': 5064.64481139183, 'accumulated_logging_time': 2.114377737045288, 'global_step': 71130, 'preemption_count': 0}), (73022, {'train/ctc_loss': Array(0.08170432, dtype=float32), 'train/wer': 0.031414565348437805, 'validation/ctc_loss': Array(0.2942599, dtype=float32), 'validation/wer': 0.08340654778570532, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15819612, dtype=float32), 'test/wer': 0.05140860804744785, 'test/num_examples': 2472, 'score': 56203.4166662693, 'total_duration': 61402.37157559395, 'accumulated_submission_time': 56203.4166662693, 'accumulated_eval_time': 5193.7060170173645, 'accumulated_logging_time': 2.1751956939697266, 'global_step': 73022, 'preemption_count': 0}), (74899, {'train/ctc_loss': Array(0.0615289, dtype=float32), 'train/wer': 0.0238534577830379, 'validation/ctc_loss': Array(0.29282883, dtype=float32), 'validation/wer': 0.08261486623478186, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1556986, dtype=float32), 'test/wer': 0.05051489854365974, 'test/num_examples': 2472, 'score': 57643.363176584244, 'total_duration': 62972.83807229996, 'accumulated_submission_time': 57643.363176584244, 'accumulated_eval_time': 5324.08319067955, 'accumulated_logging_time': 2.238858938217163, 'global_step': 74899, 'preemption_count': 0}), (76783, {'train/ctc_loss': Array(0.07354782, dtype=float32), 'train/wer': 0.02850058566596241, 'validation/ctc_loss': Array(0.29150823, dtype=float32), 'validation/wer': 0.08157216370429729, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15388285, dtype=float32), 'test/wer': 0.04974305851766092, 'test/num_examples': 2472, 'score': 59083.7230618, 'total_duration': 64544.46607041359, 'accumulated_submission_time': 59083.7230618, 'accumulated_eval_time': 5455.210937261581, 'accumulated_logging_time': 2.299424171447754, 'global_step': 76783, 'preemption_count': 0}), (78661, {'train/ctc_loss': Array(0.06644049, dtype=float32), 'train/wer': 0.02468937875751503, 'validation/ctc_loss': Array(0.2898537, dtype=float32), 'validation/wer': 0.08116666827577551, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15340821, dtype=float32), 'test/wer': 0.049864927995450205, 'test/num_examples': 2472, 'score': 60523.66146183014, 'total_duration': 66115.00144767761, 'accumulated_submission_time': 60523.66146183014, 'accumulated_eval_time': 5585.668171644211, 'accumulated_logging_time': 2.3606507778167725, 'global_step': 78661, 'preemption_count': 0})], 'global_step': 79379}
I0220 07:50:14.092010 140549388556096 submission_runner.py:586] Timing: 61068.697877407074
I0220 07:50:14.092073 140549388556096 submission_runner.py:588] Total number of evals: 43
I0220 07:50:14.092127 140549388556096 submission_runner.py:589] ====================
I0220 07:50:14.092182 140549388556096 submission_runner.py:542] Using RNG seed 4110531300
I0220 07:50:14.094518 140549388556096 submission_runner.py:551] --- Tuning run 4/5 ---
I0220 07:50:14.094655 140549388556096 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_3/librispeech_conformer_jax/trial_4.
I0220 07:50:14.096900 140549388556096 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_3/librispeech_conformer_jax/trial_4/hparams.json.
I0220 07:50:14.098183 140549388556096 submission_runner.py:206] Initializing dataset.
I0220 07:50:14.098337 140549388556096 submission_runner.py:213] Initializing model.
I0220 07:50:17.844969 140549388556096 submission_runner.py:255] Initializing optimizer.
I0220 07:50:18.291398 140549388556096 submission_runner.py:262] Initializing metrics bundle.
I0220 07:50:18.291632 140549388556096 submission_runner.py:280] Initializing checkpoint and logger.
I0220 07:50:18.296587 140549388556096 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_3/librispeech_conformer_jax/trial_4 with prefix checkpoint_
I0220 07:50:18.296727 140549388556096 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_3/librispeech_conformer_jax/trial_4/meta_data_0.json.
I0220 07:50:18.296999 140549388556096 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0220 07:50:18.297074 140549388556096 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0220 07:50:18.906636 140549388556096 logger_utils.py:220] Unable to record git information. Continuing without it.
I0220 07:50:19.469943 140549388556096 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_3/librispeech_conformer_jax/trial_4/flags_0.json.
I0220 07:50:19.489381 140549388556096 submission_runner.py:314] Starting training loop.
I0220 07:50:19.492647 140549388556096 input_pipeline.py:20] Loading split = train-clean-100
I0220 07:50:19.539082 140549388556096 input_pipeline.py:20] Loading split = train-clean-360
I0220 07:50:20.060956 140549388556096 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0220 07:50:54.731161 140367735265024 logging_writer.py:48] [0] global_step=0, grad_norm=67.55435943603516, loss=31.79118537902832
I0220 07:50:54.759635 140549388556096 spec.py:321] Evaluating on the training split.
I0220 07:51:48.563088 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 07:52:37.907461 140549388556096 spec.py:349] Evaluating on the test split.
I0220 07:53:03.486807 140549388556096 submission_runner.py:408] Time since start: 163.99s, 	Step: 1, 	{'train/ctc_loss': Array(30.981934, dtype=float32), 'train/wer': 1.1274528946597677, 'validation/ctc_loss': Array(30.090126, dtype=float32), 'validation/wer': 0.9587360128213792, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.214182, dtype=float32), 'test/wer': 0.9757885970791949, 'test/num_examples': 2472, 'score': 35.27013564109802, 'total_duration': 163.99474143981934, 'accumulated_submission_time': 35.27013564109802, 'accumulated_eval_time': 128.7245156764984, 'accumulated_logging_time': 0}
I0220 07:53:03.506259 140483498055424 logging_writer.py:48] [1] accumulated_eval_time=128.724516, accumulated_logging_time=0, accumulated_submission_time=35.270136, global_step=1, preemption_count=0, score=35.270136, test/ctc_loss=30.214181900024414, test/num_examples=2472, test/wer=0.975789, total_duration=163.994741, train/ctc_loss=30.98193359375, train/wer=1.127453, validation/ctc_loss=30.090126037597656, validation/num_examples=5348, validation/wer=0.958736
I0220 07:54:44.470133 140378514646784 logging_writer.py:48] [100] global_step=100, grad_norm=3.725709915161133, loss=5.803012847900391
I0220 07:56:00.093417 140378523039488 logging_writer.py:48] [200] global_step=200, grad_norm=3.047017812728882, loss=5.787939071655273
I0220 07:57:15.872071 140378514646784 logging_writer.py:48] [300] global_step=300, grad_norm=3.5360300540924072, loss=5.6608405113220215
I0220 07:58:31.550437 140378523039488 logging_writer.py:48] [400] global_step=400, grad_norm=1.5516668558120728, loss=5.572332859039307
I0220 07:59:47.083875 140378514646784 logging_writer.py:48] [500] global_step=500, grad_norm=3.374032497406006, loss=5.563581943511963
I0220 08:01:02.545655 140378523039488 logging_writer.py:48] [600] global_step=600, grad_norm=15.07276725769043, loss=6.658003330230713
I0220 08:02:18.051968 140378514646784 logging_writer.py:48] [700] global_step=700, grad_norm=1.805253505706787, loss=5.530485153198242
I0220 08:03:33.460527 140378523039488 logging_writer.py:48] [800] global_step=800, grad_norm=0.8086934685707092, loss=5.540224552154541
I0220 08:04:48.695433 140378514646784 logging_writer.py:48] [900] global_step=900, grad_norm=3.681596517562866, loss=5.559422016143799
I0220 08:06:08.523232 140378523039488 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.27819716930389404, loss=5.508923053741455
I0220 08:07:28.486697 140483498055424 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.2595199346542358, loss=5.496182918548584
I0220 08:08:43.829193 140468535932672 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.8013003468513489, loss=5.499599456787109
I0220 08:09:58.952571 140483498055424 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.3131473958492279, loss=5.485476016998291
I0220 08:11:14.085377 140468535932672 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.76882004737854, loss=5.501119613647461
I0220 08:12:29.572927 140483498055424 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.3325946033000946, loss=5.475556373596191
I0220 08:13:44.789089 140468535932672 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.7654036283493042, loss=5.486213207244873
I0220 08:15:06.037473 140483498055424 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.7671380639076233, loss=5.479709148406982
I0220 08:16:27.581772 140468535932672 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.2978944778442383, loss=5.4784016609191895
I0220 08:17:04.230340 140549388556096 spec.py:321] Evaluating on the training split.
I0220 08:17:42.942534 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 08:18:28.452936 140549388556096 spec.py:349] Evaluating on the test split.
I0220 08:18:51.645312 140549388556096 submission_runner.py:408] Time since start: 1712.15s, 	Step: 1847, 	{'train/ctc_loss': Array(7.2952933, dtype=float32), 'train/wer': 0.9413900245298447, 'validation/ctc_loss': Array(7.2312818, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(7.232382, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1475.9121260643005, 'total_duration': 1712.1499185562134, 'accumulated_submission_time': 1475.9121260643005, 'accumulated_eval_time': 236.133540391922, 'accumulated_logging_time': 0.030684471130371094}
I0220 08:18:51.682255 140483498055424 logging_writer.py:48] [1847] accumulated_eval_time=236.133540, accumulated_logging_time=0.030684, accumulated_submission_time=1475.912126, global_step=1847, preemption_count=0, score=1475.912126, test/ctc_loss=7.232381820678711, test/num_examples=2472, test/wer=0.899580, total_duration=1712.149919, train/ctc_loss=7.29529333114624, train/wer=0.941390, validation/ctc_loss=7.231281757354736, validation/num_examples=5348, validation/wer=0.896618
I0220 08:19:32.127053 140468535932672 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.077934503555298, loss=5.468104362487793
I0220 08:20:47.223918 140483498055424 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.8103468418121338, loss=5.460334300994873
I0220 08:22:05.881710 140483498055424 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.8706111311912537, loss=5.426196098327637
I0220 08:23:20.943242 140468535932672 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.8879873752593994, loss=5.447376728057861
I0220 08:24:36.040461 140483498055424 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.8847954273223877, loss=5.450677394866943
I0220 08:25:51.041296 140468535932672 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.3307073712348938, loss=5.4772562980651855
I0220 08:27:06.189748 140483498055424 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.5720930099487305, loss=5.431020736694336
I0220 08:28:21.400050 140468535932672 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.8287665247917175, loss=5.390334606170654
I0220 08:29:40.888684 140483498055424 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.7828454971313477, loss=5.405651092529297
I0220 08:31:03.301225 140468535932672 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.9819796085357666, loss=5.331745624542236
I0220 08:32:25.479303 140483498055424 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.8572240471839905, loss=5.277682304382324
I0220 08:33:47.187916 140468535932672 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.34418848156929016, loss=5.165585517883301
I0220 08:35:10.504804 140483498055424 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.4532989263534546, loss=4.810385704040527
I0220 08:36:25.600973 140468535932672 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.69472736120224, loss=4.436486721038818
I0220 08:37:40.876389 140483498055424 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.8212528228759766, loss=4.249783039093018
I0220 08:38:56.093991 140468535932672 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.5125105977058411, loss=4.1058831214904785
I0220 08:40:11.336538 140483498055424 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.9962108135223389, loss=4.035676956176758
I0220 08:41:26.525360 140468535932672 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.8489527702331543, loss=3.8294482231140137
I0220 08:42:43.191551 140483498055424 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.1802570819854736, loss=3.784126043319702
I0220 08:42:52.328031 140549388556096 spec.py:321] Evaluating on the training split.
I0220 08:43:30.768144 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 08:44:16.015842 140549388556096 spec.py:349] Evaluating on the test split.
I0220 08:44:39.284872 140549388556096 submission_runner.py:408] Time since start: 3259.79s, 	Step: 3713, 	{'train/ctc_loss': Array(11.699928, dtype=float32), 'train/wer': 0.9387706290361157, 'validation/ctc_loss': Array(11.644811, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(11.63979, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2916.4644272327423, 'total_duration': 3259.789473295212, 'accumulated_submission_time': 2916.4644272327423, 'accumulated_eval_time': 343.08443236351013, 'accumulated_logging_time': 0.08783602714538574}
I0220 08:44:39.317278 140483498055424 logging_writer.py:48] [3713] accumulated_eval_time=343.084432, accumulated_logging_time=0.087836, accumulated_submission_time=2916.464427, global_step=3713, preemption_count=0, score=2916.464427, test/ctc_loss=11.639789581298828, test/num_examples=2472, test/wer=0.899580, total_duration=3259.789473, train/ctc_loss=11.699928283691406, train/wer=0.938771, validation/ctc_loss=11.644810676574707, validation/num_examples=5348, validation/wer=0.896618
I0220 08:45:45.475716 140468535932672 logging_writer.py:48] [3800] global_step=3800, grad_norm=2.1757125854492188, loss=3.6895370483398438
I0220 08:47:00.571195 140483498055424 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.0521228313446045, loss=3.6404173374176025
I0220 08:48:15.742577 140468535932672 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.1126898527145386, loss=3.6015138626098633
I0220 08:49:30.856467 140483498055424 logging_writer.py:48] [4100] global_step=4100, grad_norm=1.7036795616149902, loss=3.5603749752044678
I0220 08:50:49.562160 140483498055424 logging_writer.py:48] [4200] global_step=4200, grad_norm=1.669872522354126, loss=3.425753593444824
I0220 08:52:04.753330 140468535932672 logging_writer.py:48] [4300] global_step=4300, grad_norm=1.04025399684906, loss=3.3352162837982178
I0220 08:53:19.886260 140483498055424 logging_writer.py:48] [4400] global_step=4400, grad_norm=1.989099144935608, loss=3.438230276107788
I0220 08:54:35.108484 140468535932672 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.1272839307785034, loss=3.3786447048187256
I0220 08:55:50.248845 140483498055424 logging_writer.py:48] [4600] global_step=4600, grad_norm=1.301130771636963, loss=3.320054292678833
I0220 08:57:05.471561 140468535932672 logging_writer.py:48] [4700] global_step=4700, grad_norm=1.2129987478256226, loss=3.2897472381591797
I0220 08:58:25.047424 140483498055424 logging_writer.py:48] [4800] global_step=4800, grad_norm=1.1046069860458374, loss=3.183551549911499
I0220 08:59:45.685485 140468535932672 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.899528443813324, loss=3.1753079891204834
I0220 09:01:07.196750 140483498055424 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.8386799097061157, loss=3.1300535202026367
I0220 09:02:28.754667 140468535932672 logging_writer.py:48] [5100] global_step=5100, grad_norm=1.3449881076812744, loss=3.11738657951355
I0220 09:03:50.427116 140483498055424 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.9182592034339905, loss=3.0132529735565186
I0220 09:05:05.554730 140468535932672 logging_writer.py:48] [5300] global_step=5300, grad_norm=1.230734944343567, loss=3.091093063354492
I0220 09:06:20.717692 140483498055424 logging_writer.py:48] [5400] global_step=5400, grad_norm=1.3341723680496216, loss=2.9808554649353027
I0220 09:07:35.876881 140468535932672 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.5607738494873047, loss=2.996150493621826
I0220 09:08:39.378951 140549388556096 spec.py:321] Evaluating on the training split.
I0220 09:09:27.564634 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 09:10:16.537526 140549388556096 spec.py:349] Evaluating on the test split.
I0220 09:10:41.562829 140549388556096 submission_runner.py:408] Time since start: 4822.07s, 	Step: 5586, 	{'train/ctc_loss': Array(3.2989602, dtype=float32), 'train/wer': 0.7216809110434939, 'validation/ctc_loss': Array(3.2346897, dtype=float32), 'validation/wer': 0.6911669579153673, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.911543, dtype=float32), 'test/wer': 0.6546625231044219, 'test/num_examples': 2472, 'score': 4356.433173418045, 'total_duration': 4822.067915201187, 'accumulated_submission_time': 4356.433173418045, 'accumulated_eval_time': 465.2628490924835, 'accumulated_logging_time': 0.14037322998046875}
I0220 09:10:41.598633 140483498055424 logging_writer.py:48] [5586] accumulated_eval_time=465.262849, accumulated_logging_time=0.140373, accumulated_submission_time=4356.433173, global_step=5586, preemption_count=0, score=4356.433173, test/ctc_loss=2.9115428924560547, test/num_examples=2472, test/wer=0.654663, total_duration=4822.067915, train/ctc_loss=3.2989602088928223, train/wer=0.721681, validation/ctc_loss=3.234689712524414, validation/num_examples=5348, validation/wer=0.691167
I0220 09:10:52.947418 140468535932672 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.9043030738830566, loss=2.946793794631958
I0220 09:12:08.126607 140483498055424 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.7780535817146301, loss=3.0500965118408203
I0220 09:13:23.530894 140468535932672 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.7837685346603394, loss=2.895136833190918
I0220 09:14:38.951497 140483498055424 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.6015613079071045, loss=2.9268791675567627
I0220 09:15:54.356699 140468535932672 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.6288419365882874, loss=2.91375732421875
I0220 09:17:09.708947 140483498055424 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.7835718989372253, loss=2.7937262058258057
I0220 09:18:31.330988 140483498055424 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.501431405544281, loss=2.8639111518859863
I0220 09:19:46.576290 140468535932672 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.037648320198059, loss=2.9031851291656494
I0220 09:21:01.799299 140483498055424 logging_writer.py:48] [6400] global_step=6400, grad_norm=1.852576732635498, loss=2.8498988151550293
I0220 09:22:17.062461 140468535932672 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.7162965536117554, loss=2.855517625808716
I0220 09:23:32.293593 140483498055424 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.0403352975845337, loss=2.8310952186584473
I0220 09:24:47.534771 140468535932672 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.905504584312439, loss=2.7641689777374268
I0220 09:26:03.940439 140483498055424 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.1277287006378174, loss=2.685119390487671
I0220 09:27:25.183840 140468535932672 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.3427292108535767, loss=2.759704828262329
I0220 09:28:46.674606 140483498055424 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.9897511005401611, loss=2.7544918060302734
I0220 09:30:07.756629 140468535932672 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.6050162315368652, loss=2.7172060012817383
I0220 09:31:29.033274 140483498055424 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.9243746995925903, loss=2.753999948501587
I0220 09:32:47.867469 140483498055424 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.5630936026573181, loss=2.6733555793762207
I0220 09:34:03.293805 140468535932672 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.6966546177864075, loss=2.627190113067627
I0220 09:34:42.137947 140549388556096 spec.py:321] Evaluating on the training split.
I0220 09:35:33.889490 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 09:36:24.545041 140549388556096 spec.py:349] Evaluating on the test split.
I0220 09:36:50.422699 140549388556096 submission_runner.py:408] Time since start: 6390.93s, 	Step: 7453, 	{'train/ctc_loss': Array(1.9405282, dtype=float32), 'train/wer': 0.5345338386931263, 'validation/ctc_loss': Array(1.9695325, dtype=float32), 'validation/wer': 0.5126138042229452, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.5693043, dtype=float32), 'test/wer': 0.45666524485609244, 'test/num_examples': 2472, 'score': 5796.8844957351685, 'total_duration': 6390.927451133728, 'accumulated_submission_time': 5796.8844957351685, 'accumulated_eval_time': 593.5417983531952, 'accumulated_logging_time': 0.19143939018249512}
I0220 09:36:50.462279 140483498055424 logging_writer.py:48] [7453] accumulated_eval_time=593.541798, accumulated_logging_time=0.191439, accumulated_submission_time=5796.884496, global_step=7453, preemption_count=0, score=5796.884496, test/ctc_loss=1.569304347038269, test/num_examples=2472, test/wer=0.456665, total_duration=6390.927451, train/ctc_loss=1.940528154373169, train/wer=0.534534, validation/ctc_loss=1.9695324897766113, validation/num_examples=5348, validation/wer=0.512614
I0220 09:37:26.449753 140468535932672 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.0460331439971924, loss=2.6782748699188232
I0220 09:38:41.578473 140483498055424 logging_writer.py:48] [7600] global_step=7600, grad_norm=1.1907367706298828, loss=2.603752613067627
I0220 09:39:56.733328 140468535932672 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.8393955230712891, loss=2.73398756980896
I0220 09:41:11.881280 140483498055424 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.6177327036857605, loss=2.5791501998901367
I0220 09:42:27.106354 140468535932672 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.5112500190734863, loss=2.5980582237243652
I0220 09:43:43.281057 140483498055424 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.5654106736183167, loss=2.6237130165100098
I0220 09:45:03.346014 140468535932672 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.9464887976646423, loss=2.677696704864502
I0220 09:46:24.190871 140483498055424 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.5502437353134155, loss=2.571777820587158
I0220 09:47:44.649246 140483498055424 logging_writer.py:48] [8300] global_step=8300, grad_norm=2.388309955596924, loss=2.569770574569702
I0220 09:48:59.884357 140468535932672 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.0634976625442505, loss=2.539140462875366
I0220 09:50:15.412859 140483498055424 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.0162415504455566, loss=2.5739080905914307
I0220 09:51:30.666190 140468535932672 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.5955010056495667, loss=2.5432684421539307
I0220 09:52:45.952522 140483498055424 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.9608966708183289, loss=2.5387203693389893
I0220 09:54:01.203933 140468535932672 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.0919963121414185, loss=2.519491195678711
I0220 09:55:18.710931 140483498055424 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.3226594924926758, loss=2.5545480251312256
I0220 09:56:40.206929 140468535932672 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.2461962699890137, loss=2.511232852935791
I0220 09:58:01.312921 140483498055424 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.0113211870193481, loss=2.49922513961792
I0220 09:59:22.400365 140468535932672 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.1887749433517456, loss=2.453911542892456
I0220 10:00:44.218711 140483498055424 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.6074825525283813, loss=2.4105870723724365
I0220 10:00:50.686813 140549388556096 spec.py:321] Evaluating on the training split.
I0220 10:01:42.476785 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 10:02:33.133964 140549388556096 spec.py:349] Evaluating on the test split.
I0220 10:02:59.036978 140549388556096 submission_runner.py:408] Time since start: 7959.54s, 	Step: 9310, 	{'train/ctc_loss': Array(1.5818727, dtype=float32), 'train/wer': 0.45802970526804365, 'validation/ctc_loss': Array(1.544094, dtype=float32), 'validation/wer': 0.4266487733763287, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.1772131, dtype=float32), 'test/wer': 0.36333353644912963, 'test/num_examples': 2472, 'score': 7237.020395278931, 'total_duration': 7959.54049539566, 'accumulated_submission_time': 7237.020395278931, 'accumulated_eval_time': 721.8849267959595, 'accumulated_logging_time': 0.24752020835876465}
I0220 10:02:59.074603 140483498055424 logging_writer.py:48] [9310] accumulated_eval_time=721.884927, accumulated_logging_time=0.247520, accumulated_submission_time=7237.020395, global_step=9310, preemption_count=0, score=7237.020395, test/ctc_loss=1.1772130727767944, test/num_examples=2472, test/wer=0.363334, total_duration=7959.540495, train/ctc_loss=1.5818727016448975, train/wer=0.458030, validation/ctc_loss=1.5440939664840698, validation/num_examples=5348, validation/wer=0.426649
I0220 10:04:07.197014 140468535932672 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.9169277548789978, loss=2.4105257987976074
I0220 10:05:22.334618 140483498055424 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.3772424459457397, loss=2.5040385723114014
I0220 10:06:37.541593 140468535932672 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.8374198079109192, loss=2.4423766136169434
I0220 10:07:53.006095 140483498055424 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.5108087658882141, loss=2.422126293182373
I0220 10:09:08.243556 140468535932672 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.8094777464866638, loss=2.337556838989258
I0220 10:10:23.467950 140483498055424 logging_writer.py:48] [9900] global_step=9900, grad_norm=1.522247076034546, loss=2.419546365737915
I0220 10:11:38.683586 140468535932672 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.1900304555892944, loss=2.415318012237549
I0220 10:12:54.557550 140483498055424 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.7093966007232666, loss=2.4452290534973145
I0220 10:14:17.069100 140468535932672 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.1552433967590332, loss=2.4025931358337402
I0220 10:15:41.100289 140483498055424 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.0792419910430908, loss=2.326660394668579
I0220 10:16:56.155704 140468535932672 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.0646167993545532, loss=2.2886106967926025
I0220 10:18:11.340373 140483498055424 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.7029451727867126, loss=2.28484845161438
I0220 10:19:26.597776 140468535932672 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.576135516166687, loss=2.419912815093994
I0220 10:20:41.850333 140483498055424 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.7707499861717224, loss=2.276543378829956
I0220 10:21:57.101393 140468535932672 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.963489294052124, loss=2.3352315425872803
I0220 10:23:13.246462 140483498055424 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.9177132248878479, loss=2.302602767944336
I0220 10:24:34.083558 140468535932672 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.4262772798538208, loss=2.3143467903137207
I0220 10:25:55.147293 140483498055424 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.8333427309989929, loss=2.336526870727539
I0220 10:26:59.818960 140549388556096 spec.py:321] Evaluating on the training split.
I0220 10:27:54.538858 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 10:28:45.661018 140549388556096 spec.py:349] Evaluating on the test split.
I0220 10:29:12.069296 140549388556096 submission_runner.py:408] Time since start: 9532.57s, 	Step: 11181, 	{'train/ctc_loss': Array(1.2154001, dtype=float32), 'train/wer': 0.37666020079019236, 'validation/ctc_loss': Array(1.3427572, dtype=float32), 'validation/wer': 0.38610888517721115, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.0078455, dtype=float32), 'test/wer': 0.3222635224341397, 'test/num_examples': 2472, 'score': 8677.674111127853, 'total_duration': 9532.57375240326, 'accumulated_submission_time': 8677.674111127853, 'accumulated_eval_time': 854.1291942596436, 'accumulated_logging_time': 0.30259132385253906}
I0220 10:29:12.104104 140483498055424 logging_writer.py:48] [11181] accumulated_eval_time=854.129194, accumulated_logging_time=0.302591, accumulated_submission_time=8677.674111, global_step=11181, preemption_count=0, score=8677.674111, test/ctc_loss=1.0078455209732056, test/num_examples=2472, test/wer=0.322264, total_duration=9532.573752, train/ctc_loss=1.2154000997543335, train/wer=0.376660, validation/ctc_loss=1.342757225036621, validation/num_examples=5348, validation/wer=0.386109
I0220 10:29:27.135838 140468535932672 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.0091016292572021, loss=2.3148162364959717
I0220 10:30:42.182377 140483498055424 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.4852461814880371, loss=2.264526605606079
I0220 10:32:00.604470 140483498055424 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.006440281867981, loss=2.2486116886138916
I0220 10:33:15.768566 140468535932672 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.4809951782226562, loss=2.2715442180633545
I0220 10:34:30.992618 140483498055424 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.2214826345443726, loss=2.2789268493652344
I0220 10:35:46.112477 140468535932672 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.7086452841758728, loss=2.253399610519409
I0220 10:37:01.315542 140483498055424 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.514764130115509, loss=2.2098491191864014
I0220 10:38:16.788851 140468535932672 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.4311420917510986, loss=2.2829596996307373
I0220 10:39:37.937570 140483498055424 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.786810576915741, loss=2.293287515640259
I0220 10:40:57.940150 140468535932672 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.8662822246551514, loss=2.2157838344573975
I0220 10:42:18.486287 140483498055424 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.6223351359367371, loss=2.1894121170043945
I0220 10:43:39.346004 140468535932672 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.5603914856910706, loss=2.205082893371582
I0220 10:45:01.705898 140483498055424 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.8717522025108337, loss=2.1563899517059326
I0220 10:46:16.910902 140468535932672 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.7673709988594055, loss=2.209861993789673
I0220 10:47:32.172070 140483498055424 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.168763518333435, loss=2.209325075149536
I0220 10:48:47.509204 140468535932672 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.9654296636581421, loss=2.2855465412139893
I0220 10:50:02.800346 140483498055424 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.778732180595398, loss=2.18701434135437
I0220 10:51:18.019645 140468535932672 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.6673159599304199, loss=2.127695322036743
I0220 10:52:35.067775 140483498055424 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.58328777551651, loss=2.1773715019226074
I0220 10:53:12.189475 140549388556096 spec.py:321] Evaluating on the training split.
I0220 10:54:04.940710 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 10:54:56.367645 140549388556096 spec.py:349] Evaluating on the test split.
I0220 10:55:22.295835 140549388556096 submission_runner.py:408] Time since start: 11102.80s, 	Step: 13048, 	{'train/ctc_loss': Array(1.1826919, dtype=float32), 'train/wer': 0.3672558613082949, 'validation/ctc_loss': Array(1.2360047, dtype=float32), 'validation/wer': 0.36130608146596255, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.9073358, dtype=float32), 'test/wer': 0.29782869213738755, 'test/num_examples': 2472, 'score': 10117.671419858932, 'total_duration': 11102.799887180328, 'accumulated_submission_time': 10117.671419858932, 'accumulated_eval_time': 984.2290835380554, 'accumulated_logging_time': 0.35315537452697754}
I0220 10:55:22.332514 140483498055424 logging_writer.py:48] [13048] accumulated_eval_time=984.229084, accumulated_logging_time=0.353155, accumulated_submission_time=10117.671420, global_step=13048, preemption_count=0, score=10117.671420, test/ctc_loss=0.9073358178138733, test/num_examples=2472, test/wer=0.297829, total_duration=11102.799887, train/ctc_loss=1.1826919317245483, train/wer=0.367256, validation/ctc_loss=1.2360047101974487, validation/num_examples=5348, validation/wer=0.361306
I0220 10:56:02.161659 140468535932672 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.8307734727859497, loss=2.1977951526641846
I0220 10:57:17.759320 140483498055424 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.6429228186607361, loss=2.1445090770721436
I0220 10:58:33.031372 140468535932672 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.6200258135795593, loss=2.08976149559021
I0220 10:59:52.044079 140483498055424 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.9105599522590637, loss=2.093006134033203
I0220 11:01:07.307233 140468535932672 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.0512888431549072, loss=2.123131513595581
I0220 11:02:22.443661 140483498055424 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.8793826699256897, loss=2.0910139083862305
I0220 11:03:37.637610 140468535932672 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.0332785844802856, loss=2.12996506690979
I0220 11:04:52.775437 140483498055424 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.8048785924911499, loss=2.130906105041504
I0220 11:06:08.067593 140468535932672 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.4988098442554474, loss=2.0981221199035645
I0220 11:07:24.409560 140483498055424 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.734961748123169, loss=2.1366279125213623
I0220 11:08:45.514910 140468535932672 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.7260825037956238, loss=2.1191954612731934
I0220 11:10:06.719972 140483498055424 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.652755618095398, loss=2.1294922828674316
I0220 11:11:29.021428 140468535932672 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.6608215570449829, loss=2.1183910369873047
I0220 11:12:49.875950 140483498055424 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.9476323127746582, loss=2.1079742908477783
I0220 11:14:09.645901 140483498055424 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.9990047812461853, loss=2.139725923538208
I0220 11:15:24.937821 140468535932672 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.7885858416557312, loss=2.1022610664367676
I0220 11:16:40.152125 140483498055424 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.7239431738853455, loss=2.155301809310913
I0220 11:17:55.395679 140468535932672 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.8545573949813843, loss=2.140126943588257
I0220 11:19:10.627634 140483498055424 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.1507902145385742, loss=2.0762546062469482
I0220 11:19:22.413856 140549388556096 spec.py:321] Evaluating on the training split.
I0220 11:20:14.877905 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 11:21:05.728599 140549388556096 spec.py:349] Evaluating on the test split.
I0220 11:21:31.477986 140549388556096 submission_runner.py:408] Time since start: 12671.98s, 	Step: 14917, 	{'train/ctc_loss': Array(1.1055752, dtype=float32), 'train/wer': 0.3447297012862157, 'validation/ctc_loss': Array(1.1436431, dtype=float32), 'validation/wer': 0.3354123019589291, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.825852, dtype=float32), 'test/wer': 0.2695549732902728, 'test/num_examples': 2472, 'score': 11557.662242412567, 'total_duration': 12671.982889652252, 'accumulated_submission_time': 11557.662242412567, 'accumulated_eval_time': 1113.2875900268555, 'accumulated_logging_time': 0.40856337547302246}
I0220 11:21:31.518713 140483498055424 logging_writer.py:48] [14917] accumulated_eval_time=1113.287590, accumulated_logging_time=0.408563, accumulated_submission_time=11557.662242, global_step=14917, preemption_count=0, score=11557.662242, test/ctc_loss=0.8258519768714905, test/num_examples=2472, test/wer=0.269555, total_duration=12671.982890, train/ctc_loss=1.1055752038955688, train/wer=0.344730, validation/ctc_loss=1.1436431407928467, validation/num_examples=5348, validation/wer=0.335412
I0220 11:22:34.672534 140468535932672 logging_writer.py:48] [15000] global_step=15000, grad_norm=3.938117504119873, loss=2.059314489364624
I0220 11:23:50.054208 140483498055424 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.7411221265792847, loss=2.0436112880706787
I0220 11:25:05.342870 140468535932672 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.1717420816421509, loss=2.2039504051208496
I0220 11:26:20.609379 140483498055424 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.8956037163734436, loss=2.0652551651000977
I0220 11:27:35.913361 140468535932672 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.703508734703064, loss=2.0308449268341064
I0220 11:28:57.231406 140483498055424 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.6155661940574646, loss=2.1425445079803467
I0220 11:30:12.493909 140468535932672 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.5962907075881958, loss=2.064638614654541
I0220 11:31:27.750998 140483498055424 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.6453045606613159, loss=2.0866639614105225
I0220 11:32:43.124160 140468535932672 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.9589930772781372, loss=2.0413644313812256
I0220 11:33:58.388402 140483498055424 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.5996679067611694, loss=2.0686874389648438
I0220 11:35:13.706928 140468535932672 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.6249958872795105, loss=2.0994338989257812
I0220 11:36:31.520017 140483498055424 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.9221333861351013, loss=1.9658229351043701
I0220 11:37:53.370685 140468535932672 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.9567429423332214, loss=1.9920940399169922
I0220 11:39:13.644632 140483498055424 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.6000644564628601, loss=2.0603911876678467
I0220 11:40:34.241054 140468535932672 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.9533133506774902, loss=2.066148519515991
I0220 11:41:57.423190 140483498055424 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.6847774386405945, loss=2.0220017433166504
I0220 11:43:12.605557 140468535932672 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.787781298160553, loss=2.015672206878662
I0220 11:44:28.049933 140483498055424 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.7040919661521912, loss=2.031184196472168
I0220 11:45:31.610443 140549388556096 spec.py:321] Evaluating on the training split.
I0220 11:46:24.431088 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 11:47:15.730876 140549388556096 spec.py:349] Evaluating on the test split.
I0220 11:47:41.724732 140549388556096 submission_runner.py:408] Time since start: 14242.23s, 	Step: 16786, 	{'train/ctc_loss': Array(1.0170374, dtype=float32), 'train/wer': 0.3245074974715272, 'validation/ctc_loss': Array(1.073623, dtype=float32), 'validation/wer': 0.31915386620581787, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.76820284, dtype=float32), 'test/wer': 0.2563930696890297, 'test/num_examples': 2472, 'score': 12997.665132284164, 'total_duration': 14242.229521751404, 'accumulated_submission_time': 12997.665132284164, 'accumulated_eval_time': 1243.396124124527, 'accumulated_logging_time': 0.46518969535827637}
I0220 11:47:41.759368 140483498055424 logging_writer.py:48] [16786] accumulated_eval_time=1243.396124, accumulated_logging_time=0.465190, accumulated_submission_time=12997.665132, global_step=16786, preemption_count=0, score=12997.665132, test/ctc_loss=0.7682028412818909, test/num_examples=2472, test/wer=0.256393, total_duration=14242.229522, train/ctc_loss=1.0170373916625977, train/wer=0.324507, validation/ctc_loss=1.0736229419708252, validation/num_examples=5348, validation/wer=0.319154
I0220 11:47:53.060642 140468535932672 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.672958493232727, loss=2.0221786499023438
I0220 11:49:08.148375 140483498055424 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.0111174583435059, loss=2.0679357051849365
I0220 11:50:23.431594 140468535932672 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.5938512086868286, loss=2.0254204273223877
I0220 11:51:38.649904 140483498055424 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.7899420857429504, loss=2.0673980712890625
I0220 11:52:53.873866 140468535932672 logging_writer.py:48] [17200] global_step=17200, grad_norm=2.01554274559021, loss=2.017040967941284
I0220 11:54:09.120109 140483498055424 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.5490272641181946, loss=1.9959815740585327
I0220 11:55:29.471280 140468535932672 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.959818422794342, loss=2.020726203918457
I0220 11:56:52.399844 140483498055424 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.8134071230888367, loss=2.0079636573791504
I0220 11:58:11.553808 140483498055424 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.6850100755691528, loss=2.0777294635772705
I0220 11:59:26.739896 140468535932672 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.8931939601898193, loss=2.038607597351074
I0220 12:00:41.970950 140483498055424 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.5498803853988647, loss=2.0837907791137695
I0220 12:01:57.649320 140468535932672 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.8193280696868896, loss=2.04831862449646
I0220 12:03:13.035333 140483498055424 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.1593279838562012, loss=1.9927151203155518
I0220 12:04:28.359710 140468535932672 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.5003970861434937, loss=1.9427881240844727
I0220 12:05:47.443461 140483498055424 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.679506242275238, loss=2.014244556427002
I0220 12:07:08.482638 140468535932672 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.6753150820732117, loss=2.0286786556243896
I0220 12:08:29.663024 140483498055424 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.023919701576233, loss=1.9950895309448242
I0220 12:09:50.596822 140468535932672 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.1279102563858032, loss=2.0284292697906494
I0220 12:11:11.287012 140483498055424 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.7043534517288208, loss=1.990793228149414
I0220 12:11:41.817193 140549388556096 spec.py:321] Evaluating on the training split.
I0220 12:12:34.274482 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 12:13:25.166354 140549388556096 spec.py:349] Evaluating on the test split.
I0220 12:13:50.851773 140549388556096 submission_runner.py:408] Time since start: 15811.36s, 	Step: 18642, 	{'train/ctc_loss': Array(0.9199811, dtype=float32), 'train/wer': 0.30017488435067136, 'validation/ctc_loss': Array(1.0531265, dtype=float32), 'validation/wer': 0.3134962395126331, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.7460582, dtype=float32), 'test/wer': 0.24928401681798792, 'test/num_examples': 2472, 'score': 14437.630206108093, 'total_duration': 15811.356358766556, 'accumulated_submission_time': 14437.630206108093, 'accumulated_eval_time': 1372.4247515201569, 'accumulated_logging_time': 0.5202996730804443}
I0220 12:13:50.887832 140483498055424 logging_writer.py:48] [18642] accumulated_eval_time=1372.424752, accumulated_logging_time=0.520300, accumulated_submission_time=14437.630206, global_step=18642, preemption_count=0, score=14437.630206, test/ctc_loss=0.7460582256317139, test/num_examples=2472, test/wer=0.249284, total_duration=15811.356359, train/ctc_loss=0.9199811220169067, train/wer=0.300175, validation/ctc_loss=1.0531264543533325, validation/num_examples=5348, validation/wer=0.313496
I0220 12:14:35.201439 140468535932672 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.9238938689231873, loss=1.9594675302505493
I0220 12:15:50.534788 140483498055424 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.152703881263733, loss=2.002117395401001
I0220 12:17:05.884709 140468535932672 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.9548838138580322, loss=1.944743037223816
I0220 12:18:21.324416 140483498055424 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.117866039276123, loss=2.0042121410369873
I0220 12:19:36.880603 140468535932672 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.6076883673667908, loss=1.9522112607955933
I0220 12:20:52.132766 140483498055424 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.6404608488082886, loss=1.9845643043518066
I0220 12:22:07.456400 140468535932672 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.0229679346084595, loss=2.017634153366089
I0220 12:23:27.218451 140483498055424 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.0944960117340088, loss=1.9969474077224731
I0220 12:24:47.903170 140468535932672 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.6616868376731873, loss=2.03167462348938
I0220 12:26:10.796389 140483498055424 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.7458127737045288, loss=1.9497368335723877
I0220 12:27:26.006920 140468535932672 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.5945818424224854, loss=1.956264615058899
I0220 12:28:41.354969 140483498055424 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.7296847105026245, loss=2.021132707595825
I0220 12:29:56.754767 140468535932672 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.6373963356018066, loss=1.987845540046692
I0220 12:31:12.065485 140483498055424 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.8593361377716064, loss=1.946221947669983
I0220 12:32:27.325982 140468535932672 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.6476995348930359, loss=1.9550319910049438
I0220 12:33:45.141744 140483498055424 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.9604317545890808, loss=1.9513977766036987
I0220 12:35:06.852564 140468535932672 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.0744253396987915, loss=1.9569659233093262
I0220 12:36:27.397647 140483498055424 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.6469888687133789, loss=2.036370277404785
I0220 12:37:49.124864 140468535932672 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.9807722568511963, loss=1.957607626914978
I0220 12:37:51.153576 140549388556096 spec.py:321] Evaluating on the training split.
I0220 12:38:43.241533 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 12:39:34.075048 140549388556096 spec.py:349] Evaluating on the test split.
I0220 12:40:00.116934 140549388556096 submission_runner.py:408] Time since start: 17380.62s, 	Step: 20504, 	{'train/ctc_loss': Array(0.8688427, dtype=float32), 'train/wer': 0.28561108594820517, 'validation/ctc_loss': Array(1.0261918, dtype=float32), 'validation/wer': 0.30537667628913756, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.7135042, dtype=float32), 'test/wer': 0.23792984380395263, 'test/num_examples': 2472, 'score': 15877.807674646378, 'total_duration': 17380.621470928192, 'accumulated_submission_time': 15877.807674646378, 'accumulated_eval_time': 1501.3820896148682, 'accumulated_logging_time': 0.5721523761749268}
I0220 12:40:00.159467 140483498055424 logging_writer.py:48] [20504] accumulated_eval_time=1501.382090, accumulated_logging_time=0.572152, accumulated_submission_time=15877.807675, global_step=20504, preemption_count=0, score=15877.807675, test/ctc_loss=0.7135041952133179, test/num_examples=2472, test/wer=0.237930, total_duration=17380.621471, train/ctc_loss=0.8688427209854126, train/wer=0.285611, validation/ctc_loss=1.0261918306350708, validation/num_examples=5348, validation/wer=0.305377
I0220 12:41:16.515580 140483498055424 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.0021229982376099, loss=2.0245625972747803
I0220 12:42:31.617805 140468535932672 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.5755157470703125, loss=1.9244252443313599
I0220 12:43:46.884049 140483498055424 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.7861561179161072, loss=1.9431066513061523
I0220 12:45:02.222918 140468535932672 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.6697211265563965, loss=1.9060876369476318
I0220 12:46:17.635007 140483498055424 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.8894845843315125, loss=1.9901361465454102
I0220 12:47:32.970807 140468535932672 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.6089118123054504, loss=1.9165008068084717
I0220 12:48:49.733699 140483498055424 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.573036253452301, loss=1.9136087894439697
I0220 12:50:10.879253 140468535932672 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.7388395071029663, loss=1.942788004875183
I0220 12:51:32.418033 140483498055424 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.8941996693611145, loss=1.8842101097106934
I0220 12:52:53.123563 140468535932672 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.8143495321273804, loss=1.9578039646148682
I0220 12:54:14.042845 140483498055424 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.6946673393249512, loss=1.9175630807876587
I0220 12:55:34.110758 140483498055424 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.8277729749679565, loss=1.946964144706726
I0220 12:56:49.368390 140468535932672 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.9471055865287781, loss=1.9119572639465332
I0220 12:58:04.785053 140483498055424 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.7661786079406738, loss=1.913801908493042
I0220 12:59:20.122564 140468535932672 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.8845927119255066, loss=1.9531038999557495
I0220 13:00:35.613062 140483498055424 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.5587651133537292, loss=1.9364707469940186
I0220 13:01:50.993824 140468535932672 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.084248661994934, loss=1.9495294094085693
I0220 13:03:07.401541 140483498055424 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.6974018812179565, loss=1.9047476053237915
I0220 13:04:00.730757 140549388556096 spec.py:321] Evaluating on the training split.
I0220 13:04:53.943027 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 13:05:44.608930 140549388556096 spec.py:349] Evaluating on the test split.
I0220 13:06:11.376944 140549388556096 submission_runner.py:408] Time since start: 18951.88s, 	Step: 22366, 	{'train/ctc_loss': Array(0.8891562, dtype=float32), 'train/wer': 0.29315699773507053, 'validation/ctc_loss': Array(0.9662605, dtype=float32), 'validation/wer': 0.2899099220869498, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.66886187, dtype=float32), 'test/wer': 0.22558040338797147, 'test/num_examples': 2472, 'score': 17318.290395498276, 'total_duration': 18951.881212711334, 'accumulated_submission_time': 17318.290395498276, 'accumulated_eval_time': 1632.0220003128052, 'accumulated_logging_time': 0.6320977210998535}
I0220 13:06:11.413424 140483498055424 logging_writer.py:48] [22366] accumulated_eval_time=1632.022000, accumulated_logging_time=0.632098, accumulated_submission_time=17318.290395, global_step=22366, preemption_count=0, score=17318.290395, test/ctc_loss=0.6688618659973145, test/num_examples=2472, test/wer=0.225580, total_duration=18951.881213, train/ctc_loss=0.8891562223434448, train/wer=0.293157, validation/ctc_loss=0.9662604928016663, validation/num_examples=5348, validation/wer=0.289910
I0220 13:06:37.757843 140468535932672 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.7633194923400879, loss=1.9132713079452515
I0220 13:07:53.013868 140483498055424 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.6091135144233704, loss=1.8433438539505005
I0220 13:09:08.659722 140468535932672 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.7173610925674438, loss=1.878743052482605
I0220 13:10:27.738854 140483498055424 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.9196650981903076, loss=1.9007022380828857
I0220 13:11:43.184072 140468535932672 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.5779180526733398, loss=1.8617271184921265
I0220 13:12:58.502875 140483498055424 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.7200668454170227, loss=1.9102864265441895
I0220 13:14:13.982792 140468535932672 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.7807685732841492, loss=1.8497436046600342
I0220 13:15:29.352052 140483498055424 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.6632601022720337, loss=1.7978094816207886
I0220 13:16:44.673709 140468535932672 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.5577866435050964, loss=1.8925918340682983
I0220 13:18:01.101804 140483498055424 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.110751748085022, loss=1.9378031492233276
I0220 13:19:21.762639 140468535932672 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.8763906359672546, loss=1.8515902757644653
I0220 13:20:42.964811 140483498055424 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.9392971992492676, loss=1.9070532321929932
I0220 13:22:03.941640 140468535932672 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.6295260787010193, loss=1.900282859802246
I0220 13:23:28.292810 140483498055424 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.868692934513092, loss=1.8106154203414917
I0220 13:24:43.879954 140468535932672 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.5311120748519897, loss=1.8801729679107666
I0220 13:25:59.502937 140483498055424 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.6733787655830383, loss=1.868865966796875
I0220 13:27:14.894247 140468535932672 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.897470235824585, loss=1.872032642364502
I0220 13:28:30.409485 140483498055424 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.5788261890411377, loss=1.854627251625061
I0220 13:29:45.864474 140468535932672 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.7033053040504456, loss=1.8602572679519653
I0220 13:30:11.948482 140549388556096 spec.py:321] Evaluating on the training split.
I0220 13:31:04.365588 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 13:31:54.622783 140549388556096 spec.py:349] Evaluating on the test split.
I0220 13:32:20.466641 140549388556096 submission_runner.py:408] Time since start: 20520.89s, 	Step: 24236, 	{'train/ctc_loss': Array(0.8207097, dtype=float32), 'train/wer': 0.27074612720154345, 'validation/ctc_loss': Array(0.93003184, dtype=float32), 'validation/wer': 0.28164553906755363, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6421993, dtype=float32), 'test/wer': 0.21526212093514513, 'test/num_examples': 2472, 'score': 18758.737268686295, 'total_duration': 20520.893973350525, 'accumulated_submission_time': 18758.737268686295, 'accumulated_eval_time': 1760.4569537639618, 'accumulated_logging_time': 0.6845858097076416}
I0220 13:32:20.508151 140483498055424 logging_writer.py:48] [24236] accumulated_eval_time=1760.456954, accumulated_logging_time=0.684586, accumulated_submission_time=18758.737269, global_step=24236, preemption_count=0, score=18758.737269, test/ctc_loss=0.6421992778778076, test/num_examples=2472, test/wer=0.215262, total_duration=20520.893973, train/ctc_loss=0.8207097053527832, train/wer=0.270746, validation/ctc_loss=0.9300318360328674, validation/num_examples=5348, validation/wer=0.281646
I0220 13:33:09.383480 140468535932672 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.5692792534828186, loss=1.8591504096984863
I0220 13:34:24.678403 140483498055424 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.1627238988876343, loss=1.9487601518630981
I0220 13:35:40.201759 140468535932672 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.7070600986480713, loss=1.7880544662475586
I0220 13:36:55.563410 140483498055424 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.4780408442020416, loss=1.8458592891693115
I0220 13:38:10.919310 140468535932672 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.8477100133895874, loss=1.8002556562423706
I0220 13:39:29.493916 140483498055424 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.9144837260246277, loss=1.867348313331604
I0220 13:40:45.158805 140468535932672 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.8769281506538391, loss=1.8591663837432861
I0220 13:42:00.498968 140483498055424 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.8082588911056519, loss=1.8184853792190552
I0220 13:43:15.864952 140468535932672 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.6256504654884338, loss=1.8298908472061157
I0220 13:44:31.233461 140483498055424 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.5162184834480286, loss=1.8870888948440552
I0220 13:45:46.617042 140468535932672 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.6191222667694092, loss=1.799477458000183
I0220 13:47:05.920004 140483498055424 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.697718620300293, loss=1.8383139371871948
I0220 13:48:26.386039 140468535932672 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.5670296549797058, loss=1.8379144668579102
I0220 13:49:48.199883 140483498055424 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.5769881010055542, loss=1.7926021814346313
I0220 13:51:09.029718 140468535932672 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.7124559283256531, loss=1.820855736732483
I0220 13:52:30.029507 140483498055424 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.49737897515296936, loss=1.817520260810852
I0220 13:53:45.550386 140468535932672 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.5240122675895691, loss=1.7732911109924316
I0220 13:55:01.073213 140483498055424 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.5242907404899597, loss=1.86257803440094
I0220 13:56:16.890139 140468535932672 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.0021268129348755, loss=1.8471784591674805
I0220 13:56:20.392936 140549388556096 spec.py:321] Evaluating on the training split.
I0220 13:57:13.943423 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 13:58:04.373107 140549388556096 spec.py:349] Evaluating on the test split.
I0220 13:58:30.318612 140549388556096 submission_runner.py:408] Time since start: 22090.82s, 	Step: 26106, 	{'train/ctc_loss': Array(0.8356706, dtype=float32), 'train/wer': 0.2724414374372397, 'validation/ctc_loss': Array(0.9050223, dtype=float32), 'validation/wer': 0.2752927773540458, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6293675, dtype=float32), 'test/wer': 0.216257388337091, 'test/num_examples': 2472, 'score': 20198.5342566967, 'total_duration': 22090.823832035065, 'accumulated_submission_time': 20198.5342566967, 'accumulated_eval_time': 1890.377283334732, 'accumulated_logging_time': 0.7414276599884033}
I0220 13:58:30.355326 140483498055424 logging_writer.py:48] [26106] accumulated_eval_time=1890.377283, accumulated_logging_time=0.741428, accumulated_submission_time=20198.534257, global_step=26106, preemption_count=0, score=20198.534257, test/ctc_loss=0.629367470741272, test/num_examples=2472, test/wer=0.216257, total_duration=22090.823832, train/ctc_loss=0.8356705904006958, train/wer=0.272441, validation/ctc_loss=0.9050223231315613, validation/num_examples=5348, validation/wer=0.275293
I0220 13:59:41.733518 140468535932672 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.708987832069397, loss=1.7940770387649536
I0220 14:00:57.132210 140483498055424 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.5757558941841125, loss=1.846488356590271
I0220 14:02:12.516735 140468535932672 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.5360023379325867, loss=1.8609877824783325
I0220 14:03:27.936105 140483498055424 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.5962267518043518, loss=1.8362776041030884
I0220 14:04:43.306534 140468535932672 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.5729647278785706, loss=1.7713778018951416
I0220 14:06:02.086343 140483498055424 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.7169621586799622, loss=1.855634093284607
I0220 14:07:25.626584 140483498055424 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.6123018264770508, loss=1.8108590841293335
I0220 14:08:40.943662 140468535932672 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.5199642181396484, loss=1.7943320274353027
I0220 14:09:56.294977 140483498055424 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.6047461628913879, loss=1.730441689491272
I0220 14:11:11.741362 140468535932672 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.5094010233879089, loss=1.8381749391555786
I0220 14:12:26.993151 140483498055424 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.6849943399429321, loss=1.8113247156143188
I0220 14:13:42.584050 140468535932672 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.5764075517654419, loss=1.8257049322128296
I0220 14:14:57.882524 140483498055424 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.5001406073570251, loss=1.7807916402816772
I0220 14:16:17.319534 140468535932672 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.5520035028457642, loss=1.7762335538864136
I0220 14:17:38.617683 140483498055424 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.682680606842041, loss=1.8641650676727295
I0220 14:18:59.243845 140468535932672 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.4561135768890381, loss=1.7483534812927246
I0220 14:20:20.411823 140483498055424 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.4398419260978699, loss=1.7780801057815552
I0220 14:21:39.507833 140483498055424 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.6596686840057373, loss=1.7730097770690918
I0220 14:22:30.561046 140549388556096 spec.py:321] Evaluating on the training split.
I0220 14:23:24.152392 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 14:24:15.538208 140549388556096 spec.py:349] Evaluating on the test split.
I0220 14:24:41.224202 140549388556096 submission_runner.py:408] Time since start: 23661.73s, 	Step: 27969, 	{'train/ctc_loss': Array(0.7512261, dtype=float32), 'train/wer': 0.2498455009324634, 'validation/ctc_loss': Array(0.8914386, dtype=float32), 'validation/wer': 0.26941309364048005, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6123362, dtype=float32), 'test/wer': 0.20799057542705096, 'test/num_examples': 2472, 'score': 21638.650775671005, 'total_duration': 23661.728536605835, 'accumulated_submission_time': 21638.650775671005, 'accumulated_eval_time': 2021.034260749817, 'accumulated_logging_time': 0.7939121723175049}
I0220 14:24:41.260461 140483498055424 logging_writer.py:48] [27969] accumulated_eval_time=2021.034261, accumulated_logging_time=0.793912, accumulated_submission_time=21638.650776, global_step=27969, preemption_count=0, score=21638.650776, test/ctc_loss=0.6123362183570862, test/num_examples=2472, test/wer=0.207991, total_duration=23661.728537, train/ctc_loss=0.7512261271476746, train/wer=0.249846, validation/ctc_loss=0.8914386034011841, validation/num_examples=5348, validation/wer=0.269413
I0220 14:25:05.419895 140468535932672 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.688564658164978, loss=1.7720140218734741
I0220 14:26:20.815995 140483498055424 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.49931275844573975, loss=1.7878227233886719
I0220 14:27:36.359973 140468535932672 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.6479025483131409, loss=1.824873447418213
I0220 14:28:51.797722 140483498055424 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.5140653848648071, loss=1.7490588426589966
I0220 14:30:07.678442 140468535932672 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.6180465817451477, loss=1.8262253999710083
I0220 14:31:23.084980 140483498055424 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.7772241234779358, loss=1.8261555433273315
I0220 14:32:38.447533 140468535932672 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.6602353453636169, loss=1.762705683708191
I0220 14:33:56.653431 140483498055424 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.5410650372505188, loss=1.8216228485107422
I0220 14:35:17.612440 140468535932672 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.681240975856781, loss=1.8103033304214478
I0220 14:36:38.065904 140483498055424 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.595228374004364, loss=1.8391658067703247
I0220 14:37:53.545854 140468535932672 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.5465709567070007, loss=1.784652829170227
I0220 14:39:08.983205 140483498055424 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.7045761346817017, loss=1.784044861793518
I0220 14:40:24.562190 140468535932672 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.6996086835861206, loss=1.7763193845748901
I0220 14:41:40.074771 140483498055424 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.8515489101409912, loss=1.7731342315673828
I0220 14:42:55.520458 140468535932672 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.5602586269378662, loss=1.747518539428711
I0220 14:44:10.980894 140483498055424 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.6187119483947754, loss=1.7631391286849976
I0220 14:45:30.804727 140468535932672 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.5603161454200745, loss=1.7525744438171387
I0220 14:46:50.812177 140483498055424 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.6651440858840942, loss=1.776748538017273
I0220 14:48:11.607025 140468535932672 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.7648349404335022, loss=2.184455633163452
I0220 14:48:41.447451 140549388556096 spec.py:321] Evaluating on the training split.
I0220 14:49:34.370180 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 14:50:25.183659 140549388556096 spec.py:349] Evaluating on the test split.
I0220 14:50:50.930870 140549388556096 submission_runner.py:408] Time since start: 25231.43s, 	Step: 29839, 	{'train/ctc_loss': Array(1.465784, dtype=float32), 'train/wer': 0.4232012911945225, 'validation/ctc_loss': Array(1.4207444, dtype=float32), 'validation/wer': 0.3876053564015177, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.012293, dtype=float32), 'test/wer': 0.3135701663518372, 'test/num_examples': 2472, 'score': 23078.74978494644, 'total_duration': 25231.434408426285, 'accumulated_submission_time': 23078.74978494644, 'accumulated_eval_time': 2150.510663509369, 'accumulated_logging_time': 0.8455498218536377}
I0220 14:50:50.971026 140483498055424 logging_writer.py:48] [29839] accumulated_eval_time=2150.510664, accumulated_logging_time=0.845550, accumulated_submission_time=23078.749785, global_step=29839, preemption_count=0, score=23078.749785, test/ctc_loss=1.0122929811477661, test/num_examples=2472, test/wer=0.313570, total_duration=25231.434408, train/ctc_loss=1.465783953666687, train/wer=0.423201, validation/ctc_loss=1.4207444190979004, validation/num_examples=5348, validation/wer=0.387605
I0220 14:51:41.221186 140483498055424 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.5097391605377197, loss=1.859344720840454
I0220 14:52:56.424968 140468535932672 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.5093600749969482, loss=1.835462212562561
I0220 14:54:11.856514 140483498055424 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.5774663090705872, loss=1.830478549003601
I0220 14:55:27.311909 140468535932672 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.6802979111671448, loss=1.7642782926559448
I0220 14:56:42.732716 140483498055424 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.6026013493537903, loss=1.750183343887329
I0220 14:57:58.130545 140468535932672 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.7909762859344482, loss=1.8235563039779663
I0220 14:59:13.547412 140483498055424 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.526412308216095, loss=1.726552963256836
I0220 15:00:30.226224 140468535932672 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.6388762593269348, loss=1.8029634952545166
I0220 15:01:51.543179 140483498055424 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.6713921427726746, loss=1.7956006526947021
I0220 15:03:11.865228 140468535932672 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.4836690425872803, loss=1.7969611883163452
I0220 15:04:34.947091 140483498055424 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.7339955568313599, loss=1.7599451541900635
I0220 15:05:50.230271 140468535932672 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.567586362361908, loss=1.7670198678970337
I0220 15:07:05.498940 140483498055424 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.6786970496177673, loss=1.8031774759292603
I0220 15:08:20.840507 140468535932672 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.6303911805152893, loss=1.7641257047653198
I0220 15:09:36.146068 140483498055424 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.6412160396575928, loss=1.7775269746780396
I0220 15:10:51.610504 140468535932672 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.6146988272666931, loss=1.7465704679489136
I0220 15:12:06.958138 140483498055424 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.6501725316047668, loss=1.7794395685195923
I0220 15:13:22.256152 140468535932672 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.6347005367279053, loss=1.722009539604187
I0220 15:14:41.187439 140483498055424 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.5908719301223755, loss=1.7276074886322021
I0220 15:14:51.319949 140549388556096 spec.py:321] Evaluating on the training split.
I0220 15:15:43.938321 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 15:16:35.024081 140549388556096 spec.py:349] Evaluating on the test split.
I0220 15:17:00.777252 140549388556096 submission_runner.py:408] Time since start: 26801.28s, 	Step: 31714, 	{'train/ctc_loss': Array(0.7389771, dtype=float32), 'train/wer': 0.2490417309784311, 'validation/ctc_loss': Array(0.8475726, dtype=float32), 'validation/wer': 0.2582426600500111, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.564204, dtype=float32), 'test/wer': 0.19117258749212926, 'test/num_examples': 2472, 'score': 24519.01113843918, 'total_duration': 26801.28219127655, 'accumulated_submission_time': 24519.01113843918, 'accumulated_eval_time': 2279.962345123291, 'accumulated_logging_time': 0.9009816646575928}
I0220 15:17:00.816789 140483498055424 logging_writer.py:48] [31714] accumulated_eval_time=2279.962345, accumulated_logging_time=0.900982, accumulated_submission_time=24519.011138, global_step=31714, preemption_count=0, score=24519.011138, test/ctc_loss=0.5642039775848389, test/num_examples=2472, test/wer=0.191173, total_duration=26801.282191, train/ctc_loss=0.7389770746231079, train/wer=0.249042, validation/ctc_loss=0.8475726246833801, validation/num_examples=5348, validation/wer=0.258243
I0220 15:18:06.335037 140468535932672 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.5488780736923218, loss=1.7502115964889526
I0220 15:19:21.740813 140483498055424 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.7139312624931335, loss=1.8084862232208252
I0220 15:20:40.614719 140483498055424 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.577438473701477, loss=1.7435663938522339
I0220 15:21:55.916571 140468535932672 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.5559526681900024, loss=1.783316731452942
I0220 15:23:11.329493 140483498055424 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.605106770992279, loss=1.714166522026062
I0220 15:24:26.680377 140468535932672 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.645573616027832, loss=1.7772047519683838
I0220 15:25:42.029515 140483498055424 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.615645706653595, loss=1.7398154735565186
I0220 15:26:57.341741 140468535932672 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.47284621000289917, loss=1.746982216835022
I0220 15:28:12.661398 140483498055424 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.6282523274421692, loss=1.7207707166671753
I0220 15:29:28.181584 140468535932672 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.4968739151954651, loss=1.7435168027877808
I0220 15:30:49.244209 140483498055424 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.5220537781715393, loss=1.7152003049850464
I0220 15:32:09.171668 140468535932672 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.6146270632743835, loss=1.7464439868927002
I0220 15:33:30.394771 140483498055424 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.7263996601104736, loss=1.692253589630127
I0220 15:34:46.019361 140468535932672 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.5717474818229675, loss=1.7087583541870117
I0220 15:36:01.317040 140483498055424 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.604913592338562, loss=1.699070692062378
I0220 15:37:16.787370 140468535932672 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.46984022855758667, loss=1.772431492805481
I0220 15:38:32.163620 140483498055424 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.6469064354896545, loss=1.7310618162155151
I0220 15:39:47.437223 140468535932672 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.6649388670921326, loss=1.7769709825515747
I0220 15:41:00.916279 140549388556096 spec.py:321] Evaluating on the training split.
I0220 15:42:03.627956 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 15:42:53.694363 140549388556096 spec.py:349] Evaluating on the test split.
I0220 15:43:19.700690 140549388556096 submission_runner.py:408] Time since start: 28380.21s, 	Step: 33599, 	{'train/ctc_loss': Array(0.525048, dtype=float32), 'train/wer': 0.1873489863803322, 'validation/ctc_loss': Array(0.8184007, dtype=float32), 'validation/wer': 0.250229298010176, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5521909, dtype=float32), 'test/wer': 0.18952734954197387, 'test/num_examples': 2472, 'score': 25959.022108078003, 'total_duration': 28380.205656528473, 'accumulated_submission_time': 25959.022108078003, 'accumulated_eval_time': 2418.7411739826202, 'accumulated_logging_time': 0.9561948776245117}
I0220 15:43:19.738090 140483498055424 logging_writer.py:48] [33599] accumulated_eval_time=2418.741174, accumulated_logging_time=0.956195, accumulated_submission_time=25959.022108, global_step=33599, preemption_count=0, score=25959.022108, test/ctc_loss=0.552190899848938, test/num_examples=2472, test/wer=0.189527, total_duration=28380.205657, train/ctc_loss=0.525048017501831, train/wer=0.187349, validation/ctc_loss=0.8184006810188293, validation/num_examples=5348, validation/wer=0.250229
I0220 15:43:21.324915 140468535932672 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.5338376760482788, loss=1.7094192504882812
I0220 15:44:36.532376 140483498055424 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.6819796562194824, loss=1.7102495431900024
I0220 15:45:51.929495 140468535932672 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.5977730751037598, loss=1.7836978435516357
I0220 15:47:07.404343 140483498055424 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.6917420029640198, loss=1.7249406576156616
I0220 15:48:26.202661 140483498055424 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.7792282104492188, loss=1.7308241128921509
I0220 15:49:41.548281 140468535932672 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.5526915192604065, loss=1.76865553855896
I0220 15:50:56.902295 140483498055424 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.5637979507446289, loss=1.6588940620422363
I0220 15:52:12.573108 140468535932672 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.7525551915168762, loss=1.6955420970916748
I0220 15:53:28.098422 140483498055424 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.646496057510376, loss=1.7278488874435425
I0220 15:54:43.510679 140468535932672 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.5894280076026917, loss=1.6646101474761963
I0220 15:55:59.019808 140483498055424 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.020449161529541, loss=1.825776219367981
I0220 15:57:14.340638 140468535932672 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.515383243560791, loss=1.6947004795074463
I0220 15:58:34.734959 140483498055424 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.5913262963294983, loss=1.7207800149917603
I0220 15:59:55.339555 140468535932672 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.5603625774383545, loss=1.6929583549499512
I0220 16:01:16.434713 140483498055424 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.5618608593940735, loss=1.657776117324829
I0220 16:02:36.134428 140483498055424 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.64056396484375, loss=1.7491446733474731
I0220 16:03:51.686346 140468535932672 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.619971513748169, loss=1.6959702968597412
I0220 16:05:07.295808 140483498055424 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.6576630473136902, loss=1.6769315004348755
I0220 16:06:22.738846 140468535932672 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.703797459602356, loss=1.724716305732727
I0220 16:07:19.936127 140549388556096 spec.py:321] Evaluating on the training split.
I0220 16:08:14.787223 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 16:09:06.605651 140549388556096 spec.py:349] Evaluating on the test split.
I0220 16:09:32.139116 140549388556096 submission_runner.py:408] Time since start: 29952.64s, 	Step: 35477, 	{'train/ctc_loss': Array(0.4653179, dtype=float32), 'train/wer': 0.17109447016540824, 'validation/ctc_loss': Array(0.8033444, dtype=float32), 'validation/wer': 0.24607779719435782, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.52806854, dtype=float32), 'test/wer': 0.18117929031340768, 'test/num_examples': 2472, 'score': 27399.132900476456, 'total_duration': 29952.644336223602, 'accumulated_submission_time': 27399.132900476456, 'accumulated_eval_time': 2550.938821077347, 'accumulated_logging_time': 1.008819818496704}
I0220 16:09:32.181371 140483498055424 logging_writer.py:48] [35477] accumulated_eval_time=2550.938821, accumulated_logging_time=1.008820, accumulated_submission_time=27399.132900, global_step=35477, preemption_count=0, score=27399.132900, test/ctc_loss=0.5280685424804688, test/num_examples=2472, test/wer=0.181179, total_duration=29952.644336, train/ctc_loss=0.46531790494918823, train/wer=0.171094, validation/ctc_loss=0.8033444285392761, validation/num_examples=5348, validation/wer=0.246078
I0220 16:09:50.305327 140468535932672 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.5105180740356445, loss=1.6706210374832153
I0220 16:11:05.674995 140483498055424 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.7456993460655212, loss=1.765339732170105
I0220 16:12:21.233759 140468535932672 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.7908979654312134, loss=1.6973119974136353
I0220 16:13:36.810217 140483498055424 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.5081404447555542, loss=1.7090554237365723
I0220 16:14:52.426444 140468535932672 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.59706050157547, loss=1.7159134149551392
I0220 16:16:08.024290 140483498055424 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.6677785515785217, loss=1.672066330909729
I0220 16:17:26.968786 140483498055424 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.6529145240783691, loss=1.6718112230300903
I0220 16:18:42.491853 140468535932672 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.6006100177764893, loss=1.6900484561920166
I0220 16:19:58.057435 140483498055424 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.5825555920600891, loss=1.6700299978256226
I0220 16:21:13.673834 140468535932672 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.7183851003646851, loss=1.6445391178131104
I0220 16:22:29.359015 140483498055424 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.5680066347122192, loss=1.69148588180542
I0220 16:23:44.938081 140468535932672 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.528099775314331, loss=1.6650710105895996
I0220 16:25:01.208891 140483498055424 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.7809817790985107, loss=1.6741052865982056
I0220 16:26:21.742643 140468535932672 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.6491355895996094, loss=1.6802924871444702
I0220 16:27:42.986737 140483498055424 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.6669612526893616, loss=1.7055656909942627
I0220 16:29:03.743380 140468535932672 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.609265148639679, loss=1.734273076057434
I0220 16:30:26.463866 140483498055424 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.6649357676506042, loss=1.6099786758422852
I0220 16:31:41.863592 140468535932672 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.6687014698982239, loss=1.6963751316070557
I0220 16:32:57.250442 140483498055424 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.5405580401420593, loss=1.684539556503296
I0220 16:33:32.374283 140549388556096 spec.py:321] Evaluating on the training split.
I0220 16:34:26.707257 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 16:35:17.306182 140549388556096 spec.py:349] Evaluating on the test split.
I0220 16:35:43.641908 140549388556096 submission_runner.py:408] Time since start: 31524.15s, 	Step: 37348, 	{'train/ctc_loss': Array(0.44106385, dtype=float32), 'train/wer': 0.15810233143636412, 'validation/ctc_loss': Array(0.7626492, dtype=float32), 'validation/wer': 0.2333819284204022, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.49983346, dtype=float32), 'test/wer': 0.17283123108484147, 'test/num_examples': 2472, 'score': 28839.237825155258, 'total_duration': 31524.145983934402, 'accumulated_submission_time': 28839.237825155258, 'accumulated_eval_time': 2682.19997549057, 'accumulated_logging_time': 1.0661985874176025}
I0220 16:35:43.681146 140483498055424 logging_writer.py:48] [37348] accumulated_eval_time=2682.199975, accumulated_logging_time=1.066199, accumulated_submission_time=28839.237825, global_step=37348, preemption_count=0, score=28839.237825, test/ctc_loss=0.49983346462249756, test/num_examples=2472, test/wer=0.172831, total_duration=31524.145984, train/ctc_loss=0.44106385111808777, train/wer=0.158102, validation/ctc_loss=0.7626491785049438, validation/num_examples=5348, validation/wer=0.233382
I0220 16:36:23.557655 140468535932672 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.7789803743362427, loss=1.676657795906067
I0220 16:37:38.905641 140483498055424 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.7928770780563354, loss=1.682610273361206
I0220 16:38:54.278206 140468535932672 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.5366047024726868, loss=1.6885249614715576
I0220 16:40:09.681786 140483498055424 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.6192064881324768, loss=1.7196098566055298
I0220 16:41:25.601299 140468535932672 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.6695318222045898, loss=1.6504353284835815
I0220 16:42:41.043686 140483498055424 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.6200490593910217, loss=1.6511121988296509
I0220 16:44:00.200954 140468535932672 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.583164393901825, loss=1.68095862865448
I0220 16:45:20.997918 140483498055424 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.5763528347015381, loss=1.6766928434371948
I0220 16:46:39.925803 140483498055424 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.6987330913543701, loss=1.701093077659607
I0220 16:47:55.407628 140468535932672 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.7149292826652527, loss=1.6989762783050537
I0220 16:49:10.983537 140483498055424 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.6679242253303528, loss=1.633358120918274
I0220 16:50:26.499744 140468535932672 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.7321295738220215, loss=1.633848786354065
I0220 16:51:42.090795 140483498055424 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.5217205286026001, loss=1.6380435228347778
I0220 16:52:57.607942 140468535932672 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.5896861553192139, loss=1.6450413465499878
I0220 16:54:14.913523 140483498055424 logging_writer.py:48] [38800] global_step=38800, grad_norm=2.642927885055542, loss=1.6994062662124634
I0220 16:55:35.173402 140468535932672 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.5323859453201294, loss=1.6561923027038574
I0220 16:56:55.531820 140483498055424 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.6013656258583069, loss=1.635366678237915
I0220 16:58:16.608737 140468535932672 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.6430345177650452, loss=1.6581461429595947
I0220 16:59:37.351941 140483498055424 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.5367043018341064, loss=1.622670292854309
I0220 16:59:43.843126 140549388556096 spec.py:321] Evaluating on the training split.
I0220 17:00:38.393950 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 17:01:29.255047 140549388556096 spec.py:349] Evaluating on the test split.
I0220 17:01:55.057717 140549388556096 submission_runner.py:408] Time since start: 33095.56s, 	Step: 39210, 	{'train/ctc_loss': Array(0.41415095, dtype=float32), 'train/wer': 0.1518818386404201, 'validation/ctc_loss': Array(0.74457777, dtype=float32), 'validation/wer': 0.2290180252372631, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.48848337, dtype=float32), 'test/wer': 0.16635183718237767, 'test/num_examples': 2472, 'score': 30279.30905532837, 'total_duration': 33095.56187868118, 'accumulated_submission_time': 30279.30905532837, 'accumulated_eval_time': 2813.408171415329, 'accumulated_logging_time': 1.1235594749450684}
I0220 17:01:55.104240 140483498055424 logging_writer.py:48] [39210] accumulated_eval_time=2813.408171, accumulated_logging_time=1.123559, accumulated_submission_time=30279.309055, global_step=39210, preemption_count=0, score=30279.309055, test/ctc_loss=0.48848336935043335, test/num_examples=2472, test/wer=0.166352, total_duration=33095.561879, train/ctc_loss=0.4141509532928467, train/wer=0.151882, validation/ctc_loss=0.7445777654647827, validation/num_examples=5348, validation/wer=0.229018
I0220 17:03:03.390036 140468535932672 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.733994722366333, loss=1.639747977256775
I0220 17:04:18.634967 140483498055424 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.524810254573822, loss=1.621830940246582
I0220 17:05:33.939429 140468535932672 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.5651952624320984, loss=1.6196231842041016
I0220 17:06:49.300809 140483498055424 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.5853258371353149, loss=1.6466890573501587
I0220 17:08:04.658212 140468535932672 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.619042694568634, loss=1.6578624248504639
I0220 17:09:20.035062 140483498055424 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.6323058009147644, loss=1.5401984453201294
I0220 17:10:35.318574 140468535932672 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.7789238691329956, loss=1.666872262954712
I0220 17:11:52.998678 140483498055424 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.6082372069358826, loss=1.6858570575714111
I0220 17:13:14.134173 140468535932672 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.8007063269615173, loss=1.6281964778900146
I0220 17:14:36.451089 140483498055424 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.6416214108467102, loss=1.611730933189392
I0220 17:15:51.684960 140468535932672 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.600863516330719, loss=1.5924725532531738
I0220 17:17:07.049162 140483498055424 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.6888905763626099, loss=1.5610541105270386
I0220 17:18:22.471550 140468535932672 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.6060915589332581, loss=1.6179544925689697
I0220 17:19:38.005733 140483498055424 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.5711917281150818, loss=1.618083119392395
I0220 17:20:53.422048 140468535932672 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.840815007686615, loss=1.6496014595031738
I0220 17:22:08.925146 140483498055424 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.5539934039115906, loss=1.6584432125091553
I0220 17:23:28.298230 140468535932672 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.7180144786834717, loss=1.6217204332351685
I0220 17:24:48.136543 140483498055424 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.777350127696991, loss=1.6201964616775513
I0220 17:25:55.329793 140549388556096 spec.py:321] Evaluating on the training split.
I0220 17:26:49.560216 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 17:27:40.001292 140549388556096 spec.py:349] Evaluating on the test split.
I0220 17:28:06.693951 140549388556096 submission_runner.py:408] Time since start: 34667.20s, 	Step: 41085, 	{'train/ctc_loss': Array(0.4113284, dtype=float32), 'train/wer': 0.15069143569351298, 'validation/ctc_loss': Array(0.7187782, dtype=float32), 'validation/wer': 0.2191123511976597, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.46268943, dtype=float32), 'test/wer': 0.15932403062986208, 'test/num_examples': 2472, 'score': 31719.445224285126, 'total_duration': 34667.19851708412, 'accumulated_submission_time': 31719.445224285126, 'accumulated_eval_time': 2944.7663497924805, 'accumulated_logging_time': 1.187300205230713}
I0220 17:28:06.737266 140483498055424 logging_writer.py:48] [41085] accumulated_eval_time=2944.766350, accumulated_logging_time=1.187300, accumulated_submission_time=31719.445224, global_step=41085, preemption_count=0, score=31719.445224, test/ctc_loss=0.46268942952156067, test/num_examples=2472, test/wer=0.159324, total_duration=34667.198517, train/ctc_loss=0.41132840514183044, train/wer=0.150691, validation/ctc_loss=0.7187781929969788, validation/num_examples=5348, validation/wer=0.219112
I0220 17:28:18.850273 140468535932672 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.6312901377677917, loss=1.6108615398406982
I0220 17:29:37.575117 140483498055424 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.7719041109085083, loss=1.558031439781189
I0220 17:30:53.173275 140468535932672 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.66936194896698, loss=1.5793622732162476
I0220 17:32:08.618029 140483498055424 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.7066393494606018, loss=1.6211557388305664
I0220 17:33:24.076002 140468535932672 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.48843759298324585, loss=1.5222281217575073
I0220 17:34:39.512819 140483498055424 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.6070342063903809, loss=1.6147912740707397
I0220 17:35:55.016906 140468535932672 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.5359392166137695, loss=1.5495692491531372
I0220 17:37:10.480746 140483498055424 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.5157005190849304, loss=1.5434612035751343
I0220 17:38:31.941796 140468535932672 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.6481884121894836, loss=1.5895426273345947
I0220 17:39:52.128063 140483498055424 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.6206499934196472, loss=1.6165812015533447
I0220 17:41:12.926774 140468535932672 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.7542652487754822, loss=1.6942520141601562
I0220 17:42:34.029228 140483498055424 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.678887665271759, loss=1.6311012506484985
I0220 17:43:54.453269 140483498055424 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.6053136587142944, loss=1.6105238199234009
I0220 17:45:09.904469 140468535932672 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.6549160480499268, loss=1.6313140392303467
I0220 17:46:25.743911 140483498055424 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.6800510287284851, loss=1.5816031694412231
I0220 17:47:41.235900 140468535932672 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.6059867739677429, loss=1.623022437095642
I0220 17:48:56.688251 140483498055424 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.5761294364929199, loss=1.5207574367523193
I0220 17:50:12.134364 140468535932672 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.8062068819999695, loss=1.582414984703064
I0220 17:51:27.616502 140483498055424 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.7868552803993225, loss=1.617200493812561
I0220 17:52:07.253314 140549388556096 spec.py:321] Evaluating on the training split.
I0220 17:53:02.571243 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 17:53:53.177325 140549388556096 spec.py:349] Evaluating on the test split.
I0220 17:54:18.968453 140549388556096 submission_runner.py:408] Time since start: 36239.47s, 	Step: 42952, 	{'train/ctc_loss': Array(0.38013545, dtype=float32), 'train/wer': 0.14035069309631992, 'validation/ctc_loss': Array(0.7010598, dtype=float32), 'validation/wer': 0.21693039960609015, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4484339, dtype=float32), 'test/wer': 0.15552576523876263, 'test/num_examples': 2472, 'score': 33159.86926102638, 'total_duration': 36239.47293305397, 'accumulated_submission_time': 33159.86926102638, 'accumulated_eval_time': 3076.475456237793, 'accumulated_logging_time': 1.2487943172454834}
I0220 17:54:19.008492 140483498055424 logging_writer.py:48] [42952] accumulated_eval_time=3076.475456, accumulated_logging_time=1.248794, accumulated_submission_time=33159.869261, global_step=42952, preemption_count=0, score=33159.869261, test/ctc_loss=0.44843390583992004, test/num_examples=2472, test/wer=0.155526, total_duration=36239.472933, train/ctc_loss=0.3801354467868805, train/wer=0.140351, validation/ctc_loss=0.7010598182678223, validation/num_examples=5348, validation/wer=0.216930
I0220 17:54:55.827052 140468535932672 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.6153169274330139, loss=1.59810471534729
I0220 17:56:11.186374 140483498055424 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.5354157090187073, loss=1.5941531658172607
I0220 17:57:26.558885 140468535932672 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.6295666694641113, loss=1.57908296585083
I0220 17:58:45.325469 140483498055424 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.8168646097183228, loss=1.5747236013412476
I0220 18:00:00.573237 140468535932672 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.7092683911323547, loss=1.6183195114135742
I0220 18:01:15.960039 140483498055424 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.6682891249656677, loss=1.6165721416473389
I0220 18:02:31.356772 140468535932672 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.5945950150489807, loss=1.6149121522903442
I0220 18:03:47.069143 140483498055424 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.6484740376472473, loss=1.6159789562225342
I0220 18:05:02.410288 140468535932672 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.7246302366256714, loss=1.561966061592102
I0220 18:06:17.772478 140483498055424 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.8500878214836121, loss=1.5881901979446411
I0220 18:07:37.210068 140468535932672 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.6927070617675781, loss=1.5480934381484985
I0220 18:08:58.376376 140483498055424 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.7282087802886963, loss=1.5608853101730347
I0220 18:10:18.674597 140468535932672 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.5916644334793091, loss=1.525002121925354
I0220 18:11:42.706805 140483498055424 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.6529882550239563, loss=1.552065134048462
I0220 18:12:58.163827 140468535932672 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.6190614104270935, loss=1.579859972000122
I0220 18:14:13.657086 140483498055424 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.6349454522132874, loss=1.5379456281661987
I0220 18:15:29.164131 140468535932672 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.6151719689369202, loss=1.5670342445373535
I0220 18:16:44.643087 140483498055424 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.7273596525192261, loss=1.5538454055786133
I0220 18:18:00.131884 140468535932672 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.6323353052139282, loss=1.5271409749984741
I0220 18:18:19.694397 140549388556096 spec.py:321] Evaluating on the training split.
I0220 18:19:13.511220 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 18:20:04.394384 140549388556096 spec.py:349] Evaluating on the test split.
I0220 18:20:30.054414 140549388556096 submission_runner.py:408] Time since start: 37810.56s, 	Step: 44827, 	{'train/ctc_loss': Array(0.42511886, dtype=float32), 'train/wer': 0.14946901252179426, 'validation/ctc_loss': Array(0.67514026, dtype=float32), 'validation/wer': 0.2071695453623874, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4318812, dtype=float32), 'test/wer': 0.14880263238072025, 'test/num_examples': 2472, 'score': 34600.46166920662, 'total_duration': 37810.55836892128, 'accumulated_submission_time': 34600.46166920662, 'accumulated_eval_time': 3206.82887673378, 'accumulated_logging_time': 1.3093175888061523}
I0220 18:20:30.094185 140483498055424 logging_writer.py:48] [44827] accumulated_eval_time=3206.828877, accumulated_logging_time=1.309318, accumulated_submission_time=34600.461669, global_step=44827, preemption_count=0, score=34600.461669, test/ctc_loss=0.4318811893463135, test/num_examples=2472, test/wer=0.148803, total_duration=37810.558369, train/ctc_loss=0.4251188635826111, train/wer=0.149469, validation/ctc_loss=0.6751402616500854, validation/num_examples=5348, validation/wer=0.207170
I0220 18:21:25.687437 140468535932672 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.6391177177429199, loss=1.540600299835205
I0220 18:22:40.928766 140483498055424 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.6656058430671692, loss=1.5910598039627075
I0220 18:23:56.216503 140468535932672 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.5826804041862488, loss=1.6084585189819336
I0220 18:25:11.570019 140483498055424 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.7551671862602234, loss=1.6152479648590088
I0220 18:26:26.918088 140468535932672 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.5874692797660828, loss=1.557102084159851
I0220 18:27:45.536209 140483498055424 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.5898528695106506, loss=1.5590604543685913
I0220 18:29:00.945369 140468535932672 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.6423875093460083, loss=1.5646809339523315
I0220 18:30:16.383815 140483498055424 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.6693275570869446, loss=1.5375416278839111
I0220 18:31:31.793003 140468535932672 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.770750880241394, loss=1.6326801776885986
I0220 18:32:47.170482 140483498055424 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.831455647945404, loss=1.5132298469543457
I0220 18:34:02.625624 140468535932672 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.695361852645874, loss=1.5844110250473022
I0220 18:35:19.340754 140483498055424 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.5980845093727112, loss=1.5527091026306152
I0220 18:36:39.260035 140468535932672 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.7172037959098816, loss=1.4929590225219727
I0220 18:37:59.439416 140483498055424 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.7715376615524292, loss=1.4960914850234985
I0220 18:39:19.578897 140468535932672 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.6600589156150818, loss=1.5463241338729858
I0220 18:40:40.739807 140483498055424 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.7055856585502625, loss=1.5192158222198486
I0220 18:41:56.124595 140468535932672 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.6727977395057678, loss=1.5037899017333984
I0220 18:43:11.577618 140483498055424 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.6606102585792542, loss=1.5435923337936401
I0220 18:44:26.933867 140468535932672 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.7095241546630859, loss=1.4917582273483276
I0220 18:44:30.435101 140549388556096 spec.py:321] Evaluating on the training split.
I0220 18:45:24.672628 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 18:46:15.617393 140549388556096 spec.py:349] Evaluating on the test split.
I0220 18:46:41.118485 140549388556096 submission_runner.py:408] Time since start: 39381.62s, 	Step: 46706, 	{'train/ctc_loss': Array(0.36075482, dtype=float32), 'train/wer': 0.13570823174878882, 'validation/ctc_loss': Array(0.66591084, dtype=float32), 'validation/wer': 0.2046786448728965, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42331767, dtype=float32), 'test/wer': 0.1455121564804095, 'test/num_examples': 2472, 'score': 36040.71513128281, 'total_duration': 39381.62409090996, 'accumulated_submission_time': 36040.71513128281, 'accumulated_eval_time': 3337.507324695587, 'accumulated_logging_time': 1.3644671440124512}
I0220 18:46:41.158345 140483498055424 logging_writer.py:48] [46706] accumulated_eval_time=3337.507325, accumulated_logging_time=1.364467, accumulated_submission_time=36040.715131, global_step=46706, preemption_count=0, score=36040.715131, test/ctc_loss=0.42331767082214355, test/num_examples=2472, test/wer=0.145512, total_duration=39381.624091, train/ctc_loss=0.3607548177242279, train/wer=0.135708, validation/ctc_loss=0.6659108400344849, validation/num_examples=5348, validation/wer=0.204679
I0220 18:47:52.529903 140468535932672 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.6642748117446899, loss=1.4736151695251465
I0220 18:49:08.023654 140483498055424 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.777898371219635, loss=1.549774169921875
I0220 18:50:23.390368 140468535932672 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.768505334854126, loss=1.5103302001953125
I0220 18:51:38.731405 140483498055424 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.6300493478775024, loss=1.5470085144042969
I0220 18:52:54.380965 140468535932672 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.7105633616447449, loss=1.5315910577774048
I0220 18:54:10.478606 140483498055424 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.7620434165000916, loss=1.499995231628418
I0220 18:55:33.097442 140483498055424 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.7278165221214294, loss=1.5110901594161987
I0220 18:56:48.373351 140468535932672 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.668349027633667, loss=1.5108544826507568
I0220 18:58:03.679116 140483498055424 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.6756662130355835, loss=1.5437180995941162
I0220 18:59:19.028712 140468535932672 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.6698190569877625, loss=1.5286800861358643
I0220 19:00:34.346906 140483498055424 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.7760975956916809, loss=1.5141034126281738
I0220 19:01:49.696359 140468535932672 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.582068145275116, loss=1.4908360242843628
I0220 19:03:05.153603 140483498055424 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.7890905141830444, loss=1.4866349697113037
I0220 19:04:24.988050 140468535932672 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.6389020085334778, loss=1.5088660717010498
I0220 19:05:45.155806 140483498055424 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.6517424583435059, loss=1.517789363861084
I0220 19:07:07.067733 140468535932672 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.6284186244010925, loss=1.5040287971496582
I0220 19:08:28.045899 140483498055424 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.5806732177734375, loss=1.4974393844604492
I0220 19:09:46.882602 140483498055424 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.7375295162200928, loss=1.4712151288986206
I0220 19:10:41.584150 140549388556096 spec.py:321] Evaluating on the training split.
I0220 19:11:36.473431 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 19:12:26.999875 140549388556096 spec.py:349] Evaluating on the test split.
I0220 19:12:52.677065 140549388556096 submission_runner.py:408] Time since start: 40953.18s, 	Step: 48574, 	{'train/ctc_loss': Array(0.33703363, dtype=float32), 'train/wer': 0.1252548560952095, 'validation/ctc_loss': Array(0.63967717, dtype=float32), 'validation/wer': 0.19583498266989777, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40389547, dtype=float32), 'test/wer': 0.13694066987589626, 'test/num_examples': 2472, 'score': 37481.050798654556, 'total_duration': 40953.1812107563, 'accumulated_submission_time': 37481.050798654556, 'accumulated_eval_time': 3468.593843460083, 'accumulated_logging_time': 1.4214856624603271}
I0220 19:12:52.719416 140483498055424 logging_writer.py:48] [48574] accumulated_eval_time=3468.593843, accumulated_logging_time=1.421486, accumulated_submission_time=37481.050799, global_step=48574, preemption_count=0, score=37481.050799, test/ctc_loss=0.40389546751976013, test/num_examples=2472, test/wer=0.136941, total_duration=40953.181211, train/ctc_loss=0.33703362941741943, train/wer=0.125255, validation/ctc_loss=0.6396771669387817, validation/num_examples=5348, validation/wer=0.195835
I0220 19:13:13.074983 140468535932672 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.6638115644454956, loss=1.5394366979599
I0220 19:14:28.360821 140483498055424 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.6682143807411194, loss=1.4759726524353027
I0220 19:15:43.634273 140468535932672 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.6793837547302246, loss=1.495125412940979
I0220 19:16:59.063530 140483498055424 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.6332725882530212, loss=1.5722763538360596
I0220 19:18:14.426061 140468535932672 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.6652366518974304, loss=1.5677874088287354
I0220 19:19:29.758858 140483498055424 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.6532177925109863, loss=1.4926419258117676
I0220 19:20:45.165768 140468535932672 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.634126603603363, loss=1.4543253183364868
I0220 19:22:04.279335 140483498055424 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.7503189444541931, loss=1.4712692499160767
I0220 19:23:24.578240 140468535932672 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.7303518056869507, loss=1.4409706592559814
I0220 19:24:45.166679 140483498055424 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.5811328291893005, loss=1.4587788581848145
I0220 19:26:00.777960 140468535932672 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.9422260522842407, loss=1.468306303024292
I0220 19:27:16.211599 140483498055424 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.6714683771133423, loss=1.482172966003418
I0220 19:28:31.672125 140468535932672 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.8558697700500488, loss=1.4725806713104248
I0220 19:29:47.012281 140483498055424 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.6281954050064087, loss=1.4659292697906494
I0220 19:31:02.454499 140468535932672 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.6349901556968689, loss=1.4640262126922607
I0220 19:32:18.991385 140483498055424 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.7522692084312439, loss=1.4646986722946167
I0220 19:33:39.561058 140468535932672 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.6624357104301453, loss=1.4987858533859253
I0220 19:34:59.779234 140483498055424 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.6202620267868042, loss=1.487961769104004
I0220 19:36:21.334081 140468535932672 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.6665709614753723, loss=1.4893585443496704
I0220 19:36:52.812167 140549388556096 spec.py:321] Evaluating on the training split.
I0220 19:37:46.996551 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 19:38:37.969766 140549388556096 spec.py:349] Evaluating on the test split.
I0220 19:39:03.778555 140549388556096 submission_runner.py:408] Time since start: 42524.28s, 	Step: 50440, 	{'train/ctc_loss': Array(0.30615988, dtype=float32), 'train/wer': 0.11298640164597736, 'validation/ctc_loss': Array(0.6176672, dtype=float32), 'validation/wer': 0.1900711547930525, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38614887, dtype=float32), 'test/wer': 0.1320049560254301, 'test/num_examples': 2472, 'score': 38921.05453419685, 'total_duration': 42524.28307437897, 'accumulated_submission_time': 38921.05453419685, 'accumulated_eval_time': 3599.5541915893555, 'accumulated_logging_time': 1.4804182052612305}
I0220 19:39:03.818883 140483498055424 logging_writer.py:48] [50440] accumulated_eval_time=3599.554192, accumulated_logging_time=1.480418, accumulated_submission_time=38921.054534, global_step=50440, preemption_count=0, score=38921.054534, test/ctc_loss=0.3861488699913025, test/num_examples=2472, test/wer=0.132005, total_duration=42524.283074, train/ctc_loss=0.3061598837375641, train/wer=0.112986, validation/ctc_loss=0.6176671981811523, validation/num_examples=5348, validation/wer=0.190071
I0220 19:39:53.127473 140483498055424 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.9507205486297607, loss=1.4462648630142212
I0220 19:41:08.504286 140468535932672 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.6181859970092773, loss=1.447881817817688
I0220 19:42:24.156244 140483498055424 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.6683007478713989, loss=1.4567373991012573
I0220 19:43:39.493098 140468535932672 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.6521063446998596, loss=1.4524259567260742
I0220 19:44:54.922795 140483498055424 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.75734543800354, loss=1.4761885404586792
I0220 19:46:10.366051 140468535932672 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.6503012776374817, loss=1.4704270362854004
I0220 19:47:26.716060 140483498055424 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.6873746514320374, loss=1.4859294891357422
I0220 19:48:48.230601 140468535932672 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.7238638997077942, loss=1.4814738035202026
I0220 19:50:09.369249 140483498055424 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.7534505724906921, loss=1.4591846466064453
I0220 19:51:30.416584 140468535932672 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.634238064289093, loss=1.4849565029144287
I0220 19:52:54.946486 140483498055424 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.7254171371459961, loss=1.424428105354309
I0220 19:54:10.261712 140468535932672 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.6019431352615356, loss=1.4337859153747559
I0220 19:55:25.800981 140483498055424 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.6881538033485413, loss=1.4941718578338623
I0220 19:56:41.225574 140468535932672 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.6960846185684204, loss=1.4183018207550049
I0220 19:57:56.960878 140483498055424 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.6779186129570007, loss=1.5041166543960571
I0220 19:59:12.414963 140468535932672 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.6402925252914429, loss=1.4799835681915283
I0220 20:00:27.861489 140483498055424 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.8067358732223511, loss=1.478753685951233
I0220 20:01:45.458947 140468535932672 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.7060737013816833, loss=1.546464204788208
I0220 20:03:03.939786 140549388556096 spec.py:321] Evaluating on the training split.
I0220 20:03:58.870666 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 20:04:49.616450 140549388556096 spec.py:349] Evaluating on the test split.
I0220 20:05:15.790117 140549388556096 submission_runner.py:408] Time since start: 44096.29s, 	Step: 52298, 	{'train/ctc_loss': Array(0.29809442, dtype=float32), 'train/wer': 0.11231785100332567, 'validation/ctc_loss': Array(0.604947, dtype=float32), 'validation/wer': 0.18492522471204997, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3775948, dtype=float32), 'test/wer': 0.1291207117177503, 'test/num_examples': 2472, 'score': 40361.0865046978, 'total_duration': 44096.294939517975, 'accumulated_submission_time': 40361.0865046978, 'accumulated_eval_time': 3731.398805141449, 'accumulated_logging_time': 1.538435935974121}
I0220 20:05:15.831110 140483498055424 logging_writer.py:48] [52298] accumulated_eval_time=3731.398805, accumulated_logging_time=1.538436, accumulated_submission_time=40361.086505, global_step=52298, preemption_count=0, score=40361.086505, test/ctc_loss=0.37759479880332947, test/num_examples=2472, test/wer=0.129121, total_duration=44096.294940, train/ctc_loss=0.29809442162513733, train/wer=0.112318, validation/ctc_loss=0.6049469709396362, validation/num_examples=5348, validation/wer=0.184925
I0220 20:05:18.193050 140468535932672 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.841601550579071, loss=1.4684209823608398
I0220 20:06:33.147370 140483498055424 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.7290035486221313, loss=1.4371706247329712
I0220 20:07:48.511303 140468535932672 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.7352951765060425, loss=1.4564943313598633
I0220 20:09:07.374787 140483498055424 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.7915799021720886, loss=1.4739469289779663
I0220 20:10:22.875823 140468535932672 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.6428596377372742, loss=1.4159237146377563
I0220 20:11:38.514541 140483498055424 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.5705357193946838, loss=1.414843201637268
I0220 20:12:54.097159 140468535932672 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.608795166015625, loss=1.4730660915374756
I0220 20:14:09.691074 140483498055424 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.853996753692627, loss=1.4237561225891113
I0220 20:15:25.674681 140468535932672 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.7745382189750671, loss=1.4046735763549805
I0220 20:16:44.074361 140483498055424 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.7658570408821106, loss=1.4402340650558472
I0220 20:18:04.763585 140468535932672 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.7078433036804199, loss=1.4934531450271606
I0220 20:19:26.273587 140483498055424 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.7258700728416443, loss=1.4362928867340088
I0220 20:20:47.598030 140468535932672 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.6595256924629211, loss=1.437637448310852
I0220 20:22:09.152065 140483498055424 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.7456412315368652, loss=1.373170256614685
I0220 20:23:24.609888 140468535932672 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.7369553446769714, loss=1.3889026641845703
I0220 20:24:40.093229 140483498055424 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.7905733585357666, loss=1.4203194379806519
I0220 20:25:55.593641 140468535932672 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.6959181427955627, loss=1.3921514749526978
I0220 20:27:11.134113 140483498055424 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.6622741222381592, loss=1.4304234981536865
I0220 20:28:26.503887 140468535932672 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.7959631681442261, loss=1.392637014389038
I0220 20:29:15.907537 140549388556096 spec.py:321] Evaluating on the training split.
I0220 20:30:11.297617 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 20:31:03.030727 140549388556096 spec.py:349] Evaluating on the test split.
I0220 20:31:29.247592 140549388556096 submission_runner.py:408] Time since start: 45669.75s, 	Step: 54167, 	{'train/ctc_loss': Array(0.30563256, dtype=float32), 'train/wer': 0.11277734537428964, 'validation/ctc_loss': Array(0.58232206, dtype=float32), 'validation/wer': 0.18026202728404955, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35808083, dtype=float32), 'test/wer': 0.12197103568744541, 'test/num_examples': 2472, 'score': 41801.07320189476, 'total_duration': 45669.7519903183, 'accumulated_submission_time': 41801.07320189476, 'accumulated_eval_time': 3864.7327134609222, 'accumulated_logging_time': 1.5968918800354004}
I0220 20:31:29.296687 140483498055424 logging_writer.py:48] [54167] accumulated_eval_time=3864.732713, accumulated_logging_time=1.596892, accumulated_submission_time=41801.073202, global_step=54167, preemption_count=0, score=41801.073202, test/ctc_loss=0.35808083415031433, test/num_examples=2472, test/wer=0.121971, total_duration=45669.751990, train/ctc_loss=0.3056325614452362, train/wer=0.112777, validation/ctc_loss=0.5823220610618591, validation/num_examples=5348, validation/wer=0.180262
I0220 20:31:54.944777 140468535932672 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.5481374859809875, loss=1.4230245351791382
I0220 20:33:10.873728 140483498055424 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.7197644710540771, loss=1.4228780269622803
I0220 20:34:26.548148 140468535932672 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.6933132410049438, loss=1.4024578332901
I0220 20:35:42.145604 140483498055424 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.761675238609314, loss=1.438409447669983
I0220 20:37:01.097222 140483498055424 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.8347578644752502, loss=1.4071128368377686
I0220 20:38:16.504126 140468535932672 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.6799232363700867, loss=1.3568370342254639
I0220 20:39:32.007460 140483498055424 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.6855348348617554, loss=1.397689938545227
I0220 20:40:47.567398 140468535932672 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.6859331727027893, loss=1.4083514213562012
I0220 20:42:03.071244 140483498055424 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.7300500869750977, loss=1.4080278873443604
I0220 20:43:18.527158 140468535932672 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.6681569814682007, loss=1.3849574327468872
I0220 20:44:33.985170 140483498055424 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.7460506558418274, loss=1.4176360368728638
I0220 20:45:53.530949 140468535932672 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.7877441048622131, loss=1.4249368906021118
I0220 20:47:15.036501 140483498055424 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.6780213713645935, loss=1.4053153991699219
I0220 20:48:35.169452 140468535932672 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.6777801513671875, loss=1.4298592805862427
I0220 20:49:56.680241 140483498055424 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.8029316067695618, loss=1.3279523849487305
I0220 20:51:16.359502 140483498055424 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.8082869052886963, loss=1.4009300470352173
I0220 20:52:31.917190 140468535932672 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.7183720469474792, loss=1.3674609661102295
I0220 20:53:47.523165 140483498055424 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.6712376475334167, loss=1.3827913999557495
I0220 20:55:03.132918 140468535932672 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.6597302556037903, loss=1.401192307472229
I0220 20:55:29.939943 140549388556096 spec.py:321] Evaluating on the training split.
I0220 20:56:24.245862 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 20:57:15.443011 140549388556096 spec.py:349] Evaluating on the test split.
I0220 20:57:41.389861 140549388556096 submission_runner.py:408] Time since start: 47241.89s, 	Step: 56037, 	{'train/ctc_loss': Array(0.28636116, dtype=float32), 'train/wer': 0.10623146024866935, 'validation/ctc_loss': Array(0.56730247, dtype=float32), 'validation/wer': 0.17507747859080683, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34402737, dtype=float32), 'test/wer': 0.1187008713667662, 'test/num_examples': 2472, 'score': 43241.62187099457, 'total_duration': 47241.89470553398, 'accumulated_submission_time': 43241.62187099457, 'accumulated_eval_time': 3996.1769256591797, 'accumulated_logging_time': 1.6657493114471436}
I0220 20:57:41.433077 140483498055424 logging_writer.py:48] [56037] accumulated_eval_time=3996.176926, accumulated_logging_time=1.665749, accumulated_submission_time=43241.621871, global_step=56037, preemption_count=0, score=43241.621871, test/ctc_loss=0.3440273702144623, test/num_examples=2472, test/wer=0.118701, total_duration=47241.894706, train/ctc_loss=0.2863611578941345, train/wer=0.106231, validation/ctc_loss=0.5673024654388428, validation/num_examples=5348, validation/wer=0.175077
I0220 20:58:29.537749 140468535932672 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.7826274633407593, loss=1.444506287574768
I0220 20:59:44.952805 140483498055424 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.8613964319229126, loss=1.4186092615127563
I0220 21:01:00.394148 140468535932672 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.7554372549057007, loss=1.386732816696167
I0220 21:02:15.859777 140483498055424 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.646405816078186, loss=1.3336304426193237
I0220 21:03:31.355430 140468535932672 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.6750321388244629, loss=1.3576802015304565
I0220 21:04:47.483226 140483498055424 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.8718485236167908, loss=1.4251444339752197
I0220 21:06:09.335662 140483498055424 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.7106018662452698, loss=1.3799246549606323
I0220 21:07:24.730528 140468535932672 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.6824125647544861, loss=1.3271907567977905
I0220 21:08:40.212555 140483498055424 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.7084323167800903, loss=1.3659265041351318
I0220 21:09:55.644357 140468535932672 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.7890457510948181, loss=1.410249948501587
I0220 21:11:11.078291 140483498055424 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.6877131462097168, loss=1.3575687408447266
I0220 21:12:26.510228 140468535932672 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.6307098865509033, loss=1.4138572216033936
I0220 21:13:41.949793 140483498055424 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.7309677600860596, loss=1.377240538597107
I0220 21:15:02.577654 140468535932672 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.7656340003013611, loss=1.3482774496078491
I0220 21:16:23.863225 140483498055424 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.6869816780090332, loss=1.376818299293518
I0220 21:17:44.842536 140468535932672 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.7853030562400818, loss=1.346622347831726
I0220 21:19:07.226567 140483498055424 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.7087059617042542, loss=1.3464075326919556
I0220 21:20:22.844918 140468535932672 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.7561497092247009, loss=1.3627033233642578
I0220 21:21:38.235053 140483498055424 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.8538506627082825, loss=1.3731846809387207
I0220 21:21:41.734958 140549388556096 spec.py:321] Evaluating on the training split.
I0220 21:22:36.312909 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 21:23:27.265935 140549388556096 spec.py:349] Evaluating on the test split.
I0220 21:23:52.748963 140549388556096 submission_runner.py:408] Time since start: 48813.25s, 	Step: 57906, 	{'train/ctc_loss': Array(0.26780295, dtype=float32), 'train/wer': 0.0981005955978449, 'validation/ctc_loss': Array(0.5353414, dtype=float32), 'validation/wer': 0.16503663940836286, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32470948, dtype=float32), 'test/wer': 0.11075904373083095, 'test/num_examples': 2472, 'score': 44681.83616948128, 'total_duration': 48813.25409722328, 'accumulated_submission_time': 44681.83616948128, 'accumulated_eval_time': 4127.185501813889, 'accumulated_logging_time': 1.7246437072753906}
I0220 21:23:52.789900 140483498055424 logging_writer.py:48] [57906] accumulated_eval_time=4127.185502, accumulated_logging_time=1.724644, accumulated_submission_time=44681.836169, global_step=57906, preemption_count=0, score=44681.836169, test/ctc_loss=0.3247094750404358, test/num_examples=2472, test/wer=0.110759, total_duration=48813.254097, train/ctc_loss=0.2678029537200928, train/wer=0.098101, validation/ctc_loss=0.5353413820266724, validation/num_examples=5348, validation/wer=0.165037
I0220 21:25:04.248019 140468535932672 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.7179683446884155, loss=1.3637081384658813
I0220 21:26:19.662503 140483498055424 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.8302763104438782, loss=1.315376877784729
I0220 21:27:35.126610 140468535932672 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.776427149772644, loss=1.3954437971115112
I0220 21:28:50.642137 140483498055424 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.863277018070221, loss=1.3458423614501953
I0220 21:30:06.084910 140468535932672 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.7067515254020691, loss=1.3742903470993042
I0220 21:31:21.443605 140483498055424 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.7456927299499512, loss=1.348023533821106
I0220 21:32:41.466319 140468535932672 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.9064722061157227, loss=1.3114084005355835
I0220 21:34:01.560039 140483498055424 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.6999088525772095, loss=1.3350186347961426
I0220 21:35:20.777218 140483498055424 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.8252280354499817, loss=1.3737916946411133
I0220 21:36:36.275489 140468535932672 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.7781946659088135, loss=1.3150150775909424
I0220 21:37:51.997022 140483498055424 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.9109482765197754, loss=1.356795310974121
I0220 21:39:07.446986 140468535932672 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.8644349575042725, loss=1.3650332689285278
I0220 21:40:22.934664 140483498055424 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.799112856388092, loss=1.2738125324249268
I0220 21:41:38.444503 140468535932672 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.8136149048805237, loss=1.3739802837371826
I0220 21:42:55.269080 140483498055424 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.780102550983429, loss=1.3006529808044434
I0220 21:44:15.461717 140468535932672 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.7040109038352966, loss=1.3191910982131958
I0220 21:45:37.468223 140483498055424 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.7190729975700378, loss=1.3125977516174316
I0220 21:46:57.795509 140468535932672 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.7792152166366577, loss=1.3309926986694336
I0220 21:47:53.045917 140549388556096 spec.py:321] Evaluating on the training split.
I0220 21:48:47.866962 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 21:49:38.203351 140549388556096 spec.py:349] Evaluating on the test split.
I0220 21:50:04.115085 140549388556096 submission_runner.py:408] Time since start: 50384.62s, 	Step: 59768, 	{'train/ctc_loss': Array(0.2401949, dtype=float32), 'train/wer': 0.09038596195660757, 'validation/ctc_loss': Array(0.51465917, dtype=float32), 'validation/wer': 0.157988742674532, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30931154, dtype=float32), 'test/wer': 0.10484837405805049, 'test/num_examples': 2472, 'score': 46122.00344848633, 'total_duration': 50384.61937427521, 'accumulated_submission_time': 46122.00344848633, 'accumulated_eval_time': 4258.248637199402, 'accumulated_logging_time': 1.7822024822235107}
I0220 21:50:04.160442 140483498055424 logging_writer.py:48] [59768] accumulated_eval_time=4258.248637, accumulated_logging_time=1.782202, accumulated_submission_time=46122.003448, global_step=59768, preemption_count=0, score=46122.003448, test/ctc_loss=0.30931153893470764, test/num_examples=2472, test/wer=0.104848, total_duration=50384.619374, train/ctc_loss=0.2401949018239975, train/wer=0.090386, validation/ctc_loss=0.5146591663360596, validation/num_examples=5348, validation/wer=0.157989
I0220 21:50:28.931835 140468535932672 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.7248606085777283, loss=1.2405264377593994
I0220 21:51:44.268904 140483498055424 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.8798989057540894, loss=1.3200292587280273
I0220 21:52:59.617643 140468535932672 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.728135347366333, loss=1.3540314435958862
I0220 21:54:15.185069 140483498055424 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.6709575653076172, loss=1.3120604753494263
I0220 21:55:30.540349 140468535932672 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.7498728632926941, loss=1.3141590356826782
I0220 21:56:45.947354 140483498055424 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.6698077917098999, loss=1.2839356660842896
I0220 21:58:01.394247 140468535932672 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.7873954772949219, loss=1.2906137704849243
I0220 21:59:16.920941 140483498055424 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.8433501124382019, loss=1.2861446142196655
I0220 22:00:33.349882 140468535932672 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.9185861945152283, loss=1.3387677669525146
I0220 22:01:53.971747 140483498055424 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.7265036106109619, loss=1.3065204620361328
I0220 22:03:16.432093 140483498055424 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.6859703660011292, loss=1.279504418373108
I0220 22:04:31.765302 140468535932672 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.7444294691085815, loss=1.3038047552108765
I0220 22:05:47.094748 140483498055424 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.9415624141693115, loss=1.3056488037109375
I0220 22:07:02.441411 140468535932672 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.827785313129425, loss=1.3343898057937622
I0220 22:08:17.845780 140483498055424 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.9111726880073547, loss=1.3505805730819702
I0220 22:09:33.540823 140468535932672 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.6943356990814209, loss=1.314955711364746
I0220 22:10:48.808304 140483498055424 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.8404451608657837, loss=1.3010790348052979
I0220 22:12:06.342988 140468535932672 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.643398106098175, loss=1.2719770669937134
I0220 22:13:27.289290 140483498055424 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.8451666831970215, loss=1.281301736831665
I0220 22:14:04.180361 140549388556096 spec.py:321] Evaluating on the training split.
I0220 22:14:58.934765 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 22:15:49.935044 140549388556096 spec.py:349] Evaluating on the test split.
I0220 22:16:15.761560 140549388556096 submission_runner.py:408] Time since start: 51956.27s, 	Step: 61647, 	{'train/ctc_loss': Array(0.22045113, dtype=float32), 'train/wer': 0.08282399636866496, 'validation/ctc_loss': Array(0.49300355, dtype=float32), 'validation/wer': 0.1518483833283451, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29717535, dtype=float32), 'test/wer': 0.10074543497247782, 'test/num_examples': 2472, 'score': 47561.935584545135, 'total_duration': 51956.265993356705, 'accumulated_submission_time': 47561.935584545135, 'accumulated_eval_time': 4389.823725938797, 'accumulated_logging_time': 1.842402458190918}
I0220 22:16:15.810223 140483498055424 logging_writer.py:48] [61647] accumulated_eval_time=4389.823726, accumulated_logging_time=1.842402, accumulated_submission_time=47561.935585, global_step=61647, preemption_count=0, score=47561.935585, test/ctc_loss=0.2971753478050232, test/num_examples=2472, test/wer=0.100745, total_duration=51956.265993, train/ctc_loss=0.22045113146305084, train/wer=0.082824, validation/ctc_loss=0.4930035471916199, validation/num_examples=5348, validation/wer=0.151848
I0220 22:16:56.510581 140468535932672 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.7644094824790955, loss=1.292757511138916
I0220 22:18:15.590869 140483498055424 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.7624103426933289, loss=1.2248224020004272
I0220 22:19:30.944455 140468535932672 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.7522522211074829, loss=1.2510745525360107
I0220 22:20:46.408564 140483498055424 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.7977920770645142, loss=1.2605376243591309
I0220 22:22:01.856933 140468535932672 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.8183533549308777, loss=1.2979207038879395
I0220 22:23:17.177016 140483498055424 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.7811871767044067, loss=1.2977063655853271
I0220 22:24:32.634835 140468535932672 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.6585677266120911, loss=1.2617276906967163
I0220 22:25:48.074327 140483498055424 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.7459188103675842, loss=1.2943423986434937
I0220 22:27:08.485679 140468535932672 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.8196770548820496, loss=1.2450464963912964
I0220 22:28:29.000681 140483498055424 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.7518924474716187, loss=1.2479612827301025
I0220 22:29:49.818749 140468535932672 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.818611741065979, loss=1.2844345569610596
I0220 22:31:10.513523 140483498055424 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.8390625715255737, loss=1.229432463645935
I0220 22:32:29.997552 140483498055424 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.8809037804603577, loss=1.2685401439666748
I0220 22:33:45.440302 140468535932672 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.8441110253334045, loss=1.2060546875
I0220 22:35:00.978104 140483498055424 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.7953562140464783, loss=1.2998158931732178
I0220 22:36:16.491415 140468535932672 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.748706042766571, loss=1.2436115741729736
I0220 22:37:32.021691 140483498055424 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.8121632933616638, loss=1.2229344844818115
I0220 22:38:47.624264 140468535932672 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.7426772117614746, loss=1.2816879749298096
I0220 22:40:03.107309 140483498055424 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.8041879534721375, loss=1.228596806526184
I0220 22:40:16.444251 140549388556096 spec.py:321] Evaluating on the training split.
I0220 22:41:12.424459 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 22:42:02.977076 140549388556096 spec.py:349] Evaluating on the test split.
I0220 22:42:28.934200 140549388556096 submission_runner.py:408] Time since start: 53529.44s, 	Step: 63519, 	{'train/ctc_loss': Array(0.20646651, dtype=float32), 'train/wer': 0.07630230788739538, 'validation/ctc_loss': Array(0.47628415, dtype=float32), 'validation/wer': 0.1478513569614876, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28151563, dtype=float32), 'test/wer': 0.0974346474925355, 'test/num_examples': 2472, 'score': 49002.48093056679, 'total_duration': 53529.43874812126, 'accumulated_submission_time': 49002.48093056679, 'accumulated_eval_time': 4522.307675123215, 'accumulated_logging_time': 1.9072229862213135}
I0220 22:42:28.979139 140483498055424 logging_writer.py:48] [63519] accumulated_eval_time=4522.307675, accumulated_logging_time=1.907223, accumulated_submission_time=49002.480931, global_step=63519, preemption_count=0, score=49002.480931, test/ctc_loss=0.28151562809944153, test/num_examples=2472, test/wer=0.097435, total_duration=53529.438748, train/ctc_loss=0.20646651089191437, train/wer=0.076302, validation/ctc_loss=0.4762841463088989, validation/num_examples=5348, validation/wer=0.147851
I0220 22:43:30.522331 140468535932672 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.6627388000488281, loss=1.2094593048095703
I0220 22:44:46.169746 140483498055424 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.8226062655448914, loss=1.240233063697815
I0220 22:46:01.588954 140468535932672 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.7460631132125854, loss=1.2323203086853027
I0220 22:47:20.515706 140483498055424 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.8438249230384827, loss=1.2014005184173584
I0220 22:48:35.848103 140468535932672 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.8185104727745056, loss=1.2311813831329346
I0220 22:49:51.206681 140483498055424 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.8824024796485901, loss=1.2548314332962036
I0220 22:51:06.602161 140468535932672 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.9129087924957275, loss=1.2369906902313232
I0220 22:52:21.935448 140483498055424 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.7935418486595154, loss=1.2344807386398315
I0220 22:53:37.320001 140468535932672 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.9102820158004761, loss=1.259792685508728
I0220 22:54:52.765512 140483498055424 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.9789055585861206, loss=1.2198907136917114
I0220 22:56:10.116421 140468535932672 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.8904868960380554, loss=1.2161645889282227
I0220 22:57:29.670266 140483498055424 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.9257118701934814, loss=1.2745012044906616
I0220 22:58:50.630437 140468535932672 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.7448939085006714, loss=1.2357162237167358
I0220 23:00:13.864681 140483498055424 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.7602426409721375, loss=1.206511378288269
I0220 23:01:29.166944 140468535932672 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.8807522058486938, loss=1.2284387350082397
I0220 23:02:44.580412 140483498055424 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.8124116063117981, loss=1.252716064453125
I0220 23:04:00.024425 140468535932672 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.747803270816803, loss=1.2466061115264893
I0220 23:05:15.498848 140483498055424 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.060167670249939, loss=1.284759759902954
I0220 23:06:29.650431 140549388556096 spec.py:321] Evaluating on the training split.
I0220 23:07:23.466967 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 23:08:13.758360 140549388556096 spec.py:349] Evaluating on the test split.
I0220 23:08:39.369151 140549388556096 submission_runner.py:408] Time since start: 55099.87s, 	Step: 65400, 	{'train/ctc_loss': Array(0.19849926, dtype=float32), 'train/wer': 0.07513543645446034, 'validation/ctc_loss': Array(0.45558718, dtype=float32), 'validation/wer': 0.1399828147175531, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2692499, dtype=float32), 'test/wer': 0.0920114557309122, 'test/num_examples': 2472, 'score': 50443.06334590912, 'total_duration': 55099.87467169762, 'accumulated_submission_time': 50443.06334590912, 'accumulated_eval_time': 4652.021373748779, 'accumulated_logging_time': 1.967803716659546}
I0220 23:08:39.412860 140483498055424 logging_writer.py:48] [65400] accumulated_eval_time=4652.021374, accumulated_logging_time=1.967804, accumulated_submission_time=50443.063346, global_step=65400, preemption_count=0, score=50443.063346, test/ctc_loss=0.26924988627433777, test/num_examples=2472, test/wer=0.092011, total_duration=55099.874672, train/ctc_loss=0.19849926233291626, train/wer=0.075135, validation/ctc_loss=0.4555871784687042, validation/num_examples=5348, validation/wer=0.139983
I0220 23:08:40.280624 140468535932672 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.7948983907699585, loss=1.2481191158294678
I0220 23:09:55.374495 140483498055424 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.8088510632514954, loss=1.2382330894470215
I0220 23:11:10.752075 140468535932672 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.8685288429260254, loss=1.244309425354004
I0220 23:12:26.123600 140483498055424 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.8882826566696167, loss=1.2737215757369995
I0220 23:13:41.528338 140468535932672 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.774229884147644, loss=1.2277742624282837
I0220 23:14:56.937594 140483498055424 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.8680092692375183, loss=1.2040650844573975
I0220 23:16:16.000414 140483498055424 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.8286049365997314, loss=1.1838380098342896
I0220 23:17:31.506374 140468535932672 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.8169571757316589, loss=1.1907275915145874
I0220 23:18:46.944266 140483498055424 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.9073848128318787, loss=1.2090892791748047
I0220 23:20:02.339177 140468535932672 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.9041063785552979, loss=1.2677253484725952
I0220 23:21:17.700012 140483498055424 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.8820860981941223, loss=1.2434805631637573
I0220 23:22:33.114660 140468535932672 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.1836011409759521, loss=1.2567543983459473
I0220 23:23:50.876153 140483498055424 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.9054390788078308, loss=1.2881840467453003
I0220 23:25:11.807126 140468535932672 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.887445330619812, loss=1.279784083366394
I0220 23:26:31.710972 140483498055424 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.8051495552062988, loss=1.1666277647018433
I0220 23:27:51.617496 140468535932672 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.9824203848838806, loss=1.2305651903152466
I0220 23:29:12.738678 140483498055424 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.8053021430969238, loss=1.1935306787490845
I0220 23:30:28.110211 140468535932672 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.9721183180809021, loss=1.2573060989379883
I0220 23:31:43.616508 140483498055424 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.882703959941864, loss=1.1841988563537598
I0220 23:32:39.736811 140549388556096 spec.py:321] Evaluating on the training split.
I0220 23:33:33.545446 140549388556096 spec.py:333] Evaluating on the validation split.
I0220 23:34:24.053101 140549388556096 spec.py:349] Evaluating on the test split.
I0220 23:34:49.744850 140549388556096 submission_runner.py:408] Time since start: 56670.25s, 	Step: 67276, 	{'train/ctc_loss': Array(0.17537242, dtype=float32), 'train/wer': 0.06553948712434815, 'validation/ctc_loss': Array(0.4380229, dtype=float32), 'validation/wer': 0.13615957210577637, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25713265, dtype=float32), 'test/wer': 0.08601953973960555, 'test/num_examples': 2472, 'score': 51883.29848217964, 'total_duration': 56670.25017309189, 'accumulated_submission_time': 51883.29848217964, 'accumulated_eval_time': 4782.024187803268, 'accumulated_logging_time': 2.0273022651672363}
I0220 23:34:49.785697 140483498055424 logging_writer.py:48] [67276] accumulated_eval_time=4782.024188, accumulated_logging_time=2.027302, accumulated_submission_time=51883.298482, global_step=67276, preemption_count=0, score=51883.298482, test/ctc_loss=0.2571326494216919, test/num_examples=2472, test/wer=0.086020, total_duration=56670.250173, train/ctc_loss=0.1753724217414856, train/wer=0.065539, validation/ctc_loss=0.4380229115486145, validation/num_examples=5348, validation/wer=0.136160
I0220 23:35:08.609397 140468535932672 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.850662887096405, loss=1.2183046340942383
I0220 23:36:23.841069 140483498055424 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.0861623287200928, loss=1.1700767278671265
I0220 23:37:39.366263 140468535932672 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.0327308177947998, loss=1.2314081192016602
I0220 23:38:54.792857 140483498055424 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.9972201585769653, loss=1.2090349197387695
I0220 23:40:10.041638 140468535932672 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.9393996596336365, loss=1.1847946643829346
I0220 23:41:25.412174 140483498055424 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.9363679885864258, loss=1.175204873085022
I0220 23:42:42.454703 140468535932672 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.1076173782348633, loss=1.1850084066390991
I0220 23:44:04.642767 140483498055424 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.8613640069961548, loss=1.1436679363250732
I0220 23:45:19.964031 140468535932672 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.8513412475585938, loss=1.1712725162506104
I0220 23:46:35.332007 140483498055424 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.9043576121330261, loss=1.1834101676940918
I0220 23:47:50.766580 140468535932672 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.8590797185897827, loss=1.1713051795959473
I0220 23:49:06.433180 140483498055424 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.8626789450645447, loss=1.1575921773910522
I0220 23:50:21.943900 140468535932672 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.047745943069458, loss=1.223047137260437
I0220 23:51:37.444738 140483498055424 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.0981367826461792, loss=1.197972059249878
I0220 23:52:53.014045 140468535932672 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.9580500721931458, loss=1.1881853342056274
I0220 23:54:12.677593 140483498055424 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.8050029873847961, loss=1.1624749898910522
I0220 23:55:32.976749 140468535932672 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.8751555681228638, loss=1.173218846321106
I0220 23:56:53.837225 140483498055424 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.8649193644523621, loss=1.1462726593017578
I0220 23:58:12.956094 140483498055424 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.8849894404411316, loss=1.1778806447982788
I0220 23:58:50.311973 140549388556096 spec.py:321] Evaluating on the training split.
I0220 23:59:44.491366 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 00:00:35.051698 140549388556096 spec.py:349] Evaluating on the test split.
I0221 00:01:00.911340 140549388556096 submission_runner.py:408] Time since start: 58241.42s, 	Step: 69151, 	{'train/ctc_loss': Array(0.1733773, dtype=float32), 'train/wer': 0.06558027993876667, 'validation/ctc_loss': Array(0.4252374, dtype=float32), 'validation/wer': 0.12969095455554805, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24621879, dtype=float32), 'test/wer': 0.08419149757276623, 'test/num_examples': 2472, 'score': 53323.735122442245, 'total_duration': 58241.41588020325, 'accumulated_submission_time': 53323.735122442245, 'accumulated_eval_time': 4912.617544412613, 'accumulated_logging_time': 2.0856642723083496}
I0221 00:01:00.959609 140483498055424 logging_writer.py:48] [69151] accumulated_eval_time=4912.617544, accumulated_logging_time=2.085664, accumulated_submission_time=53323.735122, global_step=69151, preemption_count=0, score=53323.735122, test/ctc_loss=0.24621878564357758, test/num_examples=2472, test/wer=0.084191, total_duration=58241.415880, train/ctc_loss=0.17337730526924133, train/wer=0.065580, validation/ctc_loss=0.42523738741874695, validation/num_examples=5348, validation/wer=0.129691
I0221 00:01:38.488928 140468535932672 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.8434672355651855, loss=1.1320093870162964
I0221 00:02:53.731621 140483498055424 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.7635248303413391, loss=1.1190377473831177
I0221 00:04:09.094155 140468535932672 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.9371585845947266, loss=1.0918655395507812
I0221 00:05:24.849536 140483498055424 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.8187075853347778, loss=1.1458592414855957
I0221 00:06:40.224585 140468535932672 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.024965763092041, loss=1.146406888961792
I0221 00:07:55.538855 140483498055424 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.8227784037590027, loss=1.1754207611083984
I0221 00:09:10.993567 140468535932672 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.9955746531486511, loss=1.121316909790039
I0221 00:10:27.016800 140483498055424 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.9521740674972534, loss=1.1220208406448364
I0221 00:11:47.950847 140468535932672 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.8076330423355103, loss=1.1351150274276733
I0221 00:13:08.887321 140483498055424 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.821372926235199, loss=1.1254057884216309
I0221 00:14:24.143430 140468535932672 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.7446244955062866, loss=1.1321903467178345
I0221 00:15:39.588557 140483498055424 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.8913444876670837, loss=1.1559299230575562
I0221 00:16:54.901814 140468535932672 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.0884243249893188, loss=1.1324466466903687
I0221 00:18:10.354853 140483498055424 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.9422066807746887, loss=1.1168266534805298
I0221 00:19:25.732727 140468535932672 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.166405200958252, loss=1.1836882829666138
I0221 00:20:41.559829 140483498055424 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.0271042585372925, loss=1.1468366384506226
I0221 00:22:03.144531 140468535932672 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.8385704159736633, loss=1.1171422004699707
I0221 00:23:23.781393 140483498055424 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.027194857597351, loss=1.2018852233886719
I0221 00:24:43.639853 140468535932672 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.0118076801300049, loss=1.1205884218215942
I0221 00:25:01.447719 140549388556096 spec.py:321] Evaluating on the training split.
I0221 00:25:56.180390 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 00:26:46.812674 140549388556096 spec.py:349] Evaluating on the test split.
I0221 00:27:13.108632 140549388556096 submission_runner.py:408] Time since start: 59813.61s, 	Step: 71024, 	{'train/ctc_loss': Array(0.18672377, dtype=float32), 'train/wer': 0.06449292989117214, 'validation/ctc_loss': Array(0.41258916, dtype=float32), 'validation/wer': 0.12459329773984572, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2360904, dtype=float32), 'test/wer': 0.08063697113724534, 'test/num_examples': 2472, 'score': 54764.132786273956, 'total_duration': 59813.611429452896, 'accumulated_submission_time': 54764.132786273956, 'accumulated_eval_time': 5044.270725250244, 'accumulated_logging_time': 2.151578187942505}
I0221 00:27:13.153126 140483498055424 logging_writer.py:48] [71024] accumulated_eval_time=5044.270725, accumulated_logging_time=2.151578, accumulated_submission_time=54764.132786, global_step=71024, preemption_count=0, score=54764.132786, test/ctc_loss=0.23609040677547455, test/num_examples=2472, test/wer=0.080637, total_duration=59813.611429, train/ctc_loss=0.1867237687110901, train/wer=0.064493, validation/ctc_loss=0.4125891625881195, validation/num_examples=5348, validation/wer=0.124593
I0221 00:28:14.455634 140483498055424 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.8673402070999146, loss=1.0811933279037476
I0221 00:29:29.696736 140468535932672 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.9408648610115051, loss=1.0836166143417358
I0221 00:30:45.083752 140483498055424 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.844666063785553, loss=1.130509376525879
I0221 00:32:00.512332 140468535932672 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.8679168224334717, loss=1.0787031650543213
I0221 00:33:15.915889 140483498055424 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.9005768299102783, loss=1.1747139692306519
I0221 00:34:31.379021 140468535932672 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.9447934627532959, loss=1.113731861114502
I0221 00:35:48.575468 140483498055424 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.9132795929908752, loss=1.1295379400253296
I0221 00:37:09.642112 140468535932672 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.8538172245025635, loss=1.1200093030929565
I0221 00:38:30.506696 140483498055424 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.8517686724662781, loss=1.1142770051956177
I0221 00:39:50.422075 140468535932672 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.0577853918075562, loss=1.1243866682052612
I0221 00:41:15.108415 140483498055424 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.8952317237854004, loss=1.141322135925293
I0221 00:42:30.503666 140468535932672 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.9127423167228699, loss=1.0979028940200806
I0221 00:43:46.003460 140483498055424 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.8285641074180603, loss=1.188735008239746
I0221 00:45:01.584737 140468535932672 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.0390324592590332, loss=1.1166942119598389
I0221 00:46:17.173305 140483498055424 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.8691518902778625, loss=1.1486471891403198
I0221 00:47:32.641538 140468535932672 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.8896043300628662, loss=1.0881458520889282
I0221 00:48:48.090379 140483498055424 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.9642914533615112, loss=1.1402463912963867
I0221 00:50:03.640595 140468535932672 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.9998059272766113, loss=1.0885999202728271
I0221 00:51:14.020009 140549388556096 spec.py:321] Evaluating on the training split.
I0221 00:52:12.009809 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 00:53:02.740296 140549388556096 spec.py:349] Evaluating on the test split.
I0221 00:53:28.447476 140549388556096 submission_runner.py:408] Time since start: 61388.95s, 	Step: 72889, 	{'train/ctc_loss': Array(0.13206606, dtype=float32), 'train/wer': 0.04925590715452096, 'validation/ctc_loss': Array(0.4033787, dtype=float32), 'validation/wer': 0.12259478455641697, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22824389, dtype=float32), 'test/wer': 0.07832145105924888, 'test/num_examples': 2472, 'score': 56204.91225242615, 'total_duration': 61388.951934337616, 'accumulated_submission_time': 56204.91225242615, 'accumulated_eval_time': 5178.692124843597, 'accumulated_logging_time': 2.211843967437744}
I0221 00:53:28.493733 140483498055424 logging_writer.py:48] [72889] accumulated_eval_time=5178.692125, accumulated_logging_time=2.211844, accumulated_submission_time=56204.912252, global_step=72889, preemption_count=0, score=56204.912252, test/ctc_loss=0.228243887424469, test/num_examples=2472, test/wer=0.078321, total_duration=61388.951934, train/ctc_loss=0.1320660561323166, train/wer=0.049256, validation/ctc_loss=0.4033786952495575, validation/num_examples=5348, validation/wer=0.122595
I0221 00:53:37.597314 140468535932672 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.9280403852462769, loss=1.1117055416107178
I0221 00:54:52.862409 140483498055424 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.9935479760169983, loss=1.0905622243881226
I0221 00:56:08.680550 140468535932672 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.0058096647262573, loss=1.0992099046707153
I0221 00:57:27.424761 140483498055424 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.9286340475082397, loss=1.0805339813232422
I0221 00:58:42.777451 140468535932672 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.0493369102478027, loss=1.1171627044677734
I0221 00:59:58.093675 140483498055424 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.8699855804443359, loss=1.1182971000671387
I0221 01:01:13.436850 140468535932672 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.011879563331604, loss=1.0894558429718018
I0221 01:02:28.849527 140483498055424 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.9263526797294617, loss=1.0827447175979614
I0221 01:03:44.173685 140468535932672 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.8504844307899475, loss=1.053361177444458
I0221 01:04:59.540755 140483498055424 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.8796730637550354, loss=1.083522915840149
I0221 01:06:19.289963 140468535932672 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.9858475923538208, loss=1.1144448518753052
I0221 01:07:39.337082 140483498055424 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.0280839204788208, loss=1.0650067329406738
I0221 01:08:59.589084 140468535932672 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.0260846614837646, loss=1.078893780708313
I0221 01:10:22.229220 140483498055424 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.9087812900543213, loss=1.0441097021102905
I0221 01:11:37.531440 140468535932672 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.9109275937080383, loss=1.10904061794281
I0221 01:12:53.245600 140483498055424 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.8918679356575012, loss=1.0813672542572021
I0221 01:14:08.677343 140468535932672 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.2410049438476562, loss=1.058624267578125
I0221 01:15:24.156790 140483498055424 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.0915539264678955, loss=1.0885556936264038
I0221 01:16:39.576491 140468535932672 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.8181043863296509, loss=1.0217251777648926
I0221 01:17:29.020589 140549388556096 spec.py:321] Evaluating on the training split.
I0221 01:18:24.750411 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 01:19:16.637213 140549388556096 spec.py:349] Evaluating on the test split.
I0221 01:19:42.507547 140549388556096 submission_runner.py:408] Time since start: 62963.01s, 	Step: 74767, 	{'train/ctc_loss': Array(0.13103484, dtype=float32), 'train/wer': 0.04741151818234074, 'validation/ctc_loss': Array(0.39362144, dtype=float32), 'validation/wer': 0.11933151182212268, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22153029, dtype=float32), 'test/wer': 0.07545751833120062, 'test/num_examples': 2472, 'score': 57645.34727025032, 'total_duration': 62963.01133060455, 'accumulated_submission_time': 57645.34727025032, 'accumulated_eval_time': 5312.1723392009735, 'accumulated_logging_time': 2.2762951850891113}
I0221 01:19:42.553819 140483498055424 logging_writer.py:48] [74767] accumulated_eval_time=5312.172339, accumulated_logging_time=2.276295, accumulated_submission_time=57645.347270, global_step=74767, preemption_count=0, score=57645.347270, test/ctc_loss=0.22153028845787048, test/num_examples=2472, test/wer=0.075458, total_duration=62963.011331, train/ctc_loss=0.13103483617305756, train/wer=0.047412, validation/ctc_loss=0.39362144470214844, validation/num_examples=5348, validation/wer=0.119332
I0221 01:20:08.125587 140468535932672 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.9011027812957764, loss=1.1270619630813599
I0221 01:21:23.318740 140483498055424 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.0537728071212769, loss=1.058942198753357
I0221 01:22:38.780577 140468535932672 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.032706618309021, loss=1.077750325202942
I0221 01:23:54.226707 140483498055424 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.9887809753417969, loss=1.0609970092773438
I0221 01:25:13.274809 140483498055424 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.1113967895507812, loss=1.0512467622756958
I0221 01:26:28.485999 140468535932672 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.059594988822937, loss=1.0785584449768066
I0221 01:27:44.070389 140483498055424 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.8057898879051208, loss=1.0605450868606567
I0221 01:28:59.439105 140468535932672 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.8666903376579285, loss=1.08164644241333
I0221 01:30:14.771816 140483498055424 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.9971421360969543, loss=1.0888969898223877
I0221 01:31:30.238945 140468535932672 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.8617809414863586, loss=1.0205817222595215
I0221 01:32:45.502646 140483498055424 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.1948965787887573, loss=1.0599197149276733
I0221 01:34:00.971794 140468535932672 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.1331907510757446, loss=1.1097835302352905
I0221 01:35:21.338764 140483498055424 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.9864813685417175, loss=1.1137346029281616
I0221 01:36:41.011563 140468535932672 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.8882103562355042, loss=1.1242297887802124
I0221 01:38:01.331104 140483498055424 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.9389369487762451, loss=1.0534816980361938
I0221 01:39:20.770047 140483498055424 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.9670138359069824, loss=1.023992657661438
I0221 01:40:36.187507 140468535932672 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.3395241498947144, loss=1.0821149349212646
I0221 01:41:51.577805 140483498055424 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.8129357099533081, loss=1.0673259496688843
I0221 01:43:07.292251 140468535932672 logging_writer.py:48] [76600] global_step=76600, grad_norm=1.0831714868545532, loss=1.08766770362854
I0221 01:43:43.155188 140549388556096 spec.py:321] Evaluating on the training split.
I0221 01:44:36.553647 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 01:45:27.305845 140549388556096 spec.py:349] Evaluating on the test split.
I0221 01:45:52.875287 140549388556096 submission_runner.py:408] Time since start: 64533.38s, 	Step: 76649, 	{'train/ctc_loss': Array(0.1626185, dtype=float32), 'train/wer': 0.061350264086916066, 'validation/ctc_loss': Array(0.3823941, dtype=float32), 'validation/wer': 0.11631926006738948, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21700285, dtype=float32), 'test/wer': 0.07369041090325594, 'test/num_examples': 2472, 'score': 59085.857170820236, 'total_duration': 64533.37995290756, 'accumulated_submission_time': 59085.857170820236, 'accumulated_eval_time': 5441.886587142944, 'accumulated_logging_time': 2.340604066848755}
I0221 01:45:52.918985 140483498055424 logging_writer.py:48] [76649] accumulated_eval_time=5441.886587, accumulated_logging_time=2.340604, accumulated_submission_time=59085.857171, global_step=76649, preemption_count=0, score=59085.857171, test/ctc_loss=0.21700285375118256, test/num_examples=2472, test/wer=0.073690, total_duration=64533.379953, train/ctc_loss=0.1626185029745102, train/wer=0.061350, validation/ctc_loss=0.38239410519599915, validation/num_examples=5348, validation/wer=0.116319
I0221 01:46:31.978539 140468535932672 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.0200368165969849, loss=1.048213243484497
I0221 01:47:47.410978 140483498055424 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.0767544507980347, loss=1.0008624792099
I0221 01:49:02.963554 140468535932672 logging_writer.py:48] [76900] global_step=76900, grad_norm=1.0629353523254395, loss=1.007200002670288
I0221 01:50:18.336767 140483498055424 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.8326775431632996, loss=1.0435384511947632
I0221 01:51:33.812646 140468535932672 logging_writer.py:48] [77100] global_step=77100, grad_norm=1.1751428842544556, loss=1.10297429561615
I0221 01:52:49.321257 140483498055424 logging_writer.py:48] [77200] global_step=77200, grad_norm=1.1299445629119873, loss=1.096706509590149
I0221 01:54:08.062655 140483498055424 logging_writer.py:48] [77300] global_step=77300, grad_norm=1.0319148302078247, loss=1.097773790359497
I0221 01:55:23.316285 140468535932672 logging_writer.py:48] [77400] global_step=77400, grad_norm=1.060410976409912, loss=1.0799874067306519
I0221 01:56:38.590623 140483498055424 logging_writer.py:48] [77500] global_step=77500, grad_norm=1.0139005184173584, loss=1.0377767086029053
I0221 01:57:54.006609 140468535932672 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.9759843945503235, loss=1.058708906173706
I0221 01:59:09.412548 140483498055424 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.8662004470825195, loss=1.0313653945922852
I0221 02:00:25.104872 140468535932672 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.9082527756690979, loss=1.0221587419509888
I0221 02:01:40.444306 140483498055424 logging_writer.py:48] [77900] global_step=77900, grad_norm=1.0702540874481201, loss=1.0741244554519653
I0221 02:03:00.198562 140468535932672 logging_writer.py:48] [78000] global_step=78000, grad_norm=1.1325032711029053, loss=1.102476954460144
I0221 02:04:20.077703 140483498055424 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.917722225189209, loss=1.0389741659164429
I0221 02:05:40.521667 140468535932672 logging_writer.py:48] [78200] global_step=78200, grad_norm=1.075498104095459, loss=1.0817605257034302
I0221 02:07:02.657613 140483498055424 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.8821983337402344, loss=1.0684258937835693
I0221 02:08:17.935850 140468535932672 logging_writer.py:48] [78400] global_step=78400, grad_norm=1.049309253692627, loss=1.035712480545044
I0221 02:09:33.364849 140483498055424 logging_writer.py:48] [78500] global_step=78500, grad_norm=1.6837077140808105, loss=1.0750988721847534
I0221 02:09:53.418109 140549388556096 spec.py:321] Evaluating on the training split.
I0221 02:10:46.564822 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 02:11:37.249107 140549388556096 spec.py:349] Evaluating on the test split.
I0221 02:12:02.993429 140549388556096 submission_runner.py:408] Time since start: 66103.50s, 	Step: 78528, 	{'train/ctc_loss': Array(0.16589448, dtype=float32), 'train/wer': 0.06081247166399922, 'validation/ctc_loss': Array(0.38134545, dtype=float32), 'validation/wer': 0.11600065651640808, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21509795, dtype=float32), 'test/wer': 0.07287794771799402, 'test/num_examples': 2472, 'score': 60526.26091170311, 'total_duration': 66103.49896073341, 'accumulated_submission_time': 60526.26091170311, 'accumulated_eval_time': 5571.456894159317, 'accumulated_logging_time': 2.406038999557495}
I0221 02:12:03.035865 140483498055424 logging_writer.py:48] [78528] accumulated_eval_time=5571.456894, accumulated_logging_time=2.406039, accumulated_submission_time=60526.260912, global_step=78528, preemption_count=0, score=60526.260912, test/ctc_loss=0.21509794890880585, test/num_examples=2472, test/wer=0.072878, total_duration=66103.498961, train/ctc_loss=0.16589447855949402, train/wer=0.060812, validation/ctc_loss=0.3813454508781433, validation/num_examples=5348, validation/wer=0.116001
I0221 02:12:57.990610 140468535932672 logging_writer.py:48] [78600] global_step=78600, grad_norm=1.1537724733352661, loss=1.0478811264038086
I0221 02:14:13.506238 140483498055424 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.9962918758392334, loss=1.018820881843567
I0221 02:15:29.003748 140468535932672 logging_writer.py:48] [78800] global_step=78800, grad_norm=1.0544863939285278, loss=1.0619562864303589
I0221 02:16:44.977324 140483498055424 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.9157202243804932, loss=1.019696831703186
I0221 02:18:00.452008 140468535932672 logging_writer.py:48] [79000] global_step=79000, grad_norm=1.2727806568145752, loss=1.0781584978103638
I0221 02:19:16.042520 140483498055424 logging_writer.py:48] [79100] global_step=79100, grad_norm=1.5859944820404053, loss=1.0428051948547363
I0221 02:20:32.341577 140468535932672 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.9962216019630432, loss=1.0647650957107544
I0221 02:21:05.320362 140483498055424 logging_writer.py:48] [79242] global_step=79242, preemption_count=0, score=61068.479226
I0221 02:21:06.222596 140549388556096 checkpoints.py:490] Saving checkpoint at step: 79242
I0221 02:21:07.674317 140549388556096 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_3/librispeech_conformer_jax/trial_4/checkpoint_79242
I0221 02:21:07.704335 140549388556096 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_3/librispeech_conformer_jax/trial_4/checkpoint_79242.
I0221 02:21:11.262036 140549388556096 submission_runner.py:583] Tuning trial 4/5
I0221 02:21:11.262300 140549388556096 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.004958460849689891, one_minus_beta1=0.13625575743, beta2=0.6291854735396584, weight_decay=0.1147386261512052, warmup_factor=0.02)
I0221 02:21:11.290841 140549388556096 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(30.981934, dtype=float32), 'train/wer': 1.1274528946597677, 'validation/ctc_loss': Array(30.090126, dtype=float32), 'validation/wer': 0.9587360128213792, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.214182, dtype=float32), 'test/wer': 0.9757885970791949, 'test/num_examples': 2472, 'score': 35.27013564109802, 'total_duration': 163.99474143981934, 'accumulated_submission_time': 35.27013564109802, 'accumulated_eval_time': 128.7245156764984, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1847, {'train/ctc_loss': Array(7.2952933, dtype=float32), 'train/wer': 0.9413900245298447, 'validation/ctc_loss': Array(7.2312818, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(7.232382, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1475.9121260643005, 'total_duration': 1712.1499185562134, 'accumulated_submission_time': 1475.9121260643005, 'accumulated_eval_time': 236.133540391922, 'accumulated_logging_time': 0.030684471130371094, 'global_step': 1847, 'preemption_count': 0}), (3713, {'train/ctc_loss': Array(11.699928, dtype=float32), 'train/wer': 0.9387706290361157, 'validation/ctc_loss': Array(11.644811, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(11.63979, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2916.4644272327423, 'total_duration': 3259.789473295212, 'accumulated_submission_time': 2916.4644272327423, 'accumulated_eval_time': 343.08443236351013, 'accumulated_logging_time': 0.08783602714538574, 'global_step': 3713, 'preemption_count': 0}), (5586, {'train/ctc_loss': Array(3.2989602, dtype=float32), 'train/wer': 0.7216809110434939, 'validation/ctc_loss': Array(3.2346897, dtype=float32), 'validation/wer': 0.6911669579153673, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.911543, dtype=float32), 'test/wer': 0.6546625231044219, 'test/num_examples': 2472, 'score': 4356.433173418045, 'total_duration': 4822.067915201187, 'accumulated_submission_time': 4356.433173418045, 'accumulated_eval_time': 465.2628490924835, 'accumulated_logging_time': 0.14037322998046875, 'global_step': 5586, 'preemption_count': 0}), (7453, {'train/ctc_loss': Array(1.9405282, dtype=float32), 'train/wer': 0.5345338386931263, 'validation/ctc_loss': Array(1.9695325, dtype=float32), 'validation/wer': 0.5126138042229452, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.5693043, dtype=float32), 'test/wer': 0.45666524485609244, 'test/num_examples': 2472, 'score': 5796.8844957351685, 'total_duration': 6390.927451133728, 'accumulated_submission_time': 5796.8844957351685, 'accumulated_eval_time': 593.5417983531952, 'accumulated_logging_time': 0.19143939018249512, 'global_step': 7453, 'preemption_count': 0}), (9310, {'train/ctc_loss': Array(1.5818727, dtype=float32), 'train/wer': 0.45802970526804365, 'validation/ctc_loss': Array(1.544094, dtype=float32), 'validation/wer': 0.4266487733763287, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.1772131, dtype=float32), 'test/wer': 0.36333353644912963, 'test/num_examples': 2472, 'score': 7237.020395278931, 'total_duration': 7959.54049539566, 'accumulated_submission_time': 7237.020395278931, 'accumulated_eval_time': 721.8849267959595, 'accumulated_logging_time': 0.24752020835876465, 'global_step': 9310, 'preemption_count': 0}), (11181, {'train/ctc_loss': Array(1.2154001, dtype=float32), 'train/wer': 0.37666020079019236, 'validation/ctc_loss': Array(1.3427572, dtype=float32), 'validation/wer': 0.38610888517721115, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.0078455, dtype=float32), 'test/wer': 0.3222635224341397, 'test/num_examples': 2472, 'score': 8677.674111127853, 'total_duration': 9532.57375240326, 'accumulated_submission_time': 8677.674111127853, 'accumulated_eval_time': 854.1291942596436, 'accumulated_logging_time': 0.30259132385253906, 'global_step': 11181, 'preemption_count': 0}), (13048, {'train/ctc_loss': Array(1.1826919, dtype=float32), 'train/wer': 0.3672558613082949, 'validation/ctc_loss': Array(1.2360047, dtype=float32), 'validation/wer': 0.36130608146596255, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.9073358, dtype=float32), 'test/wer': 0.29782869213738755, 'test/num_examples': 2472, 'score': 10117.671419858932, 'total_duration': 11102.799887180328, 'accumulated_submission_time': 10117.671419858932, 'accumulated_eval_time': 984.2290835380554, 'accumulated_logging_time': 0.35315537452697754, 'global_step': 13048, 'preemption_count': 0}), (14917, {'train/ctc_loss': Array(1.1055752, dtype=float32), 'train/wer': 0.3447297012862157, 'validation/ctc_loss': Array(1.1436431, dtype=float32), 'validation/wer': 0.3354123019589291, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.825852, dtype=float32), 'test/wer': 0.2695549732902728, 'test/num_examples': 2472, 'score': 11557.662242412567, 'total_duration': 12671.982889652252, 'accumulated_submission_time': 11557.662242412567, 'accumulated_eval_time': 1113.2875900268555, 'accumulated_logging_time': 0.40856337547302246, 'global_step': 14917, 'preemption_count': 0}), (16786, {'train/ctc_loss': Array(1.0170374, dtype=float32), 'train/wer': 0.3245074974715272, 'validation/ctc_loss': Array(1.073623, dtype=float32), 'validation/wer': 0.31915386620581787, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.76820284, dtype=float32), 'test/wer': 0.2563930696890297, 'test/num_examples': 2472, 'score': 12997.665132284164, 'total_duration': 14242.229521751404, 'accumulated_submission_time': 12997.665132284164, 'accumulated_eval_time': 1243.396124124527, 'accumulated_logging_time': 0.46518969535827637, 'global_step': 16786, 'preemption_count': 0}), (18642, {'train/ctc_loss': Array(0.9199811, dtype=float32), 'train/wer': 0.30017488435067136, 'validation/ctc_loss': Array(1.0531265, dtype=float32), 'validation/wer': 0.3134962395126331, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.7460582, dtype=float32), 'test/wer': 0.24928401681798792, 'test/num_examples': 2472, 'score': 14437.630206108093, 'total_duration': 15811.356358766556, 'accumulated_submission_time': 14437.630206108093, 'accumulated_eval_time': 1372.4247515201569, 'accumulated_logging_time': 0.5202996730804443, 'global_step': 18642, 'preemption_count': 0}), (20504, {'train/ctc_loss': Array(0.8688427, dtype=float32), 'train/wer': 0.28561108594820517, 'validation/ctc_loss': Array(1.0261918, dtype=float32), 'validation/wer': 0.30537667628913756, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.7135042, dtype=float32), 'test/wer': 0.23792984380395263, 'test/num_examples': 2472, 'score': 15877.807674646378, 'total_duration': 17380.621470928192, 'accumulated_submission_time': 15877.807674646378, 'accumulated_eval_time': 1501.3820896148682, 'accumulated_logging_time': 0.5721523761749268, 'global_step': 20504, 'preemption_count': 0}), (22366, {'train/ctc_loss': Array(0.8891562, dtype=float32), 'train/wer': 0.29315699773507053, 'validation/ctc_loss': Array(0.9662605, dtype=float32), 'validation/wer': 0.2899099220869498, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.66886187, dtype=float32), 'test/wer': 0.22558040338797147, 'test/num_examples': 2472, 'score': 17318.290395498276, 'total_duration': 18951.881212711334, 'accumulated_submission_time': 17318.290395498276, 'accumulated_eval_time': 1632.0220003128052, 'accumulated_logging_time': 0.6320977210998535, 'global_step': 22366, 'preemption_count': 0}), (24236, {'train/ctc_loss': Array(0.8207097, dtype=float32), 'train/wer': 0.27074612720154345, 'validation/ctc_loss': Array(0.93003184, dtype=float32), 'validation/wer': 0.28164553906755363, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6421993, dtype=float32), 'test/wer': 0.21526212093514513, 'test/num_examples': 2472, 'score': 18758.737268686295, 'total_duration': 20520.893973350525, 'accumulated_submission_time': 18758.737268686295, 'accumulated_eval_time': 1760.4569537639618, 'accumulated_logging_time': 0.6845858097076416, 'global_step': 24236, 'preemption_count': 0}), (26106, {'train/ctc_loss': Array(0.8356706, dtype=float32), 'train/wer': 0.2724414374372397, 'validation/ctc_loss': Array(0.9050223, dtype=float32), 'validation/wer': 0.2752927773540458, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6293675, dtype=float32), 'test/wer': 0.216257388337091, 'test/num_examples': 2472, 'score': 20198.5342566967, 'total_duration': 22090.823832035065, 'accumulated_submission_time': 20198.5342566967, 'accumulated_eval_time': 1890.377283334732, 'accumulated_logging_time': 0.7414276599884033, 'global_step': 26106, 'preemption_count': 0}), (27969, {'train/ctc_loss': Array(0.7512261, dtype=float32), 'train/wer': 0.2498455009324634, 'validation/ctc_loss': Array(0.8914386, dtype=float32), 'validation/wer': 0.26941309364048005, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6123362, dtype=float32), 'test/wer': 0.20799057542705096, 'test/num_examples': 2472, 'score': 21638.650775671005, 'total_duration': 23661.728536605835, 'accumulated_submission_time': 21638.650775671005, 'accumulated_eval_time': 2021.034260749817, 'accumulated_logging_time': 0.7939121723175049, 'global_step': 27969, 'preemption_count': 0}), (29839, {'train/ctc_loss': Array(1.465784, dtype=float32), 'train/wer': 0.4232012911945225, 'validation/ctc_loss': Array(1.4207444, dtype=float32), 'validation/wer': 0.3876053564015177, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.012293, dtype=float32), 'test/wer': 0.3135701663518372, 'test/num_examples': 2472, 'score': 23078.74978494644, 'total_duration': 25231.434408426285, 'accumulated_submission_time': 23078.74978494644, 'accumulated_eval_time': 2150.510663509369, 'accumulated_logging_time': 0.8455498218536377, 'global_step': 29839, 'preemption_count': 0}), (31714, {'train/ctc_loss': Array(0.7389771, dtype=float32), 'train/wer': 0.2490417309784311, 'validation/ctc_loss': Array(0.8475726, dtype=float32), 'validation/wer': 0.2582426600500111, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.564204, dtype=float32), 'test/wer': 0.19117258749212926, 'test/num_examples': 2472, 'score': 24519.01113843918, 'total_duration': 26801.28219127655, 'accumulated_submission_time': 24519.01113843918, 'accumulated_eval_time': 2279.962345123291, 'accumulated_logging_time': 0.9009816646575928, 'global_step': 31714, 'preemption_count': 0}), (33599, {'train/ctc_loss': Array(0.525048, dtype=float32), 'train/wer': 0.1873489863803322, 'validation/ctc_loss': Array(0.8184007, dtype=float32), 'validation/wer': 0.250229298010176, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5521909, dtype=float32), 'test/wer': 0.18952734954197387, 'test/num_examples': 2472, 'score': 25959.022108078003, 'total_duration': 28380.205656528473, 'accumulated_submission_time': 25959.022108078003, 'accumulated_eval_time': 2418.7411739826202, 'accumulated_logging_time': 0.9561948776245117, 'global_step': 33599, 'preemption_count': 0}), (35477, {'train/ctc_loss': Array(0.4653179, dtype=float32), 'train/wer': 0.17109447016540824, 'validation/ctc_loss': Array(0.8033444, dtype=float32), 'validation/wer': 0.24607779719435782, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.52806854, dtype=float32), 'test/wer': 0.18117929031340768, 'test/num_examples': 2472, 'score': 27399.132900476456, 'total_duration': 29952.644336223602, 'accumulated_submission_time': 27399.132900476456, 'accumulated_eval_time': 2550.938821077347, 'accumulated_logging_time': 1.008819818496704, 'global_step': 35477, 'preemption_count': 0}), (37348, {'train/ctc_loss': Array(0.44106385, dtype=float32), 'train/wer': 0.15810233143636412, 'validation/ctc_loss': Array(0.7626492, dtype=float32), 'validation/wer': 0.2333819284204022, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.49983346, dtype=float32), 'test/wer': 0.17283123108484147, 'test/num_examples': 2472, 'score': 28839.237825155258, 'total_duration': 31524.145983934402, 'accumulated_submission_time': 28839.237825155258, 'accumulated_eval_time': 2682.19997549057, 'accumulated_logging_time': 1.0661985874176025, 'global_step': 37348, 'preemption_count': 0}), (39210, {'train/ctc_loss': Array(0.41415095, dtype=float32), 'train/wer': 0.1518818386404201, 'validation/ctc_loss': Array(0.74457777, dtype=float32), 'validation/wer': 0.2290180252372631, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.48848337, dtype=float32), 'test/wer': 0.16635183718237767, 'test/num_examples': 2472, 'score': 30279.30905532837, 'total_duration': 33095.56187868118, 'accumulated_submission_time': 30279.30905532837, 'accumulated_eval_time': 2813.408171415329, 'accumulated_logging_time': 1.1235594749450684, 'global_step': 39210, 'preemption_count': 0}), (41085, {'train/ctc_loss': Array(0.4113284, dtype=float32), 'train/wer': 0.15069143569351298, 'validation/ctc_loss': Array(0.7187782, dtype=float32), 'validation/wer': 0.2191123511976597, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.46268943, dtype=float32), 'test/wer': 0.15932403062986208, 'test/num_examples': 2472, 'score': 31719.445224285126, 'total_duration': 34667.19851708412, 'accumulated_submission_time': 31719.445224285126, 'accumulated_eval_time': 2944.7663497924805, 'accumulated_logging_time': 1.187300205230713, 'global_step': 41085, 'preemption_count': 0}), (42952, {'train/ctc_loss': Array(0.38013545, dtype=float32), 'train/wer': 0.14035069309631992, 'validation/ctc_loss': Array(0.7010598, dtype=float32), 'validation/wer': 0.21693039960609015, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4484339, dtype=float32), 'test/wer': 0.15552576523876263, 'test/num_examples': 2472, 'score': 33159.86926102638, 'total_duration': 36239.47293305397, 'accumulated_submission_time': 33159.86926102638, 'accumulated_eval_time': 3076.475456237793, 'accumulated_logging_time': 1.2487943172454834, 'global_step': 42952, 'preemption_count': 0}), (44827, {'train/ctc_loss': Array(0.42511886, dtype=float32), 'train/wer': 0.14946901252179426, 'validation/ctc_loss': Array(0.67514026, dtype=float32), 'validation/wer': 0.2071695453623874, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4318812, dtype=float32), 'test/wer': 0.14880263238072025, 'test/num_examples': 2472, 'score': 34600.46166920662, 'total_duration': 37810.55836892128, 'accumulated_submission_time': 34600.46166920662, 'accumulated_eval_time': 3206.82887673378, 'accumulated_logging_time': 1.3093175888061523, 'global_step': 44827, 'preemption_count': 0}), (46706, {'train/ctc_loss': Array(0.36075482, dtype=float32), 'train/wer': 0.13570823174878882, 'validation/ctc_loss': Array(0.66591084, dtype=float32), 'validation/wer': 0.2046786448728965, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42331767, dtype=float32), 'test/wer': 0.1455121564804095, 'test/num_examples': 2472, 'score': 36040.71513128281, 'total_duration': 39381.62409090996, 'accumulated_submission_time': 36040.71513128281, 'accumulated_eval_time': 3337.507324695587, 'accumulated_logging_time': 1.3644671440124512, 'global_step': 46706, 'preemption_count': 0}), (48574, {'train/ctc_loss': Array(0.33703363, dtype=float32), 'train/wer': 0.1252548560952095, 'validation/ctc_loss': Array(0.63967717, dtype=float32), 'validation/wer': 0.19583498266989777, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40389547, dtype=float32), 'test/wer': 0.13694066987589626, 'test/num_examples': 2472, 'score': 37481.050798654556, 'total_duration': 40953.1812107563, 'accumulated_submission_time': 37481.050798654556, 'accumulated_eval_time': 3468.593843460083, 'accumulated_logging_time': 1.4214856624603271, 'global_step': 48574, 'preemption_count': 0}), (50440, {'train/ctc_loss': Array(0.30615988, dtype=float32), 'train/wer': 0.11298640164597736, 'validation/ctc_loss': Array(0.6176672, dtype=float32), 'validation/wer': 0.1900711547930525, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38614887, dtype=float32), 'test/wer': 0.1320049560254301, 'test/num_examples': 2472, 'score': 38921.05453419685, 'total_duration': 42524.28307437897, 'accumulated_submission_time': 38921.05453419685, 'accumulated_eval_time': 3599.5541915893555, 'accumulated_logging_time': 1.4804182052612305, 'global_step': 50440, 'preemption_count': 0}), (52298, {'train/ctc_loss': Array(0.29809442, dtype=float32), 'train/wer': 0.11231785100332567, 'validation/ctc_loss': Array(0.604947, dtype=float32), 'validation/wer': 0.18492522471204997, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3775948, dtype=float32), 'test/wer': 0.1291207117177503, 'test/num_examples': 2472, 'score': 40361.0865046978, 'total_duration': 44096.294939517975, 'accumulated_submission_time': 40361.0865046978, 'accumulated_eval_time': 3731.398805141449, 'accumulated_logging_time': 1.538435935974121, 'global_step': 52298, 'preemption_count': 0}), (54167, {'train/ctc_loss': Array(0.30563256, dtype=float32), 'train/wer': 0.11277734537428964, 'validation/ctc_loss': Array(0.58232206, dtype=float32), 'validation/wer': 0.18026202728404955, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35808083, dtype=float32), 'test/wer': 0.12197103568744541, 'test/num_examples': 2472, 'score': 41801.07320189476, 'total_duration': 45669.7519903183, 'accumulated_submission_time': 41801.07320189476, 'accumulated_eval_time': 3864.7327134609222, 'accumulated_logging_time': 1.5968918800354004, 'global_step': 54167, 'preemption_count': 0}), (56037, {'train/ctc_loss': Array(0.28636116, dtype=float32), 'train/wer': 0.10623146024866935, 'validation/ctc_loss': Array(0.56730247, dtype=float32), 'validation/wer': 0.17507747859080683, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34402737, dtype=float32), 'test/wer': 0.1187008713667662, 'test/num_examples': 2472, 'score': 43241.62187099457, 'total_duration': 47241.89470553398, 'accumulated_submission_time': 43241.62187099457, 'accumulated_eval_time': 3996.1769256591797, 'accumulated_logging_time': 1.6657493114471436, 'global_step': 56037, 'preemption_count': 0}), (57906, {'train/ctc_loss': Array(0.26780295, dtype=float32), 'train/wer': 0.0981005955978449, 'validation/ctc_loss': Array(0.5353414, dtype=float32), 'validation/wer': 0.16503663940836286, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32470948, dtype=float32), 'test/wer': 0.11075904373083095, 'test/num_examples': 2472, 'score': 44681.83616948128, 'total_duration': 48813.25409722328, 'accumulated_submission_time': 44681.83616948128, 'accumulated_eval_time': 4127.185501813889, 'accumulated_logging_time': 1.7246437072753906, 'global_step': 57906, 'preemption_count': 0}), (59768, {'train/ctc_loss': Array(0.2401949, dtype=float32), 'train/wer': 0.09038596195660757, 'validation/ctc_loss': Array(0.51465917, dtype=float32), 'validation/wer': 0.157988742674532, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30931154, dtype=float32), 'test/wer': 0.10484837405805049, 'test/num_examples': 2472, 'score': 46122.00344848633, 'total_duration': 50384.61937427521, 'accumulated_submission_time': 46122.00344848633, 'accumulated_eval_time': 4258.248637199402, 'accumulated_logging_time': 1.7822024822235107, 'global_step': 59768, 'preemption_count': 0}), (61647, {'train/ctc_loss': Array(0.22045113, dtype=float32), 'train/wer': 0.08282399636866496, 'validation/ctc_loss': Array(0.49300355, dtype=float32), 'validation/wer': 0.1518483833283451, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29717535, dtype=float32), 'test/wer': 0.10074543497247782, 'test/num_examples': 2472, 'score': 47561.935584545135, 'total_duration': 51956.265993356705, 'accumulated_submission_time': 47561.935584545135, 'accumulated_eval_time': 4389.823725938797, 'accumulated_logging_time': 1.842402458190918, 'global_step': 61647, 'preemption_count': 0}), (63519, {'train/ctc_loss': Array(0.20646651, dtype=float32), 'train/wer': 0.07630230788739538, 'validation/ctc_loss': Array(0.47628415, dtype=float32), 'validation/wer': 0.1478513569614876, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28151563, dtype=float32), 'test/wer': 0.0974346474925355, 'test/num_examples': 2472, 'score': 49002.48093056679, 'total_duration': 53529.43874812126, 'accumulated_submission_time': 49002.48093056679, 'accumulated_eval_time': 4522.307675123215, 'accumulated_logging_time': 1.9072229862213135, 'global_step': 63519, 'preemption_count': 0}), (65400, {'train/ctc_loss': Array(0.19849926, dtype=float32), 'train/wer': 0.07513543645446034, 'validation/ctc_loss': Array(0.45558718, dtype=float32), 'validation/wer': 0.1399828147175531, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2692499, dtype=float32), 'test/wer': 0.0920114557309122, 'test/num_examples': 2472, 'score': 50443.06334590912, 'total_duration': 55099.87467169762, 'accumulated_submission_time': 50443.06334590912, 'accumulated_eval_time': 4652.021373748779, 'accumulated_logging_time': 1.967803716659546, 'global_step': 65400, 'preemption_count': 0}), (67276, {'train/ctc_loss': Array(0.17537242, dtype=float32), 'train/wer': 0.06553948712434815, 'validation/ctc_loss': Array(0.4380229, dtype=float32), 'validation/wer': 0.13615957210577637, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25713265, dtype=float32), 'test/wer': 0.08601953973960555, 'test/num_examples': 2472, 'score': 51883.29848217964, 'total_duration': 56670.25017309189, 'accumulated_submission_time': 51883.29848217964, 'accumulated_eval_time': 4782.024187803268, 'accumulated_logging_time': 2.0273022651672363, 'global_step': 67276, 'preemption_count': 0}), (69151, {'train/ctc_loss': Array(0.1733773, dtype=float32), 'train/wer': 0.06558027993876667, 'validation/ctc_loss': Array(0.4252374, dtype=float32), 'validation/wer': 0.12969095455554805, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24621879, dtype=float32), 'test/wer': 0.08419149757276623, 'test/num_examples': 2472, 'score': 53323.735122442245, 'total_duration': 58241.41588020325, 'accumulated_submission_time': 53323.735122442245, 'accumulated_eval_time': 4912.617544412613, 'accumulated_logging_time': 2.0856642723083496, 'global_step': 69151, 'preemption_count': 0}), (71024, {'train/ctc_loss': Array(0.18672377, dtype=float32), 'train/wer': 0.06449292989117214, 'validation/ctc_loss': Array(0.41258916, dtype=float32), 'validation/wer': 0.12459329773984572, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2360904, dtype=float32), 'test/wer': 0.08063697113724534, 'test/num_examples': 2472, 'score': 54764.132786273956, 'total_duration': 59813.611429452896, 'accumulated_submission_time': 54764.132786273956, 'accumulated_eval_time': 5044.270725250244, 'accumulated_logging_time': 2.151578187942505, 'global_step': 71024, 'preemption_count': 0}), (72889, {'train/ctc_loss': Array(0.13206606, dtype=float32), 'train/wer': 0.04925590715452096, 'validation/ctc_loss': Array(0.4033787, dtype=float32), 'validation/wer': 0.12259478455641697, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22824389, dtype=float32), 'test/wer': 0.07832145105924888, 'test/num_examples': 2472, 'score': 56204.91225242615, 'total_duration': 61388.951934337616, 'accumulated_submission_time': 56204.91225242615, 'accumulated_eval_time': 5178.692124843597, 'accumulated_logging_time': 2.211843967437744, 'global_step': 72889, 'preemption_count': 0}), (74767, {'train/ctc_loss': Array(0.13103484, dtype=float32), 'train/wer': 0.04741151818234074, 'validation/ctc_loss': Array(0.39362144, dtype=float32), 'validation/wer': 0.11933151182212268, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22153029, dtype=float32), 'test/wer': 0.07545751833120062, 'test/num_examples': 2472, 'score': 57645.34727025032, 'total_duration': 62963.01133060455, 'accumulated_submission_time': 57645.34727025032, 'accumulated_eval_time': 5312.1723392009735, 'accumulated_logging_time': 2.2762951850891113, 'global_step': 74767, 'preemption_count': 0}), (76649, {'train/ctc_loss': Array(0.1626185, dtype=float32), 'train/wer': 0.061350264086916066, 'validation/ctc_loss': Array(0.3823941, dtype=float32), 'validation/wer': 0.11631926006738948, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21700285, dtype=float32), 'test/wer': 0.07369041090325594, 'test/num_examples': 2472, 'score': 59085.857170820236, 'total_duration': 64533.37995290756, 'accumulated_submission_time': 59085.857170820236, 'accumulated_eval_time': 5441.886587142944, 'accumulated_logging_time': 2.340604066848755, 'global_step': 76649, 'preemption_count': 0}), (78528, {'train/ctc_loss': Array(0.16589448, dtype=float32), 'train/wer': 0.06081247166399922, 'validation/ctc_loss': Array(0.38134545, dtype=float32), 'validation/wer': 0.11600065651640808, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21509795, dtype=float32), 'test/wer': 0.07287794771799402, 'test/num_examples': 2472, 'score': 60526.26091170311, 'total_duration': 66103.49896073341, 'accumulated_submission_time': 60526.26091170311, 'accumulated_eval_time': 5571.456894159317, 'accumulated_logging_time': 2.406038999557495, 'global_step': 78528, 'preemption_count': 0})], 'global_step': 79242}
I0221 02:21:11.291102 140549388556096 submission_runner.py:586] Timing: 61068.479226350784
I0221 02:21:11.291159 140549388556096 submission_runner.py:588] Total number of evals: 43
I0221 02:21:11.291221 140549388556096 submission_runner.py:589] ====================
I0221 02:21:11.291287 140549388556096 submission_runner.py:542] Using RNG seed 4110531300
I0221 02:21:11.293634 140549388556096 submission_runner.py:551] --- Tuning run 5/5 ---
I0221 02:21:11.293771 140549388556096 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_3/librispeech_conformer_jax/trial_5.
I0221 02:21:11.295912 140549388556096 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_3/librispeech_conformer_jax/trial_5/hparams.json.
I0221 02:21:11.297320 140549388556096 submission_runner.py:206] Initializing dataset.
I0221 02:21:11.297463 140549388556096 submission_runner.py:213] Initializing model.
I0221 02:21:15.062145 140549388556096 submission_runner.py:255] Initializing optimizer.
I0221 02:21:15.501172 140549388556096 submission_runner.py:262] Initializing metrics bundle.
I0221 02:21:15.501380 140549388556096 submission_runner.py:280] Initializing checkpoint and logger.
I0221 02:21:15.505728 140549388556096 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_3/librispeech_conformer_jax/trial_5 with prefix checkpoint_
I0221 02:21:15.505861 140549388556096 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_3/librispeech_conformer_jax/trial_5/meta_data_0.json.
I0221 02:21:15.506115 140549388556096 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0221 02:21:15.506196 140549388556096 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0221 02:21:16.167594 140549388556096 logger_utils.py:220] Unable to record git information. Continuing without it.
I0221 02:21:16.762080 140549388556096 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_3/librispeech_conformer_jax/trial_5/flags_0.json.
I0221 02:21:16.781516 140549388556096 submission_runner.py:314] Starting training loop.
I0221 02:21:16.785168 140549388556096 input_pipeline.py:20] Loading split = train-clean-100
I0221 02:21:16.831395 140549388556096 input_pipeline.py:20] Loading split = train-clean-360
I0221 02:21:17.343872 140549388556096 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0221 02:21:53.466698 140367341020928 logging_writer.py:48] [0] global_step=0, grad_norm=67.5689926147461, loss=32.31706619262695
I0221 02:21:53.494855 140549388556096 spec.py:321] Evaluating on the training split.
I0221 02:22:46.881818 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 02:23:37.241537 140549388556096 spec.py:349] Evaluating on the test split.
I0221 02:24:02.766169 140549388556096 submission_runner.py:408] Time since start: 165.98s, 	Step: 1, 	{'train/ctc_loss': Array(31.602995, dtype=float32), 'train/wer': 1.1353257853575371, 'validation/ctc_loss': Array(30.090126, dtype=float32), 'validation/wer': 0.9587263581683192, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.214182, dtype=float32), 'test/wer': 0.9758089086588264, 'test/num_examples': 2472, 'score': 36.713239431381226, 'total_duration': 165.9817259311676, 'accumulated_submission_time': 36.713239431381226, 'accumulated_eval_time': 129.26838898658752, 'accumulated_logging_time': 0}
I0221 02:24:02.784870 140483498055424 logging_writer.py:48] [1] accumulated_eval_time=129.268389, accumulated_logging_time=0, accumulated_submission_time=36.713239, global_step=1, preemption_count=0, score=36.713239, test/ctc_loss=30.214181900024414, test/num_examples=2472, test/wer=0.975809, total_duration=165.981726, train/ctc_loss=31.602994918823242, train/wer=1.135326, validation/ctc_loss=30.090126037597656, validation/num_examples=5348, validation/wer=0.958726
I0221 02:25:46.472365 140378523039488 logging_writer.py:48] [100] global_step=100, grad_norm=0.5141085982322693, loss=5.921081066131592
I0221 02:27:02.797605 140379138533120 logging_writer.py:48] [200] global_step=200, grad_norm=0.4076564908027649, loss=5.833981990814209
I0221 02:28:19.129026 140378523039488 logging_writer.py:48] [300] global_step=300, grad_norm=1.865877389907837, loss=5.81596565246582
I0221 02:29:35.476306 140379138533120 logging_writer.py:48] [400] global_step=400, grad_norm=1.8619894981384277, loss=5.795149326324463
I0221 02:30:51.821367 140378523039488 logging_writer.py:48] [500] global_step=500, grad_norm=0.4125520884990692, loss=5.781665325164795
I0221 02:32:08.205306 140379138533120 logging_writer.py:48] [600] global_step=600, grad_norm=3.128797769546509, loss=5.729547500610352
I0221 02:33:24.473790 140378523039488 logging_writer.py:48] [700] global_step=700, grad_norm=0.5489954352378845, loss=5.51773738861084
I0221 02:34:40.709686 140379138533120 logging_writer.py:48] [800] global_step=800, grad_norm=2.1245100498199463, loss=5.172236919403076
I0221 02:35:56.807659 140378523039488 logging_writer.py:48] [900] global_step=900, grad_norm=0.9470715522766113, loss=4.063634872436523
I0221 02:37:14.728658 140379138533120 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.4907786846160889, loss=3.6185600757598877
I0221 02:38:35.415544 140483498055424 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.0071625709533691, loss=3.370637893676758
I0221 02:39:51.406451 140468535932672 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.691394329071045, loss=3.056741714477539
I0221 02:41:07.730458 140483498055424 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.844907283782959, loss=2.9810843467712402
I0221 02:42:23.532578 140468535932672 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.705472469329834, loss=2.8414273262023926
I0221 02:43:39.490840 140483498055424 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.7949841022491455, loss=2.746312141418457
I0221 02:44:55.295305 140468535932672 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.5925578474998474, loss=2.609861135482788
I0221 02:46:11.098390 140483498055424 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.6518115997314453, loss=2.5227508544921875
I0221 02:47:29.635095 140468535932672 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.7244784235954285, loss=2.475679397583008
I0221 02:48:03.541313 140549388556096 spec.py:321] Evaluating on the training split.
I0221 02:48:51.857239 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 02:49:42.231997 140549388556096 spec.py:349] Evaluating on the test split.
I0221 02:50:08.314275 140549388556096 submission_runner.py:408] Time since start: 1731.53s, 	Step: 1843, 	{'train/ctc_loss': Array(3.174557, dtype=float32), 'train/wer': 0.6152079537398865, 'validation/ctc_loss': Array(3.1326847, dtype=float32), 'validation/wer': 0.5973044208656362, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.8049276, dtype=float32), 'test/wer': 0.5481079763573213, 'test/num_examples': 2472, 'score': 1477.383898973465, 'total_duration': 1731.5264666080475, 'accumulated_submission_time': 1477.383898973465, 'accumulated_eval_time': 254.03513860702515, 'accumulated_logging_time': 0.03236651420593262}
I0221 02:50:08.352464 140483498055424 logging_writer.py:48] [1843] accumulated_eval_time=254.035139, accumulated_logging_time=0.032367, accumulated_submission_time=1477.383899, global_step=1843, preemption_count=0, score=1477.383899, test/ctc_loss=2.8049275875091553, test/num_examples=2472, test/wer=0.548108, total_duration=1731.526467, train/ctc_loss=3.1745569705963135, train/wer=0.615208, validation/ctc_loss=3.1326847076416016, validation/num_examples=5348, validation/wer=0.597304
I0221 02:50:52.148732 140468535932672 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.6450388431549072, loss=2.3644278049468994
I0221 02:52:07.792669 140483498055424 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.7621703147888184, loss=2.3181567192077637
I0221 02:53:27.090739 140483498055424 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.6525802612304688, loss=2.232455015182495
I0221 02:54:42.862033 140468535932672 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.0080103874206543, loss=2.262624740600586
I0221 02:55:58.668314 140483498055424 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.8993011116981506, loss=2.1288599967956543
I0221 02:57:14.443865 140468535932672 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.8498084545135498, loss=2.1452364921569824
I0221 02:58:30.509173 140483498055424 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.8699177503585815, loss=2.050842761993408
I0221 02:59:46.349583 140468535932672 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.6040764451026917, loss=2.0356967449188232
I0221 03:01:02.080607 140483498055424 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.5823261141777039, loss=2.006847620010376
I0221 03:02:19.712020 140468535932672 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.5847686529159546, loss=1.9889453649520874
I0221 03:03:39.691055 140483498055424 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.5941194891929626, loss=2.0093650817871094
I0221 03:04:59.828296 140468535932672 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.8071034550666809, loss=1.9830772876739502
I0221 03:06:23.157573 140483498055424 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.5800504088401794, loss=1.8546863794326782
I0221 03:07:38.736253 140468535932672 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.45872437953948975, loss=1.8717037439346313
I0221 03:08:54.367006 140483498055424 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.6216246485710144, loss=1.8703560829162598
I0221 03:10:10.137320 140468535932672 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.7209872603416443, loss=1.8957515954971313
I0221 03:11:25.893326 140483498055424 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.5344111323356628, loss=1.87019681930542
I0221 03:12:41.624309 140468535932672 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.5777987241744995, loss=1.9102210998535156
I0221 03:13:57.325483 140483498055424 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.584349513053894, loss=1.8645071983337402
I0221 03:14:08.684749 140549388556096 spec.py:321] Evaluating on the training split.
I0221 03:15:01.868885 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 03:15:53.596024 140549388556096 spec.py:349] Evaluating on the test split.
I0221 03:16:19.596624 140549388556096 submission_runner.py:408] Time since start: 3302.81s, 	Step: 3716, 	{'train/ctc_loss': Array(0.86722195, dtype=float32), 'train/wer': 0.27813964778841876, 'validation/ctc_loss': Array(0.9116082, dtype=float32), 'validation/wer': 0.2709771474362069, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6313826, dtype=float32), 'test/wer': 0.2064875185343164, 'test/num_examples': 2472, 'score': 2917.6238310337067, 'total_duration': 3302.8093264102936, 'accumulated_submission_time': 2917.6238310337067, 'accumulated_eval_time': 384.9413070678711, 'accumulated_logging_time': 0.08956027030944824}
I0221 03:16:19.632915 140483498055424 logging_writer.py:48] [3716] accumulated_eval_time=384.941307, accumulated_logging_time=0.089560, accumulated_submission_time=2917.623831, global_step=3716, preemption_count=0, score=2917.623831, test/ctc_loss=0.6313825845718384, test/num_examples=2472, test/wer=0.206488, total_duration=3302.809326, train/ctc_loss=0.8672219514846802, train/wer=0.278140, validation/ctc_loss=0.9116082191467285, validation/num_examples=5348, validation/wer=0.270977
I0221 03:17:23.852556 140468535932672 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.6125904321670532, loss=1.8471637964248657
I0221 03:18:39.488745 140483498055424 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.5193867683410645, loss=1.8519810438156128
I0221 03:19:55.299530 140468535932672 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.5130967497825623, loss=1.8145641088485718
I0221 03:21:11.082222 140483498055424 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.6561899185180664, loss=1.8507509231567383
I0221 03:22:30.223299 140483498055424 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.6685576438903809, loss=1.762444019317627
I0221 03:23:45.821265 140468535932672 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.525708019733429, loss=1.7404744625091553
I0221 03:25:01.590352 140483498055424 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.6090219020843506, loss=1.7317888736724854
I0221 03:26:17.254733 140468535932672 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.45203253626823425, loss=1.7384780645370483
I0221 03:27:32.862706 140483498055424 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.47724661231040955, loss=1.748539686203003
I0221 03:28:48.555390 140468535932672 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.5211617350578308, loss=1.8087067604064941
I0221 03:30:04.186356 140483498055424 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.4410769045352936, loss=1.7125728130340576
I0221 03:31:23.875945 140468535932672 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.42982804775238037, loss=1.7353335618972778
I0221 03:32:44.535327 140483498055424 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.5109524726867676, loss=1.6874425411224365
I0221 03:34:04.561388 140468535932672 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.5553409457206726, loss=1.6938281059265137
I0221 03:35:26.501623 140483498055424 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.5603621602058411, loss=1.631711721420288
I0221 03:36:42.208095 140468535932672 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.4781218469142914, loss=1.6364306211471558
I0221 03:37:57.817913 140483498055424 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.5803486704826355, loss=1.6967517137527466
I0221 03:39:13.611299 140468535932672 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.6969389319419861, loss=1.608383059501648
I0221 03:40:19.859526 140549388556096 spec.py:321] Evaluating on the training split.
I0221 03:41:14.315259 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 03:42:06.356907 140549388556096 spec.py:349] Evaluating on the test split.
I0221 03:42:32.439841 140549388556096 submission_runner.py:408] Time since start: 4875.65s, 	Step: 5589, 	{'train/ctc_loss': Array(0.54877484, dtype=float32), 'train/wer': 0.18695944673178871, 'validation/ctc_loss': Array(0.7388587, dtype=float32), 'validation/wer': 0.22154532376879038, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4836131, dtype=float32), 'test/wer': 0.1620051591412264, 'test/num_examples': 2472, 'score': 4357.761467218399, 'total_duration': 4875.6513612270355, 'accumulated_submission_time': 4357.761467218399, 'accumulated_eval_time': 517.5147247314453, 'accumulated_logging_time': 0.14196038246154785}
I0221 03:42:32.479619 140483498055424 logging_writer.py:48] [5589] accumulated_eval_time=517.514725, accumulated_logging_time=0.141960, accumulated_submission_time=4357.761467, global_step=5589, preemption_count=0, score=4357.761467, test/ctc_loss=0.48361310362815857, test/num_examples=2472, test/wer=0.162005, total_duration=4875.651361, train/ctc_loss=0.5487748384475708, train/wer=0.186959, validation/ctc_loss=0.738858699798584, validation/num_examples=5348, validation/wer=0.221545
I0221 03:42:41.612870 140468535932672 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.46430960297584534, loss=1.6364047527313232
I0221 03:43:57.264005 140483498055424 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.596591055393219, loss=1.7313567399978638
I0221 03:45:12.950018 140468535932672 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.47446274757385254, loss=1.6168402433395386
I0221 03:46:28.634417 140483498055424 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.5634595155715942, loss=1.7294682264328003
I0221 03:47:44.358141 140468535932672 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.6106564998626709, loss=1.696130394935608
I0221 03:49:00.503315 140483498055424 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.5989408493041992, loss=1.6224696636199951
I0221 03:50:20.740066 140483498055424 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.5820925831794739, loss=1.666139006614685
I0221 03:51:36.398833 140468535932672 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.46607786417007446, loss=1.663294792175293
I0221 03:52:52.176290 140483498055424 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.4991583228111267, loss=1.6454170942306519
I0221 03:54:07.886640 140468535932672 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.512529194355011, loss=1.7018355131149292
I0221 03:55:23.630945 140483498055424 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.5519059896469116, loss=1.6202037334442139
I0221 03:56:39.335280 140468535932672 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.5722110867500305, loss=1.619376540184021
I0221 03:57:55.019042 140483498055424 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.5973979830741882, loss=1.5687479972839355
I0221 03:59:11.712775 140468535932672 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.42441126704216003, loss=1.6112648248672485
I0221 04:00:31.228569 140483498055424 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.6600992679595947, loss=1.6504846811294556
I0221 04:01:51.420382 140468535932672 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.5133114457130432, loss=1.6088865995407104
I0221 04:03:12.131551 140483498055424 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.427017480134964, loss=1.5416817665100098
I0221 04:04:31.611722 140483498055424 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.7948254346847534, loss=1.521383285522461
I0221 04:05:47.875772 140468535932672 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.5866208672523499, loss=1.619446039199829
I0221 04:06:33.076488 140549388556096 spec.py:321] Evaluating on the training split.
I0221 04:07:26.880213 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 04:08:18.214042 140549388556096 spec.py:349] Evaluating on the test split.
I0221 04:08:44.583601 140549388556096 submission_runner.py:408] Time since start: 6447.80s, 	Step: 7461, 	{'train/ctc_loss': Array(0.5382412, dtype=float32), 'train/wer': 0.18225457817180352, 'validation/ctc_loss': Array(0.6600867, dtype=float32), 'validation/wer': 0.19768867605742588, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42196423, dtype=float32), 'test/wer': 0.14197794162452013, 'test/num_examples': 2472, 'score': 5798.268486738205, 'total_duration': 6447.796489477158, 'accumulated_submission_time': 5798.268486738205, 'accumulated_eval_time': 649.0162920951843, 'accumulated_logging_time': 0.19747233390808105}
I0221 04:08:44.616449 140483498055424 logging_writer.py:48] [7461] accumulated_eval_time=649.016292, accumulated_logging_time=0.197472, accumulated_submission_time=5798.268487, global_step=7461, preemption_count=0, score=5798.268487, test/ctc_loss=0.42196422815322876, test/num_examples=2472, test/wer=0.141978, total_duration=6447.796489, train/ctc_loss=0.5382412075996399, train/wer=0.182255, validation/ctc_loss=0.6600866913795471, validation/num_examples=5348, validation/wer=0.197689
I0221 04:09:14.871713 140468535932672 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.6909335255622864, loss=1.5688364505767822
I0221 04:10:30.628695 140483498055424 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.6022657752037048, loss=1.5308830738067627
I0221 04:11:46.390416 140468535932672 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.4420177638530731, loss=1.5581930875778198
I0221 04:13:02.160512 140483498055424 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.553203284740448, loss=1.5506421327590942
I0221 04:14:18.008247 140468535932672 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.4920980632305145, loss=1.643896222114563
I0221 04:15:33.913176 140483498055424 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.479896605014801, loss=1.6173404455184937
I0221 04:16:49.655837 140468535932672 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.5522553324699402, loss=1.5758053064346313
I0221 04:18:10.000179 140483498055424 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.454343318939209, loss=1.5484418869018555
I0221 04:19:30.937863 140483498055424 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.49957385659217834, loss=1.5537029504776
I0221 04:20:46.881223 140468535932672 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.5411598682403564, loss=1.5758541822433472
I0221 04:22:02.831809 140483498055424 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.5683151483535767, loss=1.6385982036590576
I0221 04:23:18.956025 140468535932672 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.6369290947914124, loss=1.5623115301132202
I0221 04:24:34.876233 140483498055424 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.5296363830566406, loss=1.6039164066314697
I0221 04:25:50.839308 140468535932672 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.4045150578022003, loss=1.5381357669830322
I0221 04:27:06.835434 140483498055424 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.5950592160224915, loss=1.5948338508605957
I0221 04:28:25.777759 140468535932672 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.557765543460846, loss=1.504075288772583
I0221 04:29:46.016667 140483498055424 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.5442831516265869, loss=1.5939595699310303
I0221 04:31:06.050371 140468535932672 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.5915014147758484, loss=1.5449551343917847
I0221 04:32:28.061795 140483498055424 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.5712457895278931, loss=1.5976907014846802
I0221 04:32:45.115140 140549388556096 spec.py:321] Evaluating on the training split.
I0221 04:33:39.275038 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 04:34:30.352952 140549388556096 spec.py:349] Evaluating on the test split.
I0221 04:34:56.208219 140549388556096 submission_runner.py:408] Time since start: 8019.42s, 	Step: 9324, 	{'train/ctc_loss': Array(0.4458762, dtype=float32), 'train/wer': 0.15523081749969422, 'validation/ctc_loss': Array(0.6294824, dtype=float32), 'validation/wer': 0.18646031454859668, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3988943, dtype=float32), 'test/wer': 0.1319237097069039, 'test/num_examples': 2472, 'score': 7238.677075624466, 'total_duration': 8019.420377254486, 'accumulated_submission_time': 7238.677075624466, 'accumulated_eval_time': 780.1031017303467, 'accumulated_logging_time': 0.2476940155029297}
I0221 04:34:56.246237 140483498055424 logging_writer.py:48] [9324] accumulated_eval_time=780.103102, accumulated_logging_time=0.247694, accumulated_submission_time=7238.677076, global_step=9324, preemption_count=0, score=7238.677076, test/ctc_loss=0.3988943099975586, test/num_examples=2472, test/wer=0.131924, total_duration=8019.420377, train/ctc_loss=0.44587621092796326, train/wer=0.155231, validation/ctc_loss=0.6294823884963989, validation/num_examples=5348, validation/wer=0.186460
I0221 04:35:54.432654 140468535932672 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.6427409648895264, loss=1.4971507787704468
I0221 04:37:10.135133 140483498055424 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.47015103697776794, loss=1.5237668752670288
I0221 04:38:25.854042 140468535932672 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.5568171143531799, loss=1.514013409614563
I0221 04:39:41.497555 140483498055424 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.41395843029022217, loss=1.5421241521835327
I0221 04:40:57.516066 140468535932672 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.5828980803489685, loss=1.5335962772369385
I0221 04:42:13.287230 140483498055424 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.46484825015068054, loss=1.4486408233642578
I0221 04:43:29.038856 140468535932672 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.5516873598098755, loss=1.5431649684906006
I0221 04:44:44.662593 140483498055424 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.84771329164505, loss=1.5195213556289673
I0221 04:46:03.817263 140468535932672 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.5109406113624573, loss=1.482922077178955
I0221 04:47:27.100067 140483498055424 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.4688721001148224, loss=1.4881700277328491
I0221 04:48:42.861027 140468535932672 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.4615418612957001, loss=1.4694455862045288
I0221 04:49:58.628723 140483498055424 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.48611685633659363, loss=1.4973913431167603
I0221 04:51:14.328351 140468535932672 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.6122647523880005, loss=1.5509037971496582
I0221 04:52:30.048163 140483498055424 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.4611867368221283, loss=1.5249799489974976
I0221 04:53:45.725649 140468535932672 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.5024394392967224, loss=1.4440574645996094
I0221 04:55:01.549515 140483498055424 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.4661983549594879, loss=1.5533721446990967
I0221 04:56:17.588374 140468535932672 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.45307958126068115, loss=1.5377833843231201
I0221 04:57:36.846121 140483498055424 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.4767363369464874, loss=1.5235308408737183
I0221 04:58:57.231190 140468535932672 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.5118976831436157, loss=1.5065919160842896
I0221 04:58:57.240108 140549388556096 spec.py:321] Evaluating on the training split.
I0221 04:59:52.202492 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 05:00:44.215613 140549388556096 spec.py:349] Evaluating on the test split.
I0221 05:01:11.000979 140549388556096 submission_runner.py:408] Time since start: 9594.21s, 	Step: 11201, 	{'train/ctc_loss': Array(0.42947593, dtype=float32), 'train/wer': 0.15295235114116562, 'validation/ctc_loss': Array(0.589256, dtype=float32), 'validation/wer': 0.1787172827944428, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36695814, dtype=float32), 'test/wer': 0.12471309893770438, 'test/num_examples': 2472, 'score': 8679.58162689209, 'total_duration': 9594.21326494217, 'accumulated_submission_time': 8679.58162689209, 'accumulated_eval_time': 913.8577964305878, 'accumulated_logging_time': 0.3009212017059326}
I0221 05:01:11.037103 140483498055424 logging_writer.py:48] [11201] accumulated_eval_time=913.857796, accumulated_logging_time=0.300921, accumulated_submission_time=8679.581627, global_step=11201, preemption_count=0, score=8679.581627, test/ctc_loss=0.3669581413269043, test/num_examples=2472, test/wer=0.124713, total_duration=9594.213265, train/ctc_loss=0.42947593331336975, train/wer=0.152952, validation/ctc_loss=0.5892559885978699, validation/num_examples=5348, validation/wer=0.178717
I0221 05:02:26.548330 140468535932672 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.45475852489471436, loss=1.5209892988204956
I0221 05:03:45.846271 140483498055424 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.7407127022743225, loss=1.4601128101348877
I0221 05:05:01.588957 140468535932672 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.5449138283729553, loss=1.4753996133804321
I0221 05:06:17.391833 140483498055424 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.5354606509208679, loss=1.4813708066940308
I0221 05:07:33.061702 140468535932672 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.5209749937057495, loss=1.4824678897857666
I0221 05:08:48.754666 140483498055424 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.5273827910423279, loss=1.4598065614700317
I0221 05:10:04.413443 140468535932672 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.49418720602989197, loss=1.5079426765441895
I0221 05:11:22.043027 140483498055424 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.6378553509712219, loss=1.5482133626937866
I0221 05:12:43.741549 140468535932672 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.464275062084198, loss=1.522644281387329
I0221 05:14:04.754241 140483498055424 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.5054090023040771, loss=1.487630009651184
I0221 05:15:26.201999 140468535932672 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.5377944111824036, loss=1.462773084640503
I0221 05:16:47.813068 140483498055424 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.5226048231124878, loss=1.4665026664733887
I0221 05:18:03.451377 140468535932672 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.4730396568775177, loss=1.4389666318893433
I0221 05:19:19.065136 140483498055424 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.5055971741676331, loss=1.4472086429595947
I0221 05:20:34.743514 140468535932672 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.5461655855178833, loss=1.4634464979171753
I0221 05:21:50.418001 140483498055424 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.46364519000053406, loss=1.435644268989563
I0221 05:23:06.150413 140468535932672 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.5091310143470764, loss=1.470829725265503
I0221 05:24:21.946821 140483498055424 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.4792012572288513, loss=1.4771448373794556
I0221 05:25:11.076717 140549388556096 spec.py:321] Evaluating on the training split.
I0221 05:26:05.318388 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 05:26:56.309851 140549388556096 spec.py:349] Evaluating on the test split.
I0221 05:27:22.455085 140549388556096 submission_runner.py:408] Time since start: 11165.67s, 	Step: 13062, 	{'train/ctc_loss': Array(0.4079645, dtype=float32), 'train/wer': 0.14124043715846996, 'validation/ctc_loss': Array(0.5708126, dtype=float32), 'validation/wer': 0.17354238875426012, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35079175, dtype=float32), 'test/wer': 0.11987894298539598, 'test/num_examples': 2472, 'score': 10119.531209468842, 'total_duration': 11165.667533397675, 'accumulated_submission_time': 10119.531209468842, 'accumulated_eval_time': 1045.2302241325378, 'accumulated_logging_time': 0.35329461097717285}
I0221 05:27:22.493515 140483498055424 logging_writer.py:48] [13062] accumulated_eval_time=1045.230224, accumulated_logging_time=0.353295, accumulated_submission_time=10119.531209, global_step=13062, preemption_count=0, score=10119.531209, test/ctc_loss=0.3507917523384094, test/num_examples=2472, test/wer=0.119879, total_duration=11165.667533, train/ctc_loss=0.4079644978046417, train/wer=0.141240, validation/ctc_loss=0.5708125829696655, validation/num_examples=5348, validation/wer=0.173542
I0221 05:27:51.911213 140468535932672 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.45466509461402893, loss=1.4774143695831299
I0221 05:29:07.563064 140483498055424 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.4415089190006256, loss=1.4818639755249023
I0221 05:30:23.309437 140468535932672 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.5516772270202637, loss=1.4457000494003296
I0221 05:31:42.878923 140483498055424 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.49256131052970886, loss=1.4407416582107544
I0221 05:32:58.482578 140468535932672 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.43209418654441833, loss=1.4363138675689697
I0221 05:34:14.260156 140483498055424 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.5480089783668518, loss=1.3808902502059937
I0221 05:35:29.961078 140468535932672 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.44470107555389404, loss=1.388853907585144
I0221 05:36:45.549864 140483498055424 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.5862510800361633, loss=1.4022252559661865
I0221 05:38:01.292362 140468535932672 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.4911249577999115, loss=1.4229815006256104
I0221 05:39:16.909123 140483498055424 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.5747795104980469, loss=1.5334914922714233
I0221 05:40:34.475878 140468535932672 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.5223944187164307, loss=1.5199458599090576
I0221 05:41:54.779693 140483498055424 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.5223618745803833, loss=1.488033652305603
I0221 05:43:16.077918 140468535932672 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.6999613046646118, loss=1.48252534866333
I0221 05:44:36.182015 140483498055424 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.5078951120376587, loss=1.4802483320236206
I0221 05:45:55.713088 140483498055424 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.4136044979095459, loss=1.443914771080017
I0221 05:47:11.715437 140468535932672 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.7255982160568237, loss=1.452524185180664
I0221 05:48:27.402249 140483498055424 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.4493122696876526, loss=1.409061312675476
I0221 05:49:43.051902 140468535932672 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.5727006196975708, loss=1.5294575691223145
I0221 05:50:58.816179 140483498055424 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.4502464532852173, loss=1.347381830215454
I0221 05:51:22.733303 140549388556096 spec.py:321] Evaluating on the training split.
I0221 05:52:16.611607 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 05:53:08.502413 140549388556096 spec.py:349] Evaluating on the test split.
I0221 05:53:34.831055 140549388556096 submission_runner.py:408] Time since start: 12738.04s, 	Step: 14933, 	{'train/ctc_loss': Array(0.39691854, dtype=float32), 'train/wer': 0.13824616316063035, 'validation/ctc_loss': Array(0.5532182, dtype=float32), 'validation/wer': 0.16562557324502544, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.337386, dtype=float32), 'test/wer': 0.11307456380882741, 'test/num_examples': 2472, 'score': 11559.680967330933, 'total_duration': 12738.043025493622, 'accumulated_submission_time': 11559.680967330933, 'accumulated_eval_time': 1177.3215169906616, 'accumulated_logging_time': 0.4089851379394531}
I0221 05:53:34.870905 140483498055424 logging_writer.py:48] [14933] accumulated_eval_time=1177.321517, accumulated_logging_time=0.408985, accumulated_submission_time=11559.680967, global_step=14933, preemption_count=0, score=11559.680967, test/ctc_loss=0.33738601207733154, test/num_examples=2472, test/wer=0.113075, total_duration=12738.043025, train/ctc_loss=0.39691853523254395, train/wer=0.138246, validation/ctc_loss=0.5532181859016418, validation/num_examples=5348, validation/wer=0.165626
I0221 05:54:26.204933 140468535932672 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.5062354207038879, loss=1.3877102136611938
I0221 05:55:41.949534 140483498055424 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.5357640385627747, loss=1.4382954835891724
I0221 05:56:57.748835 140468535932672 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.6095914840698242, loss=1.4364629983901978
I0221 05:58:13.504837 140483498055424 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.4797746241092682, loss=1.4583040475845337
I0221 05:59:29.122085 140468535932672 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.7060336470603943, loss=1.4029085636138916
I0221 06:00:48.320060 140483498055424 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.5074076056480408, loss=1.379980444908142
I0221 06:02:03.966908 140468535932672 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.5127350687980652, loss=1.39874267578125
I0221 06:03:19.650047 140483498055424 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.5962817072868347, loss=1.3913244009017944
I0221 06:04:35.580169 140468535932672 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.6269603371620178, loss=1.3854737281799316
I0221 06:05:51.207050 140483498055424 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.6176794767379761, loss=1.441752314567566
I0221 06:07:06.734448 140468535932672 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.5284761786460876, loss=1.429054856300354
I0221 06:08:22.392827 140483498055424 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.5439221262931824, loss=1.4283281564712524
I0221 06:09:40.740403 140468535932672 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.5196649432182312, loss=1.4143277406692505
I0221 06:11:01.581433 140483498055424 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.43886351585388184, loss=1.396162509918213
I0221 06:12:22.450143 140468535932672 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.46245110034942627, loss=1.4648470878601074
I0221 06:13:45.730249 140483498055424 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.5115657448768616, loss=1.4019993543624878
I0221 06:15:01.484183 140468535932672 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.45678794384002686, loss=1.4220359325408936
I0221 06:16:17.258489 140483498055424 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.5780379772186279, loss=1.360864520072937
I0221 06:17:32.962109 140468535932672 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.4733816385269165, loss=1.3897472620010376
I0221 06:17:34.966044 140549388556096 spec.py:321] Evaluating on the training split.
I0221 06:18:28.858491 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 06:19:21.042641 140549388556096 spec.py:349] Evaluating on the test split.
I0221 06:19:47.526959 140549388556096 submission_runner.py:408] Time since start: 14310.74s, 	Step: 16804, 	{'train/ctc_loss': Array(0.39982402, dtype=float32), 'train/wer': 0.13525741719949558, 'validation/ctc_loss': Array(0.53366345, dtype=float32), 'validation/wer': 0.15911833708255693, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3234783, dtype=float32), 'test/wer': 0.10779355310462495, 'test/num_examples': 2472, 'score': 12999.688222646713, 'total_duration': 14310.739187717438, 'accumulated_submission_time': 12999.688222646713, 'accumulated_eval_time': 1309.8762323856354, 'accumulated_logging_time': 0.4639158248901367}
I0221 06:19:47.564836 140483498055424 logging_writer.py:48] [16804] accumulated_eval_time=1309.876232, accumulated_logging_time=0.463916, accumulated_submission_time=12999.688223, global_step=16804, preemption_count=0, score=12999.688223, test/ctc_loss=0.3234783113002777, test/num_examples=2472, test/wer=0.107794, total_duration=14310.739188, train/ctc_loss=0.39982402324676514, train/wer=0.135257, validation/ctc_loss=0.5336634516716003, validation/num_examples=5348, validation/wer=0.159118
I0221 06:21:00.688412 140468535932672 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.4640776216983795, loss=1.3978290557861328
I0221 06:22:16.653757 140483498055424 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.4021916389465332, loss=1.3990942239761353
I0221 06:23:32.342530 140468535932672 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.4445783495903015, loss=1.4201046228408813
I0221 06:24:48.090829 140483498055424 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.42519065737724304, loss=1.3842660188674927
I0221 06:26:03.800303 140468535932672 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.5983492136001587, loss=1.3655939102172852
I0221 06:27:19.560286 140483498055424 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.5393080115318298, loss=1.3858495950698853
I0221 06:28:39.874129 140468535932672 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.5034708380699158, loss=1.405409574508667
I0221 06:29:59.436733 140483498055424 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.39205417037010193, loss=1.3809850215911865
I0221 06:31:15.021068 140468535932672 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.41053134202957153, loss=1.3305625915527344
I0221 06:32:30.705732 140483498055424 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.4976876676082611, loss=1.4181400537490845
I0221 06:33:46.331359 140468535932672 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.5166357755661011, loss=1.456404447555542
I0221 06:35:02.075589 140483498055424 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.5747725367546082, loss=1.373781442642212
I0221 06:36:17.770429 140468535932672 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.5216004848480225, loss=1.4053784608840942
I0221 06:37:34.300572 140483498055424 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.5028831362724304, loss=1.4106451272964478
I0221 06:38:53.650785 140468535932672 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.45068883895874023, loss=1.3696320056915283
I0221 06:40:14.581919 140483498055424 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.6074461936950684, loss=1.3977209329605103
I0221 06:41:35.878808 140468535932672 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.5122768878936768, loss=1.4363882541656494
I0221 06:42:56.505491 140483498055424 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.5224499106407166, loss=1.3670780658721924
I0221 06:43:47.575983 140549388556096 spec.py:321] Evaluating on the training split.
I0221 06:44:42.141709 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 06:45:33.165551 140549388556096 spec.py:349] Evaluating on the test split.
I0221 06:45:58.909602 140549388556096 submission_runner.py:408] Time since start: 15882.12s, 	Step: 18669, 	{'train/ctc_loss': Array(0.36982483, dtype=float32), 'train/wer': 0.13034681920725455, 'validation/ctc_loss': Array(0.5220176, dtype=float32), 'validation/wer': 0.15708120528688801, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31250748, dtype=float32), 'test/wer': 0.10627018463225885, 'test/num_examples': 2472, 'score': 14439.60961842537, 'total_duration': 15882.122144460678, 'accumulated_submission_time': 14439.60961842537, 'accumulated_eval_time': 1441.2039613723755, 'accumulated_logging_time': 0.5192501544952393}
I0221 06:45:58.950350 140483498055424 logging_writer.py:48] [18669] accumulated_eval_time=1441.203961, accumulated_logging_time=0.519250, accumulated_submission_time=14439.609618, global_step=18669, preemption_count=0, score=14439.609618, test/ctc_loss=0.3125074803829193, test/num_examples=2472, test/wer=0.106270, total_duration=15882.122144, train/ctc_loss=0.3698248267173767, train/wer=0.130347, validation/ctc_loss=0.5220175981521606, validation/num_examples=5348, validation/wer=0.157081
I0221 06:46:23.168488 140468535932672 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.4081804156303406, loss=1.346286416053772
I0221 06:47:38.758268 140483498055424 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.5282125473022461, loss=1.371960997581482
I0221 06:48:54.460205 140468535932672 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.520516037940979, loss=1.3740880489349365
I0221 06:50:10.248993 140483498055424 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.4621194303035736, loss=1.405800223350525
I0221 06:51:25.955859 140468535932672 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.5099455714225769, loss=1.3669778108596802
I0221 06:52:41.611510 140483498055424 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.40889862179756165, loss=1.339614748954773
I0221 06:53:57.305074 140468535932672 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.5374436378479004, loss=1.356947898864746
I0221 06:55:13.491421 140483498055424 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.4998472332954407, loss=1.3780100345611572
I0221 06:56:31.846433 140468535932672 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.4343337416648865, loss=1.4061474800109863
I0221 06:57:54.839950 140483498055424 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.553261935710907, loss=1.3321608304977417
I0221 06:59:10.497140 140468535932672 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.6169934868812561, loss=1.3754795789718628
I0221 07:00:26.242042 140483498055424 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.4372667670249939, loss=1.3831050395965576
I0221 07:01:41.917235 140468535932672 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.6474922299385071, loss=1.387537956237793
I0221 07:02:57.587719 140483498055424 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.419148325920105, loss=1.305250883102417
I0221 07:04:13.310839 140468535932672 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.5006687641143799, loss=1.2801791429519653
I0221 07:05:28.945589 140483498055424 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.5650062561035156, loss=1.3510078191757202
I0221 07:06:46.096460 140468535932672 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.5547755360603333, loss=1.4008063077926636
I0221 07:08:07.706886 140483498055424 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.46557140350341797, loss=1.4027456045150757
I0221 07:09:28.223310 140468535932672 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.6004512906074524, loss=1.4136979579925537
I0221 07:09:59.438765 140549388556096 spec.py:321] Evaluating on the training split.
I0221 07:10:51.853113 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 07:11:43.328436 140549388556096 spec.py:349] Evaluating on the test split.
I0221 07:12:10.089170 140549388556096 submission_runner.py:408] Time since start: 17453.30s, 	Step: 20541, 	{'train/ctc_loss': Array(0.37734643, dtype=float32), 'train/wer': 0.1359049127256365, 'validation/ctc_loss': Array(0.5120922, dtype=float32), 'validation/wer': 0.1548606350830783, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31171122, dtype=float32), 'test/wer': 0.10403591087278857, 'test/num_examples': 2472, 'score': 15880.007965564728, 'total_duration': 17453.30059313774, 'accumulated_submission_time': 15880.007965564728, 'accumulated_eval_time': 1571.8473567962646, 'accumulated_logging_time': 0.5762767791748047}
I0221 07:12:10.127887 140483498055424 logging_writer.py:48] [20541] accumulated_eval_time=1571.847357, accumulated_logging_time=0.576277, accumulated_submission_time=15880.007966, global_step=20541, preemption_count=0, score=15880.007966, test/ctc_loss=0.31171122193336487, test/num_examples=2472, test/wer=0.104036, total_duration=17453.300593, train/ctc_loss=0.3773464262485504, train/wer=0.135905, validation/ctc_loss=0.5120921730995178, validation/num_examples=5348, validation/wer=0.154861
I0221 07:12:59.104332 140483498055424 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.47329792380332947, loss=1.3514858484268188
I0221 07:14:14.948509 140468535932672 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.4889164865016937, loss=1.3379510641098022
I0221 07:15:30.788794 140483498055424 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.5278943777084351, loss=1.3798511028289795
I0221 07:16:46.656019 140468535932672 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.4113425612449646, loss=1.342826008796692
I0221 07:18:02.421996 140483498055424 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.5369395613670349, loss=1.33311128616333
I0221 07:19:18.282631 140468535932672 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.4571217894554138, loss=1.3586851358413696
I0221 07:20:33.998142 140483498055424 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.47242334485054016, loss=1.3699440956115723
I0221 07:21:52.537036 140468535932672 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.49305909872055054, loss=1.3677152395248413
I0221 07:23:13.857139 140483498055424 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.6059408783912659, loss=1.3283530473709106
I0221 07:24:34.405961 140468535932672 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.5637689232826233, loss=1.4123295545578003
I0221 07:25:55.051538 140483498055424 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.5027928948402405, loss=1.3659788370132446
I0221 07:27:15.828103 140483498055424 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.5109210014343262, loss=1.3121334314346313
I0221 07:28:31.801127 140468535932672 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.6731810569763184, loss=1.368504524230957
I0221 07:29:47.469577 140483498055424 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.4391041398048401, loss=1.3349639177322388
I0221 07:31:03.157506 140468535932672 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.5198439359664917, loss=1.3810200691223145
I0221 07:32:18.802654 140483498055424 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.541269838809967, loss=1.3630636930465698
I0221 07:33:34.440814 140468535932672 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.4906088709831238, loss=1.3880534172058105
I0221 07:34:51.619528 140483498055424 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.5362778306007385, loss=1.3366127014160156
I0221 07:36:10.794314 140549388556096 spec.py:321] Evaluating on the training split.
I0221 07:37:05.062243 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 07:37:56.820498 140549388556096 spec.py:349] Evaluating on the test split.
I0221 07:38:23.140857 140549388556096 submission_runner.py:408] Time since start: 19026.35s, 	Step: 22399, 	{'train/ctc_loss': Array(0.3108637, dtype=float32), 'train/wer': 0.10999774267608503, 'validation/ctc_loss': Array(0.4965751, dtype=float32), 'validation/wer': 0.14681830908406307, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2929786, dtype=float32), 'test/wer': 0.09751589381106168, 'test/num_examples': 2472, 'score': 17320.585915327072, 'total_duration': 19026.353321552277, 'accumulated_submission_time': 17320.585915327072, 'accumulated_eval_time': 1704.1879363059998, 'accumulated_logging_time': 0.6316831111907959}
I0221 07:38:23.184288 140483498055424 logging_writer.py:48] [22399] accumulated_eval_time=1704.187936, accumulated_logging_time=0.631683, accumulated_submission_time=17320.585915, global_step=22399, preemption_count=0, score=17320.585915, test/ctc_loss=0.2929786145687103, test/num_examples=2472, test/wer=0.097516, total_duration=19026.353322, train/ctc_loss=0.3108637034893036, train/wer=0.109998, validation/ctc_loss=0.49657508730888367, validation/num_examples=5348, validation/wer=0.146818
I0221 07:38:24.810466 140468535932672 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.6042506694793701, loss=1.3640133142471313
I0221 07:39:40.160174 140483498055424 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.4276333153247833, loss=1.3583904504776
I0221 07:40:55.880252 140468535932672 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.5023426413536072, loss=1.2998870611190796
I0221 07:42:15.001388 140483498055424 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.5310730934143066, loss=1.3266408443450928
I0221 07:43:30.634741 140468535932672 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.5181354880332947, loss=1.3219619989395142
I0221 07:44:46.336724 140483498055424 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.5121901035308838, loss=1.3291020393371582
I0221 07:46:02.400508 140468535932672 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.5399547219276428, loss=1.336479902267456
I0221 07:47:18.019110 140483498055424 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.7359307408332825, loss=1.2776508331298828
I0221 07:48:33.694567 140468535932672 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.5135588049888611, loss=1.3873990774154663
I0221 07:49:49.372943 140483498055424 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.5757450461387634, loss=1.3365217447280884
I0221 07:51:08.915124 140468535932672 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.50193852186203, loss=1.3036620616912842
I0221 07:52:29.272538 140483498055424 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.5251731872558594, loss=1.3002018928527832
I0221 07:53:50.007879 140468535932672 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.39383554458618164, loss=1.3122493028640747
I0221 07:55:14.206284 140483498055424 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.5535609126091003, loss=1.3440605401992798
I0221 07:56:29.775845 140468535932672 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.4809536039829254, loss=1.2595747709274292
I0221 07:57:45.486517 140483498055424 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.5126627683639526, loss=1.3162847757339478
I0221 07:59:01.130680 140468535932672 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.5317689776420593, loss=1.3205102682113647
I0221 08:00:17.000267 140483498055424 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.5774877667427063, loss=1.3715660572052002
I0221 08:01:33.056426 140468535932672 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.5185484290122986, loss=1.328830361366272
I0221 08:02:23.451588 140549388556096 spec.py:321] Evaluating on the training split.
I0221 08:03:16.089583 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 08:04:07.592760 140549388556096 spec.py:349] Evaluating on the test split.
I0221 08:04:33.737401 140549388556096 submission_runner.py:408] Time since start: 20596.95s, 	Step: 24268, 	{'train/ctc_loss': Array(0.3285102, dtype=float32), 'train/wer': 0.11573634472779888, 'validation/ctc_loss': Array(0.48729917, dtype=float32), 'validation/wer': 0.14549562161483728, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28838947, dtype=float32), 'test/wer': 0.09629719903316881, 'test/num_examples': 2472, 'score': 18760.76453447342, 'total_duration': 20596.949521303177, 'accumulated_submission_time': 18760.76453447342, 'accumulated_eval_time': 1834.4674425125122, 'accumulated_logging_time': 0.6906123161315918}
I0221 08:04:33.775220 140483498055424 logging_writer.py:48] [24268] accumulated_eval_time=1834.467443, accumulated_logging_time=0.690612, accumulated_submission_time=18760.764534, global_step=24268, preemption_count=0, score=18760.764534, test/ctc_loss=0.2883894741535187, test/num_examples=2472, test/wer=0.096297, total_duration=20596.949521, train/ctc_loss=0.32851019501686096, train/wer=0.115736, validation/ctc_loss=0.4872991740703583, validation/num_examples=5348, validation/wer=0.145496
I0221 08:04:58.683714 140468535932672 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.5342007279396057, loss=1.3803212642669678
I0221 08:06:14.194212 140483498055424 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.4610280692577362, loss=1.3691984415054321
I0221 08:07:29.871983 140468535932672 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.4875612258911133, loss=1.2885349988937378
I0221 08:08:45.656179 140483498055424 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.41138705611228943, loss=1.3406763076782227
I0221 08:10:01.382337 140468535932672 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.5502235889434814, loss=1.2943832874298096
I0221 08:11:20.794889 140483498055424 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.5093271732330322, loss=1.301013708114624
I0221 08:12:36.577315 140468535932672 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.5240105986595154, loss=1.3267923593521118
I0221 08:13:52.393939 140483498055424 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.5240815281867981, loss=1.2952346801757812
I0221 08:15:08.184085 140468535932672 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.5698338150978088, loss=1.3585585355758667
I0221 08:16:23.890395 140483498055424 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.6654322743415833, loss=1.347773551940918
I0221 08:17:39.696673 140468535932672 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.6291506886482239, loss=1.3339287042617798
I0221 08:18:56.054126 140483498055424 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.5477162003517151, loss=1.3259657621383667
I0221 08:20:16.683187 140468535932672 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.5844699144363403, loss=1.2859488725662231
I0221 08:21:38.310286 140483498055424 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.43472832441329956, loss=1.3026070594787598
I0221 08:22:59.768497 140468535932672 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.5508518815040588, loss=1.3173755407333374
I0221 08:24:20.844992 140483498055424 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.6824716925621033, loss=1.2899839878082275
I0221 08:25:36.581871 140468535932672 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.45618587732315063, loss=1.2717713117599487
I0221 08:26:52.337702 140483498055424 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.546673059463501, loss=1.2819267511367798
I0221 08:28:08.238169 140468535932672 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.5884630084037781, loss=1.296517252922058
I0221 08:28:34.467384 140549388556096 spec.py:321] Evaluating on the training split.
I0221 08:29:27.334024 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 08:30:18.063839 140549388556096 spec.py:349] Evaluating on the test split.
I0221 08:30:43.936799 140549388556096 submission_runner.py:408] Time since start: 22167.15s, 	Step: 26136, 	{'train/ctc_loss': Array(0.31813964, dtype=float32), 'train/wer': 0.11223466630410768, 'validation/ctc_loss': Array(0.47977257, dtype=float32), 'validation/wer': 0.14233855006420346, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27694356, dtype=float32), 'test/wer': 0.09205207889017529, 'test/num_examples': 2472, 'score': 20201.36756658554, 'total_duration': 22167.149206638336, 'accumulated_submission_time': 20201.36756658554, 'accumulated_eval_time': 1963.9308321475983, 'accumulated_logging_time': 0.7447621822357178}
I0221 08:30:43.976008 140483498055424 logging_writer.py:48] [26136] accumulated_eval_time=1963.930832, accumulated_logging_time=0.744762, accumulated_submission_time=20201.367567, global_step=26136, preemption_count=0, score=20201.367567, test/ctc_loss=0.276943564414978, test/num_examples=2472, test/wer=0.092052, total_duration=22167.149207, train/ctc_loss=0.3181396424770355, train/wer=0.112235, validation/ctc_loss=0.47977256774902344, validation/num_examples=5348, validation/wer=0.142339
I0221 08:31:32.996549 140468535932672 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.4498178958892822, loss=1.2778998613357544
I0221 08:32:48.702591 140483498055424 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.6259564757347107, loss=1.3053289651870728
I0221 08:34:04.403319 140468535932672 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.5326318740844727, loss=1.304726481437683
I0221 08:35:20.118790 140483498055424 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.5334616899490356, loss=1.3074177503585815
I0221 08:36:36.170813 140468535932672 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.5322707891464233, loss=1.3030418157577515
I0221 08:37:51.938182 140483498055424 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.5243732333183289, loss=1.3354347944259644
I0221 08:39:14.253796 140483498055424 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.4563349187374115, loss=1.2693291902542114
I0221 08:40:29.868948 140468535932672 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.4522458612918854, loss=1.3286470174789429
I0221 08:41:45.463103 140483498055424 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.4741326570510864, loss=1.3192088603973389
I0221 08:43:01.175919 140468535932672 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.5807414650917053, loss=1.3110464811325073
I0221 08:44:16.877737 140483498055424 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.5312375426292419, loss=1.3154842853546143
I0221 08:45:32.634170 140468535932672 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.690960168838501, loss=1.2617040872573853
I0221 08:46:48.342392 140483498055424 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.4718409478664398, loss=1.2420001029968262
I0221 08:48:07.043223 140468535932672 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.8398469090461731, loss=1.295348048210144
I0221 08:49:28.249050 140483498055424 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.5068591237068176, loss=1.2901028394699097
I0221 08:50:49.250755 140468535932672 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.5623751878738403, loss=1.2506436109542847
I0221 08:52:10.641141 140483498055424 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.6012263298034668, loss=1.274557113647461
I0221 08:53:30.489115 140483498055424 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.608677327632904, loss=1.302925944328308
I0221 08:54:44.437086 140549388556096 spec.py:321] Evaluating on the training split.
I0221 08:55:38.621442 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 08:56:29.972291 140549388556096 spec.py:349] Evaluating on the test split.
I0221 08:56:56.096534 140549388556096 submission_runner.py:408] Time since start: 23739.31s, 	Step: 27999, 	{'train/ctc_loss': Array(0.3083447, dtype=float32), 'train/wer': 0.10892823796917736, 'validation/ctc_loss': Array(0.46320656, dtype=float32), 'validation/wer': 0.13850565279936666, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27267453, dtype=float32), 'test/wer': 0.09190989783275445, 'test/num_examples': 2472, 'score': 21641.74042582512, 'total_duration': 23739.309053897858, 'accumulated_submission_time': 21641.74042582512, 'accumulated_eval_time': 2095.584389448166, 'accumulated_logging_time': 0.7992823123931885}
I0221 08:56:56.135709 140483498055424 logging_writer.py:48] [27999] accumulated_eval_time=2095.584389, accumulated_logging_time=0.799282, accumulated_submission_time=21641.740426, global_step=27999, preemption_count=0, score=21641.740426, test/ctc_loss=0.2726745307445526, test/num_examples=2472, test/wer=0.091910, total_duration=23739.309054, train/ctc_loss=0.30834469199180603, train/wer=0.108928, validation/ctc_loss=0.46320655941963196, validation/num_examples=5348, validation/wer=0.138506
I0221 08:56:57.770441 140468535932672 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.5083954334259033, loss=1.2338635921478271
I0221 08:58:13.279801 140483498055424 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.4771701395511627, loss=1.2641140222549438
I0221 08:59:29.155478 140468535932672 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.4878188669681549, loss=1.2556589841842651
I0221 09:00:44.973677 140483498055424 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.5177852511405945, loss=1.2562052011489868
I0221 09:02:00.644998 140468535932672 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.5768134593963623, loss=1.2712815999984741
I0221 09:03:16.491453 140483498055424 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.5372633934020996, loss=1.328269124031067
I0221 09:04:32.238046 140468535932672 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.5800774693489075, loss=1.2912489175796509
I0221 09:05:48.021515 140483498055424 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.46951743960380554, loss=1.3401490449905396
I0221 09:07:08.653151 140468535932672 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.4695785939693451, loss=1.2662129402160645
I0221 09:08:29.860516 140483498055424 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.432841956615448, loss=1.2939438819885254
I0221 09:09:45.846887 140468535932672 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.5147650241851807, loss=1.3120429515838623
I0221 09:11:01.495898 140483498055424 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.519029438495636, loss=1.297501564025879
I0221 09:12:17.163510 140468535932672 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.4585784077644348, loss=1.2721495628356934
I0221 09:13:32.799753 140483498055424 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.5068002343177795, loss=1.2407286167144775
I0221 09:14:48.495081 140468535932672 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.5970206260681152, loss=1.2353076934814453
I0221 09:16:06.388872 140483498055424 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.48500341176986694, loss=1.2910609245300293
I0221 09:17:28.020053 140468535932672 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.4966570734977722, loss=1.2341173887252808
I0221 09:18:48.222290 140483498055424 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.5799819231033325, loss=1.2736597061157227
I0221 09:20:09.647297 140468535932672 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.597029983997345, loss=1.2901651859283447
I0221 09:20:56.255699 140549388556096 spec.py:321] Evaluating on the training split.
I0221 09:21:48.051089 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 09:22:39.382435 140549388556096 spec.py:349] Evaluating on the test split.
I0221 09:23:05.765734 140549388556096 submission_runner.py:408] Time since start: 25308.98s, 	Step: 29860, 	{'train/ctc_loss': Array(0.27677372, dtype=float32), 'train/wer': 0.10168670900328784, 'validation/ctc_loss': Array(0.4609452, dtype=float32), 'validation/wer': 0.13718296533014085, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2670367, dtype=float32), 'test/wer': 0.08916783458249548, 'test/num_examples': 2472, 'score': 23081.77032160759, 'total_duration': 25308.97817516327, 'accumulated_submission_time': 23081.77032160759, 'accumulated_eval_time': 2225.088423728943, 'accumulated_logging_time': 0.8564896583557129}
I0221 09:23:05.807046 140483498055424 logging_writer.py:48] [29860] accumulated_eval_time=2225.088424, accumulated_logging_time=0.856490, accumulated_submission_time=23081.770322, global_step=29860, preemption_count=0, score=23081.770322, test/ctc_loss=0.26703670620918274, test/num_examples=2472, test/wer=0.089168, total_duration=25308.978175, train/ctc_loss=0.27677372097969055, train/wer=0.101687, validation/ctc_loss=0.460945188999176, validation/num_examples=5348, validation/wer=0.137183
I0221 09:23:40.397103 140483498055424 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.5346567034721375, loss=1.2726811170578003
I0221 09:24:55.986252 140468535932672 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.46713849902153015, loss=1.217093825340271
I0221 09:26:11.696684 140483498055424 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.5327521562576294, loss=1.314509630203247
I0221 09:27:27.742170 140468535932672 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.46032947301864624, loss=1.2282105684280396
I0221 09:28:43.492563 140483498055424 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.5456740260124207, loss=1.2091926336288452
I0221 09:29:59.184127 140468535932672 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.5048055648803711, loss=1.3099418878555298
I0221 09:31:17.212259 140483498055424 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.5532258152961731, loss=1.270159125328064
I0221 09:32:39.288616 140468535932672 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.5324613451957703, loss=1.239719033241272
I0221 09:34:00.112529 140483498055424 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.587378203868866, loss=1.3290330171585083
I0221 09:35:21.434789 140468535932672 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.5618495345115662, loss=1.2963882684707642
I0221 09:36:46.637129 140483498055424 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.5247701406478882, loss=1.2192693948745728
I0221 09:38:02.287304 140468535932672 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.5053952932357788, loss=1.2787566184997559
I0221 09:39:17.970463 140483498055424 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.5252379179000854, loss=1.2242296934127808
I0221 09:40:33.681711 140468535932672 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.45729419589042664, loss=1.2122414112091064
I0221 09:41:49.392459 140483498055424 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.4744862914085388, loss=1.2586525678634644
I0221 09:43:05.415771 140468535932672 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.44280433654785156, loss=1.2267255783081055
I0221 09:44:21.096649 140483498055424 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.538036584854126, loss=1.2815266847610474
I0221 09:45:41.353044 140468535932672 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.5189816951751709, loss=1.24873685836792
I0221 09:47:02.240944 140483498055424 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.5781011581420898, loss=1.2137045860290527
I0221 09:47:05.915483 140549388556096 spec.py:321] Evaluating on the training split.
I0221 09:47:58.803831 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 09:48:49.878261 140549388556096 spec.py:349] Evaluating on the test split.
I0221 09:49:16.098955 140549388556096 submission_runner.py:408] Time since start: 26879.31s, 	Step: 31706, 	{'train/ctc_loss': Array(0.24583429, dtype=float32), 'train/wer': 0.09052715700545139, 'validation/ctc_loss': Array(0.4394589, dtype=float32), 'validation/wer': 0.13130328161657512, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2575679, dtype=float32), 'test/wer': 0.08711636503970914, 'test/num_examples': 2472, 'score': 24521.790633678436, 'total_duration': 26879.311678886414, 'accumulated_submission_time': 24521.790633678436, 'accumulated_eval_time': 2355.266172170639, 'accumulated_logging_time': 0.9134845733642578}
I0221 09:49:16.136433 140483498055424 logging_writer.py:48] [31706] accumulated_eval_time=2355.266172, accumulated_logging_time=0.913485, accumulated_submission_time=24521.790634, global_step=31706, preemption_count=0, score=24521.790634, test/ctc_loss=0.2575679123401642, test/num_examples=2472, test/wer=0.087116, total_duration=26879.311679, train/ctc_loss=0.24583429098129272, train/wer=0.090527, validation/ctc_loss=0.4394589066505432, validation/num_examples=5348, validation/wer=0.131303
I0221 09:50:27.767196 140468535932672 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.48953235149383545, loss=1.2181466817855835
I0221 09:51:43.573911 140483498055424 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.593605101108551, loss=1.26345956325531
I0221 09:53:02.796740 140483498055424 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.5756521821022034, loss=1.275070071220398
I0221 09:54:18.337699 140468535932672 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.5906835794448853, loss=1.2085117101669312
I0221 09:55:34.089371 140483498055424 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.5775260329246521, loss=1.2099971771240234
I0221 09:56:49.829311 140468535932672 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.5136492252349854, loss=1.2230511903762817
I0221 09:58:05.582669 140483498055424 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.5183584690093994, loss=1.2528040409088135
I0221 09:59:21.313592 140468535932672 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.5077782273292542, loss=1.2361059188842773
I0221 10:00:39.011209 140483498055424 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.511030912399292, loss=1.1935559511184692
I0221 10:02:00.633131 140468535932672 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.4472556710243225, loss=1.232027530670166
I0221 10:03:22.110137 140483498055424 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.674835741519928, loss=1.2302087545394897
I0221 10:04:43.290533 140468535932672 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.6341211199760437, loss=1.2929733991622925
I0221 10:06:05.721034 140483498055424 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.7458049058914185, loss=1.1978107690811157
I0221 10:07:21.386110 140468535932672 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.6540111899375916, loss=1.185874581336975
I0221 10:08:37.059618 140483498055424 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.5108534693717957, loss=1.2798335552215576
I0221 10:09:52.732012 140468535932672 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.6043826341629028, loss=1.230423927307129
I0221 10:11:08.509543 140483498055424 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.715987503528595, loss=1.2946902513504028
I0221 10:12:24.174589 140468535932672 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.5811734795570374, loss=1.243830680847168
I0221 10:13:16.398130 140549388556096 spec.py:321] Evaluating on the training split.
I0221 10:14:09.679819 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 10:15:01.085027 140549388556096 spec.py:349] Evaluating on the test split.
I0221 10:15:27.469183 140549388556096 submission_runner.py:408] Time since start: 28450.68s, 	Step: 33570, 	{'train/ctc_loss': Array(0.2740817, dtype=float32), 'train/wer': 0.09920870593159463, 'validation/ctc_loss': Array(0.4404438, dtype=float32), 'validation/wer': 0.1302991976983307, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25297192, dtype=float32), 'test/wer': 0.0839680701968192, 'test/num_examples': 2472, 'score': 25961.964494228363, 'total_duration': 28450.681255578995, 'accumulated_submission_time': 25961.964494228363, 'accumulated_eval_time': 2486.330880880356, 'accumulated_logging_time': 0.966865062713623}
I0221 10:15:27.513655 140483498055424 logging_writer.py:48] [33570] accumulated_eval_time=2486.330881, accumulated_logging_time=0.966865, accumulated_submission_time=25961.964494, global_step=33570, preemption_count=0, score=25961.964494, test/ctc_loss=0.25297191739082336, test/num_examples=2472, test/wer=0.083968, total_duration=28450.681256, train/ctc_loss=0.2740817070007324, train/wer=0.099209, validation/ctc_loss=0.44044381380081177, validation/num_examples=5348, validation/wer=0.130299
I0221 10:15:50.947893 140468535932672 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.46762216091156006, loss=1.2353742122650146
I0221 10:17:06.635055 140483498055424 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.5198707580566406, loss=1.2798658609390259
I0221 10:18:22.718420 140468535932672 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.47757309675216675, loss=1.2979568243026733
I0221 10:19:38.513473 140483498055424 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.5167705416679382, loss=1.284295678138733
I0221 10:20:57.848795 140483498055424 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.44763869047164917, loss=1.2339860200881958
I0221 10:22:13.525554 140468535932672 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.49948805570602417, loss=1.27785325050354
I0221 10:23:29.219996 140483498055424 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.5800065398216248, loss=1.2219314575195312
I0221 10:24:45.015034 140468535932672 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.49957841634750366, loss=1.2220723628997803
I0221 10:26:00.699757 140483498055424 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.5747868418693542, loss=1.2353655099868774
I0221 10:27:16.397093 140468535932672 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.44865405559539795, loss=1.2389732599258423
I0221 10:28:32.563344 140483498055424 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.4412234127521515, loss=1.2354234457015991
I0221 10:29:53.697487 140468535932672 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.7551942467689514, loss=1.2261370420455933
I0221 10:31:15.209138 140483498055424 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.495192289352417, loss=1.2538594007492065
I0221 10:32:36.465353 140468535932672 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.6007562279701233, loss=1.229236125946045
I0221 10:33:56.655515 140483498055424 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.5482960343360901, loss=1.1879092454910278
I0221 10:35:16.510093 140483498055424 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.5416863560676575, loss=1.2881795167922974
I0221 10:36:32.126174 140468535932672 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.48142504692077637, loss=1.196540117263794
I0221 10:37:47.700346 140483498055424 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.5390551686286926, loss=1.2079447507858276
I0221 10:39:03.318840 140468535932672 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.574476957321167, loss=1.1921957731246948
I0221 10:39:27.961895 140549388556096 spec.py:321] Evaluating on the training split.
I0221 10:40:22.633200 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 10:41:13.487654 140549388556096 spec.py:349] Evaluating on the test split.
I0221 10:41:39.803797 140549388556096 submission_runner.py:408] Time since start: 30023.02s, 	Step: 35434, 	{'train/ctc_loss': Array(0.2441035, dtype=float32), 'train/wer': 0.08939278377490634, 'validation/ctc_loss': Array(0.426791, dtype=float32), 'validation/wer': 0.12841654035162248, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23996721, dtype=float32), 'test/wer': 0.07998700058903581, 'test/num_examples': 2472, 'score': 27402.324479341507, 'total_duration': 30023.01597237587, 'accumulated_submission_time': 27402.324479341507, 'accumulated_eval_time': 2618.166530609131, 'accumulated_logging_time': 1.0274250507354736}
I0221 10:41:39.843415 140483498055424 logging_writer.py:48] [35434] accumulated_eval_time=2618.166531, accumulated_logging_time=1.027425, accumulated_submission_time=27402.324479, global_step=35434, preemption_count=0, score=27402.324479, test/ctc_loss=0.2399672120809555, test/num_examples=2472, test/wer=0.079987, total_duration=30023.015972, train/ctc_loss=0.24410350620746613, train/wer=0.089393, validation/ctc_loss=0.4267910122871399, validation/num_examples=5348, validation/wer=0.128417
I0221 10:42:30.445990 140468535932672 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.5431590676307678, loss=1.1998834609985352
I0221 10:43:46.221879 140483498055424 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.4870574474334717, loss=1.2015748023986816
I0221 10:45:01.922814 140468535932672 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.4971048831939697, loss=1.253621220588684
I0221 10:46:17.685129 140483498055424 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.6730868220329285, loss=1.2408387660980225
I0221 10:47:33.550917 140468535932672 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.583666205406189, loss=1.216005563735962
I0221 10:48:49.274983 140483498055424 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.5614213347434998, loss=1.200917363166809
I0221 10:50:10.477793 140483498055424 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.5560168027877808, loss=1.1970232725143433
I0221 10:51:26.568333 140468535932672 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.5834060311317444, loss=1.18538498878479
I0221 10:52:42.219360 140483498055424 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.5027962923049927, loss=1.2172242403030396
I0221 10:53:57.921285 140468535932672 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.5658634901046753, loss=1.169736385345459
I0221 10:55:13.730957 140483498055424 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.6264629364013672, loss=1.2118827104568481
I0221 10:56:29.448027 140468535932672 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.6017791628837585, loss=1.2081125974655151
I0221 10:57:45.139081 140483498055424 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.6023772954940796, loss=1.1896570920944214
I0221 10:59:05.477953 140468535932672 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.5236644148826599, loss=1.213351845741272
I0221 11:00:26.216243 140483498055424 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.5447351336479187, loss=1.226821780204773
I0221 11:01:47.121773 140468535932672 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.5009602308273315, loss=1.216528296470642
I0221 11:03:11.370270 140483498055424 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.4805043935775757, loss=1.1524596214294434
I0221 11:04:27.065739 140468535932672 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.5171318650245667, loss=1.187453031539917
I0221 11:05:40.136442 140549388556096 spec.py:321] Evaluating on the training split.
I0221 11:06:33.353191 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 11:07:25.278913 140549388556096 spec.py:349] Evaluating on the test split.
I0221 11:07:51.633517 140549388556096 submission_runner.py:408] Time since start: 31594.85s, 	Step: 37298, 	{'train/ctc_loss': Array(0.26563275, dtype=float32), 'train/wer': 0.09124326501837958, 'validation/ctc_loss': Array(0.4151319, dtype=float32), 'validation/wer': 0.12274925900537764, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23475464, dtype=float32), 'test/wer': 0.07992606585014117, 'test/num_examples': 2472, 'score': 28842.528745412827, 'total_duration': 31594.84669661522, 'accumulated_submission_time': 28842.528745412827, 'accumulated_eval_time': 2749.658364534378, 'accumulated_logging_time': 1.0829048156738281}
I0221 11:07:51.670527 140483498055424 logging_writer.py:48] [37298] accumulated_eval_time=2749.658365, accumulated_logging_time=1.082905, accumulated_submission_time=28842.528745, global_step=37298, preemption_count=0, score=28842.528745, test/ctc_loss=0.23475463688373566, test/num_examples=2472, test/wer=0.079926, total_duration=31594.846697, train/ctc_loss=0.2656327486038208, train/wer=0.091243, validation/ctc_loss=0.41513189673423767, validation/num_examples=5348, validation/wer=0.122749
I0221 11:07:54.039358 140468535932672 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.5685356855392456, loss=1.227115511894226
I0221 11:09:09.849933 140483498055424 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.6657520532608032, loss=1.210287094116211
I0221 11:10:25.497451 140468535932672 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.5168800354003906, loss=1.1731956005096436
I0221 11:11:41.167175 140483498055424 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.6521959900856018, loss=1.2215754985809326
I0221 11:12:56.814981 140468535932672 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.5467660427093506, loss=1.193084478378296
I0221 11:14:12.468223 140483498055424 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.5609675049781799, loss=1.1915335655212402
I0221 11:15:28.232402 140468535932672 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.6130691170692444, loss=1.207013487815857
I0221 11:16:46.415382 140483498055424 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.6041402816772461, loss=1.1394134759902954
I0221 11:18:07.213456 140468535932672 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.5456394553184509, loss=1.2228214740753174
I0221 11:19:26.483538 140483498055424 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.5768261551856995, loss=1.1708418130874634
I0221 11:20:42.240843 140468535932672 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.5802471041679382, loss=1.1613608598709106
I0221 11:21:57.942755 140483498055424 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.5275049209594727, loss=1.2137348651885986
I0221 11:23:13.572991 140468535932672 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.603319525718689, loss=1.1973375082015991
I0221 11:24:29.611273 140483498055424 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.5414672493934631, loss=1.1743123531341553
I0221 11:25:45.309586 140468535932672 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.5282776355743408, loss=1.170791745185852
I0221 11:27:00.972141 140483498055424 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.5091803073883057, loss=1.1880757808685303
I0221 11:28:20.864670 140468535932672 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.5771633982658386, loss=1.2294702529907227
I0221 11:29:41.983994 140483498055424 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.7390923500061035, loss=1.1786515712738037
I0221 11:31:03.111308 140468535932672 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.6006807684898376, loss=1.246361494064331
I0221 11:31:52.058558 140549388556096 spec.py:321] Evaluating on the training split.
I0221 11:32:46.131415 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 11:33:36.898420 140549388556096 spec.py:349] Evaluating on the test split.
I0221 11:34:02.746809 140549388556096 submission_runner.py:408] Time since start: 33165.96s, 	Step: 39159, 	{'train/ctc_loss': Array(0.21324308, dtype=float32), 'train/wer': 0.07876427602502954, 'validation/ctc_loss': Array(0.41352835, dtype=float32), 'validation/wer': 0.1211658959035307, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23633035, dtype=float32), 'test/wer': 0.07763085735177624, 'test/num_examples': 2472, 'score': 30282.826360464096, 'total_duration': 33165.95847392082, 'accumulated_submission_time': 30282.826360464096, 'accumulated_eval_time': 2880.340024471283, 'accumulated_logging_time': 1.1378488540649414}
I0221 11:34:02.793872 140483498055424 logging_writer.py:48] [39159] accumulated_eval_time=2880.340024, accumulated_logging_time=1.137849, accumulated_submission_time=30282.826360, global_step=39159, preemption_count=0, score=30282.826360, test/ctc_loss=0.23633034527301788, test/num_examples=2472, test/wer=0.077631, total_duration=33165.958474, train/ctc_loss=0.21324308216571808, train/wer=0.078764, validation/ctc_loss=0.41352835297584534, validation/num_examples=5348, validation/wer=0.121166
I0221 11:34:34.468338 140468535932672 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.6373149156570435, loss=1.1448777914047241
I0221 11:35:49.906481 140483498055424 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.66453617811203, loss=1.2207190990447998
I0221 11:37:05.489690 140468535932672 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.6482884287834167, loss=1.1828429698944092
I0221 11:38:21.195613 140483498055424 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.5579593181610107, loss=1.2194819450378418
I0221 11:39:36.863591 140468535932672 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.5594359040260315, loss=1.223441243171692
I0221 11:40:52.602780 140483498055424 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.6286953091621399, loss=1.1851475238800049
I0221 11:42:08.645594 140468535932672 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.5397089719772339, loss=1.142600655555725
I0221 11:43:24.359857 140483498055424 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.4866199195384979, loss=1.1682238578796387
I0221 11:44:40.008313 140468535932672 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.5896642208099365, loss=1.1754190921783447
I0221 11:46:00.231562 140483498055424 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.6359595656394958, loss=1.2187694311141968
I0221 11:47:23.025032 140483498055424 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.533700704574585, loss=1.2250627279281616
I0221 11:48:38.591113 140468535932672 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.6728679537773132, loss=1.2049405574798584
I0221 11:49:54.280998 140483498055424 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.5014321208000183, loss=1.121075987815857
I0221 11:51:10.076648 140468535932672 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.6172569990158081, loss=1.209168791770935
I0221 11:52:25.832827 140483498055424 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.6573333144187927, loss=1.185837984085083
I0221 11:53:41.562234 140468535932672 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.7241771221160889, loss=1.1420170068740845
I0221 11:54:58.504701 140483498055424 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.5853550434112549, loss=1.1481335163116455
I0221 11:56:19.512557 140468535932672 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.6408692598342896, loss=1.1868454217910767
I0221 11:57:39.786977 140483498055424 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.49826544523239136, loss=1.19255793094635
I0221 11:58:02.920488 140549388556096 spec.py:321] Evaluating on the training split.
I0221 11:58:56.299730 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 11:59:47.835317 140549388556096 spec.py:349] Evaluating on the test split.
I0221 12:00:14.213802 140549388556096 submission_runner.py:408] Time since start: 34737.43s, 	Step: 41031, 	{'train/ctc_loss': Array(0.19461007, dtype=float32), 'train/wer': 0.07236089991897622, 'validation/ctc_loss': Array(0.39671153, dtype=float32), 'validation/wer': 0.11686957529181188, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22629774, dtype=float32), 'test/wer': 0.07537627201267443, 'test/num_examples': 2472, 'score': 31722.86250114441, 'total_duration': 34737.42574644089, 'accumulated_submission_time': 31722.86250114441, 'accumulated_eval_time': 3011.626853942871, 'accumulated_logging_time': 1.2022075653076172}
I0221 12:00:14.254595 140483498055424 logging_writer.py:48] [41031] accumulated_eval_time=3011.626854, accumulated_logging_time=1.202208, accumulated_submission_time=31722.862501, global_step=41031, preemption_count=0, score=31722.862501, test/ctc_loss=0.22629773616790771, test/num_examples=2472, test/wer=0.075376, total_duration=34737.425746, train/ctc_loss=0.19461007416248322, train/wer=0.072361, validation/ctc_loss=0.396711528301239, validation/num_examples=5348, validation/wer=0.116870
I0221 12:01:07.043866 140468535932672 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.5243962407112122, loss=1.1443179845809937
I0221 12:02:26.274726 140483498055424 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.5381969213485718, loss=1.1582305431365967
I0221 12:03:42.055210 140468535932672 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.5798536539077759, loss=1.1298209428787231
I0221 12:04:57.828893 140483498055424 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.5538744330406189, loss=1.177565574645996
I0221 12:06:13.522481 140468535932672 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.5087370276451111, loss=1.1624399423599243
I0221 12:07:29.324769 140483498055424 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.5456861257553101, loss=1.1959084272384644
I0221 12:08:45.156010 140468535932672 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.7200385928153992, loss=1.1327719688415527
I0221 12:10:00.933939 140483498055424 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.5735954642295837, loss=1.1036112308502197
I0221 12:11:17.670610 140468535932672 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.6490390300750732, loss=1.160707712173462
I0221 12:12:38.146057 140483498055424 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.6252581477165222, loss=1.2247575521469116
I0221 12:13:59.279896 140468535932672 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.49639713764190674, loss=1.2292368412017822
I0221 12:15:20.129428 140483498055424 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.665555477142334, loss=1.1611279249191284
I0221 12:16:40.997278 140483498055424 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.4950954020023346, loss=1.1223591566085815
I0221 12:17:56.658410 140468535932672 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.478385865688324, loss=1.1222487688064575
I0221 12:19:12.433408 140483498055424 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.6163923144340515, loss=1.2031254768371582
I0221 12:20:28.215718 140468535932672 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.5926641821861267, loss=1.1977605819702148
I0221 12:21:44.015336 140483498055424 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.5929338932037354, loss=1.0946123600006104
I0221 12:22:59.741634 140468535932672 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.5858686566352844, loss=1.1570653915405273
I0221 12:24:14.831708 140549388556096 spec.py:321] Evaluating on the training split.
I0221 12:25:08.474672 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 12:26:00.315313 140549388556096 spec.py:349] Evaluating on the test split.
I0221 12:26:26.949121 140549388556096 submission_runner.py:408] Time since start: 36310.16s, 	Step: 42900, 	{'train/ctc_loss': Array(0.22017571, dtype=float32), 'train/wer': 0.08099192420740234, 'validation/ctc_loss': Array(0.39385065, dtype=float32), 'validation/wer': 0.11726541606727363, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22514184, dtype=float32), 'test/wer': 0.0751528446367274, 'test/num_examples': 2472, 'score': 33163.35210299492, 'total_duration': 36310.161403894424, 'accumulated_submission_time': 33163.35210299492, 'accumulated_eval_time': 3143.738133907318, 'accumulated_logging_time': 1.2583415508270264}
I0221 12:26:26.989145 140483498055424 logging_writer.py:48] [42900] accumulated_eval_time=3143.738134, accumulated_logging_time=1.258342, accumulated_submission_time=33163.352103, global_step=42900, preemption_count=0, score=33163.352103, test/ctc_loss=0.22514183819293976, test/num_examples=2472, test/wer=0.075153, total_duration=36310.161404, train/ctc_loss=0.22017571330070496, train/wer=0.080992, validation/ctc_loss=0.3938506543636322, validation/num_examples=5348, validation/wer=0.117265
I0221 12:26:27.862903 140468535932672 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.5780048966407776, loss=1.1513644456863403
I0221 12:27:43.420182 140483498055424 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.5591991543769836, loss=1.1527818441390991
I0221 12:28:59.145719 140468535932672 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.6758242845535278, loss=1.1693446636199951
I0221 12:30:14.904561 140483498055424 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.52070152759552, loss=1.1861604452133179
I0221 12:31:34.230379 140483498055424 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.5671714544296265, loss=1.0843415260314941
I0221 12:32:50.150415 140468535932672 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.49738842248916626, loss=1.1560472249984741
I0221 12:34:05.659495 140483498055424 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.6325166821479797, loss=1.1417590379714966
I0221 12:35:21.314845 140468535932672 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.5770320892333984, loss=1.0770277976989746
I0221 12:36:37.002449 140483498055424 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.4656371474266052, loss=1.1414591073989868
I0221 12:37:52.684937 140468535932672 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.5549995303153992, loss=1.0903480052947998
I0221 12:39:08.405340 140483498055424 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.6475008130073547, loss=1.186638355255127
I0221 12:40:24.117161 140468535932672 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.567718505859375, loss=1.1781399250030518
I0221 12:41:42.152251 140483498055424 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.6032835841178894, loss=1.1646884679794312
I0221 12:43:02.359716 140468535932672 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.5467500686645508, loss=1.114423394203186
I0221 12:44:26.020758 140483498055424 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.622918963432312, loss=1.1548880338668823
I0221 12:45:41.634397 140468535932672 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.570504367351532, loss=1.1702502965927124
I0221 12:46:57.239247 140483498055424 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.5133960247039795, loss=1.0554258823394775
I0221 12:48:13.194800 140468535932672 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.5221096873283386, loss=1.1580226421356201
I0221 12:49:28.815435 140483498055424 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.7793251872062683, loss=1.173158049583435
I0221 12:50:27.482675 140549388556096 spec.py:321] Evaluating on the training split.
I0221 12:51:30.256717 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 12:52:22.191490 140549388556096 spec.py:349] Evaluating on the test split.
I0221 12:52:48.221680 140549388556096 submission_runner.py:408] Time since start: 37891.44s, 	Step: 44779, 	{'train/ctc_loss': Array(0.14650366, dtype=float32), 'train/wer': 0.05508156402694497, 'validation/ctc_loss': Array(0.39157462, dtype=float32), 'validation/wer': 0.11330700831265629, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21745369, dtype=float32), 'test/wer': 0.07322324457173035, 'test/num_examples': 2472, 'score': 34603.75563144684, 'total_duration': 37891.43506240845, 'accumulated_submission_time': 34603.75563144684, 'accumulated_eval_time': 3284.47208070755, 'accumulated_logging_time': 1.315403699874878}
I0221 12:52:48.263024 140483498055424 logging_writer.py:48] [44779] accumulated_eval_time=3284.472081, accumulated_logging_time=1.315404, accumulated_submission_time=34603.755631, global_step=44779, preemption_count=0, score=34603.755631, test/ctc_loss=0.21745368838310242, test/num_examples=2472, test/wer=0.073223, total_duration=37891.435062, train/ctc_loss=0.14650365710258484, train/wer=0.055082, validation/ctc_loss=0.3915746212005615, validation/num_examples=5348, validation/wer=0.113307
I0221 12:53:04.919406 140468535932672 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.6221364140510559, loss=1.1094715595245361
I0221 12:54:20.535556 140483498055424 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.61015385389328, loss=1.0661224126815796
I0221 12:55:36.225806 140468535932672 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.7584524750709534, loss=1.1736505031585693
I0221 12:56:51.906826 140483498055424 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.5455675721168518, loss=1.1775325536727905
I0221 12:58:07.671973 140468535932672 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.5632672905921936, loss=1.102560043334961
I0221 12:59:23.498075 140483498055424 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.6427232623100281, loss=1.1482146978378296
I0221 13:00:42.667536 140483498055424 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.6960992813110352, loss=1.154296636581421
I0221 13:01:58.286987 140468535932672 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.6004869341850281, loss=1.0893465280532837
I0221 13:03:13.988981 140483498055424 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.6759219765663147, loss=1.1477575302124023
I0221 13:04:29.657436 140468535932672 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.59795081615448, loss=1.160751223564148
I0221 13:05:45.635625 140483498055424 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.6682982444763184, loss=1.1278741359710693
I0221 13:07:01.325213 140468535932672 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.6072145104408264, loss=1.1094590425491333
I0221 13:08:17.008598 140483498055424 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.7297373414039612, loss=1.1328191757202148
I0221 13:09:34.492728 140468535932672 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.6724066138267517, loss=1.136414885520935
I0221 13:10:55.063866 140483498055424 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.5942896604537964, loss=1.1530588865280151
I0221 13:12:15.572594 140468535932672 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.6615369915962219, loss=1.1762921810150146
I0221 13:13:37.227849 140483498055424 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.6234365105628967, loss=1.097080945968628
I0221 13:14:52.880351 140468535932672 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.56703782081604, loss=1.1290639638900757
I0221 13:16:08.619070 140483498055424 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.5284981727600098, loss=1.1816540956497192
I0221 13:16:48.356296 140549388556096 spec.py:321] Evaluating on the training split.
I0221 13:17:43.558198 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 13:18:35.455257 140549388556096 spec.py:349] Evaluating on the test split.
I0221 13:19:01.444048 140549388556096 submission_runner.py:408] Time since start: 39464.66s, 	Step: 46654, 	{'train/ctc_loss': Array(0.12380371, dtype=float32), 'train/wer': 0.047383894887472355, 'validation/ctc_loss': Array(0.38062832, dtype=float32), 'validation/wer': 0.11105747414966644, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21394913, dtype=float32), 'test/wer': 0.07186236873641663, 'test/num_examples': 2472, 'score': 36043.75965952873, 'total_duration': 39464.65749335289, 'accumulated_submission_time': 36043.75965952873, 'accumulated_eval_time': 3417.5548565387726, 'accumulated_logging_time': 1.3723094463348389}
I0221 13:19:01.483894 140483498055424 logging_writer.py:48] [46654] accumulated_eval_time=3417.554857, accumulated_logging_time=1.372309, accumulated_submission_time=36043.759660, global_step=46654, preemption_count=0, score=36043.759660, test/ctc_loss=0.21394912898540497, test/num_examples=2472, test/wer=0.071862, total_duration=39464.657493, train/ctc_loss=0.12380371242761612, train/wer=0.047384, validation/ctc_loss=0.3806283175945282, validation/num_examples=5348, validation/wer=0.111057
I0221 13:19:36.969450 140468535932672 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.644862711429596, loss=1.1594587564468384
I0221 13:20:52.597258 140483498055424 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.6133815050125122, loss=1.112334966659546
I0221 13:22:08.384252 140468535932672 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.8014736771583557, loss=1.1783024072647095
I0221 13:23:24.432643 140483498055424 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.5599421858787537, loss=1.1299470663070679
I0221 13:24:40.195885 140468535932672 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.48296645283699036, loss=1.1420365571975708
I0221 13:25:55.930980 140483498055424 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.51685631275177, loss=1.1073590517044067
I0221 13:27:13.521859 140468535932672 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.6228733658790588, loss=1.129032850265503
I0221 13:28:36.773036 140483498055424 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.5175763368606567, loss=1.1184875965118408
I0221 13:29:52.371335 140468535932672 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.5494002103805542, loss=1.1503548622131348
I0221 13:31:08.098515 140483498055424 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.576216459274292, loss=1.1081393957138062
I0221 13:32:23.800873 140468535932672 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.1339771747589111, loss=1.103784203529358
I0221 13:33:39.464483 140483498055424 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.6903528571128845, loss=1.104515790939331
I0221 13:34:55.157075 140468535932672 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.5599183440208435, loss=1.0920201539993286
I0221 13:36:10.911127 140483498055424 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.6277016401290894, loss=1.0713568925857544
I0221 13:37:26.692198 140468535932672 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.556345522403717, loss=1.0466104745864868
I0221 13:38:46.834112 140483498055424 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.6973732709884644, loss=1.0944236516952515
I0221 13:40:07.514658 140468535932672 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.7192967534065247, loss=1.0888081789016724
I0221 13:41:27.852286 140483498055424 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.8112027645111084, loss=1.1103664636611938
I0221 13:42:46.875580 140483498055424 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.592560350894928, loss=1.0885894298553467
I0221 13:43:01.708252 140549388556096 spec.py:321] Evaluating on the training split.
I0221 13:43:56.230225 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 13:44:48.485861 140549388556096 spec.py:349] Evaluating on the test split.
I0221 13:45:14.592907 140549388556096 submission_runner.py:408] Time since start: 41037.81s, 	Step: 48521, 	{'train/ctc_loss': Array(0.12260363, dtype=float32), 'train/wer': 0.046820854160731395, 'validation/ctc_loss': Array(0.3690256, dtype=float32), 'validation/wer': 0.10806453170105332, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20314053, dtype=float32), 'test/wer': 0.06800316860642253, 'test/num_examples': 2472, 'score': 37483.8937060833, 'total_duration': 41037.80516719818, 'accumulated_submission_time': 37483.8937060833, 'accumulated_eval_time': 3550.433384656906, 'accumulated_logging_time': 1.4295480251312256}
I0221 13:45:14.635185 140483498055424 logging_writer.py:48] [48521] accumulated_eval_time=3550.433385, accumulated_logging_time=1.429548, accumulated_submission_time=37483.893706, global_step=48521, preemption_count=0, score=37483.893706, test/ctc_loss=0.203140527009964, test/num_examples=2472, test/wer=0.068003, total_duration=41037.805167, train/ctc_loss=0.1226036325097084, train/wer=0.046821, validation/ctc_loss=0.3690255880355835, validation/num_examples=5348, validation/wer=0.108065
I0221 13:46:15.015927 140468535932672 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.6213345527648926, loss=1.0884389877319336
I0221 13:47:30.630572 140483498055424 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.532612681388855, loss=1.0578759908676147
I0221 13:48:46.458991 140468535932672 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.694486141204834, loss=1.0856692790985107
I0221 13:50:02.310733 140483498055424 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.7044691443443298, loss=1.1631466150283813
I0221 13:51:18.082513 140468535932672 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.4570585787296295, loss=1.084063172340393
I0221 13:52:33.936425 140483498055424 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.6105597019195557, loss=1.067870020866394
I0221 13:53:49.797094 140468535932672 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.6472574472427368, loss=1.0662118196487427
I0221 13:55:07.788728 140483498055424 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.625018298625946, loss=1.086547613143921
I0221 13:56:28.834625 140468535932672 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.6943758130073547, loss=1.109582781791687
I0221 13:57:49.943667 140483498055424 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.5770782828330994, loss=1.0937117338180542
I0221 13:59:05.600295 140468535932672 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.7459001541137695, loss=1.083223819732666
I0221 14:00:21.411906 140483498055424 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.7728666067123413, loss=1.1079412698745728
I0221 14:01:37.125513 140468535932672 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.5374990701675415, loss=1.0830196142196655
I0221 14:02:52.814396 140483498055424 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.7012811303138733, loss=1.0851569175720215
I0221 14:04:08.548838 140468535932672 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.5650355219841003, loss=1.0892575979232788
I0221 14:05:24.333287 140483498055424 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.5134614706039429, loss=1.0774213075637817
I0221 14:06:43.407804 140468535932672 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.6320421695709229, loss=1.0427978038787842
I0221 14:08:04.945472 140483498055424 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.6353225111961365, loss=1.1002204418182373
I0221 14:09:15.203225 140549388556096 spec.py:321] Evaluating on the training split.
I0221 14:10:10.828013 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 14:11:02.478830 140549388556096 spec.py:349] Evaluating on the test split.
I0221 14:11:28.191278 140549388556096 submission_runner.py:408] Time since start: 42611.40s, 	Step: 50389, 	{'train/ctc_loss': Array(0.1194732, dtype=float32), 'train/wer': 0.04600134181415354, 'validation/ctc_loss': Array(0.36581305, dtype=float32), 'validation/wer': 0.10608532782374465, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20159614, dtype=float32), 'test/wer': 0.06865313915463206, 'test/num_examples': 2472, 'score': 38924.372160196304, 'total_duration': 42611.40343165398, 'accumulated_submission_time': 38924.372160196304, 'accumulated_eval_time': 3683.415236711502, 'accumulated_logging_time': 1.4886324405670166}
I0221 14:11:28.235902 140483498055424 logging_writer.py:48] [50389] accumulated_eval_time=3683.415237, accumulated_logging_time=1.488632, accumulated_submission_time=38924.372160, global_step=50389, preemption_count=0, score=38924.372160, test/ctc_loss=0.20159614086151123, test/num_examples=2472, test/wer=0.068653, total_duration=42611.403432, train/ctc_loss=0.11947320401668549, train/wer=0.046001, validation/ctc_loss=0.3658130466938019, validation/num_examples=5348, validation/wer=0.106085
I0221 14:11:37.385812 140468535932672 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.5123744010925293, loss=1.0868017673492432
I0221 14:12:56.455348 140483498055424 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.5589552521705627, loss=1.0699036121368408
I0221 14:14:12.202221 140468535932672 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.5864943861961365, loss=1.0720603466033936
I0221 14:15:27.863548 140483498055424 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.6499959826469421, loss=1.0404090881347656
I0221 14:16:43.529892 140468535932672 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.6006277203559875, loss=1.064984917640686
I0221 14:17:59.288836 140483498055424 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.4920378029346466, loss=1.0253933668136597
I0221 14:19:14.972311 140468535932672 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.5941808223724365, loss=1.1230498552322388
I0221 14:20:30.655652 140483498055424 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.7070039510726929, loss=1.0622085332870483
I0221 14:21:50.529427 140468535932672 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.5690782070159912, loss=1.0718708038330078
I0221 14:23:11.071099 140483498055424 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.6181788444519043, loss=1.1065548658370972
I0221 14:24:31.702995 140468535932672 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.6382211446762085, loss=1.0958971977233887
I0221 14:25:56.791546 140483498055424 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.6490074396133423, loss=1.0491900444030762
I0221 14:27:12.459925 140468535932672 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.6365162134170532, loss=1.0663689374923706
I0221 14:28:28.041529 140483498055424 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.8865228891372681, loss=1.072461485862732
I0221 14:29:43.948660 140468535932672 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.5905972719192505, loss=1.0439976453781128
I0221 14:30:59.579864 140483498055424 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.5764024257659912, loss=1.0966583490371704
I0221 14:32:15.256466 140468535932672 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.6646370887756348, loss=1.0277916193008423
I0221 14:33:30.877749 140483498055424 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.5504856109619141, loss=1.1123499870300293
I0221 14:34:48.847622 140468535932672 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.715081512928009, loss=1.0887095928192139
I0221 14:35:28.359484 140549388556096 spec.py:321] Evaluating on the training split.
I0221 14:36:22.163094 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 14:37:13.249911 140549388556096 spec.py:349] Evaluating on the test split.
I0221 14:37:39.067749 140549388556096 submission_runner.py:408] Time since start: 44182.28s, 	Step: 52250, 	{'train/ctc_loss': Array(0.10834797, dtype=float32), 'train/wer': 0.042986734242475545, 'validation/ctc_loss': Array(0.35260636, dtype=float32), 'validation/wer': 0.10291860162005079, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19382338, dtype=float32), 'test/wer': 0.06505798955984807, 'test/num_examples': 2472, 'score': 40364.40458655357, 'total_duration': 44182.28010845184, 'accumulated_submission_time': 40364.40458655357, 'accumulated_eval_time': 3814.117434978485, 'accumulated_logging_time': 1.552154541015625}
I0221 14:37:39.109115 140483498055424 logging_writer.py:48] [52250] accumulated_eval_time=3814.117435, accumulated_logging_time=1.552155, accumulated_submission_time=40364.404587, global_step=52250, preemption_count=0, score=40364.404587, test/ctc_loss=0.19382338225841522, test/num_examples=2472, test/wer=0.065058, total_duration=44182.280108, train/ctc_loss=0.10834797471761703, train/wer=0.042987, validation/ctc_loss=0.3526063561439514, validation/num_examples=5348, validation/wer=0.102919
I0221 14:38:17.574350 140468535932672 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.627605676651001, loss=1.107875108718872
I0221 14:39:33.247507 140483498055424 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.5566719174385071, loss=1.0753282308578491
I0221 14:40:48.926415 140468535932672 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.6063839197158813, loss=1.0821248292922974
I0221 14:42:08.063085 140483498055424 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.6221710443496704, loss=1.0715866088867188
I0221 14:43:23.670422 140468535932672 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.6498050689697266, loss=1.1134631633758545
I0221 14:44:39.439304 140483498055424 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.5101057887077332, loss=1.0664341449737549
I0221 14:45:55.142125 140468535932672 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.5905537605285645, loss=1.0278565883636475
I0221 14:47:11.082744 140483498055424 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.6224243640899658, loss=1.0666589736938477
I0221 14:48:26.771405 140468535932672 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.5262755155563354, loss=1.0411969423294067
I0221 14:49:43.188189 140483498055424 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.6351612210273743, loss=1.0436145067214966
I0221 14:51:04.539448 140468535932672 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.5266121029853821, loss=1.0708242654800415
I0221 14:52:25.139253 140483498055424 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.6707774996757507, loss=1.0418578386306763
I0221 14:53:45.548566 140468535932672 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.6400573253631592, loss=1.0360697507858276
I0221 14:55:07.513855 140483498055424 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.6666719317436218, loss=1.0925759077072144
I0221 14:56:23.115777 140468535932672 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.5443060398101807, loss=1.0513179302215576
I0221 14:57:38.692778 140483498055424 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.639650821685791, loss=1.0197051763534546
I0221 14:58:54.376459 140468535932672 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.7149563431739807, loss=1.0385116338729858
I0221 15:00:09.984171 140483498055424 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.6819219589233398, loss=1.021026849746704
I0221 15:01:25.632914 140468535932672 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.6158601641654968, loss=1.0172243118286133
I0221 15:01:39.702944 140549388556096 spec.py:321] Evaluating on the training split.
I0221 15:02:35.215634 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 15:03:26.462313 140549388556096 spec.py:349] Evaluating on the test split.
I0221 15:03:52.843197 140549388556096 submission_runner.py:408] Time since start: 45756.06s, 	Step: 54120, 	{'train/ctc_loss': Array(0.09962546, dtype=float32), 'train/wer': 0.03944931886995858, 'validation/ctc_loss': Array(0.35237452, dtype=float32), 'validation/wer': 0.10118076406924317, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19269316, dtype=float32), 'test/wer': 0.06341275160969269, 'test/num_examples': 2472, 'score': 41804.90568423271, 'total_duration': 45756.05579662323, 'accumulated_submission_time': 41804.90568423271, 'accumulated_eval_time': 3947.251857280731, 'accumulated_logging_time': 1.6132729053497314}
I0221 15:03:52.884855 140483498055424 logging_writer.py:48] [54120] accumulated_eval_time=3947.251857, accumulated_logging_time=1.613273, accumulated_submission_time=41804.905684, global_step=54120, preemption_count=0, score=41804.905684, test/ctc_loss=0.19269315898418427, test/num_examples=2472, test/wer=0.063413, total_duration=45756.055797, train/ctc_loss=0.09962546080350876, train/wer=0.039449, validation/ctc_loss=0.35237452387809753, validation/num_examples=5348, validation/wer=0.101181
I0221 15:04:54.467390 140468535932672 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.5579037666320801, loss=1.0598180294036865
I0221 15:06:10.269338 140483498055424 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.6171258091926575, loss=1.0629570484161377
I0221 15:07:26.094441 140468535932672 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.7572636604309082, loss=1.0359621047973633
I0221 15:08:41.926839 140483498055424 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.5592873096466064, loss=1.0683910846710205
I0221 15:10:01.216208 140483498055424 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.6234205961227417, loss=1.0077879428863525
I0221 15:11:16.957273 140468535932672 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.7431278824806213, loss=1.04501211643219
I0221 15:12:32.694096 140483498055424 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.6738538146018982, loss=1.025984525680542
I0221 15:13:48.397814 140468535932672 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.6785217523574829, loss=1.0719335079193115
I0221 15:15:04.227180 140483498055424 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.6702144742012024, loss=1.0519359111785889
I0221 15:16:20.069320 140468535932672 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.8012406826019287, loss=1.0196499824523926
I0221 15:17:35.838597 140483498055424 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.5554879903793335, loss=1.0499732494354248
I0221 15:18:52.898472 140468535932672 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.831825315952301, loss=1.007812738418579
I0221 15:20:13.397530 140483498055424 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.566576361656189, loss=1.0577160120010376
I0221 15:21:34.004540 140468535932672 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.512140691280365, loss=1.0463881492614746
I0221 15:22:55.651132 140483498055424 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.6191338300704956, loss=1.0449308156967163
I0221 15:24:15.689456 140483498055424 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.6178149580955505, loss=1.0512361526489258
I0221 15:25:31.334095 140468535932672 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.6018057465553284, loss=1.054719090461731
I0221 15:26:47.035574 140483498055424 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.5460637807846069, loss=1.0069586038589478
I0221 15:27:53.231977 140549388556096 spec.py:321] Evaluating on the training split.
I0221 15:28:46.978572 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 15:29:37.928703 140549388556096 spec.py:349] Evaluating on the test split.
I0221 15:30:03.949332 140549388556096 submission_runner.py:408] Time since start: 47327.16s, 	Step: 55989, 	{'train/ctc_loss': Array(0.11142533, dtype=float32), 'train/wer': 0.042615227489869206, 'validation/ctc_loss': Array(0.34446225, dtype=float32), 'validation/wer': 0.09837126002877086, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18800361, dtype=float32), 'test/wer': 0.06121910100948551, 'test/num_examples': 2472, 'score': 43245.16593647003, 'total_duration': 47327.161296606064, 'accumulated_submission_time': 43245.16593647003, 'accumulated_eval_time': 4077.9627606868744, 'accumulated_logging_time': 1.6707770824432373}
I0221 15:30:03.995579 140483498055424 logging_writer.py:48] [55989] accumulated_eval_time=4077.962761, accumulated_logging_time=1.670777, accumulated_submission_time=43245.165936, global_step=55989, preemption_count=0, score=43245.165936, test/ctc_loss=0.18800361454486847, test/num_examples=2472, test/wer=0.061219, total_duration=47327.161297, train/ctc_loss=0.11142533272504807, train/wer=0.042615, validation/ctc_loss=0.34446224570274353, validation/num_examples=5348, validation/wer=0.098371
I0221 15:30:13.122838 140468535932672 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.6878713369369507, loss=1.0216213464736938
I0221 15:31:28.826976 140483498055424 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.6877330541610718, loss=1.0391960144042969
I0221 15:32:44.592628 140468535932672 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.6448178291320801, loss=0.9944779276847839
I0221 15:34:00.304667 140483498055424 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.6690158247947693, loss=1.0351978540420532
I0221 15:35:16.038665 140468535932672 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.5834468007087708, loss=1.0574809312820435
I0221 15:36:31.737264 140483498055424 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.6510125994682312, loss=1.071876883506775
I0221 15:37:49.034101 140468535932672 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.6067031025886536, loss=1.0559965372085571
I0221 15:39:10.599712 140483498055424 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.5962071418762207, loss=0.9524612426757812
I0221 15:40:26.256439 140468535932672 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.567507266998291, loss=0.9899644255638123
I0221 15:41:41.961082 140483498055424 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.6809144616127014, loss=1.040534257888794
I0221 15:42:57.624259 140468535932672 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.5909073948860168, loss=1.0548051595687866
I0221 15:44:13.321161 140483498055424 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.6526898741722107, loss=1.0395535230636597
I0221 15:45:29.106215 140468535932672 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.735457718372345, loss=1.0300899744033813
I0221 15:46:44.785394 140483498055424 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.5890092849731445, loss=1.0320242643356323
I0221 15:48:03.074975 140468535932672 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.7684322595596313, loss=1.0093778371810913
I0221 15:49:24.016897 140483498055424 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.6096556782722473, loss=1.052904725074768
I0221 15:50:45.526562 140468535932672 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.5917524695396423, loss=0.9878472685813904
I0221 15:52:09.254566 140483498055424 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.5528661012649536, loss=1.0665340423583984
I0221 15:53:25.300014 140468535932672 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.6187493801116943, loss=0.9905736446380615
I0221 15:54:04.314187 140549388556096 spec.py:321] Evaluating on the training split.
I0221 15:54:59.480128 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 15:55:50.760845 140549388556096 spec.py:349] Evaluating on the test split.
I0221 15:56:17.530924 140549388556096 submission_runner.py:408] Time since start: 48900.74s, 	Step: 57853, 	{'train/ctc_loss': Array(0.09508877, dtype=float32), 'train/wer': 0.034736090182440074, 'validation/ctc_loss': Array(0.33794376, dtype=float32), 'validation/wer': 0.09599621537600046, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1831914, dtype=float32), 'test/wer': 0.05945199358154084, 'test/num_examples': 2472, 'score': 44685.39541554451, 'total_duration': 48900.743406534195, 'accumulated_submission_time': 44685.39541554451, 'accumulated_eval_time': 4211.173542499542, 'accumulated_logging_time': 1.7332322597503662}
I0221 15:56:17.577000 140483498055424 logging_writer.py:48] [57853] accumulated_eval_time=4211.173542, accumulated_logging_time=1.733232, accumulated_submission_time=44685.395416, global_step=57853, preemption_count=0, score=44685.395416, test/ctc_loss=0.18319140374660492, test/num_examples=2472, test/wer=0.059452, total_duration=48900.743407, train/ctc_loss=0.09508877247571945, train/wer=0.034736, validation/ctc_loss=0.33794376254081726, validation/num_examples=5348, validation/wer=0.095996
I0221 15:56:53.882946 140468535932672 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.6578568816184998, loss=1.058409571647644
I0221 15:58:09.784346 140483498055424 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.5646480321884155, loss=1.0155190229415894
I0221 15:59:25.715985 140468535932672 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.7215142250061035, loss=1.0398870706558228
I0221 16:00:41.657478 140483498055424 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.6142632365226746, loss=1.0442256927490234
I0221 16:01:57.603727 140468535932672 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.5776798129081726, loss=0.9797965884208679
I0221 16:03:13.590043 140483498055424 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.6549628376960754, loss=1.0287610292434692
I0221 16:04:29.457986 140468535932672 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.5903928279876709, loss=1.0052456855773926
I0221 16:05:47.184766 140483498055424 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.6328448057174683, loss=1.0088903903961182
I0221 16:07:08.469923 140468535932672 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.99941486120224, loss=1.0100669860839844
I0221 16:08:27.653510 140483498055424 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.6916923522949219, loss=0.9928850531578064
I0221 16:09:43.338328 140468535932672 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.8095521926879883, loss=0.9791374206542969
I0221 16:10:59.284302 140483498055424 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.6987730264663696, loss=1.0226223468780518
I0221 16:12:14.959777 140468535932672 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.6212862730026245, loss=1.0078260898590088
I0221 16:13:30.707785 140483498055424 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.6990877985954285, loss=1.0058820247650146
I0221 16:14:46.453087 140468535932672 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.6299333572387695, loss=1.01882004737854
I0221 16:16:03.201219 140483498055424 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.5417740345001221, loss=1.0233018398284912
I0221 16:17:23.358689 140468535932672 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.6518220901489258, loss=0.9740146994590759
I0221 16:18:44.212282 140483498055424 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.6347934007644653, loss=0.9988064169883728
I0221 16:20:05.496333 140468535932672 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.6514312028884888, loss=0.9920787215232849
I0221 16:20:17.967619 140549388556096 spec.py:321] Evaluating on the training split.
I0221 16:21:12.243736 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 16:22:03.467248 140549388556096 spec.py:349] Evaluating on the test split.
I0221 16:22:29.419190 140549388556096 submission_runner.py:408] Time since start: 50472.63s, 	Step: 59717, 	{'train/ctc_loss': Array(0.09259246, dtype=float32), 'train/wer': 0.03545706371191136, 'validation/ctc_loss': Array(0.33490953, dtype=float32), 'validation/wer': 0.09581277696785966, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1784495, dtype=float32), 'test/wer': 0.058273921962911056, 'test/num_examples': 2472, 'score': 46125.69869709015, 'total_duration': 50472.63144659996, 'accumulated_submission_time': 46125.69869709015, 'accumulated_eval_time': 4342.618939638138, 'accumulated_logging_time': 1.7944409847259521}
I0221 16:22:29.462127 140483498055424 logging_writer.py:48] [59717] accumulated_eval_time=4342.618940, accumulated_logging_time=1.794441, accumulated_submission_time=46125.698697, global_step=59717, preemption_count=0, score=46125.698697, test/ctc_loss=0.17844949662685394, test/num_examples=2472, test/wer=0.058274, total_duration=50472.631447, train/ctc_loss=0.09259245544672012, train/wer=0.035457, validation/ctc_loss=0.3349095284938812, validation/num_examples=5348, validation/wer=0.095813
I0221 16:23:36.458085 140483498055424 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.5332069993019104, loss=0.9438719153404236
I0221 16:24:52.133642 140468535932672 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.7051899433135986, loss=0.9957852363586426
I0221 16:26:07.852516 140483498055424 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.6555083990097046, loss=0.9976030588150024
I0221 16:27:23.618572 140468535932672 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.5795789361000061, loss=0.9517747163772583
I0221 16:28:39.606829 140483498055424 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.6619260907173157, loss=0.9988436698913574
I0221 16:29:55.364729 140468535932672 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.6233028769493103, loss=0.9894267320632935
I0221 16:31:11.110177 140483498055424 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.658957839012146, loss=0.9881237745285034
I0221 16:32:31.361038 140468535932672 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.7797063589096069, loss=0.9540937542915344
I0221 16:33:52.580678 140483498055424 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.7444285154342651, loss=0.9853119254112244
I0221 16:35:13.807010 140468535932672 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.8773488402366638, loss=0.9994049072265625
I0221 16:36:36.145054 140483498055424 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.6601670384407043, loss=0.9847151041030884
I0221 16:37:51.710556 140468535932672 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.725868284702301, loss=1.0028626918792725
I0221 16:39:07.366390 140483498055424 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.7076709866523743, loss=0.9883865714073181
I0221 16:40:22.991444 140468535932672 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.6555341482162476, loss=1.0262203216552734
I0221 16:41:38.743931 140483498055424 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.7007997632026672, loss=1.0153605937957764
I0221 16:42:54.487446 140468535932672 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.7601478099822998, loss=0.9812604784965515
I0221 16:44:10.231171 140483498055424 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.6766452789306641, loss=0.9487751126289368
I0221 16:45:28.707725 140468535932672 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.8836783766746521, loss=1.0032212734222412
I0221 16:46:30.221323 140549388556096 spec.py:321] Evaluating on the training split.
I0221 16:47:25.248749 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 16:48:16.563562 140549388556096 spec.py:349] Evaluating on the test split.
I0221 16:48:42.706598 140549388556096 submission_runner.py:408] Time since start: 52045.92s, 	Step: 61577, 	{'train/ctc_loss': Array(0.07366502, dtype=float32), 'train/wer': 0.028728088745871794, 'validation/ctc_loss': Array(0.3230342, dtype=float32), 'validation/wer': 0.09263639611110575, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17452674, dtype=float32), 'test/wer': 0.05693335770722889, 'test/num_examples': 2472, 'score': 47566.37073302269, 'total_duration': 52045.9193854332, 'accumulated_submission_time': 47566.37073302269, 'accumulated_eval_time': 4475.0985696315765, 'accumulated_logging_time': 1.8526763916015625}
I0221 16:48:42.751724 140483498055424 logging_writer.py:48] [61577] accumulated_eval_time=4475.098570, accumulated_logging_time=1.852676, accumulated_submission_time=47566.370733, global_step=61577, preemption_count=0, score=47566.370733, test/ctc_loss=0.17452673614025116, test/num_examples=2472, test/wer=0.056933, total_duration=52045.919385, train/ctc_loss=0.07366501539945602, train/wer=0.028728, validation/ctc_loss=0.3230341970920563, validation/num_examples=5348, validation/wer=0.092636
I0221 16:49:00.897490 140468535932672 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.6524075269699097, loss=0.95802903175354
I0221 16:50:16.514639 140483498055424 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.7241232991218567, loss=0.9539620280265808
I0221 16:51:35.680693 140483498055424 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.6411488056182861, loss=0.9847798347473145
I0221 16:52:51.317201 140468535932672 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.7927573919296265, loss=0.9770150780677795
I0221 16:54:07.081613 140483498055424 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.6552977561950684, loss=0.972052276134491
I0221 16:55:22.806852 140468535932672 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.6618425846099854, loss=0.9522478580474854
I0221 16:56:38.528157 140483498055424 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.8176745772361755, loss=0.9428115487098694
I0221 16:57:54.164374 140468535932672 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.6031790971755981, loss=0.9270499348640442
I0221 16:59:09.887363 140483498055424 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.6142711639404297, loss=0.9671263098716736
I0221 17:00:27.749577 140468535932672 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.7193418741226196, loss=0.9326663613319397
I0221 17:01:48.535726 140483498055424 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.5803517699241638, loss=0.9668750762939453
I0221 17:03:09.104859 140468535932672 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.5919738411903381, loss=0.9650827050209045
I0221 17:04:30.034004 140483498055424 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.8540582060813904, loss=0.9915947318077087
I0221 17:05:50.409432 140483498055424 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.0219202041625977, loss=0.9590200185775757
I0221 17:07:06.270242 140468535932672 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.7593345642089844, loss=0.9401199817657471
I0221 17:08:22.063305 140483498055424 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.6300727128982544, loss=0.9818819165229797
I0221 17:09:37.767534 140468535932672 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.6447725892066956, loss=0.9671810865402222
I0221 17:10:53.520041 140483498055424 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.890273928642273, loss=0.9281474947929382
I0221 17:12:09.304943 140468535932672 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.6097239851951599, loss=0.9834949970245361
I0221 17:12:43.080627 140549388556096 spec.py:321] Evaluating on the training split.
I0221 17:13:38.368844 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 17:14:30.142849 140549388556096 spec.py:349] Evaluating on the test split.
I0221 17:14:56.350461 140549388556096 submission_runner.py:408] Time since start: 53619.56s, 	Step: 63446, 	{'train/ctc_loss': Array(0.06885544, dtype=float32), 'train/wer': 0.027767634591264863, 'validation/ctc_loss': Array(0.32002056, dtype=float32), 'validation/wer': 0.08985585602981357, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17051816, dtype=float32), 'test/wer': 0.055592793451546725, 'test/num_examples': 2472, 'score': 49006.61024451256, 'total_duration': 53619.56326055527, 'accumulated_submission_time': 49006.61024451256, 'accumulated_eval_time': 4608.362781047821, 'accumulated_logging_time': 1.9141466617584229}
I0221 17:14:56.393627 140483498055424 logging_writer.py:48] [63446] accumulated_eval_time=4608.362781, accumulated_logging_time=1.914147, accumulated_submission_time=49006.610245, global_step=63446, preemption_count=0, score=49006.610245, test/ctc_loss=0.170518159866333, test/num_examples=2472, test/wer=0.055593, total_duration=53619.563261, train/ctc_loss=0.06885544210672379, train/wer=0.027768, validation/ctc_loss=0.32002055644989014, validation/num_examples=5348, validation/wer=0.089856
I0221 17:15:37.963469 140468535932672 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.7093197107315063, loss=1.0033453702926636
I0221 17:16:53.655219 140483498055424 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.7125051021575928, loss=0.9391547441482544
I0221 17:18:09.408677 140468535932672 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.647423267364502, loss=0.9529790282249451
I0221 17:19:25.140307 140483498055424 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.7324679493904114, loss=0.9539290070533752
I0221 17:20:44.581176 140483498055424 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.6844967007637024, loss=0.975536048412323
I0221 17:22:00.184875 140468535932672 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.6754540205001831, loss=0.9171845316886902
I0221 17:23:15.753354 140483498055424 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.6469340324401855, loss=0.9690155386924744
I0221 17:24:31.350520 140468535932672 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.7336786389350891, loss=0.9278139472007751
I0221 17:25:47.003653 140483498055424 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.799957811832428, loss=0.9523913860321045
I0221 17:27:02.662817 140468535932672 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.6169847846031189, loss=0.9903476238250732
I0221 17:28:18.270140 140483498055424 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.6750239133834839, loss=0.9685066938400269
I0221 17:29:35.733143 140468535932672 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.0729165077209473, loss=0.9303798079490662
I0221 17:30:56.223648 140483498055424 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.6732262969017029, loss=0.9664167165756226
I0221 17:32:17.343304 140468535932672 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.6924564838409424, loss=0.9775639176368713
I0221 17:33:41.242182 140483498055424 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.7255964279174805, loss=0.9028658866882324
I0221 17:34:56.879705 140468535932672 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.7315341830253601, loss=0.9105415940284729
I0221 17:36:13.026829 140483498055424 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.7692285776138306, loss=0.9075777530670166
I0221 17:37:28.793932 140468535932672 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.593681275844574, loss=0.9050158858299255
I0221 17:38:44.553273 140483498055424 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.7532297372817993, loss=0.9427633881568909
I0221 17:38:56.370873 140549388556096 spec.py:321] Evaluating on the training split.
I0221 17:39:50.057071 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 17:40:41.219991 140549388556096 spec.py:349] Evaluating on the test split.
I0221 17:41:07.691046 140549388556096 submission_runner.py:408] Time since start: 55190.90s, 	Step: 65317, 	{'train/ctc_loss': Array(0.07590765, dtype=float32), 'train/wer': 0.02990616824854604, 'validation/ctc_loss': Array(0.3158513, dtype=float32), 'validation/wer': 0.08801181729534549, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16880462, dtype=float32), 'test/wer': 0.05437409867365385, 'test/num_examples': 2472, 'score': 50446.498838186264, 'total_duration': 55190.903685331345, 'accumulated_submission_time': 50446.498838186264, 'accumulated_eval_time': 4739.67716550827, 'accumulated_logging_time': 1.9725675582885742}
I0221 17:41:07.733045 140483498055424 logging_writer.py:48] [65317] accumulated_eval_time=4739.677166, accumulated_logging_time=1.972568, accumulated_submission_time=50446.498838, global_step=65317, preemption_count=0, score=50446.498838, test/ctc_loss=0.1688046157360077, test/num_examples=2472, test/wer=0.054374, total_duration=55190.903685, train/ctc_loss=0.0759076476097107, train/wer=0.029906, validation/ctc_loss=0.3158513009548187, validation/num_examples=5348, validation/wer=0.088012
I0221 17:42:11.195262 140468535932672 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.8636541962623596, loss=0.9661704897880554
I0221 17:43:26.897712 140483498055424 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.7828072905540466, loss=0.9314572811126709
I0221 17:44:42.607377 140468535932672 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.7265272736549377, loss=0.9565781950950623
I0221 17:45:58.307302 140483498055424 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.6467596292495728, loss=0.9482998847961426
I0221 17:47:14.090499 140468535932672 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.6237521171569824, loss=0.9572749137878418
I0221 17:48:31.693751 140483498055424 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.7575203776359558, loss=0.9479552507400513
I0221 17:49:51.613832 140483498055424 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.0556219816207886, loss=0.8999781608581543
I0221 17:51:07.584387 140468535932672 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.6437994241714478, loss=0.9380802512168884
I0221 17:52:23.545516 140483498055424 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.7657840251922607, loss=1.021414875984192
I0221 17:53:39.683853 140468535932672 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.7603103518486023, loss=0.954880952835083
I0221 17:54:55.651922 140483498055424 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.7029316425323486, loss=0.9486249089241028
I0221 17:56:11.600556 140468535932672 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.705900251865387, loss=0.9712361693382263
I0221 17:57:27.602860 140483498055424 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.6511971950531006, loss=0.9291345477104187
I0221 17:58:46.173002 140468535932672 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.6677036881446838, loss=0.9411851167678833
I0221 18:00:06.344113 140483498055424 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.6354736089706421, loss=0.889812707901001
I0221 18:01:27.302150 140468535932672 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.5970457792282104, loss=0.9402786493301392
I0221 18:02:48.479091 140483498055424 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.6592761874198914, loss=0.9155232310295105
I0221 18:04:04.303765 140468535932672 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.6438634395599365, loss=0.9625644087791443
I0221 18:05:08.272700 140549388556096 spec.py:321] Evaluating on the training split.
I0221 18:06:03.832749 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 18:06:55.331082 140549388556096 spec.py:349] Evaluating on the test split.
I0221 18:07:21.654706 140549388556096 submission_runner.py:408] Time since start: 56764.87s, 	Step: 67186, 	{'train/ctc_loss': Array(0.06836843, dtype=float32), 'train/wer': 0.026875016304489605, 'validation/ctc_loss': Array(0.3090984, dtype=float32), 'validation/wer': 0.08585882966295606, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1675134, dtype=float32), 'test/wer': 0.05362257022728657, 'test/num_examples': 2472, 'score': 51886.949083805084, 'total_duration': 56764.86767911911, 'accumulated_submission_time': 51886.949083805084, 'accumulated_eval_time': 4873.053730249405, 'accumulated_logging_time': 2.0313730239868164}
I0221 18:07:21.697582 140483498055424 logging_writer.py:48] [67186] accumulated_eval_time=4873.053730, accumulated_logging_time=2.031373, accumulated_submission_time=51886.949084, global_step=67186, preemption_count=0, score=51886.949084, test/ctc_loss=0.1675134003162384, test/num_examples=2472, test/wer=0.053623, total_duration=56764.867679, train/ctc_loss=0.06836842745542526, train/wer=0.026875, validation/ctc_loss=0.30909839272499084, validation/num_examples=5348, validation/wer=0.085859
I0221 18:07:33.101469 140468535932672 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.6237574815750122, loss=0.9707393646240234
I0221 18:08:48.548370 140483498055424 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.7862352728843689, loss=0.9087852835655212
I0221 18:10:04.347478 140468535932672 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.8275009393692017, loss=0.9034658670425415
I0221 18:11:20.355497 140483498055424 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.6437214612960815, loss=0.9266331195831299
I0221 18:12:36.073574 140468535932672 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.8550557494163513, loss=0.9287031292915344
I0221 18:13:51.869007 140483498055424 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.744755744934082, loss=0.9114091396331787
I0221 18:15:07.590522 140468535932672 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.806244969367981, loss=0.9254695177078247
I0221 18:16:26.895741 140483498055424 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.695502758026123, loss=0.9076851010322571
I0221 18:17:49.661531 140483498055424 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.6005597710609436, loss=0.9108309745788574
I0221 18:19:05.372985 140468535932672 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.7741013169288635, loss=0.951810896396637
I0221 18:20:21.120966 140483498055424 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.6457740664482117, loss=0.928706705570221
I0221 18:21:36.817596 140468535932672 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.6982100009918213, loss=0.9325713515281677
I0221 18:22:52.443841 140483498055424 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.6977413892745972, loss=0.9151992797851562
I0221 18:24:08.114211 140468535932672 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.7111462950706482, loss=0.9423993229866028
I0221 18:25:23.706335 140483498055424 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.7974398136138916, loss=0.9921846985816956
I0221 18:26:42.854794 140468535932672 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.6135266423225403, loss=0.9540173411369324
I0221 18:28:04.788068 140483498055424 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.6912590861320496, loss=0.9108249545097351
I0221 18:29:25.430860 140468535932672 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.7440202236175537, loss=0.9612937569618225
I0221 18:30:46.028164 140483498055424 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.7086694240570068, loss=0.9159448146820068
I0221 18:31:21.741253 140549388556096 spec.py:321] Evaluating on the training split.
I0221 18:32:17.209804 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 18:33:08.744628 140549388556096 spec.py:349] Evaluating on the test split.
I0221 18:33:35.018627 140549388556096 submission_runner.py:408] Time since start: 58338.23s, 	Step: 69044, 	{'train/ctc_loss': Array(0.06659844, dtype=float32), 'train/wer': 0.025888907946506997, 'validation/ctc_loss': Array(0.3053904, dtype=float32), 'validation/wer': 0.08537609700995395, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16303761, dtype=float32), 'test/wer': 0.051672658582657974, 'test/num_examples': 2472, 'score': 53326.903443574905, 'total_duration': 58338.23026275635, 'accumulated_submission_time': 53326.903443574905, 'accumulated_eval_time': 5006.324478149414, 'accumulated_logging_time': 2.09139347076416}
I0221 18:33:35.064271 140483498055424 logging_writer.py:48] [69044] accumulated_eval_time=5006.324478, accumulated_logging_time=2.091393, accumulated_submission_time=53326.903444, global_step=69044, preemption_count=0, score=53326.903444, test/ctc_loss=0.16303761303424835, test/num_examples=2472, test/wer=0.051673, total_duration=58338.230263, train/ctc_loss=0.06659843772649765, train/wer=0.025889, validation/ctc_loss=0.3053903877735138, validation/num_examples=5348, validation/wer=0.085376
I0221 18:34:18.121221 140468535932672 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.6484441757202148, loss=0.9601342678070068
I0221 18:35:33.774532 140483498055424 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.7009627223014832, loss=0.8849926590919495
I0221 18:36:49.531653 140468535932672 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.6699123382568359, loss=0.913657546043396
I0221 18:38:05.242346 140483498055424 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.9681889414787292, loss=0.9031338691711426
I0221 18:39:20.862975 140468535932672 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.6504611968994141, loss=0.9364235401153564
I0221 18:40:36.585877 140483498055424 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.0288821458816528, loss=0.9329096078872681
I0221 18:41:52.359308 140468535932672 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.6441433429718018, loss=0.9301535487174988
I0221 18:43:08.053232 140483498055424 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.6024354696273804, loss=0.8886489272117615
I0221 18:44:25.433088 140468535932672 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.6756951212882996, loss=0.9086164832115173
I0221 18:45:46.745146 140483498055424 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.6666561365127563, loss=0.8898869156837463
I0221 18:47:07.936228 140483498055424 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.9753812551498413, loss=0.9090378880500793
I0221 18:48:23.565819 140468535932672 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.842321515083313, loss=0.9128448367118835
I0221 18:49:39.270275 140483498055424 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.8418070077896118, loss=0.9167783260345459
I0221 18:50:55.040478 140468535932672 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.7028295397758484, loss=0.85719233751297
I0221 18:52:10.766654 140483498055424 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.634469747543335, loss=0.9114383459091187
I0221 18:53:26.483125 140468535932672 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.7091212868690491, loss=0.8730188608169556
I0221 18:54:42.255573 140483498055424 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.6612849235534668, loss=0.9271357655525208
I0221 18:56:01.895498 140468535932672 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.5657429695129395, loss=0.9264394044876099
I0221 18:57:23.669495 140483498055424 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.7068076729774475, loss=0.9155502915382385
I0221 18:57:35.331181 140549388556096 spec.py:321] Evaluating on the training split.
I0221 18:58:30.583305 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 18:59:22.244447 140549388556096 spec.py:349] Evaluating on the test split.
I0221 18:59:48.600699 140549388556096 submission_runner.py:408] Time since start: 59911.81s, 	Step: 70916, 	{'train/ctc_loss': Array(0.05856837, dtype=float32), 'train/wer': 0.022984151038706827, 'validation/ctc_loss': Array(0.30260167, dtype=float32), 'validation/wer': 0.08377342460198693, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16102883, dtype=float32), 'test/wer': 0.051632035423394874, 'test/num_examples': 2472, 'score': 54767.0825843811, 'total_duration': 59911.81316590309, 'accumulated_submission_time': 54767.0825843811, 'accumulated_eval_time': 5139.588047981262, 'accumulated_logging_time': 2.1525721549987793}
I0221 18:59:48.646292 140483498055424 logging_writer.py:48] [70916] accumulated_eval_time=5139.588048, accumulated_logging_time=2.152572, accumulated_submission_time=54767.082584, global_step=70916, preemption_count=0, score=54767.082584, test/ctc_loss=0.16102883219718933, test/num_examples=2472, test/wer=0.051632, total_duration=59911.813166, train/ctc_loss=0.05856836959719658, train/wer=0.022984, validation/ctc_loss=0.3026016652584076, validation/num_examples=5348, validation/wer=0.083773
I0221 19:00:52.738823 140468535932672 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.7747386693954468, loss=0.8876762986183167
I0221 19:02:12.127803 140483498055424 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.6216214895248413, loss=0.8616405725479126
I0221 19:03:27.877367 140468535932672 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.8301044702529907, loss=0.9120509624481201
I0221 19:04:43.690691 140483498055424 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.6277434229850769, loss=0.9110472202301025
I0221 19:05:59.434419 140468535932672 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.703951895236969, loss=0.8957461714744568
I0221 19:07:15.180084 140483498055424 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.910984992980957, loss=0.9858902096748352
I0221 19:08:30.993811 140468535932672 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.8978459239006042, loss=0.8398849368095398
I0221 19:09:46.836183 140483498055424 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.9941643476486206, loss=0.8730343580245972
I0221 19:11:04.549435 140468535932672 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.7065882682800293, loss=0.9160188436508179
I0221 19:12:25.774967 140483498055424 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.6400946378707886, loss=0.8761782646179199
I0221 19:13:47.357408 140468535932672 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.6471527814865112, loss=0.8694501519203186
I0221 19:15:10.857645 140483498055424 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.6787728667259216, loss=0.9061523675918579
I0221 19:16:26.398606 140468535932672 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.9531584978103638, loss=0.8532559275627136
I0221 19:17:42.470776 140483498055424 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.7197670936584473, loss=0.901336133480072
I0221 19:18:58.101386 140468535932672 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.6768609881401062, loss=0.9139419794082642
I0221 19:20:13.797696 140483498055424 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.6169312000274658, loss=0.921949028968811
I0221 19:21:29.394145 140468535932672 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.6761118769645691, loss=0.8712295293807983
I0221 19:22:45.107907 140483498055424 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.6261894702911377, loss=0.9551277756690979
I0221 19:23:48.997045 140549388556096 spec.py:321] Evaluating on the training split.
I0221 19:24:44.635921 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 19:25:36.445218 140549388556096 spec.py:349] Evaluating on the test split.
I0221 19:26:02.538743 140549388556096 submission_runner.py:408] Time since start: 61485.75s, 	Step: 72786, 	{'train/ctc_loss': Array(0.05365826, dtype=float32), 'train/wer': 0.020761263701913057, 'validation/ctc_loss': Array(0.30205798, dtype=float32), 'validation/wer': 0.08348378501018566, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15916285, dtype=float32), 'test/wer': 0.050961753295553795, 'test/num_examples': 2472, 'score': 56207.34043216705, 'total_duration': 61485.750854730606, 'accumulated_submission_time': 56207.34043216705, 'accumulated_eval_time': 5273.123430967331, 'accumulated_logging_time': 2.2175686359405518}
I0221 19:26:02.584606 140483498055424 logging_writer.py:48] [72786] accumulated_eval_time=5273.123431, accumulated_logging_time=2.217569, accumulated_submission_time=56207.340432, global_step=72786, preemption_count=0, score=56207.340432, test/ctc_loss=0.15916284918785095, test/num_examples=2472, test/wer=0.050962, total_duration=61485.750855, train/ctc_loss=0.05365825816988945, train/wer=0.020761, validation/ctc_loss=0.30205798149108887, validation/num_examples=5348, validation/wer=0.083484
I0221 19:26:13.979517 140468535932672 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.6250141263008118, loss=0.8909404277801514
I0221 19:27:29.471240 140483498055424 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.7450737953186035, loss=0.9127064347267151
I0221 19:28:45.235556 140468535932672 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.7148386240005493, loss=0.894561767578125
I0221 19:30:01.006456 140483498055424 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.8412722945213318, loss=0.8937394618988037
I0221 19:31:20.218501 140483498055424 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.6405762434005737, loss=0.8674504160881042
I0221 19:32:35.968827 140468535932672 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.6324412822723389, loss=0.9014310836791992
I0221 19:33:51.613210 140483498055424 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.6602892875671387, loss=0.9295076727867126
I0221 19:35:07.710123 140468535932672 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.2487643957138062, loss=0.9167656302452087
I0221 19:36:23.430859 140483498055424 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.7504436373710632, loss=0.9240439534187317
I0221 19:37:39.143110 140468535932672 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.0468146800994873, loss=0.8581146001815796
I0221 19:38:57.049179 140483498055424 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.6743464469909668, loss=0.8912096619606018
I0221 19:40:17.501741 140468535932672 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.6967800855636597, loss=0.9114536643028259
I0221 19:41:37.708671 140483498055424 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.6891288161277771, loss=0.8795169591903687
I0221 19:42:58.821425 140468535932672 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.6602521538734436, loss=0.8724779486656189
I0221 19:44:20.063771 140483498055424 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.6152943968772888, loss=0.8776389360427856
I0221 19:45:35.623781 140468535932672 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.8946400284767151, loss=0.8938480615615845
I0221 19:46:51.269221 140483498055424 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.7265805602073669, loss=0.9046504497528076
I0221 19:48:06.989166 140468535932672 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.0261834859848022, loss=0.8731308579444885
I0221 19:49:22.704530 140483498055424 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.6236156821250916, loss=0.8637086749076843
I0221 19:50:02.743288 140549388556096 spec.py:321] Evaluating on the training split.
I0221 19:50:58.350770 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 19:51:49.166402 140549388556096 spec.py:349] Evaluating on the test split.
I0221 19:52:15.885517 140549388556096 submission_runner.py:408] Time since start: 63059.10s, 	Step: 74654, 	{'train/ctc_loss': Array(0.05984657, dtype=float32), 'train/wer': 0.022029474976972673, 'validation/ctc_loss': Array(0.2980946, dtype=float32), 'validation/wer': 0.08164940092877762, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15752992, dtype=float32), 'test/wer': 0.050088355371397233, 'test/num_examples': 2472, 'score': 57647.41107773781, 'total_duration': 63059.09764456749, 'accumulated_submission_time': 57647.41107773781, 'accumulated_eval_time': 5406.259341955185, 'accumulated_logging_time': 2.2783915996551514}
I0221 19:52:15.929464 140483498055424 logging_writer.py:48] [74654] accumulated_eval_time=5406.259342, accumulated_logging_time=2.278392, accumulated_submission_time=57647.411078, global_step=74654, preemption_count=0, score=57647.411078, test/ctc_loss=0.15752992033958435, test/num_examples=2472, test/wer=0.050088, total_duration=63059.097645, train/ctc_loss=0.05984656885266304, train/wer=0.022029, validation/ctc_loss=0.29809460043907166, validation/num_examples=5348, validation/wer=0.081649
I0221 19:52:51.460051 140468535932672 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.7904557585716248, loss=0.8541860580444336
I0221 19:54:07.112327 140483498055424 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.7841247916221619, loss=0.9362332224845886
I0221 19:55:22.836172 140468535932672 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.8528273701667786, loss=0.8845653533935547
I0221 19:56:38.553598 140483498055424 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.6692758798599243, loss=0.8686537742614746
I0221 19:57:54.265135 140468535932672 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.9400764107704163, loss=0.8962640762329102
I0221 19:59:13.427933 140483498055424 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.7263907194137573, loss=0.8952550292015076
I0221 20:00:29.051196 140468535932672 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.9934805631637573, loss=0.9002085328102112
I0221 20:01:44.638094 140483498055424 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.6675475835800171, loss=0.8889554738998413
I0221 20:03:00.342266 140468535932672 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.828381359577179, loss=0.8868687748908997
I0221 20:04:16.017507 140483498055424 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.7516523599624634, loss=0.8509973287582397
I0221 20:05:31.737265 140468535932672 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.6982449293136597, loss=0.8740158677101135
I0221 20:06:47.382357 140483498055424 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.6284477114677429, loss=0.8799943327903748
I0221 20:08:03.575776 140468535932672 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.7099463939666748, loss=0.9080397486686707
I0221 20:09:22.442634 140483498055424 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.6910709142684937, loss=0.9337210655212402
I0221 20:10:43.096673 140468535932672 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.0340797901153564, loss=0.9577335119247437
I0221 20:12:04.411447 140483498055424 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.5821629762649536, loss=0.9006851315498352
I0221 20:13:24.443750 140483498055424 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.9424891471862793, loss=0.8544186949729919
I0221 20:14:40.228692 140468535932672 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.812833845615387, loss=0.9005528688430786
I0221 20:15:56.032203 140483498055424 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.8799823522567749, loss=0.9106536507606506
I0221 20:16:16.266578 140549388556096 spec.py:321] Evaluating on the training split.
I0221 20:17:10.368877 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 20:18:02.392120 140549388556096 spec.py:349] Evaluating on the test split.
I0221 20:18:28.749740 140549388556096 submission_runner.py:408] Time since start: 64631.96s, 	Step: 76528, 	{'train/ctc_loss': Array(0.05743812, dtype=float32), 'train/wer': 0.021416039085486005, 'validation/ctc_loss': Array(0.29819798, dtype=float32), 'validation/wer': 0.08163009162265754, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15713376, dtype=float32), 'test/wer': 0.050291471167712716, 'test/num_examples': 2472, 'score': 59087.65986609459, 'total_duration': 64631.961369514465, 'accumulated_submission_time': 59087.65986609459, 'accumulated_eval_time': 5538.735710859299, 'accumulated_logging_time': 2.3376240730285645}
I0221 20:18:28.798346 140483498055424 logging_writer.py:48] [76528] accumulated_eval_time=5538.735711, accumulated_logging_time=2.337624, accumulated_submission_time=59087.659866, global_step=76528, preemption_count=0, score=59087.659866, test/ctc_loss=0.15713375806808472, test/num_examples=2472, test/wer=0.050291, total_duration=64631.961370, train/ctc_loss=0.057438116520643234, train/wer=0.021416, validation/ctc_loss=0.29819798469543457, validation/num_examples=5348, validation/wer=0.081630
I0221 20:19:23.953400 140468535932672 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.8292527198791504, loss=0.9469555020332336
I0221 20:20:39.664828 140483498055424 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.7855498790740967, loss=0.9027338624000549
I0221 20:21:55.508292 140468535932672 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.6474612355232239, loss=0.8934928178787231
I0221 20:23:11.359284 140483498055424 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.766566276550293, loss=0.8874049186706543
I0221 20:24:27.236470 140468535932672 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.6580818891525269, loss=0.9027934670448303
I0221 20:25:44.465217 140483498055424 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.8061048984527588, loss=0.884509801864624
I0221 20:27:05.543386 140468535932672 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.8802247047424316, loss=0.9046993255615234
I0221 20:28:27.082884 140483498055424 logging_writer.py:48] [77300] global_step=77300, grad_norm=1.206612467765808, loss=0.8740092515945435
I0221 20:29:42.662046 140468535932672 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.7267102003097534, loss=0.8675620555877686
I0221 20:30:58.285575 140483498055424 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.8721393346786499, loss=0.8837687373161316
I0221 20:32:13.886927 140468535932672 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.6326289176940918, loss=0.8948565125465393
I0221 20:33:29.683286 140483498055424 logging_writer.py:48] [77700] global_step=77700, grad_norm=1.1692101955413818, loss=0.8619109392166138
I0221 20:34:45.415169 140468535932672 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.7853448987007141, loss=0.8616090416908264
I0221 20:36:01.502897 140483498055424 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.9019681811332703, loss=0.8623641133308411
I0221 20:37:23.648166 140468535932672 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.7841730117797852, loss=0.9023983478546143
I0221 20:38:45.066071 140483498055424 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.6956260204315186, loss=0.8775079250335693
I0221 20:40:06.300319 140468535932672 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.9681569933891296, loss=0.8970269560813904
I0221 20:41:30.493704 140483498055424 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.660457193851471, loss=0.869437575340271
I0221 20:42:29.064549 140549388556096 spec.py:321] Evaluating on the training split.
I0221 20:43:23.075190 140549388556096 spec.py:333] Evaluating on the validation split.
I0221 20:44:15.023530 140549388556096 spec.py:349] Evaluating on the test split.
I0221 20:44:41.297420 140549388556096 submission_runner.py:408] Time since start: 66204.51s, 	Step: 78379, 	{'train/ctc_loss': Array(0.05857571, dtype=float32), 'train/wer': 0.022106165300765975, 'validation/ctc_loss': Array(0.29666498, dtype=float32), 'validation/wer': 0.08126321480637594, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15672202, dtype=float32), 'test/wer': 0.049986797473239496, 'test/num_examples': 2472, 'score': 60527.83698248863, 'total_duration': 66204.51050138474, 'accumulated_submission_time': 60527.83698248863, 'accumulated_eval_time': 5670.963258266449, 'accumulated_logging_time': 2.402360677719116}
I0221 20:44:41.346616 140483498055424 logging_writer.py:48] [78379] accumulated_eval_time=5670.963258, accumulated_logging_time=2.402361, accumulated_submission_time=60527.836982, global_step=78379, preemption_count=0, score=60527.836982, test/ctc_loss=0.1567220240831375, test/num_examples=2472, test/wer=0.049987, total_duration=66204.510501, train/ctc_loss=0.05857571214437485, train/wer=0.022106, validation/ctc_loss=0.2966649830341339, validation/num_examples=5348, validation/wer=0.081263
I0221 20:44:58.001679 140468535932672 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.7979919910430908, loss=0.8739925622940063
I0221 20:46:13.583223 140483498055424 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.676174521446228, loss=0.8966380953788757
I0221 20:47:29.337817 140468535932672 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.8907471895217896, loss=0.9042856097221375
I0221 20:48:45.071034 140483498055424 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.6806482672691345, loss=0.9027103185653687
I0221 20:50:00.870724 140468535932672 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.7351542711257935, loss=0.8597691655158997
I0221 20:51:16.655960 140483498055424 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.7175498008728027, loss=0.8520566821098328
I0221 20:52:32.444924 140468535932672 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.8417603969573975, loss=0.8321967124938965
I0221 20:53:41.685089 140483498055424 logging_writer.py:48] [79091] global_step=79091, preemption_count=0, score=61068.111580
I0221 20:53:42.624840 140549388556096 checkpoints.py:490] Saving checkpoint at step: 79091
I0221 20:53:44.158383 140549388556096 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_3/librispeech_conformer_jax/trial_5/checkpoint_79091
I0221 20:53:44.189218 140549388556096 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_3/librispeech_conformer_jax/trial_5/checkpoint_79091.
I0221 20:53:47.908190 140549388556096 submission_runner.py:583] Tuning trial 5/5
I0221 20:53:47.908487 140549388556096 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0221 20:53:47.936734 140549388556096 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.602995, dtype=float32), 'train/wer': 1.1353257853575371, 'validation/ctc_loss': Array(30.090126, dtype=float32), 'validation/wer': 0.9587263581683192, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.214182, dtype=float32), 'test/wer': 0.9758089086588264, 'test/num_examples': 2472, 'score': 36.713239431381226, 'total_duration': 165.9817259311676, 'accumulated_submission_time': 36.713239431381226, 'accumulated_eval_time': 129.26838898658752, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1843, {'train/ctc_loss': Array(3.174557, dtype=float32), 'train/wer': 0.6152079537398865, 'validation/ctc_loss': Array(3.1326847, dtype=float32), 'validation/wer': 0.5973044208656362, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.8049276, dtype=float32), 'test/wer': 0.5481079763573213, 'test/num_examples': 2472, 'score': 1477.383898973465, 'total_duration': 1731.5264666080475, 'accumulated_submission_time': 1477.383898973465, 'accumulated_eval_time': 254.03513860702515, 'accumulated_logging_time': 0.03236651420593262, 'global_step': 1843, 'preemption_count': 0}), (3716, {'train/ctc_loss': Array(0.86722195, dtype=float32), 'train/wer': 0.27813964778841876, 'validation/ctc_loss': Array(0.9116082, dtype=float32), 'validation/wer': 0.2709771474362069, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6313826, dtype=float32), 'test/wer': 0.2064875185343164, 'test/num_examples': 2472, 'score': 2917.6238310337067, 'total_duration': 3302.8093264102936, 'accumulated_submission_time': 2917.6238310337067, 'accumulated_eval_time': 384.9413070678711, 'accumulated_logging_time': 0.08956027030944824, 'global_step': 3716, 'preemption_count': 0}), (5589, {'train/ctc_loss': Array(0.54877484, dtype=float32), 'train/wer': 0.18695944673178871, 'validation/ctc_loss': Array(0.7388587, dtype=float32), 'validation/wer': 0.22154532376879038, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4836131, dtype=float32), 'test/wer': 0.1620051591412264, 'test/num_examples': 2472, 'score': 4357.761467218399, 'total_duration': 4875.6513612270355, 'accumulated_submission_time': 4357.761467218399, 'accumulated_eval_time': 517.5147247314453, 'accumulated_logging_time': 0.14196038246154785, 'global_step': 5589, 'preemption_count': 0}), (7461, {'train/ctc_loss': Array(0.5382412, dtype=float32), 'train/wer': 0.18225457817180352, 'validation/ctc_loss': Array(0.6600867, dtype=float32), 'validation/wer': 0.19768867605742588, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42196423, dtype=float32), 'test/wer': 0.14197794162452013, 'test/num_examples': 2472, 'score': 5798.268486738205, 'total_duration': 6447.796489477158, 'accumulated_submission_time': 5798.268486738205, 'accumulated_eval_time': 649.0162920951843, 'accumulated_logging_time': 0.19747233390808105, 'global_step': 7461, 'preemption_count': 0}), (9324, {'train/ctc_loss': Array(0.4458762, dtype=float32), 'train/wer': 0.15523081749969422, 'validation/ctc_loss': Array(0.6294824, dtype=float32), 'validation/wer': 0.18646031454859668, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3988943, dtype=float32), 'test/wer': 0.1319237097069039, 'test/num_examples': 2472, 'score': 7238.677075624466, 'total_duration': 8019.420377254486, 'accumulated_submission_time': 7238.677075624466, 'accumulated_eval_time': 780.1031017303467, 'accumulated_logging_time': 0.2476940155029297, 'global_step': 9324, 'preemption_count': 0}), (11201, {'train/ctc_loss': Array(0.42947593, dtype=float32), 'train/wer': 0.15295235114116562, 'validation/ctc_loss': Array(0.589256, dtype=float32), 'validation/wer': 0.1787172827944428, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36695814, dtype=float32), 'test/wer': 0.12471309893770438, 'test/num_examples': 2472, 'score': 8679.58162689209, 'total_duration': 9594.21326494217, 'accumulated_submission_time': 8679.58162689209, 'accumulated_eval_time': 913.8577964305878, 'accumulated_logging_time': 0.3009212017059326, 'global_step': 11201, 'preemption_count': 0}), (13062, {'train/ctc_loss': Array(0.4079645, dtype=float32), 'train/wer': 0.14124043715846996, 'validation/ctc_loss': Array(0.5708126, dtype=float32), 'validation/wer': 0.17354238875426012, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35079175, dtype=float32), 'test/wer': 0.11987894298539598, 'test/num_examples': 2472, 'score': 10119.531209468842, 'total_duration': 11165.667533397675, 'accumulated_submission_time': 10119.531209468842, 'accumulated_eval_time': 1045.2302241325378, 'accumulated_logging_time': 0.35329461097717285, 'global_step': 13062, 'preemption_count': 0}), (14933, {'train/ctc_loss': Array(0.39691854, dtype=float32), 'train/wer': 0.13824616316063035, 'validation/ctc_loss': Array(0.5532182, dtype=float32), 'validation/wer': 0.16562557324502544, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.337386, dtype=float32), 'test/wer': 0.11307456380882741, 'test/num_examples': 2472, 'score': 11559.680967330933, 'total_duration': 12738.043025493622, 'accumulated_submission_time': 11559.680967330933, 'accumulated_eval_time': 1177.3215169906616, 'accumulated_logging_time': 0.4089851379394531, 'global_step': 14933, 'preemption_count': 0}), (16804, {'train/ctc_loss': Array(0.39982402, dtype=float32), 'train/wer': 0.13525741719949558, 'validation/ctc_loss': Array(0.53366345, dtype=float32), 'validation/wer': 0.15911833708255693, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3234783, dtype=float32), 'test/wer': 0.10779355310462495, 'test/num_examples': 2472, 'score': 12999.688222646713, 'total_duration': 14310.739187717438, 'accumulated_submission_time': 12999.688222646713, 'accumulated_eval_time': 1309.8762323856354, 'accumulated_logging_time': 0.4639158248901367, 'global_step': 16804, 'preemption_count': 0}), (18669, {'train/ctc_loss': Array(0.36982483, dtype=float32), 'train/wer': 0.13034681920725455, 'validation/ctc_loss': Array(0.5220176, dtype=float32), 'validation/wer': 0.15708120528688801, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31250748, dtype=float32), 'test/wer': 0.10627018463225885, 'test/num_examples': 2472, 'score': 14439.60961842537, 'total_duration': 15882.122144460678, 'accumulated_submission_time': 14439.60961842537, 'accumulated_eval_time': 1441.2039613723755, 'accumulated_logging_time': 0.5192501544952393, 'global_step': 18669, 'preemption_count': 0}), (20541, {'train/ctc_loss': Array(0.37734643, dtype=float32), 'train/wer': 0.1359049127256365, 'validation/ctc_loss': Array(0.5120922, dtype=float32), 'validation/wer': 0.1548606350830783, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31171122, dtype=float32), 'test/wer': 0.10403591087278857, 'test/num_examples': 2472, 'score': 15880.007965564728, 'total_duration': 17453.30059313774, 'accumulated_submission_time': 15880.007965564728, 'accumulated_eval_time': 1571.8473567962646, 'accumulated_logging_time': 0.5762767791748047, 'global_step': 20541, 'preemption_count': 0}), (22399, {'train/ctc_loss': Array(0.3108637, dtype=float32), 'train/wer': 0.10999774267608503, 'validation/ctc_loss': Array(0.4965751, dtype=float32), 'validation/wer': 0.14681830908406307, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2929786, dtype=float32), 'test/wer': 0.09751589381106168, 'test/num_examples': 2472, 'score': 17320.585915327072, 'total_duration': 19026.353321552277, 'accumulated_submission_time': 17320.585915327072, 'accumulated_eval_time': 1704.1879363059998, 'accumulated_logging_time': 0.6316831111907959, 'global_step': 22399, 'preemption_count': 0}), (24268, {'train/ctc_loss': Array(0.3285102, dtype=float32), 'train/wer': 0.11573634472779888, 'validation/ctc_loss': Array(0.48729917, dtype=float32), 'validation/wer': 0.14549562161483728, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28838947, dtype=float32), 'test/wer': 0.09629719903316881, 'test/num_examples': 2472, 'score': 18760.76453447342, 'total_duration': 20596.949521303177, 'accumulated_submission_time': 18760.76453447342, 'accumulated_eval_time': 1834.4674425125122, 'accumulated_logging_time': 0.6906123161315918, 'global_step': 24268, 'preemption_count': 0}), (26136, {'train/ctc_loss': Array(0.31813964, dtype=float32), 'train/wer': 0.11223466630410768, 'validation/ctc_loss': Array(0.47977257, dtype=float32), 'validation/wer': 0.14233855006420346, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27694356, dtype=float32), 'test/wer': 0.09205207889017529, 'test/num_examples': 2472, 'score': 20201.36756658554, 'total_duration': 22167.149206638336, 'accumulated_submission_time': 20201.36756658554, 'accumulated_eval_time': 1963.9308321475983, 'accumulated_logging_time': 0.7447621822357178, 'global_step': 26136, 'preemption_count': 0}), (27999, {'train/ctc_loss': Array(0.3083447, dtype=float32), 'train/wer': 0.10892823796917736, 'validation/ctc_loss': Array(0.46320656, dtype=float32), 'validation/wer': 0.13850565279936666, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27267453, dtype=float32), 'test/wer': 0.09190989783275445, 'test/num_examples': 2472, 'score': 21641.74042582512, 'total_duration': 23739.309053897858, 'accumulated_submission_time': 21641.74042582512, 'accumulated_eval_time': 2095.584389448166, 'accumulated_logging_time': 0.7992823123931885, 'global_step': 27999, 'preemption_count': 0}), (29860, {'train/ctc_loss': Array(0.27677372, dtype=float32), 'train/wer': 0.10168670900328784, 'validation/ctc_loss': Array(0.4609452, dtype=float32), 'validation/wer': 0.13718296533014085, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2670367, dtype=float32), 'test/wer': 0.08916783458249548, 'test/num_examples': 2472, 'score': 23081.77032160759, 'total_duration': 25308.97817516327, 'accumulated_submission_time': 23081.77032160759, 'accumulated_eval_time': 2225.088423728943, 'accumulated_logging_time': 0.8564896583557129, 'global_step': 29860, 'preemption_count': 0}), (31706, {'train/ctc_loss': Array(0.24583429, dtype=float32), 'train/wer': 0.09052715700545139, 'validation/ctc_loss': Array(0.4394589, dtype=float32), 'validation/wer': 0.13130328161657512, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2575679, dtype=float32), 'test/wer': 0.08711636503970914, 'test/num_examples': 2472, 'score': 24521.790633678436, 'total_duration': 26879.311678886414, 'accumulated_submission_time': 24521.790633678436, 'accumulated_eval_time': 2355.266172170639, 'accumulated_logging_time': 0.9134845733642578, 'global_step': 31706, 'preemption_count': 0}), (33570, {'train/ctc_loss': Array(0.2740817, dtype=float32), 'train/wer': 0.09920870593159463, 'validation/ctc_loss': Array(0.4404438, dtype=float32), 'validation/wer': 0.1302991976983307, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25297192, dtype=float32), 'test/wer': 0.0839680701968192, 'test/num_examples': 2472, 'score': 25961.964494228363, 'total_duration': 28450.681255578995, 'accumulated_submission_time': 25961.964494228363, 'accumulated_eval_time': 2486.330880880356, 'accumulated_logging_time': 0.966865062713623, 'global_step': 33570, 'preemption_count': 0}), (35434, {'train/ctc_loss': Array(0.2441035, dtype=float32), 'train/wer': 0.08939278377490634, 'validation/ctc_loss': Array(0.426791, dtype=float32), 'validation/wer': 0.12841654035162248, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23996721, dtype=float32), 'test/wer': 0.07998700058903581, 'test/num_examples': 2472, 'score': 27402.324479341507, 'total_duration': 30023.01597237587, 'accumulated_submission_time': 27402.324479341507, 'accumulated_eval_time': 2618.166530609131, 'accumulated_logging_time': 1.0274250507354736, 'global_step': 35434, 'preemption_count': 0}), (37298, {'train/ctc_loss': Array(0.26563275, dtype=float32), 'train/wer': 0.09124326501837958, 'validation/ctc_loss': Array(0.4151319, dtype=float32), 'validation/wer': 0.12274925900537764, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23475464, dtype=float32), 'test/wer': 0.07992606585014117, 'test/num_examples': 2472, 'score': 28842.528745412827, 'total_duration': 31594.84669661522, 'accumulated_submission_time': 28842.528745412827, 'accumulated_eval_time': 2749.658364534378, 'accumulated_logging_time': 1.0829048156738281, 'global_step': 37298, 'preemption_count': 0}), (39159, {'train/ctc_loss': Array(0.21324308, dtype=float32), 'train/wer': 0.07876427602502954, 'validation/ctc_loss': Array(0.41352835, dtype=float32), 'validation/wer': 0.1211658959035307, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23633035, dtype=float32), 'test/wer': 0.07763085735177624, 'test/num_examples': 2472, 'score': 30282.826360464096, 'total_duration': 33165.95847392082, 'accumulated_submission_time': 30282.826360464096, 'accumulated_eval_time': 2880.340024471283, 'accumulated_logging_time': 1.1378488540649414, 'global_step': 39159, 'preemption_count': 0}), (41031, {'train/ctc_loss': Array(0.19461007, dtype=float32), 'train/wer': 0.07236089991897622, 'validation/ctc_loss': Array(0.39671153, dtype=float32), 'validation/wer': 0.11686957529181188, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22629774, dtype=float32), 'test/wer': 0.07537627201267443, 'test/num_examples': 2472, 'score': 31722.86250114441, 'total_duration': 34737.42574644089, 'accumulated_submission_time': 31722.86250114441, 'accumulated_eval_time': 3011.626853942871, 'accumulated_logging_time': 1.2022075653076172, 'global_step': 41031, 'preemption_count': 0}), (42900, {'train/ctc_loss': Array(0.22017571, dtype=float32), 'train/wer': 0.08099192420740234, 'validation/ctc_loss': Array(0.39385065, dtype=float32), 'validation/wer': 0.11726541606727363, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22514184, dtype=float32), 'test/wer': 0.0751528446367274, 'test/num_examples': 2472, 'score': 33163.35210299492, 'total_duration': 36310.161403894424, 'accumulated_submission_time': 33163.35210299492, 'accumulated_eval_time': 3143.738133907318, 'accumulated_logging_time': 1.2583415508270264, 'global_step': 42900, 'preemption_count': 0}), (44779, {'train/ctc_loss': Array(0.14650366, dtype=float32), 'train/wer': 0.05508156402694497, 'validation/ctc_loss': Array(0.39157462, dtype=float32), 'validation/wer': 0.11330700831265629, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21745369, dtype=float32), 'test/wer': 0.07322324457173035, 'test/num_examples': 2472, 'score': 34603.75563144684, 'total_duration': 37891.43506240845, 'accumulated_submission_time': 34603.75563144684, 'accumulated_eval_time': 3284.47208070755, 'accumulated_logging_time': 1.315403699874878, 'global_step': 44779, 'preemption_count': 0}), (46654, {'train/ctc_loss': Array(0.12380371, dtype=float32), 'train/wer': 0.047383894887472355, 'validation/ctc_loss': Array(0.38062832, dtype=float32), 'validation/wer': 0.11105747414966644, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21394913, dtype=float32), 'test/wer': 0.07186236873641663, 'test/num_examples': 2472, 'score': 36043.75965952873, 'total_duration': 39464.65749335289, 'accumulated_submission_time': 36043.75965952873, 'accumulated_eval_time': 3417.5548565387726, 'accumulated_logging_time': 1.3723094463348389, 'global_step': 46654, 'preemption_count': 0}), (48521, {'train/ctc_loss': Array(0.12260363, dtype=float32), 'train/wer': 0.046820854160731395, 'validation/ctc_loss': Array(0.3690256, dtype=float32), 'validation/wer': 0.10806453170105332, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20314053, dtype=float32), 'test/wer': 0.06800316860642253, 'test/num_examples': 2472, 'score': 37483.8937060833, 'total_duration': 41037.80516719818, 'accumulated_submission_time': 37483.8937060833, 'accumulated_eval_time': 3550.433384656906, 'accumulated_logging_time': 1.4295480251312256, 'global_step': 48521, 'preemption_count': 0}), (50389, {'train/ctc_loss': Array(0.1194732, dtype=float32), 'train/wer': 0.04600134181415354, 'validation/ctc_loss': Array(0.36581305, dtype=float32), 'validation/wer': 0.10608532782374465, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20159614, dtype=float32), 'test/wer': 0.06865313915463206, 'test/num_examples': 2472, 'score': 38924.372160196304, 'total_duration': 42611.40343165398, 'accumulated_submission_time': 38924.372160196304, 'accumulated_eval_time': 3683.415236711502, 'accumulated_logging_time': 1.4886324405670166, 'global_step': 50389, 'preemption_count': 0}), (52250, {'train/ctc_loss': Array(0.10834797, dtype=float32), 'train/wer': 0.042986734242475545, 'validation/ctc_loss': Array(0.35260636, dtype=float32), 'validation/wer': 0.10291860162005079, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19382338, dtype=float32), 'test/wer': 0.06505798955984807, 'test/num_examples': 2472, 'score': 40364.40458655357, 'total_duration': 44182.28010845184, 'accumulated_submission_time': 40364.40458655357, 'accumulated_eval_time': 3814.117434978485, 'accumulated_logging_time': 1.552154541015625, 'global_step': 52250, 'preemption_count': 0}), (54120, {'train/ctc_loss': Array(0.09962546, dtype=float32), 'train/wer': 0.03944931886995858, 'validation/ctc_loss': Array(0.35237452, dtype=float32), 'validation/wer': 0.10118076406924317, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19269316, dtype=float32), 'test/wer': 0.06341275160969269, 'test/num_examples': 2472, 'score': 41804.90568423271, 'total_duration': 45756.05579662323, 'accumulated_submission_time': 41804.90568423271, 'accumulated_eval_time': 3947.251857280731, 'accumulated_logging_time': 1.6132729053497314, 'global_step': 54120, 'preemption_count': 0}), (55989, {'train/ctc_loss': Array(0.11142533, dtype=float32), 'train/wer': 0.042615227489869206, 'validation/ctc_loss': Array(0.34446225, dtype=float32), 'validation/wer': 0.09837126002877086, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18800361, dtype=float32), 'test/wer': 0.06121910100948551, 'test/num_examples': 2472, 'score': 43245.16593647003, 'total_duration': 47327.161296606064, 'accumulated_submission_time': 43245.16593647003, 'accumulated_eval_time': 4077.9627606868744, 'accumulated_logging_time': 1.6707770824432373, 'global_step': 55989, 'preemption_count': 0}), (57853, {'train/ctc_loss': Array(0.09508877, dtype=float32), 'train/wer': 0.034736090182440074, 'validation/ctc_loss': Array(0.33794376, dtype=float32), 'validation/wer': 0.09599621537600046, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1831914, dtype=float32), 'test/wer': 0.05945199358154084, 'test/num_examples': 2472, 'score': 44685.39541554451, 'total_duration': 48900.743406534195, 'accumulated_submission_time': 44685.39541554451, 'accumulated_eval_time': 4211.173542499542, 'accumulated_logging_time': 1.7332322597503662, 'global_step': 57853, 'preemption_count': 0}), (59717, {'train/ctc_loss': Array(0.09259246, dtype=float32), 'train/wer': 0.03545706371191136, 'validation/ctc_loss': Array(0.33490953, dtype=float32), 'validation/wer': 0.09581277696785966, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1784495, dtype=float32), 'test/wer': 0.058273921962911056, 'test/num_examples': 2472, 'score': 46125.69869709015, 'total_duration': 50472.63144659996, 'accumulated_submission_time': 46125.69869709015, 'accumulated_eval_time': 4342.618939638138, 'accumulated_logging_time': 1.7944409847259521, 'global_step': 59717, 'preemption_count': 0}), (61577, {'train/ctc_loss': Array(0.07366502, dtype=float32), 'train/wer': 0.028728088745871794, 'validation/ctc_loss': Array(0.3230342, dtype=float32), 'validation/wer': 0.09263639611110575, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17452674, dtype=float32), 'test/wer': 0.05693335770722889, 'test/num_examples': 2472, 'score': 47566.37073302269, 'total_duration': 52045.9193854332, 'accumulated_submission_time': 47566.37073302269, 'accumulated_eval_time': 4475.0985696315765, 'accumulated_logging_time': 1.8526763916015625, 'global_step': 61577, 'preemption_count': 0}), (63446, {'train/ctc_loss': Array(0.06885544, dtype=float32), 'train/wer': 0.027767634591264863, 'validation/ctc_loss': Array(0.32002056, dtype=float32), 'validation/wer': 0.08985585602981357, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17051816, dtype=float32), 'test/wer': 0.055592793451546725, 'test/num_examples': 2472, 'score': 49006.61024451256, 'total_duration': 53619.56326055527, 'accumulated_submission_time': 49006.61024451256, 'accumulated_eval_time': 4608.362781047821, 'accumulated_logging_time': 1.9141466617584229, 'global_step': 63446, 'preemption_count': 0}), (65317, {'train/ctc_loss': Array(0.07590765, dtype=float32), 'train/wer': 0.02990616824854604, 'validation/ctc_loss': Array(0.3158513, dtype=float32), 'validation/wer': 0.08801181729534549, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16880462, dtype=float32), 'test/wer': 0.05437409867365385, 'test/num_examples': 2472, 'score': 50446.498838186264, 'total_duration': 55190.903685331345, 'accumulated_submission_time': 50446.498838186264, 'accumulated_eval_time': 4739.67716550827, 'accumulated_logging_time': 1.9725675582885742, 'global_step': 65317, 'preemption_count': 0}), (67186, {'train/ctc_loss': Array(0.06836843, dtype=float32), 'train/wer': 0.026875016304489605, 'validation/ctc_loss': Array(0.3090984, dtype=float32), 'validation/wer': 0.08585882966295606, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1675134, dtype=float32), 'test/wer': 0.05362257022728657, 'test/num_examples': 2472, 'score': 51886.949083805084, 'total_duration': 56764.86767911911, 'accumulated_submission_time': 51886.949083805084, 'accumulated_eval_time': 4873.053730249405, 'accumulated_logging_time': 2.0313730239868164, 'global_step': 67186, 'preemption_count': 0}), (69044, {'train/ctc_loss': Array(0.06659844, dtype=float32), 'train/wer': 0.025888907946506997, 'validation/ctc_loss': Array(0.3053904, dtype=float32), 'validation/wer': 0.08537609700995395, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16303761, dtype=float32), 'test/wer': 0.051672658582657974, 'test/num_examples': 2472, 'score': 53326.903443574905, 'total_duration': 58338.23026275635, 'accumulated_submission_time': 53326.903443574905, 'accumulated_eval_time': 5006.324478149414, 'accumulated_logging_time': 2.09139347076416, 'global_step': 69044, 'preemption_count': 0}), (70916, {'train/ctc_loss': Array(0.05856837, dtype=float32), 'train/wer': 0.022984151038706827, 'validation/ctc_loss': Array(0.30260167, dtype=float32), 'validation/wer': 0.08377342460198693, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16102883, dtype=float32), 'test/wer': 0.051632035423394874, 'test/num_examples': 2472, 'score': 54767.0825843811, 'total_duration': 59911.81316590309, 'accumulated_submission_time': 54767.0825843811, 'accumulated_eval_time': 5139.588047981262, 'accumulated_logging_time': 2.1525721549987793, 'global_step': 70916, 'preemption_count': 0}), (72786, {'train/ctc_loss': Array(0.05365826, dtype=float32), 'train/wer': 0.020761263701913057, 'validation/ctc_loss': Array(0.30205798, dtype=float32), 'validation/wer': 0.08348378501018566, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15916285, dtype=float32), 'test/wer': 0.050961753295553795, 'test/num_examples': 2472, 'score': 56207.34043216705, 'total_duration': 61485.750854730606, 'accumulated_submission_time': 56207.34043216705, 'accumulated_eval_time': 5273.123430967331, 'accumulated_logging_time': 2.2175686359405518, 'global_step': 72786, 'preemption_count': 0}), (74654, {'train/ctc_loss': Array(0.05984657, dtype=float32), 'train/wer': 0.022029474976972673, 'validation/ctc_loss': Array(0.2980946, dtype=float32), 'validation/wer': 0.08164940092877762, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15752992, dtype=float32), 'test/wer': 0.050088355371397233, 'test/num_examples': 2472, 'score': 57647.41107773781, 'total_duration': 63059.09764456749, 'accumulated_submission_time': 57647.41107773781, 'accumulated_eval_time': 5406.259341955185, 'accumulated_logging_time': 2.2783915996551514, 'global_step': 74654, 'preemption_count': 0}), (76528, {'train/ctc_loss': Array(0.05743812, dtype=float32), 'train/wer': 0.021416039085486005, 'validation/ctc_loss': Array(0.29819798, dtype=float32), 'validation/wer': 0.08163009162265754, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15713376, dtype=float32), 'test/wer': 0.050291471167712716, 'test/num_examples': 2472, 'score': 59087.65986609459, 'total_duration': 64631.961369514465, 'accumulated_submission_time': 59087.65986609459, 'accumulated_eval_time': 5538.735710859299, 'accumulated_logging_time': 2.3376240730285645, 'global_step': 76528, 'preemption_count': 0}), (78379, {'train/ctc_loss': Array(0.05857571, dtype=float32), 'train/wer': 0.022106165300765975, 'validation/ctc_loss': Array(0.29666498, dtype=float32), 'validation/wer': 0.08126321480637594, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15672202, dtype=float32), 'test/wer': 0.049986797473239496, 'test/num_examples': 2472, 'score': 60527.83698248863, 'total_duration': 66204.51050138474, 'accumulated_submission_time': 60527.83698248863, 'accumulated_eval_time': 5670.963258266449, 'accumulated_logging_time': 2.402360677719116, 'global_step': 78379, 'preemption_count': 0})], 'global_step': 79091}
I0221 20:53:47.936998 140549388556096 submission_runner.py:586] Timing: 61068.11158013344
I0221 20:53:47.937059 140549388556096 submission_runner.py:588] Total number of evals: 43
I0221 20:53:47.937111 140549388556096 submission_runner.py:589] ====================
I0221 20:53:48.019783 140549388556096 submission_runner.py:673] Final librispeech_conformer score: 61068.11158013344
