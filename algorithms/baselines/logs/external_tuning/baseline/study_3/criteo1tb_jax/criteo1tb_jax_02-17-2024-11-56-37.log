python3 submission_runner.py --framework=jax --workload=criteo1tb --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification/study_3 --overwrite=true --save_checkpoints=false --num_tuning_trials=5 --rng_seed=2861073867 --max_global_steps=10666 2>&1 | tee -a /logs/criteo1tb_jax_02-17-2024-11-56-37.log
I0217 11:56:56.782563 139973357201216 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification/study_3/criteo1tb_jax.
I0217 11:56:58.489041 139973357201216 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0217 11:56:58.490108 139973357201216 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0217 11:56:58.490266 139973357201216 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0217 11:56:58.491606 139973357201216 submission_runner.py:542] Using RNG seed 2861073867
I0217 11:56:59.646206 139973357201216 submission_runner.py:551] --- Tuning run 1/5 ---
I0217 11:56:59.646406 139973357201216 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_3/criteo1tb_jax/trial_1.
I0217 11:56:59.646593 139973357201216 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_3/criteo1tb_jax/trial_1/hparams.json.
I0217 11:56:59.833619 139973357201216 submission_runner.py:206] Initializing dataset.
I0217 11:56:59.833852 139973357201216 submission_runner.py:213] Initializing model.
I0217 11:57:05.598546 139973357201216 submission_runner.py:255] Initializing optimizer.
I0217 11:57:09.073840 139973357201216 submission_runner.py:262] Initializing metrics bundle.
I0217 11:57:09.074044 139973357201216 submission_runner.py:280] Initializing checkpoint and logger.
I0217 11:57:09.075236 139973357201216 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_3/criteo1tb_jax/trial_1 with prefix checkpoint_
I0217 11:57:09.075378 139973357201216 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_3/criteo1tb_jax/trial_1/meta_data_0.json.
I0217 11:57:09.075575 139973357201216 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0217 11:57:09.075637 139973357201216 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0217 11:57:09.424466 139973357201216 logger_utils.py:220] Unable to record git information. Continuing without it.
I0217 11:57:09.746305 139973357201216 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_3/criteo1tb_jax/trial_1/flags_0.json.
I0217 11:57:09.847106 139973357201216 submission_runner.py:314] Starting training loop.
I0217 11:57:30.727034 139811328345856 logging_writer.py:48] [0] global_step=0, grad_norm=9.023993492126465, loss=1.5136961936950684
I0217 11:57:30.739403 139973357201216 spec.py:321] Evaluating on the training split.
I0217 12:01:36.096440 139973357201216 spec.py:333] Evaluating on the validation split.
I0217 12:05:43.234424 139973357201216 spec.py:349] Evaluating on the test split.
I0217 12:10:21.349163 139973357201216 submission_runner.py:408] Time since start: 791.50s, 	Step: 1, 	{'train/loss': 1.5196934795979433, 'validation/loss': 1.5212185261462023, 'validation/num_examples': 83274637, 'test/loss': 1.5194805184210527, 'test/num_examples': 95000000, 'score': 20.892271518707275, 'total_duration': 791.5020091533661, 'accumulated_submission_time': 20.892271518707275, 'accumulated_eval_time': 770.6096827983856, 'accumulated_logging_time': 0}
I0217 12:10:21.367696 139793442785024 logging_writer.py:48] [1] accumulated_eval_time=770.609683, accumulated_logging_time=0, accumulated_submission_time=20.892272, global_step=1, preemption_count=0, score=20.892272, test/loss=1.519481, test/num_examples=95000000, total_duration=791.502009, train/loss=1.519693, validation/loss=1.521219, validation/num_examples=83274637
I0217 12:11:19.760452 139793434392320 logging_writer.py:48] [100] global_step=100, grad_norm=0.5914985537528992, loss=0.17034649848937988
I0217 12:12:34.979806 139793442785024 logging_writer.py:48] [200] global_step=200, grad_norm=0.04087948054075241, loss=0.13404704630374908
I0217 12:13:50.215991 139793434392320 logging_writer.py:48] [300] global_step=300, grad_norm=0.021523552015423775, loss=0.14619004726409912
I0217 12:15:05.762714 139793442785024 logging_writer.py:48] [400] global_step=400, grad_norm=0.035920850932598114, loss=0.12268497049808502
I0217 12:16:20.896840 139793434392320 logging_writer.py:48] [500] global_step=500, grad_norm=0.009572292678058147, loss=0.11867894977331161
I0217 12:17:36.327960 139793442785024 logging_writer.py:48] [600] global_step=600, grad_norm=0.055113840848207474, loss=0.134273499250412
I0217 12:18:51.318002 139793434392320 logging_writer.py:48] [700] global_step=700, grad_norm=0.03770047053694725, loss=0.1329713612794876
I0217 12:20:06.633258 139793442785024 logging_writer.py:48] [800] global_step=800, grad_norm=0.013373003341257572, loss=0.12577246129512787
I0217 12:21:21.468992 139793434392320 logging_writer.py:48] [900] global_step=900, grad_norm=0.009553155861794949, loss=0.12183991074562073
I0217 12:22:34.677154 139793442785024 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.024308089166879654, loss=0.11983142793178558
I0217 12:23:49.808831 139793434392320 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.011022753082215786, loss=0.1339627504348755
I0217 12:25:05.136221 139793442785024 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.032452188432216644, loss=0.12207293510437012
I0217 12:26:20.939321 139793434392320 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.017761578783392906, loss=0.12638302147388458
I0217 12:27:36.588814 139793442785024 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.012035799212753773, loss=0.1277949959039688
I0217 12:28:49.529087 139793434392320 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.015720311552286148, loss=0.12385158240795135
I0217 12:30:05.186791 139793442785024 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.03990823030471802, loss=0.12158015370368958
I0217 12:30:21.779084 139973357201216 spec.py:321] Evaluating on the training split.
I0217 12:33:39.360112 139973357201216 spec.py:333] Evaluating on the validation split.
I0217 12:37:00.583330 139973357201216 spec.py:349] Evaluating on the test split.
I0217 12:40:54.119120 139973357201216 submission_runner.py:408] Time since start: 2624.27s, 	Step: 1623, 	{'train/loss': 0.12422609460428825, 'validation/loss': 0.126126404030062, 'validation/num_examples': 83274637, 'test/loss': 0.12872753095189146, 'test/num_examples': 95000000, 'score': 1221.2437376976013, 'total_duration': 2624.271951198578, 'accumulated_submission_time': 1221.2437376976013, 'accumulated_eval_time': 1402.949651002884, 'accumulated_logging_time': 0.026587724685668945}
I0217 12:40:54.135906 139793434392320 logging_writer.py:48] [1623] accumulated_eval_time=1402.949651, accumulated_logging_time=0.026588, accumulated_submission_time=1221.243738, global_step=1623, preemption_count=0, score=1221.243738, test/loss=0.128728, test/num_examples=95000000, total_duration=2624.271951, train/loss=0.124226, validation/loss=0.126126, validation/num_examples=83274637
I0217 12:41:36.467815 139793442785024 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.03195682168006897, loss=0.13137847185134888
I0217 12:42:51.633544 139793434392320 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.025007426738739014, loss=0.13372930884361267
I0217 12:44:07.036465 139793442785024 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.02280597947537899, loss=0.12515974044799805
I0217 12:45:22.698809 139793434392320 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.026964666321873665, loss=0.12542128562927246
I0217 12:46:35.040647 139793442785024 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.026957297697663307, loss=0.13155655562877655
I0217 12:47:50.744737 139793434392320 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.018338646739721298, loss=0.13025206327438354
I0217 12:49:06.319509 139793442785024 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.031066717579960823, loss=0.12165134400129318
I0217 12:50:21.635605 139793434392320 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.009387830272316933, loss=0.12180499732494354
I0217 12:51:36.733558 139793442785024 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.010528533719480038, loss=0.12850594520568848
I0217 12:52:52.413527 139793434392320 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.006848369725048542, loss=0.12225479632616043
I0217 12:54:08.197142 139793442785024 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.006524496246129274, loss=0.13450945913791656
I0217 12:55:23.790998 139793434392320 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.0161175224930048, loss=0.12625737488269806
I0217 12:56:39.639752 139793442785024 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.033490099012851715, loss=0.13021202385425568
I0217 12:57:55.637964 139793434392320 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.013702957890927792, loss=0.12835745513439178
I0217 12:59:11.861596 139793442785024 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.0385669507086277, loss=0.13027630746364594
I0217 13:00:28.006510 139793434392320 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.006470310967415571, loss=0.1200454905629158
I0217 13:00:54.194355 139973357201216 spec.py:321] Evaluating on the training split.
I0217 13:04:12.362774 139973357201216 spec.py:333] Evaluating on the validation split.
I0217 13:06:53.037173 139973357201216 spec.py:349] Evaluating on the test split.
I0217 13:09:58.911189 139973357201216 submission_runner.py:408] Time since start: 4369.06s, 	Step: 3236, 	{'train/loss': 0.1254858712917604, 'validation/loss': 0.12513278784241352, 'validation/num_examples': 83274637, 'test/loss': 0.12760526905838815, 'test/num_examples': 95000000, 'score': 2421.242283344269, 'total_duration': 4369.06401515007, 'accumulated_submission_time': 2421.242283344269, 'accumulated_eval_time': 1947.666404247284, 'accumulated_logging_time': 0.05207657814025879}
I0217 13:09:58.930225 139793442785024 logging_writer.py:48] [3236] accumulated_eval_time=1947.666404, accumulated_logging_time=0.052077, accumulated_submission_time=2421.242283, global_step=3236, preemption_count=0, score=2421.242283, test/loss=0.127605, test/num_examples=95000000, total_duration=4369.064015, train/loss=0.125486, validation/loss=0.125133, validation/num_examples=83274637
I0217 13:10:30.650884 139793434392320 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.02019413188099861, loss=0.11912735551595688
I0217 13:11:46.985509 139793442785024 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.008855128660798073, loss=0.11943016946315765
I0217 13:13:02.805315 139793434392320 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.009954477660357952, loss=0.11913010478019714
I0217 13:14:15.624972 139793442785024 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.012218357063829899, loss=0.12673000991344452
I0217 13:15:30.871824 139793434392320 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.03221982344985008, loss=0.12077760696411133
I0217 13:16:46.287188 139793442785024 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.010829806327819824, loss=0.1176038533449173
I0217 13:18:01.381948 139793434392320 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.02455931529402733, loss=0.12997384369373322
I0217 13:19:16.533541 139793442785024 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.014479303732514381, loss=0.12122216075658798
I0217 13:20:31.781559 139793434392320 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.01986221969127655, loss=0.13290569186210632
I0217 13:21:46.992753 139793442785024 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.006717560812830925, loss=0.11834721267223358
I0217 13:23:02.228724 139793434392320 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.009315123781561852, loss=0.12942250072956085
I0217 13:24:17.505165 139793442785024 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.00681762769818306, loss=0.12510152161121368
I0217 13:25:32.765246 139793434392320 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.024885360151529312, loss=0.12558384239673615
I0217 13:26:48.413282 139793442785024 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.01347854733467102, loss=0.11703862994909286
I0217 13:28:03.728172 139793434392320 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.010237154550850391, loss=0.12187063694000244
I0217 13:29:18.964529 139793442785024 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.00959214847534895, loss=0.1167205274105072
I0217 13:29:59.329938 139973357201216 spec.py:321] Evaluating on the training split.
I0217 13:33:07.395966 139973357201216 spec.py:333] Evaluating on the validation split.
I0217 13:35:48.738225 139973357201216 spec.py:349] Evaluating on the test split.
I0217 13:38:51.431068 139973357201216 submission_runner.py:408] Time since start: 6101.58s, 	Step: 4855, 	{'train/loss': 0.1232307549074011, 'validation/loss': 0.12472948058302553, 'validation/num_examples': 83274637, 'test/loss': 0.12705782748766448, 'test/num_examples': 95000000, 'score': 3621.5827877521515, 'total_duration': 6101.583913326263, 'accumulated_submission_time': 3621.5827877521515, 'accumulated_eval_time': 2479.767473459244, 'accumulated_logging_time': 0.07892727851867676}
I0217 13:38:51.450547 139793434392320 logging_writer.py:48] [4855] accumulated_eval_time=2479.767473, accumulated_logging_time=0.078927, accumulated_submission_time=3621.582788, global_step=4855, preemption_count=0, score=3621.582788, test/loss=0.127058, test/num_examples=95000000, total_duration=6101.583913, train/loss=0.123231, validation/loss=0.124729, validation/num_examples=83274637
I0217 13:39:08.506398 139793442785024 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.026922015473246574, loss=0.12698140740394592
I0217 13:40:22.558579 139793434392320 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.008157369680702686, loss=0.1143706738948822
I0217 13:41:37.865966 139793442785024 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.008869592100381851, loss=0.12281842529773712
I0217 13:42:52.772330 139793434392320 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.008722714148461819, loss=0.127021923661232
I0217 13:44:08.059585 139793442785024 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.03836267814040184, loss=0.12062958627939224
I0217 13:45:23.760506 139793434392320 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.014714241959154606, loss=0.1242758259177208
I0217 13:46:39.043607 139793442785024 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.006972943432629108, loss=0.11983302235603333
I0217 13:47:53.945711 139793434392320 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.013342662714421749, loss=0.12564726173877716
I0217 13:49:09.740264 139793442785024 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.008813414722681046, loss=0.1210576519370079
I0217 13:50:24.974691 139793434392320 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.0089879734441638, loss=0.1222432479262352
I0217 13:51:40.408233 139793442785024 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.008721335791051388, loss=0.12186218798160553
I0217 13:52:50.982443 139793434392320 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.008517333306372166, loss=0.12777550518512726
I0217 13:54:05.985450 139793442785024 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.012254135683178902, loss=0.12728720903396606
I0217 13:55:18.765087 139793434392320 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.007195816840976477, loss=0.11624019593000412
I0217 13:56:34.207434 139793442785024 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.008680577389895916, loss=0.12729300558567047
I0217 13:57:50.039779 139793434392320 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.008285585790872574, loss=0.11873811483383179
I0217 13:58:51.653951 139973357201216 spec.py:321] Evaluating on the training split.
I0217 14:01:48.191076 139973357201216 spec.py:333] Evaluating on the validation split.
I0217 14:04:16.111095 139973357201216 spec.py:349] Evaluating on the test split.
I0217 14:07:08.631186 139973357201216 submission_runner.py:408] Time since start: 7798.78s, 	Step: 6483, 	{'train/loss': 0.12396965841264845, 'validation/loss': 0.12418653489034423, 'validation/num_examples': 83274637, 'test/loss': 0.12655360554070724, 'test/num_examples': 95000000, 'score': 4821.726407766342, 'total_duration': 7798.784013032913, 'accumulated_submission_time': 4821.726407766342, 'accumulated_eval_time': 2976.7446382045746, 'accumulated_logging_time': 0.10718870162963867}
I0217 14:07:08.650024 139793442785024 logging_writer.py:48] [6483] accumulated_eval_time=2976.744638, accumulated_logging_time=0.107189, accumulated_submission_time=4821.726408, global_step=6483, preemption_count=0, score=4821.726408, test/loss=0.126554, test/num_examples=95000000, total_duration=7798.784013, train/loss=0.123970, validation/loss=0.124187, validation/num_examples=83274637
I0217 14:07:10.380668 139793434392320 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.011000238358974457, loss=0.11637337505817413
I0217 14:08:22.463915 139793442785024 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.012804174795746803, loss=0.12119559943675995
I0217 14:09:37.466854 139793434392320 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.009167755022644997, loss=0.1222689151763916
I0217 14:10:52.923831 139793442785024 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.020086858421564102, loss=0.12017112225294113
I0217 14:12:08.373737 139793434392320 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.007055082358419895, loss=0.12387287616729736
I0217 14:13:24.502831 139793442785024 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.008102303370833397, loss=0.12027957290410995
I0217 14:14:39.431709 139793434392320 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.006700488738715649, loss=0.12688595056533813
I0217 14:15:55.031949 139793442785024 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.010626472532749176, loss=0.13232485949993134
I0217 14:17:10.575389 139793434392320 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.007750936783850193, loss=0.1161392480134964
I0217 14:18:26.207306 139793442785024 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.013071284629404545, loss=0.11827240884304047
I0217 14:19:41.561556 139793434392320 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.013380684889853, loss=0.12251296639442444
I0217 14:20:57.265186 139793442785024 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.011878158897161484, loss=0.11887102574110031
I0217 14:22:12.844481 139793434392320 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.01813199743628502, loss=0.1196809858083725
I0217 14:23:28.572441 139793442785024 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.007953034713864326, loss=0.1181800439953804
I0217 14:24:43.911297 139793434392320 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.010707397013902664, loss=0.12762606143951416
I0217 14:25:59.166059 139793442785024 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.009229191578924656, loss=0.13282456994056702
I0217 14:27:08.661879 139973357201216 spec.py:321] Evaluating on the training split.
I0217 14:29:44.343613 139973357201216 spec.py:333] Evaluating on the validation split.
I0217 14:31:53.565442 139973357201216 spec.py:349] Evaluating on the test split.
I0217 14:34:30.025036 139973357201216 submission_runner.py:408] Time since start: 9440.18s, 	Step: 8093, 	{'train/loss': 0.12175970477690487, 'validation/loss': 0.12394061334590387, 'validation/num_examples': 83274637, 'test/loss': 0.1263109411800987, 'test/num_examples': 95000000, 'score': 6021.679481744766, 'total_duration': 9440.177884578705, 'accumulated_submission_time': 6021.679481744766, 'accumulated_eval_time': 3418.1077494621277, 'accumulated_logging_time': 0.1339259147644043}
I0217 14:34:30.040605 139793434392320 logging_writer.py:48] [8093] accumulated_eval_time=3418.107749, accumulated_logging_time=0.133926, accumulated_submission_time=6021.679482, global_step=8093, preemption_count=0, score=6021.679482, test/loss=0.126311, test/num_examples=95000000, total_duration=9440.177885, train/loss=0.121760, validation/loss=0.123941, validation/num_examples=83274637
I0217 14:34:30.826500 139793442785024 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.019505901262164116, loss=0.1220768615603447
I0217 14:35:35.857999 139793434392320 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.007951798848807812, loss=0.12521584331989288
I0217 14:36:52.812339 139793442785024 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.013263587839901447, loss=0.12134315073490143
I0217 14:38:05.708969 139793434392320 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.017835179343819618, loss=0.11931189894676208
I0217 14:39:21.025657 139793442785024 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.011419841088354588, loss=0.12251293659210205
I0217 14:40:36.328477 139793434392320 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.019996384158730507, loss=0.11905666440725327
I0217 14:41:51.713218 139793442785024 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.007278149947524071, loss=0.12429921329021454
I0217 14:43:06.621670 139793434392320 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.012584212236106396, loss=0.11498229950666428
I0217 14:44:21.864374 139793442785024 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.03373894467949867, loss=0.12326522171497345
I0217 14:45:37.042838 139793434392320 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.008871404454112053, loss=0.12244847416877747
I0217 14:46:52.272702 139793442785024 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.013457509689033031, loss=0.1205001175403595
I0217 14:48:07.589143 139793434392320 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.017661582678556442, loss=0.12028704583644867
I0217 14:49:23.268394 139793442785024 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.018736261874437332, loss=0.12527990341186523
I0217 14:50:38.585481 139793434392320 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.007191835902631283, loss=0.12274185568094254
I0217 14:51:54.097739 139793442785024 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.011903811246156693, loss=0.12667638063430786
I0217 14:53:10.193372 139793434392320 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.008293517865240574, loss=0.1192716434597969
I0217 14:54:26.380405 139793442785024 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.00875127874314785, loss=0.12274052202701569
I0217 14:54:30.191812 139973357201216 spec.py:321] Evaluating on the training split.
I0217 14:56:18.226473 139973357201216 spec.py:333] Evaluating on the validation split.
I0217 14:57:46.521741 139973357201216 spec.py:349] Evaluating on the test split.
I0217 14:59:48.032205 139973357201216 submission_runner.py:408] Time since start: 10958.19s, 	Step: 9706, 	{'train/loss': 0.12191009296561187, 'validation/loss': 0.123722453762017, 'validation/num_examples': 83274637, 'test/loss': 0.12604977984169408, 'test/num_examples': 95000000, 'score': 7221.771959543228, 'total_duration': 10958.185050487518, 'accumulated_submission_time': 7221.771959543228, 'accumulated_eval_time': 3735.948078393936, 'accumulated_logging_time': 0.15744614601135254}
I0217 14:59:48.050515 139793434392320 logging_writer.py:48] [9706] accumulated_eval_time=3735.948078, accumulated_logging_time=0.157446, accumulated_submission_time=7221.771960, global_step=9706, preemption_count=0, score=7221.771960, test/loss=0.126050, test/num_examples=95000000, total_duration=10958.185050, train/loss=0.121910, validation/loss=0.123722, validation/num_examples=83274637
I0217 15:00:45.028642 139793442785024 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.013005114160478115, loss=0.12488574534654617
I0217 15:02:03.188777 139793434392320 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.01358484011143446, loss=0.1205281913280487
I0217 15:03:19.517786 139793442785024 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.026311369612812996, loss=0.12445898354053497
I0217 15:04:34.925882 139793434392320 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.01662895642220974, loss=0.11980736255645752
I0217 15:05:50.117513 139793442785024 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.02550472505390644, loss=0.13037998974323273
I0217 15:07:02.964246 139793434392320 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.015183275565505028, loss=0.12677215039730072
I0217 15:07:49.695628 139793442785024 logging_writer.py:48] [10363] global_step=10363, preemption_count=0, score=7703.379246
I0217 15:07:56.012346 139973357201216 checkpoints.py:490] Saving checkpoint at step: 10363
I0217 15:08:31.643553 139973357201216 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_3/criteo1tb_jax/trial_1/checkpoint_10363
I0217 15:08:31.998697 139973357201216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_3/criteo1tb_jax/trial_1/checkpoint_10363.
I0217 15:08:32.411959 139973357201216 submission_runner.py:583] Tuning trial 1/5
I0217 15:08:32.412222 139973357201216 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0217 15:08:32.413360 139973357201216 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/loss': 1.5196934795979433, 'validation/loss': 1.5212185261462023, 'validation/num_examples': 83274637, 'test/loss': 1.5194805184210527, 'test/num_examples': 95000000, 'score': 20.892271518707275, 'total_duration': 791.5020091533661, 'accumulated_submission_time': 20.892271518707275, 'accumulated_eval_time': 770.6096827983856, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1623, {'train/loss': 0.12422609460428825, 'validation/loss': 0.126126404030062, 'validation/num_examples': 83274637, 'test/loss': 0.12872753095189146, 'test/num_examples': 95000000, 'score': 1221.2437376976013, 'total_duration': 2624.271951198578, 'accumulated_submission_time': 1221.2437376976013, 'accumulated_eval_time': 1402.949651002884, 'accumulated_logging_time': 0.026587724685668945, 'global_step': 1623, 'preemption_count': 0}), (3236, {'train/loss': 0.1254858712917604, 'validation/loss': 0.12513278784241352, 'validation/num_examples': 83274637, 'test/loss': 0.12760526905838815, 'test/num_examples': 95000000, 'score': 2421.242283344269, 'total_duration': 4369.06401515007, 'accumulated_submission_time': 2421.242283344269, 'accumulated_eval_time': 1947.666404247284, 'accumulated_logging_time': 0.05207657814025879, 'global_step': 3236, 'preemption_count': 0}), (4855, {'train/loss': 0.1232307549074011, 'validation/loss': 0.12472948058302553, 'validation/num_examples': 83274637, 'test/loss': 0.12705782748766448, 'test/num_examples': 95000000, 'score': 3621.5827877521515, 'total_duration': 6101.583913326263, 'accumulated_submission_time': 3621.5827877521515, 'accumulated_eval_time': 2479.767473459244, 'accumulated_logging_time': 0.07892727851867676, 'global_step': 4855, 'preemption_count': 0}), (6483, {'train/loss': 0.12396965841264845, 'validation/loss': 0.12418653489034423, 'validation/num_examples': 83274637, 'test/loss': 0.12655360554070724, 'test/num_examples': 95000000, 'score': 4821.726407766342, 'total_duration': 7798.784013032913, 'accumulated_submission_time': 4821.726407766342, 'accumulated_eval_time': 2976.7446382045746, 'accumulated_logging_time': 0.10718870162963867, 'global_step': 6483, 'preemption_count': 0}), (8093, {'train/loss': 0.12175970477690487, 'validation/loss': 0.12394061334590387, 'validation/num_examples': 83274637, 'test/loss': 0.1263109411800987, 'test/num_examples': 95000000, 'score': 6021.679481744766, 'total_duration': 9440.177884578705, 'accumulated_submission_time': 6021.679481744766, 'accumulated_eval_time': 3418.1077494621277, 'accumulated_logging_time': 0.1339259147644043, 'global_step': 8093, 'preemption_count': 0}), (9706, {'train/loss': 0.12191009296561187, 'validation/loss': 0.123722453762017, 'validation/num_examples': 83274637, 'test/loss': 0.12604977984169408, 'test/num_examples': 95000000, 'score': 7221.771959543228, 'total_duration': 10958.185050487518, 'accumulated_submission_time': 7221.771959543228, 'accumulated_eval_time': 3735.948078393936, 'accumulated_logging_time': 0.15744614601135254, 'global_step': 9706, 'preemption_count': 0})], 'global_step': 10363}
I0217 15:08:32.413499 139973357201216 submission_runner.py:586] Timing: 7703.379245758057
I0217 15:08:32.413550 139973357201216 submission_runner.py:588] Total number of evals: 7
I0217 15:08:32.413594 139973357201216 submission_runner.py:589] ====================
I0217 15:08:32.413641 139973357201216 submission_runner.py:542] Using RNG seed 2861073867
I0217 15:08:32.415323 139973357201216 submission_runner.py:551] --- Tuning run 2/5 ---
I0217 15:08:32.415450 139973357201216 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_3/criteo1tb_jax/trial_2.
I0217 15:08:32.421739 139973357201216 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_3/criteo1tb_jax/trial_2/hparams.json.
I0217 15:08:32.423861 139973357201216 submission_runner.py:206] Initializing dataset.
I0217 15:08:32.423988 139973357201216 submission_runner.py:213] Initializing model.
I0217 15:08:35.649279 139973357201216 submission_runner.py:255] Initializing optimizer.
I0217 15:08:38.399287 139973357201216 submission_runner.py:262] Initializing metrics bundle.
I0217 15:08:38.399518 139973357201216 submission_runner.py:280] Initializing checkpoint and logger.
I0217 15:08:38.517812 139973357201216 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_3/criteo1tb_jax/trial_2 with prefix checkpoint_
I0217 15:08:38.518004 139973357201216 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_3/criteo1tb_jax/trial_2/meta_data_0.json.
I0217 15:08:38.518226 139973357201216 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0217 15:08:38.518288 139973357201216 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0217 15:08:44.928148 139973357201216 logger_utils.py:220] Unable to record git information. Continuing without it.
I0217 15:08:50.931815 139973357201216 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_3/criteo1tb_jax/trial_2/flags_0.json.
I0217 15:08:51.021224 139973357201216 submission_runner.py:314] Starting training loop.
I0217 15:08:56.893124 139812778530560 logging_writer.py:48] [0] global_step=0, grad_norm=9.141043663024902, loss=1.5220379829406738
I0217 15:08:56.898281 139973357201216 spec.py:321] Evaluating on the training split.
I0217 15:09:52.317121 139973357201216 spec.py:333] Evaluating on the validation split.
I0217 15:10:24.155989 139973357201216 spec.py:349] Evaluating on the test split.
I0217 15:11:36.083353 139973357201216 submission_runner.py:408] Time since start: 165.06s, 	Step: 1, 	{'train/loss': 1.5198267240944148, 'validation/loss': 1.5212185261462023, 'validation/num_examples': 83274637, 'test/loss': 1.5194805184210527, 'test/num_examples': 95000000, 'score': 5.877012491226196, 'total_duration': 165.06203317642212, 'accumulated_submission_time': 5.877012491226196, 'accumulated_eval_time': 159.1849820613861, 'accumulated_logging_time': 0}
I0217 15:11:36.093506 139812786923264 logging_writer.py:48] [1] accumulated_eval_time=159.184982, accumulated_logging_time=0, accumulated_submission_time=5.877012, global_step=1, preemption_count=0, score=5.877012, test/loss=1.519481, test/num_examples=95000000, total_duration=165.062033, train/loss=1.519827, validation/loss=1.521219, validation/num_examples=83274637
I0217 15:13:05.482346 139812778530560 logging_writer.py:48] [100] global_step=100, grad_norm=0.029509635642170906, loss=0.12898318469524384
I0217 15:15:19.057917 139812786923264 logging_writer.py:48] [200] global_step=200, grad_norm=0.16108988225460052, loss=0.12228629738092422
I0217 15:17:14.220499 139812778530560 logging_writer.py:48] [300] global_step=300, grad_norm=0.1574196219444275, loss=0.12679120898246765
I0217 15:18:31.306625 139812786923264 logging_writer.py:48] [400] global_step=400, grad_norm=0.05136265978217125, loss=0.1268576979637146
I0217 15:19:44.417470 139812778530560 logging_writer.py:48] [500] global_step=500, grad_norm=0.02210388146340847, loss=0.1250096708536148
I0217 15:21:00.790528 139812786923264 logging_writer.py:48] [600] global_step=600, grad_norm=0.06452728062868118, loss=0.12308532744646072
I0217 15:22:16.653197 139812778530560 logging_writer.py:48] [700] global_step=700, grad_norm=0.021651245653629303, loss=0.12363691627979279
I0217 15:23:32.459741 139812786923264 logging_writer.py:48] [800] global_step=800, grad_norm=0.1687198430299759, loss=0.12561443448066711
I0217 15:24:48.271854 139812778530560 logging_writer.py:48] [900] global_step=900, grad_norm=0.059794872999191284, loss=0.11934749782085419
I0217 15:26:04.450605 139812786923264 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.06656663119792938, loss=0.12685906887054443
I0217 15:27:20.477847 139812778530560 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.05947270616889, loss=0.12826228141784668
I0217 15:28:36.400959 139812786923264 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.09216242283582687, loss=0.1261502504348755
I0217 15:29:52.212082 139812778530560 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.0313149020075798, loss=0.1313396543264389
I0217 15:31:05.664022 139812786923264 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.08692164719104767, loss=0.1212201714515686
I0217 15:31:36.502044 139973357201216 spec.py:321] Evaluating on the training split.
I0217 15:31:43.501959 139973357201216 spec.py:333] Evaluating on the validation split.
I0217 15:31:50.874230 139973357201216 spec.py:349] Evaluating on the test split.
I0217 15:32:00.752011 139973357201216 submission_runner.py:408] Time since start: 1389.73s, 	Step: 1441, 	{'train/loss': 0.12354151768691884, 'validation/loss': 0.12716335743003598, 'validation/num_examples': 83274637, 'test/loss': 0.12938557531866776, 'test/num_examples': 95000000, 'score': 1206.2299313545227, 'total_duration': 1389.7307302951813, 'accumulated_submission_time': 1206.2299313545227, 'accumulated_eval_time': 183.43490958213806, 'accumulated_logging_time': 0.018738269805908203}
I0217 15:32:00.765955 139812778530560 logging_writer.py:48] [1441] accumulated_eval_time=183.434910, accumulated_logging_time=0.018738, accumulated_submission_time=1206.229931, global_step=1441, preemption_count=0, score=1206.229931, test/loss=0.129386, test/num_examples=95000000, total_duration=1389.730730, train/loss=0.123542, validation/loss=0.127163, validation/num_examples=83274637
I0217 15:32:44.730380 139812786923264 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.025581547990441322, loss=0.12067516148090363
I0217 15:34:55.509508 139812778530560 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.17085254192352295, loss=0.13292530179023743
I0217 15:36:46.655646 139812786923264 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.04083709046244621, loss=0.12270525842905045
I0217 15:38:02.702932 139812778530560 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.008738771080970764, loss=0.1280066817998886
I0217 15:39:18.715954 139812786923264 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.04617425799369812, loss=0.13082994520664215
I0217 15:40:34.664407 139812778530560 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.016886167228221893, loss=0.12237345427274704
I0217 15:41:50.847941 139812786923264 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.018704598769545555, loss=0.12335164844989777
I0217 15:43:06.773603 139812778530560 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.005856851115822792, loss=0.11966190487146378
I0217 15:44:22.752769 139812786923264 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.0885084718465805, loss=0.1301167756319046
I0217 15:45:38.773208 139812778530560 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.024382557719945908, loss=0.13192331790924072
I0217 15:46:55.448705 139812786923264 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.014884036965668201, loss=0.12971918284893036
I0217 15:48:11.221333 139812778530560 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.0200814176350832, loss=0.12188122421503067
I0217 15:49:27.265395 139812786923264 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.05044572055339813, loss=0.1319236308336258
I0217 15:50:43.610683 139812778530560 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.032977186143398285, loss=0.12067364156246185
I0217 15:51:59.837683 139812786923264 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.14176668226718903, loss=0.13593025505542755
I0217 15:52:01.144623 139973357201216 spec.py:321] Evaluating on the training split.
I0217 15:52:08.203282 139973357201216 spec.py:333] Evaluating on the validation split.
I0217 15:52:15.592496 139973357201216 spec.py:349] Evaluating on the test split.
I0217 15:52:24.712801 139973357201216 submission_runner.py:408] Time since start: 2613.69s, 	Step: 2903, 	{'train/loss': 0.12664193791226022, 'validation/loss': 0.1266030428628587, 'validation/num_examples': 83274637, 'test/loss': 0.12874441206825657, 'test/num_examples': 95000000, 'score': 2406.5499982833862, 'total_duration': 2613.691510438919, 'accumulated_submission_time': 2406.5499982833862, 'accumulated_eval_time': 207.00303983688354, 'accumulated_logging_time': 0.04346036911010742}
I0217 15:52:24.728606 139812778530560 logging_writer.py:48] [2903] accumulated_eval_time=207.003040, accumulated_logging_time=0.043460, accumulated_submission_time=2406.549998, global_step=2903, preemption_count=0, score=2406.549998, test/loss=0.128744, test/num_examples=95000000, total_duration=2613.691510, train/loss=0.126642, validation/loss=0.126603, validation/num_examples=83274637
I0217 15:54:01.702454 139812786923264 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.023513594642281532, loss=0.12245536595582962
I0217 15:56:20.453955 139812778530560 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.01055222563445568, loss=0.12366609275341034
I0217 15:57:37.837554 139812786923264 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.0156736820936203, loss=0.1180916428565979
I0217 15:58:53.992126 139812778530560 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.06736788898706436, loss=0.12010399252176285
I0217 16:00:09.892071 139812786923264 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.01963745430111885, loss=0.1171061098575592
I0217 16:01:22.898245 139812778530560 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.026441214606165886, loss=0.12128248065710068
I0217 16:02:38.713606 139812786923264 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.03645459562540054, loss=0.12004875391721725
I0217 16:03:54.809511 139812778530560 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.0202830508351326, loss=0.12850502133369446
I0217 16:05:10.891873 139812786923264 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.009850350208580494, loss=0.12691496312618256
I0217 16:06:26.712629 139812778530560 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.06523962318897247, loss=0.12142973393201828
I0217 16:07:42.869179 139812786923264 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.017974233254790306, loss=0.11903072893619537
I0217 16:08:59.391104 139812778530560 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.008326592855155468, loss=0.11969763040542603
I0217 16:10:15.469844 139812786923264 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.028637735173106194, loss=0.13343042135238647
I0217 16:11:31.385293 139812778530560 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.042858537286520004, loss=0.11929269134998322
I0217 16:12:25.200784 139973357201216 spec.py:321] Evaluating on the training split.
I0217 16:12:32.270542 139973357201216 spec.py:333] Evaluating on the validation split.
I0217 16:12:39.711218 139973357201216 spec.py:349] Evaluating on the test split.
I0217 16:12:48.061845 139973357201216 submission_runner.py:408] Time since start: 3837.04s, 	Step: 4372, 	{'train/loss': 0.12361618426610839, 'validation/loss': 0.12529285786994784, 'validation/num_examples': 83274637, 'test/loss': 0.12779231689967105, 'test/num_examples': 95000000, 'score': 3606.9658749103546, 'total_duration': 3837.0405600070953, 'accumulated_submission_time': 3606.9658749103546, 'accumulated_eval_time': 229.86406087875366, 'accumulated_logging_time': 0.06807160377502441}
I0217 16:12:48.076144 139812786923264 logging_writer.py:48] [4372] accumulated_eval_time=229.864061, accumulated_logging_time=0.068072, accumulated_submission_time=3606.965875, global_step=4372, preemption_count=0, score=3606.965875, test/loss=0.127792, test/num_examples=95000000, total_duration=3837.040560, train/loss=0.123616, validation/loss=0.125293, validation/num_examples=83274637
I0217 16:12:52.567874 139812778530560 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.0109790600836277, loss=0.12173028290271759
I0217 16:15:09.208576 139812786923264 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.007891062647104263, loss=0.11796699464321136
I0217 16:17:07.029606 139812778530560 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.005846582818776369, loss=0.11575361341238022
I0217 16:18:22.534642 139812786923264 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.035292550921440125, loss=0.11872264742851257
I0217 16:19:38.227509 139812778530560 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.019800491631031036, loss=0.12481357157230377
I0217 16:20:54.785839 139812786923264 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.035362061113119125, loss=0.12239018827676773
I0217 16:22:09.487217 139812778530560 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.00877565797418356, loss=0.1242363452911377
I0217 16:23:25.348383 139812786923264 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.017312368378043175, loss=0.11828873306512833
I0217 16:24:41.019771 139812778530560 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.03053399734199047, loss=0.11988267302513123
I0217 16:25:53.929682 139812786923264 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.02653747797012329, loss=0.12537020444869995
I0217 16:27:10.286108 139812778530560 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.009561846032738686, loss=0.12137673795223236
I0217 16:28:26.042456 139812786923264 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.014828515239059925, loss=0.12484588474035263
I0217 16:29:41.875587 139812778530560 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.010699857026338577, loss=0.1189211755990982
I0217 16:30:57.720937 139812786923264 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.017329592257738113, loss=0.12465612590312958
I0217 16:32:13.610445 139812778530560 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.015498794615268707, loss=0.11932544410228729
I0217 16:32:48.648467 139973357201216 spec.py:321] Evaluating on the training split.
I0217 16:32:56.122599 139973357201216 spec.py:333] Evaluating on the validation split.
I0217 16:33:03.456358 139973357201216 spec.py:349] Evaluating on the test split.
I0217 16:33:11.821752 139973357201216 submission_runner.py:408] Time since start: 5060.80s, 	Step: 5847, 	{'train/loss': 0.12341466925616534, 'validation/loss': 0.12422187193645468, 'validation/num_examples': 83274637, 'test/loss': 0.12660984350328947, 'test/num_examples': 95000000, 'score': 4807.4817843437195, 'total_duration': 5060.800463438034, 'accumulated_submission_time': 4807.4817843437195, 'accumulated_eval_time': 253.03732204437256, 'accumulated_logging_time': 0.0901024341583252}
I0217 16:33:11.837210 139812786923264 logging_writer.py:48] [5847] accumulated_eval_time=253.037322, accumulated_logging_time=0.090102, accumulated_submission_time=4807.481784, global_step=5847, preemption_count=0, score=4807.481784, test/loss=0.126610, test/num_examples=95000000, total_duration=5060.800463, train/loss=0.123415, validation/loss=0.124222, validation/num_examples=83274637
I0217 16:33:47.915668 139812778530560 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.018098322674632072, loss=0.12174201011657715
I0217 16:35:53.250101 139812786923264 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.04763041436672211, loss=0.12585321068763733
I0217 16:37:43.040395 139812778530560 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.005875857081264257, loss=0.12588681280612946
I0217 16:38:58.887409 139812786923264 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.016659913584589958, loss=0.12095709145069122
I0217 16:40:14.473039 139812778530560 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.0069722020998597145, loss=0.1269763857126236
I0217 16:41:30.320077 139812786923264 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.008517646230757236, loss=0.11868974566459656
I0217 16:42:46.192737 139812778530560 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.009840013459324837, loss=0.12189540266990662
I0217 16:44:02.112562 139812786923264 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.012757383286952972, loss=0.12390421330928802
I0217 16:45:18.417505 139812778530560 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.0077505591325461864, loss=0.1175580695271492
I0217 16:46:34.273721 139812786923264 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.01821736805140972, loss=0.11514545977115631
I0217 16:47:50.394047 139812778530560 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.01951122283935547, loss=0.12425682693719864
I0217 16:49:06.651422 139812786923264 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.023977311328053474, loss=0.12543635070323944
I0217 16:50:22.338505 139812778530560 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.007794370874762535, loss=0.12314555048942566
I0217 16:51:38.226820 139812786923264 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.006280555855482817, loss=0.11954500526189804
I0217 16:52:54.127842 139812778530560 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.01228142250329256, loss=0.12245545536279678
I0217 16:53:12.429471 139973357201216 spec.py:321] Evaluating on the training split.
I0217 16:53:19.775118 139973357201216 spec.py:333] Evaluating on the validation split.
I0217 16:53:27.086546 139973357201216 spec.py:349] Evaluating on the test split.
I0217 16:53:36.063178 139973357201216 submission_runner.py:408] Time since start: 6285.04s, 	Step: 7325, 	{'train/loss': 0.12190544117921553, 'validation/loss': 0.12413563774375865, 'validation/num_examples': 83274637, 'test/loss': 0.12656714360608554, 'test/num_examples': 95000000, 'score': 6008.018783092499, 'total_duration': 6285.041886091232, 'accumulated_submission_time': 6008.018783092499, 'accumulated_eval_time': 276.6709907054901, 'accumulated_logging_time': 0.11337637901306152}
I0217 16:53:36.080824 139812786923264 logging_writer.py:48] [7325] accumulated_eval_time=276.670991, accumulated_logging_time=0.113376, accumulated_submission_time=6008.018783, global_step=7325, preemption_count=0, score=6008.018783, test/loss=0.126567, test/num_examples=95000000, total_duration=6285.041886, train/loss=0.121905, validation/loss=0.124136, validation/num_examples=83274637
I0217 16:54:47.640670 139812778530560 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.019129665568470955, loss=0.12135615199804306
I0217 16:56:51.923636 139812786923264 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.010784853249788284, loss=0.12741222977638245
I0217 16:58:26.356536 139812778530560 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.008081018924713135, loss=0.11841239780187607
I0217 16:59:42.078876 139812786923264 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.014115081168711185, loss=0.12963014841079712
I0217 17:00:57.668611 139812778530560 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.013089506886899471, loss=0.12927033007144928
I0217 17:02:13.784420 139812786923264 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.028797077015042305, loss=0.11962778866291046
I0217 17:03:30.242500 139812778530560 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.00799535121768713, loss=0.12213321030139923
I0217 17:04:46.383615 139812786923264 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.006282217334955931, loss=0.1168375238776207
I0217 17:06:02.379421 139812778530560 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.0065623316913843155, loss=0.11460176110267639
I0217 17:07:18.070089 139812786923264 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.00609147734940052, loss=0.1216382309794426
I0217 17:08:33.925455 139812778530560 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.008659464307129383, loss=0.12170176208019257
I0217 17:09:49.741816 139812786923264 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.00809907540678978, loss=0.1184816062450409
I0217 17:11:05.274102 139812778530560 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.008841545321047306, loss=0.11905546486377716
I0217 17:12:21.500969 139812786923264 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.00626567704603076, loss=0.12477806955575943
I0217 17:13:36.508300 139973357201216 spec.py:321] Evaluating on the training split.
I0217 17:13:43.611758 139973357201216 spec.py:333] Evaluating on the validation split.
I0217 17:13:50.985838 139973357201216 spec.py:349] Evaluating on the test split.
I0217 17:14:00.021087 139973357201216 submission_runner.py:408] Time since start: 7509.00s, 	Step: 8800, 	{'train/loss': 0.1216050774421332, 'validation/loss': 0.12398396416156097, 'validation/num_examples': 83274637, 'test/loss': 0.12635513429276315, 'test/num_examples': 95000000, 'score': 7208.39083981514, 'total_duration': 7508.999789476395, 'accumulated_submission_time': 7208.39083981514, 'accumulated_eval_time': 300.1837706565857, 'accumulated_logging_time': 0.13881635665893555}
I0217 17:14:00.036544 139812778530560 logging_writer.py:48] [8800] accumulated_eval_time=300.183771, accumulated_logging_time=0.138816, accumulated_submission_time=7208.390840, global_step=8800, preemption_count=0, score=7208.390840, test/loss=0.126355, test/num_examples=95000000, total_duration=7508.999789, train/loss=0.121605, validation/loss=0.123984, validation/num_examples=83274637
I0217 17:14:00.146064 139812786923264 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.010197923518717289, loss=0.12692174315452576
I0217 17:15:29.916352 139812778530560 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.006978978402912617, loss=0.11799255013465881
I0217 17:17:38.150398 139812786923264 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.010040322318673134, loss=0.1224779337644577
I0217 17:19:05.009866 139812778530560 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.007137315813452005, loss=0.12703853845596313
I0217 17:20:20.458090 139812786923264 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.0070422631688416, loss=0.12285716086626053
I0217 17:21:35.904402 139812778530560 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.00968695618212223, loss=0.12391963601112366
I0217 17:22:15.485037 139812786923264 logging_writer.py:48] [9353] global_step=9353, preemption_count=0, score=7703.805795
I0217 17:22:21.899791 139973357201216 checkpoints.py:490] Saving checkpoint at step: 9353
I0217 17:22:57.045481 139973357201216 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_3/criteo1tb_jax/trial_2/checkpoint_9353
I0217 17:22:57.399356 139973357201216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_3/criteo1tb_jax/trial_2/checkpoint_9353.
I0217 17:22:57.966764 139973357201216 submission_runner.py:583] Tuning trial 2/5
I0217 17:22:57.967005 139973357201216 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.2, learning_rate=0.0008445074561975979, one_minus_beta1=0.11042418465, beta2=0.9978504782314613, weight_decay=0.08135402759553023, warmup_factor=0.05)
I0217 17:22:57.967791 139973357201216 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/loss': 1.5198267240944148, 'validation/loss': 1.5212185261462023, 'validation/num_examples': 83274637, 'test/loss': 1.5194805184210527, 'test/num_examples': 95000000, 'score': 5.877012491226196, 'total_duration': 165.06203317642212, 'accumulated_submission_time': 5.877012491226196, 'accumulated_eval_time': 159.1849820613861, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1441, {'train/loss': 0.12354151768691884, 'validation/loss': 0.12716335743003598, 'validation/num_examples': 83274637, 'test/loss': 0.12938557531866776, 'test/num_examples': 95000000, 'score': 1206.2299313545227, 'total_duration': 1389.7307302951813, 'accumulated_submission_time': 1206.2299313545227, 'accumulated_eval_time': 183.43490958213806, 'accumulated_logging_time': 0.018738269805908203, 'global_step': 1441, 'preemption_count': 0}), (2903, {'train/loss': 0.12664193791226022, 'validation/loss': 0.1266030428628587, 'validation/num_examples': 83274637, 'test/loss': 0.12874441206825657, 'test/num_examples': 95000000, 'score': 2406.5499982833862, 'total_duration': 2613.691510438919, 'accumulated_submission_time': 2406.5499982833862, 'accumulated_eval_time': 207.00303983688354, 'accumulated_logging_time': 0.04346036911010742, 'global_step': 2903, 'preemption_count': 0}), (4372, {'train/loss': 0.12361618426610839, 'validation/loss': 0.12529285786994784, 'validation/num_examples': 83274637, 'test/loss': 0.12779231689967105, 'test/num_examples': 95000000, 'score': 3606.9658749103546, 'total_duration': 3837.0405600070953, 'accumulated_submission_time': 3606.9658749103546, 'accumulated_eval_time': 229.86406087875366, 'accumulated_logging_time': 0.06807160377502441, 'global_step': 4372, 'preemption_count': 0}), (5847, {'train/loss': 0.12341466925616534, 'validation/loss': 0.12422187193645468, 'validation/num_examples': 83274637, 'test/loss': 0.12660984350328947, 'test/num_examples': 95000000, 'score': 4807.4817843437195, 'total_duration': 5060.800463438034, 'accumulated_submission_time': 4807.4817843437195, 'accumulated_eval_time': 253.03732204437256, 'accumulated_logging_time': 0.0901024341583252, 'global_step': 5847, 'preemption_count': 0}), (7325, {'train/loss': 0.12190544117921553, 'validation/loss': 0.12413563774375865, 'validation/num_examples': 83274637, 'test/loss': 0.12656714360608554, 'test/num_examples': 95000000, 'score': 6008.018783092499, 'total_duration': 6285.041886091232, 'accumulated_submission_time': 6008.018783092499, 'accumulated_eval_time': 276.6709907054901, 'accumulated_logging_time': 0.11337637901306152, 'global_step': 7325, 'preemption_count': 0}), (8800, {'train/loss': 0.1216050774421332, 'validation/loss': 0.12398396416156097, 'validation/num_examples': 83274637, 'test/loss': 0.12635513429276315, 'test/num_examples': 95000000, 'score': 7208.39083981514, 'total_duration': 7508.999789476395, 'accumulated_submission_time': 7208.39083981514, 'accumulated_eval_time': 300.1837706565857, 'accumulated_logging_time': 0.13881635665893555, 'global_step': 8800, 'preemption_count': 0})], 'global_step': 9353}
I0217 17:22:57.967911 139973357201216 submission_runner.py:586] Timing: 7703.8057949543
I0217 17:22:57.967982 139973357201216 submission_runner.py:588] Total number of evals: 7
I0217 17:22:57.968045 139973357201216 submission_runner.py:589] ====================
I0217 17:22:57.968112 139973357201216 submission_runner.py:542] Using RNG seed 2861073867
I0217 17:22:57.969804 139973357201216 submission_runner.py:551] --- Tuning run 3/5 ---
I0217 17:22:57.969910 139973357201216 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_3/criteo1tb_jax/trial_3.
I0217 17:22:57.972716 139973357201216 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_3/criteo1tb_jax/trial_3/hparams.json.
I0217 17:22:57.974466 139973357201216 submission_runner.py:206] Initializing dataset.
I0217 17:22:57.974584 139973357201216 submission_runner.py:213] Initializing model.
I0217 17:23:01.242603 139973357201216 submission_runner.py:255] Initializing optimizer.
I0217 17:23:03.989959 139973357201216 submission_runner.py:262] Initializing metrics bundle.
I0217 17:23:03.990182 139973357201216 submission_runner.py:280] Initializing checkpoint and logger.
I0217 17:23:04.098864 139973357201216 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_3/criteo1tb_jax/trial_3 with prefix checkpoint_
I0217 17:23:04.099029 139973357201216 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_3/criteo1tb_jax/trial_3/meta_data_0.json.
I0217 17:23:04.099302 139973357201216 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0217 17:23:04.099379 139973357201216 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0217 17:23:14.623177 139973357201216 logger_utils.py:220] Unable to record git information. Continuing without it.
I0217 17:23:24.885298 139973357201216 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_3/criteo1tb_jax/trial_3/flags_0.json.
I0217 17:23:24.897495 139973357201216 submission_runner.py:314] Starting training loop.
I0217 17:23:30.351397 139812770137856 logging_writer.py:48] [0] global_step=0, grad_norm=8.940546035766602, loss=1.5185662508010864
I0217 17:23:30.356318 139973357201216 spec.py:321] Evaluating on the training split.
I0217 17:23:37.592111 139973357201216 spec.py:333] Evaluating on the validation split.
I0217 17:23:44.796100 139973357201216 spec.py:349] Evaluating on the test split.
I0217 17:23:53.830461 139973357201216 submission_runner.py:408] Time since start: 28.93s, 	Step: 1, 	{'train/loss': 1.5213117389558997, 'validation/loss': 1.5212185261462023, 'validation/num_examples': 83274637, 'test/loss': 1.5194805184210527, 'test/num_examples': 95000000, 'score': 5.458779335021973, 'total_duration': 28.932892322540283, 'accumulated_submission_time': 5.458779335021973, 'accumulated_eval_time': 23.474072694778442, 'accumulated_logging_time': 0}
I0217 17:23:53.840965 139812778530560 logging_writer.py:48] [1] accumulated_eval_time=23.474073, accumulated_logging_time=0, accumulated_submission_time=5.458779, global_step=1, preemption_count=0, score=5.458779, test/loss=1.519481, test/num_examples=95000000, total_duration=28.932892, train/loss=1.521312, validation/loss=1.521219, validation/num_examples=83274637
I0217 17:25:38.141467 139812770137856 logging_writer.py:48] [100] global_step=100, grad_norm=0.4242366552352905, loss=0.16428327560424805
I0217 17:27:52.704276 139812778530560 logging_writer.py:48] [200] global_step=200, grad_norm=0.029830092564225197, loss=0.12725870311260223
I0217 17:29:14.699093 139812770137856 logging_writer.py:48] [300] global_step=300, grad_norm=0.03885175287723541, loss=0.12128368765115738
I0217 17:30:29.175460 139812778530560 logging_writer.py:48] [400] global_step=400, grad_norm=0.011392353102564812, loss=0.12422066926956177
I0217 17:31:40.942352 139812770137856 logging_writer.py:48] [500] global_step=500, grad_norm=0.018292004242539406, loss=0.12449876964092255
I0217 17:32:53.842730 139812778530560 logging_writer.py:48] [600] global_step=600, grad_norm=0.053326565772295, loss=0.12571418285369873
I0217 17:34:07.612548 139812770137856 logging_writer.py:48] [700] global_step=700, grad_norm=0.09233309328556061, loss=0.13371069729328156
I0217 17:35:19.800956 139812778530560 logging_writer.py:48] [800] global_step=800, grad_norm=0.017438653856515884, loss=0.12779508531093597
I0217 17:36:34.143543 139812770137856 logging_writer.py:48] [900] global_step=900, grad_norm=0.03641838952898979, loss=0.1311493217945099
I0217 17:37:49.133401 139812778530560 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.008883786387741566, loss=0.12612049281597137
I0217 17:39:04.071629 139812770137856 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.021566646173596382, loss=0.11891539394855499
I0217 17:40:16.492802 139812778530560 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.015532701276242733, loss=0.13124029338359833
I0217 17:41:30.877183 139812770137856 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.049245767295360565, loss=0.1284211426973343
I0217 17:42:45.643180 139812778530560 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.030681846663355827, loss=0.12506727874279022
I0217 17:43:53.876263 139973357201216 spec.py:321] Evaluating on the training split.
I0217 17:44:00.994790 139973357201216 spec.py:333] Evaluating on the validation split.
I0217 17:44:08.252149 139973357201216 spec.py:349] Evaluating on the test split.
I0217 17:44:16.603531 139973357201216 submission_runner.py:408] Time since start: 1251.71s, 	Step: 1492, 	{'train/loss': 0.12461108142662349, 'validation/loss': 0.12592963338660365, 'validation/num_examples': 83274637, 'test/loss': 0.12824286206825658, 'test/num_examples': 95000000, 'score': 1205.438355922699, 'total_duration': 1251.7059638500214, 'accumulated_submission_time': 1205.438355922699, 'accumulated_eval_time': 46.20129990577698, 'accumulated_logging_time': 0.018410205841064453}
I0217 17:44:16.620625 139812770137856 logging_writer.py:48] [1492] accumulated_eval_time=46.201300, accumulated_logging_time=0.018410, accumulated_submission_time=1205.438356, global_step=1492, preemption_count=0, score=1205.438356, test/loss=0.128243, test/num_examples=95000000, total_duration=1251.705964, train/loss=0.124611, validation/loss=0.125930, validation/num_examples=83274637
I0217 17:44:17.495044 139812778530560 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.03881538659334183, loss=0.12918521463871002
I0217 17:46:15.900358 139812770137856 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.007928084582090378, loss=0.1250467151403427
I0217 17:48:31.536938 139812778530560 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.0259332787245512, loss=0.12186090648174286
I0217 17:49:48.997007 139812770137856 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.008465161547064781, loss=0.12423473596572876
I0217 17:51:04.255242 139812778530560 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.01019495539367199, loss=0.12743565440177917
I0217 17:52:18.764150 139812770137856 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.013820966705679893, loss=0.12577354907989502
I0217 17:53:34.182509 139812778530560 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.03707024082541466, loss=0.12323367595672607
I0217 17:54:49.182796 139812770137856 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.011483324691653252, loss=0.12168201804161072
I0217 17:56:01.838642 139812778530560 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.007753395475447178, loss=0.11896754056215286
I0217 17:57:16.601780 139812770137856 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.010134890675544739, loss=0.12401748448610306
I0217 17:58:31.272903 139812778530560 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.02828887477517128, loss=0.12432000041007996
I0217 17:59:45.792541 139812770137856 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.03289257362484932, loss=0.11870153248310089
I0217 18:01:00.387166 139812778530560 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.02505546435713768, loss=0.12450224906206131
I0217 18:02:15.706075 139812770137856 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.03006814606487751, loss=0.1370367556810379
I0217 18:03:29.951243 139812778530560 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.01636437512934208, loss=0.11324992030858994
I0217 18:04:16.962566 139973357201216 spec.py:321] Evaluating on the training split.
I0217 18:04:24.122664 139973357201216 spec.py:333] Evaluating on the validation split.
I0217 18:04:31.300945 139973357201216 spec.py:349] Evaluating on the test split.
I0217 18:04:39.673993 139973357201216 submission_runner.py:408] Time since start: 2474.78s, 	Step: 2968, 	{'train/loss': 0.12178133554616065, 'validation/loss': 0.1253189053416408, 'validation/num_examples': 83274637, 'test/loss': 0.12775123813733552, 'test/num_examples': 95000000, 'score': 2405.723908662796, 'total_duration': 2474.7764031887054, 'accumulated_submission_time': 2405.723908662796, 'accumulated_eval_time': 68.91265916824341, 'accumulated_logging_time': 0.044144630432128906}
I0217 18:04:39.702499 139812770137856 logging_writer.py:48] [2968] accumulated_eval_time=68.912659, accumulated_logging_time=0.044145, accumulated_submission_time=2405.723909, global_step=2968, preemption_count=0, score=2405.723909, test/loss=0.127751, test/num_examples=95000000, total_duration=2474.776403, train/loss=0.121781, validation/loss=0.125319, validation/num_examples=83274637
I0217 18:04:57.617212 139812778530560 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.0402737595140934, loss=0.12175454199314117
I0217 18:07:05.044106 139812770137856 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.017414184287190437, loss=0.12067248672246933
I0217 18:09:08.427594 139812778530560 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.025671284645795822, loss=0.11864370107650757
I0217 18:10:23.593002 139812770137856 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.03332933783531189, loss=0.1213628500699997
I0217 18:11:38.145688 139812778530560 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.011007661931216717, loss=0.11594241857528687
I0217 18:12:53.123025 139812770137856 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.0399576835334301, loss=0.11889977008104324
I0217 18:14:08.204293 139812778530560 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.010171147994697094, loss=0.12036585807800293
I0217 18:15:20.345327 139812770137856 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.008768076077103615, loss=0.12586772441864014
I0217 18:16:35.259433 139812778530560 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.009480739012360573, loss=0.11977208405733109
I0217 18:17:50.080469 139812770137856 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.01897668093442917, loss=0.12818853557109833
I0217 18:19:02.223707 139812778530560 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.007364614401012659, loss=0.13338081538677216
I0217 18:20:16.593513 139812770137856 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.02181641384959221, loss=0.12161430716514587
I0217 18:21:31.813261 139812778530560 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.024112196639180183, loss=0.11944002658128738
I0217 18:22:46.993547 139812770137856 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.0070140305906534195, loss=0.12486236542463303
I0217 18:24:01.972486 139812778530560 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.006704575382173061, loss=0.11568032950162888
I0217 18:24:40.135552 139973357201216 spec.py:321] Evaluating on the training split.
I0217 18:24:47.473318 139973357201216 spec.py:333] Evaluating on the validation split.
I0217 18:24:54.675843 139973357201216 spec.py:349] Evaluating on the test split.
I0217 18:25:03.084252 139973357201216 submission_runner.py:408] Time since start: 3698.19s, 	Step: 4452, 	{'train/loss': 0.12527741538653583, 'validation/loss': 0.12460799377869099, 'validation/num_examples': 83274637, 'test/loss': 0.12685656526521383, 'test/num_examples': 95000000, 'score': 3606.100022315979, 'total_duration': 3698.1866977214813, 'accumulated_submission_time': 3606.100022315979, 'accumulated_eval_time': 91.86134266853333, 'accumulated_logging_time': 0.08154058456420898}
I0217 18:25:03.098650 139812770137856 logging_writer.py:48] [4452] accumulated_eval_time=91.861343, accumulated_logging_time=0.081541, accumulated_submission_time=3606.100022, global_step=4452, preemption_count=0, score=3606.100022, test/loss=0.126857, test/num_examples=95000000, total_duration=3698.186698, train/loss=0.125277, validation/loss=0.124608, validation/num_examples=83274637
I0217 18:25:44.047004 139812778530560 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.007038382813334465, loss=0.1245625838637352
I0217 18:27:42.920017 139812770137856 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.02063826471567154, loss=0.1248733177781105
I0217 18:29:43.001624 139812778530560 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.02035103738307953, loss=0.12251453101634979
I0217 18:30:56.654939 139812770137856 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.023124374449253082, loss=0.12250567972660065
I0217 18:32:11.224586 139812778530560 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.014623507857322693, loss=0.11729619652032852
I0217 18:33:26.389367 139812770137856 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.007075262255966663, loss=0.13285645842552185
I0217 18:34:41.549488 139812778530560 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.025643011555075645, loss=0.12028952687978745
I0217 18:35:56.649305 139812770137856 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.022638633847236633, loss=0.11903548240661621
I0217 18:37:12.124749 139812778530560 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.006300221662968397, loss=0.11689680069684982
I0217 18:38:26.643877 139812770137856 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.02413705550134182, loss=0.12164641916751862
I0217 18:39:41.728487 139812778530560 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.01346572395414114, loss=0.12345033138990402
I0217 18:40:56.580554 139812770137856 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.02675842121243477, loss=0.12302746623754501
I0217 18:42:11.803317 139812778530560 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.013569682836532593, loss=0.11983954906463623
I0217 18:43:27.214172 139812770137856 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.02256394363939762, loss=0.11549733579158783
I0217 18:44:42.795841 139812778530560 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.011300475336611271, loss=0.1255761682987213
I0217 18:45:03.712372 139973357201216 spec.py:321] Evaluating on the training split.
I0217 18:45:10.829675 139973357201216 spec.py:333] Evaluating on the validation split.
I0217 18:45:17.998234 139973357201216 spec.py:349] Evaluating on the test split.
I0217 18:45:26.375817 139973357201216 submission_runner.py:408] Time since start: 4921.48s, 	Step: 5929, 	{'train/loss': 0.12275578189383513, 'validation/loss': 0.12445131989324973, 'validation/num_examples': 83274637, 'test/loss': 0.12674955613692435, 'test/num_examples': 95000000, 'score': 4806.658087968826, 'total_duration': 4921.478246450424, 'accumulated_submission_time': 4806.658087968826, 'accumulated_eval_time': 114.52473449707031, 'accumulated_logging_time': 0.1039731502532959}
I0217 18:45:26.392000 139812770137856 logging_writer.py:48] [5929] accumulated_eval_time=114.524734, accumulated_logging_time=0.103973, accumulated_submission_time=4806.658088, global_step=5929, preemption_count=0, score=4806.658088, test/loss=0.126750, test/num_examples=95000000, total_duration=4921.478246, train/loss=0.122756, validation/loss=0.124451, validation/num_examples=83274637
I0217 18:46:34.740366 139812778530560 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.01910165324807167, loss=0.12289607524871826
I0217 18:48:58.419574 139812770137856 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.01879304274916649, loss=0.13892553746700287
I0217 18:50:36.015130 139812778530560 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.01611940748989582, loss=0.1251544952392578
I0217 18:51:51.292136 139812770137856 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.006764991674572229, loss=0.12518830597400665
I0217 18:53:06.503052 139812778530560 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.018077058717608452, loss=0.12586332857608795
I0217 18:54:18.929734 139812770137856 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.007205676287412643, loss=0.1278165578842163
I0217 18:55:33.833658 139812778530560 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.02192527800798416, loss=0.12243565171957016
I0217 18:56:49.457779 139812770137856 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.008675362914800644, loss=0.1213490217924118
I0217 18:58:04.766963 139812778530560 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.011822451837360859, loss=0.121698297560215
I0217 18:59:19.938819 139812770137856 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.010866140946745872, loss=0.12149359285831451
I0217 19:00:34.822626 139812778530560 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.009062803350389004, loss=0.11821965873241425
I0217 19:01:49.896934 139812770137856 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.008510245941579342, loss=0.11680033802986145
I0217 19:03:05.035187 139812778530560 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.009611678309738636, loss=0.12401829659938812
I0217 19:04:19.985353 139812770137856 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.010121631436049938, loss=0.13322746753692627
I0217 19:05:26.389184 139973357201216 spec.py:321] Evaluating on the training split.
I0217 19:05:33.571611 139973357201216 spec.py:333] Evaluating on the validation split.
I0217 19:05:40.762613 139973357201216 spec.py:349] Evaluating on the test split.
I0217 19:05:49.134521 139973357201216 submission_runner.py:408] Time since start: 6144.24s, 	Step: 7390, 	{'train/loss': 0.12213283214928969, 'validation/loss': 0.12411537440865698, 'validation/num_examples': 83274637, 'test/loss': 0.12649275671258223, 'test/num_examples': 95000000, 'score': 6006.600213289261, 'total_duration': 6144.236940383911, 'accumulated_submission_time': 6006.600213289261, 'accumulated_eval_time': 137.27002453804016, 'accumulated_logging_time': 0.12862515449523926}
I0217 19:05:49.150746 139812778530560 logging_writer.py:48] [7390] accumulated_eval_time=137.270025, accumulated_logging_time=0.128625, accumulated_submission_time=6006.600213, global_step=7390, preemption_count=0, score=6006.600213, test/loss=0.126493, test/num_examples=95000000, total_duration=6144.236940, train/loss=0.122133, validation/loss=0.124115, validation/num_examples=83274637
I0217 19:05:50.212273 139812770137856 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.007415843661874533, loss=0.12179397791624069
I0217 19:07:49.870394 139812778530560 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.008030055090785027, loss=0.12801140546798706
I0217 19:10:09.627555 139812770137856 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.010911648161709309, loss=0.13463839888572693
I0217 19:11:24.868867 139812778530560 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.008433843962848186, loss=0.1268806755542755
I0217 19:12:39.327779 139812770137856 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.007538095116615295, loss=0.1226608157157898
I0217 19:13:53.805256 139812778530560 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.010470951907336712, loss=0.12239015847444534
I0217 19:15:08.752308 139812770137856 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.016902320086956024, loss=0.1272176206111908
I0217 19:16:23.654240 139812778530560 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.008596472442150116, loss=0.12176261842250824
I0217 19:17:38.550502 139812770137856 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.01239071786403656, loss=0.13535866141319275
I0217 19:18:53.601308 139812778530560 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.00967757310718298, loss=0.11919251829385757
I0217 19:20:08.397737 139812770137856 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.007954488508403301, loss=0.11925680935382843
I0217 19:21:23.889281 139812778530560 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.006963340565562248, loss=0.1184040755033493
I0217 19:22:38.491274 139812770137856 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.007858113385736942, loss=0.11890869587659836
I0217 19:23:53.554497 139812778530560 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.007444149814546108, loss=0.1150042787194252
I0217 19:25:08.306906 139812770137856 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.01473237480968237, loss=0.11672699451446533
I0217 19:25:49.246726 139973357201216 spec.py:321] Evaluating on the training split.
I0217 19:25:56.459793 139973357201216 spec.py:333] Evaluating on the validation split.
I0217 19:26:03.678507 139973357201216 spec.py:349] Evaluating on the test split.
I0217 19:26:15.144968 139973357201216 submission_runner.py:408] Time since start: 7370.25s, 	Step: 8856, 	{'train/loss': 0.12047344727336236, 'validation/loss': 0.12387181979700494, 'validation/num_examples': 83274637, 'test/loss': 0.12617349113898027, 'test/num_examples': 95000000, 'score': 7206.640828609467, 'total_duration': 7370.247405529022, 'accumulated_submission_time': 7206.640828609467, 'accumulated_eval_time': 163.1682367324829, 'accumulated_logging_time': 0.1530437469482422}
I0217 19:26:15.164944 139812778530560 logging_writer.py:48] [8856] accumulated_eval_time=163.168237, accumulated_logging_time=0.153044, accumulated_submission_time=7206.640829, global_step=8856, preemption_count=0, score=7206.640829, test/loss=0.126173, test/num_examples=95000000, total_duration=7370.247406, train/loss=0.120473, validation/loss=0.123872, validation/num_examples=83274637
I0217 19:26:47.179397 139812770137856 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.008766532875597477, loss=0.12900188565254211
I0217 19:29:01.701298 139812778530560 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.008402598090469837, loss=0.12663768231868744
I0217 19:30:50.783205 139812770137856 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.026436803862452507, loss=0.12884023785591125
I0217 19:32:05.971113 139812778530560 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.02057640813291073, loss=0.12263823300600052
I0217 19:33:21.503302 139812770137856 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.006163580343127251, loss=0.11844325065612793
I0217 19:34:31.957296 139812778530560 logging_writer.py:48] [9395] global_step=9395, preemption_count=0, score=7703.400138
I0217 19:34:38.417214 139973357201216 checkpoints.py:490] Saving checkpoint at step: 9395
I0217 19:35:15.364780 139973357201216 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_3/criteo1tb_jax/trial_3/checkpoint_9395
I0217 19:35:15.808309 139973357201216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_3/criteo1tb_jax/trial_3/checkpoint_9395.
I0217 19:35:16.293876 139973357201216 submission_runner.py:583] Tuning trial 3/5
I0217 19:35:16.294107 139973357201216 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0217 19:35:16.295994 139973357201216 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/loss': 1.5213117389558997, 'validation/loss': 1.5212185261462023, 'validation/num_examples': 83274637, 'test/loss': 1.5194805184210527, 'test/num_examples': 95000000, 'score': 5.458779335021973, 'total_duration': 28.932892322540283, 'accumulated_submission_time': 5.458779335021973, 'accumulated_eval_time': 23.474072694778442, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1492, {'train/loss': 0.12461108142662349, 'validation/loss': 0.12592963338660365, 'validation/num_examples': 83274637, 'test/loss': 0.12824286206825658, 'test/num_examples': 95000000, 'score': 1205.438355922699, 'total_duration': 1251.7059638500214, 'accumulated_submission_time': 1205.438355922699, 'accumulated_eval_time': 46.20129990577698, 'accumulated_logging_time': 0.018410205841064453, 'global_step': 1492, 'preemption_count': 0}), (2968, {'train/loss': 0.12178133554616065, 'validation/loss': 0.1253189053416408, 'validation/num_examples': 83274637, 'test/loss': 0.12775123813733552, 'test/num_examples': 95000000, 'score': 2405.723908662796, 'total_duration': 2474.7764031887054, 'accumulated_submission_time': 2405.723908662796, 'accumulated_eval_time': 68.91265916824341, 'accumulated_logging_time': 0.044144630432128906, 'global_step': 2968, 'preemption_count': 0}), (4452, {'train/loss': 0.12527741538653583, 'validation/loss': 0.12460799377869099, 'validation/num_examples': 83274637, 'test/loss': 0.12685656526521383, 'test/num_examples': 95000000, 'score': 3606.100022315979, 'total_duration': 3698.1866977214813, 'accumulated_submission_time': 3606.100022315979, 'accumulated_eval_time': 91.86134266853333, 'accumulated_logging_time': 0.08154058456420898, 'global_step': 4452, 'preemption_count': 0}), (5929, {'train/loss': 0.12275578189383513, 'validation/loss': 0.12445131989324973, 'validation/num_examples': 83274637, 'test/loss': 0.12674955613692435, 'test/num_examples': 95000000, 'score': 4806.658087968826, 'total_duration': 4921.478246450424, 'accumulated_submission_time': 4806.658087968826, 'accumulated_eval_time': 114.52473449707031, 'accumulated_logging_time': 0.1039731502532959, 'global_step': 5929, 'preemption_count': 0}), (7390, {'train/loss': 0.12213283214928969, 'validation/loss': 0.12411537440865698, 'validation/num_examples': 83274637, 'test/loss': 0.12649275671258223, 'test/num_examples': 95000000, 'score': 6006.600213289261, 'total_duration': 6144.236940383911, 'accumulated_submission_time': 6006.600213289261, 'accumulated_eval_time': 137.27002453804016, 'accumulated_logging_time': 0.12862515449523926, 'global_step': 7390, 'preemption_count': 0}), (8856, {'train/loss': 0.12047344727336236, 'validation/loss': 0.12387181979700494, 'validation/num_examples': 83274637, 'test/loss': 0.12617349113898027, 'test/num_examples': 95000000, 'score': 7206.640828609467, 'total_duration': 7370.247405529022, 'accumulated_submission_time': 7206.640828609467, 'accumulated_eval_time': 163.1682367324829, 'accumulated_logging_time': 0.1530437469482422, 'global_step': 8856, 'preemption_count': 0})], 'global_step': 9395}
I0217 19:35:16.296122 139973357201216 submission_runner.py:586] Timing: 7703.400137901306
I0217 19:35:16.296178 139973357201216 submission_runner.py:588] Total number of evals: 7
I0217 19:35:16.296232 139973357201216 submission_runner.py:589] ====================
I0217 19:35:16.296291 139973357201216 submission_runner.py:542] Using RNG seed 2861073867
I0217 19:35:16.298008 139973357201216 submission_runner.py:551] --- Tuning run 4/5 ---
I0217 19:35:16.298110 139973357201216 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_3/criteo1tb_jax/trial_4.
I0217 19:35:16.301223 139973357201216 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_3/criteo1tb_jax/trial_4/hparams.json.
I0217 19:35:16.302034 139973357201216 submission_runner.py:206] Initializing dataset.
I0217 19:35:16.302150 139973357201216 submission_runner.py:213] Initializing model.
I0217 19:35:18.922604 139973357201216 submission_runner.py:255] Initializing optimizer.
I0217 19:35:21.667489 139973357201216 submission_runner.py:262] Initializing metrics bundle.
I0217 19:35:21.667685 139973357201216 submission_runner.py:280] Initializing checkpoint and logger.
I0217 19:35:21.764657 139973357201216 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_3/criteo1tb_jax/trial_4 with prefix checkpoint_
I0217 19:35:21.764813 139973357201216 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_3/criteo1tb_jax/trial_4/meta_data_0.json.
I0217 19:35:21.765057 139973357201216 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0217 19:35:21.765122 139973357201216 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0217 19:35:32.440260 139973357201216 logger_utils.py:220] Unable to record git information. Continuing without it.
I0217 19:35:42.866209 139973357201216 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_3/criteo1tb_jax/trial_4/flags_0.json.
I0217 19:35:42.874817 139973357201216 submission_runner.py:314] Starting training loop.
I0217 19:35:52.956704 139812786923264 logging_writer.py:48] [0] global_step=0, grad_norm=8.981046676635742, loss=1.522173523902893
I0217 19:35:52.961437 139973357201216 spec.py:321] Evaluating on the training split.
I0217 19:36:00.228520 139973357201216 spec.py:333] Evaluating on the validation split.
I0217 19:36:07.654476 139973357201216 spec.py:349] Evaluating on the test split.
I0217 19:36:16.444684 139973357201216 submission_runner.py:408] Time since start: 33.57s, 	Step: 1, 	{'train/loss': 1.5201884942984432, 'validation/loss': 1.5212185261462023, 'validation/num_examples': 83274637, 'test/loss': 1.5194805184210527, 'test/num_examples': 95000000, 'score': 10.086588859558105, 'total_duration': 33.56980895996094, 'accumulated_submission_time': 10.086588859558105, 'accumulated_eval_time': 23.48318362236023, 'accumulated_logging_time': 0}
I0217 19:36:16.454149 139812795315968 logging_writer.py:48] [1] accumulated_eval_time=23.483184, accumulated_logging_time=0, accumulated_submission_time=10.086589, global_step=1, preemption_count=0, score=10.086589, test/loss=1.519481, test/num_examples=95000000, total_duration=33.569809, train/loss=1.520188, validation/loss=1.521219, validation/num_examples=83274637
I0217 19:37:18.360711 139812786923264 logging_writer.py:48] [100] global_step=100, grad_norm=0.23051877319812775, loss=0.13221493363380432
I0217 19:38:39.846701 139812795315968 logging_writer.py:48] [200] global_step=200, grad_norm=0.14136242866516113, loss=0.13516688346862793
I0217 19:40:01.405720 139812786923264 logging_writer.py:48] [300] global_step=300, grad_norm=0.09770084172487259, loss=0.13236205279827118
I0217 19:41:19.465276 139812795315968 logging_writer.py:48] [400] global_step=400, grad_norm=0.04879077896475792, loss=0.13272418081760406
I0217 19:42:35.863003 139812786923264 logging_writer.py:48] [500] global_step=500, grad_norm=0.0176385585218668, loss=0.12692409753799438
I0217 19:43:52.156083 139812795315968 logging_writer.py:48] [600] global_step=600, grad_norm=0.012218684889376163, loss=0.12089262157678604
I0217 19:45:08.620007 139812786923264 logging_writer.py:48] [700] global_step=700, grad_norm=0.01937883161008358, loss=0.1281222403049469
I0217 19:46:24.800468 139812795315968 logging_writer.py:48] [800] global_step=800, grad_norm=0.005252414382994175, loss=0.1320396512746811
I0217 19:47:41.029186 139812786923264 logging_writer.py:48] [900] global_step=900, grad_norm=0.033324241638183594, loss=0.12886987626552582
I0217 19:48:57.147094 139812795315968 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.03413333743810654, loss=0.12858089804649353
I0217 19:50:13.170047 139812786923264 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.0341259241104126, loss=0.1401510238647461
I0217 19:51:28.866305 139812795315968 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.054283805191516876, loss=0.12508049607276917
I0217 19:52:44.870928 139812786923264 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.04084177687764168, loss=0.1228979229927063
I0217 19:54:01.654473 139812795315968 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.006222772412002087, loss=0.12123747915029526
I0217 19:55:18.045164 139812786923264 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.018354255706071854, loss=0.12275437265634537
I0217 19:56:16.968350 139973357201216 spec.py:321] Evaluating on the training split.
I0217 19:56:24.191636 139973357201216 spec.py:333] Evaluating on the validation split.
I0217 19:56:31.523134 139973357201216 spec.py:349] Evaluating on the test split.
I0217 19:56:39.859195 139973357201216 submission_runner.py:408] Time since start: 1256.98s, 	Step: 1579, 	{'train/loss': 0.12770419792746598, 'validation/loss': 0.1264533591197161, 'validation/num_examples': 83274637, 'test/loss': 0.12885436389802632, 'test/num_examples': 95000000, 'score': 1210.5401918888092, 'total_duration': 1256.9842765331268, 'accumulated_submission_time': 1210.5401918888092, 'accumulated_eval_time': 46.37395644187927, 'accumulated_logging_time': 0.0187075138092041}
I0217 19:56:39.883014 139812795315968 logging_writer.py:48] [1579] accumulated_eval_time=46.373956, accumulated_logging_time=0.018708, accumulated_submission_time=1210.540192, global_step=1579, preemption_count=0, score=1210.540192, test/loss=0.128854, test/num_examples=95000000, total_duration=1256.984277, train/loss=0.127704, validation/loss=0.126453, validation/num_examples=83274637
I0217 19:56:41.999024 139812786923264 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.026928072795271873, loss=0.12894798815250397
I0217 19:57:59.273468 139812795315968 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.039214231073856354, loss=0.1262197494506836
I0217 19:59:20.529046 139812786923264 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.031404267996549606, loss=0.13381925225257874
I0217 20:00:42.881852 139812795315968 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.01001729629933834, loss=0.12098771333694458
I0217 20:01:59.669949 139812786923264 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.03128425404429436, loss=0.13086535036563873
I0217 20:03:15.757171 139812795315968 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.04457005113363266, loss=0.12709957361221313
I0217 20:04:31.872547 139812786923264 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.020545214414596558, loss=0.1315823197364807
I0217 20:05:47.763524 139812795315968 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.050174739211797714, loss=0.12482191622257233
I0217 20:07:04.083214 139812786923264 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.03433370217680931, loss=0.13483041524887085
I0217 20:08:19.965397 139812795315968 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.027864918112754822, loss=0.11925297230482101
I0217 20:09:36.215717 139812786923264 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.050252582877874374, loss=0.12108474224805832
I0217 20:10:52.451063 139812795315968 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.019789980724453926, loss=0.12077416479587555
I0217 20:12:08.441740 139812786923264 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.038230083882808685, loss=0.12076422572135925
I0217 20:13:24.587017 139812795315968 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.014381607994437218, loss=0.12597526609897614
I0217 20:14:40.618806 139812786923264 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.057458605617284775, loss=0.12772290408611298
I0217 20:15:56.300478 139812795315968 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.03714964538812637, loss=0.11792576313018799
I0217 20:16:40.290529 139973357201216 spec.py:321] Evaluating on the training split.
I0217 20:16:47.314309 139973357201216 spec.py:333] Evaluating on the validation split.
I0217 20:16:54.641722 139973357201216 spec.py:349] Evaluating on the test split.
I0217 20:17:03.004657 139973357201216 submission_runner.py:408] Time since start: 2480.13s, 	Step: 3159, 	{'train/loss': 0.12486584522064377, 'validation/loss': 0.12615875974187674, 'validation/num_examples': 83274637, 'test/loss': 0.12844116784539475, 'test/num_examples': 95000000, 'score': 2410.887097120285, 'total_duration': 2480.129752635956, 'accumulated_submission_time': 2410.887097120285, 'accumulated_eval_time': 69.08802628517151, 'accumulated_logging_time': 0.05174875259399414}
I0217 20:17:03.021785 139812786923264 logging_writer.py:48] [3159] accumulated_eval_time=69.088026, accumulated_logging_time=0.051749, accumulated_submission_time=2410.887097, global_step=3159, preemption_count=0, score=2410.887097, test/loss=0.128441, test/num_examples=95000000, total_duration=2480.129753, train/loss=0.124866, validation/loss=0.126159, validation/num_examples=83274637
I0217 20:17:18.058691 139812795315968 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.05881614238023758, loss=0.1333911269903183
I0217 20:18:39.269039 139812786923264 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.0754479318857193, loss=0.13030338287353516
I0217 20:20:00.712388 139812795315968 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.038465943187475204, loss=0.11923200637102127
I0217 20:21:21.983012 139812786923264 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.011779609136283398, loss=0.12725959718227386
I0217 20:22:37.736891 139812795315968 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.03660985082387924, loss=0.13011756539344788
I0217 20:23:53.550229 139812786923264 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.07270222902297974, loss=0.12186109274625778
I0217 20:25:09.519706 139812795315968 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.039539992809295654, loss=0.119666188955307
I0217 20:26:25.195853 139812786923264 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.027474230155348778, loss=0.1190357506275177
I0217 20:27:41.101284 139812795315968 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.04870884492993355, loss=0.13419212400913239
I0217 20:28:57.322798 139812786923264 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.07252225279808044, loss=0.12455134093761444
I0217 20:30:13.482764 139812795315968 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.008264448493719101, loss=0.12410759925842285
I0217 20:31:26.917765 139812786923264 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.07299084216356277, loss=0.12073078006505966
I0217 20:32:42.901339 139812795315968 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.016562310978770256, loss=0.12463733553886414
I0217 20:33:58.934680 139812786923264 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.023966697975993156, loss=0.12517313659191132
I0217 20:35:14.891123 139812795315968 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.050815362483263016, loss=0.12022503465414047
I0217 20:36:31.493383 139812786923264 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.019153544679284096, loss=0.1261938363313675
I0217 20:37:03.426666 139973357201216 spec.py:321] Evaluating on the training split.
I0217 20:37:10.370458 139973357201216 spec.py:333] Evaluating on the validation split.
I0217 20:37:17.512335 139973357201216 spec.py:349] Evaluating on the test split.
I0217 20:37:25.987764 139973357201216 submission_runner.py:408] Time since start: 3703.11s, 	Step: 4743, 	{'train/loss': 0.12441820168645128, 'validation/loss': 0.12578316045188825, 'validation/num_examples': 83274637, 'test/loss': 0.12792773310032896, 'test/num_examples': 95000000, 'score': 3611.227801322937, 'total_duration': 3703.112888813019, 'accumulated_submission_time': 3611.227801322937, 'accumulated_eval_time': 91.64909315109253, 'accumulated_logging_time': 0.08120036125183105}
I0217 20:37:26.003936 139812795315968 logging_writer.py:48] [4743] accumulated_eval_time=91.649093, accumulated_logging_time=0.081200, accumulated_submission_time=3611.227801, global_step=4743, preemption_count=0, score=3611.227801, test/loss=0.127928, test/num_examples=95000000, total_duration=3703.112889, train/loss=0.124418, validation/loss=0.125783, validation/num_examples=83274637
I0217 20:37:54.231034 139812786923264 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.05127367377281189, loss=0.1263345181941986
I0217 20:39:12.786940 139812795315968 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.008255190216004848, loss=0.12477049231529236
I0217 20:40:33.753602 139812786923264 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.04213026538491249, loss=0.12221112102270126
I0217 20:41:54.737034 139812795315968 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.024990344420075417, loss=0.12037952989339828
I0217 20:43:10.932209 139812786923264 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.03546461462974548, loss=0.12418200075626373
I0217 20:44:26.616402 139812795315968 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.04043036326766014, loss=0.12436932325363159
I0217 20:45:42.829277 139812786923264 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.04231042042374611, loss=0.12363716959953308
I0217 20:46:59.262334 139812795315968 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.004310977179557085, loss=0.11371123790740967
I0217 20:48:15.365968 139812786923264 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.01783287152647972, loss=0.12272009253501892
I0217 20:49:31.276568 139812795315968 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.011637150309979916, loss=0.11888180673122406
I0217 20:50:47.355796 139812786923264 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.04195907711982727, loss=0.12110970169305801
I0217 20:52:03.778181 139812795315968 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.02733292616903782, loss=0.13830003142356873
I0217 20:53:20.510035 139812786923264 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.041133005172014236, loss=0.12074799090623856
I0217 20:54:36.715367 139812795315968 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.013946732506155968, loss=0.12541480362415314
I0217 20:55:52.676810 139812786923264 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.012781472876667976, loss=0.12184929102659225
I0217 20:57:08.848752 139812795315968 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.01156259048730135, loss=0.1264657974243164
I0217 20:57:26.137966 139973357201216 spec.py:321] Evaluating on the training split.
I0217 20:57:33.104455 139973357201216 spec.py:333] Evaluating on the validation split.
I0217 20:57:40.528714 139973357201216 spec.py:349] Evaluating on the test split.
I0217 20:57:48.926711 139973357201216 submission_runner.py:408] Time since start: 4926.05s, 	Step: 6324, 	{'train/loss': 0.12290746334401316, 'validation/loss': 0.1256336655136065, 'validation/num_examples': 83274637, 'test/loss': 0.1278399025801809, 'test/num_examples': 95000000, 'score': 4811.301905870438, 'total_duration': 4926.051818847656, 'accumulated_submission_time': 4811.301905870438, 'accumulated_eval_time': 114.43777704238892, 'accumulated_logging_time': 0.10660719871520996}
I0217 20:57:48.945254 139812786923264 logging_writer.py:48] [6324] accumulated_eval_time=114.437777, accumulated_logging_time=0.106607, accumulated_submission_time=4811.301906, global_step=6324, preemption_count=0, score=4811.301906, test/loss=0.127840, test/num_examples=95000000, total_duration=4926.051819, train/loss=0.122907, validation/loss=0.125634, validation/num_examples=83274637
I0217 20:58:32.646107 139812795315968 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.012414001859724522, loss=0.12513230741024017
I0217 20:59:54.028468 139812786923264 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.03556123748421669, loss=0.11872612684965134
I0217 21:01:15.107492 139812795315968 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.02776033617556095, loss=0.13606184720993042
I0217 21:02:32.445635 139812786923264 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.027895784005522728, loss=0.13324061036109924
I0217 21:03:47.643855 139812795315968 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.019138073548674583, loss=0.1295221596956253
I0217 21:05:03.483232 139812786923264 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.008442156948149204, loss=0.12168747931718826
I0217 21:06:19.505245 139812795315968 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.005535664968192577, loss=0.11928214132785797
I0217 21:07:35.306572 139812786923264 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.03387167677283287, loss=0.12356230616569519
I0217 21:08:48.543653 139812795315968 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.021305784583091736, loss=0.11644596606492996
I0217 21:10:04.851774 139812786923264 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.007715897168964148, loss=0.12429166585206985
I0217 21:11:20.823728 139812795315968 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.019473955035209656, loss=0.12847018241882324
I0217 21:12:37.389208 139812786923264 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.02046237885951996, loss=0.12874385714530945
I0217 21:13:53.108885 139812795315968 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.0065142004750669, loss=0.12940534949302673
I0217 21:15:09.593399 139812786923264 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.0058914413675665855, loss=0.11429604887962341
I0217 21:16:25.744038 139812795315968 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.008974798955023289, loss=0.133900985121727
I0217 21:17:41.975443 139812786923264 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.011898835189640522, loss=0.12223115563392639
I0217 21:17:49.386539 139973357201216 spec.py:321] Evaluating on the training split.
I0217 21:17:56.330162 139973357201216 spec.py:333] Evaluating on the validation split.
I0217 21:18:03.623263 139973357201216 spec.py:349] Evaluating on the test split.
I0217 21:18:12.069361 139973357201216 submission_runner.py:408] Time since start: 6149.19s, 	Step: 7911, 	{'train/loss': 0.1258539432812037, 'validation/loss': 0.12559553621950942, 'validation/num_examples': 83274637, 'test/loss': 0.12789141796875, 'test/num_examples': 95000000, 'score': 6011.683310031891, 'total_duration': 6149.194463253021, 'accumulated_submission_time': 6011.683310031891, 'accumulated_eval_time': 137.12053418159485, 'accumulated_logging_time': 0.13332557678222656}
I0217 21:18:12.089723 139812795315968 logging_writer.py:48] [7911] accumulated_eval_time=137.120534, accumulated_logging_time=0.133326, accumulated_submission_time=6011.683310, global_step=7911, preemption_count=0, score=6011.683310, test/loss=0.127891, test/num_examples=95000000, total_duration=6149.194463, train/loss=0.125854, validation/loss=0.125596, validation/num_examples=83274637
I0217 21:19:05.451843 139812786923264 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.017113924026489258, loss=0.12517563998699188
I0217 21:20:25.070235 139812795315968 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.006516940426081419, loss=0.12163262069225311
I0217 21:21:46.444145 139812786923264 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.0103682866320014, loss=0.1205536499619484
I0217 21:23:05.880804 139812795315968 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.008870860561728477, loss=0.12713848054409027
I0217 21:24:21.565212 139812786923264 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.013552830554544926, loss=0.11944378167390823
I0217 21:25:37.135297 139812795315968 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.01303176674991846, loss=0.12044431269168854
I0217 21:26:52.675682 139812786923264 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.0060356659814715385, loss=0.12172938883304596
I0217 21:28:08.642873 139812795315968 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.010368513874709606, loss=0.12223462760448456
I0217 21:29:24.669838 139812786923264 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.010484478436410427, loss=0.12534084916114807
I0217 21:30:40.436912 139812795315968 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.009311431087553501, loss=0.11927173286676407
I0217 21:31:56.456985 139812786923264 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.01736694574356079, loss=0.12152099609375
I0217 21:33:09.746261 139812795315968 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.006620111875236034, loss=0.12010529637336731
I0217 21:34:25.382584 139812786923264 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.013876593671739101, loss=0.1291227489709854
I0217 21:35:40.960209 139812795315968 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.008475683629512787, loss=0.12109013646841049
I0217 21:36:57.103564 139812786923264 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.010075308382511139, loss=0.1282753199338913
I0217 21:38:12.390842 139973357201216 spec.py:321] Evaluating on the training split.
I0217 21:38:19.341571 139973357201216 spec.py:333] Evaluating on the validation split.
I0217 21:38:26.785477 139973357201216 spec.py:349] Evaluating on the test split.
I0217 21:38:35.288278 139973357201216 submission_runner.py:408] Time since start: 7372.41s, 	Step: 9500, 	{'train/loss': 0.12368268950742746, 'validation/loss': 0.12541117594236706, 'validation/num_examples': 83274637, 'test/loss': 0.1276304653885691, 'test/num_examples': 95000000, 'score': 7211.92481803894, 'total_duration': 7372.413366794586, 'accumulated_submission_time': 7211.92481803894, 'accumulated_eval_time': 160.01790714263916, 'accumulated_logging_time': 0.16205430030822754}
I0217 21:38:35.303922 139812795315968 logging_writer.py:48] [9500] accumulated_eval_time=160.017907, accumulated_logging_time=0.162054, accumulated_submission_time=7211.924818, global_step=9500, preemption_count=0, score=7211.924818, test/loss=0.127630, test/num_examples=95000000, total_duration=7372.413367, train/loss=0.123683, validation/loss=0.125411, validation/num_examples=83274637
I0217 21:38:35.413145 139812786923264 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.007803174201399088, loss=0.12080319970846176
I0217 21:39:37.540810 139812795315968 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.011158597655594349, loss=0.12075665593147278
I0217 21:40:58.120581 139812786923264 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.006987661588937044, loss=0.12284457683563232
I0217 21:42:19.034146 139812795315968 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.007335061207413673, loss=0.1252588927745819
I0217 21:43:37.452793 139812786923264 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.011520402505993843, loss=0.1194397360086441
I0217 21:44:53.252319 139812795315968 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.01147924643009901, loss=0.12470240145921707
I0217 21:46:09.674192 139812786923264 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.006401951890438795, loss=0.11738011240959167
I0217 21:46:46.839641 139812795315968 logging_writer.py:48] [10150] global_step=10150, preemption_count=0, score=7703.422815
I0217 21:46:53.260542 139973357201216 checkpoints.py:490] Saving checkpoint at step: 10150
I0217 21:47:29.474384 139973357201216 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_3/criteo1tb_jax/trial_4/checkpoint_10150
I0217 21:47:29.940252 139973357201216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_3/criteo1tb_jax/trial_4/checkpoint_10150.
I0217 21:47:30.721041 139973357201216 submission_runner.py:583] Tuning trial 4/5
I0217 21:47:30.721297 139973357201216 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.004958460849689891, one_minus_beta1=0.13625575743, beta2=0.6291854735396584, weight_decay=0.1147386261512052, warmup_factor=0.02)
I0217 21:47:30.722331 139973357201216 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/loss': 1.5201884942984432, 'validation/loss': 1.5212185261462023, 'validation/num_examples': 83274637, 'test/loss': 1.5194805184210527, 'test/num_examples': 95000000, 'score': 10.086588859558105, 'total_duration': 33.56980895996094, 'accumulated_submission_time': 10.086588859558105, 'accumulated_eval_time': 23.48318362236023, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1579, {'train/loss': 0.12770419792746598, 'validation/loss': 0.1264533591197161, 'validation/num_examples': 83274637, 'test/loss': 0.12885436389802632, 'test/num_examples': 95000000, 'score': 1210.5401918888092, 'total_duration': 1256.9842765331268, 'accumulated_submission_time': 1210.5401918888092, 'accumulated_eval_time': 46.37395644187927, 'accumulated_logging_time': 0.0187075138092041, 'global_step': 1579, 'preemption_count': 0}), (3159, {'train/loss': 0.12486584522064377, 'validation/loss': 0.12615875974187674, 'validation/num_examples': 83274637, 'test/loss': 0.12844116784539475, 'test/num_examples': 95000000, 'score': 2410.887097120285, 'total_duration': 2480.129752635956, 'accumulated_submission_time': 2410.887097120285, 'accumulated_eval_time': 69.08802628517151, 'accumulated_logging_time': 0.05174875259399414, 'global_step': 3159, 'preemption_count': 0}), (4743, {'train/loss': 0.12441820168645128, 'validation/loss': 0.12578316045188825, 'validation/num_examples': 83274637, 'test/loss': 0.12792773310032896, 'test/num_examples': 95000000, 'score': 3611.227801322937, 'total_duration': 3703.112888813019, 'accumulated_submission_time': 3611.227801322937, 'accumulated_eval_time': 91.64909315109253, 'accumulated_logging_time': 0.08120036125183105, 'global_step': 4743, 'preemption_count': 0}), (6324, {'train/loss': 0.12290746334401316, 'validation/loss': 0.1256336655136065, 'validation/num_examples': 83274637, 'test/loss': 0.1278399025801809, 'test/num_examples': 95000000, 'score': 4811.301905870438, 'total_duration': 4926.051818847656, 'accumulated_submission_time': 4811.301905870438, 'accumulated_eval_time': 114.43777704238892, 'accumulated_logging_time': 0.10660719871520996, 'global_step': 6324, 'preemption_count': 0}), (7911, {'train/loss': 0.1258539432812037, 'validation/loss': 0.12559553621950942, 'validation/num_examples': 83274637, 'test/loss': 0.12789141796875, 'test/num_examples': 95000000, 'score': 6011.683310031891, 'total_duration': 6149.194463253021, 'accumulated_submission_time': 6011.683310031891, 'accumulated_eval_time': 137.12053418159485, 'accumulated_logging_time': 0.13332557678222656, 'global_step': 7911, 'preemption_count': 0}), (9500, {'train/loss': 0.12368268950742746, 'validation/loss': 0.12541117594236706, 'validation/num_examples': 83274637, 'test/loss': 0.1276304653885691, 'test/num_examples': 95000000, 'score': 7211.92481803894, 'total_duration': 7372.413366794586, 'accumulated_submission_time': 7211.92481803894, 'accumulated_eval_time': 160.01790714263916, 'accumulated_logging_time': 0.16205430030822754, 'global_step': 9500, 'preemption_count': 0})], 'global_step': 10150}
I0217 21:47:30.722446 139973357201216 submission_runner.py:586] Timing: 7703.422815322876
I0217 21:47:30.722495 139973357201216 submission_runner.py:588] Total number of evals: 7
I0217 21:47:30.722549 139973357201216 submission_runner.py:589] ====================
I0217 21:47:30.722601 139973357201216 submission_runner.py:542] Using RNG seed 2861073867
I0217 21:47:30.724428 139973357201216 submission_runner.py:551] --- Tuning run 5/5 ---
I0217 21:47:30.724558 139973357201216 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_3/criteo1tb_jax/trial_5.
I0217 21:47:30.727376 139973357201216 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_3/criteo1tb_jax/trial_5/hparams.json.
I0217 21:47:30.728226 139973357201216 submission_runner.py:206] Initializing dataset.
I0217 21:47:30.728353 139973357201216 submission_runner.py:213] Initializing model.
I0217 21:47:33.451845 139973357201216 submission_runner.py:255] Initializing optimizer.
I0217 21:47:36.203232 139973357201216 submission_runner.py:262] Initializing metrics bundle.
I0217 21:47:36.203434 139973357201216 submission_runner.py:280] Initializing checkpoint and logger.
I0217 21:47:36.310240 139973357201216 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_3/criteo1tb_jax/trial_5 with prefix checkpoint_
I0217 21:47:36.310400 139973357201216 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_3/criteo1tb_jax/trial_5/meta_data_0.json.
I0217 21:47:36.310671 139973357201216 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0217 21:47:36.310762 139973357201216 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0217 21:47:48.865863 139973357201216 logger_utils.py:220] Unable to record git information. Continuing without it.
I0217 21:48:01.176810 139973357201216 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_3/criteo1tb_jax/trial_5/flags_0.json.
I0217 21:48:01.184371 139973357201216 submission_runner.py:314] Starting training loop.
I0217 21:48:07.392555 139813248292608 logging_writer.py:48] [0] global_step=0, grad_norm=8.973965644836426, loss=1.5170930624008179
I0217 21:48:07.398160 139973357201216 spec.py:321] Evaluating on the training split.
I0217 21:48:14.360188 139973357201216 spec.py:333] Evaluating on the validation split.
I0217 21:48:21.455936 139973357201216 spec.py:349] Evaluating on the test split.
I0217 21:48:29.691563 139973357201216 submission_runner.py:408] Time since start: 28.51s, 	Step: 1, 	{'train/loss': 1.5197636916202568, 'validation/loss': 1.5212185261462023, 'validation/num_examples': 83274637, 'test/loss': 1.5194805184210527, 'test/num_examples': 95000000, 'score': 6.213699102401733, 'total_duration': 28.507106065750122, 'accumulated_submission_time': 6.213699102401733, 'accumulated_eval_time': 22.293355464935303, 'accumulated_logging_time': 0}
I0217 21:48:29.701242 139813256685312 logging_writer.py:48] [1] accumulated_eval_time=22.293355, accumulated_logging_time=0, accumulated_submission_time=6.213699, global_step=1, preemption_count=0, score=6.213699, test/loss=1.519481, test/num_examples=95000000, total_duration=28.507106, train/loss=1.519764, validation/loss=1.521219, validation/num_examples=83274637
I0217 21:49:32.125768 139813248292608 logging_writer.py:48] [100] global_step=100, grad_norm=0.0534985177218914, loss=0.12956613302230835
I0217 21:50:52.351197 139813256685312 logging_writer.py:48] [200] global_step=200, grad_norm=0.06650593876838684, loss=0.13353443145751953
I0217 21:52:12.738629 139813248292608 logging_writer.py:48] [300] global_step=300, grad_norm=0.01916918344795704, loss=0.1270756721496582
I0217 21:53:30.947089 139813256685312 logging_writer.py:48] [400] global_step=400, grad_norm=0.05394793301820755, loss=0.12329275906085968
I0217 21:54:45.751381 139813248292608 logging_writer.py:48] [500] global_step=500, grad_norm=0.12247029691934586, loss=0.12496350705623627
I0217 21:56:00.018933 139813256685312 logging_writer.py:48] [600] global_step=600, grad_norm=0.0647377297282219, loss=0.12629348039627075
I0217 21:57:16.397866 139813248292608 logging_writer.py:48] [700] global_step=700, grad_norm=0.03326188772916794, loss=0.12603434920310974
I0217 21:58:32.253113 139813256685312 logging_writer.py:48] [800] global_step=800, grad_norm=0.09383754432201385, loss=0.12344692647457123
I0217 21:59:49.292506 139813248292608 logging_writer.py:48] [900] global_step=900, grad_norm=0.022596100345253944, loss=0.11942420154809952
I0217 22:01:06.956900 139813256685312 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.006245036143809557, loss=0.12135349214076996
I0217 22:02:24.322407 139813248292608 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.09219340980052948, loss=0.1326427459716797
I0217 22:03:41.816856 139813256685312 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.028173301368951797, loss=0.12046897411346436
I0217 22:04:58.084789 139813248292608 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.09431876987218857, loss=0.14273178577423096
I0217 22:06:14.390499 139813256685312 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.0725744441151619, loss=0.1321951299905777
I0217 22:07:30.289885 139813248292608 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.0652083158493042, loss=0.11948620527982712
I0217 22:08:29.719739 139973357201216 spec.py:321] Evaluating on the training split.
I0217 22:08:36.735534 139973357201216 spec.py:333] Evaluating on the validation split.
I0217 22:08:43.787464 139973357201216 spec.py:349] Evaluating on the test split.
I0217 22:08:52.020507 139973357201216 submission_runner.py:408] Time since start: 1250.84s, 	Step: 1579, 	{'train/loss': 0.12438066814493083, 'validation/loss': 0.1254170771614261, 'validation/num_examples': 83274637, 'test/loss': 0.12779967911184212, 'test/num_examples': 95000000, 'score': 1206.1726896762848, 'total_duration': 1250.8360691070557, 'accumulated_submission_time': 1206.1726896762848, 'accumulated_eval_time': 44.59408211708069, 'accumulated_logging_time': 0.01841425895690918}
I0217 22:08:52.037101 139813256685312 logging_writer.py:48] [1579] accumulated_eval_time=44.594082, accumulated_logging_time=0.018414, accumulated_submission_time=1206.172690, global_step=1579, preemption_count=0, score=1206.172690, test/loss=0.127800, test/num_examples=95000000, total_duration=1250.836069, train/loss=0.124381, validation/loss=0.125417, validation/num_examples=83274637
I0217 22:08:54.157245 139813248292608 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.02916114218533039, loss=0.11945992708206177
I0217 22:10:12.046079 139813256685312 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.03588620945811272, loss=0.13199011981487274
I0217 22:11:33.325757 139813248292608 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.010051076300442219, loss=0.1280074566602707
I0217 22:12:52.978328 139813256685312 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.028374264016747475, loss=0.12268944084644318
I0217 22:14:09.424785 139813248292608 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.05144388973712921, loss=0.12757638096809387
I0217 22:15:27.225787 139813256685312 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.01229903195053339, loss=0.13462214171886444
I0217 22:16:44.061140 139813248292608 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.05528413504362106, loss=0.11978168040513992
I0217 22:17:57.714112 139813256685312 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.0079769566655159, loss=0.12027761340141296
I0217 22:19:14.485091 139813248292608 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.006218188442289829, loss=0.12051916122436523
I0217 22:20:30.695311 139813256685312 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.005858485121279955, loss=0.11981629580259323
I0217 22:21:47.039790 139813248292608 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.030919859185814857, loss=0.12211567163467407
I0217 22:23:03.251542 139813256685312 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.016867373138666153, loss=0.12703926861286163
I0217 22:24:19.472554 139813248292608 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.02144687995314598, loss=0.12155388295650482
I0217 22:25:35.670156 139813256685312 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.01660367101430893, loss=0.12532316148281097
I0217 22:26:51.570076 139813248292608 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.02874491922557354, loss=0.12294948101043701
I0217 22:28:08.000293 139813256685312 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.03335854783654213, loss=0.11605578660964966
I0217 22:28:52.586620 139973357201216 spec.py:321] Evaluating on the training split.
I0217 22:28:59.612807 139973357201216 spec.py:333] Evaluating on the validation split.
I0217 22:29:06.657900 139973357201216 spec.py:349] Evaluating on the test split.
I0217 22:29:14.960739 139973357201216 submission_runner.py:408] Time since start: 2473.78s, 	Step: 3160, 	{'train/loss': 0.12393245279039226, 'validation/loss': 0.12483939831674079, 'validation/num_examples': 83274637, 'test/loss': 0.1272734294921875, 'test/num_examples': 95000000, 'score': 2406.662718772888, 'total_duration': 2473.7763113975525, 'accumulated_submission_time': 2406.662718772888, 'accumulated_eval_time': 66.96817874908447, 'accumulated_logging_time': 0.04338717460632324}
I0217 22:29:14.977478 139813248292608 logging_writer.py:48] [3160] accumulated_eval_time=66.968179, accumulated_logging_time=0.043387, accumulated_submission_time=2406.662719, global_step=3160, preemption_count=0, score=2406.662719, test/loss=0.127273, test/num_examples=95000000, total_duration=2473.776311, train/loss=0.123932, validation/loss=0.124839, validation/num_examples=83274637
I0217 22:29:29.451714 139813256685312 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.018224872648715973, loss=0.12200164794921875
I0217 22:30:51.136793 139813248292608 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.007288974244147539, loss=0.1224108636379242
I0217 22:32:11.174281 139813256685312 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.05985976383090019, loss=0.11675909906625748
I0217 22:33:30.810938 139813248292608 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.024699484929442406, loss=0.12937679886817932
I0217 22:34:48.134038 139813256685312 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.02788456901907921, loss=0.12179691344499588
I0217 22:36:05.463216 139813248292608 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.019902396947145462, loss=0.11872313916683197
I0217 22:37:20.520621 139813256685312 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.0174716729670763, loss=0.12136876583099365
I0217 22:38:35.613963 139813248292608 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.007762192748486996, loss=0.13028813898563385
I0217 22:39:51.727206 139813256685312 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.015475147403776646, loss=0.12303726375102997
I0217 22:41:07.642532 139813248292608 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.01139791589230299, loss=0.12620794773101807
I0217 22:42:23.549853 139813256685312 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.03162510693073273, loss=0.13099056482315063
I0217 22:43:39.381927 139813248292608 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.013971737585961819, loss=0.12915419042110443
I0217 22:44:55.676033 139813256685312 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.02157655358314514, loss=0.1228141039609909
I0217 22:46:11.469146 139813248292608 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.016411101445555687, loss=0.12060124427080154
I0217 22:47:27.013724 139813256685312 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.014893750660121441, loss=0.1189003735780716
I0217 22:48:42.422162 139813248292608 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.029586751013994217, loss=0.13443221151828766
I0217 22:49:15.216141 139973357201216 spec.py:321] Evaluating on the training split.
I0217 22:49:22.124258 139973357201216 spec.py:333] Evaluating on the validation split.
I0217 22:49:29.134654 139973357201216 spec.py:349] Evaluating on the test split.
I0217 22:49:37.362059 139973357201216 submission_runner.py:408] Time since start: 3696.18s, 	Step: 4744, 	{'train/loss': 0.12325972365508289, 'validation/loss': 0.12432915371977245, 'validation/num_examples': 83274637, 'test/loss': 0.12665475383429275, 'test/num_examples': 95000000, 'score': 3606.8427715301514, 'total_duration': 3696.1776309013367, 'accumulated_submission_time': 3606.8427715301514, 'accumulated_eval_time': 89.11408042907715, 'accumulated_logging_time': 0.06798839569091797}
I0217 22:49:37.377766 139813256685312 logging_writer.py:48] [4744] accumulated_eval_time=89.114080, accumulated_logging_time=0.067988, accumulated_submission_time=3606.842772, global_step=4744, preemption_count=0, score=3606.842772, test/loss=0.126655, test/num_examples=95000000, total_duration=3696.177631, train/loss=0.123260, validation/loss=0.124329, validation/num_examples=83274637
I0217 22:50:04.381567 139813248292608 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.00921611674129963, loss=0.11462125182151794
I0217 22:51:25.175437 139813256685312 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.014781005680561066, loss=0.12713918089866638
I0217 22:52:45.328726 139813248292608 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.007128195837140083, loss=0.1261562705039978
I0217 22:54:02.622411 139813256685312 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.010989686474204063, loss=0.12395311146974564
I0217 22:55:21.201209 139813248292608 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.0054136463440954685, loss=0.12084228545427322
I0217 22:56:37.687953 139813256685312 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.008895297534763813, loss=0.1278029829263687
I0217 22:57:54.302843 139813248292608 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.006539213936775923, loss=0.12462632358074188
I0217 22:59:10.786249 139813256685312 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.014138584956526756, loss=0.1244521215558052
I0217 23:00:26.778049 139813248292608 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.017117196694016457, loss=0.1285126656293869
I0217 23:01:39.802255 139813256685312 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.008935386314988136, loss=0.1329142302274704
I0217 23:02:55.998847 139813248292608 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.006524910684674978, loss=0.11542285233736038
I0217 23:04:12.045796 139813256685312 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.006786036305129528, loss=0.1233157217502594
I0217 23:05:28.550764 139813248292608 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.01232981774955988, loss=0.12274117767810822
I0217 23:06:44.201548 139813256685312 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.007768651004880667, loss=0.12444204092025757
I0217 23:08:00.018971 139813248292608 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.006771900225430727, loss=0.12286315858364105
I0217 23:09:16.145822 139813256685312 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.007634678389877081, loss=0.11524535715579987
I0217 23:09:37.962328 139973357201216 spec.py:321] Evaluating on the training split.
I0217 23:09:45.004191 139973357201216 spec.py:333] Evaluating on the validation split.
I0217 23:09:52.004232 139973357201216 spec.py:349] Evaluating on the test split.
I0217 23:10:00.268831 139973357201216 submission_runner.py:408] Time since start: 4919.08s, 	Step: 6330, 	{'train/loss': 0.12469348639437237, 'validation/loss': 0.1241852502667619, 'validation/num_examples': 83274637, 'test/loss': 0.1265950968955592, 'test/num_examples': 95000000, 'score': 4807.368099927902, 'total_duration': 4919.084401607513, 'accumulated_submission_time': 4807.368099927902, 'accumulated_eval_time': 111.42054176330566, 'accumulated_logging_time': 0.09139680862426758}
I0217 23:10:00.286877 139813248292608 logging_writer.py:48] [6330] accumulated_eval_time=111.420542, accumulated_logging_time=0.091397, accumulated_submission_time=4807.368100, global_step=6330, preemption_count=0, score=4807.368100, test/loss=0.126595, test/num_examples=95000000, total_duration=4919.084402, train/loss=0.124693, validation/loss=0.124185, validation/num_examples=83274637
I0217 23:10:38.555570 139813256685312 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.015630846843123436, loss=0.11853571981191635
I0217 23:11:59.126476 139813248292608 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.009713167324662209, loss=0.12232151627540588
I0217 23:13:20.210203 139813256685312 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.006623434368520975, loss=0.12220567464828491
I0217 23:14:40.346514 139813248292608 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.014266622252762318, loss=0.12718039751052856
I0217 23:15:57.446899 139813256685312 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.006459439173340797, loss=0.12587834894657135
I0217 23:17:16.200589 139813248292608 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.014334216713905334, loss=0.11792603135108948
I0217 23:18:31.400341 139813256685312 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.006169773172587156, loss=0.12065978348255157
I0217 23:19:47.430150 139813248292608 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.005572715308517218, loss=0.126050665974617
I0217 23:21:03.900452 139813256685312 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.0070342556573450565, loss=0.11980655044317245
I0217 23:22:20.434622 139813248292608 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.007821908220648766, loss=0.11793389916419983
I0217 23:23:36.632578 139813256685312 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.007124141324311495, loss=0.12638157606124878
I0217 23:24:52.792501 139813248292608 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.011498553678393364, loss=0.11733630299568176
I0217 23:26:09.245638 139813256685312 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.007933419197797775, loss=0.1307298243045807
I0217 23:27:25.518634 139813248292608 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.008320397697389126, loss=0.11553016304969788
I0217 23:28:38.739264 139813256685312 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.008270065300166607, loss=0.11991764605045319
I0217 23:29:54.684579 139813248292608 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.006222442723810673, loss=0.11912699788808823
I0217 23:30:00.830162 139973357201216 spec.py:321] Evaluating on the training split.
I0217 23:30:07.807511 139973357201216 spec.py:333] Evaluating on the validation split.
I0217 23:30:14.828707 139973357201216 spec.py:349] Evaluating on the test split.
I0217 23:30:23.039271 139973357201216 submission_runner.py:408] Time since start: 6141.85s, 	Step: 7909, 	{'train/loss': 0.12235302042286351, 'validation/loss': 0.12379040756318457, 'validation/num_examples': 83274637, 'test/loss': 0.12611440473889804, 'test/num_examples': 95000000, 'score': 6007.8526430130005, 'total_duration': 6141.8548221588135, 'accumulated_submission_time': 6007.8526430130005, 'accumulated_eval_time': 133.62959718704224, 'accumulated_logging_time': 0.11743664741516113}
I0217 23:30:23.057118 139813256685312 logging_writer.py:48] [7909] accumulated_eval_time=133.629597, accumulated_logging_time=0.117437, accumulated_submission_time=6007.852643, global_step=7909, preemption_count=0, score=6007.852643, test/loss=0.126114, test/num_examples=95000000, total_duration=6141.854822, train/loss=0.122353, validation/loss=0.123790, validation/num_examples=83274637
I0217 23:31:19.409366 139813248292608 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.010525942780077457, loss=0.1185552179813385
I0217 23:32:38.980502 139813256685312 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.00822437647730112, loss=0.12648196518421173
I0217 23:34:00.150100 139813248292608 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.016276458278298378, loss=0.13722670078277588
I0217 23:35:15.583903 139813256685312 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.00745094008743763, loss=0.11988027393817902
I0217 23:36:32.991870 139813248292608 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.005461233668029308, loss=0.1181851401925087
I0217 23:37:50.115512 139813256685312 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.007058272138237953, loss=0.13453929126262665
I0217 23:39:06.795000 139813248292608 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.00879382062703371, loss=0.11441825330257416
I0217 23:40:22.664993 139813256685312 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.005759996362030506, loss=0.11679967492818832
I0217 23:41:38.958415 139813248292608 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.006605653557926416, loss=0.12281239032745361
I0217 23:42:55.280254 139813256685312 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.006012404803186655, loss=0.11930260807275772
I0217 23:44:11.578357 139813248292608 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.01395343616604805, loss=0.1360068917274475
I0217 23:45:27.500015 139813256685312 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.01005979347974062, loss=0.12681713700294495
I0217 23:46:43.233020 139813248292608 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.008851030841469765, loss=0.12485627830028534
I0217 23:47:59.134364 139813256685312 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.005798923783004284, loss=0.12115496397018433
I0217 23:49:14.974245 139813248292608 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.008212067186832428, loss=0.12007473409175873
I0217 23:50:23.659479 139973357201216 spec.py:321] Evaluating on the training split.
I0217 23:50:30.635100 139973357201216 spec.py:333] Evaluating on the validation split.
I0217 23:50:37.682258 139973357201216 spec.py:349] Evaluating on the test split.
I0217 23:50:45.891528 139973357201216 submission_runner.py:408] Time since start: 7364.71s, 	Step: 9492, 	{'train/loss': 0.12048418599667039, 'validation/loss': 0.123623332617319, 'validation/num_examples': 83274637, 'test/loss': 0.12595479833470394, 'test/num_examples': 95000000, 'score': 7208.395542383194, 'total_duration': 7364.707081079483, 'accumulated_submission_time': 7208.395542383194, 'accumulated_eval_time': 155.8615963459015, 'accumulated_logging_time': 0.14365887641906738}
I0217 23:50:45.908288 139813256685312 logging_writer.py:48] [9492] accumulated_eval_time=155.861596, accumulated_logging_time=0.143659, accumulated_submission_time=7208.395542, global_step=9492, preemption_count=0, score=7208.395542, test/loss=0.125955, test/num_examples=95000000, total_duration=7364.707081, train/loss=0.120484, validation/loss=0.123623, validation/num_examples=83274637
I0217 23:50:46.781615 139813248292608 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.006469749845564365, loss=0.13295575976371765
I0217 23:51:56.046967 139813256685312 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.009990905411541462, loss=0.12393096089363098
I0217 23:53:15.568976 139813248292608 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.010130010545253754, loss=0.11874765902757645
I0217 23:54:35.900999 139813256685312 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.008642093278467655, loss=0.11714746057987213
I0217 23:55:53.496087 139813248292608 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.00899987481534481, loss=0.11386282742023468
I0217 23:57:11.674988 139813256685312 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.00854379404336214, loss=0.12195758521556854
I0217 23:58:29.184651 139813248292608 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.006604854017496109, loss=0.1308198720216751
I0217 23:59:01.176934 139813256685312 logging_writer.py:48] [10145] global_step=10145, preemption_count=0, score=7703.626192
I0217 23:59:07.813208 139973357201216 checkpoints.py:490] Saving checkpoint at step: 10145
I0217 23:59:44.387095 139973357201216 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_3/criteo1tb_jax/trial_5/checkpoint_10145
I0217 23:59:44.882377 139973357201216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_3/criteo1tb_jax/trial_5/checkpoint_10145.
I0217 23:59:45.543997 139973357201216 submission_runner.py:583] Tuning trial 5/5
I0217 23:59:45.544228 139973357201216 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0217 23:59:45.547313 139973357201216 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/loss': 1.5197636916202568, 'validation/loss': 1.5212185261462023, 'validation/num_examples': 83274637, 'test/loss': 1.5194805184210527, 'test/num_examples': 95000000, 'score': 6.213699102401733, 'total_duration': 28.507106065750122, 'accumulated_submission_time': 6.213699102401733, 'accumulated_eval_time': 22.293355464935303, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1579, {'train/loss': 0.12438066814493083, 'validation/loss': 0.1254170771614261, 'validation/num_examples': 83274637, 'test/loss': 0.12779967911184212, 'test/num_examples': 95000000, 'score': 1206.1726896762848, 'total_duration': 1250.8360691070557, 'accumulated_submission_time': 1206.1726896762848, 'accumulated_eval_time': 44.59408211708069, 'accumulated_logging_time': 0.01841425895690918, 'global_step': 1579, 'preemption_count': 0}), (3160, {'train/loss': 0.12393245279039226, 'validation/loss': 0.12483939831674079, 'validation/num_examples': 83274637, 'test/loss': 0.1272734294921875, 'test/num_examples': 95000000, 'score': 2406.662718772888, 'total_duration': 2473.7763113975525, 'accumulated_submission_time': 2406.662718772888, 'accumulated_eval_time': 66.96817874908447, 'accumulated_logging_time': 0.04338717460632324, 'global_step': 3160, 'preemption_count': 0}), (4744, {'train/loss': 0.12325972365508289, 'validation/loss': 0.12432915371977245, 'validation/num_examples': 83274637, 'test/loss': 0.12665475383429275, 'test/num_examples': 95000000, 'score': 3606.8427715301514, 'total_duration': 3696.1776309013367, 'accumulated_submission_time': 3606.8427715301514, 'accumulated_eval_time': 89.11408042907715, 'accumulated_logging_time': 0.06798839569091797, 'global_step': 4744, 'preemption_count': 0}), (6330, {'train/loss': 0.12469348639437237, 'validation/loss': 0.1241852502667619, 'validation/num_examples': 83274637, 'test/loss': 0.1265950968955592, 'test/num_examples': 95000000, 'score': 4807.368099927902, 'total_duration': 4919.084401607513, 'accumulated_submission_time': 4807.368099927902, 'accumulated_eval_time': 111.42054176330566, 'accumulated_logging_time': 0.09139680862426758, 'global_step': 6330, 'preemption_count': 0}), (7909, {'train/loss': 0.12235302042286351, 'validation/loss': 0.12379040756318457, 'validation/num_examples': 83274637, 'test/loss': 0.12611440473889804, 'test/num_examples': 95000000, 'score': 6007.8526430130005, 'total_duration': 6141.8548221588135, 'accumulated_submission_time': 6007.8526430130005, 'accumulated_eval_time': 133.62959718704224, 'accumulated_logging_time': 0.11743664741516113, 'global_step': 7909, 'preemption_count': 0}), (9492, {'train/loss': 0.12048418599667039, 'validation/loss': 0.123623332617319, 'validation/num_examples': 83274637, 'test/loss': 0.12595479833470394, 'test/num_examples': 95000000, 'score': 7208.395542383194, 'total_duration': 7364.707081079483, 'accumulated_submission_time': 7208.395542383194, 'accumulated_eval_time': 155.8615963459015, 'accumulated_logging_time': 0.14365887641906738, 'global_step': 9492, 'preemption_count': 0})], 'global_step': 10145}
I0217 23:59:45.547439 139973357201216 submission_runner.py:586] Timing: 7703.6261920928955
I0217 23:59:45.547488 139973357201216 submission_runner.py:588] Total number of evals: 7
I0217 23:59:45.547549 139973357201216 submission_runner.py:589] ====================
I0217 23:59:45.547784 139973357201216 submission_runner.py:673] Final criteo1tb score: 7703.379245758057
