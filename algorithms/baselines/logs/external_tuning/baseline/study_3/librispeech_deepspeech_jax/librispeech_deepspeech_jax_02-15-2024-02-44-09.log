python3 submission_runner.py --framework=jax --workload=librispeech_deepspeech --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification/study_3 --overwrite=true --save_checkpoints=false --num_tuning_trials=5 --rng_seed=4294683350 --max_global_steps=48000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_deepspeech_jax_02-15-2024-02-44-09.log
I0215 02:44:31.155146 140202902193984 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification/study_3/librispeech_deepspeech_jax.
I0215 02:44:32.180508 140202902193984 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0215 02:44:32.181267 140202902193984 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0215 02:44:32.181402 140202902193984 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0215 02:44:32.182454 140202902193984 submission_runner.py:542] Using RNG seed 4294683350
I0215 02:44:33.336725 140202902193984 submission_runner.py:551] --- Tuning run 1/5 ---
I0215 02:44:33.336920 140202902193984 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_3/librispeech_deepspeech_jax/trial_1.
I0215 02:44:33.337104 140202902193984 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_3/librispeech_deepspeech_jax/trial_1/hparams.json.
I0215 02:44:33.519957 140202902193984 submission_runner.py:206] Initializing dataset.
I0215 02:44:33.520186 140202902193984 submission_runner.py:213] Initializing model.
I0215 02:44:36.122246 140202902193984 submission_runner.py:255] Initializing optimizer.
I0215 02:44:36.831902 140202902193984 submission_runner.py:262] Initializing metrics bundle.
I0215 02:44:36.832087 140202902193984 submission_runner.py:280] Initializing checkpoint and logger.
I0215 02:44:36.833128 140202902193984 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_3/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I0215 02:44:36.833270 140202902193984 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_3/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I0215 02:44:36.833478 140202902193984 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0215 02:44:36.833545 140202902193984 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0215 02:44:37.134342 140202902193984 logger_utils.py:220] Unable to record git information. Continuing without it.
I0215 02:44:37.480710 140202902193984 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_3/librispeech_deepspeech_jax/trial_1/flags_0.json.
I0215 02:44:37.495610 140202902193984 submission_runner.py:314] Starting training loop.
I0215 02:44:37.793986 140202902193984 input_pipeline.py:20] Loading split = train-clean-100
I0215 02:44:37.838200 140202902193984 input_pipeline.py:20] Loading split = train-clean-360
I0215 02:44:37.962493 140202902193984 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0215 02:45:22.023444 140038760429312 logging_writer.py:48] [0] global_step=0, grad_norm=22.14777374267578, loss=32.61582565307617
I0215 02:45:22.056889 140202902193984 spec.py:321] Evaluating on the training split.
I0215 02:45:22.319680 140202902193984 input_pipeline.py:20] Loading split = train-clean-100
I0215 02:45:22.355595 140202902193984 input_pipeline.py:20] Loading split = train-clean-360
I0215 02:45:22.725930 140202902193984 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0215 02:47:19.008030 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 02:47:19.203665 140202902193984 input_pipeline.py:20] Loading split = dev-clean
I0215 02:47:19.209315 140202902193984 input_pipeline.py:20] Loading split = dev-other
I0215 02:48:33.211546 140202902193984 spec.py:349] Evaluating on the test split.
I0215 02:48:33.410020 140202902193984 input_pipeline.py:20] Loading split = test-clean
I0215 02:49:15.637539 140202902193984 submission_runner.py:408] Time since start: 278.14s, 	Step: 1, 	{'train/ctc_loss': Array(31.23901, dtype=float32), 'train/wer': 3.532270995148158, 'validation/ctc_loss': Array(30.351154, dtype=float32), 'validation/wer': 3.0460816590555817, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.536453, dtype=float32), 'test/wer': 3.361383624804501, 'test/num_examples': 2472, 'score': 44.56120181083679, 'total_duration': 278.1395950317383, 'accumulated_submission_time': 44.56120181083679, 'accumulated_eval_time': 233.57833003997803, 'accumulated_logging_time': 0}
I0215 02:49:15.667321 140028939695872 logging_writer.py:48] [1] accumulated_eval_time=233.578330, accumulated_logging_time=0, accumulated_submission_time=44.561202, global_step=1, preemption_count=0, score=44.561202, test/ctc_loss=30.536453247070312, test/num_examples=2472, test/wer=3.361384, total_duration=278.139595, train/ctc_loss=31.239009857177734, train/wer=3.532271, validation/ctc_loss=30.351154327392578, validation/num_examples=5348, validation/wer=3.046082
I0215 02:50:41.918658 140045086775040 logging_writer.py:48] [100] global_step=100, grad_norm=7.838081359863281, loss=9.6026611328125
I0215 02:51:58.924609 140045095167744 logging_writer.py:48] [200] global_step=200, grad_norm=1.8056128025054932, loss=6.609450340270996
I0215 02:53:16.831153 140045086775040 logging_writer.py:48] [300] global_step=300, grad_norm=0.5066530704498291, loss=5.895442485809326
I0215 02:54:33.542595 140045095167744 logging_writer.py:48] [400] global_step=400, grad_norm=0.3404887616634369, loss=5.8645124435424805
I0215 02:55:50.674345 140045086775040 logging_writer.py:48] [500] global_step=500, grad_norm=0.4674813449382782, loss=5.847517490386963
I0215 02:57:08.296138 140045095167744 logging_writer.py:48] [600] global_step=600, grad_norm=0.3229435384273529, loss=5.793175220489502
I0215 02:58:25.082705 140045086775040 logging_writer.py:48] [700] global_step=700, grad_norm=0.40629443526268005, loss=5.778879165649414
I0215 02:59:41.687709 140045095167744 logging_writer.py:48] [800] global_step=800, grad_norm=0.3693951368331909, loss=5.7178168296813965
I0215 03:00:59.024303 140045086775040 logging_writer.py:48] [900] global_step=900, grad_norm=0.46345606446266174, loss=5.604882717132568
I0215 03:02:19.045759 140045095167744 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.0211232900619507, loss=5.501336574554443
I0215 03:03:39.770209 140046431065856 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.6452600955963135, loss=5.3542327880859375
I0215 03:04:56.256636 140046422673152 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.6563786864280701, loss=5.055330276489258
I0215 03:06:13.630599 140046431065856 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.3352800607681274, loss=4.550919532775879
I0215 03:07:29.452541 140046422673152 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.360425591468811, loss=4.121361255645752
I0215 03:08:48.059408 140046431065856 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.429443597793579, loss=3.840203285217285
I0215 03:10:10.803556 140046422673152 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.0069520473480225, loss=3.5816516876220703
I0215 03:11:31.326380 140046431065856 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.8443293571472168, loss=3.424828052520752
I0215 03:12:53.008691 140046422673152 logging_writer.py:48] [1800] global_step=1800, grad_norm=3.2240731716156006, loss=3.2808568477630615
I0215 03:13:16.652399 140202902193984 spec.py:321] Evaluating on the training split.
I0215 03:13:55.116529 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 03:14:41.261339 140202902193984 spec.py:349] Evaluating on the test split.
I0215 03:15:04.571325 140202902193984 submission_runner.py:408] Time since start: 1827.07s, 	Step: 1829, 	{'train/ctc_loss': Array(6.2994285, dtype=float32), 'train/wer': 0.9422720438301991, 'validation/ctc_loss': Array(6.226698, dtype=float32), 'validation/wer': 0.8960097318902845, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.149786, dtype=float32), 'test/wer': 0.8985639713200496, 'test/num_examples': 2472, 'score': 1485.454912185669, 'total_duration': 1827.0729236602783, 'accumulated_submission_time': 1485.454912185669, 'accumulated_eval_time': 341.49453115463257, 'accumulated_logging_time': 0.044753074645996094}
I0215 03:15:04.598261 140046431065856 logging_writer.py:48] [1829] accumulated_eval_time=341.494531, accumulated_logging_time=0.044753, accumulated_submission_time=1485.454912, global_step=1829, preemption_count=0, score=1485.454912, test/ctc_loss=6.149785995483398, test/num_examples=2472, test/wer=0.898564, total_duration=1827.072924, train/ctc_loss=6.299428462982178, train/wer=0.942272, validation/ctc_loss=6.22669792175293, validation/num_examples=5348, validation/wer=0.896010
I0215 03:15:58.748253 140046422673152 logging_writer.py:48] [1900] global_step=1900, grad_norm=3.7624504566192627, loss=3.14778208732605
I0215 03:17:14.970062 140046431065856 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.8948482275009155, loss=2.9829630851745605
I0215 03:18:34.155488 140046431065856 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.5351879596710205, loss=2.7986598014831543
I0215 03:19:51.369384 140046422673152 logging_writer.py:48] [2200] global_step=2200, grad_norm=4.890322685241699, loss=2.8255066871643066
I0215 03:21:08.286307 140046431065856 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.6208066940307617, loss=2.7002830505371094
I0215 03:22:25.870105 140046422673152 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.829808235168457, loss=2.6490910053253174
I0215 03:23:44.581489 140046431065856 logging_writer.py:48] [2500] global_step=2500, grad_norm=4.675197124481201, loss=2.5984244346618652
I0215 03:25:08.619907 140046422673152 logging_writer.py:48] [2600] global_step=2600, grad_norm=3.662404775619507, loss=2.509406328201294
I0215 03:26:32.639019 140046431065856 logging_writer.py:48] [2700] global_step=2700, grad_norm=4.6315789222717285, loss=2.5270164012908936
I0215 03:27:57.642236 140046422673152 logging_writer.py:48] [2800] global_step=2800, grad_norm=3.4223923683166504, loss=2.3934860229492188
I0215 03:29:17.434689 140046431065856 logging_writer.py:48] [2900] global_step=2900, grad_norm=2.404284715652466, loss=2.381842851638794
I0215 03:30:39.439711 140046422673152 logging_writer.py:48] [3000] global_step=3000, grad_norm=5.669597625732422, loss=2.340925455093384
I0215 03:32:05.830429 140047086425856 logging_writer.py:48] [3100] global_step=3100, grad_norm=3.1537084579467773, loss=2.3471791744232178
I0215 03:33:21.216964 140047078033152 logging_writer.py:48] [3200] global_step=3200, grad_norm=3.9230310916900635, loss=2.261043071746826
I0215 03:34:37.661134 140047086425856 logging_writer.py:48] [3300] global_step=3300, grad_norm=3.4046337604522705, loss=2.260425090789795
I0215 03:35:52.597153 140047078033152 logging_writer.py:48] [3400] global_step=3400, grad_norm=2.718353033065796, loss=2.185800552368164
I0215 03:37:10.594874 140047086425856 logging_writer.py:48] [3500] global_step=3500, grad_norm=3.3597514629364014, loss=2.208317518234253
I0215 03:38:32.764725 140047078033152 logging_writer.py:48] [3600] global_step=3600, grad_norm=5.071929454803467, loss=2.154470205307007
I0215 03:39:04.653979 140202902193984 spec.py:321] Evaluating on the training split.
I0215 03:39:53.562898 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 03:40:42.776058 140202902193984 spec.py:349] Evaluating on the test split.
I0215 03:41:07.906526 140202902193984 submission_runner.py:408] Time since start: 3390.41s, 	Step: 3638, 	{'train/ctc_loss': Array(3.389681, dtype=float32), 'train/wer': 0.697909064272967, 'validation/ctc_loss': Array(3.6511831, dtype=float32), 'validation/wer': 0.7208646707280574, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.2384326, dtype=float32), 'test/wer': 0.6621168728292, 'test/num_examples': 2472, 'score': 2925.4223692417145, 'total_duration': 3390.408249616623, 'accumulated_submission_time': 2925.4223692417145, 'accumulated_eval_time': 464.7445025444031, 'accumulated_logging_time': 0.08341050148010254}
I0215 03:41:07.934138 140047086425856 logging_writer.py:48] [3638] accumulated_eval_time=464.744503, accumulated_logging_time=0.083411, accumulated_submission_time=2925.422369, global_step=3638, preemption_count=0, score=2925.422369, test/ctc_loss=3.2384326457977295, test/num_examples=2472, test/wer=0.662117, total_duration=3390.408250, train/ctc_loss=3.389681100845337, train/wer=0.697909, validation/ctc_loss=3.6511831283569336, validation/num_examples=5348, validation/wer=0.720865
I0215 03:41:54.975956 140047078033152 logging_writer.py:48] [3700] global_step=3700, grad_norm=4.244440078735352, loss=2.130631685256958
I0215 03:43:09.831929 140047086425856 logging_writer.py:48] [3800] global_step=3800, grad_norm=3.4006030559539795, loss=2.139986276626587
I0215 03:44:25.989963 140047078033152 logging_writer.py:48] [3900] global_step=3900, grad_norm=4.965709209442139, loss=2.1259982585906982
I0215 03:45:44.882242 140047086425856 logging_writer.py:48] [4000] global_step=4000, grad_norm=3.877946615219116, loss=2.154930830001831
I0215 03:47:05.985209 140047078033152 logging_writer.py:48] [4100] global_step=4100, grad_norm=3.6390984058380127, loss=2.1001224517822266
I0215 03:48:28.315882 140046431065856 logging_writer.py:48] [4200] global_step=4200, grad_norm=4.703920364379883, loss=2.0646471977233887
I0215 03:49:43.898460 140046422673152 logging_writer.py:48] [4300] global_step=4300, grad_norm=3.240823984146118, loss=2.064523696899414
I0215 03:50:58.735962 140046431065856 logging_writer.py:48] [4400] global_step=4400, grad_norm=3.0007827281951904, loss=2.032872438430786
I0215 03:52:14.638450 140046422673152 logging_writer.py:48] [4500] global_step=4500, grad_norm=3.2843422889709473, loss=2.0156571865081787
I0215 03:53:39.597278 140046431065856 logging_writer.py:48] [4600] global_step=4600, grad_norm=3.3876051902770996, loss=2.0031545162200928
I0215 03:55:01.827939 140046422673152 logging_writer.py:48] [4700] global_step=4700, grad_norm=4.305121421813965, loss=1.9461629390716553
I0215 03:56:25.195669 140046431065856 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.9411840438842773, loss=1.9721966981887817
I0215 03:57:43.318558 140046422673152 logging_writer.py:48] [4900] global_step=4900, grad_norm=3.799720525741577, loss=1.9294577836990356
I0215 03:59:04.869978 140046431065856 logging_writer.py:48] [5000] global_step=5000, grad_norm=4.584596157073975, loss=2.045379161834717
I0215 04:00:30.998282 140046422673152 logging_writer.py:48] [5100] global_step=5100, grad_norm=2.8581817150115967, loss=1.905234694480896
I0215 04:01:53.091506 140047086425856 logging_writer.py:48] [5200] global_step=5200, grad_norm=4.109742641448975, loss=1.9303689002990723
I0215 04:03:09.492258 140047078033152 logging_writer.py:48] [5300] global_step=5300, grad_norm=3.6417176723480225, loss=1.9246035814285278
I0215 04:04:25.373038 140047086425856 logging_writer.py:48] [5400] global_step=5400, grad_norm=3.4324309825897217, loss=2.002192735671997
I0215 04:05:08.418945 140202902193984 spec.py:321] Evaluating on the training split.
I0215 04:06:04.984815 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 04:06:56.514700 140202902193984 spec.py:349] Evaluating on the test split.
I0215 04:07:22.302992 140202902193984 submission_runner.py:408] Time since start: 4964.80s, 	Step: 5458, 	{'train/ctc_loss': Array(0.6259818, dtype=float32), 'train/wer': 0.2104833140935423, 'validation/ctc_loss': Array(1.0288459, dtype=float32), 'validation/wer': 0.28744798555663903, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.68839693, dtype=float32), 'test/wer': 0.21467308512583025, 'test/num_examples': 2472, 'score': 4365.819350719452, 'total_duration': 4964.804627656937, 'accumulated_submission_time': 4365.819350719452, 'accumulated_eval_time': 598.6258668899536, 'accumulated_logging_time': 0.12372756004333496}
I0215 04:07:22.327838 140047086425856 logging_writer.py:48] [5458] accumulated_eval_time=598.625867, accumulated_logging_time=0.123728, accumulated_submission_time=4365.819351, global_step=5458, preemption_count=0, score=4365.819351, test/ctc_loss=0.6883969306945801, test/num_examples=2472, test/wer=0.214673, total_duration=4964.804628, train/ctc_loss=0.6259818077087402, train/wer=0.210483, validation/ctc_loss=1.0288459062576294, validation/num_examples=5348, validation/wer=0.287448
I0215 04:07:54.645825 140047078033152 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.799380302429199, loss=1.8749723434448242
I0215 04:09:11.357651 140047086425856 logging_writer.py:48] [5600] global_step=5600, grad_norm=3.3403263092041016, loss=1.8817154169082642
I0215 04:10:26.266321 140047078033152 logging_writer.py:48] [5700] global_step=5700, grad_norm=3.892305374145508, loss=1.8327635526657104
I0215 04:11:44.474022 140047086425856 logging_writer.py:48] [5800] global_step=5800, grad_norm=3.486438751220703, loss=1.8673744201660156
I0215 04:13:05.623410 140047078033152 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.921774387359619, loss=1.9208283424377441
I0215 04:14:27.895539 140047086425856 logging_writer.py:48] [6000] global_step=6000, grad_norm=3.923670530319214, loss=1.849089503288269
I0215 04:15:52.805764 140047078033152 logging_writer.py:48] [6100] global_step=6100, grad_norm=5.7859954833984375, loss=1.9528346061706543
I0215 04:17:19.505534 140046431065856 logging_writer.py:48] [6200] global_step=6200, grad_norm=5.186900615692139, loss=1.8350896835327148
I0215 04:18:34.980291 140046422673152 logging_writer.py:48] [6300] global_step=6300, grad_norm=3.025472402572632, loss=1.8169398307800293
I0215 04:19:53.290523 140046431065856 logging_writer.py:48] [6400] global_step=6400, grad_norm=3.0765411853790283, loss=1.7624315023422241
I0215 04:21:08.743419 140046422673152 logging_writer.py:48] [6500] global_step=6500, grad_norm=3.30877947807312, loss=1.7738181352615356
I0215 04:22:28.729044 140046431065856 logging_writer.py:48] [6600] global_step=6600, grad_norm=3.893984079360962, loss=1.790062665939331
I0215 04:23:52.479751 140046422673152 logging_writer.py:48] [6700] global_step=6700, grad_norm=3.0177841186523438, loss=1.8089492321014404
I0215 04:25:18.891501 140046431065856 logging_writer.py:48] [6800] global_step=6800, grad_norm=2.5479397773742676, loss=1.7747466564178467
I0215 04:26:43.495881 140046422673152 logging_writer.py:48] [6900] global_step=6900, grad_norm=4.561333179473877, loss=1.7940937280654907
I0215 04:28:10.029932 140046431065856 logging_writer.py:48] [7000] global_step=7000, grad_norm=3.337599754333496, loss=1.7794045209884644
I0215 04:29:35.431201 140046422673152 logging_writer.py:48] [7100] global_step=7100, grad_norm=4.245173931121826, loss=1.8144280910491943
I0215 04:30:59.787352 140046431065856 logging_writer.py:48] [7200] global_step=7200, grad_norm=5.764108657836914, loss=1.7352455854415894
I0215 04:31:22.772768 140202902193984 spec.py:321] Evaluating on the training split.
I0215 04:32:23.113235 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 04:33:14.673907 140202902193984 spec.py:349] Evaluating on the test split.
I0215 04:33:40.754290 140202902193984 submission_runner.py:408] Time since start: 6543.26s, 	Step: 7226, 	{'train/ctc_loss': Array(0.48525193, dtype=float32), 'train/wer': 0.16322822524060362, 'validation/ctc_loss': Array(0.8257652, dtype=float32), 'validation/wer': 0.23722448033829904, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5244182, dtype=float32), 'test/wer': 0.1696220015030569, 'test/num_examples': 2472, 'score': 5806.1792895793915, 'total_duration': 6543.255874633789, 'accumulated_submission_time': 5806.1792895793915, 'accumulated_eval_time': 736.6049258708954, 'accumulated_logging_time': 0.16009974479675293}
I0215 04:33:40.780743 140046502745856 logging_writer.py:48] [7226] accumulated_eval_time=736.604926, accumulated_logging_time=0.160100, accumulated_submission_time=5806.179290, global_step=7226, preemption_count=0, score=5806.179290, test/ctc_loss=0.5244181752204895, test/num_examples=2472, test/wer=0.169622, total_duration=6543.255875, train/ctc_loss=0.48525193333625793, train/wer=0.163228, validation/ctc_loss=0.8257651925086975, validation/num_examples=5348, validation/wer=0.237224
I0215 04:34:36.835781 140046494353152 logging_writer.py:48] [7300] global_step=7300, grad_norm=3.8560094833374023, loss=1.802605390548706
I0215 04:35:51.657081 140046502745856 logging_writer.py:48] [7400] global_step=7400, grad_norm=3.516148567199707, loss=1.731151819229126
I0215 04:37:07.653529 140046494353152 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.2340710163116455, loss=1.765595555305481
I0215 04:38:22.661799 140046502745856 logging_writer.py:48] [7600] global_step=7600, grad_norm=5.4089508056640625, loss=1.6685242652893066
I0215 04:39:37.344424 140046494353152 logging_writer.py:48] [7700] global_step=7700, grad_norm=4.655694961547852, loss=1.743796944618225
I0215 04:40:57.775483 140046502745856 logging_writer.py:48] [7800] global_step=7800, grad_norm=4.362552642822266, loss=1.728509783744812
I0215 04:42:19.750872 140046494353152 logging_writer.py:48] [7900] global_step=7900, grad_norm=4.2866129875183105, loss=1.7238085269927979
I0215 04:43:44.943873 140046502745856 logging_writer.py:48] [8000] global_step=8000, grad_norm=3.79101300239563, loss=1.7625656127929688
I0215 04:45:11.517922 140046494353152 logging_writer.py:48] [8100] global_step=8100, grad_norm=3.965216875076294, loss=1.7285877466201782
I0215 04:46:33.474469 140046502745856 logging_writer.py:48] [8200] global_step=8200, grad_norm=3.5078907012939453, loss=1.6571714878082275
I0215 04:47:56.051075 140046502745856 logging_writer.py:48] [8300] global_step=8300, grad_norm=4.252633094787598, loss=1.740952730178833
I0215 04:49:11.643531 140046494353152 logging_writer.py:48] [8400] global_step=8400, grad_norm=2.9313433170318604, loss=1.6696044206619263
I0215 04:50:29.165053 140046502745856 logging_writer.py:48] [8500] global_step=8500, grad_norm=4.030802249908447, loss=1.6124533414840698
I0215 04:51:45.252441 140046494353152 logging_writer.py:48] [8600] global_step=8600, grad_norm=2.841610908508301, loss=1.7398658990859985
I0215 04:53:07.191186 140046502745856 logging_writer.py:48] [8700] global_step=8700, grad_norm=2.7040278911590576, loss=1.69007408618927
I0215 04:54:30.550350 140046494353152 logging_writer.py:48] [8800] global_step=8800, grad_norm=3.3157269954681396, loss=1.6648741960525513
I0215 04:55:52.416724 140046502745856 logging_writer.py:48] [8900] global_step=8900, grad_norm=2.6739492416381836, loss=1.627549171447754
I0215 04:57:18.310586 140046494353152 logging_writer.py:48] [9000] global_step=9000, grad_norm=3.6436655521392822, loss=1.6602470874786377
I0215 04:57:41.305500 140202902193984 spec.py:321] Evaluating on the training split.
I0215 04:58:40.501346 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 04:59:31.344704 140202902193984 spec.py:349] Evaluating on the test split.
I0215 04:59:56.580571 140202902193984 submission_runner.py:408] Time since start: 8119.08s, 	Step: 9028, 	{'train/ctc_loss': Array(0.43553168, dtype=float32), 'train/wer': 0.14760594924873338, 'validation/ctc_loss': Array(0.76645046, dtype=float32), 'validation/wer': 0.2220570203809726, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.47539398, dtype=float32), 'test/wer': 0.15251965145329352, 'test/num_examples': 2472, 'score': 7246.616269826889, 'total_duration': 8119.081926584244, 'accumulated_submission_time': 7246.616269826889, 'accumulated_eval_time': 871.8770303726196, 'accumulated_logging_time': 0.19861793518066406}
I0215 04:59:56.607568 140046364505856 logging_writer.py:48] [9028] accumulated_eval_time=871.877030, accumulated_logging_time=0.198618, accumulated_submission_time=7246.616270, global_step=9028, preemption_count=0, score=7246.616270, test/ctc_loss=0.47539398074150085, test/num_examples=2472, test/wer=0.152520, total_duration=8119.081927, train/ctc_loss=0.4355316758155823, train/wer=0.147606, validation/ctc_loss=0.7664504647254944, validation/num_examples=5348, validation/wer=0.222057
I0215 05:00:51.312248 140046356113152 logging_writer.py:48] [9100] global_step=9100, grad_norm=2.8042054176330566, loss=1.7064553499221802
I0215 05:02:07.318243 140046364505856 logging_writer.py:48] [9200] global_step=9200, grad_norm=2.5497450828552246, loss=1.6451431512832642
I0215 05:03:25.948333 140046364505856 logging_writer.py:48] [9300] global_step=9300, grad_norm=3.188077688217163, loss=1.6365200281143188
I0215 05:04:42.678955 140046356113152 logging_writer.py:48] [9400] global_step=9400, grad_norm=3.338663101196289, loss=1.7031170129776
I0215 05:06:00.172731 140046364505856 logging_writer.py:48] [9500] global_step=9500, grad_norm=4.443976402282715, loss=1.6954338550567627
I0215 05:07:16.448997 140046356113152 logging_writer.py:48] [9600] global_step=9600, grad_norm=3.30319881439209, loss=1.6840882301330566
I0215 05:08:40.630033 140046364505856 logging_writer.py:48] [9700] global_step=9700, grad_norm=3.501880645751953, loss=1.643706202507019
I0215 05:10:04.015131 140046356113152 logging_writer.py:48] [9800] global_step=9800, grad_norm=5.272682189941406, loss=1.6168776750564575
I0215 05:11:27.303208 140046364505856 logging_writer.py:48] [9900] global_step=9900, grad_norm=3.1560282707214355, loss=1.626660943031311
I0215 05:12:52.894905 140046356113152 logging_writer.py:48] [10000] global_step=10000, grad_norm=2.805602788925171, loss=1.5705689191818237
I0215 05:14:19.179104 140046364505856 logging_writer.py:48] [10100] global_step=10100, grad_norm=3.073086738586426, loss=1.6477446556091309
I0215 05:15:39.787697 140046356113152 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.9653699398040771, loss=1.5735642910003662
I0215 05:17:04.108476 140046036825856 logging_writer.py:48] [10300] global_step=10300, grad_norm=2.9325246810913086, loss=1.6322369575500488
I0215 05:18:19.271716 140046028433152 logging_writer.py:48] [10400] global_step=10400, grad_norm=3.159276247024536, loss=1.6243069171905518
I0215 05:19:37.376518 140046036825856 logging_writer.py:48] [10500] global_step=10500, grad_norm=4.311147689819336, loss=1.6609529256820679
I0215 05:20:57.253327 140046028433152 logging_writer.py:48] [10600] global_step=10600, grad_norm=3.405231475830078, loss=1.534010887145996
I0215 05:22:19.574252 140046036825856 logging_writer.py:48] [10700] global_step=10700, grad_norm=2.3439130783081055, loss=1.650469183921814
I0215 05:23:41.342606 140046028433152 logging_writer.py:48] [10800] global_step=10800, grad_norm=2.9018421173095703, loss=1.6125141382217407
I0215 05:23:57.104701 140202902193984 spec.py:321] Evaluating on the training split.
I0215 05:24:52.476402 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 05:25:44.598967 140202902193984 spec.py:349] Evaluating on the test split.
I0215 05:26:11.928888 140202902193984 submission_runner.py:408] Time since start: 9694.43s, 	Step: 10819, 	{'train/ctc_loss': Array(0.4950031, dtype=float32), 'train/wer': 0.1622237278008653, 'validation/ctc_loss': Array(0.8763321, dtype=float32), 'validation/wer': 0.24400204678644874, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.50596225, dtype=float32), 'test/wer': 0.16094895700038592, 'test/num_examples': 2472, 'score': 8687.026130914688, 'total_duration': 9694.430442094803, 'accumulated_submission_time': 8687.026130914688, 'accumulated_eval_time': 1006.6984577178955, 'accumulated_logging_time': 0.23746633529663086}
I0215 05:26:11.955410 140046794585856 logging_writer.py:48] [10819] accumulated_eval_time=1006.698458, accumulated_logging_time=0.237466, accumulated_submission_time=8687.026131, global_step=10819, preemption_count=0, score=8687.026131, test/ctc_loss=0.5059622526168823, test/num_examples=2472, test/wer=0.160949, total_duration=9694.430442, train/ctc_loss=0.4950031042098999, train/wer=0.162224, validation/ctc_loss=0.8763321042060852, validation/num_examples=5348, validation/wer=0.244002
I0215 05:27:14.017771 140046786193152 logging_writer.py:48] [10900] global_step=10900, grad_norm=2.5998778343200684, loss=1.633121371269226
I0215 05:28:30.022288 140046794585856 logging_writer.py:48] [11000] global_step=11000, grad_norm=3.4133975505828857, loss=1.6157954931259155
I0215 05:29:45.512134 140046786193152 logging_writer.py:48] [11100] global_step=11100, grad_norm=3.2592289447784424, loss=1.605659008026123
I0215 05:31:01.190729 140046794585856 logging_writer.py:48] [11200] global_step=11200, grad_norm=3.0416860580444336, loss=1.5762927532196045
I0215 05:32:22.113937 140046786193152 logging_writer.py:48] [11300] global_step=11300, grad_norm=3.2224550247192383, loss=1.6366870403289795
I0215 05:33:44.482427 140045483865856 logging_writer.py:48] [11400] global_step=11400, grad_norm=2.847539186477661, loss=1.629699945449829
I0215 05:35:01.238841 140045475473152 logging_writer.py:48] [11500] global_step=11500, grad_norm=2.867107629776001, loss=1.5803810358047485
I0215 05:36:17.238818 140045483865856 logging_writer.py:48] [11600] global_step=11600, grad_norm=3.377476215362549, loss=1.6053987741470337
I0215 05:37:33.631999 140045475473152 logging_writer.py:48] [11700] global_step=11700, grad_norm=2.8733437061309814, loss=1.5419195890426636
I0215 05:38:58.134579 140045483865856 logging_writer.py:48] [11800] global_step=11800, grad_norm=2.3776776790618896, loss=1.638657808303833
I0215 05:40:23.444911 140045475473152 logging_writer.py:48] [11900] global_step=11900, grad_norm=3.4755308628082275, loss=1.5530848503112793
I0215 05:41:46.918397 140045483865856 logging_writer.py:48] [12000] global_step=12000, grad_norm=2.3125014305114746, loss=1.5571205615997314
I0215 05:43:07.512156 140045475473152 logging_writer.py:48] [12100] global_step=12100, grad_norm=2.3247275352478027, loss=1.587165117263794
I0215 05:44:32.601482 140045483865856 logging_writer.py:48] [12200] global_step=12200, grad_norm=2.4868853092193604, loss=1.6289963722229004
I0215 05:45:57.728781 140045475473152 logging_writer.py:48] [12300] global_step=12300, grad_norm=2.7718327045440674, loss=1.6067653894424438
I0215 05:47:23.273994 140046794585856 logging_writer.py:48] [12400] global_step=12400, grad_norm=4.6886115074157715, loss=1.5479919910430908
I0215 05:48:38.962904 140046786193152 logging_writer.py:48] [12500] global_step=12500, grad_norm=4.92236328125, loss=1.6011141538619995
I0215 05:49:55.341827 140046794585856 logging_writer.py:48] [12600] global_step=12600, grad_norm=4.352078914642334, loss=1.6367603540420532
I0215 05:50:12.666215 140202902193984 spec.py:321] Evaluating on the training split.
I0215 05:51:08.001276 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 05:51:58.488313 140202902193984 spec.py:349] Evaluating on the test split.
I0215 05:52:25.465595 140202902193984 submission_runner.py:408] Time since start: 11267.97s, 	Step: 12624, 	{'train/ctc_loss': Array(0.37129277, dtype=float32), 'train/wer': 0.1258162635866675, 'validation/ctc_loss': Array(0.6872197, dtype=float32), 'validation/wer': 0.1990210181797117, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4055817, dtype=float32), 'test/wer': 0.12944569699185507, 'test/num_examples': 2472, 'score': 10127.646817445755, 'total_duration': 11267.967316865921, 'accumulated_submission_time': 10127.646817445755, 'accumulated_eval_time': 1139.495243549347, 'accumulated_logging_time': 0.2780308723449707}
I0215 05:52:25.494410 140046794585856 logging_writer.py:48] [12624] accumulated_eval_time=1139.495244, accumulated_logging_time=0.278031, accumulated_submission_time=10127.646817, global_step=12624, preemption_count=0, score=10127.646817, test/ctc_loss=0.4055817127227783, test/num_examples=2472, test/wer=0.129446, total_duration=11267.967317, train/ctc_loss=0.37129276990890503, train/wer=0.125816, validation/ctc_loss=0.6872196793556213, validation/num_examples=5348, validation/wer=0.199021
I0215 05:53:23.117076 140046786193152 logging_writer.py:48] [12700] global_step=12700, grad_norm=3.1468749046325684, loss=1.5648030042648315
I0215 05:54:38.669895 140046794585856 logging_writer.py:48] [12800] global_step=12800, grad_norm=3.0083422660827637, loss=1.656649112701416
I0215 05:55:54.060605 140046786193152 logging_writer.py:48] [12900] global_step=12900, grad_norm=2.415961503982544, loss=1.5803587436676025
I0215 05:57:16.772699 140046794585856 logging_writer.py:48] [13000] global_step=13000, grad_norm=3.3574576377868652, loss=1.5638192892074585
I0215 05:58:38.736161 140046786193152 logging_writer.py:48] [13100] global_step=13100, grad_norm=2.6762795448303223, loss=1.5821384191513062
I0215 06:00:03.411830 140046794585856 logging_writer.py:48] [13200] global_step=13200, grad_norm=2.927581310272217, loss=1.5578068494796753
I0215 06:01:29.279469 140046786193152 logging_writer.py:48] [13300] global_step=13300, grad_norm=2.633639335632324, loss=1.6022515296936035
I0215 06:02:58.592710 140046794585856 logging_writer.py:48] [13400] global_step=13400, grad_norm=3.618624210357666, loss=1.545750379562378
I0215 06:04:15.518001 140046786193152 logging_writer.py:48] [13500] global_step=13500, grad_norm=2.7730066776275635, loss=1.5810582637786865
I0215 06:05:34.268802 140046794585856 logging_writer.py:48] [13600] global_step=13600, grad_norm=3.0540835857391357, loss=1.577052354812622
I0215 06:06:52.222296 140046786193152 logging_writer.py:48] [13700] global_step=13700, grad_norm=3.842982769012451, loss=1.6404883861541748
I0215 06:08:10.431858 140046794585856 logging_writer.py:48] [13800] global_step=13800, grad_norm=2.039897918701172, loss=1.5235899686813354
I0215 06:09:33.960833 140046786193152 logging_writer.py:48] [13900] global_step=13900, grad_norm=3.1474833488464355, loss=1.591416597366333
I0215 06:10:57.378133 140046794585856 logging_writer.py:48] [14000] global_step=14000, grad_norm=4.237532138824463, loss=1.5007940530776978
I0215 06:12:17.256394 140046786193152 logging_writer.py:48] [14100] global_step=14100, grad_norm=2.969910144805908, loss=1.4780173301696777
I0215 06:13:40.290226 140046794585856 logging_writer.py:48] [14200] global_step=14200, grad_norm=2.502739906311035, loss=1.5631988048553467
I0215 06:15:01.173380 140046786193152 logging_writer.py:48] [14300] global_step=14300, grad_norm=2.304074287414551, loss=1.5651941299438477
I0215 06:16:26.086248 140202902193984 spec.py:321] Evaluating on the training split.
I0215 06:17:22.093930 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 06:18:12.513804 140202902193984 spec.py:349] Evaluating on the test split.
I0215 06:18:38.930861 140202902193984 submission_runner.py:408] Time since start: 12841.43s, 	Step: 14400, 	{'train/ctc_loss': Array(0.31816357, dtype=float32), 'train/wer': 0.11004966327777634, 'validation/ctc_loss': Array(0.6623367, dtype=float32), 'validation/wer': 0.19065043397665504, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39234465, dtype=float32), 'test/wer': 0.12615522109154428, 'test/num_examples': 2472, 'score': 11568.152488470078, 'total_duration': 12841.431944847107, 'accumulated_submission_time': 11568.152488470078, 'accumulated_eval_time': 1272.3366241455078, 'accumulated_logging_time': 0.3184385299682617}
I0215 06:18:38.961805 140046794585856 logging_writer.py:48] [14400] accumulated_eval_time=1272.336624, accumulated_logging_time=0.318439, accumulated_submission_time=11568.152488, global_step=14400, preemption_count=0, score=11568.152488, test/ctc_loss=0.3923446536064148, test/num_examples=2472, test/wer=0.126155, total_duration=12841.431945, train/ctc_loss=0.31816357374191284, train/wer=0.110050, validation/ctc_loss=0.6623367071151733, validation/num_examples=5348, validation/wer=0.190650
I0215 06:18:39.833936 140046786193152 logging_writer.py:48] [14400] global_step=14400, grad_norm=3.3625831604003906, loss=1.544814109802246
I0215 06:19:59.097065 140046794585856 logging_writer.py:48] [14500] global_step=14500, grad_norm=3.112954616546631, loss=1.5786837339401245
I0215 06:21:14.557727 140046786193152 logging_writer.py:48] [14600] global_step=14600, grad_norm=3.2424559593200684, loss=1.5189462900161743
I0215 06:22:32.372726 140046794585856 logging_writer.py:48] [14700] global_step=14700, grad_norm=3.7221200466156006, loss=1.5159989595413208
I0215 06:23:55.797062 140046786193152 logging_writer.py:48] [14800] global_step=14800, grad_norm=3.1599154472351074, loss=1.5321763753890991
I0215 06:25:19.246751 140046794585856 logging_writer.py:48] [14900] global_step=14900, grad_norm=2.7404162883758545, loss=1.6069121360778809
I0215 06:26:41.827265 140046786193152 logging_writer.py:48] [15000] global_step=15000, grad_norm=4.947569370269775, loss=1.5200281143188477
I0215 06:28:08.771801 140046794585856 logging_writer.py:48] [15100] global_step=15100, grad_norm=3.6111063957214355, loss=1.5137437582015991
I0215 06:29:32.119080 140046786193152 logging_writer.py:48] [15200] global_step=15200, grad_norm=3.184866428375244, loss=1.5118950605392456
I0215 06:30:55.292340 140046794585856 logging_writer.py:48] [15300] global_step=15300, grad_norm=2.821418285369873, loss=1.5869303941726685
I0215 06:32:19.090871 140046786193152 logging_writer.py:48] [15400] global_step=15400, grad_norm=2.238234281539917, loss=1.5629327297210693
I0215 06:33:42.716676 140046794585856 logging_writer.py:48] [15500] global_step=15500, grad_norm=2.5207278728485107, loss=1.4942904710769653
I0215 06:34:58.790533 140046786193152 logging_writer.py:48] [15600] global_step=15600, grad_norm=3.655402421951294, loss=1.5352457761764526
I0215 06:36:15.120669 140046794585856 logging_writer.py:48] [15700] global_step=15700, grad_norm=2.3811519145965576, loss=1.5336220264434814
I0215 06:37:36.345039 140046786193152 logging_writer.py:48] [15800] global_step=15800, grad_norm=2.1201305389404297, loss=1.5507946014404297
I0215 06:39:01.049623 140046794585856 logging_writer.py:48] [15900] global_step=15900, grad_norm=3.410875082015991, loss=1.4710701704025269
I0215 06:40:20.851784 140046786193152 logging_writer.py:48] [16000] global_step=16000, grad_norm=3.271491765975952, loss=1.530975103378296
I0215 06:41:44.728590 140046794585856 logging_writer.py:48] [16100] global_step=16100, grad_norm=2.548309326171875, loss=1.5083401203155518
I0215 06:42:39.289159 140202902193984 spec.py:321] Evaluating on the training split.
I0215 06:43:34.881541 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 06:44:28.683131 140202902193984 spec.py:349] Evaluating on the test split.
I0215 06:44:55.087285 140202902193984 submission_runner.py:408] Time since start: 14417.59s, 	Step: 16169, 	{'train/ctc_loss': Array(0.3033341, dtype=float32), 'train/wer': 0.10425655294224048, 'validation/ctc_loss': Array(0.64782745, dtype=float32), 'validation/wer': 0.18617067495679543, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3739464, dtype=float32), 'test/wer': 0.12258038307639185, 'test/num_examples': 2472, 'score': 13008.389578580856, 'total_duration': 14417.589095115662, 'accumulated_submission_time': 13008.389578580856, 'accumulated_eval_time': 1408.132239818573, 'accumulated_logging_time': 0.36313295364379883}
I0215 06:44:55.114800 140046205781760 logging_writer.py:48] [16169] accumulated_eval_time=1408.132240, accumulated_logging_time=0.363133, accumulated_submission_time=13008.389579, global_step=16169, preemption_count=0, score=13008.389579, test/ctc_loss=0.3739463984966278, test/num_examples=2472, test/wer=0.122580, total_duration=14417.589095, train/ctc_loss=0.3033340871334076, train/wer=0.104257, validation/ctc_loss=0.6478274464607239, validation/num_examples=5348, validation/wer=0.186171
I0215 06:45:19.204892 140046197389056 logging_writer.py:48] [16200] global_step=16200, grad_norm=3.1357622146606445, loss=1.583846092224121
I0215 06:46:34.844711 140046205781760 logging_writer.py:48] [16300] global_step=16300, grad_norm=2.4729671478271484, loss=1.4911776781082153
I0215 06:47:51.037834 140046197389056 logging_writer.py:48] [16400] global_step=16400, grad_norm=3.449596643447876, loss=1.5340850353240967
I0215 06:49:09.805917 140046205781760 logging_writer.py:48] [16500] global_step=16500, grad_norm=3.203514814376831, loss=1.498628854751587
I0215 06:50:25.153134 140046197389056 logging_writer.py:48] [16600] global_step=16600, grad_norm=3.327209949493408, loss=1.485935926437378
I0215 06:51:40.539820 140046205781760 logging_writer.py:48] [16700] global_step=16700, grad_norm=2.0885963439941406, loss=1.499627709388733
I0215 06:52:55.512345 140046197389056 logging_writer.py:48] [16800] global_step=16800, grad_norm=2.579958438873291, loss=1.5384559631347656
I0215 06:54:13.261823 140046205781760 logging_writer.py:48] [16900] global_step=16900, grad_norm=2.134690761566162, loss=1.5712436437606812
I0215 06:55:36.864076 140046197389056 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.803514838218689, loss=1.4699336290359497
I0215 06:56:58.586077 140046205781760 logging_writer.py:48] [17100] global_step=17100, grad_norm=2.684267282485962, loss=1.5139546394348145
I0215 06:58:25.487593 140046197389056 logging_writer.py:48] [17200] global_step=17200, grad_norm=3.4934818744659424, loss=1.5293749570846558
I0215 06:59:49.645326 140046205781760 logging_writer.py:48] [17300] global_step=17300, grad_norm=2.616628646850586, loss=1.5340094566345215
I0215 07:01:12.984176 140046197389056 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.8977397680282593, loss=1.520746111869812
I0215 07:02:38.153204 140046205781760 logging_writer.py:48] [17500] global_step=17500, grad_norm=2.74237322807312, loss=1.5330342054367065
I0215 07:03:58.287618 140045550421760 logging_writer.py:48] [17600] global_step=17600, grad_norm=2.9717774391174316, loss=1.479586124420166
I0215 07:05:14.466038 140045542029056 logging_writer.py:48] [17700] global_step=17700, grad_norm=2.849745035171509, loss=1.4219260215759277
I0215 07:06:30.230445 140045550421760 logging_writer.py:48] [17800] global_step=17800, grad_norm=3.1735785007476807, loss=1.5462454557418823
I0215 07:07:51.749225 140045542029056 logging_writer.py:48] [17900] global_step=17900, grad_norm=2.5829081535339355, loss=1.5967961549758911
I0215 07:08:55.526760 140202902193984 spec.py:321] Evaluating on the training split.
I0215 07:09:50.437457 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 07:10:42.041521 140202902193984 spec.py:349] Evaluating on the test split.
I0215 07:11:08.980032 140202902193984 submission_runner.py:408] Time since start: 15991.48s, 	Step: 17980, 	{'train/ctc_loss': Array(0.29420593, dtype=float32), 'train/wer': 0.10390042205859482, 'validation/ctc_loss': Array(0.6150627, dtype=float32), 'validation/wer': 0.17816696757002037, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36189067, dtype=float32), 'test/wer': 0.11705563341661081, 'test/num_examples': 2472, 'score': 14448.712196111679, 'total_duration': 15991.481305122375, 'accumulated_submission_time': 14448.712196111679, 'accumulated_eval_time': 1541.5824666023254, 'accumulated_logging_time': 0.4025120735168457}
I0215 07:11:09.009899 140046656345856 logging_writer.py:48] [17980] accumulated_eval_time=1541.582467, accumulated_logging_time=0.402512, accumulated_submission_time=14448.712196, global_step=17980, preemption_count=0, score=14448.712196, test/ctc_loss=0.36189067363739014, test/num_examples=2472, test/wer=0.117056, total_duration=15991.481305, train/ctc_loss=0.2942059338092804, train/wer=0.103900, validation/ctc_loss=0.6150627136230469, validation/num_examples=5348, validation/wer=0.178167
I0215 07:11:24.847981 140046647953152 logging_writer.py:48] [18000] global_step=18000, grad_norm=4.263153553009033, loss=1.4501793384552002
I0215 07:12:41.221347 140046656345856 logging_writer.py:48] [18100] global_step=18100, grad_norm=2.7290022373199463, loss=1.4853980541229248
I0215 07:13:56.636446 140046647953152 logging_writer.py:48] [18200] global_step=18200, grad_norm=2.8158392906188965, loss=1.4827781915664673
I0215 07:15:12.658873 140046656345856 logging_writer.py:48] [18300] global_step=18300, grad_norm=3.183178663253784, loss=1.46658194065094
I0215 07:16:35.156565 140046647953152 logging_writer.py:48] [18400] global_step=18400, grad_norm=2.90456223487854, loss=1.5308287143707275
I0215 07:18:00.147265 140046656345856 logging_writer.py:48] [18500] global_step=18500, grad_norm=2.478624105453491, loss=1.487815499305725
I0215 07:19:23.640234 140046000985856 logging_writer.py:48] [18600] global_step=18600, grad_norm=4.92849063873291, loss=1.4887347221374512
I0215 07:20:41.121807 140045992593152 logging_writer.py:48] [18700] global_step=18700, grad_norm=2.3802216053009033, loss=1.5334405899047852
I0215 07:21:56.896858 140046000985856 logging_writer.py:48] [18800] global_step=18800, grad_norm=3.766845941543579, loss=1.4570635557174683
I0215 07:23:16.755080 140045992593152 logging_writer.py:48] [18900] global_step=18900, grad_norm=2.7761449813842773, loss=1.507314682006836
I0215 07:24:40.408598 140046000985856 logging_writer.py:48] [19000] global_step=19000, grad_norm=3.6898181438446045, loss=1.489790678024292
I0215 07:26:00.257575 140045992593152 logging_writer.py:48] [19100] global_step=19100, grad_norm=3.349879741668701, loss=1.4714092016220093
I0215 07:27:25.761738 140046000985856 logging_writer.py:48] [19200] global_step=19200, grad_norm=2.3113770484924316, loss=1.4118804931640625
I0215 07:28:50.773828 140045992593152 logging_writer.py:48] [19300] global_step=19300, grad_norm=3.0993072986602783, loss=1.46239173412323
I0215 07:30:19.311521 140046000985856 logging_writer.py:48] [19400] global_step=19400, grad_norm=2.5907180309295654, loss=1.4833539724349976
I0215 07:31:42.706585 140045992593152 logging_writer.py:48] [19500] global_step=19500, grad_norm=2.8760788440704346, loss=1.5326883792877197
I0215 07:33:06.537299 140046656345856 logging_writer.py:48] [19600] global_step=19600, grad_norm=2.5325629711151123, loss=1.5065677165985107
I0215 07:34:22.784202 140046647953152 logging_writer.py:48] [19700] global_step=19700, grad_norm=3.134648561477661, loss=1.4360231161117554
I0215 07:35:09.203958 140202902193984 spec.py:321] Evaluating on the training split.
I0215 07:36:04.563208 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 07:36:56.131665 140202902193984 spec.py:349] Evaluating on the test split.
I0215 07:37:22.422583 140202902193984 submission_runner.py:408] Time since start: 17564.92s, 	Step: 19763, 	{'train/ctc_loss': Array(0.30935612, dtype=float32), 'train/wer': 0.1035617017901055, 'validation/ctc_loss': Array(0.6012965, dtype=float32), 'validation/wer': 0.17383202834606137, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35354367, dtype=float32), 'test/wer': 0.11299331749030123, 'test/num_examples': 2472, 'score': 15888.818793296814, 'total_duration': 17564.92405796051, 'accumulated_submission_time': 15888.818793296814, 'accumulated_eval_time': 1674.7982478141785, 'accumulated_logging_time': 0.44518375396728516}
I0215 07:37:22.452084 140047086425856 logging_writer.py:48] [19763] accumulated_eval_time=1674.798248, accumulated_logging_time=0.445184, accumulated_submission_time=15888.818793, global_step=19763, preemption_count=0, score=15888.818793, test/ctc_loss=0.3535436689853668, test/num_examples=2472, test/wer=0.112993, total_duration=17564.924058, train/ctc_loss=0.30935612320899963, train/wer=0.103562, validation/ctc_loss=0.6012964844703674, validation/num_examples=5348, validation/wer=0.173832
I0215 07:37:51.012730 140047078033152 logging_writer.py:48] [19800] global_step=19800, grad_norm=2.642296075820923, loss=1.5055094957351685
I0215 07:39:06.453589 140047086425856 logging_writer.py:48] [19900] global_step=19900, grad_norm=2.740309476852417, loss=1.4708530902862549
I0215 07:40:22.500962 140047078033152 logging_writer.py:48] [20000] global_step=20000, grad_norm=3.776395320892334, loss=1.5099050998687744
I0215 07:41:37.746461 140047086425856 logging_writer.py:48] [20100] global_step=20100, grad_norm=2.462595224380493, loss=1.3920114040374756
I0215 07:43:01.580044 140047078033152 logging_writer.py:48] [20200] global_step=20200, grad_norm=3.09370756149292, loss=1.4602869749069214
I0215 07:44:22.105640 140047086425856 logging_writer.py:48] [20300] global_step=20300, grad_norm=2.7970945835113525, loss=1.4913808107376099
I0215 07:45:47.069539 140047078033152 logging_writer.py:48] [20400] global_step=20400, grad_norm=3.215871572494507, loss=1.4847804307937622
I0215 07:47:09.271695 140047086425856 logging_writer.py:48] [20500] global_step=20500, grad_norm=2.249474048614502, loss=1.4455804824829102
I0215 07:48:36.550625 140047086425856 logging_writer.py:48] [20600] global_step=20600, grad_norm=2.4993398189544678, loss=1.4276254177093506
I0215 07:49:52.029676 140047078033152 logging_writer.py:48] [20700] global_step=20700, grad_norm=2.9799880981445312, loss=1.402080774307251
I0215 07:51:08.155026 140047086425856 logging_writer.py:48] [20800] global_step=20800, grad_norm=2.994509696960449, loss=1.4195737838745117
I0215 07:52:27.172985 140047078033152 logging_writer.py:48] [20900] global_step=20900, grad_norm=3.2199344635009766, loss=1.505587100982666
I0215 07:53:43.876711 140047086425856 logging_writer.py:48] [21000] global_step=21000, grad_norm=2.3693177700042725, loss=1.4481956958770752
I0215 07:55:10.519371 140047078033152 logging_writer.py:48] [21100] global_step=21100, grad_norm=3.905719041824341, loss=1.4623968601226807
I0215 07:56:34.783797 140047086425856 logging_writer.py:48] [21200] global_step=21200, grad_norm=3.2667319774627686, loss=1.493343472480774
I0215 07:57:57.772003 140047078033152 logging_writer.py:48] [21300] global_step=21300, grad_norm=3.090362787246704, loss=1.4750103950500488
I0215 07:59:23.777496 140047086425856 logging_writer.py:48] [21400] global_step=21400, grad_norm=3.8159501552581787, loss=1.4101672172546387
I0215 08:00:49.157428 140047078033152 logging_writer.py:48] [21500] global_step=21500, grad_norm=2.2560153007507324, loss=1.4381277561187744
I0215 08:01:22.808453 140202902193984 spec.py:321] Evaluating on the training split.
I0215 08:02:18.914129 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 08:03:11.033373 140202902193984 spec.py:349] Evaluating on the test split.
I0215 08:03:37.996337 140202902193984 submission_runner.py:408] Time since start: 19140.50s, 	Step: 21541, 	{'train/ctc_loss': Array(0.30396807, dtype=float32), 'train/wer': 0.1025509800300736, 'validation/ctc_loss': Array(0.5889155, dtype=float32), 'validation/wer': 0.17066530214236753, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34354234, dtype=float32), 'test/wer': 0.11157150691609287, 'test/num_examples': 2472, 'score': 17329.08687067032, 'total_duration': 19140.497346639633, 'accumulated_submission_time': 17329.08687067032, 'accumulated_eval_time': 1809.9828248023987, 'accumulated_logging_time': 0.48740100860595703}
I0215 08:03:38.028425 140047086425856 logging_writer.py:48] [21541] accumulated_eval_time=1809.982825, accumulated_logging_time=0.487401, accumulated_submission_time=17329.086871, global_step=21541, preemption_count=0, score=17329.086871, test/ctc_loss=0.34354233741760254, test/num_examples=2472, test/wer=0.111572, total_duration=19140.497347, train/ctc_loss=0.30396807193756104, train/wer=0.102551, validation/ctc_loss=0.5889155268669128, validation/num_examples=5348, validation/wer=0.170665
I0215 08:04:22.978555 140047078033152 logging_writer.py:48] [21600] global_step=21600, grad_norm=2.027660369873047, loss=1.4575635194778442
I0215 08:05:41.925080 140046758745856 logging_writer.py:48] [21700] global_step=21700, grad_norm=2.481207847595215, loss=1.4414715766906738
I0215 08:06:58.792208 140046750353152 logging_writer.py:48] [21800] global_step=21800, grad_norm=2.696260690689087, loss=1.424631118774414
I0215 08:08:14.727862 140046758745856 logging_writer.py:48] [21900] global_step=21900, grad_norm=3.6346781253814697, loss=1.4673333168029785
I0215 08:09:33.550157 140046750353152 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.9218484163284302, loss=1.4462502002716064
I0215 08:11:00.993824 140046758745856 logging_writer.py:48] [22100] global_step=22100, grad_norm=2.742959976196289, loss=1.4493217468261719
I0215 08:12:26.289175 140046750353152 logging_writer.py:48] [22200] global_step=22200, grad_norm=2.725311756134033, loss=1.408355474472046
I0215 08:13:52.052323 140046758745856 logging_writer.py:48] [22300] global_step=22300, grad_norm=3.3874101638793945, loss=1.5600632429122925
I0215 08:15:18.495684 140046750353152 logging_writer.py:48] [22400] global_step=22400, grad_norm=3.429821252822876, loss=1.4693189859390259
I0215 08:16:40.998798 140046758745856 logging_writer.py:48] [22500] global_step=22500, grad_norm=2.532028913497925, loss=1.4362577199935913
I0215 08:18:01.180653 140046750353152 logging_writer.py:48] [22600] global_step=22600, grad_norm=3.465463161468506, loss=1.458109736442566
I0215 08:19:27.307286 140047086425856 logging_writer.py:48] [22700] global_step=22700, grad_norm=3.3067209720611572, loss=1.4858042001724243
I0215 08:20:43.876129 140047078033152 logging_writer.py:48] [22800] global_step=22800, grad_norm=2.897855520248413, loss=1.4127562046051025
I0215 08:21:59.863576 140047086425856 logging_writer.py:48] [22900] global_step=22900, grad_norm=3.823561906814575, loss=1.4913228750228882
I0215 08:23:20.265484 140047078033152 logging_writer.py:48] [23000] global_step=23000, grad_norm=2.352989673614502, loss=1.4026004076004028
I0215 08:24:44.204985 140047086425856 logging_writer.py:48] [23100] global_step=23100, grad_norm=2.5126779079437256, loss=1.4578274488449097
I0215 08:26:05.652040 140047078033152 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.9574873447418213, loss=1.408478856086731
I0215 08:27:31.527390 140047086425856 logging_writer.py:48] [23300] global_step=23300, grad_norm=2.905550241470337, loss=1.4356606006622314
I0215 08:27:38.108945 140202902193984 spec.py:321] Evaluating on the training split.
I0215 08:28:33.912598 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 08:29:24.725389 140202902193984 spec.py:349] Evaluating on the test split.
I0215 08:29:52.145211 140202902193984 submission_runner.py:408] Time since start: 20714.65s, 	Step: 23309, 	{'train/ctc_loss': Array(0.28539538, dtype=float32), 'train/wer': 0.09700586480090544, 'validation/ctc_loss': Array(0.5723087, dtype=float32), 'validation/wer': 0.16646552806124912, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33128116, dtype=float32), 'test/wer': 0.10635143095078504, 'test/num_examples': 2472, 'score': 18769.07917690277, 'total_duration': 20714.64650440216, 'accumulated_submission_time': 18769.07917690277, 'accumulated_eval_time': 1944.0160570144653, 'accumulated_logging_time': 0.5339093208312988}
I0215 08:29:52.172211 140047086425856 logging_writer.py:48] [23309] accumulated_eval_time=1944.016057, accumulated_logging_time=0.533909, accumulated_submission_time=18769.079177, global_step=23309, preemption_count=0, score=18769.079177, test/ctc_loss=0.3312811553478241, test/num_examples=2472, test/wer=0.106351, total_duration=20714.646504, train/ctc_loss=0.28539538383483887, train/wer=0.097006, validation/ctc_loss=0.5723087191581726, validation/num_examples=5348, validation/wer=0.166466
I0215 08:31:01.898888 140047078033152 logging_writer.py:48] [23400] global_step=23400, grad_norm=4.02805233001709, loss=1.4713436365127563
I0215 08:32:17.252061 140047086425856 logging_writer.py:48] [23500] global_step=23500, grad_norm=3.644467353820801, loss=1.4352779388427734
I0215 08:33:33.862806 140047078033152 logging_writer.py:48] [23600] global_step=23600, grad_norm=2.8729193210601807, loss=1.4194415807724
I0215 08:35:02.866619 140046758745856 logging_writer.py:48] [23700] global_step=23700, grad_norm=2.807595729827881, loss=1.411430835723877
I0215 08:36:20.307349 140046750353152 logging_writer.py:48] [23800] global_step=23800, grad_norm=2.4393324851989746, loss=1.3436753749847412
I0215 08:37:38.903387 140046758745856 logging_writer.py:48] [23900] global_step=23900, grad_norm=2.384056568145752, loss=1.3826024532318115
I0215 08:39:00.744386 140046750353152 logging_writer.py:48] [24000] global_step=24000, grad_norm=4.38023042678833, loss=1.4991925954818726
I0215 08:40:22.605187 140046758745856 logging_writer.py:48] [24100] global_step=24100, grad_norm=3.133099317550659, loss=1.4018179178237915
I0215 08:41:47.828474 140046750353152 logging_writer.py:48] [24200] global_step=24200, grad_norm=3.3062589168548584, loss=1.3733099699020386
I0215 08:43:11.960415 140046758745856 logging_writer.py:48] [24300] global_step=24300, grad_norm=2.250211477279663, loss=1.3689454793930054
I0215 08:44:35.360939 140046750353152 logging_writer.py:48] [24400] global_step=24400, grad_norm=3.2781460285186768, loss=1.4386560916900635
I0215 08:46:01.981487 140046758745856 logging_writer.py:48] [24500] global_step=24500, grad_norm=2.778660774230957, loss=1.4088568687438965
I0215 08:47:23.272604 140046750353152 logging_writer.py:48] [24600] global_step=24600, grad_norm=3.2656469345092773, loss=1.4226096868515015
I0215 08:48:45.144790 140046758745856 logging_writer.py:48] [24700] global_step=24700, grad_norm=2.188368320465088, loss=1.3634730577468872
I0215 08:50:06.580141 140046758745856 logging_writer.py:48] [24800] global_step=24800, grad_norm=2.7273919582366943, loss=1.4105019569396973
I0215 08:51:22.793241 140046750353152 logging_writer.py:48] [24900] global_step=24900, grad_norm=3.531644105911255, loss=1.437917947769165
I0215 08:52:39.126973 140046758745856 logging_writer.py:48] [25000] global_step=25000, grad_norm=3.2864198684692383, loss=1.380958914756775
I0215 08:53:53.023660 140202902193984 spec.py:321] Evaluating on the training split.
I0215 08:54:48.442035 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 08:55:39.946928 140202902193984 spec.py:349] Evaluating on the test split.
I0215 08:56:07.518882 140202902193984 submission_runner.py:408] Time since start: 22290.02s, 	Step: 25094, 	{'train/ctc_loss': Array(0.26591775, dtype=float32), 'train/wer': 0.09114374392973269, 'validation/ctc_loss': Array(0.5673303, dtype=float32), 'validation/wer': 0.1649111289185823, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3237855, dtype=float32), 'test/wer': 0.10454370036357727, 'test/num_examples': 2472, 'score': 20209.84367251396, 'total_duration': 22290.02009320259, 'accumulated_submission_time': 20209.84367251396, 'accumulated_eval_time': 2078.508171081543, 'accumulated_logging_time': 0.5718889236450195}
I0215 08:56:07.551717 140046328665856 logging_writer.py:48] [25094] accumulated_eval_time=2078.508171, accumulated_logging_time=0.571889, accumulated_submission_time=20209.843673, global_step=25094, preemption_count=0, score=20209.843673, test/ctc_loss=0.3237855136394501, test/num_examples=2472, test/wer=0.104544, total_duration=22290.020093, train/ctc_loss=0.26591774821281433, train/wer=0.091144, validation/ctc_loss=0.5673303008079529, validation/num_examples=5348, validation/wer=0.164911
I0215 08:56:13.053676 140046320273152 logging_writer.py:48] [25100] global_step=25100, grad_norm=2.63169002532959, loss=1.4013193845748901
I0215 08:57:28.681834 140046328665856 logging_writer.py:48] [25200] global_step=25200, grad_norm=2.8414621353149414, loss=1.3806718587875366
I0215 08:58:43.551912 140046320273152 logging_writer.py:48] [25300] global_step=25300, grad_norm=4.417436122894287, loss=1.4717063903808594
I0215 08:59:59.407184 140046328665856 logging_writer.py:48] [25400] global_step=25400, grad_norm=3.0087730884552, loss=1.4316322803497314
I0215 09:01:24.211791 140046320273152 logging_writer.py:48] [25500] global_step=25500, grad_norm=2.717172384262085, loss=1.4709396362304688
I0215 09:02:48.917041 140046328665856 logging_writer.py:48] [25600] global_step=25600, grad_norm=4.054471015930176, loss=1.4080253839492798
I0215 09:04:11.873276 140046320273152 logging_writer.py:48] [25700] global_step=25700, grad_norm=2.7638661861419678, loss=1.4200242757797241
I0215 09:05:36.846231 140046000985856 logging_writer.py:48] [25800] global_step=25800, grad_norm=2.9956352710723877, loss=1.4193757772445679
I0215 09:06:53.367693 140045992593152 logging_writer.py:48] [25900] global_step=25900, grad_norm=2.7205774784088135, loss=1.4084724187850952
I0215 09:08:09.275373 140046000985856 logging_writer.py:48] [26000] global_step=26000, grad_norm=2.9562485218048096, loss=1.4213364124298096
I0215 09:09:28.858943 140045992593152 logging_writer.py:48] [26100] global_step=26100, grad_norm=2.652461528778076, loss=1.3845475912094116
I0215 09:10:52.678847 140046000985856 logging_writer.py:48] [26200] global_step=26200, grad_norm=3.3616042137145996, loss=1.415984869003296
I0215 09:12:19.810938 140045992593152 logging_writer.py:48] [26300] global_step=26300, grad_norm=2.3349475860595703, loss=1.4152103662490845
I0215 09:13:45.725332 140046000985856 logging_writer.py:48] [26400] global_step=26400, grad_norm=2.260624885559082, loss=1.3733928203582764
I0215 09:15:09.017726 140045992593152 logging_writer.py:48] [26500] global_step=26500, grad_norm=4.646801471710205, loss=1.402437448501587
I0215 09:16:33.077292 140046000985856 logging_writer.py:48] [26600] global_step=26600, grad_norm=2.3978686332702637, loss=1.402559757232666
I0215 09:17:57.425455 140045992593152 logging_writer.py:48] [26700] global_step=26700, grad_norm=2.3061749935150146, loss=1.4186424016952515
I0215 09:19:24.641397 140047086425856 logging_writer.py:48] [26800] global_step=26800, grad_norm=2.596107244491577, loss=1.4609768390655518
I0215 09:20:08.250660 140202902193984 spec.py:321] Evaluating on the training split.
I0215 09:21:12.572677 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 09:22:05.496786 140202902193984 spec.py:349] Evaluating on the test split.
I0215 09:22:33.441982 140202902193984 submission_runner.py:408] Time since start: 23875.94s, 	Step: 26858, 	{'train/ctc_loss': Array(0.24384421, dtype=float32), 'train/wer': 0.08516654059834633, 'validation/ctc_loss': Array(0.55111766, dtype=float32), 'validation/wer': 0.15865491373567492, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31746915, dtype=float32), 'test/wer': 0.10072512339284626, 'test/num_examples': 2472, 'score': 21650.451526880264, 'total_duration': 23875.94362139702, 'accumulated_submission_time': 21650.451526880264, 'accumulated_eval_time': 2223.6968109607697, 'accumulated_logging_time': 0.6204798221588135}
I0215 09:22:33.471270 140046364505856 logging_writer.py:48] [26858] accumulated_eval_time=2223.696811, accumulated_logging_time=0.620480, accumulated_submission_time=21650.451527, global_step=26858, preemption_count=0, score=21650.451527, test/ctc_loss=0.31746914982795715, test/num_examples=2472, test/wer=0.100725, total_duration=23875.943621, train/ctc_loss=0.24384421110153198, train/wer=0.085167, validation/ctc_loss=0.5511176586151123, validation/num_examples=5348, validation/wer=0.158655
I0215 09:23:05.768348 140046356113152 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.9244462251663208, loss=1.370400071144104
I0215 09:24:20.562165 140046364505856 logging_writer.py:48] [27000] global_step=27000, grad_norm=3.1414272785186768, loss=1.3961422443389893
I0215 09:25:36.450263 140046356113152 logging_writer.py:48] [27100] global_step=27100, grad_norm=2.7594399452209473, loss=1.3973416090011597
I0215 09:26:52.503852 140046364505856 logging_writer.py:48] [27200] global_step=27200, grad_norm=2.71199107170105, loss=1.394727349281311
I0215 09:28:10.617665 140046356113152 logging_writer.py:48] [27300] global_step=27300, grad_norm=2.847076177597046, loss=1.3769534826278687
I0215 09:29:33.946074 140046364505856 logging_writer.py:48] [27400] global_step=27400, grad_norm=2.415944814682007, loss=1.4458216428756714
I0215 09:30:58.324297 140046356113152 logging_writer.py:48] [27500] global_step=27500, grad_norm=2.4470880031585693, loss=1.384106159210205
I0215 09:32:25.153758 140046364505856 logging_writer.py:48] [27600] global_step=27600, grad_norm=3.587956190109253, loss=1.354621171951294
I0215 09:33:49.273715 140046356113152 logging_writer.py:48] [27700] global_step=27700, grad_norm=3.1010663509368896, loss=1.3927333354949951
I0215 09:35:12.302502 140046364505856 logging_writer.py:48] [27800] global_step=27800, grad_norm=3.7387235164642334, loss=1.365813136100769
I0215 09:36:31.537267 140046364505856 logging_writer.py:48] [27900] global_step=27900, grad_norm=2.361013174057007, loss=1.4043071269989014
I0215 09:37:47.657114 140046356113152 logging_writer.py:48] [28000] global_step=28000, grad_norm=3.0416736602783203, loss=1.3866699934005737
I0215 09:39:05.594087 140046364505856 logging_writer.py:48] [28100] global_step=28100, grad_norm=3.38214111328125, loss=1.393673300743103
I0215 09:40:25.422894 140046356113152 logging_writer.py:48] [28200] global_step=28200, grad_norm=5.181467056274414, loss=1.361445426940918
I0215 09:41:48.768871 140046364505856 logging_writer.py:48] [28300] global_step=28300, grad_norm=3.413186550140381, loss=1.3782954216003418
I0215 09:43:11.683822 140046356113152 logging_writer.py:48] [28400] global_step=28400, grad_norm=5.350907802581787, loss=1.3985525369644165
I0215 09:44:35.737776 140046364505856 logging_writer.py:48] [28500] global_step=28500, grad_norm=3.169579029083252, loss=1.3602914810180664
I0215 09:45:59.733756 140046356113152 logging_writer.py:48] [28600] global_step=28600, grad_norm=2.639119863510132, loss=1.3933557271957397
I0215 09:46:33.486663 140202902193984 spec.py:321] Evaluating on the training split.
I0215 09:47:29.982794 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 09:48:21.563720 140202902193984 spec.py:349] Evaluating on the test split.
I0215 09:48:47.921368 140202902193984 submission_runner.py:408] Time since start: 25450.42s, 	Step: 28641, 	{'train/ctc_loss': Array(0.22982687, dtype=float32), 'train/wer': 0.07900604473719201, 'validation/ctc_loss': Array(0.52235276, dtype=float32), 'validation/wer': 0.15247593577724786, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2958047, dtype=float32), 'test/wer': 0.09585034428127476, 'test/num_examples': 2472, 'score': 23090.378248929977, 'total_duration': 25450.42253112793, 'accumulated_submission_time': 23090.378248929977, 'accumulated_eval_time': 2358.1283671855927, 'accumulated_logging_time': 0.6628987789154053}
I0215 09:48:47.955804 140046579545856 logging_writer.py:48] [28641] accumulated_eval_time=2358.128367, accumulated_logging_time=0.662899, accumulated_submission_time=23090.378249, global_step=28641, preemption_count=0, score=23090.378249, test/ctc_loss=0.2958047091960907, test/num_examples=2472, test/wer=0.095850, total_duration=25450.422531, train/ctc_loss=0.22982686758041382, train/wer=0.079006, validation/ctc_loss=0.5223527550697327, validation/num_examples=5348, validation/wer=0.152476
I0215 09:49:33.247333 140046571153152 logging_writer.py:48] [28700] global_step=28700, grad_norm=3.049454689025879, loss=1.402594804763794
I0215 09:50:49.463581 140046579545856 logging_writer.py:48] [28800] global_step=28800, grad_norm=2.860872983932495, loss=1.3689022064208984
I0215 09:52:09.052476 140045924185856 logging_writer.py:48] [28900] global_step=28900, grad_norm=2.3118600845336914, loss=1.3427191972732544
I0215 09:53:25.321235 140045915793152 logging_writer.py:48] [29000] global_step=29000, grad_norm=2.435685873031616, loss=1.3370436429977417
I0215 09:54:42.718117 140045924185856 logging_writer.py:48] [29100] global_step=29100, grad_norm=3.317737340927124, loss=1.3983557224273682
I0215 09:56:01.142246 140045915793152 logging_writer.py:48] [29200] global_step=29200, grad_norm=2.726944923400879, loss=1.303354263305664
I0215 09:57:24.103507 140045924185856 logging_writer.py:48] [29300] global_step=29300, grad_norm=2.5360031127929688, loss=1.362977147102356
I0215 09:58:45.248402 140045915793152 logging_writer.py:48] [29400] global_step=29400, grad_norm=2.733078956604004, loss=1.4026349782943726
I0215 10:00:12.151015 140045924185856 logging_writer.py:48] [29500] global_step=29500, grad_norm=3.1447219848632812, loss=1.3350145816802979
I0215 10:01:35.965574 140045915793152 logging_writer.py:48] [29600] global_step=29600, grad_norm=3.0407679080963135, loss=1.3825139999389648
I0215 10:02:58.861627 140045924185856 logging_writer.py:48] [29700] global_step=29700, grad_norm=3.178262948989868, loss=1.3168226480484009
I0215 10:04:26.458514 140045915793152 logging_writer.py:48] [29800] global_step=29800, grad_norm=2.8488540649414062, loss=1.3095356225967407
I0215 10:05:50.386275 140045924185856 logging_writer.py:48] [29900] global_step=29900, grad_norm=2.850327730178833, loss=1.3717598915100098
I0215 10:07:06.600022 140045915793152 logging_writer.py:48] [30000] global_step=30000, grad_norm=2.473205089569092, loss=1.3376493453979492
I0215 10:08:23.479139 140045924185856 logging_writer.py:48] [30100] global_step=30100, grad_norm=3.380901336669922, loss=1.3526490926742554
I0215 10:09:39.113102 140045915793152 logging_writer.py:48] [30200] global_step=30200, grad_norm=2.3023221492767334, loss=1.393272042274475
I0215 10:11:01.911316 140045924185856 logging_writer.py:48] [30300] global_step=30300, grad_norm=2.1253747940063477, loss=1.296561360359192
I0215 10:12:23.954531 140045915793152 logging_writer.py:48] [30400] global_step=30400, grad_norm=3.3155813217163086, loss=1.3801172971725464
I0215 10:12:48.528823 140202902193984 spec.py:321] Evaluating on the training split.
I0215 10:13:48.951596 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 10:14:41.210058 140202902193984 spec.py:349] Evaluating on the test split.
I0215 10:15:06.998832 140202902193984 submission_runner.py:408] Time since start: 27029.50s, 	Step: 30429, 	{'train/ctc_loss': Array(0.24319935, dtype=float32), 'train/wer': 0.08418920090136939, 'validation/ctc_loss': Array(0.51407164, dtype=float32), 'validation/wer': 0.148855440879732, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2912043, dtype=float32), 'test/wer': 0.09469258424227653, 'test/num_examples': 2472, 'score': 24530.86066389084, 'total_duration': 27029.50042438507, 'accumulated_submission_time': 24530.86066389084, 'accumulated_eval_time': 2496.595645904541, 'accumulated_logging_time': 0.7129337787628174}
I0215 10:15:07.028603 140045709145856 logging_writer.py:48] [30429] accumulated_eval_time=2496.595646, accumulated_logging_time=0.712934, accumulated_submission_time=24530.860664, global_step=30429, preemption_count=0, score=24530.860664, test/ctc_loss=0.2912043035030365, test/num_examples=2472, test/wer=0.094693, total_duration=27029.500424, train/ctc_loss=0.24319934844970703, train/wer=0.084189, validation/ctc_loss=0.5140716433525085, validation/num_examples=5348, validation/wer=0.148855
I0215 10:16:01.097481 140045700753152 logging_writer.py:48] [30500] global_step=30500, grad_norm=3.276026487350464, loss=1.3067688941955566
I0215 10:17:16.053446 140045709145856 logging_writer.py:48] [30600] global_step=30600, grad_norm=3.4472646713256836, loss=1.3850170373916626
I0215 10:18:31.407641 140045700753152 logging_writer.py:48] [30700] global_step=30700, grad_norm=3.517608165740967, loss=1.371675968170166
I0215 10:19:49.809213 140045709145856 logging_writer.py:48] [30800] global_step=30800, grad_norm=2.668402910232544, loss=1.3960237503051758
I0215 10:21:18.509032 140045709145856 logging_writer.py:48] [30900] global_step=30900, grad_norm=2.980154275894165, loss=1.3590210676193237
I0215 10:22:33.515033 140045700753152 logging_writer.py:48] [31000] global_step=31000, grad_norm=3.3157589435577393, loss=1.4009811878204346
I0215 10:23:48.341300 140045709145856 logging_writer.py:48] [31100] global_step=31100, grad_norm=2.3506393432617188, loss=1.2980719804763794
I0215 10:25:06.585540 140045700753152 logging_writer.py:48] [31200] global_step=31200, grad_norm=2.086759567260742, loss=1.3269824981689453
I0215 10:26:27.190527 140045709145856 logging_writer.py:48] [31300] global_step=31300, grad_norm=2.387617588043213, loss=1.3294975757598877
I0215 10:27:52.177313 140045700753152 logging_writer.py:48] [31400] global_step=31400, grad_norm=2.4285590648651123, loss=1.2694472074508667
I0215 10:29:19.372761 140045709145856 logging_writer.py:48] [31500] global_step=31500, grad_norm=3.196136236190796, loss=1.3228821754455566
I0215 10:30:45.589329 140045700753152 logging_writer.py:48] [31600] global_step=31600, grad_norm=3.3390541076660156, loss=1.2659916877746582
I0215 10:32:08.442186 140045709145856 logging_writer.py:48] [31700] global_step=31700, grad_norm=2.2111423015594482, loss=1.3163158893585205
I0215 10:33:35.609079 140045700753152 logging_writer.py:48] [31800] global_step=31800, grad_norm=2.6081924438476562, loss=1.2875455617904663
I0215 10:35:02.107737 140045709145856 logging_writer.py:48] [31900] global_step=31900, grad_norm=2.914569616317749, loss=1.3518993854522705
I0215 10:36:23.133921 140046579545856 logging_writer.py:48] [32000] global_step=32000, grad_norm=2.7317752838134766, loss=1.3401691913604736
I0215 10:37:40.357387 140046571153152 logging_writer.py:48] [32100] global_step=32100, grad_norm=2.284200668334961, loss=1.3272783756256104
I0215 10:38:57.592973 140046579545856 logging_writer.py:48] [32200] global_step=32200, grad_norm=3.8929641246795654, loss=1.304566502571106
I0215 10:39:07.233374 140202902193984 spec.py:321] Evaluating on the training split.
I0215 10:40:01.595456 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 10:40:53.554749 140202902193984 spec.py:349] Evaluating on the test split.
I0215 10:41:20.239246 140202902193984 submission_runner.py:408] Time since start: 28602.74s, 	Step: 32214, 	{'train/ctc_loss': Array(0.221281, dtype=float32), 'train/wer': 0.07492173044889013, 'validation/ctc_loss': Array(0.4996829, dtype=float32), 'validation/wer': 0.14480048659451422, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27680507, dtype=float32), 'test/wer': 0.08916783458249548, 'test/num_examples': 2472, 'score': 25970.978246688843, 'total_duration': 28602.740975379944, 'accumulated_submission_time': 25970.978246688843, 'accumulated_eval_time': 2629.5989384651184, 'accumulated_logging_time': 0.7543802261352539}
I0215 10:41:20.272682 140046794585856 logging_writer.py:48] [32214] accumulated_eval_time=2629.598938, accumulated_logging_time=0.754380, accumulated_submission_time=25970.978247, global_step=32214, preemption_count=0, score=25970.978247, test/ctc_loss=0.2768050730228424, test/num_examples=2472, test/wer=0.089168, total_duration=28602.740975, train/ctc_loss=0.2212810069322586, train/wer=0.074922, validation/ctc_loss=0.4996829032897949, validation/num_examples=5348, validation/wer=0.144800
I0215 10:42:25.792392 140046786193152 logging_writer.py:48] [32300] global_step=32300, grad_norm=2.624892234802246, loss=1.3238133192062378
I0215 10:43:40.708637 140046794585856 logging_writer.py:48] [32400] global_step=32400, grad_norm=2.841822624206543, loss=1.2400715351104736
I0215 10:44:56.548621 140046786193152 logging_writer.py:48] [32500] global_step=32500, grad_norm=3.1545028686523438, loss=1.3285958766937256
I0215 10:46:22.308861 140046794585856 logging_writer.py:48] [32600] global_step=32600, grad_norm=3.627631425857544, loss=1.3455320596694946
I0215 10:47:46.599279 140046786193152 logging_writer.py:48] [32700] global_step=32700, grad_norm=3.051312208175659, loss=1.2225621938705444
I0215 10:49:12.937678 140046794585856 logging_writer.py:48] [32800] global_step=32800, grad_norm=2.970078229904175, loss=1.2906713485717773
I0215 10:50:32.832999 140046786193152 logging_writer.py:48] [32900] global_step=32900, grad_norm=2.423083543777466, loss=1.359459400177002
I0215 10:51:53.618125 140045089625856 logging_writer.py:48] [33000] global_step=33000, grad_norm=2.4129276275634766, loss=1.2168399095535278
I0215 10:53:10.147905 140045081233152 logging_writer.py:48] [33100] global_step=33100, grad_norm=2.6745455265045166, loss=1.3042619228363037
I0215 10:54:26.324052 140045089625856 logging_writer.py:48] [33200] global_step=33200, grad_norm=2.938607931137085, loss=1.2407625913619995
I0215 10:55:42.270034 140045081233152 logging_writer.py:48] [33300] global_step=33300, grad_norm=3.6484439373016357, loss=1.304793357849121
I0215 10:57:03.579548 140045089625856 logging_writer.py:48] [33400] global_step=33400, grad_norm=2.813986301422119, loss=1.2839351892471313
I0215 10:58:28.041909 140045081233152 logging_writer.py:48] [33500] global_step=33500, grad_norm=3.184391736984253, loss=1.3107759952545166
I0215 10:59:50.187807 140045089625856 logging_writer.py:48] [33600] global_step=33600, grad_norm=3.7690322399139404, loss=1.3006923198699951
I0215 11:01:13.244516 140045081233152 logging_writer.py:48] [33700] global_step=33700, grad_norm=2.5138978958129883, loss=1.2339383363723755
I0215 11:02:36.380166 140045089625856 logging_writer.py:48] [33800] global_step=33800, grad_norm=2.8866586685180664, loss=1.2635469436645508
I0215 11:04:00.872895 140045081233152 logging_writer.py:48] [33900] global_step=33900, grad_norm=2.9382145404815674, loss=1.264966368675232
I0215 11:05:21.443456 140202902193984 spec.py:321] Evaluating on the training split.
I0215 11:06:17.894860 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 11:07:09.263678 140202902193984 spec.py:349] Evaluating on the test split.
I0215 11:07:35.675431 140202902193984 submission_runner.py:408] Time since start: 30178.18s, 	Step: 33991, 	{'train/ctc_loss': Array(0.22154376, dtype=float32), 'train/wer': 0.07495286871561471, 'validation/ctc_loss': Array(0.47980833, dtype=float32), 'validation/wer': 0.13818704924838526, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2669557, dtype=float32), 'test/wer': 0.08575548920439542, 'test/num_examples': 2472, 'score': 27412.06226158142, 'total_duration': 30178.17682671547, 'accumulated_submission_time': 27412.06226158142, 'accumulated_eval_time': 2763.8282132148743, 'accumulated_logging_time': 0.8003880977630615}
I0215 11:07:35.707061 140046794585856 logging_writer.py:48] [33991] accumulated_eval_time=2763.828213, accumulated_logging_time=0.800388, accumulated_submission_time=27412.062262, global_step=33991, preemption_count=0, score=27412.062262, test/ctc_loss=0.266955703496933, test/num_examples=2472, test/wer=0.085755, total_duration=30178.176827, train/ctc_loss=0.22154375910758972, train/wer=0.074953, validation/ctc_loss=0.47980833053588867, validation/num_examples=5348, validation/wer=0.138187
I0215 11:07:43.275034 140046786193152 logging_writer.py:48] [34000] global_step=34000, grad_norm=3.1108076572418213, loss=1.2475510835647583
I0215 11:08:58.489067 140046794585856 logging_writer.py:48] [34100] global_step=34100, grad_norm=2.863121509552002, loss=1.3103277683258057
I0215 11:10:14.496635 140046786193152 logging_writer.py:48] [34200] global_step=34200, grad_norm=2.6483004093170166, loss=1.2957119941711426
I0215 11:11:30.442845 140046794585856 logging_writer.py:48] [34300] global_step=34300, grad_norm=2.3576087951660156, loss=1.2910124063491821
I0215 11:12:45.200028 140046786193152 logging_writer.py:48] [34400] global_step=34400, grad_norm=3.3400959968566895, loss=1.2532178163528442
I0215 11:14:01.797173 140046794585856 logging_writer.py:48] [34500] global_step=34500, grad_norm=4.197723388671875, loss=1.3264985084533691
I0215 11:15:25.098977 140046786193152 logging_writer.py:48] [34600] global_step=34600, grad_norm=3.7830758094787598, loss=1.313256859779358
I0215 11:16:48.950153 140046794585856 logging_writer.py:48] [34700] global_step=34700, grad_norm=3.501666784286499, loss=1.268540382385254
I0215 11:18:09.180156 140046786193152 logging_writer.py:48] [34800] global_step=34800, grad_norm=3.4543046951293945, loss=1.2693296670913696
I0215 11:19:35.955053 140046794585856 logging_writer.py:48] [34900] global_step=34900, grad_norm=2.7203102111816406, loss=1.2582024335861206
I0215 11:21:00.586961 140046786193152 logging_writer.py:48] [35000] global_step=35000, grad_norm=2.0241923332214355, loss=1.253028392791748
I0215 11:22:20.926861 140046794585856 logging_writer.py:48] [35100] global_step=35100, grad_norm=3.186904191970825, loss=1.276853322982788
I0215 11:23:36.867651 140046786193152 logging_writer.py:48] [35200] global_step=35200, grad_norm=3.075683832168579, loss=1.1891047954559326
I0215 11:24:53.465715 140046794585856 logging_writer.py:48] [35300] global_step=35300, grad_norm=4.705302715301514, loss=1.2599526643753052
I0215 11:26:11.386492 140046786193152 logging_writer.py:48] [35400] global_step=35400, grad_norm=2.448669672012329, loss=1.2401195764541626
I0215 11:27:34.216624 140046794585856 logging_writer.py:48] [35500] global_step=35500, grad_norm=2.7420156002044678, loss=1.2477689981460571
I0215 11:28:57.769245 140046786193152 logging_writer.py:48] [35600] global_step=35600, grad_norm=3.6202187538146973, loss=1.2491023540496826
I0215 11:30:22.834914 140046794585856 logging_writer.py:48] [35700] global_step=35700, grad_norm=2.998215675354004, loss=1.2220784425735474
I0215 11:31:35.820576 140202902193984 spec.py:321] Evaluating on the training split.
I0215 11:32:30.680637 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 11:33:22.365134 140202902193984 spec.py:349] Evaluating on the test split.
I0215 11:33:49.221251 140202902193984 submission_runner.py:408] Time since start: 31751.72s, 	Step: 35787, 	{'train/ctc_loss': Array(0.21164294, dtype=float32), 'train/wer': 0.07021217643360295, 'validation/ctc_loss': Array(0.46737054, dtype=float32), 'validation/wer': 0.13578304063643473, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25955525, dtype=float32), 'test/wer': 0.08297280279487336, 'test/num_examples': 2472, 'score': 28852.088437318802, 'total_duration': 31751.722824573517, 'accumulated_submission_time': 28852.088437318802, 'accumulated_eval_time': 2897.226144552231, 'accumulated_logging_time': 0.8438427448272705}
I0215 11:33:49.253618 140045775701760 logging_writer.py:48] [35787] accumulated_eval_time=2897.226145, accumulated_logging_time=0.843843, accumulated_submission_time=28852.088437, global_step=35787, preemption_count=0, score=28852.088437, test/ctc_loss=0.25955525040626526, test/num_examples=2472, test/wer=0.082973, total_duration=31751.722825, train/ctc_loss=0.21164293587207794, train/wer=0.070212, validation/ctc_loss=0.46737053990364075, validation/num_examples=5348, validation/wer=0.135783
I0215 11:33:59.871120 140045767309056 logging_writer.py:48] [35800] global_step=35800, grad_norm=2.5064709186553955, loss=1.2265366315841675
I0215 11:35:15.947600 140045775701760 logging_writer.py:48] [35900] global_step=35900, grad_norm=2.4498038291931152, loss=1.2393237352371216
I0215 11:36:30.911662 140045767309056 logging_writer.py:48] [36000] global_step=36000, grad_norm=3.7383556365966797, loss=1.2823988199234009
I0215 11:37:50.258310 140045448021760 logging_writer.py:48] [36100] global_step=36100, grad_norm=2.682147979736328, loss=1.2692463397979736
I0215 11:39:05.627933 140045439629056 logging_writer.py:48] [36200] global_step=36200, grad_norm=2.4429569244384766, loss=1.2626123428344727
I0215 11:40:23.460001 140045448021760 logging_writer.py:48] [36300] global_step=36300, grad_norm=2.917051076889038, loss=1.1909706592559814
I0215 11:41:41.534663 140045439629056 logging_writer.py:48] [36400] global_step=36400, grad_norm=2.9999542236328125, loss=1.2431501150131226
I0215 11:43:08.479743 140045448021760 logging_writer.py:48] [36500] global_step=36500, grad_norm=3.9352834224700928, loss=1.2638646364212036
I0215 11:44:36.229607 140045439629056 logging_writer.py:48] [36600] global_step=36600, grad_norm=2.871753454208374, loss=1.2343876361846924
I0215 11:45:57.916141 140045448021760 logging_writer.py:48] [36700] global_step=36700, grad_norm=2.8353283405303955, loss=1.1764767169952393
I0215 11:47:22.953963 140045439629056 logging_writer.py:48] [36800] global_step=36800, grad_norm=2.8387553691864014, loss=1.1959270238876343
I0215 11:48:50.209224 140045448021760 logging_writer.py:48] [36900] global_step=36900, grad_norm=3.1350462436676025, loss=1.2166306972503662
I0215 11:50:17.121009 140045439629056 logging_writer.py:48] [37000] global_step=37000, grad_norm=3.933016538619995, loss=1.1951676607131958
I0215 11:51:43.572801 140045448021760 logging_writer.py:48] [37100] global_step=37100, grad_norm=3.000756025314331, loss=1.161170244216919
I0215 11:52:59.863108 140045439629056 logging_writer.py:48] [37200] global_step=37200, grad_norm=2.9187674522399902, loss=1.236144781112671
I0215 11:54:18.451924 140045448021760 logging_writer.py:48] [37300] global_step=37300, grad_norm=2.581049680709839, loss=1.2352356910705566
I0215 11:55:36.777225 140045439629056 logging_writer.py:48] [37400] global_step=37400, grad_norm=2.7243199348449707, loss=1.164828896522522
I0215 11:56:55.769813 140045448021760 logging_writer.py:48] [37500] global_step=37500, grad_norm=5.105662822723389, loss=1.2073811292648315
I0215 11:57:49.291039 140202902193984 spec.py:321] Evaluating on the training split.
I0215 11:58:46.013215 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 11:59:38.201863 140202902193984 spec.py:349] Evaluating on the test split.
I0215 12:00:04.639843 140202902193984 submission_runner.py:408] Time since start: 33327.14s, 	Step: 37564, 	{'train/ctc_loss': Array(0.15737125, dtype=float32), 'train/wer': 0.0545032440576858, 'validation/ctc_loss': Array(0.44882238, dtype=float32), 'validation/wer': 0.1306371105554322, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24834183, dtype=float32), 'test/wer': 0.07893079844819531, 'test/num_examples': 2472, 'score': 30292.036976099014, 'total_duration': 33327.14091706276, 'accumulated_submission_time': 30292.036976099014, 'accumulated_eval_time': 3032.571701526642, 'accumulated_logging_time': 0.8902647495269775}
I0215 12:00:04.672090 140046205781760 logging_writer.py:48] [37564] accumulated_eval_time=3032.571702, accumulated_logging_time=0.890265, accumulated_submission_time=30292.036976, global_step=37564, preemption_count=0, score=30292.036976, test/ctc_loss=0.24834182858467102, test/num_examples=2472, test/wer=0.078931, total_duration=33327.140917, train/ctc_loss=0.15737125277519226, train/wer=0.054503, validation/ctc_loss=0.44882237911224365, validation/num_examples=5348, validation/wer=0.130637
I0215 12:00:32.446507 140046197389056 logging_writer.py:48] [37600] global_step=37600, grad_norm=2.5495126247406006, loss=1.2353541851043701
I0215 12:01:47.770901 140046205781760 logging_writer.py:48] [37700] global_step=37700, grad_norm=2.3986403942108154, loss=1.223555326461792
I0215 12:03:02.986666 140046197389056 logging_writer.py:48] [37800] global_step=37800, grad_norm=3.3608906269073486, loss=1.2056875228881836
I0215 12:04:21.729461 140046205781760 logging_writer.py:48] [37900] global_step=37900, grad_norm=4.652299404144287, loss=1.2000727653503418
I0215 12:05:48.573160 140046197389056 logging_writer.py:48] [38000] global_step=38000, grad_norm=3.295585870742798, loss=1.2276304960250854
I0215 12:07:13.775960 140046205781760 logging_writer.py:48] [38100] global_step=38100, grad_norm=2.596038818359375, loss=1.237362027168274
I0215 12:08:36.056632 140045878101760 logging_writer.py:48] [38200] global_step=38200, grad_norm=2.8081371784210205, loss=1.1984496116638184
I0215 12:09:52.373798 140045869709056 logging_writer.py:48] [38300] global_step=38300, grad_norm=3.484128475189209, loss=1.1803056001663208
I0215 12:11:07.324145 140045878101760 logging_writer.py:48] [38400] global_step=38400, grad_norm=2.714661121368408, loss=1.1704156398773193
I0215 12:12:23.848410 140045869709056 logging_writer.py:48] [38500] global_step=38500, grad_norm=2.799266815185547, loss=1.2015241384506226
I0215 12:13:46.468659 140045878101760 logging_writer.py:48] [38600] global_step=38600, grad_norm=3.4049901962280273, loss=1.1631591320037842
I0215 12:15:06.724795 140045869709056 logging_writer.py:48] [38700] global_step=38700, grad_norm=4.057486057281494, loss=1.2057619094848633
I0215 12:16:29.566223 140045878101760 logging_writer.py:48] [38800] global_step=38800, grad_norm=3.2311301231384277, loss=1.212525725364685
I0215 12:17:55.130908 140045869709056 logging_writer.py:48] [38900] global_step=38900, grad_norm=3.4572746753692627, loss=1.1765985488891602
I0215 12:19:22.054966 140045878101760 logging_writer.py:48] [39000] global_step=39000, grad_norm=2.269522190093994, loss=1.208056926727295
I0215 12:20:45.613295 140045869709056 logging_writer.py:48] [39100] global_step=39100, grad_norm=2.664644241333008, loss=1.2123581171035767
I0215 12:22:08.697738 140045878101760 logging_writer.py:48] [39200] global_step=39200, grad_norm=3.33119535446167, loss=1.1939743757247925
I0215 12:23:24.908617 140045869709056 logging_writer.py:48] [39300] global_step=39300, grad_norm=3.953035354614258, loss=1.1640297174453735
I0215 12:24:05.318915 140202902193984 spec.py:321] Evaluating on the training split.
I0215 12:25:00.932203 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 12:25:52.731113 140202902193984 spec.py:349] Evaluating on the test split.
I0215 12:26:19.471775 140202902193984 submission_runner.py:408] Time since start: 34901.97s, 	Step: 39355, 	{'train/ctc_loss': Array(0.17283808, dtype=float32), 'train/wer': 0.05924901269244578, 'validation/ctc_loss': Array(0.43858835, dtype=float32), 'validation/wer': 0.12688145051507574, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24206062, dtype=float32), 'test/wer': 0.07645278573314647, 'test/num_examples': 2472, 'score': 31732.59752678871, 'total_duration': 34901.9730963707, 'accumulated_submission_time': 31732.59752678871, 'accumulated_eval_time': 3166.7215616703033, 'accumulated_logging_time': 0.934283971786499}
I0215 12:26:19.507353 140046502745856 logging_writer.py:48] [39355] accumulated_eval_time=3166.721562, accumulated_logging_time=0.934284, accumulated_submission_time=31732.597527, global_step=39355, preemption_count=0, score=31732.597527, test/ctc_loss=0.2420606166124344, test/num_examples=2472, test/wer=0.076453, total_duration=34901.973096, train/ctc_loss=0.17283807694911957, train/wer=0.059249, validation/ctc_loss=0.43858835101127625, validation/num_examples=5348, validation/wer=0.126881
I0215 12:26:54.915057 140046494353152 logging_writer.py:48] [39400] global_step=39400, grad_norm=2.6224522590637207, loss=1.135155439376831
I0215 12:28:11.108262 140046502745856 logging_writer.py:48] [39500] global_step=39500, grad_norm=2.466895580291748, loss=1.1679950952529907
I0215 12:29:26.432098 140046494353152 logging_writer.py:48] [39600] global_step=39600, grad_norm=3.752960205078125, loss=1.13906991481781
I0215 12:30:42.573158 140046502745856 logging_writer.py:48] [39700] global_step=39700, grad_norm=3.5930142402648926, loss=1.2360676527023315
I0215 12:32:05.263369 140046494353152 logging_writer.py:48] [39800] global_step=39800, grad_norm=4.2798943519592285, loss=1.1189488172531128
I0215 12:33:31.084691 140046502745856 logging_writer.py:48] [39900] global_step=39900, grad_norm=3.0717618465423584, loss=1.208051085472107
I0215 12:34:57.439874 140046494353152 logging_writer.py:48] [40000] global_step=40000, grad_norm=3.5242016315460205, loss=1.1174808740615845
I0215 12:36:18.282297 140046502745856 logging_writer.py:48] [40100] global_step=40100, grad_norm=3.0848982334136963, loss=1.2527531385421753
I0215 12:37:42.422802 140045847385856 logging_writer.py:48] [40200] global_step=40200, grad_norm=3.4757933616638184, loss=1.1631042957305908
I0215 12:38:57.725421 140045838993152 logging_writer.py:48] [40300] global_step=40300, grad_norm=2.994971513748169, loss=1.1642248630523682
I0215 12:40:14.335952 140045847385856 logging_writer.py:48] [40400] global_step=40400, grad_norm=3.715420722961426, loss=1.1526265144348145
I0215 12:41:31.231176 140045838993152 logging_writer.py:48] [40500] global_step=40500, grad_norm=3.850482940673828, loss=1.16183340549469
I0215 12:42:53.622511 140045847385856 logging_writer.py:48] [40600] global_step=40600, grad_norm=2.6582190990448, loss=1.1909730434417725
I0215 12:44:17.595340 140045838993152 logging_writer.py:48] [40700] global_step=40700, grad_norm=3.066924571990967, loss=1.1935219764709473
I0215 12:45:42.065686 140045847385856 logging_writer.py:48] [40800] global_step=40800, grad_norm=2.779545783996582, loss=1.2013418674468994
I0215 12:47:05.692527 140045838993152 logging_writer.py:48] [40900] global_step=40900, grad_norm=2.8743107318878174, loss=1.1678528785705566
I0215 12:48:32.872355 140045847385856 logging_writer.py:48] [41000] global_step=41000, grad_norm=3.157247543334961, loss=1.1198862791061401
I0215 12:49:55.913562 140045838993152 logging_writer.py:48] [41100] global_step=41100, grad_norm=3.3990418910980225, loss=1.1792227029800415
I0215 12:50:19.821261 140202902193984 spec.py:321] Evaluating on the training split.
I0215 12:51:14.833979 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 12:52:06.285979 140202902193984 spec.py:349] Evaluating on the test split.
I0215 12:52:32.622772 140202902193984 submission_runner.py:408] Time since start: 36475.12s, 	Step: 41130, 	{'train/ctc_loss': Array(0.21339306, dtype=float32), 'train/wer': 0.07325222522957427, 'validation/ctc_loss': Array(0.42771238, dtype=float32), 'validation/wer': 0.1242940034949844, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23490724, dtype=float32), 'test/wer': 0.07523409095525359, 'test/num_examples': 2472, 'score': 33172.82326436043, 'total_duration': 36475.123975515366, 'accumulated_submission_time': 33172.82326436043, 'accumulated_eval_time': 3299.519961833954, 'accumulated_logging_time': 0.9837052822113037}
I0215 12:52:32.659423 140045120341760 logging_writer.py:48] [41130] accumulated_eval_time=3299.519962, accumulated_logging_time=0.983705, accumulated_submission_time=33172.823264, global_step=41130, preemption_count=0, score=33172.823264, test/ctc_loss=0.23490723967552185, test/num_examples=2472, test/wer=0.075234, total_duration=36475.123976, train/ctc_loss=0.21339306235313416, train/wer=0.073252, validation/ctc_loss=0.4277123808860779, validation/num_examples=5348, validation/wer=0.124294
I0215 12:53:29.364625 140045120341760 logging_writer.py:48] [41200] global_step=41200, grad_norm=4.207845687866211, loss=1.1647875308990479
I0215 12:54:46.664800 140045111949056 logging_writer.py:48] [41300] global_step=41300, grad_norm=3.5336179733276367, loss=1.173764705657959
I0215 12:56:05.477063 140045120341760 logging_writer.py:48] [41400] global_step=41400, grad_norm=2.6153640747070312, loss=1.1532601118087769
I0215 12:57:23.432677 140045111949056 logging_writer.py:48] [41500] global_step=41500, grad_norm=3.8332951068878174, loss=1.1488544940948486
I0215 12:58:43.617115 140045120341760 logging_writer.py:48] [41600] global_step=41600, grad_norm=3.232511281967163, loss=1.1817255020141602
I0215 13:00:04.253226 140045111949056 logging_writer.py:48] [41700] global_step=41700, grad_norm=3.3316574096679688, loss=1.0909970998764038
I0215 13:01:28.238337 140045120341760 logging_writer.py:48] [41800] global_step=41800, grad_norm=2.791830539703369, loss=1.114457368850708
I0215 13:02:50.970995 140045111949056 logging_writer.py:48] [41900] global_step=41900, grad_norm=3.6548233032226562, loss=1.0987982749938965
I0215 13:04:13.632678 140045120341760 logging_writer.py:48] [42000] global_step=42000, grad_norm=4.7361273765563965, loss=1.1217772960662842
I0215 13:05:40.045683 140045111949056 logging_writer.py:48] [42100] global_step=42100, grad_norm=3.886453151702881, loss=1.1434721946716309
I0215 13:07:07.610412 140045120341760 logging_writer.py:48] [42200] global_step=42200, grad_norm=3.5143625736236572, loss=1.159480094909668
I0215 13:08:28.271377 140044792661760 logging_writer.py:48] [42300] global_step=42300, grad_norm=2.931553602218628, loss=1.1421197652816772
I0215 13:09:45.124093 140044784269056 logging_writer.py:48] [42400] global_step=42400, grad_norm=6.313635349273682, loss=1.1386547088623047
I0215 13:11:03.626375 140044792661760 logging_writer.py:48] [42500] global_step=42500, grad_norm=3.5549590587615967, loss=1.1533750295639038
I0215 13:12:19.755232 140044784269056 logging_writer.py:48] [42600] global_step=42600, grad_norm=4.972744464874268, loss=1.1811933517456055
I0215 13:13:39.993924 140044792661760 logging_writer.py:48] [42700] global_step=42700, grad_norm=3.5525810718536377, loss=1.1638094186782837
I0215 13:15:06.895437 140044784269056 logging_writer.py:48] [42800] global_step=42800, grad_norm=3.3583765029907227, loss=1.1375610828399658
I0215 13:16:27.188716 140044792661760 logging_writer.py:48] [42900] global_step=42900, grad_norm=3.876309871673584, loss=1.0890700817108154
I0215 13:16:33.254832 140202902193984 spec.py:321] Evaluating on the training split.
I0215 13:17:26.931813 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 13:18:17.246561 140202902193984 spec.py:349] Evaluating on the test split.
I0215 13:18:43.675588 140202902193984 submission_runner.py:408] Time since start: 38046.18s, 	Step: 42909, 	{'train/ctc_loss': Array(0.21680024, dtype=float32), 'train/wer': 0.0741213631846872, 'validation/ctc_loss': Array(0.41661587, dtype=float32), 'validation/wer': 0.12060592602604825, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22871256, dtype=float32), 'test/wer': 0.07245140454573153, 'test/num_examples': 2472, 'score': 34613.33181476593, 'total_duration': 38046.176903009415, 'accumulated_submission_time': 34613.33181476593, 'accumulated_eval_time': 3429.9377114772797, 'accumulated_logging_time': 1.0329890251159668}
I0215 13:18:43.706358 140046502745856 logging_writer.py:48] [42909] accumulated_eval_time=3429.937711, accumulated_logging_time=1.032989, accumulated_submission_time=34613.331815, global_step=42909, preemption_count=0, score=34613.331815, test/ctc_loss=0.2287125587463379, test/num_examples=2472, test/wer=0.072451, total_duration=38046.176903, train/ctc_loss=0.2168002426624298, train/wer=0.074121, validation/ctc_loss=0.41661587357521057, validation/num_examples=5348, validation/wer=0.120606
I0215 13:19:53.759583 140046494353152 logging_writer.py:48] [43000] global_step=43000, grad_norm=3.2518203258514404, loss=1.1786317825317383
I0215 13:21:10.148148 140046502745856 logging_writer.py:48] [43100] global_step=43100, grad_norm=2.8756394386291504, loss=1.1552969217300415
I0215 13:22:25.061100 140046494353152 logging_writer.py:48] [43200] global_step=43200, grad_norm=2.9625706672668457, loss=1.2062106132507324
I0215 13:23:50.280604 140046502745856 logging_writer.py:48] [43300] global_step=43300, grad_norm=4.598161697387695, loss=1.1825027465820312
I0215 13:25:06.501291 140046494353152 logging_writer.py:48] [43400] global_step=43400, grad_norm=3.6315133571624756, loss=1.132147192955017
I0215 13:26:21.784044 140046502745856 logging_writer.py:48] [43500] global_step=43500, grad_norm=3.49481463432312, loss=1.1517106294631958
I0215 13:27:38.362163 140046494353152 logging_writer.py:48] [43600] global_step=43600, grad_norm=2.8118793964385986, loss=1.1678245067596436
I0215 13:29:00.966470 140046502745856 logging_writer.py:48] [43700] global_step=43700, grad_norm=3.2155752182006836, loss=1.1235584020614624
I0215 13:30:28.671808 140046494353152 logging_writer.py:48] [43800] global_step=43800, grad_norm=4.00756311416626, loss=1.131052851676941
I0215 13:31:54.292334 140046502745856 logging_writer.py:48] [43900] global_step=43900, grad_norm=3.131168842315674, loss=1.1766287088394165
I0215 13:33:17.902548 140046494353152 logging_writer.py:48] [44000] global_step=44000, grad_norm=3.085106372833252, loss=1.180982232093811
I0215 13:34:42.665167 140046502745856 logging_writer.py:48] [44100] global_step=44100, grad_norm=2.737835168838501, loss=1.1031080484390259
I0215 13:36:07.166530 140046494353152 logging_writer.py:48] [44200] global_step=44200, grad_norm=3.3594143390655518, loss=1.1596264839172363
I0215 13:37:33.103137 140046502745856 logging_writer.py:48] [44300] global_step=44300, grad_norm=2.622121572494507, loss=1.0733518600463867
I0215 13:38:50.096288 140046494353152 logging_writer.py:48] [44400] global_step=44400, grad_norm=2.5625836849212646, loss=1.084259271621704
I0215 13:40:07.776602 140046502745856 logging_writer.py:48] [44500] global_step=44500, grad_norm=3.10308837890625, loss=1.170459270477295
I0215 13:41:24.101640 140046494353152 logging_writer.py:48] [44600] global_step=44600, grad_norm=2.2506959438323975, loss=1.1220530271530151
I0215 13:42:41.216798 140046502745856 logging_writer.py:48] [44700] global_step=44700, grad_norm=2.9337923526763916, loss=1.072927713394165
I0215 13:42:44.129667 140202902193984 spec.py:321] Evaluating on the training split.
I0215 13:43:36.429033 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 13:44:29.352225 140202902193984 spec.py:349] Evaluating on the test split.
I0215 13:44:55.250350 140202902193984 submission_runner.py:408] Time since start: 39617.75s, 	Step: 44705, 	{'train/ctc_loss': Array(0.24842209, dtype=float32), 'train/wer': 0.08614749255634656, 'validation/ctc_loss': Array(0.412366, dtype=float32), 'validation/wer': 0.11900325361808123, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22644746, dtype=float32), 'test/wer': 0.07104990555115472, 'test/num_examples': 2472, 'score': 36053.66913104057, 'total_duration': 39617.75190496445, 'accumulated_submission_time': 36053.66913104057, 'accumulated_eval_time': 3561.055627822876, 'accumulated_logging_time': 1.075854778289795}
I0215 13:44:55.284262 140046072665856 logging_writer.py:48] [44705] accumulated_eval_time=3561.055628, accumulated_logging_time=1.075855, accumulated_submission_time=36053.669131, global_step=44705, preemption_count=0, score=36053.669131, test/ctc_loss=0.2264474630355835, test/num_examples=2472, test/wer=0.071050, total_duration=39617.751905, train/ctc_loss=0.24842208623886108, train/wer=0.086147, validation/ctc_loss=0.41236600279808044, validation/num_examples=5348, validation/wer=0.119003
I0215 13:46:07.260967 140046064273152 logging_writer.py:48] [44800] global_step=44800, grad_norm=4.954470634460449, loss=1.1209018230438232
I0215 13:47:22.673656 140046072665856 logging_writer.py:48] [44900] global_step=44900, grad_norm=3.476759195327759, loss=1.1188199520111084
I0215 13:48:37.665497 140046064273152 logging_writer.py:48] [45000] global_step=45000, grad_norm=3.1932413578033447, loss=1.1396117210388184
I0215 13:50:04.363357 140046072665856 logging_writer.py:48] [45100] global_step=45100, grad_norm=3.2687480449676514, loss=1.100858211517334
I0215 13:51:29.180625 140046064273152 logging_writer.py:48] [45200] global_step=45200, grad_norm=3.1049389839172363, loss=1.0877922773361206
I0215 13:52:53.398803 140046072665856 logging_writer.py:48] [45300] global_step=45300, grad_norm=4.20242166519165, loss=1.14804208278656
I0215 13:54:18.198718 140044761945856 logging_writer.py:48] [45400] global_step=45400, grad_norm=3.3489439487457275, loss=1.1578500270843506
I0215 13:55:35.924141 140044753553152 logging_writer.py:48] [45500] global_step=45500, grad_norm=3.6174874305725098, loss=1.1428581476211548
I0215 13:56:57.719340 140044761945856 logging_writer.py:48] [45600] global_step=45600, grad_norm=2.506296396255493, loss=1.1461451053619385
I0215 13:58:16.563226 140044753553152 logging_writer.py:48] [45700] global_step=45700, grad_norm=2.7569639682769775, loss=1.1103520393371582
I0215 13:59:40.206059 140044761945856 logging_writer.py:48] [45800] global_step=45800, grad_norm=2.672715902328491, loss=1.1410744190216064
I0215 14:01:03.281675 140044753553152 logging_writer.py:48] [45900] global_step=45900, grad_norm=3.5554497241973877, loss=1.0502585172653198
I0215 14:02:25.064603 140044761945856 logging_writer.py:48] [46000] global_step=46000, grad_norm=2.3138139247894287, loss=1.126632809638977
I0215 14:03:52.745122 140044753553152 logging_writer.py:48] [46100] global_step=46100, grad_norm=2.5122835636138916, loss=1.1398755311965942
I0215 14:05:14.608503 140044761945856 logging_writer.py:48] [46200] global_step=46200, grad_norm=4.047719478607178, loss=1.1602284908294678
I0215 14:06:42.080682 140044753553152 logging_writer.py:48] [46300] global_step=46300, grad_norm=4.182689189910889, loss=1.111482858657837
I0215 14:08:06.391138 140046072665856 logging_writer.py:48] [46400] global_step=46400, grad_norm=2.5248425006866455, loss=1.1281592845916748
I0215 14:08:55.776606 140202902193984 spec.py:321] Evaluating on the training split.
I0215 14:09:47.875327 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 14:10:39.259962 140202902193984 spec.py:349] Evaluating on the test split.
I0215 14:11:06.414661 140202902193984 submission_runner.py:408] Time since start: 41188.92s, 	Step: 46467, 	{'train/ctc_loss': Array(0.22267985, dtype=float32), 'train/wer': 0.07397747410935596, 'validation/ctc_loss': Array(0.40985504, dtype=float32), 'validation/wer': 0.11820191741409772, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22343448, dtype=float32), 'test/wer': 0.07082647817520768, 'test/num_examples': 2472, 'score': 37494.07298588753, 'total_duration': 41188.915818452835, 'accumulated_submission_time': 37494.07298588753, 'accumulated_eval_time': 3691.6905381679535, 'accumulated_logging_time': 1.123474359512329}
I0215 14:11:06.448032 140046072665856 logging_writer.py:48] [46467] accumulated_eval_time=3691.690538, accumulated_logging_time=1.123474, accumulated_submission_time=37494.072986, global_step=46467, preemption_count=0, score=37494.072986, test/ctc_loss=0.2234344780445099, test/num_examples=2472, test/wer=0.070826, total_duration=41188.915818, train/ctc_loss=0.22267985343933105, train/wer=0.073977, validation/ctc_loss=0.40985503792762756, validation/num_examples=5348, validation/wer=0.118202
I0215 14:11:32.001162 140046064273152 logging_writer.py:48] [46500] global_step=46500, grad_norm=2.7568252086639404, loss=1.1321637630462646
I0215 14:12:47.649370 140046072665856 logging_writer.py:48] [46600] global_step=46600, grad_norm=3.0671000480651855, loss=1.105006217956543
I0215 14:14:03.633510 140046064273152 logging_writer.py:48] [46700] global_step=46700, grad_norm=3.789480447769165, loss=1.1521689891815186
I0215 14:15:19.616912 140046072665856 logging_writer.py:48] [46800] global_step=46800, grad_norm=2.638307571411133, loss=1.10773503780365
I0215 14:16:43.430969 140046064273152 logging_writer.py:48] [46900] global_step=46900, grad_norm=3.0353386402130127, loss=1.157105565071106
I0215 14:18:10.516779 140046072665856 logging_writer.py:48] [47000] global_step=47000, grad_norm=3.494155168533325, loss=1.167060375213623
I0215 14:19:32.603232 140046064273152 logging_writer.py:48] [47100] global_step=47100, grad_norm=4.912015438079834, loss=1.0881279706954956
I0215 14:20:58.002759 140046072665856 logging_writer.py:48] [47200] global_step=47200, grad_norm=3.564485549926758, loss=1.1217561960220337
I0215 14:22:22.124631 140046064273152 logging_writer.py:48] [47300] global_step=47300, grad_norm=2.6226329803466797, loss=1.17869234085083
I0215 14:23:44.498428 140045417305856 logging_writer.py:48] [47400] global_step=47400, grad_norm=4.850033283233643, loss=1.1613333225250244
I0215 14:25:01.012702 140045408913152 logging_writer.py:48] [47500] global_step=47500, grad_norm=5.006147384643555, loss=1.0754945278167725
I0215 14:26:17.429461 140045417305856 logging_writer.py:48] [47600] global_step=47600, grad_norm=3.1186795234680176, loss=1.0744575262069702
I0215 14:27:36.376283 140045408913152 logging_writer.py:48] [47700] global_step=47700, grad_norm=4.490019798278809, loss=1.157186508178711
I0215 14:28:57.418995 140045417305856 logging_writer.py:48] [47800] global_step=47800, grad_norm=3.600511312484741, loss=1.0394541025161743
I0215 14:30:24.846188 140045408913152 logging_writer.py:48] [47900] global_step=47900, grad_norm=4.387879848480225, loss=1.1346999406814575
I0215 14:31:46.155752 140202902193984 spec.py:321] Evaluating on the training split.
I0215 14:32:49.335363 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 14:33:40.005251 140202902193984 spec.py:349] Evaluating on the test split.
I0215 14:34:06.930811 140202902193984 submission_runner.py:408] Time since start: 42569.43s, 	Step: 48000, 	{'train/ctc_loss': Array(0.2067998, dtype=float32), 'train/wer': 0.0725563114471179, 'validation/ctc_loss': Array(0.41027457, dtype=float32), 'validation/wer': 0.11796055108759666, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22379321, dtype=float32), 'test/wer': 0.07068429711778686, 'test/num_examples': 2472, 'score': 38733.70230102539, 'total_duration': 42569.4317715168, 'accumulated_submission_time': 38733.70230102539, 'accumulated_eval_time': 3832.4622428417206, 'accumulated_logging_time': 1.1707794666290283}
I0215 14:34:06.967458 140045417305856 logging_writer.py:48] [48000] accumulated_eval_time=3832.462243, accumulated_logging_time=1.170779, accumulated_submission_time=38733.702301, global_step=48000, preemption_count=0, score=38733.702301, test/ctc_loss=0.22379320859909058, test/num_examples=2472, test/wer=0.070684, total_duration=42569.431772, train/ctc_loss=0.20679980516433716, train/wer=0.072556, validation/ctc_loss=0.41027456521987915, validation/num_examples=5348, validation/wer=0.117961
I0215 14:34:06.993688 140045408913152 logging_writer.py:48] [48000] global_step=48000, preemption_count=0, score=38733.702301
I0215 14:34:07.160815 140202902193984 checkpoints.py:490] Saving checkpoint at step: 48000
I0215 14:34:07.953155 140202902193984 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_3/librispeech_deepspeech_jax/trial_1/checkpoint_48000
I0215 14:34:07.972645 140202902193984 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_3/librispeech_deepspeech_jax/trial_1/checkpoint_48000.
I0215 14:34:09.046135 140202902193984 submission_runner.py:583] Tuning trial 1/5
I0215 14:34:09.046387 140202902193984 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0215 14:34:09.058734 140202902193984 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.23901, dtype=float32), 'train/wer': 3.532270995148158, 'validation/ctc_loss': Array(30.351154, dtype=float32), 'validation/wer': 3.0460816590555817, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.536453, dtype=float32), 'test/wer': 3.361383624804501, 'test/num_examples': 2472, 'score': 44.56120181083679, 'total_duration': 278.1395950317383, 'accumulated_submission_time': 44.56120181083679, 'accumulated_eval_time': 233.57833003997803, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1829, {'train/ctc_loss': Array(6.2994285, dtype=float32), 'train/wer': 0.9422720438301991, 'validation/ctc_loss': Array(6.226698, dtype=float32), 'validation/wer': 0.8960097318902845, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.149786, dtype=float32), 'test/wer': 0.8985639713200496, 'test/num_examples': 2472, 'score': 1485.454912185669, 'total_duration': 1827.0729236602783, 'accumulated_submission_time': 1485.454912185669, 'accumulated_eval_time': 341.49453115463257, 'accumulated_logging_time': 0.044753074645996094, 'global_step': 1829, 'preemption_count': 0}), (3638, {'train/ctc_loss': Array(3.389681, dtype=float32), 'train/wer': 0.697909064272967, 'validation/ctc_loss': Array(3.6511831, dtype=float32), 'validation/wer': 0.7208646707280574, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.2384326, dtype=float32), 'test/wer': 0.6621168728292, 'test/num_examples': 2472, 'score': 2925.4223692417145, 'total_duration': 3390.408249616623, 'accumulated_submission_time': 2925.4223692417145, 'accumulated_eval_time': 464.7445025444031, 'accumulated_logging_time': 0.08341050148010254, 'global_step': 3638, 'preemption_count': 0}), (5458, {'train/ctc_loss': Array(0.6259818, dtype=float32), 'train/wer': 0.2104833140935423, 'validation/ctc_loss': Array(1.0288459, dtype=float32), 'validation/wer': 0.28744798555663903, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.68839693, dtype=float32), 'test/wer': 0.21467308512583025, 'test/num_examples': 2472, 'score': 4365.819350719452, 'total_duration': 4964.804627656937, 'accumulated_submission_time': 4365.819350719452, 'accumulated_eval_time': 598.6258668899536, 'accumulated_logging_time': 0.12372756004333496, 'global_step': 5458, 'preemption_count': 0}), (7226, {'train/ctc_loss': Array(0.48525193, dtype=float32), 'train/wer': 0.16322822524060362, 'validation/ctc_loss': Array(0.8257652, dtype=float32), 'validation/wer': 0.23722448033829904, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5244182, dtype=float32), 'test/wer': 0.1696220015030569, 'test/num_examples': 2472, 'score': 5806.1792895793915, 'total_duration': 6543.255874633789, 'accumulated_submission_time': 5806.1792895793915, 'accumulated_eval_time': 736.6049258708954, 'accumulated_logging_time': 0.16009974479675293, 'global_step': 7226, 'preemption_count': 0}), (9028, {'train/ctc_loss': Array(0.43553168, dtype=float32), 'train/wer': 0.14760594924873338, 'validation/ctc_loss': Array(0.76645046, dtype=float32), 'validation/wer': 0.2220570203809726, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.47539398, dtype=float32), 'test/wer': 0.15251965145329352, 'test/num_examples': 2472, 'score': 7246.616269826889, 'total_duration': 8119.081926584244, 'accumulated_submission_time': 7246.616269826889, 'accumulated_eval_time': 871.8770303726196, 'accumulated_logging_time': 0.19861793518066406, 'global_step': 9028, 'preemption_count': 0}), (10819, {'train/ctc_loss': Array(0.4950031, dtype=float32), 'train/wer': 0.1622237278008653, 'validation/ctc_loss': Array(0.8763321, dtype=float32), 'validation/wer': 0.24400204678644874, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.50596225, dtype=float32), 'test/wer': 0.16094895700038592, 'test/num_examples': 2472, 'score': 8687.026130914688, 'total_duration': 9694.430442094803, 'accumulated_submission_time': 8687.026130914688, 'accumulated_eval_time': 1006.6984577178955, 'accumulated_logging_time': 0.23746633529663086, 'global_step': 10819, 'preemption_count': 0}), (12624, {'train/ctc_loss': Array(0.37129277, dtype=float32), 'train/wer': 0.1258162635866675, 'validation/ctc_loss': Array(0.6872197, dtype=float32), 'validation/wer': 0.1990210181797117, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4055817, dtype=float32), 'test/wer': 0.12944569699185507, 'test/num_examples': 2472, 'score': 10127.646817445755, 'total_duration': 11267.967316865921, 'accumulated_submission_time': 10127.646817445755, 'accumulated_eval_time': 1139.495243549347, 'accumulated_logging_time': 0.2780308723449707, 'global_step': 12624, 'preemption_count': 0}), (14400, {'train/ctc_loss': Array(0.31816357, dtype=float32), 'train/wer': 0.11004966327777634, 'validation/ctc_loss': Array(0.6623367, dtype=float32), 'validation/wer': 0.19065043397665504, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39234465, dtype=float32), 'test/wer': 0.12615522109154428, 'test/num_examples': 2472, 'score': 11568.152488470078, 'total_duration': 12841.431944847107, 'accumulated_submission_time': 11568.152488470078, 'accumulated_eval_time': 1272.3366241455078, 'accumulated_logging_time': 0.3184385299682617, 'global_step': 14400, 'preemption_count': 0}), (16169, {'train/ctc_loss': Array(0.3033341, dtype=float32), 'train/wer': 0.10425655294224048, 'validation/ctc_loss': Array(0.64782745, dtype=float32), 'validation/wer': 0.18617067495679543, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3739464, dtype=float32), 'test/wer': 0.12258038307639185, 'test/num_examples': 2472, 'score': 13008.389578580856, 'total_duration': 14417.589095115662, 'accumulated_submission_time': 13008.389578580856, 'accumulated_eval_time': 1408.132239818573, 'accumulated_logging_time': 0.36313295364379883, 'global_step': 16169, 'preemption_count': 0}), (17980, {'train/ctc_loss': Array(0.29420593, dtype=float32), 'train/wer': 0.10390042205859482, 'validation/ctc_loss': Array(0.6150627, dtype=float32), 'validation/wer': 0.17816696757002037, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36189067, dtype=float32), 'test/wer': 0.11705563341661081, 'test/num_examples': 2472, 'score': 14448.712196111679, 'total_duration': 15991.481305122375, 'accumulated_submission_time': 14448.712196111679, 'accumulated_eval_time': 1541.5824666023254, 'accumulated_logging_time': 0.4025120735168457, 'global_step': 17980, 'preemption_count': 0}), (19763, {'train/ctc_loss': Array(0.30935612, dtype=float32), 'train/wer': 0.1035617017901055, 'validation/ctc_loss': Array(0.6012965, dtype=float32), 'validation/wer': 0.17383202834606137, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35354367, dtype=float32), 'test/wer': 0.11299331749030123, 'test/num_examples': 2472, 'score': 15888.818793296814, 'total_duration': 17564.92405796051, 'accumulated_submission_time': 15888.818793296814, 'accumulated_eval_time': 1674.7982478141785, 'accumulated_logging_time': 0.44518375396728516, 'global_step': 19763, 'preemption_count': 0}), (21541, {'train/ctc_loss': Array(0.30396807, dtype=float32), 'train/wer': 0.1025509800300736, 'validation/ctc_loss': Array(0.5889155, dtype=float32), 'validation/wer': 0.17066530214236753, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34354234, dtype=float32), 'test/wer': 0.11157150691609287, 'test/num_examples': 2472, 'score': 17329.08687067032, 'total_duration': 19140.497346639633, 'accumulated_submission_time': 17329.08687067032, 'accumulated_eval_time': 1809.9828248023987, 'accumulated_logging_time': 0.48740100860595703, 'global_step': 21541, 'preemption_count': 0}), (23309, {'train/ctc_loss': Array(0.28539538, dtype=float32), 'train/wer': 0.09700586480090544, 'validation/ctc_loss': Array(0.5723087, dtype=float32), 'validation/wer': 0.16646552806124912, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33128116, dtype=float32), 'test/wer': 0.10635143095078504, 'test/num_examples': 2472, 'score': 18769.07917690277, 'total_duration': 20714.64650440216, 'accumulated_submission_time': 18769.07917690277, 'accumulated_eval_time': 1944.0160570144653, 'accumulated_logging_time': 0.5339093208312988, 'global_step': 23309, 'preemption_count': 0}), (25094, {'train/ctc_loss': Array(0.26591775, dtype=float32), 'train/wer': 0.09114374392973269, 'validation/ctc_loss': Array(0.5673303, dtype=float32), 'validation/wer': 0.1649111289185823, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3237855, dtype=float32), 'test/wer': 0.10454370036357727, 'test/num_examples': 2472, 'score': 20209.84367251396, 'total_duration': 22290.02009320259, 'accumulated_submission_time': 20209.84367251396, 'accumulated_eval_time': 2078.508171081543, 'accumulated_logging_time': 0.5718889236450195, 'global_step': 25094, 'preemption_count': 0}), (26858, {'train/ctc_loss': Array(0.24384421, dtype=float32), 'train/wer': 0.08516654059834633, 'validation/ctc_loss': Array(0.55111766, dtype=float32), 'validation/wer': 0.15865491373567492, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31746915, dtype=float32), 'test/wer': 0.10072512339284626, 'test/num_examples': 2472, 'score': 21650.451526880264, 'total_duration': 23875.94362139702, 'accumulated_submission_time': 21650.451526880264, 'accumulated_eval_time': 2223.6968109607697, 'accumulated_logging_time': 0.6204798221588135, 'global_step': 26858, 'preemption_count': 0}), (28641, {'train/ctc_loss': Array(0.22982687, dtype=float32), 'train/wer': 0.07900604473719201, 'validation/ctc_loss': Array(0.52235276, dtype=float32), 'validation/wer': 0.15247593577724786, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2958047, dtype=float32), 'test/wer': 0.09585034428127476, 'test/num_examples': 2472, 'score': 23090.378248929977, 'total_duration': 25450.42253112793, 'accumulated_submission_time': 23090.378248929977, 'accumulated_eval_time': 2358.1283671855927, 'accumulated_logging_time': 0.6628987789154053, 'global_step': 28641, 'preemption_count': 0}), (30429, {'train/ctc_loss': Array(0.24319935, dtype=float32), 'train/wer': 0.08418920090136939, 'validation/ctc_loss': Array(0.51407164, dtype=float32), 'validation/wer': 0.148855440879732, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2912043, dtype=float32), 'test/wer': 0.09469258424227653, 'test/num_examples': 2472, 'score': 24530.86066389084, 'total_duration': 27029.50042438507, 'accumulated_submission_time': 24530.86066389084, 'accumulated_eval_time': 2496.595645904541, 'accumulated_logging_time': 0.7129337787628174, 'global_step': 30429, 'preemption_count': 0}), (32214, {'train/ctc_loss': Array(0.221281, dtype=float32), 'train/wer': 0.07492173044889013, 'validation/ctc_loss': Array(0.4996829, dtype=float32), 'validation/wer': 0.14480048659451422, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27680507, dtype=float32), 'test/wer': 0.08916783458249548, 'test/num_examples': 2472, 'score': 25970.978246688843, 'total_duration': 28602.740975379944, 'accumulated_submission_time': 25970.978246688843, 'accumulated_eval_time': 2629.5989384651184, 'accumulated_logging_time': 0.7543802261352539, 'global_step': 32214, 'preemption_count': 0}), (33991, {'train/ctc_loss': Array(0.22154376, dtype=float32), 'train/wer': 0.07495286871561471, 'validation/ctc_loss': Array(0.47980833, dtype=float32), 'validation/wer': 0.13818704924838526, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2669557, dtype=float32), 'test/wer': 0.08575548920439542, 'test/num_examples': 2472, 'score': 27412.06226158142, 'total_duration': 30178.17682671547, 'accumulated_submission_time': 27412.06226158142, 'accumulated_eval_time': 2763.8282132148743, 'accumulated_logging_time': 0.8003880977630615, 'global_step': 33991, 'preemption_count': 0}), (35787, {'train/ctc_loss': Array(0.21164294, dtype=float32), 'train/wer': 0.07021217643360295, 'validation/ctc_loss': Array(0.46737054, dtype=float32), 'validation/wer': 0.13578304063643473, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25955525, dtype=float32), 'test/wer': 0.08297280279487336, 'test/num_examples': 2472, 'score': 28852.088437318802, 'total_duration': 31751.722824573517, 'accumulated_submission_time': 28852.088437318802, 'accumulated_eval_time': 2897.226144552231, 'accumulated_logging_time': 0.8438427448272705, 'global_step': 35787, 'preemption_count': 0}), (37564, {'train/ctc_loss': Array(0.15737125, dtype=float32), 'train/wer': 0.0545032440576858, 'validation/ctc_loss': Array(0.44882238, dtype=float32), 'validation/wer': 0.1306371105554322, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24834183, dtype=float32), 'test/wer': 0.07893079844819531, 'test/num_examples': 2472, 'score': 30292.036976099014, 'total_duration': 33327.14091706276, 'accumulated_submission_time': 30292.036976099014, 'accumulated_eval_time': 3032.571701526642, 'accumulated_logging_time': 0.8902647495269775, 'global_step': 37564, 'preemption_count': 0}), (39355, {'train/ctc_loss': Array(0.17283808, dtype=float32), 'train/wer': 0.05924901269244578, 'validation/ctc_loss': Array(0.43858835, dtype=float32), 'validation/wer': 0.12688145051507574, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24206062, dtype=float32), 'test/wer': 0.07645278573314647, 'test/num_examples': 2472, 'score': 31732.59752678871, 'total_duration': 34901.9730963707, 'accumulated_submission_time': 31732.59752678871, 'accumulated_eval_time': 3166.7215616703033, 'accumulated_logging_time': 0.934283971786499, 'global_step': 39355, 'preemption_count': 0}), (41130, {'train/ctc_loss': Array(0.21339306, dtype=float32), 'train/wer': 0.07325222522957427, 'validation/ctc_loss': Array(0.42771238, dtype=float32), 'validation/wer': 0.1242940034949844, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23490724, dtype=float32), 'test/wer': 0.07523409095525359, 'test/num_examples': 2472, 'score': 33172.82326436043, 'total_duration': 36475.123975515366, 'accumulated_submission_time': 33172.82326436043, 'accumulated_eval_time': 3299.519961833954, 'accumulated_logging_time': 0.9837052822113037, 'global_step': 41130, 'preemption_count': 0}), (42909, {'train/ctc_loss': Array(0.21680024, dtype=float32), 'train/wer': 0.0741213631846872, 'validation/ctc_loss': Array(0.41661587, dtype=float32), 'validation/wer': 0.12060592602604825, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22871256, dtype=float32), 'test/wer': 0.07245140454573153, 'test/num_examples': 2472, 'score': 34613.33181476593, 'total_duration': 38046.176903009415, 'accumulated_submission_time': 34613.33181476593, 'accumulated_eval_time': 3429.9377114772797, 'accumulated_logging_time': 1.0329890251159668, 'global_step': 42909, 'preemption_count': 0}), (44705, {'train/ctc_loss': Array(0.24842209, dtype=float32), 'train/wer': 0.08614749255634656, 'validation/ctc_loss': Array(0.412366, dtype=float32), 'validation/wer': 0.11900325361808123, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22644746, dtype=float32), 'test/wer': 0.07104990555115472, 'test/num_examples': 2472, 'score': 36053.66913104057, 'total_duration': 39617.75190496445, 'accumulated_submission_time': 36053.66913104057, 'accumulated_eval_time': 3561.055627822876, 'accumulated_logging_time': 1.075854778289795, 'global_step': 44705, 'preemption_count': 0}), (46467, {'train/ctc_loss': Array(0.22267985, dtype=float32), 'train/wer': 0.07397747410935596, 'validation/ctc_loss': Array(0.40985504, dtype=float32), 'validation/wer': 0.11820191741409772, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22343448, dtype=float32), 'test/wer': 0.07082647817520768, 'test/num_examples': 2472, 'score': 37494.07298588753, 'total_duration': 41188.915818452835, 'accumulated_submission_time': 37494.07298588753, 'accumulated_eval_time': 3691.6905381679535, 'accumulated_logging_time': 1.123474359512329, 'global_step': 46467, 'preemption_count': 0}), (48000, {'train/ctc_loss': Array(0.2067998, dtype=float32), 'train/wer': 0.0725563114471179, 'validation/ctc_loss': Array(0.41027457, dtype=float32), 'validation/wer': 0.11796055108759666, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22379321, dtype=float32), 'test/wer': 0.07068429711778686, 'test/num_examples': 2472, 'score': 38733.70230102539, 'total_duration': 42569.4317715168, 'accumulated_submission_time': 38733.70230102539, 'accumulated_eval_time': 3832.4622428417206, 'accumulated_logging_time': 1.1707794666290283, 'global_step': 48000, 'preemption_count': 0})], 'global_step': 48000}
I0215 14:34:09.058954 140202902193984 submission_runner.py:586] Timing: 38733.70230102539
I0215 14:34:09.059020 140202902193984 submission_runner.py:588] Total number of evals: 28
I0215 14:34:09.059088 140202902193984 submission_runner.py:589] ====================
I0215 14:34:09.059173 140202902193984 submission_runner.py:542] Using RNG seed 4294683350
I0215 14:34:09.062521 140202902193984 submission_runner.py:551] --- Tuning run 2/5 ---
I0215 14:34:09.062664 140202902193984 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_3/librispeech_deepspeech_jax/trial_2.
I0215 14:34:09.063068 140202902193984 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_3/librispeech_deepspeech_jax/trial_2/hparams.json.
I0215 14:34:09.064632 140202902193984 submission_runner.py:206] Initializing dataset.
I0215 14:34:09.064775 140202902193984 submission_runner.py:213] Initializing model.
I0215 14:34:10.202255 140202902193984 submission_runner.py:255] Initializing optimizer.
I0215 14:34:10.357353 140202902193984 submission_runner.py:262] Initializing metrics bundle.
I0215 14:34:10.357526 140202902193984 submission_runner.py:280] Initializing checkpoint and logger.
I0215 14:34:10.362375 140202902193984 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_3/librispeech_deepspeech_jax/trial_2 with prefix checkpoint_
I0215 14:34:10.362522 140202902193984 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_3/librispeech_deepspeech_jax/trial_2/meta_data_0.json.
I0215 14:34:10.362993 140202902193984 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0215 14:34:10.363127 140202902193984 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0215 14:34:10.839727 140202902193984 logger_utils.py:220] Unable to record git information. Continuing without it.
I0215 14:34:11.292987 140202902193984 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_3/librispeech_deepspeech_jax/trial_2/flags_0.json.
I0215 14:34:11.327486 140202902193984 submission_runner.py:314] Starting training loop.
I0215 14:34:11.331058 140202902193984 input_pipeline.py:20] Loading split = train-clean-100
I0215 14:34:11.375388 140202902193984 input_pipeline.py:20] Loading split = train-clean-360
I0215 14:34:11.940541 140202902193984 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0215 14:34:26.755795 140046030702336 logging_writer.py:48] [0] global_step=0, grad_norm=21.7947998046875, loss=32.231231689453125
I0215 14:34:26.773506 140202902193984 spec.py:321] Evaluating on the training split.
I0215 14:35:59.127168 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 14:36:59.316649 140202902193984 spec.py:349] Evaluating on the test split.
I0215 14:37:37.075860 140202902193984 submission_runner.py:408] Time since start: 205.75s, 	Step: 1, 	{'train/ctc_loss': Array(31.72197, dtype=float32), 'train/wer': 3.2662889210323116, 'validation/ctc_loss': Array(30.351294, dtype=float32), 'validation/wer': 3.0462071695453625, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.536596, dtype=float32), 'test/wer': 3.3615054942822904, 'test/num_examples': 2472, 'score': 15.445929288864136, 'total_duration': 205.7453055381775, 'accumulated_submission_time': 15.445929288864136, 'accumulated_eval_time': 190.29930567741394, 'accumulated_logging_time': 0}
I0215 14:37:37.094368 140046536316672 logging_writer.py:48] [1] accumulated_eval_time=190.299306, accumulated_logging_time=0, accumulated_submission_time=15.445929, global_step=1, preemption_count=0, score=15.445929, test/ctc_loss=30.536596298217773, test/num_examples=2472, test/wer=3.361505, total_duration=205.745306, train/ctc_loss=31.721969604492188, train/wer=3.266289, validation/ctc_loss=30.351293563842773, validation/num_examples=5348, validation/wer=3.046207
I0215 14:39:03.401125 140046410426112 logging_writer.py:48] [100] global_step=100, grad_norm=3.0941081047058105, loss=7.534158229827881
I0215 14:40:20.430311 140046418818816 logging_writer.py:48] [200] global_step=200, grad_norm=0.887584924697876, loss=5.965072154998779
I0215 14:41:36.767635 140046410426112 logging_writer.py:48] [300] global_step=300, grad_norm=0.7276108860969543, loss=5.8689446449279785
I0215 14:42:54.293233 140046418818816 logging_writer.py:48] [400] global_step=400, grad_norm=1.169891119003296, loss=5.829629421234131
I0215 14:44:12.087828 140046410426112 logging_writer.py:48] [500] global_step=500, grad_norm=1.0879340171813965, loss=5.785863876342773
I0215 14:45:33.864089 140046418818816 logging_writer.py:48] [600] global_step=600, grad_norm=0.5251650810241699, loss=5.642248153686523
I0215 14:47:00.900357 140046410426112 logging_writer.py:48] [700] global_step=700, grad_norm=0.9153068661689758, loss=5.482491970062256
I0215 14:48:20.970343 140046418818816 logging_writer.py:48] [800] global_step=800, grad_norm=1.7861486673355103, loss=5.3052167892456055
I0215 14:49:43.824292 140046410426112 logging_writer.py:48] [900] global_step=900, grad_norm=1.4999476671218872, loss=4.809887886047363
I0215 14:51:06.728947 140046418818816 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.2624144554138184, loss=4.283008098602295
I0215 14:52:31.716236 140046536316672 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.611042857170105, loss=3.9371495246887207
I0215 14:53:48.065776 140046527923968 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.8768064975738525, loss=3.7358133792877197
I0215 14:55:04.347925 140046536316672 logging_writer.py:48] [1300] global_step=1300, grad_norm=2.427682638168335, loss=3.4767918586730957
I0215 14:56:21.209244 140046527923968 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.8327999114990234, loss=3.3205106258392334
I0215 14:57:42.193810 140046536316672 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.4818477630615234, loss=3.239837646484375
I0215 14:59:06.139851 140046527923968 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.130128860473633, loss=3.0221855640411377
I0215 15:00:29.616777 140046536316672 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.7751524448394775, loss=2.9446802139282227
I0215 15:01:37.661220 140202902193984 spec.py:321] Evaluating on the training split.
I0215 15:02:21.295709 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 15:03:09.217071 140202902193984 spec.py:349] Evaluating on the test split.
I0215 15:03:33.923369 140202902193984 submission_runner.py:408] Time since start: 1762.59s, 	Step: 1783, 	{'train/ctc_loss': Array(6.9688888, dtype=float32), 'train/wer': 0.9123050937668022, 'validation/ctc_loss': Array(6.7451887, dtype=float32), 'validation/wer': 0.8757156511580756, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.6782165, dtype=float32), 'test/wer': 0.8730932504620884, 'test/num_examples': 2472, 'score': 1455.9250264167786, 'total_duration': 1762.5931413173676, 'accumulated_submission_time': 1455.9250264167786, 'accumulated_eval_time': 306.5587999820709, 'accumulated_logging_time': 0.03134512901306152}
I0215 15:03:33.949637 140046536316672 logging_writer.py:48] [1783] accumulated_eval_time=306.558800, accumulated_logging_time=0.031345, accumulated_submission_time=1455.925026, global_step=1783, preemption_count=0, score=1455.925026, test/ctc_loss=6.678216457366943, test/num_examples=2472, test/wer=0.873093, total_duration=1762.593141, train/ctc_loss=6.968888759613037, train/wer=0.912305, validation/ctc_loss=6.7451887130737305, validation/num_examples=5348, validation/wer=0.875716
I0215 15:03:47.593485 140046527923968 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.9640166759490967, loss=2.908804416656494
I0215 15:05:03.184497 140046536316672 logging_writer.py:48] [1900] global_step=1900, grad_norm=3.3236911296844482, loss=2.8182272911071777
I0215 15:06:18.707466 140046527923968 logging_writer.py:48] [2000] global_step=2000, grad_norm=4.0811333656311035, loss=2.7351737022399902
I0215 15:07:38.979881 140046208636672 logging_writer.py:48] [2100] global_step=2100, grad_norm=3.075723171234131, loss=2.6139798164367676
I0215 15:08:56.116188 140046200243968 logging_writer.py:48] [2200] global_step=2200, grad_norm=2.6084976196289062, loss=2.6042144298553467
I0215 15:10:13.295733 140046208636672 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.7226197719573975, loss=2.5262765884399414
I0215 15:11:29.880286 140046200243968 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.5231211185455322, loss=2.4795424938201904
I0215 15:12:54.798354 140046208636672 logging_writer.py:48] [2500] global_step=2500, grad_norm=2.019392251968384, loss=2.4429221153259277
I0215 15:14:19.034739 140046200243968 logging_writer.py:48] [2600] global_step=2600, grad_norm=3.331251859664917, loss=2.359907865524292
I0215 15:15:42.374073 140046208636672 logging_writer.py:48] [2700] global_step=2700, grad_norm=2.3408315181732178, loss=2.3096022605895996
I0215 15:17:06.577964 140046200243968 logging_writer.py:48] [2800] global_step=2800, grad_norm=4.158865451812744, loss=2.3625648021698
I0215 15:18:30.174279 140046208636672 logging_writer.py:48] [2900] global_step=2900, grad_norm=3.4948954582214355, loss=2.2148475646972656
I0215 15:19:50.443993 140046200243968 logging_writer.py:48] [3000] global_step=3000, grad_norm=4.04193115234375, loss=2.2058145999908447
I0215 15:21:12.848730 140046208636672 logging_writer.py:48] [3100] global_step=3100, grad_norm=4.549140930175781, loss=2.193970203399658
I0215 15:22:29.030616 140046200243968 logging_writer.py:48] [3200] global_step=3200, grad_norm=3.0594980716705322, loss=2.194391965866089
I0215 15:23:45.175326 140046208636672 logging_writer.py:48] [3300] global_step=3300, grad_norm=2.2930893898010254, loss=2.1979470252990723
I0215 15:25:00.148515 140046200243968 logging_writer.py:48] [3400] global_step=3400, grad_norm=2.16374135017395, loss=2.1226837635040283
I0215 15:26:19.111902 140046208636672 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.478219985961914, loss=2.096109628677368
I0215 15:27:33.986492 140202902193984 spec.py:321] Evaluating on the training split.
I0215 15:28:23.833323 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 15:29:13.401320 140202902193984 spec.py:349] Evaluating on the test split.
I0215 15:29:38.990603 140202902193984 submission_runner.py:408] Time since start: 3327.66s, 	Step: 3592, 	{'train/ctc_loss': Array(3.1315818, dtype=float32), 'train/wer': 0.6487877941934376, 'validation/ctc_loss': Array(2.9454577, dtype=float32), 'validation/wer': 0.6002201260897689, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.390297, dtype=float32), 'test/wer': 0.5203623585806268, 'test/num_examples': 2472, 'score': 2895.8729717731476, 'total_duration': 3327.6603693962097, 'accumulated_submission_time': 2895.8729717731476, 'accumulated_eval_time': 431.56023931503296, 'accumulated_logging_time': 0.06965899467468262}
I0215 15:29:39.016292 140046208636672 logging_writer.py:48] [3592] accumulated_eval_time=431.560239, accumulated_logging_time=0.069659, accumulated_submission_time=2895.872972, global_step=3592, preemption_count=0, score=2895.872972, test/ctc_loss=2.3902969360351562, test/num_examples=2472, test/wer=0.520362, total_duration=3327.660369, train/ctc_loss=3.1315817832946777, train/wer=0.648788, validation/ctc_loss=2.945457696914673, validation/num_examples=5348, validation/wer=0.600220
I0215 15:29:45.998320 140046200243968 logging_writer.py:48] [3600] global_step=3600, grad_norm=2.5156595706939697, loss=2.0526368618011475
I0215 15:31:01.028884 140046208636672 logging_writer.py:48] [3700] global_step=3700, grad_norm=4.285062313079834, loss=2.000523567199707
I0215 15:32:15.889346 140046200243968 logging_writer.py:48] [3800] global_step=3800, grad_norm=3.518230438232422, loss=2.0835580825805664
I0215 15:33:35.179330 140046208636672 logging_writer.py:48] [3900] global_step=3900, grad_norm=3.180628776550293, loss=2.1023895740509033
I0215 15:35:01.779028 140046200243968 logging_writer.py:48] [4000] global_step=4000, grad_norm=3.3839595317840576, loss=2.035076379776001
I0215 15:36:25.049017 140046208636672 logging_writer.py:48] [4100] global_step=4100, grad_norm=4.509757041931152, loss=2.0145771503448486
I0215 15:37:45.339751 140046208636672 logging_writer.py:48] [4200] global_step=4200, grad_norm=2.677300453186035, loss=1.983375906944275
I0215 15:39:01.246548 140046200243968 logging_writer.py:48] [4300] global_step=4300, grad_norm=2.810842990875244, loss=1.9743543863296509
I0215 15:40:17.027928 140046208636672 logging_writer.py:48] [4400] global_step=4400, grad_norm=2.9209320545196533, loss=1.9625675678253174
I0215 15:41:37.210128 140046200243968 logging_writer.py:48] [4500] global_step=4500, grad_norm=4.042014122009277, loss=1.9159226417541504
I0215 15:42:57.815072 140046208636672 logging_writer.py:48] [4600] global_step=4600, grad_norm=1.9292714595794678, loss=1.86127769947052
I0215 15:44:21.700958 140046200243968 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.0470190048217773, loss=1.9038580656051636
I0215 15:45:45.064266 140046208636672 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.807785749435425, loss=1.9218153953552246
I0215 15:47:10.045071 140046200243968 logging_writer.py:48] [4900] global_step=4900, grad_norm=2.0801310539245605, loss=1.9296380281448364
I0215 15:48:32.598223 140046208636672 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.9571733474731445, loss=1.8727341890335083
I0215 15:49:57.518304 140046200243968 logging_writer.py:48] [5100] global_step=5100, grad_norm=1.7771939039230347, loss=1.870231032371521
I0215 15:51:21.490605 140046208636672 logging_writer.py:48] [5200] global_step=5200, grad_norm=2.817535877227783, loss=1.8144944906234741
I0215 15:52:37.460610 140046200243968 logging_writer.py:48] [5300] global_step=5300, grad_norm=2.5194830894470215, loss=1.8362840414047241
I0215 15:53:39.398788 140202902193984 spec.py:321] Evaluating on the training split.
I0215 15:54:33.940239 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 15:55:24.783050 140202902193984 spec.py:349] Evaluating on the test split.
I0215 15:55:50.418625 140202902193984 submission_runner.py:408] Time since start: 4899.09s, 	Step: 5384, 	{'train/ctc_loss': Array(0.7923331, dtype=float32), 'train/wer': 0.24815847427288212, 'validation/ctc_loss': Array(0.9136013, dtype=float32), 'validation/wer': 0.2602025546211997, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.58996063, dtype=float32), 'test/wer': 0.1858103304694006, 'test/num_examples': 2472, 'score': 4336.168129205704, 'total_duration': 4899.088443279266, 'accumulated_submission_time': 4336.168129205704, 'accumulated_eval_time': 562.577470779419, 'accumulated_logging_time': 0.10723614692687988}
I0215 15:55:50.443660 140046689916672 logging_writer.py:48] [5384] accumulated_eval_time=562.577471, accumulated_logging_time=0.107236, accumulated_submission_time=4336.168129, global_step=5384, preemption_count=0, score=4336.168129, test/ctc_loss=0.5899606347084045, test/num_examples=2472, test/wer=0.185810, total_duration=4899.088443, train/ctc_loss=0.7923331260681152, train/wer=0.248158, validation/ctc_loss=0.913601279258728, validation/num_examples=5348, validation/wer=0.260203
I0215 15:56:03.307926 140046681523968 logging_writer.py:48] [5400] global_step=5400, grad_norm=6.849619388580322, loss=1.9141238927841187
I0215 15:57:18.174963 140046689916672 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.134887933731079, loss=1.742010474205017
I0215 15:58:33.612879 140046681523968 logging_writer.py:48] [5600] global_step=5600, grad_norm=3.7274694442749023, loss=1.7992429733276367
I0215 15:59:48.908704 140046689916672 logging_writer.py:48] [5700] global_step=5700, grad_norm=3.032261848449707, loss=1.8908416032791138
I0215 16:01:11.656072 140046681523968 logging_writer.py:48] [5800] global_step=5800, grad_norm=4.188907146453857, loss=1.7139533758163452
I0215 16:02:38.535094 140046689916672 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.323125123977661, loss=1.8050223588943481
I0215 16:04:02.855471 140046681523968 logging_writer.py:48] [6000] global_step=6000, grad_norm=3.966622829437256, loss=1.807111144065857
I0215 16:05:28.796635 140046689916672 logging_writer.py:48] [6100] global_step=6100, grad_norm=2.251051902770996, loss=1.786327600479126
I0215 16:06:52.568214 140046034556672 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.5580286979675293, loss=1.8089650869369507
I0215 16:08:09.023232 140046026163968 logging_writer.py:48] [6300] global_step=6300, grad_norm=3.1909403800964355, loss=1.7702229022979736
I0215 16:09:24.957206 140046034556672 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.240933895111084, loss=1.7389273643493652
I0215 16:10:42.064859 140046026163968 logging_writer.py:48] [6500] global_step=6500, grad_norm=4.990896701812744, loss=1.7581411600112915
I0215 16:12:02.501326 140046034556672 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.7199723720550537, loss=1.6739553213119507
I0215 16:13:25.910421 140046026163968 logging_writer.py:48] [6700] global_step=6700, grad_norm=3.6145880222320557, loss=1.7071704864501953
I0215 16:14:49.380203 140046034556672 logging_writer.py:48] [6800] global_step=6800, grad_norm=2.72285532951355, loss=1.71939218044281
I0215 16:16:14.086660 140046026163968 logging_writer.py:48] [6900] global_step=6900, grad_norm=2.530565023422241, loss=1.7207000255584717
I0215 16:17:40.683928 140046034556672 logging_writer.py:48] [7000] global_step=7000, grad_norm=3.554849147796631, loss=1.7127199172973633
I0215 16:19:03.569544 140046026163968 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.023707866668701, loss=1.718396544456482
I0215 16:19:50.643845 140202902193984 spec.py:321] Evaluating on the training split.
I0215 16:20:44.668022 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 16:21:36.106587 140202902193984 spec.py:349] Evaluating on the test split.
I0215 16:22:02.989640 140202902193984 submission_runner.py:408] Time since start: 6471.66s, 	Step: 7156, 	{'train/ctc_loss': Array(0.67502964, dtype=float32), 'train/wer': 0.21462525879917185, 'validation/ctc_loss': Array(0.7972467, dtype=float32), 'validation/wer': 0.23102619307375188, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4931348, dtype=float32), 'test/wer': 0.16184266650417403, 'test/num_examples': 2472, 'score': 5776.282276391983, 'total_duration': 6471.658866643906, 'accumulated_submission_time': 5776.282276391983, 'accumulated_eval_time': 694.9200584888458, 'accumulated_logging_time': 0.1430966854095459}
I0215 16:22:03.020489 140046689916672 logging_writer.py:48] [7156] accumulated_eval_time=694.920058, accumulated_logging_time=0.143097, accumulated_submission_time=5776.282276, global_step=7156, preemption_count=0, score=5776.282276, test/ctc_loss=0.4931347966194153, test/num_examples=2472, test/wer=0.161843, total_duration=6471.658867, train/ctc_loss=0.6750296354293823, train/wer=0.214625, validation/ctc_loss=0.7972466945648193, validation/num_examples=5348, validation/wer=0.231026
I0215 16:22:37.271137 140046681523968 logging_writer.py:48] [7200] global_step=7200, grad_norm=4.457023620605469, loss=1.7010245323181152
I0215 16:23:56.906594 140046689916672 logging_writer.py:48] [7300] global_step=7300, grad_norm=2.204267740249634, loss=1.7180426120758057
I0215 16:25:13.163834 140046681523968 logging_writer.py:48] [7400] global_step=7400, grad_norm=2.4209330081939697, loss=1.7306984663009644
I0215 16:26:31.031805 140046689916672 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.8495664596557617, loss=1.7717787027359009
I0215 16:27:54.284768 140046681523968 logging_writer.py:48] [7600] global_step=7600, grad_norm=2.966130018234253, loss=1.7217820882797241
I0215 16:29:17.624901 140046689916672 logging_writer.py:48] [7700] global_step=7700, grad_norm=3.541208505630493, loss=1.6739284992218018
I0215 16:30:42.837620 140046681523968 logging_writer.py:48] [7800] global_step=7800, grad_norm=2.2985451221466064, loss=1.659770131111145
I0215 16:32:04.284707 140046689916672 logging_writer.py:48] [7900] global_step=7900, grad_norm=2.929367780685425, loss=1.6939325332641602
I0215 16:33:28.011389 140046681523968 logging_writer.py:48] [8000] global_step=8000, grad_norm=4.128589153289795, loss=1.6225337982177734
I0215 16:34:52.477356 140046689916672 logging_writer.py:48] [8100] global_step=8100, grad_norm=3.821638584136963, loss=1.70380699634552
I0215 16:36:17.408590 140046681523968 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.6689907312393188, loss=1.6404436826705933
I0215 16:37:40.879831 140046689916672 logging_writer.py:48] [8300] global_step=8300, grad_norm=2.716261625289917, loss=1.6685655117034912
I0215 16:38:58.081643 140046681523968 logging_writer.py:48] [8400] global_step=8400, grad_norm=2.824437379837036, loss=1.6901981830596924
I0215 16:40:14.336269 140046689916672 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.9069055318832397, loss=1.5713149309158325
I0215 16:41:35.175845 140046681523968 logging_writer.py:48] [8600] global_step=8600, grad_norm=5.016488552093506, loss=1.6410208940505981
I0215 16:42:53.377689 140046689916672 logging_writer.py:48] [8700] global_step=8700, grad_norm=2.4939815998077393, loss=1.6445895433425903
I0215 16:44:14.197354 140046681523968 logging_writer.py:48] [8800] global_step=8800, grad_norm=4.064261436462402, loss=1.6015301942825317
I0215 16:45:38.406392 140046689916672 logging_writer.py:48] [8900] global_step=8900, grad_norm=2.287789821624756, loss=1.6812986135482788
I0215 16:46:03.206250 140202902193984 spec.py:321] Evaluating on the training split.
I0215 16:46:56.117964 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 16:47:49.196059 140202902193984 spec.py:349] Evaluating on the test split.
I0215 16:48:16.008487 140202902193984 submission_runner.py:408] Time since start: 8044.68s, 	Step: 8929, 	{'train/ctc_loss': Array(0.61008316, dtype=float32), 'train/wer': 0.19599258267229835, 'validation/ctc_loss': Array(0.7310794, dtype=float32), 'validation/wer': 0.21135966479044577, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4402412, dtype=float32), 'test/wer': 0.14279040480978206, 'test/num_examples': 2472, 'score': 7216.382225990295, 'total_duration': 8044.677983999252, 'accumulated_submission_time': 7216.382225990295, 'accumulated_eval_time': 827.719379901886, 'accumulated_logging_time': 0.18550825119018555}
I0215 16:48:16.037945 140046106236672 logging_writer.py:48] [8929] accumulated_eval_time=827.719380, accumulated_logging_time=0.185508, accumulated_submission_time=7216.382226, global_step=8929, preemption_count=0, score=7216.382226, test/ctc_loss=0.4402411878108978, test/num_examples=2472, test/wer=0.142790, total_duration=8044.677984, train/ctc_loss=0.6100831627845764, train/wer=0.195993, validation/ctc_loss=0.7310793995857239, validation/num_examples=5348, validation/wer=0.211360
I0215 16:49:10.187447 140046097843968 logging_writer.py:48] [9000] global_step=9000, grad_norm=3.003542184829712, loss=1.7137528657913208
I0215 16:50:25.857353 140046106236672 logging_writer.py:48] [9100] global_step=9100, grad_norm=3.4685614109039307, loss=1.626651406288147
I0215 16:51:40.711408 140046097843968 logging_writer.py:48] [9200] global_step=9200, grad_norm=3.346604347229004, loss=1.6332722902297974
I0215 16:53:05.124903 140046106236672 logging_writer.py:48] [9300] global_step=9300, grad_norm=3.8282949924468994, loss=1.584621548652649
I0215 16:54:21.501962 140046097843968 logging_writer.py:48] [9400] global_step=9400, grad_norm=2.621048927307129, loss=1.604931116104126
I0215 16:55:40.225660 140046106236672 logging_writer.py:48] [9500] global_step=9500, grad_norm=2.4296622276306152, loss=1.5660101175308228
I0215 16:57:01.777556 140046097843968 logging_writer.py:48] [9600] global_step=9600, grad_norm=3.850722074508667, loss=1.6602768898010254
I0215 16:58:21.088480 140046106236672 logging_writer.py:48] [9700] global_step=9700, grad_norm=2.80614972114563, loss=1.5415198802947998
I0215 16:59:43.748006 140046097843968 logging_writer.py:48] [9800] global_step=9800, grad_norm=2.015139579772949, loss=1.5979996919631958
I0215 17:01:05.124560 140046106236672 logging_writer.py:48] [9900] global_step=9900, grad_norm=4.772369861602783, loss=1.6088591814041138
I0215 17:02:25.940415 140046097843968 logging_writer.py:48] [10000] global_step=10000, grad_norm=2.5447604656219482, loss=1.619194746017456
I0215 17:03:46.250475 140046106236672 logging_writer.py:48] [10100] global_step=10100, grad_norm=2.2812795639038086, loss=1.6631526947021484
I0215 17:05:09.705523 140046097843968 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.5946606397628784, loss=1.5829699039459229
I0215 17:06:35.434764 140046106236672 logging_writer.py:48] [10300] global_step=10300, grad_norm=3.0983026027679443, loss=1.5594799518585205
I0215 17:07:52.034874 140046097843968 logging_writer.py:48] [10400] global_step=10400, grad_norm=4.4030632972717285, loss=1.5527572631835938
I0215 17:09:08.322904 140046106236672 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.4263242483139038, loss=1.5753597021102905
I0215 17:10:24.354835 140046097843968 logging_writer.py:48] [10600] global_step=10600, grad_norm=4.002843379974365, loss=1.542892336845398
I0215 17:11:49.306739 140046106236672 logging_writer.py:48] [10700] global_step=10700, grad_norm=2.0603854656219482, loss=1.5182019472122192
I0215 17:12:16.683933 140202902193984 spec.py:321] Evaluating on the training split.
I0215 17:13:11.979357 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 17:14:05.761859 140202902193984 spec.py:349] Evaluating on the test split.
I0215 17:14:32.215199 140202902193984 submission_runner.py:408] Time since start: 9620.88s, 	Step: 10735, 	{'train/ctc_loss': Array(0.5634109, dtype=float32), 'train/wer': 0.18198303183436979, 'validation/ctc_loss': Array(0.69694114, dtype=float32), 'validation/wer': 0.20091333017948, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4191527, dtype=float32), 'test/wer': 0.1371844088314748, 'test/num_examples': 2472, 'score': 8656.938082933426, 'total_duration': 9620.884849786758, 'accumulated_submission_time': 8656.938082933426, 'accumulated_eval_time': 963.2478840351105, 'accumulated_logging_time': 0.22718000411987305}
I0215 17:14:32.241979 140046106236672 logging_writer.py:48] [10735] accumulated_eval_time=963.247884, accumulated_logging_time=0.227180, accumulated_submission_time=8656.938083, global_step=10735, preemption_count=0, score=8656.938083, test/ctc_loss=0.41915270686149597, test/num_examples=2472, test/wer=0.137184, total_duration=9620.884850, train/ctc_loss=0.5634108781814575, train/wer=0.181983, validation/ctc_loss=0.6969411373138428, validation/num_examples=5348, validation/wer=0.200913
I0215 17:15:22.246994 140046097843968 logging_writer.py:48] [10800] global_step=10800, grad_norm=2.8957021236419678, loss=1.6122190952301025
I0215 17:16:38.290738 140046106236672 logging_writer.py:48] [10900] global_step=10900, grad_norm=3.064288377761841, loss=1.53697669506073
I0215 17:17:53.905270 140046097843968 logging_writer.py:48] [11000] global_step=11000, grad_norm=6.486134052276611, loss=1.5811747312545776
I0215 17:19:12.745457 140046106236672 logging_writer.py:48] [11100] global_step=11100, grad_norm=2.1808969974517822, loss=1.543430209159851
I0215 17:20:35.529573 140046097843968 logging_writer.py:48] [11200] global_step=11200, grad_norm=2.236908435821533, loss=1.5436937808990479
I0215 17:21:59.141977 140046106236672 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.9722533226013184, loss=1.5776268243789673
I0215 17:23:22.677368 140046106236672 logging_writer.py:48] [11400] global_step=11400, grad_norm=2.1178934574127197, loss=1.575098991394043
I0215 17:24:38.367897 140046097843968 logging_writer.py:48] [11500] global_step=11500, grad_norm=2.835843801498413, loss=1.5481913089752197
I0215 17:25:54.067964 140046106236672 logging_writer.py:48] [11600] global_step=11600, grad_norm=2.3311593532562256, loss=1.5469605922698975
I0215 17:27:13.503227 140046097843968 logging_writer.py:48] [11700] global_step=11700, grad_norm=4.728335857391357, loss=1.5096368789672852
I0215 17:28:35.980311 140046106236672 logging_writer.py:48] [11800] global_step=11800, grad_norm=2.0847020149230957, loss=1.5258463621139526
I0215 17:30:01.537160 140046097843968 logging_writer.py:48] [11900] global_step=11900, grad_norm=2.9035232067108154, loss=1.5625221729278564
I0215 17:31:24.622106 140046106236672 logging_writer.py:48] [12000] global_step=12000, grad_norm=2.702101469039917, loss=1.5786771774291992
I0215 17:32:48.972428 140046097843968 logging_writer.py:48] [12100] global_step=12100, grad_norm=3.408350944519043, loss=1.5326707363128662
I0215 17:34:15.143585 140046106236672 logging_writer.py:48] [12200] global_step=12200, grad_norm=2.4136853218078613, loss=1.593779444694519
I0215 17:35:36.635802 140046097843968 logging_writer.py:48] [12300] global_step=12300, grad_norm=2.484605312347412, loss=1.5026741027832031
I0215 17:37:00.533766 140046106236672 logging_writer.py:48] [12400] global_step=12400, grad_norm=2.34311842918396, loss=1.5644959211349487
I0215 17:38:16.206360 140046097843968 logging_writer.py:48] [12500] global_step=12500, grad_norm=2.763385772705078, loss=1.558438777923584
I0215 17:38:32.508862 140202902193984 spec.py:321] Evaluating on the training split.
I0215 17:39:26.227440 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 17:40:17.519059 140202902193984 spec.py:349] Evaluating on the test split.
I0215 17:40:43.858546 140202902193984 submission_runner.py:408] Time since start: 11192.53s, 	Step: 12523, 	{'train/ctc_loss': Array(0.5032294, dtype=float32), 'train/wer': 0.1660165444885332, 'validation/ctc_loss': Array(0.6597988, dtype=float32), 'validation/wer': 0.19204070401730114, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39195168, dtype=float32), 'test/wer': 0.12796295167875205, 'test/num_examples': 2472, 'score': 10097.114848613739, 'total_duration': 11192.528175830841, 'accumulated_submission_time': 10097.114848613739, 'accumulated_eval_time': 1094.5947580337524, 'accumulated_logging_time': 0.2684915065765381}
I0215 17:40:43.885694 140046106236672 logging_writer.py:48] [12523] accumulated_eval_time=1094.594758, accumulated_logging_time=0.268492, accumulated_submission_time=10097.114849, global_step=12523, preemption_count=0, score=10097.114849, test/ctc_loss=0.39195168018341064, test/num_examples=2472, test/wer=0.127963, total_duration=11192.528176, train/ctc_loss=0.5032293796539307, train/wer=0.166017, validation/ctc_loss=0.659798800945282, validation/num_examples=5348, validation/wer=0.192041
I0215 17:41:42.672363 140046097843968 logging_writer.py:48] [12600] global_step=12600, grad_norm=2.768339157104492, loss=1.5359388589859009
I0215 17:42:57.631928 140046106236672 logging_writer.py:48] [12700] global_step=12700, grad_norm=2.737191677093506, loss=1.5446339845657349
I0215 17:44:13.739381 140046097843968 logging_writer.py:48] [12800] global_step=12800, grad_norm=3.0869152545928955, loss=1.62699556350708
I0215 17:45:29.716803 140046106236672 logging_writer.py:48] [12900] global_step=12900, grad_norm=2.10078501701355, loss=1.5901108980178833
I0215 17:46:53.586893 140046097843968 logging_writer.py:48] [13000] global_step=13000, grad_norm=2.158402681350708, loss=1.5754674673080444
I0215 17:48:18.722715 140046106236672 logging_writer.py:48] [13100] global_step=13100, grad_norm=3.8786239624023438, loss=1.5366684198379517
I0215 17:49:40.754310 140046097843968 logging_writer.py:48] [13200] global_step=13200, grad_norm=2.4923901557922363, loss=1.514328122138977
I0215 17:51:06.088132 140046106236672 logging_writer.py:48] [13300] global_step=13300, grad_norm=2.273993968963623, loss=1.4872372150421143
I0215 17:52:34.130576 140046106236672 logging_writer.py:48] [13400] global_step=13400, grad_norm=3.2690539360046387, loss=1.5600461959838867
I0215 17:53:49.395816 140046097843968 logging_writer.py:48] [13500] global_step=13500, grad_norm=6.092494487762451, loss=1.4923202991485596
I0215 17:55:04.417178 140046106236672 logging_writer.py:48] [13600] global_step=13600, grad_norm=2.3743722438812256, loss=1.5826369524002075
I0215 17:56:21.310838 140046097843968 logging_writer.py:48] [13700] global_step=13700, grad_norm=3.378441333770752, loss=1.5711121559143066
I0215 17:57:44.695647 140046106236672 logging_writer.py:48] [13800] global_step=13800, grad_norm=2.794220209121704, loss=1.4968774318695068
I0215 17:59:08.133877 140046097843968 logging_writer.py:48] [13900] global_step=13900, grad_norm=3.074984073638916, loss=1.568259835243225
I0215 18:00:27.709425 140046106236672 logging_writer.py:48] [14000] global_step=14000, grad_norm=2.129497528076172, loss=1.4666070938110352
I0215 18:01:52.072463 140046097843968 logging_writer.py:48] [14100] global_step=14100, grad_norm=5.53791618347168, loss=1.514768362045288
I0215 18:03:16.839519 140046106236672 logging_writer.py:48] [14200] global_step=14200, grad_norm=3.2134835720062256, loss=1.5654033422470093
I0215 18:04:38.634832 140046097843968 logging_writer.py:48] [14300] global_step=14300, grad_norm=4.925414085388184, loss=1.4999638795852661
I0215 18:04:44.607839 140202902193984 spec.py:321] Evaluating on the training split.
I0215 18:05:37.830980 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 18:06:29.289497 140202902193984 spec.py:349] Evaluating on the test split.
I0215 18:06:55.995538 140202902193984 submission_runner.py:408] Time since start: 12764.67s, 	Step: 14309, 	{'train/ctc_loss': Array(0.4797483, dtype=float32), 'train/wer': 0.15956026653279176, 'validation/ctc_loss': Array(0.62959296, dtype=float32), 'validation/wer': 0.18249225214091933, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3724739, dtype=float32), 'test/wer': 0.1187008713667662, 'test/num_examples': 2472, 'score': 11537.74647140503, 'total_duration': 12764.665168046951, 'accumulated_submission_time': 11537.74647140503, 'accumulated_eval_time': 1225.9796528816223, 'accumulated_logging_time': 0.30987048149108887}
I0215 18:06:56.021556 140046106236672 logging_writer.py:48] [14309] accumulated_eval_time=1225.979653, accumulated_logging_time=0.309870, accumulated_submission_time=11537.746471, global_step=14309, preemption_count=0, score=11537.746471, test/ctc_loss=0.37247389554977417, test/num_examples=2472, test/wer=0.118701, total_duration=12764.665168, train/ctc_loss=0.47974830865859985, train/wer=0.159560, validation/ctc_loss=0.6295929551124573, validation/num_examples=5348, validation/wer=0.182492
I0215 18:08:05.977977 140046097843968 logging_writer.py:48] [14400] global_step=14400, grad_norm=2.319978952407837, loss=1.5281596183776855
I0215 18:09:26.180958 140046106236672 logging_writer.py:48] [14500] global_step=14500, grad_norm=2.6364309787750244, loss=1.4766970872879028
I0215 18:10:42.192583 140046097843968 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.9019526243209839, loss=1.4979982376098633
I0215 18:11:58.676644 140046106236672 logging_writer.py:48] [14700] global_step=14700, grad_norm=3.157742977142334, loss=1.4753764867782593
I0215 18:13:13.695357 140046097843968 logging_writer.py:48] [14800] global_step=14800, grad_norm=3.1304898262023926, loss=1.4898943901062012
I0215 18:14:35.552650 140046106236672 logging_writer.py:48] [14900] global_step=14900, grad_norm=2.525130271911621, loss=1.5221308469772339
I0215 18:15:59.060910 140046097843968 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.9892101287841797, loss=1.4925110340118408
I0215 18:17:24.239650 140046106236672 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.8619648218154907, loss=1.418294906616211
I0215 18:18:47.903453 140046097843968 logging_writer.py:48] [15200] global_step=15200, grad_norm=3.2084550857543945, loss=1.5236364603042603
I0215 18:20:14.671591 140046106236672 logging_writer.py:48] [15300] global_step=15300, grad_norm=2.881399393081665, loss=1.479538083076477
I0215 18:21:39.156844 140046097843968 logging_writer.py:48] [15400] global_step=15400, grad_norm=2.0331742763519287, loss=1.4688327312469482
I0215 18:23:01.569432 140046106236672 logging_writer.py:48] [15500] global_step=15500, grad_norm=2.214879274368286, loss=1.493152141571045
I0215 18:24:17.893767 140046097843968 logging_writer.py:48] [15600] global_step=15600, grad_norm=2.3737857341766357, loss=1.4158486127853394
I0215 18:25:35.539175 140046106236672 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.9936423301696777, loss=1.4965837001800537
I0215 18:26:54.820760 140046097843968 logging_writer.py:48] [15800] global_step=15800, grad_norm=2.544300079345703, loss=1.4802370071411133
I0215 18:28:18.675675 140046106236672 logging_writer.py:48] [15900] global_step=15900, grad_norm=2.314363718032837, loss=1.4436392784118652
I0215 18:29:42.570837 140046097843968 logging_writer.py:48] [16000] global_step=16000, grad_norm=2.8668222427368164, loss=1.483349084854126
I0215 18:30:56.218208 140202902193984 spec.py:321] Evaluating on the training split.
I0215 18:31:50.029869 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 18:32:41.134092 140202902193984 spec.py:349] Evaluating on the test split.
I0215 18:33:07.568138 140202902193984 submission_runner.py:408] Time since start: 14336.23s, 	Step: 16084, 	{'train/ctc_loss': Array(0.43343863, dtype=float32), 'train/wer': 0.14823907512386733, 'validation/ctc_loss': Array(0.6055314, dtype=float32), 'validation/wer': 0.17722081157013622, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35472116, dtype=float32), 'test/wer': 0.1176446692259257, 'test/num_examples': 2472, 'score': 12977.85252571106, 'total_duration': 14336.233743190765, 'accumulated_submission_time': 12977.85252571106, 'accumulated_eval_time': 1357.3227698802948, 'accumulated_logging_time': 0.35144686698913574}
I0215 18:33:07.612135 140046904956672 logging_writer.py:48] [16084] accumulated_eval_time=1357.322770, accumulated_logging_time=0.351447, accumulated_submission_time=12977.852526, global_step=16084, preemption_count=0, score=12977.852526, test/ctc_loss=0.35472115874290466, test/num_examples=2472, test/wer=0.117645, total_duration=14336.233743, train/ctc_loss=0.43343862891197205, train/wer=0.148239, validation/ctc_loss=0.6055313944816589, validation/num_examples=5348, validation/wer=0.177221
I0215 18:33:20.436386 140046896563968 logging_writer.py:48] [16100] global_step=16100, grad_norm=2.681931972503662, loss=1.4493309259414673
I0215 18:34:35.735067 140046904956672 logging_writer.py:48] [16200] global_step=16200, grad_norm=4.935433387756348, loss=1.4954330921173096
I0215 18:35:51.351759 140046896563968 logging_writer.py:48] [16300] global_step=16300, grad_norm=2.551037549972534, loss=1.474044680595398
I0215 18:37:07.659913 140046904956672 logging_writer.py:48] [16400] global_step=16400, grad_norm=2.433932304382324, loss=1.5640758275985718
I0215 18:38:34.721269 140046249596672 logging_writer.py:48] [16500] global_step=16500, grad_norm=2.7447803020477295, loss=1.4739693403244019
I0215 18:39:50.995105 140046241203968 logging_writer.py:48] [16600] global_step=16600, grad_norm=2.004044771194458, loss=1.4874143600463867
I0215 18:41:07.365925 140046249596672 logging_writer.py:48] [16700] global_step=16700, grad_norm=2.6117279529571533, loss=1.4106348752975464
I0215 18:42:22.486497 140046241203968 logging_writer.py:48] [16800] global_step=16800, grad_norm=3.9861812591552734, loss=1.458275556564331
I0215 18:43:41.775903 140046249596672 logging_writer.py:48] [16900] global_step=16900, grad_norm=2.6390023231506348, loss=1.4407055377960205
I0215 18:45:07.056579 140046241203968 logging_writer.py:48] [17000] global_step=17000, grad_norm=2.5927734375, loss=1.411812424659729
I0215 18:46:31.541677 140046249596672 logging_writer.py:48] [17100] global_step=17100, grad_norm=3.2115981578826904, loss=1.4824581146240234
I0215 18:47:56.564202 140046241203968 logging_writer.py:48] [17200] global_step=17200, grad_norm=3.184131145477295, loss=1.5121362209320068
I0215 18:49:22.916114 140046249596672 logging_writer.py:48] [17300] global_step=17300, grad_norm=2.2689406871795654, loss=1.456547498703003
I0215 18:50:48.152113 140046241203968 logging_writer.py:48] [17400] global_step=17400, grad_norm=2.4030399322509766, loss=1.4081753492355347
I0215 18:52:15.135429 140046249596672 logging_writer.py:48] [17500] global_step=17500, grad_norm=3.275883436203003, loss=1.4659931659698486
I0215 18:53:35.874342 140046904956672 logging_writer.py:48] [17600] global_step=17600, grad_norm=3.239450454711914, loss=1.3875092267990112
I0215 18:54:51.640316 140046896563968 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.7531418800354004, loss=1.4126214981079102
I0215 18:56:08.208188 140046904956672 logging_writer.py:48] [17800] global_step=17800, grad_norm=2.6849465370178223, loss=1.513543963432312
I0215 18:57:07.634512 140202902193984 spec.py:321] Evaluating on the training split.
I0215 18:58:01.645735 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 18:58:52.894106 140202902193984 spec.py:349] Evaluating on the test split.
I0215 18:59:20.495578 140202902193984 submission_runner.py:408] Time since start: 15909.16s, 	Step: 17879, 	{'train/ctc_loss': Array(0.4497294, dtype=float32), 'train/wer': 0.14671168375683938, 'validation/ctc_loss': Array(0.5981819, dtype=float32), 'validation/wer': 0.17235486642787493, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3461188, dtype=float32), 'test/wer': 0.11144963743830358, 'test/num_examples': 2472, 'score': 14417.783019304276, 'total_duration': 15909.162315368652, 'accumulated_submission_time': 14417.783019304276, 'accumulated_eval_time': 1490.1781721115112, 'accumulated_logging_time': 0.41077733039855957}
I0215 18:59:20.530076 140046904956672 logging_writer.py:48] [17879] accumulated_eval_time=1490.178172, accumulated_logging_time=0.410777, accumulated_submission_time=14417.783019, global_step=17879, preemption_count=0, score=14417.783019, test/ctc_loss=0.3461188077926636, test/num_examples=2472, test/wer=0.111450, total_duration=15909.162315, train/ctc_loss=0.44972941279411316, train/wer=0.146712, validation/ctc_loss=0.5981819033622742, validation/num_examples=5348, validation/wer=0.172355
I0215 18:59:37.228056 140046896563968 logging_writer.py:48] [17900] global_step=17900, grad_norm=3.101555824279785, loss=1.5238884687423706
I0215 19:00:53.636882 140046904956672 logging_writer.py:48] [18000] global_step=18000, grad_norm=2.3195199966430664, loss=1.4378862380981445
I0215 19:02:08.825378 140046896563968 logging_writer.py:48] [18100] global_step=18100, grad_norm=2.4288415908813477, loss=1.4635084867477417
I0215 19:03:26.172818 140046904956672 logging_writer.py:48] [18200] global_step=18200, grad_norm=2.4149115085601807, loss=1.4284228086471558
I0215 19:04:52.758272 140046896563968 logging_writer.py:48] [18300] global_step=18300, grad_norm=2.333271026611328, loss=1.3984730243682861
I0215 19:06:15.525264 140046904956672 logging_writer.py:48] [18400] global_step=18400, grad_norm=2.4506404399871826, loss=1.4009336233139038
I0215 19:07:40.687421 140046896563968 logging_writer.py:48] [18500] global_step=18500, grad_norm=2.2845404148101807, loss=1.4143106937408447
I0215 19:09:03.863314 140046249596672 logging_writer.py:48] [18600] global_step=18600, grad_norm=2.264000654220581, loss=1.4216253757476807
I0215 19:10:18.827960 140046241203968 logging_writer.py:48] [18700] global_step=18700, grad_norm=2.2921178340911865, loss=1.5375069379806519
I0215 19:11:35.565878 140046249596672 logging_writer.py:48] [18800] global_step=18800, grad_norm=2.402334451675415, loss=1.381087303161621
I0215 19:12:58.169646 140046241203968 logging_writer.py:48] [18900] global_step=18900, grad_norm=4.8774919509887695, loss=1.489768385887146
I0215 19:14:17.491600 140046249596672 logging_writer.py:48] [19000] global_step=19000, grad_norm=3.634345531463623, loss=1.4569748640060425
I0215 19:15:41.021701 140046241203968 logging_writer.py:48] [19100] global_step=19100, grad_norm=2.719308376312256, loss=1.4146767854690552
I0215 19:17:04.943454 140046249596672 logging_writer.py:48] [19200] global_step=19200, grad_norm=5.353213787078857, loss=1.4429707527160645
I0215 19:18:31.258147 140046241203968 logging_writer.py:48] [19300] global_step=19300, grad_norm=2.0387141704559326, loss=1.322044849395752
I0215 19:19:57.184351 140046249596672 logging_writer.py:48] [19400] global_step=19400, grad_norm=2.6400272846221924, loss=1.3698683977127075
I0215 19:21:20.253104 140046241203968 logging_writer.py:48] [19500] global_step=19500, grad_norm=3.9745290279388428, loss=1.4028407335281372
I0215 19:22:46.548042 140046904956672 logging_writer.py:48] [19600] global_step=19600, grad_norm=2.9127655029296875, loss=1.4317635297775269
I0215 19:23:20.899713 140202902193984 spec.py:321] Evaluating on the training split.
I0215 19:24:15.410811 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 19:25:06.987736 140202902193984 spec.py:349] Evaluating on the test split.
I0215 19:25:33.378683 140202902193984 submission_runner.py:408] Time since start: 17482.05s, 	Step: 19647, 	{'train/ctc_loss': Array(0.41132402, dtype=float32), 'train/wer': 0.13723418528208225, 'validation/ctc_loss': Array(0.5722585, dtype=float32), 'validation/wer': 0.16724755495911253, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32342452, dtype=float32), 'test/wer': 0.1053561635488392, 'test/num_examples': 2472, 'score': 15858.06008553505, 'total_duration': 17482.04532623291, 'accumulated_submission_time': 15858.06008553505, 'accumulated_eval_time': 1622.651347875595, 'accumulated_logging_time': 0.46189212799072266}
I0215 19:25:33.412695 140046904956672 logging_writer.py:48] [19647] accumulated_eval_time=1622.651348, accumulated_logging_time=0.461892, accumulated_submission_time=15858.060086, global_step=19647, preemption_count=0, score=15858.060086, test/ctc_loss=0.3234245181083679, test/num_examples=2472, test/wer=0.105356, total_duration=17482.045326, train/ctc_loss=0.41132402420043945, train/wer=0.137234, validation/ctc_loss=0.572258472442627, validation/num_examples=5348, validation/wer=0.167248
I0215 19:26:14.637320 140046896563968 logging_writer.py:48] [19700] global_step=19700, grad_norm=2.197214365005493, loss=1.3597872257232666
I0215 19:27:29.603762 140046904956672 logging_writer.py:48] [19800] global_step=19800, grad_norm=2.5624332427978516, loss=1.4604032039642334
I0215 19:28:44.958244 140046896563968 logging_writer.py:48] [19900] global_step=19900, grad_norm=3.0977139472961426, loss=1.4488155841827393
I0215 19:29:59.635249 140046904956672 logging_writer.py:48] [20000] global_step=20000, grad_norm=3.292846441268921, loss=1.4037115573883057
I0215 19:31:16.219448 140046896563968 logging_writer.py:48] [20100] global_step=20100, grad_norm=2.8940367698669434, loss=1.4526718854904175
I0215 19:32:39.671346 140046904956672 logging_writer.py:48] [20200] global_step=20200, grad_norm=3.2203750610351562, loss=1.4736716747283936
I0215 19:34:04.160380 140046896563968 logging_writer.py:48] [20300] global_step=20300, grad_norm=5.000812530517578, loss=1.3786354064941406
I0215 19:35:29.689673 140046904956672 logging_writer.py:48] [20400] global_step=20400, grad_norm=3.132687568664551, loss=1.4610199928283691
I0215 19:36:57.297729 140046896563968 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.8046045303344727, loss=1.3611897230148315
I0215 19:38:25.385783 140046249596672 logging_writer.py:48] [20600] global_step=20600, grad_norm=2.4664177894592285, loss=1.4256445169448853
I0215 19:39:40.909509 140046241203968 logging_writer.py:48] [20700] global_step=20700, grad_norm=2.1711905002593994, loss=1.336548924446106
I0215 19:40:57.117960 140046249596672 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.8817710876464844, loss=1.3838448524475098
I0215 19:42:12.648389 140046241203968 logging_writer.py:48] [20900] global_step=20900, grad_norm=2.1756396293640137, loss=1.448180913925171
I0215 19:43:34.608617 140046249596672 logging_writer.py:48] [21000] global_step=21000, grad_norm=2.067291498184204, loss=1.4118753671646118
I0215 19:44:55.954709 140046241203968 logging_writer.py:48] [21100] global_step=21100, grad_norm=2.486509084701538, loss=1.4175957441329956
I0215 19:46:23.283864 140046249596672 logging_writer.py:48] [21200] global_step=21200, grad_norm=2.9567131996154785, loss=1.4216084480285645
I0215 19:47:53.453234 140046241203968 logging_writer.py:48] [21300] global_step=21300, grad_norm=2.0310447216033936, loss=1.4454386234283447
I0215 19:49:16.905818 140046249596672 logging_writer.py:48] [21400] global_step=21400, grad_norm=2.4219508171081543, loss=1.3962311744689941
I0215 19:49:34.295038 140202902193984 spec.py:321] Evaluating on the training split.
I0215 19:50:29.240962 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 19:51:20.766582 140202902193984 spec.py:349] Evaluating on the test split.
I0215 19:51:47.625963 140202902193984 submission_runner.py:408] Time since start: 19056.29s, 	Step: 21421, 	{'train/ctc_loss': Array(0.4032407, dtype=float32), 'train/wer': 0.13661822166631615, 'validation/ctc_loss': Array(0.5550742, dtype=float32), 'validation/wer': 0.16248781100051168, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31684622, dtype=float32), 'test/wer': 0.10295939715231654, 'test/num_examples': 2472, 'score': 17298.850484371185, 'total_duration': 19056.292241811752, 'accumulated_submission_time': 17298.850484371185, 'accumulated_eval_time': 1755.9761242866516, 'accumulated_logging_time': 0.511904239654541}
I0215 19:51:47.664892 140046034556672 logging_writer.py:48] [21421] accumulated_eval_time=1755.976124, accumulated_logging_time=0.511904, accumulated_submission_time=17298.850484, global_step=21421, preemption_count=0, score=17298.850484, test/ctc_loss=0.31684622168540955, test/num_examples=2472, test/wer=0.102959, total_duration=19056.292242, train/ctc_loss=0.40324071049690247, train/wer=0.136618, validation/ctc_loss=0.5550742149353027, validation/num_examples=5348, validation/wer=0.162488
I0215 19:52:47.956839 140046026163968 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.7451794147491455, loss=1.422384262084961
I0215 19:54:03.671885 140046034556672 logging_writer.py:48] [21600] global_step=21600, grad_norm=2.190113067626953, loss=1.4552178382873535
I0215 19:55:22.520176 140046034556672 logging_writer.py:48] [21700] global_step=21700, grad_norm=2.0752618312835693, loss=1.3542958498001099
I0215 19:56:39.124741 140046026163968 logging_writer.py:48] [21800] global_step=21800, grad_norm=2.3269529342651367, loss=1.3403170108795166
I0215 19:57:59.655695 140046034556672 logging_writer.py:48] [21900] global_step=21900, grad_norm=2.713165760040283, loss=1.398217797279358
I0215 19:59:18.525471 140046026163968 logging_writer.py:48] [22000] global_step=22000, grad_norm=2.5674729347229004, loss=1.434596300125122
I0215 20:00:40.116655 140046034556672 logging_writer.py:48] [22100] global_step=22100, grad_norm=3.2862844467163086, loss=1.3710864782333374
I0215 20:02:05.752501 140046026163968 logging_writer.py:48] [22200] global_step=22200, grad_norm=2.367107391357422, loss=1.3886699676513672
I0215 20:03:33.829859 140046034556672 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.7350561618804932, loss=1.327467441558838
I0215 20:05:00.923086 140046026163968 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.9726347923278809, loss=1.4110932350158691
I0215 20:06:23.500198 140046034556672 logging_writer.py:48] [22500] global_step=22500, grad_norm=7.0465216636657715, loss=1.352819800376892
I0215 20:07:51.423495 140046026163968 logging_writer.py:48] [22600] global_step=22600, grad_norm=2.840430498123169, loss=1.3288707733154297
I0215 20:09:15.827031 140046904956672 logging_writer.py:48] [22700] global_step=22700, grad_norm=6.704124927520752, loss=1.368087649345398
I0215 20:10:31.371664 140046896563968 logging_writer.py:48] [22800] global_step=22800, grad_norm=2.184587240219116, loss=1.347306489944458
I0215 20:11:46.241346 140046904956672 logging_writer.py:48] [22900] global_step=22900, grad_norm=2.7171905040740967, loss=1.3839365243911743
I0215 20:13:05.115697 140046896563968 logging_writer.py:48] [23000] global_step=23000, grad_norm=2.221968650817871, loss=1.3833662271499634
I0215 20:14:29.461323 140046904956672 logging_writer.py:48] [23100] global_step=23100, grad_norm=2.5021159648895264, loss=1.4258003234863281
I0215 20:15:47.854599 140202902193984 spec.py:321] Evaluating on the training split.
I0215 20:16:41.966322 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 20:17:36.078794 140202902193984 spec.py:349] Evaluating on the test split.
I0215 20:18:03.004415 140202902193984 submission_runner.py:408] Time since start: 20631.67s, 	Step: 23194, 	{'train/ctc_loss': Array(0.3486651, dtype=float32), 'train/wer': 0.11755327268718233, 'validation/ctc_loss': Array(0.53862715, dtype=float32), 'validation/wer': 0.15761221120519034, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30597436, dtype=float32), 'test/wer': 0.09910019702232242, 'test/num_examples': 2472, 'score': 18738.948315143585, 'total_duration': 20631.66925215721, 'accumulated_submission_time': 18738.948315143585, 'accumulated_eval_time': 1891.1183724403381, 'accumulated_logging_time': 0.5660281181335449}
I0215 20:18:03.042004 140046904956672 logging_writer.py:48] [23194] accumulated_eval_time=1891.118372, accumulated_logging_time=0.566028, accumulated_submission_time=18738.948315, global_step=23194, preemption_count=0, score=18738.948315, test/ctc_loss=0.3059743642807007, test/num_examples=2472, test/wer=0.099100, total_duration=20631.669252, train/ctc_loss=0.3486650884151459, train/wer=0.117553, validation/ctc_loss=0.5386271476745605, validation/num_examples=5348, validation/wer=0.157612
I0215 20:18:08.494116 140046896563968 logging_writer.py:48] [23200] global_step=23200, grad_norm=3.3923115730285645, loss=1.3334542512893677
I0215 20:19:23.346835 140046904956672 logging_writer.py:48] [23300] global_step=23300, grad_norm=4.21328592300415, loss=1.4055458307266235
I0215 20:20:38.562678 140046896563968 logging_writer.py:48] [23400] global_step=23400, grad_norm=2.4432179927825928, loss=1.3888581991195679
I0215 20:21:57.844974 140046904956672 logging_writer.py:48] [23500] global_step=23500, grad_norm=2.5566904544830322, loss=1.4312019348144531
I0215 20:23:27.158858 140046896563968 logging_writer.py:48] [23600] global_step=23600, grad_norm=3.1399972438812256, loss=1.405458927154541
I0215 20:24:54.727727 140046904956672 logging_writer.py:48] [23700] global_step=23700, grad_norm=2.1277618408203125, loss=1.4147021770477295
I0215 20:26:09.654105 140046896563968 logging_writer.py:48] [23800] global_step=23800, grad_norm=3.1281015872955322, loss=1.352381706237793
I0215 20:27:24.814735 140046904956672 logging_writer.py:48] [23900] global_step=23900, grad_norm=2.483856678009033, loss=1.411513090133667
I0215 20:28:40.926583 140046896563968 logging_writer.py:48] [24000] global_step=24000, grad_norm=2.616304636001587, loss=1.3567708730697632
I0215 20:30:03.338678 140046904956672 logging_writer.py:48] [24100] global_step=24100, grad_norm=2.0142927169799805, loss=1.3549479246139526
I0215 20:31:26.921685 140046896563968 logging_writer.py:48] [24200] global_step=24200, grad_norm=2.1232011318206787, loss=1.366104245185852
I0215 20:32:51.776110 140046904956672 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.8224979639053345, loss=1.3929393291473389
I0215 20:34:19.849159 140046896563968 logging_writer.py:48] [24400] global_step=24400, grad_norm=2.7119855880737305, loss=1.3335860967636108
I0215 20:35:48.498768 140046904956672 logging_writer.py:48] [24500] global_step=24500, grad_norm=2.1716508865356445, loss=1.3759682178497314
I0215 20:37:12.799731 140046896563968 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.8968884944915771, loss=1.3342316150665283
I0215 20:38:39.325066 140046904956672 logging_writer.py:48] [24700] global_step=24700, grad_norm=2.1323859691619873, loss=1.36077880859375
I0215 20:39:59.693426 140046577276672 logging_writer.py:48] [24800] global_step=24800, grad_norm=3.6593010425567627, loss=1.34856116771698
I0215 20:41:14.446911 140046568883968 logging_writer.py:48] [24900] global_step=24900, grad_norm=3.0456769466400146, loss=1.3889901638031006
I0215 20:42:03.718832 140202902193984 spec.py:321] Evaluating on the training split.
I0215 20:42:57.639143 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 20:43:48.498527 140202902193984 spec.py:349] Evaluating on the test split.
I0215 20:44:14.977395 140202902193984 submission_runner.py:408] Time since start: 22203.64s, 	Step: 24964, 	{'train/ctc_loss': Array(0.3170169, dtype=float32), 'train/wer': 0.10613243615231686, 'validation/ctc_loss': Array(0.5284512, dtype=float32), 'validation/wer': 0.1534027824710119, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29627094, dtype=float32), 'test/wer': 0.09595190217943249, 'test/num_examples': 2472, 'score': 20179.532305002213, 'total_duration': 22203.64442896843, 'accumulated_submission_time': 20179.532305002213, 'accumulated_eval_time': 2022.3715333938599, 'accumulated_logging_time': 0.6189842224121094}
I0215 20:44:15.032437 140046070396672 logging_writer.py:48] [24964] accumulated_eval_time=2022.371533, accumulated_logging_time=0.618984, accumulated_submission_time=20179.532305, global_step=24964, preemption_count=0, score=20179.532305, test/ctc_loss=0.2962709367275238, test/num_examples=2472, test/wer=0.095952, total_duration=22203.644429, train/ctc_loss=0.3170168995857239, train/wer=0.106132, validation/ctc_loss=0.5284512042999268, validation/num_examples=5348, validation/wer=0.153403
I0215 20:44:43.184385 140046062003968 logging_writer.py:48] [25000] global_step=25000, grad_norm=4.379775524139404, loss=1.3485112190246582
I0215 20:45:58.471983 140046070396672 logging_writer.py:48] [25100] global_step=25100, grad_norm=2.164858102798462, loss=1.2935384511947632
I0215 20:47:14.207936 140046062003968 logging_writer.py:48] [25200] global_step=25200, grad_norm=3.5858662128448486, loss=1.339923620223999
I0215 20:48:29.611918 140046070396672 logging_writer.py:48] [25300] global_step=25300, grad_norm=2.9322853088378906, loss=1.3310885429382324
I0215 20:49:54.780072 140046062003968 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.8239166736602783, loss=1.3727363348007202
I0215 20:51:18.517143 140046070396672 logging_writer.py:48] [25500] global_step=25500, grad_norm=2.9904680252075195, loss=1.376639485359192
I0215 20:52:41.970613 140046062003968 logging_writer.py:48] [25600] global_step=25600, grad_norm=3.340766668319702, loss=1.3591748476028442
I0215 20:54:05.218518 140046070396672 logging_writer.py:48] [25700] global_step=25700, grad_norm=2.522282123565674, loss=1.3720735311508179
I0215 20:55:29.629951 140046904956672 logging_writer.py:48] [25800] global_step=25800, grad_norm=3.3752400875091553, loss=1.3545655012130737
I0215 20:56:44.539135 140046896563968 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.8528928756713867, loss=1.3127955198287964
I0215 20:58:00.296702 140046904956672 logging_writer.py:48] [26000] global_step=26000, grad_norm=4.133004188537598, loss=1.3966736793518066
I0215 20:59:17.624034 140046896563968 logging_writer.py:48] [26100] global_step=26100, grad_norm=3.8191616535186768, loss=1.370891809463501
I0215 21:00:39.427313 140046904956672 logging_writer.py:48] [26200] global_step=26200, grad_norm=2.556858539581299, loss=1.365703821182251
I0215 21:02:03.968388 140046896563968 logging_writer.py:48] [26300] global_step=26300, grad_norm=2.475151538848877, loss=1.3486912250518799
I0215 21:03:31.044722 140046904956672 logging_writer.py:48] [26400] global_step=26400, grad_norm=2.3009557723999023, loss=1.3455427885055542
I0215 21:04:56.746525 140046896563968 logging_writer.py:48] [26500] global_step=26500, grad_norm=2.9948172569274902, loss=1.3584429025650024
I0215 21:06:22.917186 140046904956672 logging_writer.py:48] [26600] global_step=26600, grad_norm=2.119795322418213, loss=1.309409499168396
I0215 21:07:47.498059 140046896563968 logging_writer.py:48] [26700] global_step=26700, grad_norm=2.181673288345337, loss=1.3699629306793213
I0215 21:08:15.321245 140202902193984 spec.py:321] Evaluating on the training split.
I0215 21:09:09.554697 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 21:10:00.848581 140202902193984 spec.py:349] Evaluating on the test split.
I0215 21:10:27.738664 140202902193984 submission_runner.py:408] Time since start: 23776.40s, 	Step: 26733, 	{'train/ctc_loss': Array(0.35497886, dtype=float32), 'train/wer': 0.12230037142824887, 'validation/ctc_loss': Array(0.5095953, dtype=float32), 'validation/wer': 0.14857545594099075, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28524765, dtype=float32), 'test/wer': 0.09485507687932891, 'test/num_examples': 2472, 'score': 21619.72815155983, 'total_duration': 23776.40490913391, 'accumulated_submission_time': 21619.72815155983, 'accumulated_eval_time': 2154.782774209976, 'accumulated_logging_time': 0.691807746887207}
I0215 21:10:27.776285 140046689916672 logging_writer.py:48] [26733] accumulated_eval_time=2154.782774, accumulated_logging_time=0.691808, accumulated_submission_time=21619.728152, global_step=26733, preemption_count=0, score=21619.728152, test/ctc_loss=0.28524765372276306, test/num_examples=2472, test/wer=0.094855, total_duration=23776.404909, train/ctc_loss=0.35497885942459106, train/wer=0.122300, validation/ctc_loss=0.5095952749252319, validation/num_examples=5348, validation/wer=0.148575
I0215 21:11:23.097107 140046689916672 logging_writer.py:48] [26800] global_step=26800, grad_norm=3.0423834323883057, loss=1.4056015014648438
I0215 21:12:39.298670 140046681523968 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.947826623916626, loss=1.3362317085266113
I0215 21:13:55.152301 140046689916672 logging_writer.py:48] [27000] global_step=27000, grad_norm=2.145172119140625, loss=1.330379843711853
I0215 21:15:16.829360 140046681523968 logging_writer.py:48] [27100] global_step=27100, grad_norm=2.100151538848877, loss=1.2980055809020996
I0215 21:16:40.865827 140046689916672 logging_writer.py:48] [27200] global_step=27200, grad_norm=2.134758472442627, loss=1.3159371614456177
I0215 21:18:08.013703 140046681523968 logging_writer.py:48] [27300] global_step=27300, grad_norm=4.146176338195801, loss=1.3041492700576782
I0215 21:19:34.792978 140046689916672 logging_writer.py:48] [27400] global_step=27400, grad_norm=2.8151438236236572, loss=1.3315913677215576
I0215 21:21:02.168357 140046681523968 logging_writer.py:48] [27500] global_step=27500, grad_norm=2.1557302474975586, loss=1.2888267040252686
I0215 21:22:26.906187 140046689916672 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.7421016693115234, loss=1.3330762386322021
I0215 21:23:55.391264 140046681523968 logging_writer.py:48] [27700] global_step=27700, grad_norm=2.470425844192505, loss=1.2773157358169556
I0215 21:25:23.385178 140046689916672 logging_writer.py:48] [27800] global_step=27800, grad_norm=2.0024421215057373, loss=1.3483394384384155
I0215 21:26:42.483415 140046689916672 logging_writer.py:48] [27900] global_step=27900, grad_norm=2.596134901046753, loss=1.3480851650238037
I0215 21:27:58.407086 140046681523968 logging_writer.py:48] [28000] global_step=28000, grad_norm=4.3192667961120605, loss=1.2921676635742188
I0215 21:29:15.129388 140046689916672 logging_writer.py:48] [28100] global_step=28100, grad_norm=2.4250075817108154, loss=1.2453442811965942
I0215 21:30:37.263411 140046681523968 logging_writer.py:48] [28200] global_step=28200, grad_norm=2.5162341594696045, loss=1.2690579891204834
I0215 21:32:02.902646 140046689916672 logging_writer.py:48] [28300] global_step=28300, grad_norm=2.669767141342163, loss=1.3123692274093628
I0215 21:33:31.744542 140046681523968 logging_writer.py:48] [28400] global_step=28400, grad_norm=2.6911981105804443, loss=1.3265516757965088
I0215 21:34:27.980590 140202902193984 spec.py:321] Evaluating on the training split.
I0215 21:35:21.925147 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 21:36:14.657381 140202902193984 spec.py:349] Evaluating on the test split.
I0215 21:36:41.639118 140202902193984 submission_runner.py:408] Time since start: 25350.31s, 	Step: 28465, 	{'train/ctc_loss': Array(0.32599312, dtype=float32), 'train/wer': 0.1092377701934016, 'validation/ctc_loss': Array(0.5059581, dtype=float32), 'validation/wer': 0.14743620687990577, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2816703, dtype=float32), 'test/wer': 0.09004123250665204, 'test/num_examples': 2472, 'score': 23059.842032194138, 'total_duration': 25350.305683374405, 'accumulated_submission_time': 23059.842032194138, 'accumulated_eval_time': 2288.435446739197, 'accumulated_logging_time': 0.7465569972991943}
I0215 21:36:41.686712 140047119996672 logging_writer.py:48] [28465] accumulated_eval_time=2288.435447, accumulated_logging_time=0.746557, accumulated_submission_time=23059.842032, global_step=28465, preemption_count=0, score=23059.842032, test/ctc_loss=0.28167030215263367, test/num_examples=2472, test/wer=0.090041, total_duration=25350.305683, train/ctc_loss=0.3259931206703186, train/wer=0.109238, validation/ctc_loss=0.505958080291748, validation/num_examples=5348, validation/wer=0.147436
I0215 21:37:09.145941 140047111603968 logging_writer.py:48] [28500] global_step=28500, grad_norm=2.4857654571533203, loss=1.3246077299118042
I0215 21:38:24.508980 140047119996672 logging_writer.py:48] [28600] global_step=28600, grad_norm=3.0518414974212646, loss=1.312124252319336
I0215 21:39:39.944154 140047111603968 logging_writer.py:48] [28700] global_step=28700, grad_norm=2.122361898422241, loss=1.3337152004241943
I0215 21:41:02.137821 140047119996672 logging_writer.py:48] [28800] global_step=28800, grad_norm=2.214390993118286, loss=1.2518682479858398
I0215 21:42:22.255753 140047119996672 logging_writer.py:48] [28900] global_step=28900, grad_norm=2.292285442352295, loss=1.3239144086837769
I0215 21:43:38.217218 140047111603968 logging_writer.py:48] [29000] global_step=29000, grad_norm=3.8428666591644287, loss=1.2662874460220337
I0215 21:44:57.241320 140047119996672 logging_writer.py:48] [29100] global_step=29100, grad_norm=2.743588924407959, loss=1.2872591018676758
I0215 21:46:12.352079 140047111603968 logging_writer.py:48] [29200] global_step=29200, grad_norm=2.579686164855957, loss=1.2710233926773071
I0215 21:47:27.620262 140047119996672 logging_writer.py:48] [29300] global_step=29300, grad_norm=3.038226366043091, loss=1.3291875123977661
I0215 21:48:43.121212 140047111603968 logging_writer.py:48] [29400] global_step=29400, grad_norm=2.2140886783599854, loss=1.3106173276901245
I0215 21:49:59.716043 140047119996672 logging_writer.py:48] [29500] global_step=29500, grad_norm=4.579712867736816, loss=1.2943146228790283
I0215 21:51:17.207927 140047111603968 logging_writer.py:48] [29600] global_step=29600, grad_norm=2.6006250381469727, loss=1.2937761545181274
I0215 21:52:35.456900 140047119996672 logging_writer.py:48] [29700] global_step=29700, grad_norm=2.7356724739074707, loss=1.2889502048492432
I0215 21:53:52.751175 140047111603968 logging_writer.py:48] [29800] global_step=29800, grad_norm=2.2304024696350098, loss=1.2563347816467285
I0215 21:55:12.147639 140046792316672 logging_writer.py:48] [29900] global_step=29900, grad_norm=2.1826798915863037, loss=1.3167061805725098
I0215 21:56:27.106264 140046783923968 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.8947798013687134, loss=1.260983943939209
I0215 21:57:42.283652 140046792316672 logging_writer.py:48] [30100] global_step=30100, grad_norm=3.5302000045776367, loss=1.2595785856246948
I0215 21:58:57.366593 140046783923968 logging_writer.py:48] [30200] global_step=30200, grad_norm=2.185802698135376, loss=1.3191958665847778
I0215 22:00:12.080541 140046792316672 logging_writer.py:48] [30300] global_step=30300, grad_norm=3.9813876152038574, loss=1.2633899450302124
I0215 22:00:41.774074 140202902193984 spec.py:321] Evaluating on the training split.
I0215 22:01:35.336715 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 22:02:24.314808 140202902193984 spec.py:349] Evaluating on the test split.
I0215 22:02:49.301561 140202902193984 submission_runner.py:408] Time since start: 26917.97s, 	Step: 30341, 	{'train/ctc_loss': Array(0.3049627, dtype=float32), 'train/wer': 0.10203451023760347, 'validation/ctc_loss': Array(0.48760784, dtype=float32), 'validation/wer': 0.14285990132944573, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27179763, dtype=float32), 'test/wer': 0.0894318851177056, 'test/num_examples': 2472, 'score': 24499.839766025543, 'total_duration': 26917.968510866165, 'accumulated_submission_time': 24499.839766025543, 'accumulated_eval_time': 2415.9574434757233, 'accumulated_logging_time': 0.8124251365661621}
I0215 22:02:49.337717 140046904956672 logging_writer.py:48] [30341] accumulated_eval_time=2415.957443, accumulated_logging_time=0.812425, accumulated_submission_time=24499.839766, global_step=30341, preemption_count=0, score=24499.839766, test/ctc_loss=0.27179762721061707, test/num_examples=2472, test/wer=0.089432, total_duration=26917.968511, train/ctc_loss=0.304962694644928, train/wer=0.102035, validation/ctc_loss=0.48760783672332764, validation/num_examples=5348, validation/wer=0.142860
I0215 22:03:34.314216 140046896563968 logging_writer.py:48] [30400] global_step=30400, grad_norm=2.4621667861938477, loss=1.2641171216964722
I0215 22:04:49.294109 140046904956672 logging_writer.py:48] [30500] global_step=30500, grad_norm=2.262491226196289, loss=1.2469995021820068
I0215 22:06:04.334600 140046896563968 logging_writer.py:48] [30600] global_step=30600, grad_norm=3.1875734329223633, loss=1.2399462461471558
I0215 22:07:19.244879 140046904956672 logging_writer.py:48] [30700] global_step=30700, grad_norm=2.4521830081939697, loss=1.2868101596832275
I0215 22:08:34.167860 140046896563968 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.8071378469467163, loss=1.3041173219680786
I0215 22:09:52.180032 140046904956672 logging_writer.py:48] [30900] global_step=30900, grad_norm=2.837754011154175, loss=1.291308045387268
I0215 22:11:07.217586 140046896563968 logging_writer.py:48] [31000] global_step=31000, grad_norm=4.175926208496094, loss=1.2867788076400757
I0215 22:12:22.137281 140046904956672 logging_writer.py:48] [31100] global_step=31100, grad_norm=4.039034366607666, loss=1.2643910646438599
I0215 22:13:37.137964 140046896563968 logging_writer.py:48] [31200] global_step=31200, grad_norm=2.257892608642578, loss=1.2445257902145386
I0215 22:14:52.304038 140046904956672 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.654721736907959, loss=1.2928714752197266
I0215 22:16:07.423324 140046896563968 logging_writer.py:48] [31400] global_step=31400, grad_norm=2.8215296268463135, loss=1.2333393096923828
I0215 22:17:22.382368 140046904956672 logging_writer.py:48] [31500] global_step=31500, grad_norm=3.3805744647979736, loss=1.266756296157837
I0215 22:18:37.183964 140046896563968 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.917822003364563, loss=1.2899715900421143
I0215 22:19:52.276316 140046904956672 logging_writer.py:48] [31700] global_step=31700, grad_norm=3.527186393737793, loss=1.3523083925247192
I0215 22:21:07.207985 140046896563968 logging_writer.py:48] [31800] global_step=31800, grad_norm=2.8880982398986816, loss=1.240188479423523
I0215 22:22:22.427041 140046904956672 logging_writer.py:48] [31900] global_step=31900, grad_norm=2.7788994312286377, loss=1.2711704969406128
I0215 22:23:41.409148 140046904956672 logging_writer.py:48] [32000] global_step=32000, grad_norm=3.0855743885040283, loss=1.2969794273376465
I0215 22:24:56.170772 140046896563968 logging_writer.py:48] [32100] global_step=32100, grad_norm=2.4767937660217285, loss=1.2851512432098389
I0215 22:26:11.314053 140046904956672 logging_writer.py:48] [32200] global_step=32200, grad_norm=2.0811681747436523, loss=1.3423194885253906
I0215 22:26:49.979725 140202902193984 spec.py:321] Evaluating on the training split.
I0215 22:27:41.873226 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 22:28:30.299942 140202902193984 spec.py:349] Evaluating on the test split.
I0215 22:28:54.727828 140202902193984 submission_runner.py:408] Time since start: 28483.40s, 	Step: 32253, 	{'train/ctc_loss': Array(0.28022093, dtype=float32), 'train/wer': 0.09712408920291456, 'validation/ctc_loss': Array(0.4823358, dtype=float32), 'validation/wer': 0.14037865549301487, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26502502, dtype=float32), 'test/wer': 0.08620234395628948, 'test/num_examples': 2472, 'score': 25940.395836114883, 'total_duration': 28483.39506316185, 'accumulated_submission_time': 25940.395836114883, 'accumulated_eval_time': 2540.7003180980682, 'accumulated_logging_time': 0.8646657466888428}
I0215 22:28:54.765238 140047119996672 logging_writer.py:48] [32253] accumulated_eval_time=2540.700318, accumulated_logging_time=0.864666, accumulated_submission_time=25940.395836, global_step=32253, preemption_count=0, score=25940.395836, test/ctc_loss=0.2650250196456909, test/num_examples=2472, test/wer=0.086202, total_duration=28483.395063, train/ctc_loss=0.2802209258079529, train/wer=0.097124, validation/ctc_loss=0.48233580589294434, validation/num_examples=5348, validation/wer=0.140379
I0215 22:29:30.827887 140047111603968 logging_writer.py:48] [32300] global_step=32300, grad_norm=2.1855082511901855, loss=1.2570115327835083
I0215 22:30:45.879882 140047119996672 logging_writer.py:48] [32400] global_step=32400, grad_norm=2.7525839805603027, loss=1.2399474382400513
I0215 22:32:00.759862 140047111603968 logging_writer.py:48] [32500] global_step=32500, grad_norm=4.726807594299316, loss=1.216776728630066
I0215 22:33:15.752060 140047119996672 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.8681790828704834, loss=1.2875369787216187
I0215 22:34:30.936973 140047111603968 logging_writer.py:48] [32700] global_step=32700, grad_norm=3.06083607673645, loss=1.211827039718628
I0215 22:35:46.089993 140047119996672 logging_writer.py:48] [32800] global_step=32800, grad_norm=2.3190784454345703, loss=1.2627685070037842
I0215 22:37:00.988641 140047111603968 logging_writer.py:48] [32900] global_step=32900, grad_norm=3.0106630325317383, loss=1.3419270515441895
I0215 22:38:19.381654 140047119996672 logging_writer.py:48] [33000] global_step=33000, grad_norm=2.859234094619751, loss=1.1953688859939575
I0215 22:39:34.463142 140047111603968 logging_writer.py:48] [33100] global_step=33100, grad_norm=2.005718946456909, loss=1.2080836296081543
I0215 22:40:49.410835 140047119996672 logging_writer.py:48] [33200] global_step=33200, grad_norm=2.950727939605713, loss=1.2526530027389526
I0215 22:42:04.594440 140047111603968 logging_writer.py:48] [33300] global_step=33300, grad_norm=3.0995898246765137, loss=1.2899998426437378
I0215 22:43:19.707080 140047119996672 logging_writer.py:48] [33400] global_step=33400, grad_norm=2.3016302585601807, loss=1.2364437580108643
I0215 22:44:34.986947 140047111603968 logging_writer.py:48] [33500] global_step=33500, grad_norm=2.263172149658203, loss=1.2415683269500732
I0215 22:45:50.233971 140047119996672 logging_writer.py:48] [33600] global_step=33600, grad_norm=2.966444730758667, loss=1.2563726902008057
I0215 22:47:05.335836 140047111603968 logging_writer.py:48] [33700] global_step=33700, grad_norm=2.219557523727417, loss=1.20900559425354
I0215 22:48:20.542411 140047119996672 logging_writer.py:48] [33800] global_step=33800, grad_norm=2.527742862701416, loss=1.2340668439865112
I0215 22:49:35.877843 140047111603968 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.5970271825790405, loss=1.1954854726791382
I0215 22:50:53.944572 140047119996672 logging_writer.py:48] [34000] global_step=34000, grad_norm=2.3635473251342773, loss=1.2317084074020386
I0215 22:52:09.253394 140047111603968 logging_writer.py:48] [34100] global_step=34100, grad_norm=2.364716053009033, loss=1.1903796195983887
I0215 22:52:54.764378 140202902193984 spec.py:321] Evaluating on the training split.
I0215 22:53:46.410809 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 22:54:34.981820 140202902193984 spec.py:349] Evaluating on the test split.
I0215 22:54:59.557281 140202902193984 submission_runner.py:408] Time since start: 30048.22s, 	Step: 34162, 	{'train/ctc_loss': Array(0.26070222, dtype=float32), 'train/wer': 0.0920251056153354, 'validation/ctc_loss': Array(0.46313033, dtype=float32), 'validation/wer': 0.13380383675912605, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2560349, dtype=float32), 'test/wer': 0.08435399020981862, 'test/num_examples': 2472, 'score': 27380.309321403503, 'total_duration': 30048.223422050476, 'accumulated_submission_time': 27380.309321403503, 'accumulated_eval_time': 2665.4868993759155, 'accumulated_logging_time': 0.9170644283294678}
I0215 22:54:59.593918 140047119996672 logging_writer.py:48] [34162] accumulated_eval_time=2665.486899, accumulated_logging_time=0.917064, accumulated_submission_time=27380.309321, global_step=34162, preemption_count=0, score=27380.309321, test/ctc_loss=0.2560349106788635, test/num_examples=2472, test/wer=0.084354, total_duration=30048.223422, train/ctc_loss=0.2607022225856781, train/wer=0.092025, validation/ctc_loss=0.46313032507896423, validation/num_examples=5348, validation/wer=0.133804
I0215 22:55:28.912353 140047111603968 logging_writer.py:48] [34200] global_step=34200, grad_norm=3.132460832595825, loss=1.2307460308074951
I0215 22:56:43.972959 140047119996672 logging_writer.py:48] [34300] global_step=34300, grad_norm=2.453005790710449, loss=1.2158246040344238
I0215 22:57:58.948493 140047111603968 logging_writer.py:48] [34400] global_step=34400, grad_norm=2.7861905097961426, loss=1.1900631189346313
I0215 22:59:13.730420 140047119996672 logging_writer.py:48] [34500] global_step=34500, grad_norm=5.534140110015869, loss=1.2294222116470337
I0215 23:00:28.908403 140047111603968 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.9405783414840698, loss=1.284696102142334
I0215 23:01:43.985291 140047119996672 logging_writer.py:48] [34700] global_step=34700, grad_norm=2.245431661605835, loss=1.2579727172851562
I0215 23:02:59.180799 140047111603968 logging_writer.py:48] [34800] global_step=34800, grad_norm=3.0116894245147705, loss=1.258013129234314
I0215 23:04:14.449541 140047119996672 logging_writer.py:48] [34900] global_step=34900, grad_norm=2.4270384311676025, loss=1.2148033380508423
I0215 23:05:29.508913 140047111603968 logging_writer.py:48] [35000] global_step=35000, grad_norm=2.501035690307617, loss=1.2955846786499023
I0215 23:06:47.649060 140047119996672 logging_writer.py:48] [35100] global_step=35100, grad_norm=2.5326313972473145, loss=1.2065683603286743
I0215 23:08:02.810335 140047111603968 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.7373510599136353, loss=1.2005481719970703
I0215 23:09:17.788705 140047119996672 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.8762089014053345, loss=1.2267889976501465
I0215 23:10:32.891064 140047111603968 logging_writer.py:48] [35400] global_step=35400, grad_norm=4.2399749755859375, loss=1.2230247259140015
I0215 23:11:48.208926 140047119996672 logging_writer.py:48] [35500] global_step=35500, grad_norm=2.8556008338928223, loss=1.2169137001037598
I0215 23:13:03.367923 140047111603968 logging_writer.py:48] [35600] global_step=35600, grad_norm=2.355116605758667, loss=1.2590688467025757
I0215 23:14:18.559333 140047119996672 logging_writer.py:48] [35700] global_step=35700, grad_norm=2.1675846576690674, loss=1.1682718992233276
I0215 23:15:33.665568 140047111603968 logging_writer.py:48] [35800] global_step=35800, grad_norm=2.2788381576538086, loss=1.1829593181610107
I0215 23:16:49.160100 140047119996672 logging_writer.py:48] [35900] global_step=35900, grad_norm=2.6433634757995605, loss=1.1465567350387573
I0215 23:18:04.240255 140047111603968 logging_writer.py:48] [36000] global_step=36000, grad_norm=2.7899086475372314, loss=1.1729416847229004
I0215 23:19:00.213345 140202902193984 spec.py:321] Evaluating on the training split.
I0215 23:19:53.452254 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 23:20:43.150121 140202902193984 spec.py:349] Evaluating on the test split.
I0215 23:21:08.197674 140202902193984 submission_runner.py:408] Time since start: 31616.86s, 	Step: 36072, 	{'train/ctc_loss': Array(0.24969074, dtype=float32), 'train/wer': 0.08759570029776484, 'validation/ctc_loss': Array(0.45965692, dtype=float32), 'validation/wer': 0.13327283084082372, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25247654, dtype=float32), 'test/wer': 0.08262750594113705, 'test/num_examples': 2472, 'score': 28820.84319806099, 'total_duration': 31616.8638048172, 'accumulated_submission_time': 28820.84319806099, 'accumulated_eval_time': 2793.4651124477386, 'accumulated_logging_time': 0.9679844379425049}
I0215 23:21:08.236444 140047119996672 logging_writer.py:48] [36072] accumulated_eval_time=2793.465112, accumulated_logging_time=0.967984, accumulated_submission_time=28820.843198, global_step=36072, preemption_count=0, score=28820.843198, test/ctc_loss=0.2524765431880951, test/num_examples=2472, test/wer=0.082628, total_duration=31616.863805, train/ctc_loss=0.24969074130058289, train/wer=0.087596, validation/ctc_loss=0.4596569240093231, validation/num_examples=5348, validation/wer=0.133273
I0215 23:21:30.048727 140047111603968 logging_writer.py:48] [36100] global_step=36100, grad_norm=2.2243378162384033, loss=1.2278488874435425
I0215 23:22:45.029307 140047119996672 logging_writer.py:48] [36200] global_step=36200, grad_norm=2.2981104850769043, loss=1.2108367681503296
I0215 23:23:59.986503 140047111603968 logging_writer.py:48] [36300] global_step=36300, grad_norm=2.3975746631622314, loss=1.232969045639038
I0215 23:25:15.297827 140047119996672 logging_writer.py:48] [36400] global_step=36400, grad_norm=3.283094644546509, loss=1.1570194959640503
I0215 23:26:30.382659 140047111603968 logging_writer.py:48] [36500] global_step=36500, grad_norm=2.844888687133789, loss=1.190559983253479
I0215 23:27:45.436469 140047119996672 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.9408918619155884, loss=1.2316155433654785
I0215 23:29:00.601124 140047111603968 logging_writer.py:48] [36700] global_step=36700, grad_norm=4.1456170082092285, loss=1.2311574220657349
I0215 23:30:16.018742 140047119996672 logging_writer.py:48] [36800] global_step=36800, grad_norm=3.2910306453704834, loss=1.1292058229446411
I0215 23:31:31.436022 140047111603968 logging_writer.py:48] [36900] global_step=36900, grad_norm=2.293945550918579, loss=1.198084831237793
I0215 23:32:46.595786 140047119996672 logging_writer.py:48] [37000] global_step=37000, grad_norm=2.054185628890991, loss=1.1882514953613281
I0215 23:34:05.067275 140046500476672 logging_writer.py:48] [37100] global_step=37100, grad_norm=2.339925765991211, loss=1.1891142129898071
I0215 23:35:20.057134 140046492083968 logging_writer.py:48] [37200] global_step=37200, grad_norm=2.63222336769104, loss=1.2376760244369507
I0215 23:36:35.225031 140046500476672 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.6614785194396973, loss=1.195447564125061
I0215 23:37:50.523849 140046492083968 logging_writer.py:48] [37400] global_step=37400, grad_norm=3.5445621013641357, loss=1.2525224685668945
I0215 23:39:05.769002 140046500476672 logging_writer.py:48] [37500] global_step=37500, grad_norm=3.6612093448638916, loss=1.1885558366775513
I0215 23:40:20.894768 140046492083968 logging_writer.py:48] [37600] global_step=37600, grad_norm=2.5233657360076904, loss=1.2306967973709106
I0215 23:41:35.793627 140046500476672 logging_writer.py:48] [37700] global_step=37700, grad_norm=5.277288436889648, loss=1.208293080329895
I0215 23:42:50.832949 140046492083968 logging_writer.py:48] [37800] global_step=37800, grad_norm=3.0220253467559814, loss=1.1957765817642212
I0215 23:44:05.794581 140046500476672 logging_writer.py:48] [37900] global_step=37900, grad_norm=2.662263870239258, loss=1.2139040231704712
I0215 23:45:08.825584 140202902193984 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0215 23:46:21.632996 140202902193984 spec.py:333] Evaluating on the validation split.
I0215 23:47:10.727463 140202902193984 spec.py:349] Evaluating on the test split.
I0215 23:47:35.596898 140202902193984 submission_runner.py:408] Time since start: 33204.27s, 	Step: 37985, 	{'train/ctc_loss': Array(0.1514019, dtype=float32), 'train/wer': 0.05364202516578813, 'validation/ctc_loss': Array(0.44699508, dtype=float32), 'validation/wer': 0.12868687063730364, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24373694, dtype=float32), 'test/wer': 0.07836207421851198, 'test/num_examples': 2472, 'score': 30261.34621310234, 'total_duration': 33204.265234947205, 'accumulated_submission_time': 30261.34621310234, 'accumulated_eval_time': 2940.232330560684, 'accumulated_logging_time': 1.021756649017334}
I0215 23:47:35.636286 140046024312576 logging_writer.py:48] [37985] accumulated_eval_time=2940.232331, accumulated_logging_time=1.021757, accumulated_submission_time=30261.346213, global_step=37985, preemption_count=0, score=30261.346213, test/ctc_loss=0.24373693764209747, test/num_examples=2472, test/wer=0.078362, total_duration=33204.265235, train/ctc_loss=0.15140190720558167, train/wer=0.053642, validation/ctc_loss=0.4469950795173645, validation/num_examples=5348, validation/wer=0.128687
I0215 23:47:47.687455 140045652666112 logging_writer.py:48] [38000] global_step=38000, grad_norm=2.3050589561462402, loss=1.118037223815918
I0215 23:49:02.635141 140046024312576 logging_writer.py:48] [38100] global_step=38100, grad_norm=2.3225550651550293, loss=1.2252739667892456
I0215 23:50:20.759809 140046024312576 logging_writer.py:48] [38200] global_step=38200, grad_norm=5.739749431610107, loss=1.1631077527999878
I0215 23:51:35.547133 140045980346112 logging_writer.py:48] [38300] global_step=38300, grad_norm=2.8565828800201416, loss=1.1999034881591797
I0215 23:52:50.655668 140046024312576 logging_writer.py:48] [38400] global_step=38400, grad_norm=2.056983709335327, loss=1.1799440383911133
I0215 23:54:05.826663 140045980346112 logging_writer.py:48] [38500] global_step=38500, grad_norm=3.4298830032348633, loss=1.1849383115768433
I0215 23:55:20.860943 140046024312576 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.5564888715744019, loss=1.1450849771499634
I0215 23:56:36.088999 140045980346112 logging_writer.py:48] [38700] global_step=38700, grad_norm=2.564763307571411, loss=1.1940603256225586
I0215 23:57:50.892399 140046024312576 logging_writer.py:48] [38800] global_step=38800, grad_norm=3.0948877334594727, loss=1.1762934923171997
I0215 23:59:05.686569 140045980346112 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.6955366134643555, loss=1.1952205896377563
I0216 00:00:20.856967 140046024312576 logging_writer.py:48] [39000] global_step=39000, grad_norm=3.3332767486572266, loss=1.175689697265625
I0216 00:01:35.895695 140045980346112 logging_writer.py:48] [39100] global_step=39100, grad_norm=3.3658607006073, loss=1.134667158126831
I0216 00:02:54.325082 140046024312576 logging_writer.py:48] [39200] global_step=39200, grad_norm=4.5274152755737305, loss=1.1869810819625854
I0216 00:04:09.456398 140045324986112 logging_writer.py:48] [39300] global_step=39300, grad_norm=3.0289759635925293, loss=1.1993399858474731
I0216 00:05:24.483626 140046024312576 logging_writer.py:48] [39400] global_step=39400, grad_norm=2.1680290699005127, loss=1.1539851427078247
I0216 00:06:39.537677 140045324986112 logging_writer.py:48] [39500] global_step=39500, grad_norm=2.2040913105010986, loss=1.1946513652801514
I0216 00:07:54.589265 140046024312576 logging_writer.py:48] [39600] global_step=39600, grad_norm=2.425205945968628, loss=1.1734172105789185
I0216 00:09:09.534704 140045324986112 logging_writer.py:48] [39700] global_step=39700, grad_norm=2.3159561157226562, loss=1.2053115367889404
I0216 00:10:24.476764 140046024312576 logging_writer.py:48] [39800] global_step=39800, grad_norm=2.4744555950164795, loss=1.1784783601760864
I0216 00:11:36.147598 140202902193984 spec.py:321] Evaluating on the training split.
I0216 00:12:31.006650 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 00:13:20.788527 140202902193984 spec.py:349] Evaluating on the test split.
I0216 00:13:45.628815 140202902193984 submission_runner.py:408] Time since start: 34774.30s, 	Step: 39897, 	{'train/ctc_loss': Array(0.14774501, dtype=float32), 'train/wer': 0.05148758656158925, 'validation/ctc_loss': Array(0.44163772, dtype=float32), 'validation/wer': 0.1280593181884009, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24011387, dtype=float32), 'test/wer': 0.07785428472772328, 'test/num_examples': 2472, 'score': 31701.76964020729, 'total_duration': 34774.295803546906, 'accumulated_submission_time': 31701.76964020729, 'accumulated_eval_time': 3069.7080800533295, 'accumulated_logging_time': 1.0760719776153564}
I0216 00:13:45.667592 140046024312576 logging_writer.py:48] [39897] accumulated_eval_time=3069.708080, accumulated_logging_time=1.076072, accumulated_submission_time=31701.769640, global_step=39897, preemption_count=0, score=31701.769640, test/ctc_loss=0.24011386930942535, test/num_examples=2472, test/wer=0.077854, total_duration=34774.295804, train/ctc_loss=0.1477450132369995, train/wer=0.051488, validation/ctc_loss=0.44163772463798523, validation/num_examples=5348, validation/wer=0.128059
I0216 00:13:48.817070 140045324986112 logging_writer.py:48] [39900] global_step=39900, grad_norm=6.514328956604004, loss=1.22604238986969
I0216 00:15:03.974188 140046024312576 logging_writer.py:48] [40000] global_step=40000, grad_norm=3.159142017364502, loss=1.130580186843872
I0216 00:16:18.877192 140045324986112 logging_writer.py:48] [40100] global_step=40100, grad_norm=3.138280153274536, loss=1.1633516550064087
I0216 00:17:37.241044 140046024312576 logging_writer.py:48] [40200] global_step=40200, grad_norm=2.5893404483795166, loss=1.149422287940979
I0216 00:18:52.163844 140045324986112 logging_writer.py:48] [40300] global_step=40300, grad_norm=4.049192428588867, loss=1.179335355758667
I0216 00:20:07.142819 140046024312576 logging_writer.py:48] [40400] global_step=40400, grad_norm=2.7345380783081055, loss=1.1499569416046143
I0216 00:21:22.172789 140045324986112 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.8427908420562744, loss=1.1846028566360474
I0216 00:22:37.113864 140046024312576 logging_writer.py:48] [40600] global_step=40600, grad_norm=2.6749606132507324, loss=1.1553795337677002
I0216 00:23:52.302283 140045324986112 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.9128446578979492, loss=1.1836568117141724
I0216 00:25:07.122306 140046024312576 logging_writer.py:48] [40800] global_step=40800, grad_norm=3.1919405460357666, loss=1.198094129562378
I0216 00:26:22.353576 140045324986112 logging_writer.py:48] [40900] global_step=40900, grad_norm=3.314460515975952, loss=1.1516427993774414
I0216 00:27:39.308421 140046024312576 logging_writer.py:48] [41000] global_step=41000, grad_norm=3.259316921234131, loss=1.172200322151184
I0216 00:28:58.705980 140045324986112 logging_writer.py:48] [41100] global_step=41100, grad_norm=2.982330560684204, loss=1.1738502979278564
I0216 00:30:20.644365 140046024312576 logging_writer.py:48] [41200] global_step=41200, grad_norm=2.8078768253326416, loss=1.1263304948806763
I0216 00:31:35.598242 140045980346112 logging_writer.py:48] [41300] global_step=41300, grad_norm=3.109694719314575, loss=1.1804190874099731
I0216 00:32:50.308886 140046024312576 logging_writer.py:48] [41400] global_step=41400, grad_norm=2.165984630584717, loss=1.1427525281906128
I0216 00:34:05.329462 140045980346112 logging_writer.py:48] [41500] global_step=41500, grad_norm=2.7069883346557617, loss=1.1139624118804932
I0216 00:35:20.361473 140046024312576 logging_writer.py:48] [41600] global_step=41600, grad_norm=2.15555739402771, loss=1.134844183921814
I0216 00:36:35.364098 140045980346112 logging_writer.py:48] [41700] global_step=41700, grad_norm=3.8126728534698486, loss=1.1261000633239746
I0216 00:37:46.354949 140202902193984 spec.py:321] Evaluating on the training split.
I0216 00:38:41.741368 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 00:39:31.482815 140202902193984 spec.py:349] Evaluating on the test split.
I0216 00:39:56.599265 140202902193984 submission_runner.py:408] Time since start: 36345.27s, 	Step: 41796, 	{'train/ctc_loss': Array(0.15505517, dtype=float32), 'train/wer': 0.05396548534580724, 'validation/ctc_loss': Array(0.4351663, dtype=float32), 'validation/wer': 0.12635044459677341, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23733485, dtype=float32), 'test/wer': 0.07614811203867325, 'test/num_examples': 2472, 'score': 33142.368851184845, 'total_duration': 36345.265635252, 'accumulated_submission_time': 33142.368851184845, 'accumulated_eval_time': 3199.9463255405426, 'accumulated_logging_time': 1.1304571628570557}
I0216 00:39:56.635277 140046024312576 logging_writer.py:48] [41796] accumulated_eval_time=3199.946326, accumulated_logging_time=1.130457, accumulated_submission_time=33142.368851, global_step=41796, preemption_count=0, score=33142.368851, test/ctc_loss=0.23733484745025635, test/num_examples=2472, test/wer=0.076148, total_duration=36345.265635, train/ctc_loss=0.15505516529083252, train/wer=0.053965, validation/ctc_loss=0.43516629934310913, validation/num_examples=5348, validation/wer=0.126350
I0216 00:40:00.479261 140045980346112 logging_writer.py:48] [41800] global_step=41800, grad_norm=4.009940147399902, loss=1.1592328548431396
I0216 00:41:15.564585 140046024312576 logging_writer.py:48] [41900] global_step=41900, grad_norm=3.343487501144409, loss=1.1530450582504272
I0216 00:42:30.428746 140045980346112 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.9433928728103638, loss=1.137391448020935
I0216 00:43:45.451002 140046024312576 logging_writer.py:48] [42100] global_step=42100, grad_norm=2.737844944000244, loss=1.2067104578018188
I0216 00:45:00.594265 140045980346112 logging_writer.py:48] [42200] global_step=42200, grad_norm=3.1844210624694824, loss=1.1597614288330078
I0216 00:46:19.014304 140046024312576 logging_writer.py:48] [42300] global_step=42300, grad_norm=2.885763168334961, loss=1.2091389894485474
I0216 00:47:33.851379 140045980346112 logging_writer.py:48] [42400] global_step=42400, grad_norm=2.7453627586364746, loss=1.135435938835144
I0216 00:48:48.768534 140046024312576 logging_writer.py:48] [42500] global_step=42500, grad_norm=3.6493778228759766, loss=1.1555885076522827
I0216 00:50:03.667977 140045980346112 logging_writer.py:48] [42600] global_step=42600, grad_norm=2.105537176132202, loss=1.1206294298171997
I0216 00:51:18.502274 140046024312576 logging_writer.py:48] [42700] global_step=42700, grad_norm=3.099323272705078, loss=1.1474547386169434
I0216 00:52:33.383990 140045980346112 logging_writer.py:48] [42800] global_step=42800, grad_norm=2.865938901901245, loss=1.096953272819519
I0216 00:53:48.348741 140046024312576 logging_writer.py:48] [42900] global_step=42900, grad_norm=3.303873062133789, loss=1.127216100692749
I0216 00:55:03.334411 140045980346112 logging_writer.py:48] [43000] global_step=43000, grad_norm=3.9700276851654053, loss=1.1712452173233032
I0216 00:56:20.996407 140046024312576 logging_writer.py:48] [43100] global_step=43100, grad_norm=2.2932288646698, loss=1.1467636823654175
I0216 00:57:40.389093 140045980346112 logging_writer.py:48] [43200] global_step=43200, grad_norm=3.931262493133545, loss=1.1537748575210571
I0216 00:59:00.524592 140046024312576 logging_writer.py:48] [43300] global_step=43300, grad_norm=2.6131718158721924, loss=1.20035982131958
I0216 01:00:15.668900 140045980346112 logging_writer.py:48] [43400] global_step=43400, grad_norm=2.4896087646484375, loss=1.1353007555007935
I0216 01:01:30.603948 140046024312576 logging_writer.py:48] [43500] global_step=43500, grad_norm=5.1919965744018555, loss=1.1502546072006226
I0216 01:02:45.447995 140045980346112 logging_writer.py:48] [43600] global_step=43600, grad_norm=4.249560356140137, loss=1.1818890571594238
I0216 01:03:57.256917 140202902193984 spec.py:321] Evaluating on the training split.
I0216 01:04:51.331406 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 01:05:40.070563 140202902193984 spec.py:349] Evaluating on the test split.
I0216 01:06:04.826340 140202902193984 submission_runner.py:408] Time since start: 37913.49s, 	Step: 43697, 	{'train/ctc_loss': Array(0.1378165, dtype=float32), 'train/wer': 0.049158944820101844, 'validation/ctc_loss': Array(0.43216133, dtype=float32), 'validation/wer': 0.12456433378066559, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23436242, dtype=float32), 'test/wer': 0.07531533727377979, 'test/num_examples': 2472, 'score': 34582.90179729462, 'total_duration': 37913.49289941788, 'accumulated_submission_time': 34582.90179729462, 'accumulated_eval_time': 3327.5098538398743, 'accumulated_logging_time': 1.182192325592041}
I0216 01:06:04.864807 140046024312576 logging_writer.py:48] [43697] accumulated_eval_time=3327.509854, accumulated_logging_time=1.182192, accumulated_submission_time=34582.901797, global_step=43697, preemption_count=0, score=34582.901797, test/ctc_loss=0.2343624234199524, test/num_examples=2472, test/wer=0.075315, total_duration=37913.492899, train/ctc_loss=0.13781650364398956, train/wer=0.049159, validation/ctc_loss=0.4321613311767578, validation/num_examples=5348, validation/wer=0.124564
I0216 01:06:08.027965 140045980346112 logging_writer.py:48] [43700] global_step=43700, grad_norm=3.624972105026245, loss=1.155535340309143
I0216 01:07:23.516531 140046024312576 logging_writer.py:48] [43800] global_step=43800, grad_norm=3.201735496520996, loss=1.1479343175888062
I0216 01:08:38.922065 140045980346112 logging_writer.py:48] [43900] global_step=43900, grad_norm=4.270748615264893, loss=1.2532888650894165
I0216 01:09:54.018278 140046024312576 logging_writer.py:48] [44000] global_step=44000, grad_norm=2.1557419300079346, loss=1.2474654912948608
I0216 01:11:09.265774 140045980346112 logging_writer.py:48] [44100] global_step=44100, grad_norm=3.606245756149292, loss=1.1773664951324463
I0216 01:12:24.276906 140046024312576 logging_writer.py:48] [44200] global_step=44200, grad_norm=2.771294355392456, loss=1.1891120672225952
I0216 01:13:42.640677 140046024312576 logging_writer.py:48] [44300] global_step=44300, grad_norm=6.669731140136719, loss=1.1147912740707397
I0216 01:14:57.594114 140045980346112 logging_writer.py:48] [44400] global_step=44400, grad_norm=2.4659082889556885, loss=1.1176843643188477
I0216 01:16:12.439278 140046024312576 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.6624515056610107, loss=1.1518524885177612
I0216 01:17:27.283810 140045980346112 logging_writer.py:48] [44600] global_step=44600, grad_norm=2.6915037631988525, loss=1.1723895072937012
I0216 01:18:42.193989 140046024312576 logging_writer.py:48] [44700] global_step=44700, grad_norm=2.6458890438079834, loss=1.1835203170776367
I0216 01:19:57.392383 140045980346112 logging_writer.py:48] [44800] global_step=44800, grad_norm=9.537492752075195, loss=1.153665542602539
I0216 01:21:12.318794 140046024312576 logging_writer.py:48] [44900] global_step=44900, grad_norm=2.6133193969726562, loss=1.1927121877670288
I0216 01:22:27.353874 140045980346112 logging_writer.py:48] [45000] global_step=45000, grad_norm=4.722851753234863, loss=1.1477500200271606
I0216 01:23:42.512511 140046024312576 logging_writer.py:48] [45100] global_step=45100, grad_norm=4.555037498474121, loss=1.1447436809539795
I0216 01:25:01.426912 140045980346112 logging_writer.py:48] [45200] global_step=45200, grad_norm=2.229078769683838, loss=1.1308850049972534
I0216 01:26:20.107069 140046024312576 logging_writer.py:48] [45300] global_step=45300, grad_norm=8.3121919631958, loss=1.1241075992584229
I0216 01:27:39.179941 140046024312576 logging_writer.py:48] [45400] global_step=45400, grad_norm=2.2727293968200684, loss=1.1752322912216187
I0216 01:28:54.216190 140045980346112 logging_writer.py:48] [45500] global_step=45500, grad_norm=4.490700721740723, loss=1.1555967330932617
I0216 01:30:05.186030 140202902193984 spec.py:321] Evaluating on the training split.
I0216 01:30:58.854915 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 01:31:48.655421 140202902193984 spec.py:349] Evaluating on the test split.
I0216 01:32:13.674068 140202902193984 submission_runner.py:408] Time since start: 39482.34s, 	Step: 45596, 	{'train/ctc_loss': Array(0.14721335, dtype=float32), 'train/wer': 0.052225557255652526, 'validation/ctc_loss': Array(0.43008903, dtype=float32), 'validation/wer': 0.12445813259700512, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2329999, dtype=float32), 'test/wer': 0.07484817094225418, 'test/num_examples': 2472, 'score': 36023.13359427452, 'total_duration': 39482.34113240242, 'accumulated_submission_time': 36023.13359427452, 'accumulated_eval_time': 3455.9925067424774, 'accumulated_logging_time': 1.236759901046753}
I0216 01:32:13.710012 140046024312576 logging_writer.py:48] [45596] accumulated_eval_time=3455.992507, accumulated_logging_time=1.236760, accumulated_submission_time=36023.133594, global_step=45596, preemption_count=0, score=36023.133594, test/ctc_loss=0.23299990594387054, test/num_examples=2472, test/wer=0.074848, total_duration=39482.341132, train/ctc_loss=0.14721335470676422, train/wer=0.052226, validation/ctc_loss=0.4300890266895294, validation/num_examples=5348, validation/wer=0.124458
I0216 01:32:17.567328 140045980346112 logging_writer.py:48] [45600] global_step=45600, grad_norm=3.325438976287842, loss=1.129659652709961
I0216 01:33:32.505210 140046024312576 logging_writer.py:48] [45700] global_step=45700, grad_norm=3.9282524585723877, loss=1.142200231552124
I0216 01:34:47.462000 140045980346112 logging_writer.py:48] [45800] global_step=45800, grad_norm=3.4797401428222656, loss=1.142011046409607
I0216 01:36:02.508672 140046024312576 logging_writer.py:48] [45900] global_step=45900, grad_norm=2.003378391265869, loss=1.1412158012390137
I0216 01:37:17.641251 140045980346112 logging_writer.py:48] [46000] global_step=46000, grad_norm=2.305521249771118, loss=1.1263829469680786
I0216 01:38:32.668287 140046024312576 logging_writer.py:48] [46100] global_step=46100, grad_norm=2.77302885055542, loss=1.1370052099227905
I0216 01:39:47.877268 140045980346112 logging_writer.py:48] [46200] global_step=46200, grad_norm=2.337005615234375, loss=1.171532392501831
I0216 01:41:03.730560 140046024312576 logging_writer.py:48] [46300] global_step=46300, grad_norm=2.082813262939453, loss=1.166664958000183
I0216 01:42:24.276780 140046024312576 logging_writer.py:48] [46400] global_step=46400, grad_norm=2.4563729763031006, loss=1.1307162046432495
I0216 01:43:39.520771 140045980346112 logging_writer.py:48] [46500] global_step=46500, grad_norm=4.046014785766602, loss=1.1015985012054443
I0216 01:44:54.495175 140046024312576 logging_writer.py:48] [46600] global_step=46600, grad_norm=3.4159393310546875, loss=1.1552574634552002
I0216 01:46:09.650290 140045980346112 logging_writer.py:48] [46700] global_step=46700, grad_norm=2.6121456623077393, loss=1.1507844924926758
I0216 01:47:24.710511 140046024312576 logging_writer.py:48] [46800] global_step=46800, grad_norm=2.3868820667266846, loss=1.1726423501968384
I0216 01:48:39.968919 140045980346112 logging_writer.py:48] [46900] global_step=46900, grad_norm=2.099468231201172, loss=1.153641700744629
I0216 01:49:54.958900 140046024312576 logging_writer.py:48] [47000] global_step=47000, grad_norm=4.585788249969482, loss=1.1128811836242676
I0216 01:51:09.915532 140045980346112 logging_writer.py:48] [47100] global_step=47100, grad_norm=2.1282973289489746, loss=1.1655640602111816
I0216 01:52:27.021543 140046024312576 logging_writer.py:48] [47200] global_step=47200, grad_norm=6.659487724304199, loss=1.141053318977356
I0216 01:53:45.714904 140045980346112 logging_writer.py:48] [47300] global_step=47300, grad_norm=8.536850929260254, loss=1.1570031642913818
I0216 01:55:08.504550 140046024312576 logging_writer.py:48] [47400] global_step=47400, grad_norm=4.4274210929870605, loss=1.1185431480407715
I0216 01:56:13.777579 140202902193984 spec.py:321] Evaluating on the training split.
I0216 01:57:08.327941 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 01:57:57.714563 140202902193984 spec.py:349] Evaluating on the test split.
I0216 01:58:22.739309 140202902193984 submission_runner.py:408] Time since start: 41051.41s, 	Step: 47488, 	{'train/ctc_loss': Array(0.13965625, dtype=float32), 'train/wer': 0.050840394499236004, 'validation/ctc_loss': Array(0.42927903, dtype=float32), 'validation/wer': 0.12420711161744402, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23242217, dtype=float32), 'test/wer': 0.07470598988483335, 'test/num_examples': 2472, 'score': 37463.108900785446, 'total_duration': 41051.405695438385, 'accumulated_submission_time': 37463.108900785446, 'accumulated_eval_time': 3584.9481728076935, 'accumulated_logging_time': 1.291504144668579}
I0216 01:58:22.781959 140046689916672 logging_writer.py:48] [47488] accumulated_eval_time=3584.948173, accumulated_logging_time=1.291504, accumulated_submission_time=37463.108901, global_step=47488, preemption_count=0, score=37463.108901, test/ctc_loss=0.23242217302322388, test/num_examples=2472, test/wer=0.074706, total_duration=41051.405695, train/ctc_loss=0.13965624570846558, train/wer=0.050840, validation/ctc_loss=0.42927902936935425, validation/num_examples=5348, validation/wer=0.124207
I0216 01:58:32.596788 140046681523968 logging_writer.py:48] [47500] global_step=47500, grad_norm=3.628743886947632, loss=1.1891382932662964
I0216 01:59:47.551189 140046689916672 logging_writer.py:48] [47600] global_step=47600, grad_norm=7.49681282043457, loss=1.1211062669754028
I0216 02:01:02.524075 140046681523968 logging_writer.py:48] [47700] global_step=47700, grad_norm=2.1530983448028564, loss=1.1391665935516357
I0216 02:02:17.238289 140046689916672 logging_writer.py:48] [47800] global_step=47800, grad_norm=3.3724308013916016, loss=1.1098202466964722
I0216 02:03:32.195067 140046681523968 logging_writer.py:48] [47900] global_step=47900, grad_norm=2.4894306659698486, loss=1.1409333944320679
I0216 02:04:46.222746 140202902193984 spec.py:321] Evaluating on the training split.
I0216 02:05:39.670128 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 02:06:30.007691 140202902193984 spec.py:349] Evaluating on the test split.
I0216 02:06:55.364969 140202902193984 submission_runner.py:408] Time since start: 41564.03s, 	Step: 48000, 	{'train/ctc_loss': Array(0.16727167, dtype=float32), 'train/wer': 0.05534273328182677, 'validation/ctc_loss': Array(0.4293102, dtype=float32), 'validation/wer': 0.12426503953580428, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23241976, dtype=float32), 'test/wer': 0.07482785936262264, 'test/num_examples': 2472, 'score': 37846.51352763176, 'total_duration': 41564.03453874588, 'accumulated_submission_time': 37846.51352763176, 'accumulated_eval_time': 3714.087497472763, 'accumulated_logging_time': 1.3495750427246094}
I0216 02:06:55.399427 140047119996672 logging_writer.py:48] [48000] accumulated_eval_time=3714.087497, accumulated_logging_time=1.349575, accumulated_submission_time=37846.513528, global_step=48000, preemption_count=0, score=37846.513528, test/ctc_loss=0.23241975903511047, test/num_examples=2472, test/wer=0.074828, total_duration=41564.034539, train/ctc_loss=0.1672716736793518, train/wer=0.055343, validation/ctc_loss=0.4293102025985718, validation/num_examples=5348, validation/wer=0.124265
I0216 02:06:55.421357 140047111603968 logging_writer.py:48] [48000] global_step=48000, preemption_count=0, score=37846.513528
I0216 02:06:55.622226 140202902193984 checkpoints.py:490] Saving checkpoint at step: 48000
I0216 02:06:56.617904 140202902193984 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_3/librispeech_deepspeech_jax/trial_2/checkpoint_48000
I0216 02:06:56.638639 140202902193984 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_3/librispeech_deepspeech_jax/trial_2/checkpoint_48000.
I0216 02:06:58.157658 140202902193984 submission_runner.py:583] Tuning trial 2/5
I0216 02:06:58.157959 140202902193984 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.2, learning_rate=0.0008445074561975979, one_minus_beta1=0.11042418465, beta2=0.9978504782314613, weight_decay=0.08135402759553023, warmup_factor=0.05)
I0216 02:06:58.172142 140202902193984 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.72197, dtype=float32), 'train/wer': 3.2662889210323116, 'validation/ctc_loss': Array(30.351294, dtype=float32), 'validation/wer': 3.0462071695453625, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.536596, dtype=float32), 'test/wer': 3.3615054942822904, 'test/num_examples': 2472, 'score': 15.445929288864136, 'total_duration': 205.7453055381775, 'accumulated_submission_time': 15.445929288864136, 'accumulated_eval_time': 190.29930567741394, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1783, {'train/ctc_loss': Array(6.9688888, dtype=float32), 'train/wer': 0.9123050937668022, 'validation/ctc_loss': Array(6.7451887, dtype=float32), 'validation/wer': 0.8757156511580756, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.6782165, dtype=float32), 'test/wer': 0.8730932504620884, 'test/num_examples': 2472, 'score': 1455.9250264167786, 'total_duration': 1762.5931413173676, 'accumulated_submission_time': 1455.9250264167786, 'accumulated_eval_time': 306.5587999820709, 'accumulated_logging_time': 0.03134512901306152, 'global_step': 1783, 'preemption_count': 0}), (3592, {'train/ctc_loss': Array(3.1315818, dtype=float32), 'train/wer': 0.6487877941934376, 'validation/ctc_loss': Array(2.9454577, dtype=float32), 'validation/wer': 0.6002201260897689, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.390297, dtype=float32), 'test/wer': 0.5203623585806268, 'test/num_examples': 2472, 'score': 2895.8729717731476, 'total_duration': 3327.6603693962097, 'accumulated_submission_time': 2895.8729717731476, 'accumulated_eval_time': 431.56023931503296, 'accumulated_logging_time': 0.06965899467468262, 'global_step': 3592, 'preemption_count': 0}), (5384, {'train/ctc_loss': Array(0.7923331, dtype=float32), 'train/wer': 0.24815847427288212, 'validation/ctc_loss': Array(0.9136013, dtype=float32), 'validation/wer': 0.2602025546211997, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.58996063, dtype=float32), 'test/wer': 0.1858103304694006, 'test/num_examples': 2472, 'score': 4336.168129205704, 'total_duration': 4899.088443279266, 'accumulated_submission_time': 4336.168129205704, 'accumulated_eval_time': 562.577470779419, 'accumulated_logging_time': 0.10723614692687988, 'global_step': 5384, 'preemption_count': 0}), (7156, {'train/ctc_loss': Array(0.67502964, dtype=float32), 'train/wer': 0.21462525879917185, 'validation/ctc_loss': Array(0.7972467, dtype=float32), 'validation/wer': 0.23102619307375188, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4931348, dtype=float32), 'test/wer': 0.16184266650417403, 'test/num_examples': 2472, 'score': 5776.282276391983, 'total_duration': 6471.658866643906, 'accumulated_submission_time': 5776.282276391983, 'accumulated_eval_time': 694.9200584888458, 'accumulated_logging_time': 0.1430966854095459, 'global_step': 7156, 'preemption_count': 0}), (8929, {'train/ctc_loss': Array(0.61008316, dtype=float32), 'train/wer': 0.19599258267229835, 'validation/ctc_loss': Array(0.7310794, dtype=float32), 'validation/wer': 0.21135966479044577, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4402412, dtype=float32), 'test/wer': 0.14279040480978206, 'test/num_examples': 2472, 'score': 7216.382225990295, 'total_duration': 8044.677983999252, 'accumulated_submission_time': 7216.382225990295, 'accumulated_eval_time': 827.719379901886, 'accumulated_logging_time': 0.18550825119018555, 'global_step': 8929, 'preemption_count': 0}), (10735, {'train/ctc_loss': Array(0.5634109, dtype=float32), 'train/wer': 0.18198303183436979, 'validation/ctc_loss': Array(0.69694114, dtype=float32), 'validation/wer': 0.20091333017948, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4191527, dtype=float32), 'test/wer': 0.1371844088314748, 'test/num_examples': 2472, 'score': 8656.938082933426, 'total_duration': 9620.884849786758, 'accumulated_submission_time': 8656.938082933426, 'accumulated_eval_time': 963.2478840351105, 'accumulated_logging_time': 0.22718000411987305, 'global_step': 10735, 'preemption_count': 0}), (12523, {'train/ctc_loss': Array(0.5032294, dtype=float32), 'train/wer': 0.1660165444885332, 'validation/ctc_loss': Array(0.6597988, dtype=float32), 'validation/wer': 0.19204070401730114, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39195168, dtype=float32), 'test/wer': 0.12796295167875205, 'test/num_examples': 2472, 'score': 10097.114848613739, 'total_duration': 11192.528175830841, 'accumulated_submission_time': 10097.114848613739, 'accumulated_eval_time': 1094.5947580337524, 'accumulated_logging_time': 0.2684915065765381, 'global_step': 12523, 'preemption_count': 0}), (14309, {'train/ctc_loss': Array(0.4797483, dtype=float32), 'train/wer': 0.15956026653279176, 'validation/ctc_loss': Array(0.62959296, dtype=float32), 'validation/wer': 0.18249225214091933, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3724739, dtype=float32), 'test/wer': 0.1187008713667662, 'test/num_examples': 2472, 'score': 11537.74647140503, 'total_duration': 12764.665168046951, 'accumulated_submission_time': 11537.74647140503, 'accumulated_eval_time': 1225.9796528816223, 'accumulated_logging_time': 0.30987048149108887, 'global_step': 14309, 'preemption_count': 0}), (16084, {'train/ctc_loss': Array(0.43343863, dtype=float32), 'train/wer': 0.14823907512386733, 'validation/ctc_loss': Array(0.6055314, dtype=float32), 'validation/wer': 0.17722081157013622, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35472116, dtype=float32), 'test/wer': 0.1176446692259257, 'test/num_examples': 2472, 'score': 12977.85252571106, 'total_duration': 14336.233743190765, 'accumulated_submission_time': 12977.85252571106, 'accumulated_eval_time': 1357.3227698802948, 'accumulated_logging_time': 0.35144686698913574, 'global_step': 16084, 'preemption_count': 0}), (17879, {'train/ctc_loss': Array(0.4497294, dtype=float32), 'train/wer': 0.14671168375683938, 'validation/ctc_loss': Array(0.5981819, dtype=float32), 'validation/wer': 0.17235486642787493, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3461188, dtype=float32), 'test/wer': 0.11144963743830358, 'test/num_examples': 2472, 'score': 14417.783019304276, 'total_duration': 15909.162315368652, 'accumulated_submission_time': 14417.783019304276, 'accumulated_eval_time': 1490.1781721115112, 'accumulated_logging_time': 0.41077733039855957, 'global_step': 17879, 'preemption_count': 0}), (19647, {'train/ctc_loss': Array(0.41132402, dtype=float32), 'train/wer': 0.13723418528208225, 'validation/ctc_loss': Array(0.5722585, dtype=float32), 'validation/wer': 0.16724755495911253, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32342452, dtype=float32), 'test/wer': 0.1053561635488392, 'test/num_examples': 2472, 'score': 15858.06008553505, 'total_duration': 17482.04532623291, 'accumulated_submission_time': 15858.06008553505, 'accumulated_eval_time': 1622.651347875595, 'accumulated_logging_time': 0.46189212799072266, 'global_step': 19647, 'preemption_count': 0}), (21421, {'train/ctc_loss': Array(0.4032407, dtype=float32), 'train/wer': 0.13661822166631615, 'validation/ctc_loss': Array(0.5550742, dtype=float32), 'validation/wer': 0.16248781100051168, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31684622, dtype=float32), 'test/wer': 0.10295939715231654, 'test/num_examples': 2472, 'score': 17298.850484371185, 'total_duration': 19056.292241811752, 'accumulated_submission_time': 17298.850484371185, 'accumulated_eval_time': 1755.9761242866516, 'accumulated_logging_time': 0.511904239654541, 'global_step': 21421, 'preemption_count': 0}), (23194, {'train/ctc_loss': Array(0.3486651, dtype=float32), 'train/wer': 0.11755327268718233, 'validation/ctc_loss': Array(0.53862715, dtype=float32), 'validation/wer': 0.15761221120519034, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30597436, dtype=float32), 'test/wer': 0.09910019702232242, 'test/num_examples': 2472, 'score': 18738.948315143585, 'total_duration': 20631.66925215721, 'accumulated_submission_time': 18738.948315143585, 'accumulated_eval_time': 1891.1183724403381, 'accumulated_logging_time': 0.5660281181335449, 'global_step': 23194, 'preemption_count': 0}), (24964, {'train/ctc_loss': Array(0.3170169, dtype=float32), 'train/wer': 0.10613243615231686, 'validation/ctc_loss': Array(0.5284512, dtype=float32), 'validation/wer': 0.1534027824710119, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29627094, dtype=float32), 'test/wer': 0.09595190217943249, 'test/num_examples': 2472, 'score': 20179.532305002213, 'total_duration': 22203.64442896843, 'accumulated_submission_time': 20179.532305002213, 'accumulated_eval_time': 2022.3715333938599, 'accumulated_logging_time': 0.6189842224121094, 'global_step': 24964, 'preemption_count': 0}), (26733, {'train/ctc_loss': Array(0.35497886, dtype=float32), 'train/wer': 0.12230037142824887, 'validation/ctc_loss': Array(0.5095953, dtype=float32), 'validation/wer': 0.14857545594099075, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28524765, dtype=float32), 'test/wer': 0.09485507687932891, 'test/num_examples': 2472, 'score': 21619.72815155983, 'total_duration': 23776.40490913391, 'accumulated_submission_time': 21619.72815155983, 'accumulated_eval_time': 2154.782774209976, 'accumulated_logging_time': 0.691807746887207, 'global_step': 26733, 'preemption_count': 0}), (28465, {'train/ctc_loss': Array(0.32599312, dtype=float32), 'train/wer': 0.1092377701934016, 'validation/ctc_loss': Array(0.5059581, dtype=float32), 'validation/wer': 0.14743620687990577, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2816703, dtype=float32), 'test/wer': 0.09004123250665204, 'test/num_examples': 2472, 'score': 23059.842032194138, 'total_duration': 25350.305683374405, 'accumulated_submission_time': 23059.842032194138, 'accumulated_eval_time': 2288.435446739197, 'accumulated_logging_time': 0.7465569972991943, 'global_step': 28465, 'preemption_count': 0}), (30341, {'train/ctc_loss': Array(0.3049627, dtype=float32), 'train/wer': 0.10203451023760347, 'validation/ctc_loss': Array(0.48760784, dtype=float32), 'validation/wer': 0.14285990132944573, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27179763, dtype=float32), 'test/wer': 0.0894318851177056, 'test/num_examples': 2472, 'score': 24499.839766025543, 'total_duration': 26917.968510866165, 'accumulated_submission_time': 24499.839766025543, 'accumulated_eval_time': 2415.9574434757233, 'accumulated_logging_time': 0.8124251365661621, 'global_step': 30341, 'preemption_count': 0}), (32253, {'train/ctc_loss': Array(0.28022093, dtype=float32), 'train/wer': 0.09712408920291456, 'validation/ctc_loss': Array(0.4823358, dtype=float32), 'validation/wer': 0.14037865549301487, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26502502, dtype=float32), 'test/wer': 0.08620234395628948, 'test/num_examples': 2472, 'score': 25940.395836114883, 'total_duration': 28483.39506316185, 'accumulated_submission_time': 25940.395836114883, 'accumulated_eval_time': 2540.7003180980682, 'accumulated_logging_time': 0.8646657466888428, 'global_step': 32253, 'preemption_count': 0}), (34162, {'train/ctc_loss': Array(0.26070222, dtype=float32), 'train/wer': 0.0920251056153354, 'validation/ctc_loss': Array(0.46313033, dtype=float32), 'validation/wer': 0.13380383675912605, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2560349, dtype=float32), 'test/wer': 0.08435399020981862, 'test/num_examples': 2472, 'score': 27380.309321403503, 'total_duration': 30048.223422050476, 'accumulated_submission_time': 27380.309321403503, 'accumulated_eval_time': 2665.4868993759155, 'accumulated_logging_time': 0.9170644283294678, 'global_step': 34162, 'preemption_count': 0}), (36072, {'train/ctc_loss': Array(0.24969074, dtype=float32), 'train/wer': 0.08759570029776484, 'validation/ctc_loss': Array(0.45965692, dtype=float32), 'validation/wer': 0.13327283084082372, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25247654, dtype=float32), 'test/wer': 0.08262750594113705, 'test/num_examples': 2472, 'score': 28820.84319806099, 'total_duration': 31616.8638048172, 'accumulated_submission_time': 28820.84319806099, 'accumulated_eval_time': 2793.4651124477386, 'accumulated_logging_time': 0.9679844379425049, 'global_step': 36072, 'preemption_count': 0}), (37985, {'train/ctc_loss': Array(0.1514019, dtype=float32), 'train/wer': 0.05364202516578813, 'validation/ctc_loss': Array(0.44699508, dtype=float32), 'validation/wer': 0.12868687063730364, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24373694, dtype=float32), 'test/wer': 0.07836207421851198, 'test/num_examples': 2472, 'score': 30261.34621310234, 'total_duration': 33204.265234947205, 'accumulated_submission_time': 30261.34621310234, 'accumulated_eval_time': 2940.232330560684, 'accumulated_logging_time': 1.021756649017334, 'global_step': 37985, 'preemption_count': 0}), (39897, {'train/ctc_loss': Array(0.14774501, dtype=float32), 'train/wer': 0.05148758656158925, 'validation/ctc_loss': Array(0.44163772, dtype=float32), 'validation/wer': 0.1280593181884009, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24011387, dtype=float32), 'test/wer': 0.07785428472772328, 'test/num_examples': 2472, 'score': 31701.76964020729, 'total_duration': 34774.295803546906, 'accumulated_submission_time': 31701.76964020729, 'accumulated_eval_time': 3069.7080800533295, 'accumulated_logging_time': 1.0760719776153564, 'global_step': 39897, 'preemption_count': 0}), (41796, {'train/ctc_loss': Array(0.15505517, dtype=float32), 'train/wer': 0.05396548534580724, 'validation/ctc_loss': Array(0.4351663, dtype=float32), 'validation/wer': 0.12635044459677341, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23733485, dtype=float32), 'test/wer': 0.07614811203867325, 'test/num_examples': 2472, 'score': 33142.368851184845, 'total_duration': 36345.265635252, 'accumulated_submission_time': 33142.368851184845, 'accumulated_eval_time': 3199.9463255405426, 'accumulated_logging_time': 1.1304571628570557, 'global_step': 41796, 'preemption_count': 0}), (43697, {'train/ctc_loss': Array(0.1378165, dtype=float32), 'train/wer': 0.049158944820101844, 'validation/ctc_loss': Array(0.43216133, dtype=float32), 'validation/wer': 0.12456433378066559, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23436242, dtype=float32), 'test/wer': 0.07531533727377979, 'test/num_examples': 2472, 'score': 34582.90179729462, 'total_duration': 37913.49289941788, 'accumulated_submission_time': 34582.90179729462, 'accumulated_eval_time': 3327.5098538398743, 'accumulated_logging_time': 1.182192325592041, 'global_step': 43697, 'preemption_count': 0}), (45596, {'train/ctc_loss': Array(0.14721335, dtype=float32), 'train/wer': 0.052225557255652526, 'validation/ctc_loss': Array(0.43008903, dtype=float32), 'validation/wer': 0.12445813259700512, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2329999, dtype=float32), 'test/wer': 0.07484817094225418, 'test/num_examples': 2472, 'score': 36023.13359427452, 'total_duration': 39482.34113240242, 'accumulated_submission_time': 36023.13359427452, 'accumulated_eval_time': 3455.9925067424774, 'accumulated_logging_time': 1.236759901046753, 'global_step': 45596, 'preemption_count': 0}), (47488, {'train/ctc_loss': Array(0.13965625, dtype=float32), 'train/wer': 0.050840394499236004, 'validation/ctc_loss': Array(0.42927903, dtype=float32), 'validation/wer': 0.12420711161744402, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23242217, dtype=float32), 'test/wer': 0.07470598988483335, 'test/num_examples': 2472, 'score': 37463.108900785446, 'total_duration': 41051.405695438385, 'accumulated_submission_time': 37463.108900785446, 'accumulated_eval_time': 3584.9481728076935, 'accumulated_logging_time': 1.291504144668579, 'global_step': 47488, 'preemption_count': 0}), (48000, {'train/ctc_loss': Array(0.16727167, dtype=float32), 'train/wer': 0.05534273328182677, 'validation/ctc_loss': Array(0.4293102, dtype=float32), 'validation/wer': 0.12426503953580428, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23241976, dtype=float32), 'test/wer': 0.07482785936262264, 'test/num_examples': 2472, 'score': 37846.51352763176, 'total_duration': 41564.03453874588, 'accumulated_submission_time': 37846.51352763176, 'accumulated_eval_time': 3714.087497472763, 'accumulated_logging_time': 1.3495750427246094, 'global_step': 48000, 'preemption_count': 0})], 'global_step': 48000}
I0216 02:06:58.172446 140202902193984 submission_runner.py:586] Timing: 37846.51352763176
I0216 02:06:58.172509 140202902193984 submission_runner.py:588] Total number of evals: 28
I0216 02:06:58.172568 140202902193984 submission_runner.py:589] ====================
I0216 02:06:58.172637 140202902193984 submission_runner.py:542] Using RNG seed 4294683350
I0216 02:06:58.176192 140202902193984 submission_runner.py:551] --- Tuning run 3/5 ---
I0216 02:06:58.176329 140202902193984 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_3/librispeech_deepspeech_jax/trial_3.
I0216 02:06:58.178349 140202902193984 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_3/librispeech_deepspeech_jax/trial_3/hparams.json.
I0216 02:06:58.180836 140202902193984 submission_runner.py:206] Initializing dataset.
I0216 02:06:58.180972 140202902193984 submission_runner.py:213] Initializing model.
I0216 02:06:59.304743 140202902193984 submission_runner.py:255] Initializing optimizer.
I0216 02:06:59.438326 140202902193984 submission_runner.py:262] Initializing metrics bundle.
I0216 02:06:59.438493 140202902193984 submission_runner.py:280] Initializing checkpoint and logger.
I0216 02:06:59.442218 140202902193984 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_3/librispeech_deepspeech_jax/trial_3 with prefix checkpoint_
I0216 02:06:59.442354 140202902193984 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_3/librispeech_deepspeech_jax/trial_3/meta_data_0.json.
I0216 02:06:59.442702 140202902193984 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0216 02:06:59.442776 140202902193984 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0216 02:07:00.144714 140202902193984 logger_utils.py:220] Unable to record git information. Continuing without it.
I0216 02:07:00.666105 140202902193984 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_3/librispeech_deepspeech_jax/trial_3/flags_0.json.
I0216 02:07:00.685451 140202902193984 submission_runner.py:314] Starting training loop.
I0216 02:07:00.688862 140202902193984 input_pipeline.py:20] Loading split = train-clean-100
I0216 02:07:00.730105 140202902193984 input_pipeline.py:20] Loading split = train-clean-360
I0216 02:07:00.859942 140202902193984 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0216 02:07:15.326643 140046691604224 logging_writer.py:48] [0] global_step=0, grad_norm=23.855167388916016, loss=32.94500732421875
I0216 02:07:15.340797 140202902193984 spec.py:321] Evaluating on the training split.
I0216 02:08:49.784828 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 02:09:47.885552 140202902193984 spec.py:349] Evaluating on the test split.
I0216 02:10:18.600363 140202902193984 submission_runner.py:408] Time since start: 197.91s, 	Step: 1, 	{'train/ctc_loss': Array(29.73173, dtype=float32), 'train/wer': 3.3317589058524173, 'validation/ctc_loss': Array(30.35123, dtype=float32), 'validation/wer': 3.0462844067698427, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.536522, dtype=float32), 'test/wer': 3.3616273637600798, 'test/num_examples': 2472, 'score': 14.655245780944824, 'total_duration': 197.91219639778137, 'accumulated_submission_time': 14.655245780944824, 'accumulated_eval_time': 183.25687742233276, 'accumulated_logging_time': 0}
I0216 02:10:18.617213 140047119996672 logging_writer.py:48] [1] accumulated_eval_time=183.256877, accumulated_logging_time=0, accumulated_submission_time=14.655246, global_step=1, preemption_count=0, score=14.655246, test/ctc_loss=30.536521911621094, test/num_examples=2472, test/wer=3.361627, total_duration=197.912196, train/ctc_loss=29.73172950744629, train/wer=3.331759, validation/ctc_loss=30.35123062133789, validation/num_examples=5348, validation/wer=3.046284
I0216 02:11:42.740957 140047010891520 logging_writer.py:48] [100] global_step=100, grad_norm=7.366188049316406, loss=9.443339347839355
I0216 02:12:59.070958 140047019284224 logging_writer.py:48] [200] global_step=200, grad_norm=1.7605289220809937, loss=6.55042839050293
I0216 02:14:15.988791 140047010891520 logging_writer.py:48] [300] global_step=300, grad_norm=0.7772440314292908, loss=5.909628868103027
I0216 02:15:32.802443 140047019284224 logging_writer.py:48] [400] global_step=400, grad_norm=0.5094949007034302, loss=5.856266021728516
I0216 02:16:49.605321 140047010891520 logging_writer.py:48] [500] global_step=500, grad_norm=0.5220825672149658, loss=5.84326171875
I0216 02:18:06.155233 140047019284224 logging_writer.py:48] [600] global_step=600, grad_norm=0.35727646946907043, loss=5.8216729164123535
I0216 02:19:22.444756 140047010891520 logging_writer.py:48] [700] global_step=700, grad_norm=0.6942517161369324, loss=5.77215051651001
I0216 02:20:38.995513 140047019284224 logging_writer.py:48] [800] global_step=800, grad_norm=0.423857718706131, loss=5.709427833557129
I0216 02:21:55.761739 140047010891520 logging_writer.py:48] [900] global_step=900, grad_norm=0.4014313220977783, loss=5.606819152832031
I0216 02:23:12.416072 140047019284224 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.5099694728851318, loss=5.494176864624023
I0216 02:24:32.145685 140047119996672 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.5513551235198975, loss=5.341773986816406
I0216 02:25:48.484929 140047111603968 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.5589996576309204, loss=5.056850910186768
I0216 02:27:04.864389 140047119996672 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.8142132759094238, loss=4.5545220375061035
I0216 02:28:20.992517 140047111603968 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.6148172616958618, loss=4.0945892333984375
I0216 02:29:36.904993 140047119996672 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.7605032920837402, loss=3.7985105514526367
I0216 02:30:52.710416 140047111603968 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.5146008729934692, loss=3.57086181640625
I0216 02:32:08.841871 140047119996672 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.7821426391601562, loss=3.398911952972412
I0216 02:33:24.946623 140047111603968 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.2328929901123047, loss=3.234894037246704
I0216 02:34:18.781947 140202902193984 spec.py:321] Evaluating on the training split.
I0216 02:34:57.749148 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 02:35:42.198856 140202902193984 spec.py:349] Evaluating on the test split.
I0216 02:36:05.118478 140202902193984 submission_runner.py:408] Time since start: 1744.43s, 	Step: 1870, 	{'train/ctc_loss': Array(6.4368887, dtype=float32), 'train/wer': 0.9437977009574123, 'validation/ctc_loss': Array(6.3937473, dtype=float32), 'validation/wer': 0.8966083203800072, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.3189464, dtype=float32), 'test/wer': 0.8995592387219954, 'test/num_examples': 2472, 'score': 1454.732485294342, 'total_duration': 1744.4271211624146, 'accumulated_submission_time': 1454.732485294342, 'accumulated_eval_time': 289.58756613731384, 'accumulated_logging_time': 0.029394865036010742}
I0216 02:36:05.149540 140047119996672 logging_writer.py:48] [1870] accumulated_eval_time=289.587566, accumulated_logging_time=0.029395, accumulated_submission_time=1454.732485, global_step=1870, preemption_count=0, score=1454.732485, test/ctc_loss=6.318946361541748, test/num_examples=2472, test/wer=0.899559, total_duration=1744.427121, train/ctc_loss=6.436888694763184, train/wer=0.943798, validation/ctc_loss=6.393747329711914, validation/num_examples=5348, validation/wer=0.896608
I0216 02:36:28.684234 140047111603968 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.3035361766815186, loss=3.0912222862243652
I0216 02:37:44.456224 140047119996672 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.0871317386627197, loss=3.0003745555877686
I0216 02:39:03.646448 140047119996672 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.5768985748291016, loss=2.873689889907837
I0216 02:40:19.066161 140047111603968 logging_writer.py:48] [2200] global_step=2200, grad_norm=2.0145785808563232, loss=2.7732326984405518
I0216 02:41:34.436877 140047119996672 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.2043099403381348, loss=2.675762414932251
I0216 02:42:49.710666 140047111603968 logging_writer.py:48] [2400] global_step=2400, grad_norm=3.2235260009765625, loss=2.637307643890381
I0216 02:44:04.924375 140047119996672 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.620954751968384, loss=2.6065526008605957
I0216 02:45:20.264956 140047111603968 logging_writer.py:48] [2600] global_step=2600, grad_norm=4.120243072509766, loss=2.550852060317993
I0216 02:46:35.313791 140047119996672 logging_writer.py:48] [2700] global_step=2700, grad_norm=3.5445940494537354, loss=2.5214099884033203
I0216 02:47:51.727894 140047111603968 logging_writer.py:48] [2800] global_step=2800, grad_norm=4.390825271606445, loss=2.430043935775757
I0216 02:49:11.958309 140047119996672 logging_writer.py:48] [2900] global_step=2900, grad_norm=3.245518207550049, loss=2.437464475631714
I0216 02:50:33.043368 140047111603968 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.9679081439971924, loss=2.3693602085113525
I0216 02:51:56.700475 140047119996672 logging_writer.py:48] [3100] global_step=3100, grad_norm=3.1670339107513428, loss=2.3426241874694824
I0216 02:53:11.673763 140047111603968 logging_writer.py:48] [3200] global_step=3200, grad_norm=3.4802730083465576, loss=2.2445571422576904
I0216 02:54:26.650818 140047119996672 logging_writer.py:48] [3300] global_step=3300, grad_norm=4.048018455505371, loss=2.2906134128570557
I0216 02:55:41.551320 140047111603968 logging_writer.py:48] [3400] global_step=3400, grad_norm=3.4272713661193848, loss=2.167813301086426
I0216 02:56:56.709498 140047119996672 logging_writer.py:48] [3500] global_step=3500, grad_norm=3.7257471084594727, loss=2.1016600131988525
I0216 02:58:11.608533 140047111603968 logging_writer.py:48] [3600] global_step=3600, grad_norm=2.739265203475952, loss=2.0518949031829834
I0216 02:59:26.553679 140047119996672 logging_writer.py:48] [3700] global_step=3700, grad_norm=3.4495110511779785, loss=2.057255268096924
I0216 03:00:06.001572 140202902193984 spec.py:321] Evaluating on the training split.
I0216 03:00:53.796938 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 03:01:41.751879 140202902193984 spec.py:349] Evaluating on the test split.
I0216 03:02:06.510164 140202902193984 submission_runner.py:408] Time since start: 3305.82s, 	Step: 3751, 	{'train/ctc_loss': Array(3.090075, dtype=float32), 'train/wer': 0.6942303997950864, 'validation/ctc_loss': Array(3.3290048, dtype=float32), 'validation/wer': 0.7045386524035259, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.8410707, dtype=float32), 'test/wer': 0.6370523835638697, 'test/num_examples': 2472, 'score': 2895.4914994239807, 'total_duration': 3305.8187985420227, 'accumulated_submission_time': 2895.4914994239807, 'accumulated_eval_time': 410.0903272628784, 'accumulated_logging_time': 0.07826495170593262}
I0216 03:02:06.543774 140047119996672 logging_writer.py:48] [3751] accumulated_eval_time=410.090327, accumulated_logging_time=0.078265, accumulated_submission_time=2895.491499, global_step=3751, preemption_count=0, score=2895.491499, test/ctc_loss=2.8410706520080566, test/num_examples=2472, test/wer=0.637052, total_duration=3305.818799, train/ctc_loss=3.0900750160217285, train/wer=0.694230, validation/ctc_loss=3.3290047645568848, validation/num_examples=5348, validation/wer=0.704539
I0216 03:02:43.908157 140047111603968 logging_writer.py:48] [3800] global_step=3800, grad_norm=2.8487062454223633, loss=2.1492977142333984
I0216 03:03:58.708600 140047119996672 logging_writer.py:48] [3900] global_step=3900, grad_norm=4.674115180969238, loss=2.136941909790039
I0216 03:05:13.787810 140047111603968 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.6212925910949707, loss=2.090144634246826
I0216 03:06:28.711203 140047119996672 logging_writer.py:48] [4100] global_step=4100, grad_norm=4.193833827972412, loss=2.0586299896240234
I0216 03:07:47.251710 140047119996672 logging_writer.py:48] [4200] global_step=4200, grad_norm=4.161895751953125, loss=2.012894868850708
I0216 03:09:02.722141 140047111603968 logging_writer.py:48] [4300] global_step=4300, grad_norm=4.819425106048584, loss=2.0349912643432617
I0216 03:10:17.663502 140047119996672 logging_writer.py:48] [4400] global_step=4400, grad_norm=3.7360942363739014, loss=2.067669630050659
I0216 03:11:32.798307 140047111603968 logging_writer.py:48] [4500] global_step=4500, grad_norm=3.5156362056732178, loss=2.043780565261841
I0216 03:12:47.725555 140047119996672 logging_writer.py:48] [4600] global_step=4600, grad_norm=4.480245590209961, loss=2.0878562927246094
I0216 03:14:02.985826 140047111603968 logging_writer.py:48] [4700] global_step=4700, grad_norm=3.4191503524780273, loss=2.0165815353393555
I0216 03:15:20.209866 140047119996672 logging_writer.py:48] [4800] global_step=4800, grad_norm=3.4879815578460693, loss=1.994375467300415
I0216 03:16:41.700974 140047111603968 logging_writer.py:48] [4900] global_step=4900, grad_norm=4.528217315673828, loss=1.9497319459915161
I0216 03:18:02.667119 140047119996672 logging_writer.py:48] [5000] global_step=5000, grad_norm=11.124540328979492, loss=2.0169296264648438
I0216 03:19:23.046816 140047111603968 logging_writer.py:48] [5100] global_step=5100, grad_norm=4.083558082580566, loss=1.9546831846237183
I0216 03:20:43.735043 140047119996672 logging_writer.py:48] [5200] global_step=5200, grad_norm=4.539519309997559, loss=1.9715027809143066
I0216 03:21:58.762318 140047111603968 logging_writer.py:48] [5300] global_step=5300, grad_norm=3.599647283554077, loss=1.9797906875610352
I0216 03:23:13.900071 140047119996672 logging_writer.py:48] [5400] global_step=5400, grad_norm=4.141439914703369, loss=1.9343899488449097
I0216 03:24:28.919091 140047111603968 logging_writer.py:48] [5500] global_step=5500, grad_norm=3.3098056316375732, loss=1.8074976205825806
I0216 03:25:43.940289 140047119996672 logging_writer.py:48] [5600] global_step=5600, grad_norm=2.286510944366455, loss=1.8122228384017944
I0216 03:26:07.033481 140202902193984 spec.py:321] Evaluating on the training split.
I0216 03:27:00.729092 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 03:27:50.756161 140202902193984 spec.py:349] Evaluating on the test split.
I0216 03:28:16.326698 140202902193984 submission_runner.py:408] Time since start: 4875.64s, 	Step: 5632, 	{'train/ctc_loss': Array(0.59384084, dtype=float32), 'train/wer': 0.20201960361912968, 'validation/ctc_loss': Array(0.9646992, dtype=float32), 'validation/wer': 0.2732459909053168, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6315554, dtype=float32), 'test/wer': 0.20488290374342413, 'test/num_examples': 2472, 'score': 4335.885547399521, 'total_duration': 4875.635069847107, 'accumulated_submission_time': 4335.885547399521, 'accumulated_eval_time': 539.3774290084839, 'accumulated_logging_time': 0.13100433349609375}
I0216 03:28:16.361564 140047119996672 logging_writer.py:48] [5632] accumulated_eval_time=539.377429, accumulated_logging_time=0.131004, accumulated_submission_time=4335.885547, global_step=5632, preemption_count=0, score=4335.885547, test/ctc_loss=0.6315553784370422, test/num_examples=2472, test/wer=0.204883, total_duration=4875.635070, train/ctc_loss=0.5938408374786377, train/wer=0.202020, validation/ctc_loss=0.9646992087364197, validation/num_examples=5348, validation/wer=0.273246
I0216 03:29:08.234367 140047111603968 logging_writer.py:48] [5700] global_step=5700, grad_norm=4.394530773162842, loss=1.8911399841308594
I0216 03:30:23.373931 140047119996672 logging_writer.py:48] [5800] global_step=5800, grad_norm=3.053537368774414, loss=1.8257055282592773
I0216 03:31:38.419982 140047111603968 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.9763991832733154, loss=1.945763111114502
I0216 03:32:53.607952 140047119996672 logging_writer.py:48] [6000] global_step=6000, grad_norm=3.435596227645874, loss=1.8180789947509766
I0216 03:34:08.833571 140047111603968 logging_writer.py:48] [6100] global_step=6100, grad_norm=3.1265151500701904, loss=1.9172905683517456
I0216 03:35:27.592513 140047119996672 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.8485264778137207, loss=1.8608684539794922
I0216 03:36:42.703068 140047111603968 logging_writer.py:48] [6300] global_step=6300, grad_norm=3.8178563117980957, loss=1.8066928386688232
I0216 03:37:57.807936 140047119996672 logging_writer.py:48] [6400] global_step=6400, grad_norm=7.065276622772217, loss=1.8035125732421875
I0216 03:39:13.281223 140047111603968 logging_writer.py:48] [6500] global_step=6500, grad_norm=3.6595356464385986, loss=1.8773150444030762
I0216 03:40:28.414541 140047119996672 logging_writer.py:48] [6600] global_step=6600, grad_norm=2.134490489959717, loss=1.8377872705459595
I0216 03:41:43.349951 140047111603968 logging_writer.py:48] [6700] global_step=6700, grad_norm=3.3864455223083496, loss=1.8222997188568115
I0216 03:42:58.250553 140047119996672 logging_writer.py:48] [6800] global_step=6800, grad_norm=4.214159965515137, loss=1.8284608125686646
I0216 03:44:15.522543 140047111603968 logging_writer.py:48] [6900] global_step=6900, grad_norm=3.0955429077148438, loss=1.8429646492004395
I0216 03:45:35.361745 140047119996672 logging_writer.py:48] [7000] global_step=7000, grad_norm=3.1053953170776367, loss=1.793152093887329
I0216 03:46:54.757029 140047111603968 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.726494550704956, loss=1.8261134624481201
I0216 03:48:15.526148 140047119996672 logging_writer.py:48] [7200] global_step=7200, grad_norm=4.708413124084473, loss=1.7483426332473755
I0216 03:49:34.180253 140047119996672 logging_writer.py:48] [7300] global_step=7300, grad_norm=3.180683135986328, loss=1.7647960186004639
I0216 03:50:49.134102 140047111603968 logging_writer.py:48] [7400] global_step=7400, grad_norm=2.448059320449829, loss=1.8008980751037598
I0216 03:52:04.064266 140047119996672 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.2293341159820557, loss=1.8335546255111694
I0216 03:52:16.744671 140202902193984 spec.py:321] Evaluating on the training split.
I0216 03:53:12.646576 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 03:54:03.278491 140202902193984 spec.py:349] Evaluating on the test split.
I0216 03:54:29.189677 140202902193984 submission_runner.py:408] Time since start: 6448.50s, 	Step: 7518, 	{'train/ctc_loss': Array(0.4997202, dtype=float32), 'train/wer': 0.16799694940398457, 'validation/ctc_loss': Array(0.8595321, dtype=float32), 'validation/wer': 0.24581712156173668, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.54608774, dtype=float32), 'test/wer': 0.1780513070501493, 'test/num_examples': 2472, 'score': 5776.177357435226, 'total_duration': 6448.498773574829, 'accumulated_submission_time': 5776.177357435226, 'accumulated_eval_time': 671.8170447349548, 'accumulated_logging_time': 0.18126487731933594}
I0216 03:54:29.224249 140047119996672 logging_writer.py:48] [7518] accumulated_eval_time=671.817045, accumulated_logging_time=0.181265, accumulated_submission_time=5776.177357, global_step=7518, preemption_count=0, score=5776.177357, test/ctc_loss=0.5460877418518066, test/num_examples=2472, test/wer=0.178051, total_duration=6448.498774, train/ctc_loss=0.49972018599510193, train/wer=0.167997, validation/ctc_loss=0.8595321178436279, validation/num_examples=5348, validation/wer=0.245817
I0216 03:55:31.452884 140047111603968 logging_writer.py:48] [7600] global_step=7600, grad_norm=2.6770358085632324, loss=1.7463160753250122
I0216 03:56:46.534118 140047119996672 logging_writer.py:48] [7700] global_step=7700, grad_norm=2.988373041152954, loss=1.7346255779266357
I0216 03:58:01.334845 140047111603968 logging_writer.py:48] [7800] global_step=7800, grad_norm=2.585495948791504, loss=1.775404930114746
I0216 03:59:16.555413 140047119996672 logging_writer.py:48] [7900] global_step=7900, grad_norm=3.041882038116455, loss=1.6612123250961304
I0216 04:00:31.593662 140047111603968 logging_writer.py:48] [8000] global_step=8000, grad_norm=5.2289276123046875, loss=1.8109122514724731
I0216 04:01:46.523937 140047119996672 logging_writer.py:48] [8100] global_step=8100, grad_norm=2.649226665496826, loss=1.8074136972427368
I0216 04:03:03.057721 140047111603968 logging_writer.py:48] [8200] global_step=8200, grad_norm=3.203807830810547, loss=1.6924726963043213
I0216 04:04:22.941543 140047119996672 logging_writer.py:48] [8300] global_step=8300, grad_norm=3.509185791015625, loss=1.695474624633789
I0216 04:05:38.064005 140047111603968 logging_writer.py:48] [8400] global_step=8400, grad_norm=2.9353513717651367, loss=1.8083432912826538
I0216 04:06:52.878708 140047119996672 logging_writer.py:48] [8500] global_step=8500, grad_norm=2.3716752529144287, loss=1.7178103923797607
I0216 04:08:08.029143 140047111603968 logging_writer.py:48] [8600] global_step=8600, grad_norm=3.302686929702759, loss=1.7286880016326904
I0216 04:09:23.125072 140047119996672 logging_writer.py:48] [8700] global_step=8700, grad_norm=2.789987564086914, loss=1.7595027685165405
I0216 04:10:37.986875 140047111603968 logging_writer.py:48] [8800] global_step=8800, grad_norm=3.7314863204956055, loss=1.6522520780563354
I0216 04:11:52.988977 140047119996672 logging_writer.py:48] [8900] global_step=8900, grad_norm=2.7546327114105225, loss=1.701134204864502
I0216 04:13:09.750889 140047111603968 logging_writer.py:48] [9000] global_step=9000, grad_norm=2.358006715774536, loss=1.7204968929290771
I0216 04:14:30.022756 140047119996672 logging_writer.py:48] [9100] global_step=9100, grad_norm=2.9105067253112793, loss=1.7320219278335571
I0216 04:15:50.447759 140047111603968 logging_writer.py:48] [9200] global_step=9200, grad_norm=2.855100154876709, loss=1.674281120300293
I0216 04:17:12.090017 140047119996672 logging_writer.py:48] [9300] global_step=9300, grad_norm=2.5787055492401123, loss=1.6891412734985352
I0216 04:18:27.037587 140047111603968 logging_writer.py:48] [9400] global_step=9400, grad_norm=3.3104147911071777, loss=1.7040303945541382
I0216 04:18:29.903422 140202902193984 spec.py:321] Evaluating on the training split.
I0216 04:19:24.665018 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 04:20:13.879263 140202902193984 spec.py:349] Evaluating on the test split.
I0216 04:20:39.194402 140202902193984 submission_runner.py:408] Time since start: 8018.50s, 	Step: 9405, 	{'train/ctc_loss': Array(0.44924083, dtype=float32), 'train/wer': 0.1499989441898769, 'validation/ctc_loss': Array(0.76941633, dtype=float32), 'validation/wer': 0.22184461801365168, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4813573, dtype=float32), 'test/wer': 0.15560701155728882, 'test/num_examples': 2472, 'score': 7216.76273560524, 'total_duration': 8018.502638101578, 'accumulated_submission_time': 7216.76273560524, 'accumulated_eval_time': 801.1017694473267, 'accumulated_logging_time': 0.23203825950622559}
I0216 04:20:39.233600 140047119996672 logging_writer.py:48] [9405] accumulated_eval_time=801.101769, accumulated_logging_time=0.232038, accumulated_submission_time=7216.762736, global_step=9405, preemption_count=0, score=7216.762736, test/ctc_loss=0.48135730624198914, test/num_examples=2472, test/wer=0.155607, total_duration=8018.502638, train/ctc_loss=0.4492408335208893, train/wer=0.149999, validation/ctc_loss=0.769416332244873, validation/num_examples=5348, validation/wer=0.221845
I0216 04:21:51.530855 140047111603968 logging_writer.py:48] [9500] global_step=9500, grad_norm=3.1786389350891113, loss=1.746066689491272
I0216 04:23:06.616150 140047119996672 logging_writer.py:48] [9600] global_step=9600, grad_norm=4.017993450164795, loss=1.6686209440231323
I0216 04:24:21.451880 140047111603968 logging_writer.py:48] [9700] global_step=9700, grad_norm=2.428309917449951, loss=1.6780139207839966
I0216 04:25:36.574630 140047119996672 logging_writer.py:48] [9800] global_step=9800, grad_norm=3.4365055561065674, loss=1.7502975463867188
I0216 04:26:51.719877 140047111603968 logging_writer.py:48] [9900] global_step=9900, grad_norm=2.9726884365081787, loss=1.6309093236923218
I0216 04:28:06.813940 140047119996672 logging_writer.py:48] [10000] global_step=10000, grad_norm=2.0425188541412354, loss=1.6407804489135742
I0216 04:29:21.836318 140047111603968 logging_writer.py:48] [10100] global_step=10100, grad_norm=2.312211275100708, loss=1.6955565214157104
I0216 04:30:39.740690 140047119996672 logging_writer.py:48] [10200] global_step=10200, grad_norm=2.926849842071533, loss=1.6946531534194946
I0216 04:32:02.800045 140047119996672 logging_writer.py:48] [10300] global_step=10300, grad_norm=3.0845701694488525, loss=1.711764931678772
I0216 04:33:17.905811 140047111603968 logging_writer.py:48] [10400] global_step=10400, grad_norm=5.115328788757324, loss=1.6150245666503906
I0216 04:34:32.946495 140047119996672 logging_writer.py:48] [10500] global_step=10500, grad_norm=2.830720901489258, loss=1.6769548654556274
I0216 04:35:48.214983 140047111603968 logging_writer.py:48] [10600] global_step=10600, grad_norm=2.533083200454712, loss=1.6536729335784912
I0216 04:37:03.565811 140047119996672 logging_writer.py:48] [10700] global_step=10700, grad_norm=3.175217866897583, loss=1.6587787866592407
I0216 04:38:18.482603 140047111603968 logging_writer.py:48] [10800] global_step=10800, grad_norm=3.5781726837158203, loss=1.712981939315796
I0216 04:39:33.616552 140047119996672 logging_writer.py:48] [10900] global_step=10900, grad_norm=2.586709976196289, loss=1.693570613861084
I0216 04:40:50.760031 140047111603968 logging_writer.py:48] [11000] global_step=11000, grad_norm=2.360628128051758, loss=1.6384625434875488
I0216 04:42:09.966651 140047119996672 logging_writer.py:48] [11100] global_step=11100, grad_norm=3.0034520626068115, loss=1.6304962635040283
I0216 04:43:29.802591 140047111603968 logging_writer.py:48] [11200] global_step=11200, grad_norm=2.6419570446014404, loss=1.6545047760009766
I0216 04:44:39.742356 140202902193984 spec.py:321] Evaluating on the training split.
I0216 04:45:36.687055 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 04:46:27.427834 140202902193984 spec.py:349] Evaluating on the test split.
I0216 04:46:52.992207 140202902193984 submission_runner.py:408] Time since start: 9592.30s, 	Step: 11289, 	{'train/ctc_loss': Array(0.41514525, dtype=float32), 'train/wer': 0.13922525048234338, 'validation/ctc_loss': Array(0.7317493, dtype=float32), 'validation/wer': 0.21033627156608128, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.44250494, dtype=float32), 'test/wer': 0.14214043426157252, 'test/num_examples': 2472, 'score': 8657.179950237274, 'total_duration': 9592.300470352173, 'accumulated_submission_time': 8657.179950237274, 'accumulated_eval_time': 934.3454160690308, 'accumulated_logging_time': 0.2858431339263916}
I0216 04:46:53.025865 140047119996672 logging_writer.py:48] [11289] accumulated_eval_time=934.345416, accumulated_logging_time=0.285843, accumulated_submission_time=8657.179950, global_step=11289, preemption_count=0, score=8657.179950, test/ctc_loss=0.4425049424171448, test/num_examples=2472, test/wer=0.142140, total_duration=9592.300470, train/ctc_loss=0.41514524817466736, train/wer=0.139225, validation/ctc_loss=0.7317492961883545, validation/num_examples=5348, validation/wer=0.210336
I0216 04:47:02.125036 140047111603968 logging_writer.py:48] [11300] global_step=11300, grad_norm=4.061968803405762, loss=1.659040093421936
I0216 04:48:20.602890 140047119996672 logging_writer.py:48] [11400] global_step=11400, grad_norm=2.611281394958496, loss=1.672044277191162
I0216 04:49:35.721783 140047111603968 logging_writer.py:48] [11500] global_step=11500, grad_norm=3.144061803817749, loss=1.5597246885299683
I0216 04:50:51.249340 140047119996672 logging_writer.py:48] [11600] global_step=11600, grad_norm=4.1896820068359375, loss=1.6698510646820068
I0216 04:52:06.444101 140047111603968 logging_writer.py:48] [11700] global_step=11700, grad_norm=3.4316837787628174, loss=1.6244642734527588
I0216 04:53:21.820770 140047119996672 logging_writer.py:48] [11800] global_step=11800, grad_norm=2.870178461074829, loss=1.6764564514160156
I0216 04:54:36.958193 140047111603968 logging_writer.py:48] [11900] global_step=11900, grad_norm=2.7052001953125, loss=1.6351317167282104
I0216 04:55:52.870619 140047119996672 logging_writer.py:48] [12000] global_step=12000, grad_norm=3.2411649227142334, loss=1.6320866346359253
I0216 04:57:14.048168 140047111603968 logging_writer.py:48] [12100] global_step=12100, grad_norm=5.566329002380371, loss=1.6282761096954346
I0216 04:58:34.757799 140047119996672 logging_writer.py:48] [12200] global_step=12200, grad_norm=2.5754683017730713, loss=1.6978545188903809
I0216 04:59:54.815501 140047111603968 logging_writer.py:48] [12300] global_step=12300, grad_norm=3.096597909927368, loss=1.6390043497085571
I0216 05:01:16.501223 140047119996672 logging_writer.py:48] [12400] global_step=12400, grad_norm=2.38795804977417, loss=1.6168904304504395
I0216 05:02:31.318348 140047111603968 logging_writer.py:48] [12500] global_step=12500, grad_norm=3.655090808868408, loss=1.5911939144134521
I0216 05:03:46.373141 140047119996672 logging_writer.py:48] [12600] global_step=12600, grad_norm=3.053328275680542, loss=1.6189054250717163
I0216 05:05:01.418532 140047111603968 logging_writer.py:48] [12700] global_step=12700, grad_norm=2.900693655014038, loss=1.6999250650405884
I0216 05:06:16.502350 140047119996672 logging_writer.py:48] [12800] global_step=12800, grad_norm=3.6930856704711914, loss=1.6439937353134155
I0216 05:07:31.618376 140047111603968 logging_writer.py:48] [12900] global_step=12900, grad_norm=2.3168249130249023, loss=1.5926263332366943
I0216 05:08:48.906241 140047119996672 logging_writer.py:48] [13000] global_step=13000, grad_norm=2.182445764541626, loss=1.6020874977111816
I0216 05:10:09.234775 140047111603968 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.941771388053894, loss=1.6499035358428955
I0216 05:10:53.431236 140202902193984 spec.py:321] Evaluating on the training split.
I0216 05:11:49.134101 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 05:12:39.641515 140202902193984 spec.py:349] Evaluating on the test split.
I0216 05:13:05.476431 140202902193984 submission_runner.py:408] Time since start: 11164.78s, 	Step: 13156, 	{'train/ctc_loss': Array(0.38317287, dtype=float32), 'train/wer': 0.12914786928022384, 'validation/ctc_loss': Array(0.6829772, dtype=float32), 'validation/wer': 0.1974473097309248, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4097351, dtype=float32), 'test/wer': 0.1304409643938009, 'test/num_examples': 2472, 'score': 10097.492826223373, 'total_duration': 11164.784424304962, 'accumulated_submission_time': 10097.492826223373, 'accumulated_eval_time': 1066.3841433525085, 'accumulated_logging_time': 0.3352360725402832}
I0216 05:13:05.510332 140047119996672 logging_writer.py:48] [13156] accumulated_eval_time=1066.384143, accumulated_logging_time=0.335236, accumulated_submission_time=10097.492826, global_step=13156, preemption_count=0, score=10097.492826, test/ctc_loss=0.4097351133823395, test/num_examples=2472, test/wer=0.130441, total_duration=11164.784424, train/ctc_loss=0.383172869682312, train/wer=0.129148, validation/ctc_loss=0.6829771995544434, validation/num_examples=5348, validation/wer=0.197447
I0216 05:13:39.377717 140047111603968 logging_writer.py:48] [13200] global_step=13200, grad_norm=4.3559088706970215, loss=1.5464998483657837
I0216 05:14:54.684402 140047119996672 logging_writer.py:48] [13300] global_step=13300, grad_norm=8.224518775939941, loss=1.5925840139389038
I0216 05:16:13.201975 140047119996672 logging_writer.py:48] [13400] global_step=13400, grad_norm=3.496345281600952, loss=1.6097497940063477
I0216 05:17:28.375680 140047111603968 logging_writer.py:48] [13500] global_step=13500, grad_norm=3.042637586593628, loss=1.6142085790634155
I0216 05:18:43.594085 140047119996672 logging_writer.py:48] [13600] global_step=13600, grad_norm=2.867652177810669, loss=1.6028611660003662
I0216 05:19:58.735549 140047111603968 logging_writer.py:48] [13700] global_step=13700, grad_norm=2.219587564468384, loss=1.640931248664856
I0216 05:21:14.091067 140047119996672 logging_writer.py:48] [13800] global_step=13800, grad_norm=2.596862316131592, loss=1.5887726545333862
I0216 05:22:29.159060 140047111603968 logging_writer.py:48] [13900] global_step=13900, grad_norm=2.4962034225463867, loss=1.6097909212112427
I0216 05:23:44.128008 140047119996672 logging_writer.py:48] [14000] global_step=14000, grad_norm=3.1399712562561035, loss=1.6100425720214844
I0216 05:25:05.232751 140047111603968 logging_writer.py:48] [14100] global_step=14100, grad_norm=2.9344890117645264, loss=1.5988619327545166
I0216 05:26:26.890026 140047119996672 logging_writer.py:48] [14200] global_step=14200, grad_norm=2.590186834335327, loss=1.5905033349990845
I0216 05:27:47.663999 140047111603968 logging_writer.py:48] [14300] global_step=14300, grad_norm=2.8476781845092773, loss=1.5965502262115479
I0216 05:29:08.867942 140047119996672 logging_writer.py:48] [14400] global_step=14400, grad_norm=2.921769618988037, loss=1.596008062362671
I0216 05:30:27.815821 140047119996672 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.8845802545547485, loss=1.6088050603866577
I0216 05:31:42.882232 140047111603968 logging_writer.py:48] [14600] global_step=14600, grad_norm=2.331083297729492, loss=1.5815932750701904
I0216 05:32:57.973134 140047119996672 logging_writer.py:48] [14700] global_step=14700, grad_norm=4.258265972137451, loss=1.5296399593353271
I0216 05:34:12.844815 140047111603968 logging_writer.py:48] [14800] global_step=14800, grad_norm=3.531839609146118, loss=1.6032756567001343
I0216 05:35:27.870593 140047119996672 logging_writer.py:48] [14900] global_step=14900, grad_norm=3.0805985927581787, loss=1.5780612230300903
I0216 05:36:42.868826 140047111603968 logging_writer.py:48] [15000] global_step=15000, grad_norm=2.82749080657959, loss=1.6446675062179565
I0216 05:37:06.030169 140202902193984 spec.py:321] Evaluating on the training split.
I0216 05:38:01.572120 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 05:38:52.113919 140202902193984 spec.py:349] Evaluating on the test split.
I0216 05:39:18.126999 140202902193984 submission_runner.py:408] Time since start: 12737.43s, 	Step: 15032, 	{'train/ctc_loss': Array(0.332868, dtype=float32), 'train/wer': 0.11580051773861054, 'validation/ctc_loss': Array(0.666079, dtype=float32), 'validation/wer': 0.19206966797648126, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39503682, dtype=float32), 'test/wer': 0.12934413909369732, 'test/num_examples': 2472, 'score': 11537.915561676025, 'total_duration': 12737.434754610062, 'accumulated_submission_time': 11537.915561676025, 'accumulated_eval_time': 1198.4742548465729, 'accumulated_logging_time': 0.388714075088501}
I0216 05:39:18.159871 140047119996672 logging_writer.py:48] [15032] accumulated_eval_time=1198.474255, accumulated_logging_time=0.388714, accumulated_submission_time=11537.915562, global_step=15032, preemption_count=0, score=11537.915562, test/ctc_loss=0.39503681659698486, test/num_examples=2472, test/wer=0.129344, total_duration=12737.434755, train/ctc_loss=0.3328680098056793, train/wer=0.115801, validation/ctc_loss=0.6660789847373962, validation/num_examples=5348, validation/wer=0.192070
I0216 05:40:10.000842 140047111603968 logging_writer.py:48] [15100] global_step=15100, grad_norm=3.259056329727173, loss=1.6099241971969604
I0216 05:41:25.404883 140047119996672 logging_writer.py:48] [15200] global_step=15200, grad_norm=2.5506410598754883, loss=1.5179883241653442
I0216 05:42:40.668411 140047111603968 logging_writer.py:48] [15300] global_step=15300, grad_norm=2.7499399185180664, loss=1.6303164958953857
I0216 05:43:55.959403 140047119996672 logging_writer.py:48] [15400] global_step=15400, grad_norm=4.058528900146484, loss=1.5409780740737915
I0216 05:45:14.233447 140047119996672 logging_writer.py:48] [15500] global_step=15500, grad_norm=3.595668315887451, loss=1.5818697214126587
I0216 05:46:29.454493 140047111603968 logging_writer.py:48] [15600] global_step=15600, grad_norm=3.2283763885498047, loss=1.5973775386810303
I0216 05:47:44.510962 140047119996672 logging_writer.py:48] [15700] global_step=15700, grad_norm=3.836667537689209, loss=1.5384063720703125
I0216 05:48:59.753632 140047111603968 logging_writer.py:48] [15800] global_step=15800, grad_norm=3.828291893005371, loss=1.5197943449020386
I0216 05:50:14.932721 140047119996672 logging_writer.py:48] [15900] global_step=15900, grad_norm=2.209745168685913, loss=1.5948044061660767
I0216 05:51:30.158951 140047111603968 logging_writer.py:48] [16000] global_step=16000, grad_norm=2.7653980255126953, loss=1.5381293296813965
I0216 05:52:46.197835 140047119996672 logging_writer.py:48] [16100] global_step=16100, grad_norm=3.9878745079040527, loss=1.6762808561325073
I0216 05:54:07.217373 140047111603968 logging_writer.py:48] [16200] global_step=16200, grad_norm=4.584741592407227, loss=1.570968508720398
I0216 05:55:28.515938 140047119996672 logging_writer.py:48] [16300] global_step=16300, grad_norm=2.795013904571533, loss=1.656601071357727
I0216 05:56:48.932853 140047111603968 logging_writer.py:48] [16400] global_step=16400, grad_norm=3.05064058303833, loss=1.5703096389770508
I0216 05:58:11.541618 140047119996672 logging_writer.py:48] [16500] global_step=16500, grad_norm=3.36014461517334, loss=1.5460302829742432
I0216 05:59:26.661978 140047111603968 logging_writer.py:48] [16600] global_step=16600, grad_norm=2.851219892501831, loss=1.5875004529953003
I0216 06:00:41.513660 140047119996672 logging_writer.py:48] [16700] global_step=16700, grad_norm=2.163194417953491, loss=1.535784363746643
I0216 06:01:56.727302 140047111603968 logging_writer.py:48] [16800] global_step=16800, grad_norm=2.2197320461273193, loss=1.5228484869003296
I0216 06:03:11.831939 140047119996672 logging_writer.py:48] [16900] global_step=16900, grad_norm=5.869942665100098, loss=1.581005334854126
I0216 06:03:18.405936 140202902193984 spec.py:321] Evaluating on the training split.
I0216 06:04:14.581145 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 06:05:05.200239 140202902193984 spec.py:349] Evaluating on the test split.
I0216 06:05:30.420793 140202902193984 submission_runner.py:408] Time since start: 14309.73s, 	Step: 16910, 	{'train/ctc_loss': Array(0.33133, dtype=float32), 'train/wer': 0.1122018775264075, 'validation/ctc_loss': Array(0.6564473, dtype=float32), 'validation/wer': 0.1873099240178804, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38526803, dtype=float32), 'test/wer': 0.12528182316738773, 'test/num_examples': 2472, 'score': 12978.067933797836, 'total_duration': 14309.729179859161, 'accumulated_submission_time': 12978.067933797836, 'accumulated_eval_time': 1330.4830236434937, 'accumulated_logging_time': 0.43863677978515625}
I0216 06:05:30.454041 140047119996672 logging_writer.py:48] [16910] accumulated_eval_time=1330.483024, accumulated_logging_time=0.438637, accumulated_submission_time=12978.067934, global_step=16910, preemption_count=0, score=12978.067934, test/ctc_loss=0.38526803255081177, test/num_examples=2472, test/wer=0.125282, total_duration=14309.729180, train/ctc_loss=0.33133000135421753, train/wer=0.112202, validation/ctc_loss=0.6564472913742065, validation/num_examples=5348, validation/wer=0.187310
I0216 06:06:38.812069 140047111603968 logging_writer.py:48] [17000] global_step=17000, grad_norm=3.539106607437134, loss=1.5552167892456055
I0216 06:07:53.937120 140047119996672 logging_writer.py:48] [17100] global_step=17100, grad_norm=2.675262212753296, loss=1.532996654510498
I0216 06:09:09.236659 140047111603968 logging_writer.py:48] [17200] global_step=17200, grad_norm=3.941612958908081, loss=1.5681270360946655
I0216 06:10:24.297229 140047119996672 logging_writer.py:48] [17300] global_step=17300, grad_norm=2.317434549331665, loss=1.5583380460739136
I0216 06:11:41.467564 140047111603968 logging_writer.py:48] [17400] global_step=17400, grad_norm=2.2909975051879883, loss=1.6033437252044678
I0216 06:13:01.251801 140047119996672 logging_writer.py:48] [17500] global_step=17500, grad_norm=4.0664591789245605, loss=1.5488648414611816
I0216 06:14:20.363619 140047119996672 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.8656426668167114, loss=1.4771203994750977
I0216 06:15:35.353401 140047111603968 logging_writer.py:48] [17700] global_step=17700, grad_norm=3.1231424808502197, loss=1.4933677911758423
I0216 06:16:50.092977 140047119996672 logging_writer.py:48] [17800] global_step=17800, grad_norm=3.455895185470581, loss=1.5635267496109009
I0216 06:18:05.069580 140047111603968 logging_writer.py:48] [17900] global_step=17900, grad_norm=2.680448532104492, loss=1.5563522577285767
I0216 06:19:19.989897 140047119996672 logging_writer.py:48] [18000] global_step=18000, grad_norm=4.267260551452637, loss=1.5573375225067139
I0216 06:20:35.985125 140047111603968 logging_writer.py:48] [18100] global_step=18100, grad_norm=5.541860103607178, loss=1.6168097257614136
I0216 06:21:55.575187 140047119996672 logging_writer.py:48] [18200] global_step=18200, grad_norm=2.9854605197906494, loss=1.5865085124969482
I0216 06:23:16.349725 140047111603968 logging_writer.py:48] [18300] global_step=18300, grad_norm=2.8136425018310547, loss=1.5409260988235474
I0216 06:24:36.808625 140047119996672 logging_writer.py:48] [18400] global_step=18400, grad_norm=3.451889991760254, loss=1.5051709413528442
I0216 06:25:57.434041 140047111603968 logging_writer.py:48] [18500] global_step=18500, grad_norm=2.1089532375335693, loss=1.4700837135314941
I0216 06:27:17.807153 140047119996672 logging_writer.py:48] [18600] global_step=18600, grad_norm=2.0338008403778076, loss=1.5315066576004028
I0216 06:28:32.750002 140047111603968 logging_writer.py:48] [18700] global_step=18700, grad_norm=2.339494228363037, loss=1.5340759754180908
I0216 06:29:30.916566 140202902193984 spec.py:321] Evaluating on the training split.
I0216 06:30:25.635454 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 06:31:15.497101 140202902193984 spec.py:349] Evaluating on the test split.
I0216 06:31:41.027433 140202902193984 submission_runner.py:408] Time since start: 15880.34s, 	Step: 18779, 	{'train/ctc_loss': Array(0.33849165, dtype=float32), 'train/wer': 0.1135576714996754, 'validation/ctc_loss': Array(0.6315583, dtype=float32), 'validation/wer': 0.1813240391206542, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36856124, dtype=float32), 'test/wer': 0.11796965450003047, 'test/num_examples': 2472, 'score': 14418.436093091965, 'total_duration': 15880.336900472641, 'accumulated_submission_time': 14418.436093091965, 'accumulated_eval_time': 1460.588921546936, 'accumulated_logging_time': 0.4884147644042969}
I0216 06:31:41.060867 140047119996672 logging_writer.py:48] [18779] accumulated_eval_time=1460.588922, accumulated_logging_time=0.488415, accumulated_submission_time=14418.436093, global_step=18779, preemption_count=0, score=14418.436093, test/ctc_loss=0.3685612380504608, test/num_examples=2472, test/wer=0.117970, total_duration=15880.336900, train/ctc_loss=0.33849164843559265, train/wer=0.113558, validation/ctc_loss=0.6315582990646362, validation/num_examples=5348, validation/wer=0.181324
I0216 06:31:57.673592 140047111603968 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.9438947439193726, loss=1.4675344228744507
I0216 06:33:12.796039 140047119996672 logging_writer.py:48] [18900] global_step=18900, grad_norm=3.3750739097595215, loss=1.5129451751708984
I0216 06:34:28.036000 140047111603968 logging_writer.py:48] [19000] global_step=19000, grad_norm=3.018156051635742, loss=1.6022719144821167
I0216 06:35:43.159569 140047119996672 logging_writer.py:48] [19100] global_step=19100, grad_norm=2.5835251808166504, loss=1.5299688577651978
I0216 06:36:58.435422 140047111603968 logging_writer.py:48] [19200] global_step=19200, grad_norm=2.2227063179016113, loss=1.4383143186569214
I0216 06:38:13.735261 140047119996672 logging_writer.py:48] [19300] global_step=19300, grad_norm=3.359546661376953, loss=1.5186156034469604
I0216 06:39:31.329324 140047111603968 logging_writer.py:48] [19400] global_step=19400, grad_norm=2.612520456314087, loss=1.4986774921417236
I0216 06:40:52.406223 140047119996672 logging_writer.py:48] [19500] global_step=19500, grad_norm=2.353215217590332, loss=1.5180964469909668
I0216 06:42:14.783488 140047119996672 logging_writer.py:48] [19600] global_step=19600, grad_norm=2.6291451454162598, loss=1.540626049041748
I0216 06:43:29.867962 140047111603968 logging_writer.py:48] [19700] global_step=19700, grad_norm=2.6216461658477783, loss=1.4055724143981934
I0216 06:44:44.731840 140047119996672 logging_writer.py:48] [19800] global_step=19800, grad_norm=2.250547170639038, loss=1.4945632219314575
I0216 06:45:59.736496 140047111603968 logging_writer.py:48] [19900] global_step=19900, grad_norm=3.380811929702759, loss=1.446706771850586
I0216 06:47:14.644839 140047119996672 logging_writer.py:48] [20000] global_step=20000, grad_norm=2.743260383605957, loss=1.5238044261932373
I0216 06:48:29.734422 140047111603968 logging_writer.py:48] [20100] global_step=20100, grad_norm=4.10338830947876, loss=1.4686179161071777
I0216 06:49:45.570802 140047119996672 logging_writer.py:48] [20200] global_step=20200, grad_norm=3.364368438720703, loss=1.5151296854019165
I0216 06:51:05.863488 140047111603968 logging_writer.py:48] [20300] global_step=20300, grad_norm=3.5948431491851807, loss=1.5313975811004639
I0216 06:52:26.918792 140047119996672 logging_writer.py:48] [20400] global_step=20400, grad_norm=2.7284762859344482, loss=1.5441049337387085
I0216 06:53:47.766458 140047111603968 logging_writer.py:48] [20500] global_step=20500, grad_norm=2.66432523727417, loss=1.5110067129135132
I0216 06:55:12.841082 140047119996672 logging_writer.py:48] [20600] global_step=20600, grad_norm=2.76039457321167, loss=1.4989840984344482
I0216 06:55:41.180824 140202902193984 spec.py:321] Evaluating on the training split.
I0216 06:56:35.454907 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 06:57:25.293845 140202902193984 spec.py:349] Evaluating on the test split.
I0216 06:57:50.779086 140202902193984 submission_runner.py:408] Time since start: 17450.09s, 	Step: 20639, 	{'train/ctc_loss': Array(0.30381674, dtype=float32), 'train/wer': 0.10153564560851115, 'validation/ctc_loss': Array(0.6062795, dtype=float32), 'validation/wer': 0.1727410525502766, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35437745, dtype=float32), 'test/wer': 0.11307456380882741, 'test/num_examples': 2472, 'score': 15858.46332526207, 'total_duration': 17450.086770772934, 'accumulated_submission_time': 15858.46332526207, 'accumulated_eval_time': 1590.1804230213165, 'accumulated_logging_time': 0.5372920036315918}
I0216 06:57:50.817887 140047119996672 logging_writer.py:48] [20639] accumulated_eval_time=1590.180423, accumulated_logging_time=0.537292, accumulated_submission_time=15858.463325, global_step=20639, preemption_count=0, score=15858.463325, test/ctc_loss=0.3543774485588074, test/num_examples=2472, test/wer=0.113075, total_duration=17450.086771, train/ctc_loss=0.3038167357444763, train/wer=0.101536, validation/ctc_loss=0.6062794923782349, validation/num_examples=5348, validation/wer=0.172741
I0216 06:58:37.413614 140047111603968 logging_writer.py:48] [20700] global_step=20700, grad_norm=2.077958822250366, loss=1.4901707172393799
I0216 06:59:52.580818 140047119996672 logging_writer.py:48] [20800] global_step=20800, grad_norm=2.135714292526245, loss=1.4715176820755005
I0216 07:01:07.654311 140047111603968 logging_writer.py:48] [20900] global_step=20900, grad_norm=3.380953073501587, loss=1.5099152326583862
I0216 07:02:22.602743 140047119996672 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.981317400932312, loss=1.451617956161499
I0216 07:03:37.649142 140047111603968 logging_writer.py:48] [21100] global_step=21100, grad_norm=3.0602800846099854, loss=1.5579134225845337
I0216 07:04:52.874465 140047119996672 logging_writer.py:48] [21200] global_step=21200, grad_norm=2.381704330444336, loss=1.5128768682479858
I0216 07:06:08.080380 140047111603968 logging_writer.py:48] [21300] global_step=21300, grad_norm=2.8852319717407227, loss=1.5231778621673584
I0216 07:07:23.258649 140047119996672 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.647679090499878, loss=1.3968913555145264
I0216 07:08:42.718355 140047111603968 logging_writer.py:48] [21500] global_step=21500, grad_norm=3.110138416290283, loss=1.458909273147583
I0216 07:10:04.229191 140047119996672 logging_writer.py:48] [21600] global_step=21600, grad_norm=3.864245891571045, loss=1.457655429840088
I0216 07:11:24.630670 140047119996672 logging_writer.py:48] [21700] global_step=21700, grad_norm=3.6801674365997314, loss=1.4822585582733154
I0216 07:12:39.688582 140047111603968 logging_writer.py:48] [21800] global_step=21800, grad_norm=2.202084541320801, loss=1.4528967142105103
I0216 07:13:54.733490 140047119996672 logging_writer.py:48] [21900] global_step=21900, grad_norm=3.79941725730896, loss=1.4918849468231201
I0216 07:15:09.854496 140047111603968 logging_writer.py:48] [22000] global_step=22000, grad_norm=2.8718979358673096, loss=1.5420171022415161
I0216 07:16:24.892287 140047119996672 logging_writer.py:48] [22100] global_step=22100, grad_norm=2.5508921146392822, loss=1.4981094598770142
I0216 07:17:39.994627 140047111603968 logging_writer.py:48] [22200] global_step=22200, grad_norm=2.700394630432129, loss=1.4390065670013428
I0216 07:18:55.658954 140047119996672 logging_writer.py:48] [22300] global_step=22300, grad_norm=3.3064324855804443, loss=1.5100117921829224
I0216 07:20:17.890466 140047111603968 logging_writer.py:48] [22400] global_step=22400, grad_norm=3.4273173809051514, loss=1.475449562072754
I0216 07:21:38.950368 140047119996672 logging_writer.py:48] [22500] global_step=22500, grad_norm=2.3057665824890137, loss=1.5145021677017212
I0216 07:21:51.305721 140202902193984 spec.py:321] Evaluating on the training split.
I0216 07:22:48.002082 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 07:23:38.766384 140202902193984 spec.py:349] Evaluating on the test split.
I0216 07:24:04.336976 140202902193984 submission_runner.py:408] Time since start: 19023.65s, 	Step: 22516, 	{'train/ctc_loss': Array(0.29839107, dtype=float32), 'train/wer': 0.10266235764850723, 'validation/ctc_loss': Array(0.59857935, dtype=float32), 'validation/wer': 0.17382237369300135, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34771985, dtype=float32), 'test/wer': 0.11266833221619646, 'test/num_examples': 2472, 'score': 17298.85318994522, 'total_duration': 19023.645206451416, 'accumulated_submission_time': 17298.85318994522, 'accumulated_eval_time': 1723.2054243087769, 'accumulated_logging_time': 0.5952105522155762}
I0216 07:24:04.373530 140047119996672 logging_writer.py:48] [22516] accumulated_eval_time=1723.205424, accumulated_logging_time=0.595211, accumulated_submission_time=17298.853190, global_step=22516, preemption_count=0, score=17298.853190, test/ctc_loss=0.34771984815597534, test/num_examples=2472, test/wer=0.112668, total_duration=19023.645206, train/ctc_loss=0.29839107394218445, train/wer=0.102662, validation/ctc_loss=0.5985793471336365, validation/num_examples=5348, validation/wer=0.173822
I0216 07:25:08.190677 140047111603968 logging_writer.py:48] [22600] global_step=22600, grad_norm=3.3824987411499023, loss=1.441119909286499
I0216 07:26:26.415411 140047119996672 logging_writer.py:48] [22700] global_step=22700, grad_norm=3.383296251296997, loss=1.5115745067596436
I0216 07:27:41.676247 140047111603968 logging_writer.py:48] [22800] global_step=22800, grad_norm=2.317615032196045, loss=1.4570772647857666
I0216 07:28:56.790517 140047119996672 logging_writer.py:48] [22900] global_step=22900, grad_norm=3.0338995456695557, loss=1.4679160118103027
I0216 07:30:11.934542 140047111603968 logging_writer.py:48] [23000] global_step=23000, grad_norm=2.8796918392181396, loss=1.472240686416626
I0216 07:31:27.168589 140047119996672 logging_writer.py:48] [23100] global_step=23100, grad_norm=3.4592111110687256, loss=1.4401582479476929
I0216 07:32:42.324615 140047111603968 logging_writer.py:48] [23200] global_step=23200, grad_norm=2.663550615310669, loss=1.4595755338668823
I0216 07:33:57.775491 140047119996672 logging_writer.py:48] [23300] global_step=23300, grad_norm=2.7668981552124023, loss=1.4714041948318481
I0216 07:35:19.977918 140047111603968 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.7452304363250732, loss=1.4636086225509644
I0216 07:36:43.129115 140047119996672 logging_writer.py:48] [23500] global_step=23500, grad_norm=3.055835485458374, loss=1.441452980041504
I0216 07:38:03.206039 140047111603968 logging_writer.py:48] [23600] global_step=23600, grad_norm=2.915909767150879, loss=1.4601293802261353
I0216 07:39:25.633144 140047119996672 logging_writer.py:48] [23700] global_step=23700, grad_norm=3.4262235164642334, loss=1.4267219305038452
I0216 07:40:40.723238 140047111603968 logging_writer.py:48] [23800] global_step=23800, grad_norm=2.2673816680908203, loss=1.4394856691360474
I0216 07:41:55.944934 140047119996672 logging_writer.py:48] [23900] global_step=23900, grad_norm=2.3265485763549805, loss=1.4553909301757812
I0216 07:43:11.088850 140047111603968 logging_writer.py:48] [24000] global_step=24000, grad_norm=2.722869634628296, loss=1.4654290676116943
I0216 07:44:26.172023 140047119996672 logging_writer.py:48] [24100] global_step=24100, grad_norm=3.036895751953125, loss=1.4638471603393555
I0216 07:45:41.133199 140047111603968 logging_writer.py:48] [24200] global_step=24200, grad_norm=3.6173043251037598, loss=1.4839953184127808
I0216 07:46:56.341752 140047119996672 logging_writer.py:48] [24300] global_step=24300, grad_norm=2.187685251235962, loss=1.4302111864089966
I0216 07:48:04.820659 140202902193984 spec.py:321] Evaluating on the training split.
I0216 07:48:59.898319 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 07:49:50.188552 140202902193984 spec.py:349] Evaluating on the test split.
I0216 07:50:15.948564 140202902193984 submission_runner.py:408] Time since start: 20595.26s, 	Step: 24387, 	{'train/ctc_loss': Array(0.31947687, dtype=float32), 'train/wer': 0.10229532956805684, 'validation/ctc_loss': Array(0.5687596, dtype=float32), 'validation/wer': 0.16620485242862798, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33003947, dtype=float32), 'test/wer': 0.10759043730830947, 'test/num_examples': 2472, 'score': 18739.204642295837, 'total_duration': 20595.2571952343, 'accumulated_submission_time': 18739.204642295837, 'accumulated_eval_time': 1854.3274881839752, 'accumulated_logging_time': 0.6483442783355713}
I0216 07:50:15.982079 140047119996672 logging_writer.py:48] [24387] accumulated_eval_time=1854.327488, accumulated_logging_time=0.648344, accumulated_submission_time=18739.204642, global_step=24387, preemption_count=0, score=18739.204642, test/ctc_loss=0.33003947138786316, test/num_examples=2472, test/wer=0.107590, total_duration=20595.257195, train/ctc_loss=0.3194768726825714, train/wer=0.102295, validation/ctc_loss=0.5687596201896667, validation/num_examples=5348, validation/wer=0.166205
I0216 07:50:26.573153 140047111603968 logging_writer.py:48] [24400] global_step=24400, grad_norm=3.1275482177734375, loss=1.4668015241622925
I0216 07:51:41.663926 140047119996672 logging_writer.py:48] [24500] global_step=24500, grad_norm=2.9907991886138916, loss=1.416213035583496
I0216 07:52:56.837824 140047111603968 logging_writer.py:48] [24600] global_step=24600, grad_norm=2.767535448074341, loss=1.5155565738677979
I0216 07:54:11.966499 140047119996672 logging_writer.py:48] [24700] global_step=24700, grad_norm=2.9945106506347656, loss=1.4380837678909302
I0216 07:55:30.473159 140047119996672 logging_writer.py:48] [24800] global_step=24800, grad_norm=2.481325149536133, loss=1.4658881425857544
I0216 07:56:45.498308 140047111603968 logging_writer.py:48] [24900] global_step=24900, grad_norm=2.0371580123901367, loss=1.4718859195709229
I0216 07:58:00.481593 140047119996672 logging_writer.py:48] [25000] global_step=25000, grad_norm=3.093231678009033, loss=1.4285268783569336
I0216 07:59:15.732843 140047111603968 logging_writer.py:48] [25100] global_step=25100, grad_norm=3.0423202514648438, loss=1.4581737518310547
I0216 08:00:30.835139 140047119996672 logging_writer.py:48] [25200] global_step=25200, grad_norm=2.642483711242676, loss=1.388021469116211
I0216 08:01:45.953212 140047111603968 logging_writer.py:48] [25300] global_step=25300, grad_norm=2.7490336894989014, loss=1.441826343536377
I0216 08:03:04.268423 140047119996672 logging_writer.py:48] [25400] global_step=25400, grad_norm=3.0091781616210938, loss=1.4295151233673096
I0216 08:04:24.952993 140047111603968 logging_writer.py:48] [25500] global_step=25500, grad_norm=2.694807529449463, loss=1.4661237001419067
I0216 08:05:45.218244 140047119996672 logging_writer.py:48] [25600] global_step=25600, grad_norm=2.0825414657592773, loss=1.371472954750061
I0216 08:07:07.109352 140047111603968 logging_writer.py:48] [25700] global_step=25700, grad_norm=2.1395881175994873, loss=1.4563920497894287
I0216 08:08:28.199252 140047119996672 logging_writer.py:48] [25800] global_step=25800, grad_norm=3.2591168880462646, loss=1.39774489402771
I0216 08:09:43.188602 140047111603968 logging_writer.py:48] [25900] global_step=25900, grad_norm=3.420454740524292, loss=1.3918547630310059
I0216 08:10:58.069396 140047119996672 logging_writer.py:48] [26000] global_step=26000, grad_norm=2.638076066970825, loss=1.4211446046829224
I0216 08:12:13.082321 140047111603968 logging_writer.py:48] [26100] global_step=26100, grad_norm=3.2205262184143066, loss=1.379347562789917
I0216 08:13:28.045434 140047119996672 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.6361379623413086, loss=1.3985564708709717
I0216 08:14:16.551528 140202902193984 spec.py:321] Evaluating on the training split.
I0216 08:15:13.082034 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 08:16:03.742582 140202902193984 spec.py:349] Evaluating on the test split.
I0216 08:16:29.149207 140202902193984 submission_runner.py:408] Time since start: 22168.46s, 	Step: 26266, 	{'train/ctc_loss': Array(0.23455991, dtype=float32), 'train/wer': 0.08128063662653567, 'validation/ctc_loss': Array(0.5503892, dtype=float32), 'validation/wer': 0.1592052289600973, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31949303, dtype=float32), 'test/wer': 0.1042187150894725, 'test/num_examples': 2472, 'score': 20179.67861032486, 'total_duration': 22168.45689558983, 'accumulated_submission_time': 20179.67861032486, 'accumulated_eval_time': 1986.9184362888336, 'accumulated_logging_time': 0.6978366374969482}
I0216 08:16:29.185974 140047119996672 logging_writer.py:48] [26266] accumulated_eval_time=1986.918436, accumulated_logging_time=0.697837, accumulated_submission_time=20179.678610, global_step=26266, preemption_count=0, score=20179.678610, test/ctc_loss=0.31949302554130554, test/num_examples=2472, test/wer=0.104219, total_duration=22168.456896, train/ctc_loss=0.23455990850925446, train/wer=0.081281, validation/ctc_loss=0.5503891706466675, validation/num_examples=5348, validation/wer=0.159205
I0216 08:16:55.491560 140047111603968 logging_writer.py:48] [26300] global_step=26300, grad_norm=3.200427293777466, loss=1.3848260641098022
I0216 08:18:10.630023 140047119996672 logging_writer.py:48] [26400] global_step=26400, grad_norm=3.410118341445923, loss=1.4076098203659058
I0216 08:19:25.614587 140047111603968 logging_writer.py:48] [26500] global_step=26500, grad_norm=2.381955623626709, loss=1.45856511592865
I0216 08:20:40.730558 140047119996672 logging_writer.py:48] [26600] global_step=26600, grad_norm=2.490818977355957, loss=1.3972971439361572
I0216 08:21:56.171032 140047111603968 logging_writer.py:48] [26700] global_step=26700, grad_norm=3.437255859375, loss=1.4200853109359741
I0216 08:23:14.721282 140047119996672 logging_writer.py:48] [26800] global_step=26800, grad_norm=2.3962998390197754, loss=1.4472638368606567
I0216 08:24:29.731830 140047111603968 logging_writer.py:48] [26900] global_step=26900, grad_norm=2.227243661880493, loss=1.3961093425750732
I0216 08:25:44.762212 140047119996672 logging_writer.py:48] [27000] global_step=27000, grad_norm=4.1194610595703125, loss=1.4536547660827637
I0216 08:26:59.802051 140047111603968 logging_writer.py:48] [27100] global_step=27100, grad_norm=2.740037202835083, loss=1.4159777164459229
I0216 08:28:14.969238 140047119996672 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.915854811668396, loss=1.44109308719635
I0216 08:29:29.870614 140047111603968 logging_writer.py:48] [27300] global_step=27300, grad_norm=2.0567989349365234, loss=1.424895167350769
I0216 08:30:45.673014 140047119996672 logging_writer.py:48] [27400] global_step=27400, grad_norm=2.3408455848693848, loss=1.3916507959365845
I0216 08:32:05.680391 140047111603968 logging_writer.py:48] [27500] global_step=27500, grad_norm=4.030029296875, loss=1.4211578369140625
I0216 08:33:25.949488 140047119996672 logging_writer.py:48] [27600] global_step=27600, grad_norm=2.7856481075286865, loss=1.3769490718841553
I0216 08:34:46.861221 140047111603968 logging_writer.py:48] [27700] global_step=27700, grad_norm=2.894570827484131, loss=1.3695459365844727
I0216 08:36:08.027674 140047119996672 logging_writer.py:48] [27800] global_step=27800, grad_norm=4.915823459625244, loss=1.398779034614563
I0216 08:37:26.459125 140047119996672 logging_writer.py:48] [27900] global_step=27900, grad_norm=3.065866470336914, loss=1.3671021461486816
I0216 08:38:41.525383 140047111603968 logging_writer.py:48] [28000] global_step=28000, grad_norm=2.804241895675659, loss=1.392899990081787
I0216 08:39:56.605766 140047119996672 logging_writer.py:48] [28100] global_step=28100, grad_norm=2.716506242752075, loss=1.3847800493240356
I0216 08:40:29.415516 140202902193984 spec.py:321] Evaluating on the training split.
I0216 08:41:24.676216 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 08:42:15.644026 140202902193984 spec.py:349] Evaluating on the test split.
I0216 08:42:41.107021 140202902193984 submission_runner.py:408] Time since start: 23740.42s, 	Step: 28145, 	{'train/ctc_loss': Array(0.24839616, dtype=float32), 'train/wer': 0.08341915550978372, 'validation/ctc_loss': Array(0.541494, dtype=float32), 'validation/wer': 0.15600953879722332, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30606657, dtype=float32), 'test/wer': 0.0993845591371641, 'test/num_examples': 2472, 'score': 21619.813318490982, 'total_duration': 23740.416083574295, 'accumulated_submission_time': 21619.813318490982, 'accumulated_eval_time': 2118.6045274734497, 'accumulated_logging_time': 0.7499384880065918}
I0216 08:42:41.139317 140047119996672 logging_writer.py:48] [28145] accumulated_eval_time=2118.604527, accumulated_logging_time=0.749938, accumulated_submission_time=21619.813318, global_step=28145, preemption_count=0, score=21619.813318, test/ctc_loss=0.3060665726661682, test/num_examples=2472, test/wer=0.099385, total_duration=23740.416084, train/ctc_loss=0.2483961582183838, train/wer=0.083419, validation/ctc_loss=0.5414940118789673, validation/num_examples=5348, validation/wer=0.156010
I0216 08:43:23.213722 140047111603968 logging_writer.py:48] [28200] global_step=28200, grad_norm=2.444661855697632, loss=1.356421947479248
I0216 08:44:38.325772 140047119996672 logging_writer.py:48] [28300] global_step=28300, grad_norm=2.7813689708709717, loss=1.3829796314239502
I0216 08:45:53.435894 140047111603968 logging_writer.py:48] [28400] global_step=28400, grad_norm=2.052067518234253, loss=1.3950649499893188
I0216 08:47:08.756759 140047119996672 logging_writer.py:48] [28500] global_step=28500, grad_norm=2.7865476608276367, loss=1.3957453966140747
I0216 08:48:23.826745 140047111603968 logging_writer.py:48] [28600] global_step=28600, grad_norm=3.0388264656066895, loss=1.3635691404342651
I0216 08:49:41.642717 140047119996672 logging_writer.py:48] [28700] global_step=28700, grad_norm=2.5676331520080566, loss=1.4063146114349365
I0216 08:51:02.092817 140047111603968 logging_writer.py:48] [28800] global_step=28800, grad_norm=3.2293508052825928, loss=1.380517601966858
I0216 08:52:22.953596 140047119996672 logging_writer.py:48] [28900] global_step=28900, grad_norm=4.847710132598877, loss=1.3585184812545776
I0216 08:53:37.986324 140047111603968 logging_writer.py:48] [29000] global_step=29000, grad_norm=2.9494879245758057, loss=1.3497713804244995
I0216 08:54:52.955943 140047119996672 logging_writer.py:48] [29100] global_step=29100, grad_norm=2.6705081462860107, loss=1.3868920803070068
I0216 08:56:08.213863 140047111603968 logging_writer.py:48] [29200] global_step=29200, grad_norm=2.192603826522827, loss=1.3870340585708618
I0216 08:57:23.568176 140047119996672 logging_writer.py:48] [29300] global_step=29300, grad_norm=3.786440849304199, loss=1.4366295337677002
I0216 08:58:38.786892 140047111603968 logging_writer.py:48] [29400] global_step=29400, grad_norm=2.2552638053894043, loss=1.3500885963439941
I0216 08:59:57.062593 140047119996672 logging_writer.py:48] [29500] global_step=29500, grad_norm=3.500340223312378, loss=1.3850347995758057
I0216 09:01:17.781252 140047111603968 logging_writer.py:48] [29600] global_step=29600, grad_norm=2.4157516956329346, loss=1.317444920539856
I0216 09:02:38.989233 140047119996672 logging_writer.py:48] [29700] global_step=29700, grad_norm=3.346256971359253, loss=1.3344473838806152
I0216 09:03:59.487683 140047111603968 logging_writer.py:48] [29800] global_step=29800, grad_norm=2.3025190830230713, loss=1.3078930377960205
I0216 09:05:21.078778 140047119996672 logging_writer.py:48] [29900] global_step=29900, grad_norm=2.2724897861480713, loss=1.3803519010543823
I0216 09:06:35.934145 140047111603968 logging_writer.py:48] [30000] global_step=30000, grad_norm=2.4272494316101074, loss=1.3151695728302002
I0216 09:06:41.791390 140202902193984 spec.py:321] Evaluating on the training split.
I0216 09:07:35.704564 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 09:08:26.793145 140202902193984 spec.py:349] Evaluating on the test split.
I0216 09:08:52.505824 140202902193984 submission_runner.py:408] Time since start: 25311.81s, 	Step: 30009, 	{'train/ctc_loss': Array(0.32168937, dtype=float32), 'train/wer': 0.10790995229406204, 'validation/ctc_loss': Array(0.52409434, dtype=float32), 'validation/wer': 0.15208974965484615, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.291443, dtype=float32), 'test/wer': 0.0950175695163813, 'test/num_examples': 2472, 'score': 23060.3713555336, 'total_duration': 25311.81352329254, 'accumulated_submission_time': 23060.3713555336, 'accumulated_eval_time': 2249.3122136592865, 'accumulated_logging_time': 0.7996718883514404}
I0216 09:08:52.543349 140047119996672 logging_writer.py:48] [30009] accumulated_eval_time=2249.312214, accumulated_logging_time=0.799672, accumulated_submission_time=23060.371356, global_step=30009, preemption_count=0, score=23060.371356, test/ctc_loss=0.29144299030303955, test/num_examples=2472, test/wer=0.095018, total_duration=25311.813523, train/ctc_loss=0.3216893672943115, train/wer=0.107910, validation/ctc_loss=0.5240943431854248, validation/num_examples=5348, validation/wer=0.152090
I0216 09:10:01.396770 140047111603968 logging_writer.py:48] [30100] global_step=30100, grad_norm=3.4616074562072754, loss=1.4025869369506836
I0216 09:11:16.552563 140047119996672 logging_writer.py:48] [30200] global_step=30200, grad_norm=2.062908887863159, loss=1.384800910949707
I0216 09:12:31.568381 140047111603968 logging_writer.py:48] [30300] global_step=30300, grad_norm=2.9004647731781006, loss=1.375548243522644
I0216 09:13:46.699243 140047119996672 logging_writer.py:48] [30400] global_step=30400, grad_norm=2.777698040008545, loss=1.371289849281311
I0216 09:15:02.361531 140047111603968 logging_writer.py:48] [30500] global_step=30500, grad_norm=3.3849968910217285, loss=1.3555514812469482
I0216 09:16:17.355313 140047119996672 logging_writer.py:48] [30600] global_step=30600, grad_norm=5.520185947418213, loss=1.2829409837722778
I0216 09:17:34.372445 140047111603968 logging_writer.py:48] [30700] global_step=30700, grad_norm=3.926359176635742, loss=1.3183571100234985
I0216 09:18:55.034395 140047119996672 logging_writer.py:48] [30800] global_step=30800, grad_norm=2.4621942043304443, loss=1.3408924341201782
I0216 09:20:19.730617 140047119996672 logging_writer.py:48] [30900] global_step=30900, grad_norm=3.6131908893585205, loss=1.324897289276123
I0216 09:21:34.824965 140047111603968 logging_writer.py:48] [31000] global_step=31000, grad_norm=3.0284595489501953, loss=1.3485623598098755
I0216 09:22:49.801107 140047119996672 logging_writer.py:48] [31100] global_step=31100, grad_norm=2.0810365676879883, loss=1.306272268295288
I0216 09:24:04.917179 140047111603968 logging_writer.py:48] [31200] global_step=31200, grad_norm=2.82149338722229, loss=1.3937790393829346
I0216 09:25:19.666821 140047119996672 logging_writer.py:48] [31300] global_step=31300, grad_norm=4.162432670593262, loss=1.3437824249267578
I0216 09:26:34.561462 140047111603968 logging_writer.py:48] [31400] global_step=31400, grad_norm=2.5739364624023438, loss=1.3256045579910278
I0216 09:27:51.599920 140047119996672 logging_writer.py:48] [31500] global_step=31500, grad_norm=2.5487725734710693, loss=1.3624866008758545
I0216 09:29:11.642735 140047111603968 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.8099737167358398, loss=1.303310513496399
I0216 09:30:32.295053 140047119996672 logging_writer.py:48] [31700] global_step=31700, grad_norm=2.8123350143432617, loss=1.2800240516662598
I0216 09:31:53.931049 140047111603968 logging_writer.py:48] [31800] global_step=31800, grad_norm=4.690214157104492, loss=1.3043181896209717
I0216 09:32:53.206047 140202902193984 spec.py:321] Evaluating on the training split.
I0216 09:33:47.218626 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 09:34:37.731769 140202902193984 spec.py:349] Evaluating on the test split.
I0216 09:35:03.358818 140202902193984 submission_runner.py:408] Time since start: 26882.67s, 	Step: 31874, 	{'train/ctc_loss': Array(0.33718112, dtype=float32), 'train/wer': 0.1116452902589013, 'validation/ctc_loss': Array(0.50746393, dtype=float32), 'validation/wer': 0.14898095136951253, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2857252, dtype=float32), 'test/wer': 0.09276298417727946, 'test/num_examples': 2472, 'score': 24500.939710855484, 'total_duration': 26882.66648888588, 'accumulated_submission_time': 24500.939710855484, 'accumulated_eval_time': 2379.4582090377808, 'accumulated_logging_time': 0.8533070087432861}
I0216 09:35:03.395557 140047119996672 logging_writer.py:48] [31874] accumulated_eval_time=2379.458209, accumulated_logging_time=0.853307, accumulated_submission_time=24500.939711, global_step=31874, preemption_count=0, score=24500.939711, test/ctc_loss=0.2857252061367035, test/num_examples=2472, test/wer=0.092763, total_duration=26882.666489, train/ctc_loss=0.33718112111091614, train/wer=0.111645, validation/ctc_loss=0.5074639320373535, validation/num_examples=5348, validation/wer=0.148981
I0216 09:35:23.803970 140047111603968 logging_writer.py:48] [31900] global_step=31900, grad_norm=3.0633440017700195, loss=1.3252018690109253
I0216 09:36:42.280634 140047119996672 logging_writer.py:48] [32000] global_step=32000, grad_norm=2.6634202003479004, loss=1.3383712768554688
I0216 09:37:57.367283 140047111603968 logging_writer.py:48] [32100] global_step=32100, grad_norm=3.847698926925659, loss=1.3680013418197632
I0216 09:39:12.619789 140047119996672 logging_writer.py:48] [32200] global_step=32200, grad_norm=3.4059853553771973, loss=1.3476804494857788
I0216 09:40:27.961217 140047111603968 logging_writer.py:48] [32300] global_step=32300, grad_norm=3.6199774742126465, loss=1.295335292816162
I0216 09:41:43.118523 140047119996672 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.9758726358413696, loss=1.2635852098464966
I0216 09:42:59.253755 140047111603968 logging_writer.py:48] [32500] global_step=32500, grad_norm=3.280345916748047, loss=1.335536003112793
I0216 09:44:19.751898 140047119996672 logging_writer.py:48] [32600] global_step=32600, grad_norm=2.168971538543701, loss=1.3070333003997803
I0216 09:45:40.098223 140047111603968 logging_writer.py:48] [32700] global_step=32700, grad_norm=2.432326555252075, loss=1.197555661201477
I0216 09:47:01.569930 140047119996672 logging_writer.py:48] [32800] global_step=32800, grad_norm=2.180725336074829, loss=1.2998902797698975
I0216 09:48:21.649748 140047111603968 logging_writer.py:48] [32900] global_step=32900, grad_norm=5.475536823272705, loss=1.3113058805465698
I0216 09:49:42.347520 140047119996672 logging_writer.py:48] [33000] global_step=33000, grad_norm=2.6104164123535156, loss=1.2411112785339355
I0216 09:50:57.443764 140047111603968 logging_writer.py:48] [33100] global_step=33100, grad_norm=2.6044509410858154, loss=1.2540501356124878
I0216 09:52:12.719937 140047119996672 logging_writer.py:48] [33200] global_step=33200, grad_norm=2.5826945304870605, loss=1.2984192371368408
I0216 09:53:27.891435 140047111603968 logging_writer.py:48] [33300] global_step=33300, grad_norm=3.0974280834198, loss=1.2472255229949951
I0216 09:54:42.943608 140047119996672 logging_writer.py:48] [33400] global_step=33400, grad_norm=2.3799889087677, loss=1.293994665145874
I0216 09:55:58.084095 140047111603968 logging_writer.py:48] [33500] global_step=33500, grad_norm=2.411041498184204, loss=1.2906898260116577
I0216 09:57:13.299438 140047119996672 logging_writer.py:48] [33600] global_step=33600, grad_norm=2.2991139888763428, loss=1.3057845830917358
I0216 09:58:32.902595 140047111603968 logging_writer.py:48] [33700] global_step=33700, grad_norm=3.236647367477417, loss=1.2236688137054443
I0216 09:59:04.064105 140202902193984 spec.py:321] Evaluating on the training split.
I0216 09:59:56.893200 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 10:00:47.302829 140202902193984 spec.py:349] Evaluating on the test split.
I0216 10:01:13.275670 140202902193984 submission_runner.py:408] Time since start: 28452.58s, 	Step: 33739, 	{'train/ctc_loss': Array(0.36467797, dtype=float32), 'train/wer': 0.12276081306849823, 'validation/ctc_loss': Array(0.487907, dtype=float32), 'validation/wer': 0.14117033704393833, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2731824, dtype=float32), 'test/wer': 0.08809132086202344, 'test/num_examples': 2472, 'score': 25941.51305270195, 'total_duration': 28452.583513498306, 'accumulated_submission_time': 25941.51305270195, 'accumulated_eval_time': 2508.6631367206573, 'accumulated_logging_time': 0.9063496589660645}
I0216 10:01:13.313895 140047119996672 logging_writer.py:48] [33739] accumulated_eval_time=2508.663137, accumulated_logging_time=0.906350, accumulated_submission_time=25941.513053, global_step=33739, preemption_count=0, score=25941.513053, test/ctc_loss=0.27318239212036133, test/num_examples=2472, test/wer=0.088091, total_duration=28452.583513, train/ctc_loss=0.36467796564102173, train/wer=0.122761, validation/ctc_loss=0.4879069924354553, validation/num_examples=5348, validation/wer=0.141170
I0216 10:01:59.848592 140047111603968 logging_writer.py:48] [33800] global_step=33800, grad_norm=2.5843043327331543, loss=1.2761152982711792
I0216 10:03:15.206453 140047119996672 logging_writer.py:48] [33900] global_step=33900, grad_norm=3.8021254539489746, loss=1.2396128177642822
I0216 10:04:33.782042 140047119996672 logging_writer.py:48] [34000] global_step=34000, grad_norm=2.983248472213745, loss=1.2792540788650513
I0216 10:05:48.835893 140047111603968 logging_writer.py:48] [34100] global_step=34100, grad_norm=3.985551118850708, loss=1.2632428407669067
I0216 10:07:04.084179 140047119996672 logging_writer.py:48] [34200] global_step=34200, grad_norm=2.142673969268799, loss=1.2933509349822998
I0216 10:08:18.922611 140047111603968 logging_writer.py:48] [34300] global_step=34300, grad_norm=2.831449270248413, loss=1.257222294807434
I0216 10:09:34.020690 140047119996672 logging_writer.py:48] [34400] global_step=34400, grad_norm=3.2667040824890137, loss=1.291656255722046
I0216 10:10:48.984380 140047111603968 logging_writer.py:48] [34500] global_step=34500, grad_norm=2.4288036823272705, loss=1.2378089427947998
I0216 10:12:03.983134 140047119996672 logging_writer.py:48] [34600] global_step=34600, grad_norm=5.359385013580322, loss=1.2811917066574097
I0216 10:13:21.246815 140047111603968 logging_writer.py:48] [34700] global_step=34700, grad_norm=3.958585500717163, loss=1.2756073474884033
I0216 10:14:41.433161 140047119996672 logging_writer.py:48] [34800] global_step=34800, grad_norm=2.8973636627197266, loss=1.3351290225982666
I0216 10:16:01.635356 140047111603968 logging_writer.py:48] [34900] global_step=34900, grad_norm=2.9946019649505615, loss=1.32278573513031
I0216 10:17:23.165228 140047119996672 logging_writer.py:48] [35000] global_step=35000, grad_norm=2.367129325866699, loss=1.2620391845703125
I0216 10:18:42.064320 140047119996672 logging_writer.py:48] [35100] global_step=35100, grad_norm=2.5153448581695557, loss=1.2402037382125854
I0216 10:19:57.382400 140047111603968 logging_writer.py:48] [35200] global_step=35200, grad_norm=4.056915283203125, loss=1.2512331008911133
I0216 10:21:12.476215 140047119996672 logging_writer.py:48] [35300] global_step=35300, grad_norm=2.843353509902954, loss=1.2875375747680664
I0216 10:22:27.396611 140047111603968 logging_writer.py:48] [35400] global_step=35400, grad_norm=2.539412021636963, loss=1.2257887125015259
I0216 10:23:42.649854 140047119996672 logging_writer.py:48] [35500] global_step=35500, grad_norm=3.046536684036255, loss=1.291700839996338
I0216 10:24:57.753165 140047111603968 logging_writer.py:48] [35600] global_step=35600, grad_norm=4.434329986572266, loss=1.2565135955810547
I0216 10:25:13.609494 140202902193984 spec.py:321] Evaluating on the training split.
I0216 10:26:06.778789 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 10:26:57.388804 140202902193984 spec.py:349] Evaluating on the test split.
I0216 10:27:22.977349 140202902193984 submission_runner.py:408] Time since start: 30022.29s, 	Step: 35622, 	{'train/ctc_loss': Array(0.30550236, dtype=float32), 'train/wer': 0.09948937668740462, 'validation/ctc_loss': Array(0.47197154, dtype=float32), 'validation/wer': 0.13709607345260047, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2628657, dtype=float32), 'test/wer': 0.08384620071902991, 'test/num_examples': 2472, 'score': 27381.709954977036, 'total_duration': 30022.285489320755, 'accumulated_submission_time': 27381.709954977036, 'accumulated_eval_time': 2638.0246579647064, 'accumulated_logging_time': 0.9644997119903564}
I0216 10:27:23.018979 140047119996672 logging_writer.py:48] [35622] accumulated_eval_time=2638.024658, accumulated_logging_time=0.964500, accumulated_submission_time=27381.709955, global_step=35622, preemption_count=0, score=27381.709955, test/ctc_loss=0.26286569237709045, test/num_examples=2472, test/wer=0.083846, total_duration=30022.285489, train/ctc_loss=0.30550235509872437, train/wer=0.099489, validation/ctc_loss=0.4719715416431427, validation/num_examples=5348, validation/wer=0.137096
I0216 10:28:22.355412 140047111603968 logging_writer.py:48] [35700] global_step=35700, grad_norm=2.6322743892669678, loss=1.2112525701522827
I0216 10:29:37.600666 140047119996672 logging_writer.py:48] [35800] global_step=35800, grad_norm=2.6559340953826904, loss=1.2468008995056152
I0216 10:30:52.731775 140047111603968 logging_writer.py:48] [35900] global_step=35900, grad_norm=2.556633234024048, loss=1.3005715608596802
I0216 10:32:07.735436 140047119996672 logging_writer.py:48] [36000] global_step=36000, grad_norm=2.768602132797241, loss=1.3127659559249878
I0216 10:33:26.254953 140047119996672 logging_writer.py:48] [36100] global_step=36100, grad_norm=2.9541516304016113, loss=1.2787959575653076
I0216 10:34:41.606271 140047111603968 logging_writer.py:48] [36200] global_step=36200, grad_norm=2.6299500465393066, loss=1.244128704071045
I0216 10:35:56.763801 140047119996672 logging_writer.py:48] [36300] global_step=36300, grad_norm=3.187394380569458, loss=1.2422089576721191
I0216 10:37:11.943284 140047111603968 logging_writer.py:48] [36400] global_step=36400, grad_norm=2.2400362491607666, loss=1.2208034992218018
I0216 10:38:27.101528 140047119996672 logging_writer.py:48] [36500] global_step=36500, grad_norm=2.7148146629333496, loss=1.2279020547866821
I0216 10:39:42.015118 140047111603968 logging_writer.py:48] [36600] global_step=36600, grad_norm=2.501037836074829, loss=1.2616018056869507
I0216 10:40:59.531849 140047119996672 logging_writer.py:48] [36700] global_step=36700, grad_norm=2.9595768451690674, loss=1.2076274156570435
I0216 10:42:23.289539 140047111603968 logging_writer.py:48] [36800] global_step=36800, grad_norm=2.2660720348358154, loss=1.256127953529358
I0216 10:43:44.259216 140047119996672 logging_writer.py:48] [36900] global_step=36900, grad_norm=2.692246675491333, loss=1.2285523414611816
I0216 10:45:05.927724 140047111603968 logging_writer.py:48] [37000] global_step=37000, grad_norm=3.1740305423736572, loss=1.2034085988998413
I0216 10:46:28.798892 140047119996672 logging_writer.py:48] [37100] global_step=37100, grad_norm=3.0737364292144775, loss=1.2570180892944336
I0216 10:47:43.938579 140047111603968 logging_writer.py:48] [37200] global_step=37200, grad_norm=2.4723339080810547, loss=1.3239928483963013
I0216 10:48:59.005615 140047119996672 logging_writer.py:48] [37300] global_step=37300, grad_norm=2.8484671115875244, loss=1.2451716661453247
I0216 10:50:14.189683 140047111603968 logging_writer.py:48] [37400] global_step=37400, grad_norm=2.527015447616577, loss=1.1976637840270996
I0216 10:51:23.127306 140202902193984 spec.py:321] Evaluating on the training split.
I0216 10:52:16.609830 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 10:53:07.987786 140202902193984 spec.py:349] Evaluating on the test split.
I0216 10:53:33.906468 140202902193984 submission_runner.py:408] Time since start: 31593.21s, 	Step: 37493, 	{'train/ctc_loss': Array(0.2715685, dtype=float32), 'train/wer': 0.0926955085160065, 'validation/ctc_loss': Array(0.45568103, dtype=float32), 'validation/wer': 0.131959798024658, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25255555, dtype=float32), 'test/wer': 0.08031198586314058, 'test/num_examples': 2472, 'score': 28821.724100589752, 'total_duration': 31593.214646816254, 'accumulated_submission_time': 28821.724100589752, 'accumulated_eval_time': 2768.7975289821625, 'accumulated_logging_time': 1.0214431285858154}
I0216 10:53:33.946025 140047119996672 logging_writer.py:48] [37493] accumulated_eval_time=2768.797529, accumulated_logging_time=1.021443, accumulated_submission_time=28821.724101, global_step=37493, preemption_count=0, score=28821.724101, test/ctc_loss=0.2525555491447449, test/num_examples=2472, test/wer=0.080312, total_duration=31593.214647, train/ctc_loss=0.27156850695610046, train/wer=0.092696, validation/ctc_loss=0.4556810259819031, validation/num_examples=5348, validation/wer=0.131960
I0216 10:53:40.133772 140047111603968 logging_writer.py:48] [37500] global_step=37500, grad_norm=2.967374086380005, loss=1.2279455661773682
I0216 10:54:55.534972 140047119996672 logging_writer.py:48] [37600] global_step=37600, grad_norm=2.096520185470581, loss=1.2118561267852783
I0216 10:56:10.798301 140047111603968 logging_writer.py:48] [37700] global_step=37700, grad_norm=3.030222177505493, loss=1.2678216695785522
I0216 10:57:26.595437 140047119996672 logging_writer.py:48] [37800] global_step=37800, grad_norm=3.321002244949341, loss=1.211503028869629
I0216 10:58:41.745655 140047111603968 logging_writer.py:48] [37900] global_step=37900, grad_norm=3.39408278465271, loss=1.238712191581726
I0216 10:59:56.943478 140047119996672 logging_writer.py:48] [38000] global_step=38000, grad_norm=4.072559356689453, loss=1.22328519821167
I0216 11:01:15.105108 140047111603968 logging_writer.py:48] [38100] global_step=38100, grad_norm=3.449681282043457, loss=1.2294974327087402
I0216 11:02:33.414698 140047119996672 logging_writer.py:48] [38200] global_step=38200, grad_norm=3.4256865978240967, loss=1.2195829153060913
I0216 11:03:48.639596 140047111603968 logging_writer.py:48] [38300] global_step=38300, grad_norm=2.689713716506958, loss=1.2221778631210327
I0216 11:05:03.528166 140047119996672 logging_writer.py:48] [38400] global_step=38400, grad_norm=2.8846397399902344, loss=1.218459963798523
I0216 11:06:18.397656 140047111603968 logging_writer.py:48] [38500] global_step=38500, grad_norm=2.132789373397827, loss=1.2606267929077148
I0216 11:07:33.330214 140047119996672 logging_writer.py:48] [38600] global_step=38600, grad_norm=2.793877601623535, loss=1.2029491662979126
I0216 11:08:48.514031 140047111603968 logging_writer.py:48] [38700] global_step=38700, grad_norm=2.950535774230957, loss=1.2064615488052368
I0216 11:10:08.397691 140047119996672 logging_writer.py:48] [38800] global_step=38800, grad_norm=3.438631296157837, loss=1.2160800695419312
I0216 11:11:29.130811 140047111603968 logging_writer.py:48] [38900] global_step=38900, grad_norm=4.550838947296143, loss=1.1539661884307861
I0216 11:12:49.899188 140047119996672 logging_writer.py:48] [39000] global_step=39000, grad_norm=3.106680154800415, loss=1.2640460729599
I0216 11:14:12.128475 140047111603968 logging_writer.py:48] [39100] global_step=39100, grad_norm=3.0245351791381836, loss=1.2054142951965332
I0216 11:15:32.326682 140047119996672 logging_writer.py:48] [39200] global_step=39200, grad_norm=2.5265560150146484, loss=1.2375397682189941
I0216 11:16:47.426430 140047111603968 logging_writer.py:48] [39300] global_step=39300, grad_norm=2.078232765197754, loss=1.1894584894180298
I0216 11:17:34.639039 140202902193984 spec.py:321] Evaluating on the training split.
I0216 11:18:29.988510 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 11:19:20.693596 140202902193984 spec.py:349] Evaluating on the test split.
I0216 11:19:46.402291 140202902193984 submission_runner.py:408] Time since start: 33165.71s, 	Step: 39364, 	{'train/ctc_loss': Array(0.22365406, dtype=float32), 'train/wer': 0.07677500635330886, 'validation/ctc_loss': Array(0.44460142, dtype=float32), 'validation/wer': 0.12757658553539877, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24169599, dtype=float32), 'test/wer': 0.07830113947961732, 'test/num_examples': 2472, 'score': 30262.321855068207, 'total_duration': 33165.71136927605, 'accumulated_submission_time': 30262.321855068207, 'accumulated_eval_time': 2900.555379629135, 'accumulated_logging_time': 1.07637619972229}
I0216 11:19:46.442075 140047119996672 logging_writer.py:48] [39364] accumulated_eval_time=2900.555380, accumulated_logging_time=1.076376, accumulated_submission_time=30262.321855, global_step=39364, preemption_count=0, score=30262.321855, test/ctc_loss=0.24169598519802094, test/num_examples=2472, test/wer=0.078301, total_duration=33165.711369, train/ctc_loss=0.22365406155586243, train/wer=0.076775, validation/ctc_loss=0.4446014165878296, validation/num_examples=5348, validation/wer=0.127577
I0216 11:20:14.364829 140047111603968 logging_writer.py:48] [39400] global_step=39400, grad_norm=2.427227020263672, loss=1.1647287607192993
I0216 11:21:29.500021 140047119996672 logging_writer.py:48] [39500] global_step=39500, grad_norm=2.861052989959717, loss=1.205926537513733
I0216 11:22:44.605807 140047111603968 logging_writer.py:48] [39600] global_step=39600, grad_norm=2.6611130237579346, loss=1.183737874031067
I0216 11:23:59.799198 140047119996672 logging_writer.py:48] [39700] global_step=39700, grad_norm=2.8231704235076904, loss=1.1856900453567505
I0216 11:25:14.990466 140047111603968 logging_writer.py:48] [39800] global_step=39800, grad_norm=2.906714677810669, loss=1.177375078201294
I0216 11:26:30.244642 140047119996672 logging_writer.py:48] [39900] global_step=39900, grad_norm=4.395349025726318, loss=1.2091710567474365
I0216 11:27:45.923300 140047111603968 logging_writer.py:48] [40000] global_step=40000, grad_norm=3.1083779335021973, loss=1.1416345834732056
I0216 11:29:07.097646 140047119996672 logging_writer.py:48] [40100] global_step=40100, grad_norm=2.3630032539367676, loss=1.1326388120651245
I0216 11:30:29.124226 140047119996672 logging_writer.py:48] [40200] global_step=40200, grad_norm=4.342706680297852, loss=1.1885768175125122
I0216 11:31:44.090968 140047111603968 logging_writer.py:48] [40300] global_step=40300, grad_norm=4.190025329589844, loss=1.1414780616760254
I0216 11:32:59.056414 140047119996672 logging_writer.py:48] [40400] global_step=40400, grad_norm=2.5579118728637695, loss=1.2341829538345337
I0216 11:34:14.244529 140047111603968 logging_writer.py:48] [40500] global_step=40500, grad_norm=2.1554548740386963, loss=1.2597359418869019
I0216 11:35:29.134078 140047119996672 logging_writer.py:48] [40600] global_step=40600, grad_norm=3.3908796310424805, loss=1.1767613887786865
I0216 11:36:44.518770 140047111603968 logging_writer.py:48] [40700] global_step=40700, grad_norm=2.048860549926758, loss=1.1468451023101807
I0216 11:37:59.806396 140047119996672 logging_writer.py:48] [40800] global_step=40800, grad_norm=2.4770009517669678, loss=1.1663025617599487
I0216 11:39:19.285541 140047111603968 logging_writer.py:48] [40900] global_step=40900, grad_norm=2.947979688644409, loss=1.1823241710662842
I0216 11:40:40.168832 140047119996672 logging_writer.py:48] [41000] global_step=41000, grad_norm=4.688105583190918, loss=1.1889779567718506
I0216 11:42:01.591699 140047111603968 logging_writer.py:48] [41100] global_step=41100, grad_norm=4.014047622680664, loss=1.2021307945251465
I0216 11:43:25.260318 140047119996672 logging_writer.py:48] [41200] global_step=41200, grad_norm=3.8564374446868896, loss=1.1741364002227783
I0216 11:43:46.922070 140202902193984 spec.py:321] Evaluating on the training split.
I0216 11:44:41.888456 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 11:45:32.809075 140202902193984 spec.py:349] Evaluating on the test split.
I0216 11:45:58.244700 140202902193984 submission_runner.py:408] Time since start: 34737.55s, 	Step: 41230, 	{'train/ctc_loss': Array(0.25303861, dtype=float32), 'train/wer': 0.08619467289108004, 'validation/ctc_loss': Array(0.429852, dtype=float32), 'validation/wer': 0.12485397337246686, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23356742, dtype=float32), 'test/wer': 0.07413726565515, 'test/num_examples': 2472, 'score': 31702.706634283066, 'total_duration': 34737.552344322205, 'accumulated_submission_time': 31702.706634283066, 'accumulated_eval_time': 3031.871179819107, 'accumulated_logging_time': 1.132845163345337}
I0216 11:45:58.283996 140047119996672 logging_writer.py:48] [41230] accumulated_eval_time=3031.871180, accumulated_logging_time=1.132845, accumulated_submission_time=31702.706634, global_step=41230, preemption_count=0, score=31702.706634, test/ctc_loss=0.23356741666793823, test/num_examples=2472, test/wer=0.074137, total_duration=34737.552344, train/ctc_loss=0.253038614988327, train/wer=0.086195, validation/ctc_loss=0.4298520088195801, validation/num_examples=5348, validation/wer=0.124854
I0216 11:46:51.545914 140047111603968 logging_writer.py:48] [41300] global_step=41300, grad_norm=3.188448429107666, loss=1.219914436340332
I0216 11:48:06.926740 140047119996672 logging_writer.py:48] [41400] global_step=41400, grad_norm=3.8731701374053955, loss=1.1767566204071045
I0216 11:49:22.168062 140047111603968 logging_writer.py:48] [41500] global_step=41500, grad_norm=3.483654737472534, loss=1.179512619972229
I0216 11:50:37.190198 140047119996672 logging_writer.py:48] [41600] global_step=41600, grad_norm=2.989647388458252, loss=1.1430946588516235
I0216 11:51:52.248751 140047111603968 logging_writer.py:48] [41700] global_step=41700, grad_norm=2.673661708831787, loss=1.1383596658706665
I0216 11:53:07.760032 140047119996672 logging_writer.py:48] [41800] global_step=41800, grad_norm=2.2668566703796387, loss=1.1318442821502686
I0216 11:54:22.938053 140047111603968 logging_writer.py:48] [41900] global_step=41900, grad_norm=3.3534603118896484, loss=1.232078194618225
I0216 11:55:38.069360 140047119996672 logging_writer.py:48] [42000] global_step=42000, grad_norm=2.7443273067474365, loss=1.1352273225784302
I0216 11:56:56.149806 140047111603968 logging_writer.py:48] [42100] global_step=42100, grad_norm=2.450805425643921, loss=1.143005132675171
I0216 11:58:16.226331 140047119996672 logging_writer.py:48] [42200] global_step=42200, grad_norm=3.6166343688964844, loss=1.1496021747589111
I0216 11:59:35.713787 140047119996672 logging_writer.py:48] [42300] global_step=42300, grad_norm=3.5115244388580322, loss=1.1506478786468506
I0216 12:00:51.083299 140047111603968 logging_writer.py:48] [42400] global_step=42400, grad_norm=2.9496681690216064, loss=1.1889640092849731
I0216 12:02:06.314537 140047119996672 logging_writer.py:48] [42500] global_step=42500, grad_norm=3.118807315826416, loss=1.1667319536209106
I0216 12:03:21.244259 140047111603968 logging_writer.py:48] [42600] global_step=42600, grad_norm=2.9319796562194824, loss=1.1637969017028809
I0216 12:04:36.395232 140047119996672 logging_writer.py:48] [42700] global_step=42700, grad_norm=3.067664384841919, loss=1.1660370826721191
I0216 12:05:51.545938 140047111603968 logging_writer.py:48] [42800] global_step=42800, grad_norm=3.0847697257995605, loss=1.1720163822174072
I0216 12:07:07.253177 140047119996672 logging_writer.py:48] [42900] global_step=42900, grad_norm=2.730341672897339, loss=1.133225679397583
I0216 12:08:27.686931 140047111603968 logging_writer.py:48] [43000] global_step=43000, grad_norm=2.8107826709747314, loss=1.1522725820541382
I0216 12:09:49.001527 140047119996672 logging_writer.py:48] [43100] global_step=43100, grad_norm=2.7291910648345947, loss=1.1460204124450684
I0216 12:09:58.556606 140202902193984 spec.py:321] Evaluating on the training split.
I0216 12:10:53.899955 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 12:11:44.642334 140202902193984 spec.py:349] Evaluating on the test split.
I0216 12:12:10.142124 140202902193984 submission_runner.py:408] Time since start: 36309.45s, 	Step: 43113, 	{'train/ctc_loss': Array(0.21915351, dtype=float32), 'train/wer': 0.0752219014009197, 'validation/ctc_loss': Array(0.42223984, dtype=float32), 'validation/wer': 0.1220541239850546, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22784421, dtype=float32), 'test/wer': 0.07338573720878273, 'test/num_examples': 2472, 'score': 33142.88335800171, 'total_duration': 36309.45052433014, 'accumulated_submission_time': 33142.88335800171, 'accumulated_eval_time': 3163.4506227970123, 'accumulated_logging_time': 1.1886072158813477}
I0216 12:12:10.180840 140047119996672 logging_writer.py:48] [43113] accumulated_eval_time=3163.450623, accumulated_logging_time=1.188607, accumulated_submission_time=33142.883358, global_step=43113, preemption_count=0, score=33142.883358, test/ctc_loss=0.2278442084789276, test/num_examples=2472, test/wer=0.073386, total_duration=36309.450524, train/ctc_loss=0.2191535085439682, train/wer=0.075222, validation/ctc_loss=0.42223984003067017, validation/num_examples=5348, validation/wer=0.122054
I0216 12:13:16.338829 140047111603968 logging_writer.py:48] [43200] global_step=43200, grad_norm=4.477978706359863, loss=1.1999781131744385
I0216 12:14:34.951977 140047119996672 logging_writer.py:48] [43300] global_step=43300, grad_norm=2.8841536045074463, loss=1.1370670795440674
I0216 12:15:49.985702 140047111603968 logging_writer.py:48] [43400] global_step=43400, grad_norm=3.9104256629943848, loss=1.1495972871780396
I0216 12:17:05.234865 140047119996672 logging_writer.py:48] [43500] global_step=43500, grad_norm=3.052915334701538, loss=1.1866202354431152
I0216 12:18:20.557373 140047111603968 logging_writer.py:48] [43600] global_step=43600, grad_norm=2.1961605548858643, loss=1.102859616279602
I0216 12:19:35.852975 140047119996672 logging_writer.py:48] [43700] global_step=43700, grad_norm=3.971996784210205, loss=1.1772512197494507
I0216 12:20:51.132914 140047111603968 logging_writer.py:48] [43800] global_step=43800, grad_norm=2.649933338165283, loss=1.144453763961792
I0216 12:22:07.382650 140047119996672 logging_writer.py:48] [43900] global_step=43900, grad_norm=2.652928113937378, loss=1.1458059549331665
I0216 12:23:29.130577 140047111603968 logging_writer.py:48] [44000] global_step=44000, grad_norm=2.231334686279297, loss=1.166685700416565
I0216 12:24:50.596394 140047119996672 logging_writer.py:48] [44100] global_step=44100, grad_norm=4.334742546081543, loss=1.1168386936187744
I0216 12:26:11.071050 140047111603968 logging_writer.py:48] [44200] global_step=44200, grad_norm=2.4442338943481445, loss=1.1811002492904663
I0216 12:27:34.292420 140047119996672 logging_writer.py:48] [44300] global_step=44300, grad_norm=3.3667526245117188, loss=1.0849984884262085
I0216 12:28:49.339519 140047111603968 logging_writer.py:48] [44400] global_step=44400, grad_norm=4.704137802124023, loss=1.156839370727539
I0216 12:30:04.586237 140047119996672 logging_writer.py:48] [44500] global_step=44500, grad_norm=3.2890708446502686, loss=1.1693344116210938
I0216 12:31:19.760615 140047111603968 logging_writer.py:48] [44600] global_step=44600, grad_norm=2.6427829265594482, loss=1.1280839443206787
I0216 12:32:35.018903 140047119996672 logging_writer.py:48] [44700] global_step=44700, grad_norm=3.980417013168335, loss=1.150315761566162
I0216 12:33:50.053694 140047111603968 logging_writer.py:48] [44800] global_step=44800, grad_norm=2.6039962768554688, loss=1.2144277095794678
I0216 12:35:05.178379 140047119996672 logging_writer.py:48] [44900] global_step=44900, grad_norm=2.742647409439087, loss=1.1149975061416626
I0216 12:36:10.898092 140202902193984 spec.py:321] Evaluating on the training split.
I0216 12:37:05.644340 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 12:37:56.382745 140202902193984 spec.py:349] Evaluating on the test split.
I0216 12:38:21.723669 140202902193984 submission_runner.py:408] Time since start: 37881.03s, 	Step: 44985, 	{'train/ctc_loss': Array(0.21478947, dtype=float32), 'train/wer': 0.07625128710821516, 'validation/ctc_loss': Array(0.41844097, dtype=float32), 'validation/wer': 0.120547998107688, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22397295, dtype=float32), 'test/wer': 0.07241078138646842, 'test/num_examples': 2472, 'score': 34583.50731277466, 'total_duration': 37881.03183031082, 'accumulated_submission_time': 34583.50731277466, 'accumulated_eval_time': 3294.2698872089386, 'accumulated_logging_time': 1.2424731254577637}
I0216 12:38:21.762042 140047119996672 logging_writer.py:48] [44985] accumulated_eval_time=3294.269887, accumulated_logging_time=1.242473, accumulated_submission_time=34583.507313, global_step=44985, preemption_count=0, score=34583.507313, test/ctc_loss=0.22397294640541077, test/num_examples=2472, test/wer=0.072411, total_duration=37881.031830, train/ctc_loss=0.2147894650697708, train/wer=0.076251, validation/ctc_loss=0.41844096779823303, validation/num_examples=5348, validation/wer=0.120548
I0216 12:38:33.910892 140047111603968 logging_writer.py:48] [45000] global_step=45000, grad_norm=3.405989408493042, loss=1.1652474403381348
I0216 12:39:49.357712 140047119996672 logging_writer.py:48] [45100] global_step=45100, grad_norm=2.837587356567383, loss=1.1296145915985107
I0216 12:41:04.542585 140047111603968 logging_writer.py:48] [45200] global_step=45200, grad_norm=4.4966630935668945, loss=1.1822468042373657
I0216 12:42:19.537944 140047119996672 logging_writer.py:48] [45300] global_step=45300, grad_norm=2.5074663162231445, loss=1.2081871032714844
I0216 12:43:38.124770 140047119996672 logging_writer.py:48] [45400] global_step=45400, grad_norm=2.335813045501709, loss=1.1811209917068481
I0216 12:44:53.422341 140047111603968 logging_writer.py:48] [45500] global_step=45500, grad_norm=2.8802995681762695, loss=1.1240026950836182
I0216 12:46:08.534935 140047119996672 logging_writer.py:48] [45600] global_step=45600, grad_norm=2.3207528591156006, loss=1.0935050249099731
I0216 12:47:23.814619 140047111603968 logging_writer.py:48] [45700] global_step=45700, grad_norm=3.427905559539795, loss=1.2111706733703613
I0216 12:48:38.766589 140047119996672 logging_writer.py:48] [45800] global_step=45800, grad_norm=3.325856924057007, loss=1.1288247108459473
I0216 12:49:53.890153 140047111603968 logging_writer.py:48] [45900] global_step=45900, grad_norm=2.3865764141082764, loss=1.1223753690719604
I0216 12:51:10.720195 140047119996672 logging_writer.py:48] [46000] global_step=46000, grad_norm=3.305598497390747, loss=1.1486330032348633
I0216 12:52:32.326812 140047111603968 logging_writer.py:48] [46100] global_step=46100, grad_norm=2.9608824253082275, loss=1.1526180505752563
I0216 12:53:53.268942 140047119996672 logging_writer.py:48] [46200] global_step=46200, grad_norm=4.784359931945801, loss=1.0880463123321533
I0216 12:55:14.264022 140047111603968 logging_writer.py:48] [46300] global_step=46300, grad_norm=2.5356290340423584, loss=1.1140400171279907
I0216 12:56:34.771856 140047119996672 logging_writer.py:48] [46400] global_step=46400, grad_norm=3.5085935592651367, loss=1.132289171218872
I0216 12:57:49.907207 140047111603968 logging_writer.py:48] [46500] global_step=46500, grad_norm=2.5743396282196045, loss=1.1480896472930908
I0216 12:59:05.078057 140047119996672 logging_writer.py:48] [46600] global_step=46600, grad_norm=4.4819464683532715, loss=1.1017568111419678
I0216 13:00:20.071563 140047111603968 logging_writer.py:48] [46700] global_step=46700, grad_norm=5.005669116973877, loss=1.1293312311172485
I0216 13:01:35.202570 140047119996672 logging_writer.py:48] [46800] global_step=46800, grad_norm=2.6807892322540283, loss=1.0942147970199585
I0216 13:02:22.384976 140202902193984 spec.py:321] Evaluating on the training split.
I0216 13:03:17.008602 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 13:04:07.797741 140202902193984 spec.py:349] Evaluating on the test split.
I0216 13:04:33.617130 140202902193984 submission_runner.py:408] Time since start: 39452.93s, 	Step: 46864, 	{'train/ctc_loss': Array(0.21869154, dtype=float32), 'train/wer': 0.07526009406141398, 'validation/ctc_loss': Array(0.41614845, dtype=float32), 'validation/wer': 0.11984320843430492, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22283582, dtype=float32), 'test/wer': 0.07188268031604818, 'test/num_examples': 2472, 'score': 36024.03443598747, 'total_duration': 39452.92549777031, 'accumulated_submission_time': 36024.03443598747, 'accumulated_eval_time': 3425.495941877365, 'accumulated_logging_time': 1.2960054874420166}
I0216 13:04:33.655272 140047119996672 logging_writer.py:48] [46864] accumulated_eval_time=3425.495942, accumulated_logging_time=1.296005, accumulated_submission_time=36024.034436, global_step=46864, preemption_count=0, score=36024.034436, test/ctc_loss=0.22283582389354706, test/num_examples=2472, test/wer=0.071883, total_duration=39452.925498, train/ctc_loss=0.21869154274463654, train/wer=0.075260, validation/ctc_loss=0.41614845395088196, validation/num_examples=5348, validation/wer=0.119843
I0216 13:05:01.511808 140047111603968 logging_writer.py:48] [46900] global_step=46900, grad_norm=2.5866479873657227, loss=1.1528311967849731
I0216 13:06:16.783467 140047119996672 logging_writer.py:48] [47000] global_step=47000, grad_norm=3.1750712394714355, loss=1.1581496000289917
I0216 13:07:32.072280 140047111603968 logging_writer.py:48] [47100] global_step=47100, grad_norm=3.1502845287323, loss=1.1074352264404297
I0216 13:08:47.337923 140047119996672 logging_writer.py:48] [47200] global_step=47200, grad_norm=4.222103118896484, loss=1.156401515007019
I0216 13:10:02.875128 140047111603968 logging_writer.py:48] [47300] global_step=47300, grad_norm=4.327976703643799, loss=1.1599235534667969
I0216 13:11:23.109957 140047119996672 logging_writer.py:48] [47400] global_step=47400, grad_norm=2.7730636596679688, loss=1.0875991582870483
I0216 13:12:38.232027 140047111603968 logging_writer.py:48] [47500] global_step=47500, grad_norm=2.6101796627044678, loss=1.1374001502990723
I0216 13:13:53.505837 140047119996672 logging_writer.py:48] [47600] global_step=47600, grad_norm=3.0916614532470703, loss=1.0945446491241455
I0216 13:15:08.582747 140047111603968 logging_writer.py:48] [47700] global_step=47700, grad_norm=2.1200759410858154, loss=1.1368249654769897
I0216 13:16:23.711044 140047119996672 logging_writer.py:48] [47800] global_step=47800, grad_norm=3.2265405654907227, loss=1.0762394666671753
I0216 13:17:38.808454 140047111603968 logging_writer.py:48] [47900] global_step=47900, grad_norm=2.7727975845336914, loss=1.170803189277649
I0216 13:18:53.297540 140202902193984 spec.py:321] Evaluating on the training split.
I0216 13:19:47.290432 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 13:20:37.613795 140202902193984 spec.py:349] Evaluating on the test split.
I0216 13:21:03.466508 140202902193984 submission_runner.py:408] Time since start: 40442.78s, 	Step: 48000, 	{'train/ctc_loss': Array(0.23044662, dtype=float32), 'train/wer': 0.07937063940968733, 'validation/ctc_loss': Array(0.4161281, dtype=float32), 'validation/wer': 0.11978528051594466, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22271666, dtype=float32), 'test/wer': 0.0716795645197327, 'test/num_examples': 2472, 'score': 36883.611184597015, 'total_duration': 40442.77611327171, 'accumulated_submission_time': 36883.611184597015, 'accumulated_eval_time': 3555.660044193268, 'accumulated_logging_time': 1.351287841796875}
I0216 13:21:03.503847 140047119996672 logging_writer.py:48] [48000] accumulated_eval_time=3555.660044, accumulated_logging_time=1.351288, accumulated_submission_time=36883.611185, global_step=48000, preemption_count=0, score=36883.611185, test/ctc_loss=0.22271665930747986, test/num_examples=2472, test/wer=0.071680, total_duration=40442.776113, train/ctc_loss=0.23044662177562714, train/wer=0.079371, validation/ctc_loss=0.41612809896469116, validation/num_examples=5348, validation/wer=0.119785
I0216 13:21:03.530739 140047111603968 logging_writer.py:48] [48000] global_step=48000, preemption_count=0, score=36883.611185
I0216 13:21:03.705901 140202902193984 checkpoints.py:490] Saving checkpoint at step: 48000
I0216 13:21:04.705970 140202902193984 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_3/librispeech_deepspeech_jax/trial_3/checkpoint_48000
I0216 13:21:04.725676 140202902193984 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_3/librispeech_deepspeech_jax/trial_3/checkpoint_48000.
I0216 13:21:05.826509 140202902193984 submission_runner.py:583] Tuning trial 3/5
I0216 13:21:05.826766 140202902193984 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0216 13:21:05.839557 140202902193984 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(29.73173, dtype=float32), 'train/wer': 3.3317589058524173, 'validation/ctc_loss': Array(30.35123, dtype=float32), 'validation/wer': 3.0462844067698427, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.536522, dtype=float32), 'test/wer': 3.3616273637600798, 'test/num_examples': 2472, 'score': 14.655245780944824, 'total_duration': 197.91219639778137, 'accumulated_submission_time': 14.655245780944824, 'accumulated_eval_time': 183.25687742233276, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1870, {'train/ctc_loss': Array(6.4368887, dtype=float32), 'train/wer': 0.9437977009574123, 'validation/ctc_loss': Array(6.3937473, dtype=float32), 'validation/wer': 0.8966083203800072, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.3189464, dtype=float32), 'test/wer': 0.8995592387219954, 'test/num_examples': 2472, 'score': 1454.732485294342, 'total_duration': 1744.4271211624146, 'accumulated_submission_time': 1454.732485294342, 'accumulated_eval_time': 289.58756613731384, 'accumulated_logging_time': 0.029394865036010742, 'global_step': 1870, 'preemption_count': 0}), (3751, {'train/ctc_loss': Array(3.090075, dtype=float32), 'train/wer': 0.6942303997950864, 'validation/ctc_loss': Array(3.3290048, dtype=float32), 'validation/wer': 0.7045386524035259, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.8410707, dtype=float32), 'test/wer': 0.6370523835638697, 'test/num_examples': 2472, 'score': 2895.4914994239807, 'total_duration': 3305.8187985420227, 'accumulated_submission_time': 2895.4914994239807, 'accumulated_eval_time': 410.0903272628784, 'accumulated_logging_time': 0.07826495170593262, 'global_step': 3751, 'preemption_count': 0}), (5632, {'train/ctc_loss': Array(0.59384084, dtype=float32), 'train/wer': 0.20201960361912968, 'validation/ctc_loss': Array(0.9646992, dtype=float32), 'validation/wer': 0.2732459909053168, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6315554, dtype=float32), 'test/wer': 0.20488290374342413, 'test/num_examples': 2472, 'score': 4335.885547399521, 'total_duration': 4875.635069847107, 'accumulated_submission_time': 4335.885547399521, 'accumulated_eval_time': 539.3774290084839, 'accumulated_logging_time': 0.13100433349609375, 'global_step': 5632, 'preemption_count': 0}), (7518, {'train/ctc_loss': Array(0.4997202, dtype=float32), 'train/wer': 0.16799694940398457, 'validation/ctc_loss': Array(0.8595321, dtype=float32), 'validation/wer': 0.24581712156173668, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.54608774, dtype=float32), 'test/wer': 0.1780513070501493, 'test/num_examples': 2472, 'score': 5776.177357435226, 'total_duration': 6448.498773574829, 'accumulated_submission_time': 5776.177357435226, 'accumulated_eval_time': 671.8170447349548, 'accumulated_logging_time': 0.18126487731933594, 'global_step': 7518, 'preemption_count': 0}), (9405, {'train/ctc_loss': Array(0.44924083, dtype=float32), 'train/wer': 0.1499989441898769, 'validation/ctc_loss': Array(0.76941633, dtype=float32), 'validation/wer': 0.22184461801365168, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4813573, dtype=float32), 'test/wer': 0.15560701155728882, 'test/num_examples': 2472, 'score': 7216.76273560524, 'total_duration': 8018.502638101578, 'accumulated_submission_time': 7216.76273560524, 'accumulated_eval_time': 801.1017694473267, 'accumulated_logging_time': 0.23203825950622559, 'global_step': 9405, 'preemption_count': 0}), (11289, {'train/ctc_loss': Array(0.41514525, dtype=float32), 'train/wer': 0.13922525048234338, 'validation/ctc_loss': Array(0.7317493, dtype=float32), 'validation/wer': 0.21033627156608128, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.44250494, dtype=float32), 'test/wer': 0.14214043426157252, 'test/num_examples': 2472, 'score': 8657.179950237274, 'total_duration': 9592.300470352173, 'accumulated_submission_time': 8657.179950237274, 'accumulated_eval_time': 934.3454160690308, 'accumulated_logging_time': 0.2858431339263916, 'global_step': 11289, 'preemption_count': 0}), (13156, {'train/ctc_loss': Array(0.38317287, dtype=float32), 'train/wer': 0.12914786928022384, 'validation/ctc_loss': Array(0.6829772, dtype=float32), 'validation/wer': 0.1974473097309248, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4097351, dtype=float32), 'test/wer': 0.1304409643938009, 'test/num_examples': 2472, 'score': 10097.492826223373, 'total_duration': 11164.784424304962, 'accumulated_submission_time': 10097.492826223373, 'accumulated_eval_time': 1066.3841433525085, 'accumulated_logging_time': 0.3352360725402832, 'global_step': 13156, 'preemption_count': 0}), (15032, {'train/ctc_loss': Array(0.332868, dtype=float32), 'train/wer': 0.11580051773861054, 'validation/ctc_loss': Array(0.666079, dtype=float32), 'validation/wer': 0.19206966797648126, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39503682, dtype=float32), 'test/wer': 0.12934413909369732, 'test/num_examples': 2472, 'score': 11537.915561676025, 'total_duration': 12737.434754610062, 'accumulated_submission_time': 11537.915561676025, 'accumulated_eval_time': 1198.4742548465729, 'accumulated_logging_time': 0.388714075088501, 'global_step': 15032, 'preemption_count': 0}), (16910, {'train/ctc_loss': Array(0.33133, dtype=float32), 'train/wer': 0.1122018775264075, 'validation/ctc_loss': Array(0.6564473, dtype=float32), 'validation/wer': 0.1873099240178804, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38526803, dtype=float32), 'test/wer': 0.12528182316738773, 'test/num_examples': 2472, 'score': 12978.067933797836, 'total_duration': 14309.729179859161, 'accumulated_submission_time': 12978.067933797836, 'accumulated_eval_time': 1330.4830236434937, 'accumulated_logging_time': 0.43863677978515625, 'global_step': 16910, 'preemption_count': 0}), (18779, {'train/ctc_loss': Array(0.33849165, dtype=float32), 'train/wer': 0.1135576714996754, 'validation/ctc_loss': Array(0.6315583, dtype=float32), 'validation/wer': 0.1813240391206542, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36856124, dtype=float32), 'test/wer': 0.11796965450003047, 'test/num_examples': 2472, 'score': 14418.436093091965, 'total_duration': 15880.336900472641, 'accumulated_submission_time': 14418.436093091965, 'accumulated_eval_time': 1460.588921546936, 'accumulated_logging_time': 0.4884147644042969, 'global_step': 18779, 'preemption_count': 0}), (20639, {'train/ctc_loss': Array(0.30381674, dtype=float32), 'train/wer': 0.10153564560851115, 'validation/ctc_loss': Array(0.6062795, dtype=float32), 'validation/wer': 0.1727410525502766, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35437745, dtype=float32), 'test/wer': 0.11307456380882741, 'test/num_examples': 2472, 'score': 15858.46332526207, 'total_duration': 17450.086770772934, 'accumulated_submission_time': 15858.46332526207, 'accumulated_eval_time': 1590.1804230213165, 'accumulated_logging_time': 0.5372920036315918, 'global_step': 20639, 'preemption_count': 0}), (22516, {'train/ctc_loss': Array(0.29839107, dtype=float32), 'train/wer': 0.10266235764850723, 'validation/ctc_loss': Array(0.59857935, dtype=float32), 'validation/wer': 0.17382237369300135, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34771985, dtype=float32), 'test/wer': 0.11266833221619646, 'test/num_examples': 2472, 'score': 17298.85318994522, 'total_duration': 19023.645206451416, 'accumulated_submission_time': 17298.85318994522, 'accumulated_eval_time': 1723.2054243087769, 'accumulated_logging_time': 0.5952105522155762, 'global_step': 22516, 'preemption_count': 0}), (24387, {'train/ctc_loss': Array(0.31947687, dtype=float32), 'train/wer': 0.10229532956805684, 'validation/ctc_loss': Array(0.5687596, dtype=float32), 'validation/wer': 0.16620485242862798, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33003947, dtype=float32), 'test/wer': 0.10759043730830947, 'test/num_examples': 2472, 'score': 18739.204642295837, 'total_duration': 20595.2571952343, 'accumulated_submission_time': 18739.204642295837, 'accumulated_eval_time': 1854.3274881839752, 'accumulated_logging_time': 0.6483442783355713, 'global_step': 24387, 'preemption_count': 0}), (26266, {'train/ctc_loss': Array(0.23455991, dtype=float32), 'train/wer': 0.08128063662653567, 'validation/ctc_loss': Array(0.5503892, dtype=float32), 'validation/wer': 0.1592052289600973, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31949303, dtype=float32), 'test/wer': 0.1042187150894725, 'test/num_examples': 2472, 'score': 20179.67861032486, 'total_duration': 22168.45689558983, 'accumulated_submission_time': 20179.67861032486, 'accumulated_eval_time': 1986.9184362888336, 'accumulated_logging_time': 0.6978366374969482, 'global_step': 26266, 'preemption_count': 0}), (28145, {'train/ctc_loss': Array(0.24839616, dtype=float32), 'train/wer': 0.08341915550978372, 'validation/ctc_loss': Array(0.541494, dtype=float32), 'validation/wer': 0.15600953879722332, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30606657, dtype=float32), 'test/wer': 0.0993845591371641, 'test/num_examples': 2472, 'score': 21619.813318490982, 'total_duration': 23740.416083574295, 'accumulated_submission_time': 21619.813318490982, 'accumulated_eval_time': 2118.6045274734497, 'accumulated_logging_time': 0.7499384880065918, 'global_step': 28145, 'preemption_count': 0}), (30009, {'train/ctc_loss': Array(0.32168937, dtype=float32), 'train/wer': 0.10790995229406204, 'validation/ctc_loss': Array(0.52409434, dtype=float32), 'validation/wer': 0.15208974965484615, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.291443, dtype=float32), 'test/wer': 0.0950175695163813, 'test/num_examples': 2472, 'score': 23060.3713555336, 'total_duration': 25311.81352329254, 'accumulated_submission_time': 23060.3713555336, 'accumulated_eval_time': 2249.3122136592865, 'accumulated_logging_time': 0.7996718883514404, 'global_step': 30009, 'preemption_count': 0}), (31874, {'train/ctc_loss': Array(0.33718112, dtype=float32), 'train/wer': 0.1116452902589013, 'validation/ctc_loss': Array(0.50746393, dtype=float32), 'validation/wer': 0.14898095136951253, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2857252, dtype=float32), 'test/wer': 0.09276298417727946, 'test/num_examples': 2472, 'score': 24500.939710855484, 'total_duration': 26882.66648888588, 'accumulated_submission_time': 24500.939710855484, 'accumulated_eval_time': 2379.4582090377808, 'accumulated_logging_time': 0.8533070087432861, 'global_step': 31874, 'preemption_count': 0}), (33739, {'train/ctc_loss': Array(0.36467797, dtype=float32), 'train/wer': 0.12276081306849823, 'validation/ctc_loss': Array(0.487907, dtype=float32), 'validation/wer': 0.14117033704393833, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2731824, dtype=float32), 'test/wer': 0.08809132086202344, 'test/num_examples': 2472, 'score': 25941.51305270195, 'total_duration': 28452.583513498306, 'accumulated_submission_time': 25941.51305270195, 'accumulated_eval_time': 2508.6631367206573, 'accumulated_logging_time': 0.9063496589660645, 'global_step': 33739, 'preemption_count': 0}), (35622, {'train/ctc_loss': Array(0.30550236, dtype=float32), 'train/wer': 0.09948937668740462, 'validation/ctc_loss': Array(0.47197154, dtype=float32), 'validation/wer': 0.13709607345260047, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2628657, dtype=float32), 'test/wer': 0.08384620071902991, 'test/num_examples': 2472, 'score': 27381.709954977036, 'total_duration': 30022.285489320755, 'accumulated_submission_time': 27381.709954977036, 'accumulated_eval_time': 2638.0246579647064, 'accumulated_logging_time': 0.9644997119903564, 'global_step': 35622, 'preemption_count': 0}), (37493, {'train/ctc_loss': Array(0.2715685, dtype=float32), 'train/wer': 0.0926955085160065, 'validation/ctc_loss': Array(0.45568103, dtype=float32), 'validation/wer': 0.131959798024658, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25255555, dtype=float32), 'test/wer': 0.08031198586314058, 'test/num_examples': 2472, 'score': 28821.724100589752, 'total_duration': 31593.214646816254, 'accumulated_submission_time': 28821.724100589752, 'accumulated_eval_time': 2768.7975289821625, 'accumulated_logging_time': 1.0214431285858154, 'global_step': 37493, 'preemption_count': 0}), (39364, {'train/ctc_loss': Array(0.22365406, dtype=float32), 'train/wer': 0.07677500635330886, 'validation/ctc_loss': Array(0.44460142, dtype=float32), 'validation/wer': 0.12757658553539877, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24169599, dtype=float32), 'test/wer': 0.07830113947961732, 'test/num_examples': 2472, 'score': 30262.321855068207, 'total_duration': 33165.71136927605, 'accumulated_submission_time': 30262.321855068207, 'accumulated_eval_time': 2900.555379629135, 'accumulated_logging_time': 1.07637619972229, 'global_step': 39364, 'preemption_count': 0}), (41230, {'train/ctc_loss': Array(0.25303861, dtype=float32), 'train/wer': 0.08619467289108004, 'validation/ctc_loss': Array(0.429852, dtype=float32), 'validation/wer': 0.12485397337246686, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23356742, dtype=float32), 'test/wer': 0.07413726565515, 'test/num_examples': 2472, 'score': 31702.706634283066, 'total_duration': 34737.552344322205, 'accumulated_submission_time': 31702.706634283066, 'accumulated_eval_time': 3031.871179819107, 'accumulated_logging_time': 1.132845163345337, 'global_step': 41230, 'preemption_count': 0}), (43113, {'train/ctc_loss': Array(0.21915351, dtype=float32), 'train/wer': 0.0752219014009197, 'validation/ctc_loss': Array(0.42223984, dtype=float32), 'validation/wer': 0.1220541239850546, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22784421, dtype=float32), 'test/wer': 0.07338573720878273, 'test/num_examples': 2472, 'score': 33142.88335800171, 'total_duration': 36309.45052433014, 'accumulated_submission_time': 33142.88335800171, 'accumulated_eval_time': 3163.4506227970123, 'accumulated_logging_time': 1.1886072158813477, 'global_step': 43113, 'preemption_count': 0}), (44985, {'train/ctc_loss': Array(0.21478947, dtype=float32), 'train/wer': 0.07625128710821516, 'validation/ctc_loss': Array(0.41844097, dtype=float32), 'validation/wer': 0.120547998107688, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22397295, dtype=float32), 'test/wer': 0.07241078138646842, 'test/num_examples': 2472, 'score': 34583.50731277466, 'total_duration': 37881.03183031082, 'accumulated_submission_time': 34583.50731277466, 'accumulated_eval_time': 3294.2698872089386, 'accumulated_logging_time': 1.2424731254577637, 'global_step': 44985, 'preemption_count': 0}), (46864, {'train/ctc_loss': Array(0.21869154, dtype=float32), 'train/wer': 0.07526009406141398, 'validation/ctc_loss': Array(0.41614845, dtype=float32), 'validation/wer': 0.11984320843430492, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22283582, dtype=float32), 'test/wer': 0.07188268031604818, 'test/num_examples': 2472, 'score': 36024.03443598747, 'total_duration': 39452.92549777031, 'accumulated_submission_time': 36024.03443598747, 'accumulated_eval_time': 3425.495941877365, 'accumulated_logging_time': 1.2960054874420166, 'global_step': 46864, 'preemption_count': 0}), (48000, {'train/ctc_loss': Array(0.23044662, dtype=float32), 'train/wer': 0.07937063940968733, 'validation/ctc_loss': Array(0.4161281, dtype=float32), 'validation/wer': 0.11978528051594466, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22271666, dtype=float32), 'test/wer': 0.0716795645197327, 'test/num_examples': 2472, 'score': 36883.611184597015, 'total_duration': 40442.77611327171, 'accumulated_submission_time': 36883.611184597015, 'accumulated_eval_time': 3555.660044193268, 'accumulated_logging_time': 1.351287841796875, 'global_step': 48000, 'preemption_count': 0})], 'global_step': 48000}
I0216 13:21:05.839777 140202902193984 submission_runner.py:586] Timing: 36883.611184597015
I0216 13:21:05.839837 140202902193984 submission_runner.py:588] Total number of evals: 27
I0216 13:21:05.839888 140202902193984 submission_runner.py:589] ====================
I0216 13:21:05.839949 140202902193984 submission_runner.py:542] Using RNG seed 4294683350
I0216 13:21:05.843086 140202902193984 submission_runner.py:551] --- Tuning run 4/5 ---
I0216 13:21:05.843243 140202902193984 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_3/librispeech_deepspeech_jax/trial_4.
I0216 13:21:05.845276 140202902193984 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_3/librispeech_deepspeech_jax/trial_4/hparams.json.
I0216 13:21:05.847840 140202902193984 submission_runner.py:206] Initializing dataset.
I0216 13:21:05.847970 140202902193984 submission_runner.py:213] Initializing model.
I0216 13:21:06.997905 140202902193984 submission_runner.py:255] Initializing optimizer.
I0216 13:21:07.137676 140202902193984 submission_runner.py:262] Initializing metrics bundle.
I0216 13:21:07.137864 140202902193984 submission_runner.py:280] Initializing checkpoint and logger.
I0216 13:21:07.141791 140202902193984 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_3/librispeech_deepspeech_jax/trial_4 with prefix checkpoint_
I0216 13:21:07.141947 140202902193984 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_3/librispeech_deepspeech_jax/trial_4/meta_data_0.json.
I0216 13:21:07.142281 140202902193984 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0216 13:21:07.142353 140202902193984 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0216 13:21:07.668498 140202902193984 logger_utils.py:220] Unable to record git information. Continuing without it.
I0216 13:21:08.142692 140202902193984 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_3/librispeech_deepspeech_jax/trial_4/flags_0.json.
I0216 13:21:08.159441 140202902193984 submission_runner.py:314] Starting training loop.
I0216 13:21:08.163144 140202902193984 input_pipeline.py:20] Loading split = train-clean-100
I0216 13:21:08.684007 140202902193984 input_pipeline.py:20] Loading split = train-clean-360
I0216 13:21:08.817579 140202902193984 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0216 13:21:23.985627 140047027676928 logging_writer.py:48] [0] global_step=0, grad_norm=22.59840202331543, loss=33.018985748291016
I0216 13:21:23.999741 140202902193984 spec.py:321] Evaluating on the training split.
I0216 13:22:55.601838 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 13:23:54.677504 140202902193984 spec.py:349] Evaluating on the test split.
I0216 13:24:25.381938 140202902193984 submission_runner.py:408] Time since start: 197.22s, 	Step: 1, 	{'train/ctc_loss': Array(31.12199, dtype=float32), 'train/wer': 3.282438151130771, 'validation/ctc_loss': Array(30.351221, dtype=float32), 'validation/wer': 3.0459561485658013, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.536505, dtype=float32), 'test/wer': 3.361383624804501, 'test/num_examples': 2472, 'score': 15.840157985687256, 'total_duration': 197.21963810920715, 'accumulated_submission_time': 15.840157985687256, 'accumulated_eval_time': 181.3793807029724, 'accumulated_logging_time': 0}
I0216 13:24:25.398832 140047119996672 logging_writer.py:48] [1] accumulated_eval_time=181.379381, accumulated_logging_time=0, accumulated_submission_time=15.840158, global_step=1, preemption_count=0, score=15.840158, test/ctc_loss=30.5365047454834, test/num_examples=2472, test/wer=3.361384, total_duration=197.219638, train/ctc_loss=31.121990203857422, train/wer=3.282438, validation/ctc_loss=30.351221084594727, validation/num_examples=5348, validation/wer=3.045956
I0216 13:25:50.784810 140047027676928 logging_writer.py:48] [100] global_step=100, grad_norm=4.004505157470703, loss=5.795886516571045
I0216 13:27:06.287251 140047036069632 logging_writer.py:48] [200] global_step=200, grad_norm=2.391824960708618, loss=5.059957504272461
I0216 13:28:21.575440 140047027676928 logging_writer.py:48] [300] global_step=300, grad_norm=2.4677982330322266, loss=3.776069402694702
I0216 13:29:36.676047 140047036069632 logging_writer.py:48] [400] global_step=400, grad_norm=3.0403494834899902, loss=3.308995246887207
I0216 13:30:51.793963 140047027676928 logging_writer.py:48] [500] global_step=500, grad_norm=2.481264591217041, loss=3.0648465156555176
I0216 13:32:06.528954 140047036069632 logging_writer.py:48] [600] global_step=600, grad_norm=2.2952778339385986, loss=2.876270055770874
I0216 13:33:21.592212 140047027676928 logging_writer.py:48] [700] global_step=700, grad_norm=2.478005886077881, loss=2.8306448459625244
I0216 13:34:36.656116 140047036069632 logging_writer.py:48] [800] global_step=800, grad_norm=5.307389259338379, loss=2.6918132305145264
I0216 13:35:57.905333 140047027676928 logging_writer.py:48] [900] global_step=900, grad_norm=3.2582383155822754, loss=2.6748647689819336
I0216 13:37:19.195913 140047036069632 logging_writer.py:48] [1000] global_step=1000, grad_norm=3.8907570838928223, loss=2.5825085639953613
I0216 13:38:38.943977 140047119996672 logging_writer.py:48] [1100] global_step=1100, grad_norm=2.5635576248168945, loss=2.458303928375244
I0216 13:39:53.651222 140047111603968 logging_writer.py:48] [1200] global_step=1200, grad_norm=2.569974184036255, loss=2.4497382640838623
I0216 13:41:08.485258 140047119996672 logging_writer.py:48] [1300] global_step=1300, grad_norm=2.4727783203125, loss=2.3307933807373047
I0216 13:42:23.289479 140047111603968 logging_writer.py:48] [1400] global_step=1400, grad_norm=3.179387331008911, loss=2.3054821491241455
I0216 13:43:38.368014 140047119996672 logging_writer.py:48] [1500] global_step=1500, grad_norm=3.078123092651367, loss=2.3373687267303467
I0216 13:44:53.634003 140047111603968 logging_writer.py:48] [1600] global_step=1600, grad_norm=5.197296619415283, loss=2.2939796447753906
I0216 13:46:12.484938 140047119996672 logging_writer.py:48] [1700] global_step=1700, grad_norm=4.4539690017700195, loss=2.2445626258850098
I0216 13:47:33.542068 140047111603968 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.0023088455200195, loss=2.2587289810180664
I0216 13:48:25.829788 140202902193984 spec.py:321] Evaluating on the training split.
I0216 13:49:19.306376 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 13:50:09.628576 140202902193984 spec.py:349] Evaluating on the test split.
I0216 13:50:35.514905 140202902193984 submission_runner.py:408] Time since start: 1767.35s, 	Step: 1866, 	{'train/ctc_loss': Array(1.4378089, dtype=float32), 'train/wer': 0.39355622829171477, 'validation/ctc_loss': Array(1.5070161, dtype=float32), 'validation/wer': 0.3872867528505363, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.1032181, dtype=float32), 'test/wer': 0.32169479820445634, 'test/num_examples': 2472, 'score': 1456.1781809329987, 'total_duration': 1767.3490042686462, 'accumulated_submission_time': 1456.1781809329987, 'accumulated_eval_time': 311.0581033229828, 'accumulated_logging_time': 0.029671907424926758}
I0216 13:50:35.551487 140047119996672 logging_writer.py:48] [1866] accumulated_eval_time=311.058103, accumulated_logging_time=0.029672, accumulated_submission_time=1456.178181, global_step=1866, preemption_count=0, score=1456.178181, test/ctc_loss=1.1032180786132812, test/num_examples=2472, test/wer=0.321695, total_duration=1767.349004, train/ctc_loss=1.437808871269226, train/wer=0.393556, validation/ctc_loss=1.5070160627365112, validation/num_examples=5348, validation/wer=0.387287
I0216 13:51:01.878269 140047111603968 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.6432390213012695, loss=2.1645469665527344
I0216 13:52:16.820760 140047119996672 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.5756795406341553, loss=2.1390790939331055
I0216 13:53:35.287472 140047119996672 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.60697603225708, loss=2.1484217643737793
I0216 13:54:50.331374 140047111603968 logging_writer.py:48] [2200] global_step=2200, grad_norm=2.413433790206909, loss=2.161240816116333
I0216 13:56:05.622319 140047119996672 logging_writer.py:48] [2300] global_step=2300, grad_norm=3.2076151371002197, loss=2.1555469036102295
I0216 13:57:20.677013 140047111603968 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.9048027992248535, loss=2.1613359451293945
I0216 13:58:35.718585 140047119996672 logging_writer.py:48] [2500] global_step=2500, grad_norm=2.874812364578247, loss=2.217489242553711
I0216 13:59:50.657190 140047111603968 logging_writer.py:48] [2600] global_step=2600, grad_norm=2.998537302017212, loss=2.14626145362854
I0216 14:01:07.836196 140047119996672 logging_writer.py:48] [2700] global_step=2700, grad_norm=2.26515793800354, loss=2.1462230682373047
I0216 14:02:28.604185 140047111603968 logging_writer.py:48] [2800] global_step=2800, grad_norm=2.7730698585510254, loss=2.2158782482147217
I0216 14:03:50.375784 140047119996672 logging_writer.py:48] [2900] global_step=2900, grad_norm=2.5874664783477783, loss=2.0791306495666504
I0216 14:05:13.082717 140047111603968 logging_writer.py:48] [3000] global_step=3000, grad_norm=4.292550563812256, loss=2.1161715984344482
I0216 14:06:37.181409 140047119996672 logging_writer.py:48] [3100] global_step=3100, grad_norm=2.328295946121216, loss=2.097562789916992
I0216 14:07:52.263446 140047111603968 logging_writer.py:48] [3200] global_step=3200, grad_norm=2.901559352874756, loss=2.105782985687256
I0216 14:09:07.245663 140047119996672 logging_writer.py:48] [3300] global_step=3300, grad_norm=3.6202428340911865, loss=2.10622501373291
I0216 14:10:22.215864 140047111603968 logging_writer.py:48] [3400] global_step=3400, grad_norm=3.8997249603271484, loss=2.173953056335449
I0216 14:11:37.159571 140047119996672 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.963260293006897, loss=2.0847179889678955
I0216 14:12:52.010659 140047111603968 logging_writer.py:48] [3600] global_step=3600, grad_norm=3.2614645957946777, loss=2.1195199489593506
I0216 14:14:07.375228 140047119996672 logging_writer.py:48] [3700] global_step=3700, grad_norm=2.374206304550171, loss=1.9985679388046265
I0216 14:14:35.659908 140202902193984 spec.py:321] Evaluating on the training split.
I0216 14:15:28.902592 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 14:16:19.520555 140202902193984 spec.py:349] Evaluating on the test split.
I0216 14:16:45.084177 140202902193984 submission_runner.py:408] Time since start: 3336.92s, 	Step: 3736, 	{'train/ctc_loss': Array(1.0180577, dtype=float32), 'train/wer': 0.30271556292916724, 'validation/ctc_loss': Array(1.064015, dtype=float32), 'validation/wer': 0.2916574142908175, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.7128284, dtype=float32), 'test/wer': 0.22086811691345237, 'test/num_examples': 2472, 'score': 2896.1909997463226, 'total_duration': 3336.9190402030945, 'accumulated_submission_time': 2896.1909997463226, 'accumulated_eval_time': 440.4767470359802, 'accumulated_logging_time': 0.08244967460632324}
I0216 14:16:45.117576 140047119996672 logging_writer.py:48] [3736] accumulated_eval_time=440.476747, accumulated_logging_time=0.082450, accumulated_submission_time=2896.191000, global_step=3736, preemption_count=0, score=2896.191000, test/ctc_loss=0.7128283977508545, test/num_examples=2472, test/wer=0.220868, total_duration=3336.919040, train/ctc_loss=1.0180577039718628, train/wer=0.302716, validation/ctc_loss=1.0640150308609009, validation/num_examples=5348, validation/wer=0.291657
I0216 14:17:33.936087 140047111603968 logging_writer.py:48] [3800] global_step=3800, grad_norm=3.4248716831207275, loss=2.06945538520813
I0216 14:18:49.007319 140047119996672 logging_writer.py:48] [3900] global_step=3900, grad_norm=4.192684173583984, loss=2.0303657054901123
I0216 14:20:04.626994 140047111603968 logging_writer.py:48] [4000] global_step=4000, grad_norm=3.31335711479187, loss=2.054180383682251
I0216 14:21:19.791539 140047119996672 logging_writer.py:48] [4100] global_step=4100, grad_norm=3.8359668254852295, loss=1.9805275201797485
I0216 14:22:38.651100 140047119996672 logging_writer.py:48] [4200] global_step=4200, grad_norm=4.312417030334473, loss=2.0738070011138916
I0216 14:23:53.600822 140047111603968 logging_writer.py:48] [4300] global_step=4300, grad_norm=3.3598709106445312, loss=2.0482637882232666
I0216 14:25:08.746537 140047119996672 logging_writer.py:48] [4400] global_step=4400, grad_norm=4.272013187408447, loss=2.0825748443603516
I0216 14:26:23.728369 140047111603968 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.6972033977508545, loss=2.0940115451812744
I0216 14:27:38.670760 140047119996672 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.8963663578033447, loss=2.0319414138793945
I0216 14:28:57.211293 140047111603968 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.780874013900757, loss=2.0582644939422607
I0216 14:30:18.100790 140047119996672 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.833359718322754, loss=2.0288655757904053
I0216 14:31:38.428597 140047111603968 logging_writer.py:48] [4900] global_step=4900, grad_norm=5.377690315246582, loss=2.0402581691741943
I0216 14:32:59.691047 140047119996672 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.4699654579162598, loss=2.0818681716918945
I0216 14:34:21.121788 140047111603968 logging_writer.py:48] [5100] global_step=5100, grad_norm=2.802410125732422, loss=1.9870381355285645
I0216 14:35:42.658190 140047119996672 logging_writer.py:48] [5200] global_step=5200, grad_norm=2.7540619373321533, loss=2.0245649814605713
I0216 14:36:57.564794 140047111603968 logging_writer.py:48] [5300] global_step=5300, grad_norm=3.2122695446014404, loss=1.993807315826416
I0216 14:38:12.640995 140047119996672 logging_writer.py:48] [5400] global_step=5400, grad_norm=5.159331321716309, loss=2.0528905391693115
I0216 14:39:27.362345 140047111603968 logging_writer.py:48] [5500] global_step=5500, grad_norm=3.309631824493408, loss=1.941041350364685
I0216 14:40:42.349587 140047119996672 logging_writer.py:48] [5600] global_step=5600, grad_norm=2.8930516242980957, loss=2.013890266418457
I0216 14:40:45.218987 140202902193984 spec.py:321] Evaluating on the training split.
I0216 14:41:39.830956 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 14:42:30.130157 140202902193984 spec.py:349] Evaluating on the test split.
I0216 14:42:56.082251 140202902193984 submission_runner.py:408] Time since start: 4907.92s, 	Step: 5605, 	{'train/ctc_loss': Array(0.8513874, dtype=float32), 'train/wer': 0.2599757026892705, 'validation/ctc_loss': Array(0.9749225, dtype=float32), 'validation/wer': 0.2722998349054327, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6446429, dtype=float32), 'test/wer': 0.19945971198180082, 'test/num_examples': 2472, 'score': 4336.195417165756, 'total_duration': 4907.917762994766, 'accumulated_submission_time': 4336.195417165756, 'accumulated_eval_time': 571.3350307941437, 'accumulated_logging_time': 0.13245534896850586}
I0216 14:42:56.115378 140047119996672 logging_writer.py:48] [5605] accumulated_eval_time=571.335031, accumulated_logging_time=0.132455, accumulated_submission_time=4336.195417, global_step=5605, preemption_count=0, score=4336.195417, test/ctc_loss=0.6446428894996643, test/num_examples=2472, test/wer=0.199460, total_duration=4907.917763, train/ctc_loss=0.8513873815536499, train/wer=0.259976, validation/ctc_loss=0.9749224781990051, validation/num_examples=5348, validation/wer=0.272300
I0216 14:44:08.083070 140047111603968 logging_writer.py:48] [5700] global_step=5700, grad_norm=3.162446975708008, loss=2.0260071754455566
I0216 14:45:23.241969 140047119996672 logging_writer.py:48] [5800] global_step=5800, grad_norm=2.3473081588745117, loss=1.9469999074935913
I0216 14:46:38.556296 140047111603968 logging_writer.py:48] [5900] global_step=5900, grad_norm=4.860159873962402, loss=2.197545289993286
I0216 14:47:53.571820 140047119996672 logging_writer.py:48] [6000] global_step=6000, grad_norm=4.0322699546813965, loss=2.060213804244995
I0216 14:49:09.805954 140047111603968 logging_writer.py:48] [6100] global_step=6100, grad_norm=2.9007973670959473, loss=2.9852583408355713
I0216 14:50:32.729815 140047119996672 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.5030977725982666, loss=2.0945258140563965
I0216 14:51:48.253819 140047111603968 logging_writer.py:48] [6300] global_step=6300, grad_norm=2.6152334213256836, loss=2.0091142654418945
I0216 14:53:03.469803 140047119996672 logging_writer.py:48] [6400] global_step=6400, grad_norm=1.8529789447784424, loss=2.121009111404419
I0216 14:54:18.558798 140047111603968 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.9105072021484375, loss=2.036003589630127
I0216 14:55:33.389238 140047119996672 logging_writer.py:48] [6600] global_step=6600, grad_norm=3.318471908569336, loss=2.0120961666107178
I0216 14:56:48.341597 140047111603968 logging_writer.py:48] [6700] global_step=6700, grad_norm=3.172434091567993, loss=2.037485361099243
I0216 14:58:03.316026 140047119996672 logging_writer.py:48] [6800] global_step=6800, grad_norm=4.543857574462891, loss=1.9800771474838257
I0216 14:59:23.579695 140047111603968 logging_writer.py:48] [6900] global_step=6900, grad_norm=3.650555372238159, loss=1.9582407474517822
I0216 15:00:44.527482 140047119996672 logging_writer.py:48] [7000] global_step=7000, grad_norm=4.542690277099609, loss=1.961518406867981
I0216 15:02:06.303575 140047111603968 logging_writer.py:48] [7100] global_step=7100, grad_norm=4.555604934692383, loss=2.001340627670288
I0216 15:03:28.895849 140047119996672 logging_writer.py:48] [7200] global_step=7200, grad_norm=3.253408193588257, loss=2.0303921699523926
I0216 15:04:47.961655 140047119996672 logging_writer.py:48] [7300] global_step=7300, grad_norm=3.07794189453125, loss=1.9759677648544312
I0216 15:06:03.148216 140047111603968 logging_writer.py:48] [7400] global_step=7400, grad_norm=5.899155139923096, loss=1.991513729095459
I0216 15:06:56.238756 140202902193984 spec.py:321] Evaluating on the training split.
I0216 15:07:49.904050 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 15:08:40.695463 140202902193984 spec.py:349] Evaluating on the test split.
I0216 15:09:06.380635 140202902193984 submission_runner.py:408] Time since start: 6478.22s, 	Step: 7472, 	{'train/ctc_loss': Array(0.8907499, dtype=float32), 'train/wer': 0.26349455919281495, 'validation/ctc_loss': Array(0.97102195, dtype=float32), 'validation/wer': 0.2661498209061857, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6244865, dtype=float32), 'test/wer': 0.19153819592549712, 'test/num_examples': 2472, 'score': 5776.223026514053, 'total_duration': 6478.2153515815735, 'accumulated_submission_time': 5776.223026514053, 'accumulated_eval_time': 701.4711322784424, 'accumulated_logging_time': 0.18075060844421387}
I0216 15:09:06.415669 140047119996672 logging_writer.py:48] [7472] accumulated_eval_time=701.471132, accumulated_logging_time=0.180751, accumulated_submission_time=5776.223027, global_step=7472, preemption_count=0, score=5776.223027, test/ctc_loss=0.62448650598526, test/num_examples=2472, test/wer=0.191538, total_duration=6478.215352, train/ctc_loss=0.8907498717308044, train/wer=0.263495, validation/ctc_loss=0.9710219502449036, validation/num_examples=5348, validation/wer=0.266150
I0216 15:09:28.192515 140047111603968 logging_writer.py:48] [7500] global_step=7500, grad_norm=4.033105850219727, loss=1.9566783905029297
I0216 15:10:43.032482 140047119996672 logging_writer.py:48] [7600] global_step=7600, grad_norm=3.1090054512023926, loss=1.9046273231506348
I0216 15:11:58.021240 140047111603968 logging_writer.py:48] [7700] global_step=7700, grad_norm=3.884826183319092, loss=1.942277431488037
I0216 15:13:13.124579 140047119996672 logging_writer.py:48] [7800] global_step=7800, grad_norm=2.5309596061706543, loss=1.954314947128296
I0216 15:14:28.178308 140047111603968 logging_writer.py:48] [7900] global_step=7900, grad_norm=3.4335594177246094, loss=1.9744701385498047
I0216 15:15:43.078290 140047119996672 logging_writer.py:48] [8000] global_step=8000, grad_norm=2.0941059589385986, loss=1.9884642362594604
I0216 15:17:04.542614 140047111603968 logging_writer.py:48] [8100] global_step=8100, grad_norm=2.0096349716186523, loss=1.9560281038284302
I0216 15:18:25.354266 140047119996672 logging_writer.py:48] [8200] global_step=8200, grad_norm=4.629458904266357, loss=1.8684501647949219
I0216 15:19:46.225284 140047119996672 logging_writer.py:48] [8300] global_step=8300, grad_norm=2.226236581802368, loss=1.877618670463562
I0216 15:21:01.207899 140047111603968 logging_writer.py:48] [8400] global_step=8400, grad_norm=2.359311580657959, loss=1.9051846265792847
I0216 15:22:16.111006 140047119996672 logging_writer.py:48] [8500] global_step=8500, grad_norm=3.4219119548797607, loss=1.8453041315078735
I0216 15:23:31.071669 140047111603968 logging_writer.py:48] [8600] global_step=8600, grad_norm=3.7629587650299072, loss=1.994092583656311
I0216 15:24:46.036054 140047119996672 logging_writer.py:48] [8700] global_step=8700, grad_norm=2.6499147415161133, loss=2.0416595935821533
I0216 15:26:01.267316 140047111603968 logging_writer.py:48] [8800] global_step=8800, grad_norm=3.104524850845337, loss=1.8721411228179932
I0216 15:27:20.733398 140047119996672 logging_writer.py:48] [8900] global_step=8900, grad_norm=3.1781046390533447, loss=1.8406529426574707
I0216 15:28:41.435265 140047111603968 logging_writer.py:48] [9000] global_step=9000, grad_norm=2.4384377002716064, loss=1.9322293996810913
I0216 15:30:04.440973 140047119996672 logging_writer.py:48] [9100] global_step=9100, grad_norm=3.490875244140625, loss=1.9637588262557983
I0216 15:31:25.170892 140047111603968 logging_writer.py:48] [9200] global_step=9200, grad_norm=2.2548844814300537, loss=1.9936151504516602
I0216 15:32:47.592195 140047119996672 logging_writer.py:48] [9300] global_step=9300, grad_norm=3.385174512863159, loss=1.8350188732147217
I0216 15:33:06.960144 140202902193984 spec.py:321] Evaluating on the training split.
I0216 15:34:00.939250 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 15:34:51.081930 140202902193984 spec.py:349] Evaluating on the test split.
I0216 15:35:16.719549 140202902193984 submission_runner.py:408] Time since start: 8048.55s, 	Step: 9327, 	{'train/ctc_loss': Array(0.8192644, dtype=float32), 'train/wer': 0.24301999101527402, 'validation/ctc_loss': Array(0.89382845, dtype=float32), 'validation/wer': 0.24706257180648214, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.57379514, dtype=float32), 'test/wer': 0.17669043121483557, 'test/num_examples': 2472, 'score': 7216.673100471497, 'total_duration': 8048.553774595261, 'accumulated_submission_time': 7216.673100471497, 'accumulated_eval_time': 831.224268913269, 'accumulated_logging_time': 0.2314159870147705}
I0216 15:35:16.755888 140047119996672 logging_writer.py:48] [9327] accumulated_eval_time=831.224269, accumulated_logging_time=0.231416, accumulated_submission_time=7216.673100, global_step=9327, preemption_count=0, score=7216.673100, test/ctc_loss=0.5737951397895813, test/num_examples=2472, test/wer=0.176690, total_duration=8048.553775, train/ctc_loss=0.8192644119262695, train/wer=0.243020, validation/ctc_loss=0.8938284516334534, validation/num_examples=5348, validation/wer=0.247063
I0216 15:36:12.421095 140047111603968 logging_writer.py:48] [9400] global_step=9400, grad_norm=4.369131565093994, loss=1.9300503730773926
I0216 15:37:27.465563 140047119996672 logging_writer.py:48] [9500] global_step=9500, grad_norm=3.4559590816497803, loss=1.9012105464935303
I0216 15:38:42.625913 140047111603968 logging_writer.py:48] [9600] global_step=9600, grad_norm=2.0350260734558105, loss=1.8601847887039185
I0216 15:39:57.613428 140047119996672 logging_writer.py:48] [9700] global_step=9700, grad_norm=2.9804790019989014, loss=1.9562623500823975
I0216 15:41:12.510997 140047111603968 logging_writer.py:48] [9800] global_step=9800, grad_norm=2.4756436347961426, loss=1.8705005645751953
I0216 15:42:27.560204 140047119996672 logging_writer.py:48] [9900] global_step=9900, grad_norm=1.9848265647888184, loss=1.8510431051254272
I0216 15:43:42.803529 140047111603968 logging_writer.py:48] [10000] global_step=10000, grad_norm=2.418614625930786, loss=1.8745107650756836
I0216 15:45:00.763575 140047119996672 logging_writer.py:48] [10100] global_step=10100, grad_norm=4.233340263366699, loss=1.8556245565414429
I0216 15:46:21.793215 140047111603968 logging_writer.py:48] [10200] global_step=10200, grad_norm=4.115132808685303, loss=1.86605966091156
I0216 15:47:46.730757 140047119996672 logging_writer.py:48] [10300] global_step=10300, grad_norm=2.4362685680389404, loss=1.876783013343811
I0216 15:49:01.668967 140047111603968 logging_writer.py:48] [10400] global_step=10400, grad_norm=4.459298610687256, loss=1.9404184818267822
I0216 15:50:16.738007 140047119996672 logging_writer.py:48] [10500] global_step=10500, grad_norm=2.820899248123169, loss=1.8746548891067505
I0216 15:51:31.681058 140047111603968 logging_writer.py:48] [10600] global_step=10600, grad_norm=2.506267786026001, loss=1.8835527896881104
I0216 15:52:46.683632 140047119996672 logging_writer.py:48] [10700] global_step=10700, grad_norm=3.2962424755096436, loss=1.8906400203704834
I0216 15:54:01.780845 140047111603968 logging_writer.py:48] [10800] global_step=10800, grad_norm=3.3371572494506836, loss=1.9052866697311401
I0216 15:55:16.723337 140047119996672 logging_writer.py:48] [10900] global_step=10900, grad_norm=3.831331491470337, loss=1.7727915048599243
I0216 15:56:35.414463 140047111603968 logging_writer.py:48] [11000] global_step=11000, grad_norm=4.121456623077393, loss=1.9309184551239014
I0216 15:57:55.578443 140047119996672 logging_writer.py:48] [11100] global_step=11100, grad_norm=2.6807806491851807, loss=1.8905175924301147
I0216 15:59:16.661958 140047111603968 logging_writer.py:48] [11200] global_step=11200, grad_norm=2.126591205596924, loss=1.865723729133606
I0216 15:59:17.316709 140202902193984 spec.py:321] Evaluating on the training split.
I0216 16:00:11.868388 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 16:01:02.892090 140202902193984 spec.py:349] Evaluating on the test split.
I0216 16:01:28.426159 140202902193984 submission_runner.py:408] Time since start: 9620.26s, 	Step: 11202, 	{'train/ctc_loss': Array(0.77553225, dtype=float32), 'train/wer': 0.23671578197918577, 'validation/ctc_loss': Array(0.8555793, dtype=float32), 'validation/wer': 0.24154011025613795, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.54874396, dtype=float32), 'test/wer': 0.17151097840879084, 'test/num_examples': 2472, 'score': 8657.137380123138, 'total_duration': 9620.260332584381, 'accumulated_submission_time': 8657.137380123138, 'accumulated_eval_time': 962.3273937702179, 'accumulated_logging_time': 0.2845883369445801}
I0216 16:01:28.459946 140047119996672 logging_writer.py:48] [11202] accumulated_eval_time=962.327394, accumulated_logging_time=0.284588, accumulated_submission_time=8657.137380, global_step=11202, preemption_count=0, score=8657.137380, test/ctc_loss=0.5487439632415771, test/num_examples=2472, test/wer=0.171511, total_duration=9620.260333, train/ctc_loss=0.7755322456359863, train/wer=0.236716, validation/ctc_loss=0.8555793166160583, validation/num_examples=5348, validation/wer=0.241540
I0216 16:02:42.599956 140047111603968 logging_writer.py:48] [11300] global_step=11300, grad_norm=6.338749885559082, loss=1.8647983074188232
I0216 16:04:01.117952 140047119996672 logging_writer.py:48] [11400] global_step=11400, grad_norm=2.0215346813201904, loss=1.9545947313308716
I0216 16:05:16.311110 140047111603968 logging_writer.py:48] [11500] global_step=11500, grad_norm=2.098982572555542, loss=1.8319385051727295
I0216 16:06:31.346236 140047119996672 logging_writer.py:48] [11600] global_step=11600, grad_norm=2.302658796310425, loss=1.7846510410308838
I0216 16:07:46.640073 140047111603968 logging_writer.py:48] [11700] global_step=11700, grad_norm=4.9598212242126465, loss=1.768660068511963
I0216 16:09:02.108028 140047119996672 logging_writer.py:48] [11800] global_step=11800, grad_norm=4.1056742668151855, loss=1.8654199838638306
I0216 16:10:17.349465 140047111603968 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.7103856801986694, loss=1.895233154296875
I0216 16:11:34.188220 140047119996672 logging_writer.py:48] [12000] global_step=12000, grad_norm=4.2939066886901855, loss=1.84939444065094
I0216 16:12:55.328611 140047111603968 logging_writer.py:48] [12100] global_step=12100, grad_norm=2.4461755752563477, loss=1.8539530038833618
I0216 16:14:18.369920 140047119996672 logging_writer.py:48] [12200] global_step=12200, grad_norm=4.951642036437988, loss=1.849928379058838
I0216 16:15:39.101156 140047111603968 logging_writer.py:48] [12300] global_step=12300, grad_norm=2.521839141845703, loss=1.8841980695724487
I0216 16:17:01.068153 140047119996672 logging_writer.py:48] [12400] global_step=12400, grad_norm=2.679401397705078, loss=1.8285789489746094
I0216 16:18:15.919849 140047111603968 logging_writer.py:48] [12500] global_step=12500, grad_norm=4.1188459396362305, loss=1.8822177648544312
I0216 16:19:30.777379 140047119996672 logging_writer.py:48] [12600] global_step=12600, grad_norm=2.452706813812256, loss=1.8795475959777832
I0216 16:20:46.061629 140047111603968 logging_writer.py:48] [12700] global_step=12700, grad_norm=2.150141716003418, loss=1.8235875368118286
I0216 16:22:01.024900 140047119996672 logging_writer.py:48] [12800] global_step=12800, grad_norm=3.0315067768096924, loss=1.8779512643814087
I0216 16:23:15.947324 140047111603968 logging_writer.py:48] [12900] global_step=12900, grad_norm=2.081878185272217, loss=1.8514336347579956
I0216 16:24:31.008052 140047119996672 logging_writer.py:48] [13000] global_step=13000, grad_norm=4.234014511108398, loss=1.7850159406661987
I0216 16:25:28.849889 140202902193984 spec.py:321] Evaluating on the training split.
I0216 16:26:22.638153 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 16:27:13.465961 140202902193984 spec.py:349] Evaluating on the test split.
I0216 16:27:39.218200 140202902193984 submission_runner.py:408] Time since start: 11191.05s, 	Step: 13074, 	{'train/ctc_loss': Array(0.687591, dtype=float32), 'train/wer': 0.2090459084094702, 'validation/ctc_loss': Array(0.8205807, dtype=float32), 'validation/wer': 0.22957799511474555, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5158074, dtype=float32), 'test/wer': 0.1610708264781752, 'test/num_examples': 2472, 'score': 10097.431697845459, 'total_duration': 11191.052989721298, 'accumulated_submission_time': 10097.431697845459, 'accumulated_eval_time': 1092.690021276474, 'accumulated_logging_time': 0.3334639072418213}
I0216 16:27:39.253830 140047119996672 logging_writer.py:48] [13074] accumulated_eval_time=1092.690021, accumulated_logging_time=0.333464, accumulated_submission_time=10097.431698, global_step=13074, preemption_count=0, score=10097.431698, test/ctc_loss=0.5158073902130127, test/num_examples=2472, test/wer=0.161071, total_duration=11191.052990, train/ctc_loss=0.687591016292572, train/wer=0.209046, validation/ctc_loss=0.8205807209014893, validation/num_examples=5348, validation/wer=0.229578
I0216 16:27:59.555320 140047111603968 logging_writer.py:48] [13100] global_step=13100, grad_norm=2.477278470993042, loss=1.8452414274215698
I0216 16:29:14.817755 140047119996672 logging_writer.py:48] [13200] global_step=13200, grad_norm=3.876234292984009, loss=1.770159363746643
I0216 16:30:30.034601 140047111603968 logging_writer.py:48] [13300] global_step=13300, grad_norm=2.8173294067382812, loss=1.870097041130066
I0216 16:31:48.448949 140047119996672 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.5132339000701904, loss=1.7920175790786743
I0216 16:33:03.517098 140047111603968 logging_writer.py:48] [13500] global_step=13500, grad_norm=2.5127203464508057, loss=1.8380208015441895
I0216 16:34:18.483174 140047119996672 logging_writer.py:48] [13600] global_step=13600, grad_norm=2.5098893642425537, loss=1.79970383644104
I0216 16:35:33.688436 140047111603968 logging_writer.py:48] [13700] global_step=13700, grad_norm=2.690413475036621, loss=1.9148720502853394
I0216 16:36:48.784298 140047119996672 logging_writer.py:48] [13800] global_step=13800, grad_norm=2.7258803844451904, loss=1.7380785942077637
I0216 16:38:03.664322 140047111603968 logging_writer.py:48] [13900] global_step=13900, grad_norm=2.6785356998443604, loss=1.8272002935409546
I0216 16:39:18.768003 140047119996672 logging_writer.py:48] [14000] global_step=14000, grad_norm=2.639803171157837, loss=1.7333369255065918
I0216 16:40:37.762700 140047111603968 logging_writer.py:48] [14100] global_step=14100, grad_norm=2.058675527572632, loss=1.7651444673538208
I0216 16:42:00.050086 140047119996672 logging_writer.py:48] [14200] global_step=14200, grad_norm=2.2423572540283203, loss=1.797088861465454
I0216 16:43:20.883560 140047111603968 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.7796318531036377, loss=1.7596060037612915
I0216 16:44:42.050749 140047119996672 logging_writer.py:48] [14400] global_step=14400, grad_norm=2.6165988445281982, loss=1.8116391897201538
I0216 16:46:01.233985 140047119996672 logging_writer.py:48] [14500] global_step=14500, grad_norm=2.4639534950256348, loss=1.8296741247177124
I0216 16:47:16.396455 140047111603968 logging_writer.py:48] [14600] global_step=14600, grad_norm=3.293212890625, loss=1.7216600179672241
I0216 16:48:31.407855 140047119996672 logging_writer.py:48] [14700] global_step=14700, grad_norm=2.678560256958008, loss=1.7732341289520264
I0216 16:49:46.484352 140047111603968 logging_writer.py:48] [14800] global_step=14800, grad_norm=2.727780342102051, loss=1.7141749858856201
I0216 16:51:01.537469 140047119996672 logging_writer.py:48] [14900] global_step=14900, grad_norm=2.6251301765441895, loss=1.8055775165557861
I0216 16:51:39.666111 140202902193984 spec.py:321] Evaluating on the training split.
I0216 16:52:32.936777 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 16:53:24.435220 140202902193984 spec.py:349] Evaluating on the test split.
I0216 16:53:50.345055 140202902193984 submission_runner.py:408] Time since start: 12762.18s, 	Step: 14952, 	{'train/ctc_loss': Array(0.63921106, dtype=float32), 'train/wer': 0.19762669623769621, 'validation/ctc_loss': Array(0.79561263, dtype=float32), 'validation/wer': 0.22467343136024406, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.49638095, dtype=float32), 'test/wer': 0.1571303800296549, 'test/num_examples': 2472, 'score': 11537.74842619896, 'total_duration': 12762.179047107697, 'accumulated_submission_time': 11537.74842619896, 'accumulated_eval_time': 1223.362502336502, 'accumulated_logging_time': 0.38450098037719727}
I0216 16:53:50.379713 140047119996672 logging_writer.py:48] [14952] accumulated_eval_time=1223.362502, accumulated_logging_time=0.384501, accumulated_submission_time=11537.748426, global_step=14952, preemption_count=0, score=11537.748426, test/ctc_loss=0.4963809549808502, test/num_examples=2472, test/wer=0.157130, total_duration=12762.179047, train/ctc_loss=0.6392110586166382, train/wer=0.197627, validation/ctc_loss=0.795612633228302, validation/num_examples=5348, validation/wer=0.224673
I0216 16:54:27.311752 140047111603968 logging_writer.py:48] [15000] global_step=15000, grad_norm=2.102616786956787, loss=1.7862440347671509
I0216 16:55:42.183532 140047119996672 logging_writer.py:48] [15100] global_step=15100, grad_norm=2.0116159915924072, loss=1.7998710870742798
I0216 16:56:56.996400 140047111603968 logging_writer.py:48] [15200] global_step=15200, grad_norm=3.8916096687316895, loss=1.7595292329788208
I0216 16:58:12.410206 140047119996672 logging_writer.py:48] [15300] global_step=15300, grad_norm=2.279008626937866, loss=1.7697137594223022
I0216 16:59:27.416141 140047111603968 logging_writer.py:48] [15400] global_step=15400, grad_norm=2.1480460166931152, loss=1.818469762802124
I0216 17:00:47.538788 140047119996672 logging_writer.py:48] [15500] global_step=15500, grad_norm=2.637179136276245, loss=1.7855818271636963
I0216 17:02:02.632980 140047111603968 logging_writer.py:48] [15600] global_step=15600, grad_norm=3.5262906551361084, loss=1.7480576038360596
I0216 17:03:17.619622 140047119996672 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.9236618280410767, loss=1.689484715461731
I0216 17:04:32.673187 140047111603968 logging_writer.py:48] [15800] global_step=15800, grad_norm=2.231379985809326, loss=1.7657907009124756
I0216 17:05:47.713301 140047119996672 logging_writer.py:48] [15900] global_step=15900, grad_norm=2.535313606262207, loss=1.7934914827346802
I0216 17:07:02.619074 140047111603968 logging_writer.py:48] [16000] global_step=16000, grad_norm=2.7865731716156006, loss=1.8073996305465698
I0216 17:08:18.465653 140047119996672 logging_writer.py:48] [16100] global_step=16100, grad_norm=2.479151725769043, loss=1.7538838386535645
I0216 17:09:40.396457 140047111603968 logging_writer.py:48] [16200] global_step=16200, grad_norm=2.800318479537964, loss=1.7564514875411987
I0216 17:11:02.384687 140047119996672 logging_writer.py:48] [16300] global_step=16300, grad_norm=3.326946973800659, loss=1.798943281173706
I0216 17:12:22.497594 140047111603968 logging_writer.py:48] [16400] global_step=16400, grad_norm=2.564661741256714, loss=1.7892224788665771
I0216 17:13:45.484046 140047119996672 logging_writer.py:48] [16500] global_step=16500, grad_norm=2.665210485458374, loss=1.7938674688339233
I0216 17:15:00.418061 140047111603968 logging_writer.py:48] [16600] global_step=16600, grad_norm=2.803135871887207, loss=1.7363049983978271
I0216 17:16:15.517753 140047119996672 logging_writer.py:48] [16700] global_step=16700, grad_norm=4.322357177734375, loss=1.7319598197937012
I0216 17:17:30.351731 140047111603968 logging_writer.py:48] [16800] global_step=16800, grad_norm=3.1413156986236572, loss=1.7545009851455688
I0216 17:17:50.364280 140202902193984 spec.py:321] Evaluating on the training split.
I0216 17:18:44.543544 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 17:19:35.601884 140202902193984 spec.py:349] Evaluating on the test split.
I0216 17:20:01.256556 140202902193984 submission_runner.py:408] Time since start: 14333.09s, 	Step: 16828, 	{'train/ctc_loss': Array(0.7159693, dtype=float32), 'train/wer': 0.22133968015178476, 'validation/ctc_loss': Array(0.7896422, dtype=float32), 'validation/wer': 0.2200198885853037, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.49312958, dtype=float32), 'test/wer': 0.15516015680539474, 'test/num_examples': 2472, 'score': 12977.63620853424, 'total_duration': 14333.092150449753, 'accumulated_submission_time': 12977.63620853424, 'accumulated_eval_time': 1354.249887228012, 'accumulated_logging_time': 0.43480873107910156}
I0216 17:20:01.289942 140047119996672 logging_writer.py:48] [16828] accumulated_eval_time=1354.249887, accumulated_logging_time=0.434809, accumulated_submission_time=12977.636209, global_step=16828, preemption_count=0, score=12977.636209, test/ctc_loss=0.49312958121299744, test/num_examples=2472, test/wer=0.155160, total_duration=14333.092150, train/ctc_loss=0.7159693241119385, train/wer=0.221340, validation/ctc_loss=0.7896422147750854, validation/num_examples=5348, validation/wer=0.220020
I0216 17:20:56.060937 140047111603968 logging_writer.py:48] [16900] global_step=16900, grad_norm=2.1441361904144287, loss=1.7752900123596191
I0216 17:22:10.990494 140047119996672 logging_writer.py:48] [17000] global_step=17000, grad_norm=2.797697067260742, loss=1.7095094919204712
I0216 17:23:25.913010 140047111603968 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.965132236480713, loss=1.7168476581573486
I0216 17:24:40.919271 140047119996672 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.922651767730713, loss=1.801356315612793
I0216 17:25:57.434154 140047111603968 logging_writer.py:48] [17300] global_step=17300, grad_norm=2.9793622493743896, loss=1.716644048690796
I0216 17:27:18.069106 140047119996672 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.925133466720581, loss=1.7296663522720337
I0216 17:28:38.310120 140047111603968 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.9168899059295654, loss=1.7195709943771362
I0216 17:29:57.111531 140047119996672 logging_writer.py:48] [17600] global_step=17600, grad_norm=2.561149835586548, loss=1.6980679035186768
I0216 17:31:12.095382 140047111603968 logging_writer.py:48] [17700] global_step=17700, grad_norm=3.1567046642303467, loss=1.6665269136428833
I0216 17:32:27.257233 140047119996672 logging_writer.py:48] [17800] global_step=17800, grad_norm=2.7761807441711426, loss=1.7395226955413818
I0216 17:33:42.365143 140047111603968 logging_writer.py:48] [17900] global_step=17900, grad_norm=2.653681516647339, loss=1.7790061235427856
I0216 17:34:57.392899 140047119996672 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.7362542152404785, loss=1.7044365406036377
I0216 17:36:12.558573 140047111603968 logging_writer.py:48] [18100] global_step=18100, grad_norm=2.487086772918701, loss=1.7414573431015015
I0216 17:37:28.690998 140047119996672 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.649530291557312, loss=1.6810598373413086
I0216 17:38:49.816149 140047111603968 logging_writer.py:48] [18300] global_step=18300, grad_norm=2.616009473800659, loss=1.730104684829712
I0216 17:40:10.072645 140047119996672 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.6465427875518799, loss=1.7350398302078247
I0216 17:41:31.982052 140047111603968 logging_writer.py:48] [18500] global_step=18500, grad_norm=4.339132785797119, loss=1.7401094436645508
I0216 17:42:51.714947 140047119996672 logging_writer.py:48] [18600] global_step=18600, grad_norm=4.087635517120361, loss=1.7093063592910767
I0216 17:44:01.816301 140202902193984 spec.py:321] Evaluating on the training split.
I0216 17:44:55.523359 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 17:45:46.738006 140202902193984 spec.py:349] Evaluating on the test split.
I0216 17:46:12.277336 140202902193984 submission_runner.py:408] Time since start: 15904.11s, 	Step: 18695, 	{'train/ctc_loss': Array(0.6039645, dtype=float32), 'train/wer': 0.18995187399737495, 'validation/ctc_loss': Array(0.74285376, dtype=float32), 'validation/wer': 0.21174585091284745, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.45597544, dtype=float32), 'test/wer': 0.14569496069709342, 'test/num_examples': 2472, 'score': 14418.066945314407, 'total_duration': 15904.111874103546, 'accumulated_submission_time': 14418.066945314407, 'accumulated_eval_time': 1484.7049746513367, 'accumulated_logging_time': 0.48461151123046875}
I0216 17:46:12.311290 140047119996672 logging_writer.py:48] [18695] accumulated_eval_time=1484.704975, accumulated_logging_time=0.484612, accumulated_submission_time=14418.066945, global_step=18695, preemption_count=0, score=14418.066945, test/ctc_loss=0.4559754431247711, test/num_examples=2472, test/wer=0.145695, total_duration=15904.111874, train/ctc_loss=0.6039645075798035, train/wer=0.189952, validation/ctc_loss=0.7428537607192993, validation/num_examples=5348, validation/wer=0.211746
I0216 17:46:16.990067 140047111603968 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.8629659414291382, loss=1.6825785636901855
I0216 17:47:31.895629 140047119996672 logging_writer.py:48] [18800] global_step=18800, grad_norm=4.020267486572266, loss=1.6591713428497314
I0216 17:48:46.850846 140047111603968 logging_writer.py:48] [18900] global_step=18900, grad_norm=2.523790121078491, loss=1.7585344314575195
I0216 17:50:02.122262 140047119996672 logging_writer.py:48] [19000] global_step=19000, grad_norm=4.080765724182129, loss=1.6837148666381836
I0216 17:51:17.313061 140047111603968 logging_writer.py:48] [19100] global_step=19100, grad_norm=2.7733235359191895, loss=1.6419705152511597
I0216 17:52:32.262528 140047119996672 logging_writer.py:48] [19200] global_step=19200, grad_norm=3.1051371097564697, loss=1.746101975440979
I0216 17:53:47.319788 140047111603968 logging_writer.py:48] [19300] global_step=19300, grad_norm=2.129584312438965, loss=1.6169705390930176
I0216 17:55:05.508795 140047119996672 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.7818982601165771, loss=1.6572918891906738
I0216 17:56:25.794652 140047111603968 logging_writer.py:48] [19500] global_step=19500, grad_norm=2.2731122970581055, loss=1.7309504747390747
I0216 17:57:47.906806 140047119996672 logging_writer.py:48] [19600] global_step=19600, grad_norm=2.014726400375366, loss=1.7311174869537354
I0216 17:59:03.255695 140047111603968 logging_writer.py:48] [19700] global_step=19700, grad_norm=2.360860586166382, loss=1.6582881212234497
I0216 18:00:18.449836 140047119996672 logging_writer.py:48] [19800] global_step=19800, grad_norm=4.127238750457764, loss=1.6805764436721802
I0216 18:01:33.825634 140047111603968 logging_writer.py:48] [19900] global_step=19900, grad_norm=3.1093873977661133, loss=1.6435480117797852
I0216 18:02:49.044402 140047119996672 logging_writer.py:48] [20000] global_step=20000, grad_norm=2.855738639831543, loss=1.650722861289978
I0216 18:04:04.230165 140047111603968 logging_writer.py:48] [20100] global_step=20100, grad_norm=3.085160732269287, loss=1.6378285884857178
I0216 18:05:19.557697 140047119996672 logging_writer.py:48] [20200] global_step=20200, grad_norm=2.767826557159424, loss=1.6729092597961426
I0216 18:06:39.813846 140047111603968 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.6183830499649048, loss=1.6678828001022339
I0216 18:08:01.462157 140047119996672 logging_writer.py:48] [20400] global_step=20400, grad_norm=2.9612486362457275, loss=1.7520776987075806
I0216 18:09:23.419819 140047111603968 logging_writer.py:48] [20500] global_step=20500, grad_norm=2.4455864429473877, loss=1.6732858419418335
I0216 18:10:12.566465 140202902193984 spec.py:321] Evaluating on the training split.
I0216 18:11:07.299663 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 18:11:57.661402 140202902193984 spec.py:349] Evaluating on the test split.
I0216 18:12:23.920186 140202902193984 submission_runner.py:408] Time since start: 17475.75s, 	Step: 20562, 	{'train/ctc_loss': Array(0.6238497, dtype=float32), 'train/wer': 0.18968322011914968, 'validation/ctc_loss': Array(0.70414096, dtype=float32), 'validation/wer': 0.20152157332226267, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43188784, dtype=float32), 'test/wer': 0.13627038774805517, 'test/num_examples': 2472, 'score': 15858.226209878922, 'total_duration': 17475.754253149033, 'accumulated_submission_time': 15858.226209878922, 'accumulated_eval_time': 1616.0522694587708, 'accumulated_logging_time': 0.5335447788238525}
I0216 18:12:23.953952 140047119996672 logging_writer.py:48] [20562] accumulated_eval_time=1616.052269, accumulated_logging_time=0.533545, accumulated_submission_time=15858.226210, global_step=20562, preemption_count=0, score=15858.226210, test/ctc_loss=0.43188783526420593, test/num_examples=2472, test/wer=0.136270, total_duration=17475.754253, train/ctc_loss=0.6238496899604797, train/wer=0.189683, validation/ctc_loss=0.7041409611701965, validation/num_examples=5348, validation/wer=0.201522
I0216 18:12:56.445658 140047119996672 logging_writer.py:48] [20600] global_step=20600, grad_norm=2.2117347717285156, loss=1.629765510559082
I0216 18:14:11.501181 140047111603968 logging_writer.py:48] [20700] global_step=20700, grad_norm=2.709812879562378, loss=1.6499041318893433
I0216 18:15:26.672655 140047119996672 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.9896488189697266, loss=1.596969485282898
I0216 18:16:41.896481 140047111603968 logging_writer.py:48] [20900] global_step=20900, grad_norm=4.698736667633057, loss=1.6857277154922485
I0216 18:17:56.919567 140047119996672 logging_writer.py:48] [21000] global_step=21000, grad_norm=3.0744707584381104, loss=1.664223313331604
I0216 18:19:11.837647 140047111603968 logging_writer.py:48] [21100] global_step=21100, grad_norm=2.303684949874878, loss=1.7154104709625244
I0216 18:20:27.532291 140047119996672 logging_writer.py:48] [21200] global_step=21200, grad_norm=2.935487985610962, loss=1.6937185525894165
I0216 18:21:48.091037 140047111603968 logging_writer.py:48] [21300] global_step=21300, grad_norm=2.212880849838257, loss=1.6396734714508057
I0216 18:23:09.141968 140047119996672 logging_writer.py:48] [21400] global_step=21400, grad_norm=2.4368879795074463, loss=1.663199543952942
I0216 18:24:29.258502 140047111603968 logging_writer.py:48] [21500] global_step=21500, grad_norm=2.8470444679260254, loss=1.6215763092041016
I0216 18:25:49.816158 140047119996672 logging_writer.py:48] [21600] global_step=21600, grad_norm=2.8805534839630127, loss=1.731376051902771
I0216 18:27:09.534305 140047119996672 logging_writer.py:48] [21700] global_step=21700, grad_norm=2.2718706130981445, loss=1.662361979484558
I0216 18:28:24.577399 140047111603968 logging_writer.py:48] [21800] global_step=21800, grad_norm=3.058528184890747, loss=1.652974247932434
I0216 18:29:39.329368 140047119996672 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.797376036643982, loss=1.6450917720794678
I0216 18:30:54.430328 140047111603968 logging_writer.py:48] [22000] global_step=22000, grad_norm=2.9539613723754883, loss=1.667868971824646
I0216 18:32:09.611562 140047119996672 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.6323379278182983, loss=1.6860898733139038
I0216 18:33:24.737586 140047111603968 logging_writer.py:48] [22200] global_step=22200, grad_norm=2.2163190841674805, loss=1.6282492876052856
I0216 18:34:41.146170 140047119996672 logging_writer.py:48] [22300] global_step=22300, grad_norm=3.3640382289886475, loss=1.6307415962219238
I0216 18:36:02.120301 140047111603968 logging_writer.py:48] [22400] global_step=22400, grad_norm=3.8347597122192383, loss=1.7035396099090576
I0216 18:36:24.078851 140202902193984 spec.py:321] Evaluating on the training split.
I0216 18:37:18.753907 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 18:38:09.464903 140202902193984 spec.py:349] Evaluating on the test split.
I0216 18:38:35.021118 140202902193984 submission_runner.py:408] Time since start: 19046.86s, 	Step: 22428, 	{'train/ctc_loss': Array(0.5352325, dtype=float32), 'train/wer': 0.16937924670288323, 'validation/ctc_loss': Array(0.69400257, dtype=float32), 'validation/wer': 0.19813279009818782, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.41050902, dtype=float32), 'test/wer': 0.13096906546422116, 'test/num_examples': 2472, 'score': 17298.253359794617, 'total_duration': 19046.85565328598, 'accumulated_submission_time': 17298.253359794617, 'accumulated_eval_time': 1746.9885816574097, 'accumulated_logging_time': 0.5863668918609619}
I0216 18:38:35.060671 140047119996672 logging_writer.py:48] [22428] accumulated_eval_time=1746.988582, accumulated_logging_time=0.586367, accumulated_submission_time=17298.253360, global_step=22428, preemption_count=0, score=17298.253360, test/ctc_loss=0.41050902009010315, test/num_examples=2472, test/wer=0.130969, total_duration=19046.855653, train/ctc_loss=0.5352324843406677, train/wer=0.169379, validation/ctc_loss=0.6940025687217712, validation/num_examples=5348, validation/wer=0.198133
I0216 18:39:29.832901 140047111603968 logging_writer.py:48] [22500] global_step=22500, grad_norm=2.1917922496795654, loss=1.6378021240234375
I0216 18:40:44.855784 140047119996672 logging_writer.py:48] [22600] global_step=22600, grad_norm=2.5995447635650635, loss=1.5635102987289429
I0216 18:42:03.578153 140047119996672 logging_writer.py:48] [22700] global_step=22700, grad_norm=2.0620126724243164, loss=1.5760911703109741
I0216 18:43:18.786631 140047111603968 logging_writer.py:48] [22800] global_step=22800, grad_norm=2.905768394470215, loss=1.6648670434951782
I0216 18:44:33.642998 140047119996672 logging_writer.py:48] [22900] global_step=22900, grad_norm=2.2544972896575928, loss=1.6681625843048096
I0216 18:45:48.629020 140047111603968 logging_writer.py:48] [23000] global_step=23000, grad_norm=2.206676959991455, loss=1.646878719329834
I0216 18:47:03.704993 140047119996672 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.796058177947998, loss=1.6002141237258911
I0216 18:48:18.803732 140047111603968 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.5942002534866333, loss=1.5745224952697754
I0216 18:49:34.279541 140047119996672 logging_writer.py:48] [23300] global_step=23300, grad_norm=2.2948551177978516, loss=1.63296377658844
I0216 18:50:56.041923 140047111603968 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.8136107921600342, loss=1.5854988098144531
I0216 18:52:17.053406 140047119996672 logging_writer.py:48] [23500] global_step=23500, grad_norm=2.665479898452759, loss=1.6279401779174805
I0216 18:53:37.671978 140047111603968 logging_writer.py:48] [23600] global_step=23600, grad_norm=2.4261741638183594, loss=1.5852738618850708
I0216 18:55:02.996165 140047119996672 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.7867655754089355, loss=1.531239628791809
I0216 18:56:18.008385 140047111603968 logging_writer.py:48] [23800] global_step=23800, grad_norm=2.5891573429107666, loss=1.6533749103546143
I0216 18:57:33.074948 140047119996672 logging_writer.py:48] [23900] global_step=23900, grad_norm=2.005711317062378, loss=1.5929497480392456
I0216 18:58:48.484259 140047111603968 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.7896524667739868, loss=1.577495813369751
I0216 19:00:03.849250 140047119996672 logging_writer.py:48] [24100] global_step=24100, grad_norm=3.915485382080078, loss=1.5388797521591187
I0216 19:01:18.889420 140047111603968 logging_writer.py:48] [24200] global_step=24200, grad_norm=2.0178747177124023, loss=1.5856075286865234
I0216 19:02:33.971560 140047119996672 logging_writer.py:48] [24300] global_step=24300, grad_norm=2.4627833366394043, loss=1.627906084060669
I0216 19:02:35.344358 140202902193984 spec.py:321] Evaluating on the training split.
I0216 19:03:28.548864 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 19:04:19.010942 140202902193984 spec.py:349] Evaluating on the test split.
I0216 19:04:44.759448 140202902193984 submission_runner.py:408] Time since start: 20616.59s, 	Step: 24303, 	{'train/ctc_loss': Array(0.5379182, dtype=float32), 'train/wer': 0.17296872508598893, 'validation/ctc_loss': Array(0.6751597, dtype=float32), 'validation/wer': 0.19240758083358275, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40436313, dtype=float32), 'test/wer': 0.12861292222696158, 'test/num_examples': 2472, 'score': 18738.440461158752, 'total_duration': 20616.59484243393, 'accumulated_submission_time': 18738.440461158752, 'accumulated_eval_time': 1876.3985664844513, 'accumulated_logging_time': 0.6419112682342529}
I0216 19:04:44.796473 140047119996672 logging_writer.py:48] [24303] accumulated_eval_time=1876.398566, accumulated_logging_time=0.641911, accumulated_submission_time=18738.440461, global_step=24303, preemption_count=0, score=18738.440461, test/ctc_loss=0.40436312556266785, test/num_examples=2472, test/wer=0.128613, total_duration=20616.594842, train/ctc_loss=0.537918210029602, train/wer=0.172969, validation/ctc_loss=0.6751596927642822, validation/num_examples=5348, validation/wer=0.192408
I0216 19:05:58.372653 140047111603968 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.7218117713928223, loss=1.615189790725708
I0216 19:07:13.741752 140047119996672 logging_writer.py:48] [24500] global_step=24500, grad_norm=2.5628135204315186, loss=1.61649751663208
I0216 19:08:28.840811 140047111603968 logging_writer.py:48] [24600] global_step=24600, grad_norm=3.1111221313476562, loss=1.629032850265503
I0216 19:09:43.819349 140047119996672 logging_writer.py:48] [24700] global_step=24700, grad_norm=2.3581016063690186, loss=1.5890684127807617
I0216 19:11:02.122878 140047119996672 logging_writer.py:48] [24800] global_step=24800, grad_norm=3.81746244430542, loss=1.5473932027816772
I0216 19:12:17.279455 140047111603968 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.6390427350997925, loss=1.585667610168457
I0216 19:13:32.264407 140047119996672 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.9724500179290771, loss=1.6538134813308716
I0216 19:14:47.289879 140047111603968 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.9546873569488525, loss=1.5226202011108398
I0216 19:16:02.495776 140047119996672 logging_writer.py:48] [25200] global_step=25200, grad_norm=2.225792646408081, loss=1.6107220649719238
I0216 19:17:17.773955 140047111603968 logging_writer.py:48] [25300] global_step=25300, grad_norm=2.193852424621582, loss=1.5041162967681885
I0216 19:18:33.189444 140047119996672 logging_writer.py:48] [25400] global_step=25400, grad_norm=2.783726930618286, loss=1.5958439111709595
I0216 19:19:54.514947 140047111603968 logging_writer.py:48] [25500] global_step=25500, grad_norm=2.0905601978302, loss=1.5265668630599976
I0216 19:21:14.917345 140047119996672 logging_writer.py:48] [25600] global_step=25600, grad_norm=2.1048707962036133, loss=1.5289411544799805
I0216 19:22:35.032272 140047111603968 logging_writer.py:48] [25700] global_step=25700, grad_norm=2.1692581176757812, loss=1.528869390487671
I0216 19:23:56.501831 140047119996672 logging_writer.py:48] [25800] global_step=25800, grad_norm=2.181957483291626, loss=1.6206024885177612
I0216 19:25:11.765789 140047111603968 logging_writer.py:48] [25900] global_step=25900, grad_norm=3.340609550476074, loss=1.579879879951477
I0216 19:26:26.673150 140047119996672 logging_writer.py:48] [26000] global_step=26000, grad_norm=2.715909004211426, loss=1.6060000658035278
I0216 19:27:41.554196 140047111603968 logging_writer.py:48] [26100] global_step=26100, grad_norm=2.120800256729126, loss=1.5804789066314697
I0216 19:28:45.149017 140202902193984 spec.py:321] Evaluating on the training split.
I0216 19:29:38.181642 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 19:30:29.514917 140202902193984 spec.py:349] Evaluating on the test split.
I0216 19:30:55.450533 140202902193984 submission_runner.py:408] Time since start: 22187.28s, 	Step: 26186, 	{'train/ctc_loss': Array(0.5195341, dtype=float32), 'train/wer': 0.16668715976794357, 'validation/ctc_loss': Array(0.64813316, dtype=float32), 'validation/wer': 0.1873002693648204, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38142642, dtype=float32), 'test/wer': 0.12446935998212581, 'test/num_examples': 2472, 'score': 20178.6946849823, 'total_duration': 22187.284984588623, 'accumulated_submission_time': 20178.6946849823, 'accumulated_eval_time': 2006.694060087204, 'accumulated_logging_time': 0.6964304447174072}
I0216 19:30:55.486803 140047119996672 logging_writer.py:48] [26186] accumulated_eval_time=2006.694060, accumulated_logging_time=0.696430, accumulated_submission_time=20178.694685, global_step=26186, preemption_count=0, score=20178.694685, test/ctc_loss=0.3814264237880707, test/num_examples=2472, test/wer=0.124469, total_duration=22187.284985, train/ctc_loss=0.5195341110229492, train/wer=0.166687, validation/ctc_loss=0.6481331586837769, validation/num_examples=5348, validation/wer=0.187300
I0216 19:31:06.864732 140047111603968 logging_writer.py:48] [26200] global_step=26200, grad_norm=2.1314308643341064, loss=1.5608856678009033
I0216 19:32:21.980465 140047119996672 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.988250970840454, loss=1.545905590057373
I0216 19:33:37.045034 140047111603968 logging_writer.py:48] [26400] global_step=26400, grad_norm=2.8382792472839355, loss=1.5225924253463745
I0216 19:34:51.872296 140047119996672 logging_writer.py:48] [26500] global_step=26500, grad_norm=2.718139410018921, loss=1.5821903944015503
I0216 19:36:07.053210 140047111603968 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.9127979278564453, loss=1.5774258375167847
I0216 19:37:24.444489 140047119996672 logging_writer.py:48] [26700] global_step=26700, grad_norm=3.729691743850708, loss=1.5849030017852783
I0216 19:38:47.299638 140047119996672 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.917478084564209, loss=1.546410083770752
I0216 19:40:02.404888 140047111603968 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.4474536180496216, loss=1.5224043130874634
I0216 19:41:17.681525 140047119996672 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.9454245567321777, loss=1.5516859292984009
I0216 19:42:32.776405 140047111603968 logging_writer.py:48] [27100] global_step=27100, grad_norm=3.326401710510254, loss=1.4983439445495605
I0216 19:43:48.080194 140047119996672 logging_writer.py:48] [27200] global_step=27200, grad_norm=2.425612211227417, loss=1.516486406326294
I0216 19:45:03.190572 140047111603968 logging_writer.py:48] [27300] global_step=27300, grad_norm=3.2838737964630127, loss=1.559604525566101
I0216 19:46:18.209543 140047119996672 logging_writer.py:48] [27400] global_step=27400, grad_norm=2.5592544078826904, loss=1.5878444910049438
I0216 19:47:33.207593 140047111603968 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.617166519165039, loss=1.5376712083816528
I0216 19:48:50.942094 140047119996672 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.9443398714065552, loss=1.5836538076400757
I0216 19:50:11.905532 140047111603968 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.6767622232437134, loss=1.438807487487793
I0216 19:51:33.823012 140047119996672 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.650123953819275, loss=1.5696574449539185
I0216 19:52:52.391841 140047119996672 logging_writer.py:48] [27900] global_step=27900, grad_norm=2.402921199798584, loss=1.4853819608688354
I0216 19:54:07.478601 140047111603968 logging_writer.py:48] [28000] global_step=28000, grad_norm=2.1790575981140137, loss=1.509595513343811
I0216 19:54:55.818155 140202902193984 spec.py:321] Evaluating on the training split.
I0216 19:55:58.461132 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 19:56:48.736366 140202902193984 spec.py:349] Evaluating on the test split.
I0216 19:57:14.072589 140202902193984 submission_runner.py:408] Time since start: 23765.91s, 	Step: 28066, 	{'train/ctc_loss': Array(0.31689537, dtype=float32), 'train/wer': 0.10663526136880705, 'validation/ctc_loss': Array(0.6243423, dtype=float32), 'validation/wer': 0.17715322899871594, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3617607, dtype=float32), 'test/wer': 0.11589787337761258, 'test/num_examples': 2472, 'score': 21618.925753593445, 'total_duration': 23765.90765786171, 'accumulated_submission_time': 21618.925753593445, 'accumulated_eval_time': 2144.9430723190308, 'accumulated_logging_time': 0.751788854598999}
I0216 19:57:14.113735 140047119996672 logging_writer.py:48] [28066] accumulated_eval_time=2144.943072, accumulated_logging_time=0.751789, accumulated_submission_time=21618.925754, global_step=28066, preemption_count=0, score=21618.925754, test/ctc_loss=0.3617607057094574, test/num_examples=2472, test/wer=0.115898, total_duration=23765.907658, train/ctc_loss=0.31689536571502686, train/wer=0.106635, validation/ctc_loss=0.6243423223495483, validation/num_examples=5348, validation/wer=0.177153
I0216 19:57:40.360069 140047111603968 logging_writer.py:48] [28100] global_step=28100, grad_norm=2.0172181129455566, loss=1.5247362852096558
I0216 19:58:55.622341 140047119996672 logging_writer.py:48] [28200] global_step=28200, grad_norm=2.8033130168914795, loss=1.5028154850006104
I0216 20:00:10.668498 140047111603968 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.8070696592330933, loss=1.5010604858398438
I0216 20:01:25.631461 140047119996672 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.6126071214675903, loss=1.5158578157424927
I0216 20:02:40.713113 140047111603968 logging_writer.py:48] [28500] global_step=28500, grad_norm=2.130237102508545, loss=1.5344196557998657
I0216 20:03:55.829379 140047119996672 logging_writer.py:48] [28600] global_step=28600, grad_norm=2.1107163429260254, loss=1.5283719301223755
I0216 20:05:12.184488 140047111603968 logging_writer.py:48] [28700] global_step=28700, grad_norm=3.091409921646118, loss=1.6364880800247192
I0216 20:06:35.489373 140047119996672 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.8189464807510376, loss=1.508594274520874
I0216 20:07:55.375337 140047119996672 logging_writer.py:48] [28900] global_step=28900, grad_norm=2.62493896484375, loss=1.5018527507781982
I0216 20:09:10.712610 140047111603968 logging_writer.py:48] [29000] global_step=29000, grad_norm=3.9401278495788574, loss=1.5462092161178589
I0216 20:10:25.839449 140047119996672 logging_writer.py:48] [29100] global_step=29100, grad_norm=2.2115232944488525, loss=1.5140873193740845
I0216 20:11:41.040047 140047111603968 logging_writer.py:48] [29200] global_step=29200, grad_norm=3.162785053253174, loss=1.480761170387268
I0216 20:12:56.153058 140047119996672 logging_writer.py:48] [29300] global_step=29300, grad_norm=2.0938875675201416, loss=1.5068998336791992
I0216 20:14:11.405847 140047111603968 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.4344372749328613, loss=1.5340039730072021
I0216 20:15:28.104773 140047119996672 logging_writer.py:48] [29500] global_step=29500, grad_norm=4.142569065093994, loss=1.533754825592041
I0216 20:16:48.384041 140047111603968 logging_writer.py:48] [29600] global_step=29600, grad_norm=2.758307456970215, loss=1.4925364255905151
I0216 20:18:08.930422 140047119996672 logging_writer.py:48] [29700] global_step=29700, grad_norm=2.5219762325286865, loss=1.5338603258132935
I0216 20:19:30.272493 140047111603968 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.8375734090805054, loss=1.4241498708724976
I0216 20:20:51.981369 140047119996672 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.7351713180541992, loss=1.5290659666061401
I0216 20:21:14.383562 140202902193984 spec.py:321] Evaluating on the training split.
I0216 20:22:10.131985 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 20:23:00.758638 140202902193984 spec.py:349] Evaluating on the test split.
I0216 20:23:26.131524 140202902193984 submission_runner.py:408] Time since start: 25337.97s, 	Step: 29931, 	{'train/ctc_loss': Array(0.28561276, dtype=float32), 'train/wer': 0.09474287839806578, 'validation/ctc_loss': Array(0.57865715, dtype=float32), 'validation/wer': 0.16738272010195313, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33669013, dtype=float32), 'test/wer': 0.10907318262141247, 'test/num_examples': 2472, 'score': 23059.09961295128, 'total_duration': 25337.965800762177, 'accumulated_submission_time': 23059.09961295128, 'accumulated_eval_time': 2276.684823989868, 'accumulated_logging_time': 0.8093042373657227}
I0216 20:23:26.169116 140047119996672 logging_writer.py:48] [29931] accumulated_eval_time=2276.684824, accumulated_logging_time=0.809304, accumulated_submission_time=23059.099613, global_step=29931, preemption_count=0, score=23059.099613, test/ctc_loss=0.33669012784957886, test/num_examples=2472, test/wer=0.109073, total_duration=25337.965801, train/ctc_loss=0.2856127619743347, train/wer=0.094743, validation/ctc_loss=0.5786571502685547, validation/num_examples=5348, validation/wer=0.167383
I0216 20:24:18.631894 140047111603968 logging_writer.py:48] [30000] global_step=30000, grad_norm=2.8621666431427, loss=1.4537858963012695
I0216 20:25:33.958903 140047119996672 logging_writer.py:48] [30100] global_step=30100, grad_norm=2.4549880027770996, loss=1.4991151094436646
I0216 20:26:49.095273 140047111603968 logging_writer.py:48] [30200] global_step=30200, grad_norm=3.0824174880981445, loss=1.4674031734466553
I0216 20:28:04.357556 140047119996672 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.7246365547180176, loss=1.472624659538269
I0216 20:29:19.307305 140047111603968 logging_writer.py:48] [30400] global_step=30400, grad_norm=2.0230400562286377, loss=1.497907042503357
I0216 20:30:34.583022 140047119996672 logging_writer.py:48] [30500] global_step=30500, grad_norm=2.8368124961853027, loss=1.4391506910324097
I0216 20:31:49.677096 140047111603968 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.753111720085144, loss=1.483984351158142
I0216 20:33:07.733338 140047119996672 logging_writer.py:48] [30700] global_step=30700, grad_norm=2.187936782836914, loss=1.4951484203338623
I0216 20:34:28.697544 140047111603968 logging_writer.py:48] [30800] global_step=30800, grad_norm=3.8829500675201416, loss=1.5223445892333984
I0216 20:35:52.716354 140047119996672 logging_writer.py:48] [30900] global_step=30900, grad_norm=2.206653118133545, loss=1.4172070026397705
I0216 20:37:08.080825 140047111603968 logging_writer.py:48] [31000] global_step=31000, grad_norm=2.4546267986297607, loss=1.5070222616195679
I0216 20:38:23.307368 140047119996672 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.6676759719848633, loss=1.4614448547363281
I0216 20:39:38.476891 140047111603968 logging_writer.py:48] [31200] global_step=31200, grad_norm=3.435556173324585, loss=1.4763261079788208
I0216 20:40:53.617452 140047119996672 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.8825788497924805, loss=1.4767638444900513
I0216 20:42:08.823899 140047111603968 logging_writer.py:48] [31400] global_step=31400, grad_norm=3.6013708114624023, loss=1.515841007232666
I0216 20:43:24.014997 140047119996672 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.7604652643203735, loss=1.4908168315887451
I0216 20:44:42.866836 140047111603968 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.8783341646194458, loss=1.4413938522338867
I0216 20:46:04.169259 140047119996672 logging_writer.py:48] [31700] global_step=31700, grad_norm=2.6171610355377197, loss=1.4756335020065308
I0216 20:47:24.923906 140047111603968 logging_writer.py:48] [31800] global_step=31800, grad_norm=2.6378397941589355, loss=1.453450083732605
I0216 20:47:26.294681 140202902193984 spec.py:321] Evaluating on the training split.
I0216 20:48:22.273120 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 20:49:12.619574 140202902193984 spec.py:349] Evaluating on the test split.
I0216 20:49:38.343957 140202902193984 submission_runner.py:408] Time since start: 26910.18s, 	Step: 31803, 	{'train/ctc_loss': Array(0.27346468, dtype=float32), 'train/wer': 0.09277049516508132, 'validation/ctc_loss': Array(0.5726918, dtype=float32), 'validation/wer': 0.16434150438803982, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32298186, dtype=float32), 'test/wer': 0.10346718664310524, 'test/num_examples': 2472, 'score': 24499.127238988876, 'total_duration': 26910.178052663803, 'accumulated_submission_time': 24499.127238988876, 'accumulated_eval_time': 2408.727742433548, 'accumulated_logging_time': 0.86600661277771}
I0216 20:49:38.384888 140047119996672 logging_writer.py:48] [31803] accumulated_eval_time=2408.727742, accumulated_logging_time=0.866007, accumulated_submission_time=24499.127239, global_step=31803, preemption_count=0, score=24499.127239, test/ctc_loss=0.3229818642139435, test/num_examples=2472, test/wer=0.103467, total_duration=26910.178053, train/ctc_loss=0.2734646797180176, train/wer=0.092770, validation/ctc_loss=0.572691798210144, validation/num_examples=5348, validation/wer=0.164342
I0216 20:50:51.845923 140047111603968 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.5967785120010376, loss=1.4574452638626099
I0216 20:52:10.463199 140047119996672 logging_writer.py:48] [32000] global_step=32000, grad_norm=2.325500249862671, loss=1.5202109813690186
I0216 20:53:25.490546 140047111603968 logging_writer.py:48] [32100] global_step=32100, grad_norm=2.646542549133301, loss=1.3849830627441406
I0216 20:54:40.463785 140047119996672 logging_writer.py:48] [32200] global_step=32200, grad_norm=2.1471927165985107, loss=1.4357486963272095
I0216 20:55:55.498721 140047111603968 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.7819006443023682, loss=1.4552339315414429
I0216 20:57:10.662872 140047119996672 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.6304705142974854, loss=1.3924751281738281
I0216 20:58:25.751627 140047111603968 logging_writer.py:48] [32500] global_step=32500, grad_norm=2.512117862701416, loss=1.4494171142578125
I0216 20:59:45.472123 140047119996672 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.7471562623977661, loss=1.40509033203125
I0216 21:01:06.237894 140047111603968 logging_writer.py:48] [32700] global_step=32700, grad_norm=2.1247830390930176, loss=1.37053644657135
I0216 21:02:26.404095 140047119996672 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.8191053867340088, loss=1.386225938796997
I0216 21:03:46.743140 140047111603968 logging_writer.py:48] [32900] global_step=32900, grad_norm=3.538688898086548, loss=1.449048399925232
I0216 21:05:07.853211 140047119996672 logging_writer.py:48] [33000] global_step=33000, grad_norm=2.0560641288757324, loss=1.424796223640442
I0216 21:06:22.995552 140047111603968 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.4346816539764404, loss=1.3622990846633911
I0216 21:07:37.968088 140047119996672 logging_writer.py:48] [33200] global_step=33200, grad_norm=2.4151480197906494, loss=1.4277820587158203
I0216 21:08:52.999085 140047111603968 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.9102360010147095, loss=1.4553334712982178
I0216 21:10:07.790480 140047119996672 logging_writer.py:48] [33400] global_step=33400, grad_norm=3.8761017322540283, loss=1.4048666954040527
I0216 21:11:22.912818 140047111603968 logging_writer.py:48] [33500] global_step=33500, grad_norm=2.3131871223449707, loss=1.4199079275131226
I0216 21:12:37.856667 140047119996672 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.8875280618667603, loss=1.4072368144989014
I0216 21:13:38.449162 140202902193984 spec.py:321] Evaluating on the training split.
I0216 21:14:34.403452 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 21:15:24.908138 140202902193984 spec.py:349] Evaluating on the test split.
I0216 21:15:50.221428 140202902193984 submission_runner.py:408] Time since start: 28482.06s, 	Step: 33678, 	{'train/ctc_loss': Array(0.24806608, dtype=float32), 'train/wer': 0.0840600825978491, 'validation/ctc_loss': Array(0.53813505, dtype=float32), 'validation/wer': 0.15382758720565376, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.306992, dtype=float32), 'test/wer': 0.09763776328885097, 'test/num_examples': 2472, 'score': 25939.097049236298, 'total_duration': 28482.056171655655, 'accumulated_submission_time': 25939.097049236298, 'accumulated_eval_time': 2540.49427819252, 'accumulated_logging_time': 0.921947717666626}
I0216 21:15:50.256604 140047119996672 logging_writer.py:48] [33678] accumulated_eval_time=2540.494278, accumulated_logging_time=0.921948, accumulated_submission_time=25939.097049, global_step=33678, preemption_count=0, score=25939.097049, test/ctc_loss=0.3069919943809509, test/num_examples=2472, test/wer=0.097638, total_duration=28482.056172, train/ctc_loss=0.24806608259677887, train/wer=0.084060, validation/ctc_loss=0.5381350517272949, validation/num_examples=5348, validation/wer=0.153828
I0216 21:16:07.760488 140047111603968 logging_writer.py:48] [33700] global_step=33700, grad_norm=3.3962936401367188, loss=1.406785249710083
I0216 21:17:22.924997 140047119996672 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.7786706686019897, loss=1.4954382181167603
I0216 21:18:38.099413 140047111603968 logging_writer.py:48] [33900] global_step=33900, grad_norm=2.218428611755371, loss=1.3881149291992188
I0216 21:19:56.493480 140047119996672 logging_writer.py:48] [34000] global_step=34000, grad_norm=2.4885497093200684, loss=1.3589473962783813
I0216 21:21:11.630456 140047111603968 logging_writer.py:48] [34100] global_step=34100, grad_norm=3.2816989421844482, loss=1.3940335512161255
I0216 21:22:26.708503 140047119996672 logging_writer.py:48] [34200] global_step=34200, grad_norm=2.351548671722412, loss=1.3846410512924194
I0216 21:23:41.751391 140047111603968 logging_writer.py:48] [34300] global_step=34300, grad_norm=2.0152230262756348, loss=1.3998217582702637
I0216 21:24:56.743935 140047119996672 logging_writer.py:48] [34400] global_step=34400, grad_norm=2.7073912620544434, loss=1.3813774585723877
I0216 21:26:11.713881 140047111603968 logging_writer.py:48] [34500] global_step=34500, grad_norm=2.360616445541382, loss=1.4079865217208862
I0216 21:27:26.817757 140047119996672 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.636094331741333, loss=1.3226066827774048
I0216 21:28:44.705319 140047111603968 logging_writer.py:48] [34700] global_step=34700, grad_norm=2.5900166034698486, loss=1.3437354564666748
I0216 21:30:05.142002 140047119996672 logging_writer.py:48] [34800] global_step=34800, grad_norm=3.075382709503174, loss=1.4326369762420654
I0216 21:31:25.142921 140047111603968 logging_writer.py:48] [34900] global_step=34900, grad_norm=3.3098654747009277, loss=1.3301066160202026
I0216 21:32:46.166890 140047119996672 logging_writer.py:48] [35000] global_step=35000, grad_norm=2.077056884765625, loss=1.3481911420822144
I0216 21:34:05.690390 140047119996672 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.816257119178772, loss=1.3210445642471313
I0216 21:35:20.733849 140047111603968 logging_writer.py:48] [35200] global_step=35200, grad_norm=2.575932264328003, loss=1.313961386680603
I0216 21:36:35.772982 140047119996672 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.780355453491211, loss=1.3171889781951904
I0216 21:37:50.811934 140047111603968 logging_writer.py:48] [35400] global_step=35400, grad_norm=2.4598631858825684, loss=1.358812928199768
I0216 21:39:05.752714 140047119996672 logging_writer.py:48] [35500] global_step=35500, grad_norm=2.8146634101867676, loss=1.333044171333313
I0216 21:39:50.733924 140202902193984 spec.py:321] Evaluating on the training split.
I0216 21:40:46.327543 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 21:41:36.970745 140202902193984 spec.py:349] Evaluating on the test split.
I0216 21:42:02.785205 140202902193984 submission_runner.py:408] Time since start: 30054.62s, 	Step: 35561, 	{'train/ctc_loss': Array(0.23072459, dtype=float32), 'train/wer': 0.07805270676531417, 'validation/ctc_loss': Array(0.5039565, dtype=float32), 'validation/wer': 0.1447715226353341, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28383914, dtype=float32), 'test/wer': 0.08965531249365263, 'test/num_examples': 2472, 'score': 27379.475867271423, 'total_duration': 30054.618946552277, 'accumulated_submission_time': 27379.475867271423, 'accumulated_eval_time': 2672.538813829422, 'accumulated_logging_time': 0.9750196933746338}
I0216 21:42:02.828570 140047119996672 logging_writer.py:48] [35561] accumulated_eval_time=2672.538814, accumulated_logging_time=0.975020, accumulated_submission_time=27379.475867, global_step=35561, preemption_count=0, score=27379.475867, test/ctc_loss=0.2838391363620758, test/num_examples=2472, test/wer=0.089655, total_duration=30054.618947, train/ctc_loss=0.23072458803653717, train/wer=0.078053, validation/ctc_loss=0.5039564967155457, validation/num_examples=5348, validation/wer=0.144772
I0216 21:42:32.930243 140047111603968 logging_writer.py:48] [35600] global_step=35600, grad_norm=4.134631156921387, loss=1.4244053363800049
I0216 21:43:48.136369 140047119996672 logging_writer.py:48] [35700] global_step=35700, grad_norm=3.405433177947998, loss=1.333289384841919
I0216 21:45:03.818249 140047111603968 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.9404902458190918, loss=1.3089799880981445
I0216 21:46:19.110995 140047119996672 logging_writer.py:48] [35900] global_step=35900, grad_norm=2.160804271697998, loss=1.4085575342178345
I0216 21:47:34.087330 140047111603968 logging_writer.py:48] [36000] global_step=36000, grad_norm=4.6715898513793945, loss=1.3427510261535645
I0216 21:48:52.655880 140047119996672 logging_writer.py:48] [36100] global_step=36100, grad_norm=2.909562349319458, loss=1.3753337860107422
I0216 21:50:07.675468 140047111603968 logging_writer.py:48] [36200] global_step=36200, grad_norm=2.493746519088745, loss=1.3012256622314453
I0216 21:51:22.781326 140047119996672 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.7189387083053589, loss=1.3750309944152832
I0216 21:52:37.819487 140047111603968 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.816446304321289, loss=1.2754802703857422
I0216 21:53:52.942471 140047119996672 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.595749855041504, loss=1.3519296646118164
I0216 21:55:08.081394 140047111603968 logging_writer.py:48] [36600] global_step=36600, grad_norm=2.7710530757904053, loss=1.3434935808181763
I0216 21:56:24.636438 140047119996672 logging_writer.py:48] [36700] global_step=36700, grad_norm=3.3069958686828613, loss=1.2928990125656128
I0216 21:57:44.988742 140047111603968 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.7094413042068481, loss=1.2851430177688599
I0216 21:59:06.110831 140047119996672 logging_writer.py:48] [36900] global_step=36900, grad_norm=2.2656540870666504, loss=1.3462013006210327
I0216 22:00:26.809125 140047111603968 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.8410301208496094, loss=1.3387068510055542
I0216 22:01:49.377810 140047119996672 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.983949899673462, loss=1.294105052947998
I0216 22:03:04.410536 140047111603968 logging_writer.py:48] [37200] global_step=37200, grad_norm=2.2638533115386963, loss=1.3116235733032227
I0216 22:04:19.379087 140047119996672 logging_writer.py:48] [37300] global_step=37300, grad_norm=2.9188990592956543, loss=1.3302345275878906
I0216 22:05:34.466871 140047111603968 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.8569387197494507, loss=1.3601080179214478
I0216 22:06:03.478433 140202902193984 spec.py:321] Evaluating on the training split.
I0216 22:06:58.769537 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 22:07:50.052374 140202902193984 spec.py:349] Evaluating on the test split.
I0216 22:08:15.953912 140202902193984 submission_runner.py:408] Time since start: 31627.79s, 	Step: 37440, 	{'train/ctc_loss': Array(0.20172007, dtype=float32), 'train/wer': 0.06983904385830798, 'validation/ctc_loss': Array(0.47738373, dtype=float32), 'validation/wer': 0.13859254467690704, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26603636, dtype=float32), 'test/wer': 0.08486177970060732, 'test/num_examples': 2472, 'score': 28820.02853703499, 'total_duration': 31627.78924536705, 'accumulated_submission_time': 28820.02853703499, 'accumulated_eval_time': 2805.0091433525085, 'accumulated_logging_time': 1.0354893207550049}
I0216 22:08:15.996069 140047119996672 logging_writer.py:48] [37440] accumulated_eval_time=2805.009143, accumulated_logging_time=1.035489, accumulated_submission_time=28820.028537, global_step=37440, preemption_count=0, score=28820.028537, test/ctc_loss=0.26603636145591736, test/num_examples=2472, test/wer=0.084862, total_duration=31627.789245, train/ctc_loss=0.20172007381916046, train/wer=0.069839, validation/ctc_loss=0.47738373279571533, validation/num_examples=5348, validation/wer=0.138593
I0216 22:09:01.703963 140047111603968 logging_writer.py:48] [37500] global_step=37500, grad_norm=2.7335798740386963, loss=1.2983282804489136
I0216 22:10:16.526996 140047119996672 logging_writer.py:48] [37600] global_step=37600, grad_norm=2.8258633613586426, loss=1.3012349605560303
I0216 22:11:31.611994 140047111603968 logging_writer.py:48] [37700] global_step=37700, grad_norm=4.33758020401001, loss=1.3122042417526245
I0216 22:12:46.543582 140047119996672 logging_writer.py:48] [37800] global_step=37800, grad_norm=2.571664810180664, loss=1.3293936252593994
I0216 22:14:01.660608 140047111603968 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.806808352470398, loss=1.2608009576797485
I0216 22:15:18.754252 140047119996672 logging_writer.py:48] [38000] global_step=38000, grad_norm=2.0760836601257324, loss=1.2825629711151123
I0216 22:16:39.982938 140047111603968 logging_writer.py:48] [38100] global_step=38100, grad_norm=3.3351805210113525, loss=1.2713871002197266
I0216 22:17:58.684796 140047119996672 logging_writer.py:48] [38200] global_step=38200, grad_norm=2.7387430667877197, loss=1.2407678365707397
I0216 22:19:13.795240 140047111603968 logging_writer.py:48] [38300] global_step=38300, grad_norm=2.7828173637390137, loss=1.321239948272705
I0216 22:20:28.826653 140047119996672 logging_writer.py:48] [38400] global_step=38400, grad_norm=2.1682186126708984, loss=1.3057620525360107
I0216 22:21:44.039839 140047111603968 logging_writer.py:48] [38500] global_step=38500, grad_norm=2.4175522327423096, loss=1.3221195936203003
I0216 22:22:59.002468 140047119996672 logging_writer.py:48] [38600] global_step=38600, grad_norm=2.431614398956299, loss=1.2485207319259644
I0216 22:24:13.726635 140047111603968 logging_writer.py:48] [38700] global_step=38700, grad_norm=2.754275321960449, loss=1.2560399770736694
I0216 22:25:31.547713 140047119996672 logging_writer.py:48] [38800] global_step=38800, grad_norm=2.6114068031311035, loss=1.3113865852355957
I0216 22:26:52.646689 140047111603968 logging_writer.py:48] [38900] global_step=38900, grad_norm=2.8655786514282227, loss=1.2833889722824097
I0216 22:28:13.337194 140047119996672 logging_writer.py:48] [39000] global_step=39000, grad_norm=2.0047824382781982, loss=1.277037501335144
I0216 22:29:35.348858 140047111603968 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.579066514968872, loss=1.2498630285263062
I0216 22:30:55.670648 140047119996672 logging_writer.py:48] [39200] global_step=39200, grad_norm=2.4644646644592285, loss=1.2518224716186523
I0216 22:32:10.678453 140047111603968 logging_writer.py:48] [39300] global_step=39300, grad_norm=2.3621785640716553, loss=1.2555150985717773
I0216 22:32:16.535219 140202902193984 spec.py:321] Evaluating on the training split.
I0216 22:33:10.949193 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 22:34:02.245958 140202902193984 spec.py:349] Evaluating on the test split.
I0216 22:34:27.989686 140202902193984 submission_runner.py:408] Time since start: 33199.83s, 	Step: 39309, 	{'train/ctc_loss': Array(0.22671203, dtype=float32), 'train/wer': 0.07333054206745801, 'validation/ctc_loss': Array(0.45624575, dtype=float32), 'validation/wer': 0.13130328161657512, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25134808, dtype=float32), 'test/wer': 0.07992606585014117, 'test/num_examples': 2472, 'score': 30260.46949505806, 'total_duration': 33199.82506918907, 'accumulated_submission_time': 30260.46949505806, 'accumulated_eval_time': 2936.458508014679, 'accumulated_logging_time': 1.0954077243804932}
I0216 22:34:28.032446 140047119996672 logging_writer.py:48] [39309] accumulated_eval_time=2936.458508, accumulated_logging_time=1.095408, accumulated_submission_time=30260.469495, global_step=39309, preemption_count=0, score=30260.469495, test/ctc_loss=0.251348078250885, test/num_examples=2472, test/wer=0.079926, total_duration=33199.825069, train/ctc_loss=0.22671203315258026, train/wer=0.073331, validation/ctc_loss=0.4562457501888275, validation/num_examples=5348, validation/wer=0.131303
I0216 22:35:37.019023 140047111603968 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.3348777294158936, loss=1.1686041355133057
I0216 22:36:51.907901 140047119996672 logging_writer.py:48] [39500] global_step=39500, grad_norm=2.6345415115356445, loss=1.2684099674224854
I0216 22:38:07.142142 140047111603968 logging_writer.py:48] [39600] global_step=39600, grad_norm=2.1856014728546143, loss=1.2587053775787354
I0216 22:39:22.348440 140047119996672 logging_writer.py:48] [39700] global_step=39700, grad_norm=10.15748119354248, loss=1.2853877544403076
I0216 22:40:37.315759 140047111603968 logging_writer.py:48] [39800] global_step=39800, grad_norm=2.117600679397583, loss=1.2302343845367432
I0216 22:41:52.628031 140047119996672 logging_writer.py:48] [39900] global_step=39900, grad_norm=2.2903506755828857, loss=1.3035780191421509
I0216 22:43:07.761914 140047111603968 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.7700684070587158, loss=1.2285076379776
I0216 22:44:29.572056 140047119996672 logging_writer.py:48] [40100] global_step=40100, grad_norm=2.8924059867858887, loss=1.257405400276184
I0216 22:45:51.995034 140047119996672 logging_writer.py:48] [40200] global_step=40200, grad_norm=3.660087823867798, loss=1.2794636487960815
I0216 22:47:07.113624 140047111603968 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.660106897354126, loss=1.226683497428894
I0216 22:48:22.289434 140047119996672 logging_writer.py:48] [40400] global_step=40400, grad_norm=2.2342724800109863, loss=1.2533397674560547
I0216 22:49:37.046358 140047111603968 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.9134811162948608, loss=1.2118340730667114
I0216 22:50:52.115287 140047119996672 logging_writer.py:48] [40600] global_step=40600, grad_norm=2.1863973140716553, loss=1.2395846843719482
I0216 22:52:07.217629 140047111603968 logging_writer.py:48] [40700] global_step=40700, grad_norm=4.211133003234863, loss=1.2703964710235596
I0216 22:53:22.497997 140047119996672 logging_writer.py:48] [40800] global_step=40800, grad_norm=2.4929816722869873, loss=1.173640489578247
I0216 22:54:40.842544 140047111603968 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.8315484523773193, loss=1.1930615901947021
I0216 22:56:01.801323 140047119996672 logging_writer.py:48] [41000] global_step=41000, grad_norm=2.0455589294433594, loss=1.1684974431991577
I0216 22:57:22.491536 140047111603968 logging_writer.py:48] [41100] global_step=41100, grad_norm=2.300264596939087, loss=1.2321910858154297
I0216 22:58:28.382124 140202902193984 spec.py:321] Evaluating on the training split.
I0216 22:59:24.216152 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 23:00:15.643974 140202902193984 spec.py:349] Evaluating on the test split.
I0216 23:00:41.365643 140202902193984 submission_runner.py:408] Time since start: 34773.20s, 	Step: 41183, 	{'train/ctc_loss': Array(0.17770247, dtype=float32), 'train/wer': 0.061686958081999484, 'validation/ctc_loss': Array(0.4342081, dtype=float32), 'validation/wer': 0.12494086525000724, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23483199, dtype=float32), 'test/wer': 0.07438100461072858, 'test/num_examples': 2472, 'score': 31700.723588705063, 'total_duration': 34773.198744535446, 'accumulated_submission_time': 31700.723588705063, 'accumulated_eval_time': 3069.434670448303, 'accumulated_logging_time': 1.153717041015625}
I0216 23:00:41.406613 140047119996672 logging_writer.py:48] [41183] accumulated_eval_time=3069.434670, accumulated_logging_time=1.153717, accumulated_submission_time=31700.723589, global_step=41183, preemption_count=0, score=31700.723589, test/ctc_loss=0.23483198881149292, test/num_examples=2472, test/wer=0.074381, total_duration=34773.198745, train/ctc_loss=0.17770247161388397, train/wer=0.061687, validation/ctc_loss=0.43420809507369995, validation/num_examples=5348, validation/wer=0.124941
I0216 23:00:58.370226 140047119996672 logging_writer.py:48] [41200] global_step=41200, grad_norm=2.269231081008911, loss=1.198612928390503
I0216 23:02:13.393573 140047111603968 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.2600109577178955, loss=1.1816219091415405
I0216 23:03:28.474606 140047119996672 logging_writer.py:48] [41400] global_step=41400, grad_norm=2.4685261249542236, loss=1.1873414516448975
I0216 23:04:43.487437 140047111603968 logging_writer.py:48] [41500] global_step=41500, grad_norm=2.2041187286376953, loss=1.2524170875549316
I0216 23:05:58.731660 140047119996672 logging_writer.py:48] [41600] global_step=41600, grad_norm=2.2167415618896484, loss=1.2759873867034912
I0216 23:07:13.912359 140047111603968 logging_writer.py:48] [41700] global_step=41700, grad_norm=2.136892080307007, loss=1.1574995517730713
I0216 23:08:30.228886 140047119996672 logging_writer.py:48] [41800] global_step=41800, grad_norm=2.744443416595459, loss=1.2135870456695557
I0216 23:09:50.956840 140047111603968 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.9473028182983398, loss=1.2777332067489624
I0216 23:11:11.961748 140047119996672 logging_writer.py:48] [42000] global_step=42000, grad_norm=2.20890736579895, loss=1.1668272018432617
I0216 23:12:33.291892 140047111603968 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.524368405342102, loss=1.2682257890701294
I0216 23:13:52.918168 140047119996672 logging_writer.py:48] [42200] global_step=42200, grad_norm=2.00384783744812, loss=1.2550089359283447
I0216 23:15:13.132757 140047119996672 logging_writer.py:48] [42300] global_step=42300, grad_norm=2.8003642559051514, loss=1.2098729610443115
I0216 23:16:28.285621 140047111603968 logging_writer.py:48] [42400] global_step=42400, grad_norm=2.072385549545288, loss=1.2189010381698608
I0216 23:17:43.463044 140047119996672 logging_writer.py:48] [42500] global_step=42500, grad_norm=2.913095235824585, loss=1.1358439922332764
I0216 23:18:58.435939 140047111603968 logging_writer.py:48] [42600] global_step=42600, grad_norm=3.3642187118530273, loss=1.2379902601242065
I0216 23:20:13.489238 140047119996672 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.7302805185317993, loss=1.1819325685501099
I0216 23:21:28.318288 140047111603968 logging_writer.py:48] [42800] global_step=42800, grad_norm=2.997042417526245, loss=1.108609914779663
I0216 23:22:43.431671 140047119996672 logging_writer.py:48] [42900] global_step=42900, grad_norm=2.3836557865142822, loss=1.2065006494522095
I0216 23:24:04.817974 140047111603968 logging_writer.py:48] [43000] global_step=43000, grad_norm=2.759909152984619, loss=1.1956959962844849
I0216 23:24:42.093077 140202902193984 spec.py:321] Evaluating on the training split.
I0216 23:25:38.103837 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 23:26:28.435804 140202902193984 spec.py:349] Evaluating on the test split.
I0216 23:26:53.850542 140202902193984 submission_runner.py:408] Time since start: 36345.68s, 	Step: 43048, 	{'train/ctc_loss': Array(0.15259655, dtype=float32), 'train/wer': 0.05289856212853238, 'validation/ctc_loss': Array(0.41574052, dtype=float32), 'validation/wer': 0.11968873398534424, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2274025, dtype=float32), 'test/wer': 0.07068429711778686, 'test/num_examples': 2472, 'score': 33141.31151199341, 'total_duration': 36345.68494963646, 'accumulated_submission_time': 33141.31151199341, 'accumulated_eval_time': 3201.1860456466675, 'accumulated_logging_time': 1.2138450145721436}
I0216 23:26:53.891504 140047119996672 logging_writer.py:48] [43048] accumulated_eval_time=3201.186046, accumulated_logging_time=1.213845, accumulated_submission_time=33141.311512, global_step=43048, preemption_count=0, score=33141.311512, test/ctc_loss=0.2274024933576584, test/num_examples=2472, test/wer=0.070684, total_duration=36345.684950, train/ctc_loss=0.15259654819965363, train/wer=0.052899, validation/ctc_loss=0.4157405197620392, validation/num_examples=5348, validation/wer=0.119689
I0216 23:27:33.652708 140047111603968 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.8818316459655762, loss=1.1901062726974487
I0216 23:28:48.550299 140047119996672 logging_writer.py:48] [43200] global_step=43200, grad_norm=3.173348903656006, loss=1.2035952806472778
I0216 23:30:06.835594 140047119996672 logging_writer.py:48] [43300] global_step=43300, grad_norm=2.513660430908203, loss=1.1589961051940918
I0216 23:31:21.933120 140047111603968 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.6213735342025757, loss=1.1384150981903076
I0216 23:32:36.889946 140047119996672 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.7300394773483276, loss=1.2123925685882568
I0216 23:33:52.082889 140047111603968 logging_writer.py:48] [43600] global_step=43600, grad_norm=5.021717548370361, loss=1.1599961519241333
I0216 23:35:07.420670 140047119996672 logging_writer.py:48] [43700] global_step=43700, grad_norm=2.5769643783569336, loss=1.1930906772613525
I0216 23:36:22.472357 140047111603968 logging_writer.py:48] [43800] global_step=43800, grad_norm=2.461322784423828, loss=1.184685230255127
I0216 23:37:39.717956 140047119996672 logging_writer.py:48] [43900] global_step=43900, grad_norm=2.5380465984344482, loss=1.2110751867294312
I0216 23:39:01.309992 140047111603968 logging_writer.py:48] [44000] global_step=44000, grad_norm=2.1456298828125, loss=1.1965267658233643
I0216 23:40:21.773195 140047119996672 logging_writer.py:48] [44100] global_step=44100, grad_norm=2.2978687286376953, loss=1.1522681713104248
I0216 23:41:42.939522 140047111603968 logging_writer.py:48] [44200] global_step=44200, grad_norm=2.7787344455718994, loss=1.2527636289596558
I0216 23:43:05.921936 140047119996672 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.9613784551620483, loss=1.1862492561340332
I0216 23:44:20.967230 140047111603968 logging_writer.py:48] [44400] global_step=44400, grad_norm=2.5243635177612305, loss=1.1591793298721313
I0216 23:45:35.779287 140047119996672 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.7645474672317505, loss=1.158758282661438
I0216 23:46:50.864104 140047111603968 logging_writer.py:48] [44600] global_step=44600, grad_norm=2.076214075088501, loss=1.1705940961837769
I0216 23:48:06.003768 140047119996672 logging_writer.py:48] [44700] global_step=44700, grad_norm=2.0307116508483887, loss=1.1156471967697144
I0216 23:49:21.181948 140047111603968 logging_writer.py:48] [44800] global_step=44800, grad_norm=3.2971692085266113, loss=1.1283257007598877
I0216 23:50:36.097859 140047119996672 logging_writer.py:48] [44900] global_step=44900, grad_norm=2.0891172885894775, loss=1.1182090044021606
I0216 23:50:53.937046 140202902193984 spec.py:321] Evaluating on the training split.
I0216 23:51:49.486913 140202902193984 spec.py:333] Evaluating on the validation split.
I0216 23:52:40.676059 140202902193984 spec.py:349] Evaluating on the test split.
I0216 23:53:06.267393 140202902193984 submission_runner.py:408] Time since start: 37918.10s, 	Step: 44925, 	{'train/ctc_loss': Array(0.14536211, dtype=float32), 'train/wer': 0.05018736545565684, 'validation/ctc_loss': Array(0.40926024, dtype=float32), 'validation/wer': 0.11703370439383261, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21977012, dtype=float32), 'test/wer': 0.06828753072126419, 'test/num_examples': 2472, 'score': 34581.259950876236, 'total_duration': 37918.101712465286, 'accumulated_submission_time': 34581.259950876236, 'accumulated_eval_time': 3333.5102150440216, 'accumulated_logging_time': 1.2725615501403809}
I0216 23:53:06.309982 140047119996672 logging_writer.py:48] [44925] accumulated_eval_time=3333.510215, accumulated_logging_time=1.272562, accumulated_submission_time=34581.259951, global_step=44925, preemption_count=0, score=34581.259951, test/ctc_loss=0.21977011859416962, test/num_examples=2472, test/wer=0.068288, total_duration=37918.101712, train/ctc_loss=0.14536210894584656, train/wer=0.050187, validation/ctc_loss=0.40926024317741394, validation/num_examples=5348, validation/wer=0.117034
I0216 23:54:03.551908 140047111603968 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.7961788177490234, loss=1.146462082862854
I0216 23:55:18.788321 140047119996672 logging_writer.py:48] [45100] global_step=45100, grad_norm=2.4509873390197754, loss=1.1490144729614258
I0216 23:56:33.845792 140047111603968 logging_writer.py:48] [45200] global_step=45200, grad_norm=3.7567532062530518, loss=1.172297477722168
I0216 23:57:48.967831 140047119996672 logging_writer.py:48] [45300] global_step=45300, grad_norm=2.4004387855529785, loss=1.155468463897705
I0216 23:59:07.481875 140047119996672 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.9843310117721558, loss=1.145809531211853
I0217 00:00:22.463222 140047111603968 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.895050048828125, loss=1.1098089218139648
I0217 00:01:37.453729 140047119996672 logging_writer.py:48] [45600] global_step=45600, grad_norm=2.1729063987731934, loss=1.1821119785308838
I0217 00:02:52.497888 140047111603968 logging_writer.py:48] [45700] global_step=45700, grad_norm=2.023392915725708, loss=1.1160106658935547
I0217 00:04:07.767433 140047119996672 logging_writer.py:48] [45800] global_step=45800, grad_norm=2.5531692504882812, loss=1.1140607595443726
I0217 00:05:22.832320 140047111603968 logging_writer.py:48] [45900] global_step=45900, grad_norm=2.299929618835449, loss=1.1824986934661865
I0217 00:06:39.753667 140047119996672 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.640735387802124, loss=1.121027946472168
I0217 00:08:01.404091 140047111603968 logging_writer.py:48] [46100] global_step=46100, grad_norm=2.6307778358459473, loss=1.1937592029571533
I0217 00:09:22.272567 140047119996672 logging_writer.py:48] [46200] global_step=46200, grad_norm=2.68900465965271, loss=1.1556636095046997
I0217 00:10:43.066640 140047111603968 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.4717804193496704, loss=1.1388030052185059
I0217 00:12:03.726063 140047119996672 logging_writer.py:48] [46400] global_step=46400, grad_norm=2.400062084197998, loss=1.158338189125061
I0217 00:13:18.634600 140047111603968 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.5023759603500366, loss=1.1605422496795654
I0217 00:14:33.395589 140047119996672 logging_writer.py:48] [46600] global_step=46600, grad_norm=2.574388027191162, loss=1.1791127920150757
I0217 00:15:48.380685 140047111603968 logging_writer.py:48] [46700] global_step=46700, grad_norm=2.0768871307373047, loss=1.1570276021957397
I0217 00:17:03.357275 140047119996672 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.951255440711975, loss=1.0978038311004639
I0217 00:17:06.277997 140202902193984 spec.py:321] Evaluating on the training split.
I0217 00:18:00.710802 140202902193984 spec.py:333] Evaluating on the validation split.
I0217 00:18:51.294450 140202902193984 spec.py:349] Evaluating on the test split.
I0217 00:19:16.938710 140202902193984 submission_runner.py:408] Time since start: 39488.77s, 	Step: 46805, 	{'train/ctc_loss': Array(0.13815887, dtype=float32), 'train/wer': 0.04856266149870801, 'validation/ctc_loss': Array(0.40407094, dtype=float32), 'validation/wer': 0.11580756345520724, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21717538, dtype=float32), 'test/wer': 0.06794223386752787, 'test/num_examples': 2472, 'score': 36021.1313123703, 'total_duration': 39488.77329707146, 'accumulated_submission_time': 36021.1313123703, 'accumulated_eval_time': 3464.165018558502, 'accumulated_logging_time': 1.3316552639007568}
I0217 00:19:16.978415 140047119996672 logging_writer.py:48] [46805] accumulated_eval_time=3464.165019, accumulated_logging_time=1.331655, accumulated_submission_time=36021.131312, global_step=46805, preemption_count=0, score=36021.131312, test/ctc_loss=0.21717537939548492, test/num_examples=2472, test/wer=0.067942, total_duration=39488.773297, train/ctc_loss=0.1381588727235794, train/wer=0.048563, validation/ctc_loss=0.4040709435939789, validation/num_examples=5348, validation/wer=0.115808
I0217 00:20:29.110059 140047111603968 logging_writer.py:48] [46900] global_step=46900, grad_norm=2.0429959297180176, loss=1.1320921182632446
I0217 00:21:44.045658 140047119996672 logging_writer.py:48] [47000] global_step=47000, grad_norm=3.2832460403442383, loss=1.1098037958145142
I0217 00:22:58.945914 140047111603968 logging_writer.py:48] [47100] global_step=47100, grad_norm=2.184488534927368, loss=1.157654047012329
I0217 00:24:13.874111 140047119996672 logging_writer.py:48] [47200] global_step=47200, grad_norm=2.8126957416534424, loss=1.1276472806930542
I0217 00:25:28.814294 140047111603968 logging_writer.py:48] [47300] global_step=47300, grad_norm=2.7233777046203613, loss=1.149399757385254
I0217 00:26:49.966974 140047119996672 logging_writer.py:48] [47400] global_step=47400, grad_norm=2.3656938076019287, loss=1.1106021404266357
I0217 00:28:05.151622 140047111603968 logging_writer.py:48] [47500] global_step=47500, grad_norm=2.785306930541992, loss=1.107906699180603
I0217 00:29:20.304361 140047119996672 logging_writer.py:48] [47600] global_step=47600, grad_norm=2.5912489891052246, loss=1.1270345449447632
I0217 00:30:35.383998 140047111603968 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.7138161659240723, loss=1.144252896308899
I0217 00:31:50.498490 140047119996672 logging_writer.py:48] [47800] global_step=47800, grad_norm=2.7243149280548096, loss=1.0991756916046143
I0217 00:33:05.473813 140047111603968 logging_writer.py:48] [47900] global_step=47900, grad_norm=2.3861842155456543, loss=1.1296783685684204
I0217 00:34:19.649942 140202902193984 spec.py:321] Evaluating on the training split.
I0217 00:35:16.212413 140202902193984 spec.py:333] Evaluating on the validation split.
I0217 00:36:06.658764 140202902193984 spec.py:349] Evaluating on the test split.
I0217 00:36:32.382100 140202902193984 submission_runner.py:408] Time since start: 40524.22s, 	Step: 48000, 	{'train/ctc_loss': Array(0.15959443, dtype=float32), 'train/wer': 0.053562875984863235, 'validation/ctc_loss': Array(0.40380514, dtype=float32), 'validation/wer': 0.11573998088378694, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21718435, dtype=float32), 'test/wer': 0.06792192228789633, 'test/num_examples': 2472, 'score': 36923.73557591438, 'total_duration': 40524.21762919426, 'accumulated_submission_time': 36923.73557591438, 'accumulated_eval_time': 3596.8922271728516, 'accumulated_logging_time': 1.3875088691711426}
I0217 00:36:32.422371 140047119996672 logging_writer.py:48] [48000] accumulated_eval_time=3596.892227, accumulated_logging_time=1.387509, accumulated_submission_time=36923.735576, global_step=48000, preemption_count=0, score=36923.735576, test/ctc_loss=0.21718434989452362, test/num_examples=2472, test/wer=0.067922, total_duration=40524.217629, train/ctc_loss=0.15959443151950836, train/wer=0.053563, validation/ctc_loss=0.403805136680603, validation/num_examples=5348, validation/wer=0.115740
I0217 00:36:32.451125 140047111603968 logging_writer.py:48] [48000] global_step=48000, preemption_count=0, score=36923.735576
I0217 00:36:32.619237 140202902193984 checkpoints.py:490] Saving checkpoint at step: 48000
I0217 00:36:33.634783 140202902193984 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_3/librispeech_deepspeech_jax/trial_4/checkpoint_48000
I0217 00:36:33.654898 140202902193984 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_3/librispeech_deepspeech_jax/trial_4/checkpoint_48000.
I0217 00:36:34.907236 140202902193984 submission_runner.py:583] Tuning trial 4/5
I0217 00:36:34.907493 140202902193984 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.004958460849689891, one_minus_beta1=0.13625575743, beta2=0.6291854735396584, weight_decay=0.1147386261512052, warmup_factor=0.02)
I0217 00:36:34.920981 140202902193984 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.12199, dtype=float32), 'train/wer': 3.282438151130771, 'validation/ctc_loss': Array(30.351221, dtype=float32), 'validation/wer': 3.0459561485658013, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.536505, dtype=float32), 'test/wer': 3.361383624804501, 'test/num_examples': 2472, 'score': 15.840157985687256, 'total_duration': 197.21963810920715, 'accumulated_submission_time': 15.840157985687256, 'accumulated_eval_time': 181.3793807029724, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1866, {'train/ctc_loss': Array(1.4378089, dtype=float32), 'train/wer': 0.39355622829171477, 'validation/ctc_loss': Array(1.5070161, dtype=float32), 'validation/wer': 0.3872867528505363, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.1032181, dtype=float32), 'test/wer': 0.32169479820445634, 'test/num_examples': 2472, 'score': 1456.1781809329987, 'total_duration': 1767.3490042686462, 'accumulated_submission_time': 1456.1781809329987, 'accumulated_eval_time': 311.0581033229828, 'accumulated_logging_time': 0.029671907424926758, 'global_step': 1866, 'preemption_count': 0}), (3736, {'train/ctc_loss': Array(1.0180577, dtype=float32), 'train/wer': 0.30271556292916724, 'validation/ctc_loss': Array(1.064015, dtype=float32), 'validation/wer': 0.2916574142908175, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.7128284, dtype=float32), 'test/wer': 0.22086811691345237, 'test/num_examples': 2472, 'score': 2896.1909997463226, 'total_duration': 3336.9190402030945, 'accumulated_submission_time': 2896.1909997463226, 'accumulated_eval_time': 440.4767470359802, 'accumulated_logging_time': 0.08244967460632324, 'global_step': 3736, 'preemption_count': 0}), (5605, {'train/ctc_loss': Array(0.8513874, dtype=float32), 'train/wer': 0.2599757026892705, 'validation/ctc_loss': Array(0.9749225, dtype=float32), 'validation/wer': 0.2722998349054327, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6446429, dtype=float32), 'test/wer': 0.19945971198180082, 'test/num_examples': 2472, 'score': 4336.195417165756, 'total_duration': 4907.917762994766, 'accumulated_submission_time': 4336.195417165756, 'accumulated_eval_time': 571.3350307941437, 'accumulated_logging_time': 0.13245534896850586, 'global_step': 5605, 'preemption_count': 0}), (7472, {'train/ctc_loss': Array(0.8907499, dtype=float32), 'train/wer': 0.26349455919281495, 'validation/ctc_loss': Array(0.97102195, dtype=float32), 'validation/wer': 0.2661498209061857, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6244865, dtype=float32), 'test/wer': 0.19153819592549712, 'test/num_examples': 2472, 'score': 5776.223026514053, 'total_duration': 6478.2153515815735, 'accumulated_submission_time': 5776.223026514053, 'accumulated_eval_time': 701.4711322784424, 'accumulated_logging_time': 0.18075060844421387, 'global_step': 7472, 'preemption_count': 0}), (9327, {'train/ctc_loss': Array(0.8192644, dtype=float32), 'train/wer': 0.24301999101527402, 'validation/ctc_loss': Array(0.89382845, dtype=float32), 'validation/wer': 0.24706257180648214, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.57379514, dtype=float32), 'test/wer': 0.17669043121483557, 'test/num_examples': 2472, 'score': 7216.673100471497, 'total_duration': 8048.553774595261, 'accumulated_submission_time': 7216.673100471497, 'accumulated_eval_time': 831.224268913269, 'accumulated_logging_time': 0.2314159870147705, 'global_step': 9327, 'preemption_count': 0}), (11202, {'train/ctc_loss': Array(0.77553225, dtype=float32), 'train/wer': 0.23671578197918577, 'validation/ctc_loss': Array(0.8555793, dtype=float32), 'validation/wer': 0.24154011025613795, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.54874396, dtype=float32), 'test/wer': 0.17151097840879084, 'test/num_examples': 2472, 'score': 8657.137380123138, 'total_duration': 9620.260332584381, 'accumulated_submission_time': 8657.137380123138, 'accumulated_eval_time': 962.3273937702179, 'accumulated_logging_time': 0.2845883369445801, 'global_step': 11202, 'preemption_count': 0}), (13074, {'train/ctc_loss': Array(0.687591, dtype=float32), 'train/wer': 0.2090459084094702, 'validation/ctc_loss': Array(0.8205807, dtype=float32), 'validation/wer': 0.22957799511474555, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5158074, dtype=float32), 'test/wer': 0.1610708264781752, 'test/num_examples': 2472, 'score': 10097.431697845459, 'total_duration': 11191.052989721298, 'accumulated_submission_time': 10097.431697845459, 'accumulated_eval_time': 1092.690021276474, 'accumulated_logging_time': 0.3334639072418213, 'global_step': 13074, 'preemption_count': 0}), (14952, {'train/ctc_loss': Array(0.63921106, dtype=float32), 'train/wer': 0.19762669623769621, 'validation/ctc_loss': Array(0.79561263, dtype=float32), 'validation/wer': 0.22467343136024406, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.49638095, dtype=float32), 'test/wer': 0.1571303800296549, 'test/num_examples': 2472, 'score': 11537.74842619896, 'total_duration': 12762.179047107697, 'accumulated_submission_time': 11537.74842619896, 'accumulated_eval_time': 1223.362502336502, 'accumulated_logging_time': 0.38450098037719727, 'global_step': 14952, 'preemption_count': 0}), (16828, {'train/ctc_loss': Array(0.7159693, dtype=float32), 'train/wer': 0.22133968015178476, 'validation/ctc_loss': Array(0.7896422, dtype=float32), 'validation/wer': 0.2200198885853037, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.49312958, dtype=float32), 'test/wer': 0.15516015680539474, 'test/num_examples': 2472, 'score': 12977.63620853424, 'total_duration': 14333.092150449753, 'accumulated_submission_time': 12977.63620853424, 'accumulated_eval_time': 1354.249887228012, 'accumulated_logging_time': 0.43480873107910156, 'global_step': 16828, 'preemption_count': 0}), (18695, {'train/ctc_loss': Array(0.6039645, dtype=float32), 'train/wer': 0.18995187399737495, 'validation/ctc_loss': Array(0.74285376, dtype=float32), 'validation/wer': 0.21174585091284745, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.45597544, dtype=float32), 'test/wer': 0.14569496069709342, 'test/num_examples': 2472, 'score': 14418.066945314407, 'total_duration': 15904.111874103546, 'accumulated_submission_time': 14418.066945314407, 'accumulated_eval_time': 1484.7049746513367, 'accumulated_logging_time': 0.48461151123046875, 'global_step': 18695, 'preemption_count': 0}), (20562, {'train/ctc_loss': Array(0.6238497, dtype=float32), 'train/wer': 0.18968322011914968, 'validation/ctc_loss': Array(0.70414096, dtype=float32), 'validation/wer': 0.20152157332226267, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43188784, dtype=float32), 'test/wer': 0.13627038774805517, 'test/num_examples': 2472, 'score': 15858.226209878922, 'total_duration': 17475.754253149033, 'accumulated_submission_time': 15858.226209878922, 'accumulated_eval_time': 1616.0522694587708, 'accumulated_logging_time': 0.5335447788238525, 'global_step': 20562, 'preemption_count': 0}), (22428, {'train/ctc_loss': Array(0.5352325, dtype=float32), 'train/wer': 0.16937924670288323, 'validation/ctc_loss': Array(0.69400257, dtype=float32), 'validation/wer': 0.19813279009818782, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.41050902, dtype=float32), 'test/wer': 0.13096906546422116, 'test/num_examples': 2472, 'score': 17298.253359794617, 'total_duration': 19046.85565328598, 'accumulated_submission_time': 17298.253359794617, 'accumulated_eval_time': 1746.9885816574097, 'accumulated_logging_time': 0.5863668918609619, 'global_step': 22428, 'preemption_count': 0}), (24303, {'train/ctc_loss': Array(0.5379182, dtype=float32), 'train/wer': 0.17296872508598893, 'validation/ctc_loss': Array(0.6751597, dtype=float32), 'validation/wer': 0.19240758083358275, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40436313, dtype=float32), 'test/wer': 0.12861292222696158, 'test/num_examples': 2472, 'score': 18738.440461158752, 'total_duration': 20616.59484243393, 'accumulated_submission_time': 18738.440461158752, 'accumulated_eval_time': 1876.3985664844513, 'accumulated_logging_time': 0.6419112682342529, 'global_step': 24303, 'preemption_count': 0}), (26186, {'train/ctc_loss': Array(0.5195341, dtype=float32), 'train/wer': 0.16668715976794357, 'validation/ctc_loss': Array(0.64813316, dtype=float32), 'validation/wer': 0.1873002693648204, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38142642, dtype=float32), 'test/wer': 0.12446935998212581, 'test/num_examples': 2472, 'score': 20178.6946849823, 'total_duration': 22187.284984588623, 'accumulated_submission_time': 20178.6946849823, 'accumulated_eval_time': 2006.694060087204, 'accumulated_logging_time': 0.6964304447174072, 'global_step': 26186, 'preemption_count': 0}), (28066, {'train/ctc_loss': Array(0.31689537, dtype=float32), 'train/wer': 0.10663526136880705, 'validation/ctc_loss': Array(0.6243423, dtype=float32), 'validation/wer': 0.17715322899871594, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3617607, dtype=float32), 'test/wer': 0.11589787337761258, 'test/num_examples': 2472, 'score': 21618.925753593445, 'total_duration': 23765.90765786171, 'accumulated_submission_time': 21618.925753593445, 'accumulated_eval_time': 2144.9430723190308, 'accumulated_logging_time': 0.751788854598999, 'global_step': 28066, 'preemption_count': 0}), (29931, {'train/ctc_loss': Array(0.28561276, dtype=float32), 'train/wer': 0.09474287839806578, 'validation/ctc_loss': Array(0.57865715, dtype=float32), 'validation/wer': 0.16738272010195313, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33669013, dtype=float32), 'test/wer': 0.10907318262141247, 'test/num_examples': 2472, 'score': 23059.09961295128, 'total_duration': 25337.965800762177, 'accumulated_submission_time': 23059.09961295128, 'accumulated_eval_time': 2276.684823989868, 'accumulated_logging_time': 0.8093042373657227, 'global_step': 29931, 'preemption_count': 0}), (31803, {'train/ctc_loss': Array(0.27346468, dtype=float32), 'train/wer': 0.09277049516508132, 'validation/ctc_loss': Array(0.5726918, dtype=float32), 'validation/wer': 0.16434150438803982, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32298186, dtype=float32), 'test/wer': 0.10346718664310524, 'test/num_examples': 2472, 'score': 24499.127238988876, 'total_duration': 26910.178052663803, 'accumulated_submission_time': 24499.127238988876, 'accumulated_eval_time': 2408.727742433548, 'accumulated_logging_time': 0.86600661277771, 'global_step': 31803, 'preemption_count': 0}), (33678, {'train/ctc_loss': Array(0.24806608, dtype=float32), 'train/wer': 0.0840600825978491, 'validation/ctc_loss': Array(0.53813505, dtype=float32), 'validation/wer': 0.15382758720565376, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.306992, dtype=float32), 'test/wer': 0.09763776328885097, 'test/num_examples': 2472, 'score': 25939.097049236298, 'total_duration': 28482.056171655655, 'accumulated_submission_time': 25939.097049236298, 'accumulated_eval_time': 2540.49427819252, 'accumulated_logging_time': 0.921947717666626, 'global_step': 33678, 'preemption_count': 0}), (35561, {'train/ctc_loss': Array(0.23072459, dtype=float32), 'train/wer': 0.07805270676531417, 'validation/ctc_loss': Array(0.5039565, dtype=float32), 'validation/wer': 0.1447715226353341, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28383914, dtype=float32), 'test/wer': 0.08965531249365263, 'test/num_examples': 2472, 'score': 27379.475867271423, 'total_duration': 30054.618946552277, 'accumulated_submission_time': 27379.475867271423, 'accumulated_eval_time': 2672.538813829422, 'accumulated_logging_time': 0.9750196933746338, 'global_step': 35561, 'preemption_count': 0}), (37440, {'train/ctc_loss': Array(0.20172007, dtype=float32), 'train/wer': 0.06983904385830798, 'validation/ctc_loss': Array(0.47738373, dtype=float32), 'validation/wer': 0.13859254467690704, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26603636, dtype=float32), 'test/wer': 0.08486177970060732, 'test/num_examples': 2472, 'score': 28820.02853703499, 'total_duration': 31627.78924536705, 'accumulated_submission_time': 28820.02853703499, 'accumulated_eval_time': 2805.0091433525085, 'accumulated_logging_time': 1.0354893207550049, 'global_step': 37440, 'preemption_count': 0}), (39309, {'train/ctc_loss': Array(0.22671203, dtype=float32), 'train/wer': 0.07333054206745801, 'validation/ctc_loss': Array(0.45624575, dtype=float32), 'validation/wer': 0.13130328161657512, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25134808, dtype=float32), 'test/wer': 0.07992606585014117, 'test/num_examples': 2472, 'score': 30260.46949505806, 'total_duration': 33199.82506918907, 'accumulated_submission_time': 30260.46949505806, 'accumulated_eval_time': 2936.458508014679, 'accumulated_logging_time': 1.0954077243804932, 'global_step': 39309, 'preemption_count': 0}), (41183, {'train/ctc_loss': Array(0.17770247, dtype=float32), 'train/wer': 0.061686958081999484, 'validation/ctc_loss': Array(0.4342081, dtype=float32), 'validation/wer': 0.12494086525000724, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23483199, dtype=float32), 'test/wer': 0.07438100461072858, 'test/num_examples': 2472, 'score': 31700.723588705063, 'total_duration': 34773.198744535446, 'accumulated_submission_time': 31700.723588705063, 'accumulated_eval_time': 3069.434670448303, 'accumulated_logging_time': 1.153717041015625, 'global_step': 41183, 'preemption_count': 0}), (43048, {'train/ctc_loss': Array(0.15259655, dtype=float32), 'train/wer': 0.05289856212853238, 'validation/ctc_loss': Array(0.41574052, dtype=float32), 'validation/wer': 0.11968873398534424, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2274025, dtype=float32), 'test/wer': 0.07068429711778686, 'test/num_examples': 2472, 'score': 33141.31151199341, 'total_duration': 36345.68494963646, 'accumulated_submission_time': 33141.31151199341, 'accumulated_eval_time': 3201.1860456466675, 'accumulated_logging_time': 1.2138450145721436, 'global_step': 43048, 'preemption_count': 0}), (44925, {'train/ctc_loss': Array(0.14536211, dtype=float32), 'train/wer': 0.05018736545565684, 'validation/ctc_loss': Array(0.40926024, dtype=float32), 'validation/wer': 0.11703370439383261, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21977012, dtype=float32), 'test/wer': 0.06828753072126419, 'test/num_examples': 2472, 'score': 34581.259950876236, 'total_duration': 37918.101712465286, 'accumulated_submission_time': 34581.259950876236, 'accumulated_eval_time': 3333.5102150440216, 'accumulated_logging_time': 1.2725615501403809, 'global_step': 44925, 'preemption_count': 0}), (46805, {'train/ctc_loss': Array(0.13815887, dtype=float32), 'train/wer': 0.04856266149870801, 'validation/ctc_loss': Array(0.40407094, dtype=float32), 'validation/wer': 0.11580756345520724, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21717538, dtype=float32), 'test/wer': 0.06794223386752787, 'test/num_examples': 2472, 'score': 36021.1313123703, 'total_duration': 39488.77329707146, 'accumulated_submission_time': 36021.1313123703, 'accumulated_eval_time': 3464.165018558502, 'accumulated_logging_time': 1.3316552639007568, 'global_step': 46805, 'preemption_count': 0}), (48000, {'train/ctc_loss': Array(0.15959443, dtype=float32), 'train/wer': 0.053562875984863235, 'validation/ctc_loss': Array(0.40380514, dtype=float32), 'validation/wer': 0.11573998088378694, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21718435, dtype=float32), 'test/wer': 0.06792192228789633, 'test/num_examples': 2472, 'score': 36923.73557591438, 'total_duration': 40524.21762919426, 'accumulated_submission_time': 36923.73557591438, 'accumulated_eval_time': 3596.8922271728516, 'accumulated_logging_time': 1.3875088691711426, 'global_step': 48000, 'preemption_count': 0})], 'global_step': 48000}
I0217 00:36:34.921199 140202902193984 submission_runner.py:586] Timing: 36923.73557591438
I0217 00:36:34.921255 140202902193984 submission_runner.py:588] Total number of evals: 27
I0217 00:36:34.921308 140202902193984 submission_runner.py:589] ====================
I0217 00:36:34.921392 140202902193984 submission_runner.py:542] Using RNG seed 4294683350
I0217 00:36:34.924580 140202902193984 submission_runner.py:551] --- Tuning run 5/5 ---
I0217 00:36:34.924701 140202902193984 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_3/librispeech_deepspeech_jax/trial_5.
I0217 00:36:34.927107 140202902193984 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_3/librispeech_deepspeech_jax/trial_5/hparams.json.
I0217 00:36:34.929792 140202902193984 submission_runner.py:206] Initializing dataset.
I0217 00:36:34.929912 140202902193984 submission_runner.py:213] Initializing model.
I0217 00:36:36.107874 140202902193984 submission_runner.py:255] Initializing optimizer.
I0217 00:36:36.247234 140202902193984 submission_runner.py:262] Initializing metrics bundle.
I0217 00:36:36.247407 140202902193984 submission_runner.py:280] Initializing checkpoint and logger.
I0217 00:36:36.251249 140202902193984 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_3/librispeech_deepspeech_jax/trial_5 with prefix checkpoint_
I0217 00:36:36.251377 140202902193984 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_3/librispeech_deepspeech_jax/trial_5/meta_data_0.json.
I0217 00:36:36.251758 140202902193984 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0217 00:36:36.251856 140202902193984 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0217 00:36:36.800513 140202902193984 logger_utils.py:220] Unable to record git information. Continuing without it.
I0217 00:36:37.294805 140202902193984 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_3/librispeech_deepspeech_jax/trial_5/flags_0.json.
I0217 00:36:37.312576 140202902193984 submission_runner.py:314] Starting training loop.
I0217 00:36:37.316109 140202902193984 input_pipeline.py:20] Loading split = train-clean-100
I0217 00:36:37.359123 140202902193984 input_pipeline.py:20] Loading split = train-clean-360
I0217 00:36:38.004835 140202902193984 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0217 00:36:53.671472 140047002498816 logging_writer.py:48] [0] global_step=0, grad_norm=21.422195434570312, loss=33.24811935424805
I0217 00:36:53.685694 140202902193984 spec.py:321] Evaluating on the training split.
I0217 00:38:29.317854 140202902193984 spec.py:333] Evaluating on the validation split.
I0217 00:39:28.811639 140202902193984 spec.py:349] Evaluating on the test split.
I0217 00:39:59.693908 140202902193984 submission_runner.py:408] Time since start: 202.38s, 	Step: 1, 	{'train/ctc_loss': Array(30.62459, dtype=float32), 'train/wer': 3.3513766830074823, 'validation/ctc_loss': Array(30.351221, dtype=float32), 'validation/wer': 3.0459271846066214, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.536512, dtype=float32), 'test/wer': 3.361485182702659, 'test/num_examples': 2472, 'score': 16.372990608215332, 'total_duration': 202.37854552268982, 'accumulated_submission_time': 16.372990608215332, 'accumulated_eval_time': 186.00546002388, 'accumulated_logging_time': 0}
I0217 00:39:59.711293 140047119996672 logging_writer.py:48] [1] accumulated_eval_time=186.005460, accumulated_logging_time=0, accumulated_submission_time=16.372991, global_step=1, preemption_count=0, score=16.372991, test/ctc_loss=30.53651237487793, test/num_examples=2472, test/wer=3.361485, total_duration=202.378546, train/ctc_loss=30.624589920043945, train/wer=3.351377, validation/ctc_loss=30.351221084594727, validation/num_examples=5348, validation/wer=3.045927
I0217 00:41:25.036730 140047002498816 logging_writer.py:48] [100] global_step=100, grad_norm=1.2104547023773193, loss=6.012604236602783
I0217 00:42:41.478604 140047010891520 logging_writer.py:48] [200] global_step=200, grad_norm=0.31214606761932373, loss=5.823805332183838
I0217 00:43:58.184444 140047002498816 logging_writer.py:48] [300] global_step=300, grad_norm=0.5862525105476379, loss=5.763003826141357
I0217 00:45:14.767614 140047010891520 logging_writer.py:48] [400] global_step=400, grad_norm=1.2817597389221191, loss=5.540213108062744
I0217 00:46:31.292568 140047002498816 logging_writer.py:48] [500] global_step=500, grad_norm=1.2565951347351074, loss=5.069798469543457
I0217 00:47:47.420237 140047010891520 logging_writer.py:48] [600] global_step=600, grad_norm=2.184257745742798, loss=4.161656379699707
I0217 00:49:03.140393 140047002498816 logging_writer.py:48] [700] global_step=700, grad_norm=1.7467344999313354, loss=3.590864658355713
I0217 00:50:18.766327 140047010891520 logging_writer.py:48] [800] global_step=800, grad_norm=2.011700391769409, loss=3.2705721855163574
I0217 00:51:34.246601 140047002498816 logging_writer.py:48] [900] global_step=900, grad_norm=2.518498182296753, loss=3.093191146850586
I0217 00:52:55.058398 140047010891520 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.4293103218078613, loss=2.9027836322784424
I0217 00:54:15.055523 140047119996672 logging_writer.py:48] [1100] global_step=1100, grad_norm=2.2892966270446777, loss=2.69319748878479
I0217 00:55:30.290017 140047111603968 logging_writer.py:48] [1200] global_step=1200, grad_norm=3.3487656116485596, loss=2.7040328979492188
I0217 00:56:45.534541 140047119996672 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.8318768739700317, loss=2.5680978298187256
I0217 00:58:00.699500 140047111603968 logging_writer.py:48] [1400] global_step=1400, grad_norm=3.633699893951416, loss=2.5628979206085205
I0217 00:59:15.970995 140047119996672 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.957139015197754, loss=2.4325504302978516
I0217 01:00:31.030557 140047111603968 logging_writer.py:48] [1600] global_step=1600, grad_norm=3.2660982608795166, loss=2.4605445861816406
I0217 01:01:51.637925 140047119996672 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.5939455032348633, loss=2.3479111194610596
I0217 01:03:14.167673 140047111603968 logging_writer.py:48] [1800] global_step=1800, grad_norm=3.4035191535949707, loss=2.364057779312134
I0217 01:04:00.267262 140202902193984 spec.py:321] Evaluating on the training split.
I0217 01:04:53.836920 140202902193984 spec.py:333] Evaluating on the validation split.
I0217 01:05:43.793134 140202902193984 spec.py:349] Evaluating on the test split.
I0217 01:06:09.494157 140202902193984 submission_runner.py:408] Time since start: 1772.17s, 	Step: 1857, 	{'train/ctc_loss': Array(2.3449352, dtype=float32), 'train/wer': 0.5320623826954385, 'validation/ctc_loss': Array(2.671262, dtype=float32), 'validation/wer': 0.5683211523793893, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.1457593, dtype=float32), 'test/wer': 0.49158085024272336, 'test/num_examples': 2472, 'score': 1456.8387684822083, 'total_duration': 1772.1749081611633, 'accumulated_submission_time': 1456.8387684822083, 'accumulated_eval_time': 315.22576928138733, 'accumulated_logging_time': 0.028501510620117188}
I0217 01:06:09.525017 140047119996672 logging_writer.py:48] [1857] accumulated_eval_time=315.225769, accumulated_logging_time=0.028502, accumulated_submission_time=1456.838768, global_step=1857, preemption_count=0, score=1456.838768, test/ctc_loss=2.145759344100952, test/num_examples=2472, test/wer=0.491581, total_duration=1772.174908, train/ctc_loss=2.344935178756714, train/wer=0.532062, validation/ctc_loss=2.67126202583313, validation/num_examples=5348, validation/wer=0.568321
I0217 01:06:42.505860 140047111603968 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.675450086593628, loss=2.3174991607666016
I0217 01:07:57.599393 140047119996672 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.753903388977051, loss=2.2507596015930176
I0217 01:09:16.190691 140047119996672 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.406538963317871, loss=2.1423826217651367
I0217 01:10:30.986081 140047111603968 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.9945608377456665, loss=2.1801652908325195
I0217 01:11:45.940913 140047119996672 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.8849802017211914, loss=2.159830331802368
I0217 01:13:01.321183 140047111603968 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.491781234741211, loss=2.033156156539917
I0217 01:14:16.400149 140047119996672 logging_writer.py:48] [2500] global_step=2500, grad_norm=2.0244879722595215, loss=2.126321315765381
I0217 01:15:31.369935 140047111603968 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.8875406980514526, loss=2.048295021057129
I0217 01:16:51.225354 140047119996672 logging_writer.py:48] [2700] global_step=2700, grad_norm=2.4460794925689697, loss=2.012087345123291
I0217 01:18:13.068883 140047111603968 logging_writer.py:48] [2800] global_step=2800, grad_norm=2.0105385780334473, loss=1.9875388145446777
I0217 01:19:33.625764 140047119996672 logging_writer.py:48] [2900] global_step=2900, grad_norm=4.927469730377197, loss=1.9694775342941284
I0217 01:20:53.927975 140047111603968 logging_writer.py:48] [3000] global_step=3000, grad_norm=2.441673517227173, loss=2.01015567779541
I0217 01:22:17.510900 140047119996672 logging_writer.py:48] [3100] global_step=3100, grad_norm=3.128054141998291, loss=1.9007854461669922
I0217 01:23:32.582784 140047111603968 logging_writer.py:48] [3200] global_step=3200, grad_norm=2.5571393966674805, loss=1.8842490911483765
I0217 01:24:47.801482 140047119996672 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.4740337133407593, loss=1.8994117975234985
I0217 01:26:02.668909 140047111603968 logging_writer.py:48] [3400] global_step=3400, grad_norm=2.1690316200256348, loss=1.9308773279190063
I0217 01:27:18.001415 140047119996672 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.177427053451538, loss=1.9063646793365479
I0217 01:28:33.156118 140047111603968 logging_writer.py:48] [3600] global_step=3600, grad_norm=2.6001596450805664, loss=1.8531538248062134
I0217 01:29:49.130083 140047119996672 logging_writer.py:48] [3700] global_step=3700, grad_norm=2.8724286556243896, loss=1.8630781173706055
I0217 01:30:09.906214 140202902193984 spec.py:321] Evaluating on the training split.
I0217 01:31:08.908611 140202902193984 spec.py:333] Evaluating on the validation split.
I0217 01:31:59.097921 140202902193984 spec.py:349] Evaluating on the test split.
I0217 01:32:24.950231 140202902193984 submission_runner.py:408] Time since start: 3347.63s, 	Step: 3727, 	{'train/ctc_loss': Array(0.5966041, dtype=float32), 'train/wer': 0.1932851115758293, 'validation/ctc_loss': Array(0.9358604, dtype=float32), 'validation/wer': 0.26263552719233035, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6082362, dtype=float32), 'test/wer': 0.19340686125159953, 'test/num_examples': 2472, 'score': 2897.1221652030945, 'total_duration': 3347.6318669319153, 'accumulated_submission_time': 2897.1221652030945, 'accumulated_eval_time': 450.2640874385834, 'accumulated_logging_time': 0.07642841339111328}
I0217 01:32:24.981887 140047119996672 logging_writer.py:48] [3727] accumulated_eval_time=450.264087, accumulated_logging_time=0.076428, accumulated_submission_time=2897.122165, global_step=3727, preemption_count=0, score=2897.122165, test/ctc_loss=0.6082361936569214, test/num_examples=2472, test/wer=0.193407, total_duration=3347.631867, train/ctc_loss=0.5966041088104248, train/wer=0.193285, validation/ctc_loss=0.9358603954315186, validation/num_examples=5348, validation/wer=0.262636
I0217 01:33:20.607931 140047111603968 logging_writer.py:48] [3800] global_step=3800, grad_norm=2.586989641189575, loss=1.9270603656768799
I0217 01:34:35.884630 140047119996672 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.9965485334396362, loss=1.87214195728302
I0217 01:35:51.266599 140047111603968 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.0187246799468994, loss=1.835448980331421
I0217 01:37:06.338297 140047119996672 logging_writer.py:48] [4100] global_step=4100, grad_norm=2.1679558753967285, loss=1.9105243682861328
I0217 01:38:24.865825 140047119996672 logging_writer.py:48] [4200] global_step=4200, grad_norm=3.90767765045166, loss=1.9324824810028076
I0217 01:39:39.904443 140047111603968 logging_writer.py:48] [4300] global_step=4300, grad_norm=2.925346851348877, loss=1.9262691736221313
I0217 01:40:54.925559 140047119996672 logging_writer.py:48] [4400] global_step=4400, grad_norm=2.331416130065918, loss=1.8334144353866577
I0217 01:42:09.957853 140047111603968 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.7302387952804565, loss=1.810048222541809
I0217 01:43:25.091814 140047119996672 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.679905652999878, loss=1.8335987329483032
I0217 01:44:40.079029 140047111603968 logging_writer.py:48] [4700] global_step=4700, grad_norm=1.7918473482131958, loss=1.792253851890564
I0217 01:45:57.536437 140047119996672 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.294079542160034, loss=1.8533954620361328
I0217 01:47:17.253238 140047111603968 logging_writer.py:48] [4900] global_step=4900, grad_norm=1.9764654636383057, loss=1.8204209804534912
I0217 01:48:37.748527 140047119996672 logging_writer.py:48] [5000] global_step=5000, grad_norm=4.174540042877197, loss=1.8156485557556152
I0217 01:50:00.314314 140047111603968 logging_writer.py:48] [5100] global_step=5100, grad_norm=2.521984338760376, loss=1.7376409769058228
I0217 01:51:21.368044 140047119996672 logging_writer.py:48] [5200] global_step=5200, grad_norm=3.730769157409668, loss=1.7532769441604614
I0217 01:52:36.376892 140047111603968 logging_writer.py:48] [5300] global_step=5300, grad_norm=2.7452609539031982, loss=1.7291743755340576
I0217 01:53:51.581626 140047119996672 logging_writer.py:48] [5400] global_step=5400, grad_norm=2.564197540283203, loss=1.750260829925537
I0217 01:55:06.785480 140047111603968 logging_writer.py:48] [5500] global_step=5500, grad_norm=3.3811142444610596, loss=1.6939338445663452
I0217 01:56:21.675003 140047119996672 logging_writer.py:48] [5600] global_step=5600, grad_norm=2.915745973587036, loss=1.7428839206695557
I0217 01:56:25.279984 140202902193984 spec.py:321] Evaluating on the training split.
I0217 01:57:20.918433 140202902193984 spec.py:333] Evaluating on the validation split.
I0217 01:58:11.944241 140202902193984 spec.py:349] Evaluating on the test split.
I0217 01:58:37.771950 140202902193984 submission_runner.py:408] Time since start: 4920.45s, 	Step: 5606, 	{'train/ctc_loss': Array(0.4300289, dtype=float32), 'train/wer': 0.1458097903575012, 'validation/ctc_loss': Array(0.8080621, dtype=float32), 'validation/wer': 0.2312289407880128, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5005587, dtype=float32), 'test/wer': 0.16019742855401864, 'test/num_examples': 2472, 'score': 4337.322243690491, 'total_duration': 4920.453497171402, 'accumulated_submission_time': 4337.322243690491, 'accumulated_eval_time': 582.7502326965332, 'accumulated_logging_time': 0.12636256217956543}
I0217 01:58:37.803372 140047119996672 logging_writer.py:48] [5606] accumulated_eval_time=582.750233, accumulated_logging_time=0.126363, accumulated_submission_time=4337.322244, global_step=5606, preemption_count=0, score=4337.322244, test/ctc_loss=0.5005586743354797, test/num_examples=2472, test/wer=0.160197, total_duration=4920.453497, train/ctc_loss=0.43002888560295105, train/wer=0.145810, validation/ctc_loss=0.8080620765686035, validation/num_examples=5348, validation/wer=0.231229
I0217 01:59:49.275833 140047111603968 logging_writer.py:48] [5700] global_step=5700, grad_norm=5.005967140197754, loss=1.7617437839508057
I0217 02:01:04.480451 140047119996672 logging_writer.py:48] [5800] global_step=5800, grad_norm=2.446411371231079, loss=1.7117050886154175
I0217 02:02:19.627609 140047111603968 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.848459005355835, loss=1.7646108865737915
I0217 02:03:34.788302 140047119996672 logging_writer.py:48] [6000] global_step=6000, grad_norm=4.173259735107422, loss=1.7806743383407593
I0217 02:04:50.174756 140047111603968 logging_writer.py:48] [6100] global_step=6100, grad_norm=3.6934449672698975, loss=1.758488416671753
I0217 02:06:09.663909 140047119996672 logging_writer.py:48] [6200] global_step=6200, grad_norm=3.46669864654541, loss=1.740507960319519
I0217 02:07:24.676816 140047111603968 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.8440163135528564, loss=1.707391381263733
I0217 02:08:39.706922 140047119996672 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.3956074714660645, loss=1.6425464153289795
I0217 02:09:54.900033 140047111603968 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.4540774822235107, loss=1.7449171543121338
I0217 02:11:10.110462 140047119996672 logging_writer.py:48] [6600] global_step=6600, grad_norm=3.7121286392211914, loss=1.7201900482177734
I0217 02:12:25.508487 140047111603968 logging_writer.py:48] [6700] global_step=6700, grad_norm=1.9410501718521118, loss=1.7367877960205078
I0217 02:13:40.590785 140047119996672 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.7171496152877808, loss=1.7007312774658203
I0217 02:14:57.468975 140047111603968 logging_writer.py:48] [6900] global_step=6900, grad_norm=2.525322198867798, loss=1.701680064201355
I0217 02:16:18.020604 140047119996672 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.504662036895752, loss=1.6892149448394775
I0217 02:17:38.908525 140047111603968 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.360459089279175, loss=1.7064812183380127
I0217 02:19:00.347283 140047119996672 logging_writer.py:48] [7200] global_step=7200, grad_norm=2.0003931522369385, loss=1.734505534172058
I0217 02:20:19.389712 140047119996672 logging_writer.py:48] [7300] global_step=7300, grad_norm=2.2320103645324707, loss=1.7579792737960815
I0217 02:21:34.452016 140047111603968 logging_writer.py:48] [7400] global_step=7400, grad_norm=2.5660364627838135, loss=1.652567982673645
I0217 02:22:38.202045 140202902193984 spec.py:321] Evaluating on the training split.
I0217 02:23:34.097898 140202902193984 spec.py:333] Evaluating on the validation split.
I0217 02:24:24.917600 140202902193984 spec.py:349] Evaluating on the test split.
I0217 02:24:50.396195 140202902193984 submission_runner.py:408] Time since start: 6493.08s, 	Step: 7486, 	{'train/ctc_loss': Array(0.3892914, dtype=float32), 'train/wer': 0.1317028818944293, 'validation/ctc_loss': Array(0.7440744, dtype=float32), 'validation/wer': 0.21083831352520346, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.44369552, dtype=float32), 'test/wer': 0.14362317957467552, 'test/num_examples': 2472, 'score': 5777.62396812439, 'total_duration': 6493.077576160431, 'accumulated_submission_time': 5777.62396812439, 'accumulated_eval_time': 714.9384062290192, 'accumulated_logging_time': 0.17426013946533203}
I0217 02:24:50.428616 140047119996672 logging_writer.py:48] [7486] accumulated_eval_time=714.938406, accumulated_logging_time=0.174260, accumulated_submission_time=5777.623968, global_step=7486, preemption_count=0, score=5777.623968, test/ctc_loss=0.4436955153942108, test/num_examples=2472, test/wer=0.143623, total_duration=6493.077576, train/ctc_loss=0.3892914056777954, train/wer=0.131703, validation/ctc_loss=0.7440744042396545, validation/num_examples=5348, validation/wer=0.210838
I0217 02:25:01.826005 140047111603968 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.55839204788208, loss=1.6434712409973145
I0217 02:26:16.751034 140047119996672 logging_writer.py:48] [7600] global_step=7600, grad_norm=2.8727383613586426, loss=1.603953242301941
I0217 02:27:31.684144 140047111603968 logging_writer.py:48] [7700] global_step=7700, grad_norm=2.6682398319244385, loss=1.6265463829040527
I0217 02:28:46.690025 140047119996672 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.8729276657104492, loss=1.6138677597045898
I0217 02:30:01.785473 140047111603968 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.6818881034851074, loss=1.640315294265747
I0217 02:31:16.813159 140047119996672 logging_writer.py:48] [8000] global_step=8000, grad_norm=4.182618141174316, loss=1.6368087530136108
I0217 02:32:33.659468 140047111603968 logging_writer.py:48] [8100] global_step=8100, grad_norm=8.136972427368164, loss=1.685006022453308
I0217 02:33:53.696605 140047119996672 logging_writer.py:48] [8200] global_step=8200, grad_norm=3.205101728439331, loss=1.672631025314331
I0217 02:35:14.316111 140047119996672 logging_writer.py:48] [8300] global_step=8300, grad_norm=3.946415424346924, loss=1.6655868291854858
I0217 02:36:29.590045 140047111603968 logging_writer.py:48] [8400] global_step=8400, grad_norm=2.2420687675476074, loss=1.6636782884597778
I0217 02:37:44.981389 140047119996672 logging_writer.py:48] [8500] global_step=8500, grad_norm=3.3350894451141357, loss=1.6699028015136719
I0217 02:39:00.310765 140047111603968 logging_writer.py:48] [8600] global_step=8600, grad_norm=2.25154185295105, loss=1.7203325033187866
I0217 02:40:15.703686 140047119996672 logging_writer.py:48] [8700] global_step=8700, grad_norm=2.8811933994293213, loss=1.6310491561889648
I0217 02:41:30.848424 140047111603968 logging_writer.py:48] [8800] global_step=8800, grad_norm=12.913555145263672, loss=1.632429599761963
I0217 02:42:48.479256 140047119996672 logging_writer.py:48] [8900] global_step=8900, grad_norm=3.203867197036743, loss=1.6085631847381592
I0217 02:44:09.019483 140047111603968 logging_writer.py:48] [9000] global_step=9000, grad_norm=2.9798848628997803, loss=1.6623579263687134
I0217 02:45:30.261994 140047119996672 logging_writer.py:48] [9100] global_step=9100, grad_norm=2.4885623455047607, loss=1.6321024894714355
I0217 02:46:50.525138 140047111603968 logging_writer.py:48] [9200] global_step=9200, grad_norm=2.2780253887176514, loss=1.6033393144607544
I0217 02:48:12.531737 140047119996672 logging_writer.py:48] [9300] global_step=9300, grad_norm=2.137523651123047, loss=1.6090182065963745
I0217 02:48:50.595897 140202902193984 spec.py:321] Evaluating on the training split.
I0217 02:49:46.199741 140202902193984 spec.py:333] Evaluating on the validation split.
I0217 02:50:37.026311 140202902193984 spec.py:349] Evaluating on the test split.
I0217 02:51:02.941958 140202902193984 submission_runner.py:408] Time since start: 8065.62s, 	Step: 9352, 	{'train/ctc_loss': Array(0.39885035, dtype=float32), 'train/wer': 0.13448043002620136, 'validation/ctc_loss': Array(0.7127357, dtype=float32), 'validation/wer': 0.20256427585274722, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43267328, dtype=float32), 'test/wer': 0.1383624804501046, 'test/num_examples': 2472, 'score': 7217.692116260529, 'total_duration': 8065.623285531998, 'accumulated_submission_time': 7217.692116260529, 'accumulated_eval_time': 847.2784371376038, 'accumulated_logging_time': 0.22581052780151367}
I0217 02:51:02.975021 140047119996672 logging_writer.py:48] [9352] accumulated_eval_time=847.278437, accumulated_logging_time=0.225811, accumulated_submission_time=7217.692116, global_step=9352, preemption_count=0, score=7217.692116, test/ctc_loss=0.43267327547073364, test/num_examples=2472, test/wer=0.138362, total_duration=8065.623286, train/ctc_loss=0.39885035157203674, train/wer=0.134480, validation/ctc_loss=0.7127357125282288, validation/num_examples=5348, validation/wer=0.202564
I0217 02:51:39.836515 140047111603968 logging_writer.py:48] [9400] global_step=9400, grad_norm=2.807211399078369, loss=1.6179611682891846
I0217 02:52:55.046157 140047119996672 logging_writer.py:48] [9500] global_step=9500, grad_norm=2.7259254455566406, loss=1.6125239133834839
I0217 02:54:10.091256 140047111603968 logging_writer.py:48] [9600] global_step=9600, grad_norm=2.6549413204193115, loss=1.6110725402832031
I0217 02:55:25.237701 140047119996672 logging_writer.py:48] [9700] global_step=9700, grad_norm=2.6561243534088135, loss=1.5886850357055664
I0217 02:56:40.462391 140047111603968 logging_writer.py:48] [9800] global_step=9800, grad_norm=4.904083251953125, loss=1.5521087646484375
I0217 02:57:55.851464 140047119996672 logging_writer.py:48] [9900] global_step=9900, grad_norm=2.4513895511627197, loss=1.5959104299545288
I0217 02:59:10.927553 140047111603968 logging_writer.py:48] [10000] global_step=10000, grad_norm=2.1417808532714844, loss=1.594791293144226
I0217 03:00:26.027862 140047119996672 logging_writer.py:48] [10100] global_step=10100, grad_norm=3.298577070236206, loss=1.5936243534088135
I0217 03:01:45.614204 140047111603968 logging_writer.py:48] [10200] global_step=10200, grad_norm=3.1339385509490967, loss=1.6740636825561523
I0217 03:03:11.142800 140047119996672 logging_writer.py:48] [10300] global_step=10300, grad_norm=2.2470133304595947, loss=1.6074169874191284
I0217 03:04:26.239869 140047111603968 logging_writer.py:48] [10400] global_step=10400, grad_norm=3.4686331748962402, loss=1.6233395338058472
I0217 03:05:41.385557 140047119996672 logging_writer.py:48] [10500] global_step=10500, grad_norm=3.9058685302734375, loss=1.6518478393554688
I0217 03:06:56.758810 140047111603968 logging_writer.py:48] [10600] global_step=10600, grad_norm=2.267033815383911, loss=1.629460096359253
I0217 03:08:12.364516 140047119996672 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.7621580362319946, loss=1.5829358100891113
I0217 03:09:27.498435 140047111603968 logging_writer.py:48] [10800] global_step=10800, grad_norm=2.1665875911712646, loss=1.6079583168029785
I0217 03:10:44.055919 140047119996672 logging_writer.py:48] [10900] global_step=10900, grad_norm=2.2278847694396973, loss=1.6104741096496582
I0217 03:12:06.444478 140047111603968 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.7388118505477905, loss=1.6122705936431885
I0217 03:13:27.316440 140047119996672 logging_writer.py:48] [11100] global_step=11100, grad_norm=2.3458480834960938, loss=1.5522814989089966
I0217 03:14:48.538184 140047111603968 logging_writer.py:48] [11200] global_step=11200, grad_norm=2.664008140563965, loss=1.5806318521499634
I0217 03:15:03.442964 140202902193984 spec.py:321] Evaluating on the training split.
I0217 03:15:58.672204 140202902193984 spec.py:333] Evaluating on the validation split.
I0217 03:16:49.432765 140202902193984 spec.py:349] Evaluating on the test split.
I0217 03:17:14.977392 140202902193984 submission_runner.py:408] Time since start: 9637.66s, 	Step: 11220, 	{'train/ctc_loss': Array(0.33331928, dtype=float32), 'train/wer': 0.11130403835057995, 'validation/ctc_loss': Array(0.67308295, dtype=float32), 'validation/wer': 0.19158693532347915, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39661396, dtype=float32), 'test/wer': 0.12672394532122763, 'test/num_examples': 2472, 'score': 8658.064115285873, 'total_duration': 9637.65848660469, 'accumulated_submission_time': 8658.064115285873, 'accumulated_eval_time': 978.8065919876099, 'accumulated_logging_time': 0.2755625247955322}
I0217 03:17:15.009710 140047119996672 logging_writer.py:48] [11220] accumulated_eval_time=978.806592, accumulated_logging_time=0.275563, accumulated_submission_time=8658.064115, global_step=11220, preemption_count=0, score=8658.064115, test/ctc_loss=0.3966139554977417, test/num_examples=2472, test/wer=0.126724, total_duration=9637.658487, train/ctc_loss=0.3333192765712738, train/wer=0.111304, validation/ctc_loss=0.6730829477310181, validation/num_examples=5348, validation/wer=0.191587
I0217 03:18:15.830073 140047111603968 logging_writer.py:48] [11300] global_step=11300, grad_norm=3.097902297973633, loss=1.5924574136734009
I0217 03:19:34.543417 140047119996672 logging_writer.py:48] [11400] global_step=11400, grad_norm=2.948000907897949, loss=1.6128684282302856
I0217 03:20:49.560859 140047111603968 logging_writer.py:48] [11500] global_step=11500, grad_norm=2.1546401977539062, loss=1.5460803508758545
I0217 03:22:04.826783 140047119996672 logging_writer.py:48] [11600] global_step=11600, grad_norm=2.2415318489074707, loss=1.5921944379806519
I0217 03:23:20.017678 140047111603968 logging_writer.py:48] [11700] global_step=11700, grad_norm=3.2198901176452637, loss=1.500946283340454
I0217 03:24:35.298482 140047119996672 logging_writer.py:48] [11800] global_step=11800, grad_norm=2.248608350753784, loss=1.5648142099380493
I0217 03:25:50.443975 140047111603968 logging_writer.py:48] [11900] global_step=11900, grad_norm=2.773723840713501, loss=1.5982606410980225
I0217 03:27:08.202428 140047119996672 logging_writer.py:48] [12000] global_step=12000, grad_norm=2.414318561553955, loss=1.5898579359054565
I0217 03:28:29.090049 140047111603968 logging_writer.py:48] [12100] global_step=12100, grad_norm=2.6029253005981445, loss=1.6199194192886353
I0217 03:29:49.532326 140047119996672 logging_writer.py:48] [12200] global_step=12200, grad_norm=2.9581620693206787, loss=1.6292487382888794
I0217 03:31:10.578632 140047111603968 logging_writer.py:48] [12300] global_step=12300, grad_norm=4.997992992401123, loss=1.619273066520691
I0217 03:32:32.694824 140047119996672 logging_writer.py:48] [12400] global_step=12400, grad_norm=2.5497498512268066, loss=1.5626552104949951
I0217 03:33:48.670224 140047111603968 logging_writer.py:48] [12500] global_step=12500, grad_norm=3.8329319953918457, loss=1.5619091987609863
I0217 03:35:03.529195 140047119996672 logging_writer.py:48] [12600] global_step=12600, grad_norm=3.4422600269317627, loss=1.575774908065796
I0217 03:36:18.834188 140047111603968 logging_writer.py:48] [12700] global_step=12700, grad_norm=2.036940574645996, loss=1.541965365409851
I0217 03:37:34.421782 140047119996672 logging_writer.py:48] [12800] global_step=12800, grad_norm=3.6306416988372803, loss=1.5803563594818115
I0217 03:38:49.620318 140047111603968 logging_writer.py:48] [12900] global_step=12900, grad_norm=2.459501266479492, loss=1.535413384437561
I0217 03:40:05.163750 140047119996672 logging_writer.py:48] [13000] global_step=13000, grad_norm=2.5465457439422607, loss=1.6218698024749756
I0217 03:41:15.255211 140202902193984 spec.py:321] Evaluating on the training split.
I0217 03:42:12.541230 140202902193984 spec.py:333] Evaluating on the validation split.
I0217 03:43:03.767600 140202902193984 spec.py:349] Evaluating on the test split.
I0217 03:43:29.396407 140202902193984 submission_runner.py:408] Time since start: 11212.08s, 	Step: 13089, 	{'train/ctc_loss': Array(0.3510411, dtype=float32), 'train/wer': 0.11835662247700944, 'validation/ctc_loss': Array(0.6683462, dtype=float32), 'validation/wer': 0.1900421908338724, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3859156, dtype=float32), 'test/wer': 0.12377876627465317, 'test/num_examples': 2472, 'score': 10098.212848901749, 'total_duration': 11212.078014612198, 'accumulated_submission_time': 10098.212848901749, 'accumulated_eval_time': 1112.9421019554138, 'accumulated_logging_time': 0.32501649856567383}
I0217 03:43:29.432147 140047119996672 logging_writer.py:48] [13089] accumulated_eval_time=1112.942102, accumulated_logging_time=0.325016, accumulated_submission_time=10098.212849, global_step=13089, preemption_count=0, score=10098.212849, test/ctc_loss=0.385915607213974, test/num_examples=2472, test/wer=0.123779, total_duration=11212.078015, train/ctc_loss=0.35104110836982727, train/wer=0.118357, validation/ctc_loss=0.6683462262153625, validation/num_examples=5348, validation/wer=0.190042
I0217 03:43:38.574131 140047111603968 logging_writer.py:48] [13100] global_step=13100, grad_norm=3.206563949584961, loss=1.6401968002319336
I0217 03:44:53.817985 140047119996672 logging_writer.py:48] [13200] global_step=13200, grad_norm=2.70696759223938, loss=1.5492000579833984
I0217 03:46:09.154072 140047111603968 logging_writer.py:48] [13300] global_step=13300, grad_norm=2.1001594066619873, loss=1.6138334274291992
I0217 03:47:28.042967 140047119996672 logging_writer.py:48] [13400] global_step=13400, grad_norm=3.525186061859131, loss=1.5773916244506836
I0217 03:48:43.235512 140047111603968 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.6681922674179077, loss=1.4888837337493896
I0217 03:49:58.690397 140047119996672 logging_writer.py:48] [13600] global_step=13600, grad_norm=2.5003018379211426, loss=1.510646939277649
I0217 03:51:13.830458 140047111603968 logging_writer.py:48] [13700] global_step=13700, grad_norm=3.0599892139434814, loss=1.596381664276123
I0217 03:52:29.046738 140047119996672 logging_writer.py:48] [13800] global_step=13800, grad_norm=2.2569563388824463, loss=1.5352182388305664
I0217 03:53:44.326917 140047111603968 logging_writer.py:48] [13900] global_step=13900, grad_norm=2.103966236114502, loss=1.653419852256775
I0217 03:55:00.562213 140047119996672 logging_writer.py:48] [14000] global_step=14000, grad_norm=4.6706953048706055, loss=1.5357897281646729
I0217 03:56:23.360485 140047111603968 logging_writer.py:48] [14100] global_step=14100, grad_norm=2.690349578857422, loss=1.5726902484893799
I0217 03:57:43.411526 140047119996672 logging_writer.py:48] [14200] global_step=14200, grad_norm=2.8567895889282227, loss=1.5583330392837524
I0217 03:59:05.038763 140047111603968 logging_writer.py:48] [14300] global_step=14300, grad_norm=3.0576670169830322, loss=1.5154528617858887
I0217 04:00:25.663829 140047119996672 logging_writer.py:48] [14400] global_step=14400, grad_norm=4.221023082733154, loss=1.543615698814392
I0217 04:01:44.639764 140047119996672 logging_writer.py:48] [14500] global_step=14500, grad_norm=5.576323509216309, loss=1.5087260007858276
I0217 04:03:00.134617 140047111603968 logging_writer.py:48] [14600] global_step=14600, grad_norm=4.278933525085449, loss=1.554158329963684
I0217 04:04:15.596301 140047119996672 logging_writer.py:48] [14700] global_step=14700, grad_norm=3.7174274921417236, loss=1.574711561203003
I0217 04:05:30.860802 140047111603968 logging_writer.py:48] [14800] global_step=14800, grad_norm=4.240459442138672, loss=1.5400243997573853
I0217 04:06:46.109865 140047119996672 logging_writer.py:48] [14900] global_step=14900, grad_norm=2.4688775539398193, loss=1.4922528266906738
I0217 04:07:29.489395 140202902193984 spec.py:321] Evaluating on the training split.
I0217 04:08:24.946955 140202902193984 spec.py:333] Evaluating on the validation split.
I0217 04:09:15.304551 140202902193984 spec.py:349] Evaluating on the test split.
I0217 04:09:40.780339 140202902193984 submission_runner.py:408] Time since start: 12783.46s, 	Step: 14959, 	{'train/ctc_loss': Array(0.3626199, dtype=float32), 'train/wer': 0.1157342155230644, 'validation/ctc_loss': Array(0.6363704, dtype=float32), 'validation/wer': 0.18241501491643897, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36685, dtype=float32), 'test/wer': 0.11784778502224118, 'test/num_examples': 2472, 'score': 11538.172552585602, 'total_duration': 12783.459538698196, 'accumulated_submission_time': 11538.172552585602, 'accumulated_eval_time': 1244.2248899936676, 'accumulated_logging_time': 0.37923097610473633}
I0217 04:09:40.819097 140047119996672 logging_writer.py:48] [14959] accumulated_eval_time=1244.224890, accumulated_logging_time=0.379231, accumulated_submission_time=11538.172553, global_step=14959, preemption_count=0, score=11538.172553, test/ctc_loss=0.36684998869895935, test/num_examples=2472, test/wer=0.117848, total_duration=12783.459539, train/ctc_loss=0.36261990666389465, train/wer=0.115734, validation/ctc_loss=0.6363704204559326, validation/num_examples=5348, validation/wer=0.182415
I0217 04:10:12.572408 140047111603968 logging_writer.py:48] [15000] global_step=15000, grad_norm=2.274385929107666, loss=1.5802782773971558
I0217 04:11:28.253001 140047119996672 logging_writer.py:48] [15100] global_step=15100, grad_norm=3.3624608516693115, loss=1.5645915269851685
I0217 04:12:43.434393 140047111603968 logging_writer.py:48] [15200] global_step=15200, grad_norm=2.2079715728759766, loss=1.512263536453247
I0217 04:13:58.898410 140047119996672 logging_writer.py:48] [15300] global_step=15300, grad_norm=2.4007656574249268, loss=1.5368404388427734
I0217 04:15:14.090080 140047111603968 logging_writer.py:48] [15400] global_step=15400, grad_norm=3.0333333015441895, loss=1.5640318393707275
I0217 04:16:32.413929 140047119996672 logging_writer.py:48] [15500] global_step=15500, grad_norm=3.743812084197998, loss=1.5384118556976318
I0217 04:17:47.899877 140047111603968 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.7896567583084106, loss=1.5110352039337158
I0217 04:19:03.083918 140047119996672 logging_writer.py:48] [15700] global_step=15700, grad_norm=2.9396703243255615, loss=1.4964290857315063
I0217 04:20:18.172325 140047111603968 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.9570012092590332, loss=1.537880778312683
I0217 04:21:33.212663 140047119996672 logging_writer.py:48] [15900] global_step=15900, grad_norm=2.342186689376831, loss=1.5041133165359497
I0217 04:22:48.477780 140047111603968 logging_writer.py:48] [16000] global_step=16000, grad_norm=2.6229939460754395, loss=1.4861723184585571
I0217 04:24:04.897588 140047119996672 logging_writer.py:48] [16100] global_step=16100, grad_norm=3.0965919494628906, loss=1.58278226852417
I0217 04:25:27.063876 140047111603968 logging_writer.py:48] [16200] global_step=16200, grad_norm=2.345978021621704, loss=1.5527774095535278
I0217 04:26:47.105372 140047119996672 logging_writer.py:48] [16300] global_step=16300, grad_norm=3.454174280166626, loss=1.5136462450027466
I0217 04:28:07.237987 140047111603968 logging_writer.py:48] [16400] global_step=16400, grad_norm=2.3667736053466797, loss=1.540574073791504
I0217 04:29:29.819781 140047119996672 logging_writer.py:48] [16500] global_step=16500, grad_norm=3.190781593322754, loss=1.5084543228149414
I0217 04:30:45.033363 140047111603968 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.776580572128296, loss=1.5395146608352661
I0217 04:32:00.366627 140047119996672 logging_writer.py:48] [16700] global_step=16700, grad_norm=2.8038299083709717, loss=1.5224939584732056
I0217 04:33:15.793277 140047111603968 logging_writer.py:48] [16800] global_step=16800, grad_norm=2.962334394454956, loss=1.4597278833389282
I0217 04:33:41.190090 140202902193984 spec.py:321] Evaluating on the training split.
I0217 04:34:38.174454 140202902193984 spec.py:333] Evaluating on the validation split.
I0217 04:35:29.140781 140202902193984 spec.py:349] Evaluating on the test split.
I0217 04:35:55.143012 140202902193984 submission_runner.py:408] Time since start: 14357.83s, 	Step: 16835, 	{'train/ctc_loss': Array(0.2640011, dtype=float32), 'train/wer': 0.09059689540470617, 'validation/ctc_loss': Array(0.5966437, dtype=float32), 'validation/wer': 0.1708680498566284, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3457338, dtype=float32), 'test/wer': 0.11155119533646132, 'test/num_examples': 2472, 'score': 12978.370990991592, 'total_duration': 14357.825068950653, 'accumulated_submission_time': 12978.370990991592, 'accumulated_eval_time': 1378.1725063323975, 'accumulated_logging_time': 0.5102035999298096}
I0217 04:35:55.176800 140047119996672 logging_writer.py:48] [16835] accumulated_eval_time=1378.172506, accumulated_logging_time=0.510204, accumulated_submission_time=12978.370991, global_step=16835, preemption_count=0, score=12978.370991, test/ctc_loss=0.34573379158973694, test/num_examples=2472, test/wer=0.111551, total_duration=14357.825069, train/ctc_loss=0.26400110125541687, train/wer=0.090597, validation/ctc_loss=0.5966436862945557, validation/num_examples=5348, validation/wer=0.170868
I0217 04:36:44.805038 140047111603968 logging_writer.py:48] [16900] global_step=16900, grad_norm=9.575913429260254, loss=1.5237900018692017
I0217 04:37:59.921694 140047119996672 logging_writer.py:48] [17000] global_step=17000, grad_norm=4.184394836425781, loss=1.4971858263015747
I0217 04:39:15.402629 140047111603968 logging_writer.py:48] [17100] global_step=17100, grad_norm=2.2445733547210693, loss=1.5257883071899414
I0217 04:40:30.582849 140047119996672 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.9729632139205933, loss=1.459750771522522
I0217 04:41:46.098401 140047111603968 logging_writer.py:48] [17300] global_step=17300, grad_norm=3.1989622116088867, loss=1.494115948677063
I0217 04:43:02.689376 140047119996672 logging_writer.py:48] [17400] global_step=17400, grad_norm=2.304903984069824, loss=1.528620719909668
I0217 04:44:22.647817 140047111603968 logging_writer.py:48] [17500] global_step=17500, grad_norm=2.81843900680542, loss=1.4965896606445312
I0217 04:45:41.385727 140047119996672 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.937208652496338, loss=1.470221757888794
I0217 04:46:56.265754 140047111603968 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.8785738945007324, loss=1.3932005167007446
I0217 04:48:11.225466 140047119996672 logging_writer.py:48] [17800] global_step=17800, grad_norm=2.8429675102233887, loss=1.470164179801941
I0217 04:49:26.208780 140047111603968 logging_writer.py:48] [17900] global_step=17900, grad_norm=2.0000357627868652, loss=1.5175211429595947
I0217 04:50:41.348931 140047119996672 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.9504591226577759, loss=1.5077928304672241
I0217 04:51:56.844508 140047111603968 logging_writer.py:48] [18100] global_step=18100, grad_norm=3.190065622329712, loss=1.5457159280776978
I0217 04:53:12.186781 140047119996672 logging_writer.py:48] [18200] global_step=18200, grad_norm=3.4915971755981445, loss=1.565261721611023
I0217 04:54:32.482137 140047111603968 logging_writer.py:48] [18300] global_step=18300, grad_norm=2.3297884464263916, loss=1.5074381828308105
I0217 04:55:52.537828 140047119996672 logging_writer.py:48] [18400] global_step=18400, grad_norm=3.1478312015533447, loss=1.5230491161346436
I0217 04:57:12.625057 140047111603968 logging_writer.py:48] [18500] global_step=18500, grad_norm=2.2215867042541504, loss=1.4890750646591187
I0217 04:58:32.903455 140047119996672 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.5637027025222778, loss=1.4397639036178589
I0217 04:59:48.161128 140047111603968 logging_writer.py:48] [18700] global_step=18700, grad_norm=3.092961549758911, loss=1.5314147472381592
I0217 04:59:55.566200 140202902193984 spec.py:321] Evaluating on the training split.
I0217 05:00:51.384198 140202902193984 spec.py:333] Evaluating on the validation split.
I0217 05:01:41.806453 140202902193984 spec.py:349] Evaluating on the test split.
I0217 05:02:07.502743 140202902193984 submission_runner.py:408] Time since start: 15930.18s, 	Step: 18711, 	{'train/ctc_loss': Array(0.26224798, dtype=float32), 'train/wer': 0.08935853688985061, 'validation/ctc_loss': Array(0.5874665, dtype=float32), 'validation/wer': 0.16828060283653706, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33608237, dtype=float32), 'test/wer': 0.10872788576767616, 'test/num_examples': 2472, 'score': 14418.6655626297, 'total_duration': 15930.184390544891, 'accumulated_submission_time': 14418.6655626297, 'accumulated_eval_time': 1510.1033344268799, 'accumulated_logging_time': 0.5596368312835693}
I0217 05:02:07.535552 140047119996672 logging_writer.py:48] [18711] accumulated_eval_time=1510.103334, accumulated_logging_time=0.559637, accumulated_submission_time=14418.665563, global_step=18711, preemption_count=0, score=14418.665563, test/ctc_loss=0.3360823690891266, test/num_examples=2472, test/wer=0.108728, total_duration=15930.184391, train/ctc_loss=0.2622479796409607, train/wer=0.089359, validation/ctc_loss=0.5874664783477783, validation/num_examples=5348, validation/wer=0.168281
I0217 05:03:14.962774 140047111603968 logging_writer.py:48] [18800] global_step=18800, grad_norm=2.702338933944702, loss=1.4589593410491943
I0217 05:04:30.001830 140047119996672 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.6857067346572876, loss=1.4790531396865845
I0217 05:05:45.192009 140047111603968 logging_writer.py:48] [19000] global_step=19000, grad_norm=2.9851090908050537, loss=1.4930853843688965
I0217 05:07:00.379708 140047119996672 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.6556590795516968, loss=1.4786819219589233
I0217 05:08:16.110548 140047111603968 logging_writer.py:48] [19200] global_step=19200, grad_norm=2.824077844619751, loss=1.453963279724121
I0217 05:09:31.737093 140047119996672 logging_writer.py:48] [19300] global_step=19300, grad_norm=2.3754138946533203, loss=1.4373217821121216
I0217 05:10:47.133493 140047111603968 logging_writer.py:48] [19400] global_step=19400, grad_norm=2.3875768184661865, loss=1.4519562721252441
I0217 05:12:04.761506 140047119996672 logging_writer.py:48] [19500] global_step=19500, grad_norm=2.5829153060913086, loss=1.5293503999710083
I0217 05:13:26.832705 140047119996672 logging_writer.py:48] [19600] global_step=19600, grad_norm=2.429029941558838, loss=1.4925284385681152
I0217 05:14:42.054507 140047111603968 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.8678760528564453, loss=1.4517208337783813
I0217 05:15:57.084460 140047119996672 logging_writer.py:48] [19800] global_step=19800, grad_norm=2.7178378105163574, loss=1.453911542892456
I0217 05:17:12.395519 140047111603968 logging_writer.py:48] [19900] global_step=19900, grad_norm=2.190376043319702, loss=1.4681657552719116
I0217 05:18:27.436986 140047119996672 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.9626870155334473, loss=1.5006239414215088
I0217 05:19:42.452729 140047111603968 logging_writer.py:48] [20100] global_step=20100, grad_norm=2.363053560256958, loss=1.4525866508483887
I0217 05:20:57.513365 140047119996672 logging_writer.py:48] [20200] global_step=20200, grad_norm=3.0596773624420166, loss=1.457906723022461
I0217 05:22:17.892760 140047111603968 logging_writer.py:48] [20300] global_step=20300, grad_norm=2.6840405464172363, loss=1.4568325281143188
I0217 05:23:38.407092 140047119996672 logging_writer.py:48] [20400] global_step=20400, grad_norm=4.533480167388916, loss=1.5007712841033936
I0217 05:25:00.749389 140047111603968 logging_writer.py:48] [20500] global_step=20500, grad_norm=2.824862480163574, loss=1.515934705734253
I0217 05:26:08.225978 140202902193984 spec.py:321] Evaluating on the training split.
I0217 05:27:01.982723 140202902193984 spec.py:333] Evaluating on the validation split.
I0217 05:27:53.123049 140202902193984 spec.py:349] Evaluating on the test split.
I0217 05:28:19.494869 140202902193984 submission_runner.py:408] Time since start: 17502.18s, 	Step: 20585, 	{'train/ctc_loss': Array(0.38126385, dtype=float32), 'train/wer': 0.12747945476748407, 'validation/ctc_loss': Array(0.5813514, dtype=float32), 'validation/wer': 0.16634001757146857, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33556062, dtype=float32), 'test/wer': 0.10919505209920176, 'test/num_examples': 2472, 'score': 15859.262013435364, 'total_duration': 17502.176399946213, 'accumulated_submission_time': 15859.262013435364, 'accumulated_eval_time': 1641.366404056549, 'accumulated_logging_time': 0.6074838638305664}
I0217 05:28:19.526968 140047119996672 logging_writer.py:48] [20585] accumulated_eval_time=1641.366404, accumulated_logging_time=0.607484, accumulated_submission_time=15859.262013, global_step=20585, preemption_count=0, score=15859.262013, test/ctc_loss=0.3355606198310852, test/num_examples=2472, test/wer=0.109195, total_duration=17502.176400, train/ctc_loss=0.3812638521194458, train/wer=0.127479, validation/ctc_loss=0.5813513994216919, validation/num_examples=5348, validation/wer=0.166340
I0217 05:28:34.982573 140047119996672 logging_writer.py:48] [20600] global_step=20600, grad_norm=3.258701801300049, loss=1.444926381111145
I0217 05:29:50.217831 140047111603968 logging_writer.py:48] [20700] global_step=20700, grad_norm=4.111458778381348, loss=1.4270988702774048
I0217 05:31:05.271746 140047119996672 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.9314640760421753, loss=1.4028058052062988
I0217 05:32:20.462322 140047111603968 logging_writer.py:48] [20900] global_step=20900, grad_norm=3.3827528953552246, loss=1.4815943241119385
I0217 05:33:35.513020 140047119996672 logging_writer.py:48] [21000] global_step=21000, grad_norm=2.708071708679199, loss=1.4674404859542847
I0217 05:34:50.512202 140047111603968 logging_writer.py:48] [21100] global_step=21100, grad_norm=2.648446559906006, loss=1.5356645584106445
I0217 05:36:05.662885 140047119996672 logging_writer.py:48] [21200] global_step=21200, grad_norm=3.649472713470459, loss=1.501666784286499
I0217 05:37:25.449950 140047111603968 logging_writer.py:48] [21300] global_step=21300, grad_norm=2.5148532390594482, loss=1.4995990991592407
I0217 05:38:44.933926 140047119996672 logging_writer.py:48] [21400] global_step=21400, grad_norm=6.755698204040527, loss=1.4396003484725952
I0217 05:40:06.433294 140047111603968 logging_writer.py:48] [21500] global_step=21500, grad_norm=2.5565600395202637, loss=1.4189881086349487
I0217 05:41:27.036191 140047119996672 logging_writer.py:48] [21600] global_step=21600, grad_norm=2.660871744155884, loss=1.5419397354125977
I0217 05:42:47.263989 140047119996672 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.7926400899887085, loss=1.4207843542099
I0217 05:44:02.483676 140047111603968 logging_writer.py:48] [21800] global_step=21800, grad_norm=5.1120123863220215, loss=1.4888454675674438
I0217 05:45:17.692976 140047119996672 logging_writer.py:48] [21900] global_step=21900, grad_norm=4.400636672973633, loss=1.4264159202575684
I0217 05:46:32.747958 140047111603968 logging_writer.py:48] [22000] global_step=22000, grad_norm=2.3047263622283936, loss=1.4793771505355835
I0217 05:47:48.017308 140047119996672 logging_writer.py:48] [22100] global_step=22100, grad_norm=3.529735565185547, loss=1.4429908990859985
I0217 05:49:03.448395 140047111603968 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.951245903968811, loss=1.4827170372009277
I0217 05:50:20.529192 140047119996672 logging_writer.py:48] [22300] global_step=22300, grad_norm=2.533003091812134, loss=1.507673978805542
I0217 05:51:42.215688 140047111603968 logging_writer.py:48] [22400] global_step=22400, grad_norm=3.2963969707489014, loss=1.4589604139328003
I0217 05:52:20.230875 140202902193984 spec.py:321] Evaluating on the training split.
I0217 05:53:14.817443 140202902193984 spec.py:333] Evaluating on the validation split.
I0217 05:54:05.780742 140202902193984 spec.py:349] Evaluating on the test split.
I0217 05:54:32.512874 140202902193984 submission_runner.py:408] Time since start: 19075.19s, 	Step: 22448, 	{'train/ctc_loss': Array(0.3882775, dtype=float32), 'train/wer': 0.1288513710578304, 'validation/ctc_loss': Array(0.57250744, dtype=float32), 'validation/wer': 0.16418702993907913, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32734656, dtype=float32), 'test/wer': 0.1047671277395243, 'test/num_examples': 2472, 'score': 17299.8719329834, 'total_duration': 19075.193665504456, 'accumulated_submission_time': 17299.8719329834, 'accumulated_eval_time': 1773.641833782196, 'accumulated_logging_time': 0.6536881923675537}
I0217 05:54:32.551241 140047119996672 logging_writer.py:48] [22448] accumulated_eval_time=1773.641834, accumulated_logging_time=0.653688, accumulated_submission_time=17299.871933, global_step=22448, preemption_count=0, score=17299.871933, test/ctc_loss=0.3273465633392334, test/num_examples=2472, test/wer=0.104767, total_duration=19075.193666, train/ctc_loss=0.38827750086784363, train/wer=0.128851, validation/ctc_loss=0.5725074410438538, validation/num_examples=5348, validation/wer=0.164187
I0217 05:55:12.366606 140047111603968 logging_writer.py:48] [22500] global_step=22500, grad_norm=2.8825631141662598, loss=1.5143523216247559
I0217 05:56:27.781158 140047119996672 logging_writer.py:48] [22600] global_step=22600, grad_norm=2.3515350818634033, loss=1.4172484874725342
I0217 05:57:46.669981 140047119996672 logging_writer.py:48] [22700] global_step=22700, grad_norm=2.5842854976654053, loss=1.4714090824127197
I0217 05:59:01.830877 140047111603968 logging_writer.py:48] [22800] global_step=22800, grad_norm=2.5649681091308594, loss=1.3696153163909912
I0217 06:00:16.993444 140047119996672 logging_writer.py:48] [22900] global_step=22900, grad_norm=3.4971389770507812, loss=1.5068694353103638
I0217 06:01:32.013073 140047111603968 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.5163534879684448, loss=1.411436915397644
I0217 06:02:46.894567 140047119996672 logging_writer.py:48] [23100] global_step=23100, grad_norm=2.2332046031951904, loss=1.4129558801651
I0217 06:04:01.971907 140047111603968 logging_writer.py:48] [23200] global_step=23200, grad_norm=2.382338523864746, loss=1.4098271131515503
I0217 06:05:18.578558 140047119996672 logging_writer.py:48] [23300] global_step=23300, grad_norm=2.5693182945251465, loss=1.4257450103759766
I0217 06:06:38.819090 140047111603968 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.8733843564987183, loss=1.4597663879394531
I0217 06:08:01.101675 140047119996672 logging_writer.py:48] [23500] global_step=23500, grad_norm=2.628570318222046, loss=1.493533730506897
I0217 06:09:21.988471 140047111603968 logging_writer.py:48] [23600] global_step=23600, grad_norm=2.05830717086792, loss=1.4180153608322144
I0217 06:10:44.882089 140047119996672 logging_writer.py:48] [23700] global_step=23700, grad_norm=2.37536883354187, loss=1.3526220321655273
I0217 06:12:00.005850 140047111603968 logging_writer.py:48] [23800] global_step=23800, grad_norm=2.7756459712982178, loss=1.384981393814087
I0217 06:13:15.317934 140047119996672 logging_writer.py:48] [23900] global_step=23900, grad_norm=2.347972869873047, loss=1.4348145723342896
I0217 06:14:30.667970 140047111603968 logging_writer.py:48] [24000] global_step=24000, grad_norm=3.7558374404907227, loss=1.442404866218567
I0217 06:15:45.787694 140047119996672 logging_writer.py:48] [24100] global_step=24100, grad_norm=2.659334659576416, loss=1.3570705652236938
I0217 06:17:00.729324 140047111603968 logging_writer.py:48] [24200] global_step=24200, grad_norm=3.2754030227661133, loss=1.412577509880066
I0217 06:18:15.917894 140047119996672 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.9497545957565308, loss=1.3910188674926758
I0217 06:18:33.016181 140202902193984 spec.py:321] Evaluating on the training split.
I0217 06:19:26.598583 140202902193984 spec.py:333] Evaluating on the validation split.
I0217 06:20:18.464211 140202902193984 spec.py:349] Evaluating on the test split.
I0217 06:20:45.163099 140202902193984 submission_runner.py:408] Time since start: 20647.84s, 	Step: 24324, 	{'train/ctc_loss': Array(0.44150472, dtype=float32), 'train/wer': 0.14325196832106277, 'validation/ctc_loss': Array(0.5418794, dtype=float32), 'validation/wer': 0.15532405842996033, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30448222, dtype=float32), 'test/wer': 0.09710966221843073, 'test/num_examples': 2472, 'score': 18740.242106437683, 'total_duration': 20647.843740701675, 'accumulated_submission_time': 18740.242106437683, 'accumulated_eval_time': 1905.782042503357, 'accumulated_logging_time': 0.7064037322998047}
I0217 06:20:45.196243 140047119996672 logging_writer.py:48] [24324] accumulated_eval_time=1905.782043, accumulated_logging_time=0.706404, accumulated_submission_time=18740.242106, global_step=24324, preemption_count=0, score=18740.242106, test/ctc_loss=0.30448222160339355, test/num_examples=2472, test/wer=0.097110, total_duration=20647.843741, train/ctc_loss=0.44150471687316895, train/wer=0.143252, validation/ctc_loss=0.541879415512085, validation/num_examples=5348, validation/wer=0.155324
I0217 06:21:43.029539 140047111603968 logging_writer.py:48] [24400] global_step=24400, grad_norm=3.0077974796295166, loss=1.400173544883728
I0217 06:22:58.137244 140047119996672 logging_writer.py:48] [24500] global_step=24500, grad_norm=5.458872318267822, loss=1.4281487464904785
I0217 06:24:13.386966 140047111603968 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.8011599779129028, loss=1.4086931943893433
I0217 06:25:28.315887 140047119996672 logging_writer.py:48] [24700] global_step=24700, grad_norm=3.5592527389526367, loss=1.4184190034866333
I0217 06:26:47.406974 140047119996672 logging_writer.py:48] [24800] global_step=24800, grad_norm=2.9749667644500732, loss=1.3954501152038574
I0217 06:28:02.638931 140047111603968 logging_writer.py:48] [24900] global_step=24900, grad_norm=2.1986939907073975, loss=1.4339503049850464
I0217 06:29:17.723258 140047119996672 logging_writer.py:48] [25000] global_step=25000, grad_norm=2.0375702381134033, loss=1.3778232336044312
I0217 06:30:32.828934 140047111603968 logging_writer.py:48] [25100] global_step=25100, grad_norm=2.6057448387145996, loss=1.3962950706481934
I0217 06:31:47.751673 140047119996672 logging_writer.py:48] [25200] global_step=25200, grad_norm=2.5444507598876953, loss=1.4728254079818726
I0217 06:33:02.959869 140047111603968 logging_writer.py:48] [25300] global_step=25300, grad_norm=3.935229539871216, loss=1.3819020986557007
I0217 06:34:21.277443 140047119996672 logging_writer.py:48] [25400] global_step=25400, grad_norm=2.328341245651245, loss=1.4664074182510376
I0217 06:35:41.496195 140047111603968 logging_writer.py:48] [25500] global_step=25500, grad_norm=2.2141830921173096, loss=1.4416065216064453
I0217 06:37:02.052652 140047119996672 logging_writer.py:48] [25600] global_step=25600, grad_norm=2.4618642330169678, loss=1.3617362976074219
I0217 06:38:23.219753 140047111603968 logging_writer.py:48] [25700] global_step=25700, grad_norm=3.1419150829315186, loss=1.4183151721954346
I0217 06:39:45.072059 140047119996672 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.925788402557373, loss=1.3962682485580444
I0217 06:41:00.436920 140047111603968 logging_writer.py:48] [25900] global_step=25900, grad_norm=2.1610231399536133, loss=1.4374876022338867
I0217 06:42:15.750143 140047119996672 logging_writer.py:48] [26000] global_step=26000, grad_norm=4.975571155548096, loss=1.4178670644760132
I0217 06:43:30.782311 140047111603968 logging_writer.py:48] [26100] global_step=26100, grad_norm=3.9147424697875977, loss=1.3968876600265503
I0217 06:44:45.765391 140047119996672 logging_writer.py:48] [26200] global_step=26200, grad_norm=2.7838258743286133, loss=1.3975419998168945
I0217 06:44:45.772475 140202902193984 spec.py:321] Evaluating on the training split.
I0217 06:45:38.729365 140202902193984 spec.py:333] Evaluating on the validation split.
I0217 06:46:30.059211 140202902193984 spec.py:349] Evaluating on the test split.
I0217 06:46:56.076006 140202902193984 submission_runner.py:408] Time since start: 22218.76s, 	Step: 26201, 	{'train/ctc_loss': Array(0.36288148, dtype=float32), 'train/wer': 0.11984659635666348, 'validation/ctc_loss': Array(0.5348055, dtype=float32), 'validation/wer': 0.15417515471581528, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30177844, dtype=float32), 'test/wer': 0.09664249588690513, 'test/num_examples': 2472, 'score': 20180.723200559616, 'total_duration': 22218.75596666336, 'accumulated_submission_time': 20180.723200559616, 'accumulated_eval_time': 2036.0781605243683, 'accumulated_logging_time': 0.7543544769287109}
I0217 06:46:56.121749 140047119996672 logging_writer.py:48] [26201] accumulated_eval_time=2036.078161, accumulated_logging_time=0.754354, accumulated_submission_time=20180.723201, global_step=26201, preemption_count=0, score=20180.723201, test/ctc_loss=0.3017784357070923, test/num_examples=2472, test/wer=0.096642, total_duration=22218.755967, train/ctc_loss=0.36288148164749146, train/wer=0.119847, validation/ctc_loss=0.5348054766654968, validation/num_examples=5348, validation/wer=0.154175
I0217 06:48:11.316861 140047111603968 logging_writer.py:48] [26300] global_step=26300, grad_norm=2.281805992126465, loss=1.3866842985153198
I0217 06:49:26.914825 140047119996672 logging_writer.py:48] [26400] global_step=26400, grad_norm=2.430210590362549, loss=1.3321853876113892
I0217 06:50:42.121495 140047111603968 logging_writer.py:48] [26500] global_step=26500, grad_norm=3.3025810718536377, loss=1.3714245557785034
I0217 06:51:57.246891 140047119996672 logging_writer.py:48] [26600] global_step=26600, grad_norm=2.0624313354492188, loss=1.3763184547424316
I0217 06:53:12.657440 140047111603968 logging_writer.py:48] [26700] global_step=26700, grad_norm=2.921760082244873, loss=1.4657310247421265
I0217 06:54:34.151445 140047119996672 logging_writer.py:48] [26800] global_step=26800, grad_norm=2.0460920333862305, loss=1.3772988319396973
I0217 06:55:49.285267 140047111603968 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.911097526550293, loss=1.391153335571289
I0217 06:57:04.303275 140047119996672 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.8161771297454834, loss=1.3467179536819458
I0217 06:58:19.213839 140047111603968 logging_writer.py:48] [27100] global_step=27100, grad_norm=2.040700912475586, loss=1.4126313924789429
I0217 06:59:34.316905 140047119996672 logging_writer.py:48] [27200] global_step=27200, grad_norm=3.717064142227173, loss=1.4085814952850342
I0217 07:00:49.516698 140047111603968 logging_writer.py:48] [27300] global_step=27300, grad_norm=2.3589205741882324, loss=1.426345944404602
I0217 07:02:04.772025 140047119996672 logging_writer.py:48] [27400] global_step=27400, grad_norm=2.172722339630127, loss=1.434441089630127
I0217 07:03:21.222561 140047111603968 logging_writer.py:48] [27500] global_step=27500, grad_norm=2.889035940170288, loss=1.3837554454803467
I0217 07:04:42.287619 140047119996672 logging_writer.py:48] [27600] global_step=27600, grad_norm=2.2146124839782715, loss=1.3040432929992676
I0217 07:06:02.883338 140047111603968 logging_writer.py:48] [27700] global_step=27700, grad_norm=2.63287091255188, loss=1.3339864015579224
I0217 07:07:24.542901 140047119996672 logging_writer.py:48] [27800] global_step=27800, grad_norm=2.540247917175293, loss=1.399782419204712
I0217 07:08:43.214660 140047119996672 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.8872848749160767, loss=1.3692539930343628
I0217 07:09:58.304237 140047111603968 logging_writer.py:48] [28000] global_step=28000, grad_norm=2.2892258167266846, loss=1.3133996725082397
I0217 07:10:56.506927 140202902193984 spec.py:321] Evaluating on the training split.
I0217 07:11:49.149849 140202902193984 spec.py:333] Evaluating on the validation split.
I0217 07:12:40.312580 140202902193984 spec.py:349] Evaluating on the test split.
I0217 07:13:06.376636 140202902193984 submission_runner.py:408] Time since start: 23789.06s, 	Step: 28079, 	{'train/ctc_loss': Array(0.37025255, dtype=float32), 'train/wer': 0.12474835363565019, 'validation/ctc_loss': Array(0.52608824, dtype=float32), 'validation/wer': 0.15051604120605927, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29157224, dtype=float32), 'test/wer': 0.09440822212743485, 'test/num_examples': 2472, 'score': 21621.010961294174, 'total_duration': 23789.058510541916, 'accumulated_submission_time': 21621.010961294174, 'accumulated_eval_time': 2165.942393541336, 'accumulated_logging_time': 0.8169291019439697}
I0217 07:13:06.410160 140047119996672 logging_writer.py:48] [28079] accumulated_eval_time=2165.942394, accumulated_logging_time=0.816929, accumulated_submission_time=21621.010961, global_step=28079, preemption_count=0, score=21621.010961, test/ctc_loss=0.291572242975235, test/num_examples=2472, test/wer=0.094408, total_duration=23789.058511, train/ctc_loss=0.3702525496482849, train/wer=0.124748, validation/ctc_loss=0.5260882377624512, validation/num_examples=5348, validation/wer=0.150516
I0217 07:13:22.992050 140047111603968 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.8862216472625732, loss=1.3928110599517822
I0217 07:14:38.187658 140047119996672 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.9856685400009155, loss=1.2830119132995605
I0217 07:15:53.437115 140047111603968 logging_writer.py:48] [28300] global_step=28300, grad_norm=3.493795394897461, loss=1.3674670457839966
I0217 07:17:09.122632 140047119996672 logging_writer.py:48] [28400] global_step=28400, grad_norm=2.534363031387329, loss=1.350862979888916
I0217 07:18:24.182517 140047111603968 logging_writer.py:48] [28500] global_step=28500, grad_norm=3.6687333583831787, loss=1.3766982555389404
I0217 07:19:39.214770 140047119996672 logging_writer.py:48] [28600] global_step=28600, grad_norm=2.258944272994995, loss=1.3250384330749512
I0217 07:20:55.963984 140047111603968 logging_writer.py:48] [28700] global_step=28700, grad_norm=2.045041084289551, loss=1.3758982419967651
I0217 07:22:16.764638 140047119996672 logging_writer.py:48] [28800] global_step=28800, grad_norm=3.272961139678955, loss=1.3792802095413208
I0217 07:23:36.746449 140047119996672 logging_writer.py:48] [28900] global_step=28900, grad_norm=2.585188388824463, loss=1.3449058532714844
I0217 07:24:52.253533 140047111603968 logging_writer.py:48] [29000] global_step=29000, grad_norm=4.435660362243652, loss=1.3397144079208374
I0217 07:26:07.397810 140047119996672 logging_writer.py:48] [29100] global_step=29100, grad_norm=2.166046380996704, loss=1.4209959506988525
I0217 07:27:22.588130 140047111603968 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.9530888795852661, loss=1.3740558624267578
I0217 07:28:37.902310 140047119996672 logging_writer.py:48] [29300] global_step=29300, grad_norm=4.1273884773254395, loss=1.3499910831451416
I0217 07:29:52.978169 140047111603968 logging_writer.py:48] [29400] global_step=29400, grad_norm=2.730229139328003, loss=1.3071433305740356
I0217 07:31:08.336418 140047119996672 logging_writer.py:48] [29500] global_step=29500, grad_norm=6.3310866355896, loss=1.3600283861160278
I0217 07:32:28.592001 140047111603968 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.867625117301941, loss=1.362708330154419
I0217 07:33:49.194328 140047119996672 logging_writer.py:48] [29700] global_step=29700, grad_norm=2.4115617275238037, loss=1.3585715293884277
I0217 07:35:11.391473 140047111603968 logging_writer.py:48] [29800] global_step=29800, grad_norm=2.4073333740234375, loss=1.247390866279602
I0217 07:36:33.816749 140047119996672 logging_writer.py:48] [29900] global_step=29900, grad_norm=3.274980068206787, loss=1.402936577796936
I0217 07:37:06.768247 140202902193984 spec.py:321] Evaluating on the training split.
I0217 07:38:01.733228 140202902193984 spec.py:333] Evaluating on the validation split.
I0217 07:38:51.846162 140202902193984 spec.py:349] Evaluating on the test split.
I0217 07:39:17.943544 140202902193984 submission_runner.py:408] Time since start: 25360.62s, 	Step: 29945, 	{'train/ctc_loss': Array(0.27366316, dtype=float32), 'train/wer': 0.09416574763966969, 'validation/ctc_loss': Array(0.50481427, dtype=float32), 'validation/wer': 0.14388329455381021, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.279768, dtype=float32), 'test/wer': 0.0905693335770723, 'test/num_examples': 2472, 'score': 23061.27196407318, 'total_duration': 25360.624280691147, 'accumulated_submission_time': 23061.27196407318, 'accumulated_eval_time': 2297.1110696792603, 'accumulated_logging_time': 0.867875337600708}
I0217 07:39:17.981881 140047119996672 logging_writer.py:48] [29945] accumulated_eval_time=2297.111070, accumulated_logging_time=0.867875, accumulated_submission_time=23061.271964, global_step=29945, preemption_count=0, score=23061.271964, test/ctc_loss=0.2797679901123047, test/num_examples=2472, test/wer=0.090569, total_duration=25360.624281, train/ctc_loss=0.27366316318511963, train/wer=0.094166, validation/ctc_loss=0.5048142671585083, validation/num_examples=5348, validation/wer=0.143883
I0217 07:40:00.112119 140047111603968 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.855681300163269, loss=1.2705373764038086
I0217 07:41:15.582994 140047119996672 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.8942772150039673, loss=1.3744853734970093
I0217 07:42:30.780242 140047111603968 logging_writer.py:48] [30200] global_step=30200, grad_norm=2.94838547706604, loss=1.3444119691848755
I0217 07:43:45.854920 140047119996672 logging_writer.py:48] [30300] global_step=30300, grad_norm=3.008676052093506, loss=1.3192163705825806
I0217 07:45:01.411751 140047111603968 logging_writer.py:48] [30400] global_step=30400, grad_norm=3.630441188812256, loss=1.3412442207336426
I0217 07:46:16.989434 140047119996672 logging_writer.py:48] [30500] global_step=30500, grad_norm=2.9627444744110107, loss=1.2800440788269043
I0217 07:47:32.373842 140047111603968 logging_writer.py:48] [30600] global_step=30600, grad_norm=3.030702590942383, loss=1.3114080429077148
I0217 07:48:47.341836 140047119996672 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.8091353178024292, loss=1.361069917678833
I0217 07:50:07.630173 140047111603968 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.9358519315719604, loss=1.3808903694152832
I0217 07:51:31.701188 140047119996672 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.8495436906814575, loss=1.3036984205245972
I0217 07:52:46.926984 140047111603968 logging_writer.py:48] [31000] global_step=31000, grad_norm=2.289350986480713, loss=1.3762297630310059
I0217 07:54:02.137289 140047119996672 logging_writer.py:48] [31100] global_step=31100, grad_norm=2.982093095779419, loss=1.2813674211502075
I0217 07:55:17.601430 140047111603968 logging_writer.py:48] [31200] global_step=31200, grad_norm=2.343567371368408, loss=1.3051525354385376
I0217 07:56:32.816447 140047119996672 logging_writer.py:48] [31300] global_step=31300, grad_norm=2.4785101413726807, loss=1.3091051578521729
I0217 07:57:48.117969 140047111603968 logging_writer.py:48] [31400] global_step=31400, grad_norm=3.9493610858917236, loss=1.2356573343276978
I0217 07:59:03.441565 140047119996672 logging_writer.py:48] [31500] global_step=31500, grad_norm=2.2339701652526855, loss=1.2960383892059326
I0217 08:00:18.567990 140047111603968 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.7900004386901855, loss=1.305991291999817
I0217 08:01:38.012067 140047119996672 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.7687952518463135, loss=1.287328839302063
I0217 08:02:57.503818 140047111603968 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.94492769241333, loss=1.254677176475525
I0217 08:03:18.001077 140202902193984 spec.py:321] Evaluating on the training split.
I0217 08:04:13.149619 140202902193984 spec.py:333] Evaluating on the validation split.
I0217 08:05:04.233601 140202902193984 spec.py:349] Evaluating on the test split.
I0217 08:05:30.164799 140202902193984 submission_runner.py:408] Time since start: 26932.85s, 	Step: 31827, 	{'train/ctc_loss': Array(0.31801972, dtype=float32), 'train/wer': 0.10824967979945047, 'validation/ctc_loss': Array(0.48673412, dtype=float32), 'validation/wer': 0.13978006700329224, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26446828, dtype=float32), 'test/wer': 0.08579611236365853, 'test/num_examples': 2472, 'score': 24501.19046640396, 'total_duration': 26932.845405817032, 'accumulated_submission_time': 24501.19046640396, 'accumulated_eval_time': 2429.268038749695, 'accumulated_logging_time': 0.9267137050628662}
I0217 08:05:30.201585 140047119996672 logging_writer.py:48] [31827] accumulated_eval_time=2429.268039, accumulated_logging_time=0.926714, accumulated_submission_time=24501.190466, global_step=31827, preemption_count=0, score=24501.190466, test/ctc_loss=0.2644682824611664, test/num_examples=2472, test/wer=0.085796, total_duration=26932.845406, train/ctc_loss=0.31801971793174744, train/wer=0.108250, validation/ctc_loss=0.4867341220378876, validation/num_examples=5348, validation/wer=0.139780
I0217 08:06:25.860195 140047111603968 logging_writer.py:48] [31900] global_step=31900, grad_norm=2.0385591983795166, loss=1.3015620708465576
I0217 08:07:44.163642 140047119996672 logging_writer.py:48] [32000] global_step=32000, grad_norm=3.0901074409484863, loss=1.323211431503296
I0217 08:08:59.127050 140047111603968 logging_writer.py:48] [32100] global_step=32100, grad_norm=3.114513397216797, loss=1.3202486038208008
I0217 08:10:13.970512 140047119996672 logging_writer.py:48] [32200] global_step=32200, grad_norm=2.2351279258728027, loss=1.3074918985366821
I0217 08:11:28.937676 140047111603968 logging_writer.py:48] [32300] global_step=32300, grad_norm=2.1193697452545166, loss=1.290490746498108
I0217 08:12:44.168819 140047119996672 logging_writer.py:48] [32400] global_step=32400, grad_norm=2.363614320755005, loss=1.2714486122131348
I0217 08:13:59.357996 140047111603968 logging_writer.py:48] [32500] global_step=32500, grad_norm=2.0035152435302734, loss=1.273594617843628
I0217 08:15:14.792453 140047119996672 logging_writer.py:48] [32600] global_step=32600, grad_norm=3.391954183578491, loss=1.2911702394485474
I0217 08:16:35.000729 140047111603968 logging_writer.py:48] [32700] global_step=32700, grad_norm=2.2621090412139893, loss=1.2805942296981812
I0217 08:17:56.349942 140047119996672 logging_writer.py:48] [32800] global_step=32800, grad_norm=3.8185012340545654, loss=1.2696441411972046
I0217 08:19:16.893314 140047111603968 logging_writer.py:48] [32900] global_step=32900, grad_norm=2.65690016746521, loss=1.3797105550765991
I0217 08:20:38.141755 140047119996672 logging_writer.py:48] [33000] global_step=33000, grad_norm=2.313875198364258, loss=1.2824710607528687
I0217 08:21:53.476250 140047111603968 logging_writer.py:48] [33100] global_step=33100, grad_norm=2.98669171333313, loss=1.2464077472686768
I0217 08:23:08.911939 140047119996672 logging_writer.py:48] [33200] global_step=33200, grad_norm=2.206838369369507, loss=1.2817471027374268
I0217 08:24:24.015966 140047111603968 logging_writer.py:48] [33300] global_step=33300, grad_norm=3.169398546218872, loss=1.2981771230697632
I0217 08:25:39.364562 140047119996672 logging_writer.py:48] [33400] global_step=33400, grad_norm=3.4710981845855713, loss=1.2893425226211548
I0217 08:26:54.603851 140047111603968 logging_writer.py:48] [33500] global_step=33500, grad_norm=2.942197322845459, loss=1.2653387784957886
I0217 08:28:10.158619 140047119996672 logging_writer.py:48] [33600] global_step=33600, grad_norm=2.0292153358459473, loss=1.2185226678848267
I0217 08:29:25.512435 140047111603968 logging_writer.py:48] [33700] global_step=33700, grad_norm=2.704629898071289, loss=1.2449848651885986
I0217 08:29:30.733681 140202902193984 spec.py:321] Evaluating on the training split.
I0217 08:30:26.456127 140202902193984 spec.py:333] Evaluating on the validation split.
I0217 08:31:17.221161 140202902193984 spec.py:349] Evaluating on the test split.
I0217 08:31:42.813812 140202902193984 submission_runner.py:408] Time since start: 28505.49s, 	Step: 33708, 	{'train/ctc_loss': Array(0.2762536, dtype=float32), 'train/wer': 0.0937875751503006, 'validation/ctc_loss': Array(0.47617844, dtype=float32), 'validation/wer': 0.13711538275872057, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25827703, dtype=float32), 'test/wer': 0.08281031015782098, 'test/num_examples': 2472, 'score': 25941.62428545952, 'total_duration': 28505.49468064308, 'accumulated_submission_time': 25941.62428545952, 'accumulated_eval_time': 2561.341689825058, 'accumulated_logging_time': 0.9807431697845459}
I0217 08:31:42.851574 140047119996672 logging_writer.py:48] [33708] accumulated_eval_time=2561.341690, accumulated_logging_time=0.980743, accumulated_submission_time=25941.624285, global_step=33708, preemption_count=0, score=25941.624285, test/ctc_loss=0.258277028799057, test/num_examples=2472, test/wer=0.082810, total_duration=28505.494681, train/ctc_loss=0.2762536108493805, train/wer=0.093788, validation/ctc_loss=0.47617843747138977, validation/num_examples=5348, validation/wer=0.137115
I0217 08:32:52.849391 140047111603968 logging_writer.py:48] [33800] global_step=33800, grad_norm=3.468596935272217, loss=1.2621489763259888
I0217 08:34:07.895684 140047119996672 logging_writer.py:48] [33900] global_step=33900, grad_norm=2.030013084411621, loss=1.2104548215866089
I0217 08:35:26.429322 140047119996672 logging_writer.py:48] [34000] global_step=34000, grad_norm=3.0991053581237793, loss=1.3350721597671509
I0217 08:36:41.818727 140047111603968 logging_writer.py:48] [34100] global_step=34100, grad_norm=2.7109458446502686, loss=1.2587363719940186
I0217 08:37:56.713089 140047119996672 logging_writer.py:48] [34200] global_step=34200, grad_norm=6.614810466766357, loss=1.2546542882919312
I0217 08:39:11.825265 140047111603968 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.8778555393218994, loss=1.221163272857666
I0217 08:40:26.931549 140047119996672 logging_writer.py:48] [34400] global_step=34400, grad_norm=5.272453308105469, loss=1.2258367538452148
I0217 08:41:42.066591 140047111603968 logging_writer.py:48] [34500] global_step=34500, grad_norm=4.732860565185547, loss=1.298043966293335
I0217 08:42:57.219993 140047119996672 logging_writer.py:48] [34600] global_step=34600, grad_norm=3.567103624343872, loss=1.277144193649292
I0217 08:44:14.577070 140047111603968 logging_writer.py:48] [34700] global_step=34700, grad_norm=2.3276827335357666, loss=1.2615153789520264
I0217 08:45:35.619270 140047119996672 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.8728235960006714, loss=1.353938341140747
I0217 08:46:56.449192 140047111603968 logging_writer.py:48] [34900] global_step=34900, grad_norm=3.0069429874420166, loss=1.2879741191864014
I0217 08:48:18.681456 140047119996672 logging_writer.py:48] [35000] global_step=35000, grad_norm=3.5745677947998047, loss=1.2678425312042236
I0217 08:49:38.422466 140047119996672 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.5788581371307373, loss=1.2212146520614624
I0217 08:50:53.639437 140047111603968 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.8292186260223389, loss=1.2152533531188965
I0217 08:52:08.833733 140047119996672 logging_writer.py:48] [35300] global_step=35300, grad_norm=2.328843116760254, loss=1.2699668407440186
I0217 08:53:24.062440 140047111603968 logging_writer.py:48] [35400] global_step=35400, grad_norm=3.104604959487915, loss=1.199424386024475
I0217 08:54:39.591114 140047119996672 logging_writer.py:48] [35500] global_step=35500, grad_norm=3.1602001190185547, loss=1.270283579826355
I0217 08:55:43.271126 140202902193984 spec.py:321] Evaluating on the training split.
I0217 08:56:38.424814 140202902193984 spec.py:333] Evaluating on the validation split.
I0217 08:57:30.312178 140202902193984 spec.py:349] Evaluating on the test split.
I0217 08:57:55.725953 140202902193984 submission_runner.py:408] Time since start: 30078.41s, 	Step: 35586, 	{'train/ctc_loss': Array(0.2669341, dtype=float32), 'train/wer': 0.09287643593999154, 'validation/ctc_loss': Array(0.4645654, dtype=float32), 'validation/wer': 0.132191509698099, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24837476, dtype=float32), 'test/wer': 0.07864643633335365, 'test/num_examples': 2472, 'score': 27381.948726415634, 'total_duration': 30078.406727552414, 'accumulated_submission_time': 27381.948726415634, 'accumulated_eval_time': 2693.789966583252, 'accumulated_logging_time': 1.0337035655975342}
I0217 08:57:55.762836 140047119996672 logging_writer.py:48] [35586] accumulated_eval_time=2693.789967, accumulated_logging_time=1.033704, accumulated_submission_time=27381.948726, global_step=35586, preemption_count=0, score=27381.948726, test/ctc_loss=0.24837476015090942, test/num_examples=2472, test/wer=0.078646, total_duration=30078.406728, train/ctc_loss=0.2669340968132019, train/wer=0.092876, validation/ctc_loss=0.4645653963088989, validation/num_examples=5348, validation/wer=0.132192
I0217 08:58:07.222454 140047111603968 logging_writer.py:48] [35600] global_step=35600, grad_norm=3.5365607738494873, loss=1.2132247686386108
I0217 08:59:22.561752 140047119996672 logging_writer.py:48] [35700] global_step=35700, grad_norm=3.1511428356170654, loss=1.19852614402771
I0217 09:00:37.853433 140047111603968 logging_writer.py:48] [35800] global_step=35800, grad_norm=2.896871566772461, loss=1.2062281370162964
I0217 09:01:53.187458 140047119996672 logging_writer.py:48] [35900] global_step=35900, grad_norm=5.497447967529297, loss=1.2645337581634521
I0217 09:03:08.529475 140047111603968 logging_writer.py:48] [36000] global_step=36000, grad_norm=2.432537078857422, loss=1.1917376518249512
I0217 09:04:26.987919 140047119996672 logging_writer.py:48] [36100] global_step=36100, grad_norm=2.7925286293029785, loss=1.2534440755844116
I0217 09:05:41.998359 140047111603968 logging_writer.py:48] [36200] global_step=36200, grad_norm=3.979707717895508, loss=1.2457369565963745
I0217 09:06:57.109965 140047119996672 logging_writer.py:48] [36300] global_step=36300, grad_norm=2.939389228820801, loss=1.2225077152252197
I0217 09:08:12.193961 140047111603968 logging_writer.py:48] [36400] global_step=36400, grad_norm=2.3294808864593506, loss=1.240220546722412
I0217 09:09:27.562670 140047119996672 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.7509181499481201, loss=1.238789439201355
I0217 09:10:43.031431 140047111603968 logging_writer.py:48] [36600] global_step=36600, grad_norm=2.2650845050811768, loss=1.2301459312438965
I0217 09:11:58.394959 140047119996672 logging_writer.py:48] [36700] global_step=36700, grad_norm=2.609501600265503, loss=1.2355518341064453
I0217 09:13:19.191244 140047111603968 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.8963533639907837, loss=1.177634835243225
I0217 09:14:39.689898 140047119996672 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.7075055837631226, loss=1.2231661081314087
I0217 09:15:59.961283 140047111603968 logging_writer.py:48] [37000] global_step=37000, grad_norm=4.6421027183532715, loss=1.1884613037109375
I0217 09:17:22.685791 140047119996672 logging_writer.py:48] [37100] global_step=37100, grad_norm=3.105621099472046, loss=1.1621702909469604
I0217 09:18:37.769515 140047111603968 logging_writer.py:48] [37200] global_step=37200, grad_norm=4.228558540344238, loss=1.2337836027145386
I0217 09:19:53.061489 140047119996672 logging_writer.py:48] [37300] global_step=37300, grad_norm=3.6156327724456787, loss=1.2426843643188477
I0217 09:21:08.634135 140047111603968 logging_writer.py:48] [37400] global_step=37400, grad_norm=4.600537300109863, loss=1.2300082445144653
I0217 09:21:55.832108 140202902193984 spec.py:321] Evaluating on the training split.
I0217 09:22:51.036146 140202902193984 spec.py:333] Evaluating on the validation split.
I0217 09:23:41.761761 140202902193984 spec.py:349] Evaluating on the test split.
I0217 09:24:07.627442 140202902193984 submission_runner.py:408] Time since start: 31650.31s, 	Step: 37464, 	{'train/ctc_loss': Array(0.24166115, dtype=float32), 'train/wer': 0.08285636413191605, 'validation/ctc_loss': Array(0.4427787, dtype=float32), 'validation/wer': 0.1273738378211379, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24016106, dtype=float32), 'test/wer': 0.07732618365730302, 'test/num_examples': 2472, 'score': 28821.92249059677, 'total_duration': 31650.309789180756, 'accumulated_submission_time': 28821.92249059677, 'accumulated_eval_time': 2825.580310821533, 'accumulated_logging_time': 1.0857267379760742}
I0217 09:24:07.662894 140047119996672 logging_writer.py:48] [37464] accumulated_eval_time=2825.580311, accumulated_logging_time=1.085727, accumulated_submission_time=28821.922491, global_step=37464, preemption_count=0, score=28821.922491, test/ctc_loss=0.24016106128692627, test/num_examples=2472, test/wer=0.077326, total_duration=31650.309789, train/ctc_loss=0.24166114628314972, train/wer=0.082856, validation/ctc_loss=0.44277870655059814, validation/num_examples=5348, validation/wer=0.127374
I0217 09:24:35.614337 140047111603968 logging_writer.py:48] [37500] global_step=37500, grad_norm=3.9338526725769043, loss=1.2492939233779907
I0217 09:25:50.824576 140047119996672 logging_writer.py:48] [37600] global_step=37600, grad_norm=2.0528223514556885, loss=1.2192473411560059
I0217 09:27:06.062234 140047111603968 logging_writer.py:48] [37700] global_step=37700, grad_norm=3.6242055892944336, loss=1.2800439596176147
I0217 09:28:21.152083 140047119996672 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.757517695426941, loss=1.219978928565979
I0217 09:29:35.984425 140047111603968 logging_writer.py:48] [37900] global_step=37900, grad_norm=3.594843626022339, loss=1.2065190076828003
I0217 09:30:51.117313 140047119996672 logging_writer.py:48] [38000] global_step=38000, grad_norm=2.3403780460357666, loss=1.2056772708892822
I0217 09:32:09.095301 140047111603968 logging_writer.py:48] [38100] global_step=38100, grad_norm=2.207724094390869, loss=1.2237119674682617
I0217 09:33:28.091293 140047119996672 logging_writer.py:48] [38200] global_step=38200, grad_norm=2.3617169857025146, loss=1.2357306480407715
I0217 09:34:43.095221 140047111603968 logging_writer.py:48] [38300] global_step=38300, grad_norm=2.301870822906494, loss=1.2031564712524414
I0217 09:35:58.081782 140047119996672 logging_writer.py:48] [38400] global_step=38400, grad_norm=2.9733738899230957, loss=1.2569385766983032
I0217 09:37:13.073495 140047111603968 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.8471430540084839, loss=1.2765507698059082
I0217 09:38:28.153614 140047119996672 logging_writer.py:48] [38600] global_step=38600, grad_norm=2.1188318729400635, loss=1.1961253881454468
I0217 09:39:43.502832 140047111603968 logging_writer.py:48] [38700] global_step=38700, grad_norm=2.9164750576019287, loss=1.2278414964675903
I0217 09:41:01.285863 140047119996672 logging_writer.py:48] [38800] global_step=38800, grad_norm=2.7151384353637695, loss=1.2039927244186401
I0217 09:42:21.277078 140047111603968 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.8613543510437012, loss=1.2076988220214844
I0217 09:43:41.758237 140047119996672 logging_writer.py:48] [39000] global_step=39000, grad_norm=5.548300743103027, loss=1.1497489213943481
I0217 09:45:03.297637 140047111603968 logging_writer.py:48] [39100] global_step=39100, grad_norm=2.157763957977295, loss=1.2014647722244263
I0217 09:46:23.496357 140047119996672 logging_writer.py:48] [39200] global_step=39200, grad_norm=2.8245394229888916, loss=1.2312052249908447
I0217 09:47:38.637465 140047111603968 logging_writer.py:48] [39300] global_step=39300, grad_norm=4.646050930023193, loss=1.1679102182388306
I0217 09:48:07.827249 140202902193984 spec.py:321] Evaluating on the training split.
I0217 09:49:01.541394 140202902193984 spec.py:333] Evaluating on the validation split.
I0217 09:49:52.707981 140202902193984 spec.py:349] Evaluating on the test split.
I0217 09:50:18.459635 140202902193984 submission_runner.py:408] Time since start: 33221.14s, 	Step: 39340, 	{'train/ctc_loss': Array(0.24197032, dtype=float32), 'train/wer': 0.08477693875923964, 'validation/ctc_loss': Array(0.43660167, dtype=float32), 'validation/wer': 0.12530774206628884, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23111513, dtype=float32), 'test/wer': 0.0736700993236244, 'test/num_examples': 2472, 'score': 30261.989458084106, 'total_duration': 33221.140043735504, 'accumulated_submission_time': 30261.989458084106, 'accumulated_eval_time': 2956.2057723999023, 'accumulated_logging_time': 1.1383299827575684}
I0217 09:50:18.497108 140047119996672 logging_writer.py:48] [39340] accumulated_eval_time=2956.205772, accumulated_logging_time=1.138330, accumulated_submission_time=30261.989458, global_step=39340, preemption_count=0, score=30261.989458, test/ctc_loss=0.23111513257026672, test/num_examples=2472, test/wer=0.073670, total_duration=33221.140044, train/ctc_loss=0.24197031557559967, train/wer=0.084777, validation/ctc_loss=0.4366016685962677, validation/num_examples=5348, validation/wer=0.125308
I0217 09:51:04.459850 140047111603968 logging_writer.py:48] [39400] global_step=39400, grad_norm=2.6149508953094482, loss=1.1855772733688354
I0217 09:52:19.646205 140047119996672 logging_writer.py:48] [39500] global_step=39500, grad_norm=3.7426445484161377, loss=1.2085338830947876
I0217 09:53:34.749307 140047111603968 logging_writer.py:48] [39600] global_step=39600, grad_norm=2.965827703475952, loss=1.1874459981918335
I0217 09:54:49.860637 140047119996672 logging_writer.py:48] [39700] global_step=39700, grad_norm=2.5341715812683105, loss=1.1871477365493774
I0217 09:56:04.866410 140047111603968 logging_writer.py:48] [39800] global_step=39800, grad_norm=2.155486822128296, loss=1.1418237686157227
I0217 09:57:20.072528 140047119996672 logging_writer.py:48] [39900] global_step=39900, grad_norm=2.08164644241333, loss=1.2121562957763672
I0217 09:58:35.114579 140047111603968 logging_writer.py:48] [40000] global_step=40000, grad_norm=2.109600067138672, loss=1.1677117347717285
I0217 09:59:56.656471 140047119996672 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.995152473449707, loss=1.16025710105896
I0217 10:01:18.439806 140047119996672 logging_writer.py:48] [40200] global_step=40200, grad_norm=9.412529945373535, loss=1.2065342664718628
I0217 10:02:33.710547 140047111603968 logging_writer.py:48] [40300] global_step=40300, grad_norm=3.1696383953094482, loss=1.1580935716629028
I0217 10:03:48.929901 140047119996672 logging_writer.py:48] [40400] global_step=40400, grad_norm=4.4064226150512695, loss=1.2152280807495117
I0217 10:05:04.054143 140047111603968 logging_writer.py:48] [40500] global_step=40500, grad_norm=3.614988327026367, loss=1.1967804431915283
I0217 10:06:19.379630 140047119996672 logging_writer.py:48] [40600] global_step=40600, grad_norm=3.467028856277466, loss=1.149741530418396
I0217 10:07:34.430764 140047111603968 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.7316993474960327, loss=1.190593957901001
I0217 10:08:49.593662 140047119996672 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.7978218793869019, loss=1.121886968612671
I0217 10:10:09.232249 140047111603968 logging_writer.py:48] [40900] global_step=40900, grad_norm=2.1968111991882324, loss=1.1486973762512207
I0217 10:11:30.073042 140047119996672 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.594599962234497, loss=1.259432077407837
I0217 10:12:52.405492 140047111603968 logging_writer.py:48] [41100] global_step=41100, grad_norm=2.0394089221954346, loss=1.120991587638855
I0217 10:14:16.933969 140047119996672 logging_writer.py:48] [41200] global_step=41200, grad_norm=3.0215351581573486, loss=1.1680244207382202
I0217 10:14:19.095430 140202902193984 spec.py:321] Evaluating on the training split.
I0217 10:15:14.279250 140202902193984 spec.py:333] Evaluating on the validation split.
I0217 10:16:05.849951 140202902193984 spec.py:349] Evaluating on the test split.
I0217 10:16:32.094639 140202902193984 submission_runner.py:408] Time since start: 34794.78s, 	Step: 41204, 	{'train/ctc_loss': Array(0.26820278, dtype=float32), 'train/wer': 0.08890057275818865, 'validation/ctc_loss': Array(0.42843142, dtype=float32), 'validation/wer': 0.12244031010745629, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22799182, dtype=float32), 'test/wer': 0.07295919403652022, 'test/num_examples': 2472, 'score': 31702.491810321808, 'total_duration': 34794.77567911148, 'accumulated_submission_time': 31702.491810321808, 'accumulated_eval_time': 3089.1986536979675, 'accumulated_logging_time': 1.1911566257476807}
I0217 10:16:32.132867 140047119996672 logging_writer.py:48] [41204] accumulated_eval_time=3089.198654, accumulated_logging_time=1.191157, accumulated_submission_time=31702.491810, global_step=41204, preemption_count=0, score=31702.491810, test/ctc_loss=0.22799181938171387, test/num_examples=2472, test/wer=0.072959, total_duration=34794.775679, train/ctc_loss=0.2682027816772461, train/wer=0.088901, validation/ctc_loss=0.4284314215183258, validation/num_examples=5348, validation/wer=0.122440
I0217 10:17:45.009587 140047111603968 logging_writer.py:48] [41300] global_step=41300, grad_norm=2.351153612136841, loss=1.1858787536621094
I0217 10:19:00.314059 140047119996672 logging_writer.py:48] [41400] global_step=41400, grad_norm=3.4466445446014404, loss=1.224351406097412
I0217 10:20:15.729400 140047111603968 logging_writer.py:48] [41500] global_step=41500, grad_norm=2.4557275772094727, loss=1.1821208000183105
I0217 10:21:31.098635 140047119996672 logging_writer.py:48] [41600] global_step=41600, grad_norm=4.862011909484863, loss=1.2136833667755127
I0217 10:22:46.429147 140047111603968 logging_writer.py:48] [41700] global_step=41700, grad_norm=2.620727777481079, loss=1.1490201950073242
I0217 10:24:02.053699 140047119996672 logging_writer.py:48] [41800] global_step=41800, grad_norm=4.47265100479126, loss=1.1857435703277588
I0217 10:25:17.147284 140047111603968 logging_writer.py:48] [41900] global_step=41900, grad_norm=3.930454969406128, loss=1.204498052597046
I0217 10:26:32.375846 140047119996672 logging_writer.py:48] [42000] global_step=42000, grad_norm=2.7879750728607178, loss=1.1480271816253662
I0217 10:27:53.595916 140047111603968 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.514700174331665, loss=1.194168210029602
I0217 10:29:14.339504 140047119996672 logging_writer.py:48] [42200] global_step=42200, grad_norm=3.863271474838257, loss=1.146962285041809
I0217 10:30:34.054899 140047119996672 logging_writer.py:48] [42300] global_step=42300, grad_norm=3.2700231075286865, loss=1.176253318786621
I0217 10:31:49.030904 140047111603968 logging_writer.py:48] [42400] global_step=42400, grad_norm=3.301130771636963, loss=1.2197812795639038
I0217 10:33:04.014527 140047119996672 logging_writer.py:48] [42500] global_step=42500, grad_norm=3.699892520904541, loss=1.1612797975540161
I0217 10:34:19.071898 140047111603968 logging_writer.py:48] [42600] global_step=42600, grad_norm=2.492934226989746, loss=1.1976449489593506
I0217 10:35:34.287201 140047119996672 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.7885708808898926, loss=1.163954257965088
I0217 10:36:49.256458 140047111603968 logging_writer.py:48] [42800] global_step=42800, grad_norm=3.974480152130127, loss=1.1050975322723389
I0217 10:38:07.206645 140047119996672 logging_writer.py:48] [42900] global_step=42900, grad_norm=4.770790100097656, loss=1.1943436861038208
I0217 10:39:27.697043 140047111603968 logging_writer.py:48] [43000] global_step=43000, grad_norm=2.804424285888672, loss=1.2117053270339966
I0217 10:40:32.691515 140202902193984 spec.py:321] Evaluating on the training split.
I0217 10:41:28.014647 140202902193984 spec.py:333] Evaluating on the validation split.
I0217 10:42:19.196249 140202902193984 spec.py:349] Evaluating on the test split.
I0217 10:42:45.077435 140202902193984 submission_runner.py:408] Time since start: 36367.76s, 	Step: 43081, 	{'train/ctc_loss': Array(0.21494322, dtype=float32), 'train/wer': 0.07583155409170404, 'validation/ctc_loss': Array(0.42295602, dtype=float32), 'validation/wer': 0.12120451451577087, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22370486, dtype=float32), 'test/wer': 0.07145613714378567, 'test/num_examples': 2472, 'score': 33142.95555186272, 'total_duration': 36367.7583668232, 'accumulated_submission_time': 33142.95555186272, 'accumulated_eval_time': 3221.5781757831573, 'accumulated_logging_time': 1.2448463439941406}
I0217 10:42:45.115702 140047119996672 logging_writer.py:48] [43081] accumulated_eval_time=3221.578176, accumulated_logging_time=1.244846, accumulated_submission_time=33142.955552, global_step=43081, preemption_count=0, score=33142.955552, test/ctc_loss=0.22370485961437225, test/num_examples=2472, test/wer=0.071456, total_duration=36367.758367, train/ctc_loss=0.21494321525096893, train/wer=0.075832, validation/ctc_loss=0.42295601963996887, validation/num_examples=5348, validation/wer=0.121205
I0217 10:43:00.208686 140047111603968 logging_writer.py:48] [43100] global_step=43100, grad_norm=5.439459323883057, loss=1.2036141157150269
I0217 10:44:15.401720 140047119996672 logging_writer.py:48] [43200] global_step=43200, grad_norm=3.698842763900757, loss=1.1691484451293945
I0217 10:45:33.992871 140047119996672 logging_writer.py:48] [43300] global_step=43300, grad_norm=3.8000903129577637, loss=1.1927855014801025
I0217 10:46:49.171504 140047111603968 logging_writer.py:48] [43400] global_step=43400, grad_norm=3.3577444553375244, loss=1.1248841285705566
I0217 10:48:04.574840 140047119996672 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.5901504755020142, loss=1.2345757484436035
I0217 10:49:19.667370 140047111603968 logging_writer.py:48] [43600] global_step=43600, grad_norm=3.5234243869781494, loss=1.1958667039871216
I0217 10:50:34.802262 140047119996672 logging_writer.py:48] [43700] global_step=43700, grad_norm=2.01385760307312, loss=1.1381758451461792
I0217 10:51:49.938416 140047111603968 logging_writer.py:48] [43800] global_step=43800, grad_norm=2.595931053161621, loss=1.1858562231063843
I0217 10:53:06.363831 140047119996672 logging_writer.py:48] [43900] global_step=43900, grad_norm=3.149879217147827, loss=1.168089509010315
I0217 10:54:26.405433 140047111603968 logging_writer.py:48] [44000] global_step=44000, grad_norm=3.459627866744995, loss=1.2016113996505737
I0217 10:55:47.214160 140047119996672 logging_writer.py:48] [44100] global_step=44100, grad_norm=5.079348564147949, loss=1.1573874950408936
I0217 10:57:07.988170 140047111603968 logging_writer.py:48] [44200] global_step=44200, grad_norm=3.101583242416382, loss=1.2150362730026245
I0217 10:58:32.147082 140047119996672 logging_writer.py:48] [44300] global_step=44300, grad_norm=4.9458208084106445, loss=1.1389775276184082
I0217 10:59:47.212405 140047111603968 logging_writer.py:48] [44400] global_step=44400, grad_norm=2.34682559967041, loss=1.1413328647613525
I0217 11:01:02.429843 140047119996672 logging_writer.py:48] [44500] global_step=44500, grad_norm=3.4981889724731445, loss=1.1820476055145264
I0217 11:02:17.642549 140047111603968 logging_writer.py:48] [44600] global_step=44600, grad_norm=2.4938559532165527, loss=1.1699073314666748
I0217 11:03:32.666354 140047119996672 logging_writer.py:48] [44700] global_step=44700, grad_norm=3.654492139816284, loss=1.1558297872543335
I0217 11:04:47.740004 140047111603968 logging_writer.py:48] [44800] global_step=44800, grad_norm=2.278122663497925, loss=1.15839421749115
I0217 11:06:03.053035 140047119996672 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.8176196813583374, loss=1.1762864589691162
I0217 11:06:45.375861 140202902193984 spec.py:321] Evaluating on the training split.
I0217 11:07:39.449519 140202902193984 spec.py:333] Evaluating on the validation split.
I0217 11:08:30.156558 140202902193984 spec.py:349] Evaluating on the test split.
I0217 11:08:56.353760 140202902193984 submission_runner.py:408] Time since start: 37939.03s, 	Step: 44955, 	{'train/ctc_loss': Array(0.22929652, dtype=float32), 'train/wer': 0.08149222557437921, 'validation/ctc_loss': Array(0.4198003, dtype=float32), 'validation/wer': 0.11973700725064444, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2223216, dtype=float32), 'test/wer': 0.07100928239189162, 'test/num_examples': 2472, 'score': 34583.11553454399, 'total_duration': 37939.03490805626, 'accumulated_submission_time': 34583.11553454399, 'accumulated_eval_time': 3352.5498700141907, 'accumulated_logging_time': 1.303229808807373}
I0217 11:08:56.393433 140047119996672 logging_writer.py:48] [44955] accumulated_eval_time=3352.549870, accumulated_logging_time=1.303230, accumulated_submission_time=34583.115535, global_step=44955, preemption_count=0, score=34583.115535, test/ctc_loss=0.22232159972190857, test/num_examples=2472, test/wer=0.071009, total_duration=37939.034908, train/ctc_loss=0.2292965203523636, train/wer=0.081492, validation/ctc_loss=0.4198003113269806, validation/num_examples=5348, validation/wer=0.119737
I0217 11:09:31.098242 140047111603968 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.694023847579956, loss=1.1181467771530151
I0217 11:10:46.335999 140047119996672 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.7140707969665527, loss=1.1138392686843872
I0217 11:12:01.398405 140047111603968 logging_writer.py:48] [45200] global_step=45200, grad_norm=3.037785291671753, loss=1.1889421939849854
I0217 11:13:16.730570 140047119996672 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.6582145690917969, loss=1.1589633226394653
I0217 11:14:35.129256 140047119996672 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.6925476789474487, loss=1.1687500476837158
I0217 11:15:50.196219 140047111603968 logging_writer.py:48] [45500] global_step=45500, grad_norm=3.0608408451080322, loss=1.1099815368652344
I0217 11:17:05.161945 140047119996672 logging_writer.py:48] [45600] global_step=45600, grad_norm=4.542060852050781, loss=1.164595603942871
I0217 11:18:20.225065 140047111603968 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.8866535425186157, loss=1.1636348962783813
I0217 11:19:35.087821 140047119996672 logging_writer.py:48] [45800] global_step=45800, grad_norm=3.8185956478118896, loss=1.1775908470153809
I0217 11:20:50.100934 140047111603968 logging_writer.py:48] [45900] global_step=45900, grad_norm=4.119185447692871, loss=1.1482744216918945
I0217 11:22:10.855809 140047119996672 logging_writer.py:48] [46000] global_step=46000, grad_norm=2.421245574951172, loss=1.1419285535812378
I0217 11:23:31.508571 140047111603968 logging_writer.py:48] [46100] global_step=46100, grad_norm=2.9620676040649414, loss=1.168489933013916
I0217 11:24:52.179461 140047119996672 logging_writer.py:48] [46200] global_step=46200, grad_norm=2.1689090728759766, loss=1.19535231590271
I0217 11:26:12.705135 140047111603968 logging_writer.py:48] [46300] global_step=46300, grad_norm=3.272571086883545, loss=1.1530241966247559
I0217 11:27:34.271800 140047119996672 logging_writer.py:48] [46400] global_step=46400, grad_norm=2.583953380584717, loss=1.1756598949432373
I0217 11:28:49.322037 140047111603968 logging_writer.py:48] [46500] global_step=46500, grad_norm=2.070868492126465, loss=1.1761088371276855
I0217 11:30:04.606234 140047119996672 logging_writer.py:48] [46600] global_step=46600, grad_norm=2.1742279529571533, loss=1.0871518850326538
I0217 11:31:19.737186 140047111603968 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.978855013847351, loss=1.1462708711624146
I0217 11:32:35.044888 140047119996672 logging_writer.py:48] [46800] global_step=46800, grad_norm=4.784493446350098, loss=1.1358731985092163
I0217 11:32:56.793737 140202902193984 spec.py:321] Evaluating on the training split.
I0217 11:33:51.786907 140202902193984 spec.py:333] Evaluating on the validation split.
I0217 11:34:43.418568 140202902193984 spec.py:349] Evaluating on the test split.
I0217 11:35:09.550290 140202902193984 submission_runner.py:408] Time since start: 39512.23s, 	Step: 46830, 	{'train/ctc_loss': Array(0.19979791, dtype=float32), 'train/wer': 0.069397354668894, 'validation/ctc_loss': Array(0.41824093, dtype=float32), 'validation/wer': 0.11956322349556368, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22184521, dtype=float32), 'test/wer': 0.07058273921962911, 'test/num_examples': 2472, 'score': 36023.417892456055, 'total_duration': 39512.232201099396, 'accumulated_submission_time': 36023.417892456055, 'accumulated_eval_time': 3485.3009746074677, 'accumulated_logging_time': 1.3599789142608643}
I0217 11:35:09.591024 140047119996672 logging_writer.py:48] [46830] accumulated_eval_time=3485.300975, accumulated_logging_time=1.359979, accumulated_submission_time=36023.417892, global_step=46830, preemption_count=0, score=36023.417892, test/ctc_loss=0.22184520959854126, test/num_examples=2472, test/wer=0.070583, total_duration=39512.232201, train/ctc_loss=0.19979791343212128, train/wer=0.069397, validation/ctc_loss=0.4182409346103668, validation/num_examples=5348, validation/wer=0.119563
I0217 11:36:03.277131 140047111603968 logging_writer.py:48] [46900] global_step=46900, grad_norm=4.9975738525390625, loss=1.138205647468567
I0217 11:37:18.328774 140047119996672 logging_writer.py:48] [47000] global_step=47000, grad_norm=4.90495491027832, loss=1.1784374713897705
I0217 11:38:33.808442 140047111603968 logging_writer.py:48] [47100] global_step=47100, grad_norm=2.674206256866455, loss=1.1804307699203491
I0217 11:39:49.217968 140047119996672 logging_writer.py:48] [47200] global_step=47200, grad_norm=2.9153618812561035, loss=1.1574792861938477
I0217 11:41:04.383826 140047111603968 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.969887375831604, loss=1.1820485591888428
I0217 11:42:23.235411 140047119996672 logging_writer.py:48] [47400] global_step=47400, grad_norm=2.5120596885681152, loss=1.1352205276489258
I0217 11:43:38.370757 140047111603968 logging_writer.py:48] [47500] global_step=47500, grad_norm=2.0347886085510254, loss=1.1653215885162354
I0217 11:44:53.555672 140047119996672 logging_writer.py:48] [47600] global_step=47600, grad_norm=5.942016124725342, loss=1.1041711568832397
I0217 11:46:08.613325 140047111603968 logging_writer.py:48] [47700] global_step=47700, grad_norm=2.0510153770446777, loss=1.156532645225525
I0217 11:47:23.569461 140047119996672 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.9419646263122559, loss=1.0992356538772583
I0217 11:48:38.892631 140047111603968 logging_writer.py:48] [47900] global_step=47900, grad_norm=2.2721729278564453, loss=1.1599390506744385
I0217 11:49:52.882428 140202902193984 spec.py:321] Evaluating on the training split.
I0217 11:50:46.755841 140202902193984 spec.py:333] Evaluating on the validation split.
I0217 11:51:37.547263 140202902193984 spec.py:349] Evaluating on the test split.
I0217 11:52:03.430166 140202902193984 submission_runner.py:408] Time since start: 40526.11s, 	Step: 48000, 	{'train/ctc_loss': Array(0.224268, dtype=float32), 'train/wer': 0.07741169150711276, 'validation/ctc_loss': Array(0.4184095, dtype=float32), 'validation/wer': 0.1196790793322842, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22146653, dtype=float32), 'test/wer': 0.07098897081226006, 'test/num_examples': 2472, 'score': 36906.64269685745, 'total_duration': 40526.11241531372, 'accumulated_submission_time': 36906.64269685745, 'accumulated_eval_time': 3615.8436131477356, 'accumulated_logging_time': 1.4163413047790527}
I0217 11:52:03.470863 140047119996672 logging_writer.py:48] [48000] accumulated_eval_time=3615.843613, accumulated_logging_time=1.416341, accumulated_submission_time=36906.642697, global_step=48000, preemption_count=0, score=36906.642697, test/ctc_loss=0.221466526389122, test/num_examples=2472, test/wer=0.070989, total_duration=40526.112415, train/ctc_loss=0.22426800429821014, train/wer=0.077412, validation/ctc_loss=0.4184094965457916, validation/num_examples=5348, validation/wer=0.119679
I0217 11:52:03.499166 140047111603968 logging_writer.py:48] [48000] global_step=48000, preemption_count=0, score=36906.642697
I0217 11:52:03.670410 140202902193984 checkpoints.py:490] Saving checkpoint at step: 48000
I0217 11:52:04.609929 140202902193984 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_3/librispeech_deepspeech_jax/trial_5/checkpoint_48000
I0217 11:52:04.628812 140202902193984 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_3/librispeech_deepspeech_jax/trial_5/checkpoint_48000.
I0217 11:52:05.864936 140202902193984 submission_runner.py:583] Tuning trial 5/5
I0217 11:52:05.865176 140202902193984 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0217 11:52:05.877690 140202902193984 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(30.62459, dtype=float32), 'train/wer': 3.3513766830074823, 'validation/ctc_loss': Array(30.351221, dtype=float32), 'validation/wer': 3.0459271846066214, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.536512, dtype=float32), 'test/wer': 3.361485182702659, 'test/num_examples': 2472, 'score': 16.372990608215332, 'total_duration': 202.37854552268982, 'accumulated_submission_time': 16.372990608215332, 'accumulated_eval_time': 186.00546002388, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1857, {'train/ctc_loss': Array(2.3449352, dtype=float32), 'train/wer': 0.5320623826954385, 'validation/ctc_loss': Array(2.671262, dtype=float32), 'validation/wer': 0.5683211523793893, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.1457593, dtype=float32), 'test/wer': 0.49158085024272336, 'test/num_examples': 2472, 'score': 1456.8387684822083, 'total_duration': 1772.1749081611633, 'accumulated_submission_time': 1456.8387684822083, 'accumulated_eval_time': 315.22576928138733, 'accumulated_logging_time': 0.028501510620117188, 'global_step': 1857, 'preemption_count': 0}), (3727, {'train/ctc_loss': Array(0.5966041, dtype=float32), 'train/wer': 0.1932851115758293, 'validation/ctc_loss': Array(0.9358604, dtype=float32), 'validation/wer': 0.26263552719233035, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6082362, dtype=float32), 'test/wer': 0.19340686125159953, 'test/num_examples': 2472, 'score': 2897.1221652030945, 'total_duration': 3347.6318669319153, 'accumulated_submission_time': 2897.1221652030945, 'accumulated_eval_time': 450.2640874385834, 'accumulated_logging_time': 0.07642841339111328, 'global_step': 3727, 'preemption_count': 0}), (5606, {'train/ctc_loss': Array(0.4300289, dtype=float32), 'train/wer': 0.1458097903575012, 'validation/ctc_loss': Array(0.8080621, dtype=float32), 'validation/wer': 0.2312289407880128, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5005587, dtype=float32), 'test/wer': 0.16019742855401864, 'test/num_examples': 2472, 'score': 4337.322243690491, 'total_duration': 4920.453497171402, 'accumulated_submission_time': 4337.322243690491, 'accumulated_eval_time': 582.7502326965332, 'accumulated_logging_time': 0.12636256217956543, 'global_step': 5606, 'preemption_count': 0}), (7486, {'train/ctc_loss': Array(0.3892914, dtype=float32), 'train/wer': 0.1317028818944293, 'validation/ctc_loss': Array(0.7440744, dtype=float32), 'validation/wer': 0.21083831352520346, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.44369552, dtype=float32), 'test/wer': 0.14362317957467552, 'test/num_examples': 2472, 'score': 5777.62396812439, 'total_duration': 6493.077576160431, 'accumulated_submission_time': 5777.62396812439, 'accumulated_eval_time': 714.9384062290192, 'accumulated_logging_time': 0.17426013946533203, 'global_step': 7486, 'preemption_count': 0}), (9352, {'train/ctc_loss': Array(0.39885035, dtype=float32), 'train/wer': 0.13448043002620136, 'validation/ctc_loss': Array(0.7127357, dtype=float32), 'validation/wer': 0.20256427585274722, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43267328, dtype=float32), 'test/wer': 0.1383624804501046, 'test/num_examples': 2472, 'score': 7217.692116260529, 'total_duration': 8065.623285531998, 'accumulated_submission_time': 7217.692116260529, 'accumulated_eval_time': 847.2784371376038, 'accumulated_logging_time': 0.22581052780151367, 'global_step': 9352, 'preemption_count': 0}), (11220, {'train/ctc_loss': Array(0.33331928, dtype=float32), 'train/wer': 0.11130403835057995, 'validation/ctc_loss': Array(0.67308295, dtype=float32), 'validation/wer': 0.19158693532347915, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39661396, dtype=float32), 'test/wer': 0.12672394532122763, 'test/num_examples': 2472, 'score': 8658.064115285873, 'total_duration': 9637.65848660469, 'accumulated_submission_time': 8658.064115285873, 'accumulated_eval_time': 978.8065919876099, 'accumulated_logging_time': 0.2755625247955322, 'global_step': 11220, 'preemption_count': 0}), (13089, {'train/ctc_loss': Array(0.3510411, dtype=float32), 'train/wer': 0.11835662247700944, 'validation/ctc_loss': Array(0.6683462, dtype=float32), 'validation/wer': 0.1900421908338724, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3859156, dtype=float32), 'test/wer': 0.12377876627465317, 'test/num_examples': 2472, 'score': 10098.212848901749, 'total_duration': 11212.078014612198, 'accumulated_submission_time': 10098.212848901749, 'accumulated_eval_time': 1112.9421019554138, 'accumulated_logging_time': 0.32501649856567383, 'global_step': 13089, 'preemption_count': 0}), (14959, {'train/ctc_loss': Array(0.3626199, dtype=float32), 'train/wer': 0.1157342155230644, 'validation/ctc_loss': Array(0.6363704, dtype=float32), 'validation/wer': 0.18241501491643897, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36685, dtype=float32), 'test/wer': 0.11784778502224118, 'test/num_examples': 2472, 'score': 11538.172552585602, 'total_duration': 12783.459538698196, 'accumulated_submission_time': 11538.172552585602, 'accumulated_eval_time': 1244.2248899936676, 'accumulated_logging_time': 0.37923097610473633, 'global_step': 14959, 'preemption_count': 0}), (16835, {'train/ctc_loss': Array(0.2640011, dtype=float32), 'train/wer': 0.09059689540470617, 'validation/ctc_loss': Array(0.5966437, dtype=float32), 'validation/wer': 0.1708680498566284, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3457338, dtype=float32), 'test/wer': 0.11155119533646132, 'test/num_examples': 2472, 'score': 12978.370990991592, 'total_duration': 14357.825068950653, 'accumulated_submission_time': 12978.370990991592, 'accumulated_eval_time': 1378.1725063323975, 'accumulated_logging_time': 0.5102035999298096, 'global_step': 16835, 'preemption_count': 0}), (18711, {'train/ctc_loss': Array(0.26224798, dtype=float32), 'train/wer': 0.08935853688985061, 'validation/ctc_loss': Array(0.5874665, dtype=float32), 'validation/wer': 0.16828060283653706, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33608237, dtype=float32), 'test/wer': 0.10872788576767616, 'test/num_examples': 2472, 'score': 14418.6655626297, 'total_duration': 15930.184390544891, 'accumulated_submission_time': 14418.6655626297, 'accumulated_eval_time': 1510.1033344268799, 'accumulated_logging_time': 0.5596368312835693, 'global_step': 18711, 'preemption_count': 0}), (20585, {'train/ctc_loss': Array(0.38126385, dtype=float32), 'train/wer': 0.12747945476748407, 'validation/ctc_loss': Array(0.5813514, dtype=float32), 'validation/wer': 0.16634001757146857, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33556062, dtype=float32), 'test/wer': 0.10919505209920176, 'test/num_examples': 2472, 'score': 15859.262013435364, 'total_duration': 17502.176399946213, 'accumulated_submission_time': 15859.262013435364, 'accumulated_eval_time': 1641.366404056549, 'accumulated_logging_time': 0.6074838638305664, 'global_step': 20585, 'preemption_count': 0}), (22448, {'train/ctc_loss': Array(0.3882775, dtype=float32), 'train/wer': 0.1288513710578304, 'validation/ctc_loss': Array(0.57250744, dtype=float32), 'validation/wer': 0.16418702993907913, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32734656, dtype=float32), 'test/wer': 0.1047671277395243, 'test/num_examples': 2472, 'score': 17299.8719329834, 'total_duration': 19075.193665504456, 'accumulated_submission_time': 17299.8719329834, 'accumulated_eval_time': 1773.641833782196, 'accumulated_logging_time': 0.6536881923675537, 'global_step': 22448, 'preemption_count': 0}), (24324, {'train/ctc_loss': Array(0.44150472, dtype=float32), 'train/wer': 0.14325196832106277, 'validation/ctc_loss': Array(0.5418794, dtype=float32), 'validation/wer': 0.15532405842996033, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30448222, dtype=float32), 'test/wer': 0.09710966221843073, 'test/num_examples': 2472, 'score': 18740.242106437683, 'total_duration': 20647.843740701675, 'accumulated_submission_time': 18740.242106437683, 'accumulated_eval_time': 1905.782042503357, 'accumulated_logging_time': 0.7064037322998047, 'global_step': 24324, 'preemption_count': 0}), (26201, {'train/ctc_loss': Array(0.36288148, dtype=float32), 'train/wer': 0.11984659635666348, 'validation/ctc_loss': Array(0.5348055, dtype=float32), 'validation/wer': 0.15417515471581528, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30177844, dtype=float32), 'test/wer': 0.09664249588690513, 'test/num_examples': 2472, 'score': 20180.723200559616, 'total_duration': 22218.75596666336, 'accumulated_submission_time': 20180.723200559616, 'accumulated_eval_time': 2036.0781605243683, 'accumulated_logging_time': 0.7543544769287109, 'global_step': 26201, 'preemption_count': 0}), (28079, {'train/ctc_loss': Array(0.37025255, dtype=float32), 'train/wer': 0.12474835363565019, 'validation/ctc_loss': Array(0.52608824, dtype=float32), 'validation/wer': 0.15051604120605927, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29157224, dtype=float32), 'test/wer': 0.09440822212743485, 'test/num_examples': 2472, 'score': 21621.010961294174, 'total_duration': 23789.058510541916, 'accumulated_submission_time': 21621.010961294174, 'accumulated_eval_time': 2165.942393541336, 'accumulated_logging_time': 0.8169291019439697, 'global_step': 28079, 'preemption_count': 0}), (29945, {'train/ctc_loss': Array(0.27366316, dtype=float32), 'train/wer': 0.09416574763966969, 'validation/ctc_loss': Array(0.50481427, dtype=float32), 'validation/wer': 0.14388329455381021, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.279768, dtype=float32), 'test/wer': 0.0905693335770723, 'test/num_examples': 2472, 'score': 23061.27196407318, 'total_duration': 25360.624280691147, 'accumulated_submission_time': 23061.27196407318, 'accumulated_eval_time': 2297.1110696792603, 'accumulated_logging_time': 0.867875337600708, 'global_step': 29945, 'preemption_count': 0}), (31827, {'train/ctc_loss': Array(0.31801972, dtype=float32), 'train/wer': 0.10824967979945047, 'validation/ctc_loss': Array(0.48673412, dtype=float32), 'validation/wer': 0.13978006700329224, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26446828, dtype=float32), 'test/wer': 0.08579611236365853, 'test/num_examples': 2472, 'score': 24501.19046640396, 'total_duration': 26932.845405817032, 'accumulated_submission_time': 24501.19046640396, 'accumulated_eval_time': 2429.268038749695, 'accumulated_logging_time': 0.9267137050628662, 'global_step': 31827, 'preemption_count': 0}), (33708, {'train/ctc_loss': Array(0.2762536, dtype=float32), 'train/wer': 0.0937875751503006, 'validation/ctc_loss': Array(0.47617844, dtype=float32), 'validation/wer': 0.13711538275872057, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25827703, dtype=float32), 'test/wer': 0.08281031015782098, 'test/num_examples': 2472, 'score': 25941.62428545952, 'total_duration': 28505.49468064308, 'accumulated_submission_time': 25941.62428545952, 'accumulated_eval_time': 2561.341689825058, 'accumulated_logging_time': 0.9807431697845459, 'global_step': 33708, 'preemption_count': 0}), (35586, {'train/ctc_loss': Array(0.2669341, dtype=float32), 'train/wer': 0.09287643593999154, 'validation/ctc_loss': Array(0.4645654, dtype=float32), 'validation/wer': 0.132191509698099, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24837476, dtype=float32), 'test/wer': 0.07864643633335365, 'test/num_examples': 2472, 'score': 27381.948726415634, 'total_duration': 30078.406727552414, 'accumulated_submission_time': 27381.948726415634, 'accumulated_eval_time': 2693.789966583252, 'accumulated_logging_time': 1.0337035655975342, 'global_step': 35586, 'preemption_count': 0}), (37464, {'train/ctc_loss': Array(0.24166115, dtype=float32), 'train/wer': 0.08285636413191605, 'validation/ctc_loss': Array(0.4427787, dtype=float32), 'validation/wer': 0.1273738378211379, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24016106, dtype=float32), 'test/wer': 0.07732618365730302, 'test/num_examples': 2472, 'score': 28821.92249059677, 'total_duration': 31650.309789180756, 'accumulated_submission_time': 28821.92249059677, 'accumulated_eval_time': 2825.580310821533, 'accumulated_logging_time': 1.0857267379760742, 'global_step': 37464, 'preemption_count': 0}), (39340, {'train/ctc_loss': Array(0.24197032, dtype=float32), 'train/wer': 0.08477693875923964, 'validation/ctc_loss': Array(0.43660167, dtype=float32), 'validation/wer': 0.12530774206628884, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23111513, dtype=float32), 'test/wer': 0.0736700993236244, 'test/num_examples': 2472, 'score': 30261.989458084106, 'total_duration': 33221.140043735504, 'accumulated_submission_time': 30261.989458084106, 'accumulated_eval_time': 2956.2057723999023, 'accumulated_logging_time': 1.1383299827575684, 'global_step': 39340, 'preemption_count': 0}), (41204, {'train/ctc_loss': Array(0.26820278, dtype=float32), 'train/wer': 0.08890057275818865, 'validation/ctc_loss': Array(0.42843142, dtype=float32), 'validation/wer': 0.12244031010745629, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22799182, dtype=float32), 'test/wer': 0.07295919403652022, 'test/num_examples': 2472, 'score': 31702.491810321808, 'total_duration': 34794.77567911148, 'accumulated_submission_time': 31702.491810321808, 'accumulated_eval_time': 3089.1986536979675, 'accumulated_logging_time': 1.1911566257476807, 'global_step': 41204, 'preemption_count': 0}), (43081, {'train/ctc_loss': Array(0.21494322, dtype=float32), 'train/wer': 0.07583155409170404, 'validation/ctc_loss': Array(0.42295602, dtype=float32), 'validation/wer': 0.12120451451577087, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22370486, dtype=float32), 'test/wer': 0.07145613714378567, 'test/num_examples': 2472, 'score': 33142.95555186272, 'total_duration': 36367.7583668232, 'accumulated_submission_time': 33142.95555186272, 'accumulated_eval_time': 3221.5781757831573, 'accumulated_logging_time': 1.2448463439941406, 'global_step': 43081, 'preemption_count': 0}), (44955, {'train/ctc_loss': Array(0.22929652, dtype=float32), 'train/wer': 0.08149222557437921, 'validation/ctc_loss': Array(0.4198003, dtype=float32), 'validation/wer': 0.11973700725064444, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2223216, dtype=float32), 'test/wer': 0.07100928239189162, 'test/num_examples': 2472, 'score': 34583.11553454399, 'total_duration': 37939.03490805626, 'accumulated_submission_time': 34583.11553454399, 'accumulated_eval_time': 3352.5498700141907, 'accumulated_logging_time': 1.303229808807373, 'global_step': 44955, 'preemption_count': 0}), (46830, {'train/ctc_loss': Array(0.19979791, dtype=float32), 'train/wer': 0.069397354668894, 'validation/ctc_loss': Array(0.41824093, dtype=float32), 'validation/wer': 0.11956322349556368, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22184521, dtype=float32), 'test/wer': 0.07058273921962911, 'test/num_examples': 2472, 'score': 36023.417892456055, 'total_duration': 39512.232201099396, 'accumulated_submission_time': 36023.417892456055, 'accumulated_eval_time': 3485.3009746074677, 'accumulated_logging_time': 1.3599789142608643, 'global_step': 46830, 'preemption_count': 0}), (48000, {'train/ctc_loss': Array(0.224268, dtype=float32), 'train/wer': 0.07741169150711276, 'validation/ctc_loss': Array(0.4184095, dtype=float32), 'validation/wer': 0.1196790793322842, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22146653, dtype=float32), 'test/wer': 0.07098897081226006, 'test/num_examples': 2472, 'score': 36906.64269685745, 'total_duration': 40526.11241531372, 'accumulated_submission_time': 36906.64269685745, 'accumulated_eval_time': 3615.8436131477356, 'accumulated_logging_time': 1.4163413047790527, 'global_step': 48000, 'preemption_count': 0})], 'global_step': 48000}
I0217 11:52:05.877853 140202902193984 submission_runner.py:586] Timing: 36906.64269685745
I0217 11:52:05.877902 140202902193984 submission_runner.py:588] Total number of evals: 27
I0217 11:52:05.877952 140202902193984 submission_runner.py:589] ====================
I0217 11:52:05.915679 140202902193984 submission_runner.py:673] Final librispeech_deepspeech score: 36883.611184597015
