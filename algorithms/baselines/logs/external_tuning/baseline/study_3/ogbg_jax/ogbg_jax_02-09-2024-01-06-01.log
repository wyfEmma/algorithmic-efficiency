python3 submission_runner.py --framework=jax --workload=ogbg --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification/study_3 --overwrite=true --save_checkpoints=false --num_tuning_trials=5 --rng_seed=1895687988 --max_global_steps=80000 2>&1 | tee -a /logs/ogbg_jax_02-09-2024-01-06-01.log
I0209 01:06:22.681417 140039251117888 logger_utils.py:61] Removing existing experiment directory /experiment_runs/prize_qualification/study_3/ogbg_jax because --overwrite was set.
I0209 01:06:22.689163 140039251117888 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification/study_3/ogbg_jax.
I0209 01:06:23.766084 140039251117888 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0209 01:06:23.767113 140039251117888 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0209 01:06:23.767314 140039251117888 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0209 01:06:23.768577 140039251117888 submission_runner.py:542] Using RNG seed 1895687988
I0209 01:06:24.934821 140039251117888 submission_runner.py:551] --- Tuning run 1/5 ---
I0209 01:06:24.935081 140039251117888 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_3/ogbg_jax/trial_1.
I0209 01:06:24.935348 140039251117888 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_3/ogbg_jax/trial_1/hparams.json.
I0209 01:06:25.134328 140039251117888 submission_runner.py:206] Initializing dataset.
I0209 01:06:25.255695 140039251117888 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0209 01:06:25.261346 140039251117888 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
W0209 01:06:25.522656 140039251117888 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
I0209 01:06:25.587010 140039251117888 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0209 01:06:25.666336 140039251117888 submission_runner.py:213] Initializing model.
I0209 01:06:30.358288 140039251117888 submission_runner.py:255] Initializing optimizer.
I0209 01:06:31.010725 140039251117888 submission_runner.py:262] Initializing metrics bundle.
I0209 01:06:31.010931 140039251117888 submission_runner.py:280] Initializing checkpoint and logger.
I0209 01:06:31.011982 140039251117888 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_3/ogbg_jax/trial_1 with prefix checkpoint_
I0209 01:06:31.012133 140039251117888 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_3/ogbg_jax/trial_1/meta_data_0.json.
I0209 01:06:31.012335 140039251117888 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0209 01:06:31.012398 140039251117888 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0209 01:06:31.354601 140039251117888 logger_utils.py:220] Unable to record git information. Continuing without it.
I0209 01:06:31.672002 140039251117888 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_3/ogbg_jax/trial_1/flags_0.json.
I0209 01:06:31.683069 140039251117888 submission_runner.py:314] Starting training loop.
I0209 01:06:49.891109 139873763718912 logging_writer.py:48] [0] global_step=0, grad_norm=2.7371044158935547, loss=0.7145921587944031
I0209 01:06:49.907669 140039251117888 spec.py:321] Evaluating on the training split.
I0209 01:06:49.914268 140039251117888 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0209 01:06:49.918685 140039251117888 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0209 01:06:49.992803 140039251117888 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0209 01:08:44.062346 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 01:08:44.066038 140039251117888 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0209 01:08:44.070046 140039251117888 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0209 01:08:44.136909 140039251117888 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0209 01:10:22.272276 140039251117888 spec.py:349] Evaluating on the test split.
I0209 01:10:22.276447 140039251117888 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0209 01:10:22.281028 140039251117888 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0209 01:10:22.356712 140039251117888 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0209 01:12:02.003302 140039251117888 submission_runner.py:408] Time since start: 330.32s, 	Step: 1, 	{'train/accuracy': 0.5250149965286255, 'train/loss': 0.7151498794555664, 'train/mean_average_precision': 0.022461442098083748, 'validation/accuracy': 0.5213832855224609, 'validation/loss': 0.7166012525558472, 'validation/mean_average_precision': 0.026167739618213733, 'validation/num_examples': 43793, 'test/accuracy': 0.5224818587303162, 'test/loss': 0.7161954641342163, 'test/mean_average_precision': 0.02782404108389581, 'test/num_examples': 43793, 'score': 18.22455358505249, 'total_duration': 330.3200616836548, 'accumulated_submission_time': 18.22455358505249, 'accumulated_eval_time': 312.095463514328, 'accumulated_logging_time': 0}
I0209 01:12:02.024600 139870542051072 logging_writer.py:48] [1] accumulated_eval_time=312.095464, accumulated_logging_time=0, accumulated_submission_time=18.224554, global_step=1, preemption_count=0, score=18.224554, test/accuracy=0.522482, test/loss=0.716195, test/mean_average_precision=0.027824, test/num_examples=43793, total_duration=330.320062, train/accuracy=0.525015, train/loss=0.715150, train/mean_average_precision=0.022461, validation/accuracy=0.521383, validation/loss=0.716601, validation/mean_average_precision=0.026168, validation/num_examples=43793
I0209 01:12:34.400303 139871892588288 logging_writer.py:48] [100] global_step=100, grad_norm=0.6022368669509888, loss=0.42012855410575867
I0209 01:13:06.796154 139870542051072 logging_writer.py:48] [200] global_step=200, grad_norm=0.3524228632450104, loss=0.31525835394859314
I0209 01:13:38.540044 139871892588288 logging_writer.py:48] [300] global_step=300, grad_norm=0.25690919160842896, loss=0.22896984219551086
I0209 01:14:10.882786 139870542051072 logging_writer.py:48] [400] global_step=400, grad_norm=0.1691177934408188, loss=0.15993839502334595
I0209 01:14:42.787642 139871892588288 logging_writer.py:48] [500] global_step=500, grad_norm=0.10653998702764511, loss=0.10994065552949905
I0209 01:15:14.629935 139870542051072 logging_writer.py:48] [600] global_step=600, grad_norm=0.06541706621646881, loss=0.08791898190975189
I0209 01:15:46.568465 139871892588288 logging_writer.py:48] [700] global_step=700, grad_norm=0.0469013974070549, loss=0.06694642454385757
I0209 01:16:02.337059 140039251117888 spec.py:321] Evaluating on the training split.
I0209 01:17:54.083971 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 01:17:57.020191 140039251117888 spec.py:349] Evaluating on the test split.
I0209 01:17:59.930277 140039251117888 submission_runner.py:408] Time since start: 688.25s, 	Step: 749, 	{'train/accuracy': 0.9867318868637085, 'train/loss': 0.06859976053237915, 'train/mean_average_precision': 0.03414312510397307, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.07741302996873856, 'validation/mean_average_precision': 0.036325129363259476, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.08034414798021317, 'test/mean_average_precision': 0.03796467688874391, 'test/num_examples': 43793, 'score': 258.50570034980774, 'total_duration': 688.2471423149109, 'accumulated_submission_time': 258.50570034980774, 'accumulated_eval_time': 429.68863344192505, 'accumulated_logging_time': 0.033779144287109375}
I0209 01:17:59.946684 139871529707264 logging_writer.py:48] [749] accumulated_eval_time=429.688633, accumulated_logging_time=0.033779, accumulated_submission_time=258.505700, global_step=749, preemption_count=0, score=258.505700, test/accuracy=0.983142, test/loss=0.080344, test/mean_average_precision=0.037965, test/num_examples=43793, total_duration=688.247142, train/accuracy=0.986732, train/loss=0.068600, train/mean_average_precision=0.034143, validation/accuracy=0.984118, validation/loss=0.077413, validation/mean_average_precision=0.036325, validation/num_examples=43793
I0209 01:18:16.775252 139871917766400 logging_writer.py:48] [800] global_step=800, grad_norm=0.03216055408120155, loss=0.06665626168251038
I0209 01:18:48.689790 139871529707264 logging_writer.py:48] [900] global_step=900, grad_norm=0.15983058512210846, loss=0.061792757362127304
I0209 01:19:20.623976 139871917766400 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.0547516904771328, loss=0.05909016728401184
I0209 01:19:52.691426 139871529707264 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.11100432276725769, loss=0.056161534041166306
I0209 01:20:24.691046 139871917766400 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.2696600556373596, loss=0.05365884676575661
I0209 01:20:56.599839 139871529707264 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.33447691798210144, loss=0.052091050893068314
I0209 01:21:28.809812 139871917766400 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.11226440966129303, loss=0.04776481166481972
I0209 01:22:00.084260 140039251117888 spec.py:321] Evaluating on the training split.
I0209 01:23:57.159025 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 01:24:00.178143 140039251117888 spec.py:349] Evaluating on the test split.
I0209 01:24:03.328852 140039251117888 submission_runner.py:408] Time since start: 1051.65s, 	Step: 1498, 	{'train/accuracy': 0.9871208071708679, 'train/loss': 0.04883696511387825, 'train/mean_average_precision': 0.0850988099663653, 'validation/accuracy': 0.9844743609428406, 'validation/loss': 0.05789928883314133, 'validation/mean_average_precision': 0.08616501175420727, 'validation/num_examples': 43793, 'test/accuracy': 0.983509361743927, 'test/loss': 0.06113697960972786, 'test/mean_average_precision': 0.08745892214083549, 'test/num_examples': 43793, 'score': 498.6134088039398, 'total_duration': 1051.6457195281982, 'accumulated_submission_time': 498.6134088039398, 'accumulated_eval_time': 552.933183670044, 'accumulated_logging_time': 0.06142616271972656}
I0209 01:24:03.348450 139871892588288 logging_writer.py:48] [1498] accumulated_eval_time=552.933184, accumulated_logging_time=0.061426, accumulated_submission_time=498.613409, global_step=1498, preemption_count=0, score=498.613409, test/accuracy=0.983509, test/loss=0.061137, test/mean_average_precision=0.087459, test/num_examples=43793, total_duration=1051.645720, train/accuracy=0.987121, train/loss=0.048837, train/mean_average_precision=0.085099, validation/accuracy=0.984474, validation/loss=0.057899, validation/mean_average_precision=0.086165, validation/num_examples=43793
I0209 01:24:04.317317 139871909373696 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.32168272137641907, loss=0.04591372609138489
I0209 01:24:36.366356 139871892588288 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.42647644877433777, loss=0.04984598234295845
I0209 01:25:08.325041 139871909373696 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.22718192636966705, loss=0.0519028902053833
I0209 01:25:40.291681 139871892588288 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.1742364764213562, loss=0.05027352645993233
I0209 01:26:12.130376 139871909373696 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.07139971852302551, loss=0.049996621906757355
I0209 01:26:43.985560 139871892588288 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.17172761261463165, loss=0.04526946321129799
I0209 01:27:16.669496 139871909373696 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.19899097084999084, loss=0.042673852294683456
I0209 01:27:48.823342 139871892588288 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.18663465976715088, loss=0.04551200941205025
I0209 01:28:03.488648 140039251117888 spec.py:321] Evaluating on the training split.
I0209 01:29:58.541568 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 01:30:01.707611 140039251117888 spec.py:349] Evaluating on the test split.
I0209 01:30:05.050457 140039251117888 submission_runner.py:408] Time since start: 1413.37s, 	Step: 2246, 	{'train/accuracy': 0.9877936840057373, 'train/loss': 0.044384416192770004, 'train/mean_average_precision': 0.13547238429811922, 'validation/accuracy': 0.9850642085075378, 'validation/loss': 0.05360514670610428, 'validation/mean_average_precision': 0.13118885010019463, 'validation/num_examples': 43793, 'test/accuracy': 0.984077513217926, 'test/loss': 0.05675095319747925, 'test/mean_average_precision': 0.12851983268766617, 'test/num_examples': 43793, 'score': 738.7220523357391, 'total_duration': 1413.3673105239868, 'accumulated_submission_time': 738.7220523357391, 'accumulated_eval_time': 674.4949362277985, 'accumulated_logging_time': 0.09388899803161621}
I0209 01:30:05.067713 139871697753856 logging_writer.py:48] [2246] accumulated_eval_time=674.494936, accumulated_logging_time=0.093889, accumulated_submission_time=738.722052, global_step=2246, preemption_count=0, score=738.722052, test/accuracy=0.984078, test/loss=0.056751, test/mean_average_precision=0.128520, test/num_examples=43793, total_duration=1413.367311, train/accuracy=0.987794, train/loss=0.044384, train/mean_average_precision=0.135472, validation/accuracy=0.985064, validation/loss=0.053605, validation/mean_average_precision=0.131189, validation/num_examples=43793
I0209 01:30:23.262243 139871917766400 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.10463923960924149, loss=0.04280965030193329
I0209 01:30:56.680664 139871697753856 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.08907805383205414, loss=0.04508516564965248
I0209 01:31:30.048854 139871917766400 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.06857383251190186, loss=0.0402359701693058
I0209 01:32:03.256132 139871697753856 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.14510436356067657, loss=0.04769253730773926
I0209 01:32:36.217150 139871917766400 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.130453959107399, loss=0.041170768439769745
I0209 01:33:09.317586 139871697753856 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.1064961701631546, loss=0.040586113929748535
I0209 01:33:41.932812 139871917766400 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.1123141273856163, loss=0.04079723730683327
I0209 01:34:05.301643 140039251117888 spec.py:321] Evaluating on the training split.
I0209 01:36:04.107381 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 01:36:07.112549 140039251117888 spec.py:349] Evaluating on the test split.
I0209 01:36:10.116424 140039251117888 submission_runner.py:408] Time since start: 1778.43s, 	Step: 2973, 	{'train/accuracy': 0.9880256056785583, 'train/loss': 0.04221920296549797, 'train/mean_average_precision': 0.16530680024698508, 'validation/accuracy': 0.9852164387702942, 'validation/loss': 0.05180754512548447, 'validation/mean_average_precision': 0.15750730277160135, 'validation/num_examples': 43793, 'test/accuracy': 0.9842742681503296, 'test/loss': 0.05467303842306137, 'test/mean_average_precision': 0.15056607856140866, 'test/num_examples': 43793, 'score': 978.9202573299408, 'total_duration': 1778.4332921504974, 'accumulated_submission_time': 978.9202573299408, 'accumulated_eval_time': 799.3096804618835, 'accumulated_logging_time': 0.12338590621948242}
I0209 01:36:10.134420 139871900980992 logging_writer.py:48] [2973] accumulated_eval_time=799.309680, accumulated_logging_time=0.123386, accumulated_submission_time=978.920257, global_step=2973, preemption_count=0, score=978.920257, test/accuracy=0.984274, test/loss=0.054673, test/mean_average_precision=0.150566, test/num_examples=43793, total_duration=1778.433292, train/accuracy=0.988026, train/loss=0.042219, train/mean_average_precision=0.165307, validation/accuracy=0.985216, validation/loss=0.051808, validation/mean_average_precision=0.157507, validation/num_examples=43793
I0209 01:36:19.169416 139871909373696 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.12232417613267899, loss=0.046032074838876724
I0209 01:36:51.769921 139871900980992 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.08033329993486404, loss=0.04571710526943207
I0209 01:37:24.059225 139871909373696 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.10522785782814026, loss=0.04094609245657921
I0209 01:37:56.028435 139871900980992 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.10653805732727051, loss=0.0430205874145031
I0209 01:38:28.549323 139871909373696 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.05059915408492088, loss=0.04338198900222778
I0209 01:39:01.011740 139871900980992 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.04340391606092453, loss=0.04356351122260094
I0209 01:39:33.066171 139871909373696 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.11608744412660599, loss=0.03991685062646866
I0209 01:40:05.476106 139871900980992 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.1434684544801712, loss=0.04448990896344185
I0209 01:40:10.342411 140039251117888 spec.py:321] Evaluating on the training split.
I0209 01:42:11.672158 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 01:42:14.748315 140039251117888 spec.py:349] Evaluating on the test split.
I0209 01:42:17.705976 140039251117888 submission_runner.py:408] Time since start: 2146.02s, 	Step: 3716, 	{'train/accuracy': 0.9881114959716797, 'train/loss': 0.04102349281311035, 'train/mean_average_precision': 0.19044968455690436, 'validation/accuracy': 0.9853609204292297, 'validation/loss': 0.050700146704912186, 'validation/mean_average_precision': 0.16617833768708773, 'validation/num_examples': 43793, 'test/accuracy': 0.9844338893890381, 'test/loss': 0.05348716676235199, 'test/mean_average_precision': 0.16572209108759198, 'test/num_examples': 43793, 'score': 1219.0944874286652, 'total_duration': 2146.022847175598, 'accumulated_submission_time': 1219.0944874286652, 'accumulated_eval_time': 926.6731991767883, 'accumulated_logging_time': 0.15257525444030762}
I0209 01:42:17.721883 139871697753856 logging_writer.py:48] [3716] accumulated_eval_time=926.673199, accumulated_logging_time=0.152575, accumulated_submission_time=1219.094487, global_step=3716, preemption_count=0, score=1219.094487, test/accuracy=0.984434, test/loss=0.053487, test/mean_average_precision=0.165722, test/num_examples=43793, total_duration=2146.022847, train/accuracy=0.988111, train/loss=0.041023, train/mean_average_precision=0.190450, validation/accuracy=0.985361, validation/loss=0.050700, validation/mean_average_precision=0.166178, validation/num_examples=43793
I0209 01:42:45.017206 139871892588288 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.062395427376031876, loss=0.042847178876399994
I0209 01:43:17.067016 139871697753856 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.04736853018403053, loss=0.04399070888757706
I0209 01:43:48.749489 139871892588288 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.08979813754558563, loss=0.04542165622115135
I0209 01:44:20.605662 139871697753856 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.13883909583091736, loss=0.039775263518095016
I0209 01:44:52.250859 139871892588288 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.09195109456777573, loss=0.04601413384079933
I0209 01:45:24.180331 139871697753856 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.0742122158408165, loss=0.043840210884809494
I0209 01:45:55.442059 139871892588288 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.07488198578357697, loss=0.03550192713737488
I0209 01:46:17.717095 140039251117888 spec.py:321] Evaluating on the training split.
I0209 01:48:18.491912 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 01:48:21.532250 140039251117888 spec.py:349] Evaluating on the test split.
I0209 01:48:24.505187 140039251117888 submission_runner.py:408] Time since start: 2512.82s, 	Step: 4471, 	{'train/accuracy': 0.9883496761322021, 'train/loss': 0.04018726199865341, 'train/mean_average_precision': 0.20600886764690074, 'validation/accuracy': 0.9855566024780273, 'validation/loss': 0.04945233464241028, 'validation/mean_average_precision': 0.1817516461108204, 'validation/num_examples': 43793, 'test/accuracy': 0.9847017526626587, 'test/loss': 0.05210498720407486, 'test/mean_average_precision': 0.18062977807607863, 'test/num_examples': 43793, 'score': 1459.0584998130798, 'total_duration': 2512.822060108185, 'accumulated_submission_time': 1459.0584998130798, 'accumulated_eval_time': 1053.461250782013, 'accumulated_logging_time': 0.17978262901306152}
I0209 01:48:24.521028 139871934551808 logging_writer.py:48] [4471] accumulated_eval_time=1053.461251, accumulated_logging_time=0.179783, accumulated_submission_time=1459.058500, global_step=4471, preemption_count=0, score=1459.058500, test/accuracy=0.984702, test/loss=0.052105, test/mean_average_precision=0.180630, test/num_examples=43793, total_duration=2512.822060, train/accuracy=0.988350, train/loss=0.040187, train/mean_average_precision=0.206009, validation/accuracy=0.985557, validation/loss=0.049452, validation/mean_average_precision=0.181752, validation/num_examples=43793
I0209 01:48:33.999090 139977228785408 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.05505213141441345, loss=0.043110575526952744
I0209 01:49:05.421184 139871934551808 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.053029920905828476, loss=0.04604775831103325
I0209 01:49:36.892118 139977228785408 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.048662059009075165, loss=0.044693924486637115
I0209 01:50:08.376507 139871934551808 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.035737041383981705, loss=0.04270118847489357
I0209 01:50:39.842841 139977228785408 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.049689989537000656, loss=0.03991428390145302
I0209 01:51:11.607128 139871934551808 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.04945067688822746, loss=0.03982531651854515
I0209 01:51:43.389627 139977228785408 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.14910843968391418, loss=0.042765531688928604
I0209 01:52:15.532246 139871934551808 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.04001229256391525, loss=0.0422208346426487
I0209 01:52:24.812275 140039251117888 spec.py:321] Evaluating on the training split.
I0209 01:54:27.171940 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 01:54:30.232626 140039251117888 spec.py:349] Evaluating on the test split.
I0209 01:54:33.250458 140039251117888 submission_runner.py:408] Time since start: 2881.57s, 	Step: 5230, 	{'train/accuracy': 0.9888502955436707, 'train/loss': 0.03828683868050575, 'train/mean_average_precision': 0.22891057221235006, 'validation/accuracy': 0.9857863783836365, 'validation/loss': 0.04814080148935318, 'validation/mean_average_precision': 0.1905339620153854, 'validation/num_examples': 43793, 'test/accuracy': 0.9849346876144409, 'test/loss': 0.05086641013622284, 'test/mean_average_precision': 0.19266720315676172, 'test/num_examples': 43793, 'score': 1699.3178343772888, 'total_duration': 2881.5673213005066, 'accumulated_submission_time': 1699.3178343772888, 'accumulated_eval_time': 1181.8993849754333, 'accumulated_logging_time': 0.20825886726379395}
I0209 01:54:33.266499 139878383003392 logging_writer.py:48] [5230] accumulated_eval_time=1181.899385, accumulated_logging_time=0.208259, accumulated_submission_time=1699.317834, global_step=5230, preemption_count=0, score=1699.317834, test/accuracy=0.984935, test/loss=0.050866, test/mean_average_precision=0.192667, test/num_examples=43793, total_duration=2881.567321, train/accuracy=0.988850, train/loss=0.038287, train/mean_average_precision=0.228911, validation/accuracy=0.985786, validation/loss=0.048141, validation/mean_average_precision=0.190534, validation/num_examples=43793
I0209 01:54:56.466443 139977237178112 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.0653507262468338, loss=0.04131009429693222
I0209 01:55:28.444239 139878383003392 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.04005644470453262, loss=0.03903389722108841
I0209 01:56:00.440638 139977237178112 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.048822224140167236, loss=0.04349382221698761
I0209 01:56:32.332237 139878383003392 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.061794593930244446, loss=0.03738674148917198
I0209 01:57:04.104762 139977237178112 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.04919036105275154, loss=0.04104197397828102
I0209 01:57:35.953750 139878383003392 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.045662350952625275, loss=0.03822809457778931
I0209 01:58:07.437417 139977237178112 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.06884069740772247, loss=0.040822193026542664
I0209 01:58:33.320612 140039251117888 spec.py:321] Evaluating on the training split.
I0209 02:00:35.344914 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 02:00:38.373600 140039251117888 spec.py:349] Evaluating on the test split.
I0209 02:00:41.343539 140039251117888 submission_runner.py:408] Time since start: 3249.66s, 	Step: 5983, 	{'train/accuracy': 0.9890902042388916, 'train/loss': 0.037576012313365936, 'train/mean_average_precision': 0.2509227669922907, 'validation/accuracy': 0.9859312772750854, 'validation/loss': 0.04736001417040825, 'validation/mean_average_precision': 0.20258524834089467, 'validation/num_examples': 43793, 'test/accuracy': 0.9850884079933167, 'test/loss': 0.049846433103084564, 'test/mean_average_precision': 0.20214781431782117, 'test/num_examples': 43793, 'score': 1939.3397772312164, 'total_duration': 3249.6604022979736, 'accumulated_submission_time': 1939.3397772312164, 'accumulated_eval_time': 1309.9222609996796, 'accumulated_logging_time': 0.23669004440307617}
I0209 02:00:41.359590 139871926159104 logging_writer.py:48] [5983] accumulated_eval_time=1309.922261, accumulated_logging_time=0.236690, accumulated_submission_time=1939.339777, global_step=5983, preemption_count=0, score=1939.339777, test/accuracy=0.985088, test/loss=0.049846, test/mean_average_precision=0.202148, test/num_examples=43793, total_duration=3249.660402, train/accuracy=0.989090, train/loss=0.037576, train/mean_average_precision=0.250923, validation/accuracy=0.985931, validation/loss=0.047360, validation/mean_average_precision=0.202585, validation/num_examples=43793
I0209 02:00:47.186390 139977228785408 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.03257019817829132, loss=0.03918463736772537
I0209 02:01:19.352438 139871926159104 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.040285054594278336, loss=0.0416361428797245
I0209 02:01:51.337996 139977228785408 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.08468678593635559, loss=0.040756706148386
I0209 02:02:23.182674 139871926159104 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.05214563384652138, loss=0.04072776809334755
I0209 02:02:55.214994 139977228785408 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.03502515330910683, loss=0.03859376907348633
I0209 02:03:27.403179 139871926159104 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.03143763542175293, loss=0.03775892034173012
I0209 02:03:59.365245 139977228785408 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.032930415123701096, loss=0.04166145250201225
I0209 02:04:31.321178 139871926159104 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.035492148250341415, loss=0.04124663025140762
I0209 02:04:41.534385 140039251117888 spec.py:321] Evaluating on the training split.
I0209 02:06:41.946854 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 02:06:44.938009 140039251117888 spec.py:349] Evaluating on the test split.
I0209 02:06:47.915680 140039251117888 submission_runner.py:408] Time since start: 3616.23s, 	Step: 6733, 	{'train/accuracy': 0.9888186454772949, 'train/loss': 0.03792993724346161, 'train/mean_average_precision': 0.2507952327458457, 'validation/accuracy': 0.9858505129814148, 'validation/loss': 0.04780815541744232, 'validation/mean_average_precision': 0.20711343067742888, 'validation/num_examples': 43793, 'test/accuracy': 0.9849119186401367, 'test/loss': 0.050428010523319244, 'test/mean_average_precision': 0.2090736040470227, 'test/num_examples': 43793, 'score': 2179.482953310013, 'total_duration': 3616.232548236847, 'accumulated_submission_time': 2179.482953310013, 'accumulated_eval_time': 1436.303508758545, 'accumulated_logging_time': 0.26384711265563965}
I0209 02:06:47.932073 139878383003392 logging_writer.py:48] [6733] accumulated_eval_time=1436.303509, accumulated_logging_time=0.263847, accumulated_submission_time=2179.482953, global_step=6733, preemption_count=0, score=2179.482953, test/accuracy=0.984912, test/loss=0.050428, test/mean_average_precision=0.209074, test/num_examples=43793, total_duration=3616.232548, train/accuracy=0.988819, train/loss=0.037930, train/mean_average_precision=0.250795, validation/accuracy=0.985851, validation/loss=0.047808, validation/mean_average_precision=0.207113, validation/num_examples=43793
I0209 02:07:09.576534 139977237178112 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.03836071491241455, loss=0.04100092872977257
I0209 02:07:41.031010 139878383003392 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.08944500982761383, loss=0.042920421808958054
I0209 02:08:12.855499 139977237178112 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.03198600932955742, loss=0.037749554961919785
I0209 02:08:44.455661 139878383003392 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.035410210490226746, loss=0.0415460504591465
I0209 02:09:16.533137 139977237178112 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.025069724768400192, loss=0.03720158338546753
I0209 02:09:48.910191 139878383003392 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.02541903406381607, loss=0.04003996402025223
I0209 02:10:21.039077 139977237178112 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.025374045595526695, loss=0.03539117053151131
I0209 02:10:48.120606 140039251117888 spec.py:321] Evaluating on the training split.
I0209 02:12:49.359722 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 02:12:52.405434 140039251117888 spec.py:349] Evaluating on the test split.
I0209 02:12:55.385418 140039251117888 submission_runner.py:408] Time since start: 3983.70s, 	Step: 7486, 	{'train/accuracy': 0.9891477227210999, 'train/loss': 0.03706908971071243, 'train/mean_average_precision': 0.26609155218859043, 'validation/accuracy': 0.9860952496528625, 'validation/loss': 0.04678123816847801, 'validation/mean_average_precision': 0.21446156417915105, 'validation/num_examples': 43793, 'test/accuracy': 0.9852008819580078, 'test/loss': 0.049382343888282776, 'test/mean_average_precision': 0.21570979354301173, 'test/num_examples': 43793, 'score': 2419.6403181552887, 'total_duration': 3983.7022864818573, 'accumulated_submission_time': 2419.6403181552887, 'accumulated_eval_time': 1563.5682861804962, 'accumulated_logging_time': 0.2922539710998535}
I0209 02:12:55.401681 139878374610688 logging_writer.py:48] [7486] accumulated_eval_time=1563.568286, accumulated_logging_time=0.292254, accumulated_submission_time=2419.640318, global_step=7486, preemption_count=0, score=2419.640318, test/accuracy=0.985201, test/loss=0.049382, test/mean_average_precision=0.215710, test/num_examples=43793, total_duration=3983.702286, train/accuracy=0.989148, train/loss=0.037069, train/mean_average_precision=0.266092, validation/accuracy=0.986095, validation/loss=0.046781, validation/mean_average_precision=0.214462, validation/num_examples=43793
I0209 02:13:00.270098 139977228785408 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.03012249805033207, loss=0.04030990228056908
I0209 02:13:32.452499 139878374610688 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.042592063546180725, loss=0.038266364485025406
I0209 02:14:04.524884 139977228785408 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.030311109498143196, loss=0.0440906286239624
I0209 02:14:36.473296 139878374610688 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.03259488567709923, loss=0.040702011436223984
I0209 02:15:08.249401 139977228785408 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.02357201650738716, loss=0.038016706705093384
I0209 02:15:40.005448 139878374610688 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.03621985763311386, loss=0.04047473147511482
I0209 02:16:12.085394 139977228785408 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.03942747786641121, loss=0.043451786041259766
I0209 02:16:44.269588 139878374610688 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.026689697057008743, loss=0.03864063322544098
I0209 02:16:55.591589 140039251117888 spec.py:321] Evaluating on the training split.
I0209 02:18:56.049395 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 02:18:59.141608 140039251117888 spec.py:349] Evaluating on the test split.
I0209 02:19:02.310993 140039251117888 submission_runner.py:408] Time since start: 4350.63s, 	Step: 8237, 	{'train/accuracy': 0.9890878200531006, 'train/loss': 0.03689158335328102, 'train/mean_average_precision': 0.25970516745874167, 'validation/accuracy': 0.9861975908279419, 'validation/loss': 0.046271905303001404, 'validation/mean_average_precision': 0.21853893914855155, 'validation/num_examples': 43793, 'test/accuracy': 0.985374391078949, 'test/loss': 0.0486757755279541, 'test/mean_average_precision': 0.22138071002078036, 'test/num_examples': 43793, 'score': 2659.7981646060944, 'total_duration': 4350.627862453461, 'accumulated_submission_time': 2659.7981646060944, 'accumulated_eval_time': 1690.2876436710358, 'accumulated_logging_time': 0.3210947513580322}
I0209 02:19:02.329417 139871934551808 logging_writer.py:48] [8237] accumulated_eval_time=1690.287644, accumulated_logging_time=0.321095, accumulated_submission_time=2659.798165, global_step=8237, preemption_count=0, score=2659.798165, test/accuracy=0.985374, test/loss=0.048676, test/mean_average_precision=0.221381, test/num_examples=43793, total_duration=4350.627862, train/accuracy=0.989088, train/loss=0.036892, train/mean_average_precision=0.259705, validation/accuracy=0.986198, validation/loss=0.046272, validation/mean_average_precision=0.218539, validation/num_examples=43793
I0209 02:19:22.612376 139977237178112 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.024231640622019768, loss=0.04106650874018669
I0209 02:19:54.321916 139871934551808 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.022832373157143593, loss=0.03911947086453438
I0209 02:20:26.983631 139977237178112 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.03181605413556099, loss=0.03711165487766266
I0209 02:20:59.722209 139871934551808 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.03523887321352959, loss=0.03985075280070305
I0209 02:21:32.647365 139977237178112 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.02420957200229168, loss=0.03928346186876297
I0209 02:22:05.487571 139871934551808 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.017225107178092003, loss=0.03662646561861038
I0209 02:22:38.125651 139977237178112 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.02911599725484848, loss=0.04392106831073761
I0209 02:23:02.473687 140039251117888 spec.py:321] Evaluating on the training split.
I0209 02:25:03.483128 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 02:25:06.565938 140039251117888 spec.py:349] Evaluating on the test split.
I0209 02:25:09.522506 140039251117888 submission_runner.py:408] Time since start: 4717.84s, 	Step: 8975, 	{'train/accuracy': 0.9893003106117249, 'train/loss': 0.03609520196914673, 'train/mean_average_precision': 0.27442771710191144, 'validation/accuracy': 0.9861196279525757, 'validation/loss': 0.04653529077768326, 'validation/mean_average_precision': 0.21946127669537574, 'validation/num_examples': 43793, 'test/accuracy': 0.9852442741394043, 'test/loss': 0.04910749942064285, 'test/mean_average_precision': 0.22047892726404594, 'test/num_examples': 43793, 'score': 2899.9090049266815, 'total_duration': 4717.839361667633, 'accumulated_submission_time': 2899.9090049266815, 'accumulated_eval_time': 1817.3364193439484, 'accumulated_logging_time': 0.3503391742706299}
I0209 02:25:09.540113 139878374610688 logging_writer.py:48] [8975] accumulated_eval_time=1817.336419, accumulated_logging_time=0.350339, accumulated_submission_time=2899.909005, global_step=8975, preemption_count=0, score=2899.909005, test/accuracy=0.985244, test/loss=0.049107, test/mean_average_precision=0.220479, test/num_examples=43793, total_duration=4717.839362, train/accuracy=0.989300, train/loss=0.036095, train/mean_average_precision=0.274428, validation/accuracy=0.986120, validation/loss=0.046535, validation/mean_average_precision=0.219461, validation/num_examples=43793
I0209 02:25:18.870521 139878383003392 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.029303397983312607, loss=0.03879161179065704
I0209 02:25:51.346537 139878374610688 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.031545985490083694, loss=0.039469387382268906
I0209 02:26:23.090808 139878383003392 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.022464614361524582, loss=0.0344865508377552
I0209 02:26:54.417059 139878374610688 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.01829882524907589, loss=0.038310520350933075
I0209 02:27:26.070595 139878383003392 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.034906283020973206, loss=0.036402378231287
I0209 02:27:57.388577 139878374610688 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.021071959286928177, loss=0.03325224295258522
I0209 02:28:28.919377 139878383003392 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.03181292489171028, loss=0.03876494988799095
I0209 02:29:00.182212 139878374610688 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.03760027140378952, loss=0.038699839264154434
I0209 02:29:09.639984 140039251117888 spec.py:321] Evaluating on the training split.
I0209 02:31:09.390891 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 02:31:12.379902 140039251117888 spec.py:349] Evaluating on the test split.
I0209 02:31:15.372555 140039251117888 submission_runner.py:408] Time since start: 5083.69s, 	Step: 9731, 	{'train/accuracy': 0.989581823348999, 'train/loss': 0.035352617502212524, 'train/mean_average_precision': 0.3135008539187511, 'validation/accuracy': 0.9864249229431152, 'validation/loss': 0.045402102172374725, 'validation/mean_average_precision': 0.23789010410381536, 'validation/num_examples': 43793, 'test/accuracy': 0.985558032989502, 'test/loss': 0.04800809919834137, 'test/mean_average_precision': 0.23561684895981633, 'test/num_examples': 43793, 'score': 3139.976670026779, 'total_duration': 5083.68940782547, 'accumulated_submission_time': 3139.976670026779, 'accumulated_eval_time': 1943.068927526474, 'accumulated_logging_time': 0.38041067123413086}
I0209 02:31:15.389120 139977228785408 logging_writer.py:48] [9731] accumulated_eval_time=1943.068928, accumulated_logging_time=0.380411, accumulated_submission_time=3139.976670, global_step=9731, preemption_count=0, score=3139.976670, test/accuracy=0.985558, test/loss=0.048008, test/mean_average_precision=0.235617, test/num_examples=43793, total_duration=5083.689408, train/accuracy=0.989582, train/loss=0.035353, train/mean_average_precision=0.313501, validation/accuracy=0.986425, validation/loss=0.045402, validation/mean_average_precision=0.237890, validation/num_examples=43793
I0209 02:31:37.392500 139977237178112 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.045728154480457306, loss=0.03821231424808502
I0209 02:32:08.943588 139977228785408 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.026788195595145226, loss=0.038473982363939285
I0209 02:32:40.571746 139977237178112 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.03319009393453598, loss=0.03729832172393799
I0209 02:33:12.592890 139977228785408 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.0250689797103405, loss=0.040601372718811035
I0209 02:33:44.370912 139977237178112 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.023108916357159615, loss=0.035385534167289734
I0209 02:34:16.163241 139977228785408 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.025456085801124573, loss=0.04076407477259636
I0209 02:34:47.720132 139977237178112 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.03727145865559578, loss=0.03725326433777809
I0209 02:35:15.597939 140039251117888 spec.py:321] Evaluating on the training split.
I0209 02:37:20.803786 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 02:37:23.852525 140039251117888 spec.py:349] Evaluating on the test split.
I0209 02:37:26.781275 140039251117888 submission_runner.py:408] Time since start: 5455.10s, 	Step: 10489, 	{'train/accuracy': 0.9896994233131409, 'train/loss': 0.03448774293065071, 'train/mean_average_precision': 0.3108126612908478, 'validation/accuracy': 0.9863623976707458, 'validation/loss': 0.045581333339214325, 'validation/mean_average_precision': 0.23510919648496872, 'validation/num_examples': 43793, 'test/accuracy': 0.9854817986488342, 'test/loss': 0.04808775708079338, 'test/mean_average_precision': 0.23582110774026796, 'test/num_examples': 43793, 'score': 3380.1543962955475, 'total_duration': 5455.0981414318085, 'accumulated_submission_time': 3380.1543962955475, 'accumulated_eval_time': 2074.2522208690643, 'accumulated_logging_time': 0.40826964378356934}
I0209 02:37:26.798316 139878374610688 logging_writer.py:48] [10489] accumulated_eval_time=2074.252221, accumulated_logging_time=0.408270, accumulated_submission_time=3380.154396, global_step=10489, preemption_count=0, score=3380.154396, test/accuracy=0.985482, test/loss=0.048088, test/mean_average_precision=0.235821, test/num_examples=43793, total_duration=5455.098141, train/accuracy=0.989699, train/loss=0.034488, train/mean_average_precision=0.310813, validation/accuracy=0.986362, validation/loss=0.045581, validation/mean_average_precision=0.235109, validation/num_examples=43793
I0209 02:37:30.632414 139878383003392 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.029419230297207832, loss=0.03744262084364891
I0209 02:38:03.124618 139878374610688 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.036601003259420395, loss=0.03859107568860054
I0209 02:38:34.813656 139878383003392 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.04179583862423897, loss=0.038662150502204895
I0209 02:39:06.630833 139878374610688 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.047974858433008194, loss=0.0361335389316082
I0209 02:39:38.649417 139878383003392 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.039110079407691956, loss=0.04126258194446564
I0209 02:40:10.317496 139878374610688 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.025778306648135185, loss=0.03753361478447914
I0209 02:40:42.121234 139878383003392 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.03154293820261955, loss=0.0365864522755146
I0209 02:41:14.012681 139878374610688 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.02959536574780941, loss=0.038490038365125656
I0209 02:41:26.909580 140039251117888 spec.py:321] Evaluating on the training split.
I0209 02:43:28.894601 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 02:43:31.867285 140039251117888 spec.py:349] Evaluating on the test split.
I0209 02:43:34.817491 140039251117888 submission_runner.py:408] Time since start: 5823.13s, 	Step: 11242, 	{'train/accuracy': 0.9899011850357056, 'train/loss': 0.03379280865192413, 'train/mean_average_precision': 0.34200087874016116, 'validation/accuracy': 0.9865202903747559, 'validation/loss': 0.04525914043188095, 'validation/mean_average_precision': 0.2445117594369473, 'validation/num_examples': 43793, 'test/accuracy': 0.985623300075531, 'test/loss': 0.04814762994647026, 'test/mean_average_precision': 0.23960741514002484, 'test/num_examples': 43793, 'score': 3620.235187292099, 'total_duration': 5823.134348869324, 'accumulated_submission_time': 3620.235187292099, 'accumulated_eval_time': 2202.160078048706, 'accumulated_logging_time': 0.4365499019622803}
I0209 02:43:34.835453 139871934551808 logging_writer.py:48] [11242] accumulated_eval_time=2202.160078, accumulated_logging_time=0.436550, accumulated_submission_time=3620.235187, global_step=11242, preemption_count=0, score=3620.235187, test/accuracy=0.985623, test/loss=0.048148, test/mean_average_precision=0.239607, test/num_examples=43793, total_duration=5823.134349, train/accuracy=0.989901, train/loss=0.033793, train/mean_average_precision=0.342001, validation/accuracy=0.986520, validation/loss=0.045259, validation/mean_average_precision=0.244512, validation/num_examples=43793
I0209 02:43:54.925126 139977237178112 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.042491365224123, loss=0.03895896300673485
I0209 02:44:26.959767 139871934551808 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.0275458712130785, loss=0.038912009447813034
I0209 02:44:58.709218 139977237178112 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.0273247342556715, loss=0.0374719612300396
I0209 02:45:30.533724 139871934551808 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.02640710584819317, loss=0.037020113319158554
I0209 02:46:02.441990 139977237178112 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.028164494782686234, loss=0.03434517979621887
I0209 02:46:34.044383 139871934551808 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.03215019404888153, loss=0.03541307896375656
I0209 02:47:06.506381 139977237178112 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.043769631534814835, loss=0.03936197608709335
I0209 02:47:35.085647 140039251117888 spec.py:321] Evaluating on the training split.
I0209 02:49:38.937264 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 02:49:42.031861 140039251117888 spec.py:349] Evaluating on the test split.
I0209 02:49:44.970520 140039251117888 submission_runner.py:408] Time since start: 6193.29s, 	Step: 11989, 	{'train/accuracy': 0.990170955657959, 'train/loss': 0.032714128494262695, 'train/mean_average_precision': 0.3666973354664275, 'validation/accuracy': 0.9865543842315674, 'validation/loss': 0.04496646299958229, 'validation/mean_average_precision': 0.2531282590109368, 'validation/num_examples': 43793, 'test/accuracy': 0.9856700897216797, 'test/loss': 0.047432009130716324, 'test/mean_average_precision': 0.2479243786116264, 'test/num_examples': 43793, 'score': 3860.4547262191772, 'total_duration': 6193.2873792648315, 'accumulated_submission_time': 3860.4547262191772, 'accumulated_eval_time': 2332.0449130535126, 'accumulated_logging_time': 0.4658217430114746}
I0209 02:49:44.988060 139878374610688 logging_writer.py:48] [11989] accumulated_eval_time=2332.044913, accumulated_logging_time=0.465822, accumulated_submission_time=3860.454726, global_step=11989, preemption_count=0, score=3860.454726, test/accuracy=0.985670, test/loss=0.047432, test/mean_average_precision=0.247924, test/num_examples=43793, total_duration=6193.287379, train/accuracy=0.990171, train/loss=0.032714, train/mean_average_precision=0.366697, validation/accuracy=0.986554, validation/loss=0.044966, validation/mean_average_precision=0.253128, validation/num_examples=43793
I0209 02:49:48.832955 139878383003392 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.030512144789099693, loss=0.036347270011901855
I0209 02:50:21.180886 139878374610688 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.030956696718931198, loss=0.03777875378727913
I0209 02:50:52.976763 139878383003392 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.028474340215325356, loss=0.03516148775815964
I0209 02:51:24.877544 139878374610688 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.029849782586097717, loss=0.03279411792755127
I0209 02:51:56.726773 139878383003392 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.03026678040623665, loss=0.034565940499305725
I0209 02:52:28.568511 139878374610688 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.02986382320523262, loss=0.034439440816640854
I0209 02:53:00.892144 139878383003392 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.033363282680511475, loss=0.03694195672869682
I0209 02:53:33.072162 139878374610688 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.029781851917505264, loss=0.03523687273263931
I0209 02:53:45.149483 140039251117888 spec.py:321] Evaluating on the training split.
I0209 02:55:45.394236 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 02:55:48.411854 140039251117888 spec.py:349] Evaluating on the test split.
I0209 02:55:51.404005 140039251117888 submission_runner.py:408] Time since start: 6559.72s, 	Step: 12739, 	{'train/accuracy': 0.990369439125061, 'train/loss': 0.032262638211250305, 'train/mean_average_precision': 0.3685053446267267, 'validation/accuracy': 0.9865767359733582, 'validation/loss': 0.04462543874979019, 'validation/mean_average_precision': 0.246913705823616, 'validation/num_examples': 43793, 'test/accuracy': 0.9857408404350281, 'test/loss': 0.04721754044294357, 'test/mean_average_precision': 0.24258684231373745, 'test/num_examples': 43793, 'score': 4100.5837614536285, 'total_duration': 6559.720872402191, 'accumulated_submission_time': 4100.5837614536285, 'accumulated_eval_time': 2458.2993903160095, 'accumulated_logging_time': 0.49584507942199707}
I0209 02:55:51.421606 139871934551808 logging_writer.py:48] [12739] accumulated_eval_time=2458.299390, accumulated_logging_time=0.495845, accumulated_submission_time=4100.583761, global_step=12739, preemption_count=0, score=4100.583761, test/accuracy=0.985741, test/loss=0.047218, test/mean_average_precision=0.242587, test/num_examples=43793, total_duration=6559.720872, train/accuracy=0.990369, train/loss=0.032263, train/mean_average_precision=0.368505, validation/accuracy=0.986577, validation/loss=0.044625, validation/mean_average_precision=0.246914, validation/num_examples=43793
I0209 02:56:11.879851 139977237178112 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.03865603730082512, loss=0.0383293554186821
I0209 02:56:43.513558 139871934551808 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.04733078181743622, loss=0.035707153379917145
I0209 02:57:15.497364 139977237178112 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.029587995260953903, loss=0.03904109075665474
I0209 02:57:47.687930 139871934551808 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.033588655292987823, loss=0.038581497967243195
I0209 02:58:20.186593 139977237178112 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.0727744773030281, loss=0.04148562252521515
I0209 02:58:51.970901 139871934551808 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.06354010105133057, loss=0.03478141129016876
I0209 02:59:24.130103 139977237178112 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.04710720479488373, loss=0.03948119655251503
I0209 02:59:51.488710 140039251117888 spec.py:321] Evaluating on the training split.
I0209 03:01:53.357327 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 03:01:56.353941 140039251117888 spec.py:349] Evaluating on the test split.
I0209 03:01:59.404905 140039251117888 submission_runner.py:408] Time since start: 6927.72s, 	Step: 13486, 	{'train/accuracy': 0.9902468919754028, 'train/loss': 0.03229084610939026, 'train/mean_average_precision': 0.36819621667420155, 'validation/accuracy': 0.9864902496337891, 'validation/loss': 0.0449402891099453, 'validation/mean_average_precision': 0.2522724175171478, 'validation/num_examples': 43793, 'test/accuracy': 0.9856376647949219, 'test/loss': 0.04771548509597778, 'test/mean_average_precision': 0.24706367211131142, 'test/num_examples': 43793, 'score': 4340.619060993195, 'total_duration': 6927.721772909164, 'accumulated_submission_time': 4340.619060993195, 'accumulated_eval_time': 2586.215543985367, 'accumulated_logging_time': 0.5247492790222168}
I0209 03:01:59.422716 139878374610688 logging_writer.py:48] [13486] accumulated_eval_time=2586.215544, accumulated_logging_time=0.524749, accumulated_submission_time=4340.619061, global_step=13486, preemption_count=0, score=4340.619061, test/accuracy=0.985638, test/loss=0.047715, test/mean_average_precision=0.247064, test/num_examples=43793, total_duration=6927.721773, train/accuracy=0.990247, train/loss=0.032291, train/mean_average_precision=0.368196, validation/accuracy=0.986490, validation/loss=0.044940, validation/mean_average_precision=0.252272, validation/num_examples=43793
I0209 03:02:04.399231 139878383003392 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.03241996467113495, loss=0.032454609870910645
I0209 03:02:35.996181 139878374610688 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.032984789460897446, loss=0.036384109407663345
I0209 03:03:07.854162 139878383003392 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.03861192613840103, loss=0.0375031940639019
I0209 03:03:39.716900 139878374610688 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.03289290517568588, loss=0.036713287234306335
I0209 03:04:11.710817 139878383003392 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.04593246802687645, loss=0.03769827261567116
I0209 03:04:43.624219 139878374610688 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.05569323152303696, loss=0.04071304202079773
I0209 03:05:15.452487 139878383003392 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.035285502672195435, loss=0.03918865695595741
I0209 03:05:46.898605 139878374610688 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.05772072449326515, loss=0.035410743206739426
I0209 03:05:59.460408 140039251117888 spec.py:321] Evaluating on the training split.
I0209 03:08:00.227485 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 03:08:03.440238 140039251117888 spec.py:349] Evaluating on the test split.
I0209 03:08:06.398647 140039251117888 submission_runner.py:408] Time since start: 7294.72s, 	Step: 14241, 	{'train/accuracy': 0.9904201030731201, 'train/loss': 0.03188631683588028, 'train/mean_average_precision': 0.3612297607359896, 'validation/accuracy': 0.9867191910743713, 'validation/loss': 0.04437703639268875, 'validation/mean_average_precision': 0.2567491818925974, 'validation/num_examples': 43793, 'test/accuracy': 0.9858179092407227, 'test/loss': 0.04711194708943367, 'test/mean_average_precision': 0.24733833804242544, 'test/num_examples': 43793, 'score': 4580.62403678894, 'total_duration': 7294.715516328812, 'accumulated_submission_time': 4580.62403678894, 'accumulated_eval_time': 2713.153738975525, 'accumulated_logging_time': 0.5549750328063965}
I0209 03:08:06.416721 139871926159104 logging_writer.py:48] [14241] accumulated_eval_time=2713.153739, accumulated_logging_time=0.554975, accumulated_submission_time=4580.624037, global_step=14241, preemption_count=0, score=4580.624037, test/accuracy=0.985818, test/loss=0.047112, test/mean_average_precision=0.247338, test/num_examples=43793, total_duration=7294.715516, train/accuracy=0.990420, train/loss=0.031886, train/mean_average_precision=0.361230, validation/accuracy=0.986719, validation/loss=0.044377, validation/mean_average_precision=0.256749, validation/num_examples=43793
I0209 03:08:25.282633 139871934551808 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.0443359911441803, loss=0.03401024267077446
I0209 03:08:56.971125 139871926159104 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.04233391582965851, loss=0.0348123274743557
I0209 03:09:28.943649 139871934551808 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.04049614071846008, loss=0.03622252121567726
I0209 03:10:00.554826 139871926159104 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.04277095943689346, loss=0.03515229374170303
I0209 03:10:32.733056 139871934551808 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.08782277256250381, loss=0.033848728984594345
I0209 03:11:04.828519 139871926159104 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.06211911141872406, loss=0.037494637072086334
I0209 03:11:36.742798 139871934551808 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.04162680730223656, loss=0.032720327377319336
I0209 03:12:06.541447 140039251117888 spec.py:321] Evaluating on the training split.
I0209 03:14:06.379795 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 03:14:09.470792 140039251117888 spec.py:349] Evaluating on the test split.
I0209 03:14:12.537600 140039251117888 submission_runner.py:408] Time since start: 7660.85s, 	Step: 14995, 	{'train/accuracy': 0.9904540181159973, 'train/loss': 0.031819697469472885, 'train/mean_average_precision': 0.3714848644505445, 'validation/accuracy': 0.9867293238639832, 'validation/loss': 0.04427880048751831, 'validation/mean_average_precision': 0.26217762286830854, 'validation/num_examples': 43793, 'test/accuracy': 0.985866367816925, 'test/loss': 0.04707420989871025, 'test/mean_average_precision': 0.25019311136580263, 'test/num_examples': 43793, 'score': 4820.717617750168, 'total_duration': 7660.8544363975525, 'accumulated_submission_time': 4820.717617750168, 'accumulated_eval_time': 2839.149816274643, 'accumulated_logging_time': 0.584143877029419}
I0209 03:14:12.555607 139878374610688 logging_writer.py:48] [14995] accumulated_eval_time=2839.149816, accumulated_logging_time=0.584144, accumulated_submission_time=4820.717618, global_step=14995, preemption_count=0, score=4820.717618, test/accuracy=0.985866, test/loss=0.047074, test/mean_average_precision=0.250193, test/num_examples=43793, total_duration=7660.854436, train/accuracy=0.990454, train/loss=0.031820, train/mean_average_precision=0.371485, validation/accuracy=0.986729, validation/loss=0.044279, validation/mean_average_precision=0.262178, validation/num_examples=43793
I0209 03:14:14.548125 139878383003392 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.05392136424779892, loss=0.03671834617853165
I0209 03:14:47.502948 139878374610688 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.043400827795267105, loss=0.03249122202396393
I0209 03:15:20.017448 139878383003392 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.04082722216844559, loss=0.03518443927168846
I0209 03:15:52.398719 139878374610688 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.05693307891488075, loss=0.03770192712545395
I0209 03:16:24.472892 139878383003392 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.04953974485397339, loss=0.03884735330939293
I0209 03:16:56.526306 139878374610688 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.06333579868078232, loss=0.035300638526678085
I0209 03:17:28.700128 139878383003392 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.045990608632564545, loss=0.031869396567344666
I0209 03:18:00.865391 139878374610688 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.054902609437704086, loss=0.032763708382844925
I0209 03:18:12.791105 140039251117888 spec.py:321] Evaluating on the training split.
I0209 03:20:16.945611 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 03:20:20.116088 140039251117888 spec.py:349] Evaluating on the test split.
I0209 03:20:23.112330 140039251117888 submission_runner.py:408] Time since start: 8031.43s, 	Step: 15738, 	{'train/accuracy': 0.9903156161308289, 'train/loss': 0.03219970688223839, 'train/mean_average_precision': 0.37523347033362875, 'validation/accuracy': 0.9866416454315186, 'validation/loss': 0.044646937400102615, 'validation/mean_average_precision': 0.25367239636785693, 'validation/num_examples': 43793, 'test/accuracy': 0.9857829809188843, 'test/loss': 0.04752212390303612, 'test/mean_average_precision': 0.25035524066505116, 'test/num_examples': 43793, 'score': 5060.922473907471, 'total_duration': 8031.429186582565, 'accumulated_submission_time': 5060.922473907471, 'accumulated_eval_time': 2969.4709827899933, 'accumulated_logging_time': 0.6131572723388672}
I0209 03:20:23.131404 139871934551808 logging_writer.py:48] [15738] accumulated_eval_time=2969.470983, accumulated_logging_time=0.613157, accumulated_submission_time=5060.922474, global_step=15738, preemption_count=0, score=5060.922474, test/accuracy=0.985783, test/loss=0.047522, test/mean_average_precision=0.250355, test/num_examples=43793, total_duration=8031.429187, train/accuracy=0.990316, train/loss=0.032200, train/mean_average_precision=0.375233, validation/accuracy=0.986642, validation/loss=0.044647, validation/mean_average_precision=0.253672, validation/num_examples=43793
I0209 03:20:43.292166 139977237178112 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.07392004132270813, loss=0.034671518951654434
I0209 03:21:15.172212 139871934551808 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.04982798174023628, loss=0.037495460361242294
I0209 03:21:47.115093 139977237178112 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.049135997891426086, loss=0.034605834633111954
I0209 03:22:19.098787 139871934551808 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.04984751716256142, loss=0.03438524901866913
I0209 03:22:50.907531 139977237178112 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.0626835972070694, loss=0.0358009934425354
I0209 03:23:23.035857 139871934551808 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.08590935170650482, loss=0.03115992806851864
I0209 03:23:55.436813 139977237178112 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.04816250875592232, loss=0.03689480572938919
I0209 03:24:23.432069 140039251117888 spec.py:321] Evaluating on the training split.
I0209 03:26:26.711520 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 03:26:29.752883 140039251117888 spec.py:349] Evaluating on the test split.
I0209 03:26:32.707537 140039251117888 submission_runner.py:408] Time since start: 8401.02s, 	Step: 16488, 	{'train/accuracy': 0.9904052019119263, 'train/loss': 0.031755246222019196, 'train/mean_average_precision': 0.364807285871065, 'validation/accuracy': 0.9867590069770813, 'validation/loss': 0.04437119886279106, 'validation/mean_average_precision': 0.2592793316141011, 'validation/num_examples': 43793, 'test/accuracy': 0.9859042763710022, 'test/loss': 0.04704057425260544, 'test/mean_average_precision': 0.25100582192182425, 'test/num_examples': 43793, 'score': 5301.190806150436, 'total_duration': 8401.024397611618, 'accumulated_submission_time': 5301.190806150436, 'accumulated_eval_time': 3098.746400117874, 'accumulated_logging_time': 0.6441349983215332}
I0209 03:26:32.725758 139871926159104 logging_writer.py:48] [16488] accumulated_eval_time=3098.746400, accumulated_logging_time=0.644135, accumulated_submission_time=5301.190806, global_step=16488, preemption_count=0, score=5301.190806, test/accuracy=0.985904, test/loss=0.047041, test/mean_average_precision=0.251006, test/num_examples=43793, total_duration=8401.024398, train/accuracy=0.990405, train/loss=0.031755, train/mean_average_precision=0.364807, validation/accuracy=0.986759, validation/loss=0.044371, validation/mean_average_precision=0.259279, validation/num_examples=43793
I0209 03:26:37.001117 139878374610688 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.06962712109088898, loss=0.03778837248682976
I0209 03:27:08.911643 139871926159104 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.05040458217263222, loss=0.035602815449237823
I0209 03:27:40.711395 139878374610688 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.051349230110645294, loss=0.03456367924809456
I0209 03:28:12.954141 139871926159104 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.04672521725296974, loss=0.03488469868898392
I0209 03:28:45.011763 139878374610688 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.07623295485973358, loss=0.034630436450242996
I0209 03:29:17.261433 139871926159104 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.06599626690149307, loss=0.03522124141454697
I0209 03:29:49.481693 139878374610688 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.049447737634181976, loss=0.034175705164670944
I0209 03:30:21.454482 139871926159104 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.10380318015813828, loss=0.03401317074894905
I0209 03:30:32.995046 140039251117888 spec.py:321] Evaluating on the training split.
I0209 03:32:34.484182 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 03:32:37.512350 140039251117888 spec.py:349] Evaluating on the test split.
I0209 03:32:40.583984 140039251117888 submission_runner.py:408] Time since start: 8768.90s, 	Step: 17237, 	{'train/accuracy': 0.9905096292495728, 'train/loss': 0.03137778863310814, 'train/mean_average_precision': 0.38399182104575913, 'validation/accuracy': 0.986682653427124, 'validation/loss': 0.04443146660923958, 'validation/mean_average_precision': 0.260249022031754, 'validation/num_examples': 43793, 'test/accuracy': 0.9857720136642456, 'test/loss': 0.04699244722723961, 'test/mean_average_precision': 0.2551105020349281, 'test/num_examples': 43793, 'score': 5541.426566362381, 'total_duration': 8768.900849103928, 'accumulated_submission_time': 5541.426566362381, 'accumulated_eval_time': 3226.335287809372, 'accumulated_logging_time': 0.6756045818328857}
I0209 03:32:40.603214 139871934551808 logging_writer.py:48] [17237] accumulated_eval_time=3226.335288, accumulated_logging_time=0.675605, accumulated_submission_time=5541.426566, global_step=17237, preemption_count=0, score=5541.426566, test/accuracy=0.985772, test/loss=0.046992, test/mean_average_precision=0.255111, test/num_examples=43793, total_duration=8768.900849, train/accuracy=0.990510, train/loss=0.031378, train/mean_average_precision=0.383992, validation/accuracy=0.986683, validation/loss=0.044431, validation/mean_average_precision=0.260249, validation/num_examples=43793
I0209 03:33:01.515954 139878383003392 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.0768527016043663, loss=0.03451786935329437
I0209 03:33:33.600182 139871934551808 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.06299249827861786, loss=0.034664254635572433
I0209 03:34:05.810811 139878383003392 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.04914436116814613, loss=0.03625905141234398
I0209 03:34:38.086627 139871934551808 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.06727747619152069, loss=0.03455507382750511
I0209 03:35:10.047530 139878383003392 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.11609930545091629, loss=0.03374631702899933
I0209 03:35:42.016222 139871934551808 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.05313307046890259, loss=0.03664533421397209
I0209 03:36:14.321101 139878383003392 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.1020989716053009, loss=0.032236237078905106
I0209 03:36:40.816104 140039251117888 spec.py:321] Evaluating on the training split.
I0209 03:38:43.121170 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 03:38:46.543599 140039251117888 spec.py:349] Evaluating on the test split.
I0209 03:38:50.006692 140039251117888 submission_runner.py:408] Time since start: 9138.32s, 	Step: 17983, 	{'train/accuracy': 0.9906891584396362, 'train/loss': 0.030599458143115044, 'train/mean_average_precision': 0.40471017588618474, 'validation/accuracy': 0.9867675304412842, 'validation/loss': 0.044316284358501434, 'validation/mean_average_precision': 0.2684684903382087, 'validation/num_examples': 43793, 'test/accuracy': 0.9858996272087097, 'test/loss': 0.04708460345864296, 'test/mean_average_precision': 0.25263314540650667, 'test/num_examples': 43793, 'score': 5781.60738492012, 'total_duration': 9138.323543548584, 'accumulated_submission_time': 5781.60738492012, 'accumulated_eval_time': 3355.5258157253265, 'accumulated_logging_time': 0.706317663192749}
I0209 03:38:50.028606 139878374610688 logging_writer.py:48] [17983] accumulated_eval_time=3355.525816, accumulated_logging_time=0.706318, accumulated_submission_time=5781.607385, global_step=17983, preemption_count=0, score=5781.607385, test/accuracy=0.985900, test/loss=0.047085, test/mean_average_precision=0.252633, test/num_examples=43793, total_duration=9138.323544, train/accuracy=0.990689, train/loss=0.030599, train/mean_average_precision=0.404710, validation/accuracy=0.986768, validation/loss=0.044316, validation/mean_average_precision=0.268468, validation/num_examples=43793
I0209 03:38:55.984846 139977237178112 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.06433799862861633, loss=0.03878680616617203
I0209 03:39:28.952117 139878374610688 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.06526654213666916, loss=0.032895900309085846
I0209 03:40:02.018714 139977237178112 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.08002359420061111, loss=0.03319408372044563
I0209 03:40:34.587217 139878374610688 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.07711758464574814, loss=0.03697972372174263
I0209 03:41:07.141993 139977237178112 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.08634717017412186, loss=0.034242305904626846
I0209 03:41:39.554735 139878374610688 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.04810081422328949, loss=0.03561088442802429
I0209 03:42:11.972596 139977237178112 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.06863054633140564, loss=0.036221325397491455
I0209 03:42:44.365828 139878374610688 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.05674160271883011, loss=0.035159606486558914
I0209 03:42:50.099881 140039251117888 spec.py:321] Evaluating on the training split.
I0209 03:44:53.215798 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 03:44:56.602415 140039251117888 spec.py:349] Evaluating on the test split.
I0209 03:44:59.949720 140039251117888 submission_runner.py:408] Time since start: 9508.27s, 	Step: 18719, 	{'train/accuracy': 0.9908905029296875, 'train/loss': 0.030075056478381157, 'train/mean_average_precision': 0.4239957922232233, 'validation/accuracy': 0.9867285490036011, 'validation/loss': 0.04435645788908005, 'validation/mean_average_precision': 0.2595765740247147, 'validation/num_examples': 43793, 'test/accuracy': 0.985871434211731, 'test/loss': 0.04702959209680557, 'test/mean_average_precision': 0.2514217514588064, 'test/num_examples': 43793, 'score': 6021.643416404724, 'total_duration': 9508.266571044922, 'accumulated_submission_time': 6021.643416404724, 'accumulated_eval_time': 3485.375589132309, 'accumulated_logging_time': 0.7408199310302734}
I0209 03:44:59.970406 139871934551808 logging_writer.py:48] [18719] accumulated_eval_time=3485.375589, accumulated_logging_time=0.740820, accumulated_submission_time=6021.643416, global_step=18719, preemption_count=0, score=6021.643416, test/accuracy=0.985871, test/loss=0.047030, test/mean_average_precision=0.251422, test/num_examples=43793, total_duration=9508.266571, train/accuracy=0.990891, train/loss=0.030075, train/mean_average_precision=0.423996, validation/accuracy=0.986729, validation/loss=0.044356, validation/mean_average_precision=0.259577, validation/num_examples=43793
I0209 03:45:27.101664 139878383003392 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.06167830154299736, loss=0.034193169325590134
I0209 03:45:59.658528 139871934551808 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.06818949431180954, loss=0.03737449273467064
I0209 03:46:31.679543 139878383003392 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.06424453854560852, loss=0.03888672590255737
I0209 03:47:04.063573 139871934551808 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.06548089534044266, loss=0.03542766720056534
I0209 03:47:35.851449 139878383003392 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.07297303527593613, loss=0.034588176757097244
I0209 03:48:08.035717 139871934551808 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.08521106094121933, loss=0.03714195638895035
I0209 03:48:40.014926 139878383003392 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.07849641889333725, loss=0.035817816853523254
I0209 03:48:59.965808 140039251117888 spec.py:321] Evaluating on the training split.
I0209 03:51:00.344435 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 03:51:03.574030 140039251117888 spec.py:349] Evaluating on the test split.
I0209 03:51:06.589356 140039251117888 submission_runner.py:408] Time since start: 9874.91s, 	Step: 19464, 	{'train/accuracy': 0.991010844707489, 'train/loss': 0.02957056649029255, 'train/mean_average_precision': 0.4344861831450195, 'validation/accuracy': 0.9867455959320068, 'validation/loss': 0.04412097483873367, 'validation/mean_average_precision': 0.2650249423300213, 'validation/num_examples': 43793, 'test/accuracy': 0.9858448505401611, 'test/loss': 0.046876560896635056, 'test/mean_average_precision': 0.2534075491441924, 'test/num_examples': 43793, 'score': 6261.605680465698, 'total_duration': 9874.90622472763, 'accumulated_submission_time': 6261.605680465698, 'accumulated_eval_time': 3611.9990952014923, 'accumulated_logging_time': 0.7735686302185059}
I0209 03:51:06.608623 139871926159104 logging_writer.py:48] [19464] accumulated_eval_time=3611.999095, accumulated_logging_time=0.773569, accumulated_submission_time=6261.605680, global_step=19464, preemption_count=0, score=6261.605680, test/accuracy=0.985845, test/loss=0.046877, test/mean_average_precision=0.253408, test/num_examples=43793, total_duration=9874.906225, train/accuracy=0.991011, train/loss=0.029571, train/mean_average_precision=0.434486, validation/accuracy=0.986746, validation/loss=0.044121, validation/mean_average_precision=0.265025, validation/num_examples=43793
I0209 03:51:18.634168 139878374610688 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.05494176968932152, loss=0.03422808647155762
I0209 03:51:50.976321 139871926159104 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.05378281697630882, loss=0.03449952229857445
I0209 03:52:23.263773 139878374610688 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.06476183980703354, loss=0.03403793275356293
I0209 03:52:55.844218 139871926159104 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.0495157428085804, loss=0.033144500106573105
I0209 03:53:28.236404 139878374610688 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.07096026837825775, loss=0.030787557363510132
I0209 03:54:00.332989 139871926159104 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.05593838542699814, loss=0.03539789095520973
I0209 03:54:32.566424 139878374610688 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.09123967587947845, loss=0.03434177115559578
I0209 03:55:04.328630 139871926159104 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.06631746143102646, loss=0.03242417797446251
I0209 03:55:06.597062 140039251117888 spec.py:321] Evaluating on the training split.
I0209 03:57:08.383788 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 03:57:11.386529 140039251117888 spec.py:349] Evaluating on the test split.
I0209 03:57:14.404408 140039251117888 submission_runner.py:408] Time since start: 10242.72s, 	Step: 20208, 	{'train/accuracy': 0.9910753965377808, 'train/loss': 0.02934957481920719, 'train/mean_average_precision': 0.43785858435166203, 'validation/accuracy': 0.9867947101593018, 'validation/loss': 0.044208772480487823, 'validation/mean_average_precision': 0.26959581501836405, 'validation/num_examples': 43793, 'test/accuracy': 0.9859581589698792, 'test/loss': 0.04711531475186348, 'test/mean_average_precision': 0.2562529002068713, 'test/num_examples': 43793, 'score': 6501.562925338745, 'total_duration': 10242.721267700195, 'accumulated_submission_time': 6501.562925338745, 'accumulated_eval_time': 3739.8063855171204, 'accumulated_logging_time': 0.8040339946746826}
I0209 03:57:14.424062 139878383003392 logging_writer.py:48] [20208] accumulated_eval_time=3739.806386, accumulated_logging_time=0.804034, accumulated_submission_time=6501.562925, global_step=20208, preemption_count=0, score=6501.562925, test/accuracy=0.985958, test/loss=0.047115, test/mean_average_precision=0.256253, test/num_examples=43793, total_duration=10242.721268, train/accuracy=0.991075, train/loss=0.029350, train/mean_average_precision=0.437859, validation/accuracy=0.986795, validation/loss=0.044209, validation/mean_average_precision=0.269596, validation/num_examples=43793
I0209 03:57:44.372778 139977237178112 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.11508290469646454, loss=0.034635476768016815
I0209 03:58:16.383246 139878383003392 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.08782322704792023, loss=0.031525786966085434
I0209 03:58:48.152117 139977237178112 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.08660005033016205, loss=0.03523605316877365
I0209 03:59:20.052845 139878383003392 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.0787397101521492, loss=0.035406410694122314
I0209 03:59:51.703297 139977237178112 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.05420903488993645, loss=0.031070468947291374
I0209 04:00:23.880810 139878383003392 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.07182882726192474, loss=0.030869921669363976
I0209 04:00:55.962783 139977237178112 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.056963399052619934, loss=0.035978637635707855
I0209 04:01:14.519683 140039251117888 spec.py:321] Evaluating on the training split.
I0209 04:03:15.737207 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 04:03:19.148025 140039251117888 spec.py:349] Evaluating on the test split.
I0209 04:03:22.537225 140039251117888 submission_runner.py:408] Time since start: 10610.85s, 	Step: 20958, 	{'train/accuracy': 0.9908106327056885, 'train/loss': 0.030077271163463593, 'train/mean_average_precision': 0.407418383224706, 'validation/accuracy': 0.9868442416191101, 'validation/loss': 0.04448039084672928, 'validation/mean_average_precision': 0.266787557603906, 'validation/num_examples': 43793, 'test/accuracy': 0.9859585762023926, 'test/loss': 0.04752559959888458, 'test/mean_average_precision': 0.25231698359918087, 'test/num_examples': 43793, 'score': 6741.6271460056305, 'total_duration': 10610.854069948196, 'accumulated_submission_time': 6741.6271460056305, 'accumulated_eval_time': 3867.823861837387, 'accumulated_logging_time': 0.8348932266235352}
I0209 04:03:22.558882 139871934551808 logging_writer.py:48] [20958] accumulated_eval_time=3867.823862, accumulated_logging_time=0.834893, accumulated_submission_time=6741.627146, global_step=20958, preemption_count=0, score=6741.627146, test/accuracy=0.985959, test/loss=0.047526, test/mean_average_precision=0.252317, test/num_examples=43793, total_duration=10610.854070, train/accuracy=0.990811, train/loss=0.030077, train/mean_average_precision=0.407418, validation/accuracy=0.986844, validation/loss=0.044480, validation/mean_average_precision=0.266788, validation/num_examples=43793
I0209 04:03:36.986560 139878374610688 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.06606580317020416, loss=0.033823128789663315
I0209 04:04:10.512892 139871934551808 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.12351882457733154, loss=0.03316638991236687
I0209 04:04:43.501705 139878374610688 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.06835858523845673, loss=0.03352882340550423
I0209 04:05:16.588120 139871934551808 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.09692026674747467, loss=0.033644240349531174
I0209 04:05:49.526961 139878374610688 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.09102708846330643, loss=0.034313399344682693
I0209 04:06:22.143598 139871934551808 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.0660727322101593, loss=0.03169841319322586
I0209 04:06:53.848971 139878374610688 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.058999110013246536, loss=0.034073177725076675
I0209 04:07:22.608006 140039251117888 spec.py:321] Evaluating on the training split.
I0209 04:09:24.202845 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 04:09:27.238908 140039251117888 spec.py:349] Evaluating on the test split.
I0209 04:09:30.230906 140039251117888 submission_runner.py:408] Time since start: 10978.55s, 	Step: 21690, 	{'train/accuracy': 0.9906601905822754, 'train/loss': 0.03099336288869381, 'train/mean_average_precision': 0.4074115877835104, 'validation/accuracy': 0.9866725206375122, 'validation/loss': 0.04473439231514931, 'validation/mean_average_precision': 0.26206147107245853, 'validation/num_examples': 43793, 'test/accuracy': 0.9858292937278748, 'test/loss': 0.04747376590967178, 'test/mean_average_precision': 0.2580787181285667, 'test/num_examples': 43793, 'score': 6981.642642021179, 'total_duration': 10978.547773361206, 'accumulated_submission_time': 6981.642642021179, 'accumulated_eval_time': 3995.446721315384, 'accumulated_logging_time': 0.8683032989501953}
I0209 04:09:30.250964 139871926159104 logging_writer.py:48] [21690] accumulated_eval_time=3995.446721, accumulated_logging_time=0.868303, accumulated_submission_time=6981.642642, global_step=21690, preemption_count=0, score=6981.642642, test/accuracy=0.985829, test/loss=0.047474, test/mean_average_precision=0.258079, test/num_examples=43793, total_duration=10978.547773, train/accuracy=0.990660, train/loss=0.030993, train/mean_average_precision=0.407412, validation/accuracy=0.986673, validation/loss=0.044734, validation/mean_average_precision=0.262061, validation/num_examples=43793
I0209 04:09:33.901681 139977237178112 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.0820498913526535, loss=0.03175319358706474
I0209 04:10:06.120766 139871926159104 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.07019511610269547, loss=0.03189263120293617
I0209 04:10:37.834912 139977237178112 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.06999686360359192, loss=0.03803173080086708
I0209 04:11:10.093441 139871926159104 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.0657113716006279, loss=0.03389272838830948
I0209 04:11:42.264065 139977237178112 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.08269694447517395, loss=0.03340586647391319
I0209 04:12:14.312078 139871926159104 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.06783495843410492, loss=0.03234778717160225
I0209 04:12:46.731964 139977237178112 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.08497984707355499, loss=0.03557359054684639
I0209 04:13:18.725810 139871926159104 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.08974697440862656, loss=0.031044848263263702
I0209 04:13:30.519881 140039251117888 spec.py:321] Evaluating on the training split.
I0209 04:15:35.197689 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 04:15:38.264309 140039251117888 spec.py:349] Evaluating on the test split.
I0209 04:15:41.264628 140039251117888 submission_runner.py:408] Time since start: 11349.58s, 	Step: 22438, 	{'train/accuracy': 0.9909005165100098, 'train/loss': 0.02995622344315052, 'train/mean_average_precision': 0.3997272030494833, 'validation/accuracy': 0.986880362033844, 'validation/loss': 0.044299740344285965, 'validation/mean_average_precision': 0.2655132926619737, 'validation/num_examples': 43793, 'test/accuracy': 0.9859687089920044, 'test/loss': 0.04701925814151764, 'test/mean_average_precision': 0.25742846013030607, 'test/num_examples': 43793, 'score': 7221.879897594452, 'total_duration': 11349.581412315369, 'accumulated_submission_time': 7221.879897594452, 'accumulated_eval_time': 4126.191339492798, 'accumulated_logging_time': 0.8996272087097168}
I0209 04:15:41.283710 139878374610688 logging_writer.py:48] [22438] accumulated_eval_time=4126.191339, accumulated_logging_time=0.899627, accumulated_submission_time=7221.879898, global_step=22438, preemption_count=0, score=7221.879898, test/accuracy=0.985969, test/loss=0.047019, test/mean_average_precision=0.257428, test/num_examples=43793, total_duration=11349.581412, train/accuracy=0.990901, train/loss=0.029956, train/mean_average_precision=0.399727, validation/accuracy=0.986880, validation/loss=0.044300, validation/mean_average_precision=0.265513, validation/num_examples=43793
I0209 04:16:01.392175 139878383003392 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.06183088570833206, loss=0.030982527881860733
I0209 04:16:33.399660 139878374610688 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.06407095491886139, loss=0.034780289977788925
I0209 04:17:05.253795 139878383003392 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.07989131659269333, loss=0.03153380751609802
I0209 04:17:37.253008 139878374610688 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.07868409156799316, loss=0.031624749302864075
I0209 04:18:09.463457 139878383003392 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.06504334509372711, loss=0.03390366956591606
I0209 04:18:41.104107 139878374610688 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.07474102079868317, loss=0.033621300011873245
I0209 04:19:12.995222 139878383003392 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.08418895304203033, loss=0.02884393371641636
I0209 04:19:41.558168 140039251117888 spec.py:321] Evaluating on the training split.
I0209 04:21:41.448223 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 04:21:44.525515 140039251117888 spec.py:349] Evaluating on the test split.
I0209 04:21:47.537944 140039251117888 submission_runner.py:408] Time since start: 11715.85s, 	Step: 23191, 	{'train/accuracy': 0.990909993648529, 'train/loss': 0.030140511691570282, 'train/mean_average_precision': 0.415039702271021, 'validation/accuracy': 0.9867057800292969, 'validation/loss': 0.04418215900659561, 'validation/mean_average_precision': 0.26283213286633994, 'validation/num_examples': 43793, 'test/accuracy': 0.9859480857849121, 'test/loss': 0.04676768556237221, 'test/mean_average_precision': 0.25236883629083695, 'test/num_examples': 43793, 'score': 7462.1232216358185, 'total_duration': 11715.854789495468, 'accumulated_submission_time': 7462.1232216358185, 'accumulated_eval_time': 4252.171051979065, 'accumulated_logging_time': 0.9298944473266602}
I0209 04:21:47.558031 139871926159104 logging_writer.py:48] [23191] accumulated_eval_time=4252.171052, accumulated_logging_time=0.929894, accumulated_submission_time=7462.123222, global_step=23191, preemption_count=0, score=7462.123222, test/accuracy=0.985948, test/loss=0.046768, test/mean_average_precision=0.252369, test/num_examples=43793, total_duration=11715.854789, train/accuracy=0.990910, train/loss=0.030141, train/mean_average_precision=0.415040, validation/accuracy=0.986706, validation/loss=0.044182, validation/mean_average_precision=0.262832, validation/num_examples=43793
I0209 04:21:51.083638 139977237178112 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.06697944551706314, loss=0.030688364058732986
I0209 04:22:23.119721 139871926159104 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.0774550512433052, loss=0.03845983371138573
I0209 04:22:55.212624 139977237178112 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.07181007415056229, loss=0.03530791029334068
I0209 04:23:27.692123 139871926159104 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.06736394017934799, loss=0.03253574296832085
I0209 04:23:59.513898 139977237178112 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.10089042782783508, loss=0.0323387086391449
I0209 04:24:31.688950 139871926159104 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.08190755546092987, loss=0.03233679011464119
I0209 04:25:03.579196 139977237178112 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.0864148736000061, loss=0.03838769719004631
I0209 04:25:35.470276 139871926159104 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.06327424198389053, loss=0.03439776599407196
I0209 04:25:47.619431 140039251117888 spec.py:321] Evaluating on the training split.
I0209 04:27:51.005247 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 04:27:54.142966 140039251117888 spec.py:349] Evaluating on the test split.
I0209 04:27:57.193432 140039251117888 submission_runner.py:408] Time since start: 12085.51s, 	Step: 23939, 	{'train/accuracy': 0.9909388422966003, 'train/loss': 0.02958683855831623, 'train/mean_average_precision': 0.4346094288609707, 'validation/accuracy': 0.9869027137756348, 'validation/loss': 0.04411791265010834, 'validation/mean_average_precision': 0.2732023445683491, 'validation/num_examples': 43793, 'test/accuracy': 0.9859236478805542, 'test/loss': 0.04697485640645027, 'test/mean_average_precision': 0.2573575046521077, 'test/num_examples': 43793, 'score': 7701.862661600113, 'total_duration': 12085.510291099548, 'accumulated_submission_time': 7701.862661600113, 'accumulated_eval_time': 4381.744997501373, 'accumulated_logging_time': 1.251793384552002}
I0209 04:27:57.213058 139871934551808 logging_writer.py:48] [23939] accumulated_eval_time=4381.744998, accumulated_logging_time=1.251793, accumulated_submission_time=7701.862662, global_step=23939, preemption_count=0, score=7701.862662, test/accuracy=0.985924, test/loss=0.046975, test/mean_average_precision=0.257358, test/num_examples=43793, total_duration=12085.510291, train/accuracy=0.990939, train/loss=0.029587, train/mean_average_precision=0.434609, validation/accuracy=0.986903, validation/loss=0.044118, validation/mean_average_precision=0.273202, validation/num_examples=43793
I0209 04:28:17.019802 139878374610688 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.0888870432972908, loss=0.03475704416632652
I0209 04:28:48.713225 139871934551808 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.07003410160541534, loss=0.030316485092043877
I0209 04:29:20.568397 139878374610688 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.08360160142183304, loss=0.03190753981471062
I0209 04:29:52.391106 139871934551808 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.07108156383037567, loss=0.0337013341486454
I0209 04:30:24.440759 139878374610688 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.0831993818283081, loss=0.036527398973703384
I0209 04:30:56.289781 139871934551808 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.10334503650665283, loss=0.03436998277902603
I0209 04:31:28.237167 139878374610688 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.07373182475566864, loss=0.03429821878671646
I0209 04:31:57.328357 140039251117888 spec.py:321] Evaluating on the training split.
I0209 04:33:55.852871 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 04:33:58.931789 140039251117888 spec.py:349] Evaluating on the test split.
I0209 04:34:02.143376 140039251117888 submission_runner.py:408] Time since start: 12450.46s, 	Step: 24693, 	{'train/accuracy': 0.991054356098175, 'train/loss': 0.02922392450273037, 'train/mean_average_precision': 0.42748873190217074, 'validation/accuracy': 0.986707866191864, 'validation/loss': 0.04421435669064522, 'validation/mean_average_precision': 0.26628845857204436, 'validation/num_examples': 43793, 'test/accuracy': 0.9858819246292114, 'test/loss': 0.04682368040084839, 'test/mean_average_precision': 0.2541363107731255, 'test/num_examples': 43793, 'score': 7941.946875095367, 'total_duration': 12450.460037469864, 'accumulated_submission_time': 7941.946875095367, 'accumulated_eval_time': 4506.559768199921, 'accumulated_logging_time': 1.282576322555542}
I0209 04:34:02.164469 139878383003392 logging_writer.py:48] [24693] accumulated_eval_time=4506.559768, accumulated_logging_time=1.282576, accumulated_submission_time=7941.946875, global_step=24693, preemption_count=0, score=7941.946875, test/accuracy=0.985882, test/loss=0.046824, test/mean_average_precision=0.254136, test/num_examples=43793, total_duration=12450.460037, train/accuracy=0.991054, train/loss=0.029224, train/mean_average_precision=0.427489, validation/accuracy=0.986708, validation/loss=0.044214, validation/mean_average_precision=0.266288, validation/num_examples=43793
I0209 04:34:04.768388 139977237178112 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.07006696611642838, loss=0.03339820355176926
I0209 04:34:36.793131 139878383003392 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.07555049657821655, loss=0.03442246466875076
I0209 04:35:08.882356 139977237178112 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.07008782774209976, loss=0.03186522796750069
I0209 04:35:40.924017 139878383003392 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.08412206172943115, loss=0.034249447286129
I0209 04:36:12.840054 139977237178112 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.06794579327106476, loss=0.03252431005239487
I0209 04:36:44.903680 139878383003392 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.07819382101297379, loss=0.03385048359632492
I0209 04:37:16.881443 139977237178112 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.06806038320064545, loss=0.037711866199970245
I0209 04:37:48.741465 139878383003392 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.10933174192905426, loss=0.03439740091562271
I0209 04:38:02.239012 140039251117888 spec.py:321] Evaluating on the training split.
I0209 04:40:02.906349 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 04:40:05.976966 140039251117888 spec.py:349] Evaluating on the test split.
I0209 04:40:08.959457 140039251117888 submission_runner.py:408] Time since start: 12817.28s, 	Step: 25443, 	{'train/accuracy': 0.9912801384925842, 'train/loss': 0.02853528782725334, 'train/mean_average_precision': 0.45084242740785924, 'validation/accuracy': 0.9867947101593018, 'validation/loss': 0.044394586235284805, 'validation/mean_average_precision': 0.26479474497860095, 'validation/num_examples': 43793, 'test/accuracy': 0.98597252368927, 'test/loss': 0.04716556519269943, 'test/mean_average_precision': 0.25968966276112876, 'test/num_examples': 43793, 'score': 8181.9895396232605, 'total_duration': 12817.27622628212, 'accumulated_submission_time': 8181.9895396232605, 'accumulated_eval_time': 4633.280071258545, 'accumulated_logging_time': 1.3154983520507812}
I0209 04:40:08.979346 139871926159104 logging_writer.py:48] [25443] accumulated_eval_time=4633.280071, accumulated_logging_time=1.315498, accumulated_submission_time=8181.989540, global_step=25443, preemption_count=0, score=8181.989540, test/accuracy=0.985973, test/loss=0.047166, test/mean_average_precision=0.259690, test/num_examples=43793, total_duration=12817.276226, train/accuracy=0.991280, train/loss=0.028535, train/mean_average_precision=0.450842, validation/accuracy=0.986795, validation/loss=0.044395, validation/mean_average_precision=0.264795, validation/num_examples=43793
I0209 04:40:27.446907 139871934551808 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.11253363639116287, loss=0.03482938930392265
I0209 04:40:59.694162 139871926159104 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.0689101368188858, loss=0.03438352048397064
I0209 04:41:31.613942 139871934551808 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.1441301852464676, loss=0.03517649695277214
I0209 04:42:03.841493 139871926159104 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.07768436521291733, loss=0.030685124918818474
I0209 04:42:35.527889 139871934551808 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.07526736706495285, loss=0.031445298343896866
I0209 04:43:07.835898 139871926159104 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.07833860069513321, loss=0.032736752182245255
I0209 04:43:40.398636 139871934551808 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.10349535942077637, loss=0.03844144195318222
I0209 04:44:09.002814 140039251117888 spec.py:321] Evaluating on the training split.
I0209 04:46:07.753026 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 04:46:10.841255 140039251117888 spec.py:349] Evaluating on the test split.
I0209 04:46:13.888703 140039251117888 submission_runner.py:408] Time since start: 13182.21s, 	Step: 26189, 	{'train/accuracy': 0.9914413094520569, 'train/loss': 0.027967434376478195, 'train/mean_average_precision': 0.47034527053164865, 'validation/accuracy': 0.9867812991142273, 'validation/loss': 0.04452061280608177, 'validation/mean_average_precision': 0.2657041952537768, 'validation/num_examples': 43793, 'test/accuracy': 0.9858335256576538, 'test/loss': 0.04735587537288666, 'test/mean_average_precision': 0.2510148645408534, 'test/num_examples': 43793, 'score': 8421.981696367264, 'total_duration': 13182.20546245575, 'accumulated_submission_time': 8421.981696367264, 'accumulated_eval_time': 4758.165808677673, 'accumulated_logging_time': 1.3469769954681396}
I0209 04:46:13.908600 139878383003392 logging_writer.py:48] [26189] accumulated_eval_time=4758.165809, accumulated_logging_time=1.346977, accumulated_submission_time=8421.981696, global_step=26189, preemption_count=0, score=8421.981696, test/accuracy=0.985834, test/loss=0.047356, test/mean_average_precision=0.251015, test/num_examples=43793, total_duration=13182.205462, train/accuracy=0.991441, train/loss=0.027967, train/mean_average_precision=0.470345, validation/accuracy=0.986781, validation/loss=0.044521, validation/mean_average_precision=0.265704, validation/num_examples=43793
I0209 04:46:17.795409 139977237178112 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.07253870368003845, loss=0.030789325013756752
I0209 04:46:49.327340 139878383003392 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.0734364315867424, loss=0.03453311696648598
I0209 04:47:21.468984 139977237178112 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.08932710438966751, loss=0.035176992416381836
I0209 04:47:53.580945 139878383003392 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.06852932274341583, loss=0.03526473045349121
I0209 04:48:25.473329 139977237178112 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.10662711411714554, loss=0.03438614308834076
I0209 04:48:57.863631 139878383003392 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.10706522315740585, loss=0.03151283040642738
I0209 04:49:29.945239 139977237178112 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.08171246945858002, loss=0.03505594655871391
I0209 04:50:02.183860 139878383003392 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.08352304995059967, loss=0.03263689950108528
I0209 04:50:14.097321 140039251117888 spec.py:321] Evaluating on the training split.
I0209 04:52:16.822176 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 04:52:19.971524 140039251117888 spec.py:349] Evaluating on the test split.
I0209 04:52:22.998616 140039251117888 submission_runner.py:408] Time since start: 13551.32s, 	Step: 26938, 	{'train/accuracy': 0.9915258884429932, 'train/loss': 0.02765055000782013, 'train/mean_average_precision': 0.47607650501801335, 'validation/accuracy': 0.9869157075881958, 'validation/loss': 0.044607408344745636, 'validation/mean_average_precision': 0.2645389905862119, 'validation/num_examples': 43793, 'test/accuracy': 0.9859678745269775, 'test/loss': 0.04747622832655907, 'test/mean_average_precision': 0.2551753097398039, 'test/num_examples': 43793, 'score': 8662.139254808426, 'total_duration': 13551.315371513367, 'accumulated_submission_time': 8662.139254808426, 'accumulated_eval_time': 4887.066943645477, 'accumulated_logging_time': 1.378154993057251}
I0209 04:52:23.019403 139871926159104 logging_writer.py:48] [26938] accumulated_eval_time=4887.066944, accumulated_logging_time=1.378155, accumulated_submission_time=8662.139255, global_step=26938, preemption_count=0, score=8662.139255, test/accuracy=0.985968, test/loss=0.047476, test/mean_average_precision=0.255175, test/num_examples=43793, total_duration=13551.315372, train/accuracy=0.991526, train/loss=0.027651, train/mean_average_precision=0.476077, validation/accuracy=0.986916, validation/loss=0.044607, validation/mean_average_precision=0.264539, validation/num_examples=43793
I0209 04:52:43.579494 139878374610688 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.08066714555025101, loss=0.031389269977808
I0209 04:53:15.865442 139871926159104 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.12299840152263641, loss=0.03440302610397339
I0209 04:53:47.947295 139878374610688 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.07980910688638687, loss=0.03310474008321762
I0209 04:54:19.958243 139871926159104 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.09459658712148666, loss=0.030221080407500267
I0209 04:54:51.862284 139878374610688 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.11438281089067459, loss=0.03902370482683182
I0209 04:55:23.423185 139871926159104 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.06811999529600143, loss=0.03254543989896774
I0209 04:55:54.767529 139878374610688 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.10076305270195007, loss=0.031040487810969353
I0209 04:56:23.105826 140039251117888 spec.py:321] Evaluating on the training split.
I0209 04:58:24.714441 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 04:58:28.117167 140039251117888 spec.py:349] Evaluating on the test split.
I0209 04:58:31.446671 140039251117888 submission_runner.py:408] Time since start: 13919.76s, 	Step: 27690, 	{'train/accuracy': 0.991478443145752, 'train/loss': 0.02810160256922245, 'train/mean_average_precision': 0.46717011640930434, 'validation/accuracy': 0.9867671132087708, 'validation/loss': 0.04437020421028137, 'validation/mean_average_precision': 0.26535027311657716, 'validation/num_examples': 43793, 'test/accuracy': 0.9857825636863708, 'test/loss': 0.047222595661878586, 'test/mean_average_precision': 0.25155722622236826, 'test/num_examples': 43793, 'score': 8902.194403409958, 'total_duration': 13919.763410568237, 'accumulated_submission_time': 8902.194403409958, 'accumulated_eval_time': 5015.407616376877, 'accumulated_logging_time': 1.4100637435913086}
I0209 04:58:31.471859 139871934551808 logging_writer.py:48] [27690] accumulated_eval_time=5015.407616, accumulated_logging_time=1.410064, accumulated_submission_time=8902.194403, global_step=27690, preemption_count=0, score=8902.194403, test/accuracy=0.985783, test/loss=0.047223, test/mean_average_precision=0.251557, test/num_examples=43793, total_duration=13919.763411, train/accuracy=0.991478, train/loss=0.028102, train/mean_average_precision=0.467170, validation/accuracy=0.986767, validation/loss=0.044370, validation/mean_average_precision=0.265350, validation/num_examples=43793
I0209 04:58:35.160198 139977237178112 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.06491054594516754, loss=0.02926849015057087
I0209 04:59:08.103281 139871934551808 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.09111166000366211, loss=0.03445778414607048
I0209 04:59:40.737615 139977237178112 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.0832129567861557, loss=0.0362861193716526
I0209 05:00:13.627999 139871934551808 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.1609637290239334, loss=0.032907355576753616
I0209 05:00:46.504585 139977237178112 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.08521271497011185, loss=0.030287260189652443
I0209 05:01:19.158695 139871934551808 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.06333183497190475, loss=0.030736926943063736
I0209 05:01:51.285355 139977237178112 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.0910874754190445, loss=0.031108850613236427
I0209 05:02:23.762440 139871934551808 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.07143311947584152, loss=0.033061422407627106
I0209 05:02:31.488555 140039251117888 spec.py:321] Evaluating on the training split.
I0209 05:04:34.147639 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 05:04:37.204762 140039251117888 spec.py:349] Evaluating on the test split.
I0209 05:04:40.204129 140039251117888 submission_runner.py:408] Time since start: 14288.52s, 	Step: 28425, 	{'train/accuracy': 0.9911364912986755, 'train/loss': 0.0287772286683321, 'train/mean_average_precision': 0.4375929169048969, 'validation/accuracy': 0.9868263602256775, 'validation/loss': 0.04454055801033974, 'validation/mean_average_precision': 0.2722830766897607, 'validation/num_examples': 43793, 'test/accuracy': 0.9859636425971985, 'test/loss': 0.047429975122213364, 'test/mean_average_precision': 0.25800364118637026, 'test/num_examples': 43793, 'score': 9142.177165269852, 'total_duration': 14288.520901203156, 'accumulated_submission_time': 9142.177165269852, 'accumulated_eval_time': 5144.1230499744415, 'accumulated_logging_time': 1.4474318027496338}
I0209 05:04:40.224687 139871926159104 logging_writer.py:48] [28425] accumulated_eval_time=5144.123050, accumulated_logging_time=1.447432, accumulated_submission_time=9142.177165, global_step=28425, preemption_count=0, score=9142.177165, test/accuracy=0.985964, test/loss=0.047430, test/mean_average_precision=0.258004, test/num_examples=43793, total_duration=14288.520901, train/accuracy=0.991136, train/loss=0.028777, train/mean_average_precision=0.437593, validation/accuracy=0.986826, validation/loss=0.044541, validation/mean_average_precision=0.272283, validation/num_examples=43793
I0209 05:05:04.912403 139878374610688 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.09290695935487747, loss=0.033578451722860336
I0209 05:05:36.932159 139871926159104 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.09622776508331299, loss=0.03538662940263748
I0209 05:06:09.566809 139878374610688 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.0843357965350151, loss=0.03499547019600868
I0209 05:06:43.909051 139871926159104 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.06644488126039505, loss=0.033069007098674774
I0209 05:07:17.223857 139878374610688 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.07889027893543243, loss=0.03135541081428528
I0209 05:07:49.509754 139871926159104 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.07893374562263489, loss=0.03357567638158798
I0209 05:08:21.786096 139878374610688 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.08341061323881149, loss=0.030213965103030205
I0209 05:08:40.293398 140039251117888 spec.py:321] Evaluating on the training split.
I0209 05:10:42.016969 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 05:10:45.029561 140039251117888 spec.py:349] Evaluating on the test split.
I0209 05:10:48.044933 140039251117888 submission_runner.py:408] Time since start: 14656.36s, 	Step: 29159, 	{'train/accuracy': 0.9911613464355469, 'train/loss': 0.028942160308361053, 'train/mean_average_precision': 0.4474896226765509, 'validation/accuracy': 0.9867890477180481, 'validation/loss': 0.04471442103385925, 'validation/mean_average_precision': 0.2646365791157178, 'validation/num_examples': 43793, 'test/accuracy': 0.9859611392021179, 'test/loss': 0.04742545634508133, 'test/mean_average_precision': 0.25873416160326057, 'test/num_examples': 43793, 'score': 9382.21336388588, 'total_duration': 14656.361695289612, 'accumulated_submission_time': 9382.21336388588, 'accumulated_eval_time': 5271.874435424805, 'accumulated_logging_time': 1.4804432392120361}
I0209 05:10:48.065557 139864233322240 logging_writer.py:48] [29159] accumulated_eval_time=5271.874435, accumulated_logging_time=1.480443, accumulated_submission_time=9382.213364, global_step=29159, preemption_count=0, score=9382.213364, test/accuracy=0.985961, test/loss=0.047425, test/mean_average_precision=0.258734, test/num_examples=43793, total_duration=14656.361695, train/accuracy=0.991161, train/loss=0.028942, train/mean_average_precision=0.447490, validation/accuracy=0.986789, validation/loss=0.044714, validation/mean_average_precision=0.264637, validation/num_examples=43793
I0209 05:11:01.489589 139878383003392 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.09225022047758102, loss=0.03422456234693527
I0209 05:11:33.502564 139864233322240 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.07256930321455002, loss=0.032336924225091934
I0209 05:12:06.015762 139878383003392 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.10284584015607834, loss=0.0301350150257349
I0209 05:12:39.152179 139864233322240 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.08447510749101639, loss=0.03330080583691597
I0209 05:13:12.636348 139878383003392 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.08083974570035934, loss=0.0334041602909565
I0209 05:13:44.533740 139864233322240 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.10614028573036194, loss=0.03455115109682083
I0209 05:14:16.867876 139878383003392 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.08646029233932495, loss=0.03310030698776245
I0209 05:14:48.225661 140039251117888 spec.py:321] Evaluating on the training split.
I0209 05:16:51.920233 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 05:16:55.304237 140039251117888 spec.py:349] Evaluating on the test split.
I0209 05:16:58.665842 140039251117888 submission_runner.py:408] Time since start: 15026.98s, 	Step: 29899, 	{'train/accuracy': 0.9912360906600952, 'train/loss': 0.028651952743530273, 'train/mean_average_precision': 0.44693550007320454, 'validation/accuracy': 0.9867979288101196, 'validation/loss': 0.044605981558561325, 'validation/mean_average_precision': 0.2667757483159876, 'validation/num_examples': 43793, 'test/accuracy': 0.9859535694122314, 'test/loss': 0.04737938195466995, 'test/mean_average_precision': 0.25972637877089966, 'test/num_examples': 43793, 'score': 9622.341174840927, 'total_duration': 15026.982689142227, 'accumulated_submission_time': 9622.341174840927, 'accumulated_eval_time': 5402.314550876617, 'accumulated_logging_time': 1.5125842094421387}
I0209 05:16:58.690561 139871926159104 logging_writer.py:48] [29899] accumulated_eval_time=5402.314551, accumulated_logging_time=1.512584, accumulated_submission_time=9622.341175, global_step=29899, preemption_count=0, score=9622.341175, test/accuracy=0.985954, test/loss=0.047379, test/mean_average_precision=0.259726, test/num_examples=43793, total_duration=15026.982689, train/accuracy=0.991236, train/loss=0.028652, train/mean_average_precision=0.446936, validation/accuracy=0.986798, validation/loss=0.044606, validation/mean_average_precision=0.266776, validation/num_examples=43793
I0209 05:16:59.457505 139878374610688 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.08149135857820511, loss=0.033147044479846954
I0209 05:17:32.334347 139871926159104 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.08728776127099991, loss=0.032696839421987534
I0209 05:18:05.294955 139878374610688 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.078373983502388, loss=0.03207298368215561
I0209 05:18:38.311947 139871926159104 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.10764598101377487, loss=0.03123442269861698
I0209 05:19:10.865960 139878374610688 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.08041378855705261, loss=0.033197902143001556
I0209 05:19:43.325484 139871926159104 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.08230675011873245, loss=0.03416159003973007
I0209 05:20:16.031453 139878374610688 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.08466370403766632, loss=0.0346621498465538
I0209 05:20:48.300128 139871926159104 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.08912704139947891, loss=0.03133094683289528
I0209 05:20:58.934794 140039251117888 spec.py:321] Evaluating on the training split.
I0209 05:23:03.950606 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 05:23:06.974995 140039251117888 spec.py:349] Evaluating on the test split.
I0209 05:23:10.014104 140039251117888 submission_runner.py:408] Time since start: 15398.33s, 	Step: 30634, 	{'train/accuracy': 0.9913098812103271, 'train/loss': 0.028394658118486404, 'train/mean_average_precision': 0.4532779013963894, 'validation/accuracy': 0.9868007898330688, 'validation/loss': 0.04478731378912926, 'validation/mean_average_precision': 0.26890931603105683, 'validation/num_examples': 43793, 'test/accuracy': 0.9859750270843506, 'test/loss': 0.047733910381793976, 'test/mean_average_precision': 0.26015990846490455, 'test/num_examples': 43793, 'score': 9862.550683259964, 'total_duration': 15398.330970048904, 'accumulated_submission_time': 9862.550683259964, 'accumulated_eval_time': 5533.393811702728, 'accumulated_logging_time': 1.550180196762085}
I0209 05:23:10.035115 139864233322240 logging_writer.py:48] [30634] accumulated_eval_time=5533.393812, accumulated_logging_time=1.550180, accumulated_submission_time=9862.550683, global_step=30634, preemption_count=0, score=9862.550683, test/accuracy=0.985975, test/loss=0.047734, test/mean_average_precision=0.260160, test/num_examples=43793, total_duration=15398.330970, train/accuracy=0.991310, train/loss=0.028395, train/mean_average_precision=0.453278, validation/accuracy=0.986801, validation/loss=0.044787, validation/mean_average_precision=0.268909, validation/num_examples=43793
I0209 05:23:31.963388 139871934551808 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.09240374714136124, loss=0.032594963908195496
I0209 05:24:04.215813 139864233322240 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.07200682163238525, loss=0.032544977962970734
I0209 05:24:36.260688 139871934551808 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.0892576202750206, loss=0.033678241074085236
I0209 05:25:08.750933 139864233322240 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.06324334442615509, loss=0.02939561754465103
I0209 05:25:40.776269 139871934551808 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.08167469501495361, loss=0.03430771082639694
I0209 05:26:12.905824 139864233322240 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.10937901586294174, loss=0.02919439598917961
I0209 05:26:45.001484 139871934551808 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.08920669555664062, loss=0.033214326947927475
I0209 05:27:10.052047 140039251117888 spec.py:321] Evaluating on the training split.
I0209 05:29:10.026444 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 05:29:13.104887 140039251117888 spec.py:349] Evaluating on the test split.
I0209 05:29:16.122650 140039251117888 submission_runner.py:408] Time since start: 15764.44s, 	Step: 31379, 	{'train/accuracy': 0.9914488196372986, 'train/loss': 0.027851860970258713, 'train/mean_average_precision': 0.4698440030144065, 'validation/accuracy': 0.9867720007896423, 'validation/loss': 0.04453831538558006, 'validation/mean_average_precision': 0.27013443583727287, 'validation/num_examples': 43793, 'test/accuracy': 0.9859640598297119, 'test/loss': 0.04747891426086426, 'test/mean_average_precision': 0.2624128359043144, 'test/num_examples': 43793, 'score': 10102.536264419556, 'total_duration': 15764.439517259598, 'accumulated_submission_time': 10102.536264419556, 'accumulated_eval_time': 5659.4643721580505, 'accumulated_logging_time': 1.5825748443603516}
I0209 05:29:16.144038 139871926159104 logging_writer.py:48] [31379] accumulated_eval_time=5659.464372, accumulated_logging_time=1.582575, accumulated_submission_time=10102.536264, global_step=31379, preemption_count=0, score=10102.536264, test/accuracy=0.985964, test/loss=0.047479, test/mean_average_precision=0.262413, test/num_examples=43793, total_duration=15764.439517, train/accuracy=0.991449, train/loss=0.027852, train/mean_average_precision=0.469844, validation/accuracy=0.986772, validation/loss=0.044538, validation/mean_average_precision=0.270134, validation/num_examples=43793
I0209 05:29:23.286736 139878374610688 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.07660669833421707, loss=0.03138326108455658
I0209 05:29:55.899310 139871926159104 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.08180464059114456, loss=0.03149602934718132
I0209 05:30:28.206403 139878374610688 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.08363752067089081, loss=0.031244153156876564
I0209 05:31:00.331532 139871926159104 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.1027638167142868, loss=0.0319024920463562
I0209 05:31:32.361069 139878374610688 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.06744161993265152, loss=0.029792463406920433
I0209 05:32:04.603366 139871926159104 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.08483167737722397, loss=0.029619676992297173
I0209 05:32:36.464248 139878374610688 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.10968375951051712, loss=0.03175034746527672
I0209 05:33:08.878513 139871926159104 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.07240907847881317, loss=0.030519388616085052
I0209 05:33:16.285921 140039251117888 spec.py:321] Evaluating on the training split.
I0209 05:35:16.350744 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 05:35:19.755996 140039251117888 spec.py:349] Evaluating on the test split.
I0209 05:35:23.187175 140039251117888 submission_runner.py:408] Time since start: 16131.50s, 	Step: 32124, 	{'train/accuracy': 0.9916564226150513, 'train/loss': 0.027042893692851067, 'train/mean_average_precision': 0.49447185356203693, 'validation/accuracy': 0.9868767261505127, 'validation/loss': 0.044268108904361725, 'validation/mean_average_precision': 0.271961120654364, 'validation/num_examples': 43793, 'test/accuracy': 0.9859771132469177, 'test/loss': 0.0472414530813694, 'test/mean_average_precision': 0.2615611187014712, 'test/num_examples': 43793, 'score': 10342.64687371254, 'total_duration': 16131.504002094269, 'accumulated_submission_time': 10342.64687371254, 'accumulated_eval_time': 5786.365542411804, 'accumulated_logging_time': 1.615133285522461}
I0209 05:35:23.212389 139864233322240 logging_writer.py:48] [32124] accumulated_eval_time=5786.365542, accumulated_logging_time=1.615133, accumulated_submission_time=10342.646874, global_step=32124, preemption_count=0, score=10342.646874, test/accuracy=0.985977, test/loss=0.047241, test/mean_average_precision=0.261561, test/num_examples=43793, total_duration=16131.504002, train/accuracy=0.991656, train/loss=0.027043, train/mean_average_precision=0.494472, validation/accuracy=0.986877, validation/loss=0.044268, validation/mean_average_precision=0.271961, validation/num_examples=43793
I0209 05:35:48.493195 139871934551808 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.07069775462150574, loss=0.03148059546947479
I0209 05:36:21.628540 139864233322240 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.09097920358181, loss=0.03242934122681618
I0209 05:36:54.590420 139871934551808 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.08378320932388306, loss=0.03154890984296799
I0209 05:37:27.533151 139864233322240 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.11714117228984833, loss=0.031043145805597305
I0209 05:38:00.132342 139871934551808 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.0951283723115921, loss=0.032586075365543365
I0209 05:38:33.204030 139864233322240 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.10260408371686935, loss=0.035833731293678284
I0209 05:39:05.857128 139871934551808 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.10194074362516403, loss=0.03165154159069061
I0209 05:39:23.316885 140039251117888 spec.py:321] Evaluating on the training split.
I0209 05:41:19.728482 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 05:41:22.815243 140039251117888 spec.py:349] Evaluating on the test split.
I0209 05:41:25.816854 140039251117888 submission_runner.py:408] Time since start: 16494.13s, 	Step: 32856, 	{'train/accuracy': 0.99176424741745, 'train/loss': 0.026609620079398155, 'train/mean_average_precision': 0.49882654041636504, 'validation/accuracy': 0.9868531823158264, 'validation/loss': 0.04436594992876053, 'validation/mean_average_precision': 0.2692807413861312, 'validation/num_examples': 43793, 'test/accuracy': 0.9860023856163025, 'test/loss': 0.04726454243063927, 'test/mean_average_precision': 0.2620336225403224, 'test/num_examples': 43793, 'score': 10582.716272592545, 'total_duration': 16494.133712530136, 'accumulated_submission_time': 10582.716272592545, 'accumulated_eval_time': 5908.865457057953, 'accumulated_logging_time': 1.652513027191162}
I0209 05:41:25.838669 139878374610688 logging_writer.py:48] [32856] accumulated_eval_time=5908.865457, accumulated_logging_time=1.652513, accumulated_submission_time=10582.716273, global_step=32856, preemption_count=0, score=10582.716273, test/accuracy=0.986002, test/loss=0.047265, test/mean_average_precision=0.262034, test/num_examples=43793, total_duration=16494.133713, train/accuracy=0.991764, train/loss=0.026610, train/mean_average_precision=0.498827, validation/accuracy=0.986853, validation/loss=0.044366, validation/mean_average_precision=0.269281, validation/num_examples=43793
I0209 05:41:41.954776 139878383003392 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.14883777499198914, loss=0.030562017112970352
I0209 05:42:15.365382 139878374610688 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.07860896736383438, loss=0.03177458420395851
I0209 05:42:47.127641 139878383003392 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.07021263241767883, loss=0.032707955688238144
I0209 05:43:19.373031 139878374610688 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.0869537889957428, loss=0.03067314624786377
I0209 05:43:51.415544 139878383003392 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.08044098317623138, loss=0.03215609863400459
I0209 05:44:24.021553 139878374610688 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.1240481361746788, loss=0.03200120851397514
I0209 05:44:55.692313 139878383003392 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.08604605495929718, loss=0.033515363931655884
I0209 05:45:25.953813 140039251117888 spec.py:321] Evaluating on the training split.
I0209 05:47:26.687742 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 05:47:30.805966 140039251117888 spec.py:349] Evaluating on the test split.
I0209 05:47:33.777737 140039251117888 submission_runner.py:408] Time since start: 16862.09s, 	Step: 33595, 	{'train/accuracy': 0.9918482303619385, 'train/loss': 0.026468100026249886, 'train/mean_average_precision': 0.5026169864173307, 'validation/accuracy': 0.9868466854095459, 'validation/loss': 0.04508126527070999, 'validation/mean_average_precision': 0.2694701894953767, 'validation/num_examples': 43793, 'test/accuracy': 0.9859889149665833, 'test/loss': 0.0477740578353405, 'test/mean_average_precision': 0.26349125057822476, 'test/num_examples': 43793, 'score': 10822.800383806229, 'total_duration': 16862.094605207443, 'accumulated_submission_time': 10822.800383806229, 'accumulated_eval_time': 6036.689339399338, 'accumulated_logging_time': 1.6855049133300781}
I0209 05:47:33.800154 139864233322240 logging_writer.py:48] [33595] accumulated_eval_time=6036.689339, accumulated_logging_time=1.685505, accumulated_submission_time=10822.800384, global_step=33595, preemption_count=0, score=10822.800384, test/accuracy=0.985989, test/loss=0.047774, test/mean_average_precision=0.263491, test/num_examples=43793, total_duration=16862.094605, train/accuracy=0.991848, train/loss=0.026468, train/mean_average_precision=0.502617, validation/accuracy=0.986847, validation/loss=0.045081, validation/mean_average_precision=0.269470, validation/num_examples=43793
I0209 05:47:35.818131 139871926159104 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.17477017641067505, loss=0.03457018360495567
I0209 05:48:08.540263 139864233322240 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.11718689650297165, loss=0.030328480526804924
I0209 05:48:40.898180 139871926159104 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.18238869309425354, loss=0.02892976440489292
I0209 05:49:13.632950 139864233322240 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.08397075533866882, loss=0.03343834728002548
I0209 05:49:46.105516 139871926159104 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.0845060870051384, loss=0.03134705871343613
I0209 05:50:18.293871 139864233322240 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.10473918169736862, loss=0.037417564541101456
I0209 05:50:50.700392 139871926159104 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.08419282734394073, loss=0.02963634394109249
I0209 05:51:22.875454 139864233322240 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.09023283421993256, loss=0.0333552286028862
I0209 05:51:33.925758 140039251117888 spec.py:321] Evaluating on the training split.
I0209 05:53:36.310295 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 05:53:39.361664 140039251117888 spec.py:349] Evaluating on the test split.
I0209 05:53:42.371374 140039251117888 submission_runner.py:408] Time since start: 17230.69s, 	Step: 34335, 	{'train/accuracy': 0.991648256778717, 'train/loss': 0.027057519182562828, 'train/mean_average_precision': 0.4754191822452325, 'validation/accuracy': 0.9869412779808044, 'validation/loss': 0.04477310925722122, 'validation/mean_average_precision': 0.2740292724154801, 'validation/num_examples': 43793, 'test/accuracy': 0.9860196709632874, 'test/loss': 0.04748547822237015, 'test/mean_average_precision': 0.25978952719180953, 'test/num_examples': 43793, 'score': 11062.89523434639, 'total_duration': 17230.688241004944, 'accumulated_submission_time': 11062.89523434639, 'accumulated_eval_time': 6165.134907007217, 'accumulated_logging_time': 1.7191433906555176}
I0209 05:53:42.394119 139871934551808 logging_writer.py:48] [34335] accumulated_eval_time=6165.134907, accumulated_logging_time=1.719143, accumulated_submission_time=11062.895234, global_step=34335, preemption_count=0, score=11062.895234, test/accuracy=0.986020, test/loss=0.047485, test/mean_average_precision=0.259790, test/num_examples=43793, total_duration=17230.688241, train/accuracy=0.991648, train/loss=0.027058, train/mean_average_precision=0.475419, validation/accuracy=0.986941, validation/loss=0.044773, validation/mean_average_precision=0.274029, validation/num_examples=43793
I0209 05:54:03.589324 139878383003392 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.09176027029752731, loss=0.03587542846798897
I0209 05:54:35.744843 139871934551808 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.0791030302643776, loss=0.031031742691993713
I0209 05:55:08.016033 139878383003392 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.08026280999183655, loss=0.029569467529654503
I0209 05:55:39.541238 139871934551808 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.09795394539833069, loss=0.03339306637644768
I0209 05:56:11.777416 139878383003392 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.0773325264453888, loss=0.030990639701485634
I0209 05:56:43.886241 139871934551808 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.1309499442577362, loss=0.033814746886491776
I0209 05:57:16.041411 139878383003392 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.11300431191921234, loss=0.03287035599350929
I0209 05:57:42.466454 140039251117888 spec.py:321] Evaluating on the training split.
I0209 05:59:44.740517 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 05:59:47.838248 140039251117888 spec.py:349] Evaluating on the test split.
I0209 05:59:50.894539 140039251117888 submission_runner.py:408] Time since start: 17599.21s, 	Step: 35083, 	{'train/accuracy': 0.9914780855178833, 'train/loss': 0.02767196297645569, 'train/mean_average_precision': 0.4771699893653773, 'validation/accuracy': 0.9869469404220581, 'validation/loss': 0.044476427137851715, 'validation/mean_average_precision': 0.2762864765832121, 'validation/num_examples': 43793, 'test/accuracy': 0.9860441088676453, 'test/loss': 0.047148123383522034, 'test/mean_average_precision': 0.26597520521896023, 'test/num_examples': 43793, 'score': 11302.936974048615, 'total_duration': 17599.211398363113, 'accumulated_submission_time': 11302.936974048615, 'accumulated_eval_time': 6293.562938928604, 'accumulated_logging_time': 1.7529196739196777}
I0209 05:59:50.917493 139864233322240 logging_writer.py:48] [35083] accumulated_eval_time=6293.562939, accumulated_logging_time=1.752920, accumulated_submission_time=11302.936974, global_step=35083, preemption_count=0, score=11302.936974, test/accuracy=0.986044, test/loss=0.047148, test/mean_average_precision=0.265975, test/num_examples=43793, total_duration=17599.211398, train/accuracy=0.991478, train/loss=0.027672, train/mean_average_precision=0.477170, validation/accuracy=0.986947, validation/loss=0.044476, validation/mean_average_precision=0.276286, validation/num_examples=43793
I0209 05:59:56.612192 139878374610688 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.1016298457980156, loss=0.03552406281232834
I0209 06:00:28.964280 139864233322240 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.15240365266799927, loss=0.03096006251871586
I0209 06:01:00.917481 139878374610688 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.07605812698602676, loss=0.03097977675497532
I0209 06:01:33.156247 139864233322240 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.1398799568414688, loss=0.03399781882762909
I0209 06:02:05.075409 139878374610688 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.09135258942842484, loss=0.031035887077450752
I0209 06:02:37.159409 139864233322240 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.09080369025468826, loss=0.031125688925385475
I0209 06:03:09.298260 139878374610688 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.10461072623729706, loss=0.031645774841308594
I0209 06:03:41.415250 139864233322240 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.1016601175069809, loss=0.031351346522569656
I0209 06:03:51.223073 140039251117888 spec.py:321] Evaluating on the training split.
I0209 06:05:53.671844 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 06:05:56.748655 140039251117888 spec.py:349] Evaluating on the test split.
I0209 06:05:59.858173 140039251117888 submission_runner.py:408] Time since start: 17968.18s, 	Step: 35831, 	{'train/accuracy': 0.9915109276771545, 'train/loss': 0.027474919334053993, 'train/mean_average_precision': 0.4855648311323703, 'validation/accuracy': 0.9869274497032166, 'validation/loss': 0.04452328383922577, 'validation/mean_average_precision': 0.2701972526758367, 'validation/num_examples': 43793, 'test/accuracy': 0.9860904216766357, 'test/loss': 0.04754072427749634, 'test/mean_average_precision': 0.2623007088811127, 'test/num_examples': 43793, 'score': 11543.209511995316, 'total_duration': 17968.17503976822, 'accumulated_submission_time': 11543.209511995316, 'accumulated_eval_time': 6422.1980040073395, 'accumulated_logging_time': 1.7894747257232666}
I0209 06:05:59.881191 139871934551808 logging_writer.py:48] [35831] accumulated_eval_time=6422.198004, accumulated_logging_time=1.789475, accumulated_submission_time=11543.209512, global_step=35831, preemption_count=0, score=11543.209512, test/accuracy=0.986090, test/loss=0.047541, test/mean_average_precision=0.262301, test/num_examples=43793, total_duration=17968.175040, train/accuracy=0.991511, train/loss=0.027475, train/mean_average_precision=0.485565, validation/accuracy=0.986927, validation/loss=0.044523, validation/mean_average_precision=0.270197, validation/num_examples=43793
I0209 06:06:22.782207 139878383003392 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.09600060433149338, loss=0.03191203996539116
I0209 06:06:55.501029 139871934551808 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.10839194059371948, loss=0.032625503838062286
I0209 06:07:27.566039 139878383003392 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.08484171330928802, loss=0.030729463323950768
I0209 06:07:59.455926 139871934551808 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.13906200230121613, loss=0.029416438192129135
I0209 06:08:31.761260 139878383003392 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.1563565582036972, loss=0.03303198888897896
I0209 06:09:03.811183 139871934551808 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.07549832761287689, loss=0.031147249042987823
I0209 06:09:35.733753 139878383003392 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.10482101887464523, loss=0.03409764915704727
I0209 06:09:59.906568 140039251117888 spec.py:321] Evaluating on the training split.
I0209 06:12:00.862186 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 06:12:04.104364 140039251117888 spec.py:349] Evaluating on the test split.
I0209 06:12:07.129272 140039251117888 submission_runner.py:408] Time since start: 18335.45s, 	Step: 36577, 	{'train/accuracy': 0.9916188716888428, 'train/loss': 0.027239065617322922, 'train/mean_average_precision': 0.48032752663675593, 'validation/accuracy': 0.9868852496147156, 'validation/loss': 0.044582299888134, 'validation/mean_average_precision': 0.271263958619182, 'validation/num_examples': 43793, 'test/accuracy': 0.9860007166862488, 'test/loss': 0.04743095487356186, 'test/mean_average_precision': 0.2636249846278241, 'test/num_examples': 43793, 'score': 11783.2023396492, 'total_duration': 18335.44613981247, 'accumulated_submission_time': 11783.2023396492, 'accumulated_eval_time': 6549.4206647872925, 'accumulated_logging_time': 1.8252544403076172}
I0209 06:12:07.151870 139864233322240 logging_writer.py:48] [36577] accumulated_eval_time=6549.420665, accumulated_logging_time=1.825254, accumulated_submission_time=11783.202340, global_step=36577, preemption_count=0, score=11783.202340, test/accuracy=0.986001, test/loss=0.047431, test/mean_average_precision=0.263625, test/num_examples=43793, total_duration=18335.446140, train/accuracy=0.991619, train/loss=0.027239, train/mean_average_precision=0.480328, validation/accuracy=0.986885, validation/loss=0.044582, validation/mean_average_precision=0.271264, validation/num_examples=43793
I0209 06:12:15.069788 139878374610688 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.09466017782688141, loss=0.03131295368075371
I0209 06:12:47.224878 139864233322240 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.0778530165553093, loss=0.029670776799321175
I0209 06:13:19.116306 139878374610688 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.08964186906814575, loss=0.03084350936114788
I0209 06:13:51.362378 139864233322240 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.08820485323667526, loss=0.032113563269376755
I0209 06:14:23.646553 139878374610688 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.1082925796508789, loss=0.03054811619222164
I0209 06:14:55.441129 139864233322240 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.10351159423589706, loss=0.029382096603512764
I0209 06:15:27.832921 139878374610688 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.07865753024816513, loss=0.029939712956547737
I0209 06:16:00.085005 139864233322240 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.11282072216272354, loss=0.031135553494095802
I0209 06:16:07.308901 140039251117888 spec.py:321] Evaluating on the training split.
I0209 06:18:07.860388 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 06:18:10.946970 140039251117888 spec.py:349] Evaluating on the test split.
I0209 06:18:13.962920 140039251117888 submission_runner.py:408] Time since start: 18702.28s, 	Step: 37323, 	{'train/accuracy': 0.9916990995407104, 'train/loss': 0.026861457154154778, 'train/mean_average_precision': 0.48426314836571605, 'validation/accuracy': 0.9868641495704651, 'validation/loss': 0.044839050620794296, 'validation/mean_average_precision': 0.2684500922044856, 'validation/num_examples': 43793, 'test/accuracy': 0.9859986305236816, 'test/loss': 0.04797624424099922, 'test/mean_average_precision': 0.2591521938112194, 'test/num_examples': 43793, 'score': 12023.32865190506, 'total_duration': 18702.27978849411, 'accumulated_submission_time': 12023.32865190506, 'accumulated_eval_time': 6676.074639558792, 'accumulated_logging_time': 1.858780860900879}
I0209 06:18:13.985683 139871926159104 logging_writer.py:48] [37323] accumulated_eval_time=6676.074640, accumulated_logging_time=1.858781, accumulated_submission_time=12023.328652, global_step=37323, preemption_count=0, score=12023.328652, test/accuracy=0.985999, test/loss=0.047976, test/mean_average_precision=0.259152, test/num_examples=43793, total_duration=18702.279788, train/accuracy=0.991699, train/loss=0.026861, train/mean_average_precision=0.484263, validation/accuracy=0.986864, validation/loss=0.044839, validation/mean_average_precision=0.268450, validation/num_examples=43793
I0209 06:18:38.982335 139871934551808 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.08871914446353912, loss=0.028805503621697426
I0209 06:19:11.587192 139871926159104 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.0944136530160904, loss=0.03103446774184704
I0209 06:19:43.394366 139871934551808 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.07777060568332672, loss=0.029602907598018646
I0209 06:20:15.275655 139871926159104 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.08124994486570358, loss=0.029080502688884735
I0209 06:20:47.262503 139871934551808 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.10844507068395615, loss=0.0320577472448349
I0209 06:21:19.295907 139871926159104 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.09315074980258942, loss=0.029546191915869713
I0209 06:21:51.366120 139871934551808 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.08640590310096741, loss=0.02909456379711628
I0209 06:22:14.271776 140039251117888 spec.py:321] Evaluating on the training split.
I0209 06:24:14.406758 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 06:24:17.442990 140039251117888 spec.py:349] Evaluating on the test split.
I0209 06:24:20.428623 140039251117888 submission_runner.py:408] Time since start: 19068.75s, 	Step: 38072, 	{'train/accuracy': 0.9920689463615417, 'train/loss': 0.02564818225800991, 'train/mean_average_precision': 0.516311374792447, 'validation/accuracy': 0.9868032336235046, 'validation/loss': 0.04461022838950157, 'validation/mean_average_precision': 0.2661784733789995, 'validation/num_examples': 43793, 'test/accuracy': 0.9859472513198853, 'test/loss': 0.04742193594574928, 'test/mean_average_precision': 0.2636671236942603, 'test/num_examples': 43793, 'score': 12263.584111452103, 'total_duration': 19068.745487689972, 'accumulated_submission_time': 12263.584111452103, 'accumulated_eval_time': 6802.231438398361, 'accumulated_logging_time': 1.8928401470184326}
I0209 06:24:20.451112 139864233322240 logging_writer.py:48] [38072] accumulated_eval_time=6802.231438, accumulated_logging_time=1.892840, accumulated_submission_time=12263.584111, global_step=38072, preemption_count=0, score=12263.584111, test/accuracy=0.985947, test/loss=0.047422, test/mean_average_precision=0.263667, test/num_examples=43793, total_duration=19068.745488, train/accuracy=0.992069, train/loss=0.025648, train/mean_average_precision=0.516311, validation/accuracy=0.986803, validation/loss=0.044610, validation/mean_average_precision=0.266178, validation/num_examples=43793
I0209 06:24:29.929461 139878374610688 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.07929103821516037, loss=0.027849815785884857
I0209 06:25:02.148365 139864233322240 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.1093810498714447, loss=0.029195768758654594
I0209 06:25:33.968819 139878374610688 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.10709631443023682, loss=0.03330010175704956
I0209 06:26:06.309876 139864233322240 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.10022392123937607, loss=0.027337484061717987
I0209 06:26:38.408446 139878374610688 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.10637035965919495, loss=0.030625076964497566
I0209 06:27:10.453761 139864233322240 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.07785975933074951, loss=0.028797730803489685
I0209 06:27:42.156792 139878374610688 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.0919039398431778, loss=0.03459152951836586
I0209 06:28:14.254612 139864233322240 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.07734396308660507, loss=0.030135679990053177
I0209 06:28:20.543863 140039251117888 spec.py:321] Evaluating on the training split.
I0209 06:30:20.667744 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 06:30:23.732448 140039251117888 spec.py:349] Evaluating on the test split.
I0209 06:30:26.741373 140039251117888 submission_runner.py:408] Time since start: 19435.06s, 	Step: 38821, 	{'train/accuracy': 0.9921510815620422, 'train/loss': 0.025551440194249153, 'train/mean_average_precision': 0.5158998215716353, 'validation/accuracy': 0.9867951273918152, 'validation/loss': 0.044609975069761276, 'validation/mean_average_precision': 0.27445774012902946, 'validation/num_examples': 43793, 'test/accuracy': 0.9859569072723389, 'test/loss': 0.04757148399949074, 'test/mean_average_precision': 0.26158094714116725, 'test/num_examples': 43793, 'score': 12503.646212339401, 'total_duration': 19435.058240175247, 'accumulated_submission_time': 12503.646212339401, 'accumulated_eval_time': 6928.428899765015, 'accumulated_logging_time': 1.9264566898345947}
I0209 06:30:26.763921 139871934551808 logging_writer.py:48] [38821] accumulated_eval_time=6928.428900, accumulated_logging_time=1.926457, accumulated_submission_time=12503.646212, global_step=38821, preemption_count=0, score=12503.646212, test/accuracy=0.985957, test/loss=0.047571, test/mean_average_precision=0.261581, test/num_examples=43793, total_duration=19435.058240, train/accuracy=0.992151, train/loss=0.025551, train/mean_average_precision=0.515900, validation/accuracy=0.986795, validation/loss=0.044610, validation/mean_average_precision=0.274458, validation/num_examples=43793
I0209 06:30:52.224050 139878383003392 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.09221483021974564, loss=0.03139123693108559
I0209 06:31:24.194257 139871934551808 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.09289223700761795, loss=0.028473911806941032
I0209 06:31:55.685151 139878383003392 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.08378373831510544, loss=0.028286583721637726
I0209 06:32:27.523313 139871934551808 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.11537705361843109, loss=0.03195132687687874
I0209 06:32:59.409590 139878383003392 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.09414336830377579, loss=0.030360255390405655
I0209 06:33:31.330692 139871934551808 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.09657023847103119, loss=0.03134173899888992
I0209 06:34:03.656683 139878383003392 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.09467583149671555, loss=0.030642934143543243
I0209 06:34:27.009076 140039251117888 spec.py:321] Evaluating on the training split.
I0209 06:36:29.206964 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 06:36:32.294610 140039251117888 spec.py:349] Evaluating on the test split.
I0209 06:36:35.342274 140039251117888 submission_runner.py:408] Time since start: 19803.66s, 	Step: 39573, 	{'train/accuracy': 0.990967333316803, 'train/loss': 0.029311208054423332, 'train/mean_average_precision': 0.45690423259645163, 'validation/accuracy': 0.986046552658081, 'validation/loss': 0.04767151176929474, 'validation/mean_average_precision': 0.233260071570542, 'validation/num_examples': 43793, 'test/accuracy': 0.9850488305091858, 'test/loss': 0.051080767065286636, 'test/mean_average_precision': 0.2129662973348563, 'test/num_examples': 43793, 'score': 12743.860652923584, 'total_duration': 19803.659139871597, 'accumulated_submission_time': 12743.860652923584, 'accumulated_eval_time': 7056.762053728104, 'accumulated_logging_time': 1.9602704048156738}
I0209 06:36:35.366258 139864233322240 logging_writer.py:48] [39573] accumulated_eval_time=7056.762054, accumulated_logging_time=1.960270, accumulated_submission_time=12743.860653, global_step=39573, preemption_count=0, score=12743.860653, test/accuracy=0.985049, test/loss=0.051081, test/mean_average_precision=0.212966, test/num_examples=43793, total_duration=19803.659140, train/accuracy=0.990967, train/loss=0.029311, train/mean_average_precision=0.456904, validation/accuracy=0.986047, validation/loss=0.047672, validation/mean_average_precision=0.233260, validation/num_examples=43793
I0209 06:36:44.434384 139871926159104 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.15209311246871948, loss=0.034922901540994644
I0209 06:37:16.520581 139864233322240 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.10895152390003204, loss=0.033491771668195724
I0209 06:37:48.498140 139871926159104 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.1099405512213707, loss=0.03218354284763336
I0209 06:38:20.974912 139864233322240 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.11021693795919418, loss=0.031991537660360336
I0209 06:38:52.756158 139871926159104 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.10318920761346817, loss=0.03003762848675251
I0209 06:39:25.375904 139864233322240 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.08604337275028229, loss=0.028019631281495094
I0209 06:39:57.332258 139871926159104 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.0966000109910965, loss=0.027602437883615494
I0209 06:40:29.217918 139864233322240 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.1535172462463379, loss=0.03239613398909569
I0209 06:40:35.478202 140039251117888 spec.py:321] Evaluating on the training split.
I0209 06:42:33.258216 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 06:42:36.322854 140039251117888 spec.py:349] Evaluating on the test split.
I0209 06:42:39.381873 140039251117888 submission_runner.py:408] Time since start: 20167.70s, 	Step: 40320, 	{'train/accuracy': 0.9924874901771545, 'train/loss': 0.024460695683956146, 'train/mean_average_precision': 0.5351341203817217, 'validation/accuracy': 0.9868787527084351, 'validation/loss': 0.04459439218044281, 'validation/mean_average_precision': 0.2759908697710495, 'validation/num_examples': 43793, 'test/accuracy': 0.9859982132911682, 'test/loss': 0.0479002371430397, 'test/mean_average_precision': 0.26398226322551643, 'test/num_examples': 43793, 'score': 12983.941451787949, 'total_duration': 20167.698737859726, 'accumulated_submission_time': 12983.941451787949, 'accumulated_eval_time': 7180.665674209595, 'accumulated_logging_time': 1.9954063892364502}
I0209 06:42:39.406366 139871934551808 logging_writer.py:48] [40320] accumulated_eval_time=7180.665674, accumulated_logging_time=1.995406, accumulated_submission_time=12983.941452, global_step=40320, preemption_count=0, score=12983.941452, test/accuracy=0.985998, test/loss=0.047900, test/mean_average_precision=0.263982, test/num_examples=43793, total_duration=20167.698738, train/accuracy=0.992487, train/loss=0.024461, train/mean_average_precision=0.535134, validation/accuracy=0.986879, validation/loss=0.044594, validation/mean_average_precision=0.275991, validation/num_examples=43793
I0209 06:43:05.725612 139878383003392 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.15976457297801971, loss=0.03358243778347969
I0209 06:43:38.331374 139871934551808 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.10909920930862427, loss=0.031640905886888504
I0209 06:44:10.529330 139878383003392 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.09794101119041443, loss=0.03033231571316719
I0209 06:44:42.282387 139871934551808 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.10681837052106857, loss=0.033005885779857635
I0209 06:45:14.385063 139878383003392 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.12360038608312607, loss=0.029559463262557983
I0209 06:45:46.450222 139871934551808 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.13560743629932404, loss=0.03191699832677841
I0209 06:46:18.680993 139878383003392 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.11948231607675552, loss=0.032188281416893005
I0209 06:46:39.579272 140039251117888 spec.py:321] Evaluating on the training split.
I0209 06:48:33.315714 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 06:48:36.421847 140039251117888 spec.py:349] Evaluating on the test split.
I0209 06:48:39.460510 140039251117888 submission_runner.py:408] Time since start: 20527.78s, 	Step: 41067, 	{'train/accuracy': 0.9923626780509949, 'train/loss': 0.02495507150888443, 'train/mean_average_precision': 0.5233125303946242, 'validation/accuracy': 0.9868706464767456, 'validation/loss': 0.044303104281425476, 'validation/mean_average_precision': 0.2764290908345192, 'validation/num_examples': 43793, 'test/accuracy': 0.9859969019889832, 'test/loss': 0.04726998880505562, 'test/mean_average_precision': 0.26029674691171534, 'test/num_examples': 43793, 'score': 13224.08312869072, 'total_duration': 20527.777365922928, 'accumulated_submission_time': 13224.08312869072, 'accumulated_eval_time': 7300.5468554496765, 'accumulated_logging_time': 2.0311975479125977}
I0209 06:48:39.484032 139864233322240 logging_writer.py:48] [41067] accumulated_eval_time=7300.546855, accumulated_logging_time=2.031198, accumulated_submission_time=13224.083129, global_step=41067, preemption_count=0, score=13224.083129, test/accuracy=0.985997, test/loss=0.047270, test/mean_average_precision=0.260297, test/num_examples=43793, total_duration=20527.777366, train/accuracy=0.992363, train/loss=0.024955, train/mean_average_precision=0.523313, validation/accuracy=0.986871, validation/loss=0.044303, validation/mean_average_precision=0.276429, validation/num_examples=43793
I0209 06:48:50.636467 139871926159104 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.10479509830474854, loss=0.030249401926994324
I0209 06:49:22.881835 139864233322240 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.09719651192426682, loss=0.030270898714661598
I0209 06:49:54.895221 139871926159104 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.09374228119850159, loss=0.03190358355641365
I0209 06:50:27.180542 139864233322240 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.10241655260324478, loss=0.02806960605084896
I0209 06:50:59.480471 139871926159104 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.138552725315094, loss=0.030549684539437294
I0209 06:51:31.566479 139864233322240 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.07400315999984741, loss=0.02753358893096447
I0209 06:52:03.423516 139871926159104 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.1027609258890152, loss=0.029685109853744507
I0209 06:52:35.796840 139864233322240 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.1117878183722496, loss=0.030664043501019478
I0209 06:52:39.698342 140039251117888 spec.py:321] Evaluating on the training split.
I0209 06:54:39.178578 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 06:54:42.271366 140039251117888 spec.py:349] Evaluating on the test split.
I0209 06:54:45.355321 140039251117888 submission_runner.py:408] Time since start: 20893.67s, 	Step: 41813, 	{'train/accuracy': 0.9921444058418274, 'train/loss': 0.02528195083141327, 'train/mean_average_precision': 0.528187037457823, 'validation/accuracy': 0.9869668483734131, 'validation/loss': 0.044966407120227814, 'validation/mean_average_precision': 0.2712547909511459, 'validation/num_examples': 43793, 'test/accuracy': 0.9860803484916687, 'test/loss': 0.047967229038476944, 'test/mean_average_precision': 0.26475872867935635, 'test/num_examples': 43793, 'score': 13464.266607761383, 'total_duration': 20893.67218565941, 'accumulated_submission_time': 13464.266607761383, 'accumulated_eval_time': 7426.20378446579, 'accumulated_logging_time': 2.0658912658691406}
I0209 06:54:45.379007 139878374610688 logging_writer.py:48] [41813] accumulated_eval_time=7426.203784, accumulated_logging_time=2.065891, accumulated_submission_time=13464.266608, global_step=41813, preemption_count=0, score=13464.266608, test/accuracy=0.986080, test/loss=0.047967, test/mean_average_precision=0.264759, test/num_examples=43793, total_duration=20893.672186, train/accuracy=0.992144, train/loss=0.025282, train/mean_average_precision=0.528187, validation/accuracy=0.986967, validation/loss=0.044966, validation/mean_average_precision=0.271255, validation/num_examples=43793
I0209 06:55:14.296339 139878383003392 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.10425672680139542, loss=0.03227419778704643
I0209 06:55:47.134602 139878374610688 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.10159751772880554, loss=0.02721545845270157
I0209 06:56:19.452947 139878383003392 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.10910817980766296, loss=0.030122317373752594
I0209 06:56:52.069937 139878374610688 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.16289709508419037, loss=0.033306658267974854
I0209 06:57:24.340338 139878383003392 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.10599129647016525, loss=0.029940230771899223
I0209 06:57:56.625869 139878374610688 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.1347486674785614, loss=0.032873451709747314
I0209 06:58:29.293889 139878383003392 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.09790219366550446, loss=0.02877543307840824
I0209 06:58:45.502121 140039251117888 spec.py:321] Evaluating on the training split.
I0209 07:00:43.597894 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 07:00:46.718557 140039251117888 spec.py:349] Evaluating on the test split.
I0209 07:00:49.780496 140039251117888 submission_runner.py:408] Time since start: 21258.10s, 	Step: 42551, 	{'train/accuracy': 0.9920623898506165, 'train/loss': 0.02556413970887661, 'train/mean_average_precision': 0.5207061522784611, 'validation/accuracy': 0.9869310855865479, 'validation/loss': 0.04514552652835846, 'validation/mean_average_precision': 0.2757223796749376, 'validation/num_examples': 43793, 'test/accuracy': 0.9861013889312744, 'test/loss': 0.0479997955262661, 'test/mean_average_precision': 0.26192487437292206, 'test/num_examples': 43793, 'score': 13704.357329368591, 'total_duration': 21258.0973508358, 'accumulated_submission_time': 13704.357329368591, 'accumulated_eval_time': 7550.482100486755, 'accumulated_logging_time': 2.1024575233459473}
I0209 07:00:49.804949 139864233322240 logging_writer.py:48] [42551] accumulated_eval_time=7550.482100, accumulated_logging_time=2.102458, accumulated_submission_time=13704.357329, global_step=42551, preemption_count=0, score=13704.357329, test/accuracy=0.986101, test/loss=0.048000, test/mean_average_precision=0.261925, test/num_examples=43793, total_duration=21258.097351, train/accuracy=0.992062, train/loss=0.025564, train/mean_average_precision=0.520706, validation/accuracy=0.986931, validation/loss=0.045146, validation/mean_average_precision=0.275722, validation/num_examples=43793
I0209 07:01:06.395815 139871934551808 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.11724497377872467, loss=0.028124749660491943
I0209 07:01:38.882856 139864233322240 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.09967520087957382, loss=0.0272348802536726
I0209 07:02:11.326569 139871934551808 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.08953499794006348, loss=0.029369758442044258
I0209 07:02:43.831489 139864233322240 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.11990991979837418, loss=0.02711639553308487
I0209 07:03:15.961772 139871934551808 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.10284500569105148, loss=0.03099818341434002
I0209 07:03:48.109781 139864233322240 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.10897327959537506, loss=0.030454471707344055
I0209 07:04:20.187262 139871934551808 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.09429079294204712, loss=0.0288163460791111
I0209 07:04:49.784569 140039251117888 spec.py:321] Evaluating on the training split.
I0209 07:06:49.780078 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 07:06:52.898426 140039251117888 spec.py:349] Evaluating on the test split.
I0209 07:06:55.989157 140039251117888 submission_runner.py:408] Time since start: 21624.31s, 	Step: 43293, 	{'train/accuracy': 0.9919952154159546, 'train/loss': 0.025652771815657616, 'train/mean_average_precision': 0.5196389960976198, 'validation/accuracy': 0.9868032336235046, 'validation/loss': 0.0451744943857193, 'validation/mean_average_precision': 0.27337764480607746, 'validation/num_examples': 43793, 'test/accuracy': 0.9859657287597656, 'test/loss': 0.048261333256959915, 'test/mean_average_precision': 0.2562264799589845, 'test/num_examples': 43793, 'score': 13944.305476903915, 'total_duration': 21624.305897712708, 'accumulated_submission_time': 13944.305476903915, 'accumulated_eval_time': 7676.686524152756, 'accumulated_logging_time': 2.1386983394622803}
I0209 07:06:56.012803 139871926159104 logging_writer.py:48] [43293] accumulated_eval_time=7676.686524, accumulated_logging_time=2.138698, accumulated_submission_time=13944.305477, global_step=43293, preemption_count=0, score=13944.305477, test/accuracy=0.985966, test/loss=0.048261, test/mean_average_precision=0.256226, test/num_examples=43793, total_duration=21624.305898, train/accuracy=0.991995, train/loss=0.025653, train/mean_average_precision=0.519639, validation/accuracy=0.986803, validation/loss=0.045174, validation/mean_average_precision=0.273378, validation/num_examples=43793
I0209 07:06:58.733642 139878383003392 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.09784644842147827, loss=0.02705918438732624
I0209 07:07:31.138927 139871926159104 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.09777680039405823, loss=0.02999466471374035
I0209 07:08:03.536233 139878383003392 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.10831613838672638, loss=0.02861189842224121
I0209 07:08:35.541472 139871926159104 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.15617705881595612, loss=0.02985626459121704
I0209 07:09:07.831719 139878383003392 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.10727521032094955, loss=0.029868781566619873
I0209 07:09:39.901346 139871926159104 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.1046280786395073, loss=0.027624865993857384
I0209 07:10:11.948043 139878383003392 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.11097343266010284, loss=0.0310054998844862
I0209 07:10:44.248069 139871926159104 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.12541057169437408, loss=0.02837361954152584
I0209 07:10:56.270969 140039251117888 spec.py:321] Evaluating on the training split.
I0209 07:12:58.114460 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 07:13:01.675143 140039251117888 spec.py:349] Evaluating on the test split.
I0209 07:13:04.798563 140039251117888 submission_runner.py:408] Time since start: 21993.12s, 	Step: 44038, 	{'train/accuracy': 0.9922802448272705, 'train/loss': 0.02483881264925003, 'train/mean_average_precision': 0.5377617439585939, 'validation/accuracy': 0.9868596792221069, 'validation/loss': 0.04524949565529823, 'validation/mean_average_precision': 0.270351017601974, 'validation/num_examples': 43793, 'test/accuracy': 0.9860146045684814, 'test/loss': 0.04815695807337761, 'test/mean_average_precision': 0.2617285586172711, 'test/num_examples': 43793, 'score': 14184.531028747559, 'total_duration': 21993.115421056747, 'accumulated_submission_time': 14184.531028747559, 'accumulated_eval_time': 7805.214061498642, 'accumulated_logging_time': 2.1753900051116943}
I0209 07:13:04.822931 139871934551808 logging_writer.py:48] [44038] accumulated_eval_time=7805.214061, accumulated_logging_time=2.175390, accumulated_submission_time=14184.531029, global_step=44038, preemption_count=0, score=14184.531029, test/accuracy=0.986015, test/loss=0.048157, test/mean_average_precision=0.261729, test/num_examples=43793, total_duration=21993.115421, train/accuracy=0.992280, train/loss=0.024839, train/mean_average_precision=0.537762, validation/accuracy=0.986860, validation/loss=0.045249, validation/mean_average_precision=0.270351, validation/num_examples=43793
I0209 07:13:25.081261 139878374610688 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.11386962980031967, loss=0.030487356707453728
I0209 07:13:57.648859 139871934551808 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.11099658906459808, loss=0.031186705455183983
I0209 07:14:30.375648 139878374610688 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.108306385576725, loss=0.03022310882806778
I0209 07:15:02.566956 139871934551808 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.10746483504772186, loss=0.028258906677365303
I0209 07:15:35.156619 139878374610688 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.12617239356040955, loss=0.02777477726340294
I0209 07:16:07.847291 139871934551808 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.1165992021560669, loss=0.029032718390226364
I0209 07:16:40.263494 139878374610688 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.1143018826842308, loss=0.02861127071082592
I0209 07:17:05.118214 140039251117888 spec.py:321] Evaluating on the training split.
I0209 07:19:04.241204 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 07:19:07.346130 140039251117888 spec.py:349] Evaluating on the test split.
I0209 07:19:10.364579 140039251117888 submission_runner.py:408] Time since start: 22358.68s, 	Step: 44777, 	{'train/accuracy': 0.9924088716506958, 'train/loss': 0.02422436513006687, 'train/mean_average_precision': 0.5362641543231005, 'validation/accuracy': 0.9869104027748108, 'validation/loss': 0.04534335061907768, 'validation/mean_average_precision': 0.26956582298002385, 'validation/num_examples': 43793, 'test/accuracy': 0.9860327243804932, 'test/loss': 0.0481882318854332, 'test/mean_average_precision': 0.26161517705688725, 'test/num_examples': 43793, 'score': 14424.795090436935, 'total_duration': 22358.681414604187, 'accumulated_submission_time': 14424.795090436935, 'accumulated_eval_time': 7930.460354089737, 'accumulated_logging_time': 2.2115097045898438}
I0209 07:19:10.388980 139871926159104 logging_writer.py:48] [44777] accumulated_eval_time=7930.460354, accumulated_logging_time=2.211510, accumulated_submission_time=14424.795090, global_step=44777, preemption_count=0, score=14424.795090, test/accuracy=0.986033, test/loss=0.048188, test/mean_average_precision=0.261615, test/num_examples=43793, total_duration=22358.681415, train/accuracy=0.992409, train/loss=0.024224, train/mean_average_precision=0.536264, validation/accuracy=0.986910, validation/loss=0.045343, validation/mean_average_precision=0.269566, validation/num_examples=43793
I0209 07:19:18.345417 139878383003392 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.100596122443676, loss=0.03161226958036423
I0209 07:19:51.354623 139871926159104 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.1156616359949112, loss=0.029587185010313988
I0209 07:20:23.744749 139878383003392 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.11247524619102478, loss=0.029725207015872
I0209 07:20:55.737812 139871926159104 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.13632605969905853, loss=0.03117661364376545
I0209 07:21:28.468431 139878383003392 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.1060757115483284, loss=0.030138244852423668
I0209 07:22:00.894623 139871926159104 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.12751568853855133, loss=0.02958683669567108
I0209 07:22:33.729647 139878383003392 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.11377879232168198, loss=0.030742939561605453
I0209 07:23:05.947312 139871926159104 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.11115793138742447, loss=0.030949821695685387
I0209 07:23:10.392864 140039251117888 spec.py:321] Evaluating on the training split.
I0209 07:25:12.042384 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 07:25:15.042237 140039251117888 spec.py:349] Evaluating on the test split.
I0209 07:25:18.028630 140039251117888 submission_runner.py:408] Time since start: 22726.35s, 	Step: 45515, 	{'train/accuracy': 0.9927688837051392, 'train/loss': 0.023114778101444244, 'train/mean_average_precision': 0.5721043220273573, 'validation/accuracy': 0.9869270324707031, 'validation/loss': 0.045256517827510834, 'validation/mean_average_precision': 0.2640619486550432, 'validation/num_examples': 43793, 'test/accuracy': 0.9860420227050781, 'test/loss': 0.04842713475227356, 'test/mean_average_precision': 0.2579358481884553, 'test/num_examples': 43793, 'score': 14664.76844573021, 'total_duration': 22726.34547829628, 'accumulated_submission_time': 14664.76844573021, 'accumulated_eval_time': 8058.0960521698, 'accumulated_logging_time': 2.247072696685791}
I0209 07:25:18.052825 139864233322240 logging_writer.py:48] [45515] accumulated_eval_time=8058.096052, accumulated_logging_time=2.247073, accumulated_submission_time=14664.768446, global_step=45515, preemption_count=0, score=14664.768446, test/accuracy=0.986042, test/loss=0.048427, test/mean_average_precision=0.257936, test/num_examples=43793, total_duration=22726.345478, train/accuracy=0.992769, train/loss=0.023115, train/mean_average_precision=0.572104, validation/accuracy=0.986927, validation/loss=0.045257, validation/mean_average_precision=0.264062, validation/num_examples=43793
I0209 07:25:45.498343 139878374610688 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.11431939899921417, loss=0.026027584448456764
I0209 07:26:17.667842 139864233322240 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.14832818508148193, loss=0.02858865261077881
I0209 07:26:49.749261 139878374610688 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.1338687390089035, loss=0.028872517868876457
I0209 07:27:22.164799 139864233322240 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.10410110652446747, loss=0.02798324264585972
I0209 07:27:54.995943 139878374610688 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.13600817322731018, loss=0.027037208899855614
I0209 07:28:26.864656 139864233322240 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.1113276258111, loss=0.028262104839086533
I0209 07:28:58.650177 139878374610688 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.10982945561408997, loss=0.030778251588344574
I0209 07:29:18.186609 140039251117888 spec.py:321] Evaluating on the training split.
I0209 07:31:18.100368 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 07:31:21.165267 140039251117888 spec.py:349] Evaluating on the test split.
I0209 07:31:24.153159 140039251117888 submission_runner.py:408] Time since start: 23092.47s, 	Step: 46262, 	{'train/accuracy': 0.9930108189582825, 'train/loss': 0.022385746240615845, 'train/mean_average_precision': 0.5915540953232792, 'validation/accuracy': 0.9868503212928772, 'validation/loss': 0.045576926320791245, 'validation/mean_average_precision': 0.26865391262902105, 'validation/num_examples': 43793, 'test/accuracy': 0.985990583896637, 'test/loss': 0.048694908618927, 'test/mean_average_precision': 0.25982432242618936, 'test/num_examples': 43793, 'score': 14904.87128996849, 'total_duration': 23092.4700255394, 'accumulated_submission_time': 14904.87128996849, 'accumulated_eval_time': 8184.0625557899475, 'accumulated_logging_time': 2.2822532653808594}
I0209 07:31:24.177080 139871926159104 logging_writer.py:48] [46262] accumulated_eval_time=8184.062556, accumulated_logging_time=2.282253, accumulated_submission_time=14904.871290, global_step=46262, preemption_count=0, score=14904.871290, test/accuracy=0.985991, test/loss=0.048695, test/mean_average_precision=0.259824, test/num_examples=43793, total_duration=23092.470026, train/accuracy=0.993011, train/loss=0.022386, train/mean_average_precision=0.591554, validation/accuracy=0.986850, validation/loss=0.045577, validation/mean_average_precision=0.268654, validation/num_examples=43793
I0209 07:31:36.747382 139871934551808 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.11403359472751617, loss=0.030602101236581802
I0209 07:32:09.234416 139871926159104 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.12803593277931213, loss=0.028508193790912628
I0209 07:32:41.457093 139871934551808 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.1328013837337494, loss=0.027566444128751755
I0209 07:33:13.830817 139871926159104 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.12719427049160004, loss=0.027062905952334404
I0209 07:33:45.748699 139871934551808 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.12152761965990067, loss=0.029528945684432983
I0209 07:34:17.911522 139871926159104 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.10859774798154831, loss=0.02845226787030697
I0209 07:34:49.797792 139871934551808 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.21572305262088776, loss=0.029678016901016235
I0209 07:35:22.020171 139871926159104 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.09659617394208908, loss=0.025832505896687508
I0209 07:35:24.291169 140039251117888 spec.py:321] Evaluating on the training split.
I0209 07:37:21.189370 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 07:37:24.266459 140039251117888 spec.py:349] Evaluating on the test split.
I0209 07:37:27.270572 140039251117888 submission_runner.py:408] Time since start: 23455.59s, 	Step: 47008, 	{'train/accuracy': 0.993267297744751, 'train/loss': 0.02183414436876774, 'train/mean_average_precision': 0.6045476022820708, 'validation/accuracy': 0.9868068695068359, 'validation/loss': 0.045824650675058365, 'validation/mean_average_precision': 0.27189078779344356, 'validation/num_examples': 43793, 'test/accuracy': 0.9859114289283752, 'test/loss': 0.048689164221286774, 'test/mean_average_precision': 0.264437696290971, 'test/num_examples': 43793, 'score': 15144.95430803299, 'total_duration': 23455.587432146072, 'accumulated_submission_time': 15144.95430803299, 'accumulated_eval_time': 8307.041923999786, 'accumulated_logging_time': 2.317678451538086}
I0209 07:37:27.295220 139878374610688 logging_writer.py:48] [47008] accumulated_eval_time=8307.041924, accumulated_logging_time=2.317678, accumulated_submission_time=15144.954308, global_step=47008, preemption_count=0, score=15144.954308, test/accuracy=0.985911, test/loss=0.048689, test/mean_average_precision=0.264438, test/num_examples=43793, total_duration=23455.587432, train/accuracy=0.993267, train/loss=0.021834, train/mean_average_precision=0.604548, validation/accuracy=0.986807, validation/loss=0.045825, validation/mean_average_precision=0.271891, validation/num_examples=43793
I0209 07:37:57.500994 139878383003392 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.12702278792858124, loss=0.02907561883330345
I0209 07:38:29.737802 139878374610688 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.10249064117670059, loss=0.028078477829694748
I0209 07:39:02.327319 139878383003392 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.10922977328300476, loss=0.02685239538550377
I0209 07:39:34.701548 139878374610688 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.14524121582508087, loss=0.028946630656719208
I0209 07:40:07.226835 139878383003392 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.1630781590938568, loss=0.027144018560647964
I0209 07:40:39.602511 139878374610688 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.11394720524549484, loss=0.026942642405629158
I0209 07:41:12.236279 139878383003392 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.12183476239442825, loss=0.025723155587911606
I0209 07:41:27.559280 140039251117888 spec.py:321] Evaluating on the training split.
I0209 07:43:29.135838 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 07:43:32.189185 140039251117888 spec.py:349] Evaluating on the test split.
I0209 07:43:35.178354 140039251117888 submission_runner.py:408] Time since start: 23823.50s, 	Step: 47748, 	{'train/accuracy': 0.992554783821106, 'train/loss': 0.02386317029595375, 'train/mean_average_precision': 0.5605475454067415, 'validation/accuracy': 0.9868738651275635, 'validation/loss': 0.04586609825491905, 'validation/mean_average_precision': 0.274083811671372, 'validation/num_examples': 43793, 'test/accuracy': 0.9859864115715027, 'test/loss': 0.04919857159256935, 'test/mean_average_precision': 0.25760008765950376, 'test/num_examples': 43793, 'score': 15385.188156366348, 'total_duration': 23823.495221614838, 'accumulated_submission_time': 15385.188156366348, 'accumulated_eval_time': 8434.66095161438, 'accumulated_logging_time': 2.353482723236084}
I0209 07:43:35.203216 139864233322240 logging_writer.py:48] [47748] accumulated_eval_time=8434.660952, accumulated_logging_time=2.353483, accumulated_submission_time=15385.188156, global_step=47748, preemption_count=0, score=15385.188156, test/accuracy=0.985986, test/loss=0.049199, test/mean_average_precision=0.257600, test/num_examples=43793, total_duration=23823.495222, train/accuracy=0.992555, train/loss=0.023863, train/mean_average_precision=0.560548, validation/accuracy=0.986874, validation/loss=0.045866, validation/mean_average_precision=0.274084, validation/num_examples=43793
I0209 07:43:52.126199 139871934551808 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.1264333724975586, loss=0.028894560411572456
I0209 07:44:24.418649 139864233322240 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.1334753781557083, loss=0.027165044099092484
I0209 07:44:56.536561 139871934551808 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.12336192280054092, loss=0.03021634742617607
I0209 07:45:28.785952 139864233322240 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.10439379513263702, loss=0.028514539822936058
I0209 07:46:00.455471 139871934551808 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.16564027965068817, loss=0.02841610088944435
I0209 07:46:32.886000 139864233322240 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.11156540364027023, loss=0.02796652726829052
I0209 07:47:05.394791 139871934551808 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.11260344833135605, loss=0.027222417294979095
I0209 07:47:35.365894 140039251117888 spec.py:321] Evaluating on the training split.
I0209 07:49:33.794041 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 07:49:36.844815 140039251117888 spec.py:349] Evaluating on the test split.
I0209 07:49:39.828641 140039251117888 submission_runner.py:408] Time since start: 24188.15s, 	Step: 48496, 	{'train/accuracy': 0.9926549792289734, 'train/loss': 0.023603614419698715, 'train/mean_average_precision': 0.5488211291481068, 'validation/accuracy': 0.9868596792221069, 'validation/loss': 0.04607483372092247, 'validation/mean_average_precision': 0.2715950909297707, 'validation/num_examples': 43793, 'test/accuracy': 0.9859628081321716, 'test/loss': 0.04901837185025215, 'test/mean_average_precision': 0.26013198771595913, 'test/num_examples': 43793, 'score': 15625.320230960846, 'total_duration': 24188.14550971985, 'accumulated_submission_time': 15625.320230960846, 'accumulated_eval_time': 8559.123657226562, 'accumulated_logging_time': 2.389343738555908}
I0209 07:49:39.853407 139871926159104 logging_writer.py:48] [48496] accumulated_eval_time=8559.123657, accumulated_logging_time=2.389344, accumulated_submission_time=15625.320231, global_step=48496, preemption_count=0, score=15625.320231, test/accuracy=0.985963, test/loss=0.049018, test/mean_average_precision=0.260132, test/num_examples=43793, total_duration=24188.145510, train/accuracy=0.992655, train/loss=0.023604, train/mean_average_precision=0.548821, validation/accuracy=0.986860, validation/loss=0.046075, validation/mean_average_precision=0.271595, validation/num_examples=43793
I0209 07:49:41.452409 139878374610688 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.12233688682317734, loss=0.029824748635292053
I0209 07:50:13.706566 139871926159104 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.12568865716457367, loss=0.024538442492485046
I0209 07:50:45.666941 139878374610688 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.11253830790519714, loss=0.027651876211166382
I0209 07:51:17.738865 139871926159104 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.11102747917175293, loss=0.02587277442216873
I0209 07:51:49.531614 139878374610688 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.14059856534004211, loss=0.028620362281799316
I0209 07:52:21.698464 139871926159104 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.12336139380931854, loss=0.028771627694368362
I0209 07:52:53.583185 139878374610688 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.12221439927816391, loss=0.028749193996191025
I0209 07:53:26.088188 139871926159104 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.11581765115261078, loss=0.028735991567373276
I0209 07:53:39.851733 140039251117888 spec.py:321] Evaluating on the training split.
I0209 07:55:35.023460 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 07:55:38.065177 140039251117888 spec.py:349] Evaluating on the test split.
I0209 07:55:42.400888 140039251117888 submission_runner.py:408] Time since start: 24550.72s, 	Step: 49243, 	{'train/accuracy': 0.9927665591239929, 'train/loss': 0.02316511981189251, 'train/mean_average_precision': 0.5600378176647713, 'validation/accuracy': 0.9868649244308472, 'validation/loss': 0.04596903175115585, 'validation/mean_average_precision': 0.27291857464366176, 'validation/num_examples': 43793, 'test/accuracy': 0.9859880805015564, 'test/loss': 0.049083225429058075, 'test/mean_average_precision': 0.2621291751755614, 'test/num_examples': 43793, 'score': 15865.286841392517, 'total_duration': 24550.717749118805, 'accumulated_submission_time': 15865.286841392517, 'accumulated_eval_time': 8681.672759532928, 'accumulated_logging_time': 2.4259297847747803}
I0209 07:55:42.427886 139864233322240 logging_writer.py:48] [49243] accumulated_eval_time=8681.672760, accumulated_logging_time=2.425930, accumulated_submission_time=15865.286841, global_step=49243, preemption_count=0, score=15865.286841, test/accuracy=0.985988, test/loss=0.049083, test/mean_average_precision=0.262129, test/num_examples=43793, total_duration=24550.717749, train/accuracy=0.992767, train/loss=0.023165, train/mean_average_precision=0.560038, validation/accuracy=0.986865, validation/loss=0.045969, validation/mean_average_precision=0.272919, validation/num_examples=43793
I0209 07:56:00.890103 139871934551808 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.15067905187606812, loss=0.026473185047507286
I0209 07:56:33.081227 139864233322240 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.1352781355381012, loss=0.029698988422751427
I0209 07:57:05.291433 139871934551808 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.15133655071258545, loss=0.027269545942544937
I0209 07:57:37.535283 139864233322240 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.13477888703346252, loss=0.02633241005241871
I0209 07:58:09.778785 139871934551808 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.1707337647676468, loss=0.02887043170630932
I0209 07:58:42.189433 139864233322240 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.13443031907081604, loss=0.02621459774672985
I0209 07:59:14.623584 139871934551808 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.1325727105140686, loss=0.026350712403655052
I0209 07:59:42.412377 140039251117888 spec.py:321] Evaluating on the training split.
I0209 08:01:35.312943 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 08:01:38.344921 140039251117888 spec.py:349] Evaluating on the test split.
I0209 08:01:41.341385 140039251117888 submission_runner.py:408] Time since start: 24909.66s, 	Step: 49988, 	{'train/accuracy': 0.9928274750709534, 'train/loss': 0.022933220490813255, 'train/mean_average_precision': 0.5803153113657378, 'validation/accuracy': 0.9868227243423462, 'validation/loss': 0.04655434563755989, 'validation/mean_average_precision': 0.2713811193405487, 'validation/num_examples': 43793, 'test/accuracy': 0.9858924746513367, 'test/loss': 0.04988021403551102, 'test/mean_average_precision': 0.26065111811903857, 'test/num_examples': 43793, 'score': 16105.240637540817, 'total_duration': 24909.658252477646, 'accumulated_submission_time': 16105.240637540817, 'accumulated_eval_time': 8800.601737260818, 'accumulated_logging_time': 2.4639928340911865}
I0209 08:01:41.366301 139878374610688 logging_writer.py:48] [49988] accumulated_eval_time=8800.601737, accumulated_logging_time=2.463993, accumulated_submission_time=16105.240638, global_step=49988, preemption_count=0, score=16105.240638, test/accuracy=0.985892, test/loss=0.049880, test/mean_average_precision=0.260651, test/num_examples=43793, total_duration=24909.658252, train/accuracy=0.992827, train/loss=0.022933, train/mean_average_precision=0.580315, validation/accuracy=0.986823, validation/loss=0.046554, validation/mean_average_precision=0.271381, validation/num_examples=43793
I0209 08:01:45.489410 139878383003392 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.11443442851305008, loss=0.022525418549776077
I0209 08:02:17.391100 139878374610688 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.1452987641096115, loss=0.027489081025123596
I0209 08:02:49.205478 139878383003392 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.14397402107715607, loss=0.026917671784758568
I0209 08:03:21.486440 139878374610688 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.136610209941864, loss=0.026610171422362328
I0209 08:03:53.449426 139878383003392 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.11172684282064438, loss=0.024958685040473938
I0209 08:04:25.714830 139878374610688 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.14855223894119263, loss=0.028061363846063614
I0209 08:04:57.580454 139878383003392 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.14535531401634216, loss=0.026252731680870056
I0209 08:05:29.726976 139878374610688 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.16331514716148376, loss=0.031559720635414124
I0209 08:05:41.550079 140039251117888 spec.py:321] Evaluating on the training split.
I0209 08:07:36.637846 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 08:07:39.680342 140039251117888 spec.py:349] Evaluating on the test split.
I0209 08:07:42.653020 140039251117888 submission_runner.py:408] Time since start: 25270.97s, 	Step: 50738, 	{'train/accuracy': 0.992751955986023, 'train/loss': 0.023076120764017105, 'train/mean_average_precision': 0.5672821856545007, 'validation/accuracy': 0.9868150353431702, 'validation/loss': 0.04626094922423363, 'validation/mean_average_precision': 0.2708342805840612, 'validation/num_examples': 43793, 'test/accuracy': 0.98593670129776, 'test/loss': 0.04970936104655266, 'test/mean_average_precision': 0.26015209116361343, 'test/num_examples': 43793, 'score': 16345.393598794937, 'total_duration': 25270.969877958298, 'accumulated_submission_time': 16345.393598794937, 'accumulated_eval_time': 8921.704628229141, 'accumulated_logging_time': 2.500322103500366}
I0209 08:07:42.677845 139864233322240 logging_writer.py:48] [50738] accumulated_eval_time=8921.704628, accumulated_logging_time=2.500322, accumulated_submission_time=16345.393599, global_step=50738, preemption_count=0, score=16345.393599, test/accuracy=0.985937, test/loss=0.049709, test/mean_average_precision=0.260152, test/num_examples=43793, total_duration=25270.969878, train/accuracy=0.992752, train/loss=0.023076, train/mean_average_precision=0.567282, validation/accuracy=0.986815, validation/loss=0.046261, validation/mean_average_precision=0.270834, validation/num_examples=43793
I0209 08:08:02.888246 139871934551808 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.15642525255680084, loss=0.029131125658750534
I0209 08:08:34.748190 139864233322240 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.1644478142261505, loss=0.0272036325186491
I0209 08:09:06.616263 139871934551808 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.12476520985364914, loss=0.026707502081990242
I0209 08:09:38.421736 139864233322240 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.12490831315517426, loss=0.02442045323550701
I0209 08:10:10.440397 139871934551808 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.1491841971874237, loss=0.026083342730998993
I0209 08:10:42.355392 139864233322240 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.12149334698915482, loss=0.026418231427669525
I0209 08:11:14.591777 139871934551808 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.1513075977563858, loss=0.025787798687815666
I0209 08:11:42.829597 140039251117888 spec.py:321] Evaluating on the training split.
I0209 08:13:39.278605 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 08:13:42.322505 140039251117888 spec.py:349] Evaluating on the test split.
I0209 08:13:45.357091 140039251117888 submission_runner.py:408] Time since start: 25633.67s, 	Step: 51489, 	{'train/accuracy': 0.9931676983833313, 'train/loss': 0.021803244948387146, 'train/mean_average_precision': 0.5953628427219528, 'validation/accuracy': 0.9868471026420593, 'validation/loss': 0.04647331312298775, 'validation/mean_average_precision': 0.2731393930659657, 'validation/num_examples': 43793, 'test/accuracy': 0.9858849048614502, 'test/loss': 0.049809858202934265, 'test/mean_average_precision': 0.263006766620887, 'test/num_examples': 43793, 'score': 16585.514472723007, 'total_duration': 25633.673954486847, 'accumulated_submission_time': 16585.514472723007, 'accumulated_eval_time': 9044.232073783875, 'accumulated_logging_time': 2.5364415645599365}
I0209 08:13:45.382987 139878374610688 logging_writer.py:48] [51489] accumulated_eval_time=9044.232074, accumulated_logging_time=2.536442, accumulated_submission_time=16585.514473, global_step=51489, preemption_count=0, score=16585.514473, test/accuracy=0.985885, test/loss=0.049810, test/mean_average_precision=0.263007, test/num_examples=43793, total_duration=25633.673954, train/accuracy=0.993168, train/loss=0.021803, train/mean_average_precision=0.595363, validation/accuracy=0.986847, validation/loss=0.046473, validation/mean_average_precision=0.273139, validation/num_examples=43793
I0209 08:13:49.234527 139878383003392 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.1337672770023346, loss=0.02649538777768612
I0209 08:14:21.826641 139878374610688 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.12897726893424988, loss=0.02584044821560383
I0209 08:14:53.912392 139878383003392 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.12835822999477386, loss=0.02660800889134407
I0209 08:15:26.252732 139878374610688 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.13861675560474396, loss=0.02777252160012722
I0209 08:15:58.424379 139878383003392 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.17885097861289978, loss=0.029200062155723572
I0209 08:16:30.337031 139878374610688 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.13600370287895203, loss=0.02459855005145073
I0209 08:17:02.428028 139878383003392 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.22597093880176544, loss=0.02651899866759777
I0209 08:17:34.399749 139878374610688 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.13366135954856873, loss=0.026711545884609222
I0209 08:17:45.397857 140039251117888 spec.py:321] Evaluating on the training split.
I0209 08:19:39.968430 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 08:19:43.040831 140039251117888 spec.py:349] Evaluating on the test split.
I0209 08:19:46.058213 140039251117888 submission_runner.py:408] Time since start: 25994.38s, 	Step: 52235, 	{'train/accuracy': 0.993556797504425, 'train/loss': 0.020664073526859283, 'train/mean_average_precision': 0.6220936529065954, 'validation/accuracy': 0.9868056774139404, 'validation/loss': 0.04686205834150314, 'validation/mean_average_precision': 0.26648923587925655, 'validation/num_examples': 43793, 'test/accuracy': 0.9858680367469788, 'test/loss': 0.05013066902756691, 'test/mean_average_precision': 0.2569849594604288, 'test/num_examples': 43793, 'score': 16825.499007225037, 'total_duration': 25994.375078201294, 'accumulated_submission_time': 16825.499007225037, 'accumulated_eval_time': 9164.892379283905, 'accumulated_logging_time': 2.573488712310791}
I0209 08:19:46.084731 139871926159104 logging_writer.py:48] [52235] accumulated_eval_time=9164.892379, accumulated_logging_time=2.573489, accumulated_submission_time=16825.499007, global_step=52235, preemption_count=0, score=16825.499007, test/accuracy=0.985868, test/loss=0.050131, test/mean_average_precision=0.256985, test/num_examples=43793, total_duration=25994.375078, train/accuracy=0.993557, train/loss=0.020664, train/mean_average_precision=0.622094, validation/accuracy=0.986806, validation/loss=0.046862, validation/mean_average_precision=0.266489, validation/num_examples=43793
I0209 08:20:07.177931 139871934551808 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.19145172834396362, loss=0.02798774279654026
I0209 08:20:39.582489 139871926159104 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.16127821803092957, loss=0.02437431365251541
I0209 08:21:11.822247 139871934551808 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.16055729985237122, loss=0.028339646756649017
I0209 08:21:43.953386 139871926159104 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.13789162039756775, loss=0.025931447744369507
I0209 08:22:16.114927 139871934551808 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.17086313664913177, loss=0.027725255116820335
I0209 08:22:48.265546 139871926159104 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.1631624847650528, loss=0.026880864053964615
I0209 08:23:20.763939 139871934551808 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.127376988530159, loss=0.02360963262617588
I0209 08:23:46.106630 140039251117888 spec.py:321] Evaluating on the training split.
I0209 08:25:43.249848 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 08:25:46.354853 140039251117888 spec.py:349] Evaluating on the test split.
I0209 08:25:49.398544 140039251117888 submission_runner.py:408] Time since start: 26357.72s, 	Step: 52980, 	{'train/accuracy': 0.9937600493431091, 'train/loss': 0.01983131468296051, 'train/mean_average_precision': 0.6459993043755188, 'validation/accuracy': 0.9867037534713745, 'validation/loss': 0.04749181494116783, 'validation/mean_average_precision': 0.2685034861718007, 'validation/num_examples': 43793, 'test/accuracy': 0.9858928918838501, 'test/loss': 0.05066568776965141, 'test/mean_average_precision': 0.25705002950830314, 'test/num_examples': 43793, 'score': 17065.48960494995, 'total_duration': 26357.715396165848, 'accumulated_submission_time': 17065.48960494995, 'accumulated_eval_time': 9288.184242248535, 'accumulated_logging_time': 2.6123759746551514}
I0209 08:25:49.425081 139878374610688 logging_writer.py:48] [52980] accumulated_eval_time=9288.184242, accumulated_logging_time=2.612376, accumulated_submission_time=17065.489605, global_step=52980, preemption_count=0, score=17065.489605, test/accuracy=0.985893, test/loss=0.050666, test/mean_average_precision=0.257050, test/num_examples=43793, total_duration=26357.715396, train/accuracy=0.993760, train/loss=0.019831, train/mean_average_precision=0.645999, validation/accuracy=0.986704, validation/loss=0.047492, validation/mean_average_precision=0.268503, validation/num_examples=43793
I0209 08:25:56.676566 139878383003392 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.15197210013866425, loss=0.026174316182732582
I0209 08:26:29.216137 139878374610688 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.16769544780254364, loss=0.0280800461769104
I0209 08:27:01.092506 139878383003392 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.1562139391899109, loss=0.027259089052677155
I0209 08:27:33.237354 139878374610688 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.17117004096508026, loss=0.024577775970101357
I0209 08:28:05.396694 139878383003392 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.15323150157928467, loss=0.026890603825449944
I0209 08:28:37.418589 139878374610688 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.14053791761398315, loss=0.02760216034948826
I0209 08:29:09.592416 139878383003392 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.13360637426376343, loss=0.026700863614678383
I0209 08:29:42.010093 139878374610688 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.14571644365787506, loss=0.024385813623666763
I0209 08:29:49.594290 140039251117888 spec.py:321] Evaluating on the training split.
I0209 08:31:43.220177 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 08:31:46.274649 140039251117888 spec.py:349] Evaluating on the test split.
I0209 08:31:49.275726 140039251117888 submission_runner.py:408] Time since start: 26717.59s, 	Step: 53724, 	{'train/accuracy': 0.9939901828765869, 'train/loss': 0.01933383196592331, 'train/mean_average_precision': 0.6436779537032165, 'validation/accuracy': 0.9866481423377991, 'validation/loss': 0.04750419408082962, 'validation/mean_average_precision': 0.26815615039251833, 'validation/num_examples': 43793, 'test/accuracy': 0.9858894944190979, 'test/loss': 0.05031247064471245, 'test/mean_average_precision': 0.261990889494677, 'test/num_examples': 43793, 'score': 17305.6275203228, 'total_duration': 26717.59257531166, 'accumulated_submission_time': 17305.6275203228, 'accumulated_eval_time': 9407.865612268448, 'accumulated_logging_time': 2.6504719257354736}
I0209 08:31:49.301282 139864233322240 logging_writer.py:48] [53724] accumulated_eval_time=9407.865612, accumulated_logging_time=2.650472, accumulated_submission_time=17305.627520, global_step=53724, preemption_count=0, score=17305.627520, test/accuracy=0.985889, test/loss=0.050312, test/mean_average_precision=0.261991, test/num_examples=43793, total_duration=26717.592575, train/accuracy=0.993990, train/loss=0.019334, train/mean_average_precision=0.643678, validation/accuracy=0.986648, validation/loss=0.047504, validation/mean_average_precision=0.268156, validation/num_examples=43793
I0209 08:32:14.281817 139871926159104 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.1366637796163559, loss=0.026934346184134483
I0209 08:32:46.359096 139864233322240 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.14280006289482117, loss=0.022163957357406616
I0209 08:33:18.225576 139871926159104 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.1744641661643982, loss=0.02587643265724182
I0209 08:33:49.885610 139864233322240 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.1559426486492157, loss=0.02562125399708748
I0209 08:34:22.278537 139871926159104 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.16412019729614258, loss=0.02393687143921852
I0209 08:34:54.348674 139864233322240 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.15319177508354187, loss=0.02634839154779911
I0209 08:35:26.946740 139871926159104 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.14684246480464935, loss=0.025148574262857437
I0209 08:35:49.363532 140039251117888 spec.py:321] Evaluating on the training split.
I0209 08:37:48.112694 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 08:37:51.230961 140039251117888 spec.py:349] Evaluating on the test split.
I0209 08:37:54.282733 140039251117888 submission_runner.py:408] Time since start: 27082.60s, 	Step: 54469, 	{'train/accuracy': 0.9936777949333191, 'train/loss': 0.020111173391342163, 'train/mean_average_precision': 0.6458658038745391, 'validation/accuracy': 0.9866968989372253, 'validation/loss': 0.04727909341454506, 'validation/mean_average_precision': 0.26744203434436603, 'validation/num_examples': 43793, 'test/accuracy': 0.9858878254890442, 'test/loss': 0.050669051706790924, 'test/mean_average_precision': 0.25698298673776826, 'test/num_examples': 43793, 'score': 17545.65809392929, 'total_duration': 27082.599598646164, 'accumulated_submission_time': 17545.65809392929, 'accumulated_eval_time': 9532.784777402878, 'accumulated_logging_time': 2.6869962215423584}
I0209 08:37:54.308743 139878374610688 logging_writer.py:48] [54469] accumulated_eval_time=9532.784777, accumulated_logging_time=2.686996, accumulated_submission_time=17545.658094, global_step=54469, preemption_count=0, score=17545.658094, test/accuracy=0.985888, test/loss=0.050669, test/mean_average_precision=0.256983, test/num_examples=43793, total_duration=27082.599599, train/accuracy=0.993678, train/loss=0.020111, train/mean_average_precision=0.645866, validation/accuracy=0.986697, validation/loss=0.047279, validation/mean_average_precision=0.267442, validation/num_examples=43793
I0209 08:38:04.946432 139878383003392 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.16235564649105072, loss=0.02533039078116417
I0209 08:38:37.060527 139878374610688 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.15745475888252258, loss=0.02562457136809826
I0209 08:39:09.485215 139878383003392 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.1446671485900879, loss=0.026707591488957405
I0209 08:39:41.662143 139878374610688 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.1798824965953827, loss=0.02408413216471672
I0209 08:40:13.875171 139878383003392 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.16503366827964783, loss=0.023634830489754677
I0209 08:40:46.531193 139878374610688 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.15593616664409637, loss=0.02614503726363182
I0209 08:41:19.109103 139878383003392 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.16611939668655396, loss=0.026769079267978668
I0209 08:41:51.304532 139878374610688 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.1919546127319336, loss=0.026382403448224068
I0209 08:41:54.479310 140039251117888 spec.py:321] Evaluating on the training split.
I0209 08:43:49.572468 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 08:43:52.651286 140039251117888 spec.py:349] Evaluating on the test split.
I0209 08:43:55.645086 140039251117888 submission_runner.py:408] Time since start: 27443.96s, 	Step: 55211, 	{'train/accuracy': 0.9936330318450928, 'train/loss': 0.020323490723967552, 'train/mean_average_precision': 0.6208121445688634, 'validation/accuracy': 0.9867082238197327, 'validation/loss': 0.047605160623788834, 'validation/mean_average_precision': 0.26579084746660736, 'validation/num_examples': 43793, 'test/accuracy': 0.985811173915863, 'test/loss': 0.05104955658316612, 'test/mean_average_precision': 0.25368147011301617, 'test/num_examples': 43793, 'score': 17785.798129081726, 'total_duration': 27443.96195435524, 'accumulated_submission_time': 17785.798129081726, 'accumulated_eval_time': 9653.950505495071, 'accumulated_logging_time': 2.7239596843719482}
I0209 08:43:55.670920 139871926159104 logging_writer.py:48] [55211] accumulated_eval_time=9653.950505, accumulated_logging_time=2.723960, accumulated_submission_time=17785.798129, global_step=55211, preemption_count=0, score=17785.798129, test/accuracy=0.985811, test/loss=0.051050, test/mean_average_precision=0.253681, test/num_examples=43793, total_duration=27443.961954, train/accuracy=0.993633, train/loss=0.020323, train/mean_average_precision=0.620812, validation/accuracy=0.986708, validation/loss=0.047605, validation/mean_average_precision=0.265791, validation/num_examples=43793
I0209 08:44:25.046858 139871934551808 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.19691772758960724, loss=0.02524847351014614
I0209 08:44:57.145524 139871926159104 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.15107546746730804, loss=0.025323528796434402
I0209 08:45:29.233169 139871934551808 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.18086884915828705, loss=0.02674660086631775
I0209 08:46:01.151061 139871926159104 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.17938180267810822, loss=0.02744615077972412
I0209 08:46:33.218817 139871934551808 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.16730880737304688, loss=0.02623542957007885
I0209 08:47:05.224617 139871926159104 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.179787278175354, loss=0.024577872827649117
I0209 08:47:37.379711 139871934551808 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.1709306240081787, loss=0.027703896164894104
I0209 08:47:55.957727 140039251117888 spec.py:321] Evaluating on the training split.
I0209 08:49:55.736800 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 08:49:58.805498 140039251117888 spec.py:349] Evaluating on the test split.
I0209 08:50:02.003674 140039251117888 submission_runner.py:408] Time since start: 27810.32s, 	Step: 55959, 	{'train/accuracy': 0.9933901429176331, 'train/loss': 0.02094298228621483, 'train/mean_average_precision': 0.6129876580194846, 'validation/accuracy': 0.9866664409637451, 'validation/loss': 0.04780498892068863, 'validation/mean_average_precision': 0.26698651889167746, 'validation/num_examples': 43793, 'test/accuracy': 0.9858027696609497, 'test/loss': 0.051106881350278854, 'test/mean_average_precision': 0.25975426581008415, 'test/num_examples': 43793, 'score': 18026.053458452225, 'total_duration': 27810.320541620255, 'accumulated_submission_time': 18026.053458452225, 'accumulated_eval_time': 9779.996412038803, 'accumulated_logging_time': 2.7612576484680176}
I0209 08:50:02.030552 139864233322240 logging_writer.py:48] [55959] accumulated_eval_time=9779.996412, accumulated_logging_time=2.761258, accumulated_submission_time=18026.053458, global_step=55959, preemption_count=0, score=18026.053458, test/accuracy=0.985803, test/loss=0.051107, test/mean_average_precision=0.259754, test/num_examples=43793, total_duration=27810.320542, train/accuracy=0.993390, train/loss=0.020943, train/mean_average_precision=0.612988, validation/accuracy=0.986666, validation/loss=0.047805, validation/mean_average_precision=0.266987, validation/num_examples=43793
I0209 08:50:15.651314 139878374610688 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.16261155903339386, loss=0.02473490871489048
I0209 08:50:47.539087 139864233322240 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.160652294754982, loss=0.02440759725868702
I0209 08:51:19.446890 139878374610688 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.15563586354255676, loss=0.025570739060640335
I0209 08:51:51.467099 139864233322240 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.20912842452526093, loss=0.02332850731909275
I0209 08:52:23.451202 139878374610688 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.21190133690834045, loss=0.02592434547841549
I0209 08:52:55.448721 139864233322240 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.1499410718679428, loss=0.0255761556327343
I0209 08:53:27.248987 139878374610688 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.1724584698677063, loss=0.02439897134900093
I0209 08:53:59.212107 139864233322240 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.15686330199241638, loss=0.023455290123820305
I0209 08:54:02.171937 140039251117888 spec.py:321] Evaluating on the training split.
I0209 08:56:00.851685 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 08:56:04.030119 140039251117888 spec.py:349] Evaluating on the test split.
I0209 08:56:07.034978 140039251117888 submission_runner.py:408] Time since start: 28175.35s, 	Step: 56710, 	{'train/accuracy': 0.9932118058204651, 'train/loss': 0.021330367773771286, 'train/mean_average_precision': 0.594974708618752, 'validation/accuracy': 0.9866802096366882, 'validation/loss': 0.04851559177041054, 'validation/mean_average_precision': 0.26519575804293455, 'validation/num_examples': 43793, 'test/accuracy': 0.9857563972473145, 'test/loss': 0.05178118124604225, 'test/mean_average_precision': 0.2562075157043629, 'test/num_examples': 43793, 'score': 18266.163946151733, 'total_duration': 28175.351845502853, 'accumulated_submission_time': 18266.163946151733, 'accumulated_eval_time': 9904.859407424927, 'accumulated_logging_time': 2.799586296081543}
I0209 08:56:07.061639 139871926159104 logging_writer.py:48] [56710] accumulated_eval_time=9904.859407, accumulated_logging_time=2.799586, accumulated_submission_time=18266.163946, global_step=56710, preemption_count=0, score=18266.163946, test/accuracy=0.985756, test/loss=0.051781, test/mean_average_precision=0.256208, test/num_examples=43793, total_duration=28175.351846, train/accuracy=0.993212, train/loss=0.021330, train/mean_average_precision=0.594975, validation/accuracy=0.986680, validation/loss=0.048516, validation/mean_average_precision=0.265196, validation/num_examples=43793
I0209 08:56:36.208668 139878383003392 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.1804838925600052, loss=0.023974010720849037
I0209 08:57:08.702574 139871926159104 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.1892286092042923, loss=0.023331455886363983
I0209 08:57:41.524430 139878383003392 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.17775070667266846, loss=0.02553604356944561
I0209 08:58:14.384687 139871926159104 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.1775582879781723, loss=0.024718234315514565
I0209 08:58:46.601055 139878383003392 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.19211123883724213, loss=0.02658745087683201
I0209 08:59:19.049144 139871926159104 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.16268105804920197, loss=0.02382160723209381
I0209 08:59:38.036598 139878383003392 logging_writer.py:48] [57359] global_step=57359, preemption_count=0, score=18477.090781
I0209 08:59:38.096109 140039251117888 checkpoints.py:490] Saving checkpoint at step: 57359
I0209 08:59:38.220612 140039251117888 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_3/ogbg_jax/trial_1/checkpoint_57359
I0209 08:59:38.221991 140039251117888 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_3/ogbg_jax/trial_1/checkpoint_57359.
I0209 08:59:38.409412 140039251117888 submission_runner.py:583] Tuning trial 1/5
I0209 08:59:38.409649 140039251117888 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0209 08:59:38.413578 140039251117888 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5250149965286255, 'train/loss': 0.7151498794555664, 'train/mean_average_precision': 0.022461442098083748, 'validation/accuracy': 0.5213832855224609, 'validation/loss': 0.7166012525558472, 'validation/mean_average_precision': 0.026167739618213733, 'validation/num_examples': 43793, 'test/accuracy': 0.5224818587303162, 'test/loss': 0.7161954641342163, 'test/mean_average_precision': 0.02782404108389581, 'test/num_examples': 43793, 'score': 18.22455358505249, 'total_duration': 330.3200616836548, 'accumulated_submission_time': 18.22455358505249, 'accumulated_eval_time': 312.095463514328, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (749, {'train/accuracy': 0.9867318868637085, 'train/loss': 0.06859976053237915, 'train/mean_average_precision': 0.03414312510397307, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.07741302996873856, 'validation/mean_average_precision': 0.036325129363259476, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.08034414798021317, 'test/mean_average_precision': 0.03796467688874391, 'test/num_examples': 43793, 'score': 258.50570034980774, 'total_duration': 688.2471423149109, 'accumulated_submission_time': 258.50570034980774, 'accumulated_eval_time': 429.68863344192505, 'accumulated_logging_time': 0.033779144287109375, 'global_step': 749, 'preemption_count': 0}), (1498, {'train/accuracy': 0.9871208071708679, 'train/loss': 0.04883696511387825, 'train/mean_average_precision': 0.0850988099663653, 'validation/accuracy': 0.9844743609428406, 'validation/loss': 0.05789928883314133, 'validation/mean_average_precision': 0.08616501175420727, 'validation/num_examples': 43793, 'test/accuracy': 0.983509361743927, 'test/loss': 0.06113697960972786, 'test/mean_average_precision': 0.08745892214083549, 'test/num_examples': 43793, 'score': 498.6134088039398, 'total_duration': 1051.6457195281982, 'accumulated_submission_time': 498.6134088039398, 'accumulated_eval_time': 552.933183670044, 'accumulated_logging_time': 0.06142616271972656, 'global_step': 1498, 'preemption_count': 0}), (2246, {'train/accuracy': 0.9877936840057373, 'train/loss': 0.044384416192770004, 'train/mean_average_precision': 0.13547238429811922, 'validation/accuracy': 0.9850642085075378, 'validation/loss': 0.05360514670610428, 'validation/mean_average_precision': 0.13118885010019463, 'validation/num_examples': 43793, 'test/accuracy': 0.984077513217926, 'test/loss': 0.05675095319747925, 'test/mean_average_precision': 0.12851983268766617, 'test/num_examples': 43793, 'score': 738.7220523357391, 'total_duration': 1413.3673105239868, 'accumulated_submission_time': 738.7220523357391, 'accumulated_eval_time': 674.4949362277985, 'accumulated_logging_time': 0.09388899803161621, 'global_step': 2246, 'preemption_count': 0}), (2973, {'train/accuracy': 0.9880256056785583, 'train/loss': 0.04221920296549797, 'train/mean_average_precision': 0.16530680024698508, 'validation/accuracy': 0.9852164387702942, 'validation/loss': 0.05180754512548447, 'validation/mean_average_precision': 0.15750730277160135, 'validation/num_examples': 43793, 'test/accuracy': 0.9842742681503296, 'test/loss': 0.05467303842306137, 'test/mean_average_precision': 0.15056607856140866, 'test/num_examples': 43793, 'score': 978.9202573299408, 'total_duration': 1778.4332921504974, 'accumulated_submission_time': 978.9202573299408, 'accumulated_eval_time': 799.3096804618835, 'accumulated_logging_time': 0.12338590621948242, 'global_step': 2973, 'preemption_count': 0}), (3716, {'train/accuracy': 0.9881114959716797, 'train/loss': 0.04102349281311035, 'train/mean_average_precision': 0.19044968455690436, 'validation/accuracy': 0.9853609204292297, 'validation/loss': 0.050700146704912186, 'validation/mean_average_precision': 0.16617833768708773, 'validation/num_examples': 43793, 'test/accuracy': 0.9844338893890381, 'test/loss': 0.05348716676235199, 'test/mean_average_precision': 0.16572209108759198, 'test/num_examples': 43793, 'score': 1219.0944874286652, 'total_duration': 2146.022847175598, 'accumulated_submission_time': 1219.0944874286652, 'accumulated_eval_time': 926.6731991767883, 'accumulated_logging_time': 0.15257525444030762, 'global_step': 3716, 'preemption_count': 0}), (4471, {'train/accuracy': 0.9883496761322021, 'train/loss': 0.04018726199865341, 'train/mean_average_precision': 0.20600886764690074, 'validation/accuracy': 0.9855566024780273, 'validation/loss': 0.04945233464241028, 'validation/mean_average_precision': 0.1817516461108204, 'validation/num_examples': 43793, 'test/accuracy': 0.9847017526626587, 'test/loss': 0.05210498720407486, 'test/mean_average_precision': 0.18062977807607863, 'test/num_examples': 43793, 'score': 1459.0584998130798, 'total_duration': 2512.822060108185, 'accumulated_submission_time': 1459.0584998130798, 'accumulated_eval_time': 1053.461250782013, 'accumulated_logging_time': 0.17978262901306152, 'global_step': 4471, 'preemption_count': 0}), (5230, {'train/accuracy': 0.9888502955436707, 'train/loss': 0.03828683868050575, 'train/mean_average_precision': 0.22891057221235006, 'validation/accuracy': 0.9857863783836365, 'validation/loss': 0.04814080148935318, 'validation/mean_average_precision': 0.1905339620153854, 'validation/num_examples': 43793, 'test/accuracy': 0.9849346876144409, 'test/loss': 0.05086641013622284, 'test/mean_average_precision': 0.19266720315676172, 'test/num_examples': 43793, 'score': 1699.3178343772888, 'total_duration': 2881.5673213005066, 'accumulated_submission_time': 1699.3178343772888, 'accumulated_eval_time': 1181.8993849754333, 'accumulated_logging_time': 0.20825886726379395, 'global_step': 5230, 'preemption_count': 0}), (5983, {'train/accuracy': 0.9890902042388916, 'train/loss': 0.037576012313365936, 'train/mean_average_precision': 0.2509227669922907, 'validation/accuracy': 0.9859312772750854, 'validation/loss': 0.04736001417040825, 'validation/mean_average_precision': 0.20258524834089467, 'validation/num_examples': 43793, 'test/accuracy': 0.9850884079933167, 'test/loss': 0.049846433103084564, 'test/mean_average_precision': 0.20214781431782117, 'test/num_examples': 43793, 'score': 1939.3397772312164, 'total_duration': 3249.6604022979736, 'accumulated_submission_time': 1939.3397772312164, 'accumulated_eval_time': 1309.9222609996796, 'accumulated_logging_time': 0.23669004440307617, 'global_step': 5983, 'preemption_count': 0}), (6733, {'train/accuracy': 0.9888186454772949, 'train/loss': 0.03792993724346161, 'train/mean_average_precision': 0.2507952327458457, 'validation/accuracy': 0.9858505129814148, 'validation/loss': 0.04780815541744232, 'validation/mean_average_precision': 0.20711343067742888, 'validation/num_examples': 43793, 'test/accuracy': 0.9849119186401367, 'test/loss': 0.050428010523319244, 'test/mean_average_precision': 0.2090736040470227, 'test/num_examples': 43793, 'score': 2179.482953310013, 'total_duration': 3616.232548236847, 'accumulated_submission_time': 2179.482953310013, 'accumulated_eval_time': 1436.303508758545, 'accumulated_logging_time': 0.26384711265563965, 'global_step': 6733, 'preemption_count': 0}), (7486, {'train/accuracy': 0.9891477227210999, 'train/loss': 0.03706908971071243, 'train/mean_average_precision': 0.26609155218859043, 'validation/accuracy': 0.9860952496528625, 'validation/loss': 0.04678123816847801, 'validation/mean_average_precision': 0.21446156417915105, 'validation/num_examples': 43793, 'test/accuracy': 0.9852008819580078, 'test/loss': 0.049382343888282776, 'test/mean_average_precision': 0.21570979354301173, 'test/num_examples': 43793, 'score': 2419.6403181552887, 'total_duration': 3983.7022864818573, 'accumulated_submission_time': 2419.6403181552887, 'accumulated_eval_time': 1563.5682861804962, 'accumulated_logging_time': 0.2922539710998535, 'global_step': 7486, 'preemption_count': 0}), (8237, {'train/accuracy': 0.9890878200531006, 'train/loss': 0.03689158335328102, 'train/mean_average_precision': 0.25970516745874167, 'validation/accuracy': 0.9861975908279419, 'validation/loss': 0.046271905303001404, 'validation/mean_average_precision': 0.21853893914855155, 'validation/num_examples': 43793, 'test/accuracy': 0.985374391078949, 'test/loss': 0.0486757755279541, 'test/mean_average_precision': 0.22138071002078036, 'test/num_examples': 43793, 'score': 2659.7981646060944, 'total_duration': 4350.627862453461, 'accumulated_submission_time': 2659.7981646060944, 'accumulated_eval_time': 1690.2876436710358, 'accumulated_logging_time': 0.3210947513580322, 'global_step': 8237, 'preemption_count': 0}), (8975, {'train/accuracy': 0.9893003106117249, 'train/loss': 0.03609520196914673, 'train/mean_average_precision': 0.27442771710191144, 'validation/accuracy': 0.9861196279525757, 'validation/loss': 0.04653529077768326, 'validation/mean_average_precision': 0.21946127669537574, 'validation/num_examples': 43793, 'test/accuracy': 0.9852442741394043, 'test/loss': 0.04910749942064285, 'test/mean_average_precision': 0.22047892726404594, 'test/num_examples': 43793, 'score': 2899.9090049266815, 'total_duration': 4717.839361667633, 'accumulated_submission_time': 2899.9090049266815, 'accumulated_eval_time': 1817.3364193439484, 'accumulated_logging_time': 0.3503391742706299, 'global_step': 8975, 'preemption_count': 0}), (9731, {'train/accuracy': 0.989581823348999, 'train/loss': 0.035352617502212524, 'train/mean_average_precision': 0.3135008539187511, 'validation/accuracy': 0.9864249229431152, 'validation/loss': 0.045402102172374725, 'validation/mean_average_precision': 0.23789010410381536, 'validation/num_examples': 43793, 'test/accuracy': 0.985558032989502, 'test/loss': 0.04800809919834137, 'test/mean_average_precision': 0.23561684895981633, 'test/num_examples': 43793, 'score': 3139.976670026779, 'total_duration': 5083.68940782547, 'accumulated_submission_time': 3139.976670026779, 'accumulated_eval_time': 1943.068927526474, 'accumulated_logging_time': 0.38041067123413086, 'global_step': 9731, 'preemption_count': 0}), (10489, {'train/accuracy': 0.9896994233131409, 'train/loss': 0.03448774293065071, 'train/mean_average_precision': 0.3108126612908478, 'validation/accuracy': 0.9863623976707458, 'validation/loss': 0.045581333339214325, 'validation/mean_average_precision': 0.23510919648496872, 'validation/num_examples': 43793, 'test/accuracy': 0.9854817986488342, 'test/loss': 0.04808775708079338, 'test/mean_average_precision': 0.23582110774026796, 'test/num_examples': 43793, 'score': 3380.1543962955475, 'total_duration': 5455.0981414318085, 'accumulated_submission_time': 3380.1543962955475, 'accumulated_eval_time': 2074.2522208690643, 'accumulated_logging_time': 0.40826964378356934, 'global_step': 10489, 'preemption_count': 0}), (11242, {'train/accuracy': 0.9899011850357056, 'train/loss': 0.03379280865192413, 'train/mean_average_precision': 0.34200087874016116, 'validation/accuracy': 0.9865202903747559, 'validation/loss': 0.04525914043188095, 'validation/mean_average_precision': 0.2445117594369473, 'validation/num_examples': 43793, 'test/accuracy': 0.985623300075531, 'test/loss': 0.04814762994647026, 'test/mean_average_precision': 0.23960741514002484, 'test/num_examples': 43793, 'score': 3620.235187292099, 'total_duration': 5823.134348869324, 'accumulated_submission_time': 3620.235187292099, 'accumulated_eval_time': 2202.160078048706, 'accumulated_logging_time': 0.4365499019622803, 'global_step': 11242, 'preemption_count': 0}), (11989, {'train/accuracy': 0.990170955657959, 'train/loss': 0.032714128494262695, 'train/mean_average_precision': 0.3666973354664275, 'validation/accuracy': 0.9865543842315674, 'validation/loss': 0.04496646299958229, 'validation/mean_average_precision': 0.2531282590109368, 'validation/num_examples': 43793, 'test/accuracy': 0.9856700897216797, 'test/loss': 0.047432009130716324, 'test/mean_average_precision': 0.2479243786116264, 'test/num_examples': 43793, 'score': 3860.4547262191772, 'total_duration': 6193.2873792648315, 'accumulated_submission_time': 3860.4547262191772, 'accumulated_eval_time': 2332.0449130535126, 'accumulated_logging_time': 0.4658217430114746, 'global_step': 11989, 'preemption_count': 0}), (12739, {'train/accuracy': 0.990369439125061, 'train/loss': 0.032262638211250305, 'train/mean_average_precision': 0.3685053446267267, 'validation/accuracy': 0.9865767359733582, 'validation/loss': 0.04462543874979019, 'validation/mean_average_precision': 0.246913705823616, 'validation/num_examples': 43793, 'test/accuracy': 0.9857408404350281, 'test/loss': 0.04721754044294357, 'test/mean_average_precision': 0.24258684231373745, 'test/num_examples': 43793, 'score': 4100.5837614536285, 'total_duration': 6559.720872402191, 'accumulated_submission_time': 4100.5837614536285, 'accumulated_eval_time': 2458.2993903160095, 'accumulated_logging_time': 0.49584507942199707, 'global_step': 12739, 'preemption_count': 0}), (13486, {'train/accuracy': 0.9902468919754028, 'train/loss': 0.03229084610939026, 'train/mean_average_precision': 0.36819621667420155, 'validation/accuracy': 0.9864902496337891, 'validation/loss': 0.0449402891099453, 'validation/mean_average_precision': 0.2522724175171478, 'validation/num_examples': 43793, 'test/accuracy': 0.9856376647949219, 'test/loss': 0.04771548509597778, 'test/mean_average_precision': 0.24706367211131142, 'test/num_examples': 43793, 'score': 4340.619060993195, 'total_duration': 6927.721772909164, 'accumulated_submission_time': 4340.619060993195, 'accumulated_eval_time': 2586.215543985367, 'accumulated_logging_time': 0.5247492790222168, 'global_step': 13486, 'preemption_count': 0}), (14241, {'train/accuracy': 0.9904201030731201, 'train/loss': 0.03188631683588028, 'train/mean_average_precision': 0.3612297607359896, 'validation/accuracy': 0.9867191910743713, 'validation/loss': 0.04437703639268875, 'validation/mean_average_precision': 0.2567491818925974, 'validation/num_examples': 43793, 'test/accuracy': 0.9858179092407227, 'test/loss': 0.04711194708943367, 'test/mean_average_precision': 0.24733833804242544, 'test/num_examples': 43793, 'score': 4580.62403678894, 'total_duration': 7294.715516328812, 'accumulated_submission_time': 4580.62403678894, 'accumulated_eval_time': 2713.153738975525, 'accumulated_logging_time': 0.5549750328063965, 'global_step': 14241, 'preemption_count': 0}), (14995, {'train/accuracy': 0.9904540181159973, 'train/loss': 0.031819697469472885, 'train/mean_average_precision': 0.3714848644505445, 'validation/accuracy': 0.9867293238639832, 'validation/loss': 0.04427880048751831, 'validation/mean_average_precision': 0.26217762286830854, 'validation/num_examples': 43793, 'test/accuracy': 0.985866367816925, 'test/loss': 0.04707420989871025, 'test/mean_average_precision': 0.25019311136580263, 'test/num_examples': 43793, 'score': 4820.717617750168, 'total_duration': 7660.8544363975525, 'accumulated_submission_time': 4820.717617750168, 'accumulated_eval_time': 2839.149816274643, 'accumulated_logging_time': 0.584143877029419, 'global_step': 14995, 'preemption_count': 0}), (15738, {'train/accuracy': 0.9903156161308289, 'train/loss': 0.03219970688223839, 'train/mean_average_precision': 0.37523347033362875, 'validation/accuracy': 0.9866416454315186, 'validation/loss': 0.044646937400102615, 'validation/mean_average_precision': 0.25367239636785693, 'validation/num_examples': 43793, 'test/accuracy': 0.9857829809188843, 'test/loss': 0.04752212390303612, 'test/mean_average_precision': 0.25035524066505116, 'test/num_examples': 43793, 'score': 5060.922473907471, 'total_duration': 8031.429186582565, 'accumulated_submission_time': 5060.922473907471, 'accumulated_eval_time': 2969.4709827899933, 'accumulated_logging_time': 0.6131572723388672, 'global_step': 15738, 'preemption_count': 0}), (16488, {'train/accuracy': 0.9904052019119263, 'train/loss': 0.031755246222019196, 'train/mean_average_precision': 0.364807285871065, 'validation/accuracy': 0.9867590069770813, 'validation/loss': 0.04437119886279106, 'validation/mean_average_precision': 0.2592793316141011, 'validation/num_examples': 43793, 'test/accuracy': 0.9859042763710022, 'test/loss': 0.04704057425260544, 'test/mean_average_precision': 0.25100582192182425, 'test/num_examples': 43793, 'score': 5301.190806150436, 'total_duration': 8401.024397611618, 'accumulated_submission_time': 5301.190806150436, 'accumulated_eval_time': 3098.746400117874, 'accumulated_logging_time': 0.6441349983215332, 'global_step': 16488, 'preemption_count': 0}), (17237, {'train/accuracy': 0.9905096292495728, 'train/loss': 0.03137778863310814, 'train/mean_average_precision': 0.38399182104575913, 'validation/accuracy': 0.986682653427124, 'validation/loss': 0.04443146660923958, 'validation/mean_average_precision': 0.260249022031754, 'validation/num_examples': 43793, 'test/accuracy': 0.9857720136642456, 'test/loss': 0.04699244722723961, 'test/mean_average_precision': 0.2551105020349281, 'test/num_examples': 43793, 'score': 5541.426566362381, 'total_duration': 8768.900849103928, 'accumulated_submission_time': 5541.426566362381, 'accumulated_eval_time': 3226.335287809372, 'accumulated_logging_time': 0.6756045818328857, 'global_step': 17237, 'preemption_count': 0}), (17983, {'train/accuracy': 0.9906891584396362, 'train/loss': 0.030599458143115044, 'train/mean_average_precision': 0.40471017588618474, 'validation/accuracy': 0.9867675304412842, 'validation/loss': 0.044316284358501434, 'validation/mean_average_precision': 0.2684684903382087, 'validation/num_examples': 43793, 'test/accuracy': 0.9858996272087097, 'test/loss': 0.04708460345864296, 'test/mean_average_precision': 0.25263314540650667, 'test/num_examples': 43793, 'score': 5781.60738492012, 'total_duration': 9138.323543548584, 'accumulated_submission_time': 5781.60738492012, 'accumulated_eval_time': 3355.5258157253265, 'accumulated_logging_time': 0.706317663192749, 'global_step': 17983, 'preemption_count': 0}), (18719, {'train/accuracy': 0.9908905029296875, 'train/loss': 0.030075056478381157, 'train/mean_average_precision': 0.4239957922232233, 'validation/accuracy': 0.9867285490036011, 'validation/loss': 0.04435645788908005, 'validation/mean_average_precision': 0.2595765740247147, 'validation/num_examples': 43793, 'test/accuracy': 0.985871434211731, 'test/loss': 0.04702959209680557, 'test/mean_average_precision': 0.2514217514588064, 'test/num_examples': 43793, 'score': 6021.643416404724, 'total_duration': 9508.266571044922, 'accumulated_submission_time': 6021.643416404724, 'accumulated_eval_time': 3485.375589132309, 'accumulated_logging_time': 0.7408199310302734, 'global_step': 18719, 'preemption_count': 0}), (19464, {'train/accuracy': 0.991010844707489, 'train/loss': 0.02957056649029255, 'train/mean_average_precision': 0.4344861831450195, 'validation/accuracy': 0.9867455959320068, 'validation/loss': 0.04412097483873367, 'validation/mean_average_precision': 0.2650249423300213, 'validation/num_examples': 43793, 'test/accuracy': 0.9858448505401611, 'test/loss': 0.046876560896635056, 'test/mean_average_precision': 0.2534075491441924, 'test/num_examples': 43793, 'score': 6261.605680465698, 'total_duration': 9874.90622472763, 'accumulated_submission_time': 6261.605680465698, 'accumulated_eval_time': 3611.9990952014923, 'accumulated_logging_time': 0.7735686302185059, 'global_step': 19464, 'preemption_count': 0}), (20208, {'train/accuracy': 0.9910753965377808, 'train/loss': 0.02934957481920719, 'train/mean_average_precision': 0.43785858435166203, 'validation/accuracy': 0.9867947101593018, 'validation/loss': 0.044208772480487823, 'validation/mean_average_precision': 0.26959581501836405, 'validation/num_examples': 43793, 'test/accuracy': 0.9859581589698792, 'test/loss': 0.04711531475186348, 'test/mean_average_precision': 0.2562529002068713, 'test/num_examples': 43793, 'score': 6501.562925338745, 'total_duration': 10242.721267700195, 'accumulated_submission_time': 6501.562925338745, 'accumulated_eval_time': 3739.8063855171204, 'accumulated_logging_time': 0.8040339946746826, 'global_step': 20208, 'preemption_count': 0}), (20958, {'train/accuracy': 0.9908106327056885, 'train/loss': 0.030077271163463593, 'train/mean_average_precision': 0.407418383224706, 'validation/accuracy': 0.9868442416191101, 'validation/loss': 0.04448039084672928, 'validation/mean_average_precision': 0.266787557603906, 'validation/num_examples': 43793, 'test/accuracy': 0.9859585762023926, 'test/loss': 0.04752559959888458, 'test/mean_average_precision': 0.25231698359918087, 'test/num_examples': 43793, 'score': 6741.6271460056305, 'total_duration': 10610.854069948196, 'accumulated_submission_time': 6741.6271460056305, 'accumulated_eval_time': 3867.823861837387, 'accumulated_logging_time': 0.8348932266235352, 'global_step': 20958, 'preemption_count': 0}), (21690, {'train/accuracy': 0.9906601905822754, 'train/loss': 0.03099336288869381, 'train/mean_average_precision': 0.4074115877835104, 'validation/accuracy': 0.9866725206375122, 'validation/loss': 0.04473439231514931, 'validation/mean_average_precision': 0.26206147107245853, 'validation/num_examples': 43793, 'test/accuracy': 0.9858292937278748, 'test/loss': 0.04747376590967178, 'test/mean_average_precision': 0.2580787181285667, 'test/num_examples': 43793, 'score': 6981.642642021179, 'total_duration': 10978.547773361206, 'accumulated_submission_time': 6981.642642021179, 'accumulated_eval_time': 3995.446721315384, 'accumulated_logging_time': 0.8683032989501953, 'global_step': 21690, 'preemption_count': 0}), (22438, {'train/accuracy': 0.9909005165100098, 'train/loss': 0.02995622344315052, 'train/mean_average_precision': 0.3997272030494833, 'validation/accuracy': 0.986880362033844, 'validation/loss': 0.044299740344285965, 'validation/mean_average_precision': 0.2655132926619737, 'validation/num_examples': 43793, 'test/accuracy': 0.9859687089920044, 'test/loss': 0.04701925814151764, 'test/mean_average_precision': 0.25742846013030607, 'test/num_examples': 43793, 'score': 7221.879897594452, 'total_duration': 11349.581412315369, 'accumulated_submission_time': 7221.879897594452, 'accumulated_eval_time': 4126.191339492798, 'accumulated_logging_time': 0.8996272087097168, 'global_step': 22438, 'preemption_count': 0}), (23191, {'train/accuracy': 0.990909993648529, 'train/loss': 0.030140511691570282, 'train/mean_average_precision': 0.415039702271021, 'validation/accuracy': 0.9867057800292969, 'validation/loss': 0.04418215900659561, 'validation/mean_average_precision': 0.26283213286633994, 'validation/num_examples': 43793, 'test/accuracy': 0.9859480857849121, 'test/loss': 0.04676768556237221, 'test/mean_average_precision': 0.25236883629083695, 'test/num_examples': 43793, 'score': 7462.1232216358185, 'total_duration': 11715.854789495468, 'accumulated_submission_time': 7462.1232216358185, 'accumulated_eval_time': 4252.171051979065, 'accumulated_logging_time': 0.9298944473266602, 'global_step': 23191, 'preemption_count': 0}), (23939, {'train/accuracy': 0.9909388422966003, 'train/loss': 0.02958683855831623, 'train/mean_average_precision': 0.4346094288609707, 'validation/accuracy': 0.9869027137756348, 'validation/loss': 0.04411791265010834, 'validation/mean_average_precision': 0.2732023445683491, 'validation/num_examples': 43793, 'test/accuracy': 0.9859236478805542, 'test/loss': 0.04697485640645027, 'test/mean_average_precision': 0.2573575046521077, 'test/num_examples': 43793, 'score': 7701.862661600113, 'total_duration': 12085.510291099548, 'accumulated_submission_time': 7701.862661600113, 'accumulated_eval_time': 4381.744997501373, 'accumulated_logging_time': 1.251793384552002, 'global_step': 23939, 'preemption_count': 0}), (24693, {'train/accuracy': 0.991054356098175, 'train/loss': 0.02922392450273037, 'train/mean_average_precision': 0.42748873190217074, 'validation/accuracy': 0.986707866191864, 'validation/loss': 0.04421435669064522, 'validation/mean_average_precision': 0.26628845857204436, 'validation/num_examples': 43793, 'test/accuracy': 0.9858819246292114, 'test/loss': 0.04682368040084839, 'test/mean_average_precision': 0.2541363107731255, 'test/num_examples': 43793, 'score': 7941.946875095367, 'total_duration': 12450.460037469864, 'accumulated_submission_time': 7941.946875095367, 'accumulated_eval_time': 4506.559768199921, 'accumulated_logging_time': 1.282576322555542, 'global_step': 24693, 'preemption_count': 0}), (25443, {'train/accuracy': 0.9912801384925842, 'train/loss': 0.02853528782725334, 'train/mean_average_precision': 0.45084242740785924, 'validation/accuracy': 0.9867947101593018, 'validation/loss': 0.044394586235284805, 'validation/mean_average_precision': 0.26479474497860095, 'validation/num_examples': 43793, 'test/accuracy': 0.98597252368927, 'test/loss': 0.04716556519269943, 'test/mean_average_precision': 0.25968966276112876, 'test/num_examples': 43793, 'score': 8181.9895396232605, 'total_duration': 12817.27622628212, 'accumulated_submission_time': 8181.9895396232605, 'accumulated_eval_time': 4633.280071258545, 'accumulated_logging_time': 1.3154983520507812, 'global_step': 25443, 'preemption_count': 0}), (26189, {'train/accuracy': 0.9914413094520569, 'train/loss': 0.027967434376478195, 'train/mean_average_precision': 0.47034527053164865, 'validation/accuracy': 0.9867812991142273, 'validation/loss': 0.04452061280608177, 'validation/mean_average_precision': 0.2657041952537768, 'validation/num_examples': 43793, 'test/accuracy': 0.9858335256576538, 'test/loss': 0.04735587537288666, 'test/mean_average_precision': 0.2510148645408534, 'test/num_examples': 43793, 'score': 8421.981696367264, 'total_duration': 13182.20546245575, 'accumulated_submission_time': 8421.981696367264, 'accumulated_eval_time': 4758.165808677673, 'accumulated_logging_time': 1.3469769954681396, 'global_step': 26189, 'preemption_count': 0}), (26938, {'train/accuracy': 0.9915258884429932, 'train/loss': 0.02765055000782013, 'train/mean_average_precision': 0.47607650501801335, 'validation/accuracy': 0.9869157075881958, 'validation/loss': 0.044607408344745636, 'validation/mean_average_precision': 0.2645389905862119, 'validation/num_examples': 43793, 'test/accuracy': 0.9859678745269775, 'test/loss': 0.04747622832655907, 'test/mean_average_precision': 0.2551753097398039, 'test/num_examples': 43793, 'score': 8662.139254808426, 'total_duration': 13551.315371513367, 'accumulated_submission_time': 8662.139254808426, 'accumulated_eval_time': 4887.066943645477, 'accumulated_logging_time': 1.378154993057251, 'global_step': 26938, 'preemption_count': 0}), (27690, {'train/accuracy': 0.991478443145752, 'train/loss': 0.02810160256922245, 'train/mean_average_precision': 0.46717011640930434, 'validation/accuracy': 0.9867671132087708, 'validation/loss': 0.04437020421028137, 'validation/mean_average_precision': 0.26535027311657716, 'validation/num_examples': 43793, 'test/accuracy': 0.9857825636863708, 'test/loss': 0.047222595661878586, 'test/mean_average_precision': 0.25155722622236826, 'test/num_examples': 43793, 'score': 8902.194403409958, 'total_duration': 13919.763410568237, 'accumulated_submission_time': 8902.194403409958, 'accumulated_eval_time': 5015.407616376877, 'accumulated_logging_time': 1.4100637435913086, 'global_step': 27690, 'preemption_count': 0}), (28425, {'train/accuracy': 0.9911364912986755, 'train/loss': 0.0287772286683321, 'train/mean_average_precision': 0.4375929169048969, 'validation/accuracy': 0.9868263602256775, 'validation/loss': 0.04454055801033974, 'validation/mean_average_precision': 0.2722830766897607, 'validation/num_examples': 43793, 'test/accuracy': 0.9859636425971985, 'test/loss': 0.047429975122213364, 'test/mean_average_precision': 0.25800364118637026, 'test/num_examples': 43793, 'score': 9142.177165269852, 'total_duration': 14288.520901203156, 'accumulated_submission_time': 9142.177165269852, 'accumulated_eval_time': 5144.1230499744415, 'accumulated_logging_time': 1.4474318027496338, 'global_step': 28425, 'preemption_count': 0}), (29159, {'train/accuracy': 0.9911613464355469, 'train/loss': 0.028942160308361053, 'train/mean_average_precision': 0.4474896226765509, 'validation/accuracy': 0.9867890477180481, 'validation/loss': 0.04471442103385925, 'validation/mean_average_precision': 0.2646365791157178, 'validation/num_examples': 43793, 'test/accuracy': 0.9859611392021179, 'test/loss': 0.04742545634508133, 'test/mean_average_precision': 0.25873416160326057, 'test/num_examples': 43793, 'score': 9382.21336388588, 'total_duration': 14656.361695289612, 'accumulated_submission_time': 9382.21336388588, 'accumulated_eval_time': 5271.874435424805, 'accumulated_logging_time': 1.4804432392120361, 'global_step': 29159, 'preemption_count': 0}), (29899, {'train/accuracy': 0.9912360906600952, 'train/loss': 0.028651952743530273, 'train/mean_average_precision': 0.44693550007320454, 'validation/accuracy': 0.9867979288101196, 'validation/loss': 0.044605981558561325, 'validation/mean_average_precision': 0.2667757483159876, 'validation/num_examples': 43793, 'test/accuracy': 0.9859535694122314, 'test/loss': 0.04737938195466995, 'test/mean_average_precision': 0.25972637877089966, 'test/num_examples': 43793, 'score': 9622.341174840927, 'total_duration': 15026.982689142227, 'accumulated_submission_time': 9622.341174840927, 'accumulated_eval_time': 5402.314550876617, 'accumulated_logging_time': 1.5125842094421387, 'global_step': 29899, 'preemption_count': 0}), (30634, {'train/accuracy': 0.9913098812103271, 'train/loss': 0.028394658118486404, 'train/mean_average_precision': 0.4532779013963894, 'validation/accuracy': 0.9868007898330688, 'validation/loss': 0.04478731378912926, 'validation/mean_average_precision': 0.26890931603105683, 'validation/num_examples': 43793, 'test/accuracy': 0.9859750270843506, 'test/loss': 0.047733910381793976, 'test/mean_average_precision': 0.26015990846490455, 'test/num_examples': 43793, 'score': 9862.550683259964, 'total_duration': 15398.330970048904, 'accumulated_submission_time': 9862.550683259964, 'accumulated_eval_time': 5533.393811702728, 'accumulated_logging_time': 1.550180196762085, 'global_step': 30634, 'preemption_count': 0}), (31379, {'train/accuracy': 0.9914488196372986, 'train/loss': 0.027851860970258713, 'train/mean_average_precision': 0.4698440030144065, 'validation/accuracy': 0.9867720007896423, 'validation/loss': 0.04453831538558006, 'validation/mean_average_precision': 0.27013443583727287, 'validation/num_examples': 43793, 'test/accuracy': 0.9859640598297119, 'test/loss': 0.04747891426086426, 'test/mean_average_precision': 0.2624128359043144, 'test/num_examples': 43793, 'score': 10102.536264419556, 'total_duration': 15764.439517259598, 'accumulated_submission_time': 10102.536264419556, 'accumulated_eval_time': 5659.4643721580505, 'accumulated_logging_time': 1.5825748443603516, 'global_step': 31379, 'preemption_count': 0}), (32124, {'train/accuracy': 0.9916564226150513, 'train/loss': 0.027042893692851067, 'train/mean_average_precision': 0.49447185356203693, 'validation/accuracy': 0.9868767261505127, 'validation/loss': 0.044268108904361725, 'validation/mean_average_precision': 0.271961120654364, 'validation/num_examples': 43793, 'test/accuracy': 0.9859771132469177, 'test/loss': 0.0472414530813694, 'test/mean_average_precision': 0.2615611187014712, 'test/num_examples': 43793, 'score': 10342.64687371254, 'total_duration': 16131.504002094269, 'accumulated_submission_time': 10342.64687371254, 'accumulated_eval_time': 5786.365542411804, 'accumulated_logging_time': 1.615133285522461, 'global_step': 32124, 'preemption_count': 0}), (32856, {'train/accuracy': 0.99176424741745, 'train/loss': 0.026609620079398155, 'train/mean_average_precision': 0.49882654041636504, 'validation/accuracy': 0.9868531823158264, 'validation/loss': 0.04436594992876053, 'validation/mean_average_precision': 0.2692807413861312, 'validation/num_examples': 43793, 'test/accuracy': 0.9860023856163025, 'test/loss': 0.04726454243063927, 'test/mean_average_precision': 0.2620336225403224, 'test/num_examples': 43793, 'score': 10582.716272592545, 'total_duration': 16494.133712530136, 'accumulated_submission_time': 10582.716272592545, 'accumulated_eval_time': 5908.865457057953, 'accumulated_logging_time': 1.652513027191162, 'global_step': 32856, 'preemption_count': 0}), (33595, {'train/accuracy': 0.9918482303619385, 'train/loss': 0.026468100026249886, 'train/mean_average_precision': 0.5026169864173307, 'validation/accuracy': 0.9868466854095459, 'validation/loss': 0.04508126527070999, 'validation/mean_average_precision': 0.2694701894953767, 'validation/num_examples': 43793, 'test/accuracy': 0.9859889149665833, 'test/loss': 0.0477740578353405, 'test/mean_average_precision': 0.26349125057822476, 'test/num_examples': 43793, 'score': 10822.800383806229, 'total_duration': 16862.094605207443, 'accumulated_submission_time': 10822.800383806229, 'accumulated_eval_time': 6036.689339399338, 'accumulated_logging_time': 1.6855049133300781, 'global_step': 33595, 'preemption_count': 0}), (34335, {'train/accuracy': 0.991648256778717, 'train/loss': 0.027057519182562828, 'train/mean_average_precision': 0.4754191822452325, 'validation/accuracy': 0.9869412779808044, 'validation/loss': 0.04477310925722122, 'validation/mean_average_precision': 0.2740292724154801, 'validation/num_examples': 43793, 'test/accuracy': 0.9860196709632874, 'test/loss': 0.04748547822237015, 'test/mean_average_precision': 0.25978952719180953, 'test/num_examples': 43793, 'score': 11062.89523434639, 'total_duration': 17230.688241004944, 'accumulated_submission_time': 11062.89523434639, 'accumulated_eval_time': 6165.134907007217, 'accumulated_logging_time': 1.7191433906555176, 'global_step': 34335, 'preemption_count': 0}), (35083, {'train/accuracy': 0.9914780855178833, 'train/loss': 0.02767196297645569, 'train/mean_average_precision': 0.4771699893653773, 'validation/accuracy': 0.9869469404220581, 'validation/loss': 0.044476427137851715, 'validation/mean_average_precision': 0.2762864765832121, 'validation/num_examples': 43793, 'test/accuracy': 0.9860441088676453, 'test/loss': 0.047148123383522034, 'test/mean_average_precision': 0.26597520521896023, 'test/num_examples': 43793, 'score': 11302.936974048615, 'total_duration': 17599.211398363113, 'accumulated_submission_time': 11302.936974048615, 'accumulated_eval_time': 6293.562938928604, 'accumulated_logging_time': 1.7529196739196777, 'global_step': 35083, 'preemption_count': 0}), (35831, {'train/accuracy': 0.9915109276771545, 'train/loss': 0.027474919334053993, 'train/mean_average_precision': 0.4855648311323703, 'validation/accuracy': 0.9869274497032166, 'validation/loss': 0.04452328383922577, 'validation/mean_average_precision': 0.2701972526758367, 'validation/num_examples': 43793, 'test/accuracy': 0.9860904216766357, 'test/loss': 0.04754072427749634, 'test/mean_average_precision': 0.2623007088811127, 'test/num_examples': 43793, 'score': 11543.209511995316, 'total_duration': 17968.17503976822, 'accumulated_submission_time': 11543.209511995316, 'accumulated_eval_time': 6422.1980040073395, 'accumulated_logging_time': 1.7894747257232666, 'global_step': 35831, 'preemption_count': 0}), (36577, {'train/accuracy': 0.9916188716888428, 'train/loss': 0.027239065617322922, 'train/mean_average_precision': 0.48032752663675593, 'validation/accuracy': 0.9868852496147156, 'validation/loss': 0.044582299888134, 'validation/mean_average_precision': 0.271263958619182, 'validation/num_examples': 43793, 'test/accuracy': 0.9860007166862488, 'test/loss': 0.04743095487356186, 'test/mean_average_precision': 0.2636249846278241, 'test/num_examples': 43793, 'score': 11783.2023396492, 'total_duration': 18335.44613981247, 'accumulated_submission_time': 11783.2023396492, 'accumulated_eval_time': 6549.4206647872925, 'accumulated_logging_time': 1.8252544403076172, 'global_step': 36577, 'preemption_count': 0}), (37323, {'train/accuracy': 0.9916990995407104, 'train/loss': 0.026861457154154778, 'train/mean_average_precision': 0.48426314836571605, 'validation/accuracy': 0.9868641495704651, 'validation/loss': 0.044839050620794296, 'validation/mean_average_precision': 0.2684500922044856, 'validation/num_examples': 43793, 'test/accuracy': 0.9859986305236816, 'test/loss': 0.04797624424099922, 'test/mean_average_precision': 0.2591521938112194, 'test/num_examples': 43793, 'score': 12023.32865190506, 'total_duration': 18702.27978849411, 'accumulated_submission_time': 12023.32865190506, 'accumulated_eval_time': 6676.074639558792, 'accumulated_logging_time': 1.858780860900879, 'global_step': 37323, 'preemption_count': 0}), (38072, {'train/accuracy': 0.9920689463615417, 'train/loss': 0.02564818225800991, 'train/mean_average_precision': 0.516311374792447, 'validation/accuracy': 0.9868032336235046, 'validation/loss': 0.04461022838950157, 'validation/mean_average_precision': 0.2661784733789995, 'validation/num_examples': 43793, 'test/accuracy': 0.9859472513198853, 'test/loss': 0.04742193594574928, 'test/mean_average_precision': 0.2636671236942603, 'test/num_examples': 43793, 'score': 12263.584111452103, 'total_duration': 19068.745487689972, 'accumulated_submission_time': 12263.584111452103, 'accumulated_eval_time': 6802.231438398361, 'accumulated_logging_time': 1.8928401470184326, 'global_step': 38072, 'preemption_count': 0}), (38821, {'train/accuracy': 0.9921510815620422, 'train/loss': 0.025551440194249153, 'train/mean_average_precision': 0.5158998215716353, 'validation/accuracy': 0.9867951273918152, 'validation/loss': 0.044609975069761276, 'validation/mean_average_precision': 0.27445774012902946, 'validation/num_examples': 43793, 'test/accuracy': 0.9859569072723389, 'test/loss': 0.04757148399949074, 'test/mean_average_precision': 0.26158094714116725, 'test/num_examples': 43793, 'score': 12503.646212339401, 'total_duration': 19435.058240175247, 'accumulated_submission_time': 12503.646212339401, 'accumulated_eval_time': 6928.428899765015, 'accumulated_logging_time': 1.9264566898345947, 'global_step': 38821, 'preemption_count': 0}), (39573, {'train/accuracy': 0.990967333316803, 'train/loss': 0.029311208054423332, 'train/mean_average_precision': 0.45690423259645163, 'validation/accuracy': 0.986046552658081, 'validation/loss': 0.04767151176929474, 'validation/mean_average_precision': 0.233260071570542, 'validation/num_examples': 43793, 'test/accuracy': 0.9850488305091858, 'test/loss': 0.051080767065286636, 'test/mean_average_precision': 0.2129662973348563, 'test/num_examples': 43793, 'score': 12743.860652923584, 'total_duration': 19803.659139871597, 'accumulated_submission_time': 12743.860652923584, 'accumulated_eval_time': 7056.762053728104, 'accumulated_logging_time': 1.9602704048156738, 'global_step': 39573, 'preemption_count': 0}), (40320, {'train/accuracy': 0.9924874901771545, 'train/loss': 0.024460695683956146, 'train/mean_average_precision': 0.5351341203817217, 'validation/accuracy': 0.9868787527084351, 'validation/loss': 0.04459439218044281, 'validation/mean_average_precision': 0.2759908697710495, 'validation/num_examples': 43793, 'test/accuracy': 0.9859982132911682, 'test/loss': 0.0479002371430397, 'test/mean_average_precision': 0.26398226322551643, 'test/num_examples': 43793, 'score': 12983.941451787949, 'total_duration': 20167.698737859726, 'accumulated_submission_time': 12983.941451787949, 'accumulated_eval_time': 7180.665674209595, 'accumulated_logging_time': 1.9954063892364502, 'global_step': 40320, 'preemption_count': 0}), (41067, {'train/accuracy': 0.9923626780509949, 'train/loss': 0.02495507150888443, 'train/mean_average_precision': 0.5233125303946242, 'validation/accuracy': 0.9868706464767456, 'validation/loss': 0.044303104281425476, 'validation/mean_average_precision': 0.2764290908345192, 'validation/num_examples': 43793, 'test/accuracy': 0.9859969019889832, 'test/loss': 0.04726998880505562, 'test/mean_average_precision': 0.26029674691171534, 'test/num_examples': 43793, 'score': 13224.08312869072, 'total_duration': 20527.777365922928, 'accumulated_submission_time': 13224.08312869072, 'accumulated_eval_time': 7300.5468554496765, 'accumulated_logging_time': 2.0311975479125977, 'global_step': 41067, 'preemption_count': 0}), (41813, {'train/accuracy': 0.9921444058418274, 'train/loss': 0.02528195083141327, 'train/mean_average_precision': 0.528187037457823, 'validation/accuracy': 0.9869668483734131, 'validation/loss': 0.044966407120227814, 'validation/mean_average_precision': 0.2712547909511459, 'validation/num_examples': 43793, 'test/accuracy': 0.9860803484916687, 'test/loss': 0.047967229038476944, 'test/mean_average_precision': 0.26475872867935635, 'test/num_examples': 43793, 'score': 13464.266607761383, 'total_duration': 20893.67218565941, 'accumulated_submission_time': 13464.266607761383, 'accumulated_eval_time': 7426.20378446579, 'accumulated_logging_time': 2.0658912658691406, 'global_step': 41813, 'preemption_count': 0}), (42551, {'train/accuracy': 0.9920623898506165, 'train/loss': 0.02556413970887661, 'train/mean_average_precision': 0.5207061522784611, 'validation/accuracy': 0.9869310855865479, 'validation/loss': 0.04514552652835846, 'validation/mean_average_precision': 0.2757223796749376, 'validation/num_examples': 43793, 'test/accuracy': 0.9861013889312744, 'test/loss': 0.0479997955262661, 'test/mean_average_precision': 0.26192487437292206, 'test/num_examples': 43793, 'score': 13704.357329368591, 'total_duration': 21258.0973508358, 'accumulated_submission_time': 13704.357329368591, 'accumulated_eval_time': 7550.482100486755, 'accumulated_logging_time': 2.1024575233459473, 'global_step': 42551, 'preemption_count': 0}), (43293, {'train/accuracy': 0.9919952154159546, 'train/loss': 0.025652771815657616, 'train/mean_average_precision': 0.5196389960976198, 'validation/accuracy': 0.9868032336235046, 'validation/loss': 0.0451744943857193, 'validation/mean_average_precision': 0.27337764480607746, 'validation/num_examples': 43793, 'test/accuracy': 0.9859657287597656, 'test/loss': 0.048261333256959915, 'test/mean_average_precision': 0.2562264799589845, 'test/num_examples': 43793, 'score': 13944.305476903915, 'total_duration': 21624.305897712708, 'accumulated_submission_time': 13944.305476903915, 'accumulated_eval_time': 7676.686524152756, 'accumulated_logging_time': 2.1386983394622803, 'global_step': 43293, 'preemption_count': 0}), (44038, {'train/accuracy': 0.9922802448272705, 'train/loss': 0.02483881264925003, 'train/mean_average_precision': 0.5377617439585939, 'validation/accuracy': 0.9868596792221069, 'validation/loss': 0.04524949565529823, 'validation/mean_average_precision': 0.270351017601974, 'validation/num_examples': 43793, 'test/accuracy': 0.9860146045684814, 'test/loss': 0.04815695807337761, 'test/mean_average_precision': 0.2617285586172711, 'test/num_examples': 43793, 'score': 14184.531028747559, 'total_duration': 21993.115421056747, 'accumulated_submission_time': 14184.531028747559, 'accumulated_eval_time': 7805.214061498642, 'accumulated_logging_time': 2.1753900051116943, 'global_step': 44038, 'preemption_count': 0}), (44777, {'train/accuracy': 0.9924088716506958, 'train/loss': 0.02422436513006687, 'train/mean_average_precision': 0.5362641543231005, 'validation/accuracy': 0.9869104027748108, 'validation/loss': 0.04534335061907768, 'validation/mean_average_precision': 0.26956582298002385, 'validation/num_examples': 43793, 'test/accuracy': 0.9860327243804932, 'test/loss': 0.0481882318854332, 'test/mean_average_precision': 0.26161517705688725, 'test/num_examples': 43793, 'score': 14424.795090436935, 'total_duration': 22358.681414604187, 'accumulated_submission_time': 14424.795090436935, 'accumulated_eval_time': 7930.460354089737, 'accumulated_logging_time': 2.2115097045898438, 'global_step': 44777, 'preemption_count': 0}), (45515, {'train/accuracy': 0.9927688837051392, 'train/loss': 0.023114778101444244, 'train/mean_average_precision': 0.5721043220273573, 'validation/accuracy': 0.9869270324707031, 'validation/loss': 0.045256517827510834, 'validation/mean_average_precision': 0.2640619486550432, 'validation/num_examples': 43793, 'test/accuracy': 0.9860420227050781, 'test/loss': 0.04842713475227356, 'test/mean_average_precision': 0.2579358481884553, 'test/num_examples': 43793, 'score': 14664.76844573021, 'total_duration': 22726.34547829628, 'accumulated_submission_time': 14664.76844573021, 'accumulated_eval_time': 8058.0960521698, 'accumulated_logging_time': 2.247072696685791, 'global_step': 45515, 'preemption_count': 0}), (46262, {'train/accuracy': 0.9930108189582825, 'train/loss': 0.022385746240615845, 'train/mean_average_precision': 0.5915540953232792, 'validation/accuracy': 0.9868503212928772, 'validation/loss': 0.045576926320791245, 'validation/mean_average_precision': 0.26865391262902105, 'validation/num_examples': 43793, 'test/accuracy': 0.985990583896637, 'test/loss': 0.048694908618927, 'test/mean_average_precision': 0.25982432242618936, 'test/num_examples': 43793, 'score': 14904.87128996849, 'total_duration': 23092.4700255394, 'accumulated_submission_time': 14904.87128996849, 'accumulated_eval_time': 8184.0625557899475, 'accumulated_logging_time': 2.2822532653808594, 'global_step': 46262, 'preemption_count': 0}), (47008, {'train/accuracy': 0.993267297744751, 'train/loss': 0.02183414436876774, 'train/mean_average_precision': 0.6045476022820708, 'validation/accuracy': 0.9868068695068359, 'validation/loss': 0.045824650675058365, 'validation/mean_average_precision': 0.27189078779344356, 'validation/num_examples': 43793, 'test/accuracy': 0.9859114289283752, 'test/loss': 0.048689164221286774, 'test/mean_average_precision': 0.264437696290971, 'test/num_examples': 43793, 'score': 15144.95430803299, 'total_duration': 23455.587432146072, 'accumulated_submission_time': 15144.95430803299, 'accumulated_eval_time': 8307.041923999786, 'accumulated_logging_time': 2.317678451538086, 'global_step': 47008, 'preemption_count': 0}), (47748, {'train/accuracy': 0.992554783821106, 'train/loss': 0.02386317029595375, 'train/mean_average_precision': 0.5605475454067415, 'validation/accuracy': 0.9868738651275635, 'validation/loss': 0.04586609825491905, 'validation/mean_average_precision': 0.274083811671372, 'validation/num_examples': 43793, 'test/accuracy': 0.9859864115715027, 'test/loss': 0.04919857159256935, 'test/mean_average_precision': 0.25760008765950376, 'test/num_examples': 43793, 'score': 15385.188156366348, 'total_duration': 23823.495221614838, 'accumulated_submission_time': 15385.188156366348, 'accumulated_eval_time': 8434.66095161438, 'accumulated_logging_time': 2.353482723236084, 'global_step': 47748, 'preemption_count': 0}), (48496, {'train/accuracy': 0.9926549792289734, 'train/loss': 0.023603614419698715, 'train/mean_average_precision': 0.5488211291481068, 'validation/accuracy': 0.9868596792221069, 'validation/loss': 0.04607483372092247, 'validation/mean_average_precision': 0.2715950909297707, 'validation/num_examples': 43793, 'test/accuracy': 0.9859628081321716, 'test/loss': 0.04901837185025215, 'test/mean_average_precision': 0.26013198771595913, 'test/num_examples': 43793, 'score': 15625.320230960846, 'total_duration': 24188.14550971985, 'accumulated_submission_time': 15625.320230960846, 'accumulated_eval_time': 8559.123657226562, 'accumulated_logging_time': 2.389343738555908, 'global_step': 48496, 'preemption_count': 0}), (49243, {'train/accuracy': 0.9927665591239929, 'train/loss': 0.02316511981189251, 'train/mean_average_precision': 0.5600378176647713, 'validation/accuracy': 0.9868649244308472, 'validation/loss': 0.04596903175115585, 'validation/mean_average_precision': 0.27291857464366176, 'validation/num_examples': 43793, 'test/accuracy': 0.9859880805015564, 'test/loss': 0.049083225429058075, 'test/mean_average_precision': 0.2621291751755614, 'test/num_examples': 43793, 'score': 15865.286841392517, 'total_duration': 24550.717749118805, 'accumulated_submission_time': 15865.286841392517, 'accumulated_eval_time': 8681.672759532928, 'accumulated_logging_time': 2.4259297847747803, 'global_step': 49243, 'preemption_count': 0}), (49988, {'train/accuracy': 0.9928274750709534, 'train/loss': 0.022933220490813255, 'train/mean_average_precision': 0.5803153113657378, 'validation/accuracy': 0.9868227243423462, 'validation/loss': 0.04655434563755989, 'validation/mean_average_precision': 0.2713811193405487, 'validation/num_examples': 43793, 'test/accuracy': 0.9858924746513367, 'test/loss': 0.04988021403551102, 'test/mean_average_precision': 0.26065111811903857, 'test/num_examples': 43793, 'score': 16105.240637540817, 'total_duration': 24909.658252477646, 'accumulated_submission_time': 16105.240637540817, 'accumulated_eval_time': 8800.601737260818, 'accumulated_logging_time': 2.4639928340911865, 'global_step': 49988, 'preemption_count': 0}), (50738, {'train/accuracy': 0.992751955986023, 'train/loss': 0.023076120764017105, 'train/mean_average_precision': 0.5672821856545007, 'validation/accuracy': 0.9868150353431702, 'validation/loss': 0.04626094922423363, 'validation/mean_average_precision': 0.2708342805840612, 'validation/num_examples': 43793, 'test/accuracy': 0.98593670129776, 'test/loss': 0.04970936104655266, 'test/mean_average_precision': 0.26015209116361343, 'test/num_examples': 43793, 'score': 16345.393598794937, 'total_duration': 25270.969877958298, 'accumulated_submission_time': 16345.393598794937, 'accumulated_eval_time': 8921.704628229141, 'accumulated_logging_time': 2.500322103500366, 'global_step': 50738, 'preemption_count': 0}), (51489, {'train/accuracy': 0.9931676983833313, 'train/loss': 0.021803244948387146, 'train/mean_average_precision': 0.5953628427219528, 'validation/accuracy': 0.9868471026420593, 'validation/loss': 0.04647331312298775, 'validation/mean_average_precision': 0.2731393930659657, 'validation/num_examples': 43793, 'test/accuracy': 0.9858849048614502, 'test/loss': 0.049809858202934265, 'test/mean_average_precision': 0.263006766620887, 'test/num_examples': 43793, 'score': 16585.514472723007, 'total_duration': 25633.673954486847, 'accumulated_submission_time': 16585.514472723007, 'accumulated_eval_time': 9044.232073783875, 'accumulated_logging_time': 2.5364415645599365, 'global_step': 51489, 'preemption_count': 0}), (52235, {'train/accuracy': 0.993556797504425, 'train/loss': 0.020664073526859283, 'train/mean_average_precision': 0.6220936529065954, 'validation/accuracy': 0.9868056774139404, 'validation/loss': 0.04686205834150314, 'validation/mean_average_precision': 0.26648923587925655, 'validation/num_examples': 43793, 'test/accuracy': 0.9858680367469788, 'test/loss': 0.05013066902756691, 'test/mean_average_precision': 0.2569849594604288, 'test/num_examples': 43793, 'score': 16825.499007225037, 'total_duration': 25994.375078201294, 'accumulated_submission_time': 16825.499007225037, 'accumulated_eval_time': 9164.892379283905, 'accumulated_logging_time': 2.573488712310791, 'global_step': 52235, 'preemption_count': 0}), (52980, {'train/accuracy': 0.9937600493431091, 'train/loss': 0.01983131468296051, 'train/mean_average_precision': 0.6459993043755188, 'validation/accuracy': 0.9867037534713745, 'validation/loss': 0.04749181494116783, 'validation/mean_average_precision': 0.2685034861718007, 'validation/num_examples': 43793, 'test/accuracy': 0.9858928918838501, 'test/loss': 0.05066568776965141, 'test/mean_average_precision': 0.25705002950830314, 'test/num_examples': 43793, 'score': 17065.48960494995, 'total_duration': 26357.715396165848, 'accumulated_submission_time': 17065.48960494995, 'accumulated_eval_time': 9288.184242248535, 'accumulated_logging_time': 2.6123759746551514, 'global_step': 52980, 'preemption_count': 0}), (53724, {'train/accuracy': 0.9939901828765869, 'train/loss': 0.01933383196592331, 'train/mean_average_precision': 0.6436779537032165, 'validation/accuracy': 0.9866481423377991, 'validation/loss': 0.04750419408082962, 'validation/mean_average_precision': 0.26815615039251833, 'validation/num_examples': 43793, 'test/accuracy': 0.9858894944190979, 'test/loss': 0.05031247064471245, 'test/mean_average_precision': 0.261990889494677, 'test/num_examples': 43793, 'score': 17305.6275203228, 'total_duration': 26717.59257531166, 'accumulated_submission_time': 17305.6275203228, 'accumulated_eval_time': 9407.865612268448, 'accumulated_logging_time': 2.6504719257354736, 'global_step': 53724, 'preemption_count': 0}), (54469, {'train/accuracy': 0.9936777949333191, 'train/loss': 0.020111173391342163, 'train/mean_average_precision': 0.6458658038745391, 'validation/accuracy': 0.9866968989372253, 'validation/loss': 0.04727909341454506, 'validation/mean_average_precision': 0.26744203434436603, 'validation/num_examples': 43793, 'test/accuracy': 0.9858878254890442, 'test/loss': 0.050669051706790924, 'test/mean_average_precision': 0.25698298673776826, 'test/num_examples': 43793, 'score': 17545.65809392929, 'total_duration': 27082.599598646164, 'accumulated_submission_time': 17545.65809392929, 'accumulated_eval_time': 9532.784777402878, 'accumulated_logging_time': 2.6869962215423584, 'global_step': 54469, 'preemption_count': 0}), (55211, {'train/accuracy': 0.9936330318450928, 'train/loss': 0.020323490723967552, 'train/mean_average_precision': 0.6208121445688634, 'validation/accuracy': 0.9867082238197327, 'validation/loss': 0.047605160623788834, 'validation/mean_average_precision': 0.26579084746660736, 'validation/num_examples': 43793, 'test/accuracy': 0.985811173915863, 'test/loss': 0.05104955658316612, 'test/mean_average_precision': 0.25368147011301617, 'test/num_examples': 43793, 'score': 17785.798129081726, 'total_duration': 27443.96195435524, 'accumulated_submission_time': 17785.798129081726, 'accumulated_eval_time': 9653.950505495071, 'accumulated_logging_time': 2.7239596843719482, 'global_step': 55211, 'preemption_count': 0}), (55959, {'train/accuracy': 0.9933901429176331, 'train/loss': 0.02094298228621483, 'train/mean_average_precision': 0.6129876580194846, 'validation/accuracy': 0.9866664409637451, 'validation/loss': 0.04780498892068863, 'validation/mean_average_precision': 0.26698651889167746, 'validation/num_examples': 43793, 'test/accuracy': 0.9858027696609497, 'test/loss': 0.051106881350278854, 'test/mean_average_precision': 0.25975426581008415, 'test/num_examples': 43793, 'score': 18026.053458452225, 'total_duration': 27810.320541620255, 'accumulated_submission_time': 18026.053458452225, 'accumulated_eval_time': 9779.996412038803, 'accumulated_logging_time': 2.7612576484680176, 'global_step': 55959, 'preemption_count': 0}), (56710, {'train/accuracy': 0.9932118058204651, 'train/loss': 0.021330367773771286, 'train/mean_average_precision': 0.594974708618752, 'validation/accuracy': 0.9866802096366882, 'validation/loss': 0.04851559177041054, 'validation/mean_average_precision': 0.26519575804293455, 'validation/num_examples': 43793, 'test/accuracy': 0.9857563972473145, 'test/loss': 0.05178118124604225, 'test/mean_average_precision': 0.2562075157043629, 'test/num_examples': 43793, 'score': 18266.163946151733, 'total_duration': 28175.351845502853, 'accumulated_submission_time': 18266.163946151733, 'accumulated_eval_time': 9904.859407424927, 'accumulated_logging_time': 2.799586296081543, 'global_step': 56710, 'preemption_count': 0})], 'global_step': 57359}
I0209 08:59:38.413779 140039251117888 submission_runner.py:586] Timing: 18477.090780735016
I0209 08:59:38.413843 140039251117888 submission_runner.py:588] Total number of evals: 77
I0209 08:59:38.413894 140039251117888 submission_runner.py:589] ====================
I0209 08:59:38.413949 140039251117888 submission_runner.py:542] Using RNG seed 1895687988
I0209 08:59:38.486286 140039251117888 submission_runner.py:551] --- Tuning run 2/5 ---
I0209 08:59:38.486447 140039251117888 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_3/ogbg_jax/trial_2.
I0209 08:59:38.490446 140039251117888 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_3/ogbg_jax/trial_2/hparams.json.
I0209 08:59:38.629315 140039251117888 submission_runner.py:206] Initializing dataset.
I0209 08:59:38.720518 140039251117888 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0209 08:59:38.724751 140039251117888 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0209 08:59:39.129090 140039251117888 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0209 08:59:39.169989 140039251117888 submission_runner.py:213] Initializing model.
I0209 08:59:41.918674 140039251117888 submission_runner.py:255] Initializing optimizer.
I0209 08:59:42.541800 140039251117888 submission_runner.py:262] Initializing metrics bundle.
I0209 08:59:42.542000 140039251117888 submission_runner.py:280] Initializing checkpoint and logger.
I0209 08:59:42.542665 140039251117888 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_3/ogbg_jax/trial_2 with prefix checkpoint_
I0209 08:59:42.542797 140039251117888 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_3/ogbg_jax/trial_2/meta_data_0.json.
I0209 08:59:42.543026 140039251117888 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0209 08:59:42.543090 140039251117888 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0209 08:59:43.783482 140039251117888 logger_utils.py:220] Unable to record git information. Continuing without it.
I0209 08:59:45.016445 140039251117888 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_3/ogbg_jax/trial_2/flags_0.json.
I0209 08:59:45.026552 140039251117888 submission_runner.py:314] Starting training loop.
I0209 08:59:57.175166 139855174698752 logging_writer.py:48] [0] global_step=0, grad_norm=2.7374305725097656, loss=0.7146591544151306
I0209 08:59:57.185837 140039251117888 spec.py:321] Evaluating on the training split.
I0209 09:01:51.754276 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 09:01:55.171608 140039251117888 spec.py:349] Evaluating on the test split.
I0209 09:01:58.559929 140039251117888 submission_runner.py:408] Time since start: 133.53s, 	Step: 1, 	{'train/accuracy': 0.5251287221908569, 'train/loss': 0.7151197195053101, 'train/mean_average_precision': 0.024144278710189954, 'validation/accuracy': 0.5213832855224609, 'validation/loss': 0.7166012525558472, 'validation/mean_average_precision': 0.026153954210312285, 'validation/num_examples': 43793, 'test/accuracy': 0.5224818587303162, 'test/loss': 0.7161954641342163, 'test/mean_average_precision': 0.027840261930176607, 'test/num_examples': 43793, 'score': 12.159255981445312, 'total_duration': 133.5333137512207, 'accumulated_submission_time': 12.159255981445312, 'accumulated_eval_time': 121.37401604652405, 'accumulated_logging_time': 0}
I0209 09:01:58.570996 139855195191040 logging_writer.py:48] [1] accumulated_eval_time=121.374016, accumulated_logging_time=0, accumulated_submission_time=12.159256, global_step=1, preemption_count=0, score=12.159256, test/accuracy=0.522482, test/loss=0.716195, test/mean_average_precision=0.027840, test/num_examples=43793, total_duration=133.533314, train/accuracy=0.525129, train/loss=0.715120, train/mean_average_precision=0.024144, validation/accuracy=0.521383, validation/loss=0.716601, validation/mean_average_precision=0.026154, validation/num_examples=43793
I0209 09:02:31.956652 139871934551808 logging_writer.py:48] [100] global_step=100, grad_norm=0.4309006333351135, loss=0.3777913749217987
I0209 09:03:04.187316 139855195191040 logging_writer.py:48] [200] global_step=200, grad_norm=0.3400985598564148, loss=0.2898842692375183
I0209 09:03:36.349160 139871934551808 logging_writer.py:48] [300] global_step=300, grad_norm=0.25765302777290344, loss=0.20068635046482086
I0209 09:04:08.829137 139855195191040 logging_writer.py:48] [400] global_step=400, grad_norm=0.15369603037834167, loss=0.13612215220928192
I0209 09:04:41.039762 139871934551808 logging_writer.py:48] [500] global_step=500, grad_norm=0.13582494854927063, loss=0.09414179623126984
I0209 09:05:13.326723 139855195191040 logging_writer.py:48] [600] global_step=600, grad_norm=0.09296991676092148, loss=0.07757966965436935
I0209 09:05:45.545109 139871934551808 logging_writer.py:48] [700] global_step=700, grad_norm=0.10689752548933029, loss=0.0610801987349987
I0209 09:05:58.854668 140039251117888 spec.py:321] Evaluating on the training split.
I0209 09:07:58.431824 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 09:08:01.523847 140039251117888 spec.py:349] Evaluating on the test split.
I0209 09:08:04.662857 140039251117888 submission_runner.py:408] Time since start: 499.64s, 	Step: 742, 	{'train/accuracy': 0.9867380261421204, 'train/loss': 0.06415025889873505, 'train/mean_average_precision': 0.04036166806203355, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.07288718968629837, 'validation/mean_average_precision': 0.04209721960526595, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.07578893005847931, 'test/mean_average_precision': 0.04366867466455112, 'test/num_examples': 43793, 'score': 252.4109787940979, 'total_duration': 499.6362552642822, 'accumulated_submission_time': 252.4109787940979, 'accumulated_eval_time': 247.18216228485107, 'accumulated_logging_time': 0.02355480194091797}
I0209 09:08:04.678725 139855203583744 logging_writer.py:48] [742] accumulated_eval_time=247.182162, accumulated_logging_time=0.023555, accumulated_submission_time=252.410979, global_step=742, preemption_count=0, score=252.410979, test/accuracy=0.983142, test/loss=0.075789, test/mean_average_precision=0.043669, test/num_examples=43793, total_duration=499.636255, train/accuracy=0.986738, train/loss=0.064150, train/mean_average_precision=0.040362, validation/accuracy=0.984118, validation/loss=0.072887, validation/mean_average_precision=0.042097, validation/num_examples=43793
I0209 09:08:23.786171 139864233322240 logging_writer.py:48] [800] global_step=800, grad_norm=0.031714580953121185, loss=0.06198599189519882
I0209 09:08:55.999357 139855203583744 logging_writer.py:48] [900] global_step=900, grad_norm=0.058275505900382996, loss=0.058535147458314896
I0209 09:09:29.994856 139864233322240 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.03278174251317978, loss=0.056980378925800323
I0209 09:10:03.263819 139855203583744 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.02790067158639431, loss=0.05412717163562775
I0209 09:10:36.599899 139864233322240 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.05251149460673332, loss=0.05226143077015877
I0209 09:11:09.911546 139855203583744 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.08766163140535355, loss=0.05104002729058266
I0209 09:11:42.342926 139864233322240 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.030923763290047646, loss=0.04751332849264145
I0209 09:12:04.783518 140039251117888 spec.py:321] Evaluating on the training split.
I0209 09:14:02.925505 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 09:14:06.027860 140039251117888 spec.py:349] Evaluating on the test split.
I0209 09:14:09.008380 140039251117888 submission_runner.py:408] Time since start: 863.98s, 	Step: 1468, 	{'train/accuracy': 0.9867783784866333, 'train/loss': 0.05364827439188957, 'train/mean_average_precision': 0.04730545077029293, 'validation/accuracy': 0.9841195344924927, 'validation/loss': 0.06393802911043167, 'validation/mean_average_precision': 0.04844746740401821, 'validation/num_examples': 43793, 'test/accuracy': 0.9831437468528748, 'test/loss': 0.06729529052972794, 'test/mean_average_precision': 0.05084376489252518, 'test/num_examples': 43793, 'score': 492.48107290267944, 'total_duration': 863.9817779064178, 'accumulated_submission_time': 492.48107290267944, 'accumulated_eval_time': 371.40698766708374, 'accumulated_logging_time': 0.0522158145904541}
I0209 09:14:09.024432 139855195191040 logging_writer.py:48] [1468] accumulated_eval_time=371.406988, accumulated_logging_time=0.052216, accumulated_submission_time=492.481073, global_step=1468, preemption_count=0, score=492.481073, test/accuracy=0.983144, test/loss=0.067295, test/mean_average_precision=0.050844, test/num_examples=43793, total_duration=863.981778, train/accuracy=0.986778, train/loss=0.053648, train/mean_average_precision=0.047305, validation/accuracy=0.984120, validation/loss=0.063938, validation/mean_average_precision=0.048447, validation/num_examples=43793
I0209 09:14:19.862217 139871934551808 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.05413845181465149, loss=0.04964867979288101
I0209 09:14:52.024187 139855195191040 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.03575883060693741, loss=0.05023670196533203
I0209 09:15:24.472449 139871934551808 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.11966937780380249, loss=0.05531620979309082
I0209 09:15:56.707917 139855195191040 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.045776158571243286, loss=0.051737572997808456
I0209 09:16:28.953944 139871934551808 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.09076722711324692, loss=0.051791876554489136
I0209 09:17:00.946682 139855195191040 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.09251802414655685, loss=0.04836317524313927
I0209 09:17:32.986472 139871934551808 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.12344508618116379, loss=0.04675102233886719
I0209 09:18:05.345852 139855195191040 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.06070580706000328, loss=0.049078479409217834
I0209 09:18:09.193404 140039251117888 spec.py:321] Evaluating on the training split.
I0209 09:20:02.452518 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 09:20:05.519030 140039251117888 spec.py:349] Evaluating on the test split.
I0209 09:20:08.502882 140039251117888 submission_runner.py:408] Time since start: 1223.48s, 	Step: 2213, 	{'train/accuracy': 0.9873389005661011, 'train/loss': 0.04617956653237343, 'train/mean_average_precision': 0.11931676578964977, 'validation/accuracy': 0.9846541881561279, 'validation/loss': 0.05546187981963158, 'validation/mean_average_precision': 0.11200725265614112, 'validation/num_examples': 43793, 'test/accuracy': 0.9836630821228027, 'test/loss': 0.05856335535645485, 'test/mean_average_precision': 0.11037638421916286, 'test/num_examples': 43793, 'score': 732.6191091537476, 'total_duration': 1223.476276397705, 'accumulated_submission_time': 732.6191091537476, 'accumulated_eval_time': 490.71641182899475, 'accumulated_logging_time': 0.07963228225708008}
I0209 09:20:08.518612 139855203583744 logging_writer.py:48] [2213] accumulated_eval_time=490.716412, accumulated_logging_time=0.079632, accumulated_submission_time=732.619109, global_step=2213, preemption_count=0, score=732.619109, test/accuracy=0.983663, test/loss=0.058563, test/mean_average_precision=0.110376, test/num_examples=43793, total_duration=1223.476276, train/accuracy=0.987339, train/loss=0.046180, train/mean_average_precision=0.119317, validation/accuracy=0.984654, validation/loss=0.055462, validation/mean_average_precision=0.112007, validation/num_examples=43793
I0209 09:20:37.341838 139871926159104 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.04074516519904137, loss=0.045088231563568115
I0209 09:21:09.597765 139855203583744 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.028631485998630524, loss=0.04715675860643387
I0209 09:21:41.647947 139871926159104 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.057712823152542114, loss=0.044467318803071976
I0209 09:22:13.809951 139855203583744 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.04963795840740204, loss=0.049661170691251755
I0209 09:22:46.124991 139871926159104 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.05641993135213852, loss=0.0439523383975029
I0209 09:23:19.348982 139855203583744 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.023665448650717735, loss=0.04335510730743408
I0209 09:23:52.250717 139871926159104 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.04534982889890671, loss=0.04343539476394653
I0209 09:24:08.727399 140039251117888 spec.py:321] Evaluating on the training split.
I0209 09:26:00.522576 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 09:26:03.753293 140039251117888 spec.py:349] Evaluating on the test split.
I0209 09:26:06.795609 140039251117888 submission_runner.py:408] Time since start: 1581.77s, 	Step: 2951, 	{'train/accuracy': 0.9876325130462646, 'train/loss': 0.04440842196345329, 'train/mean_average_precision': 0.14693599380765723, 'validation/accuracy': 0.984946072101593, 'validation/loss': 0.05403568223118782, 'validation/mean_average_precision': 0.1438901259098251, 'validation/num_examples': 43793, 'test/accuracy': 0.9839431643486023, 'test/loss': 0.05713082477450371, 'test/mean_average_precision': 0.14033399196415447, 'test/num_examples': 43793, 'score': 972.7951905727386, 'total_duration': 1581.7689995765686, 'accumulated_submission_time': 972.7951905727386, 'accumulated_eval_time': 608.784569978714, 'accumulated_logging_time': 0.1080617904663086}
I0209 09:26:06.813148 139864233322240 logging_writer.py:48] [2951] accumulated_eval_time=608.784570, accumulated_logging_time=0.108062, accumulated_submission_time=972.795191, global_step=2951, preemption_count=0, score=972.795191, test/accuracy=0.983943, test/loss=0.057131, test/mean_average_precision=0.140334, test/num_examples=43793, total_duration=1581.769000, train/accuracy=0.987633, train/loss=0.044408, train/mean_average_precision=0.146936, validation/accuracy=0.984946, validation/loss=0.054036, validation/mean_average_precision=0.143890, validation/num_examples=43793
I0209 09:26:23.049182 139871934551808 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.044870343059301376, loss=0.04844154790043831
I0209 09:26:55.616775 139864233322240 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.03439858555793762, loss=0.04819178953766823
I0209 09:27:27.807183 139871934551808 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.030830463394522667, loss=0.04358172044157982
I0209 09:28:00.124146 139864233322240 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.02590187080204487, loss=0.044616322964429855
I0209 09:28:32.480655 139871934551808 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.01875169761478901, loss=0.045788273215293884
I0209 09:29:04.669184 139864233322240 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.026292219758033752, loss=0.048173170536756516
I0209 09:29:36.676258 139871934551808 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.06846924126148224, loss=0.04487653076648712
I0209 09:30:07.059375 140039251117888 spec.py:321] Evaluating on the training split.
I0209 09:32:02.786947 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 09:32:05.856993 140039251117888 spec.py:349] Evaluating on the test split.
I0209 09:32:08.910572 140039251117888 submission_runner.py:408] Time since start: 1943.88s, 	Step: 3695, 	{'train/accuracy': 0.9878683090209961, 'train/loss': 0.04309086129069328, 'train/mean_average_precision': 0.16932275726861554, 'validation/accuracy': 0.9851047992706299, 'validation/loss': 0.05209706351161003, 'validation/mean_average_precision': 0.1507358351030023, 'validation/num_examples': 43793, 'test/accuracy': 0.9841756820678711, 'test/loss': 0.05486214533448219, 'test/mean_average_precision': 0.15040544105166512, 'test/num_examples': 43793, 'score': 1213.0103611946106, 'total_duration': 1943.883972644806, 'accumulated_submission_time': 1213.0103611946106, 'accumulated_eval_time': 730.6357326507568, 'accumulated_logging_time': 0.13688111305236816}
I0209 09:32:08.926247 139855195191040 logging_writer.py:48] [3695] accumulated_eval_time=730.635733, accumulated_logging_time=0.136881, accumulated_submission_time=1213.010361, global_step=3695, preemption_count=0, score=1213.010361, test/accuracy=0.984176, test/loss=0.054862, test/mean_average_precision=0.150405, test/num_examples=43793, total_duration=1943.883973, train/accuracy=0.987868, train/loss=0.043091, train/mean_average_precision=0.169323, validation/accuracy=0.985105, validation/loss=0.052097, validation/mean_average_precision=0.150736, validation/num_examples=43793
I0209 09:32:10.843555 139871926159104 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.04966774582862854, loss=0.04694140702486038
I0209 09:32:42.908555 139855195191040 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.02736637368798256, loss=0.045184340327978134
I0209 09:33:15.181857 139871926159104 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.032669197767972946, loss=0.046587686985731125
I0209 09:33:47.602388 139855195191040 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.027653757482767105, loss=0.0481681264936924
I0209 09:34:19.829363 139871926159104 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.043896179646253586, loss=0.042548324912786484
I0209 09:34:52.379778 139855195191040 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.017291050404310226, loss=0.048773329704999924
I0209 09:35:25.135186 139871926159104 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.018790356814861298, loss=0.04611555114388466
I0209 09:35:57.867440 139855195191040 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.034383468329906464, loss=0.03891271725296974
I0209 09:36:08.982008 140039251117888 spec.py:321] Evaluating on the training split.
I0209 09:37:58.335967 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 09:38:03.052268 140039251117888 spec.py:349] Evaluating on the test split.
I0209 09:38:06.086913 140039251117888 submission_runner.py:408] Time since start: 2301.06s, 	Step: 4435, 	{'train/accuracy': 0.9882818460464478, 'train/loss': 0.040857136249542236, 'train/mean_average_precision': 0.2046375427604755, 'validation/accuracy': 0.9853986501693726, 'validation/loss': 0.050100069493055344, 'validation/mean_average_precision': 0.17036476781593696, 'validation/num_examples': 43793, 'test/accuracy': 0.9844208359718323, 'test/loss': 0.052849117666482925, 'test/mean_average_precision': 0.1704869601797195, 'test/num_examples': 43793, 'score': 1453.0346467494965, 'total_duration': 2301.0603160858154, 'accumulated_submission_time': 1453.0346467494965, 'accumulated_eval_time': 847.740592956543, 'accumulated_logging_time': 0.1639246940612793}
I0209 09:38:06.103094 139855203583744 logging_writer.py:48] [4435] accumulated_eval_time=847.740593, accumulated_logging_time=0.163925, accumulated_submission_time=1453.034647, global_step=4435, preemption_count=0, score=1453.034647, test/accuracy=0.984421, test/loss=0.052849, test/mean_average_precision=0.170487, test/num_examples=43793, total_duration=2301.060316, train/accuracy=0.988282, train/loss=0.040857, train/mean_average_precision=0.204638, validation/accuracy=0.985399, validation/loss=0.050100, validation/mean_average_precision=0.170365, validation/num_examples=43793
I0209 09:38:27.331500 139864233322240 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.025253981351852417, loss=0.0454421304166317
I0209 09:38:59.732767 139855203583744 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.022534998133778572, loss=0.047839537262916565
I0209 09:39:32.203726 139864233322240 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.03321390599012375, loss=0.04680278152227402
I0209 09:40:05.060672 139855203583744 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.04031537100672722, loss=0.04554782435297966
I0209 09:40:37.385837 139864233322240 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.019588477909564972, loss=0.0421842485666275
I0209 09:41:09.599274 139855203583744 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.021304147318005562, loss=0.04211287200450897
I0209 09:41:41.980077 139864233322240 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.04154812917113304, loss=0.044748757034540176
I0209 09:42:06.346395 140039251117888 spec.py:321] Evaluating on the training split.
I0209 09:44:01.520426 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 09:44:04.795277 140039251117888 spec.py:349] Evaluating on the test split.
I0209 09:44:07.886544 140039251117888 submission_runner.py:408] Time since start: 2662.86s, 	Step: 5175, 	{'train/accuracy': 0.9885432720184326, 'train/loss': 0.039845600724220276, 'train/mean_average_precision': 0.21645493103937904, 'validation/accuracy': 0.9855533242225647, 'validation/loss': 0.04954756423830986, 'validation/mean_average_precision': 0.19147633222763, 'validation/num_examples': 43793, 'test/accuracy': 0.9846398234367371, 'test/loss': 0.05211753025650978, 'test/mean_average_precision': 0.18758637928052935, 'test/num_examples': 43793, 'score': 1693.2447366714478, 'total_duration': 2662.8599441051483, 'accumulated_submission_time': 1693.2447366714478, 'accumulated_eval_time': 969.2806987762451, 'accumulated_logging_time': 0.19262957572937012}
I0209 09:44:07.903037 139855195191040 logging_writer.py:48] [5175] accumulated_eval_time=969.280699, accumulated_logging_time=0.192630, accumulated_submission_time=1693.244737, global_step=5175, preemption_count=0, score=1693.244737, test/accuracy=0.984640, test/loss=0.052118, test/mean_average_precision=0.187586, test/num_examples=43793, total_duration=2662.859944, train/accuracy=0.988543, train/loss=0.039846, train/mean_average_precision=0.216455, validation/accuracy=0.985553, validation/loss=0.049548, validation/mean_average_precision=0.191476, validation/num_examples=43793
I0209 09:44:16.236546 139871934551808 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.02529113180935383, loss=0.04456750303506851
I0209 09:44:48.586962 139855195191040 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.03195139020681381, loss=0.04320133849978447
I0209 09:45:21.066394 139871934551808 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.019636258482933044, loss=0.04189494252204895
I0209 09:45:53.801413 139855195191040 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.022208096459507942, loss=0.0451679453253746
I0209 09:46:26.588921 139871934551808 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.018570315092802048, loss=0.03984428197145462
I0209 09:46:58.858377 139855195191040 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.014242339879274368, loss=0.04303533211350441
I0209 09:47:31.708951 139871934551808 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.018017718568444252, loss=0.040763627737760544
I0209 09:48:04.438813 139855195191040 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.03189801424741745, loss=0.04306662827730179
I0209 09:48:08.022093 140039251117888 spec.py:321] Evaluating on the training split.
I0209 09:50:03.839428 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 09:50:06.911554 140039251117888 spec.py:349] Evaluating on the test split.
I0209 09:50:09.956437 140039251117888 submission_runner.py:408] Time since start: 3024.93s, 	Step: 5912, 	{'train/accuracy': 0.9885270595550537, 'train/loss': 0.03891624137759209, 'train/mean_average_precision': 0.23800755044098904, 'validation/accuracy': 0.9856272339820862, 'validation/loss': 0.04862495884299278, 'validation/mean_average_precision': 0.19743590536491962, 'validation/num_examples': 43793, 'test/accuracy': 0.9847046732902527, 'test/loss': 0.05137062072753906, 'test/mean_average_precision': 0.19491386996111382, 'test/num_examples': 43793, 'score': 1933.3324942588806, 'total_duration': 3024.9298338890076, 'accumulated_submission_time': 1933.3324942588806, 'accumulated_eval_time': 1091.2149925231934, 'accumulated_logging_time': 0.2206106185913086}
I0209 09:50:09.973342 139878408181504 logging_writer.py:48] [5912] accumulated_eval_time=1091.214993, accumulated_logging_time=0.220611, accumulated_submission_time=1933.332494, global_step=5912, preemption_count=0, score=1933.332494, test/accuracy=0.984705, test/loss=0.051371, test/mean_average_precision=0.194914, test/num_examples=43793, total_duration=3024.929834, train/accuracy=0.988527, train/loss=0.038916, train/mean_average_precision=0.238008, validation/accuracy=0.985627, validation/loss=0.048625, validation/mean_average_precision=0.197436, validation/num_examples=43793
I0209 09:50:38.577297 139878416574208 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.01679357886314392, loss=0.04160885885357857
I0209 09:51:10.952904 139878408181504 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.024963369593024254, loss=0.044425081461668015
I0209 09:51:43.586487 139878416574208 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.03424423187971115, loss=0.042624302208423615
I0209 09:52:16.607934 139878408181504 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.015863660722970963, loss=0.042934197932481766
I0209 09:52:48.746784 139878416574208 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.026301512494683266, loss=0.04035589098930359
I0209 09:53:21.223578 139878408181504 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.014896394684910774, loss=0.0398196280002594
I0209 09:53:53.927492 139878416574208 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.014251415617763996, loss=0.04252989590167999
I0209 09:54:10.091266 140039251117888 spec.py:321] Evaluating on the training split.
I0209 09:56:05.784442 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 09:56:09.117151 140039251117888 spec.py:349] Evaluating on the test split.
I0209 09:56:12.407840 140039251117888 submission_runner.py:408] Time since start: 3387.38s, 	Step: 6651, 	{'train/accuracy': 0.9885249137878418, 'train/loss': 0.03850976377725601, 'train/mean_average_precision': 0.25270491767745973, 'validation/accuracy': 0.9857376217842102, 'validation/loss': 0.048177871853113174, 'validation/mean_average_precision': 0.20781442148372237, 'validation/num_examples': 43793, 'test/accuracy': 0.9848660230636597, 'test/loss': 0.05098883435130119, 'test/mean_average_precision': 0.2112681620169295, 'test/num_examples': 43793, 'score': 2173.4175686836243, 'total_duration': 3387.38121843338, 'accumulated_submission_time': 2173.4175686836243, 'accumulated_eval_time': 1213.531497001648, 'accumulated_logging_time': 0.2502303123474121}
I0209 09:56:12.427679 139864233322240 logging_writer.py:48] [6651] accumulated_eval_time=1213.531497, accumulated_logging_time=0.250230, accumulated_submission_time=2173.417569, global_step=6651, preemption_count=0, score=2173.417569, test/accuracy=0.984866, test/loss=0.050989, test/mean_average_precision=0.211268, test/num_examples=43793, total_duration=3387.381218, train/accuracy=0.988525, train/loss=0.038510, train/mean_average_precision=0.252705, validation/accuracy=0.985738, validation/loss=0.048178, validation/mean_average_precision=0.207814, validation/num_examples=43793
I0209 09:56:28.725289 139871926159104 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.01834583841264248, loss=0.04352600499987602
I0209 09:57:01.164633 139864233322240 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.030086493119597435, loss=0.04297741502523422
I0209 09:57:34.547182 139871926159104 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.024036046117544174, loss=0.04387213662266731
I0209 09:58:08.004211 139864233322240 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.016003098338842392, loss=0.03956465795636177
I0209 09:58:41.194478 139871926159104 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.017614820972085, loss=0.0435202494263649
I0209 09:59:14.301650 139864233322240 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.013864070177078247, loss=0.03964191675186157
I0209 09:59:46.644650 139871926159104 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.016293831169605255, loss=0.04184166342020035
I0209 10:00:12.612045 140039251117888 spec.py:321] Evaluating on the training split.
I0209 10:02:03.730973 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 10:02:07.100549 140039251117888 spec.py:349] Evaluating on the test split.
I0209 10:02:10.483753 140039251117888 submission_runner.py:408] Time since start: 3745.46s, 	Step: 7381, 	{'train/accuracy': 0.9886531829833984, 'train/loss': 0.037700358778238297, 'train/mean_average_precision': 0.29230596323805513, 'validation/accuracy': 0.985855758190155, 'validation/loss': 0.04768254607915878, 'validation/mean_average_precision': 0.22443028666929185, 'validation/num_examples': 43793, 'test/accuracy': 0.9849856495857239, 'test/loss': 0.05026884749531746, 'test/mean_average_precision': 0.2317656739722545, 'test/num_examples': 43793, 'score': 2413.567792892456, 'total_duration': 3745.457129716873, 'accumulated_submission_time': 2413.567792892456, 'accumulated_eval_time': 1331.4031381607056, 'accumulated_logging_time': 0.28160834312438965}
I0209 10:02:10.503969 139871934551808 logging_writer.py:48] [7381] accumulated_eval_time=1331.403138, accumulated_logging_time=0.281608, accumulated_submission_time=2413.567793, global_step=7381, preemption_count=0, score=2413.567793, test/accuracy=0.984986, test/loss=0.050269, test/mean_average_precision=0.231766, test/num_examples=43793, total_duration=3745.457130, train/accuracy=0.988653, train/loss=0.037700, train/mean_average_precision=0.292306, validation/accuracy=0.985856, validation/loss=0.047683, validation/mean_average_precision=0.224430, validation/num_examples=43793
I0209 10:02:17.199732 139878408181504 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.01358717493712902, loss=0.03673655167222023
I0209 10:02:50.364850 139871934551808 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.012333085760474205, loss=0.041828691959381104
I0209 10:03:23.672894 139878408181504 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.020408108830451965, loss=0.04063943400979042
I0209 10:03:56.395701 139871934551808 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.01689823903143406, loss=0.04570847004652023
I0209 10:04:29.340223 139878408181504 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.02350364811718464, loss=0.04242357611656189
I0209 10:05:01.978448 139871934551808 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.01248607411980629, loss=0.039697032421827316
I0209 10:05:34.732635 139878408181504 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.019338982179760933, loss=0.042595274746418
I0209 10:06:07.322867 139871934551808 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.020884597674012184, loss=0.04504718631505966
I0209 10:06:10.635923 140039251117888 spec.py:321] Evaluating on the training split.
I0209 10:08:05.111740 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 10:08:08.215388 140039251117888 spec.py:349] Evaluating on the test split.
I0209 10:08:11.274230 140039251117888 submission_runner.py:408] Time since start: 4106.25s, 	Step: 8111, 	{'train/accuracy': 0.989067792892456, 'train/loss': 0.036662716418504715, 'train/mean_average_precision': 0.3014880708827807, 'validation/accuracy': 0.9859344959259033, 'validation/loss': 0.047925643622875214, 'validation/mean_average_precision': 0.2225765737842831, 'validation/num_examples': 43793, 'test/accuracy': 0.9850509166717529, 'test/loss': 0.05051816627383232, 'test/mean_average_precision': 0.22876474455407841, 'test/num_examples': 43793, 'score': 2653.6647942066193, 'total_duration': 4106.247630119324, 'accumulated_submission_time': 2653.6647942066193, 'accumulated_eval_time': 1452.041398525238, 'accumulated_logging_time': 0.31385087966918945}
I0209 10:08:11.291399 139864233322240 logging_writer.py:48] [8111] accumulated_eval_time=1452.041399, accumulated_logging_time=0.313851, accumulated_submission_time=2653.664794, global_step=8111, preemption_count=0, score=2653.664794, test/accuracy=0.985051, test/loss=0.050518, test/mean_average_precision=0.228765, test/num_examples=43793, total_duration=4106.247630, train/accuracy=0.989068, train/loss=0.036663, train/mean_average_precision=0.301488, validation/accuracy=0.985934, validation/loss=0.047926, validation/mean_average_precision=0.222577, validation/num_examples=43793
I0209 10:08:40.544204 139878416574208 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.018101509660482407, loss=0.04087481647729874
I0209 10:09:13.252930 139864233322240 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.020410140976309776, loss=0.04310378059744835
I0209 10:09:45.778244 139878416574208 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.012440818361938, loss=0.04104088246822357
I0209 10:10:18.420526 139864233322240 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.02132243663072586, loss=0.03917092829942703
I0209 10:10:51.035666 139878416574208 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.02275136299431324, loss=0.041607338935136795
I0209 10:11:23.719619 139864233322240 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.014204340055584908, loss=0.04098827391862869
I0209 10:11:56.316659 139878416574208 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.01956796832382679, loss=0.039622899144887924
I0209 10:12:11.466343 140039251117888 spec.py:321] Evaluating on the training split.
I0209 10:14:02.970318 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 10:14:06.025969 140039251117888 spec.py:349] Evaluating on the test split.
I0209 10:14:09.062891 140039251117888 submission_runner.py:408] Time since start: 4464.04s, 	Step: 8847, 	{'train/accuracy': 0.989511251449585, 'train/loss': 0.03531685471534729, 'train/mean_average_precision': 0.3274415535340602, 'validation/accuracy': 0.9862276315689087, 'validation/loss': 0.046508222818374634, 'validation/mean_average_precision': 0.2328149372357666, 'validation/num_examples': 43793, 'test/accuracy': 0.9853474497795105, 'test/loss': 0.049215056002140045, 'test/mean_average_precision': 0.23299791283990784, 'test/num_examples': 43793, 'score': 2893.8072040081024, 'total_duration': 4464.036288499832, 'accumulated_submission_time': 2893.8072040081024, 'accumulated_eval_time': 1569.6379013061523, 'accumulated_logging_time': 0.34394145011901855}
I0209 10:14:09.080724 139871926159104 logging_writer.py:48] [8847] accumulated_eval_time=1569.637901, accumulated_logging_time=0.343941, accumulated_submission_time=2893.807204, global_step=8847, preemption_count=0, score=2893.807204, test/accuracy=0.985347, test/loss=0.049215, test/mean_average_precision=0.232998, test/num_examples=43793, total_duration=4464.036288, train/accuracy=0.989511, train/loss=0.035317, train/mean_average_precision=0.327442, validation/accuracy=0.986228, validation/loss=0.046508, validation/mean_average_precision=0.232815, validation/num_examples=43793
I0209 10:14:26.513709 139871934551808 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.016629604622721672, loss=0.045134082436561584
I0209 10:14:58.448788 139871926159104 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.022517967969179153, loss=0.04046337679028511
I0209 10:15:30.578638 139871934551808 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.016661470755934715, loss=0.04107004776597023
I0209 10:16:02.495784 139871926159104 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.017298107966780663, loss=0.036473870277404785
I0209 10:16:34.776427 139871934551808 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.013882598839700222, loss=0.0401531457901001
I0209 10:17:06.826692 139871926159104 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.01267891563475132, loss=0.03830711171030998
I0209 10:17:38.938273 139871934551808 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.012479323893785477, loss=0.035585250705480576
I0209 10:18:09.161972 140039251117888 spec.py:321] Evaluating on the training split.
I0209 10:20:04.208694 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 10:20:07.315133 140039251117888 spec.py:349] Evaluating on the test split.
I0209 10:20:10.342901 140039251117888 submission_runner.py:408] Time since start: 4825.32s, 	Step: 9594, 	{'train/accuracy': 0.9896472096443176, 'train/loss': 0.03462878614664078, 'train/mean_average_precision': 0.33390209800927395, 'validation/accuracy': 0.9862998723983765, 'validation/loss': 0.0462951585650444, 'validation/mean_average_precision': 0.23877159163973058, 'validation/num_examples': 43793, 'test/accuracy': 0.9854249358177185, 'test/loss': 0.048981741070747375, 'test/mean_average_precision': 0.2374337427035071, 'test/num_examples': 43793, 'score': 3133.8573887348175, 'total_duration': 4825.316298484802, 'accumulated_submission_time': 3133.8573887348175, 'accumulated_eval_time': 1690.8187873363495, 'accumulated_logging_time': 0.373058557510376}
I0209 10:20:10.361075 139864233322240 logging_writer.py:48] [9594] accumulated_eval_time=1690.818787, accumulated_logging_time=0.373059, accumulated_submission_time=3133.857389, global_step=9594, preemption_count=0, score=3133.857389, test/accuracy=0.985425, test/loss=0.048982, test/mean_average_precision=0.237434, test/num_examples=43793, total_duration=4825.316298, train/accuracy=0.989647, train/loss=0.034629, train/mean_average_precision=0.333902, validation/accuracy=0.986300, validation/loss=0.046295, validation/mean_average_precision=0.238772, validation/num_examples=43793
I0209 10:20:12.716415 139878416574208 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.017922157421708107, loss=0.040059205144643784
I0209 10:20:45.500147 139864233322240 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.019493145868182182, loss=0.04022746533155441
I0209 10:21:18.203751 139878416574208 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.018473295494914055, loss=0.039982229471206665
I0209 10:21:50.795949 139864233322240 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.017215179279446602, loss=0.04040477052330971
I0209 10:22:23.841881 139878416574208 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.020942671224474907, loss=0.03869111090898514
I0209 10:22:56.320794 139864233322240 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.018634572625160217, loss=0.04235439747571945
I0209 10:23:29.336936 139878416574208 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.016581300646066666, loss=0.037095751613378525
I0209 10:24:02.253500 139864233322240 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.015831362456083298, loss=0.04216394200921059
I0209 10:24:10.645254 140039251117888 spec.py:321] Evaluating on the training split.
I0209 10:26:06.768358 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 10:26:09.848460 140039251117888 spec.py:349] Evaluating on the test split.
I0209 10:26:12.935791 140039251117888 submission_runner.py:408] Time since start: 5187.91s, 	Step: 10326, 	{'train/accuracy': 0.989692211151123, 'train/loss': 0.034801095724105835, 'train/mean_average_precision': 0.33724113406074235, 'validation/accuracy': 0.9864484667778015, 'validation/loss': 0.04563755542039871, 'validation/mean_average_precision': 0.2384509236503546, 'validation/num_examples': 43793, 'test/accuracy': 0.9856414198875427, 'test/loss': 0.04823276773095131, 'test/mean_average_precision': 0.24216281094436193, 'test/num_examples': 43793, 'score': 3374.110833644867, 'total_duration': 5187.90918803215, 'accumulated_submission_time': 3374.110833644867, 'accumulated_eval_time': 1813.1092777252197, 'accumulated_logging_time': 0.40241336822509766}
I0209 10:26:12.953923 139871926159104 logging_writer.py:48] [10326] accumulated_eval_time=1813.109278, accumulated_logging_time=0.402413, accumulated_submission_time=3374.110834, global_step=10326, preemption_count=0, score=3374.110834, test/accuracy=0.985641, test/loss=0.048233, test/mean_average_precision=0.242163, test/num_examples=43793, total_duration=5187.909188, train/accuracy=0.989692, train/loss=0.034801, train/mean_average_precision=0.337241, validation/accuracy=0.986448, validation/loss=0.045638, validation/mean_average_precision=0.238451, validation/num_examples=43793
I0209 10:26:37.471034 139871934551808 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.015147740952670574, loss=0.03938143700361252
I0209 10:27:10.332933 139871926159104 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.013485590927302837, loss=0.03933589905500412
I0209 10:27:43.157294 139871934551808 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.022313086315989494, loss=0.04034394770860672
I0209 10:28:15.499746 139871926159104 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.015943173319101334, loss=0.03970031067728996
I0209 10:28:47.824216 139871934551808 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.015550042502582073, loss=0.037826575338840485
I0209 10:29:20.031399 139871926159104 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.01634962670505047, loss=0.0430821068584919
I0209 10:29:52.416436 139871934551808 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.014662167988717556, loss=0.03936973959207535
I0209 10:30:13.073241 140039251117888 spec.py:321] Evaluating on the training split.
I0209 10:32:06.456575 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 10:32:09.558768 140039251117888 spec.py:349] Evaluating on the test split.
I0209 10:32:12.574325 140039251117888 submission_runner.py:408] Time since start: 5547.55s, 	Step: 11064, 	{'train/accuracy': 0.9897984266281128, 'train/loss': 0.033979859203100204, 'train/mean_average_precision': 0.34181194912180346, 'validation/accuracy': 0.9864703416824341, 'validation/loss': 0.04572243243455887, 'validation/mean_average_precision': 0.24624071173579296, 'validation/num_examples': 43793, 'test/accuracy': 0.9856801629066467, 'test/loss': 0.048328228294849396, 'test/mean_average_precision': 0.2508927952331046, 'test/num_examples': 43793, 'score': 3614.1969878673553, 'total_duration': 5547.547726154327, 'accumulated_submission_time': 3614.1969878673553, 'accumulated_eval_time': 1932.6103360652924, 'accumulated_logging_time': 0.4336550235748291}
I0209 10:32:12.593030 139864233322240 logging_writer.py:48] [11064] accumulated_eval_time=1932.610336, accumulated_logging_time=0.433655, accumulated_submission_time=3614.196988, global_step=11064, preemption_count=0, score=3614.196988, test/accuracy=0.985680, test/loss=0.048328, test/mean_average_precision=0.250893, test/num_examples=43793, total_duration=5547.547726, train/accuracy=0.989798, train/loss=0.033980, train/mean_average_precision=0.341812, validation/accuracy=0.986470, validation/loss=0.045722, validation/mean_average_precision=0.246241, validation/num_examples=43793
I0209 10:32:24.600993 139878408181504 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.01688471809029579, loss=0.03786151483654976
I0209 10:32:57.216903 139864233322240 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.013792417012155056, loss=0.04006354510784149
I0209 10:33:29.529303 139878408181504 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.015969187021255493, loss=0.04049059748649597
I0209 10:34:01.936455 139864233322240 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.014642469584941864, loss=0.0409424714744091
I0209 10:34:34.359123 139878408181504 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.01415620930492878, loss=0.039205651730298996
I0209 10:35:06.553701 139864233322240 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.017077676951885223, loss=0.03905065730214119
I0209 10:35:38.774325 139878408181504 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.014440816827118397, loss=0.03581521287560463
I0209 10:36:11.197300 139864233322240 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.01790015771985054, loss=0.03722760081291199
I0209 10:36:12.761139 140039251117888 spec.py:321] Evaluating on the training split.
I0209 10:38:04.668688 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 10:38:08.110292 140039251117888 spec.py:349] Evaluating on the test split.
I0209 10:38:11.510841 140039251117888 submission_runner.py:408] Time since start: 5906.48s, 	Step: 11806, 	{'train/accuracy': 0.9900467395782471, 'train/loss': 0.033071935176849365, 'train/mean_average_precision': 0.3749070903184368, 'validation/accuracy': 0.9865052700042725, 'validation/loss': 0.04551073908805847, 'validation/mean_average_precision': 0.24596945471530543, 'validation/num_examples': 43793, 'test/accuracy': 0.9856789112091064, 'test/loss': 0.04816749319434166, 'test/mean_average_precision': 0.2501293202794034, 'test/num_examples': 43793, 'score': 3854.333966732025, 'total_duration': 5906.48420381546, 'accumulated_submission_time': 3854.333966732025, 'accumulated_eval_time': 2051.3599536418915, 'accumulated_logging_time': 0.463397741317749}
I0209 10:38:11.530031 139871934551808 logging_writer.py:48] [11806] accumulated_eval_time=2051.359954, accumulated_logging_time=0.463398, accumulated_submission_time=3854.333967, global_step=11806, preemption_count=0, score=3854.333967, test/accuracy=0.985679, test/loss=0.048167, test/mean_average_precision=0.250129, test/num_examples=43793, total_duration=5906.484204, train/accuracy=0.990047, train/loss=0.033072, train/mean_average_precision=0.374907, validation/accuracy=0.986505, validation/loss=0.045511, validation/mean_average_precision=0.245969, validation/num_examples=43793
I0209 10:38:42.840626 139878416574208 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.01580086536705494, loss=0.04092726856470108
I0209 10:39:16.137517 139871934551808 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.017880529165267944, loss=0.038194335997104645
I0209 10:39:49.077588 139878416574208 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.01663433201611042, loss=0.039176423102617264
I0209 10:40:21.940715 139871934551808 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.016645967960357666, loss=0.03757685050368309
I0209 10:40:54.916244 139878416574208 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.015866778790950775, loss=0.03540937975049019
I0209 10:41:27.690792 139871934551808 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.016596540808677673, loss=0.03629828244447708
I0209 10:42:00.321874 139878416574208 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.01594477705657482, loss=0.03696805238723755
I0209 10:42:11.589996 140039251117888 spec.py:321] Evaluating on the training split.
I0209 10:44:06.768904 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 10:44:09.893601 140039251117888 spec.py:349] Evaluating on the test split.
I0209 10:44:12.969799 140039251117888 submission_runner.py:408] Time since start: 6267.94s, 	Step: 12535, 	{'train/accuracy': 0.9904047846794128, 'train/loss': 0.03221077844500542, 'train/mean_average_precision': 0.37802480556687307, 'validation/accuracy': 0.9865750670433044, 'validation/loss': 0.04533899575471878, 'validation/mean_average_precision': 0.2465292262857419, 'validation/num_examples': 43793, 'test/accuracy': 0.9857109189033508, 'test/loss': 0.04792236536741257, 'test/mean_average_precision': 0.2474763344552082, 'test/num_examples': 43793, 'score': 4094.3583641052246, 'total_duration': 6267.943195104599, 'accumulated_submission_time': 4094.3583641052246, 'accumulated_eval_time': 2172.739722967148, 'accumulated_logging_time': 0.49429869651794434}
I0209 10:44:12.988287 139864233322240 logging_writer.py:48] [12535] accumulated_eval_time=2172.739723, accumulated_logging_time=0.494299, accumulated_submission_time=4094.358364, global_step=12535, preemption_count=0, score=4094.358364, test/accuracy=0.985711, test/loss=0.047922, test/mean_average_precision=0.247476, test/num_examples=43793, total_duration=6267.943195, train/accuracy=0.990405, train/loss=0.032211, train/mean_average_precision=0.378025, validation/accuracy=0.986575, validation/loss=0.045339, validation/mean_average_precision=0.246529, validation/num_examples=43793
I0209 10:44:35.213292 139871926159104 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.016958586871623993, loss=0.03829297795891762
I0209 10:45:07.950129 139864233322240 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.015758760273456573, loss=0.03691479563713074
I0209 10:45:40.161839 139871926159104 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.018495852127671242, loss=0.039428241550922394
I0209 10:46:13.025301 139864233322240 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.023070193827152252, loss=0.03791135922074318
I0209 10:46:45.232876 139871926159104 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.018276942893862724, loss=0.04022322595119476
I0209 10:47:17.961343 139864233322240 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.01795966923236847, loss=0.040557585656642914
I0209 10:47:50.385359 139871926159104 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.022107725962996483, loss=0.042486514896154404
I0209 10:48:13.294529 140039251117888 spec.py:321] Evaluating on the training split.
I0209 10:50:04.694434 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 10:50:07.998452 140039251117888 spec.py:349] Evaluating on the test split.
I0209 10:50:11.219308 140039251117888 submission_runner.py:408] Time since start: 6626.19s, 	Step: 13272, 	{'train/accuracy': 0.990593671798706, 'train/loss': 0.03138144314289093, 'train/mean_average_precision': 0.4109021595111499, 'validation/accuracy': 0.9866944551467896, 'validation/loss': 0.04504287615418434, 'validation/mean_average_precision': 0.2539900303246503, 'validation/num_examples': 43793, 'test/accuracy': 0.9858461618423462, 'test/loss': 0.04783207178115845, 'test/mean_average_precision': 0.25654210886448603, 'test/num_examples': 43793, 'score': 4334.632151842117, 'total_duration': 6626.192683458328, 'accumulated_submission_time': 4334.632151842117, 'accumulated_eval_time': 2290.664441347122, 'accumulated_logging_time': 0.525383710861206}
I0209 10:50:11.239058 139871934551808 logging_writer.py:48] [13272] accumulated_eval_time=2290.664441, accumulated_logging_time=0.525384, accumulated_submission_time=4334.632152, global_step=13272, preemption_count=0, score=4334.632152, test/accuracy=0.985846, test/loss=0.047832, test/mean_average_precision=0.256542, test/num_examples=43793, total_duration=6626.192683, train/accuracy=0.990594, train/loss=0.031381, train/mean_average_precision=0.410902, validation/accuracy=0.986694, validation/loss=0.045043, validation/mean_average_precision=0.253990, validation/num_examples=43793
I0209 10:50:20.783905 139878416574208 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.018904179334640503, loss=0.03652618080377579
I0209 10:50:53.901166 139871934551808 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.019171729683876038, loss=0.039738137274980545
I0209 10:51:26.579436 139878416574208 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.0143224922940135, loss=0.034905385226011276
I0209 10:51:59.102365 139871934551808 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.016122838482260704, loss=0.038122691214084625
I0209 10:52:31.342266 139878416574208 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.017468005418777466, loss=0.03832994028925896
I0209 10:53:03.981513 139871934551808 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.01618758775293827, loss=0.03811411187052727
I0209 10:53:36.509977 139878416574208 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.021099111065268517, loss=0.038843803107738495
I0209 10:54:09.255388 139871934551808 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.01943327859044075, loss=0.04029085114598274
I0209 10:54:11.532930 140039251117888 spec.py:321] Evaluating on the training split.
I0209 10:56:06.878255 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 10:56:09.961301 140039251117888 spec.py:349] Evaluating on the test split.
I0209 10:56:12.975783 140039251117888 submission_runner.py:408] Time since start: 6987.95s, 	Step: 14008, 	{'train/accuracy': 0.9907403588294983, 'train/loss': 0.030393077060580254, 'train/mean_average_precision': 0.43595080978843537, 'validation/accuracy': 0.9867265224456787, 'validation/loss': 0.04505808278918266, 'validation/mean_average_precision': 0.2625926952812717, 'validation/num_examples': 43793, 'test/accuracy': 0.985842764377594, 'test/loss': 0.047687798738479614, 'test/mean_average_precision': 0.2592138244146481, 'test/num_examples': 43793, 'score': 4574.894693851471, 'total_duration': 6987.949181556702, 'accumulated_submission_time': 4574.894693851471, 'accumulated_eval_time': 2412.107246160507, 'accumulated_logging_time': 0.5567858219146729}
I0209 10:56:12.994010 139864233322240 logging_writer.py:48] [14008] accumulated_eval_time=2412.107246, accumulated_logging_time=0.556786, accumulated_submission_time=4574.894694, global_step=14008, preemption_count=0, score=4574.894694, test/accuracy=0.985843, test/loss=0.047688, test/mean_average_precision=0.259214, test/num_examples=43793, total_duration=6987.949182, train/accuracy=0.990740, train/loss=0.030393, train/mean_average_precision=0.435951, validation/accuracy=0.986727, validation/loss=0.045058, validation/mean_average_precision=0.262593, validation/num_examples=43793
I0209 10:56:43.954855 139878408181504 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.02175888791680336, loss=0.040321361273527145
I0209 10:57:17.168750 139864233322240 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.022844821214675903, loss=0.03652237355709076
I0209 10:57:49.455039 139878408181504 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.015987901017069817, loss=0.03591351956129074
I0209 10:58:22.386947 139864233322240 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.016153231263160706, loss=0.036242738366127014
I0209 10:58:55.224354 139878408181504 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.016690770164132118, loss=0.03748264163732529
I0209 10:59:27.834226 139864233322240 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.019877610728144646, loss=0.036790281534194946
I0209 11:00:00.616942 139878408181504 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.028074830770492554, loss=0.035703033208847046
I0209 11:00:13.038450 140039251117888 spec.py:321] Evaluating on the training split.
I0209 11:02:10.208833 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 11:02:13.256373 140039251117888 spec.py:349] Evaluating on the test split.
I0209 11:02:16.376817 140039251117888 submission_runner.py:408] Time since start: 7351.35s, 	Step: 14739, 	{'train/accuracy': 0.9908390641212463, 'train/loss': 0.02991710603237152, 'train/mean_average_precision': 0.4426423572423156, 'validation/accuracy': 0.9866229891777039, 'validation/loss': 0.04541115462779999, 'validation/mean_average_precision': 0.25832731771207995, 'validation/num_examples': 43793, 'test/accuracy': 0.9856974482536316, 'test/loss': 0.04819463565945625, 'test/mean_average_precision': 0.25313458407153794, 'test/num_examples': 43793, 'score': 4814.908618211746, 'total_duration': 7351.350212574005, 'accumulated_submission_time': 4814.908618211746, 'accumulated_eval_time': 2535.4455637931824, 'accumulated_logging_time': 0.586010217666626}
I0209 11:02:16.395532 139871926159104 logging_writer.py:48] [14739] accumulated_eval_time=2535.445564, accumulated_logging_time=0.586010, accumulated_submission_time=4814.908618, global_step=14739, preemption_count=0, score=4814.908618, test/accuracy=0.985697, test/loss=0.048195, test/mean_average_precision=0.253135, test/num_examples=43793, total_duration=7351.350213, train/accuracy=0.990839, train/loss=0.029917, train/mean_average_precision=0.442642, validation/accuracy=0.986623, validation/loss=0.045411, validation/mean_average_precision=0.258327, validation/num_examples=43793
I0209 11:02:37.126383 139878416574208 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.021531671285629272, loss=0.03919297456741333
I0209 11:03:10.407250 139871926159104 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.014655235223472118, loss=0.033913254737854004
I0209 11:03:43.007586 139878416574208 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.02321024425327778, loss=0.03790563717484474
I0209 11:04:15.989120 139871926159104 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.015874285250902176, loss=0.03434883430600166
I0209 11:04:48.965881 139878416574208 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.017817489802837372, loss=0.03648446500301361
I0209 11:05:22.156865 139871926159104 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.018908550962805748, loss=0.03806394711136818
I0209 11:05:54.689351 139878416574208 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.022314108908176422, loss=0.03922172263264656
I0209 11:06:16.417009 140039251117888 spec.py:321] Evaluating on the training split.
I0209 11:08:11.246076 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 11:08:14.336764 140039251117888 spec.py:349] Evaluating on the test split.
I0209 11:08:17.357621 140039251117888 submission_runner.py:408] Time since start: 7712.33s, 	Step: 15467, 	{'train/accuracy': 0.9911337494850159, 'train/loss': 0.029770169407129288, 'train/mean_average_precision': 0.43945538132750406, 'validation/accuracy': 0.9866952300071716, 'validation/loss': 0.04476303607225418, 'validation/mean_average_precision': 0.25842765475475865, 'validation/num_examples': 43793, 'test/accuracy': 0.9858457446098328, 'test/loss': 0.04745708405971527, 'test/mean_average_precision': 0.2533422638624075, 'test/num_examples': 43793, 'score': 5054.8963351249695, 'total_duration': 7712.3310215473175, 'accumulated_submission_time': 5054.8963351249695, 'accumulated_eval_time': 2656.38614153862, 'accumulated_logging_time': 0.6158895492553711}
I0209 11:08:17.376244 139864233322240 logging_writer.py:48] [15467] accumulated_eval_time=2656.386142, accumulated_logging_time=0.615890, accumulated_submission_time=5054.896335, global_step=15467, preemption_count=0, score=5054.896335, test/accuracy=0.985846, test/loss=0.047457, test/mean_average_precision=0.253342, test/num_examples=43793, total_duration=7712.331022, train/accuracy=0.991134, train/loss=0.029770, train/mean_average_precision=0.439455, validation/accuracy=0.986695, validation/loss=0.044763, validation/mean_average_precision=0.258428, validation/num_examples=43793
I0209 11:08:28.650089 139878408181504 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.019198119640350342, loss=0.03682469576597214
I0209 11:09:01.302437 139864233322240 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.021875638514757156, loss=0.033456575125455856
I0209 11:09:34.220635 139878408181504 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.018862849101424217, loss=0.03407770395278931
I0209 11:10:07.052592 139864233322240 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.026734936982393265, loss=0.035655371844768524
I0209 11:10:39.613282 139878408181504 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.018835779279470444, loss=0.03804820775985718
I0209 11:11:12.457885 139864233322240 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.01828126609325409, loss=0.03580335155129433
I0209 11:11:44.963070 139878408181504 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.019584057852625847, loss=0.03611660748720169
I0209 11:12:17.625306 139864233322240 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.018431002274155617, loss=0.036024611443281174
I0209 11:12:17.631313 140039251117888 spec.py:321] Evaluating on the training split.
I0209 11:14:07.954456 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 11:14:10.988276 140039251117888 spec.py:349] Evaluating on the test split.
I0209 11:14:13.984740 140039251117888 submission_runner.py:408] Time since start: 8068.96s, 	Step: 16201, 	{'train/accuracy': 0.9909468293190002, 'train/loss': 0.029692286625504494, 'train/mean_average_precision': 0.4539990121975116, 'validation/accuracy': 0.9867402911186218, 'validation/loss': 0.045339684933423996, 'validation/mean_average_precision': 0.2613085320446217, 'validation/num_examples': 43793, 'test/accuracy': 0.9859640598297119, 'test/loss': 0.04803311079740524, 'test/mean_average_precision': 0.26270306084262335, 'test/num_examples': 43793, 'score': 5295.120626449585, 'total_duration': 8068.958131074905, 'accumulated_submission_time': 5295.120626449585, 'accumulated_eval_time': 2772.7395095825195, 'accumulated_logging_time': 0.6458499431610107}
I0209 11:14:14.003801 139871934551808 logging_writer.py:48] [16201] accumulated_eval_time=2772.739510, accumulated_logging_time=0.645850, accumulated_submission_time=5295.120626, global_step=16201, preemption_count=0, score=5295.120626, test/accuracy=0.985964, test/loss=0.048033, test/mean_average_precision=0.262703, test/num_examples=43793, total_duration=8068.958131, train/accuracy=0.990947, train/loss=0.029692, train/mean_average_precision=0.453999, validation/accuracy=0.986740, validation/loss=0.045340, validation/mean_average_precision=0.261309, validation/num_examples=43793
I0209 11:14:47.026160 139878416574208 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.01753031089901924, loss=0.03272321820259094
I0209 11:15:19.118869 139871934551808 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.018338168039917946, loss=0.037239257246255875
I0209 11:15:51.274475 139878416574208 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.02465008944272995, loss=0.037938378751277924
I0209 11:16:23.634035 139871934551808 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.02037683129310608, loss=0.037178538739681244
I0209 11:16:55.746478 139878416574208 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.019475627690553665, loss=0.03593514859676361
I0209 11:17:28.192522 139871934551808 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.024552278220653534, loss=0.0359349362552166
I0209 11:18:00.143525 139878416574208 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.020589258521795273, loss=0.03601190820336342
I0209 11:18:14.129500 140039251117888 spec.py:321] Evaluating on the training split.
I0209 11:20:05.901795 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 11:20:09.239596 140039251117888 spec.py:349] Evaluating on the test split.
I0209 11:20:12.570394 140039251117888 submission_runner.py:408] Time since start: 8427.54s, 	Step: 16944, 	{'train/accuracy': 0.9909261465072632, 'train/loss': 0.02961067296564579, 'train/mean_average_precision': 0.4592231931246184, 'validation/accuracy': 0.9867545366287231, 'validation/loss': 0.04582856222987175, 'validation/mean_average_precision': 0.26213948876819726, 'validation/num_examples': 43793, 'test/accuracy': 0.9858773350715637, 'test/loss': 0.0486006960272789, 'test/mean_average_precision': 0.2582021144128389, 'test/num_examples': 43793, 'score': 5535.215556144714, 'total_duration': 8427.543749332428, 'accumulated_submission_time': 5535.215556144714, 'accumulated_eval_time': 2891.1803154945374, 'accumulated_logging_time': 0.6761302947998047}
I0209 11:20:12.591180 139871926159104 logging_writer.py:48] [16944] accumulated_eval_time=2891.180315, accumulated_logging_time=0.676130, accumulated_submission_time=5535.215556, global_step=16944, preemption_count=0, score=5535.215556, test/accuracy=0.985877, test/loss=0.048601, test/mean_average_precision=0.258202, test/num_examples=43793, total_duration=8427.543749, train/accuracy=0.990926, train/loss=0.029611, train/mean_average_precision=0.459223, validation/accuracy=0.986755, validation/loss=0.045829, validation/mean_average_precision=0.262139, validation/num_examples=43793
I0209 11:20:31.535618 139878408181504 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.019110238179564476, loss=0.03540923446416855
I0209 11:21:04.504681 139871926159104 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.025429824367165565, loss=0.03630298376083374
I0209 11:21:37.033280 139878408181504 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.022030429914593697, loss=0.03460732474923134
I0209 11:22:09.579378 139871926159104 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.0251249298453331, loss=0.03504106402397156
I0209 11:22:41.850172 139878408181504 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.02188967540860176, loss=0.03575854375958443
I0209 11:23:14.125806 139871926159104 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.022649751976132393, loss=0.03747466206550598
I0209 11:23:46.664545 139878408181504 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.02216503955423832, loss=0.034472640603780746
I0209 11:24:12.756832 140039251117888 spec.py:321] Evaluating on the training split.
I0209 11:26:02.735043 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 11:26:05.796799 140039251117888 spec.py:349] Evaluating on the test split.
I0209 11:26:08.811370 140039251117888 submission_runner.py:408] Time since start: 8783.78s, 	Step: 17681, 	{'train/accuracy': 0.9912601709365845, 'train/loss': 0.02848539873957634, 'train/mean_average_precision': 0.47475583614365885, 'validation/accuracy': 0.9867882132530212, 'validation/loss': 0.04549652710556984, 'validation/mean_average_precision': 0.26414985254090756, 'validation/num_examples': 43793, 'test/accuracy': 0.985878586769104, 'test/loss': 0.04837343841791153, 'test/mean_average_precision': 0.2591937009499683, 'test/num_examples': 43793, 'score': 5775.349369287491, 'total_duration': 8783.784770011902, 'accumulated_submission_time': 5775.349369287491, 'accumulated_eval_time': 3007.234807729721, 'accumulated_logging_time': 0.7085833549499512}
I0209 11:26:08.831351 139871934551808 logging_writer.py:48] [17681] accumulated_eval_time=3007.234808, accumulated_logging_time=0.708583, accumulated_submission_time=5775.349369, global_step=17681, preemption_count=0, score=5775.349369, test/accuracy=0.985879, test/loss=0.048373, test/mean_average_precision=0.259194, test/num_examples=43793, total_duration=8783.784770, train/accuracy=0.991260, train/loss=0.028485, train/mean_average_precision=0.474756, validation/accuracy=0.986788, validation/loss=0.045497, validation/mean_average_precision=0.264150, validation/num_examples=43793
I0209 11:26:15.819169 139878416574208 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.026489639654755592, loss=0.03450572118163109
I0209 11:26:48.386624 139871934551808 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.029729075729846954, loss=0.03791724145412445
I0209 11:27:20.845476 139878416574208 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.018422024324536324, loss=0.0327998623251915
I0209 11:27:53.064365 139871934551808 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.023909689858555794, loss=0.03859218582510948
I0209 11:28:25.660334 139878416574208 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.020193668082356453, loss=0.03284332901239395
I0209 11:28:57.881855 139871934551808 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.021469218656420708, loss=0.03345760703086853
I0209 11:29:30.621792 139878416574208 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.021501731127500534, loss=0.03757372125983238
I0209 11:30:03.038092 139871934551808 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.034082114696502686, loss=0.034827277064323425
I0209 11:30:09.100975 140039251117888 spec.py:321] Evaluating on the training split.
I0209 11:32:05.536014 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 11:32:08.618281 140039251117888 spec.py:349] Evaluating on the test split.
I0209 11:32:11.640943 140039251117888 submission_runner.py:408] Time since start: 9146.61s, 	Step: 18420, 	{'train/accuracy': 0.9915626645088196, 'train/loss': 0.027797440066933632, 'train/mean_average_precision': 0.48316749959775956, 'validation/accuracy': 0.9867565631866455, 'validation/loss': 0.04541521891951561, 'validation/mean_average_precision': 0.2554176813356391, 'validation/num_examples': 43793, 'test/accuracy': 0.9858541488647461, 'test/loss': 0.048193056136369705, 'test/mean_average_precision': 0.2527952731086541, 'test/num_examples': 43793, 'score': 6015.588186979294, 'total_duration': 9146.614343166351, 'accumulated_submission_time': 6015.588186979294, 'accumulated_eval_time': 3129.774727344513, 'accumulated_logging_time': 0.739865779876709}
I0209 11:32:11.659948 139864233322240 logging_writer.py:48] [18420] accumulated_eval_time=3129.774727, accumulated_logging_time=0.739866, accumulated_submission_time=6015.588187, global_step=18420, preemption_count=0, score=6015.588187, test/accuracy=0.985854, test/loss=0.048193, test/mean_average_precision=0.252795, test/num_examples=43793, total_duration=9146.614343, train/accuracy=0.991563, train/loss=0.027797, train/mean_average_precision=0.483167, validation/accuracy=0.986757, validation/loss=0.045415, validation/mean_average_precision=0.255418, validation/num_examples=43793
I0209 11:32:37.510919 139871926159104 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.022713040933012962, loss=0.03656279668211937
I0209 11:33:09.588228 139864233322240 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.02397015132009983, loss=0.035660270601511
I0209 11:33:41.437195 139871926159104 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.027265870943665504, loss=0.03614356368780136
I0209 11:34:13.426459 139864233322240 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.02768549881875515, loss=0.03496471419930458
I0209 11:34:45.805051 139871926159104 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.03955967351794243, loss=0.036901578307151794
I0209 11:35:18.131416 139864233322240 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.024621549993753433, loss=0.038310617208480835
I0209 11:35:50.462485 139871926159104 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.02609904296696186, loss=0.03498021885752678
I0209 11:36:11.847051 140039251117888 spec.py:321] Evaluating on the training split.
I0209 11:38:00.081494 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 11:38:03.343100 140039251117888 spec.py:349] Evaluating on the test split.
I0209 11:38:06.460803 140039251117888 submission_runner.py:408] Time since start: 9501.43s, 	Step: 19167, 	{'train/accuracy': 0.9915648102760315, 'train/loss': 0.02735765278339386, 'train/mean_average_precision': 0.5086215310274381, 'validation/accuracy': 0.9867614507675171, 'validation/loss': 0.0455816313624382, 'validation/mean_average_precision': 0.26315183788354574, 'validation/num_examples': 43793, 'test/accuracy': 0.9859089255332947, 'test/loss': 0.04837782680988312, 'test/mean_average_precision': 0.25896516638266726, 'test/num_examples': 43793, 'score': 6255.743933677673, 'total_duration': 9501.434201717377, 'accumulated_submission_time': 6255.743933677673, 'accumulated_eval_time': 3244.3884332180023, 'accumulated_logging_time': 0.7705888748168945}
I0209 11:38:06.480602 139871934551808 logging_writer.py:48] [19167] accumulated_eval_time=3244.388433, accumulated_logging_time=0.770589, accumulated_submission_time=6255.743934, global_step=19167, preemption_count=0, score=6255.743934, test/accuracy=0.985909, test/loss=0.048378, test/mean_average_precision=0.258965, test/num_examples=43793, total_duration=9501.434202, train/accuracy=0.991565, train/loss=0.027358, train/mean_average_precision=0.508622, validation/accuracy=0.986761, validation/loss=0.045582, validation/mean_average_precision=0.263152, validation/num_examples=43793
I0209 11:38:17.557199 139878408181504 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.029324686154723167, loss=0.03548016771674156
I0209 11:38:50.031504 139871934551808 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.022521203383803368, loss=0.03603052720427513
I0209 11:39:22.800817 139878408181504 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.02735697291791439, loss=0.03631141409277916
I0209 11:39:55.164049 139871934551808 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.02328040450811386, loss=0.03514806553721428
I0209 11:40:27.931923 139878408181504 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.0221001747995615, loss=0.03413597121834755
I0209 11:41:00.962784 139871934551808 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.025728750973939896, loss=0.034349601715803146
I0209 11:41:33.590766 139878408181504 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.02253984846174717, loss=0.0339275598526001
I0209 11:42:06.100797 139871934551808 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.029707975685596466, loss=0.03282769024372101
I0209 11:42:06.775283 140039251117888 spec.py:321] Evaluating on the training split.
I0209 11:43:54.579373 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 11:43:57.631487 140039251117888 spec.py:349] Evaluating on the test split.
I0209 11:44:00.580592 140039251117888 submission_runner.py:408] Time since start: 9855.55s, 	Step: 19903, 	{'train/accuracy': 0.992231011390686, 'train/loss': 0.025556445121765137, 'train/mean_average_precision': 0.5372999658293216, 'validation/accuracy': 0.9868332743644714, 'validation/loss': 0.04510356858372688, 'validation/mean_average_precision': 0.2715457824918656, 'validation/num_examples': 43793, 'test/accuracy': 0.9860078692436218, 'test/loss': 0.04783665016293526, 'test/mean_average_precision': 0.2606229283561334, 'test/num_examples': 43793, 'score': 6496.008260965347, 'total_duration': 9855.55399274826, 'accumulated_submission_time': 6496.008260965347, 'accumulated_eval_time': 3358.1936955451965, 'accumulated_logging_time': 0.8017594814300537}
I0209 11:44:00.599874 139864233322240 logging_writer.py:48] [19903] accumulated_eval_time=3358.193696, accumulated_logging_time=0.801759, accumulated_submission_time=6496.008261, global_step=19903, preemption_count=0, score=6496.008261, test/accuracy=0.986008, test/loss=0.047837, test/mean_average_precision=0.260623, test/num_examples=43793, total_duration=9855.553993, train/accuracy=0.992231, train/loss=0.025556, train/mean_average_precision=0.537300, validation/accuracy=0.986833, validation/loss=0.045104, validation/mean_average_precision=0.271546, validation/num_examples=43793
I0209 11:44:32.626688 139878416574208 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.028074132278561592, loss=0.03535304591059685
I0209 11:45:04.777632 139864233322240 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.026197269558906555, loss=0.03470415249466896
I0209 11:45:37.137413 139878416574208 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.02882849983870983, loss=0.03326357156038284
I0209 11:46:09.351281 139864233322240 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.02387348935008049, loss=0.034460220485925674
I0209 11:46:41.815265 139878416574208 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.022997628897428513, loss=0.031993452459573746
I0209 11:47:14.914253 139864233322240 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.02897067554295063, loss=0.03511820733547211
I0209 11:47:46.981503 139878416574208 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.027345484122633934, loss=0.03455488756299019
I0209 11:48:00.795844 140039251117888 spec.py:321] Evaluating on the training split.
I0209 11:49:51.090135 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 11:49:54.200004 140039251117888 spec.py:349] Evaluating on the test split.
I0209 11:49:57.163381 140039251117888 submission_runner.py:408] Time since start: 10212.14s, 	Step: 20644, 	{'train/accuracy': 0.9922563433647156, 'train/loss': 0.025297215208411217, 'train/mean_average_precision': 0.5544247501332782, 'validation/accuracy': 0.9868401885032654, 'validation/loss': 0.04562132805585861, 'validation/mean_average_precision': 0.26867772622666897, 'validation/num_examples': 43793, 'test/accuracy': 0.9859581589698792, 'test/loss': 0.04842758551239967, 'test/mean_average_precision': 0.26032937570119197, 'test/num_examples': 43793, 'score': 6736.173248052597, 'total_duration': 10212.136778831482, 'accumulated_submission_time': 6736.173248052597, 'accumulated_eval_time': 3474.561186313629, 'accumulated_logging_time': 0.832097053527832}
I0209 11:49:57.182688 139871926159104 logging_writer.py:48] [20644] accumulated_eval_time=3474.561186, accumulated_logging_time=0.832097, accumulated_submission_time=6736.173248, global_step=20644, preemption_count=0, score=6736.173248, test/accuracy=0.985958, test/loss=0.048428, test/mean_average_precision=0.260329, test/num_examples=43793, total_duration=10212.136779, train/accuracy=0.992256, train/loss=0.025297, train/mean_average_precision=0.554425, validation/accuracy=0.986840, validation/loss=0.045621, validation/mean_average_precision=0.268678, validation/num_examples=43793
I0209 11:50:15.654153 139878408181504 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.024142149835824966, loss=0.030793601647019386
I0209 11:50:47.931747 139871926159104 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.026658814400434494, loss=0.0323602557182312
I0209 11:51:20.783256 139878408181504 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.026479588821530342, loss=0.03587549552321434
I0209 11:51:53.058304 139871926159104 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.027723204344511032, loss=0.03347786143422127
I0209 11:52:25.609075 139878408181504 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.026957113295793533, loss=0.03321768343448639
I0209 11:52:57.934188 139871926159104 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.02724599838256836, loss=0.03380749002099037
I0209 11:53:30.238525 139878408181504 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.029950449243187904, loss=0.0328790545463562
I0209 11:53:57.193064 140039251117888 spec.py:321] Evaluating on the training split.
I0209 11:55:46.820250 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 11:55:49.871114 140039251117888 spec.py:349] Evaluating on the test split.
I0209 11:55:52.900406 140039251117888 submission_runner.py:408] Time since start: 10567.87s, 	Step: 21384, 	{'train/accuracy': 0.9923427700996399, 'train/loss': 0.024966953322291374, 'train/mean_average_precision': 0.5519044794078625, 'validation/accuracy': 0.9868003726005554, 'validation/loss': 0.046172283589839935, 'validation/mean_average_precision': 0.264854724285865, 'validation/num_examples': 43793, 'test/accuracy': 0.9859493374824524, 'test/loss': 0.049079928547143936, 'test/mean_average_precision': 0.2598057736054661, 'test/num_examples': 43793, 'score': 6976.150879383087, 'total_duration': 10567.873800992966, 'accumulated_submission_time': 6976.150879383087, 'accumulated_eval_time': 3590.268481016159, 'accumulated_logging_time': 0.8643553256988525}
I0209 11:55:52.921011 139864233322240 logging_writer.py:48] [21384] accumulated_eval_time=3590.268481, accumulated_logging_time=0.864355, accumulated_submission_time=6976.150879, global_step=21384, preemption_count=0, score=6976.150879, test/accuracy=0.985949, test/loss=0.049080, test/mean_average_precision=0.259806, test/num_examples=43793, total_duration=10567.873801, train/accuracy=0.992343, train/loss=0.024967, train/mean_average_precision=0.551904, validation/accuracy=0.986800, validation/loss=0.046172, validation/mean_average_precision=0.264855, validation/num_examples=43793
I0209 11:55:58.453286 139871934551808 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.027642320841550827, loss=0.033239707350730896
I0209 11:56:31.674801 139864233322240 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.033871013671159744, loss=0.032107457518577576
I0209 11:57:04.395256 139871934551808 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.025942815467715263, loss=0.03406199440360069
I0209 11:57:36.878477 139864233322240 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.028918473049998283, loss=0.031489964574575424
I0209 11:58:09.418508 139871934551808 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.03098837099969387, loss=0.03216987103223801
I0209 11:58:41.796804 139864233322240 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.03159884363412857, loss=0.03690430521965027
I0209 11:59:14.600058 139871934551808 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.024801526218652725, loss=0.032510362565517426
I0209 11:59:46.754463 139864233322240 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.02814370207488537, loss=0.032705750316381454
I0209 11:59:52.918169 140039251117888 spec.py:321] Evaluating on the training split.
I0209 12:01:45.741657 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 12:01:48.910592 140039251117888 spec.py:349] Evaluating on the test split.
I0209 12:01:51.946751 140039251117888 submission_runner.py:408] Time since start: 10926.92s, 	Step: 22120, 	{'train/accuracy': 0.991874098777771, 'train/loss': 0.026532527059316635, 'train/mean_average_precision': 0.5180254499914452, 'validation/accuracy': 0.9867808818817139, 'validation/loss': 0.04616933688521385, 'validation/mean_average_precision': 0.26209752667709835, 'validation/num_examples': 43793, 'test/accuracy': 0.985874354839325, 'test/loss': 0.0491098128259182, 'test/mean_average_precision': 0.26179309925375405, 'test/num_examples': 43793, 'score': 7216.1168756484985, 'total_duration': 10926.920147418976, 'accumulated_submission_time': 7216.1168756484985, 'accumulated_eval_time': 3709.2970135211945, 'accumulated_logging_time': 0.8965957164764404}
I0209 12:01:51.966649 139871926159104 logging_writer.py:48] [22120] accumulated_eval_time=3709.297014, accumulated_logging_time=0.896596, accumulated_submission_time=7216.116876, global_step=22120, preemption_count=0, score=7216.116876, test/accuracy=0.985874, test/loss=0.049110, test/mean_average_precision=0.261793, test/num_examples=43793, total_duration=10926.920147, train/accuracy=0.991874, train/loss=0.026533, train/mean_average_precision=0.518025, validation/accuracy=0.986781, validation/loss=0.046169, validation/mean_average_precision=0.262098, validation/num_examples=43793
I0209 12:02:18.448571 139878408181504 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.03041878528892994, loss=0.03257693350315094
I0209 12:02:51.310936 139871926159104 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.030304724350571632, loss=0.03387647494673729
I0209 12:03:24.259507 139878408181504 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.030991043895483017, loss=0.03159062936902046
I0209 12:03:56.594562 139871926159104 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.0302381943911314, loss=0.03076854720711708
I0209 12:04:29.567671 139878408181504 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.029332105070352554, loss=0.03507249429821968
I0209 12:05:02.140951 139871926159104 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.02705126442015171, loss=0.03098086081445217
I0209 12:05:34.996433 139878408181504 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.030429814010858536, loss=0.03155004605650902
I0209 12:05:52.260830 140039251117888 spec.py:321] Evaluating on the training split.
I0209 12:07:48.170110 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 12:07:51.561914 140039251117888 spec.py:349] Evaluating on the test split.
I0209 12:07:54.909132 140039251117888 submission_runner.py:408] Time since start: 11289.88s, 	Step: 22854, 	{'train/accuracy': 0.9917595386505127, 'train/loss': 0.026461439207196236, 'train/mean_average_precision': 0.5174314820179169, 'validation/accuracy': 0.986764669418335, 'validation/loss': 0.04666900262236595, 'validation/mean_average_precision': 0.26590033672032376, 'validation/num_examples': 43793, 'test/accuracy': 0.9859341979026794, 'test/loss': 0.049701590090990067, 'test/mean_average_precision': 0.2569096916812762, 'test/num_examples': 43793, 'score': 7456.379545927048, 'total_duration': 11289.88250374794, 'accumulated_submission_time': 7456.379545927048, 'accumulated_eval_time': 3831.9452407360077, 'accumulated_logging_time': 0.9279828071594238}
I0209 12:07:54.931399 139864233322240 logging_writer.py:48] [22854] accumulated_eval_time=3831.945241, accumulated_logging_time=0.927983, accumulated_submission_time=7456.379546, global_step=22854, preemption_count=0, score=7456.379546, test/accuracy=0.985934, test/loss=0.049702, test/mean_average_precision=0.256910, test/num_examples=43793, total_duration=11289.882504, train/accuracy=0.991760, train/loss=0.026461, train/mean_average_precision=0.517431, validation/accuracy=0.986765, validation/loss=0.046669, validation/mean_average_precision=0.265900, validation/num_examples=43793
I0209 12:08:10.617139 139871934551808 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.027305584400892258, loss=0.03261691331863403
I0209 12:08:44.201433 139864233322240 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.03111577220261097, loss=0.03243137523531914
I0209 12:09:17.213921 139871934551808 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.0316549576818943, loss=0.02904827706515789
I0209 12:09:49.331344 139864233322240 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.029953354969620705, loss=0.03178989887237549
I0209 12:10:21.961943 139871934551808 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.03743064031004906, loss=0.03706477954983711
I0209 12:10:54.445787 139864233322240 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.03329146280884743, loss=0.034215133637189865
I0209 12:11:27.156160 139871934551808 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.03041948564350605, loss=0.03283222019672394
I0209 12:11:55.069879 140039251117888 spec.py:321] Evaluating on the training split.
I0209 12:13:49.482612 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 12:13:52.598392 140039251117888 spec.py:349] Evaluating on the test split.
I0209 12:13:55.758335 140039251117888 submission_runner.py:408] Time since start: 11650.73s, 	Step: 23586, 	{'train/accuracy': 0.992215096950531, 'train/loss': 0.025306835770606995, 'train/mean_average_precision': 0.5333598000806847, 'validation/accuracy': 0.9867005348205566, 'validation/loss': 0.046321846544742584, 'validation/mean_average_precision': 0.25778430552511333, 'validation/num_examples': 43793, 'test/accuracy': 0.9858810901641846, 'test/loss': 0.0492570735514164, 'test/mean_average_precision': 0.25460882352500436, 'test/num_examples': 43793, 'score': 7696.485308170319, 'total_duration': 11650.7317340374, 'accumulated_submission_time': 7696.485308170319, 'accumulated_eval_time': 3952.633655309677, 'accumulated_logging_time': 0.962378740310669}
I0209 12:13:55.779638 139871926159104 logging_writer.py:48] [23586] accumulated_eval_time=3952.633655, accumulated_logging_time=0.962379, accumulated_submission_time=7696.485308, global_step=23586, preemption_count=0, score=7696.485308, test/accuracy=0.985881, test/loss=0.049257, test/mean_average_precision=0.254609, test/num_examples=43793, total_duration=11650.731734, train/accuracy=0.992215, train/loss=0.025307, train/mean_average_precision=0.533360, validation/accuracy=0.986701, validation/loss=0.046322, validation/mean_average_precision=0.257784, validation/num_examples=43793
I0209 12:14:00.855378 139878408181504 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.03659278526902199, loss=0.031206568703055382
I0209 12:14:33.792226 139871926159104 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.039074260741472244, loss=0.03215622901916504
I0209 12:15:06.566816 139878408181504 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.03873022645711899, loss=0.03649946674704552
I0209 12:15:39.754028 139871926159104 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.03247948735952377, loss=0.03291642665863037
I0209 12:16:12.880533 139878408181504 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.04157531261444092, loss=0.03379971534013748
I0209 12:16:46.626367 139871926159104 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.032398346811532974, loss=0.030565433204174042
I0209 12:17:19.524752 139878408181504 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.035009387880563736, loss=0.03184361755847931
I0209 12:17:51.646511 139871926159104 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.030570315197110176, loss=0.03263453394174576
I0209 12:17:55.847989 140039251117888 spec.py:321] Evaluating on the training split.
I0209 12:19:49.055456 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 12:19:52.114632 140039251117888 spec.py:349] Evaluating on the test split.
I0209 12:19:55.151450 140039251117888 submission_runner.py:408] Time since start: 12010.12s, 	Step: 24314, 	{'train/accuracy': 0.9922340512275696, 'train/loss': 0.02505871281027794, 'train/mean_average_precision': 0.5520819790954581, 'validation/accuracy': 0.986710250377655, 'validation/loss': 0.04724223539233208, 'validation/mean_average_precision': 0.2548902204472823, 'validation/num_examples': 43793, 'test/accuracy': 0.9858802556991577, 'test/loss': 0.050252288579940796, 'test/mean_average_precision': 0.2530300101575706, 'test/num_examples': 43793, 'score': 7936.522948503494, 'total_duration': 12010.12484574318, 'accumulated_submission_time': 7936.522948503494, 'accumulated_eval_time': 4071.9370653629303, 'accumulated_logging_time': 0.9949030876159668}
I0209 12:19:55.172026 139864233322240 logging_writer.py:48] [24314] accumulated_eval_time=4071.937065, accumulated_logging_time=0.994903, accumulated_submission_time=7936.522949, global_step=24314, preemption_count=0, score=7936.522949, test/accuracy=0.985880, test/loss=0.050252, test/mean_average_precision=0.253030, test/num_examples=43793, total_duration=12010.124846, train/accuracy=0.992234, train/loss=0.025059, train/mean_average_precision=0.552082, validation/accuracy=0.986710, validation/loss=0.047242, validation/mean_average_precision=0.254890, validation/num_examples=43793
I0209 12:20:23.425264 139878416574208 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.03289913386106491, loss=0.035181086510419846
I0209 12:20:56.204604 139864233322240 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.03460921347141266, loss=0.032898515462875366
I0209 12:21:28.559630 139878416574208 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.030449597164988518, loss=0.032547663897275925
I0209 12:22:01.161087 139864233322240 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.030459098517894745, loss=0.032451480627059937
I0209 12:22:33.691759 139878416574208 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.03537406399846077, loss=0.03410426899790764
I0209 12:23:05.830480 139864233322240 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.03434177488088608, loss=0.03188201040029526
I0209 12:23:38.322704 139878416574208 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.0347033254802227, loss=0.03285133093595505
I0209 12:23:55.444831 140039251117888 spec.py:321] Evaluating on the training split.
I0209 12:25:45.898738 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 12:25:49.008223 140039251117888 spec.py:349] Evaluating on the test split.
I0209 12:25:52.146162 140039251117888 submission_runner.py:408] Time since start: 12367.12s, 	Step: 25054, 	{'train/accuracy': 0.9923595190048218, 'train/loss': 0.024475768208503723, 'train/mean_average_precision': 0.5629086457355192, 'validation/accuracy': 0.9867419600486755, 'validation/loss': 0.04716869443655014, 'validation/mean_average_precision': 0.2574200603232365, 'validation/num_examples': 43793, 'test/accuracy': 0.985859215259552, 'test/loss': 0.050281304866075516, 'test/mean_average_precision': 0.25572176194390206, 'test/num_examples': 43793, 'score': 8176.764466285706, 'total_duration': 12367.119560956955, 'accumulated_submission_time': 8176.764466285706, 'accumulated_eval_time': 4188.638366937637, 'accumulated_logging_time': 1.026634693145752}
I0209 12:25:52.166200 139871934551808 logging_writer.py:48] [25054] accumulated_eval_time=4188.638367, accumulated_logging_time=1.026635, accumulated_submission_time=8176.764466, global_step=25054, preemption_count=0, score=8176.764466, test/accuracy=0.985859, test/loss=0.050281, test/mean_average_precision=0.255722, test/num_examples=43793, total_duration=12367.119561, train/accuracy=0.992360, train/loss=0.024476, train/mean_average_precision=0.562909, validation/accuracy=0.986742, validation/loss=0.047169, validation/mean_average_precision=0.257420, validation/num_examples=43793
I0209 12:26:07.817042 139878408181504 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.03616559877991676, loss=0.031499430537223816
I0209 12:26:41.321958 139871934551808 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.03179968148469925, loss=0.031639233231544495
I0209 12:27:14.529749 139878408181504 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.034734636545181274, loss=0.03456827253103256
I0209 12:27:47.749813 139871934551808 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.0370381735265255, loss=0.032603271305561066
I0209 12:28:21.114355 139878408181504 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.05441921949386597, loss=0.034229252487421036
I0209 12:28:54.439086 139871934551808 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.03222696855664253, loss=0.033127572387456894
I0209 12:29:27.258410 139878408181504 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.04754885658621788, loss=0.032606642693281174
I0209 12:29:52.397533 140039251117888 spec.py:321] Evaluating on the training split.
I0209 12:31:47.135992 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 12:31:50.260262 140039251117888 spec.py:349] Evaluating on the test split.
I0209 12:31:53.294456 140039251117888 submission_runner.py:408] Time since start: 12728.27s, 	Step: 25778, 	{'train/accuracy': 0.9928763508796692, 'train/loss': 0.02270345762372017, 'train/mean_average_precision': 0.6162934071137858, 'validation/accuracy': 0.9866648316383362, 'validation/loss': 0.04745422303676605, 'validation/mean_average_precision': 0.2594423493508385, 'validation/num_examples': 43793, 'test/accuracy': 0.9858731031417847, 'test/loss': 0.050558581948280334, 'test/mean_average_precision': 0.2549490936009421, 'test/num_examples': 43793, 'score': 8416.962620973587, 'total_duration': 12728.26784825325, 'accumulated_submission_time': 8416.962620973587, 'accumulated_eval_time': 4309.53524518013, 'accumulated_logging_time': 1.0576236248016357}
I0209 12:31:53.314969 139864233322240 logging_writer.py:48] [25778] accumulated_eval_time=4309.535245, accumulated_logging_time=1.057624, accumulated_submission_time=8416.962621, global_step=25778, preemption_count=0, score=8416.962621, test/accuracy=0.985873, test/loss=0.050559, test/mean_average_precision=0.254949, test/num_examples=43793, total_duration=12728.267848, train/accuracy=0.992876, train/loss=0.022703, train/mean_average_precision=0.616293, validation/accuracy=0.986665, validation/loss=0.047454, validation/mean_average_precision=0.259442, validation/num_examples=43793
I0209 12:32:01.011388 139871926159104 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.048221178352832794, loss=0.03028656356036663
I0209 12:32:33.692186 139864233322240 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.035884108394384384, loss=0.03049893118441105
I0209 12:33:06.704802 139871926159104 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.03550070524215698, loss=0.030607666820287704
I0209 12:33:39.098832 139864233322240 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.035955000668764114, loss=0.03555590286850929
I0209 12:34:11.762415 139871926159104 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.047288257628679276, loss=0.030769625678658485
I0209 12:34:44.453634 139864233322240 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.03953666612505913, loss=0.03353133425116539
I0209 12:35:16.902875 139871926159104 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.035958170890808105, loss=0.03304430842399597
I0209 12:35:49.171660 139864233322240 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.03438892960548401, loss=0.033554863184690475
I0209 12:35:53.619682 140039251117888 spec.py:321] Evaluating on the training split.
I0209 12:37:47.835374 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 12:37:50.884202 140039251117888 spec.py:349] Evaluating on the test split.
I0209 12:37:53.891610 140039251117888 submission_runner.py:408] Time since start: 13088.86s, 	Step: 26515, 	{'train/accuracy': 0.9934829473495483, 'train/loss': 0.021650198847055435, 'train/mean_average_precision': 0.6098429034027366, 'validation/accuracy': 0.9865832328796387, 'validation/loss': 0.04722558334469795, 'validation/mean_average_precision': 0.26031340684549625, 'validation/num_examples': 43793, 'test/accuracy': 0.9857437610626221, 'test/loss': 0.05030810087919235, 'test/mean_average_precision': 0.25004458692146053, 'test/num_examples': 43793, 'score': 8657.234840393066, 'total_duration': 13088.86488366127, 'accumulated_submission_time': 8657.234840393066, 'accumulated_eval_time': 4429.807000875473, 'accumulated_logging_time': 1.0906052589416504}
I0209 12:37:53.911900 139878408181504 logging_writer.py:48] [26515] accumulated_eval_time=4429.807001, accumulated_logging_time=1.090605, accumulated_submission_time=8657.234840, global_step=26515, preemption_count=0, score=8657.234840, test/accuracy=0.985744, test/loss=0.050308, test/mean_average_precision=0.250045, test/num_examples=43793, total_duration=13088.864884, train/accuracy=0.993483, train/loss=0.021650, train/mean_average_precision=0.609843, validation/accuracy=0.986583, validation/loss=0.047226, validation/mean_average_precision=0.260313, validation/num_examples=43793
I0209 12:38:22.132489 139878416574208 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.04305360093712807, loss=0.03275199607014656
I0209 12:38:54.166630 139878408181504 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.05223837494850159, loss=0.03121221624314785
I0209 12:39:26.400357 139878416574208 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.04322018846869469, loss=0.03383909538388252
I0209 12:39:58.593112 139878408181504 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.04220942407846451, loss=0.03127937391400337
I0209 12:40:31.027390 139878416574208 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.03440656512975693, loss=0.030198827385902405
I0209 12:41:03.368112 139878408181504 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.0406847782433033, loss=0.031873103231191635
I0209 12:41:35.647234 139878416574208 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.04837190732359886, loss=0.03188980370759964
I0209 12:41:54.148730 140039251117888 spec.py:321] Evaluating on the training split.
I0209 12:43:49.162157 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 12:43:52.269414 140039251117888 spec.py:349] Evaluating on the test split.
I0209 12:43:55.356533 140039251117888 submission_runner.py:408] Time since start: 13450.33s, 	Step: 27259, 	{'train/accuracy': 0.9931455254554749, 'train/loss': 0.022433340549468994, 'train/mean_average_precision': 0.601588529797614, 'validation/accuracy': 0.986559271812439, 'validation/loss': 0.04756902530789375, 'validation/mean_average_precision': 0.2533269911318941, 'validation/num_examples': 43793, 'test/accuracy': 0.9857968688011169, 'test/loss': 0.050486188381910324, 'test/mean_average_precision': 0.25171143873459717, 'test/num_examples': 43793, 'score': 8897.440607070923, 'total_duration': 13450.329896450043, 'accumulated_submission_time': 8897.440607070923, 'accumulated_eval_time': 4551.0147252082825, 'accumulated_logging_time': 1.12199068069458}
I0209 12:43:55.377908 139871926159104 logging_writer.py:48] [27259] accumulated_eval_time=4551.014725, accumulated_logging_time=1.121991, accumulated_submission_time=8897.440607, global_step=27259, preemption_count=0, score=8897.440607, test/accuracy=0.985797, test/loss=0.050486, test/mean_average_precision=0.251711, test/num_examples=43793, total_duration=13450.329896, train/accuracy=0.993146, train/loss=0.022433, train/mean_average_precision=0.601589, validation/accuracy=0.986559, validation/loss=0.047569, validation/mean_average_precision=0.253327, validation/num_examples=43793
I0209 12:44:09.431284 139871934551808 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.04001610353589058, loss=0.029561722651124
I0209 12:44:42.345758 139871926159104 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.048785027116537094, loss=0.03517627716064453
I0209 12:45:15.406996 139871934551808 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.04311709851026535, loss=0.03133343160152435
I0209 12:45:48.308424 139871926159104 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.03772642835974693, loss=0.029384223744273186
I0209 12:46:21.520177 139871934551808 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.03982365131378174, loss=0.029545051977038383
I0209 12:46:54.405241 139871926159104 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.05034270137548447, loss=0.03144088014960289
I0209 12:47:27.303190 139871934551808 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.03840027377009392, loss=0.032712891697883606
I0209 12:47:55.471318 140039251117888 spec.py:321] Evaluating on the training split.
I0209 12:49:53.212668 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 12:49:56.464468 140039251117888 spec.py:349] Evaluating on the test split.
I0209 12:49:59.661298 140039251117888 submission_runner.py:408] Time since start: 13814.63s, 	Step: 27986, 	{'train/accuracy': 0.99298095703125, 'train/loss': 0.022835660725831985, 'train/mean_average_precision': 0.5933179400988164, 'validation/accuracy': 0.9866059422492981, 'validation/loss': 0.047744277864694595, 'validation/mean_average_precision': 0.2548442086332938, 'validation/num_examples': 43793, 'test/accuracy': 0.9857610464096069, 'test/loss': 0.05074087902903557, 'test/mean_average_precision': 0.2519577155097544, 'test/num_examples': 43793, 'score': 9137.503035783768, 'total_duration': 13814.634695529938, 'accumulated_submission_time': 9137.503035783768, 'accumulated_eval_time': 4675.2046592235565, 'accumulated_logging_time': 1.1547377109527588}
I0209 12:49:59.682206 139864233322240 logging_writer.py:48] [27986] accumulated_eval_time=4675.204659, accumulated_logging_time=1.154738, accumulated_submission_time=9137.503036, global_step=27986, preemption_count=0, score=9137.503036, test/accuracy=0.985761, test/loss=0.050741, test/mean_average_precision=0.251958, test/num_examples=43793, total_duration=13814.634696, train/accuracy=0.992981, train/loss=0.022836, train/mean_average_precision=0.593318, validation/accuracy=0.986606, validation/loss=0.047744, validation/mean_average_precision=0.254844, validation/num_examples=43793
I0209 12:50:04.819066 139878408181504 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.03645254299044609, loss=0.03067062236368656
I0209 12:50:37.729918 139864233322240 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.03735366463661194, loss=0.02894979901611805
I0209 12:51:10.703672 139878408181504 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.04025038704276085, loss=0.029546748846769333
I0209 12:51:43.186971 139864233322240 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.04255295172333717, loss=0.030231164768338203
I0209 12:52:15.779317 139878408181504 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.04180942848324776, loss=0.0317106731235981
I0209 12:52:48.159687 139864233322240 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.048203930258750916, loss=0.03186916187405586
I0209 12:53:21.075631 139878408181504 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.04402870312333107, loss=0.03251242637634277
I0209 12:53:53.918797 139864233322240 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.04547577351331711, loss=0.0335499607026577
I0209 12:53:59.774162 140039251117888 spec.py:321] Evaluating on the training split.
I0209 12:55:48.771670 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 12:55:51.900397 140039251117888 spec.py:349] Evaluating on the test split.
I0209 12:55:54.935267 140039251117888 submission_runner.py:408] Time since start: 14169.91s, 	Step: 28719, 	{'train/accuracy': 0.9927420020103455, 'train/loss': 0.023180410265922546, 'train/mean_average_precision': 0.5969560782400278, 'validation/accuracy': 0.9867175817489624, 'validation/loss': 0.0479126051068306, 'validation/mean_average_precision': 0.2682034494169585, 'validation/num_examples': 43793, 'test/accuracy': 0.9859505891799927, 'test/loss': 0.051033515483140945, 'test/mean_average_precision': 0.25550628832569594, 'test/num_examples': 43793, 'score': 9377.56341791153, 'total_duration': 14169.90866613388, 'accumulated_submission_time': 9377.56341791153, 'accumulated_eval_time': 4790.3657166957855, 'accumulated_logging_time': 1.1872403621673584}
I0209 12:55:54.956553 139871926159104 logging_writer.py:48] [28719] accumulated_eval_time=4790.365717, accumulated_logging_time=1.187240, accumulated_submission_time=9377.563418, global_step=28719, preemption_count=0, score=9377.563418, test/accuracy=0.985951, test/loss=0.051034, test/mean_average_precision=0.255506, test/num_examples=43793, total_duration=14169.908666, train/accuracy=0.992742, train/loss=0.023180, train/mean_average_precision=0.596956, validation/accuracy=0.986718, validation/loss=0.047913, validation/mean_average_precision=0.268203, validation/num_examples=43793
I0209 12:56:21.796687 139878416574208 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.04946442320942879, loss=0.03295741230249405
I0209 12:56:54.507075 139871926159104 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.043013039976358414, loss=0.030124284327030182
I0209 12:57:27.451961 139878416574208 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.03908001258969307, loss=0.031336020678281784
I0209 12:57:59.606293 139871926159104 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.04446594789624214, loss=0.028173748403787613
I0209 12:58:32.385572 139878416574208 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.04652838781476021, loss=0.03103928081691265
I0209 12:59:04.918314 139871926159104 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.04609685391187668, loss=0.03185750171542168
I0209 12:59:37.194359 139878416574208 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.045802053064107895, loss=0.029100440442562103
I0209 12:59:55.088381 140039251117888 spec.py:321] Evaluating on the training split.
I0209 13:01:49.715664 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 13:01:52.765472 140039251117888 spec.py:349] Evaluating on the test split.
I0209 13:01:55.762968 140039251117888 submission_runner.py:408] Time since start: 14530.74s, 	Step: 29456, 	{'train/accuracy': 0.9926162958145142, 'train/loss': 0.02338692918419838, 'train/mean_average_precision': 0.5947517350336357, 'validation/accuracy': 0.9866631627082825, 'validation/loss': 0.04864392429590225, 'validation/mean_average_precision': 0.2574898460922729, 'validation/num_examples': 43793, 'test/accuracy': 0.9856972694396973, 'test/loss': 0.05184772610664368, 'test/mean_average_precision': 0.24875211763305882, 'test/num_examples': 43793, 'score': 9617.663268327713, 'total_duration': 14530.736252069473, 'accumulated_submission_time': 9617.663268327713, 'accumulated_eval_time': 4911.0401475429535, 'accumulated_logging_time': 1.2209062576293945}
I0209 13:01:55.784005 139864233322240 logging_writer.py:48] [29456] accumulated_eval_time=4911.040148, accumulated_logging_time=1.220906, accumulated_submission_time=9617.663268, global_step=29456, preemption_count=0, score=9617.663268, test/accuracy=0.985697, test/loss=0.051848, test/mean_average_precision=0.248752, test/num_examples=43793, total_duration=14530.736252, train/accuracy=0.992616, train/loss=0.023387, train/mean_average_precision=0.594752, validation/accuracy=0.986663, validation/loss=0.048644, validation/mean_average_precision=0.257490, validation/num_examples=43793
I0209 13:02:10.433992 139878408181504 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.039712291210889816, loss=0.030489632859826088
I0209 13:02:42.942708 139864233322240 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.045699309557676315, loss=0.030863212421536446
I0209 13:03:15.584795 139878408181504 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.044322576373815536, loss=0.03138548508286476
I0209 13:03:51.483195 139864233322240 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.046322617679834366, loss=0.03080357052385807
I0209 13:04:23.886410 139878408181504 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.04927578940987587, loss=0.030986452475190163
I0209 13:04:56.550203 139864233322240 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.044773396104574203, loss=0.03018338978290558
I0209 13:05:29.216175 139878408181504 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.047103431075811386, loss=0.02964664436876774
I0209 13:05:56.101372 140039251117888 spec.py:321] Evaluating on the training split.
I0209 13:07:48.241733 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 13:07:51.342731 140039251117888 spec.py:349] Evaluating on the test split.
I0209 13:07:54.369611 140039251117888 submission_runner.py:408] Time since start: 14889.34s, 	Step: 30183, 	{'train/accuracy': 0.9931610822677612, 'train/loss': 0.022001130506396294, 'train/mean_average_precision': 0.6025305047501668, 'validation/accuracy': 0.986594557762146, 'validation/loss': 0.048090916126966476, 'validation/mean_average_precision': 0.25966516472182116, 'validation/num_examples': 43793, 'test/accuracy': 0.9857484102249146, 'test/loss': 0.051175639033317566, 'test/mean_average_precision': 0.2540606307889602, 'test/num_examples': 43793, 'score': 9857.94836807251, 'total_duration': 14889.342999219894, 'accumulated_submission_time': 9857.94836807251, 'accumulated_eval_time': 5029.308331251144, 'accumulated_logging_time': 1.2543549537658691}
I0209 13:07:54.390866 139871934551808 logging_writer.py:48] [30183] accumulated_eval_time=5029.308331, accumulated_logging_time=1.254355, accumulated_submission_time=9857.948368, global_step=30183, preemption_count=0, score=9857.948368, test/accuracy=0.985748, test/loss=0.051176, test/mean_average_precision=0.254061, test/num_examples=43793, total_duration=14889.342999, train/accuracy=0.993161, train/loss=0.022001, train/mean_average_precision=0.602531, validation/accuracy=0.986595, validation/loss=0.048091, validation/mean_average_precision=0.259665, validation/num_examples=43793
I0209 13:08:00.259099 139878416574208 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.06534481793642044, loss=0.030238905921578407
I0209 13:08:33.257198 139871934551808 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.05227132514119148, loss=0.03164192661643028
I0209 13:09:06.292881 139878416574208 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.04024244099855423, loss=0.031995438039302826
I0209 13:09:38.937987 139871934551808 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.04322120547294617, loss=0.032213255763053894
I0209 13:10:11.845513 139878416574208 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.05044693499803543, loss=0.029360691085457802
I0209 13:10:44.904556 139871934551808 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.0500531829893589, loss=0.03024718537926674
I0209 13:11:17.901081 139878416574208 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.05053582787513733, loss=0.030689522624015808
I0209 13:11:50.654376 139871934551808 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.04826834052801132, loss=0.031190654262900352
I0209 13:11:54.598086 140039251117888 spec.py:321] Evaluating on the training split.
I0209 13:13:47.828158 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 13:13:50.927582 140039251117888 spec.py:349] Evaluating on the test split.
I0209 13:13:53.989518 140039251117888 submission_runner.py:408] Time since start: 15248.96s, 	Step: 30913, 	{'train/accuracy': 0.9933783411979675, 'train/loss': 0.02104005217552185, 'train/mean_average_precision': 0.6411639773982251, 'validation/accuracy': 0.9866043329238892, 'validation/loss': 0.048770561814308167, 'validation/mean_average_precision': 0.25318820335208087, 'validation/num_examples': 43793, 'test/accuracy': 0.9857787489891052, 'test/loss': 0.05177788436412811, 'test/mean_average_precision': 0.25129154224252925, 'test/num_examples': 43793, 'score': 10098.119551181793, 'total_duration': 15248.962916851044, 'accumulated_submission_time': 10098.119551181793, 'accumulated_eval_time': 5148.699724435806, 'accumulated_logging_time': 1.288421869277954}
I0209 13:13:54.011363 139864233322240 logging_writer.py:48] [30913] accumulated_eval_time=5148.699724, accumulated_logging_time=1.288422, accumulated_submission_time=10098.119551, global_step=30913, preemption_count=0, score=10098.119551, test/accuracy=0.985779, test/loss=0.051778, test/mean_average_precision=0.251292, test/num_examples=43793, total_duration=15248.962917, train/accuracy=0.993378, train/loss=0.021040, train/mean_average_precision=0.641164, validation/accuracy=0.986604, validation/loss=0.048771, validation/mean_average_precision=0.253188, validation/num_examples=43793
I0209 13:14:22.691108 139871926159104 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.044359710067510605, loss=0.028622403740882874
I0209 13:14:55.075727 139864233322240 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.05419313907623291, loss=0.03182130306959152
I0209 13:15:27.784779 139871926159104 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.04791131243109703, loss=0.02811531163752079
I0209 13:16:00.514003 139864233322240 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.048549629747867584, loss=0.03089870512485504
I0209 13:16:33.705235 139871926159104 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.047011155635118484, loss=0.03018590249121189
I0209 13:17:07.203740 139864233322240 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.04681265354156494, loss=0.030095838010311127
I0209 13:17:39.755861 139871926159104 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.04912670701742172, loss=0.029950080439448357
I0209 13:17:54.243992 140039251117888 spec.py:321] Evaluating on the training split.
I0209 13:19:49.612235 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 13:19:52.673706 140039251117888 spec.py:349] Evaluating on the test split.
I0209 13:19:55.761186 140039251117888 submission_runner.py:408] Time since start: 15610.73s, 	Step: 31645, 	{'train/accuracy': 0.993828535079956, 'train/loss': 0.01970619522035122, 'train/mean_average_precision': 0.6617192203318676, 'validation/accuracy': 0.9866339564323425, 'validation/loss': 0.04894295334815979, 'validation/mean_average_precision': 0.25527283874613615, 'validation/num_examples': 43793, 'test/accuracy': 0.9857193827629089, 'test/loss': 0.0524006262421608, 'test/mean_average_precision': 0.24764914519659614, 'test/num_examples': 43793, 'score': 10338.317598104477, 'total_duration': 15610.73458480835, 'accumulated_submission_time': 10338.317598104477, 'accumulated_eval_time': 5270.216884851456, 'accumulated_logging_time': 1.32468581199646}
I0209 13:19:55.783446 139878408181504 logging_writer.py:48] [31645] accumulated_eval_time=5270.216885, accumulated_logging_time=1.324686, accumulated_submission_time=10338.317598, global_step=31645, preemption_count=0, score=10338.317598, test/accuracy=0.985719, test/loss=0.052401, test/mean_average_precision=0.247649, test/num_examples=43793, total_duration=15610.734585, train/accuracy=0.993829, train/loss=0.019706, train/mean_average_precision=0.661719, validation/accuracy=0.986634, validation/loss=0.048943, validation/mean_average_precision=0.255273, validation/num_examples=43793
I0209 13:20:14.311684 139878416574208 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.04989805445075035, loss=0.02929355390369892
I0209 13:20:47.259995 139878408181504 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.05110162869095802, loss=0.028456488624215126
I0209 13:21:20.231301 139878416574208 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.0549188107252121, loss=0.028969133272767067
I0209 13:21:53.053954 139878408181504 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.0521460697054863, loss=0.02968459017574787
I0209 13:22:26.020861 139878416574208 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.048233602195978165, loss=0.02827279083430767
I0209 13:22:58.716249 139878408181504 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.05916657671332359, loss=0.030483609065413475
I0209 13:23:31.288961 139878416574208 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.05314129218459129, loss=0.03020760789513588
I0209 13:23:55.765173 140039251117888 spec.py:321] Evaluating on the training split.
I0209 13:25:48.876196 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 13:25:51.973832 140039251117888 spec.py:349] Evaluating on the test split.
I0209 13:25:55.094939 140039251117888 submission_runner.py:408] Time since start: 15970.07s, 	Step: 32376, 	{'train/accuracy': 0.994285523891449, 'train/loss': 0.018733149394392967, 'train/mean_average_precision': 0.6851187612466829, 'validation/accuracy': 0.986394464969635, 'validation/loss': 0.049223363399505615, 'validation/mean_average_precision': 0.25345441478042446, 'validation/num_examples': 43793, 'test/accuracy': 0.9855828881263733, 'test/loss': 0.05239072069525719, 'test/mean_average_precision': 0.24381363952828328, 'test/num_examples': 43793, 'score': 10578.267916440964, 'total_duration': 15970.068334579468, 'accumulated_submission_time': 10578.267916440964, 'accumulated_eval_time': 5389.546606063843, 'accumulated_logging_time': 1.3582172393798828}
I0209 13:25:55.117562 139871926159104 logging_writer.py:48] [32376] accumulated_eval_time=5389.546606, accumulated_logging_time=1.358217, accumulated_submission_time=10578.267916, global_step=32376, preemption_count=0, score=10578.267916, test/accuracy=0.985583, test/loss=0.052391, test/mean_average_precision=0.243814, test/num_examples=43793, total_duration=15970.068335, train/accuracy=0.994286, train/loss=0.018733, train/mean_average_precision=0.685119, validation/accuracy=0.986394, validation/loss=0.049223, validation/mean_average_precision=0.253454, validation/num_examples=43793
I0209 13:26:03.503664 139871934551808 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.05096451938152313, loss=0.028999872505664825
I0209 13:26:36.181683 139871926159104 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.06068575754761696, loss=0.02930273301899433
I0209 13:27:09.362693 139871934551808 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.05285290256142616, loss=0.029630864039063454
I0209 13:27:42.013091 139871926159104 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.0549769327044487, loss=0.03128810599446297
I0209 13:28:15.017615 139871934551808 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.07480683922767639, loss=0.029961928725242615
I0209 13:28:47.912444 139871926159104 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.05331842228770256, loss=0.02902323380112648
I0209 13:29:21.054694 139871934551808 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.05686822906136513, loss=0.029779765754938126
I0209 13:29:54.485872 139871926159104 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.05624272674322128, loss=0.029778851196169853
I0209 13:29:55.166064 140039251117888 spec.py:321] Evaluating on the training split.
I0209 13:31:47.749293 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 13:31:50.836735 140039251117888 spec.py:349] Evaluating on the test split.
I0209 13:31:53.864517 140039251117888 submission_runner.py:408] Time since start: 16328.84s, 	Step: 33103, 	{'train/accuracy': 0.9938401579856873, 'train/loss': 0.019870581105351448, 'train/mean_average_precision': 0.6640449668679048, 'validation/accuracy': 0.9866132736206055, 'validation/loss': 0.04941275343298912, 'validation/mean_average_precision': 0.24770945202859193, 'validation/num_examples': 43793, 'test/accuracy': 0.9857964515686035, 'test/loss': 0.052649471908807755, 'test/mean_average_precision': 0.24871137662689824, 'test/num_examples': 43793, 'score': 10818.28539800644, 'total_duration': 16328.837913751602, 'accumulated_submission_time': 10818.28539800644, 'accumulated_eval_time': 5508.245011806488, 'accumulated_logging_time': 1.3919477462768555}
I0209 13:31:53.887549 139864233322240 logging_writer.py:48] [33103] accumulated_eval_time=5508.245012, accumulated_logging_time=1.391948, accumulated_submission_time=10818.285398, global_step=33103, preemption_count=0, score=10818.285398, test/accuracy=0.985796, test/loss=0.052649, test/mean_average_precision=0.248711, test/num_examples=43793, total_duration=16328.837914, train/accuracy=0.993840, train/loss=0.019871, train/mean_average_precision=0.664045, validation/accuracy=0.986613, validation/loss=0.049413, validation/mean_average_precision=0.247709, validation/num_examples=43793
I0209 13:32:25.910233 139878416574208 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.044989973306655884, loss=0.027685878798365593
I0209 13:32:57.847799 139864233322240 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.05274048447608948, loss=0.029622487723827362
I0209 13:33:30.427082 139878416574208 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.055962469428777695, loss=0.029407747089862823
I0209 13:34:02.953064 139864233322240 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.05388926342129707, loss=0.030326545238494873
I0209 13:34:34.947239 139878416574208 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.05821358412504196, loss=0.03024066798388958
I0209 13:35:07.660700 139864233322240 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.06088865548372269, loss=0.029075145721435547
I0209 13:35:39.257271 139878416574208 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.04760753735899925, loss=0.026588618755340576
I0209 13:35:54.111370 140039251117888 spec.py:321] Evaluating on the training split.
I0209 13:37:44.085927 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 13:37:47.077537 140039251117888 spec.py:349] Evaluating on the test split.
I0209 13:37:50.126150 140039251117888 submission_runner.py:408] Time since start: 16685.10s, 	Step: 33847, 	{'train/accuracy': 0.9930763244628906, 'train/loss': 0.021939706057310104, 'train/mean_average_precision': 0.604933073567349, 'validation/accuracy': 0.9864837527275085, 'validation/loss': 0.05017333850264549, 'validation/mean_average_precision': 0.2480326956404477, 'validation/num_examples': 43793, 'test/accuracy': 0.9856372475624084, 'test/loss': 0.05341297388076782, 'test/mean_average_precision': 0.24895635080888956, 'test/num_examples': 43793, 'score': 11058.477831840515, 'total_duration': 16685.099545240402, 'accumulated_submission_time': 11058.477831840515, 'accumulated_eval_time': 5624.259745597839, 'accumulated_logging_time': 1.4259235858917236}
I0209 13:37:50.147525 139871934551808 logging_writer.py:48] [33847] accumulated_eval_time=5624.259746, accumulated_logging_time=1.425924, accumulated_submission_time=11058.477832, global_step=33847, preemption_count=0, score=11058.477832, test/accuracy=0.985637, test/loss=0.053413, test/mean_average_precision=0.248956, test/num_examples=43793, total_duration=16685.099545, train/accuracy=0.993076, train/loss=0.021940, train/mean_average_precision=0.604933, validation/accuracy=0.986484, validation/loss=0.050173, validation/mean_average_precision=0.248033, validation/num_examples=43793
I0209 13:38:07.868153 139878408181504 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.060292549431324005, loss=0.030262740328907967
I0209 13:38:39.996598 139871934551808 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.058362532407045364, loss=0.029276616871356964
I0209 13:39:12.605391 139878408181504 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.06092566251754761, loss=0.03282679617404938
I0209 13:39:44.866575 139871934551808 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.05753582715988159, loss=0.028674086555838585
I0209 13:40:17.443775 139878408181504 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.06902095675468445, loss=0.031170442700386047
I0209 13:40:49.246306 139871934551808 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.06124625727534294, loss=0.03246398642659187
I0209 13:41:21.526737 139878408181504 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.061780985444784164, loss=0.02846456691622734
I0209 13:41:50.193115 140039251117888 spec.py:321] Evaluating on the training split.
I0209 13:43:36.633514 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 13:43:39.771954 140039251117888 spec.py:349] Evaluating on the test split.
I0209 13:43:42.751198 140039251117888 submission_runner.py:408] Time since start: 17037.72s, 	Step: 34589, 	{'train/accuracy': 0.9932107329368591, 'train/loss': 0.021749544888734818, 'train/mean_average_precision': 0.6007094082387573, 'validation/accuracy': 0.9864163994789124, 'validation/loss': 0.04977237060666084, 'validation/mean_average_precision': 0.2514264851650297, 'validation/num_examples': 43793, 'test/accuracy': 0.9856675267219543, 'test/loss': 0.05289927124977112, 'test/mean_average_precision': 0.2486676367310245, 'test/num_examples': 43793, 'score': 11298.49192237854, 'total_duration': 17037.724573135376, 'accumulated_submission_time': 11298.49192237854, 'accumulated_eval_time': 5736.817763805389, 'accumulated_logging_time': 1.4586491584777832}
I0209 13:43:42.773646 139864233322240 logging_writer.py:48] [34589] accumulated_eval_time=5736.817764, accumulated_logging_time=1.458649, accumulated_submission_time=11298.491922, global_step=34589, preemption_count=0, score=11298.491922, test/accuracy=0.985668, test/loss=0.052899, test/mean_average_precision=0.248668, test/num_examples=43793, total_duration=17037.724573, train/accuracy=0.993211, train/loss=0.021750, train/mean_average_precision=0.600709, validation/accuracy=0.986416, validation/loss=0.049772, validation/mean_average_precision=0.251426, validation/num_examples=43793
I0209 13:43:46.686002 139878416574208 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.05389663577079773, loss=0.027708370238542557
I0209 13:44:19.069617 139864233322240 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.05481113865971565, loss=0.02987539768218994
I0209 13:44:51.416568 139878416574208 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.056984126567840576, loss=0.02826288528740406
I0209 13:45:23.840095 139864233322240 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.08654992282390594, loss=0.03078283555805683
I0209 13:45:56.226618 139878416574208 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.05622199550271034, loss=0.02999194711446762
I0209 13:46:28.725524 139864233322240 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.06379572302103043, loss=0.030137227848172188
I0209 13:47:01.008768 139878416574208 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.06512852758169174, loss=0.028535569086670876
I0209 13:47:33.393553 139864233322240 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.06329759955406189, loss=0.029385266825556755
I0209 13:47:43.024339 140039251117888 spec.py:321] Evaluating on the training split.
I0209 13:49:32.282256 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 13:49:35.393512 140039251117888 spec.py:349] Evaluating on the test split.
I0209 13:49:38.400189 140039251117888 submission_runner.py:408] Time since start: 17393.37s, 	Step: 35331, 	{'train/accuracy': 0.993598461151123, 'train/loss': 0.020510371774435043, 'train/mean_average_precision': 0.6334068095016367, 'validation/accuracy': 0.986381471157074, 'validation/loss': 0.05009084194898605, 'validation/mean_average_precision': 0.24640121489804126, 'validation/num_examples': 43793, 'test/accuracy': 0.9855715036392212, 'test/loss': 0.053255315870046616, 'test/mean_average_precision': 0.251005720832414, 'test/num_examples': 43793, 'score': 11538.711616039276, 'total_duration': 17393.373587608337, 'accumulated_submission_time': 11538.711616039276, 'accumulated_eval_time': 5852.193566083908, 'accumulated_logging_time': 1.4922783374786377}
I0209 13:49:38.422088 139871926159104 logging_writer.py:48] [35331] accumulated_eval_time=5852.193566, accumulated_logging_time=1.492278, accumulated_submission_time=11538.711616, global_step=35331, preemption_count=0, score=11538.711616, test/accuracy=0.985572, test/loss=0.053255, test/mean_average_precision=0.251006, test/num_examples=43793, total_duration=17393.373588, train/accuracy=0.993598, train/loss=0.020510, train/mean_average_precision=0.633407, validation/accuracy=0.986381, validation/loss=0.050091, validation/mean_average_precision=0.246401, validation/num_examples=43793
I0209 13:50:01.381569 139871934551808 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.06573814898729324, loss=0.031111128628253937
I0209 13:50:33.420125 139871926159104 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.05869955196976662, loss=0.027934666723012924
I0209 13:51:05.622732 139871934551808 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.0672406554222107, loss=0.028918176889419556
I0209 13:51:37.459647 139871926159104 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.06665581464767456, loss=0.02912737987935543
I0209 13:52:09.687826 139871934551808 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.06516765058040619, loss=0.02902381494641304
I0209 13:52:42.228214 139871926159104 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.05974676460027695, loss=0.029048196971416473
I0209 13:53:14.238733 139871934551808 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.06455419212579727, loss=0.030376432463526726
I0209 13:53:38.488970 140039251117888 spec.py:321] Evaluating on the training split.
I0209 13:55:22.230507 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 13:55:25.331464 140039251117888 spec.py:349] Evaluating on the test split.
I0209 13:55:28.405309 140039251117888 submission_runner.py:408] Time since start: 17743.38s, 	Step: 36077, 	{'train/accuracy': 0.9936655759811401, 'train/loss': 0.019992902874946594, 'train/mean_average_precision': 0.6459082535084215, 'validation/accuracy': 0.9864638447761536, 'validation/loss': 0.05059313774108887, 'validation/mean_average_precision': 0.2457527230082513, 'validation/num_examples': 43793, 'test/accuracy': 0.9856035113334656, 'test/loss': 0.05398542433977127, 'test/mean_average_precision': 0.24352993077669655, 'test/num_examples': 43793, 'score': 11778.746633768082, 'total_duration': 17743.378707647324, 'accumulated_submission_time': 11778.746633768082, 'accumulated_eval_time': 5962.109864473343, 'accumulated_logging_time': 1.5256366729736328}
I0209 13:55:28.427776 139878408181504 logging_writer.py:48] [36077] accumulated_eval_time=5962.109864, accumulated_logging_time=1.525637, accumulated_submission_time=11778.746634, global_step=36077, preemption_count=0, score=11778.746634, test/accuracy=0.985604, test/loss=0.053985, test/mean_average_precision=0.243530, test/num_examples=43793, total_duration=17743.378708, train/accuracy=0.993666, train/loss=0.019993, train/mean_average_precision=0.645908, validation/accuracy=0.986464, validation/loss=0.050593, validation/mean_average_precision=0.245753, validation/num_examples=43793
I0209 13:55:36.682065 139878416574208 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.05709940195083618, loss=0.027061721310019493
I0209 13:56:09.047127 139878408181504 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.06588315963745117, loss=0.027904333546757698
I0209 13:56:41.643246 139878416574208 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.06309819221496582, loss=0.02975483238697052
I0209 13:57:14.436515 139878408181504 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.0607030913233757, loss=0.027274757623672485
I0209 13:57:46.915583 139878416574208 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.06179138645529747, loss=0.030945532023906708
I0209 13:58:19.311603 139878408181504 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.06422026455402374, loss=0.028731491416692734
I0209 13:58:51.486326 139878416574208 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.06611037254333496, loss=0.028068063780665398
I0209 13:59:24.220565 139878408181504 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.07046598196029663, loss=0.028626002371311188
I0209 13:59:28.518631 140039251117888 spec.py:321] Evaluating on the training split.
I0209 14:01:19.647705 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 14:01:22.796657 140039251117888 spec.py:349] Evaluating on the test split.
I0209 14:01:25.891483 140039251117888 submission_runner.py:408] Time since start: 18100.86s, 	Step: 36814, 	{'train/accuracy': 0.9936034679412842, 'train/loss': 0.02002665400505066, 'train/mean_average_precision': 0.6523864222962256, 'validation/accuracy': 0.9863453507423401, 'validation/loss': 0.05084327608346939, 'validation/mean_average_precision': 0.24100806250108817, 'validation/num_examples': 43793, 'test/accuracy': 0.9855087399482727, 'test/loss': 0.05447215214371681, 'test/mean_average_precision': 0.24192518362956272, 'test/num_examples': 43793, 'score': 12018.441509246826, 'total_duration': 18100.86488223076, 'accumulated_submission_time': 12018.441509246826, 'accumulated_eval_time': 6079.482671022415, 'accumulated_logging_time': 1.9244272708892822}
I0209 14:01:25.914147 139864233322240 logging_writer.py:48] [36814] accumulated_eval_time=6079.482671, accumulated_logging_time=1.924427, accumulated_submission_time=12018.441509, global_step=36814, preemption_count=0, score=12018.441509, test/accuracy=0.985509, test/loss=0.054472, test/mean_average_precision=0.241925, test/num_examples=43793, total_duration=18100.864882, train/accuracy=0.993603, train/loss=0.020027, train/mean_average_precision=0.652386, validation/accuracy=0.986345, validation/loss=0.050843, validation/mean_average_precision=0.241008, validation/num_examples=43793
I0209 14:01:54.368624 139871934551808 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.06697221100330353, loss=0.029699120670557022
I0209 14:02:27.073881 139864233322240 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.061282455921173096, loss=0.028208686038851738
I0209 14:02:59.709295 139871934551808 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.06022350862622261, loss=0.028161775320768356
I0209 14:03:32.564150 139864233322240 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.07603932917118073, loss=0.027522271499037743
I0209 14:04:05.295278 139871934551808 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.07256879657506943, loss=0.028489427641034126
I0209 14:04:38.317463 139864233322240 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.06162148341536522, loss=0.026254549622535706
I0209 14:05:10.881850 139871934551808 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.06016286090016365, loss=0.02774978056550026
I0209 14:05:26.170558 140039251117888 spec.py:321] Evaluating on the training split.
I0209 14:07:16.019608 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 14:07:19.171705 140039251117888 spec.py:349] Evaluating on the test split.
I0209 14:07:22.173431 140039251117888 submission_runner.py:408] Time since start: 18457.15s, 	Step: 37548, 	{'train/accuracy': 0.9947233200073242, 'train/loss': 0.01713837869465351, 'train/mean_average_precision': 0.7183873072526152, 'validation/accuracy': 0.9864943027496338, 'validation/loss': 0.0512232780456543, 'validation/mean_average_precision': 0.24664180354072704, 'validation/num_examples': 43793, 'test/accuracy': 0.9856827259063721, 'test/loss': 0.05457107350230217, 'test/mean_average_precision': 0.2465838969900322, 'test/num_examples': 43793, 'score': 12258.66541147232, 'total_duration': 18457.14681982994, 'accumulated_submission_time': 12258.66541147232, 'accumulated_eval_time': 6195.485498428345, 'accumulated_logging_time': 1.9596786499023438}
I0209 14:07:22.197262 139871926159104 logging_writer.py:48] [37548] accumulated_eval_time=6195.485498, accumulated_logging_time=1.959679, accumulated_submission_time=12258.665411, global_step=37548, preemption_count=0, score=12258.665411, test/accuracy=0.985683, test/loss=0.054571, test/mean_average_precision=0.246584, test/num_examples=43793, total_duration=18457.146820, train/accuracy=0.994723, train/loss=0.017138, train/mean_average_precision=0.718387, validation/accuracy=0.986494, validation/loss=0.051223, validation/mean_average_precision=0.246642, validation/num_examples=43793
I0209 14:07:39.675707 139878416574208 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.0640520453453064, loss=0.02777775749564171
I0209 14:08:12.450558 139871926159104 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.05685795843601227, loss=0.026663346216082573
I0209 14:08:45.221954 139878416574208 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.06394784152507782, loss=0.029048733413219452
I0209 14:09:17.696138 139871926159104 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.06859481334686279, loss=0.026977455243468285
I0209 14:09:50.411259 139878416574208 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.0644257441163063, loss=0.026841428130865097
I0209 14:10:23.184442 139871926159104 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.0594957210123539, loss=0.02498529478907585
I0209 14:10:55.649812 139878416574208 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.06287559121847153, loss=0.02668042480945587
I0209 14:11:22.235486 140039251117888 spec.py:321] Evaluating on the training split.
I0209 14:13:14.316388 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 14:13:17.657885 140039251117888 spec.py:349] Evaluating on the test split.
I0209 14:13:20.927509 140039251117888 submission_runner.py:408] Time since start: 18815.90s, 	Step: 38282, 	{'train/accuracy': 0.995094358921051, 'train/loss': 0.01605805940926075, 'train/mean_average_precision': 0.7305360156173912, 'validation/accuracy': 0.9864216446876526, 'validation/loss': 0.0519767627120018, 'validation/mean_average_precision': 0.2444132828398841, 'validation/num_examples': 43793, 'test/accuracy': 0.985569417476654, 'test/loss': 0.05545670911669731, 'test/mean_average_precision': 0.24167754008738948, 'test/num_examples': 43793, 'score': 12498.670905351639, 'total_duration': 18815.900892019272, 'accumulated_submission_time': 12498.670905351639, 'accumulated_eval_time': 6314.177472352982, 'accumulated_logging_time': 1.9961557388305664}
I0209 14:13:20.953419 139871934551808 logging_writer.py:48] [38282] accumulated_eval_time=6314.177472, accumulated_logging_time=1.996156, accumulated_submission_time=12498.670905, global_step=38282, preemption_count=0, score=12498.670905, test/accuracy=0.985569, test/loss=0.055457, test/mean_average_precision=0.241678, test/num_examples=43793, total_duration=18815.900892, train/accuracy=0.995094, train/loss=0.016058, train/mean_average_precision=0.730536, validation/accuracy=0.986422, validation/loss=0.051977, validation/mean_average_precision=0.244413, validation/num_examples=43793
I0209 14:13:27.230235 139878408181504 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.08741094917058945, loss=0.02977258525788784
I0209 14:13:59.979320 139871934551808 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.06227636709809303, loss=0.0255136601626873
I0209 14:14:32.526521 139878408181504 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.06432528793811798, loss=0.02766982465982437
I0209 14:15:05.358108 139871934551808 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.05932890623807907, loss=0.025817131623625755
I0209 14:15:37.903348 139878408181504 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.08851854503154755, loss=0.03034452348947525
I0209 14:16:10.133203 139871934551808 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.06890847533941269, loss=0.027492623776197433
I0209 14:16:42.570137 139878408181504 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.06176238879561424, loss=0.027891911566257477
I0209 14:17:14.937652 139871934551808 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.07860530912876129, loss=0.025827283039689064
I0209 14:17:21.069305 140039251117888 spec.py:321] Evaluating on the training split.
I0209 14:19:09.397523 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 14:19:14.340001 140039251117888 spec.py:349] Evaluating on the test split.
I0209 14:19:17.347350 140039251117888 submission_runner.py:408] Time since start: 19172.32s, 	Step: 39020, 	{'train/accuracy': 0.9950499534606934, 'train/loss': 0.01644003763794899, 'train/mean_average_precision': 0.7171123304405793, 'validation/accuracy': 0.9864265322685242, 'validation/loss': 0.05110594257712364, 'validation/mean_average_precision': 0.2482558400172647, 'validation/num_examples': 43793, 'test/accuracy': 0.9855411648750305, 'test/loss': 0.05495946481823921, 'test/mean_average_precision': 0.2413185281618792, 'test/num_examples': 43793, 'score': 12738.752710580826, 'total_duration': 19172.32074737549, 'accumulated_submission_time': 12738.752710580826, 'accumulated_eval_time': 6430.45547246933, 'accumulated_logging_time': 2.035463333129883}
I0209 14:19:17.371402 139864233322240 logging_writer.py:48] [39020] accumulated_eval_time=6430.455472, accumulated_logging_time=2.035463, accumulated_submission_time=12738.752711, global_step=39020, preemption_count=0, score=12738.752711, test/accuracy=0.985541, test/loss=0.054959, test/mean_average_precision=0.241319, test/num_examples=43793, total_duration=19172.320747, train/accuracy=0.995050, train/loss=0.016440, train/mean_average_precision=0.717112, validation/accuracy=0.986427, validation/loss=0.051106, validation/mean_average_precision=0.248256, validation/num_examples=43793
I0209 14:19:44.067690 139878416574208 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.06945105642080307, loss=0.027015697211027145
I0209 14:20:17.513722 139864233322240 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.07316365838050842, loss=0.028241602703928947
I0209 14:20:50.257316 139878416574208 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.06922587007284164, loss=0.02781030721962452
I0209 14:21:22.851100 139864233322240 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.06388846784830093, loss=0.0276283361017704
I0209 14:21:55.957163 139878416574208 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.07327637821435928, loss=0.02744261361658573
I0209 14:22:29.020601 139864233322240 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.0704292580485344, loss=0.028318362310528755
I0209 14:23:01.434115 139878416574208 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.08424622565507889, loss=0.02931748516857624
I0209 14:23:17.467144 140039251117888 spec.py:321] Evaluating on the training split.
I0209 14:25:06.406218 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 14:25:09.459054 140039251117888 spec.py:349] Evaluating on the test split.
I0209 14:25:12.516086 140039251117888 submission_runner.py:408] Time since start: 19527.49s, 	Step: 39750, 	{'train/accuracy': 0.9946119785308838, 'train/loss': 0.017336323857307434, 'train/mean_average_precision': 0.7018877172650734, 'validation/accuracy': 0.9864224791526794, 'validation/loss': 0.051711633801460266, 'validation/mean_average_precision': 0.24540036093258488, 'validation/num_examples': 43793, 'test/accuracy': 0.9855074882507324, 'test/loss': 0.055399827659130096, 'test/mean_average_precision': 0.23960519106040393, 'test/num_examples': 43793, 'score': 12978.81694483757, 'total_duration': 19527.48945236206, 'accumulated_submission_time': 12978.81694483757, 'accumulated_eval_time': 6545.504342556, 'accumulated_logging_time': 2.070361852645874}
I0209 14:25:12.540309 139871934551808 logging_writer.py:48] [39750] accumulated_eval_time=6545.504343, accumulated_logging_time=2.070362, accumulated_submission_time=12978.816945, global_step=39750, preemption_count=0, score=12978.816945, test/accuracy=0.985507, test/loss=0.055400, test/mean_average_precision=0.239605, test/num_examples=43793, total_duration=19527.489452, train/accuracy=0.994612, train/loss=0.017336, train/mean_average_precision=0.701888, validation/accuracy=0.986422, validation/loss=0.051712, validation/mean_average_precision=0.245400, validation/num_examples=43793
I0209 14:25:29.234710 139878408181504 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.06982294470071793, loss=0.028540657833218575
I0209 14:26:02.211286 139871934551808 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.07776056230068207, loss=0.027707241475582123
I0209 14:26:34.765091 139878408181504 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.08073113113641739, loss=0.027706457301974297
I0209 14:27:07.829413 139871934551808 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.07146172970533371, loss=0.025561800226569176
I0209 14:27:40.736133 139878408181504 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.06988200545310974, loss=0.02578870579600334
I0209 14:28:13.627310 139871934551808 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.08253494650125504, loss=0.028242086991667747
I0209 14:28:46.480432 139878408181504 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.0813494622707367, loss=0.029344109818339348
I0209 14:29:12.524296 140039251117888 spec.py:321] Evaluating on the training split.
I0209 14:31:01.882087 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 14:31:04.933682 140039251117888 spec.py:349] Evaluating on the test split.
I0209 14:31:07.961749 140039251117888 submission_runner.py:408] Time since start: 19882.94s, 	Step: 40480, 	{'train/accuracy': 0.9938182830810547, 'train/loss': 0.01904170773923397, 'train/mean_average_precision': 0.6818837267982015, 'validation/accuracy': 0.9863936305046082, 'validation/loss': 0.052733831107616425, 'validation/mean_average_precision': 0.24132601573867477, 'validation/num_examples': 43793, 'test/accuracy': 0.9855066537857056, 'test/loss': 0.05638336390256882, 'test/mean_average_precision': 0.2406186509886538, 'test/num_examples': 43793, 'score': 13218.769835948944, 'total_duration': 19882.935147047043, 'accumulated_submission_time': 13218.769835948944, 'accumulated_eval_time': 6660.941750526428, 'accumulated_logging_time': 2.1060330867767334}
I0209 14:31:07.984783 139864233322240 logging_writer.py:48] [40480] accumulated_eval_time=6660.941751, accumulated_logging_time=2.106033, accumulated_submission_time=13218.769836, global_step=40480, preemption_count=0, score=13218.769836, test/accuracy=0.985507, test/loss=0.056383, test/mean_average_precision=0.240619, test/num_examples=43793, total_duration=19882.935147, train/accuracy=0.993818, train/loss=0.019042, train/mean_average_precision=0.681884, validation/accuracy=0.986394, validation/loss=0.052734, validation/mean_average_precision=0.241326, validation/num_examples=43793
I0209 14:31:14.844938 139878416574208 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.07204121351242065, loss=0.028289860114455223
I0209 14:31:47.285761 139864233322240 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.08633578568696976, loss=0.027753200381994247
I0209 14:32:19.957386 139878416574208 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.07816042006015778, loss=0.02896028943359852
I0209 14:32:52.338290 139864233322240 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.08527272194623947, loss=0.027025723829865456
I0209 14:33:24.460594 139878416574208 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.09803873300552368, loss=0.028086522594094276
I0209 14:33:56.770250 139864233322240 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.07770083099603653, loss=0.028778601437807083
I0209 14:34:29.604354 139878416574208 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.0656469389796257, loss=0.02628878690302372
I0209 14:35:02.588232 139864233322240 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.0759049654006958, loss=0.02690846472978592
I0209 14:35:08.019659 140039251117888 spec.py:321] Evaluating on the training split.
I0209 14:36:51.655304 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 14:36:54.820008 140039251117888 spec.py:349] Evaluating on the test split.
I0209 14:36:57.896553 140039251117888 submission_runner.py:408] Time since start: 20232.87s, 	Step: 41218, 	{'train/accuracy': 0.9941673874855042, 'train/loss': 0.018260326236486435, 'train/mean_average_precision': 0.6868479320912788, 'validation/accuracy': 0.986441969871521, 'validation/loss': 0.052412815392017365, 'validation/mean_average_precision': 0.24501272741587676, 'validation/num_examples': 43793, 'test/accuracy': 0.9854856133460999, 'test/loss': 0.05603795498609543, 'test/mean_average_precision': 0.23285328554885704, 'test/num_examples': 43793, 'score': 13458.771996974945, 'total_duration': 20232.86994457245, 'accumulated_submission_time': 13458.771996974945, 'accumulated_eval_time': 6770.818592071533, 'accumulated_logging_time': 2.141692876815796}
I0209 14:36:57.920202 139871926159104 logging_writer.py:48] [41218] accumulated_eval_time=6770.818592, accumulated_logging_time=2.141693, accumulated_submission_time=13458.771997, global_step=41218, preemption_count=0, score=13458.771997, test/accuracy=0.985486, test/loss=0.056038, test/mean_average_precision=0.232853, test/num_examples=43793, total_duration=20232.869945, train/accuracy=0.994167, train/loss=0.018260, train/mean_average_precision=0.686848, validation/accuracy=0.986442, validation/loss=0.052413, validation/mean_average_precision=0.245013, validation/num_examples=43793
I0209 14:37:25.232980 139878408181504 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.08041580766439438, loss=0.02766350284218788
I0209 14:37:58.035282 139871926159104 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.06495970487594604, loss=0.025739125907421112
I0209 14:38:30.557554 139878408181504 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.07573091238737106, loss=0.02627602592110634
I0209 14:39:02.902620 139871926159104 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.06817162781953812, loss=0.025427717715501785
I0209 14:39:35.107200 139878408181504 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.0679640918970108, loss=0.025722824037075043
I0209 14:40:07.460947 139871926159104 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.08099882304668427, loss=0.02714407630264759
I0209 14:40:39.944948 139878408181504 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.0768437534570694, loss=0.02694108709692955
I0209 14:40:57.904489 140039251117888 spec.py:321] Evaluating on the training split.
I0209 14:42:46.995595 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 14:42:50.085686 140039251117888 spec.py:349] Evaluating on the test split.
I0209 14:42:53.117448 140039251117888 submission_runner.py:408] Time since start: 20588.09s, 	Step: 41956, 	{'train/accuracy': 0.9942653775215149, 'train/loss': 0.01780596934258938, 'train/mean_average_precision': 0.6941094896473745, 'validation/accuracy': 0.9863846898078918, 'validation/loss': 0.05340949445962906, 'validation/mean_average_precision': 0.2359936574011092, 'validation/num_examples': 43793, 'test/accuracy': 0.9855399131774902, 'test/loss': 0.05672483146190643, 'test/mean_average_precision': 0.2337525892945866, 'test/num_examples': 43793, 'score': 13698.72559428215, 'total_duration': 20588.090848207474, 'accumulated_submission_time': 13698.72559428215, 'accumulated_eval_time': 6886.031506538391, 'accumulated_logging_time': 2.176521062850952}
I0209 14:42:53.141229 139864233322240 logging_writer.py:48] [41956] accumulated_eval_time=6886.031507, accumulated_logging_time=2.176521, accumulated_submission_time=13698.725594, global_step=41956, preemption_count=0, score=13698.725594, test/accuracy=0.985540, test/loss=0.056725, test/mean_average_precision=0.233753, test/num_examples=43793, total_duration=20588.090848, train/accuracy=0.994265, train/loss=0.017806, train/mean_average_precision=0.694109, validation/accuracy=0.986385, validation/loss=0.053409, validation/mean_average_precision=0.235994, validation/num_examples=43793
I0209 14:43:08.006074 139871934551808 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.08490180224180222, loss=0.026165224611759186
I0209 14:43:40.402249 139864233322240 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.08859459310770035, loss=0.026723116636276245
I0209 14:44:13.070407 139871934551808 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.08637159317731857, loss=0.02869345061480999
I0209 14:44:45.775424 139864233322240 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.06793119758367538, loss=0.025883976370096207
I0209 14:45:17.962909 139871934551808 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.09346772730350494, loss=0.02933022379875183
I0209 14:45:50.083628 139864233322240 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.08228185027837753, loss=0.02616150490939617
I0209 14:46:22.361807 139871934551808 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.09097318351268768, loss=0.025490393862128258
I0209 14:46:53.379554 140039251117888 spec.py:321] Evaluating on the training split.
I0209 14:48:37.157300 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 14:48:40.135265 140039251117888 spec.py:349] Evaluating on the test split.
I0209 14:48:43.077950 140039251117888 submission_runner.py:408] Time since start: 20938.05s, 	Step: 42698, 	{'train/accuracy': 0.9948505759239197, 'train/loss': 0.016491945832967758, 'train/mean_average_precision': 0.7170254342784487, 'validation/accuracy': 0.9862925410270691, 'validation/loss': 0.052877672016620636, 'validation/mean_average_precision': 0.2373408264054241, 'validation/num_examples': 43793, 'test/accuracy': 0.9854699969291687, 'test/loss': 0.05652901530265808, 'test/mean_average_precision': 0.23399929332888234, 'test/num_examples': 43793, 'score': 13938.932447195053, 'total_duration': 20938.051344156265, 'accumulated_submission_time': 13938.932447195053, 'accumulated_eval_time': 6995.729851484299, 'accumulated_logging_time': 2.2115795612335205}
I0209 14:48:43.104393 139871926159104 logging_writer.py:48] [42698] accumulated_eval_time=6995.729851, accumulated_logging_time=2.211580, accumulated_submission_time=13938.932447, global_step=42698, preemption_count=0, score=13938.932447, test/accuracy=0.985470, test/loss=0.056529, test/mean_average_precision=0.233999, test/num_examples=43793, total_duration=20938.051344, train/accuracy=0.994851, train/loss=0.016492, train/mean_average_precision=0.717025, validation/accuracy=0.986293, validation/loss=0.052878, validation/mean_average_precision=0.237341, validation/num_examples=43793
I0209 14:48:44.163007 139878416574208 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.0789622887969017, loss=0.02487528882920742
I0209 14:49:16.253292 139871926159104 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.07601268589496613, loss=0.02617662586271763
I0209 14:49:48.180924 139878416574208 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.0698721706867218, loss=0.024281971156597137
I0209 14:50:19.861150 139871926159104 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.07776408642530441, loss=0.027636338025331497
I0209 14:50:51.693422 139878416574208 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.08215396851301193, loss=0.027394399046897888
I0209 14:51:23.882310 139871926159104 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.0776948407292366, loss=0.02601718343794346
I0209 14:51:56.440264 139878416574208 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.07761933654546738, loss=0.02415781281888485
I0209 14:52:28.555185 139871926159104 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.07776691019535065, loss=0.0253023449331522
I0209 14:52:43.222168 140039251117888 spec.py:321] Evaluating on the training split.
I0209 14:54:29.542691 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 14:54:32.569969 140039251117888 spec.py:349] Evaluating on the test split.
I0209 14:54:35.535727 140039251117888 submission_runner.py:408] Time since start: 21290.51s, 	Step: 43447, 	{'train/accuracy': 0.9954681992530823, 'train/loss': 0.014992156066000462, 'train/mean_average_precision': 0.7636412831561692, 'validation/accuracy': 0.9862580299377441, 'validation/loss': 0.05348847061395645, 'validation/mean_average_precision': 0.2398435653849362, 'validation/num_examples': 43793, 'test/accuracy': 0.985352098941803, 'test/loss': 0.05716365948319435, 'test/mean_average_precision': 0.23213330155006406, 'test/num_examples': 43793, 'score': 14179.018709421158, 'total_duration': 21290.509116888046, 'accumulated_submission_time': 14179.018709421158, 'accumulated_eval_time': 7108.043355226517, 'accumulated_logging_time': 2.2498536109924316}
I0209 14:54:35.559537 139864233322240 logging_writer.py:48] [43447] accumulated_eval_time=7108.043355, accumulated_logging_time=2.249854, accumulated_submission_time=14179.018709, global_step=43447, preemption_count=0, score=14179.018709, test/accuracy=0.985352, test/loss=0.057164, test/mean_average_precision=0.232133, test/num_examples=43793, total_duration=21290.509117, train/accuracy=0.995468, train/loss=0.014992, train/mean_average_precision=0.763641, validation/accuracy=0.986258, validation/loss=0.053488, validation/mean_average_precision=0.239844, validation/num_examples=43793
I0209 14:54:52.843365 139878408181504 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.08310526609420776, loss=0.025857144966721535
I0209 14:55:24.851345 139864233322240 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.08840734511613846, loss=0.02698587067425251
I0209 14:55:56.954885 139878408181504 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.08225991576910019, loss=0.02654021978378296
I0209 14:56:28.926335 139864233322240 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.08374699205160141, loss=0.02564261294901371
I0209 14:57:00.758466 139878408181504 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.07450857758522034, loss=0.027008865028619766
I0209 14:57:32.694370 139864233322240 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.06956341862678528, loss=0.025189397856593132
I0209 14:58:04.811517 139878408181504 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.07960502803325653, loss=0.02634207159280777
I0209 14:58:35.566235 140039251117888 spec.py:321] Evaluating on the training split.
I0209 15:00:19.706109 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 15:00:24.545809 140039251117888 spec.py:349] Evaluating on the test split.
I0209 15:00:27.574093 140039251117888 submission_runner.py:408] Time since start: 21642.55s, 	Step: 44197, 	{'train/accuracy': 0.995785117149353, 'train/loss': 0.01409078948199749, 'train/mean_average_precision': 0.7848275030564575, 'validation/accuracy': 0.9862913489341736, 'validation/loss': 0.054559968411922455, 'validation/mean_average_precision': 0.2348540301396725, 'validation/num_examples': 43793, 'test/accuracy': 0.9854013323783875, 'test/loss': 0.05836547166109085, 'test/mean_average_precision': 0.23273799650880028, 'test/num_examples': 43793, 'score': 14418.994359016418, 'total_duration': 21642.547493696213, 'accumulated_submission_time': 14418.994359016418, 'accumulated_eval_time': 7220.051170825958, 'accumulated_logging_time': 2.2852609157562256}
I0209 15:00:27.598371 139871926159104 logging_writer.py:48] [44197] accumulated_eval_time=7220.051171, accumulated_logging_time=2.285261, accumulated_submission_time=14418.994359, global_step=44197, preemption_count=0, score=14418.994359, test/accuracy=0.985401, test/loss=0.058365, test/mean_average_precision=0.232738, test/num_examples=43793, total_duration=21642.547494, train/accuracy=0.995785, train/loss=0.014091, train/mean_average_precision=0.784828, validation/accuracy=0.986291, validation/loss=0.054560, validation/mean_average_precision=0.234854, validation/num_examples=43793
I0209 15:00:28.905576 139878416574208 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.08408944308757782, loss=0.026824478060007095
I0209 15:01:00.716553 139871926159104 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.10449068993330002, loss=0.027576779946684837
I0209 15:01:32.706578 139878416574208 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.07792194187641144, loss=0.025194618850946426
I0209 15:02:04.690214 139871926159104 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.0814921110868454, loss=0.025632990524172783
I0209 15:02:36.989297 139878416574208 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.08715156465768814, loss=0.026593992486596107
I0209 15:03:09.713927 139871926159104 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.09255664795637131, loss=0.02594722993671894
I0209 15:03:41.630262 139878416574208 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.08264235407114029, loss=0.027594946324825287
I0209 15:04:14.418194 139871926159104 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.08644091337919235, loss=0.025944029912352562
I0209 15:04:27.824624 140039251117888 spec.py:321] Evaluating on the training split.
I0209 15:06:14.454749 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 15:06:17.513590 140039251117888 spec.py:349] Evaluating on the test split.
I0209 15:06:20.497077 140039251117888 submission_runner.py:408] Time since start: 21995.47s, 	Step: 44943, 	{'train/accuracy': 0.9962485432624817, 'train/loss': 0.012892408296465874, 'train/mean_average_precision': 0.8084429781038803, 'validation/accuracy': 0.9863153100013733, 'validation/loss': 0.05484171584248543, 'validation/mean_average_precision': 0.2340596465030607, 'validation/num_examples': 43793, 'test/accuracy': 0.9854552745819092, 'test/loss': 0.05840647220611572, 'test/mean_average_precision': 0.2363809785728483, 'test/num_examples': 43793, 'score': 14659.189492940903, 'total_duration': 21995.470474243164, 'accumulated_submission_time': 14659.189492940903, 'accumulated_eval_time': 7332.72357583046, 'accumulated_logging_time': 2.32065486907959}
I0209 15:06:20.521237 139871934551808 logging_writer.py:48] [44943] accumulated_eval_time=7332.723576, accumulated_logging_time=2.320655, accumulated_submission_time=14659.189493, global_step=44943, preemption_count=0, score=14659.189493, test/accuracy=0.985455, test/loss=0.058406, test/mean_average_precision=0.236381, test/num_examples=43793, total_duration=21995.470474, train/accuracy=0.996249, train/loss=0.012892, train/mean_average_precision=0.808443, validation/accuracy=0.986315, validation/loss=0.054842, validation/mean_average_precision=0.234060, validation/num_examples=43793
I0209 15:06:39.295969 139878408181504 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.093935027718544, loss=0.027129102498292923
I0209 15:07:11.683944 139871934551808 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.07359831780195236, loss=0.027070529758930206
I0209 15:07:44.033582 139878408181504 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.08576524257659912, loss=0.026504522189497948
I0209 15:08:16.851041 139871934551808 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.07679405808448792, loss=0.025624671950936317
I0209 15:08:49.166541 139878408181504 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.08270403742790222, loss=0.026676541194319725
I0209 15:09:21.647457 139871934551808 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.08068536967039108, loss=0.026650087907910347
I0209 15:09:53.829993 139878408181504 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.08271144330501556, loss=0.023923415690660477
I0209 15:10:20.508549 140039251117888 spec.py:321] Evaluating on the training split.
I0209 15:12:05.914361 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 15:12:08.936619 140039251117888 spec.py:349] Evaluating on the test split.
I0209 15:12:11.935084 140039251117888 submission_runner.py:408] Time since start: 22346.91s, 	Step: 45683, 	{'train/accuracy': 0.9945888519287109, 'train/loss': 0.01691611483693123, 'train/mean_average_precision': 0.7274850797457694, 'validation/accuracy': 0.9861624836921692, 'validation/loss': 0.05531647801399231, 'validation/mean_average_precision': 0.22963473030953668, 'validation/num_examples': 43793, 'test/accuracy': 0.9852480292320251, 'test/loss': 0.0591854453086853, 'test/mean_average_precision': 0.23166626831947487, 'test/num_examples': 43793, 'score': 14899.14623594284, 'total_duration': 22346.908483743668, 'accumulated_submission_time': 14899.14623594284, 'accumulated_eval_time': 7444.150077819824, 'accumulated_logging_time': 2.3560049533843994}
I0209 15:12:11.959835 139864233322240 logging_writer.py:48] [45683] accumulated_eval_time=7444.150078, accumulated_logging_time=2.356005, accumulated_submission_time=14899.146236, global_step=45683, preemption_count=0, score=14899.146236, test/accuracy=0.985248, test/loss=0.059185, test/mean_average_precision=0.231666, test/num_examples=43793, total_duration=22346.908484, train/accuracy=0.994589, train/loss=0.016916, train/mean_average_precision=0.727485, validation/accuracy=0.986162, validation/loss=0.055316, validation/mean_average_precision=0.229635, validation/num_examples=43793
I0209 15:12:17.707231 139871926159104 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.09962385147809982, loss=0.0253380686044693
I0209 15:12:49.540523 139864233322240 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.07385150343179703, loss=0.025359466671943665
I0209 15:13:22.019034 139871926159104 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.0853591337800026, loss=0.025269778445363045
I0209 15:13:53.927705 139864233322240 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.08003542572259903, loss=0.024946095421910286
I0209 15:14:26.386662 139871926159104 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.08569468557834625, loss=0.024884363636374474
I0209 15:14:58.584205 139864233322240 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.0804857611656189, loss=0.026630617678165436
I0209 15:15:30.990940 139871926159104 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.0830690860748291, loss=0.027090921998023987
I0209 15:16:03.674471 139864233322240 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.08972682803869247, loss=0.02480975352227688
I0209 15:16:12.167061 140039251117888 spec.py:321] Evaluating on the training split.
I0209 15:17:55.006661 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 15:17:58.039246 140039251117888 spec.py:349] Evaluating on the test split.
I0209 15:18:01.033202 140039251117888 submission_runner.py:408] Time since start: 22696.01s, 	Step: 46426, 	{'train/accuracy': 0.9955013990402222, 'train/loss': 0.014552238397300243, 'train/mean_average_precision': 0.7470824289675861, 'validation/accuracy': 0.9862040877342224, 'validation/loss': 0.055199719965457916, 'validation/mean_average_precision': 0.2317819172577487, 'validation/num_examples': 43793, 'test/accuracy': 0.9853516817092896, 'test/loss': 0.058775223791599274, 'test/mean_average_precision': 0.23659981277944173, 'test/num_examples': 43793, 'score': 15139.321603536606, 'total_duration': 22696.00659751892, 'accumulated_submission_time': 15139.321603536606, 'accumulated_eval_time': 7553.0161652565, 'accumulated_logging_time': 2.3929073810577393}
I0209 15:18:01.058349 139871934551808 logging_writer.py:48] [46426] accumulated_eval_time=7553.016165, accumulated_logging_time=2.392907, accumulated_submission_time=15139.321604, global_step=46426, preemption_count=0, score=15139.321604, test/accuracy=0.985352, test/loss=0.058775, test/mean_average_precision=0.236600, test/num_examples=43793, total_duration=22696.006598, train/accuracy=0.995501, train/loss=0.014552, train/mean_average_precision=0.747082, validation/accuracy=0.986204, validation/loss=0.055200, validation/mean_average_precision=0.231782, validation/num_examples=43793
I0209 15:18:25.517448 139878416574208 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.10156901925802231, loss=0.024562960490584373
I0209 15:18:57.953898 139871934551808 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.07776954025030136, loss=0.02419912815093994
I0209 15:19:30.286571 139878416574208 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.08404111117124557, loss=0.0257254708558321
I0209 15:20:02.786310 139871934551808 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.07911280542612076, loss=0.024908164516091347
I0209 15:20:35.120094 139878416574208 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.08941251039505005, loss=0.02537892386317253
I0209 15:21:07.532210 139871934551808 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.08711642771959305, loss=0.023967569693922997
I0209 15:21:41.150352 139878416574208 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.08177022635936737, loss=0.025220882147550583
I0209 15:22:01.171917 140039251117888 spec.py:321] Evaluating on the training split.
I0209 15:23:46.348207 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 15:23:49.404315 140039251117888 spec.py:349] Evaluating on the test split.
I0209 15:23:52.366196 140039251117888 submission_runner.py:408] Time since start: 23047.34s, 	Step: 47163, 	{'train/accuracy': 0.9950583577156067, 'train/loss': 0.015519848093390465, 'train/mean_average_precision': 0.7462739776245799, 'validation/accuracy': 0.9862487316131592, 'validation/loss': 0.0558253675699234, 'validation/mean_average_precision': 0.23322870463136203, 'validation/num_examples': 43793, 'test/accuracy': 0.9853390455245972, 'test/loss': 0.05956575646996498, 'test/mean_average_precision': 0.23422640893819913, 'test/num_examples': 43793, 'score': 15379.40378499031, 'total_duration': 23047.339591503143, 'accumulated_submission_time': 15379.40378499031, 'accumulated_eval_time': 7664.210397481918, 'accumulated_logging_time': 2.4298739433288574}
I0209 15:23:52.391497 139864233322240 logging_writer.py:48] [47163] accumulated_eval_time=7664.210397, accumulated_logging_time=2.429874, accumulated_submission_time=15379.403785, global_step=47163, preemption_count=0, score=15379.403785, test/accuracy=0.985339, test/loss=0.059566, test/mean_average_precision=0.234226, test/num_examples=43793, total_duration=23047.339592, train/accuracy=0.995058, train/loss=0.015520, train/mean_average_precision=0.746274, validation/accuracy=0.986249, validation/loss=0.055825, validation/mean_average_precision=0.233229, validation/num_examples=43793
I0209 15:24:04.814784 139871926159104 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.08744607120752335, loss=0.02527514100074768
I0209 15:24:37.121670 139864233322240 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.09070445597171783, loss=0.024026233702898026
I0209 15:25:09.587268 139871926159104 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.10465174913406372, loss=0.025384491309523582
I0209 15:25:42.077261 139864233322240 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.08330589532852173, loss=0.024796096608042717
I0209 15:26:14.800841 139871926159104 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.09076707810163498, loss=0.023577231913805008
I0209 15:26:47.309851 139864233322240 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.0815059170126915, loss=0.023825664073228836
I0209 15:27:19.935863 139871926159104 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.09770528972148895, loss=0.02545088529586792
I0209 15:27:52.199690 139864233322240 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.10283344238996506, loss=0.023771658539772034
I0209 15:27:52.526114 140039251117888 spec.py:321] Evaluating on the training split.
I0209 15:29:37.749272 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 15:29:40.788880 140039251117888 spec.py:349] Evaluating on the test split.
I0209 15:29:43.741070 140039251117888 submission_runner.py:408] Time since start: 23398.71s, 	Step: 47902, 	{'train/accuracy': 0.9947388172149658, 'train/loss': 0.016186188906431198, 'train/mean_average_precision': 0.7402327222700571, 'validation/accuracy': 0.9861894249916077, 'validation/loss': 0.05635153129696846, 'validation/mean_average_precision': 0.22443474221100657, 'validation/num_examples': 43793, 'test/accuracy': 0.9852008819580078, 'test/loss': 0.060159847140312195, 'test/mean_average_precision': 0.2281059140172945, 'test/num_examples': 43793, 'score': 15619.506531715393, 'total_duration': 23398.714463949203, 'accumulated_submission_time': 15619.506531715393, 'accumulated_eval_time': 7775.425299167633, 'accumulated_logging_time': 2.46665096282959}
I0209 15:29:43.765819 139871934551808 logging_writer.py:48] [47902] accumulated_eval_time=7775.425299, accumulated_logging_time=2.466651, accumulated_submission_time=15619.506532, global_step=47902, preemption_count=0, score=15619.506532, test/accuracy=0.985201, test/loss=0.060160, test/mean_average_precision=0.228106, test/num_examples=43793, total_duration=23398.714464, train/accuracy=0.994739, train/loss=0.016186, train/mean_average_precision=0.740233, validation/accuracy=0.986189, validation/loss=0.056352, validation/mean_average_precision=0.224435, validation/num_examples=43793
I0209 15:30:15.607573 139878416574208 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.11068036407232285, loss=0.02573651634156704
I0209 15:30:47.756478 139871934551808 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.0869232714176178, loss=0.025006990879774094
I0209 15:31:20.160269 139878416574208 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.08405600488185883, loss=0.02555541880428791
I0209 15:31:52.191930 139871934551808 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.08898596465587616, loss=0.025100627914071083
I0209 15:32:24.889333 139878416574208 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.08670007437467575, loss=0.024294622242450714
I0209 15:32:58.319006 139871934551808 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.0896361768245697, loss=0.025108760222792625
I0209 15:33:31.302702 139878416574208 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.0863436609506607, loss=0.022513002157211304
I0209 15:33:43.998657 140039251117888 spec.py:321] Evaluating on the training split.
I0209 15:35:27.748550 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 15:35:30.754032 140039251117888 spec.py:349] Evaluating on the test split.
I0209 15:35:33.723561 140039251117888 submission_runner.py:408] Time since start: 23748.70s, 	Step: 48640, 	{'train/accuracy': 0.9944795966148376, 'train/loss': 0.01678384654223919, 'train/mean_average_precision': 0.7291917979386155, 'validation/accuracy': 0.986224353313446, 'validation/loss': 0.05685719475150108, 'validation/mean_average_precision': 0.2288818870293448, 'validation/num_examples': 43793, 'test/accuracy': 0.9852421283721924, 'test/loss': 0.060674987733364105, 'test/mean_average_precision': 0.2275132350776089, 'test/num_examples': 43793, 'score': 15859.705310821533, 'total_duration': 23748.696949481964, 'accumulated_submission_time': 15859.705310821533, 'accumulated_eval_time': 7885.1501557827, 'accumulated_logging_time': 2.50447678565979}
I0209 15:35:33.749259 139864233322240 logging_writer.py:48] [48640] accumulated_eval_time=7885.150156, accumulated_logging_time=2.504477, accumulated_submission_time=15859.705311, global_step=48640, preemption_count=0, score=15859.705311, test/accuracy=0.985242, test/loss=0.060675, test/mean_average_precision=0.227513, test/num_examples=43793, total_duration=23748.696949, train/accuracy=0.994480, train/loss=0.016784, train/mean_average_precision=0.729192, validation/accuracy=0.986224, validation/loss=0.056857, validation/mean_average_precision=0.228882, validation/num_examples=43793
I0209 15:35:53.323703 139878408181504 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.0892692431807518, loss=0.024050796404480934
I0209 15:36:25.665276 139864233322240 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.09383687376976013, loss=0.023244313895702362
I0209 15:36:57.968699 139878408181504 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.09180546551942825, loss=0.025828786194324493
I0209 15:37:30.121055 139864233322240 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.09439603984355927, loss=0.024871274828910828
I0209 15:38:02.605093 139878408181504 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.08350367099046707, loss=0.024883326143026352
I0209 15:38:34.665277 139864233322240 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.08760891854763031, loss=0.02470443770289421
I0209 15:39:07.334253 139878408181504 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.0926765650510788, loss=0.024023592472076416
I0209 15:39:33.845373 140039251117888 spec.py:321] Evaluating on the training split.
I0209 15:41:21.312550 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 15:41:24.326127 140039251117888 spec.py:349] Evaluating on the test split.
I0209 15:41:27.349451 140039251117888 submission_runner.py:408] Time since start: 24102.32s, 	Step: 49384, 	{'train/accuracy': 0.9957962036132812, 'train/loss': 0.01372190099209547, 'train/mean_average_precision': 0.7867519225622568, 'validation/accuracy': 0.9862629175186157, 'validation/loss': 0.05660553649067879, 'validation/mean_average_precision': 0.2319078980631626, 'validation/num_examples': 43793, 'test/accuracy': 0.9852739572525024, 'test/loss': 0.06066294386982918, 'test/mean_average_precision': 0.22573726235715638, 'test/num_examples': 43793, 'score': 16099.769166469574, 'total_duration': 24102.322848558426, 'accumulated_submission_time': 16099.769166469574, 'accumulated_eval_time': 7998.654189348221, 'accumulated_logging_time': 2.5425143241882324}
I0209 15:41:27.375344 139871926159104 logging_writer.py:48] [49384] accumulated_eval_time=7998.654189, accumulated_logging_time=2.542514, accumulated_submission_time=16099.769166, global_step=49384, preemption_count=0, score=16099.769166, test/accuracy=0.985274, test/loss=0.060663, test/mean_average_precision=0.225737, test/num_examples=43793, total_duration=24102.322849, train/accuracy=0.995796, train/loss=0.013722, train/mean_average_precision=0.786752, validation/accuracy=0.986263, validation/loss=0.056606, validation/mean_average_precision=0.231908, validation/num_examples=43793
I0209 15:41:32.980221 139871934551808 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.099877268075943, loss=0.025376630946993828
I0209 15:42:05.082714 139871926159104 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.08323977142572403, loss=0.02355114184319973
I0209 15:42:37.139341 139871934551808 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.10022225975990295, loss=0.02347872219979763
I0209 15:43:09.634607 139871926159104 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.09337636828422546, loss=0.024850569665431976
I0209 15:43:41.844375 139871934551808 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.08062201738357544, loss=0.023340264335274696
I0209 15:44:14.236633 139871926159104 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.09991592913866043, loss=0.023462163284420967
I0209 15:44:46.430476 139871934551808 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.0819898396730423, loss=0.02120274491608143
I0209 15:45:19.138271 139871926159104 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.0918300673365593, loss=0.024138640612363815
I0209 15:45:27.566162 140039251117888 spec.py:321] Evaluating on the training split.
I0209 15:47:17.467123 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 15:47:20.458643 140039251117888 spec.py:349] Evaluating on the test split.
I0209 15:47:23.441025 140039251117888 submission_runner.py:408] Time since start: 24458.41s, 	Step: 50127, 	{'train/accuracy': 0.996640145778656, 'train/loss': 0.011927308514714241, 'train/mean_average_precision': 0.8215199199748869, 'validation/accuracy': 0.9861634969711304, 'validation/loss': 0.0570623017847538, 'validation/mean_average_precision': 0.23245748095170055, 'validation/num_examples': 43793, 'test/accuracy': 0.9852084517478943, 'test/loss': 0.06094241514801979, 'test/mean_average_precision': 0.22467612600900985, 'test/num_examples': 43793, 'score': 16339.929149627686, 'total_duration': 24458.41442155838, 'accumulated_submission_time': 16339.929149627686, 'accumulated_eval_time': 8114.529004335403, 'accumulated_logging_time': 2.5795295238494873}
I0209 15:47:23.465626 139864233322240 logging_writer.py:48] [50127] accumulated_eval_time=8114.529004, accumulated_logging_time=2.579530, accumulated_submission_time=16339.929150, global_step=50127, preemption_count=0, score=16339.929150, test/accuracy=0.985208, test/loss=0.060942, test/mean_average_precision=0.224676, test/num_examples=43793, total_duration=24458.414422, train/accuracy=0.996640, train/loss=0.011927, train/mean_average_precision=0.821520, validation/accuracy=0.986163, validation/loss=0.057062, validation/mean_average_precision=0.232457, validation/num_examples=43793
I0209 15:47:47.194432 139878416574208 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.09986158460378647, loss=0.024139702320098877
I0209 15:48:19.333373 139864233322240 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.09230963885784149, loss=0.02377016469836235
I0209 15:48:51.361128 139878416574208 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.08727570623159409, loss=0.022732887417078018
I0209 15:49:23.400134 139864233322240 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.09452105313539505, loss=0.02354641631245613
I0209 15:49:55.909124 139878416574208 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.08195619285106659, loss=0.022792795673012733
I0209 15:50:28.027563 139864233322240 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.0971156507730484, loss=0.025654971599578857
I0209 15:51:00.002347 139878416574208 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.08838659524917603, loss=0.024920200929045677
I0209 15:51:23.630490 140039251117888 spec.py:321] Evaluating on the training split.
I0209 15:53:11.050990 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 15:53:14.060847 140039251117888 spec.py:349] Evaluating on the test split.
I0209 15:53:18.853222 140039251117888 submission_runner.py:408] Time since start: 24813.83s, 	Step: 50874, 	{'train/accuracy': 0.9970665574073792, 'train/loss': 0.01100137922912836, 'train/mean_average_precision': 0.8423189663398486, 'validation/accuracy': 0.9861395359039307, 'validation/loss': 0.05757027119398117, 'validation/mean_average_precision': 0.22933294722464312, 'validation/num_examples': 43793, 'test/accuracy': 0.9852164387702942, 'test/loss': 0.061573684215545654, 'test/mean_average_precision': 0.22282484644105666, 'test/num_examples': 43793, 'score': 16580.062649965286, 'total_duration': 24813.826580524445, 'accumulated_submission_time': 16580.062649965286, 'accumulated_eval_time': 8229.751652002335, 'accumulated_logging_time': 2.6154143810272217}
I0209 15:53:18.882381 139871926159104 logging_writer.py:48] [50874] accumulated_eval_time=8229.751652, accumulated_logging_time=2.615414, accumulated_submission_time=16580.062650, global_step=50874, preemption_count=0, score=16580.062650, test/accuracy=0.985216, test/loss=0.061574, test/mean_average_precision=0.222825, test/num_examples=43793, total_duration=24813.826581, train/accuracy=0.997067, train/loss=0.011001, train/mean_average_precision=0.842319, validation/accuracy=0.986140, validation/loss=0.057570, validation/mean_average_precision=0.229333, validation/num_examples=43793
I0209 15:53:27.621147 139871934551808 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.09954037517309189, loss=0.023854197934269905
I0209 15:53:59.853718 139871926159104 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.08456450700759888, loss=0.023244235664606094
I0209 15:54:32.604945 139871934551808 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.08139660954475403, loss=0.023356182500720024
I0209 15:55:04.757428 139871926159104 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.0908413827419281, loss=0.022558311000466347
I0209 15:55:37.028438 139871934551808 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.0786244124174118, loss=0.022953039035201073
I0209 15:56:09.365304 139871926159104 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.08613623678684235, loss=0.02229350432753563
I0209 15:56:41.587769 139871934551808 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.0914999470114708, loss=0.023273054510354996
I0209 15:57:14.011520 139871926159104 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.0799570232629776, loss=0.022437628358602524
I0209 15:57:19.103545 140039251117888 spec.py:321] Evaluating on the training split.
I0209 15:59:06.876228 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 15:59:09.862686 140039251117888 spec.py:349] Evaluating on the test split.
I0209 15:59:12.785279 140039251117888 submission_runner.py:408] Time since start: 25167.76s, 	Step: 51617, 	{'train/accuracy': 0.996950089931488, 'train/loss': 0.011073999106884003, 'train/mean_average_precision': 0.8505485895345607, 'validation/accuracy': 0.986198365688324, 'validation/loss': 0.058026328682899475, 'validation/mean_average_precision': 0.23043124541505816, 'validation/num_examples': 43793, 'test/accuracy': 0.9852501749992371, 'test/loss': 0.062121957540512085, 'test/mean_average_precision': 0.2230208669252613, 'test/num_examples': 43793, 'score': 16820.25114750862, 'total_duration': 25167.758558750153, 'accumulated_submission_time': 16820.25114750862, 'accumulated_eval_time': 8343.433227062225, 'accumulated_logging_time': 2.657003402709961}
I0209 15:59:12.810778 139878408181504 logging_writer.py:48] [51617] accumulated_eval_time=8343.433227, accumulated_logging_time=2.657003, accumulated_submission_time=16820.251148, global_step=51617, preemption_count=0, score=16820.251148, test/accuracy=0.985250, test/loss=0.062122, test/mean_average_precision=0.223021, test/num_examples=43793, total_duration=25167.758559, train/accuracy=0.996950, train/loss=0.011074, train/mean_average_precision=0.850549, validation/accuracy=0.986198, validation/loss=0.058026, validation/mean_average_precision=0.230431, validation/num_examples=43793
I0209 15:59:39.918038 139878416574208 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.09450297802686691, loss=0.024046706035733223
I0209 16:00:12.664530 139878408181504 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.08911408483982086, loss=0.023372240364551544
I0209 16:00:46.425433 139878416574208 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.09278156608343124, loss=0.02399296686053276
I0209 16:01:19.751400 139878408181504 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.08392597734928131, loss=0.02230655401945114
I0209 16:01:51.970703 139878416574208 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.08631579577922821, loss=0.0228660237044096
I0209 16:02:24.197680 139878408181504 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.09765703976154327, loss=0.02338549681007862
I0209 16:02:56.126848 139878416574208 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.0944298505783081, loss=0.023965150117874146
I0209 16:03:12.931323 140039251117888 spec.py:321] Evaluating on the training split.
I0209 16:04:59.261368 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 16:05:02.650316 140039251117888 spec.py:349] Evaluating on the test split.
I0209 16:05:05.841009 140039251117888 submission_runner.py:408] Time since start: 25520.81s, 	Step: 52353, 	{'train/accuracy': 0.9966402053833008, 'train/loss': 0.01182248443365097, 'train/mean_average_precision': 0.8342763710567649, 'validation/accuracy': 0.9861005544662476, 'validation/loss': 0.05785268545150757, 'validation/mean_average_precision': 0.22868603820860048, 'validation/num_examples': 43793, 'test/accuracy': 0.9851081967353821, 'test/loss': 0.06201310455799103, 'test/mean_average_precision': 0.22348948966010676, 'test/num_examples': 43793, 'score': 17060.339695692062, 'total_duration': 25520.81440806389, 'accumulated_submission_time': 17060.339695692062, 'accumulated_eval_time': 8456.342872619629, 'accumulated_logging_time': 2.694946765899658}
I0209 16:05:05.865896 139871926159104 logging_writer.py:48] [52353] accumulated_eval_time=8456.342873, accumulated_logging_time=2.694947, accumulated_submission_time=17060.339696, global_step=52353, preemption_count=0, score=17060.339696, test/accuracy=0.985108, test/loss=0.062013, test/mean_average_precision=0.223489, test/num_examples=43793, total_duration=25520.814408, train/accuracy=0.996640, train/loss=0.011822, train/mean_average_precision=0.834276, validation/accuracy=0.986101, validation/loss=0.057853, validation/mean_average_precision=0.228686, validation/num_examples=43793
I0209 16:05:21.012438 139871934551808 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.08633840084075928, loss=0.02202783152461052
I0209 16:05:52.985695 139871926159104 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.10217678546905518, loss=0.02441454865038395
I0209 16:06:25.012727 139871934551808 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.13075095415115356, loss=0.02448326349258423
I0209 16:06:56.635474 139871926159104 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.08866654336452484, loss=0.023656008765101433
I0209 16:07:28.957997 139871934551808 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.09427154809236526, loss=0.02336948737502098
I0209 16:08:00.789763 139871926159104 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.08849937468767166, loss=0.022056711837649345
I0209 16:08:32.532080 139871934551808 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.08812785148620605, loss=0.02297566458582878
I0209 16:09:04.517778 139871926159104 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.1013323962688446, loss=0.024361787363886833
I0209 16:09:06.067116 140039251117888 spec.py:321] Evaluating on the training split.
I0209 16:10:48.750680 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 16:10:51.846856 140039251117888 spec.py:349] Evaluating on the test split.
I0209 16:10:54.800162 140039251117888 submission_runner.py:408] Time since start: 25869.77s, 	Step: 53106, 	{'train/accuracy': 0.9960552453994751, 'train/loss': 0.012784690596163273, 'train/mean_average_precision': 0.808120967576912, 'validation/accuracy': 0.9861500859260559, 'validation/loss': 0.05887707695364952, 'validation/mean_average_precision': 0.2253980118229107, 'validation/num_examples': 43793, 'test/accuracy': 0.9851238131523132, 'test/loss': 0.06284000724554062, 'test/mean_average_precision': 0.22462510513912176, 'test/num_examples': 43793, 'score': 17300.507427692413, 'total_duration': 25869.773553848267, 'accumulated_submission_time': 17300.507427692413, 'accumulated_eval_time': 8565.075862884521, 'accumulated_logging_time': 2.7332370281219482}
I0209 16:10:54.825917 139864233322240 logging_writer.py:48] [53106] accumulated_eval_time=8565.075863, accumulated_logging_time=2.733237, accumulated_submission_time=17300.507428, global_step=53106, preemption_count=0, score=17300.507428, test/accuracy=0.985124, test/loss=0.062840, test/mean_average_precision=0.224625, test/num_examples=43793, total_duration=25869.773554, train/accuracy=0.996055, train/loss=0.012785, train/mean_average_precision=0.808121, validation/accuracy=0.986150, validation/loss=0.058877, validation/mean_average_precision=0.225398, validation/num_examples=43793
I0209 16:11:25.542874 139878416574208 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.08626336604356766, loss=0.023769043385982513
I0209 16:11:57.949658 139864233322240 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.09970566630363464, loss=0.022402072325348854
I0209 16:12:30.035295 139878416574208 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.0765315517783165, loss=0.02281239442527294
I0209 16:13:03.094944 139864233322240 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.08520396798849106, loss=0.023306330665946007
I0209 16:13:35.405160 139878416574208 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.097030408680439, loss=0.0237431637942791
I0209 16:14:07.695564 139864233322240 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.10048070549964905, loss=0.022110071033239365
I0209 16:14:39.727433 139878416574208 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.09354045242071152, loss=0.02350497432053089
I0209 16:14:55.043654 140039251117888 spec.py:321] Evaluating on the training split.
I0209 16:16:42.561939 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 16:16:45.650186 140039251117888 spec.py:349] Evaluating on the test split.
I0209 16:16:48.754864 140039251117888 submission_runner.py:408] Time since start: 26223.73s, 	Step: 53849, 	{'train/accuracy': 0.9950389266014099, 'train/loss': 0.01518072560429573, 'train/mean_average_precision': 0.755597599761523, 'validation/accuracy': 0.9861545562744141, 'validation/loss': 0.05877898633480072, 'validation/mean_average_precision': 0.22820302883001248, 'validation/num_examples': 43793, 'test/accuracy': 0.9851983189582825, 'test/loss': 0.06279054284095764, 'test/mean_average_precision': 0.22112249594910546, 'test/num_examples': 43793, 'score': 17540.69450187683, 'total_duration': 26223.728258132935, 'accumulated_submission_time': 17540.69450187683, 'accumulated_eval_time': 8678.787027597427, 'accumulated_logging_time': 2.770055055618286}
I0209 16:16:48.781500 139871926159104 logging_writer.py:48] [53849] accumulated_eval_time=8678.787028, accumulated_logging_time=2.770055, accumulated_submission_time=17540.694502, global_step=53849, preemption_count=0, score=17540.694502, test/accuracy=0.985198, test/loss=0.062791, test/mean_average_precision=0.221122, test/num_examples=43793, total_duration=26223.728258, train/accuracy=0.995039, train/loss=0.015181, train/mean_average_precision=0.755598, validation/accuracy=0.986155, validation/loss=0.058779, validation/mean_average_precision=0.228203, validation/num_examples=43793
I0209 16:17:05.789558 139871934551808 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.06835603713989258, loss=0.020168205723166466
I0209 16:17:37.672771 139871926159104 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.08343280106782913, loss=0.022725949063897133
I0209 16:18:09.818469 139871934551808 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.08290520310401917, loss=0.02272755652666092
I0209 16:18:41.688419 139871926159104 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.10042037814855576, loss=0.021911311894655228
I0209 16:19:13.967403 139871934551808 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.08419501781463623, loss=0.023472808301448822
I0209 16:19:46.254952 139871926159104 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.07800377160310745, loss=0.022627200931310654
I0209 16:20:18.512215 139871934551808 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.09975949674844742, loss=0.022475894540548325
I0209 16:20:48.799282 140039251117888 spec.py:321] Evaluating on the training split.
I0209 16:22:32.494394 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 16:22:35.856860 140039251117888 spec.py:349] Evaluating on the test split.
I0209 16:22:39.098207 140039251117888 submission_runner.py:408] Time since start: 26574.07s, 	Step: 54595, 	{'train/accuracy': 0.995265007019043, 'train/loss': 0.014640597626566887, 'train/mean_average_precision': 0.7755410902328488, 'validation/accuracy': 0.9860976934432983, 'validation/loss': 0.0594463013112545, 'validation/mean_average_precision': 0.22632009670592218, 'validation/num_examples': 43793, 'test/accuracy': 0.9851747751235962, 'test/loss': 0.06338262557983398, 'test/mean_average_precision': 0.22023480651676589, 'test/num_examples': 43793, 'score': 17780.679956674576, 'total_duration': 26574.07158923149, 'accumulated_submission_time': 17780.679956674576, 'accumulated_eval_time': 8789.08589553833, 'accumulated_logging_time': 2.8089280128479004}
I0209 16:22:39.128702 139864233322240 logging_writer.py:48] [54595] accumulated_eval_time=8789.085896, accumulated_logging_time=2.808928, accumulated_submission_time=17780.679957, global_step=54595, preemption_count=0, score=17780.679957, test/accuracy=0.985175, test/loss=0.063383, test/mean_average_precision=0.220235, test/num_examples=43793, total_duration=26574.071589, train/accuracy=0.995265, train/loss=0.014641, train/mean_average_precision=0.775541, validation/accuracy=0.986098, validation/loss=0.059446, validation/mean_average_precision=0.226320, validation/num_examples=43793
I0209 16:22:41.157138 139878416574208 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.09500681608915329, loss=0.02292826399207115
I0209 16:23:13.815547 139864233322240 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.07489830255508423, loss=0.022887280210852623
I0209 16:23:46.081889 139878416574208 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.08174396306276321, loss=0.021559348329901695
I0209 16:24:18.140162 139864233322240 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.08819190412759781, loss=0.02156112529337406
I0209 16:24:50.244213 139878416574208 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.09312161803245544, loss=0.02332315780222416
I0209 16:25:22.415714 139864233322240 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.09623140096664429, loss=0.02386498637497425
I0209 16:25:54.656214 139878416574208 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.10067152976989746, loss=0.023130208253860474
I0209 16:26:27.012779 139864233322240 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.09622922539710999, loss=0.022813767194747925
I0209 16:26:39.146555 140039251117888 spec.py:321] Evaluating on the training split.
I0209 16:28:23.302758 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 16:28:26.521003 140039251117888 spec.py:349] Evaluating on the test split.
I0209 16:28:29.677311 140039251117888 submission_runner.py:408] Time since start: 26924.65s, 	Step: 55339, 	{'train/accuracy': 0.9959477186203003, 'train/loss': 0.012901960872113705, 'train/mean_average_precision': 0.8156166730274892, 'validation/accuracy': 0.9861111044883728, 'validation/loss': 0.05996517091989517, 'validation/mean_average_precision': 0.22428007329725988, 'validation/num_examples': 43793, 'test/accuracy': 0.9852118492126465, 'test/loss': 0.06408379971981049, 'test/mean_average_precision': 0.2175502037047203, 'test/num_examples': 43793, 'score': 18020.666786193848, 'total_duration': 26924.650710582733, 'accumulated_submission_time': 18020.666786193848, 'accumulated_eval_time': 8899.616615772247, 'accumulated_logging_time': 2.8510804176330566}
I0209 16:28:29.705597 139871926159104 logging_writer.py:48] [55339] accumulated_eval_time=8899.616616, accumulated_logging_time=2.851080, accumulated_submission_time=18020.666786, global_step=55339, preemption_count=0, score=18020.666786, test/accuracy=0.985212, test/loss=0.064084, test/mean_average_precision=0.217550, test/num_examples=43793, total_duration=26924.650711, train/accuracy=0.995948, train/loss=0.012902, train/mean_average_precision=0.815617, validation/accuracy=0.986111, validation/loss=0.059965, validation/mean_average_precision=0.224280, validation/num_examples=43793
I0209 16:28:49.846845 139878408181504 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.07200422137975693, loss=0.022398073226213455
I0209 16:29:22.097369 139871926159104 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.08134283125400543, loss=0.022543292492628098
I0209 16:29:53.894796 139878408181504 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.08794257044792175, loss=0.023370957002043724
I0209 16:30:25.903057 139871926159104 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.09457651525735855, loss=0.022795891389250755
I0209 16:30:57.964964 139878408181504 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.08533519506454468, loss=0.02204328030347824
I0209 16:31:29.895345 139871926159104 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.09033691138029099, loss=0.02426059916615486
I0209 16:32:02.081928 139878408181504 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.09071414917707443, loss=0.021634308621287346
I0209 16:32:29.762610 140039251117888 spec.py:321] Evaluating on the training split.
I0209 16:34:12.199493 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 16:34:15.240174 140039251117888 spec.py:349] Evaluating on the test split.
I0209 16:34:18.265820 140039251117888 submission_runner.py:408] Time since start: 27273.24s, 	Step: 56087, 	{'train/accuracy': 0.9954010248184204, 'train/loss': 0.014091824181377888, 'train/mean_average_precision': 0.8007883470937304, 'validation/accuracy': 0.9860416650772095, 'validation/loss': 0.06013806164264679, 'validation/mean_average_precision': 0.22281162844515326, 'validation/num_examples': 43793, 'test/accuracy': 0.9850656390190125, 'test/loss': 0.06437241286039352, 'test/mean_average_precision': 0.2164854876489734, 'test/num_examples': 43793, 'score': 18260.690786361694, 'total_duration': 27273.239208221436, 'accumulated_submission_time': 18260.690786361694, 'accumulated_eval_time': 9008.119769573212, 'accumulated_logging_time': 2.8922278881073}
I0209 16:34:18.298349 139871934551808 logging_writer.py:48] [56087] accumulated_eval_time=9008.119770, accumulated_logging_time=2.892228, accumulated_submission_time=18260.690786, global_step=56087, preemption_count=0, score=18260.690786, test/accuracy=0.985066, test/loss=0.064372, test/mean_average_precision=0.216485, test/num_examples=43793, total_duration=27273.239208, train/accuracy=0.995401, train/loss=0.014092, train/mean_average_precision=0.800788, validation/accuracy=0.986042, validation/loss=0.060138, validation/mean_average_precision=0.222812, validation/num_examples=43793
I0209 16:34:22.869323 139878416574208 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.07873112708330154, loss=0.021922552958130836
I0209 16:34:54.916461 139871934551808 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.09758839756250381, loss=0.02247489057481289
I0209 16:35:27.001113 139878416574208 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.10584010183811188, loss=0.021600371226668358
I0209 16:35:59.559802 139871934551808 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.10813330858945847, loss=0.02316986955702305
I0209 16:36:32.075871 139878416574208 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.09282533079385757, loss=0.022408144548535347
I0209 16:37:04.857022 139871934551808 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.09122546017169952, loss=0.022106138989329338
I0209 16:37:36.955755 139878416574208 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.07989266514778137, loss=0.02173580229282379
I0209 16:37:54.697264 139871934551808 logging_writer.py:48] [56756] global_step=56756, preemption_count=0, score=18477.043273
I0209 16:37:54.749734 140039251117888 checkpoints.py:490] Saving checkpoint at step: 56756
I0209 16:37:54.867865 140039251117888 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_3/ogbg_jax/trial_2/checkpoint_56756
I0209 16:37:54.869594 140039251117888 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_3/ogbg_jax/trial_2/checkpoint_56756.
I0209 16:37:55.008453 140039251117888 submission_runner.py:583] Tuning trial 2/5
I0209 16:37:55.008709 140039251117888 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.2, learning_rate=0.0008445074561975979, one_minus_beta1=0.11042418465, beta2=0.9978504782314613, weight_decay=0.08135402759553023, warmup_factor=0.05)
I0209 16:37:55.013172 140039251117888 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5251287221908569, 'train/loss': 0.7151197195053101, 'train/mean_average_precision': 0.024144278710189954, 'validation/accuracy': 0.5213832855224609, 'validation/loss': 0.7166012525558472, 'validation/mean_average_precision': 0.026153954210312285, 'validation/num_examples': 43793, 'test/accuracy': 0.5224818587303162, 'test/loss': 0.7161954641342163, 'test/mean_average_precision': 0.027840261930176607, 'test/num_examples': 43793, 'score': 12.159255981445312, 'total_duration': 133.5333137512207, 'accumulated_submission_time': 12.159255981445312, 'accumulated_eval_time': 121.37401604652405, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (742, {'train/accuracy': 0.9867380261421204, 'train/loss': 0.06415025889873505, 'train/mean_average_precision': 0.04036166806203355, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.07288718968629837, 'validation/mean_average_precision': 0.04209721960526595, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.07578893005847931, 'test/mean_average_precision': 0.04366867466455112, 'test/num_examples': 43793, 'score': 252.4109787940979, 'total_duration': 499.6362552642822, 'accumulated_submission_time': 252.4109787940979, 'accumulated_eval_time': 247.18216228485107, 'accumulated_logging_time': 0.02355480194091797, 'global_step': 742, 'preemption_count': 0}), (1468, {'train/accuracy': 0.9867783784866333, 'train/loss': 0.05364827439188957, 'train/mean_average_precision': 0.04730545077029293, 'validation/accuracy': 0.9841195344924927, 'validation/loss': 0.06393802911043167, 'validation/mean_average_precision': 0.04844746740401821, 'validation/num_examples': 43793, 'test/accuracy': 0.9831437468528748, 'test/loss': 0.06729529052972794, 'test/mean_average_precision': 0.05084376489252518, 'test/num_examples': 43793, 'score': 492.48107290267944, 'total_duration': 863.9817779064178, 'accumulated_submission_time': 492.48107290267944, 'accumulated_eval_time': 371.40698766708374, 'accumulated_logging_time': 0.0522158145904541, 'global_step': 1468, 'preemption_count': 0}), (2213, {'train/accuracy': 0.9873389005661011, 'train/loss': 0.04617956653237343, 'train/mean_average_precision': 0.11931676578964977, 'validation/accuracy': 0.9846541881561279, 'validation/loss': 0.05546187981963158, 'validation/mean_average_precision': 0.11200725265614112, 'validation/num_examples': 43793, 'test/accuracy': 0.9836630821228027, 'test/loss': 0.05856335535645485, 'test/mean_average_precision': 0.11037638421916286, 'test/num_examples': 43793, 'score': 732.6191091537476, 'total_duration': 1223.476276397705, 'accumulated_submission_time': 732.6191091537476, 'accumulated_eval_time': 490.71641182899475, 'accumulated_logging_time': 0.07963228225708008, 'global_step': 2213, 'preemption_count': 0}), (2951, {'train/accuracy': 0.9876325130462646, 'train/loss': 0.04440842196345329, 'train/mean_average_precision': 0.14693599380765723, 'validation/accuracy': 0.984946072101593, 'validation/loss': 0.05403568223118782, 'validation/mean_average_precision': 0.1438901259098251, 'validation/num_examples': 43793, 'test/accuracy': 0.9839431643486023, 'test/loss': 0.05713082477450371, 'test/mean_average_precision': 0.14033399196415447, 'test/num_examples': 43793, 'score': 972.7951905727386, 'total_duration': 1581.7689995765686, 'accumulated_submission_time': 972.7951905727386, 'accumulated_eval_time': 608.784569978714, 'accumulated_logging_time': 0.1080617904663086, 'global_step': 2951, 'preemption_count': 0}), (3695, {'train/accuracy': 0.9878683090209961, 'train/loss': 0.04309086129069328, 'train/mean_average_precision': 0.16932275726861554, 'validation/accuracy': 0.9851047992706299, 'validation/loss': 0.05209706351161003, 'validation/mean_average_precision': 0.1507358351030023, 'validation/num_examples': 43793, 'test/accuracy': 0.9841756820678711, 'test/loss': 0.05486214533448219, 'test/mean_average_precision': 0.15040544105166512, 'test/num_examples': 43793, 'score': 1213.0103611946106, 'total_duration': 1943.883972644806, 'accumulated_submission_time': 1213.0103611946106, 'accumulated_eval_time': 730.6357326507568, 'accumulated_logging_time': 0.13688111305236816, 'global_step': 3695, 'preemption_count': 0}), (4435, {'train/accuracy': 0.9882818460464478, 'train/loss': 0.040857136249542236, 'train/mean_average_precision': 0.2046375427604755, 'validation/accuracy': 0.9853986501693726, 'validation/loss': 0.050100069493055344, 'validation/mean_average_precision': 0.17036476781593696, 'validation/num_examples': 43793, 'test/accuracy': 0.9844208359718323, 'test/loss': 0.052849117666482925, 'test/mean_average_precision': 0.1704869601797195, 'test/num_examples': 43793, 'score': 1453.0346467494965, 'total_duration': 2301.0603160858154, 'accumulated_submission_time': 1453.0346467494965, 'accumulated_eval_time': 847.740592956543, 'accumulated_logging_time': 0.1639246940612793, 'global_step': 4435, 'preemption_count': 0}), (5175, {'train/accuracy': 0.9885432720184326, 'train/loss': 0.039845600724220276, 'train/mean_average_precision': 0.21645493103937904, 'validation/accuracy': 0.9855533242225647, 'validation/loss': 0.04954756423830986, 'validation/mean_average_precision': 0.19147633222763, 'validation/num_examples': 43793, 'test/accuracy': 0.9846398234367371, 'test/loss': 0.05211753025650978, 'test/mean_average_precision': 0.18758637928052935, 'test/num_examples': 43793, 'score': 1693.2447366714478, 'total_duration': 2662.8599441051483, 'accumulated_submission_time': 1693.2447366714478, 'accumulated_eval_time': 969.2806987762451, 'accumulated_logging_time': 0.19262957572937012, 'global_step': 5175, 'preemption_count': 0}), (5912, {'train/accuracy': 0.9885270595550537, 'train/loss': 0.03891624137759209, 'train/mean_average_precision': 0.23800755044098904, 'validation/accuracy': 0.9856272339820862, 'validation/loss': 0.04862495884299278, 'validation/mean_average_precision': 0.19743590536491962, 'validation/num_examples': 43793, 'test/accuracy': 0.9847046732902527, 'test/loss': 0.05137062072753906, 'test/mean_average_precision': 0.19491386996111382, 'test/num_examples': 43793, 'score': 1933.3324942588806, 'total_duration': 3024.9298338890076, 'accumulated_submission_time': 1933.3324942588806, 'accumulated_eval_time': 1091.2149925231934, 'accumulated_logging_time': 0.2206106185913086, 'global_step': 5912, 'preemption_count': 0}), (6651, {'train/accuracy': 0.9885249137878418, 'train/loss': 0.03850976377725601, 'train/mean_average_precision': 0.25270491767745973, 'validation/accuracy': 0.9857376217842102, 'validation/loss': 0.048177871853113174, 'validation/mean_average_precision': 0.20781442148372237, 'validation/num_examples': 43793, 'test/accuracy': 0.9848660230636597, 'test/loss': 0.05098883435130119, 'test/mean_average_precision': 0.2112681620169295, 'test/num_examples': 43793, 'score': 2173.4175686836243, 'total_duration': 3387.38121843338, 'accumulated_submission_time': 2173.4175686836243, 'accumulated_eval_time': 1213.531497001648, 'accumulated_logging_time': 0.2502303123474121, 'global_step': 6651, 'preemption_count': 0}), (7381, {'train/accuracy': 0.9886531829833984, 'train/loss': 0.037700358778238297, 'train/mean_average_precision': 0.29230596323805513, 'validation/accuracy': 0.985855758190155, 'validation/loss': 0.04768254607915878, 'validation/mean_average_precision': 0.22443028666929185, 'validation/num_examples': 43793, 'test/accuracy': 0.9849856495857239, 'test/loss': 0.05026884749531746, 'test/mean_average_precision': 0.2317656739722545, 'test/num_examples': 43793, 'score': 2413.567792892456, 'total_duration': 3745.457129716873, 'accumulated_submission_time': 2413.567792892456, 'accumulated_eval_time': 1331.4031381607056, 'accumulated_logging_time': 0.28160834312438965, 'global_step': 7381, 'preemption_count': 0}), (8111, {'train/accuracy': 0.989067792892456, 'train/loss': 0.036662716418504715, 'train/mean_average_precision': 0.3014880708827807, 'validation/accuracy': 0.9859344959259033, 'validation/loss': 0.047925643622875214, 'validation/mean_average_precision': 0.2225765737842831, 'validation/num_examples': 43793, 'test/accuracy': 0.9850509166717529, 'test/loss': 0.05051816627383232, 'test/mean_average_precision': 0.22876474455407841, 'test/num_examples': 43793, 'score': 2653.6647942066193, 'total_duration': 4106.247630119324, 'accumulated_submission_time': 2653.6647942066193, 'accumulated_eval_time': 1452.041398525238, 'accumulated_logging_time': 0.31385087966918945, 'global_step': 8111, 'preemption_count': 0}), (8847, {'train/accuracy': 0.989511251449585, 'train/loss': 0.03531685471534729, 'train/mean_average_precision': 0.3274415535340602, 'validation/accuracy': 0.9862276315689087, 'validation/loss': 0.046508222818374634, 'validation/mean_average_precision': 0.2328149372357666, 'validation/num_examples': 43793, 'test/accuracy': 0.9853474497795105, 'test/loss': 0.049215056002140045, 'test/mean_average_precision': 0.23299791283990784, 'test/num_examples': 43793, 'score': 2893.8072040081024, 'total_duration': 4464.036288499832, 'accumulated_submission_time': 2893.8072040081024, 'accumulated_eval_time': 1569.6379013061523, 'accumulated_logging_time': 0.34394145011901855, 'global_step': 8847, 'preemption_count': 0}), (9594, {'train/accuracy': 0.9896472096443176, 'train/loss': 0.03462878614664078, 'train/mean_average_precision': 0.33390209800927395, 'validation/accuracy': 0.9862998723983765, 'validation/loss': 0.0462951585650444, 'validation/mean_average_precision': 0.23877159163973058, 'validation/num_examples': 43793, 'test/accuracy': 0.9854249358177185, 'test/loss': 0.048981741070747375, 'test/mean_average_precision': 0.2374337427035071, 'test/num_examples': 43793, 'score': 3133.8573887348175, 'total_duration': 4825.316298484802, 'accumulated_submission_time': 3133.8573887348175, 'accumulated_eval_time': 1690.8187873363495, 'accumulated_logging_time': 0.373058557510376, 'global_step': 9594, 'preemption_count': 0}), (10326, {'train/accuracy': 0.989692211151123, 'train/loss': 0.034801095724105835, 'train/mean_average_precision': 0.33724113406074235, 'validation/accuracy': 0.9864484667778015, 'validation/loss': 0.04563755542039871, 'validation/mean_average_precision': 0.2384509236503546, 'validation/num_examples': 43793, 'test/accuracy': 0.9856414198875427, 'test/loss': 0.04823276773095131, 'test/mean_average_precision': 0.24216281094436193, 'test/num_examples': 43793, 'score': 3374.110833644867, 'total_duration': 5187.90918803215, 'accumulated_submission_time': 3374.110833644867, 'accumulated_eval_time': 1813.1092777252197, 'accumulated_logging_time': 0.40241336822509766, 'global_step': 10326, 'preemption_count': 0}), (11064, {'train/accuracy': 0.9897984266281128, 'train/loss': 0.033979859203100204, 'train/mean_average_precision': 0.34181194912180346, 'validation/accuracy': 0.9864703416824341, 'validation/loss': 0.04572243243455887, 'validation/mean_average_precision': 0.24624071173579296, 'validation/num_examples': 43793, 'test/accuracy': 0.9856801629066467, 'test/loss': 0.048328228294849396, 'test/mean_average_precision': 0.2508927952331046, 'test/num_examples': 43793, 'score': 3614.1969878673553, 'total_duration': 5547.547726154327, 'accumulated_submission_time': 3614.1969878673553, 'accumulated_eval_time': 1932.6103360652924, 'accumulated_logging_time': 0.4336550235748291, 'global_step': 11064, 'preemption_count': 0}), (11806, {'train/accuracy': 0.9900467395782471, 'train/loss': 0.033071935176849365, 'train/mean_average_precision': 0.3749070903184368, 'validation/accuracy': 0.9865052700042725, 'validation/loss': 0.04551073908805847, 'validation/mean_average_precision': 0.24596945471530543, 'validation/num_examples': 43793, 'test/accuracy': 0.9856789112091064, 'test/loss': 0.04816749319434166, 'test/mean_average_precision': 0.2501293202794034, 'test/num_examples': 43793, 'score': 3854.333966732025, 'total_duration': 5906.48420381546, 'accumulated_submission_time': 3854.333966732025, 'accumulated_eval_time': 2051.3599536418915, 'accumulated_logging_time': 0.463397741317749, 'global_step': 11806, 'preemption_count': 0}), (12535, {'train/accuracy': 0.9904047846794128, 'train/loss': 0.03221077844500542, 'train/mean_average_precision': 0.37802480556687307, 'validation/accuracy': 0.9865750670433044, 'validation/loss': 0.04533899575471878, 'validation/mean_average_precision': 0.2465292262857419, 'validation/num_examples': 43793, 'test/accuracy': 0.9857109189033508, 'test/loss': 0.04792236536741257, 'test/mean_average_precision': 0.2474763344552082, 'test/num_examples': 43793, 'score': 4094.3583641052246, 'total_duration': 6267.943195104599, 'accumulated_submission_time': 4094.3583641052246, 'accumulated_eval_time': 2172.739722967148, 'accumulated_logging_time': 0.49429869651794434, 'global_step': 12535, 'preemption_count': 0}), (13272, {'train/accuracy': 0.990593671798706, 'train/loss': 0.03138144314289093, 'train/mean_average_precision': 0.4109021595111499, 'validation/accuracy': 0.9866944551467896, 'validation/loss': 0.04504287615418434, 'validation/mean_average_precision': 0.2539900303246503, 'validation/num_examples': 43793, 'test/accuracy': 0.9858461618423462, 'test/loss': 0.04783207178115845, 'test/mean_average_precision': 0.25654210886448603, 'test/num_examples': 43793, 'score': 4334.632151842117, 'total_duration': 6626.192683458328, 'accumulated_submission_time': 4334.632151842117, 'accumulated_eval_time': 2290.664441347122, 'accumulated_logging_time': 0.525383710861206, 'global_step': 13272, 'preemption_count': 0}), (14008, {'train/accuracy': 0.9907403588294983, 'train/loss': 0.030393077060580254, 'train/mean_average_precision': 0.43595080978843537, 'validation/accuracy': 0.9867265224456787, 'validation/loss': 0.04505808278918266, 'validation/mean_average_precision': 0.2625926952812717, 'validation/num_examples': 43793, 'test/accuracy': 0.985842764377594, 'test/loss': 0.047687798738479614, 'test/mean_average_precision': 0.2592138244146481, 'test/num_examples': 43793, 'score': 4574.894693851471, 'total_duration': 6987.949181556702, 'accumulated_submission_time': 4574.894693851471, 'accumulated_eval_time': 2412.107246160507, 'accumulated_logging_time': 0.5567858219146729, 'global_step': 14008, 'preemption_count': 0}), (14739, {'train/accuracy': 0.9908390641212463, 'train/loss': 0.02991710603237152, 'train/mean_average_precision': 0.4426423572423156, 'validation/accuracy': 0.9866229891777039, 'validation/loss': 0.04541115462779999, 'validation/mean_average_precision': 0.25832731771207995, 'validation/num_examples': 43793, 'test/accuracy': 0.9856974482536316, 'test/loss': 0.04819463565945625, 'test/mean_average_precision': 0.25313458407153794, 'test/num_examples': 43793, 'score': 4814.908618211746, 'total_duration': 7351.350212574005, 'accumulated_submission_time': 4814.908618211746, 'accumulated_eval_time': 2535.4455637931824, 'accumulated_logging_time': 0.586010217666626, 'global_step': 14739, 'preemption_count': 0}), (15467, {'train/accuracy': 0.9911337494850159, 'train/loss': 0.029770169407129288, 'train/mean_average_precision': 0.43945538132750406, 'validation/accuracy': 0.9866952300071716, 'validation/loss': 0.04476303607225418, 'validation/mean_average_precision': 0.25842765475475865, 'validation/num_examples': 43793, 'test/accuracy': 0.9858457446098328, 'test/loss': 0.04745708405971527, 'test/mean_average_precision': 0.2533422638624075, 'test/num_examples': 43793, 'score': 5054.8963351249695, 'total_duration': 7712.3310215473175, 'accumulated_submission_time': 5054.8963351249695, 'accumulated_eval_time': 2656.38614153862, 'accumulated_logging_time': 0.6158895492553711, 'global_step': 15467, 'preemption_count': 0}), (16201, {'train/accuracy': 0.9909468293190002, 'train/loss': 0.029692286625504494, 'train/mean_average_precision': 0.4539990121975116, 'validation/accuracy': 0.9867402911186218, 'validation/loss': 0.045339684933423996, 'validation/mean_average_precision': 0.2613085320446217, 'validation/num_examples': 43793, 'test/accuracy': 0.9859640598297119, 'test/loss': 0.04803311079740524, 'test/mean_average_precision': 0.26270306084262335, 'test/num_examples': 43793, 'score': 5295.120626449585, 'total_duration': 8068.958131074905, 'accumulated_submission_time': 5295.120626449585, 'accumulated_eval_time': 2772.7395095825195, 'accumulated_logging_time': 0.6458499431610107, 'global_step': 16201, 'preemption_count': 0}), (16944, {'train/accuracy': 0.9909261465072632, 'train/loss': 0.02961067296564579, 'train/mean_average_precision': 0.4592231931246184, 'validation/accuracy': 0.9867545366287231, 'validation/loss': 0.04582856222987175, 'validation/mean_average_precision': 0.26213948876819726, 'validation/num_examples': 43793, 'test/accuracy': 0.9858773350715637, 'test/loss': 0.0486006960272789, 'test/mean_average_precision': 0.2582021144128389, 'test/num_examples': 43793, 'score': 5535.215556144714, 'total_duration': 8427.543749332428, 'accumulated_submission_time': 5535.215556144714, 'accumulated_eval_time': 2891.1803154945374, 'accumulated_logging_time': 0.6761302947998047, 'global_step': 16944, 'preemption_count': 0}), (17681, {'train/accuracy': 0.9912601709365845, 'train/loss': 0.02848539873957634, 'train/mean_average_precision': 0.47475583614365885, 'validation/accuracy': 0.9867882132530212, 'validation/loss': 0.04549652710556984, 'validation/mean_average_precision': 0.26414985254090756, 'validation/num_examples': 43793, 'test/accuracy': 0.985878586769104, 'test/loss': 0.04837343841791153, 'test/mean_average_precision': 0.2591937009499683, 'test/num_examples': 43793, 'score': 5775.349369287491, 'total_duration': 8783.784770011902, 'accumulated_submission_time': 5775.349369287491, 'accumulated_eval_time': 3007.234807729721, 'accumulated_logging_time': 0.7085833549499512, 'global_step': 17681, 'preemption_count': 0}), (18420, {'train/accuracy': 0.9915626645088196, 'train/loss': 0.027797440066933632, 'train/mean_average_precision': 0.48316749959775956, 'validation/accuracy': 0.9867565631866455, 'validation/loss': 0.04541521891951561, 'validation/mean_average_precision': 0.2554176813356391, 'validation/num_examples': 43793, 'test/accuracy': 0.9858541488647461, 'test/loss': 0.048193056136369705, 'test/mean_average_precision': 0.2527952731086541, 'test/num_examples': 43793, 'score': 6015.588186979294, 'total_duration': 9146.614343166351, 'accumulated_submission_time': 6015.588186979294, 'accumulated_eval_time': 3129.774727344513, 'accumulated_logging_time': 0.739865779876709, 'global_step': 18420, 'preemption_count': 0}), (19167, {'train/accuracy': 0.9915648102760315, 'train/loss': 0.02735765278339386, 'train/mean_average_precision': 0.5086215310274381, 'validation/accuracy': 0.9867614507675171, 'validation/loss': 0.0455816313624382, 'validation/mean_average_precision': 0.26315183788354574, 'validation/num_examples': 43793, 'test/accuracy': 0.9859089255332947, 'test/loss': 0.04837782680988312, 'test/mean_average_precision': 0.25896516638266726, 'test/num_examples': 43793, 'score': 6255.743933677673, 'total_duration': 9501.434201717377, 'accumulated_submission_time': 6255.743933677673, 'accumulated_eval_time': 3244.3884332180023, 'accumulated_logging_time': 0.7705888748168945, 'global_step': 19167, 'preemption_count': 0}), (19903, {'train/accuracy': 0.992231011390686, 'train/loss': 0.025556445121765137, 'train/mean_average_precision': 0.5372999658293216, 'validation/accuracy': 0.9868332743644714, 'validation/loss': 0.04510356858372688, 'validation/mean_average_precision': 0.2715457824918656, 'validation/num_examples': 43793, 'test/accuracy': 0.9860078692436218, 'test/loss': 0.04783665016293526, 'test/mean_average_precision': 0.2606229283561334, 'test/num_examples': 43793, 'score': 6496.008260965347, 'total_duration': 9855.55399274826, 'accumulated_submission_time': 6496.008260965347, 'accumulated_eval_time': 3358.1936955451965, 'accumulated_logging_time': 0.8017594814300537, 'global_step': 19903, 'preemption_count': 0}), (20644, {'train/accuracy': 0.9922563433647156, 'train/loss': 0.025297215208411217, 'train/mean_average_precision': 0.5544247501332782, 'validation/accuracy': 0.9868401885032654, 'validation/loss': 0.04562132805585861, 'validation/mean_average_precision': 0.26867772622666897, 'validation/num_examples': 43793, 'test/accuracy': 0.9859581589698792, 'test/loss': 0.04842758551239967, 'test/mean_average_precision': 0.26032937570119197, 'test/num_examples': 43793, 'score': 6736.173248052597, 'total_duration': 10212.136778831482, 'accumulated_submission_time': 6736.173248052597, 'accumulated_eval_time': 3474.561186313629, 'accumulated_logging_time': 0.832097053527832, 'global_step': 20644, 'preemption_count': 0}), (21384, {'train/accuracy': 0.9923427700996399, 'train/loss': 0.024966953322291374, 'train/mean_average_precision': 0.5519044794078625, 'validation/accuracy': 0.9868003726005554, 'validation/loss': 0.046172283589839935, 'validation/mean_average_precision': 0.264854724285865, 'validation/num_examples': 43793, 'test/accuracy': 0.9859493374824524, 'test/loss': 0.049079928547143936, 'test/mean_average_precision': 0.2598057736054661, 'test/num_examples': 43793, 'score': 6976.150879383087, 'total_duration': 10567.873800992966, 'accumulated_submission_time': 6976.150879383087, 'accumulated_eval_time': 3590.268481016159, 'accumulated_logging_time': 0.8643553256988525, 'global_step': 21384, 'preemption_count': 0}), (22120, {'train/accuracy': 0.991874098777771, 'train/loss': 0.026532527059316635, 'train/mean_average_precision': 0.5180254499914452, 'validation/accuracy': 0.9867808818817139, 'validation/loss': 0.04616933688521385, 'validation/mean_average_precision': 0.26209752667709835, 'validation/num_examples': 43793, 'test/accuracy': 0.985874354839325, 'test/loss': 0.0491098128259182, 'test/mean_average_precision': 0.26179309925375405, 'test/num_examples': 43793, 'score': 7216.1168756484985, 'total_duration': 10926.920147418976, 'accumulated_submission_time': 7216.1168756484985, 'accumulated_eval_time': 3709.2970135211945, 'accumulated_logging_time': 0.8965957164764404, 'global_step': 22120, 'preemption_count': 0}), (22854, {'train/accuracy': 0.9917595386505127, 'train/loss': 0.026461439207196236, 'train/mean_average_precision': 0.5174314820179169, 'validation/accuracy': 0.986764669418335, 'validation/loss': 0.04666900262236595, 'validation/mean_average_precision': 0.26590033672032376, 'validation/num_examples': 43793, 'test/accuracy': 0.9859341979026794, 'test/loss': 0.049701590090990067, 'test/mean_average_precision': 0.2569096916812762, 'test/num_examples': 43793, 'score': 7456.379545927048, 'total_duration': 11289.88250374794, 'accumulated_submission_time': 7456.379545927048, 'accumulated_eval_time': 3831.9452407360077, 'accumulated_logging_time': 0.9279828071594238, 'global_step': 22854, 'preemption_count': 0}), (23586, {'train/accuracy': 0.992215096950531, 'train/loss': 0.025306835770606995, 'train/mean_average_precision': 0.5333598000806847, 'validation/accuracy': 0.9867005348205566, 'validation/loss': 0.046321846544742584, 'validation/mean_average_precision': 0.25778430552511333, 'validation/num_examples': 43793, 'test/accuracy': 0.9858810901641846, 'test/loss': 0.0492570735514164, 'test/mean_average_precision': 0.25460882352500436, 'test/num_examples': 43793, 'score': 7696.485308170319, 'total_duration': 11650.7317340374, 'accumulated_submission_time': 7696.485308170319, 'accumulated_eval_time': 3952.633655309677, 'accumulated_logging_time': 0.962378740310669, 'global_step': 23586, 'preemption_count': 0}), (24314, {'train/accuracy': 0.9922340512275696, 'train/loss': 0.02505871281027794, 'train/mean_average_precision': 0.5520819790954581, 'validation/accuracy': 0.986710250377655, 'validation/loss': 0.04724223539233208, 'validation/mean_average_precision': 0.2548902204472823, 'validation/num_examples': 43793, 'test/accuracy': 0.9858802556991577, 'test/loss': 0.050252288579940796, 'test/mean_average_precision': 0.2530300101575706, 'test/num_examples': 43793, 'score': 7936.522948503494, 'total_duration': 12010.12484574318, 'accumulated_submission_time': 7936.522948503494, 'accumulated_eval_time': 4071.9370653629303, 'accumulated_logging_time': 0.9949030876159668, 'global_step': 24314, 'preemption_count': 0}), (25054, {'train/accuracy': 0.9923595190048218, 'train/loss': 0.024475768208503723, 'train/mean_average_precision': 0.5629086457355192, 'validation/accuracy': 0.9867419600486755, 'validation/loss': 0.04716869443655014, 'validation/mean_average_precision': 0.2574200603232365, 'validation/num_examples': 43793, 'test/accuracy': 0.985859215259552, 'test/loss': 0.050281304866075516, 'test/mean_average_precision': 0.25572176194390206, 'test/num_examples': 43793, 'score': 8176.764466285706, 'total_duration': 12367.119560956955, 'accumulated_submission_time': 8176.764466285706, 'accumulated_eval_time': 4188.638366937637, 'accumulated_logging_time': 1.026634693145752, 'global_step': 25054, 'preemption_count': 0}), (25778, {'train/accuracy': 0.9928763508796692, 'train/loss': 0.02270345762372017, 'train/mean_average_precision': 0.6162934071137858, 'validation/accuracy': 0.9866648316383362, 'validation/loss': 0.04745422303676605, 'validation/mean_average_precision': 0.2594423493508385, 'validation/num_examples': 43793, 'test/accuracy': 0.9858731031417847, 'test/loss': 0.050558581948280334, 'test/mean_average_precision': 0.2549490936009421, 'test/num_examples': 43793, 'score': 8416.962620973587, 'total_duration': 12728.26784825325, 'accumulated_submission_time': 8416.962620973587, 'accumulated_eval_time': 4309.53524518013, 'accumulated_logging_time': 1.0576236248016357, 'global_step': 25778, 'preemption_count': 0}), (26515, {'train/accuracy': 0.9934829473495483, 'train/loss': 0.021650198847055435, 'train/mean_average_precision': 0.6098429034027366, 'validation/accuracy': 0.9865832328796387, 'validation/loss': 0.04722558334469795, 'validation/mean_average_precision': 0.26031340684549625, 'validation/num_examples': 43793, 'test/accuracy': 0.9857437610626221, 'test/loss': 0.05030810087919235, 'test/mean_average_precision': 0.25004458692146053, 'test/num_examples': 43793, 'score': 8657.234840393066, 'total_duration': 13088.86488366127, 'accumulated_submission_time': 8657.234840393066, 'accumulated_eval_time': 4429.807000875473, 'accumulated_logging_time': 1.0906052589416504, 'global_step': 26515, 'preemption_count': 0}), (27259, {'train/accuracy': 0.9931455254554749, 'train/loss': 0.022433340549468994, 'train/mean_average_precision': 0.601588529797614, 'validation/accuracy': 0.986559271812439, 'validation/loss': 0.04756902530789375, 'validation/mean_average_precision': 0.2533269911318941, 'validation/num_examples': 43793, 'test/accuracy': 0.9857968688011169, 'test/loss': 0.050486188381910324, 'test/mean_average_precision': 0.25171143873459717, 'test/num_examples': 43793, 'score': 8897.440607070923, 'total_duration': 13450.329896450043, 'accumulated_submission_time': 8897.440607070923, 'accumulated_eval_time': 4551.0147252082825, 'accumulated_logging_time': 1.12199068069458, 'global_step': 27259, 'preemption_count': 0}), (27986, {'train/accuracy': 0.99298095703125, 'train/loss': 0.022835660725831985, 'train/mean_average_precision': 0.5933179400988164, 'validation/accuracy': 0.9866059422492981, 'validation/loss': 0.047744277864694595, 'validation/mean_average_precision': 0.2548442086332938, 'validation/num_examples': 43793, 'test/accuracy': 0.9857610464096069, 'test/loss': 0.05074087902903557, 'test/mean_average_precision': 0.2519577155097544, 'test/num_examples': 43793, 'score': 9137.503035783768, 'total_duration': 13814.634695529938, 'accumulated_submission_time': 9137.503035783768, 'accumulated_eval_time': 4675.2046592235565, 'accumulated_logging_time': 1.1547377109527588, 'global_step': 27986, 'preemption_count': 0}), (28719, {'train/accuracy': 0.9927420020103455, 'train/loss': 0.023180410265922546, 'train/mean_average_precision': 0.5969560782400278, 'validation/accuracy': 0.9867175817489624, 'validation/loss': 0.0479126051068306, 'validation/mean_average_precision': 0.2682034494169585, 'validation/num_examples': 43793, 'test/accuracy': 0.9859505891799927, 'test/loss': 0.051033515483140945, 'test/mean_average_precision': 0.25550628832569594, 'test/num_examples': 43793, 'score': 9377.56341791153, 'total_duration': 14169.90866613388, 'accumulated_submission_time': 9377.56341791153, 'accumulated_eval_time': 4790.3657166957855, 'accumulated_logging_time': 1.1872403621673584, 'global_step': 28719, 'preemption_count': 0}), (29456, {'train/accuracy': 0.9926162958145142, 'train/loss': 0.02338692918419838, 'train/mean_average_precision': 0.5947517350336357, 'validation/accuracy': 0.9866631627082825, 'validation/loss': 0.04864392429590225, 'validation/mean_average_precision': 0.2574898460922729, 'validation/num_examples': 43793, 'test/accuracy': 0.9856972694396973, 'test/loss': 0.05184772610664368, 'test/mean_average_precision': 0.24875211763305882, 'test/num_examples': 43793, 'score': 9617.663268327713, 'total_duration': 14530.736252069473, 'accumulated_submission_time': 9617.663268327713, 'accumulated_eval_time': 4911.0401475429535, 'accumulated_logging_time': 1.2209062576293945, 'global_step': 29456, 'preemption_count': 0}), (30183, {'train/accuracy': 0.9931610822677612, 'train/loss': 0.022001130506396294, 'train/mean_average_precision': 0.6025305047501668, 'validation/accuracy': 0.986594557762146, 'validation/loss': 0.048090916126966476, 'validation/mean_average_precision': 0.25966516472182116, 'validation/num_examples': 43793, 'test/accuracy': 0.9857484102249146, 'test/loss': 0.051175639033317566, 'test/mean_average_precision': 0.2540606307889602, 'test/num_examples': 43793, 'score': 9857.94836807251, 'total_duration': 14889.342999219894, 'accumulated_submission_time': 9857.94836807251, 'accumulated_eval_time': 5029.308331251144, 'accumulated_logging_time': 1.2543549537658691, 'global_step': 30183, 'preemption_count': 0}), (30913, {'train/accuracy': 0.9933783411979675, 'train/loss': 0.02104005217552185, 'train/mean_average_precision': 0.6411639773982251, 'validation/accuracy': 0.9866043329238892, 'validation/loss': 0.048770561814308167, 'validation/mean_average_precision': 0.25318820335208087, 'validation/num_examples': 43793, 'test/accuracy': 0.9857787489891052, 'test/loss': 0.05177788436412811, 'test/mean_average_precision': 0.25129154224252925, 'test/num_examples': 43793, 'score': 10098.119551181793, 'total_duration': 15248.962916851044, 'accumulated_submission_time': 10098.119551181793, 'accumulated_eval_time': 5148.699724435806, 'accumulated_logging_time': 1.288421869277954, 'global_step': 30913, 'preemption_count': 0}), (31645, {'train/accuracy': 0.993828535079956, 'train/loss': 0.01970619522035122, 'train/mean_average_precision': 0.6617192203318676, 'validation/accuracy': 0.9866339564323425, 'validation/loss': 0.04894295334815979, 'validation/mean_average_precision': 0.25527283874613615, 'validation/num_examples': 43793, 'test/accuracy': 0.9857193827629089, 'test/loss': 0.0524006262421608, 'test/mean_average_precision': 0.24764914519659614, 'test/num_examples': 43793, 'score': 10338.317598104477, 'total_duration': 15610.73458480835, 'accumulated_submission_time': 10338.317598104477, 'accumulated_eval_time': 5270.216884851456, 'accumulated_logging_time': 1.32468581199646, 'global_step': 31645, 'preemption_count': 0}), (32376, {'train/accuracy': 0.994285523891449, 'train/loss': 0.018733149394392967, 'train/mean_average_precision': 0.6851187612466829, 'validation/accuracy': 0.986394464969635, 'validation/loss': 0.049223363399505615, 'validation/mean_average_precision': 0.25345441478042446, 'validation/num_examples': 43793, 'test/accuracy': 0.9855828881263733, 'test/loss': 0.05239072069525719, 'test/mean_average_precision': 0.24381363952828328, 'test/num_examples': 43793, 'score': 10578.267916440964, 'total_duration': 15970.068334579468, 'accumulated_submission_time': 10578.267916440964, 'accumulated_eval_time': 5389.546606063843, 'accumulated_logging_time': 1.3582172393798828, 'global_step': 32376, 'preemption_count': 0}), (33103, {'train/accuracy': 0.9938401579856873, 'train/loss': 0.019870581105351448, 'train/mean_average_precision': 0.6640449668679048, 'validation/accuracy': 0.9866132736206055, 'validation/loss': 0.04941275343298912, 'validation/mean_average_precision': 0.24770945202859193, 'validation/num_examples': 43793, 'test/accuracy': 0.9857964515686035, 'test/loss': 0.052649471908807755, 'test/mean_average_precision': 0.24871137662689824, 'test/num_examples': 43793, 'score': 10818.28539800644, 'total_duration': 16328.837913751602, 'accumulated_submission_time': 10818.28539800644, 'accumulated_eval_time': 5508.245011806488, 'accumulated_logging_time': 1.3919477462768555, 'global_step': 33103, 'preemption_count': 0}), (33847, {'train/accuracy': 0.9930763244628906, 'train/loss': 0.021939706057310104, 'train/mean_average_precision': 0.604933073567349, 'validation/accuracy': 0.9864837527275085, 'validation/loss': 0.05017333850264549, 'validation/mean_average_precision': 0.2480326956404477, 'validation/num_examples': 43793, 'test/accuracy': 0.9856372475624084, 'test/loss': 0.05341297388076782, 'test/mean_average_precision': 0.24895635080888956, 'test/num_examples': 43793, 'score': 11058.477831840515, 'total_duration': 16685.099545240402, 'accumulated_submission_time': 11058.477831840515, 'accumulated_eval_time': 5624.259745597839, 'accumulated_logging_time': 1.4259235858917236, 'global_step': 33847, 'preemption_count': 0}), (34589, {'train/accuracy': 0.9932107329368591, 'train/loss': 0.021749544888734818, 'train/mean_average_precision': 0.6007094082387573, 'validation/accuracy': 0.9864163994789124, 'validation/loss': 0.04977237060666084, 'validation/mean_average_precision': 0.2514264851650297, 'validation/num_examples': 43793, 'test/accuracy': 0.9856675267219543, 'test/loss': 0.05289927124977112, 'test/mean_average_precision': 0.2486676367310245, 'test/num_examples': 43793, 'score': 11298.49192237854, 'total_duration': 17037.724573135376, 'accumulated_submission_time': 11298.49192237854, 'accumulated_eval_time': 5736.817763805389, 'accumulated_logging_time': 1.4586491584777832, 'global_step': 34589, 'preemption_count': 0}), (35331, {'train/accuracy': 0.993598461151123, 'train/loss': 0.020510371774435043, 'train/mean_average_precision': 0.6334068095016367, 'validation/accuracy': 0.986381471157074, 'validation/loss': 0.05009084194898605, 'validation/mean_average_precision': 0.24640121489804126, 'validation/num_examples': 43793, 'test/accuracy': 0.9855715036392212, 'test/loss': 0.053255315870046616, 'test/mean_average_precision': 0.251005720832414, 'test/num_examples': 43793, 'score': 11538.711616039276, 'total_duration': 17393.373587608337, 'accumulated_submission_time': 11538.711616039276, 'accumulated_eval_time': 5852.193566083908, 'accumulated_logging_time': 1.4922783374786377, 'global_step': 35331, 'preemption_count': 0}), (36077, {'train/accuracy': 0.9936655759811401, 'train/loss': 0.019992902874946594, 'train/mean_average_precision': 0.6459082535084215, 'validation/accuracy': 0.9864638447761536, 'validation/loss': 0.05059313774108887, 'validation/mean_average_precision': 0.2457527230082513, 'validation/num_examples': 43793, 'test/accuracy': 0.9856035113334656, 'test/loss': 0.05398542433977127, 'test/mean_average_precision': 0.24352993077669655, 'test/num_examples': 43793, 'score': 11778.746633768082, 'total_duration': 17743.378707647324, 'accumulated_submission_time': 11778.746633768082, 'accumulated_eval_time': 5962.109864473343, 'accumulated_logging_time': 1.5256366729736328, 'global_step': 36077, 'preemption_count': 0}), (36814, {'train/accuracy': 0.9936034679412842, 'train/loss': 0.02002665400505066, 'train/mean_average_precision': 0.6523864222962256, 'validation/accuracy': 0.9863453507423401, 'validation/loss': 0.05084327608346939, 'validation/mean_average_precision': 0.24100806250108817, 'validation/num_examples': 43793, 'test/accuracy': 0.9855087399482727, 'test/loss': 0.05447215214371681, 'test/mean_average_precision': 0.24192518362956272, 'test/num_examples': 43793, 'score': 12018.441509246826, 'total_duration': 18100.86488223076, 'accumulated_submission_time': 12018.441509246826, 'accumulated_eval_time': 6079.482671022415, 'accumulated_logging_time': 1.9244272708892822, 'global_step': 36814, 'preemption_count': 0}), (37548, {'train/accuracy': 0.9947233200073242, 'train/loss': 0.01713837869465351, 'train/mean_average_precision': 0.7183873072526152, 'validation/accuracy': 0.9864943027496338, 'validation/loss': 0.0512232780456543, 'validation/mean_average_precision': 0.24664180354072704, 'validation/num_examples': 43793, 'test/accuracy': 0.9856827259063721, 'test/loss': 0.05457107350230217, 'test/mean_average_precision': 0.2465838969900322, 'test/num_examples': 43793, 'score': 12258.66541147232, 'total_duration': 18457.14681982994, 'accumulated_submission_time': 12258.66541147232, 'accumulated_eval_time': 6195.485498428345, 'accumulated_logging_time': 1.9596786499023438, 'global_step': 37548, 'preemption_count': 0}), (38282, {'train/accuracy': 0.995094358921051, 'train/loss': 0.01605805940926075, 'train/mean_average_precision': 0.7305360156173912, 'validation/accuracy': 0.9864216446876526, 'validation/loss': 0.0519767627120018, 'validation/mean_average_precision': 0.2444132828398841, 'validation/num_examples': 43793, 'test/accuracy': 0.985569417476654, 'test/loss': 0.05545670911669731, 'test/mean_average_precision': 0.24167754008738948, 'test/num_examples': 43793, 'score': 12498.670905351639, 'total_duration': 18815.900892019272, 'accumulated_submission_time': 12498.670905351639, 'accumulated_eval_time': 6314.177472352982, 'accumulated_logging_time': 1.9961557388305664, 'global_step': 38282, 'preemption_count': 0}), (39020, {'train/accuracy': 0.9950499534606934, 'train/loss': 0.01644003763794899, 'train/mean_average_precision': 0.7171123304405793, 'validation/accuracy': 0.9864265322685242, 'validation/loss': 0.05110594257712364, 'validation/mean_average_precision': 0.2482558400172647, 'validation/num_examples': 43793, 'test/accuracy': 0.9855411648750305, 'test/loss': 0.05495946481823921, 'test/mean_average_precision': 0.2413185281618792, 'test/num_examples': 43793, 'score': 12738.752710580826, 'total_duration': 19172.32074737549, 'accumulated_submission_time': 12738.752710580826, 'accumulated_eval_time': 6430.45547246933, 'accumulated_logging_time': 2.035463333129883, 'global_step': 39020, 'preemption_count': 0}), (39750, {'train/accuracy': 0.9946119785308838, 'train/loss': 0.017336323857307434, 'train/mean_average_precision': 0.7018877172650734, 'validation/accuracy': 0.9864224791526794, 'validation/loss': 0.051711633801460266, 'validation/mean_average_precision': 0.24540036093258488, 'validation/num_examples': 43793, 'test/accuracy': 0.9855074882507324, 'test/loss': 0.055399827659130096, 'test/mean_average_precision': 0.23960519106040393, 'test/num_examples': 43793, 'score': 12978.81694483757, 'total_duration': 19527.48945236206, 'accumulated_submission_time': 12978.81694483757, 'accumulated_eval_time': 6545.504342556, 'accumulated_logging_time': 2.070361852645874, 'global_step': 39750, 'preemption_count': 0}), (40480, {'train/accuracy': 0.9938182830810547, 'train/loss': 0.01904170773923397, 'train/mean_average_precision': 0.6818837267982015, 'validation/accuracy': 0.9863936305046082, 'validation/loss': 0.052733831107616425, 'validation/mean_average_precision': 0.24132601573867477, 'validation/num_examples': 43793, 'test/accuracy': 0.9855066537857056, 'test/loss': 0.05638336390256882, 'test/mean_average_precision': 0.2406186509886538, 'test/num_examples': 43793, 'score': 13218.769835948944, 'total_duration': 19882.935147047043, 'accumulated_submission_time': 13218.769835948944, 'accumulated_eval_time': 6660.941750526428, 'accumulated_logging_time': 2.1060330867767334, 'global_step': 40480, 'preemption_count': 0}), (41218, {'train/accuracy': 0.9941673874855042, 'train/loss': 0.018260326236486435, 'train/mean_average_precision': 0.6868479320912788, 'validation/accuracy': 0.986441969871521, 'validation/loss': 0.052412815392017365, 'validation/mean_average_precision': 0.24501272741587676, 'validation/num_examples': 43793, 'test/accuracy': 0.9854856133460999, 'test/loss': 0.05603795498609543, 'test/mean_average_precision': 0.23285328554885704, 'test/num_examples': 43793, 'score': 13458.771996974945, 'total_duration': 20232.86994457245, 'accumulated_submission_time': 13458.771996974945, 'accumulated_eval_time': 6770.818592071533, 'accumulated_logging_time': 2.141692876815796, 'global_step': 41218, 'preemption_count': 0}), (41956, {'train/accuracy': 0.9942653775215149, 'train/loss': 0.01780596934258938, 'train/mean_average_precision': 0.6941094896473745, 'validation/accuracy': 0.9863846898078918, 'validation/loss': 0.05340949445962906, 'validation/mean_average_precision': 0.2359936574011092, 'validation/num_examples': 43793, 'test/accuracy': 0.9855399131774902, 'test/loss': 0.05672483146190643, 'test/mean_average_precision': 0.2337525892945866, 'test/num_examples': 43793, 'score': 13698.72559428215, 'total_duration': 20588.090848207474, 'accumulated_submission_time': 13698.72559428215, 'accumulated_eval_time': 6886.031506538391, 'accumulated_logging_time': 2.176521062850952, 'global_step': 41956, 'preemption_count': 0}), (42698, {'train/accuracy': 0.9948505759239197, 'train/loss': 0.016491945832967758, 'train/mean_average_precision': 0.7170254342784487, 'validation/accuracy': 0.9862925410270691, 'validation/loss': 0.052877672016620636, 'validation/mean_average_precision': 0.2373408264054241, 'validation/num_examples': 43793, 'test/accuracy': 0.9854699969291687, 'test/loss': 0.05652901530265808, 'test/mean_average_precision': 0.23399929332888234, 'test/num_examples': 43793, 'score': 13938.932447195053, 'total_duration': 20938.051344156265, 'accumulated_submission_time': 13938.932447195053, 'accumulated_eval_time': 6995.729851484299, 'accumulated_logging_time': 2.2115795612335205, 'global_step': 42698, 'preemption_count': 0}), (43447, {'train/accuracy': 0.9954681992530823, 'train/loss': 0.014992156066000462, 'train/mean_average_precision': 0.7636412831561692, 'validation/accuracy': 0.9862580299377441, 'validation/loss': 0.05348847061395645, 'validation/mean_average_precision': 0.2398435653849362, 'validation/num_examples': 43793, 'test/accuracy': 0.985352098941803, 'test/loss': 0.05716365948319435, 'test/mean_average_precision': 0.23213330155006406, 'test/num_examples': 43793, 'score': 14179.018709421158, 'total_duration': 21290.509116888046, 'accumulated_submission_time': 14179.018709421158, 'accumulated_eval_time': 7108.043355226517, 'accumulated_logging_time': 2.2498536109924316, 'global_step': 43447, 'preemption_count': 0}), (44197, {'train/accuracy': 0.995785117149353, 'train/loss': 0.01409078948199749, 'train/mean_average_precision': 0.7848275030564575, 'validation/accuracy': 0.9862913489341736, 'validation/loss': 0.054559968411922455, 'validation/mean_average_precision': 0.2348540301396725, 'validation/num_examples': 43793, 'test/accuracy': 0.9854013323783875, 'test/loss': 0.05836547166109085, 'test/mean_average_precision': 0.23273799650880028, 'test/num_examples': 43793, 'score': 14418.994359016418, 'total_duration': 21642.547493696213, 'accumulated_submission_time': 14418.994359016418, 'accumulated_eval_time': 7220.051170825958, 'accumulated_logging_time': 2.2852609157562256, 'global_step': 44197, 'preemption_count': 0}), (44943, {'train/accuracy': 0.9962485432624817, 'train/loss': 0.012892408296465874, 'train/mean_average_precision': 0.8084429781038803, 'validation/accuracy': 0.9863153100013733, 'validation/loss': 0.05484171584248543, 'validation/mean_average_precision': 0.2340596465030607, 'validation/num_examples': 43793, 'test/accuracy': 0.9854552745819092, 'test/loss': 0.05840647220611572, 'test/mean_average_precision': 0.2363809785728483, 'test/num_examples': 43793, 'score': 14659.189492940903, 'total_duration': 21995.470474243164, 'accumulated_submission_time': 14659.189492940903, 'accumulated_eval_time': 7332.72357583046, 'accumulated_logging_time': 2.32065486907959, 'global_step': 44943, 'preemption_count': 0}), (45683, {'train/accuracy': 0.9945888519287109, 'train/loss': 0.01691611483693123, 'train/mean_average_precision': 0.7274850797457694, 'validation/accuracy': 0.9861624836921692, 'validation/loss': 0.05531647801399231, 'validation/mean_average_precision': 0.22963473030953668, 'validation/num_examples': 43793, 'test/accuracy': 0.9852480292320251, 'test/loss': 0.0591854453086853, 'test/mean_average_precision': 0.23166626831947487, 'test/num_examples': 43793, 'score': 14899.14623594284, 'total_duration': 22346.908483743668, 'accumulated_submission_time': 14899.14623594284, 'accumulated_eval_time': 7444.150077819824, 'accumulated_logging_time': 2.3560049533843994, 'global_step': 45683, 'preemption_count': 0}), (46426, {'train/accuracy': 0.9955013990402222, 'train/loss': 0.014552238397300243, 'train/mean_average_precision': 0.7470824289675861, 'validation/accuracy': 0.9862040877342224, 'validation/loss': 0.055199719965457916, 'validation/mean_average_precision': 0.2317819172577487, 'validation/num_examples': 43793, 'test/accuracy': 0.9853516817092896, 'test/loss': 0.058775223791599274, 'test/mean_average_precision': 0.23659981277944173, 'test/num_examples': 43793, 'score': 15139.321603536606, 'total_duration': 22696.00659751892, 'accumulated_submission_time': 15139.321603536606, 'accumulated_eval_time': 7553.0161652565, 'accumulated_logging_time': 2.3929073810577393, 'global_step': 46426, 'preemption_count': 0}), (47163, {'train/accuracy': 0.9950583577156067, 'train/loss': 0.015519848093390465, 'train/mean_average_precision': 0.7462739776245799, 'validation/accuracy': 0.9862487316131592, 'validation/loss': 0.0558253675699234, 'validation/mean_average_precision': 0.23322870463136203, 'validation/num_examples': 43793, 'test/accuracy': 0.9853390455245972, 'test/loss': 0.05956575646996498, 'test/mean_average_precision': 0.23422640893819913, 'test/num_examples': 43793, 'score': 15379.40378499031, 'total_duration': 23047.339591503143, 'accumulated_submission_time': 15379.40378499031, 'accumulated_eval_time': 7664.210397481918, 'accumulated_logging_time': 2.4298739433288574, 'global_step': 47163, 'preemption_count': 0}), (47902, {'train/accuracy': 0.9947388172149658, 'train/loss': 0.016186188906431198, 'train/mean_average_precision': 0.7402327222700571, 'validation/accuracy': 0.9861894249916077, 'validation/loss': 0.05635153129696846, 'validation/mean_average_precision': 0.22443474221100657, 'validation/num_examples': 43793, 'test/accuracy': 0.9852008819580078, 'test/loss': 0.060159847140312195, 'test/mean_average_precision': 0.2281059140172945, 'test/num_examples': 43793, 'score': 15619.506531715393, 'total_duration': 23398.714463949203, 'accumulated_submission_time': 15619.506531715393, 'accumulated_eval_time': 7775.425299167633, 'accumulated_logging_time': 2.46665096282959, 'global_step': 47902, 'preemption_count': 0}), (48640, {'train/accuracy': 0.9944795966148376, 'train/loss': 0.01678384654223919, 'train/mean_average_precision': 0.7291917979386155, 'validation/accuracy': 0.986224353313446, 'validation/loss': 0.05685719475150108, 'validation/mean_average_precision': 0.2288818870293448, 'validation/num_examples': 43793, 'test/accuracy': 0.9852421283721924, 'test/loss': 0.060674987733364105, 'test/mean_average_precision': 0.2275132350776089, 'test/num_examples': 43793, 'score': 15859.705310821533, 'total_duration': 23748.696949481964, 'accumulated_submission_time': 15859.705310821533, 'accumulated_eval_time': 7885.1501557827, 'accumulated_logging_time': 2.50447678565979, 'global_step': 48640, 'preemption_count': 0}), (49384, {'train/accuracy': 0.9957962036132812, 'train/loss': 0.01372190099209547, 'train/mean_average_precision': 0.7867519225622568, 'validation/accuracy': 0.9862629175186157, 'validation/loss': 0.05660553649067879, 'validation/mean_average_precision': 0.2319078980631626, 'validation/num_examples': 43793, 'test/accuracy': 0.9852739572525024, 'test/loss': 0.06066294386982918, 'test/mean_average_precision': 0.22573726235715638, 'test/num_examples': 43793, 'score': 16099.769166469574, 'total_duration': 24102.322848558426, 'accumulated_submission_time': 16099.769166469574, 'accumulated_eval_time': 7998.654189348221, 'accumulated_logging_time': 2.5425143241882324, 'global_step': 49384, 'preemption_count': 0}), (50127, {'train/accuracy': 0.996640145778656, 'train/loss': 0.011927308514714241, 'train/mean_average_precision': 0.8215199199748869, 'validation/accuracy': 0.9861634969711304, 'validation/loss': 0.0570623017847538, 'validation/mean_average_precision': 0.23245748095170055, 'validation/num_examples': 43793, 'test/accuracy': 0.9852084517478943, 'test/loss': 0.06094241514801979, 'test/mean_average_precision': 0.22467612600900985, 'test/num_examples': 43793, 'score': 16339.929149627686, 'total_duration': 24458.41442155838, 'accumulated_submission_time': 16339.929149627686, 'accumulated_eval_time': 8114.529004335403, 'accumulated_logging_time': 2.5795295238494873, 'global_step': 50127, 'preemption_count': 0}), (50874, {'train/accuracy': 0.9970665574073792, 'train/loss': 0.01100137922912836, 'train/mean_average_precision': 0.8423189663398486, 'validation/accuracy': 0.9861395359039307, 'validation/loss': 0.05757027119398117, 'validation/mean_average_precision': 0.22933294722464312, 'validation/num_examples': 43793, 'test/accuracy': 0.9852164387702942, 'test/loss': 0.061573684215545654, 'test/mean_average_precision': 0.22282484644105666, 'test/num_examples': 43793, 'score': 16580.062649965286, 'total_duration': 24813.826580524445, 'accumulated_submission_time': 16580.062649965286, 'accumulated_eval_time': 8229.751652002335, 'accumulated_logging_time': 2.6154143810272217, 'global_step': 50874, 'preemption_count': 0}), (51617, {'train/accuracy': 0.996950089931488, 'train/loss': 0.011073999106884003, 'train/mean_average_precision': 0.8505485895345607, 'validation/accuracy': 0.986198365688324, 'validation/loss': 0.058026328682899475, 'validation/mean_average_precision': 0.23043124541505816, 'validation/num_examples': 43793, 'test/accuracy': 0.9852501749992371, 'test/loss': 0.062121957540512085, 'test/mean_average_precision': 0.2230208669252613, 'test/num_examples': 43793, 'score': 16820.25114750862, 'total_duration': 25167.758558750153, 'accumulated_submission_time': 16820.25114750862, 'accumulated_eval_time': 8343.433227062225, 'accumulated_logging_time': 2.657003402709961, 'global_step': 51617, 'preemption_count': 0}), (52353, {'train/accuracy': 0.9966402053833008, 'train/loss': 0.01182248443365097, 'train/mean_average_precision': 0.8342763710567649, 'validation/accuracy': 0.9861005544662476, 'validation/loss': 0.05785268545150757, 'validation/mean_average_precision': 0.22868603820860048, 'validation/num_examples': 43793, 'test/accuracy': 0.9851081967353821, 'test/loss': 0.06201310455799103, 'test/mean_average_precision': 0.22348948966010676, 'test/num_examples': 43793, 'score': 17060.339695692062, 'total_duration': 25520.81440806389, 'accumulated_submission_time': 17060.339695692062, 'accumulated_eval_time': 8456.342872619629, 'accumulated_logging_time': 2.694946765899658, 'global_step': 52353, 'preemption_count': 0}), (53106, {'train/accuracy': 0.9960552453994751, 'train/loss': 0.012784690596163273, 'train/mean_average_precision': 0.808120967576912, 'validation/accuracy': 0.9861500859260559, 'validation/loss': 0.05887707695364952, 'validation/mean_average_precision': 0.2253980118229107, 'validation/num_examples': 43793, 'test/accuracy': 0.9851238131523132, 'test/loss': 0.06284000724554062, 'test/mean_average_precision': 0.22462510513912176, 'test/num_examples': 43793, 'score': 17300.507427692413, 'total_duration': 25869.773553848267, 'accumulated_submission_time': 17300.507427692413, 'accumulated_eval_time': 8565.075862884521, 'accumulated_logging_time': 2.7332370281219482, 'global_step': 53106, 'preemption_count': 0}), (53849, {'train/accuracy': 0.9950389266014099, 'train/loss': 0.01518072560429573, 'train/mean_average_precision': 0.755597599761523, 'validation/accuracy': 0.9861545562744141, 'validation/loss': 0.05877898633480072, 'validation/mean_average_precision': 0.22820302883001248, 'validation/num_examples': 43793, 'test/accuracy': 0.9851983189582825, 'test/loss': 0.06279054284095764, 'test/mean_average_precision': 0.22112249594910546, 'test/num_examples': 43793, 'score': 17540.69450187683, 'total_duration': 26223.728258132935, 'accumulated_submission_time': 17540.69450187683, 'accumulated_eval_time': 8678.787027597427, 'accumulated_logging_time': 2.770055055618286, 'global_step': 53849, 'preemption_count': 0}), (54595, {'train/accuracy': 0.995265007019043, 'train/loss': 0.014640597626566887, 'train/mean_average_precision': 0.7755410902328488, 'validation/accuracy': 0.9860976934432983, 'validation/loss': 0.0594463013112545, 'validation/mean_average_precision': 0.22632009670592218, 'validation/num_examples': 43793, 'test/accuracy': 0.9851747751235962, 'test/loss': 0.06338262557983398, 'test/mean_average_precision': 0.22023480651676589, 'test/num_examples': 43793, 'score': 17780.679956674576, 'total_duration': 26574.07158923149, 'accumulated_submission_time': 17780.679956674576, 'accumulated_eval_time': 8789.08589553833, 'accumulated_logging_time': 2.8089280128479004, 'global_step': 54595, 'preemption_count': 0}), (55339, {'train/accuracy': 0.9959477186203003, 'train/loss': 0.012901960872113705, 'train/mean_average_precision': 0.8156166730274892, 'validation/accuracy': 0.9861111044883728, 'validation/loss': 0.05996517091989517, 'validation/mean_average_precision': 0.22428007329725988, 'validation/num_examples': 43793, 'test/accuracy': 0.9852118492126465, 'test/loss': 0.06408379971981049, 'test/mean_average_precision': 0.2175502037047203, 'test/num_examples': 43793, 'score': 18020.666786193848, 'total_duration': 26924.650710582733, 'accumulated_submission_time': 18020.666786193848, 'accumulated_eval_time': 8899.616615772247, 'accumulated_logging_time': 2.8510804176330566, 'global_step': 55339, 'preemption_count': 0}), (56087, {'train/accuracy': 0.9954010248184204, 'train/loss': 0.014091824181377888, 'train/mean_average_precision': 0.8007883470937304, 'validation/accuracy': 0.9860416650772095, 'validation/loss': 0.06013806164264679, 'validation/mean_average_precision': 0.22281162844515326, 'validation/num_examples': 43793, 'test/accuracy': 0.9850656390190125, 'test/loss': 0.06437241286039352, 'test/mean_average_precision': 0.2164854876489734, 'test/num_examples': 43793, 'score': 18260.690786361694, 'total_duration': 27273.239208221436, 'accumulated_submission_time': 18260.690786361694, 'accumulated_eval_time': 9008.119769573212, 'accumulated_logging_time': 2.8922278881073, 'global_step': 56087, 'preemption_count': 0})], 'global_step': 56756}
I0209 16:37:55.013342 140039251117888 submission_runner.py:586] Timing: 18477.04327273369
I0209 16:37:55.013397 140039251117888 submission_runner.py:588] Total number of evals: 77
I0209 16:37:55.013440 140039251117888 submission_runner.py:589] ====================
I0209 16:37:55.013488 140039251117888 submission_runner.py:542] Using RNG seed 1895687988
I0209 16:37:55.074270 140039251117888 submission_runner.py:551] --- Tuning run 3/5 ---
I0209 16:37:55.074445 140039251117888 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_3/ogbg_jax/trial_3.
I0209 16:37:55.074719 140039251117888 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_3/ogbg_jax/trial_3/hparams.json.
I0209 16:37:55.202463 140039251117888 submission_runner.py:206] Initializing dataset.
I0209 16:37:55.289119 140039251117888 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0209 16:37:55.294567 140039251117888 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0209 16:37:55.429367 140039251117888 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0209 16:37:55.463857 140039251117888 submission_runner.py:213] Initializing model.
I0209 16:37:58.057785 140039251117888 submission_runner.py:255] Initializing optimizer.
I0209 16:37:58.657469 140039251117888 submission_runner.py:262] Initializing metrics bundle.
I0209 16:37:58.657681 140039251117888 submission_runner.py:280] Initializing checkpoint and logger.
I0209 16:37:58.658356 140039251117888 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_3/ogbg_jax/trial_3 with prefix checkpoint_
I0209 16:37:58.658488 140039251117888 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_3/ogbg_jax/trial_3/meta_data_0.json.
I0209 16:37:58.658708 140039251117888 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0209 16:37:58.658769 140039251117888 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0209 16:38:00.261094 140039251117888 logger_utils.py:220] Unable to record git information. Continuing without it.
I0209 16:38:01.928648 140039251117888 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_3/ogbg_jax/trial_3/flags_0.json.
I0209 16:38:01.932777 140039251117888 submission_runner.py:314] Starting training loop.
I0209 16:38:15.602782 139836617578240 logging_writer.py:48] [0] global_step=0, grad_norm=2.7368390560150146, loss=0.7145252823829651
I0209 16:38:15.610741 140039251117888 spec.py:321] Evaluating on the training split.
I0209 16:39:58.472832 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 16:40:01.686502 140039251117888 spec.py:349] Evaluating on the test split.
I0209 16:40:04.683721 140039251117888 submission_runner.py:408] Time since start: 122.75s, 	Step: 1, 	{'train/accuracy': 0.5250248908996582, 'train/loss': 0.7151214480400085, 'train/mean_average_precision': 0.022596344481242144, 'validation/accuracy': 0.5213832855224609, 'validation/loss': 0.7166012525558472, 'validation/mean_average_precision': 0.026142361220453977, 'validation/num_examples': 43793, 'test/accuracy': 0.5224818587303162, 'test/loss': 0.7161954641342163, 'test/mean_average_precision': 0.02783086509595883, 'test/num_examples': 43793, 'score': 13.67792797088623, 'total_duration': 122.75087356567383, 'accumulated_submission_time': 13.67792797088623, 'accumulated_eval_time': 109.07290530204773, 'accumulated_logging_time': 0}
I0209 16:40:04.693641 139836634785536 logging_writer.py:48] [1] accumulated_eval_time=109.072905, accumulated_logging_time=0, accumulated_submission_time=13.677928, global_step=1, preemption_count=0, score=13.677928, test/accuracy=0.522482, test/loss=0.716195, test/mean_average_precision=0.027831, test/num_examples=43793, total_duration=122.750874, train/accuracy=0.525025, train/loss=0.715121, train/mean_average_precision=0.022596, validation/accuracy=0.521383, validation/loss=0.716601, validation/mean_average_precision=0.026142, validation/num_examples=43793
I0209 16:40:36.915654 139871926159104 logging_writer.py:48] [100] global_step=100, grad_norm=0.6021668910980225, loss=0.42064476013183594
I0209 16:41:09.636899 139836634785536 logging_writer.py:48] [200] global_step=200, grad_norm=0.3521549701690674, loss=0.3157305419445038
I0209 16:41:41.837096 139871926159104 logging_writer.py:48] [300] global_step=300, grad_norm=0.25684401392936707, loss=0.2294500768184662
I0209 16:42:14.014268 139836634785536 logging_writer.py:48] [400] global_step=400, grad_norm=0.1691627949476242, loss=0.1607891470193863
I0209 16:42:46.088350 139871926159104 logging_writer.py:48] [500] global_step=500, grad_norm=0.1068229228258133, loss=0.11055862158536911
I0209 16:43:18.162726 139836634785536 logging_writer.py:48] [600] global_step=600, grad_norm=0.06593774259090424, loss=0.08887927979230881
I0209 16:43:50.472781 139871926159104 logging_writer.py:48] [700] global_step=700, grad_norm=0.04725298658013344, loss=0.06710343807935715
I0209 16:44:04.903389 140039251117888 spec.py:321] Evaluating on the training split.
I0209 16:45:51.526483 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 16:45:54.632221 140039251117888 spec.py:349] Evaluating on the test split.
I0209 16:45:57.583660 140039251117888 submission_runner.py:408] Time since start: 475.65s, 	Step: 746, 	{'train/accuracy': 0.9866614937782288, 'train/loss': 0.06891448050737381, 'train/mean_average_precision': 0.03438290407203851, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.07732292264699936, 'validation/mean_average_precision': 0.036815222944384746, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.08021833002567291, 'test/mean_average_precision': 0.03825316566172733, 'test/num_examples': 43793, 'score': 253.85704374313354, 'total_duration': 475.6508138179779, 'accumulated_submission_time': 253.85704374313354, 'accumulated_eval_time': 221.7531237602234, 'accumulated_logging_time': 0.02077651023864746}
I0209 16:45:57.599075 139836244932352 logging_writer.py:48] [746] accumulated_eval_time=221.753124, accumulated_logging_time=0.020777, accumulated_submission_time=253.857044, global_step=746, preemption_count=0, score=253.857044, test/accuracy=0.983142, test/loss=0.080218, test/mean_average_precision=0.038253, test/num_examples=43793, total_duration=475.650814, train/accuracy=0.986661, train/loss=0.068914, train/mean_average_precision=0.034383, validation/accuracy=0.984118, validation/loss=0.077323, validation/mean_average_precision=0.036815, validation/num_examples=43793
I0209 16:46:15.395334 139836643178240 logging_writer.py:48] [800] global_step=800, grad_norm=0.03477175161242485, loss=0.06740331649780273
I0209 16:46:47.676341 139836244932352 logging_writer.py:48] [900] global_step=900, grad_norm=0.21892505884170532, loss=0.06215877830982208
I0209 16:47:19.901621 139836643178240 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.07010455429553986, loss=0.05902601033449173
I0209 16:47:52.021841 139836244932352 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.2174903005361557, loss=0.0557107999920845
I0209 16:48:24.169865 139836643178240 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.2338400036096573, loss=0.05258088931441307
I0209 16:48:56.313422 139836244932352 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.190297469496727, loss=0.04987623542547226
I0209 16:49:28.727407 139836643178240 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.18845021724700928, loss=0.046419527381658554
I0209 16:49:57.588929 140039251117888 spec.py:321] Evaluating on the training split.
I0209 16:51:38.527340 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 16:51:41.587380 140039251117888 spec.py:349] Evaluating on the test split.
I0209 16:51:44.558851 140039251117888 submission_runner.py:408] Time since start: 822.63s, 	Step: 1491, 	{'train/accuracy': 0.9872501492500305, 'train/loss': 0.04855059087276459, 'train/mean_average_precision': 0.08156753726605445, 'validation/accuracy': 0.9845539331436157, 'validation/loss': 0.05826849862933159, 'validation/mean_average_precision': 0.08706500771159309, 'validation/num_examples': 43793, 'test/accuracy': 0.9835514426231384, 'test/loss': 0.06154114007949829, 'test/mean_average_precision': 0.09099659746976167, 'test/num_examples': 43793, 'score': 493.8156957626343, 'total_duration': 822.6260054111481, 'accumulated_submission_time': 493.8156957626343, 'accumulated_eval_time': 328.72299671173096, 'accumulated_logging_time': 0.047609567642211914}
I0209 16:51:44.574306 139836634785536 logging_writer.py:48] [1491] accumulated_eval_time=328.722997, accumulated_logging_time=0.047610, accumulated_submission_time=493.815696, global_step=1491, preemption_count=0, score=493.815696, test/accuracy=0.983551, test/loss=0.061541, test/mean_average_precision=0.090997, test/num_examples=43793, total_duration=822.626005, train/accuracy=0.987250, train/loss=0.048551, train/mean_average_precision=0.081568, validation/accuracy=0.984554, validation/loss=0.058268, validation/mean_average_precision=0.087065, validation/num_examples=43793
I0209 16:51:47.829175 139871926159104 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.3349249064922333, loss=0.043733105063438416
I0209 16:52:19.894642 139836634785536 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.5290166139602661, loss=0.04891757294535637
I0209 16:52:51.596422 139871926159104 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.2435990869998932, loss=0.05035387724637985
I0209 16:53:23.549533 139836634785536 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.221307635307312, loss=0.048513248562812805
I0209 16:53:55.171484 139871926159104 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.08006981015205383, loss=0.048552580177783966
I0209 16:54:27.524607 139836634785536 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.22695930302143097, loss=0.04370696470141411
I0209 16:54:59.326779 139871926159104 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.13584576547145844, loss=0.0401034839451313
I0209 16:55:31.393167 139836634785536 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.20477715134620667, loss=0.04403557628393173
I0209 16:55:44.690737 140039251117888 spec.py:321] Evaluating on the training split.
I0209 16:57:30.233392 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 16:57:33.236071 140039251117888 spec.py:349] Evaluating on the test split.
I0209 16:57:36.224555 140039251117888 submission_runner.py:408] Time since start: 1174.29s, 	Step: 2243, 	{'train/accuracy': 0.9877920150756836, 'train/loss': 0.04368261992931366, 'train/mean_average_precision': 0.1433774872818228, 'validation/accuracy': 0.9850483536720276, 'validation/loss': 0.05292936787009239, 'validation/mean_average_precision': 0.1361727300785612, 'validation/num_examples': 43793, 'test/accuracy': 0.9840973615646362, 'test/loss': 0.055955272167921066, 'test/mean_average_precision': 0.1340190769220492, 'test/num_examples': 43793, 'score': 733.8995008468628, 'total_duration': 1174.291626214981, 'accumulated_submission_time': 733.8995008468628, 'accumulated_eval_time': 440.25667667388916, 'accumulated_logging_time': 0.07578086853027344}
I0209 16:57:36.239924 139836643178240 logging_writer.py:48] [2243] accumulated_eval_time=440.256677, accumulated_logging_time=0.075781, accumulated_submission_time=733.899501, global_step=2243, preemption_count=0, score=733.899501, test/accuracy=0.984097, test/loss=0.055955, test/mean_average_precision=0.134019, test/num_examples=43793, total_duration=1174.291626, train/accuracy=0.987792, train/loss=0.043683, train/mean_average_precision=0.143377, validation/accuracy=0.985048, validation/loss=0.052929, validation/mean_average_precision=0.136173, validation/num_examples=43793
I0209 16:57:54.860941 139864233322240 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.13167384266853333, loss=0.040757182985544205
I0209 16:58:27.047642 139836643178240 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.066449835896492, loss=0.04290499910712242
I0209 16:58:59.054174 139864233322240 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.07769976556301117, loss=0.03792320936918259
I0209 16:59:30.935077 139836643178240 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.11209747195243835, loss=0.04507673159241676
I0209 17:00:02.829050 139864233322240 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.1087379902601242, loss=0.039019763469696045
I0209 17:00:34.754620 139836643178240 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.09168805927038193, loss=0.038135044276714325
I0209 17:01:07.022012 139864233322240 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.12950420379638672, loss=0.038485657423734665
I0209 17:01:36.418778 140039251117888 spec.py:321] Evaluating on the training split.
I0209 17:03:20.117002 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 17:03:23.467909 140039251117888 spec.py:349] Evaluating on the test split.
I0209 17:03:26.806164 140039251117888 submission_runner.py:408] Time since start: 1524.87s, 	Step: 2990, 	{'train/accuracy': 0.9879328012466431, 'train/loss': 0.04259060323238373, 'train/mean_average_precision': 0.1625567684371191, 'validation/accuracy': 0.9850613474845886, 'validation/loss': 0.05261097848415375, 'validation/mean_average_precision': 0.1509572942259753, 'validation/num_examples': 43793, 'test/accuracy': 0.9841592311859131, 'test/loss': 0.05536371469497681, 'test/mean_average_precision': 0.15288593216783689, 'test/num_examples': 43793, 'score': 974.0467576980591, 'total_duration': 1524.873197555542, 'accumulated_submission_time': 974.0467576980591, 'accumulated_eval_time': 550.643904209137, 'accumulated_logging_time': 0.10276627540588379}
I0209 17:03:26.824855 139836244932352 logging_writer.py:48] [2990] accumulated_eval_time=550.643904, accumulated_logging_time=0.102766, accumulated_submission_time=974.046758, global_step=2990, preemption_count=0, score=974.046758, test/accuracy=0.984159, test/loss=0.055364, test/mean_average_precision=0.152886, test/num_examples=43793, total_duration=1524.873198, train/accuracy=0.987933, train/loss=0.042591, train/mean_average_precision=0.162557, validation/accuracy=0.985061, validation/loss=0.052611, validation/mean_average_precision=0.150957, validation/num_examples=43793
I0209 17:03:30.489655 139871926159104 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.10263752937316895, loss=0.0436277911067009
I0209 17:04:03.584326 139836244932352 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.07696136832237244, loss=0.043589163571596146
I0209 17:04:35.704917 139871926159104 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.09310426563024521, loss=0.03825979679822922
I0209 17:05:07.889426 139836244932352 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.14268948137760162, loss=0.04089939966797829
I0209 17:05:40.519493 139871926159104 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.09506090730428696, loss=0.040460094809532166
I0209 17:06:12.826435 139836244932352 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.0665392130613327, loss=0.04140806943178177
I0209 17:06:44.979190 139871926159104 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.15273423492908478, loss=0.03712453320622444
I0209 17:07:17.363033 139836244932352 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.17528192698955536, loss=0.04204057157039642
I0209 17:07:26.862729 140039251117888 spec.py:321] Evaluating on the training split.
I0209 17:09:10.070775 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 17:09:13.140927 140039251117888 spec.py:349] Evaluating on the test split.
I0209 17:09:16.153488 140039251117888 submission_runner.py:408] Time since start: 1874.22s, 	Step: 3731, 	{'train/accuracy': 0.9883593916893005, 'train/loss': 0.0405401736497879, 'train/mean_average_precision': 0.1877052384465054, 'validation/accuracy': 0.9855139851570129, 'validation/loss': 0.050175122916698456, 'validation/mean_average_precision': 0.1707993933649933, 'validation/num_examples': 43793, 'test/accuracy': 0.9846293330192566, 'test/loss': 0.052826281636953354, 'test/mean_average_precision': 0.1723300618497228, 'test/num_examples': 43793, 'score': 1214.0511775016785, 'total_duration': 1874.2206497192383, 'accumulated_submission_time': 1214.0511775016785, 'accumulated_eval_time': 659.9346287250519, 'accumulated_logging_time': 0.13471651077270508}
I0209 17:09:16.169763 139836634785536 logging_writer.py:48] [3731] accumulated_eval_time=659.934629, accumulated_logging_time=0.134717, accumulated_submission_time=1214.051178, global_step=3731, preemption_count=0, score=1214.051178, test/accuracy=0.984629, test/loss=0.052826, test/mean_average_precision=0.172330, test/num_examples=43793, total_duration=1874.220650, train/accuracy=0.988359, train/loss=0.040540, train/mean_average_precision=0.187705, validation/accuracy=0.985514, validation/loss=0.050175, validation/mean_average_precision=0.170799, validation/num_examples=43793
I0209 17:09:40.017849 139836643178240 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.09811727702617645, loss=0.039996519684791565
I0209 17:10:11.963646 139836634785536 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.05650131404399872, loss=0.042116712778806686
I0209 17:10:44.312173 139836643178240 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.11763214319944382, loss=0.04279903322458267
I0209 17:11:16.763573 139836634785536 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.15791238844394684, loss=0.03723100945353508
I0209 17:11:49.130092 139836643178240 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.0901605561375618, loss=0.04330837354063988
I0209 17:12:21.475140 139836634785536 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.05498332902789116, loss=0.04076453670859337
I0209 17:12:53.719788 139836643178240 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.07852218300104141, loss=0.033192187547683716
I0209 17:13:16.326080 140039251117888 spec.py:321] Evaluating on the training split.
I0209 17:14:59.144810 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 17:15:02.395581 140039251117888 spec.py:349] Evaluating on the test split.
I0209 17:15:05.466915 140039251117888 submission_runner.py:408] Time since start: 2223.53s, 	Step: 4471, 	{'train/accuracy': 0.9885321259498596, 'train/loss': 0.03950120508670807, 'train/mean_average_precision': 0.21052214180823317, 'validation/accuracy': 0.9856792092323303, 'validation/loss': 0.04872706159949303, 'validation/mean_average_precision': 0.1853128791746986, 'validation/num_examples': 43793, 'test/accuracy': 0.9848723411560059, 'test/loss': 0.05121447890996933, 'test/mean_average_precision': 0.18539543653180943, 'test/num_examples': 43793, 'score': 1454.1758043766022, 'total_duration': 2223.534078359604, 'accumulated_submission_time': 1454.1758043766022, 'accumulated_eval_time': 769.0754227638245, 'accumulated_logging_time': 0.16344189643859863}
I0209 17:15:05.483128 139864233322240 logging_writer.py:48] [4471] accumulated_eval_time=769.075423, accumulated_logging_time=0.163442, accumulated_submission_time=1454.175804, global_step=4471, preemption_count=0, score=1454.175804, test/accuracy=0.984872, test/loss=0.051214, test/mean_average_precision=0.185395, test/num_examples=43793, total_duration=2223.534078, train/accuracy=0.988532, train/loss=0.039501, train/mean_average_precision=0.210522, validation/accuracy=0.985679, validation/loss=0.048727, validation/mean_average_precision=0.185313, validation/num_examples=43793
I0209 17:15:16.042299 139871926159104 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.06279481202363968, loss=0.040605347603559494
I0209 17:15:48.198200 139864233322240 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.05262603238224983, loss=0.04353315755724907
I0209 17:16:20.492651 139871926159104 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.053227655589580536, loss=0.0429798848927021
I0209 17:16:52.500684 139864233322240 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.05944386124610901, loss=0.04009350389242172
I0209 17:17:25.036453 139871926159104 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.055763375014066696, loss=0.0372268445789814
I0209 17:17:57.010814 139864233322240 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.05390160530805588, loss=0.037046320736408234
I0209 17:18:28.995807 139871926159104 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.16606782376766205, loss=0.04021096229553223
I0209 17:19:01.263569 139864233322240 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.06258808821439743, loss=0.039707787334918976
I0209 17:19:05.501397 140039251117888 spec.py:321] Evaluating on the training split.
I0209 17:20:47.094566 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 17:20:50.134149 140039251117888 spec.py:349] Evaluating on the test split.
I0209 17:20:53.074836 140039251117888 submission_runner.py:408] Time since start: 2571.14s, 	Step: 5214, 	{'train/accuracy': 0.9887311458587646, 'train/loss': 0.038518089801073074, 'train/mean_average_precision': 0.2226331766038031, 'validation/accuracy': 0.9857717156410217, 'validation/loss': 0.048647440969944, 'validation/mean_average_precision': 0.18992241756155326, 'validation/num_examples': 43793, 'test/accuracy': 0.9848892092704773, 'test/loss': 0.051187995821237564, 'test/mean_average_precision': 0.1917954177815328, 'test/num_examples': 43793, 'score': 1694.1631109714508, 'total_duration': 2571.1419973373413, 'accumulated_submission_time': 1694.1631109714508, 'accumulated_eval_time': 876.6488153934479, 'accumulated_logging_time': 0.19112181663513184}
I0209 17:20:53.090974 139836244932352 logging_writer.py:48] [5214] accumulated_eval_time=876.648815, accumulated_logging_time=0.191122, accumulated_submission_time=1694.163111, global_step=5214, preemption_count=0, score=1694.163111, test/accuracy=0.984889, test/loss=0.051188, test/mean_average_precision=0.191795, test/num_examples=43793, total_duration=2571.141997, train/accuracy=0.988731, train/loss=0.038518, train/mean_average_precision=0.222633, validation/accuracy=0.985772, validation/loss=0.048647, validation/mean_average_precision=0.189922, validation/num_examples=43793
I0209 17:21:20.882032 139836634785536 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.07562118023633957, loss=0.03747396543622017
I0209 17:21:52.940265 139836244932352 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.052678294479846954, loss=0.03662889450788498
I0209 17:22:25.320590 139836634785536 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.05905289575457573, loss=0.0410391166806221
I0209 17:22:56.906970 139836244932352 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.06188621371984482, loss=0.03470133990049362
I0209 17:23:29.681519 139836634785536 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.055679164826869965, loss=0.038081683218479156
I0209 17:24:02.717589 139836244932352 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.061589937657117844, loss=0.035124585032463074
I0209 17:24:34.956357 139836634785536 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.08357498049736023, loss=0.03804251179099083
I0209 17:24:53.235125 140039251117888 spec.py:321] Evaluating on the training split.
I0209 17:26:36.079453 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 17:26:39.109401 140039251117888 spec.py:349] Evaluating on the test split.
I0209 17:26:42.109788 140039251117888 submission_runner.py:408] Time since start: 2920.18s, 	Step: 5958, 	{'train/accuracy': 0.9891310334205627, 'train/loss': 0.037137195467948914, 'train/mean_average_precision': 0.24559895721568495, 'validation/accuracy': 0.9858813285827637, 'validation/loss': 0.04740628972649574, 'validation/mean_average_precision': 0.19940986093233645, 'validation/num_examples': 43793, 'test/accuracy': 0.9849742650985718, 'test/loss': 0.05000431835651398, 'test/mean_average_precision': 0.20099796466059375, 'test/num_examples': 43793, 'score': 1934.2749044895172, 'total_duration': 2920.176949262619, 'accumulated_submission_time': 1934.2749044895172, 'accumulated_eval_time': 985.5234348773956, 'accumulated_logging_time': 0.21864557266235352}
I0209 17:26:42.126269 139864233322240 logging_writer.py:48] [5958] accumulated_eval_time=985.523435, accumulated_logging_time=0.218646, accumulated_submission_time=1934.274904, global_step=5958, preemption_count=0, score=1934.274904, test/accuracy=0.984974, test/loss=0.050004, test/mean_average_precision=0.200998, test/num_examples=43793, total_duration=2920.176949, train/accuracy=0.989131, train/loss=0.037137, train/mean_average_precision=0.245599, validation/accuracy=0.985881, validation/loss=0.047406, validation/mean_average_precision=0.199410, validation/num_examples=43793
I0209 17:26:56.101515 139976540944128 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.0400758795440197, loss=0.0361320823431015
I0209 17:27:28.098572 139864233322240 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.042878150939941406, loss=0.039033807814121246
I0209 17:28:00.111703 139976540944128 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.09314923733472824, loss=0.03779039904475212
I0209 17:28:32.025520 139864233322240 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.06799513846635818, loss=0.03817063197493553
I0209 17:29:03.883630 139976540944128 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.0463273786008358, loss=0.036061324179172516
I0209 17:29:35.963068 139864233322240 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.03874628618359566, loss=0.034776367247104645
I0209 17:30:08.234027 139976540944128 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.03790998086333275, loss=0.03906014561653137
I0209 17:30:40.415162 139864233322240 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.04153526946902275, loss=0.038674160838127136
I0209 17:30:42.317451 140039251117888 spec.py:321] Evaluating on the training split.
I0209 17:32:24.392912 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 17:32:27.432599 140039251117888 spec.py:349] Evaluating on the test split.
I0209 17:32:30.417336 140039251117888 submission_runner.py:408] Time since start: 3268.48s, 	Step: 6707, 	{'train/accuracy': 0.9892842769622803, 'train/loss': 0.03652196750044823, 'train/mean_average_precision': 0.26034770998253665, 'validation/accuracy': 0.9860348105430603, 'validation/loss': 0.04685652628540993, 'validation/mean_average_precision': 0.2105766077737178, 'validation/num_examples': 43793, 'test/accuracy': 0.9851874113082886, 'test/loss': 0.049331918358802795, 'test/mean_average_precision': 0.21646928780084507, 'test/num_examples': 43793, 'score': 2174.433957338333, 'total_duration': 3268.484499692917, 'accumulated_submission_time': 2174.433957338333, 'accumulated_eval_time': 1093.623272895813, 'accumulated_logging_time': 0.24747896194458008}
I0209 17:32:30.433696 139836643178240 logging_writer.py:48] [6707] accumulated_eval_time=1093.623273, accumulated_logging_time=0.247479, accumulated_submission_time=2174.433957, global_step=6707, preemption_count=0, score=2174.433957, test/accuracy=0.985187, test/loss=0.049332, test/mean_average_precision=0.216469, test/num_examples=43793, total_duration=3268.484500, train/accuracy=0.989284, train/loss=0.036522, train/mean_average_precision=0.260348, validation/accuracy=0.986035, validation/loss=0.046857, validation/mean_average_precision=0.210577, validation/num_examples=43793
I0209 17:33:00.922973 139878399788800 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.060686469078063965, loss=0.03917970508337021
I0209 17:33:33.204109 139836643178240 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.11329228430986404, loss=0.041014667600393295
I0209 17:34:05.605381 139878399788800 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.0393872894346714, loss=0.03443459793925285
I0209 17:34:38.233413 139836643178240 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.039499688893556595, loss=0.03765525296330452
I0209 17:35:11.043999 139878399788800 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.034650471061468124, loss=0.03452633321285248
I0209 17:35:42.926905 139836643178240 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.03333950787782669, loss=0.03677890822291374
I0209 17:36:15.495198 139878399788800 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.03081669844686985, loss=0.03154636546969414
I0209 17:36:30.497074 140039251117888 spec.py:321] Evaluating on the training split.
I0209 17:38:11.761554 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 17:38:14.879555 140039251117888 spec.py:349] Evaluating on the test split.
I0209 17:38:17.878854 140039251117888 submission_runner.py:408] Time since start: 3615.95s, 	Step: 7447, 	{'train/accuracy': 0.9892475008964539, 'train/loss': 0.03626396134495735, 'train/mean_average_precision': 0.27339726794886604, 'validation/accuracy': 0.9859694242477417, 'validation/loss': 0.04703037440776825, 'validation/mean_average_precision': 0.2144172040557444, 'validation/num_examples': 43793, 'test/accuracy': 0.9851532578468323, 'test/loss': 0.04955512657761574, 'test/mean_average_precision': 0.21660488054132188, 'test/num_examples': 43793, 'score': 2414.466232776642, 'total_duration': 3615.946009159088, 'accumulated_submission_time': 2414.466232776642, 'accumulated_eval_time': 1201.0050013065338, 'accumulated_logging_time': 0.2745974063873291}
I0209 17:38:17.895555 139864233322240 logging_writer.py:48] [7447] accumulated_eval_time=1201.005001, accumulated_logging_time=0.274597, accumulated_submission_time=2414.466233, global_step=7447, preemption_count=0, score=2414.466233, test/accuracy=0.985153, test/loss=0.049555, test/mean_average_precision=0.216605, test/num_examples=43793, total_duration=3615.946009, train/accuracy=0.989248, train/loss=0.036264, train/mean_average_precision=0.273397, validation/accuracy=0.985969, validation/loss=0.047030, validation/mean_average_precision=0.214417, validation/num_examples=43793
I0209 17:38:35.120672 139871926159104 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.02246668003499508, loss=0.036551136523485184
I0209 17:39:07.243614 139864233322240 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.03839017078280449, loss=0.03457469493150711
I0209 17:39:39.221547 139871926159104 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.04590851441025734, loss=0.041556138545274734
I0209 17:40:11.301106 139864233322240 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.05506828799843788, loss=0.03849048912525177
I0209 17:40:43.157533 139871926159104 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.02591008134186268, loss=0.03533487021923065
I0209 17:41:14.924505 139864233322240 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.03597651422023773, loss=0.03743691369891167
I0209 17:41:47.232515 139871926159104 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.038401585072278976, loss=0.040893200784921646
I0209 17:42:18.072452 140039251117888 spec.py:321] Evaluating on the training split.
I0209 17:43:58.450133 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 17:44:01.761463 140039251117888 spec.py:349] Evaluating on the test split.
I0209 17:44:04.794879 140039251117888 submission_runner.py:408] Time since start: 3962.86s, 	Step: 8197, 	{'train/accuracy': 0.9892252683639526, 'train/loss': 0.03638733550906181, 'train/mean_average_precision': 0.27329335983624375, 'validation/accuracy': 0.9859321117401123, 'validation/loss': 0.04655333608388901, 'validation/mean_average_precision': 0.2161812692848252, 'validation/num_examples': 43793, 'test/accuracy': 0.9851823449134827, 'test/loss': 0.04896998032927513, 'test/mean_average_precision': 0.2174333997986895, 'test/num_examples': 43793, 'score': 2654.612272977829, 'total_duration': 3962.862035751343, 'accumulated_submission_time': 2654.612272977829, 'accumulated_eval_time': 1307.7273774147034, 'accumulated_logging_time': 0.30209875106811523}
I0209 17:44:04.812393 139836643178240 logging_writer.py:48] [8197] accumulated_eval_time=1307.727377, accumulated_logging_time=0.302099, accumulated_submission_time=2654.612273, global_step=8197, preemption_count=0, score=2654.612273, test/accuracy=0.985182, test/loss=0.048970, test/mean_average_precision=0.217433, test/num_examples=43793, total_duration=3962.862036, train/accuracy=0.989225, train/loss=0.036387, train/mean_average_precision=0.273293, validation/accuracy=0.985932, validation/loss=0.046553, validation/mean_average_precision=0.216181, validation/num_examples=43793
I0209 17:44:06.138711 139878399788800 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.023741023615002632, loss=0.03503423184156418
I0209 17:44:38.187172 139836643178240 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.02861127071082592, loss=0.03826438635587692
I0209 17:45:10.321722 139878399788800 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.028115548193454742, loss=0.036017850041389465
I0209 17:45:42.213965 139836643178240 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.03788206726312637, loss=0.03430976718664169
I0209 17:46:14.588013 139878399788800 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.037346795201301575, loss=0.03702673316001892
I0209 17:46:46.540351 139836643178240 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.028592968359589577, loss=0.03628683090209961
I0209 17:47:18.795688 139878399788800 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.03119271621108055, loss=0.034487999975681305
I0209 17:47:50.756803 139836643178240 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.030570698902010918, loss=0.04058389365673065
I0209 17:48:04.926139 140039251117888 spec.py:321] Evaluating on the training split.
I0209 17:49:50.516471 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 17:49:53.661563 140039251117888 spec.py:349] Evaluating on the test split.
I0209 17:49:56.718574 140039251117888 submission_runner.py:408] Time since start: 4314.79s, 	Step: 8944, 	{'train/accuracy': 0.989263653755188, 'train/loss': 0.03592751920223236, 'train/mean_average_precision': 0.2766664071539449, 'validation/accuracy': 0.9862779378890991, 'validation/loss': 0.046199049800634384, 'validation/mean_average_precision': 0.2300533427368359, 'validation/num_examples': 43793, 'test/accuracy': 0.9854089617729187, 'test/loss': 0.04881441593170166, 'test/mean_average_precision': 0.22253519226643648, 'test/num_examples': 43793, 'score': 2894.6948940753937, 'total_duration': 4314.785717964172, 'accumulated_submission_time': 2894.6948940753937, 'accumulated_eval_time': 1419.5197608470917, 'accumulated_logging_time': 0.3305845260620117}
I0209 17:49:56.736722 139871926159104 logging_writer.py:48] [8944] accumulated_eval_time=1419.519761, accumulated_logging_time=0.330585, accumulated_submission_time=2894.694894, global_step=8944, preemption_count=0, score=2894.694894, test/accuracy=0.985409, test/loss=0.048814, test/mean_average_precision=0.222535, test/num_examples=43793, total_duration=4314.785718, train/accuracy=0.989264, train/loss=0.035928, train/mean_average_precision=0.276666, validation/accuracy=0.986278, validation/loss=0.046199, validation/mean_average_precision=0.230053, validation/num_examples=43793
I0209 17:50:15.209033 139976540944128 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.03298572078347206, loss=0.03547833487391472
I0209 17:50:47.119357 139871926159104 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.030820487067103386, loss=0.03639422357082367
I0209 17:51:19.596184 139976540944128 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.03660054877400398, loss=0.03148846700787544
I0209 17:51:51.917507 139871926159104 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.023644644767045975, loss=0.03526315838098526
I0209 17:52:24.313888 139976540944128 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.03133497014641762, loss=0.032986532896757126
I0209 17:52:56.314955 139871926159104 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.02202782593667507, loss=0.02994171902537346
I0209 17:53:28.489918 139976540944128 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.03074723668396473, loss=0.03554670140147209
I0209 17:53:56.925747 140039251117888 spec.py:321] Evaluating on the training split.
I0209 17:55:39.767385 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 17:55:42.889662 140039251117888 spec.py:349] Evaluating on the test split.
I0209 17:55:45.913392 140039251117888 submission_runner.py:408] Time since start: 4663.98s, 	Step: 9690, 	{'train/accuracy': 0.9895308017730713, 'train/loss': 0.03510921075940132, 'train/mean_average_precision': 0.29700261784276705, 'validation/accuracy': 0.9863014817237854, 'validation/loss': 0.04545333608984947, 'validation/mean_average_precision': 0.23079749033007863, 'validation/num_examples': 43793, 'test/accuracy': 0.9854245185852051, 'test/loss': 0.048140157014131546, 'test/mean_average_precision': 0.23213402212776624, 'test/num_examples': 43793, 'score': 3134.852992296219, 'total_duration': 4663.980555534363, 'accumulated_submission_time': 3134.852992296219, 'accumulated_eval_time': 1528.5073668956757, 'accumulated_logging_time': 0.35982394218444824}
I0209 17:55:45.930711 139836643178240 logging_writer.py:48] [9690] accumulated_eval_time=1528.507367, accumulated_logging_time=0.359824, accumulated_submission_time=3134.852992, global_step=9690, preemption_count=0, score=3134.852992, test/accuracy=0.985425, test/loss=0.048140, test/mean_average_precision=0.232134, test/num_examples=43793, total_duration=4663.980556, train/accuracy=0.989531, train/loss=0.035109, train/mean_average_precision=0.297003, validation/accuracy=0.986301, validation/loss=0.045453, validation/mean_average_precision=0.230797, validation/num_examples=43793
I0209 17:55:49.498208 139864233322240 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.04949449375271797, loss=0.03594881668686867
I0209 17:56:21.521488 139836643178240 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.04312318190932274, loss=0.03520254045724869
I0209 17:56:53.102735 139864233322240 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.03042864426970482, loss=0.03554118797183037
I0209 17:57:25.189591 139836643178240 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.04196147993206978, loss=0.033187080174684525
I0209 17:57:56.976329 139864233322240 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.032536596059799194, loss=0.0379607193171978
I0209 17:58:28.965474 139836643178240 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.04646234214305878, loss=0.03242500126361847
I0209 17:59:00.841758 139864233322240 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.028074190020561218, loss=0.038292232900857925
I0209 17:59:32.743138 139836643178240 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.05046294629573822, loss=0.03353298828005791
I0209 17:59:46.100175 140039251117888 spec.py:321] Evaluating on the training split.
I0209 18:01:25.551715 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 18:01:28.685906 140039251117888 spec.py:349] Evaluating on the test split.
I0209 18:01:31.750321 140039251117888 submission_runner.py:408] Time since start: 5009.82s, 	Step: 10443, 	{'train/accuracy': 0.9899264574050903, 'train/loss': 0.03395417705178261, 'train/mean_average_precision': 0.32117851971602984, 'validation/accuracy': 0.9864752292633057, 'validation/loss': 0.04525930806994438, 'validation/mean_average_precision': 0.2440599289904552, 'validation/num_examples': 43793, 'test/accuracy': 0.9855690002441406, 'test/loss': 0.04802022874355316, 'test/mean_average_precision': 0.24045128253413328, 'test/num_examples': 43793, 'score': 3374.9913704395294, 'total_duration': 5009.81748175621, 'accumulated_submission_time': 3374.9913704395294, 'accumulated_eval_time': 1634.1574666500092, 'accumulated_logging_time': 0.3879969120025635}
I0209 18:01:31.768083 139878399788800 logging_writer.py:48] [10443] accumulated_eval_time=1634.157467, accumulated_logging_time=0.387997, accumulated_submission_time=3374.991370, global_step=10443, preemption_count=0, score=3374.991370, test/accuracy=0.985569, test/loss=0.048020, test/mean_average_precision=0.240451, test/num_examples=43793, total_duration=5009.817482, train/accuracy=0.989926, train/loss=0.033954, train/mean_average_precision=0.321179, validation/accuracy=0.986475, validation/loss=0.045259, validation/mean_average_precision=0.244060, validation/num_examples=43793
I0209 18:01:50.228425 139976540944128 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.025333819910883904, loss=0.03372696787118912
I0209 18:02:22.284966 139878399788800 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.04094256833195686, loss=0.03610960394144058
I0209 18:02:54.081506 139976540944128 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.0382661297917366, loss=0.03455401957035065
I0209 18:03:26.489274 139878399788800 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.05882706493139267, loss=0.03240359202027321
I0209 18:03:59.013705 139976540944128 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.05221004784107208, loss=0.038215503096580505
I0209 18:04:30.975053 139878399788800 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.027165470644831657, loss=0.03522828593850136
I0209 18:05:03.667859 139976540944128 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.03619258478283882, loss=0.032407213002443314
I0209 18:05:31.756861 140039251117888 spec.py:321] Evaluating on the training split.
I0209 18:07:12.410432 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 18:07:15.422436 140039251117888 spec.py:349] Evaluating on the test split.
I0209 18:07:18.421066 140039251117888 submission_runner.py:408] Time since start: 5356.49s, 	Step: 11189, 	{'train/accuracy': 0.9897028207778931, 'train/loss': 0.03408277779817581, 'train/mean_average_precision': 0.3120466220388052, 'validation/accuracy': 0.9860798716545105, 'validation/loss': 0.04593156278133392, 'validation/mean_average_precision': 0.23448836837061954, 'validation/num_examples': 43793, 'test/accuracy': 0.9851945638656616, 'test/loss': 0.04870639741420746, 'test/mean_average_precision': 0.22935470260638427, 'test/num_examples': 43793, 'score': 3614.948825597763, 'total_duration': 5356.488221406937, 'accumulated_submission_time': 3614.948825597763, 'accumulated_eval_time': 1740.821620941162, 'accumulated_logging_time': 0.41780829429626465}
I0209 18:07:18.438373 139864233322240 logging_writer.py:48] [11189] accumulated_eval_time=1740.821621, accumulated_logging_time=0.417808, accumulated_submission_time=3614.948826, global_step=11189, preemption_count=0, score=3614.948826, test/accuracy=0.985195, test/loss=0.048706, test/mean_average_precision=0.229355, test/num_examples=43793, total_duration=5356.488221, train/accuracy=0.989703, train/loss=0.034083, train/mean_average_precision=0.312047, validation/accuracy=0.986080, validation/loss=0.045932, validation/mean_average_precision=0.234488, validation/num_examples=43793
I0209 18:07:22.297405 139871926159104 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.03654204308986664, loss=0.03467130661010742
I0209 18:07:53.948833 139864233322240 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.04492636024951935, loss=0.03561580181121826
I0209 18:08:25.967823 139871926159104 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.0361974723637104, loss=0.03640422597527504
I0209 18:08:57.812952 139864233322240 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.029020054265856743, loss=0.033831968903541565
I0209 18:09:29.831607 139871926159104 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.039071302860975266, loss=0.03302846848964691
I0209 18:10:02.294014 139864233322240 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.04401721805334091, loss=0.031303584575653076
I0209 18:10:34.604576 139871926159104 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.03423276171088219, loss=0.030956774950027466
I0209 18:11:06.932394 139864233322240 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.04463214799761772, loss=0.03664800524711609
I0209 18:11:18.677593 140039251117888 spec.py:321] Evaluating on the training split.
I0209 18:12:56.665659 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 18:12:59.686674 140039251117888 spec.py:349] Evaluating on the test split.
I0209 18:13:04.712533 140039251117888 submission_runner.py:408] Time since start: 5702.78s, 	Step: 11938, 	{'train/accuracy': 0.9901050925254822, 'train/loss': 0.0328376404941082, 'train/mean_average_precision': 0.35177024496025017, 'validation/accuracy': 0.9865832328796387, 'validation/loss': 0.0449516586959362, 'validation/mean_average_precision': 0.25467553456674824, 'validation/num_examples': 43793, 'test/accuracy': 0.9857610464096069, 'test/loss': 0.047576550394296646, 'test/mean_average_precision': 0.24912259173989285, 'test/num_examples': 43793, 'score': 3855.1541192531586, 'total_duration': 5702.779677867889, 'accumulated_submission_time': 3855.1541192531586, 'accumulated_eval_time': 1846.8564975261688, 'accumulated_logging_time': 0.4476304054260254}
I0209 18:13:04.729965 139836643178240 logging_writer.py:48] [11938] accumulated_eval_time=1846.856498, accumulated_logging_time=0.447630, accumulated_submission_time=3855.154119, global_step=11938, preemption_count=0, score=3855.154119, test/accuracy=0.985761, test/loss=0.047577, test/mean_average_precision=0.249123, test/num_examples=43793, total_duration=5702.779678, train/accuracy=0.990105, train/loss=0.032838, train/mean_average_precision=0.351770, validation/accuracy=0.986583, validation/loss=0.044952, validation/mean_average_precision=0.254676, validation/num_examples=43793
I0209 18:13:25.312176 139976540944128 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.04427451640367508, loss=0.03211969509720802
I0209 18:13:57.736411 139836643178240 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.04460308700799942, loss=0.03509236499667168
I0209 18:14:30.511779 139976540944128 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.044902607798576355, loss=0.03147271275520325
I0209 18:15:02.342153 139836643178240 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.03156859800219536, loss=0.029270030558109283
I0209 18:15:34.167180 139976540944128 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.04406670480966568, loss=0.031153885647654533
I0209 18:16:06.134713 139836643178240 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.040515873581171036, loss=0.031859155744314194
I0209 18:16:38.560462 139976540944128 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.03759389370679855, loss=0.03263513743877411
I0209 18:17:04.930254 140039251117888 spec.py:321] Evaluating on the training split.
I0209 18:18:44.991816 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 18:18:48.097853 140039251117888 spec.py:349] Evaluating on the test split.
I0209 18:18:51.057130 140039251117888 submission_runner.py:408] Time since start: 6049.12s, 	Step: 12680, 	{'train/accuracy': 0.990390956401825, 'train/loss': 0.03192305564880371, 'train/mean_average_precision': 0.3709610937739455, 'validation/accuracy': 0.9865361452102661, 'validation/loss': 0.04468625783920288, 'validation/mean_average_precision': 0.25275806647248383, 'validation/num_examples': 43793, 'test/accuracy': 0.9856886267662048, 'test/loss': 0.047468122094869614, 'test/mean_average_precision': 0.24952038505400487, 'test/num_examples': 43793, 'score': 4095.321917295456, 'total_duration': 6049.124286413193, 'accumulated_submission_time': 4095.321917295456, 'accumulated_eval_time': 1952.983324766159, 'accumulated_logging_time': 0.4773571491241455}
I0209 18:18:51.074818 139864233322240 logging_writer.py:48] [12680] accumulated_eval_time=1952.983325, accumulated_logging_time=0.477357, accumulated_submission_time=4095.321917, global_step=12680, preemption_count=0, score=4095.321917, test/accuracy=0.985689, test/loss=0.047468, test/mean_average_precision=0.249520, test/num_examples=43793, total_duration=6049.124286, train/accuracy=0.990391, train/loss=0.031923, train/mean_average_precision=0.370961, validation/accuracy=0.986536, validation/loss=0.044686, validation/mean_average_precision=0.252758, validation/num_examples=43793
I0209 18:18:57.910467 139871926159104 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.043352484703063965, loss=0.03184903413057327
I0209 18:19:30.172095 139864233322240 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.055721111595630646, loss=0.03424287959933281
I0209 18:20:02.115138 139871926159104 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.04376765713095665, loss=0.031640734523534775
I0209 18:20:33.891081 139864233322240 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.041211746633052826, loss=0.035729218274354935
I0209 18:21:06.117303 139871926159104 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.04087179899215698, loss=0.03569605574011803
I0209 18:21:38.223896 139864233322240 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.08764862269163132, loss=0.03895065560936928
I0209 18:22:10.302494 139871926159104 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.06672414392232895, loss=0.029843809083104134
I0209 18:22:42.363512 139864233322240 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.06201072782278061, loss=0.03623871132731438
I0209 18:22:51.085437 140039251117888 spec.py:321] Evaluating on the training split.
I0209 18:24:37.011633 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 18:24:40.493941 140039251117888 spec.py:349] Evaluating on the test split.
I0209 18:24:43.796730 140039251117888 submission_runner.py:408] Time since start: 6401.86s, 	Step: 13428, 	{'train/accuracy': 0.9904465079307556, 'train/loss': 0.031365860253572464, 'train/mean_average_precision': 0.3848623867266765, 'validation/accuracy': 0.986697256565094, 'validation/loss': 0.044610314071178436, 'validation/mean_average_precision': 0.25743156400662653, 'validation/num_examples': 43793, 'test/accuracy': 0.9858078360557556, 'test/loss': 0.047710102051496506, 'test/mean_average_precision': 0.2436257342937955, 'test/num_examples': 43793, 'score': 4335.301592588425, 'total_duration': 6401.863872051239, 'accumulated_submission_time': 4335.301592588425, 'accumulated_eval_time': 2065.694550514221, 'accumulated_logging_time': 0.5062572956085205}
I0209 18:24:43.816463 139878399788800 logging_writer.py:48] [13428] accumulated_eval_time=2065.694551, accumulated_logging_time=0.506257, accumulated_submission_time=4335.301593, global_step=13428, preemption_count=0, score=4335.301593, test/accuracy=0.985808, test/loss=0.047710, test/mean_average_precision=0.243626, test/num_examples=43793, total_duration=6401.863872, train/accuracy=0.990447, train/loss=0.031366, train/mean_average_precision=0.384862, validation/accuracy=0.986697, validation/loss=0.044610, validation/mean_average_precision=0.257432, validation/num_examples=43793
I0209 18:25:07.727015 139976540944128 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.036679044365882874, loss=0.028757257387042046
I0209 18:25:40.525622 139878399788800 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.05701055750250816, loss=0.03305183723568916
I0209 18:26:13.770562 139976540944128 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.04181241616606712, loss=0.0341024175286293
I0209 18:26:46.579622 139878399788800 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.04928172752261162, loss=0.033949125558137894
I0209 18:27:19.212389 139976540944128 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.046057410538196564, loss=0.03376881778240204
I0209 18:27:51.279070 139878399788800 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.04945136606693268, loss=0.03645096719264984
I0209 18:28:23.696691 139976540944128 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.040467940270900726, loss=0.03523961827158928
I0209 18:28:43.817226 140039251117888 spec.py:321] Evaluating on the training split.
I0209 18:30:24.034873 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 18:30:27.099173 140039251117888 spec.py:349] Evaluating on the test split.
I0209 18:30:30.121265 140039251117888 submission_runner.py:408] Time since start: 6748.19s, 	Step: 14163, 	{'train/accuracy': 0.9905685186386108, 'train/loss': 0.0309013519436121, 'train/mean_average_precision': 0.3852918889568212, 'validation/accuracy': 0.9867021441459656, 'validation/loss': 0.044885601848363876, 'validation/mean_average_precision': 0.24855540790643801, 'validation/num_examples': 43793, 'test/accuracy': 0.9858394265174866, 'test/loss': 0.04777158051729202, 'test/mean_average_precision': 0.24557376769917139, 'test/num_examples': 43793, 'score': 4575.268049478531, 'total_duration': 6748.188427686691, 'accumulated_submission_time': 4575.268049478531, 'accumulated_eval_time': 2171.9985449314117, 'accumulated_logging_time': 0.5388197898864746}
I0209 18:30:30.139543 139836643178240 logging_writer.py:48] [14163] accumulated_eval_time=2171.998545, accumulated_logging_time=0.538820, accumulated_submission_time=4575.268049, global_step=14163, preemption_count=0, score=4575.268049, test/accuracy=0.985839, test/loss=0.047772, test/mean_average_precision=0.245574, test/num_examples=43793, total_duration=6748.188428, train/accuracy=0.990569, train/loss=0.030901, train/mean_average_precision=0.385292, validation/accuracy=0.986702, validation/loss=0.044886, validation/mean_average_precision=0.248555, validation/num_examples=43793
I0209 18:30:42.537648 139871926159104 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.06362424045801163, loss=0.03162338212132454
I0209 18:31:14.979515 139836643178240 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.04623352363705635, loss=0.031017638742923737
I0209 18:31:47.194802 139871926159104 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.03849712759256363, loss=0.030642060562968254
I0209 18:32:19.730753 139836643178240 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.059590138494968414, loss=0.03263756260275841
I0209 18:32:51.795590 139871926159104 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.0536583848297596, loss=0.03141297772526741
I0209 18:33:24.580512 139836643178240 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.09440280497074127, loss=0.02973747067153454
I0209 18:33:57.427383 139871926159104 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.05563287436962128, loss=0.033446282148361206
I0209 18:34:30.313160 140039251117888 spec.py:321] Evaluating on the training split.
I0209 18:36:10.711061 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 18:36:15.888962 140039251117888 spec.py:349] Evaluating on the test split.
I0209 18:36:18.891602 140039251117888 submission_runner.py:408] Time since start: 7096.96s, 	Step: 14900, 	{'train/accuracy': 0.9904906153678894, 'train/loss': 0.03138909488916397, 'train/mean_average_precision': 0.37317141297512924, 'validation/accuracy': 0.9866453409194946, 'validation/loss': 0.0449206680059433, 'validation/mean_average_precision': 0.2515589492994831, 'validation/num_examples': 43793, 'test/accuracy': 0.9857408404350281, 'test/loss': 0.047922395169734955, 'test/mean_average_precision': 0.24176956981709927, 'test/num_examples': 43793, 'score': 4815.410197257996, 'total_duration': 7096.95876121521, 'accumulated_submission_time': 4815.410197257996, 'accumulated_eval_time': 2280.576951980591, 'accumulated_logging_time': 0.5679218769073486}
I0209 18:36:18.910701 139864233322240 logging_writer.py:48] [14900] accumulated_eval_time=2280.576952, accumulated_logging_time=0.567922, accumulated_submission_time=4815.410197, global_step=14900, preemption_count=0, score=4815.410197, test/accuracy=0.985741, test/loss=0.047922, test/mean_average_precision=0.241770, test/num_examples=43793, total_duration=7096.958761, train/accuracy=0.990491, train/loss=0.031389, train/mean_average_precision=0.373171, validation/accuracy=0.986645, validation/loss=0.044921, validation/mean_average_precision=0.251559, validation/num_examples=43793
I0209 18:36:19.265138 139878399788800 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.04738770052790642, loss=0.02824062667787075
I0209 18:36:51.673787 139864233322240 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.05741238221526146, loss=0.03249536082148552
I0209 18:37:24.367092 139878399788800 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.05500904843211174, loss=0.0283830463886261
I0209 18:37:56.718213 139864233322240 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.05727353319525719, loss=0.03151724860072136
I0209 18:38:29.231437 139878399788800 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.06598129868507385, loss=0.03425178304314613
I0209 18:39:01.155919 139864233322240 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.057410165667533875, loss=0.0350792370736599
I0209 18:39:33.667395 139878399788800 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.05496223643422127, loss=0.03133820369839668
I0209 18:40:06.265912 139864233322240 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.06067047640681267, loss=0.02836483158171177
I0209 18:40:19.109462 140039251117888 spec.py:321] Evaluating on the training split.
I0209 18:41:57.663181 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 18:42:00.731583 140039251117888 spec.py:349] Evaluating on the test split.
I0209 18:42:03.864868 140039251117888 submission_runner.py:408] Time since start: 7441.93s, 	Step: 15641, 	{'train/accuracy': 0.990464985370636, 'train/loss': 0.031555913388729095, 'train/mean_average_precision': 0.35899359101912764, 'validation/accuracy': 0.9865288138389587, 'validation/loss': 0.04482686519622803, 'validation/mean_average_precision': 0.2577375655676164, 'validation/num_examples': 43793, 'test/accuracy': 0.9857046008110046, 'test/loss': 0.04734693840146065, 'test/mean_average_precision': 0.2544363572094351, 'test/num_examples': 43793, 'score': 5055.575888395309, 'total_duration': 7441.932022809982, 'accumulated_submission_time': 5055.575888395309, 'accumulated_eval_time': 2385.3323168754578, 'accumulated_logging_time': 0.599492073059082}
I0209 18:42:03.883611 139836643178240 logging_writer.py:48] [15641] accumulated_eval_time=2385.332317, accumulated_logging_time=0.599492, accumulated_submission_time=5055.575888, global_step=15641, preemption_count=0, score=5055.575888, test/accuracy=0.985705, test/loss=0.047347, test/mean_average_precision=0.254436, test/num_examples=43793, total_duration=7441.932023, train/accuracy=0.990465, train/loss=0.031556, train/mean_average_precision=0.358994, validation/accuracy=0.986529, validation/loss=0.044827, validation/mean_average_precision=0.257738, validation/num_examples=43793
I0209 18:42:23.746627 139976540944128 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.04803343117237091, loss=0.02761632762849331
I0209 18:42:56.453731 139836643178240 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.04853743314743042, loss=0.02999183163046837
I0209 18:43:28.896839 139976540944128 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.0746115893125534, loss=0.034389808773994446
I0209 18:44:01.242071 139836643178240 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.05298494175076485, loss=0.03131688013672829
I0209 18:44:33.738949 139976540944128 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.0641254410147667, loss=0.03140322491526604
I0209 18:45:06.841817 139836643178240 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.0605723075568676, loss=0.03164521977305412
I0209 18:45:39.613798 139976540944128 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.09137905389070511, loss=0.02777591347694397
I0209 18:46:03.931470 140039251117888 spec.py:321] Evaluating on the training split.
I0209 18:47:51.943676 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 18:47:55.439441 140039251117888 spec.py:349] Evaluating on the test split.
I0209 18:48:01.308798 140039251117888 submission_runner.py:408] Time since start: 7799.38s, 	Step: 16376, 	{'train/accuracy': 0.990566611289978, 'train/loss': 0.03110634721815586, 'train/mean_average_precision': 0.397761553804051, 'validation/accuracy': 0.9866769909858704, 'validation/loss': 0.04449794813990593, 'validation/mean_average_precision': 0.2670592840195563, 'validation/num_examples': 43793, 'test/accuracy': 0.9857821464538574, 'test/loss': 0.04739530757069588, 'test/mean_average_precision': 0.25094880549624815, 'test/num_examples': 43793, 'score': 5295.590830564499, 'total_duration': 7799.37594294548, 'accumulated_submission_time': 5295.590830564499, 'accumulated_eval_time': 2502.7095897197723, 'accumulated_logging_time': 0.631049633026123}
I0209 18:48:01.329667 139864233322240 logging_writer.py:48] [16376] accumulated_eval_time=2502.709590, accumulated_logging_time=0.631050, accumulated_submission_time=5295.590831, global_step=16376, preemption_count=0, score=5295.590831, test/accuracy=0.985782, test/loss=0.047395, test/mean_average_precision=0.250949, test/num_examples=43793, total_duration=7799.375943, train/accuracy=0.990567, train/loss=0.031106, train/mean_average_precision=0.397762, validation/accuracy=0.986677, validation/loss=0.044498, validation/mean_average_precision=0.267059, validation/num_examples=43793
I0209 18:48:09.895401 139878399788800 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.06400132179260254, loss=0.033776067197322845
I0209 18:48:42.255899 139864233322240 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.07492142915725708, loss=0.03365020081400871
I0209 18:49:15.126982 139878399788800 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.06381624937057495, loss=0.033010147511959076
I0209 18:49:47.789772 139864233322240 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.06161917373538017, loss=0.0311504527926445
I0209 18:50:20.536365 139878399788800 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.0876065343618393, loss=0.030720407143235207
I0209 18:50:53.306503 139864233322240 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.108667753636837, loss=0.03186114504933357
I0209 18:51:26.032282 139878399788800 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.07827426493167877, loss=0.03041594661772251
I0209 18:51:58.595700 139864233322240 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.08684737235307693, loss=0.03067564032971859
I0209 18:52:01.518231 140039251117888 spec.py:321] Evaluating on the training split.
I0209 18:53:44.321307 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 18:53:47.435951 140039251117888 spec.py:349] Evaluating on the test split.
I0209 18:53:50.433053 140039251117888 submission_runner.py:408] Time since start: 8148.50s, 	Step: 17110, 	{'train/accuracy': 0.9904873967170715, 'train/loss': 0.030995003879070282, 'train/mean_average_precision': 0.39284304564245887, 'validation/accuracy': 0.9867208003997803, 'validation/loss': 0.04467495158314705, 'validation/mean_average_precision': 0.2598440136279063, 'validation/num_examples': 43793, 'test/accuracy': 0.9858613014221191, 'test/loss': 0.04753155633807182, 'test/mean_average_precision': 0.24660911204109393, 'test/num_examples': 43793, 'score': 5535.744588136673, 'total_duration': 8148.500215291977, 'accumulated_submission_time': 5535.744588136673, 'accumulated_eval_time': 2611.6243760585785, 'accumulated_logging_time': 0.6633293628692627}
I0209 18:53:50.452399 139836643178240 logging_writer.py:48] [17110] accumulated_eval_time=2611.624376, accumulated_logging_time=0.663329, accumulated_submission_time=5535.744588, global_step=17110, preemption_count=0, score=5535.744588, test/accuracy=0.985861, test/loss=0.047532, test/mean_average_precision=0.246609, test/num_examples=43793, total_duration=8148.500215, train/accuracy=0.990487, train/loss=0.030995, train/mean_average_precision=0.392843, validation/accuracy=0.986721, validation/loss=0.044675, validation/mean_average_precision=0.259844, validation/num_examples=43793
I0209 18:54:20.302263 139976540944128 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.15360260009765625, loss=0.030742883682250977
I0209 18:54:53.352954 139836643178240 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.07382238656282425, loss=0.030801421031355858
I0209 18:55:26.195629 139976540944128 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.06358830630779266, loss=0.030317779630422592
I0209 18:55:59.029500 139836643178240 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.06284588575363159, loss=0.032329827547073364
I0209 18:56:32.024565 139976540944128 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.0827036127448082, loss=0.02963162772357464
I0209 18:57:05.142631 139836643178240 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.06703516095876694, loss=0.029642583802342415
I0209 18:57:37.713765 139976540944128 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.11028485000133514, loss=0.03338572382926941
I0209 18:57:50.629950 140039251117888 spec.py:321] Evaluating on the training split.
I0209 18:59:31.361639 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 18:59:34.419835 140039251117888 spec.py:349] Evaluating on the test split.
I0209 18:59:37.481147 140039251117888 submission_runner.py:408] Time since start: 8495.55s, 	Step: 17841, 	{'train/accuracy': 0.9907149076461792, 'train/loss': 0.030316093936562538, 'train/mean_average_precision': 0.4062286195899587, 'validation/accuracy': 0.9867061972618103, 'validation/loss': 0.044766735285520554, 'validation/mean_average_precision': 0.26295826409490414, 'validation/num_examples': 43793, 'test/accuracy': 0.9859004616737366, 'test/loss': 0.0474623404443264, 'test/mean_average_precision': 0.25010620946383083, 'test/num_examples': 43793, 'score': 5775.890267133713, 'total_duration': 8495.548310518265, 'accumulated_submission_time': 5775.890267133713, 'accumulated_eval_time': 2718.475531101227, 'accumulated_logging_time': 0.6938979625701904}
I0209 18:59:37.500657 139871926159104 logging_writer.py:48] [17841] accumulated_eval_time=2718.475531, accumulated_logging_time=0.693898, accumulated_submission_time=5775.890267, global_step=17841, preemption_count=0, score=5775.890267, test/accuracy=0.985900, test/loss=0.047462, test/mean_average_precision=0.250106, test/num_examples=43793, total_duration=8495.548311, train/accuracy=0.990715, train/loss=0.030316, train/mean_average_precision=0.406229, validation/accuracy=0.986706, validation/loss=0.044767, validation/mean_average_precision=0.262958, validation/num_examples=43793
I0209 18:59:56.906037 139878399788800 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.11197488009929657, loss=0.027784094214439392
I0209 19:00:29.533327 139871926159104 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.06791536509990692, loss=0.034992482513189316
I0209 19:01:02.162351 139878399788800 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.08190066367387772, loss=0.029424814507365227
I0209 19:01:34.814012 139871926159104 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.07252690196037292, loss=0.028342191129922867
I0209 19:02:07.566315 139878399788800 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.07749699801206589, loss=0.03313861042261124
I0209 19:02:40.487290 139871926159104 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.10415501892566681, loss=0.030216075479984283
I0209 19:03:13.175235 139878399788800 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.05782414227724075, loss=0.0322384312748909
I0209 19:03:37.691916 140039251117888 spec.py:321] Evaluating on the training split.
I0209 19:05:20.455335 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 19:05:23.520210 140039251117888 spec.py:349] Evaluating on the test split.
I0209 19:05:26.524403 140039251117888 submission_runner.py:408] Time since start: 8844.59s, 	Step: 18577, 	{'train/accuracy': 0.9908282160758972, 'train/loss': 0.02974702976644039, 'train/mean_average_precision': 0.4252668941110549, 'validation/accuracy': 0.986599862575531, 'validation/loss': 0.04437005892395973, 'validation/mean_average_precision': 0.2613065277900078, 'validation/num_examples': 43793, 'test/accuracy': 0.9857437610626221, 'test/loss': 0.047278378158807755, 'test/mean_average_precision': 0.25738644262985816, 'test/num_examples': 43793, 'score': 6016.049191236496, 'total_duration': 8844.591564416885, 'accumulated_submission_time': 6016.049191236496, 'accumulated_eval_time': 2827.3079738616943, 'accumulated_logging_time': 0.7254743576049805}
I0209 19:05:26.542873 139836643178240 logging_writer.py:48] [18577] accumulated_eval_time=2827.307974, accumulated_logging_time=0.725474, accumulated_submission_time=6016.049191, global_step=18577, preemption_count=0, score=6016.049191, test/accuracy=0.985744, test/loss=0.047278, test/mean_average_precision=0.257386, test/num_examples=43793, total_duration=8844.591564, train/accuracy=0.990828, train/loss=0.029747, train/mean_average_precision=0.425267, validation/accuracy=0.986600, validation/loss=0.044370, validation/mean_average_precision=0.261307, validation/num_examples=43793
I0209 19:05:34.396689 139976540944128 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.0856291800737381, loss=0.03260130435228348
I0209 19:06:06.893973 139836643178240 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.07364147156476974, loss=0.030829982832074165
I0209 19:06:38.524509 139976540944128 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.0693780854344368, loss=0.030716633424162865
I0209 19:07:10.366036 139836643178240 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.11099942028522491, loss=0.0333116389811039
I0209 19:07:41.988224 139976540944128 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.08192560076713562, loss=0.035855695605278015
I0209 19:08:14.128169 139836643178240 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.09961621463298798, loss=0.03202421963214874
I0209 19:08:45.982804 139976540944128 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.0981556624174118, loss=0.030684756115078926
I0209 19:09:17.675894 139836643178240 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.08593390882015228, loss=0.03233673796057701
I0209 19:09:26.728130 140039251117888 spec.py:321] Evaluating on the training split.
I0209 19:11:02.630126 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 19:11:05.727893 140039251117888 spec.py:349] Evaluating on the test split.
I0209 19:11:10.758226 140039251117888 submission_runner.py:408] Time since start: 9188.83s, 	Step: 19329, 	{'train/accuracy': 0.9912741780281067, 'train/loss': 0.028726851567626, 'train/mean_average_precision': 0.4346732790575787, 'validation/accuracy': 0.9866855144500732, 'validation/loss': 0.04458357393741608, 'validation/mean_average_precision': 0.26563868494886217, 'validation/num_examples': 43793, 'test/accuracy': 0.9858258962631226, 'test/loss': 0.04710597172379494, 'test/mean_average_precision': 0.2557344872373129, 'test/num_examples': 43793, 'score': 6256.203496932983, 'total_duration': 9188.825388908386, 'accumulated_submission_time': 6256.203496932983, 'accumulated_eval_time': 2931.338026046753, 'accumulated_logging_time': 0.7550005912780762}
I0209 19:11:10.777429 139864233322240 logging_writer.py:48] [19329] accumulated_eval_time=2931.338026, accumulated_logging_time=0.755001, accumulated_submission_time=6256.203497, global_step=19329, preemption_count=0, score=6256.203497, test/accuracy=0.985826, test/loss=0.047106, test/mean_average_precision=0.255734, test/num_examples=43793, total_duration=9188.825389, train/accuracy=0.991274, train/loss=0.028727, train/mean_average_precision=0.434673, validation/accuracy=0.986686, validation/loss=0.044584, validation/mean_average_precision=0.265639, validation/num_examples=43793
I0209 19:11:33.898766 139878399788800 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.09935341775417328, loss=0.032466061413288116
I0209 19:12:05.773422 139864233322240 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.05749277025461197, loss=0.03022787533700466
I0209 19:12:37.760626 139878399788800 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.06873220950365067, loss=0.030473502352833748
I0209 19:13:09.925675 139864233322240 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.10682322084903717, loss=0.03081832081079483
I0209 19:13:42.255510 139878399788800 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.08970223367214203, loss=0.02913680113852024
I0209 19:14:14.254066 139864233322240 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.08452749252319336, loss=0.026673518121242523
I0209 19:14:46.342495 139878399788800 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.07530415803194046, loss=0.03210343047976494
I0209 19:15:10.834165 140039251117888 spec.py:321] Evaluating on the training split.
I0209 19:16:57.232583 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 19:17:00.299618 140039251117888 spec.py:349] Evaluating on the test split.
I0209 19:17:03.485605 140039251117888 submission_runner.py:408] Time since start: 9541.55s, 	Step: 20076, 	{'train/accuracy': 0.9913241863250732, 'train/loss': 0.028325732797384262, 'train/mean_average_precision': 0.4522674479990455, 'validation/accuracy': 0.9865933656692505, 'validation/loss': 0.04457477852702141, 'validation/mean_average_precision': 0.26392464655433473, 'validation/num_examples': 43793, 'test/accuracy': 0.9858339428901672, 'test/loss': 0.046953234821558, 'test/mean_average_precision': 0.26261314590518825, 'test/num_examples': 43793, 'score': 6496.22972202301, 'total_duration': 9541.552764177322, 'accumulated_submission_time': 6496.22972202301, 'accumulated_eval_time': 3043.9894185066223, 'accumulated_logging_time': 0.7850522994995117}
I0209 19:17:03.506401 139836634785536 logging_writer.py:48] [20076] accumulated_eval_time=3043.989419, accumulated_logging_time=0.785052, accumulated_submission_time=6496.229722, global_step=20076, preemption_count=0, score=6496.229722, test/accuracy=0.985834, test/loss=0.046953, test/mean_average_precision=0.262613, test/num_examples=43793, total_duration=9541.552764, train/accuracy=0.991324, train/loss=0.028326, train/mean_average_precision=0.452267, validation/accuracy=0.986593, validation/loss=0.044575, validation/mean_average_precision=0.263925, validation/num_examples=43793
I0209 19:17:11.565608 139836643178240 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.11734931170940399, loss=0.03040727972984314
I0209 19:17:43.783030 139836634785536 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.08230766654014587, loss=0.02851025201380253
I0209 19:18:15.951603 139836643178240 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.18020549416542053, loss=0.03078756108880043
I0209 19:18:48.064759 139836634785536 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.07656432688236237, loss=0.027256373316049576
I0209 19:19:20.288705 139836643178240 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.07922063767910004, loss=0.031107919290661812
I0209 19:19:52.126376 139836634785536 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.10187605768442154, loss=0.03090597689151764
I0209 19:20:24.596435 139836643178240 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.0721246674656868, loss=0.026190781965851784
I0209 19:20:56.863520 139836634785536 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.08009994775056839, loss=0.0274506825953722
I0209 19:21:03.492029 140039251117888 spec.py:321] Evaluating on the training split.
I0209 19:22:48.957993 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 19:22:54.036119 140039251117888 spec.py:349] Evaluating on the test split.
I0209 19:22:57.160899 140039251117888 submission_runner.py:408] Time since start: 9895.23s, 	Step: 20821, 	{'train/accuracy': 0.9910668134689331, 'train/loss': 0.029200004413723946, 'train/mean_average_precision': 0.4294235006191442, 'validation/accuracy': 0.9865803718566895, 'validation/loss': 0.04457120969891548, 'validation/mean_average_precision': 0.2657850786309507, 'validation/num_examples': 43793, 'test/accuracy': 0.9857593774795532, 'test/loss': 0.04739382490515709, 'test/mean_average_precision': 0.2585024581789923, 'test/num_examples': 43793, 'score': 6736.18346953392, 'total_duration': 9895.228059530258, 'accumulated_submission_time': 6736.18346953392, 'accumulated_eval_time': 3157.6582396030426, 'accumulated_logging_time': 0.8184309005737305}
I0209 19:22:57.180725 139864233322240 logging_writer.py:48] [20821] accumulated_eval_time=3157.658240, accumulated_logging_time=0.818431, accumulated_submission_time=6736.183470, global_step=20821, preemption_count=0, score=6736.183470, test/accuracy=0.985759, test/loss=0.047394, test/mean_average_precision=0.258502, test/num_examples=43793, total_duration=9895.228060, train/accuracy=0.991067, train/loss=0.029200, train/mean_average_precision=0.429424, validation/accuracy=0.986580, validation/loss=0.044571, validation/mean_average_precision=0.265785, validation/num_examples=43793
I0209 19:23:22.967366 139871926159104 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.06713912636041641, loss=0.03170616924762726
I0209 19:23:55.287709 139864233322240 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.07985363900661469, loss=0.028972581028938293
I0209 19:24:27.729909 139871926159104 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.13494503498077393, loss=0.027873946353793144
I0209 19:24:59.605903 139864233322240 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.07606559991836548, loss=0.029366567730903625
I0209 19:25:31.929231 139871926159104 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.07412901520729065, loss=0.029796453192830086
I0209 19:26:04.293501 139864233322240 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.08042670041322708, loss=0.030239026993513107
I0209 19:26:36.924521 139871926159104 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.06519442051649094, loss=0.027177156880497932
I0209 19:26:57.180919 140039251117888 spec.py:321] Evaluating on the training split.
I0209 19:28:37.905854 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 19:28:41.028286 140039251117888 spec.py:349] Evaluating on the test split.
I0209 19:28:44.094108 140039251117888 submission_runner.py:408] Time since start: 10242.16s, 	Step: 21563, 	{'train/accuracy': 0.9908154606819153, 'train/loss': 0.029763884842395782, 'train/mean_average_precision': 0.40703255615476974, 'validation/accuracy': 0.9867427349090576, 'validation/loss': 0.04434947296977043, 'validation/mean_average_precision': 0.26975878350204774, 'validation/num_examples': 43793, 'test/accuracy': 0.9858583807945251, 'test/loss': 0.047186437994241714, 'test/mean_average_precision': 0.26410273506081405, 'test/num_examples': 43793, 'score': 6976.1531801223755, 'total_duration': 10242.161269426346, 'accumulated_submission_time': 6976.1531801223755, 'accumulated_eval_time': 3264.571383714676, 'accumulated_logging_time': 0.8491370677947998}
I0209 19:28:44.114127 139836634785536 logging_writer.py:48] [21563] accumulated_eval_time=3264.571384, accumulated_logging_time=0.849137, accumulated_submission_time=6976.153180, global_step=21563, preemption_count=0, score=6976.153180, test/accuracy=0.985858, test/loss=0.047186, test/mean_average_precision=0.264103, test/num_examples=43793, total_duration=10242.161269, train/accuracy=0.990815, train/loss=0.029764, train/mean_average_precision=0.407033, validation/accuracy=0.986743, validation/loss=0.044349, validation/mean_average_precision=0.269759, validation/num_examples=43793
I0209 19:28:56.333337 139836643178240 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.08833915740251541, loss=0.0299209151417017
I0209 19:29:28.531574 139836634785536 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.08199736475944519, loss=0.027445420622825623
I0209 19:30:00.861549 139836643178240 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.09720677137374878, loss=0.027493515983223915
I0209 19:30:33.169574 139836634785536 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.08981681615114212, loss=0.03408597409725189
I0209 19:31:05.756905 139836643178240 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.08178619295358658, loss=0.0304692592471838
I0209 19:31:38.332495 139836634785536 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.0853392630815506, loss=0.02981678396463394
I0209 19:32:11.093267 139836643178240 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.08184549957513809, loss=0.027019429951906204
I0209 19:32:43.703687 139836634785536 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.09015120565891266, loss=0.030471937730908394
I0209 19:32:44.343101 140039251117888 spec.py:321] Evaluating on the training split.
I0209 19:34:29.110456 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 19:34:34.248507 140039251117888 spec.py:349] Evaluating on the test split.
I0209 19:34:37.285813 140039251117888 submission_runner.py:408] Time since start: 10595.35s, 	Step: 22303, 	{'train/accuracy': 0.9910184144973755, 'train/loss': 0.02950519323348999, 'train/mean_average_precision': 0.4229856291489511, 'validation/accuracy': 0.9867549538612366, 'validation/loss': 0.04416754096746445, 'validation/mean_average_precision': 0.2666595006962322, 'validation/num_examples': 43793, 'test/accuracy': 0.9859581589698792, 'test/loss': 0.04715448245406151, 'test/mean_average_precision': 0.25820191353976035, 'test/num_examples': 43793, 'score': 7216.347539186478, 'total_duration': 10595.352976083755, 'accumulated_submission_time': 7216.347539186478, 'accumulated_eval_time': 3377.514060497284, 'accumulated_logging_time': 0.8812205791473389}
I0209 19:34:37.305960 139864233322240 logging_writer.py:48] [22303] accumulated_eval_time=3377.514060, accumulated_logging_time=0.881221, accumulated_submission_time=7216.347539, global_step=22303, preemption_count=0, score=7216.347539, test/accuracy=0.985958, test/loss=0.047154, test/mean_average_precision=0.258202, test/num_examples=43793, total_duration=10595.352976, train/accuracy=0.991018, train/loss=0.029505, train/mean_average_precision=0.422986, validation/accuracy=0.986755, validation/loss=0.044168, validation/mean_average_precision=0.266660, validation/num_examples=43793
I0209 19:35:09.133567 139878399788800 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.09362395107746124, loss=0.02664918266236782
I0209 19:35:41.655885 139864233322240 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.06257618963718414, loss=0.026314936578273773
I0209 19:36:13.970978 139878399788800 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.10123620182275772, loss=0.030215315520763397
I0209 19:36:46.755773 139864233322240 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.0798233300447464, loss=0.02679980732500553
I0209 19:37:20.053052 139878399788800 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.09285152703523636, loss=0.02780885435640812
I0209 19:37:53.150774 139864233322240 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.06739384680986404, loss=0.02874062955379486
I0209 19:38:26.087141 139878399788800 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.08313235640525818, loss=0.02962053194642067
I0209 19:38:37.497150 140039251117888 spec.py:321] Evaluating on the training split.
I0209 19:40:17.883995 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 19:40:21.069360 140039251117888 spec.py:349] Evaluating on the test split.
I0209 19:40:24.138338 140039251117888 submission_runner.py:408] Time since start: 10942.21s, 	Step: 23037, 	{'train/accuracy': 0.991021454334259, 'train/loss': 0.02940763346850872, 'train/mean_average_precision': 0.431725123072511, 'validation/accuracy': 0.9867947101593018, 'validation/loss': 0.04461195319890976, 'validation/mean_average_precision': 0.2679407448618573, 'validation/num_examples': 43793, 'test/accuracy': 0.986004114151001, 'test/loss': 0.0473821796476841, 'test/mean_average_precision': 0.26431638324571194, 'test/num_examples': 43793, 'score': 7456.5054433345795, 'total_duration': 10942.205500364304, 'accumulated_submission_time': 7456.5054433345795, 'accumulated_eval_time': 3484.1552045345306, 'accumulated_logging_time': 0.9138262271881104}
I0209 19:40:24.160146 139836634785536 logging_writer.py:48] [23037] accumulated_eval_time=3484.155205, accumulated_logging_time=0.913826, accumulated_submission_time=7456.505443, global_step=23037, preemption_count=0, score=7456.505443, test/accuracy=0.986004, test/loss=0.047382, test/mean_average_precision=0.264316, test/num_examples=43793, total_duration=10942.205500, train/accuracy=0.991021, train/loss=0.029408, train/mean_average_precision=0.431725, validation/accuracy=0.986795, validation/loss=0.044612, validation/mean_average_precision=0.267941, validation/num_examples=43793
I0209 19:40:44.800559 139836643178240 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.0898907482624054, loss=0.02397795021533966
I0209 19:41:17.022474 139836634785536 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.08169304579496384, loss=0.027078181505203247
I0209 19:41:48.735648 139836643178240 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.08362619578838348, loss=0.03426182270050049
I0209 19:42:20.550050 139836634785536 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.10344433039426804, loss=0.03103756345808506
I0209 19:42:52.510212 139836643178240 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.07829426974058151, loss=0.027953384444117546
I0209 19:43:25.012133 139836634785536 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.10906731337308884, loss=0.028222354128956795
I0209 19:43:57.040177 139836643178240 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.08319319039583206, loss=0.02744527906179428
I0209 19:44:24.348969 140039251117888 spec.py:321] Evaluating on the training split.
I0209 19:46:04.052468 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 19:46:07.194463 140039251117888 spec.py:349] Evaluating on the test split.
I0209 19:46:12.247748 140039251117888 submission_runner.py:408] Time since start: 11290.31s, 	Step: 23786, 	{'train/accuracy': 0.9908633828163147, 'train/loss': 0.029719047248363495, 'train/mean_average_precision': 0.42751913916293544, 'validation/accuracy': 0.9866741299629211, 'validation/loss': 0.04453583061695099, 'validation/mean_average_precision': 0.26668650965521207, 'validation/num_examples': 43793, 'test/accuracy': 0.9857631921768188, 'test/loss': 0.047598451375961304, 'test/mean_average_precision': 0.2511069827343094, 'test/num_examples': 43793, 'score': 7696.663548946381, 'total_duration': 11290.314909934998, 'accumulated_submission_time': 7696.663548946381, 'accumulated_eval_time': 3592.053944826126, 'accumulated_logging_time': 0.946509599685669}
I0209 19:46:12.268118 139871926159104 logging_writer.py:48] [23786] accumulated_eval_time=3592.053945, accumulated_logging_time=0.946510, accumulated_submission_time=7696.663549, global_step=23786, preemption_count=0, score=7696.663549, test/accuracy=0.985763, test/loss=0.047598, test/mean_average_precision=0.251107, test/num_examples=43793, total_duration=11290.314910, train/accuracy=0.990863, train/loss=0.029719, train/mean_average_precision=0.427519, validation/accuracy=0.986674, validation/loss=0.044536, validation/mean_average_precision=0.266687, validation/num_examples=43793
I0209 19:46:17.268987 139878399788800 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.08973073214292526, loss=0.03489168360829353
I0209 19:46:49.439365 139871926159104 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.12447134405374527, loss=0.03132766857743263
I0209 19:47:22.054471 139878399788800 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.09353809803724289, loss=0.029639430344104767
I0209 19:47:54.161551 139871926159104 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.09559640288352966, loss=0.026068467646837234
I0209 19:48:26.654888 139878399788800 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.0912029817700386, loss=0.027855709195137024
I0209 19:48:59.065714 139871926159104 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.08115015923976898, loss=0.02911728248000145
I0209 19:49:31.027493 139878399788800 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.0839390829205513, loss=0.032801538705825806
I0209 19:50:03.252302 139871926159104 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.09302801638841629, loss=0.029361708089709282
I0209 19:50:12.537462 140039251117888 spec.py:321] Evaluating on the training split.
I0209 19:51:55.098594 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 19:51:58.152108 140039251117888 spec.py:349] Evaluating on the test split.
I0209 19:52:01.143190 140039251117888 submission_runner.py:408] Time since start: 11639.21s, 	Step: 24530, 	{'train/accuracy': 0.9912306070327759, 'train/loss': 0.028274420648813248, 'train/mean_average_precision': 0.46558060549088254, 'validation/accuracy': 0.9868429899215698, 'validation/loss': 0.044499415904283524, 'validation/mean_average_precision': 0.26683267041100056, 'validation/num_examples': 43793, 'test/accuracy': 0.9859198331832886, 'test/loss': 0.04751171916723251, 'test/mean_average_precision': 0.2620446093170533, 'test/num_examples': 43793, 'score': 7936.902099847794, 'total_duration': 11639.210319280624, 'accumulated_submission_time': 7936.902099847794, 'accumulated_eval_time': 3700.659593105316, 'accumulated_logging_time': 0.9780809879302979}
I0209 19:52:01.163644 139836634785536 logging_writer.py:48] [24530] accumulated_eval_time=3700.659593, accumulated_logging_time=0.978081, accumulated_submission_time=7936.902100, global_step=24530, preemption_count=0, score=7936.902100, test/accuracy=0.985920, test/loss=0.047512, test/mean_average_precision=0.262045, test/num_examples=43793, total_duration=11639.210319, train/accuracy=0.991231, train/loss=0.028274, train/mean_average_precision=0.465581, validation/accuracy=0.986843, validation/loss=0.044499, validation/mean_average_precision=0.266833, validation/num_examples=43793
I0209 19:52:23.928603 139864233322240 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.09260433167219162, loss=0.029962968081235886
I0209 19:52:56.103208 139836634785536 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.08827181905508041, loss=0.02972855046391487
I0209 19:53:28.241791 139864233322240 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.1177007332444191, loss=0.032009780406951904
I0209 19:54:00.400321 139836634785536 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.08574748039245605, loss=0.027505038306117058
I0209 19:54:32.583553 139864233322240 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.09341903030872345, loss=0.02976609580218792
I0209 19:55:04.506113 139836634785536 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.12239710241556168, loss=0.029021210968494415
I0209 19:55:36.360970 139864233322240 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.09125783294439316, loss=0.02910742349922657
I0209 19:56:01.345230 140039251117888 spec.py:321] Evaluating on the training split.
I0209 19:57:38.054187 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 19:57:41.094176 140039251117888 spec.py:349] Evaluating on the test split.
I0209 19:57:44.123910 140039251117888 submission_runner.py:408] Time since start: 11982.19s, 	Step: 25279, 	{'train/accuracy': 0.9913411736488342, 'train/loss': 0.02796102501451969, 'train/mean_average_precision': 0.4508059522935717, 'validation/accuracy': 0.9867439866065979, 'validation/loss': 0.044363945722579956, 'validation/mean_average_precision': 0.26568900577999616, 'validation/num_examples': 43793, 'test/accuracy': 0.9858301281929016, 'test/loss': 0.0474405437707901, 'test/mean_average_precision': 0.2627028867128916, 'test/num_examples': 43793, 'score': 8177.051397800446, 'total_duration': 11982.191071748734, 'accumulated_submission_time': 8177.051397800446, 'accumulated_eval_time': 3803.438230276108, 'accumulated_logging_time': 1.0112404823303223}
I0209 19:57:44.145277 139871926159104 logging_writer.py:48] [25279] accumulated_eval_time=3803.438230, accumulated_logging_time=1.011240, accumulated_submission_time=8177.051398, global_step=25279, preemption_count=0, score=8177.051398, test/accuracy=0.985830, test/loss=0.047441, test/mean_average_precision=0.262703, test/num_examples=43793, total_duration=11982.191072, train/accuracy=0.991341, train/loss=0.027961, train/mean_average_precision=0.450806, validation/accuracy=0.986744, validation/loss=0.044364, validation/mean_average_precision=0.265689, validation/num_examples=43793
I0209 19:57:51.376558 139878399788800 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.11706818640232086, loss=0.033682890236377716
I0209 19:58:24.324897 139871926159104 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.08544358611106873, loss=0.03087536431849003
I0209 19:58:56.164532 139878399788800 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.22676491737365723, loss=0.030859628692269325
I0209 19:59:28.298927 139871926159104 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.11034724116325378, loss=0.03066861256957054
I0209 20:00:00.149687 139878399788800 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.13057775795459747, loss=0.03080761432647705
I0209 20:00:32.146709 139871926159104 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.09347975254058838, loss=0.026281900703907013
I0209 20:01:04.565861 139878399788800 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.13092730939388275, loss=0.026941170915961266
I0209 20:01:36.795100 139871926159104 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.0937337800860405, loss=0.028728609904646873
I0209 20:01:44.186179 140039251117888 spec.py:321] Evaluating on the training split.
I0209 20:03:29.600269 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 20:03:32.674524 140039251117888 spec.py:349] Evaluating on the test split.
I0209 20:03:35.663861 140039251117888 submission_runner.py:408] Time since start: 12333.73s, 	Step: 26024, 	{'train/accuracy': 0.9914472103118896, 'train/loss': 0.027597038075327873, 'train/mean_average_precision': 0.4639268114410071, 'validation/accuracy': 0.9867309927940369, 'validation/loss': 0.04464828595519066, 'validation/mean_average_precision': 0.2693065018483168, 'validation/num_examples': 43793, 'test/accuracy': 0.9858301281929016, 'test/loss': 0.04775211587548256, 'test/mean_average_precision': 0.26042903987497107, 'test/num_examples': 43793, 'score': 8417.059885501862, 'total_duration': 12333.731023311615, 'accumulated_submission_time': 8417.059885501862, 'accumulated_eval_time': 3914.915863752365, 'accumulated_logging_time': 1.0447053909301758}
I0209 20:03:35.684444 139836643178240 logging_writer.py:48] [26024] accumulated_eval_time=3914.915864, accumulated_logging_time=1.044705, accumulated_submission_time=8417.059886, global_step=26024, preemption_count=0, score=8417.059886, test/accuracy=0.985830, test/loss=0.047752, test/mean_average_precision=0.260429, test/num_examples=43793, total_duration=12333.731023, train/accuracy=0.991447, train/loss=0.027597, train/mean_average_precision=0.463927, validation/accuracy=0.986731, validation/loss=0.044648, validation/mean_average_precision=0.269307, validation/num_examples=43793
I0209 20:04:00.642742 139864233322240 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.1003677174448967, loss=0.032743748277425766
I0209 20:04:32.945787 139836643178240 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.1185220256447792, loss=0.027453135699033737
I0209 20:05:05.511943 139864233322240 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.09645198285579681, loss=0.029531704261898994
I0209 20:05:38.216392 139836643178240 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.10355748236179352, loss=0.03190113976597786
I0209 20:06:10.777902 139864233322240 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.07907230406999588, loss=0.030764181166887283
I0209 20:06:43.131991 139836643178240 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.15135756134986877, loss=0.03051893413066864
I0209 20:07:15.303037 139864233322240 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.08217951655387878, loss=0.027449436485767365
I0209 20:07:35.972227 140039251117888 spec.py:321] Evaluating on the training split.
I0209 20:09:16.471473 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 20:09:21.479937 140039251117888 spec.py:349] Evaluating on the test split.
I0209 20:09:24.477301 140039251117888 submission_runner.py:408] Time since start: 12682.54s, 	Step: 26765, 	{'train/accuracy': 0.9916866421699524, 'train/loss': 0.026975542306900024, 'train/mean_average_precision': 0.48170280678897637, 'validation/accuracy': 0.9868255853652954, 'validation/loss': 0.04439857229590416, 'validation/mean_average_precision': 0.27580323962848435, 'validation/num_examples': 43793, 'test/accuracy': 0.9860129356384277, 'test/loss': 0.0470789335668087, 'test/mean_average_precision': 0.26394230768981414, 'test/num_examples': 43793, 'score': 8657.316404104233, 'total_duration': 12682.5444586277, 'accumulated_submission_time': 8657.316404104233, 'accumulated_eval_time': 4023.420888900757, 'accumulated_logging_time': 1.0764586925506592}
I0209 20:09:24.498545 139836634785536 logging_writer.py:48] [26765] accumulated_eval_time=4023.420889, accumulated_logging_time=1.076459, accumulated_submission_time=8657.316404, global_step=26765, preemption_count=0, score=8657.316404, test/accuracy=0.986013, test/loss=0.047079, test/mean_average_precision=0.263942, test/num_examples=43793, total_duration=12682.544459, train/accuracy=0.991687, train/loss=0.026976, train/mean_average_precision=0.481703, validation/accuracy=0.986826, validation/loss=0.044399, validation/mean_average_precision=0.275803, validation/num_examples=43793
I0209 20:09:36.670305 139878399788800 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.1256435215473175, loss=0.031153563410043716
I0209 20:10:09.114027 139836634785536 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.10667295753955841, loss=0.027623159810900688
I0209 20:10:41.285308 139878399788800 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.08137791603803635, loss=0.026562508195638657
I0209 20:11:13.495462 139836634785536 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.10878078639507294, loss=0.028957882896065712
I0209 20:11:45.990417 139878399788800 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.10358590632677078, loss=0.028427841141819954
I0209 20:12:18.345864 139836634785536 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.14564958214759827, loss=0.026334408670663834
I0209 20:12:50.380423 139878399788800 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.10146203637123108, loss=0.03272517770528793
I0209 20:13:22.826735 139836634785536 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.08902008831501007, loss=0.027793869376182556
I0209 20:13:24.763725 140039251117888 spec.py:321] Evaluating on the training split.
I0209 20:15:04.307348 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 20:15:07.392608 140039251117888 spec.py:349] Evaluating on the test split.
I0209 20:15:10.355866 140039251117888 submission_runner.py:408] Time since start: 13028.42s, 	Step: 27507, 	{'train/accuracy': 0.9914228916168213, 'train/loss': 0.02792561985552311, 'train/mean_average_precision': 0.45979459235704995, 'validation/accuracy': 0.9868003726005554, 'validation/loss': 0.04430904984474182, 'validation/mean_average_precision': 0.2730667285241964, 'validation/num_examples': 43793, 'test/accuracy': 0.9858406782150269, 'test/loss': 0.04753226414322853, 'test/mean_average_precision': 0.25606542608510635, 'test/num_examples': 43793, 'score': 8897.550789117813, 'total_duration': 13028.423025131226, 'accumulated_submission_time': 8897.550789117813, 'accumulated_eval_time': 4129.012980937958, 'accumulated_logging_time': 1.1086671352386475}
I0209 20:15:10.377180 139836643178240 logging_writer.py:48] [27507] accumulated_eval_time=4129.012981, accumulated_logging_time=1.108667, accumulated_submission_time=8897.550789, global_step=27507, preemption_count=0, score=8897.550789, test/accuracy=0.985841, test/loss=0.047532, test/mean_average_precision=0.256065, test/num_examples=43793, total_duration=13028.423025, train/accuracy=0.991423, train/loss=0.027926, train/mean_average_precision=0.459795, validation/accuracy=0.986800, validation/loss=0.044309, validation/mean_average_precision=0.273067, validation/num_examples=43793
I0209 20:15:40.658930 139871926159104 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.1277747005224228, loss=0.02717803418636322
I0209 20:16:12.952539 139836643178240 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.08325973153114319, loss=0.025142179802060127
I0209 20:16:45.606513 139871926159104 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.1030789241194725, loss=0.029019447043538094
I0209 20:17:18.083721 139836643178240 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.12137801200151443, loss=0.031186137348413467
I0209 20:17:50.181464 139871926159104 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.10359738767147064, loss=0.028519002720713615
I0209 20:18:22.668412 139836643178240 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.1784040629863739, loss=0.02606061100959778
I0209 20:18:54.782172 139871926159104 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.11847715079784393, loss=0.026912009343504906
I0209 20:19:10.456315 140039251117888 spec.py:321] Evaluating on the training split.
I0209 20:20:51.073028 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 20:20:54.132183 140039251117888 spec.py:349] Evaluating on the test split.
I0209 20:20:59.146128 140039251117888 submission_runner.py:408] Time since start: 13377.21s, 	Step: 28250, 	{'train/accuracy': 0.9913296699523926, 'train/loss': 0.028052499517798424, 'train/mean_average_precision': 0.4551051103635106, 'validation/accuracy': 0.9868763089179993, 'validation/loss': 0.04476146399974823, 'validation/mean_average_precision': 0.2724308668815321, 'validation/num_examples': 43793, 'test/accuracy': 0.9859476685523987, 'test/loss': 0.047663263976573944, 'test/mean_average_precision': 0.2613195491298001, 'test/num_examples': 43793, 'score': 9137.598731279373, 'total_duration': 13377.213291406631, 'accumulated_submission_time': 9137.598731279373, 'accumulated_eval_time': 4237.702749490738, 'accumulated_logging_time': 1.1413967609405518}
I0209 20:20:59.167370 139836634785536 logging_writer.py:48] [28250] accumulated_eval_time=4237.702749, accumulated_logging_time=1.141397, accumulated_submission_time=9137.598731, global_step=28250, preemption_count=0, score=9137.598731, test/accuracy=0.985948, test/loss=0.047663, test/mean_average_precision=0.261320, test/num_examples=43793, total_duration=13377.213291, train/accuracy=0.991330, train/loss=0.028052, train/mean_average_precision=0.455105, validation/accuracy=0.986876, validation/loss=0.044761, validation/mean_average_precision=0.272431, validation/num_examples=43793
I0209 20:21:15.444704 139864233322240 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.08713886886835098, loss=0.02706710807979107
I0209 20:21:47.454798 139836634785536 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.10126500576734543, loss=0.028068941086530685
I0209 20:22:19.581629 139864233322240 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.10898972302675247, loss=0.030141176655888557
I0209 20:22:51.685387 139836634785536 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.1283709704875946, loss=0.029682232066988945
I0209 20:23:23.742836 139864233322240 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.10846912115812302, loss=0.030539438128471375
I0209 20:23:56.355253 139836634785536 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.0790092721581459, loss=0.028486358001828194
I0209 20:24:29.332313 139864233322240 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.1082695722579956, loss=0.02764435112476349
I0209 20:24:59.471957 140039251117888 spec.py:321] Evaluating on the training split.
I0209 20:26:39.245232 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 20:26:42.273041 140039251117888 spec.py:349] Evaluating on the test split.
I0209 20:26:45.237906 140039251117888 submission_runner.py:408] Time since start: 13723.31s, 	Step: 28993, 	{'train/accuracy': 0.9911660552024841, 'train/loss': 0.028592262417078018, 'train/mean_average_precision': 0.448091841051458, 'validation/accuracy': 0.9867184162139893, 'validation/loss': 0.04482870176434517, 'validation/mean_average_precision': 0.2768478901678559, 'validation/num_examples': 43793, 'test/accuracy': 0.9859139323234558, 'test/loss': 0.04776918143033981, 'test/mean_average_precision': 0.258527015701871, 'test/num_examples': 43793, 'score': 9377.87127995491, 'total_duration': 13723.3050699234, 'accumulated_submission_time': 9377.87127995491, 'accumulated_eval_time': 4343.46866941452, 'accumulated_logging_time': 1.1738147735595703}
I0209 20:26:45.259312 139836643178240 logging_writer.py:48] [28993] accumulated_eval_time=4343.468669, accumulated_logging_time=1.173815, accumulated_submission_time=9377.871280, global_step=28993, preemption_count=0, score=9377.871280, test/accuracy=0.985914, test/loss=0.047769, test/mean_average_precision=0.258527, test/num_examples=43793, total_duration=13723.305070, train/accuracy=0.991166, train/loss=0.028592, train/mean_average_precision=0.448092, validation/accuracy=0.986718, validation/loss=0.044829, validation/mean_average_precision=0.276848, validation/num_examples=43793
I0209 20:26:47.849365 139878399788800 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.09969186037778854, loss=0.02863464690744877
I0209 20:27:20.043928 139836643178240 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.09518878161907196, loss=0.024729643017053604
I0209 20:27:51.973085 139878399788800 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.11345312744379044, loss=0.028963761404156685
I0209 20:28:24.080131 139836643178240 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.1279468685388565, loss=0.027570290490984917
I0209 20:28:55.964447 139878399788800 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.09928038716316223, loss=0.026122812181711197
I0209 20:29:27.997077 139836643178240 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.10543552041053772, loss=0.027225226163864136
I0209 20:29:59.568270 139878399788800 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.13473191857337952, loss=0.028595469892024994
I0209 20:30:31.617184 139836643178240 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.12954294681549072, loss=0.03105125203728676
I0209 20:30:45.489255 140039251117888 spec.py:321] Evaluating on the training split.
I0209 20:32:24.334583 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 20:32:27.333003 140039251117888 spec.py:349] Evaluating on the test split.
I0209 20:32:30.339259 140039251117888 submission_runner.py:408] Time since start: 14068.41s, 	Step: 29745, 	{'train/accuracy': 0.9913337826728821, 'train/loss': 0.028022971004247665, 'train/mean_average_precision': 0.4609738230187459, 'validation/accuracy': 0.986785352230072, 'validation/loss': 0.04489719495177269, 'validation/mean_average_precision': 0.2769324470043009, 'validation/num_examples': 43793, 'test/accuracy': 0.9858364462852478, 'test/loss': 0.04776325821876526, 'test/mean_average_precision': 0.2666181185574914, 'test/num_examples': 43793, 'score': 9618.070008039474, 'total_duration': 14068.406420230865, 'accumulated_submission_time': 9618.070008039474, 'accumulated_eval_time': 4448.318630695343, 'accumulated_logging_time': 1.2066032886505127}
I0209 20:32:30.360533 139836634785536 logging_writer.py:48] [29745] accumulated_eval_time=4448.318631, accumulated_logging_time=1.206603, accumulated_submission_time=9618.070008, global_step=29745, preemption_count=0, score=9618.070008, test/accuracy=0.985836, test/loss=0.047763, test/mean_average_precision=0.266618, test/num_examples=43793, total_duration=14068.406420, train/accuracy=0.991334, train/loss=0.028023, train/mean_average_precision=0.460974, validation/accuracy=0.986785, validation/loss=0.044897, validation/mean_average_precision=0.276932, validation/num_examples=43793
I0209 20:32:48.042666 139871926159104 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.104730024933815, loss=0.028959201648831367
I0209 20:33:19.831794 139836634785536 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.09308511018753052, loss=0.027966154739260674
I0209 20:33:51.367643 139871926159104 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.08819442242383957, loss=0.028253717347979546
I0209 20:34:23.536420 139836634785536 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.09065074473619461, loss=0.028158126398921013
I0209 20:34:55.237369 139871926159104 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.11775337904691696, loss=0.027619173750281334
I0209 20:35:27.032976 139836634785536 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.10604988783597946, loss=0.02845160663127899
I0209 20:35:59.030503 139871926159104 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.13626857101917267, loss=0.029592052102088928
I0209 20:36:30.431014 140039251117888 spec.py:321] Evaluating on the training split.
I0209 20:38:10.298832 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 20:38:13.347917 140039251117888 spec.py:349] Evaluating on the test split.
I0209 20:38:16.335441 140039251117888 submission_runner.py:408] Time since start: 14414.40s, 	Step: 30499, 	{'train/accuracy': 0.9914437532424927, 'train/loss': 0.027543263509869576, 'train/mean_average_precision': 0.4647404749149301, 'validation/accuracy': 0.9866364002227783, 'validation/loss': 0.04489878565073013, 'validation/mean_average_precision': 0.27435756338833756, 'validation/num_examples': 43793, 'test/accuracy': 0.9857665300369263, 'test/loss': 0.04794379323720932, 'test/mean_average_precision': 0.26346652635367124, 'test/num_examples': 43793, 'score': 9858.108284711838, 'total_duration': 14414.402602910995, 'accumulated_submission_time': 9858.108284711838, 'accumulated_eval_time': 4554.223012685776, 'accumulated_logging_time': 1.2404890060424805}
I0209 20:38:16.356903 139864233322240 logging_writer.py:48] [30499] accumulated_eval_time=4554.223013, accumulated_logging_time=1.240489, accumulated_submission_time=9858.108285, global_step=30499, preemption_count=0, score=9858.108285, test/accuracy=0.985767, test/loss=0.047944, test/mean_average_precision=0.263467, test/num_examples=43793, total_duration=14414.402603, train/accuracy=0.991444, train/loss=0.027543, train/mean_average_precision=0.464740, validation/accuracy=0.986636, validation/loss=0.044899, validation/mean_average_precision=0.274358, validation/num_examples=43793
I0209 20:38:17.063255 139878399788800 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.13226763904094696, loss=0.0301830992102623
I0209 20:38:49.758690 139864233322240 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.13926813006401062, loss=0.027455253526568413
I0209 20:39:21.892355 139878399788800 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.0966581329703331, loss=0.027728836983442307
I0209 20:39:53.428221 139864233322240 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.10668887197971344, loss=0.02822483889758587
I0209 20:40:25.573888 139878399788800 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.13785307109355927, loss=0.028756441548466682
I0209 20:40:57.846017 139864233322240 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.10939128696918488, loss=0.025221120566129684
I0209 20:41:30.227308 139878399788800 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.10790815949440002, loss=0.030227426439523697
I0209 20:42:02.005051 139864233322240 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.09318721294403076, loss=0.02547815442085266
I0209 20:42:16.375530 140039251117888 spec.py:321] Evaluating on the training split.
I0209 20:43:58.692125 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 20:44:03.938901 140039251117888 spec.py:349] Evaluating on the test split.
I0209 20:44:06.920653 140039251117888 submission_runner.py:408] Time since start: 14764.99s, 	Step: 31246, 	{'train/accuracy': 0.991542637348175, 'train/loss': 0.027170853689312935, 'train/mean_average_precision': 0.4721364663286831, 'validation/accuracy': 0.9867780804634094, 'validation/loss': 0.044928766787052155, 'validation/mean_average_precision': 0.27216707266533563, 'validation/num_examples': 43793, 'test/accuracy': 0.9858992099761963, 'test/loss': 0.04786492511630058, 'test/mean_average_precision': 0.26131082409592704, 'test/num_examples': 43793, 'score': 10098.096035957336, 'total_duration': 14764.987814426422, 'accumulated_submission_time': 10098.096035957336, 'accumulated_eval_time': 4664.768091201782, 'accumulated_logging_time': 1.2731256484985352}
I0209 20:44:06.942851 139836643178240 logging_writer.py:48] [31246] accumulated_eval_time=4664.768091, accumulated_logging_time=1.273126, accumulated_submission_time=10098.096036, global_step=31246, preemption_count=0, score=10098.096036, test/accuracy=0.985899, test/loss=0.047865, test/mean_average_precision=0.261311, test/num_examples=43793, total_duration=14764.987814, train/accuracy=0.991543, train/loss=0.027171, train/mean_average_precision=0.472136, validation/accuracy=0.986778, validation/loss=0.044929, validation/mean_average_precision=0.272167, validation/num_examples=43793
I0209 20:44:24.963313 139871926159104 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.11281897872686386, loss=0.0285867378115654
I0209 20:44:56.763640 139836643178240 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.10039009153842926, loss=0.02678830362856388
I0209 20:45:29.086525 139871926159104 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.09865777939558029, loss=0.027324547991156578
I0209 20:46:01.197569 139836643178240 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.0919531062245369, loss=0.02704578824341297
I0209 20:46:33.559378 139871926159104 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.10590771585702896, loss=0.028049848973751068
I0209 20:47:05.700687 139836643178240 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.08103708922863007, loss=0.024676166474819183
I0209 20:47:37.664558 139871926159104 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.08644234389066696, loss=0.02417672611773014
I0209 20:48:06.960569 140039251117888 spec.py:321] Evaluating on the training split.
I0209 20:49:47.791652 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 20:49:50.763905 140039251117888 spec.py:349] Evaluating on the test split.
I0209 20:49:53.720509 140039251117888 submission_runner.py:408] Time since start: 15111.79s, 	Step: 31991, 	{'train/accuracy': 0.9917091727256775, 'train/loss': 0.026494089514017105, 'train/mean_average_precision': 0.5052146545409436, 'validation/accuracy': 0.9868125915527344, 'validation/loss': 0.045099370181560516, 'validation/mean_average_precision': 0.2773346138812802, 'validation/num_examples': 43793, 'test/accuracy': 0.9860554933547974, 'test/loss': 0.04802294075489044, 'test/mean_average_precision': 0.2684884152472522, 'test/num_examples': 43793, 'score': 10338.08183336258, 'total_duration': 15111.78766322136, 'accumulated_submission_time': 10338.08183336258, 'accumulated_eval_time': 4771.527981519699, 'accumulated_logging_time': 1.307499647140503}
I0209 20:49:53.742250 139836634785536 logging_writer.py:48] [31991] accumulated_eval_time=4771.527982, accumulated_logging_time=1.307500, accumulated_submission_time=10338.081833, global_step=31991, preemption_count=0, score=10338.081833, test/accuracy=0.986055, test/loss=0.048023, test/mean_average_precision=0.268488, test/num_examples=43793, total_duration=15111.787663, train/accuracy=0.991709, train/loss=0.026494, train/mean_average_precision=0.505215, validation/accuracy=0.986813, validation/loss=0.045099, validation/mean_average_precision=0.277335, validation/num_examples=43793
I0209 20:49:57.000656 139864233322240 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.11892572045326233, loss=0.027447989210486412
I0209 20:50:28.596800 139836634785536 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.08831571042537689, loss=0.025271983817219734
I0209 20:51:00.229218 139864233322240 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.0978054329752922, loss=0.02849315106868744
I0209 20:51:32.042970 139836634785536 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.09721260517835617, loss=0.026527533307671547
I0209 20:52:03.995079 139864233322240 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.10297242552042007, loss=0.026511289179325104
I0209 20:52:35.413869 139836634785536 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.09014029055833817, loss=0.026756135746836662
I0209 20:53:07.019891 139864233322240 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.10879866778850555, loss=0.027127228677272797
I0209 20:53:38.796910 139836634785536 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.11970138549804688, loss=0.032041747123003006
I0209 20:53:53.758089 140039251117888 spec.py:321] Evaluating on the training split.
I0209 20:55:30.165690 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 20:55:33.163700 140039251117888 spec.py:349] Evaluating on the test split.
I0209 20:55:36.202601 140039251117888 submission_runner.py:408] Time since start: 15454.27s, 	Step: 32748, 	{'train/accuracy': 0.9920130968093872, 'train/loss': 0.025551490485668182, 'train/mean_average_precision': 0.5120100156677214, 'validation/accuracy': 0.9867143034934998, 'validation/loss': 0.04505418986082077, 'validation/mean_average_precision': 0.2664563256925483, 'validation/num_examples': 43793, 'test/accuracy': 0.9857795834541321, 'test/loss': 0.04795093834400177, 'test/mean_average_precision': 0.2652142004790609, 'test/num_examples': 43793, 'score': 10578.065612077713, 'total_duration': 15454.269760608673, 'accumulated_submission_time': 10578.065612077713, 'accumulated_eval_time': 4873.972446680069, 'accumulated_logging_time': 1.3417332172393799}
I0209 20:55:36.225035 139871926159104 logging_writer.py:48] [32748] accumulated_eval_time=4873.972447, accumulated_logging_time=1.341733, accumulated_submission_time=10578.065612, global_step=32748, preemption_count=0, score=10578.065612, test/accuracy=0.985780, test/loss=0.047951, test/mean_average_precision=0.265214, test/num_examples=43793, total_duration=15454.269761, train/accuracy=0.992013, train/loss=0.025551, train/mean_average_precision=0.512010, validation/accuracy=0.986714, validation/loss=0.045054, validation/mean_average_precision=0.266456, validation/num_examples=43793
I0209 20:55:53.727117 139878399788800 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.13549411296844482, loss=0.02712291292846203
I0209 20:56:26.723120 139871926159104 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.11202731728553772, loss=0.025416886433959007
I0209 20:56:59.327549 139878399788800 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.11598105728626251, loss=0.026874715462327003
I0209 20:57:31.939885 139871926159104 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.1018349826335907, loss=0.028363851830363274
I0209 20:58:05.233296 139878399788800 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.13789725303649902, loss=0.025728054344654083
I0209 20:58:38.372334 139871926159104 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.09970162808895111, loss=0.02691316232085228
I0209 20:59:10.630111 139878399788800 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.13446862995624542, loss=0.025044048205018044
I0209 20:59:36.452736 140039251117888 spec.py:321] Evaluating on the training split.
I0209 21:01:21.986556 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 21:01:25.353210 140039251117888 spec.py:349] Evaluating on the test split.
I0209 21:01:28.691774 140039251117888 submission_runner.py:408] Time since start: 15806.76s, 	Step: 33481, 	{'train/accuracy': 0.9919853806495667, 'train/loss': 0.025528879836201668, 'train/mean_average_precision': 0.5158409082918909, 'validation/accuracy': 0.9867808818817139, 'validation/loss': 0.045148346573114395, 'validation/mean_average_precision': 0.2665157521690351, 'validation/num_examples': 43793, 'test/accuracy': 0.9859358668327332, 'test/loss': 0.048278387635946274, 'test/mean_average_precision': 0.2660083338093088, 'test/num_examples': 43793, 'score': 10818.260104179382, 'total_duration': 15806.758916139603, 'accumulated_submission_time': 10818.260104179382, 'accumulated_eval_time': 4986.211427688599, 'accumulated_logging_time': 1.3757221698760986}
I0209 21:01:28.716330 139836643178240 logging_writer.py:48] [33481] accumulated_eval_time=4986.211428, accumulated_logging_time=1.375722, accumulated_submission_time=10818.260104, global_step=33481, preemption_count=0, score=10818.260104, test/accuracy=0.985936, test/loss=0.048278, test/mean_average_precision=0.266008, test/num_examples=43793, total_duration=15806.758916, train/accuracy=0.991985, train/loss=0.025529, train/mean_average_precision=0.515841, validation/accuracy=0.986781, validation/loss=0.045148, validation/mean_average_precision=0.266516, validation/num_examples=43793
I0209 21:01:35.301973 139864233322240 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.15069125592708588, loss=0.0284416526556015
I0209 21:02:08.283525 139836643178240 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.1737234741449356, loss=0.028232600539922714
I0209 21:02:41.044291 139864233322240 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.1402505338191986, loss=0.02590273879468441
I0209 21:03:13.761795 139836643178240 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.12046372890472412, loss=0.023903390392661095
I0209 21:03:46.454054 139864233322240 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.1181100457906723, loss=0.02747897431254387
I0209 21:04:19.810196 139836643178240 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.1446731984615326, loss=0.028216687962412834
I0209 21:04:53.018949 139864233322240 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.1588047593832016, loss=0.032456304877996445
I0209 21:05:26.155772 139836643178240 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.10238377749919891, loss=0.02551279217004776
I0209 21:05:28.761205 140039251117888 spec.py:321] Evaluating on the training split.
I0209 21:07:08.951194 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 21:07:12.090526 140039251117888 spec.py:349] Evaluating on the test split.
I0209 21:07:15.174779 140039251117888 submission_runner.py:408] Time since start: 16153.24s, 	Step: 34209, 	{'train/accuracy': 0.9919201135635376, 'train/loss': 0.026125213131308556, 'train/mean_average_precision': 0.4842983115492911, 'validation/accuracy': 0.9868369102478027, 'validation/loss': 0.04484208673238754, 'validation/mean_average_precision': 0.27267152273883244, 'validation/num_examples': 43793, 'test/accuracy': 0.9858587980270386, 'test/loss': 0.04812852293252945, 'test/mean_average_precision': 0.26192709755040916, 'test/num_examples': 43793, 'score': 11058.269440174103, 'total_duration': 16153.24194407463, 'accumulated_submission_time': 11058.269440174103, 'accumulated_eval_time': 5092.624956607819, 'accumulated_logging_time': 1.4126989841461182}
I0209 21:07:15.196615 139836634785536 logging_writer.py:48] [34209] accumulated_eval_time=5092.624957, accumulated_logging_time=1.412699, accumulated_submission_time=11058.269440, global_step=34209, preemption_count=0, score=11058.269440, test/accuracy=0.985859, test/loss=0.048129, test/mean_average_precision=0.261927, test/num_examples=43793, total_duration=16153.241944, train/accuracy=0.991920, train/loss=0.026125, train/mean_average_precision=0.484298, validation/accuracy=0.986837, validation/loss=0.044842, validation/mean_average_precision=0.272672, validation/num_examples=43793
I0209 21:07:44.970225 139878399788800 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.11718504875898361, loss=0.028287729248404503
I0209 21:08:17.347187 139836634785536 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.12078089267015457, loss=0.03080986626446247
I0209 21:08:49.757311 139878399788800 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.08211132884025574, loss=0.025397302582859993
I0209 21:09:22.440797 139836634785536 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.1122419685125351, loss=0.02430705353617668
I0209 21:09:55.857138 139878399788800 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.10394813120365143, loss=0.02807692624628544
I0209 21:10:28.616155 139836634785536 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.10853259265422821, loss=0.026903394609689713
I0209 21:11:00.747861 139878399788800 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.18320216238498688, loss=0.028007645159959793
I0209 21:11:15.316835 140039251117888 spec.py:321] Evaluating on the training split.
I0209 21:12:58.878978 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 21:13:02.200719 140039251117888 spec.py:349] Evaluating on the test split.
I0209 21:13:05.287703 140039251117888 submission_runner.py:408] Time since start: 16503.35s, 	Step: 34945, 	{'train/accuracy': 0.991661548614502, 'train/loss': 0.026801934465765953, 'train/mean_average_precision': 0.4983068987457481, 'validation/accuracy': 0.9867764711380005, 'validation/loss': 0.045043252408504486, 'validation/mean_average_precision': 0.2768250623646225, 'validation/num_examples': 43793, 'test/accuracy': 0.98597252368927, 'test/loss': 0.048193011432886124, 'test/mean_average_precision': 0.2647074282382107, 'test/num_examples': 43793, 'score': 11298.358862400055, 'total_duration': 16503.35486650467, 'accumulated_submission_time': 11298.358862400055, 'accumulated_eval_time': 5202.5957906246185, 'accumulated_logging_time': 1.445598840713501}
I0209 21:13:05.310427 139836643178240 logging_writer.py:48] [34945] accumulated_eval_time=5202.595791, accumulated_logging_time=1.445599, accumulated_submission_time=11298.358862, global_step=34945, preemption_count=0, score=11298.358862, test/accuracy=0.985973, test/loss=0.048193, test/mean_average_precision=0.264707, test/num_examples=43793, total_duration=16503.354867, train/accuracy=0.991662, train/loss=0.026802, train/mean_average_precision=0.498307, validation/accuracy=0.986776, validation/loss=0.045043, validation/mean_average_precision=0.276825, validation/num_examples=43793
I0209 21:13:23.208041 139864233322240 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.10754331946372986, loss=0.028117075562477112
I0209 21:13:55.478402 139836643178240 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.12772133946418762, loss=0.03038620948791504
I0209 21:14:28.022142 139864233322240 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.13400323688983917, loss=0.026037471368908882
I0209 21:15:00.587404 139836643178240 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.09382946044206619, loss=0.02614964172244072
I0209 21:15:32.892991 139864233322240 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.137168750166893, loss=0.029199304059147835
I0209 21:16:04.861418 139836643178240 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.1270836889743805, loss=0.025835197418928146
I0209 21:16:36.802394 139864233322240 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.11673726886510849, loss=0.026254769414663315
I0209 21:17:05.362336 140039251117888 spec.py:321] Evaluating on the training split.
I0209 21:18:44.796721 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 21:18:47.908039 140039251117888 spec.py:349] Evaluating on the test split.
I0209 21:18:50.971877 140039251117888 submission_runner.py:408] Time since start: 16849.04s, 	Step: 35690, 	{'train/accuracy': 0.9916203618049622, 'train/loss': 0.026906408369541168, 'train/mean_average_precision': 0.4801378497014065, 'validation/accuracy': 0.9867455959320068, 'validation/loss': 0.04513007029891014, 'validation/mean_average_precision': 0.26837549418785717, 'validation/num_examples': 43793, 'test/accuracy': 0.9858074188232422, 'test/loss': 0.04827474057674408, 'test/mean_average_precision': 0.2578979398603731, 'test/num_examples': 43793, 'score': 11538.379633426666, 'total_duration': 16849.039041757584, 'accumulated_submission_time': 11538.379633426666, 'accumulated_eval_time': 5308.205292224884, 'accumulated_logging_time': 1.4793949127197266}
I0209 21:18:50.994267 139836634785536 logging_writer.py:48] [35690] accumulated_eval_time=5308.205292, accumulated_logging_time=1.479395, accumulated_submission_time=11538.379633, global_step=35690, preemption_count=0, score=11538.379633, test/accuracy=0.985807, test/loss=0.048275, test/mean_average_precision=0.257898, test/num_examples=43793, total_duration=16849.039042, train/accuracy=0.991620, train/loss=0.026906, train/mean_average_precision=0.480138, validation/accuracy=0.986746, validation/loss=0.045130, validation/mean_average_precision=0.268375, validation/num_examples=43793
I0209 21:18:54.531579 139878399788800 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.12325691431760788, loss=0.026134522631764412
I0209 21:19:26.591088 139836634785536 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.11875008046627045, loss=0.027254872024059296
I0209 21:19:58.532859 139878399788800 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.12575702369213104, loss=0.0277200136333704
I0209 21:20:30.715099 139836634785536 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.11799576133489609, loss=0.02724924497306347
I0209 21:21:02.859333 139878399788800 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.16092507541179657, loss=0.025735752657055855
I0209 21:21:35.985536 139836634785536 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.11052284389734268, loss=0.02591879852116108
I0209 21:22:07.940184 139878399788800 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.1197890117764473, loss=0.027610186487436295
I0209 21:22:39.932743 139836634785536 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.10125429183244705, loss=0.027245184406638145
I0209 21:22:51.168031 140039251117888 spec.py:321] Evaluating on the training split.
I0209 21:24:36.162609 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 21:24:39.276224 140039251117888 spec.py:349] Evaluating on the test split.
I0209 21:24:42.300108 140039251117888 submission_runner.py:408] Time since start: 17200.37s, 	Step: 36436, 	{'train/accuracy': 0.9917700290679932, 'train/loss': 0.026203596964478493, 'train/mean_average_precision': 0.5078279699959175, 'validation/accuracy': 0.9868600368499756, 'validation/loss': 0.044907744973897934, 'validation/mean_average_precision': 0.279754069660172, 'validation/num_examples': 43793, 'test/accuracy': 0.9859842658042908, 'test/loss': 0.04798304662108421, 'test/mean_average_precision': 0.2682538265776146, 'test/num_examples': 43793, 'score': 11778.522836446762, 'total_duration': 17200.367270946503, 'accumulated_submission_time': 11778.522836446762, 'accumulated_eval_time': 5419.337324857712, 'accumulated_logging_time': 1.5126359462738037}
I0209 21:24:42.322106 139836643178240 logging_writer.py:48] [36436] accumulated_eval_time=5419.337325, accumulated_logging_time=1.512636, accumulated_submission_time=11778.522836, global_step=36436, preemption_count=0, score=11778.522836, test/accuracy=0.985984, test/loss=0.047983, test/mean_average_precision=0.268254, test/num_examples=43793, total_duration=17200.367271, train/accuracy=0.991770, train/loss=0.026204, train/mean_average_precision=0.507828, validation/accuracy=0.986860, validation/loss=0.044908, validation/mean_average_precision=0.279754, validation/num_examples=43793
I0209 21:25:03.342066 139871926159104 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.14046816527843475, loss=0.02936963364481926
I0209 21:25:35.263729 139836643178240 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.1119765043258667, loss=0.025619810447096825
I0209 21:26:07.213895 139871926159104 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.16275304555892944, loss=0.02536165341734886
I0209 21:26:38.996220 139836643178240 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.11894761770963669, loss=0.02600778080523014
I0209 21:27:11.183758 139871926159104 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.1200278103351593, loss=0.027302883565425873
I0209 21:27:42.996632 139836643178240 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.11889535933732986, loss=0.026393286883831024
I0209 21:28:14.764668 139871926159104 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.1150246113538742, loss=0.025026392191648483
I0209 21:28:42.344466 140039251117888 spec.py:321] Evaluating on the training split.
I0209 21:30:22.276927 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 21:30:25.345865 140039251117888 spec.py:349] Evaluating on the test split.
I0209 21:30:28.414878 140039251117888 submission_runner.py:408] Time since start: 17546.48s, 	Step: 37188, 	{'train/accuracy': 0.9918221831321716, 'train/loss': 0.026032911613583565, 'train/mean_average_precision': 0.4927025177955553, 'validation/accuracy': 0.9868361353874207, 'validation/loss': 0.04488707333803177, 'validation/mean_average_precision': 0.2795414593961319, 'validation/num_examples': 43793, 'test/accuracy': 0.9859581589698792, 'test/loss': 0.048093561083078384, 'test/mean_average_precision': 0.26502508387445306, 'test/num_examples': 43793, 'score': 12018.514259815216, 'total_duration': 17546.48204088211, 'accumulated_submission_time': 12018.514259815216, 'accumulated_eval_time': 5525.407692909241, 'accumulated_logging_time': 1.5457723140716553}
I0209 21:30:28.437354 139836634785536 logging_writer.py:48] [37188] accumulated_eval_time=5525.407693, accumulated_logging_time=1.545772, accumulated_submission_time=12018.514260, global_step=37188, preemption_count=0, score=12018.514260, test/accuracy=0.985958, test/loss=0.048094, test/mean_average_precision=0.265025, test/num_examples=43793, total_duration=17546.482041, train/accuracy=0.991822, train/loss=0.026033, train/mean_average_precision=0.492703, validation/accuracy=0.986836, validation/loss=0.044887, validation/mean_average_precision=0.279541, validation/num_examples=43793
I0209 21:30:32.638907 139864233322240 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.1076129600405693, loss=0.025560708716511726
I0209 21:31:04.843790 139836634785536 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.13817869126796722, loss=0.026232363656163216
I0209 21:31:36.632403 139864233322240 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.09769777208566666, loss=0.023071233183145523
I0209 21:32:08.383954 139836634785536 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.09886854887008667, loss=0.02453620173037052
I0209 21:32:40.641463 139864233322240 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.10308553278446198, loss=0.024926574900746346
I0209 21:33:12.807028 139836634785536 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.10048156976699829, loss=0.0232028067111969
I0209 21:33:45.086293 139864233322240 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.14603707194328308, loss=0.026837259531021118
I0209 21:34:17.226144 139836634785536 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.11407408118247986, loss=0.024538690224289894
I0209 21:34:28.501639 140039251117888 spec.py:321] Evaluating on the training split.
I0209 21:36:05.662890 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 21:36:08.695206 140039251117888 spec.py:349] Evaluating on the test split.
I0209 21:36:11.633286 140039251117888 submission_runner.py:408] Time since start: 17889.70s, 	Step: 37936, 	{'train/accuracy': 0.9920411109924316, 'train/loss': 0.02535792626440525, 'train/mean_average_precision': 0.521290996199623, 'validation/accuracy': 0.9868373274803162, 'validation/loss': 0.04548921436071396, 'validation/mean_average_precision': 0.27167468894968716, 'validation/num_examples': 43793, 'test/accuracy': 0.9859695434570312, 'test/loss': 0.04852062836289406, 'test/mean_average_precision': 0.2581425592235508, 'test/num_examples': 43793, 'score': 12258.54852104187, 'total_duration': 17889.70045185089, 'accumulated_submission_time': 12258.54852104187, 'accumulated_eval_time': 5628.5393006801605, 'accumulated_logging_time': 1.5792734622955322}
I0209 21:36:11.655875 139871926159104 logging_writer.py:48] [37936] accumulated_eval_time=5628.539301, accumulated_logging_time=1.579273, accumulated_submission_time=12258.548521, global_step=37936, preemption_count=0, score=12258.548521, test/accuracy=0.985970, test/loss=0.048521, test/mean_average_precision=0.258143, test/num_examples=43793, total_duration=17889.700452, train/accuracy=0.992041, train/loss=0.025358, train/mean_average_precision=0.521291, validation/accuracy=0.986837, validation/loss=0.045489, validation/mean_average_precision=0.271675, validation/num_examples=43793
I0209 21:36:32.672561 139878399788800 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.12124411761760712, loss=0.024034112691879272
I0209 21:37:04.936589 139871926159104 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.115476593375206, loss=0.021981388330459595
I0209 21:37:36.762885 139878399788800 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.10521700978279114, loss=0.024320239201188087
I0209 21:38:08.760326 139871926159104 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.16625718772411346, loss=0.028970446437597275
I0209 21:38:40.591024 139878399788800 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.13987188041210175, loss=0.02314145490527153
I0209 21:39:12.689740 139871926159104 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.1294717937707901, loss=0.02482086978852749
I0209 21:39:45.508031 139878399788800 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.10374128073453903, loss=0.024037517607212067
I0209 21:40:11.870579 140039251117888 spec.py:321] Evaluating on the training split.
I0209 21:41:51.520788 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 21:41:54.660550 140039251117888 spec.py:349] Evaluating on the test split.
I0209 21:41:57.668592 140039251117888 submission_runner.py:408] Time since start: 18235.74s, 	Step: 38681, 	{'train/accuracy': 0.9922906160354614, 'train/loss': 0.02447586879134178, 'train/mean_average_precision': 0.5456552959656337, 'validation/accuracy': 0.9868028163909912, 'validation/loss': 0.04524470493197441, 'validation/mean_average_precision': 0.2777050270904259, 'validation/num_examples': 43793, 'test/accuracy': 0.9859328866004944, 'test/loss': 0.04847346618771553, 'test/mean_average_precision': 0.2601893972789249, 'test/num_examples': 43793, 'score': 12498.732014417648, 'total_duration': 18235.73575282097, 'accumulated_submission_time': 12498.732014417648, 'accumulated_eval_time': 5734.337277889252, 'accumulated_logging_time': 1.6127097606658936}
I0209 21:41:57.691935 139836643178240 logging_writer.py:48] [38681] accumulated_eval_time=5734.337278, accumulated_logging_time=1.612710, accumulated_submission_time=12498.732014, global_step=38681, preemption_count=0, score=12498.732014, test/accuracy=0.985933, test/loss=0.048473, test/mean_average_precision=0.260189, test/num_examples=43793, total_duration=18235.735753, train/accuracy=0.992291, train/loss=0.024476, train/mean_average_precision=0.545655, validation/accuracy=0.986803, validation/loss=0.045245, validation/mean_average_precision=0.277705, validation/num_examples=43793
I0209 21:42:04.315978 139864233322240 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.12197639793157578, loss=0.028429534286260605
I0209 21:42:36.275548 139836643178240 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.10898857563734055, loss=0.025614798069000244
I0209 21:43:08.529701 139864233322240 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.10566133260726929, loss=0.02573706954717636
I0209 21:43:40.532381 139836643178240 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.10606766492128372, loss=0.023174665868282318
I0209 21:44:12.351290 139864233322240 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.10870171338319778, loss=0.022818418219685555
I0209 21:44:44.663268 139836643178240 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.14590848982334137, loss=0.02766610123217106
I0209 21:45:16.721739 139864233322240 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.18087756633758545, loss=0.024895207956433296
I0209 21:45:48.604249 139836643178240 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.10840877145528793, loss=0.02675759419798851
I0209 21:45:57.757812 140039251117888 spec.py:321] Evaluating on the training split.
I0209 21:47:40.228002 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 21:47:43.233343 140039251117888 spec.py:349] Evaluating on the test split.
I0209 21:47:46.199916 140039251117888 submission_runner.py:408] Time since start: 18584.27s, 	Step: 39429, 	{'train/accuracy': 0.9927371740341187, 'train/loss': 0.023332200944423676, 'train/mean_average_precision': 0.5693393673773479, 'validation/accuracy': 0.9867658615112305, 'validation/loss': 0.04525458440184593, 'validation/mean_average_precision': 0.2783459699096976, 'validation/num_examples': 43793, 'test/accuracy': 0.9858764410018921, 'test/loss': 0.04857688024640083, 'test/mean_average_precision': 0.26199890137701637, 'test/num_examples': 43793, 'score': 12738.766446590424, 'total_duration': 18584.267076969147, 'accumulated_submission_time': 12738.766446590424, 'accumulated_eval_time': 5842.7793345451355, 'accumulated_logging_time': 1.6485440731048584}
I0209 21:47:46.223844 139836634785536 logging_writer.py:48] [39429] accumulated_eval_time=5842.779335, accumulated_logging_time=1.648544, accumulated_submission_time=12738.766447, global_step=39429, preemption_count=0, score=12738.766447, test/accuracy=0.985876, test/loss=0.048577, test/mean_average_precision=0.261999, test/num_examples=43793, total_duration=18584.267077, train/accuracy=0.992737, train/loss=0.023332, train/mean_average_precision=0.569339, validation/accuracy=0.986766, validation/loss=0.045255, validation/mean_average_precision=0.278346, validation/num_examples=43793
I0209 21:48:09.377588 139871926159104 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.11925151944160461, loss=0.024947650730609894
I0209 21:48:41.156891 139836634785536 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.14222557842731476, loss=0.027364086359739304
I0209 21:49:13.069336 139871926159104 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.12950976192951202, loss=0.02881503663957119
I0209 21:49:44.585909 139836634785536 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.120473213493824, loss=0.027284206822514534
I0209 21:50:17.090472 139871926159104 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.13740240037441254, loss=0.0272917952388525
I0209 21:50:49.365356 139836634785536 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.16162683069705963, loss=0.024976516142487526
I0209 21:51:21.917760 139871926159104 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.11154042184352875, loss=0.023520831018686295
I0209 21:51:46.362881 140039251117888 spec.py:321] Evaluating on the training split.
I0209 21:53:22.943843 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 21:53:25.938036 140039251117888 spec.py:349] Evaluating on the test split.
I0209 21:53:28.888774 140039251117888 submission_runner.py:408] Time since start: 18926.96s, 	Step: 40178, 	{'train/accuracy': 0.9926030039787292, 'train/loss': 0.023707717657089233, 'train/mean_average_precision': 0.5622201792353998, 'validation/accuracy': 0.9866286516189575, 'validation/loss': 0.04526278376579285, 'validation/mean_average_precision': 0.27261435378096305, 'validation/num_examples': 43793, 'test/accuracy': 0.9857779145240784, 'test/loss': 0.04833337664604187, 'test/mean_average_precision': 0.2643365091769727, 'test/num_examples': 43793, 'score': 12978.871631383896, 'total_duration': 18926.955935239792, 'accumulated_submission_time': 12978.871631383896, 'accumulated_eval_time': 5945.3051841259, 'accumulated_logging_time': 1.685373306274414}
I0209 21:53:28.911815 139836643178240 logging_writer.py:48] [40178] accumulated_eval_time=5945.305184, accumulated_logging_time=1.685373, accumulated_submission_time=12978.871631, global_step=40178, preemption_count=0, score=12978.871631, test/accuracy=0.985778, test/loss=0.048333, test/mean_average_precision=0.264337, test/num_examples=43793, total_duration=18926.955935, train/accuracy=0.992603, train/loss=0.023708, train/mean_average_precision=0.562220, validation/accuracy=0.986629, validation/loss=0.045263, validation/mean_average_precision=0.272614, validation/num_examples=43793
I0209 21:53:36.303614 139878399788800 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.123830147087574, loss=0.021970776841044426
I0209 21:54:08.473711 139836643178240 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.1244882345199585, loss=0.026757730171084404
I0209 21:54:40.672345 139878399788800 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.2004036158323288, loss=0.029138362035155296
I0209 21:55:13.257770 139836643178240 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.13883569836616516, loss=0.027559854090213776
I0209 21:55:46.052995 139878399788800 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.10831514745950699, loss=0.024108385667204857
I0209 21:56:18.462402 139836643178240 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.12209359556436539, loss=0.027579475194215775
I0209 21:56:50.672519 139878399788800 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.13068339228630066, loss=0.02430698834359646
I0209 21:57:22.539401 139836643178240 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.12305358052253723, loss=0.026584219187498093
I0209 21:57:29.220773 140039251117888 spec.py:321] Evaluating on the training split.
I0209 21:59:11.492758 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 21:59:14.934555 140039251117888 spec.py:349] Evaluating on the test split.
I0209 21:59:18.285768 140039251117888 submission_runner.py:408] Time since start: 19276.35s, 	Step: 40922, 	{'train/accuracy': 0.9924303889274597, 'train/loss': 0.02415706031024456, 'train/mean_average_precision': 0.531331590513631, 'validation/accuracy': 0.9868612885475159, 'validation/loss': 0.04556350037455559, 'validation/mean_average_precision': 0.2747620842624564, 'validation/num_examples': 43793, 'test/accuracy': 0.9859089255332947, 'test/loss': 0.04895516857504845, 'test/mean_average_precision': 0.26250794162981905, 'test/num_examples': 43793, 'score': 13219.148606538773, 'total_duration': 19276.352915763855, 'accumulated_submission_time': 13219.148606538773, 'accumulated_eval_time': 6054.370141029358, 'accumulated_logging_time': 1.7200186252593994}
I0209 21:59:18.312572 139836634785536 logging_writer.py:48] [40922] accumulated_eval_time=6054.370141, accumulated_logging_time=1.720019, accumulated_submission_time=13219.148607, global_step=40922, preemption_count=0, score=13219.148607, test/accuracy=0.985909, test/loss=0.048955, test/mean_average_precision=0.262508, test/num_examples=43793, total_duration=19276.352916, train/accuracy=0.992430, train/loss=0.024157, train/mean_average_precision=0.531332, validation/accuracy=0.986861, validation/loss=0.045564, validation/mean_average_precision=0.274762, validation/num_examples=43793
I0209 21:59:44.264241 139864233322240 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.13318568468093872, loss=0.02646615542471409
I0209 22:00:17.308275 139836634785536 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.22671420872211456, loss=0.02508552558720112
I0209 22:00:50.271908 139864233322240 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.1291099190711975, loss=0.0256207138299942
I0209 22:01:22.207752 139836634785536 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.11599074304103851, loss=0.02743518352508545
I0209 22:01:53.882753 139864233322240 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.12711210548877716, loss=0.023495152592658997
I0209 22:02:25.728584 139836634785536 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.16197387874126434, loss=0.024406936019659042
I0209 22:02:57.272222 139864233322240 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.1048857793211937, loss=0.021666904911398888
I0209 22:03:18.345972 140039251117888 spec.py:321] Evaluating on the training split.
I0209 22:04:56.177816 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 22:04:59.192389 140039251117888 spec.py:349] Evaluating on the test split.
I0209 22:05:02.359747 140039251117888 submission_runner.py:408] Time since start: 19620.43s, 	Step: 41668, 	{'train/accuracy': 0.9921717047691345, 'train/loss': 0.024784540757536888, 'train/mean_average_precision': 0.5356050889597663, 'validation/accuracy': 0.986797571182251, 'validation/loss': 0.04562019556760788, 'validation/mean_average_precision': 0.27944630554689964, 'validation/num_examples': 43793, 'test/accuracy': 0.9858528971672058, 'test/loss': 0.04906835779547691, 'test/mean_average_precision': 0.2610362912805396, 'test/num_examples': 43793, 'score': 13459.148141384125, 'total_duration': 19620.42688894272, 'accumulated_submission_time': 13459.148141384125, 'accumulated_eval_time': 6158.383851766586, 'accumulated_logging_time': 1.759134292602539}
I0209 22:05:02.384073 139836643178240 logging_writer.py:48] [41668] accumulated_eval_time=6158.383852, accumulated_logging_time=1.759134, accumulated_submission_time=13459.148141, global_step=41668, preemption_count=0, score=13459.148141, test/accuracy=0.985853, test/loss=0.049068, test/mean_average_precision=0.261036, test/num_examples=43793, total_duration=19620.426889, train/accuracy=0.992172, train/loss=0.024785, train/mean_average_precision=0.535605, validation/accuracy=0.986798, validation/loss=0.045620, validation/mean_average_precision=0.279446, validation/num_examples=43793
I0209 22:05:12.890292 139871926159104 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.14603692293167114, loss=0.024634625762701035
I0209 22:05:44.787837 139836643178240 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.13069415092468262, loss=0.02595767006278038
I0209 22:06:17.011731 139871926159104 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.11557842046022415, loss=0.026115983724594116
I0209 22:06:48.801788 139836643178240 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.1385435163974762, loss=0.021613063290715218
I0209 22:07:21.215029 139871926159104 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.13591155409812927, loss=0.024310125038027763
I0209 22:07:53.019489 139836643178240 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.12670519948005676, loss=0.02851363644003868
I0209 22:08:25.691432 139871926159104 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.11943813413381577, loss=0.02483369968831539
I0209 22:08:58.714123 139836643178240 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.12414075434207916, loss=0.026768570765852928
I0209 22:09:02.460407 140039251117888 spec.py:321] Evaluating on the training split.
I0209 22:10:45.478054 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 22:10:48.567955 140039251117888 spec.py:349] Evaluating on the test split.
I0209 22:10:51.554601 140039251117888 submission_runner.py:408] Time since start: 19969.62s, 	Step: 42412, 	{'train/accuracy': 0.9920896291732788, 'train/loss': 0.02493622526526451, 'train/mean_average_precision': 0.5138407443009074, 'validation/accuracy': 0.9867837429046631, 'validation/loss': 0.046225953847169876, 'validation/mean_average_precision': 0.2705062063880303, 'validation/num_examples': 43793, 'test/accuracy': 0.9858364462852478, 'test/loss': 0.049579277634620667, 'test/mean_average_precision': 0.2634238571883222, 'test/num_examples': 43793, 'score': 13699.192795991898, 'total_duration': 19969.621764421463, 'accumulated_submission_time': 13699.192795991898, 'accumulated_eval_time': 6267.478013277054, 'accumulated_logging_time': 1.794682502746582}
I0209 22:10:51.578520 139836634785536 logging_writer.py:48] [42412] accumulated_eval_time=6267.478013, accumulated_logging_time=1.794683, accumulated_submission_time=13699.192796, global_step=42412, preemption_count=0, score=13699.192796, test/accuracy=0.985836, test/loss=0.049579, test/mean_average_precision=0.263424, test/num_examples=43793, total_duration=19969.621764, train/accuracy=0.992090, train/loss=0.024936, train/mean_average_precision=0.513841, validation/accuracy=0.986784, validation/loss=0.046226, validation/mean_average_precision=0.270506, validation/num_examples=43793
I0209 22:11:20.094460 139878399788800 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.1499091237783432, loss=0.023698443546891212
I0209 22:11:51.957295 139836634785536 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.17579826712608337, loss=0.023495493456721306
I0209 22:12:24.147629 139878399788800 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.13489581644535065, loss=0.02237086370587349
I0209 22:12:56.170450 139836634785536 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.12502485513687134, loss=0.025416851043701172
I0209 22:13:28.255552 139878399788800 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.12459325790405273, loss=0.0220682043582201
I0209 22:14:00.301631 139836634785536 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.1389511674642563, loss=0.026606490835547447
I0209 22:14:32.629770 139878399788800 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.14014597237110138, loss=0.025320591405034065
I0209 22:14:51.821664 140039251117888 spec.py:321] Evaluating on the training split.
I0209 22:16:28.064387 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 22:16:31.114437 140039251117888 spec.py:349] Evaluating on the test split.
I0209 22:16:34.132085 140039251117888 submission_runner.py:408] Time since start: 20312.20s, 	Step: 43161, 	{'train/accuracy': 0.9922144412994385, 'train/loss': 0.02469654195010662, 'train/mean_average_precision': 0.5323905620648921, 'validation/accuracy': 0.9867528676986694, 'validation/loss': 0.04586846008896828, 'validation/mean_average_precision': 0.27743120946362904, 'validation/num_examples': 43793, 'test/accuracy': 0.9857585430145264, 'test/loss': 0.04930279403924942, 'test/mean_average_precision': 0.2641423804382998, 'test/num_examples': 43793, 'score': 13939.405371427536, 'total_duration': 20312.19924545288, 'accumulated_submission_time': 13939.405371427536, 'accumulated_eval_time': 6369.788389205933, 'accumulated_logging_time': 1.8295137882232666}
I0209 22:16:34.159815 139836643178240 logging_writer.py:48] [43161] accumulated_eval_time=6369.788389, accumulated_logging_time=1.829514, accumulated_submission_time=13939.405371, global_step=43161, preemption_count=0, score=13939.405371, test/accuracy=0.985759, test/loss=0.049303, test/mean_average_precision=0.264142, test/num_examples=43793, total_duration=20312.199245, train/accuracy=0.992214, train/loss=0.024697, train/mean_average_precision=0.532391, validation/accuracy=0.986753, validation/loss=0.045868, validation/mean_average_precision=0.277431, validation/num_examples=43793
I0209 22:16:47.052976 139864233322240 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.11285074055194855, loss=0.022464245557785034
I0209 22:17:19.557416 139836643178240 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.12339846044778824, loss=0.021870389580726624
I0209 22:17:51.799257 139864233322240 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.1337776631116867, loss=0.024644067510962486
I0209 22:18:24.211624 139836643178240 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.1419796645641327, loss=0.023732831701636314
I0209 22:18:56.197599 139864233322240 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.16568417847156525, loss=0.02428792230784893
I0209 22:19:27.967417 139836643178240 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.1319863349199295, loss=0.02467316761612892
I0209 22:19:59.885087 139864233322240 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.13652890920639038, loss=0.022024646401405334
I0209 22:20:31.726436 139836643178240 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.1784098595380783, loss=0.02636401355266571
I0209 22:20:34.223597 140039251117888 spec.py:321] Evaluating on the training split.
I0209 22:22:17.570142 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 22:22:20.610091 140039251117888 spec.py:349] Evaluating on the test split.
I0209 22:22:23.716128 140039251117888 submission_runner.py:408] Time since start: 20661.78s, 	Step: 43909, 	{'train/accuracy': 0.9923882484436035, 'train/loss': 0.024186132475733757, 'train/mean_average_precision': 0.5461527346502013, 'validation/accuracy': 0.9866071343421936, 'validation/loss': 0.04615493491292, 'validation/mean_average_precision': 0.27460194184325004, 'validation/num_examples': 43793, 'test/accuracy': 0.9857884645462036, 'test/loss': 0.04912806674838066, 'test/mean_average_precision': 0.2697252043773885, 'test/num_examples': 43793, 'score': 14179.438196897507, 'total_duration': 20661.7832839489, 'accumulated_submission_time': 14179.438196897507, 'accumulated_eval_time': 6479.280866146088, 'accumulated_logging_time': 1.8684642314910889}
I0209 22:22:23.740876 139871926159104 logging_writer.py:48] [43909] accumulated_eval_time=6479.280866, accumulated_logging_time=1.868464, accumulated_submission_time=14179.438197, global_step=43909, preemption_count=0, score=14179.438197, test/accuracy=0.985788, test/loss=0.049128, test/mean_average_precision=0.269725, test/num_examples=43793, total_duration=20661.783284, train/accuracy=0.992388, train/loss=0.024186, train/mean_average_precision=0.546153, validation/accuracy=0.986607, validation/loss=0.046155, validation/mean_average_precision=0.274602, validation/num_examples=43793
I0209 22:22:52.875638 139878399788800 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.1483784019947052, loss=0.023057637736201286
I0209 22:23:24.716392 139871926159104 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.1414087563753128, loss=0.025927098467946053
I0209 22:23:56.929846 139878399788800 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.1336503028869629, loss=0.025915656238794327
I0209 22:24:29.309010 139871926159104 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.13716958463191986, loss=0.024158848449587822
I0209 22:25:01.170027 139878399788800 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.14449676871299744, loss=0.022318042814731598
I0209 22:25:33.256641 139871926159104 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.15527993440628052, loss=0.022795652970671654
I0209 22:26:05.162836 139878399788800 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.17078354954719543, loss=0.02356684021651745
I0209 22:26:23.946004 140039251117888 spec.py:321] Evaluating on the training split.
I0209 22:28:02.011992 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 22:28:05.038979 140039251117888 spec.py:349] Evaluating on the test split.
I0209 22:28:07.966807 140039251117888 submission_runner.py:408] Time since start: 21006.03s, 	Step: 44660, 	{'train/accuracy': 0.9926992058753967, 'train/loss': 0.02298043482005596, 'train/mean_average_precision': 0.573160671933495, 'validation/accuracy': 0.9866668581962585, 'validation/loss': 0.04609571397304535, 'validation/mean_average_precision': 0.27831066848668895, 'validation/num_examples': 43793, 'test/accuracy': 0.9857361912727356, 'test/loss': 0.04914650693535805, 'test/mean_average_precision': 0.26561351397120253, 'test/num_examples': 43793, 'score': 14419.610810041428, 'total_duration': 21006.03396821022, 'accumulated_submission_time': 14419.610810041428, 'accumulated_eval_time': 6583.301622629166, 'accumulated_logging_time': 1.906172752380371}
I0209 22:28:07.990581 139836634785536 logging_writer.py:48] [44660] accumulated_eval_time=6583.301623, accumulated_logging_time=1.906173, accumulated_submission_time=14419.610810, global_step=44660, preemption_count=0, score=14419.610810, test/accuracy=0.985736, test/loss=0.049147, test/mean_average_precision=0.265614, test/num_examples=43793, total_duration=21006.033968, train/accuracy=0.992699, train/loss=0.022980, train/mean_average_precision=0.573161, validation/accuracy=0.986667, validation/loss=0.046096, validation/mean_average_precision=0.278311, validation/num_examples=43793
I0209 22:28:21.113391 139836643178240 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.12481369078159332, loss=0.023058682680130005
I0209 22:28:52.861039 139836634785536 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.12408477813005447, loss=0.02579421177506447
I0209 22:29:25.091599 139836643178240 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.13099513947963715, loss=0.024425651878118515
I0209 22:29:57.054658 139836634785536 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.17665427923202515, loss=0.024804171174764633
I0209 22:30:28.966372 139836643178240 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.15774385631084442, loss=0.02599414251744747
I0209 22:31:00.593266 139836634785536 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.15533408522605896, loss=0.025191832333803177
I0209 22:31:32.930664 139836643178240 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.13686387240886688, loss=0.023817645385861397
I0209 22:32:04.986274 139836634785536 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.15715520083904266, loss=0.02548089250922203
I0209 22:32:08.216158 140039251117888 spec.py:321] Evaluating on the training split.
I0209 22:33:47.306085 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 22:33:50.327680 140039251117888 spec.py:349] Evaluating on the test split.
I0209 22:33:53.298017 140039251117888 submission_runner.py:408] Time since start: 21351.37s, 	Step: 45411, 	{'train/accuracy': 0.992841899394989, 'train/loss': 0.02246236242353916, 'train/mean_average_precision': 0.5858940454668893, 'validation/accuracy': 0.9867216348648071, 'validation/loss': 0.04655701294541359, 'validation/mean_average_precision': 0.27335724604063805, 'validation/num_examples': 43793, 'test/accuracy': 0.9858831763267517, 'test/loss': 0.05000138655304909, 'test/mean_average_precision': 0.2606782498808195, 'test/num_examples': 43793, 'score': 14659.80555152893, 'total_duration': 21351.36517882347, 'accumulated_submission_time': 14659.80555152893, 'accumulated_eval_time': 6688.38343501091, 'accumulated_logging_time': 1.9410452842712402}
I0209 22:33:53.325059 139864233322240 logging_writer.py:48] [45411] accumulated_eval_time=6688.383435, accumulated_logging_time=1.941045, accumulated_submission_time=14659.805552, global_step=45411, preemption_count=0, score=14659.805552, test/accuracy=0.985883, test/loss=0.050001, test/mean_average_precision=0.260678, test/num_examples=43793, total_duration=21351.365179, train/accuracy=0.992842, train/loss=0.022462, train/mean_average_precision=0.585894, validation/accuracy=0.986722, validation/loss=0.046557, validation/mean_average_precision=0.273357, validation/num_examples=43793
I0209 22:34:22.676570 139871926159104 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.14710097014904022, loss=0.024769999086856842
I0209 22:34:54.804059 139864233322240 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.13671237230300903, loss=0.0206750500947237
I0209 22:35:27.645240 139871926159104 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.15741540491580963, loss=0.022590480744838715
I0209 22:36:00.479251 139864233322240 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.152406707406044, loss=0.022578038275241852
I0209 22:36:32.646198 139871926159104 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.1683073192834854, loss=0.02224869839847088
I0209 22:37:04.578070 139864233322240 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.14283163845539093, loss=0.022293444722890854
I0209 22:37:36.639037 139871926159104 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.1466006338596344, loss=0.024481525644659996
I0209 22:37:53.556770 140039251117888 spec.py:321] Evaluating on the training split.
I0209 22:39:31.135481 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 22:39:34.172486 140039251117888 spec.py:349] Evaluating on the test split.
I0209 22:39:37.159273 140039251117888 submission_runner.py:408] Time since start: 21695.23s, 	Step: 46154, 	{'train/accuracy': 0.9932268857955933, 'train/loss': 0.02133258245885372, 'train/mean_average_precision': 0.6116497269315911, 'validation/accuracy': 0.9866270422935486, 'validation/loss': 0.046749576926231384, 'validation/mean_average_precision': 0.2717662805382729, 'validation/num_examples': 43793, 'test/accuracy': 0.9857484102249146, 'test/loss': 0.05015978589653969, 'test/mean_average_precision': 0.262155570037263, 'test/num_examples': 43793, 'score': 14900.003878116608, 'total_duration': 21695.22643852234, 'accumulated_submission_time': 14900.003878116608, 'accumulated_eval_time': 6791.985899925232, 'accumulated_logging_time': 1.9806413650512695}
I0209 22:39:37.183404 139836634785536 logging_writer.py:48] [46154] accumulated_eval_time=6791.985900, accumulated_logging_time=1.980641, accumulated_submission_time=14900.003878, global_step=46154, preemption_count=0, score=14900.003878, test/accuracy=0.985748, test/loss=0.050160, test/mean_average_precision=0.262156, test/num_examples=43793, total_duration=21695.226439, train/accuracy=0.993227, train/loss=0.021333, train/mean_average_precision=0.611650, validation/accuracy=0.986627, validation/loss=0.046750, validation/mean_average_precision=0.271766, validation/num_examples=43793
I0209 22:39:52.257358 139836643178240 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.15547528862953186, loss=0.026264293119311333
I0209 22:40:24.665730 139836634785536 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.15082570910453796, loss=0.025106921792030334
I0209 22:40:57.227922 139836643178240 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.1358979195356369, loss=0.022335505113005638
I0209 22:41:29.617188 139836634785536 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.15003199875354767, loss=0.02120044454932213
I0209 22:42:01.631100 139836643178240 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.1640567034482956, loss=0.02128608711063862
I0209 22:42:33.969966 139836634785536 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.14056609570980072, loss=0.023850897327065468
I0209 22:43:06.190067 139836643178240 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.12546849250793457, loss=0.02260550484061241
I0209 22:43:37.459647 140039251117888 spec.py:321] Evaluating on the training split.
I0209 22:45:17.915228 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 22:45:20.977520 140039251117888 spec.py:349] Evaluating on the test split.
I0209 22:45:23.993622 140039251117888 submission_runner.py:408] Time since start: 22042.06s, 	Step: 46898, 	{'train/accuracy': 0.9933241009712219, 'train/loss': 0.02106494829058647, 'train/mean_average_precision': 0.6244343061105562, 'validation/accuracy': 0.986622154712677, 'validation/loss': 0.04705076292157173, 'validation/mean_average_precision': 0.2749869542278762, 'validation/num_examples': 43793, 'test/accuracy': 0.9857871532440186, 'test/loss': 0.05036665126681328, 'test/mean_average_precision': 0.2615554241055192, 'test/num_examples': 43793, 'score': 15140.24789738655, 'total_duration': 22042.06078195572, 'accumulated_submission_time': 15140.24789738655, 'accumulated_eval_time': 6898.519830942154, 'accumulated_logging_time': 2.017240524291992}
I0209 22:45:24.017959 139864233322240 logging_writer.py:48] [46898] accumulated_eval_time=6898.519831, accumulated_logging_time=2.017241, accumulated_submission_time=15140.247897, global_step=46898, preemption_count=0, score=15140.247897, test/accuracy=0.985787, test/loss=0.050367, test/mean_average_precision=0.261555, test/num_examples=43793, total_duration=22042.060782, train/accuracy=0.993324, train/loss=0.021065, train/mean_average_precision=0.624434, validation/accuracy=0.986622, validation/loss=0.047051, validation/mean_average_precision=0.274987, validation/num_examples=43793
I0209 22:45:24.993146 139878399788800 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.22120621800422668, loss=0.02373800240457058
I0209 22:45:56.853265 139864233322240 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.14670884609222412, loss=0.02081320993602276
I0209 22:46:28.641999 139878399788800 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.1800331473350525, loss=0.025368448346853256
I0209 22:47:00.389935 139864233322240 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.13804949820041656, loss=0.021463608369231224
I0209 22:47:31.890992 139878399788800 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.15568172931671143, loss=0.021428201347589493
I0209 22:48:03.482431 139864233322240 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.1621229201555252, loss=0.022715605795383453
I0209 22:48:35.445574 139878399788800 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.1513986736536026, loss=0.021701399236917496
I0209 22:49:07.033294 139864233322240 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.1398695558309555, loss=0.02100013755261898
I0209 22:49:24.090009 140039251117888 spec.py:321] Evaluating on the training split.
I0209 22:50:59.895834 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 22:51:03.078559 140039251117888 spec.py:349] Evaluating on the test split.
I0209 22:51:06.552769 140039251117888 submission_runner.py:408] Time since start: 22384.62s, 	Step: 47655, 	{'train/accuracy': 0.9933059215545654, 'train/loss': 0.02119574137032032, 'train/mean_average_precision': 0.6117602752374381, 'validation/accuracy': 0.9866501688957214, 'validation/loss': 0.04724955931305885, 'validation/mean_average_precision': 0.2753124853835367, 'validation/num_examples': 43793, 'test/accuracy': 0.9858199954032898, 'test/loss': 0.05041026696562767, 'test/mean_average_precision': 0.2589229370037478, 'test/num_examples': 43793, 'score': 15380.289312124252, 'total_duration': 22384.619906663895, 'accumulated_submission_time': 15380.289312124252, 'accumulated_eval_time': 7000.982522249222, 'accumulated_logging_time': 2.052924633026123}
I0209 22:51:06.580129 139836643178240 logging_writer.py:48] [47655] accumulated_eval_time=7000.982522, accumulated_logging_time=2.052925, accumulated_submission_time=15380.289312, global_step=47655, preemption_count=0, score=15380.289312, test/accuracy=0.985820, test/loss=0.050410, test/mean_average_precision=0.258923, test/num_examples=43793, total_duration=22384.619907, train/accuracy=0.993306, train/loss=0.021196, train/mean_average_precision=0.611760, validation/accuracy=0.986650, validation/loss=0.047250, validation/mean_average_precision=0.275312, validation/num_examples=43793
I0209 22:51:21.775627 139871926159104 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.13735641539096832, loss=0.020003091543912888
I0209 22:51:55.107209 139836643178240 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.1463703066110611, loss=0.02233494631946087
I0209 22:52:27.540167 139871926159104 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.1352890580892563, loss=0.020874954760074615
I0209 22:52:59.625186 139836643178240 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.16870062053203583, loss=0.023360447958111763
I0209 22:53:31.861491 139871926159104 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.16010738909244537, loss=0.023411830887198448
I0209 22:54:03.814155 139836643178240 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.14765533804893494, loss=0.02234378270804882
I0209 22:54:35.879585 139871926159104 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.15198925137519836, loss=0.022968709468841553
I0209 22:55:06.816263 140039251117888 spec.py:321] Evaluating on the training split.
I0209 22:56:48.621587 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 22:56:51.923268 140039251117888 spec.py:349] Evaluating on the test split.
I0209 22:56:55.201142 140039251117888 submission_runner.py:408] Time since start: 22733.27s, 	Step: 48397, 	{'train/accuracy': 0.9927676320075989, 'train/loss': 0.022809168323874474, 'train/mean_average_precision': 0.5803819765058651, 'validation/accuracy': 0.986539363861084, 'validation/loss': 0.04781175032258034, 'validation/mean_average_precision': 0.2669560169577921, 'validation/num_examples': 43793, 'test/accuracy': 0.9856439828872681, 'test/loss': 0.05102235823869705, 'test/mean_average_precision': 0.2503140930026999, 'test/num_examples': 43793, 'score': 15620.493504047394, 'total_duration': 22733.268282413483, 'accumulated_submission_time': 15620.493504047394, 'accumulated_eval_time': 7109.367336988449, 'accumulated_logging_time': 2.09198260307312}
I0209 22:56:55.228832 139864233322240 logging_writer.py:48] [48397] accumulated_eval_time=7109.367337, accumulated_logging_time=2.091983, accumulated_submission_time=15620.493504, global_step=48397, preemption_count=0, score=15620.493504, test/accuracy=0.985644, test/loss=0.051022, test/mean_average_precision=0.250314, test/num_examples=43793, total_duration=22733.268282, train/accuracy=0.992768, train/loss=0.022809, train/mean_average_precision=0.580382, validation/accuracy=0.986539, validation/loss=0.047812, validation/mean_average_precision=0.266956, validation/num_examples=43793
I0209 22:56:56.623531 139878399788800 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.13692688941955566, loss=0.020228605717420578
I0209 22:57:29.379975 139864233322240 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.15410727262496948, loss=0.023780694231390953
I0209 22:58:01.770002 139878399788800 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.18246830999851227, loss=0.018379246816039085
I0209 22:58:33.616508 139864233322240 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.16222301125526428, loss=0.022360263392329216
I0209 22:59:05.995897 139878399788800 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.189915269613266, loss=0.021191434934735298
I0209 22:59:37.482674 139864233322240 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.13718461990356445, loss=0.022494884207844734
I0209 23:00:09.597352 139878399788800 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.19098539650440216, loss=0.022830095142126083
I0209 23:00:41.477385 139864233322240 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.15946626663208008, loss=0.023091070353984833
I0209 23:00:55.450122 140039251117888 spec.py:321] Evaluating on the training split.
I0209 23:02:31.154471 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 23:02:34.283798 140039251117888 spec.py:349] Evaluating on the test split.
I0209 23:02:37.391285 140039251117888 submission_runner.py:408] Time since start: 23075.46s, 	Step: 49145, 	{'train/accuracy': 0.9926386475563049, 'train/loss': 0.023112531751394272, 'train/mean_average_precision': 0.5560028585107328, 'validation/accuracy': 0.986544668674469, 'validation/loss': 0.047696709632873535, 'validation/mean_average_precision': 0.27548860230874456, 'validation/num_examples': 43793, 'test/accuracy': 0.9856220483779907, 'test/loss': 0.05133892968297005, 'test/mean_average_precision': 0.2546934945497422, 'test/num_examples': 43793, 'score': 15860.683614492416, 'total_duration': 23075.458447933197, 'accumulated_submission_time': 15860.683614492416, 'accumulated_eval_time': 7211.308455705643, 'accumulated_logging_time': 2.1313531398773193}
I0209 23:02:37.416213 139836634785536 logging_writer.py:48] [49145] accumulated_eval_time=7211.308456, accumulated_logging_time=2.131353, accumulated_submission_time=15860.683614, global_step=49145, preemption_count=0, score=15860.683614, test/accuracy=0.985622, test/loss=0.051339, test/mean_average_precision=0.254693, test/num_examples=43793, total_duration=23075.458448, train/accuracy=0.992639, train/loss=0.023113, train/mean_average_precision=0.556003, validation/accuracy=0.986545, validation/loss=0.047697, validation/mean_average_precision=0.275489, validation/num_examples=43793
I0209 23:02:55.762379 139836643178240 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.16683173179626465, loss=0.023038025945425034
I0209 23:03:27.865022 139836634785536 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.17302680015563965, loss=0.02087920904159546
I0209 23:04:00.383528 139836643178240 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.13976745307445526, loss=0.022509215399622917
I0209 23:04:32.705028 139836634785536 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.2967875897884369, loss=0.021664200350642204
I0209 23:05:04.691333 139836643178240 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.16009603440761566, loss=0.020551618188619614
I0209 23:05:37.257625 139836634785536 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.1748059242963791, loss=0.023723570629954338
I0209 23:06:09.527690 139836643178240 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.14876684546470642, loss=0.02065182663500309
I0209 23:06:37.643853 140039251117888 spec.py:321] Evaluating on the training split.
I0209 23:08:19.979253 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 23:08:23.146431 140039251117888 spec.py:349] Evaluating on the test split.
I0209 23:08:26.216071 140039251117888 submission_runner.py:408] Time since start: 23424.28s, 	Step: 49889, 	{'train/accuracy': 0.992885947227478, 'train/loss': 0.022317999973893166, 'train/mean_average_precision': 0.5716329006994851, 'validation/accuracy': 0.9866303205490112, 'validation/loss': 0.047895122319459915, 'validation/mean_average_precision': 0.2716456802267858, 'validation/num_examples': 43793, 'test/accuracy': 0.9857316017150879, 'test/loss': 0.05132818594574928, 'test/mean_average_precision': 0.256285389852942, 'test/num_examples': 43793, 'score': 16100.88074851036, 'total_duration': 23424.283234357834, 'accumulated_submission_time': 16100.88074851036, 'accumulated_eval_time': 7319.88063287735, 'accumulated_logging_time': 2.1673531532287598}
I0209 23:08:26.240712 139864233322240 logging_writer.py:48] [49889] accumulated_eval_time=7319.880633, accumulated_logging_time=2.167353, accumulated_submission_time=16100.880749, global_step=49889, preemption_count=0, score=16100.880749, test/accuracy=0.985732, test/loss=0.051328, test/mean_average_precision=0.256285, test/num_examples=43793, total_duration=23424.283234, train/accuracy=0.992886, train/loss=0.022318, train/mean_average_precision=0.571633, validation/accuracy=0.986630, validation/loss=0.047895, validation/mean_average_precision=0.271646, validation/num_examples=43793
I0209 23:08:30.447002 139871926159104 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.19719134271144867, loss=0.0213893074542284
I0209 23:09:02.923762 139864233322240 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.14810051023960114, loss=0.0173121839761734
I0209 23:09:34.919693 139871926159104 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.1906621754169464, loss=0.022015320137143135
I0209 23:10:07.129762 139864233322240 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.17327933013439178, loss=0.020549355074763298
I0209 23:10:39.425966 139871926159104 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.16700531542301178, loss=0.021712664514780045
I0209 23:11:11.752414 139864233322240 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.15321744978427887, loss=0.018744628876447678
I0209 23:11:43.542038 139871926159104 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.15031798183918, loss=0.020851746201515198
I0209 23:12:15.509490 139864233322240 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.16030089557170868, loss=0.019913291558623314
I0209 23:12:26.438072 140039251117888 spec.py:321] Evaluating on the training split.
I0209 23:14:05.005436 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 23:14:08.135625 140039251117888 spec.py:349] Evaluating on the test split.
I0209 23:14:11.249634 140039251117888 submission_runner.py:408] Time since start: 23769.32s, 	Step: 50635, 	{'train/accuracy': 0.9929296374320984, 'train/loss': 0.02187691256403923, 'train/mean_average_precision': 0.6079837780471014, 'validation/accuracy': 0.9864983558654785, 'validation/loss': 0.047983817756175995, 'validation/mean_average_precision': 0.27032804939891897, 'validation/num_examples': 43793, 'test/accuracy': 0.9856178760528564, 'test/loss': 0.051483746618032455, 'test/mean_average_precision': 0.2527862156016627, 'test/num_examples': 43793, 'score': 16341.046777009964, 'total_duration': 23769.316796541214, 'accumulated_submission_time': 16341.046777009964, 'accumulated_eval_time': 7424.692152500153, 'accumulated_logging_time': 2.2040598392486572}
I0209 23:14:11.274488 139836634785536 logging_writer.py:48] [50635] accumulated_eval_time=7424.692153, accumulated_logging_time=2.204060, accumulated_submission_time=16341.046777, global_step=50635, preemption_count=0, score=16341.046777, test/accuracy=0.985618, test/loss=0.051484, test/mean_average_precision=0.252786, test/num_examples=43793, total_duration=23769.316797, train/accuracy=0.992930, train/loss=0.021877, train/mean_average_precision=0.607984, validation/accuracy=0.986498, validation/loss=0.047984, validation/mean_average_precision=0.270328, validation/num_examples=43793
I0209 23:14:32.430336 139836643178240 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.18781299889087677, loss=0.024096161127090454
I0209 23:15:04.439702 139836634785536 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.19957569241523743, loss=0.023706970736384392
I0209 23:15:36.322043 139836643178240 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.16583450138568878, loss=0.021271757781505585
I0209 23:16:08.363369 139836634785536 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.1428397297859192, loss=0.019198482856154442
I0209 23:16:40.226499 139836643178240 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.16901685297489166, loss=0.019048145040869713
I0209 23:17:11.910663 139836634785536 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.19009046256542206, loss=0.0208846777677536
I0209 23:17:43.915807 139836643178240 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.18659596145153046, loss=0.02039930783212185
I0209 23:18:11.488420 140039251117888 spec.py:321] Evaluating on the training split.
I0209 23:19:48.207756 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 23:19:53.428800 140039251117888 spec.py:349] Evaluating on the test split.
I0209 23:19:56.507103 140039251117888 submission_runner.py:408] Time since start: 24114.57s, 	Step: 51387, 	{'train/accuracy': 0.99312424659729, 'train/loss': 0.02137378603219986, 'train/mean_average_precision': 0.6051922907115049, 'validation/accuracy': 0.9864557385444641, 'validation/loss': 0.04817109555006027, 'validation/mean_average_precision': 0.27860323962391087, 'validation/num_examples': 43793, 'test/accuracy': 0.9855256080627441, 'test/loss': 0.05173730477690697, 'test/mean_average_precision': 0.25799338239721004, 'test/num_examples': 43793, 'score': 16581.230067253113, 'total_duration': 24114.574266433716, 'accumulated_submission_time': 16581.230067253113, 'accumulated_eval_time': 7529.710796117783, 'accumulated_logging_time': 2.239814043045044}
I0209 23:19:56.533108 139871926159104 logging_writer.py:48] [51387] accumulated_eval_time=7529.710796, accumulated_logging_time=2.239814, accumulated_submission_time=16581.230067, global_step=51387, preemption_count=0, score=16581.230067, test/accuracy=0.985526, test/loss=0.051737, test/mean_average_precision=0.257993, test/num_examples=43793, total_duration=24114.574266, train/accuracy=0.993124, train/loss=0.021374, train/mean_average_precision=0.605192, validation/accuracy=0.986456, validation/loss=0.048171, validation/mean_average_precision=0.278603, validation/num_examples=43793
I0209 23:20:01.251819 139878399788800 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.16733318567276, loss=0.020299917086958885
I0209 23:20:33.818583 139871926159104 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.24201816320419312, loss=0.0210372693836689
I0209 23:21:06.082950 139878399788800 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.17517052590847015, loss=0.018680129200220108
I0209 23:21:38.480057 139871926159104 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.17790444195270538, loss=0.020874282345175743
I0209 23:22:11.295062 139878399788800 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.1806134134531021, loss=0.020571719855070114
I0209 23:22:43.290612 139871926159104 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.18845784664154053, loss=0.024398721754550934
I0209 23:23:15.641204 139878399788800 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.18438555300235748, loss=0.018895965069532394
I0209 23:23:48.380359 139871926159104 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.19223950803279877, loss=0.020741544663906097
I0209 23:23:56.652802 140039251117888 spec.py:321] Evaluating on the training split.
I0209 23:25:31.695986 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 23:25:34.894927 140039251117888 spec.py:349] Evaluating on the test split.
I0209 23:25:37.926232 140039251117888 submission_runner.py:408] Time since start: 24455.99s, 	Step: 52126, 	{'train/accuracy': 0.9935033917427063, 'train/loss': 0.0201828982681036, 'train/mean_average_precision': 0.6243054543350697, 'validation/accuracy': 0.9865494966506958, 'validation/loss': 0.04877251386642456, 'validation/mean_average_precision': 0.2713997153176349, 'validation/num_examples': 43793, 'test/accuracy': 0.9856654405593872, 'test/loss': 0.05205725133419037, 'test/mean_average_precision': 0.2631199337380247, 'test/num_examples': 43793, 'score': 16821.317676067352, 'total_duration': 24455.993393421173, 'accumulated_submission_time': 16821.317676067352, 'accumulated_eval_time': 7630.984179973602, 'accumulated_logging_time': 2.2781577110290527}
I0209 23:25:37.951849 139836643178240 logging_writer.py:48] [52126] accumulated_eval_time=7630.984180, accumulated_logging_time=2.278158, accumulated_submission_time=16821.317676, global_step=52126, preemption_count=0, score=16821.317676, test/accuracy=0.985665, test/loss=0.052057, test/mean_average_precision=0.263120, test/num_examples=43793, total_duration=24455.993393, train/accuracy=0.993503, train/loss=0.020183, train/mean_average_precision=0.624305, validation/accuracy=0.986549, validation/loss=0.048773, validation/mean_average_precision=0.271400, validation/num_examples=43793
I0209 23:26:02.241140 139864233322240 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.1646089255809784, loss=0.02005593106150627
I0209 23:26:34.380276 139836643178240 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.19666917622089386, loss=0.0211567934602499
I0209 23:27:06.559203 139864233322240 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.23579619824886322, loss=0.0180783960968256
I0209 23:27:38.477660 139836643178240 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.19642217457294464, loss=0.021608935669064522
I0209 23:28:10.368347 139864233322240 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.19221125543117523, loss=0.02059924229979515
I0209 23:28:42.893704 139836643178240 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.1928894966840744, loss=0.02076892741024494
I0209 23:29:14.825891 139864233322240 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.18969939649105072, loss=0.019050447270274162
I0209 23:29:38.119441 140039251117888 spec.py:321] Evaluating on the training split.
I0209 23:31:14.521711 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 23:31:17.790393 140039251117888 spec.py:349] Evaluating on the test split.
I0209 23:31:20.958302 140039251117888 submission_runner.py:408] Time since start: 24799.03s, 	Step: 52874, 	{'train/accuracy': 0.9940038323402405, 'train/loss': 0.018666956573724747, 'train/mean_average_precision': 0.6656468057073384, 'validation/accuracy': 0.9866250157356262, 'validation/loss': 0.048979878425598145, 'validation/mean_average_precision': 0.27065398680055736, 'validation/num_examples': 43793, 'test/accuracy': 0.9856258630752563, 'test/loss': 0.052842892706394196, 'test/mean_average_precision': 0.2543117989191361, 'test/num_examples': 43793, 'score': 17061.454341888428, 'total_duration': 24799.02546787262, 'accumulated_submission_time': 17061.454341888428, 'accumulated_eval_time': 7733.823001623154, 'accumulated_logging_time': 2.314923048019409}
I0209 23:31:20.983635 139871926159104 logging_writer.py:48] [52874] accumulated_eval_time=7733.823002, accumulated_logging_time=2.314923, accumulated_submission_time=17061.454342, global_step=52874, preemption_count=0, score=17061.454342, test/accuracy=0.985626, test/loss=0.052843, test/mean_average_precision=0.254312, test/num_examples=43793, total_duration=24799.025468, train/accuracy=0.994004, train/loss=0.018667, train/mean_average_precision=0.665647, validation/accuracy=0.986625, validation/loss=0.048980, validation/mean_average_precision=0.270654, validation/num_examples=43793
I0209 23:31:29.714064 139878399788800 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.17830222845077515, loss=0.016565294936299324
I0209 23:32:02.173257 139871926159104 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.20355631411075592, loss=0.02040671557188034
I0209 23:32:34.556986 139878399788800 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.16470645368099213, loss=0.02144254930317402
I0209 23:33:06.890646 139871926159104 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.24846318364143372, loss=0.0216621533036232
I0209 23:33:39.246526 139878399788800 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.19254325330257416, loss=0.01829863339662552
I0209 23:34:11.791450 139871926159104 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.1859724223613739, loss=0.02085980772972107
I0209 23:34:44.540928 139878399788800 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.18395653367042542, loss=0.0208126213401556
I0209 23:35:17.586229 139871926159104 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.1756751388311386, loss=0.020015576854348183
I0209 23:35:21.210945 140039251117888 spec.py:321] Evaluating on the training split.
I0209 23:37:06.579518 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 23:37:09.583585 140039251117888 spec.py:349] Evaluating on the test split.
I0209 23:37:12.531965 140039251117888 submission_runner.py:408] Time since start: 25150.60s, 	Step: 53612, 	{'train/accuracy': 0.9942200183868408, 'train/loss': 0.01806596666574478, 'train/mean_average_precision': 0.6819209665010129, 'validation/accuracy': 0.9864720106124878, 'validation/loss': 0.04959718883037567, 'validation/mean_average_precision': 0.2684478956022052, 'validation/num_examples': 43793, 'test/accuracy': 0.9855963587760925, 'test/loss': 0.053325194865465164, 'test/mean_average_precision': 0.2544714874167593, 'test/num_examples': 43793, 'score': 17301.6496155262, 'total_duration': 25150.59912610054, 'accumulated_submission_time': 17301.6496155262, 'accumulated_eval_time': 7845.143984079361, 'accumulated_logging_time': 2.351712226867676}
I0209 23:37:12.562072 139836634785536 logging_writer.py:48] [53612] accumulated_eval_time=7845.143984, accumulated_logging_time=2.351712, accumulated_submission_time=17301.649616, global_step=53612, preemption_count=0, score=17301.649616, test/accuracy=0.985596, test/loss=0.053325, test/mean_average_precision=0.254471, test/num_examples=43793, total_duration=25150.599126, train/accuracy=0.994220, train/loss=0.018066, train/mean_average_precision=0.681921, validation/accuracy=0.986472, validation/loss=0.049597, validation/mean_average_precision=0.268448, validation/num_examples=43793
I0209 23:37:40.737873 139836643178240 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.1933138370513916, loss=0.01896083913743496
I0209 23:38:12.454684 139836634785536 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.26398012042045593, loss=0.020670795813202858
I0209 23:38:44.195391 139836643178240 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.18749891221523285, loss=0.016165107488632202
I0209 23:39:16.392063 139836634785536 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.19614022970199585, loss=0.019477710127830505
I0209 23:39:47.835666 139836643178240 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.1695908010005951, loss=0.0181626845151186
I0209 23:40:19.821099 139836634785536 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.18436573445796967, loss=0.01780519261956215
I0209 23:40:51.553626 139836643178240 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.22168272733688354, loss=0.01923179440200329
I0209 23:41:12.677461 140039251117888 spec.py:321] Evaluating on the training split.
I0209 23:42:52.360622 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 23:42:55.379508 140039251117888 spec.py:349] Evaluating on the test split.
I0209 23:42:58.377547 140039251117888 submission_runner.py:408] Time since start: 25496.44s, 	Step: 54367, 	{'train/accuracy': 0.994297206401825, 'train/loss': 0.01786726526916027, 'train/mean_average_precision': 0.6827159617126499, 'validation/accuracy': 0.9865003824234009, 'validation/loss': 0.04968026280403137, 'validation/mean_average_precision': 0.2730149322743814, 'validation/num_examples': 43793, 'test/accuracy': 0.9855533838272095, 'test/loss': 0.053486552089452744, 'test/mean_average_precision': 0.25917765667164855, 'test/num_examples': 43793, 'score': 17541.733896255493, 'total_duration': 25496.444694042206, 'accumulated_submission_time': 17541.733896255493, 'accumulated_eval_time': 7950.844016551971, 'accumulated_logging_time': 2.3931870460510254}
I0209 23:42:58.403210 139864233322240 logging_writer.py:48] [54367] accumulated_eval_time=7950.844017, accumulated_logging_time=2.393187, accumulated_submission_time=17541.733896, global_step=54367, preemption_count=0, score=17541.733896, test/accuracy=0.985553, test/loss=0.053487, test/mean_average_precision=0.259178, test/num_examples=43793, total_duration=25496.444694, train/accuracy=0.994297, train/loss=0.017867, train/mean_average_precision=0.682716, validation/accuracy=0.986500, validation/loss=0.049680, validation/mean_average_precision=0.273015, validation/num_examples=43793
I0209 23:43:09.218675 139878399788800 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.19066110253334045, loss=0.019206367433071136
I0209 23:43:40.756761 139864233322240 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.1965932846069336, loss=0.01836826093494892
I0209 23:44:12.430958 139878399788800 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.23210935294628143, loss=0.019609494134783745
I0209 23:44:44.517759 139864233322240 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.23011630773544312, loss=0.020512210205197334
I0209 23:45:16.786196 139878399788800 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.18980228900909424, loss=0.017285587266087532
I0209 23:45:50.002680 139864233322240 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.18964608013629913, loss=0.018330121412873268
I0209 23:46:22.207004 139878399788800 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.17200443148612976, loss=0.018594592809677124
I0209 23:46:53.752637 139864233322240 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.29671511054039, loss=0.019147826358675957
I0209 23:46:58.537051 140039251117888 spec.py:321] Evaluating on the training split.
I0209 23:48:40.713516 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 23:48:43.713211 140039251117888 spec.py:349] Evaluating on the test split.
I0209 23:48:46.701351 140039251117888 submission_runner.py:408] Time since start: 25844.77s, 	Step: 55116, 	{'train/accuracy': 0.9939027428627014, 'train/loss': 0.01882188953459263, 'train/mean_average_precision': 0.6740066988974451, 'validation/accuracy': 0.9864963293075562, 'validation/loss': 0.0502084381878376, 'validation/mean_average_precision': 0.2762999691316146, 'validation/num_examples': 43793, 'test/accuracy': 0.9855660200119019, 'test/loss': 0.05363349989056587, 'test/mean_average_precision': 0.2579593188538949, 'test/num_examples': 43793, 'score': 17781.83658337593, 'total_duration': 25844.768416643143, 'accumulated_submission_time': 17781.83658337593, 'accumulated_eval_time': 8059.008177280426, 'accumulated_logging_time': 2.4300525188446045}
I0209 23:48:46.727317 139836634785536 logging_writer.py:48] [55116] accumulated_eval_time=8059.008177, accumulated_logging_time=2.430053, accumulated_submission_time=17781.836583, global_step=55116, preemption_count=0, score=17781.836583, test/accuracy=0.985566, test/loss=0.053633, test/mean_average_precision=0.257959, test/num_examples=43793, total_duration=25844.768417, train/accuracy=0.993903, train/loss=0.018822, train/mean_average_precision=0.674007, validation/accuracy=0.986496, validation/loss=0.050208, validation/mean_average_precision=0.276300, validation/num_examples=43793
I0209 23:49:13.863773 139836643178240 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.2156146913766861, loss=0.019219480454921722
I0209 23:49:45.394466 139836634785536 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.28292471170425415, loss=0.01971021667122841
I0209 23:50:17.351622 139836643178240 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.19421976804733276, loss=0.018830224871635437
I0209 23:50:49.062227 139836634785536 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.21743349730968475, loss=0.021044744178652763
I0209 23:51:20.671103 139836643178240 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.24166059494018555, loss=0.020009811967611313
I0209 23:51:52.522631 139836634785536 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.1991342455148697, loss=0.019104931503534317
I0209 23:52:24.499101 139836643178240 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.19367216527462006, loss=0.01753828302025795
I0209 23:52:46.907871 140039251117888 spec.py:321] Evaluating on the training split.
I0209 23:54:23.674184 140039251117888 spec.py:333] Evaluating on the validation split.
I0209 23:54:26.880163 140039251117888 spec.py:349] Evaluating on the test split.
I0209 23:54:29.910077 140039251117888 submission_runner.py:408] Time since start: 26187.98s, 	Step: 55871, 	{'train/accuracy': 0.9938641786575317, 'train/loss': 0.018877951428294182, 'train/mean_average_precision': 0.6532421190749456, 'validation/accuracy': 0.9863501787185669, 'validation/loss': 0.0509905144572258, 'validation/mean_average_precision': 0.27272084776079214, 'validation/num_examples': 43793, 'test/accuracy': 0.9854717254638672, 'test/loss': 0.054655853658914566, 'test/mean_average_precision': 0.25146160516765664, 'test/num_examples': 43793, 'score': 18021.98615550995, 'total_duration': 26187.977236509323, 'accumulated_submission_time': 18021.98615550995, 'accumulated_eval_time': 8162.010338068008, 'accumulated_logging_time': 2.467262029647827}
I0209 23:54:29.937045 139836244932352 logging_writer.py:48] [55871] accumulated_eval_time=8162.010338, accumulated_logging_time=2.467262, accumulated_submission_time=18021.986156, global_step=55871, preemption_count=0, score=18021.986156, test/accuracy=0.985472, test/loss=0.054656, test/mean_average_precision=0.251462, test/num_examples=43793, total_duration=26187.977237, train/accuracy=0.993864, train/loss=0.018878, train/mean_average_precision=0.653242, validation/accuracy=0.986350, validation/loss=0.050991, validation/mean_average_precision=0.272721, validation/num_examples=43793
I0209 23:54:39.776402 139864233322240 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.22837309539318085, loss=0.02063952013850212
I0209 23:55:12.456021 139836244932352 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.20434053242206573, loss=0.017291804775595665
I0209 23:55:44.819308 139864233322240 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.21188515424728394, loss=0.018006155267357826
I0209 23:56:17.074490 139836244932352 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.224814310669899, loss=0.01880587264895439
I0209 23:56:49.275183 139864233322240 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.20529453456401825, loss=0.01714073121547699
I0209 23:57:21.205162 139836244932352 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.2045731246471405, loss=0.018704853951931
I0209 23:57:53.026817 139864233322240 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.20750278234481812, loss=0.017918763682246208
I0209 23:58:25.331257 139836244932352 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.2373483031988144, loss=0.018878597766160965
I0209 23:58:30.106817 140039251117888 spec.py:321] Evaluating on the training split.
I0210 00:00:07.179922 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 00:00:10.240504 140039251117888 spec.py:349] Evaluating on the test split.
I0210 00:00:13.202981 140039251117888 submission_runner.py:408] Time since start: 26531.27s, 	Step: 56616, 	{'train/accuracy': 0.9935514330863953, 'train/loss': 0.019919833168387413, 'train/mean_average_precision': 0.6342096266526925, 'validation/accuracy': 0.9862856864929199, 'validation/loss': 0.051337894052267075, 'validation/mean_average_precision': 0.2656026553756837, 'validation/num_examples': 43793, 'test/accuracy': 0.985351026058197, 'test/loss': 0.05505384877324104, 'test/mean_average_precision': 0.2538779185836647, 'test/num_examples': 43793, 'score': 18262.12407398224, 'total_duration': 26531.270143032074, 'accumulated_submission_time': 18262.12407398224, 'accumulated_eval_time': 8265.10645365715, 'accumulated_logging_time': 2.506730794906616}
I0210 00:00:13.229186 139836634785536 logging_writer.py:48] [56616] accumulated_eval_time=8265.106454, accumulated_logging_time=2.506731, accumulated_submission_time=18262.124074, global_step=56616, preemption_count=0, score=18262.124074, test/accuracy=0.985351, test/loss=0.055054, test/mean_average_precision=0.253878, test/num_examples=43793, total_duration=26531.270143, train/accuracy=0.993551, train/loss=0.019920, train/mean_average_precision=0.634210, validation/accuracy=0.986286, validation/loss=0.051338, validation/mean_average_precision=0.265603, validation/num_examples=43793
I0210 00:00:40.867965 139871926159104 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.20247404277324677, loss=0.017297111451625824
I0210 00:01:13.551227 139836634785536 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.258791983127594, loss=0.017123615369200706
I0210 00:01:45.468873 139871926159104 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.2439524084329605, loss=0.01689997874200344
I0210 00:02:17.669838 139836634785536 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.20487664639949799, loss=0.01815313845872879
I0210 00:02:50.085895 139871926159104 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.22766806185245514, loss=0.017469234764575958
I0210 00:03:22.540366 139836634785536 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.2308313101530075, loss=0.0187897477298975
I0210 00:03:48.430694 139871926159104 logging_writer.py:48] [57282] global_step=57282, preemption_count=0, score=18477.278550
I0210 00:03:48.482868 140039251117888 checkpoints.py:490] Saving checkpoint at step: 57282
I0210 00:03:48.595850 140039251117888 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_3/ogbg_jax/trial_3/checkpoint_57282
I0210 00:03:48.596911 140039251117888 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_3/ogbg_jax/trial_3/checkpoint_57282.
I0210 00:03:48.781492 140039251117888 submission_runner.py:583] Tuning trial 3/5
I0210 00:03:48.781732 140039251117888 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0210 00:03:48.785875 140039251117888 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5250248908996582, 'train/loss': 0.7151214480400085, 'train/mean_average_precision': 0.022596344481242144, 'validation/accuracy': 0.5213832855224609, 'validation/loss': 0.7166012525558472, 'validation/mean_average_precision': 0.026142361220453977, 'validation/num_examples': 43793, 'test/accuracy': 0.5224818587303162, 'test/loss': 0.7161954641342163, 'test/mean_average_precision': 0.02783086509595883, 'test/num_examples': 43793, 'score': 13.67792797088623, 'total_duration': 122.75087356567383, 'accumulated_submission_time': 13.67792797088623, 'accumulated_eval_time': 109.07290530204773, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (746, {'train/accuracy': 0.9866614937782288, 'train/loss': 0.06891448050737381, 'train/mean_average_precision': 0.03438290407203851, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.07732292264699936, 'validation/mean_average_precision': 0.036815222944384746, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.08021833002567291, 'test/mean_average_precision': 0.03825316566172733, 'test/num_examples': 43793, 'score': 253.85704374313354, 'total_duration': 475.6508138179779, 'accumulated_submission_time': 253.85704374313354, 'accumulated_eval_time': 221.7531237602234, 'accumulated_logging_time': 0.02077651023864746, 'global_step': 746, 'preemption_count': 0}), (1491, {'train/accuracy': 0.9872501492500305, 'train/loss': 0.04855059087276459, 'train/mean_average_precision': 0.08156753726605445, 'validation/accuracy': 0.9845539331436157, 'validation/loss': 0.05826849862933159, 'validation/mean_average_precision': 0.08706500771159309, 'validation/num_examples': 43793, 'test/accuracy': 0.9835514426231384, 'test/loss': 0.06154114007949829, 'test/mean_average_precision': 0.09099659746976167, 'test/num_examples': 43793, 'score': 493.8156957626343, 'total_duration': 822.6260054111481, 'accumulated_submission_time': 493.8156957626343, 'accumulated_eval_time': 328.72299671173096, 'accumulated_logging_time': 0.047609567642211914, 'global_step': 1491, 'preemption_count': 0}), (2243, {'train/accuracy': 0.9877920150756836, 'train/loss': 0.04368261992931366, 'train/mean_average_precision': 0.1433774872818228, 'validation/accuracy': 0.9850483536720276, 'validation/loss': 0.05292936787009239, 'validation/mean_average_precision': 0.1361727300785612, 'validation/num_examples': 43793, 'test/accuracy': 0.9840973615646362, 'test/loss': 0.055955272167921066, 'test/mean_average_precision': 0.1340190769220492, 'test/num_examples': 43793, 'score': 733.8995008468628, 'total_duration': 1174.291626214981, 'accumulated_submission_time': 733.8995008468628, 'accumulated_eval_time': 440.25667667388916, 'accumulated_logging_time': 0.07578086853027344, 'global_step': 2243, 'preemption_count': 0}), (2990, {'train/accuracy': 0.9879328012466431, 'train/loss': 0.04259060323238373, 'train/mean_average_precision': 0.1625567684371191, 'validation/accuracy': 0.9850613474845886, 'validation/loss': 0.05261097848415375, 'validation/mean_average_precision': 0.1509572942259753, 'validation/num_examples': 43793, 'test/accuracy': 0.9841592311859131, 'test/loss': 0.05536371469497681, 'test/mean_average_precision': 0.15288593216783689, 'test/num_examples': 43793, 'score': 974.0467576980591, 'total_duration': 1524.873197555542, 'accumulated_submission_time': 974.0467576980591, 'accumulated_eval_time': 550.643904209137, 'accumulated_logging_time': 0.10276627540588379, 'global_step': 2990, 'preemption_count': 0}), (3731, {'train/accuracy': 0.9883593916893005, 'train/loss': 0.0405401736497879, 'train/mean_average_precision': 0.1877052384465054, 'validation/accuracy': 0.9855139851570129, 'validation/loss': 0.050175122916698456, 'validation/mean_average_precision': 0.1707993933649933, 'validation/num_examples': 43793, 'test/accuracy': 0.9846293330192566, 'test/loss': 0.052826281636953354, 'test/mean_average_precision': 0.1723300618497228, 'test/num_examples': 43793, 'score': 1214.0511775016785, 'total_duration': 1874.2206497192383, 'accumulated_submission_time': 1214.0511775016785, 'accumulated_eval_time': 659.9346287250519, 'accumulated_logging_time': 0.13471651077270508, 'global_step': 3731, 'preemption_count': 0}), (4471, {'train/accuracy': 0.9885321259498596, 'train/loss': 0.03950120508670807, 'train/mean_average_precision': 0.21052214180823317, 'validation/accuracy': 0.9856792092323303, 'validation/loss': 0.04872706159949303, 'validation/mean_average_precision': 0.1853128791746986, 'validation/num_examples': 43793, 'test/accuracy': 0.9848723411560059, 'test/loss': 0.05121447890996933, 'test/mean_average_precision': 0.18539543653180943, 'test/num_examples': 43793, 'score': 1454.1758043766022, 'total_duration': 2223.534078359604, 'accumulated_submission_time': 1454.1758043766022, 'accumulated_eval_time': 769.0754227638245, 'accumulated_logging_time': 0.16344189643859863, 'global_step': 4471, 'preemption_count': 0}), (5214, {'train/accuracy': 0.9887311458587646, 'train/loss': 0.038518089801073074, 'train/mean_average_precision': 0.2226331766038031, 'validation/accuracy': 0.9857717156410217, 'validation/loss': 0.048647440969944, 'validation/mean_average_precision': 0.18992241756155326, 'validation/num_examples': 43793, 'test/accuracy': 0.9848892092704773, 'test/loss': 0.051187995821237564, 'test/mean_average_precision': 0.1917954177815328, 'test/num_examples': 43793, 'score': 1694.1631109714508, 'total_duration': 2571.1419973373413, 'accumulated_submission_time': 1694.1631109714508, 'accumulated_eval_time': 876.6488153934479, 'accumulated_logging_time': 0.19112181663513184, 'global_step': 5214, 'preemption_count': 0}), (5958, {'train/accuracy': 0.9891310334205627, 'train/loss': 0.037137195467948914, 'train/mean_average_precision': 0.24559895721568495, 'validation/accuracy': 0.9858813285827637, 'validation/loss': 0.04740628972649574, 'validation/mean_average_precision': 0.19940986093233645, 'validation/num_examples': 43793, 'test/accuracy': 0.9849742650985718, 'test/loss': 0.05000431835651398, 'test/mean_average_precision': 0.20099796466059375, 'test/num_examples': 43793, 'score': 1934.2749044895172, 'total_duration': 2920.176949262619, 'accumulated_submission_time': 1934.2749044895172, 'accumulated_eval_time': 985.5234348773956, 'accumulated_logging_time': 0.21864557266235352, 'global_step': 5958, 'preemption_count': 0}), (6707, {'train/accuracy': 0.9892842769622803, 'train/loss': 0.03652196750044823, 'train/mean_average_precision': 0.26034770998253665, 'validation/accuracy': 0.9860348105430603, 'validation/loss': 0.04685652628540993, 'validation/mean_average_precision': 0.2105766077737178, 'validation/num_examples': 43793, 'test/accuracy': 0.9851874113082886, 'test/loss': 0.049331918358802795, 'test/mean_average_precision': 0.21646928780084507, 'test/num_examples': 43793, 'score': 2174.433957338333, 'total_duration': 3268.484499692917, 'accumulated_submission_time': 2174.433957338333, 'accumulated_eval_time': 1093.623272895813, 'accumulated_logging_time': 0.24747896194458008, 'global_step': 6707, 'preemption_count': 0}), (7447, {'train/accuracy': 0.9892475008964539, 'train/loss': 0.03626396134495735, 'train/mean_average_precision': 0.27339726794886604, 'validation/accuracy': 0.9859694242477417, 'validation/loss': 0.04703037440776825, 'validation/mean_average_precision': 0.2144172040557444, 'validation/num_examples': 43793, 'test/accuracy': 0.9851532578468323, 'test/loss': 0.04955512657761574, 'test/mean_average_precision': 0.21660488054132188, 'test/num_examples': 43793, 'score': 2414.466232776642, 'total_duration': 3615.946009159088, 'accumulated_submission_time': 2414.466232776642, 'accumulated_eval_time': 1201.0050013065338, 'accumulated_logging_time': 0.2745974063873291, 'global_step': 7447, 'preemption_count': 0}), (8197, {'train/accuracy': 0.9892252683639526, 'train/loss': 0.03638733550906181, 'train/mean_average_precision': 0.27329335983624375, 'validation/accuracy': 0.9859321117401123, 'validation/loss': 0.04655333608388901, 'validation/mean_average_precision': 0.2161812692848252, 'validation/num_examples': 43793, 'test/accuracy': 0.9851823449134827, 'test/loss': 0.04896998032927513, 'test/mean_average_precision': 0.2174333997986895, 'test/num_examples': 43793, 'score': 2654.612272977829, 'total_duration': 3962.862035751343, 'accumulated_submission_time': 2654.612272977829, 'accumulated_eval_time': 1307.7273774147034, 'accumulated_logging_time': 0.30209875106811523, 'global_step': 8197, 'preemption_count': 0}), (8944, {'train/accuracy': 0.989263653755188, 'train/loss': 0.03592751920223236, 'train/mean_average_precision': 0.2766664071539449, 'validation/accuracy': 0.9862779378890991, 'validation/loss': 0.046199049800634384, 'validation/mean_average_precision': 0.2300533427368359, 'validation/num_examples': 43793, 'test/accuracy': 0.9854089617729187, 'test/loss': 0.04881441593170166, 'test/mean_average_precision': 0.22253519226643648, 'test/num_examples': 43793, 'score': 2894.6948940753937, 'total_duration': 4314.785717964172, 'accumulated_submission_time': 2894.6948940753937, 'accumulated_eval_time': 1419.5197608470917, 'accumulated_logging_time': 0.3305845260620117, 'global_step': 8944, 'preemption_count': 0}), (9690, {'train/accuracy': 0.9895308017730713, 'train/loss': 0.03510921075940132, 'train/mean_average_precision': 0.29700261784276705, 'validation/accuracy': 0.9863014817237854, 'validation/loss': 0.04545333608984947, 'validation/mean_average_precision': 0.23079749033007863, 'validation/num_examples': 43793, 'test/accuracy': 0.9854245185852051, 'test/loss': 0.048140157014131546, 'test/mean_average_precision': 0.23213402212776624, 'test/num_examples': 43793, 'score': 3134.852992296219, 'total_duration': 4663.980555534363, 'accumulated_submission_time': 3134.852992296219, 'accumulated_eval_time': 1528.5073668956757, 'accumulated_logging_time': 0.35982394218444824, 'global_step': 9690, 'preemption_count': 0}), (10443, {'train/accuracy': 0.9899264574050903, 'train/loss': 0.03395417705178261, 'train/mean_average_precision': 0.32117851971602984, 'validation/accuracy': 0.9864752292633057, 'validation/loss': 0.04525930806994438, 'validation/mean_average_precision': 0.2440599289904552, 'validation/num_examples': 43793, 'test/accuracy': 0.9855690002441406, 'test/loss': 0.04802022874355316, 'test/mean_average_precision': 0.24045128253413328, 'test/num_examples': 43793, 'score': 3374.9913704395294, 'total_duration': 5009.81748175621, 'accumulated_submission_time': 3374.9913704395294, 'accumulated_eval_time': 1634.1574666500092, 'accumulated_logging_time': 0.3879969120025635, 'global_step': 10443, 'preemption_count': 0}), (11189, {'train/accuracy': 0.9897028207778931, 'train/loss': 0.03408277779817581, 'train/mean_average_precision': 0.3120466220388052, 'validation/accuracy': 0.9860798716545105, 'validation/loss': 0.04593156278133392, 'validation/mean_average_precision': 0.23448836837061954, 'validation/num_examples': 43793, 'test/accuracy': 0.9851945638656616, 'test/loss': 0.04870639741420746, 'test/mean_average_precision': 0.22935470260638427, 'test/num_examples': 43793, 'score': 3614.948825597763, 'total_duration': 5356.488221406937, 'accumulated_submission_time': 3614.948825597763, 'accumulated_eval_time': 1740.821620941162, 'accumulated_logging_time': 0.41780829429626465, 'global_step': 11189, 'preemption_count': 0}), (11938, {'train/accuracy': 0.9901050925254822, 'train/loss': 0.0328376404941082, 'train/mean_average_precision': 0.35177024496025017, 'validation/accuracy': 0.9865832328796387, 'validation/loss': 0.0449516586959362, 'validation/mean_average_precision': 0.25467553456674824, 'validation/num_examples': 43793, 'test/accuracy': 0.9857610464096069, 'test/loss': 0.047576550394296646, 'test/mean_average_precision': 0.24912259173989285, 'test/num_examples': 43793, 'score': 3855.1541192531586, 'total_duration': 5702.779677867889, 'accumulated_submission_time': 3855.1541192531586, 'accumulated_eval_time': 1846.8564975261688, 'accumulated_logging_time': 0.4476304054260254, 'global_step': 11938, 'preemption_count': 0}), (12680, {'train/accuracy': 0.990390956401825, 'train/loss': 0.03192305564880371, 'train/mean_average_precision': 0.3709610937739455, 'validation/accuracy': 0.9865361452102661, 'validation/loss': 0.04468625783920288, 'validation/mean_average_precision': 0.25275806647248383, 'validation/num_examples': 43793, 'test/accuracy': 0.9856886267662048, 'test/loss': 0.047468122094869614, 'test/mean_average_precision': 0.24952038505400487, 'test/num_examples': 43793, 'score': 4095.321917295456, 'total_duration': 6049.124286413193, 'accumulated_submission_time': 4095.321917295456, 'accumulated_eval_time': 1952.983324766159, 'accumulated_logging_time': 0.4773571491241455, 'global_step': 12680, 'preemption_count': 0}), (13428, {'train/accuracy': 0.9904465079307556, 'train/loss': 0.031365860253572464, 'train/mean_average_precision': 0.3848623867266765, 'validation/accuracy': 0.986697256565094, 'validation/loss': 0.044610314071178436, 'validation/mean_average_precision': 0.25743156400662653, 'validation/num_examples': 43793, 'test/accuracy': 0.9858078360557556, 'test/loss': 0.047710102051496506, 'test/mean_average_precision': 0.2436257342937955, 'test/num_examples': 43793, 'score': 4335.301592588425, 'total_duration': 6401.863872051239, 'accumulated_submission_time': 4335.301592588425, 'accumulated_eval_time': 2065.694550514221, 'accumulated_logging_time': 0.5062572956085205, 'global_step': 13428, 'preemption_count': 0}), (14163, {'train/accuracy': 0.9905685186386108, 'train/loss': 0.0309013519436121, 'train/mean_average_precision': 0.3852918889568212, 'validation/accuracy': 0.9867021441459656, 'validation/loss': 0.044885601848363876, 'validation/mean_average_precision': 0.24855540790643801, 'validation/num_examples': 43793, 'test/accuracy': 0.9858394265174866, 'test/loss': 0.04777158051729202, 'test/mean_average_precision': 0.24557376769917139, 'test/num_examples': 43793, 'score': 4575.268049478531, 'total_duration': 6748.188427686691, 'accumulated_submission_time': 4575.268049478531, 'accumulated_eval_time': 2171.9985449314117, 'accumulated_logging_time': 0.5388197898864746, 'global_step': 14163, 'preemption_count': 0}), (14900, {'train/accuracy': 0.9904906153678894, 'train/loss': 0.03138909488916397, 'train/mean_average_precision': 0.37317141297512924, 'validation/accuracy': 0.9866453409194946, 'validation/loss': 0.0449206680059433, 'validation/mean_average_precision': 0.2515589492994831, 'validation/num_examples': 43793, 'test/accuracy': 0.9857408404350281, 'test/loss': 0.047922395169734955, 'test/mean_average_precision': 0.24176956981709927, 'test/num_examples': 43793, 'score': 4815.410197257996, 'total_duration': 7096.95876121521, 'accumulated_submission_time': 4815.410197257996, 'accumulated_eval_time': 2280.576951980591, 'accumulated_logging_time': 0.5679218769073486, 'global_step': 14900, 'preemption_count': 0}), (15641, {'train/accuracy': 0.990464985370636, 'train/loss': 0.031555913388729095, 'train/mean_average_precision': 0.35899359101912764, 'validation/accuracy': 0.9865288138389587, 'validation/loss': 0.04482686519622803, 'validation/mean_average_precision': 0.2577375655676164, 'validation/num_examples': 43793, 'test/accuracy': 0.9857046008110046, 'test/loss': 0.04734693840146065, 'test/mean_average_precision': 0.2544363572094351, 'test/num_examples': 43793, 'score': 5055.575888395309, 'total_duration': 7441.932022809982, 'accumulated_submission_time': 5055.575888395309, 'accumulated_eval_time': 2385.3323168754578, 'accumulated_logging_time': 0.599492073059082, 'global_step': 15641, 'preemption_count': 0}), (16376, {'train/accuracy': 0.990566611289978, 'train/loss': 0.03110634721815586, 'train/mean_average_precision': 0.397761553804051, 'validation/accuracy': 0.9866769909858704, 'validation/loss': 0.04449794813990593, 'validation/mean_average_precision': 0.2670592840195563, 'validation/num_examples': 43793, 'test/accuracy': 0.9857821464538574, 'test/loss': 0.04739530757069588, 'test/mean_average_precision': 0.25094880549624815, 'test/num_examples': 43793, 'score': 5295.590830564499, 'total_duration': 7799.37594294548, 'accumulated_submission_time': 5295.590830564499, 'accumulated_eval_time': 2502.7095897197723, 'accumulated_logging_time': 0.631049633026123, 'global_step': 16376, 'preemption_count': 0}), (17110, {'train/accuracy': 0.9904873967170715, 'train/loss': 0.030995003879070282, 'train/mean_average_precision': 0.39284304564245887, 'validation/accuracy': 0.9867208003997803, 'validation/loss': 0.04467495158314705, 'validation/mean_average_precision': 0.2598440136279063, 'validation/num_examples': 43793, 'test/accuracy': 0.9858613014221191, 'test/loss': 0.04753155633807182, 'test/mean_average_precision': 0.24660911204109393, 'test/num_examples': 43793, 'score': 5535.744588136673, 'total_duration': 8148.500215291977, 'accumulated_submission_time': 5535.744588136673, 'accumulated_eval_time': 2611.6243760585785, 'accumulated_logging_time': 0.6633293628692627, 'global_step': 17110, 'preemption_count': 0}), (17841, {'train/accuracy': 0.9907149076461792, 'train/loss': 0.030316093936562538, 'train/mean_average_precision': 0.4062286195899587, 'validation/accuracy': 0.9867061972618103, 'validation/loss': 0.044766735285520554, 'validation/mean_average_precision': 0.26295826409490414, 'validation/num_examples': 43793, 'test/accuracy': 0.9859004616737366, 'test/loss': 0.0474623404443264, 'test/mean_average_precision': 0.25010620946383083, 'test/num_examples': 43793, 'score': 5775.890267133713, 'total_duration': 8495.548310518265, 'accumulated_submission_time': 5775.890267133713, 'accumulated_eval_time': 2718.475531101227, 'accumulated_logging_time': 0.6938979625701904, 'global_step': 17841, 'preemption_count': 0}), (18577, {'train/accuracy': 0.9908282160758972, 'train/loss': 0.02974702976644039, 'train/mean_average_precision': 0.4252668941110549, 'validation/accuracy': 0.986599862575531, 'validation/loss': 0.04437005892395973, 'validation/mean_average_precision': 0.2613065277900078, 'validation/num_examples': 43793, 'test/accuracy': 0.9857437610626221, 'test/loss': 0.047278378158807755, 'test/mean_average_precision': 0.25738644262985816, 'test/num_examples': 43793, 'score': 6016.049191236496, 'total_duration': 8844.591564416885, 'accumulated_submission_time': 6016.049191236496, 'accumulated_eval_time': 2827.3079738616943, 'accumulated_logging_time': 0.7254743576049805, 'global_step': 18577, 'preemption_count': 0}), (19329, {'train/accuracy': 0.9912741780281067, 'train/loss': 0.028726851567626, 'train/mean_average_precision': 0.4346732790575787, 'validation/accuracy': 0.9866855144500732, 'validation/loss': 0.04458357393741608, 'validation/mean_average_precision': 0.26563868494886217, 'validation/num_examples': 43793, 'test/accuracy': 0.9858258962631226, 'test/loss': 0.04710597172379494, 'test/mean_average_precision': 0.2557344872373129, 'test/num_examples': 43793, 'score': 6256.203496932983, 'total_duration': 9188.825388908386, 'accumulated_submission_time': 6256.203496932983, 'accumulated_eval_time': 2931.338026046753, 'accumulated_logging_time': 0.7550005912780762, 'global_step': 19329, 'preemption_count': 0}), (20076, {'train/accuracy': 0.9913241863250732, 'train/loss': 0.028325732797384262, 'train/mean_average_precision': 0.4522674479990455, 'validation/accuracy': 0.9865933656692505, 'validation/loss': 0.04457477852702141, 'validation/mean_average_precision': 0.26392464655433473, 'validation/num_examples': 43793, 'test/accuracy': 0.9858339428901672, 'test/loss': 0.046953234821558, 'test/mean_average_precision': 0.26261314590518825, 'test/num_examples': 43793, 'score': 6496.22972202301, 'total_duration': 9541.552764177322, 'accumulated_submission_time': 6496.22972202301, 'accumulated_eval_time': 3043.9894185066223, 'accumulated_logging_time': 0.7850522994995117, 'global_step': 20076, 'preemption_count': 0}), (20821, {'train/accuracy': 0.9910668134689331, 'train/loss': 0.029200004413723946, 'train/mean_average_precision': 0.4294235006191442, 'validation/accuracy': 0.9865803718566895, 'validation/loss': 0.04457120969891548, 'validation/mean_average_precision': 0.2657850786309507, 'validation/num_examples': 43793, 'test/accuracy': 0.9857593774795532, 'test/loss': 0.04739382490515709, 'test/mean_average_precision': 0.2585024581789923, 'test/num_examples': 43793, 'score': 6736.18346953392, 'total_duration': 9895.228059530258, 'accumulated_submission_time': 6736.18346953392, 'accumulated_eval_time': 3157.6582396030426, 'accumulated_logging_time': 0.8184309005737305, 'global_step': 20821, 'preemption_count': 0}), (21563, {'train/accuracy': 0.9908154606819153, 'train/loss': 0.029763884842395782, 'train/mean_average_precision': 0.40703255615476974, 'validation/accuracy': 0.9867427349090576, 'validation/loss': 0.04434947296977043, 'validation/mean_average_precision': 0.26975878350204774, 'validation/num_examples': 43793, 'test/accuracy': 0.9858583807945251, 'test/loss': 0.047186437994241714, 'test/mean_average_precision': 0.26410273506081405, 'test/num_examples': 43793, 'score': 6976.1531801223755, 'total_duration': 10242.161269426346, 'accumulated_submission_time': 6976.1531801223755, 'accumulated_eval_time': 3264.571383714676, 'accumulated_logging_time': 0.8491370677947998, 'global_step': 21563, 'preemption_count': 0}), (22303, {'train/accuracy': 0.9910184144973755, 'train/loss': 0.02950519323348999, 'train/mean_average_precision': 0.4229856291489511, 'validation/accuracy': 0.9867549538612366, 'validation/loss': 0.04416754096746445, 'validation/mean_average_precision': 0.2666595006962322, 'validation/num_examples': 43793, 'test/accuracy': 0.9859581589698792, 'test/loss': 0.04715448245406151, 'test/mean_average_precision': 0.25820191353976035, 'test/num_examples': 43793, 'score': 7216.347539186478, 'total_duration': 10595.352976083755, 'accumulated_submission_time': 7216.347539186478, 'accumulated_eval_time': 3377.514060497284, 'accumulated_logging_time': 0.8812205791473389, 'global_step': 22303, 'preemption_count': 0}), (23037, {'train/accuracy': 0.991021454334259, 'train/loss': 0.02940763346850872, 'train/mean_average_precision': 0.431725123072511, 'validation/accuracy': 0.9867947101593018, 'validation/loss': 0.04461195319890976, 'validation/mean_average_precision': 0.2679407448618573, 'validation/num_examples': 43793, 'test/accuracy': 0.986004114151001, 'test/loss': 0.0473821796476841, 'test/mean_average_precision': 0.26431638324571194, 'test/num_examples': 43793, 'score': 7456.5054433345795, 'total_duration': 10942.205500364304, 'accumulated_submission_time': 7456.5054433345795, 'accumulated_eval_time': 3484.1552045345306, 'accumulated_logging_time': 0.9138262271881104, 'global_step': 23037, 'preemption_count': 0}), (23786, {'train/accuracy': 0.9908633828163147, 'train/loss': 0.029719047248363495, 'train/mean_average_precision': 0.42751913916293544, 'validation/accuracy': 0.9866741299629211, 'validation/loss': 0.04453583061695099, 'validation/mean_average_precision': 0.26668650965521207, 'validation/num_examples': 43793, 'test/accuracy': 0.9857631921768188, 'test/loss': 0.047598451375961304, 'test/mean_average_precision': 0.2511069827343094, 'test/num_examples': 43793, 'score': 7696.663548946381, 'total_duration': 11290.314909934998, 'accumulated_submission_time': 7696.663548946381, 'accumulated_eval_time': 3592.053944826126, 'accumulated_logging_time': 0.946509599685669, 'global_step': 23786, 'preemption_count': 0}), (24530, {'train/accuracy': 0.9912306070327759, 'train/loss': 0.028274420648813248, 'train/mean_average_precision': 0.46558060549088254, 'validation/accuracy': 0.9868429899215698, 'validation/loss': 0.044499415904283524, 'validation/mean_average_precision': 0.26683267041100056, 'validation/num_examples': 43793, 'test/accuracy': 0.9859198331832886, 'test/loss': 0.04751171916723251, 'test/mean_average_precision': 0.2620446093170533, 'test/num_examples': 43793, 'score': 7936.902099847794, 'total_duration': 11639.210319280624, 'accumulated_submission_time': 7936.902099847794, 'accumulated_eval_time': 3700.659593105316, 'accumulated_logging_time': 0.9780809879302979, 'global_step': 24530, 'preemption_count': 0}), (25279, {'train/accuracy': 0.9913411736488342, 'train/loss': 0.02796102501451969, 'train/mean_average_precision': 0.4508059522935717, 'validation/accuracy': 0.9867439866065979, 'validation/loss': 0.044363945722579956, 'validation/mean_average_precision': 0.26568900577999616, 'validation/num_examples': 43793, 'test/accuracy': 0.9858301281929016, 'test/loss': 0.0474405437707901, 'test/mean_average_precision': 0.2627028867128916, 'test/num_examples': 43793, 'score': 8177.051397800446, 'total_duration': 11982.191071748734, 'accumulated_submission_time': 8177.051397800446, 'accumulated_eval_time': 3803.438230276108, 'accumulated_logging_time': 1.0112404823303223, 'global_step': 25279, 'preemption_count': 0}), (26024, {'train/accuracy': 0.9914472103118896, 'train/loss': 0.027597038075327873, 'train/mean_average_precision': 0.4639268114410071, 'validation/accuracy': 0.9867309927940369, 'validation/loss': 0.04464828595519066, 'validation/mean_average_precision': 0.2693065018483168, 'validation/num_examples': 43793, 'test/accuracy': 0.9858301281929016, 'test/loss': 0.04775211587548256, 'test/mean_average_precision': 0.26042903987497107, 'test/num_examples': 43793, 'score': 8417.059885501862, 'total_duration': 12333.731023311615, 'accumulated_submission_time': 8417.059885501862, 'accumulated_eval_time': 3914.915863752365, 'accumulated_logging_time': 1.0447053909301758, 'global_step': 26024, 'preemption_count': 0}), (26765, {'train/accuracy': 0.9916866421699524, 'train/loss': 0.026975542306900024, 'train/mean_average_precision': 0.48170280678897637, 'validation/accuracy': 0.9868255853652954, 'validation/loss': 0.04439857229590416, 'validation/mean_average_precision': 0.27580323962848435, 'validation/num_examples': 43793, 'test/accuracy': 0.9860129356384277, 'test/loss': 0.0470789335668087, 'test/mean_average_precision': 0.26394230768981414, 'test/num_examples': 43793, 'score': 8657.316404104233, 'total_duration': 12682.5444586277, 'accumulated_submission_time': 8657.316404104233, 'accumulated_eval_time': 4023.420888900757, 'accumulated_logging_time': 1.0764586925506592, 'global_step': 26765, 'preemption_count': 0}), (27507, {'train/accuracy': 0.9914228916168213, 'train/loss': 0.02792561985552311, 'train/mean_average_precision': 0.45979459235704995, 'validation/accuracy': 0.9868003726005554, 'validation/loss': 0.04430904984474182, 'validation/mean_average_precision': 0.2730667285241964, 'validation/num_examples': 43793, 'test/accuracy': 0.9858406782150269, 'test/loss': 0.04753226414322853, 'test/mean_average_precision': 0.25606542608510635, 'test/num_examples': 43793, 'score': 8897.550789117813, 'total_duration': 13028.423025131226, 'accumulated_submission_time': 8897.550789117813, 'accumulated_eval_time': 4129.012980937958, 'accumulated_logging_time': 1.1086671352386475, 'global_step': 27507, 'preemption_count': 0}), (28250, {'train/accuracy': 0.9913296699523926, 'train/loss': 0.028052499517798424, 'train/mean_average_precision': 0.4551051103635106, 'validation/accuracy': 0.9868763089179993, 'validation/loss': 0.04476146399974823, 'validation/mean_average_precision': 0.2724308668815321, 'validation/num_examples': 43793, 'test/accuracy': 0.9859476685523987, 'test/loss': 0.047663263976573944, 'test/mean_average_precision': 0.2613195491298001, 'test/num_examples': 43793, 'score': 9137.598731279373, 'total_duration': 13377.213291406631, 'accumulated_submission_time': 9137.598731279373, 'accumulated_eval_time': 4237.702749490738, 'accumulated_logging_time': 1.1413967609405518, 'global_step': 28250, 'preemption_count': 0}), (28993, {'train/accuracy': 0.9911660552024841, 'train/loss': 0.028592262417078018, 'train/mean_average_precision': 0.448091841051458, 'validation/accuracy': 0.9867184162139893, 'validation/loss': 0.04482870176434517, 'validation/mean_average_precision': 0.2768478901678559, 'validation/num_examples': 43793, 'test/accuracy': 0.9859139323234558, 'test/loss': 0.04776918143033981, 'test/mean_average_precision': 0.258527015701871, 'test/num_examples': 43793, 'score': 9377.87127995491, 'total_duration': 13723.3050699234, 'accumulated_submission_time': 9377.87127995491, 'accumulated_eval_time': 4343.46866941452, 'accumulated_logging_time': 1.1738147735595703, 'global_step': 28993, 'preemption_count': 0}), (29745, {'train/accuracy': 0.9913337826728821, 'train/loss': 0.028022971004247665, 'train/mean_average_precision': 0.4609738230187459, 'validation/accuracy': 0.986785352230072, 'validation/loss': 0.04489719495177269, 'validation/mean_average_precision': 0.2769324470043009, 'validation/num_examples': 43793, 'test/accuracy': 0.9858364462852478, 'test/loss': 0.04776325821876526, 'test/mean_average_precision': 0.2666181185574914, 'test/num_examples': 43793, 'score': 9618.070008039474, 'total_duration': 14068.406420230865, 'accumulated_submission_time': 9618.070008039474, 'accumulated_eval_time': 4448.318630695343, 'accumulated_logging_time': 1.2066032886505127, 'global_step': 29745, 'preemption_count': 0}), (30499, {'train/accuracy': 0.9914437532424927, 'train/loss': 0.027543263509869576, 'train/mean_average_precision': 0.4647404749149301, 'validation/accuracy': 0.9866364002227783, 'validation/loss': 0.04489878565073013, 'validation/mean_average_precision': 0.27435756338833756, 'validation/num_examples': 43793, 'test/accuracy': 0.9857665300369263, 'test/loss': 0.04794379323720932, 'test/mean_average_precision': 0.26346652635367124, 'test/num_examples': 43793, 'score': 9858.108284711838, 'total_duration': 14414.402602910995, 'accumulated_submission_time': 9858.108284711838, 'accumulated_eval_time': 4554.223012685776, 'accumulated_logging_time': 1.2404890060424805, 'global_step': 30499, 'preemption_count': 0}), (31246, {'train/accuracy': 0.991542637348175, 'train/loss': 0.027170853689312935, 'train/mean_average_precision': 0.4721364663286831, 'validation/accuracy': 0.9867780804634094, 'validation/loss': 0.044928766787052155, 'validation/mean_average_precision': 0.27216707266533563, 'validation/num_examples': 43793, 'test/accuracy': 0.9858992099761963, 'test/loss': 0.04786492511630058, 'test/mean_average_precision': 0.26131082409592704, 'test/num_examples': 43793, 'score': 10098.096035957336, 'total_duration': 14764.987814426422, 'accumulated_submission_time': 10098.096035957336, 'accumulated_eval_time': 4664.768091201782, 'accumulated_logging_time': 1.2731256484985352, 'global_step': 31246, 'preemption_count': 0}), (31991, {'train/accuracy': 0.9917091727256775, 'train/loss': 0.026494089514017105, 'train/mean_average_precision': 0.5052146545409436, 'validation/accuracy': 0.9868125915527344, 'validation/loss': 0.045099370181560516, 'validation/mean_average_precision': 0.2773346138812802, 'validation/num_examples': 43793, 'test/accuracy': 0.9860554933547974, 'test/loss': 0.04802294075489044, 'test/mean_average_precision': 0.2684884152472522, 'test/num_examples': 43793, 'score': 10338.08183336258, 'total_duration': 15111.78766322136, 'accumulated_submission_time': 10338.08183336258, 'accumulated_eval_time': 4771.527981519699, 'accumulated_logging_time': 1.307499647140503, 'global_step': 31991, 'preemption_count': 0}), (32748, {'train/accuracy': 0.9920130968093872, 'train/loss': 0.025551490485668182, 'train/mean_average_precision': 0.5120100156677214, 'validation/accuracy': 0.9867143034934998, 'validation/loss': 0.04505418986082077, 'validation/mean_average_precision': 0.2664563256925483, 'validation/num_examples': 43793, 'test/accuracy': 0.9857795834541321, 'test/loss': 0.04795093834400177, 'test/mean_average_precision': 0.2652142004790609, 'test/num_examples': 43793, 'score': 10578.065612077713, 'total_duration': 15454.269760608673, 'accumulated_submission_time': 10578.065612077713, 'accumulated_eval_time': 4873.972446680069, 'accumulated_logging_time': 1.3417332172393799, 'global_step': 32748, 'preemption_count': 0}), (33481, {'train/accuracy': 0.9919853806495667, 'train/loss': 0.025528879836201668, 'train/mean_average_precision': 0.5158409082918909, 'validation/accuracy': 0.9867808818817139, 'validation/loss': 0.045148346573114395, 'validation/mean_average_precision': 0.2665157521690351, 'validation/num_examples': 43793, 'test/accuracy': 0.9859358668327332, 'test/loss': 0.048278387635946274, 'test/mean_average_precision': 0.2660083338093088, 'test/num_examples': 43793, 'score': 10818.260104179382, 'total_duration': 15806.758916139603, 'accumulated_submission_time': 10818.260104179382, 'accumulated_eval_time': 4986.211427688599, 'accumulated_logging_time': 1.3757221698760986, 'global_step': 33481, 'preemption_count': 0}), (34209, {'train/accuracy': 0.9919201135635376, 'train/loss': 0.026125213131308556, 'train/mean_average_precision': 0.4842983115492911, 'validation/accuracy': 0.9868369102478027, 'validation/loss': 0.04484208673238754, 'validation/mean_average_precision': 0.27267152273883244, 'validation/num_examples': 43793, 'test/accuracy': 0.9858587980270386, 'test/loss': 0.04812852293252945, 'test/mean_average_precision': 0.26192709755040916, 'test/num_examples': 43793, 'score': 11058.269440174103, 'total_duration': 16153.24194407463, 'accumulated_submission_time': 11058.269440174103, 'accumulated_eval_time': 5092.624956607819, 'accumulated_logging_time': 1.4126989841461182, 'global_step': 34209, 'preemption_count': 0}), (34945, {'train/accuracy': 0.991661548614502, 'train/loss': 0.026801934465765953, 'train/mean_average_precision': 0.4983068987457481, 'validation/accuracy': 0.9867764711380005, 'validation/loss': 0.045043252408504486, 'validation/mean_average_precision': 0.2768250623646225, 'validation/num_examples': 43793, 'test/accuracy': 0.98597252368927, 'test/loss': 0.048193011432886124, 'test/mean_average_precision': 0.2647074282382107, 'test/num_examples': 43793, 'score': 11298.358862400055, 'total_duration': 16503.35486650467, 'accumulated_submission_time': 11298.358862400055, 'accumulated_eval_time': 5202.5957906246185, 'accumulated_logging_time': 1.445598840713501, 'global_step': 34945, 'preemption_count': 0}), (35690, {'train/accuracy': 0.9916203618049622, 'train/loss': 0.026906408369541168, 'train/mean_average_precision': 0.4801378497014065, 'validation/accuracy': 0.9867455959320068, 'validation/loss': 0.04513007029891014, 'validation/mean_average_precision': 0.26837549418785717, 'validation/num_examples': 43793, 'test/accuracy': 0.9858074188232422, 'test/loss': 0.04827474057674408, 'test/mean_average_precision': 0.2578979398603731, 'test/num_examples': 43793, 'score': 11538.379633426666, 'total_duration': 16849.039041757584, 'accumulated_submission_time': 11538.379633426666, 'accumulated_eval_time': 5308.205292224884, 'accumulated_logging_time': 1.4793949127197266, 'global_step': 35690, 'preemption_count': 0}), (36436, {'train/accuracy': 0.9917700290679932, 'train/loss': 0.026203596964478493, 'train/mean_average_precision': 0.5078279699959175, 'validation/accuracy': 0.9868600368499756, 'validation/loss': 0.044907744973897934, 'validation/mean_average_precision': 0.279754069660172, 'validation/num_examples': 43793, 'test/accuracy': 0.9859842658042908, 'test/loss': 0.04798304662108421, 'test/mean_average_precision': 0.2682538265776146, 'test/num_examples': 43793, 'score': 11778.522836446762, 'total_duration': 17200.367270946503, 'accumulated_submission_time': 11778.522836446762, 'accumulated_eval_time': 5419.337324857712, 'accumulated_logging_time': 1.5126359462738037, 'global_step': 36436, 'preemption_count': 0}), (37188, {'train/accuracy': 0.9918221831321716, 'train/loss': 0.026032911613583565, 'train/mean_average_precision': 0.4927025177955553, 'validation/accuracy': 0.9868361353874207, 'validation/loss': 0.04488707333803177, 'validation/mean_average_precision': 0.2795414593961319, 'validation/num_examples': 43793, 'test/accuracy': 0.9859581589698792, 'test/loss': 0.048093561083078384, 'test/mean_average_precision': 0.26502508387445306, 'test/num_examples': 43793, 'score': 12018.514259815216, 'total_duration': 17546.48204088211, 'accumulated_submission_time': 12018.514259815216, 'accumulated_eval_time': 5525.407692909241, 'accumulated_logging_time': 1.5457723140716553, 'global_step': 37188, 'preemption_count': 0}), (37936, {'train/accuracy': 0.9920411109924316, 'train/loss': 0.02535792626440525, 'train/mean_average_precision': 0.521290996199623, 'validation/accuracy': 0.9868373274803162, 'validation/loss': 0.04548921436071396, 'validation/mean_average_precision': 0.27167468894968716, 'validation/num_examples': 43793, 'test/accuracy': 0.9859695434570312, 'test/loss': 0.04852062836289406, 'test/mean_average_precision': 0.2581425592235508, 'test/num_examples': 43793, 'score': 12258.54852104187, 'total_duration': 17889.70045185089, 'accumulated_submission_time': 12258.54852104187, 'accumulated_eval_time': 5628.5393006801605, 'accumulated_logging_time': 1.5792734622955322, 'global_step': 37936, 'preemption_count': 0}), (38681, {'train/accuracy': 0.9922906160354614, 'train/loss': 0.02447586879134178, 'train/mean_average_precision': 0.5456552959656337, 'validation/accuracy': 0.9868028163909912, 'validation/loss': 0.04524470493197441, 'validation/mean_average_precision': 0.2777050270904259, 'validation/num_examples': 43793, 'test/accuracy': 0.9859328866004944, 'test/loss': 0.04847346618771553, 'test/mean_average_precision': 0.2601893972789249, 'test/num_examples': 43793, 'score': 12498.732014417648, 'total_duration': 18235.73575282097, 'accumulated_submission_time': 12498.732014417648, 'accumulated_eval_time': 5734.337277889252, 'accumulated_logging_time': 1.6127097606658936, 'global_step': 38681, 'preemption_count': 0}), (39429, {'train/accuracy': 0.9927371740341187, 'train/loss': 0.023332200944423676, 'train/mean_average_precision': 0.5693393673773479, 'validation/accuracy': 0.9867658615112305, 'validation/loss': 0.04525458440184593, 'validation/mean_average_precision': 0.2783459699096976, 'validation/num_examples': 43793, 'test/accuracy': 0.9858764410018921, 'test/loss': 0.04857688024640083, 'test/mean_average_precision': 0.26199890137701637, 'test/num_examples': 43793, 'score': 12738.766446590424, 'total_duration': 18584.267076969147, 'accumulated_submission_time': 12738.766446590424, 'accumulated_eval_time': 5842.7793345451355, 'accumulated_logging_time': 1.6485440731048584, 'global_step': 39429, 'preemption_count': 0}), (40178, {'train/accuracy': 0.9926030039787292, 'train/loss': 0.023707717657089233, 'train/mean_average_precision': 0.5622201792353998, 'validation/accuracy': 0.9866286516189575, 'validation/loss': 0.04526278376579285, 'validation/mean_average_precision': 0.27261435378096305, 'validation/num_examples': 43793, 'test/accuracy': 0.9857779145240784, 'test/loss': 0.04833337664604187, 'test/mean_average_precision': 0.2643365091769727, 'test/num_examples': 43793, 'score': 12978.871631383896, 'total_duration': 18926.955935239792, 'accumulated_submission_time': 12978.871631383896, 'accumulated_eval_time': 5945.3051841259, 'accumulated_logging_time': 1.685373306274414, 'global_step': 40178, 'preemption_count': 0}), (40922, {'train/accuracy': 0.9924303889274597, 'train/loss': 0.02415706031024456, 'train/mean_average_precision': 0.531331590513631, 'validation/accuracy': 0.9868612885475159, 'validation/loss': 0.04556350037455559, 'validation/mean_average_precision': 0.2747620842624564, 'validation/num_examples': 43793, 'test/accuracy': 0.9859089255332947, 'test/loss': 0.04895516857504845, 'test/mean_average_precision': 0.26250794162981905, 'test/num_examples': 43793, 'score': 13219.148606538773, 'total_duration': 19276.352915763855, 'accumulated_submission_time': 13219.148606538773, 'accumulated_eval_time': 6054.370141029358, 'accumulated_logging_time': 1.7200186252593994, 'global_step': 40922, 'preemption_count': 0}), (41668, {'train/accuracy': 0.9921717047691345, 'train/loss': 0.024784540757536888, 'train/mean_average_precision': 0.5356050889597663, 'validation/accuracy': 0.986797571182251, 'validation/loss': 0.04562019556760788, 'validation/mean_average_precision': 0.27944630554689964, 'validation/num_examples': 43793, 'test/accuracy': 0.9858528971672058, 'test/loss': 0.04906835779547691, 'test/mean_average_precision': 0.2610362912805396, 'test/num_examples': 43793, 'score': 13459.148141384125, 'total_duration': 19620.42688894272, 'accumulated_submission_time': 13459.148141384125, 'accumulated_eval_time': 6158.383851766586, 'accumulated_logging_time': 1.759134292602539, 'global_step': 41668, 'preemption_count': 0}), (42412, {'train/accuracy': 0.9920896291732788, 'train/loss': 0.02493622526526451, 'train/mean_average_precision': 0.5138407443009074, 'validation/accuracy': 0.9867837429046631, 'validation/loss': 0.046225953847169876, 'validation/mean_average_precision': 0.2705062063880303, 'validation/num_examples': 43793, 'test/accuracy': 0.9858364462852478, 'test/loss': 0.049579277634620667, 'test/mean_average_precision': 0.2634238571883222, 'test/num_examples': 43793, 'score': 13699.192795991898, 'total_duration': 19969.621764421463, 'accumulated_submission_time': 13699.192795991898, 'accumulated_eval_time': 6267.478013277054, 'accumulated_logging_time': 1.794682502746582, 'global_step': 42412, 'preemption_count': 0}), (43161, {'train/accuracy': 0.9922144412994385, 'train/loss': 0.02469654195010662, 'train/mean_average_precision': 0.5323905620648921, 'validation/accuracy': 0.9867528676986694, 'validation/loss': 0.04586846008896828, 'validation/mean_average_precision': 0.27743120946362904, 'validation/num_examples': 43793, 'test/accuracy': 0.9857585430145264, 'test/loss': 0.04930279403924942, 'test/mean_average_precision': 0.2641423804382998, 'test/num_examples': 43793, 'score': 13939.405371427536, 'total_duration': 20312.19924545288, 'accumulated_submission_time': 13939.405371427536, 'accumulated_eval_time': 6369.788389205933, 'accumulated_logging_time': 1.8295137882232666, 'global_step': 43161, 'preemption_count': 0}), (43909, {'train/accuracy': 0.9923882484436035, 'train/loss': 0.024186132475733757, 'train/mean_average_precision': 0.5461527346502013, 'validation/accuracy': 0.9866071343421936, 'validation/loss': 0.04615493491292, 'validation/mean_average_precision': 0.27460194184325004, 'validation/num_examples': 43793, 'test/accuracy': 0.9857884645462036, 'test/loss': 0.04912806674838066, 'test/mean_average_precision': 0.2697252043773885, 'test/num_examples': 43793, 'score': 14179.438196897507, 'total_duration': 20661.7832839489, 'accumulated_submission_time': 14179.438196897507, 'accumulated_eval_time': 6479.280866146088, 'accumulated_logging_time': 1.8684642314910889, 'global_step': 43909, 'preemption_count': 0}), (44660, {'train/accuracy': 0.9926992058753967, 'train/loss': 0.02298043482005596, 'train/mean_average_precision': 0.573160671933495, 'validation/accuracy': 0.9866668581962585, 'validation/loss': 0.04609571397304535, 'validation/mean_average_precision': 0.27831066848668895, 'validation/num_examples': 43793, 'test/accuracy': 0.9857361912727356, 'test/loss': 0.04914650693535805, 'test/mean_average_precision': 0.26561351397120253, 'test/num_examples': 43793, 'score': 14419.610810041428, 'total_duration': 21006.03396821022, 'accumulated_submission_time': 14419.610810041428, 'accumulated_eval_time': 6583.301622629166, 'accumulated_logging_time': 1.906172752380371, 'global_step': 44660, 'preemption_count': 0}), (45411, {'train/accuracy': 0.992841899394989, 'train/loss': 0.02246236242353916, 'train/mean_average_precision': 0.5858940454668893, 'validation/accuracy': 0.9867216348648071, 'validation/loss': 0.04655701294541359, 'validation/mean_average_precision': 0.27335724604063805, 'validation/num_examples': 43793, 'test/accuracy': 0.9858831763267517, 'test/loss': 0.05000138655304909, 'test/mean_average_precision': 0.2606782498808195, 'test/num_examples': 43793, 'score': 14659.80555152893, 'total_duration': 21351.36517882347, 'accumulated_submission_time': 14659.80555152893, 'accumulated_eval_time': 6688.38343501091, 'accumulated_logging_time': 1.9410452842712402, 'global_step': 45411, 'preemption_count': 0}), (46154, {'train/accuracy': 0.9932268857955933, 'train/loss': 0.02133258245885372, 'train/mean_average_precision': 0.6116497269315911, 'validation/accuracy': 0.9866270422935486, 'validation/loss': 0.046749576926231384, 'validation/mean_average_precision': 0.2717662805382729, 'validation/num_examples': 43793, 'test/accuracy': 0.9857484102249146, 'test/loss': 0.05015978589653969, 'test/mean_average_precision': 0.262155570037263, 'test/num_examples': 43793, 'score': 14900.003878116608, 'total_duration': 21695.22643852234, 'accumulated_submission_time': 14900.003878116608, 'accumulated_eval_time': 6791.985899925232, 'accumulated_logging_time': 1.9806413650512695, 'global_step': 46154, 'preemption_count': 0}), (46898, {'train/accuracy': 0.9933241009712219, 'train/loss': 0.02106494829058647, 'train/mean_average_precision': 0.6244343061105562, 'validation/accuracy': 0.986622154712677, 'validation/loss': 0.04705076292157173, 'validation/mean_average_precision': 0.2749869542278762, 'validation/num_examples': 43793, 'test/accuracy': 0.9857871532440186, 'test/loss': 0.05036665126681328, 'test/mean_average_precision': 0.2615554241055192, 'test/num_examples': 43793, 'score': 15140.24789738655, 'total_duration': 22042.06078195572, 'accumulated_submission_time': 15140.24789738655, 'accumulated_eval_time': 6898.519830942154, 'accumulated_logging_time': 2.017240524291992, 'global_step': 46898, 'preemption_count': 0}), (47655, {'train/accuracy': 0.9933059215545654, 'train/loss': 0.02119574137032032, 'train/mean_average_precision': 0.6117602752374381, 'validation/accuracy': 0.9866501688957214, 'validation/loss': 0.04724955931305885, 'validation/mean_average_precision': 0.2753124853835367, 'validation/num_examples': 43793, 'test/accuracy': 0.9858199954032898, 'test/loss': 0.05041026696562767, 'test/mean_average_precision': 0.2589229370037478, 'test/num_examples': 43793, 'score': 15380.289312124252, 'total_duration': 22384.619906663895, 'accumulated_submission_time': 15380.289312124252, 'accumulated_eval_time': 7000.982522249222, 'accumulated_logging_time': 2.052924633026123, 'global_step': 47655, 'preemption_count': 0}), (48397, {'train/accuracy': 0.9927676320075989, 'train/loss': 0.022809168323874474, 'train/mean_average_precision': 0.5803819765058651, 'validation/accuracy': 0.986539363861084, 'validation/loss': 0.04781175032258034, 'validation/mean_average_precision': 0.2669560169577921, 'validation/num_examples': 43793, 'test/accuracy': 0.9856439828872681, 'test/loss': 0.05102235823869705, 'test/mean_average_precision': 0.2503140930026999, 'test/num_examples': 43793, 'score': 15620.493504047394, 'total_duration': 22733.268282413483, 'accumulated_submission_time': 15620.493504047394, 'accumulated_eval_time': 7109.367336988449, 'accumulated_logging_time': 2.09198260307312, 'global_step': 48397, 'preemption_count': 0}), (49145, {'train/accuracy': 0.9926386475563049, 'train/loss': 0.023112531751394272, 'train/mean_average_precision': 0.5560028585107328, 'validation/accuracy': 0.986544668674469, 'validation/loss': 0.047696709632873535, 'validation/mean_average_precision': 0.27548860230874456, 'validation/num_examples': 43793, 'test/accuracy': 0.9856220483779907, 'test/loss': 0.05133892968297005, 'test/mean_average_precision': 0.2546934945497422, 'test/num_examples': 43793, 'score': 15860.683614492416, 'total_duration': 23075.458447933197, 'accumulated_submission_time': 15860.683614492416, 'accumulated_eval_time': 7211.308455705643, 'accumulated_logging_time': 2.1313531398773193, 'global_step': 49145, 'preemption_count': 0}), (49889, {'train/accuracy': 0.992885947227478, 'train/loss': 0.022317999973893166, 'train/mean_average_precision': 0.5716329006994851, 'validation/accuracy': 0.9866303205490112, 'validation/loss': 0.047895122319459915, 'validation/mean_average_precision': 0.2716456802267858, 'validation/num_examples': 43793, 'test/accuracy': 0.9857316017150879, 'test/loss': 0.05132818594574928, 'test/mean_average_precision': 0.256285389852942, 'test/num_examples': 43793, 'score': 16100.88074851036, 'total_duration': 23424.283234357834, 'accumulated_submission_time': 16100.88074851036, 'accumulated_eval_time': 7319.88063287735, 'accumulated_logging_time': 2.1673531532287598, 'global_step': 49889, 'preemption_count': 0}), (50635, {'train/accuracy': 0.9929296374320984, 'train/loss': 0.02187691256403923, 'train/mean_average_precision': 0.6079837780471014, 'validation/accuracy': 0.9864983558654785, 'validation/loss': 0.047983817756175995, 'validation/mean_average_precision': 0.27032804939891897, 'validation/num_examples': 43793, 'test/accuracy': 0.9856178760528564, 'test/loss': 0.051483746618032455, 'test/mean_average_precision': 0.2527862156016627, 'test/num_examples': 43793, 'score': 16341.046777009964, 'total_duration': 23769.316796541214, 'accumulated_submission_time': 16341.046777009964, 'accumulated_eval_time': 7424.692152500153, 'accumulated_logging_time': 2.2040598392486572, 'global_step': 50635, 'preemption_count': 0}), (51387, {'train/accuracy': 0.99312424659729, 'train/loss': 0.02137378603219986, 'train/mean_average_precision': 0.6051922907115049, 'validation/accuracy': 0.9864557385444641, 'validation/loss': 0.04817109555006027, 'validation/mean_average_precision': 0.27860323962391087, 'validation/num_examples': 43793, 'test/accuracy': 0.9855256080627441, 'test/loss': 0.05173730477690697, 'test/mean_average_precision': 0.25799338239721004, 'test/num_examples': 43793, 'score': 16581.230067253113, 'total_duration': 24114.574266433716, 'accumulated_submission_time': 16581.230067253113, 'accumulated_eval_time': 7529.710796117783, 'accumulated_logging_time': 2.239814043045044, 'global_step': 51387, 'preemption_count': 0}), (52126, {'train/accuracy': 0.9935033917427063, 'train/loss': 0.0201828982681036, 'train/mean_average_precision': 0.6243054543350697, 'validation/accuracy': 0.9865494966506958, 'validation/loss': 0.04877251386642456, 'validation/mean_average_precision': 0.2713997153176349, 'validation/num_examples': 43793, 'test/accuracy': 0.9856654405593872, 'test/loss': 0.05205725133419037, 'test/mean_average_precision': 0.2631199337380247, 'test/num_examples': 43793, 'score': 16821.317676067352, 'total_duration': 24455.993393421173, 'accumulated_submission_time': 16821.317676067352, 'accumulated_eval_time': 7630.984179973602, 'accumulated_logging_time': 2.2781577110290527, 'global_step': 52126, 'preemption_count': 0}), (52874, {'train/accuracy': 0.9940038323402405, 'train/loss': 0.018666956573724747, 'train/mean_average_precision': 0.6656468057073384, 'validation/accuracy': 0.9866250157356262, 'validation/loss': 0.048979878425598145, 'validation/mean_average_precision': 0.27065398680055736, 'validation/num_examples': 43793, 'test/accuracy': 0.9856258630752563, 'test/loss': 0.052842892706394196, 'test/mean_average_precision': 0.2543117989191361, 'test/num_examples': 43793, 'score': 17061.454341888428, 'total_duration': 24799.02546787262, 'accumulated_submission_time': 17061.454341888428, 'accumulated_eval_time': 7733.823001623154, 'accumulated_logging_time': 2.314923048019409, 'global_step': 52874, 'preemption_count': 0}), (53612, {'train/accuracy': 0.9942200183868408, 'train/loss': 0.01806596666574478, 'train/mean_average_precision': 0.6819209665010129, 'validation/accuracy': 0.9864720106124878, 'validation/loss': 0.04959718883037567, 'validation/mean_average_precision': 0.2684478956022052, 'validation/num_examples': 43793, 'test/accuracy': 0.9855963587760925, 'test/loss': 0.053325194865465164, 'test/mean_average_precision': 0.2544714874167593, 'test/num_examples': 43793, 'score': 17301.6496155262, 'total_duration': 25150.59912610054, 'accumulated_submission_time': 17301.6496155262, 'accumulated_eval_time': 7845.143984079361, 'accumulated_logging_time': 2.351712226867676, 'global_step': 53612, 'preemption_count': 0}), (54367, {'train/accuracy': 0.994297206401825, 'train/loss': 0.01786726526916027, 'train/mean_average_precision': 0.6827159617126499, 'validation/accuracy': 0.9865003824234009, 'validation/loss': 0.04968026280403137, 'validation/mean_average_precision': 0.2730149322743814, 'validation/num_examples': 43793, 'test/accuracy': 0.9855533838272095, 'test/loss': 0.053486552089452744, 'test/mean_average_precision': 0.25917765667164855, 'test/num_examples': 43793, 'score': 17541.733896255493, 'total_duration': 25496.444694042206, 'accumulated_submission_time': 17541.733896255493, 'accumulated_eval_time': 7950.844016551971, 'accumulated_logging_time': 2.3931870460510254, 'global_step': 54367, 'preemption_count': 0}), (55116, {'train/accuracy': 0.9939027428627014, 'train/loss': 0.01882188953459263, 'train/mean_average_precision': 0.6740066988974451, 'validation/accuracy': 0.9864963293075562, 'validation/loss': 0.0502084381878376, 'validation/mean_average_precision': 0.2762999691316146, 'validation/num_examples': 43793, 'test/accuracy': 0.9855660200119019, 'test/loss': 0.05363349989056587, 'test/mean_average_precision': 0.2579593188538949, 'test/num_examples': 43793, 'score': 17781.83658337593, 'total_duration': 25844.768416643143, 'accumulated_submission_time': 17781.83658337593, 'accumulated_eval_time': 8059.008177280426, 'accumulated_logging_time': 2.4300525188446045, 'global_step': 55116, 'preemption_count': 0}), (55871, {'train/accuracy': 0.9938641786575317, 'train/loss': 0.018877951428294182, 'train/mean_average_precision': 0.6532421190749456, 'validation/accuracy': 0.9863501787185669, 'validation/loss': 0.0509905144572258, 'validation/mean_average_precision': 0.27272084776079214, 'validation/num_examples': 43793, 'test/accuracy': 0.9854717254638672, 'test/loss': 0.054655853658914566, 'test/mean_average_precision': 0.25146160516765664, 'test/num_examples': 43793, 'score': 18021.98615550995, 'total_duration': 26187.977236509323, 'accumulated_submission_time': 18021.98615550995, 'accumulated_eval_time': 8162.010338068008, 'accumulated_logging_time': 2.467262029647827, 'global_step': 55871, 'preemption_count': 0}), (56616, {'train/accuracy': 0.9935514330863953, 'train/loss': 0.019919833168387413, 'train/mean_average_precision': 0.6342096266526925, 'validation/accuracy': 0.9862856864929199, 'validation/loss': 0.051337894052267075, 'validation/mean_average_precision': 0.2656026553756837, 'validation/num_examples': 43793, 'test/accuracy': 0.985351026058197, 'test/loss': 0.05505384877324104, 'test/mean_average_precision': 0.2538779185836647, 'test/num_examples': 43793, 'score': 18262.12407398224, 'total_duration': 26531.270143032074, 'accumulated_submission_time': 18262.12407398224, 'accumulated_eval_time': 8265.10645365715, 'accumulated_logging_time': 2.506730794906616, 'global_step': 56616, 'preemption_count': 0})], 'global_step': 57282}
I0210 00:03:48.786059 140039251117888 submission_runner.py:586] Timing: 18477.27854990959
I0210 00:03:48.786115 140039251117888 submission_runner.py:588] Total number of evals: 77
I0210 00:03:48.786158 140039251117888 submission_runner.py:589] ====================
I0210 00:03:48.786204 140039251117888 submission_runner.py:542] Using RNG seed 1895687988
I0210 00:03:48.846817 140039251117888 submission_runner.py:551] --- Tuning run 4/5 ---
I0210 00:03:48.846969 140039251117888 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_3/ogbg_jax/trial_4.
I0210 00:03:48.847239 140039251117888 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_3/ogbg_jax/trial_4/hparams.json.
I0210 00:03:48.973222 140039251117888 submission_runner.py:206] Initializing dataset.
I0210 00:03:49.054511 140039251117888 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0210 00:03:49.058862 140039251117888 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0210 00:03:49.184228 140039251117888 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0210 00:03:49.220471 140039251117888 submission_runner.py:213] Initializing model.
I0210 00:03:51.595419 140039251117888 submission_runner.py:255] Initializing optimizer.
I0210 00:03:52.177021 140039251117888 submission_runner.py:262] Initializing metrics bundle.
I0210 00:03:52.177238 140039251117888 submission_runner.py:280] Initializing checkpoint and logger.
I0210 00:03:52.177976 140039251117888 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_3/ogbg_jax/trial_4 with prefix checkpoint_
I0210 00:03:52.178119 140039251117888 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_3/ogbg_jax/trial_4/meta_data_0.json.
I0210 00:03:52.178373 140039251117888 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0210 00:03:52.178456 140039251117888 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0210 00:03:54.096510 140039251117888 logger_utils.py:220] Unable to record git information. Continuing without it.
I0210 00:03:56.003988 140039251117888 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_3/ogbg_jax/trial_4/flags_0.json.
I0210 00:03:56.007935 140039251117888 submission_runner.py:314] Starting training loop.
I0210 00:04:08.219551 139817996203776 logging_writer.py:48] [0] global_step=0, grad_norm=2.7368342876434326, loss=0.7145252823829651
I0210 00:04:08.228912 140039251117888 spec.py:321] Evaluating on the training split.
I0210 00:05:50.154489 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 00:05:53.358294 140039251117888 spec.py:349] Evaluating on the test split.
I0210 00:05:56.426055 140039251117888 submission_runner.py:408] Time since start: 120.42s, 	Step: 1, 	{'train/accuracy': 0.5250684022903442, 'train/loss': 0.715176522731781, 'train/mean_average_precision': 0.023795002749508583, 'validation/accuracy': 0.5213832855224609, 'validation/loss': 0.7166012525558472, 'validation/mean_average_precision': 0.026141387117008964, 'validation/num_examples': 43793, 'test/accuracy': 0.5224818587303162, 'test/loss': 0.7161954641342163, 'test/mean_average_precision': 0.027845703932648375, 'test/num_examples': 43793, 'score': 12.220922231674194, 'total_duration': 120.41805958747864, 'accumulated_submission_time': 12.220922231674194, 'accumulated_eval_time': 108.19709205627441, 'accumulated_logging_time': 0}
I0210 00:05:56.435792 139818015901440 logging_writer.py:48] [1] accumulated_eval_time=108.197092, accumulated_logging_time=0, accumulated_submission_time=12.220922, global_step=1, preemption_count=0, score=12.220922, test/accuracy=0.522482, test/loss=0.716195, test/mean_average_precision=0.027846, test/num_examples=43793, total_duration=120.418060, train/accuracy=0.525068, train/loss=0.715177, train/mean_average_precision=0.023795, validation/accuracy=0.521383, validation/loss=0.716601, validation/mean_average_precision=0.026141, validation/num_examples=43793
I0210 00:06:29.189963 139836634785536 logging_writer.py:48] [100] global_step=100, grad_norm=0.1030503511428833, loss=0.1124754250049591
I0210 00:07:01.676152 139818015901440 logging_writer.py:48] [200] global_step=200, grad_norm=0.006617446895688772, loss=0.05630972981452942
I0210 00:07:34.106795 139836634785536 logging_writer.py:48] [300] global_step=300, grad_norm=0.010944606736302376, loss=0.05476009473204613
I0210 00:08:06.600435 139818015901440 logging_writer.py:48] [400] global_step=400, grad_norm=0.014553423970937729, loss=0.05808673053979874
I0210 00:08:38.770832 139836634785536 logging_writer.py:48] [500] global_step=500, grad_norm=0.024274442344903946, loss=0.052038680762052536
I0210 00:09:10.988530 139818015901440 logging_writer.py:48] [600] global_step=600, grad_norm=0.022319113835692406, loss=0.057922229170799255
I0210 00:09:43.522258 139836634785536 logging_writer.py:48] [700] global_step=700, grad_norm=0.00523098511621356, loss=0.04661891609430313
I0210 00:09:56.709321 140039251117888 spec.py:321] Evaluating on the training split.
I0210 00:11:39.525495 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 00:11:42.621931 140039251117888 spec.py:349] Evaluating on the test split.
I0210 00:11:45.622404 140039251117888 submission_runner.py:408] Time since start: 469.61s, 	Step: 742, 	{'train/accuracy': 0.9868351221084595, 'train/loss': 0.05353858694434166, 'train/mean_average_precision': 0.04192054947942697, 'validation/accuracy': 0.9841658473014832, 'validation/loss': 0.06382942944765091, 'validation/mean_average_precision': 0.04026482508837749, 'validation/num_examples': 43793, 'test/accuracy': 0.9831947088241577, 'test/loss': 0.06701404601335526, 'test/mean_average_precision': 0.04095081648056769, 'test/num_examples': 43793, 'score': 252.46294784545898, 'total_duration': 469.6144058704376, 'accumulated_submission_time': 252.46294784545898, 'accumulated_eval_time': 217.11013174057007, 'accumulated_logging_time': 0.021190404891967773}
I0210 00:11:45.637715 139818172847872 logging_writer.py:48] [742] accumulated_eval_time=217.110132, accumulated_logging_time=0.021190, accumulated_submission_time=252.462948, global_step=742, preemption_count=0, score=252.462948, test/accuracy=0.983195, test/loss=0.067014, test/mean_average_precision=0.040951, test/num_examples=43793, total_duration=469.614406, train/accuracy=0.986835, train/loss=0.053539, train/mean_average_precision=0.041921, validation/accuracy=0.984166, validation/loss=0.063829, validation/mean_average_precision=0.040265, validation/num_examples=43793
I0210 00:12:05.221253 139836244932352 logging_writer.py:48] [800] global_step=800, grad_norm=0.01045104581862688, loss=0.05699751898646355
I0210 00:12:37.623283 139818172847872 logging_writer.py:48] [900] global_step=900, grad_norm=0.008805779740214348, loss=0.05686166509985924
I0210 00:13:09.984125 139836244932352 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.01244784239679575, loss=0.05575518682599068
I0210 00:13:42.139890 139818172847872 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.00990067794919014, loss=0.053012531250715256
I0210 00:14:14.387035 139836244932352 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.007015335373580456, loss=0.050054699182510376
I0210 00:14:47.146665 139818172847872 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.006085480097681284, loss=0.0487276166677475
I0210 00:15:19.618290 139836244932352 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.007847441360354424, loss=0.04648526385426521
I0210 00:15:45.842485 140039251117888 spec.py:321] Evaluating on the training split.
I0210 00:17:25.240122 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 00:17:28.312406 140039251117888 spec.py:349] Evaluating on the test split.
I0210 00:17:31.283467 140039251117888 submission_runner.py:408] Time since start: 815.28s, 	Step: 1482, 	{'train/accuracy': 0.9870707988739014, 'train/loss': 0.049404438585042953, 'train/mean_average_precision': 0.07272801721838304, 'validation/accuracy': 0.9842279553413391, 'validation/loss': 0.06012863293290138, 'validation/mean_average_precision': 0.06954331548862047, 'validation/num_examples': 43793, 'test/accuracy': 0.9832208156585693, 'test/loss': 0.06371833384037018, 'test/mean_average_precision': 0.06871401787448847, 'test/num_examples': 43793, 'score': 492.63631868362427, 'total_duration': 815.2754602432251, 'accumulated_submission_time': 492.63631868362427, 'accumulated_eval_time': 322.55107712745667, 'accumulated_logging_time': 0.0476987361907959}
I0210 00:17:31.299065 139818024294144 logging_writer.py:48] [1482] accumulated_eval_time=322.551077, accumulated_logging_time=0.047699, accumulated_submission_time=492.636319, global_step=1482, preemption_count=0, score=492.636319, test/accuracy=0.983221, test/loss=0.063718, test/mean_average_precision=0.068714, test/num_examples=43793, total_duration=815.275460, train/accuracy=0.987071, train/loss=0.049404, train/mean_average_precision=0.072728, validation/accuracy=0.984228, validation/loss=0.060129, validation/mean_average_precision=0.069543, validation/num_examples=43793
I0210 00:17:37.823059 139836634785536 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.009242539294064045, loss=0.044253215193748474
I0210 00:18:10.660482 139818024294144 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.007148185279220343, loss=0.048257648944854736
I0210 00:18:42.585767 139836634785536 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.023729117587208748, loss=0.05428504943847656
I0210 00:19:14.418210 139818024294144 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.016824202612042427, loss=0.052102573215961456
I0210 00:19:46.216842 139836634785536 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.009198830462992191, loss=0.04972274228930473
I0210 00:20:18.162719 139818024294144 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.00711748655885458, loss=0.044314391911029816
I0210 00:20:49.919084 139836634785536 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.032370734959840775, loss=0.045743439346551895
I0210 00:21:22.205854 139818024294144 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.010567335411906242, loss=0.04644661769270897
I0210 00:21:31.532450 140039251117888 spec.py:321] Evaluating on the training split.
I0210 00:23:14.449766 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 00:23:17.485740 140039251117888 spec.py:349] Evaluating on the test split.
I0210 00:23:20.492832 140039251117888 submission_runner.py:408] Time since start: 1164.48s, 	Step: 2230, 	{'train/accuracy': 0.9873882532119751, 'train/loss': 0.046789973974227905, 'train/mean_average_precision': 0.10480282761530568, 'validation/accuracy': 0.9846578240394592, 'validation/loss': 0.05671948939561844, 'validation/mean_average_precision': 0.09975522593130466, 'validation/num_examples': 43793, 'test/accuracy': 0.9836769700050354, 'test/loss': 0.06019135192036629, 'test/mean_average_precision': 0.10440029994939186, 'test/num_examples': 43793, 'score': 732.8385767936707, 'total_duration': 1164.4848325252533, 'accumulated_submission_time': 732.8385767936707, 'accumulated_eval_time': 431.51141595840454, 'accumulated_logging_time': 0.07473325729370117}
I0210 00:23:20.509410 139818172847872 logging_writer.py:48] [2230] accumulated_eval_time=431.511416, accumulated_logging_time=0.074733, accumulated_submission_time=732.838577, global_step=2230, preemption_count=0, score=732.838577, test/accuracy=0.983677, test/loss=0.060191, test/mean_average_precision=0.104400, test/num_examples=43793, total_duration=1164.484833, train/accuracy=0.987388, train/loss=0.046790, train/mean_average_precision=0.104803, validation/accuracy=0.984658, validation/loss=0.056719, validation/mean_average_precision=0.099755, validation/num_examples=43793
I0210 00:23:43.164376 139836244932352 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.014233860187232494, loss=0.04302118718624115
I0210 00:24:15.003314 139818172847872 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.024158308282494545, loss=0.04633748531341553
I0210 00:24:46.820975 139836244932352 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.018543805927038193, loss=0.04177364706993103
I0210 00:25:18.824632 139818172847872 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.01631476916372776, loss=0.0479913204908371
I0210 00:25:50.885912 139836244932352 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.012092511169612408, loss=0.04101678356528282
I0210 00:26:23.655300 139818172847872 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.035713717341423035, loss=0.04301200062036514
I0210 00:26:55.810226 139836244932352 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.020647479221224785, loss=0.039639998227357864
I0210 00:27:20.512897 140039251117888 spec.py:321] Evaluating on the training split.
I0210 00:28:59.291590 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 00:29:02.503122 140039251117888 spec.py:349] Evaluating on the test split.
I0210 00:29:05.509588 140039251117888 submission_runner.py:408] Time since start: 1509.50s, 	Step: 2978, 	{'train/accuracy': 0.9875863790512085, 'train/loss': 0.04446644335985184, 'train/mean_average_precision': 0.13305032045933401, 'validation/accuracy': 0.9849082827568054, 'validation/loss': 0.0534515306353569, 'validation/mean_average_precision': 0.12068096396737814, 'validation/num_examples': 43793, 'test/accuracy': 0.9839507341384888, 'test/loss': 0.0563553124666214, 'test/mean_average_precision': 0.1253405511373911, 'test/num_examples': 43793, 'score': 972.8108472824097, 'total_duration': 1509.5015892982483, 'accumulated_submission_time': 972.8108472824097, 'accumulated_eval_time': 536.5080606937408, 'accumulated_logging_time': 0.1023402214050293}
I0210 00:29:05.525269 139818024294144 logging_writer.py:48] [2978] accumulated_eval_time=536.508061, accumulated_logging_time=0.102340, accumulated_submission_time=972.810847, global_step=2978, preemption_count=0, score=972.810847, test/accuracy=0.983951, test/loss=0.056355, test/mean_average_precision=0.125341, test/num_examples=43793, total_duration=1509.501589, train/accuracy=0.987586, train/loss=0.044466, train/mean_average_precision=0.133050, validation/accuracy=0.984908, validation/loss=0.053452, validation/mean_average_precision=0.120681, validation/num_examples=43793
I0210 00:29:13.009424 139818032686848 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.01283926423639059, loss=0.04646347835659981
I0210 00:29:45.159367 139818024294144 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.026605963706970215, loss=0.0467059500515461
I0210 00:30:17.013778 139818032686848 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.03709403797984123, loss=0.04139748960733414
I0210 00:30:48.967184 139818024294144 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.03037858195602894, loss=0.04184914380311966
I0210 00:31:20.981213 139818032686848 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.017037283629179, loss=0.043346721678972244
I0210 00:31:53.240894 139818024294144 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.01687708869576454, loss=0.043510131537914276
I0210 00:32:25.246578 139818032686848 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.04171627014875412, loss=0.041228003799915314
I0210 00:32:56.906126 139818024294144 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.03908228874206543, loss=0.044777121394872665
I0210 00:33:05.635140 140039251117888 spec.py:321] Evaluating on the training split.
I0210 00:34:48.903940 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 00:34:51.961989 140039251117888 spec.py:349] Evaluating on the test split.
I0210 00:34:55.024433 140039251117888 submission_runner.py:408] Time since start: 1859.02s, 	Step: 3728, 	{'train/accuracy': 0.9877572655677795, 'train/loss': 0.04308393970131874, 'train/mean_average_precision': 0.1463087512056293, 'validation/accuracy': 0.9850422739982605, 'validation/loss': 0.05267150700092316, 'validation/mean_average_precision': 0.12456385688350158, 'validation/num_examples': 43793, 'test/accuracy': 0.9840299487113953, 'test/loss': 0.05582546442747116, 'test/mean_average_precision': 0.12580866930699636, 'test/num_examples': 43793, 'score': 1212.889556646347, 'total_duration': 1859.0164363384247, 'accumulated_submission_time': 1212.889556646347, 'accumulated_eval_time': 645.8973081111908, 'accumulated_logging_time': 0.1297438144683838}
I0210 00:34:55.040386 139836244932352 logging_writer.py:48] [3728] accumulated_eval_time=645.897308, accumulated_logging_time=0.129744, accumulated_submission_time=1212.889557, global_step=3728, preemption_count=0, score=1212.889557, test/accuracy=0.984030, test/loss=0.055825, test/mean_average_precision=0.125809, test/num_examples=43793, total_duration=1859.016436, train/accuracy=0.987757, train/loss=0.043084, train/mean_average_precision=0.146309, validation/accuracy=0.985042, validation/loss=0.052672, validation/mean_average_precision=0.124564, validation/num_examples=43793
I0210 00:35:18.520335 139836634785536 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.03534480556845665, loss=0.042486581951379776
I0210 00:35:50.233274 139836244932352 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.023014923557639122, loss=0.04487967491149902
I0210 00:36:22.036189 139836634785536 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.033737506717443466, loss=0.046485356986522675
I0210 00:36:53.763773 139836244932352 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.03959594666957855, loss=0.04026448354125023
I0210 00:37:25.803576 139836634785536 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.019132765009999275, loss=0.04660290479660034
I0210 00:37:57.701900 139836244932352 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.031147785484790802, loss=0.045581232756376266
I0210 00:38:29.590143 139836634785536 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.032444458454847336, loss=0.03447329252958298
I0210 00:38:55.101794 140039251117888 spec.py:321] Evaluating on the training split.
I0210 00:40:29.718687 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 00:40:32.847376 140039251117888 spec.py:349] Evaluating on the test split.
I0210 00:40:35.879408 140039251117888 submission_runner.py:408] Time since start: 2199.87s, 	Step: 4478, 	{'train/accuracy': 0.9880750179290771, 'train/loss': 0.04148411378264427, 'train/mean_average_precision': 0.1616528893787325, 'validation/accuracy': 0.985185980796814, 'validation/loss': 0.051431361585855484, 'validation/mean_average_precision': 0.144405763800987, 'validation/num_examples': 43793, 'test/accuracy': 0.9842784404754639, 'test/loss': 0.054719991981983185, 'test/mean_average_precision': 0.14370305841595316, 'test/num_examples': 43793, 'score': 1452.92014336586, 'total_duration': 2199.8714084625244, 'accumulated_submission_time': 1452.92014336586, 'accumulated_eval_time': 746.6748743057251, 'accumulated_logging_time': 0.15679001808166504}
I0210 00:40:35.896535 139818024294144 logging_writer.py:48] [4478] accumulated_eval_time=746.674874, accumulated_logging_time=0.156790, accumulated_submission_time=1452.920143, global_step=4478, preemption_count=0, score=1452.920143, test/accuracy=0.984278, test/loss=0.054720, test/mean_average_precision=0.143703, test/num_examples=43793, total_duration=2199.871408, train/accuracy=0.988075, train/loss=0.041484, train/mean_average_precision=0.161653, validation/accuracy=0.985186, validation/loss=0.051431, validation/mean_average_precision=0.144406, validation/num_examples=43793
I0210 00:40:43.394026 139818032686848 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.014154301024973392, loss=0.04260949790477753
I0210 00:41:15.789925 139818024294144 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.02903342805802822, loss=0.04905656352639198
I0210 00:41:47.923253 139818032686848 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.05667777732014656, loss=0.04572303220629692
I0210 00:42:20.575771 139818024294144 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.047811850905418396, loss=0.043670717626810074
I0210 00:42:52.756108 139818032686848 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.04736662656068802, loss=0.04052148759365082
I0210 00:43:25.126129 139818024294144 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.026765860617160797, loss=0.039941683411598206
I0210 00:43:57.196757 139818032686848 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.057699188590049744, loss=0.04222695529460907
I0210 00:44:29.282809 139818024294144 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.03399419039487839, loss=0.04291549697518349
I0210 00:44:35.998528 140039251117888 spec.py:321] Evaluating on the training split.
I0210 00:46:14.844542 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 00:46:17.882567 140039251117888 spec.py:349] Evaluating on the test split.
I0210 00:46:20.851962 140039251117888 submission_runner.py:408] Time since start: 2544.84s, 	Step: 5222, 	{'train/accuracy': 0.9881662726402283, 'train/loss': 0.04132499173283577, 'train/mean_average_precision': 0.1795573077062224, 'validation/accuracy': 0.9852378964424133, 'validation/loss': 0.050925739109516144, 'validation/mean_average_precision': 0.15389470149995021, 'validation/num_examples': 43793, 'test/accuracy': 0.9843176007270813, 'test/loss': 0.05375967174768448, 'test/mean_average_precision': 0.1524632543170833, 'test/num_examples': 43793, 'score': 1692.9912858009338, 'total_duration': 2544.843962907791, 'accumulated_submission_time': 1692.9912858009338, 'accumulated_eval_time': 851.5282611846924, 'accumulated_logging_time': 0.18505358695983887}
I0210 00:46:20.867934 139836244932352 logging_writer.py:48] [5222] accumulated_eval_time=851.528261, accumulated_logging_time=0.185054, accumulated_submission_time=1692.991286, global_step=5222, preemption_count=0, score=1692.991286, test/accuracy=0.984318, test/loss=0.053760, test/mean_average_precision=0.152463, test/num_examples=43793, total_duration=2544.843963, train/accuracy=0.988166, train/loss=0.041325, train/mean_average_precision=0.179557, validation/accuracy=0.985238, validation/loss=0.050926, validation/mean_average_precision=0.153895, validation/num_examples=43793
I0210 00:46:45.716012 139836634785536 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.03841833397746086, loss=0.04125654697418213
I0210 00:47:17.589355 139836244932352 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.031358249485492706, loss=0.04074610024690628
I0210 00:47:49.752199 139836634785536 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.024009451270103455, loss=0.043099720031023026
I0210 00:48:21.665479 139836244932352 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.0353713184595108, loss=0.03638039529323578
I0210 00:48:53.505040 139836634785536 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.026727821677923203, loss=0.03983527049422264
I0210 00:49:25.617800 139836244932352 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.022683151066303253, loss=0.03743754327297211
I0210 00:49:57.832653 139836634785536 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.034352533519268036, loss=0.040627263486385345
I0210 00:50:21.020820 140039251117888 spec.py:321] Evaluating on the training split.
I0210 00:51:58.903239 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 00:52:02.155593 140039251117888 spec.py:349] Evaluating on the test split.
I0210 00:52:05.137504 140039251117888 submission_runner.py:408] Time since start: 2889.13s, 	Step: 5973, 	{'train/accuracy': 0.9882667064666748, 'train/loss': 0.04118207469582558, 'train/mean_average_precision': 0.16472526914151106, 'validation/accuracy': 0.9852712154388428, 'validation/loss': 0.05083019286394119, 'validation/mean_average_precision': 0.1485142866215823, 'validation/num_examples': 43793, 'test/accuracy': 0.9843466877937317, 'test/loss': 0.05360308662056923, 'test/mean_average_precision': 0.15322191213047312, 'test/num_examples': 43793, 'score': 1933.113857269287, 'total_duration': 2889.129508972168, 'accumulated_submission_time': 1933.113857269287, 'accumulated_eval_time': 955.6449084281921, 'accumulated_logging_time': 0.21189594268798828}
I0210 00:52:05.154231 139818032686848 logging_writer.py:48] [5973] accumulated_eval_time=955.644908, accumulated_logging_time=0.211896, accumulated_submission_time=1933.113857, global_step=5973, preemption_count=0, score=1933.113857, test/accuracy=0.984347, test/loss=0.053603, test/mean_average_precision=0.153222, test/num_examples=43793, total_duration=2889.129509, train/accuracy=0.988267, train/loss=0.041182, train/mean_average_precision=0.164725, validation/accuracy=0.985271, validation/loss=0.050830, validation/mean_average_precision=0.148514, validation/num_examples=43793
I0210 00:52:14.325875 139878398215936 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.024012604728341103, loss=0.03979996219277382
I0210 00:52:46.108368 139818032686848 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.034868016839027405, loss=0.04132174327969551
I0210 00:53:18.138377 139878398215936 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.061948012560606, loss=0.03890381380915642
I0210 00:53:49.852673 139818032686848 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.04590758681297302, loss=0.041044533252716064
I0210 00:54:21.574519 139878398215936 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.039763059467077255, loss=0.03694747015833855
I0210 00:54:53.533358 139818032686848 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.02941843681037426, loss=0.038270480930805206
I0210 00:55:25.328032 139878398215936 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.02806645818054676, loss=0.040336403995752335
I0210 00:55:57.121859 139818032686848 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.03979123756289482, loss=0.042275525629520416
I0210 00:56:05.164555 140039251117888 spec.py:321] Evaluating on the training split.
I0210 00:57:39.839857 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 00:57:42.865547 140039251117888 spec.py:349] Evaluating on the test split.
I0210 00:57:45.896242 140039251117888 submission_runner.py:408] Time since start: 3229.89s, 	Step: 6726, 	{'train/accuracy': 0.9882901310920715, 'train/loss': 0.040386952459812164, 'train/mean_average_precision': 0.18939021860761607, 'validation/accuracy': 0.9854116439819336, 'validation/loss': 0.05021588131785393, 'validation/mean_average_precision': 0.1654686194838573, 'validation/num_examples': 43793, 'test/accuracy': 0.9844532608985901, 'test/loss': 0.053245652467012405, 'test/mean_average_precision': 0.16650507086139726, 'test/num_examples': 43793, 'score': 2173.093494415283, 'total_duration': 3229.888239145279, 'accumulated_submission_time': 2173.093494415283, 'accumulated_eval_time': 1056.376545906067, 'accumulated_logging_time': 0.2399752140045166}
I0210 00:57:45.913450 139836244932352 logging_writer.py:48] [6726] accumulated_eval_time=1056.376546, accumulated_logging_time=0.239975, accumulated_submission_time=2173.093494, global_step=6726, preemption_count=0, score=2173.093494, test/accuracy=0.984453, test/loss=0.053246, test/mean_average_precision=0.166505, test/num_examples=43793, total_duration=3229.888239, train/accuracy=0.988290, train/loss=0.040387, train/mean_average_precision=0.189390, validation/accuracy=0.985412, validation/loss=0.050216, validation/mean_average_precision=0.165469, validation/num_examples=43793
I0210 00:58:09.870622 139836634785536 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.0593956895172596, loss=0.04146978631615639
I0210 00:58:41.491195 139836244932352 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.06392613798379898, loss=0.04329357296228409
I0210 00:59:13.342652 139836634785536 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.04898189753293991, loss=0.03779841959476471
I0210 00:59:45.239661 139836244932352 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.03827676549553871, loss=0.04124220833182335
I0210 01:00:17.739513 139836634785536 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.042415618896484375, loss=0.038551099598407745
I0210 01:00:49.434523 139836244932352 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.053713250905275345, loss=0.04142221063375473
I0210 01:01:21.210608 139836634785536 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.027378078550100327, loss=0.035839349031448364
I0210 01:01:45.912160 140039251117888 spec.py:321] Evaluating on the training split.
I0210 01:03:24.514028 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 01:03:27.668716 140039251117888 spec.py:349] Evaluating on the test split.
I0210 01:03:30.735503 140039251117888 submission_runner.py:408] Time since start: 3574.73s, 	Step: 7478, 	{'train/accuracy': 0.9882825613021851, 'train/loss': 0.04071137681603432, 'train/mean_average_precision': 0.1801941639881932, 'validation/accuracy': 0.9854328036308289, 'validation/loss': 0.049915019422769547, 'validation/mean_average_precision': 0.1604554826060077, 'validation/num_examples': 43793, 'test/accuracy': 0.9845025539398193, 'test/loss': 0.05256318673491478, 'test/mean_average_precision': 0.16147580187672952, 'test/num_examples': 43793, 'score': 2413.0617480278015, 'total_duration': 3574.727502822876, 'accumulated_submission_time': 2413.0617480278015, 'accumulated_eval_time': 1161.199847459793, 'accumulated_logging_time': 0.2680511474609375}
I0210 01:03:30.752508 139878398215936 logging_writer.py:48] [7478] accumulated_eval_time=1161.199847, accumulated_logging_time=0.268051, accumulated_submission_time=2413.061748, global_step=7478, preemption_count=0, score=2413.061748, test/accuracy=0.984503, test/loss=0.052563, test/mean_average_precision=0.161476, test/num_examples=43793, total_duration=3574.727503, train/accuracy=0.988283, train/loss=0.040711, train/mean_average_precision=0.180194, validation/accuracy=0.985433, validation/loss=0.049915, validation/mean_average_precision=0.160455, validation/num_examples=43793
I0210 01:03:38.197665 139976532555520 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.041255079209804535, loss=0.041006654500961304
I0210 01:04:10.152773 139878398215936 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.03338652104139328, loss=0.03839792311191559
I0210 01:04:42.217021 139976532555520 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.05378156900405884, loss=0.04564720764756203
I0210 01:05:14.285038 139878398215936 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.05183753743767738, loss=0.04147752746939659
I0210 01:05:46.435494 139976532555520 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.05421331897377968, loss=0.03868149593472481
I0210 01:06:18.165086 139878398215936 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.04600462317466736, loss=0.042042076587677
I0210 01:06:50.265799 139976532555520 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.03210891783237457, loss=0.04505709558725357
I0210 01:07:22.601074 139878398215936 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.035772085189819336, loss=0.03923527151346207
I0210 01:07:30.866269 140039251117888 spec.py:321] Evaluating on the training split.
I0210 01:09:06.502731 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 01:09:09.605750 140039251117888 spec.py:349] Evaluating on the test split.
I0210 01:09:12.768481 140039251117888 submission_runner.py:408] Time since start: 3916.76s, 	Step: 8227, 	{'train/accuracy': 0.9884076118469238, 'train/loss': 0.04007153585553169, 'train/mean_average_precision': 0.18910998079722016, 'validation/accuracy': 0.9855748414993286, 'validation/loss': 0.04978977516293526, 'validation/mean_average_precision': 0.1705180084490086, 'validation/num_examples': 43793, 'test/accuracy': 0.9846537113189697, 'test/loss': 0.052776869386434555, 'test/mean_average_precision': 0.16179037336754432, 'test/num_examples': 43793, 'score': 2653.143779039383, 'total_duration': 3916.7604858875275, 'accumulated_submission_time': 2653.143779039383, 'accumulated_eval_time': 1263.1020185947418, 'accumulated_logging_time': 0.2975277900695801}
I0210 01:09:12.785715 139818172847872 logging_writer.py:48] [8227] accumulated_eval_time=1263.102019, accumulated_logging_time=0.297528, accumulated_submission_time=2653.143779, global_step=8227, preemption_count=0, score=2653.143779, test/accuracy=0.984654, test/loss=0.052777, test/mean_average_precision=0.161790, test/num_examples=43793, total_duration=3916.760486, train/accuracy=0.988408, train/loss=0.040072, train/mean_average_precision=0.189110, validation/accuracy=0.985575, validation/loss=0.049790, validation/mean_average_precision=0.170518, validation/num_examples=43793
I0210 01:09:36.461856 139836244932352 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.058830734342336655, loss=0.041503775864839554
I0210 01:10:08.437659 139818172847872 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.04179820418357849, loss=0.041223954409360886
I0210 01:10:40.406652 139836244932352 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.03616796433925629, loss=0.03725516051054001
I0210 01:11:12.698677 139818172847872 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.02488882653415203, loss=0.0406792126595974
I0210 01:11:44.680736 139836244932352 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.02475942298769951, loss=0.04060987010598183
I0210 01:12:16.828298 139818172847872 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.08191611617803574, loss=0.039542779326438904
I0210 01:12:49.198980 139836244932352 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.04063160717487335, loss=0.047578081488609314
I0210 01:13:12.923698 140039251117888 spec.py:321] Evaluating on the training split.
I0210 01:14:52.129787 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 01:14:55.253008 140039251117888 spec.py:349] Evaluating on the test split.
I0210 01:14:58.331256 140039251117888 submission_runner.py:408] Time since start: 4262.32s, 	Step: 8974, 	{'train/accuracy': 0.9886220097541809, 'train/loss': 0.039900485426187515, 'train/mean_average_precision': 0.1923071782334979, 'validation/accuracy': 0.9855508804321289, 'validation/loss': 0.04973052814602852, 'validation/mean_average_precision': 0.16555442965631947, 'validation/num_examples': 43793, 'test/accuracy': 0.9845378994941711, 'test/loss': 0.052378538995981216, 'test/mean_average_precision': 0.16635132712470502, 'test/num_examples': 43793, 'score': 2893.2505328655243, 'total_duration': 4262.32323884964, 'accumulated_submission_time': 2893.2505328655243, 'accumulated_eval_time': 1368.509529352188, 'accumulated_logging_time': 0.32607460021972656}
I0210 01:14:58.348204 139836634785536 logging_writer.py:48] [8974] accumulated_eval_time=1368.509529, accumulated_logging_time=0.326075, accumulated_submission_time=2893.250533, global_step=8974, preemption_count=0, score=2893.250533, test/accuracy=0.984538, test/loss=0.052379, test/mean_average_precision=0.166351, test/num_examples=43793, total_duration=4262.323239, train/accuracy=0.988622, train/loss=0.039900, train/mean_average_precision=0.192307, validation/accuracy=0.985551, validation/loss=0.049731, validation/mean_average_precision=0.165554, validation/num_examples=43793
I0210 01:15:07.129515 139976532555520 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.03848019614815712, loss=0.03980502113699913
I0210 01:15:39.092731 139836634785536 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.060324303805828094, loss=0.04251072555780411
I0210 01:16:10.852669 139976532555520 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.07636170834302902, loss=0.03615806996822357
I0210 01:16:42.718290 139836634785536 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.04932865872979164, loss=0.03924519196152687
I0210 01:17:14.438600 139976532555520 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.04500456154346466, loss=0.0365297794342041
I0210 01:17:46.427904 139836634785536 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.041040558367967606, loss=0.03354603424668312
I0210 01:18:18.650230 139976532555520 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.0670684203505516, loss=0.03962857276201248
I0210 01:18:50.214984 139836634785536 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.05933567136526108, loss=0.03904665634036064
I0210 01:18:58.339798 140039251117888 spec.py:321] Evaluating on the training split.
I0210 01:20:33.914758 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 01:20:37.027845 140039251117888 spec.py:349] Evaluating on the test split.
I0210 01:20:40.155645 140039251117888 submission_runner.py:408] Time since start: 4604.15s, 	Step: 9727, 	{'train/accuracy': 0.9885628819465637, 'train/loss': 0.039701081812381744, 'train/mean_average_precision': 0.19980620975853122, 'validation/accuracy': 0.9855829477310181, 'validation/loss': 0.050064969807863235, 'validation/mean_average_precision': 0.17189206866310625, 'validation/num_examples': 43793, 'test/accuracy': 0.9846276044845581, 'test/loss': 0.05322679132223129, 'test/mean_average_precision': 0.1667354992957371, 'test/num_examples': 43793, 'score': 3133.2117640972137, 'total_duration': 4604.147631645203, 'accumulated_submission_time': 3133.2117640972137, 'accumulated_eval_time': 1470.3253271579742, 'accumulated_logging_time': 0.35387539863586426}
I0210 01:20:40.173491 139836244932352 logging_writer.py:48] [9727] accumulated_eval_time=1470.325327, accumulated_logging_time=0.353875, accumulated_submission_time=3133.211764, global_step=9727, preemption_count=0, score=3133.211764, test/accuracy=0.984628, test/loss=0.053227, test/mean_average_precision=0.166735, test/num_examples=43793, total_duration=4604.147632, train/accuracy=0.988563, train/loss=0.039701, train/mean_average_precision=0.199806, validation/accuracy=0.985583, validation/loss=0.050065, validation/mean_average_precision=0.171892, validation/num_examples=43793
I0210 01:21:03.997624 139878398215936 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.05856826901435852, loss=0.040678419172763824
I0210 01:21:35.604459 139836244932352 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.051763467490673065, loss=0.040671225637197495
I0210 01:22:07.875317 139878398215936 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.05756034702062607, loss=0.03776232898235321
I0210 01:22:39.896579 139836244932352 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.02937096729874611, loss=0.042501892894506454
I0210 01:23:11.928259 139878398215936 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.04618500545620918, loss=0.03750495985150337
I0210 01:23:44.250181 139836244932352 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.07545597106218338, loss=0.04361525550484657
I0210 01:24:15.922613 139878398215936 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.06428291648626328, loss=0.04032352194190025
I0210 01:24:40.185794 140039251117888 spec.py:321] Evaluating on the training split.
I0210 01:26:19.803757 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 01:26:22.926128 140039251117888 spec.py:349] Evaluating on the test split.
I0210 01:26:26.009893 140039251117888 submission_runner.py:408] Time since start: 4950.00s, 	Step: 10476, 	{'train/accuracy': 0.9885133504867554, 'train/loss': 0.03942936658859253, 'train/mean_average_precision': 0.20045229589354108, 'validation/accuracy': 0.9854490160942078, 'validation/loss': 0.04966723546385765, 'validation/mean_average_precision': 0.1666263098371183, 'validation/num_examples': 43793, 'test/accuracy': 0.9844890236854553, 'test/loss': 0.052470430731773376, 'test/mean_average_precision': 0.16691585201323547, 'test/num_examples': 43793, 'score': 3373.1918189525604, 'total_duration': 4950.001894235611, 'accumulated_submission_time': 3373.1918189525604, 'accumulated_eval_time': 1576.149382352829, 'accumulated_logging_time': 0.38416481018066406}
I0210 01:26:26.026957 139818172847872 logging_writer.py:48] [10476] accumulated_eval_time=1576.149382, accumulated_logging_time=0.384165, accumulated_submission_time=3373.191819, global_step=10476, preemption_count=0, score=3373.191819, test/accuracy=0.984489, test/loss=0.052470, test/mean_average_precision=0.166916, test/num_examples=43793, total_duration=4950.001894, train/accuracy=0.988513, train/loss=0.039429, train/mean_average_precision=0.200452, validation/accuracy=0.985449, validation/loss=0.049667, validation/mean_average_precision=0.166626, validation/num_examples=43793
I0210 01:26:34.057862 139976532555520 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.025047164410352707, loss=0.03902313485741615
I0210 01:27:05.928617 139818172847872 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.09792573004961014, loss=0.04160996526479721
I0210 01:27:37.758608 139976532555520 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.044829268008470535, loss=0.03888676315546036
I0210 01:28:10.001429 139818172847872 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.05138199403882027, loss=0.03778449073433876
I0210 01:28:41.794034 139976532555520 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.05753196030855179, loss=0.04434296488761902
I0210 01:29:13.781787 139818172847872 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.030094200745224953, loss=0.04003148153424263
I0210 01:29:45.823139 139976532555520 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.050354596227407455, loss=0.038392890244722366
I0210 01:30:18.080042 139818172847872 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.03629930317401886, loss=0.041431013494729996
I0210 01:30:26.045610 140039251117888 spec.py:321] Evaluating on the training split.
I0210 01:32:03.559607 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 01:32:06.557794 140039251117888 spec.py:349] Evaluating on the test split.
I0210 01:32:09.544473 140039251117888 submission_runner.py:408] Time since start: 5293.54s, 	Step: 11226, 	{'train/accuracy': 0.9886851906776428, 'train/loss': 0.03907548263669014, 'train/mean_average_precision': 0.20898945189280574, 'validation/accuracy': 0.9855192303657532, 'validation/loss': 0.049371156841516495, 'validation/mean_average_precision': 0.17491581954717367, 'validation/num_examples': 43793, 'test/accuracy': 0.9845783710479736, 'test/loss': 0.052384376525878906, 'test/mean_average_precision': 0.1724580365021767, 'test/num_examples': 43793, 'score': 3613.1764261722565, 'total_duration': 5293.536472797394, 'accumulated_submission_time': 3613.1764261722565, 'accumulated_eval_time': 1679.6482055187225, 'accumulated_logging_time': 0.41542959213256836}
I0210 01:32:09.562010 139836634785536 logging_writer.py:48] [11226] accumulated_eval_time=1679.648206, accumulated_logging_time=0.415430, accumulated_submission_time=3613.176426, global_step=11226, preemption_count=0, score=3613.176426, test/accuracy=0.984578, test/loss=0.052384, test/mean_average_precision=0.172458, test/num_examples=43793, total_duration=5293.536473, train/accuracy=0.988685, train/loss=0.039075, train/mean_average_precision=0.208989, validation/accuracy=0.985519, validation/loss=0.049371, validation/mean_average_precision=0.174916, validation/num_examples=43793
I0210 01:32:33.463999 139878398215936 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.05148398503661156, loss=0.04219953343272209
I0210 01:33:05.542278 139836634785536 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.03123665787279606, loss=0.04234621673822403
I0210 01:33:37.134695 139878398215936 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.03691016137599945, loss=0.03854960575699806
I0210 01:34:09.332562 139836634785536 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.04011062532663345, loss=0.03775836154818535
I0210 01:34:41.805896 139878398215936 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.0318380743265152, loss=0.03631065785884857
I0210 01:35:13.895532 139836634785536 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.03407749533653259, loss=0.03773261979222298
I0210 01:35:45.538248 139878398215936 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.06423253566026688, loss=0.043850094079971313
I0210 01:36:09.646242 140039251117888 spec.py:321] Evaluating on the training split.
I0210 01:37:49.475041 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 01:37:52.636695 140039251117888 spec.py:349] Evaluating on the test split.
I0210 01:37:55.688479 140039251117888 submission_runner.py:408] Time since start: 5639.68s, 	Step: 11977, 	{'train/accuracy': 0.9885552525520325, 'train/loss': 0.039133235812187195, 'train/mean_average_precision': 0.20560365906791717, 'validation/accuracy': 0.9857209920883179, 'validation/loss': 0.04876871407032013, 'validation/mean_average_precision': 0.18172306654698742, 'validation/num_examples': 43793, 'test/accuracy': 0.9847741723060608, 'test/loss': 0.05155624821782112, 'test/mean_average_precision': 0.17483467443549086, 'test/num_examples': 43793, 'score': 3853.230010032654, 'total_duration': 5639.680479049683, 'accumulated_submission_time': 3853.230010032654, 'accumulated_eval_time': 1785.6903955936432, 'accumulated_logging_time': 0.4441530704498291}
I0210 01:37:55.706011 139813965014784 logging_writer.py:48] [11977] accumulated_eval_time=1785.690396, accumulated_logging_time=0.444153, accumulated_submission_time=3853.230010, global_step=11977, preemption_count=0, score=3853.230010, test/accuracy=0.984774, test/loss=0.051556, test/mean_average_precision=0.174835, test/num_examples=43793, total_duration=5639.680479, train/accuracy=0.988555, train/loss=0.039133, train/mean_average_precision=0.205604, validation/accuracy=0.985721, validation/loss=0.048769, validation/mean_average_precision=0.181723, validation/num_examples=43793
I0210 01:38:03.696633 139818172847872 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.07272796332836151, loss=0.03944075480103493
I0210 01:38:35.353908 139813965014784 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.027671165764331818, loss=0.039356257766485214
I0210 01:39:07.102446 139818172847872 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.059839069843292236, loss=0.037625983357429504
I0210 01:39:38.469449 139813965014784 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.033810559660196304, loss=0.03457516431808472
I0210 01:40:10.176271 139818172847872 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.043401461094617844, loss=0.036953169852495193
I0210 01:40:41.774817 139813965014784 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.028532152995467186, loss=0.03693198412656784
I0210 01:41:13.429504 139818172847872 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.045498885214328766, loss=0.03966279700398445
I0210 01:41:45.242431 139813965014784 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.03190135583281517, loss=0.03754166513681412
I0210 01:41:55.815633 140039251117888 spec.py:321] Evaluating on the training split.
I0210 01:43:36.489391 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 01:43:39.545172 140039251117888 spec.py:349] Evaluating on the test split.
I0210 01:43:42.543818 140039251117888 submission_runner.py:408] Time since start: 5986.54s, 	Step: 12734, 	{'train/accuracy': 0.9885872602462769, 'train/loss': 0.03986997902393341, 'train/mean_average_precision': 0.2044354306931992, 'validation/accuracy': 0.9855306148529053, 'validation/loss': 0.049604810774326324, 'validation/mean_average_precision': 0.16906780951342024, 'validation/num_examples': 43793, 'test/accuracy': 0.9846027493476868, 'test/loss': 0.05225344002246857, 'test/mean_average_precision': 0.1657067585304924, 'test/num_examples': 43793, 'score': 4093.308432340622, 'total_duration': 5986.535817146301, 'accumulated_submission_time': 4093.308432340622, 'accumulated_eval_time': 1892.4185304641724, 'accumulated_logging_time': 0.4730062484741211}
I0210 01:43:42.561690 139836244932352 logging_writer.py:48] [12734] accumulated_eval_time=1892.418530, accumulated_logging_time=0.473006, accumulated_submission_time=4093.308432, global_step=12734, preemption_count=0, score=4093.308432, test/accuracy=0.984603, test/loss=0.052253, test/mean_average_precision=0.165707, test/num_examples=43793, total_duration=5986.535817, train/accuracy=0.988587, train/loss=0.039870, train/mean_average_precision=0.204435, validation/accuracy=0.985531, validation/loss=0.049605, validation/mean_average_precision=0.169068, validation/num_examples=43793
I0210 01:44:04.023701 139878398215936 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.06893087923526764, loss=0.04154859483242035
I0210 01:44:35.985327 139836244932352 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.12120798975229263, loss=0.04110230877995491
I0210 01:45:07.963266 139878398215936 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.026178406551480293, loss=0.04276007413864136
I0210 01:45:39.863452 139836244932352 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.09015995264053345, loss=0.042645443230867386
I0210 01:46:11.985644 139878398215936 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.0838889554142952, loss=0.04676534980535507
I0210 01:46:43.742502 139836244932352 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.047950953245162964, loss=0.03621453046798706
I0210 01:47:15.517072 139878398215936 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.11487919092178345, loss=0.045439984649419785
I0210 01:47:42.696944 140039251117888 spec.py:321] Evaluating on the training split.
I0210 01:49:21.962075 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 01:49:25.045922 140039251117888 spec.py:349] Evaluating on the test split.
I0210 01:49:28.109732 140039251117888 submission_runner.py:408] Time since start: 6332.10s, 	Step: 13486, 	{'train/accuracy': 0.9886804819107056, 'train/loss': 0.03929399326443672, 'train/mean_average_precision': 0.20355045624948978, 'validation/accuracy': 0.9856122136116028, 'validation/loss': 0.04895460233092308, 'validation/mean_average_precision': 0.1798079085889805, 'validation/num_examples': 43793, 'test/accuracy': 0.9846764802932739, 'test/loss': 0.051828641444444656, 'test/mean_average_precision': 0.17550897646591945, 'test/num_examples': 43793, 'score': 4333.41264462471, 'total_duration': 6332.10172867775, 'accumulated_submission_time': 4333.41264462471, 'accumulated_eval_time': 1997.831268787384, 'accumulated_logging_time': 0.5020365715026855}
I0210 01:49:28.128330 139813965014784 logging_writer.py:48] [13486] accumulated_eval_time=1997.831269, accumulated_logging_time=0.502037, accumulated_submission_time=4333.412645, global_step=13486, preemption_count=0, score=4333.412645, test/accuracy=0.984676, test/loss=0.051829, test/mean_average_precision=0.175509, test/num_examples=43793, total_duration=6332.101729, train/accuracy=0.988680, train/loss=0.039294, train/mean_average_precision=0.203550, validation/accuracy=0.985612, validation/loss=0.048955, validation/mean_average_precision=0.179808, validation/num_examples=43793
I0210 01:49:33.007238 139818172847872 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.0587882474064827, loss=0.033921923488378525
I0210 01:50:04.847269 139813965014784 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.03553777188062668, loss=0.040614496916532516
I0210 01:50:36.742725 139818172847872 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.059296585619449615, loss=0.04111355543136597
I0210 01:51:08.992157 139813965014784 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.03559640794992447, loss=0.03850279748439789
I0210 01:51:41.791942 139818172847872 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.039518874138593674, loss=0.041827429085969925
I0210 01:52:14.508064 139813965014784 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.060534391552209854, loss=0.045370277017354965
I0210 01:52:46.965519 139818172847872 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.06560244411230087, loss=0.04382706433534622
I0210 01:53:19.570816 139813965014784 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.030450040474534035, loss=0.03840446099638939
I0210 01:53:28.250085 140039251117888 spec.py:321] Evaluating on the training split.
I0210 01:55:08.202762 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 01:55:11.268373 140039251117888 spec.py:349] Evaluating on the test split.
I0210 01:55:14.242004 140039251117888 submission_runner.py:408] Time since start: 6678.23s, 	Step: 14228, 	{'train/accuracy': 0.9885827898979187, 'train/loss': 0.039349667727947235, 'train/mean_average_precision': 0.1989838706543882, 'validation/accuracy': 0.9857429265975952, 'validation/loss': 0.04903091862797737, 'validation/mean_average_precision': 0.1796446625647285, 'validation/num_examples': 43793, 'test/accuracy': 0.9847055673599243, 'test/loss': 0.05213354900479317, 'test/mean_average_precision': 0.1784581841483761, 'test/num_examples': 43793, 'score': 4573.500229597092, 'total_duration': 6678.234007120132, 'accumulated_submission_time': 4573.500229597092, 'accumulated_eval_time': 2103.8231523036957, 'accumulated_logging_time': 0.5319912433624268}
I0210 01:55:14.260570 139836244932352 logging_writer.py:48] [14228] accumulated_eval_time=2103.823152, accumulated_logging_time=0.531991, accumulated_submission_time=4573.500230, global_step=14228, preemption_count=0, score=4573.500230, test/accuracy=0.984706, test/loss=0.052134, test/mean_average_precision=0.178458, test/num_examples=43793, total_duration=6678.234007, train/accuracy=0.988583, train/loss=0.039350, train/mean_average_precision=0.198984, validation/accuracy=0.985743, validation/loss=0.049031, validation/mean_average_precision=0.179645, validation/num_examples=43793
I0210 01:55:37.526488 139836634785536 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.05616613104939461, loss=0.03814681991934776
I0210 01:56:09.456487 139836244932352 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.042102739214897156, loss=0.03757431358098984
I0210 01:56:41.970796 139836634785536 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.024883542209863663, loss=0.03974426910281181
I0210 01:57:14.312465 139836244932352 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.022950557991862297, loss=0.03772144019603729
I0210 01:57:45.771526 139836634785536 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.11028017848730087, loss=0.03807817026972771
I0210 01:58:17.657657 139836244932352 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.03166399896144867, loss=0.04263913258910179
I0210 01:58:49.560232 139836634785536 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.017864257097244263, loss=0.033520475029945374
I0210 01:59:14.271189 140039251117888 spec.py:321] Evaluating on the training split.
I0210 02:00:54.274180 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 02:00:57.280433 140039251117888 spec.py:349] Evaluating on the test split.
I0210 02:01:00.261234 140039251117888 submission_runner.py:408] Time since start: 7024.25s, 	Step: 14978, 	{'train/accuracy': 0.9885103106498718, 'train/loss': 0.039425864815711975, 'train/mean_average_precision': 0.21031990673518336, 'validation/accuracy': 0.9857169389724731, 'validation/loss': 0.049071572721004486, 'validation/mean_average_precision': 0.18099567580130638, 'validation/num_examples': 43793, 'test/accuracy': 0.9847737550735474, 'test/loss': 0.051877040416002274, 'test/mean_average_precision': 0.1807104800860751, 'test/num_examples': 43793, 'score': 4813.4798884391785, 'total_duration': 7024.253228664398, 'accumulated_submission_time': 4813.4798884391785, 'accumulated_eval_time': 2209.81316947937, 'accumulated_logging_time': 0.5619616508483887}
I0210 02:01:00.281664 139818172847872 logging_writer.py:48] [14978] accumulated_eval_time=2209.813169, accumulated_logging_time=0.561962, accumulated_submission_time=4813.479888, global_step=14978, preemption_count=0, score=4813.479888, test/accuracy=0.984774, test/loss=0.051877, test/mean_average_precision=0.180710, test/num_examples=43793, total_duration=7024.253229, train/accuracy=0.988510, train/loss=0.039426, train/mean_average_precision=0.210320, validation/accuracy=0.985717, validation/loss=0.049072, validation/mean_average_precision=0.180996, validation/num_examples=43793
I0210 02:01:07.857595 139878398215936 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.05570267140865326, loss=0.040190812200307846
I0210 02:01:39.798079 139818172847872 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.06508001685142517, loss=0.03616492077708244
I0210 02:02:11.947755 139878398215936 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.045391350984573364, loss=0.03977441042661667
I0210 02:02:44.056927 139818172847872 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.03549383208155632, loss=0.042617831379175186
I0210 02:03:15.912328 139878398215936 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.03892090916633606, loss=0.04268139228224754
I0210 02:03:47.876471 139818172847872 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.10719732195138931, loss=0.03923257067799568
I0210 02:04:19.437548 139878398215936 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.05823851004242897, loss=0.035262759774923325
I0210 02:04:50.901050 139818172847872 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.0725763738155365, loss=0.03519691154360771
I0210 02:05:00.293523 140039251117888 spec.py:321] Evaluating on the training split.
I0210 02:06:34.333967 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 02:06:37.382758 140039251117888 spec.py:349] Evaluating on the test split.
I0210 02:06:40.384547 140039251117888 submission_runner.py:408] Time since start: 7364.38s, 	Step: 15730, 	{'train/accuracy': 0.9885491132736206, 'train/loss': 0.03898508846759796, 'train/mean_average_precision': 0.20316993229019112, 'validation/accuracy': 0.985655665397644, 'validation/loss': 0.0487467497587204, 'validation/mean_average_precision': 0.18462462128448548, 'validation/num_examples': 43793, 'test/accuracy': 0.9846811294555664, 'test/loss': 0.0517447367310524, 'test/mean_average_precision': 0.17993505069326723, 'test/num_examples': 43793, 'score': 5053.459381818771, 'total_duration': 7364.3765437603, 'accumulated_submission_time': 5053.459381818771, 'accumulated_eval_time': 2309.904142856598, 'accumulated_logging_time': 0.5947067737579346}
I0210 02:06:40.402879 139813965014784 logging_writer.py:48] [15730] accumulated_eval_time=2309.904143, accumulated_logging_time=0.594707, accumulated_submission_time=5053.459382, global_step=15730, preemption_count=0, score=5053.459382, test/accuracy=0.984681, test/loss=0.051745, test/mean_average_precision=0.179935, test/num_examples=43793, total_duration=7364.376544, train/accuracy=0.988549, train/loss=0.038985, train/mean_average_precision=0.203170, validation/accuracy=0.985656, validation/loss=0.048747, validation/mean_average_precision=0.184625, validation/num_examples=43793
I0210 02:07:03.092911 139836634785536 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.04569355025887489, loss=0.0384502075612545
I0210 02:07:34.841463 139813965014784 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.05549515411257744, loss=0.04210248962044716
I0210 02:08:06.399334 139836634785536 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.05742696300148964, loss=0.03825145959854126
I0210 02:08:38.095315 139813965014784 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.05845985561609268, loss=0.03777368366718292
I0210 02:09:10.338726 139836634785536 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.054313257336616516, loss=0.03944110870361328
I0210 02:09:42.861867 139813965014784 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.033372122794389725, loss=0.03446372598409653
I0210 02:10:15.034487 139836634785536 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.04156610742211342, loss=0.04030020534992218
I0210 02:10:40.665127 140039251117888 spec.py:321] Evaluating on the training split.
I0210 02:12:18.328414 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 02:12:21.363043 140039251117888 spec.py:349] Evaluating on the test split.
I0210 02:12:24.330806 140039251117888 submission_runner.py:408] Time since start: 7708.32s, 	Step: 16481, 	{'train/accuracy': 0.9886710047721863, 'train/loss': 0.03975606709718704, 'train/mean_average_precision': 0.20017878745560774, 'validation/accuracy': 0.9856000542640686, 'validation/loss': 0.04901471734046936, 'validation/mean_average_precision': 0.17581403247664895, 'validation/num_examples': 43793, 'test/accuracy': 0.9846815466880798, 'test/loss': 0.05170534551143646, 'test/mean_average_precision': 0.16965997870898855, 'test/num_examples': 43793, 'score': 5293.689656734467, 'total_duration': 7708.322807788849, 'accumulated_submission_time': 5293.689656734467, 'accumulated_eval_time': 2413.569778442383, 'accumulated_logging_time': 0.625511884689331}
I0210 02:12:24.349395 139836244932352 logging_writer.py:48] [16481] accumulated_eval_time=2413.569778, accumulated_logging_time=0.625512, accumulated_submission_time=5293.689657, global_step=16481, preemption_count=0, score=5293.689657, test/accuracy=0.984682, test/loss=0.051705, test/mean_average_precision=0.169660, test/num_examples=43793, total_duration=7708.322808, train/accuracy=0.988671, train/loss=0.039756, train/mean_average_precision=0.200179, validation/accuracy=0.985600, validation/loss=0.049015, validation/mean_average_precision=0.175814, validation/num_examples=43793
I0210 02:12:30.786082 139878398215936 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.04123838245868683, loss=0.04189467057585716
I0210 02:13:02.829451 139836244932352 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.03946070000529289, loss=0.03889545798301697
I0210 02:13:34.585636 139878398215936 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.07447490841150284, loss=0.0383116789162159
I0210 02:14:06.542715 139836244932352 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.035345908254384995, loss=0.03769664466381073
I0210 02:14:38.492568 139878398215936 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.060322459787130356, loss=0.038804419338703156
I0210 02:15:10.237845 139836244932352 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.05092649161815643, loss=0.039158422499895096
I0210 02:15:41.873818 139878398215936 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.04532459378242493, loss=0.03874150663614273
I0210 02:16:13.630925 139836244932352 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.09818106889724731, loss=0.036984238773584366
I0210 02:16:24.341971 140039251117888 spec.py:321] Evaluating on the training split.
I0210 02:18:01.008527 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 02:18:04.141650 140039251117888 spec.py:349] Evaluating on the test split.
I0210 02:18:07.128587 140039251117888 submission_runner.py:408] Time since start: 8051.12s, 	Step: 17235, 	{'train/accuracy': 0.988852858543396, 'train/loss': 0.03818158060312271, 'train/mean_average_precision': 0.22472311091833175, 'validation/accuracy': 0.9858115315437317, 'validation/loss': 0.04819267988204956, 'validation/mean_average_precision': 0.19142163577378093, 'validation/num_examples': 43793, 'test/accuracy': 0.9848251342773438, 'test/loss': 0.051109109073877335, 'test/mean_average_precision': 0.17861484881648326, 'test/num_examples': 43793, 'score': 5533.651293992996, 'total_duration': 8051.120588302612, 'accumulated_submission_time': 5533.651293992996, 'accumulated_eval_time': 2516.3563482761383, 'accumulated_logging_time': 0.6553435325622559}
I0210 02:18:07.147644 139818172847872 logging_writer.py:48] [17235] accumulated_eval_time=2516.356348, accumulated_logging_time=0.655344, accumulated_submission_time=5533.651294, global_step=17235, preemption_count=0, score=5533.651294, test/accuracy=0.984825, test/loss=0.051109, test/mean_average_precision=0.178615, test/num_examples=43793, total_duration=8051.120588, train/accuracy=0.988853, train/loss=0.038182, train/mean_average_precision=0.224723, validation/accuracy=0.985812, validation/loss=0.048193, validation/mean_average_precision=0.191422, validation/num_examples=43793
I0210 02:18:28.343387 139836634785536 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.06415589898824692, loss=0.03681720420718193
I0210 02:19:00.114772 139818172847872 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.031999897211790085, loss=0.037816595286130905
I0210 02:19:31.999518 139836634785536 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.059888049960136414, loss=0.0423508882522583
I0210 02:20:04.024102 139818172847872 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.02998463809490204, loss=0.036229364573955536
I0210 02:20:35.992083 139836634785536 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.056396421045064926, loss=0.03789617493748665
I0210 02:21:07.861581 139818172847872 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.11295424401760101, loss=0.04268847405910492
I0210 02:21:39.575750 139836634785536 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.08373367041349411, loss=0.03628962114453316
I0210 02:22:07.134607 140039251117888 spec.py:321] Evaluating on the training split.
I0210 02:23:43.846933 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 02:23:47.251926 140039251117888 spec.py:349] Evaluating on the test split.
I0210 02:23:50.642228 140039251117888 submission_runner.py:408] Time since start: 8394.63s, 	Step: 17987, 	{'train/accuracy': 0.9886994361877441, 'train/loss': 0.03855183348059654, 'train/mean_average_precision': 0.21943499417505946, 'validation/accuracy': 0.9855996370315552, 'validation/loss': 0.04912377893924713, 'validation/mean_average_precision': 0.17756952417811328, 'validation/num_examples': 43793, 'test/accuracy': 0.9846655130386353, 'test/loss': 0.05189542844891548, 'test/mean_average_precision': 0.17316708913025225, 'test/num_examples': 43793, 'score': 5773.6067237854, 'total_duration': 8394.634209156036, 'accumulated_submission_time': 5773.6067237854, 'accumulated_eval_time': 2619.8639080524445, 'accumulated_logging_time': 0.6865098476409912}
I0210 02:23:50.665214 139813965014784 logging_writer.py:48] [17987] accumulated_eval_time=2619.863908, accumulated_logging_time=0.686510, accumulated_submission_time=5773.606724, global_step=17987, preemption_count=0, score=5773.606724, test/accuracy=0.984666, test/loss=0.051895, test/mean_average_precision=0.173167, test/num_examples=43793, total_duration=8394.634209, train/accuracy=0.988699, train/loss=0.038552, train/mean_average_precision=0.219435, validation/accuracy=0.985600, validation/loss=0.049124, validation/mean_average_precision=0.177570, validation/num_examples=43793
I0210 02:23:55.279523 139878398215936 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.07967405766248703, loss=0.04426141828298569
I0210 02:24:27.743201 139813965014784 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.04836241528391838, loss=0.03585474193096161
I0210 02:24:59.842604 139878398215936 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.036169711500406265, loss=0.037573568522930145
I0210 02:25:32.253762 139813965014784 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.03072824329137802, loss=0.042169906198978424
I0210 02:26:04.723287 139878398215936 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.07760944962501526, loss=0.039317674934864044
I0210 02:26:36.738353 139813965014784 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.05855034291744232, loss=0.040212132036685944
I0210 02:27:09.215912 139878398215936 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.03635082021355629, loss=0.03926687687635422
I0210 02:27:41.916673 139813965014784 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.08316465467214584, loss=0.03952804580330849
I0210 02:27:50.866663 140039251117888 spec.py:321] Evaluating on the training split.
I0210 02:29:33.086918 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 02:29:36.114480 140039251117888 spec.py:349] Evaluating on the test split.
I0210 02:29:39.128338 140039251117888 submission_runner.py:408] Time since start: 8743.12s, 	Step: 18728, 	{'train/accuracy': 0.9887787699699402, 'train/loss': 0.03834012150764465, 'train/mean_average_precision': 0.22422281877283778, 'validation/accuracy': 0.9857478141784668, 'validation/loss': 0.04909900575876236, 'validation/mean_average_precision': 0.18295043848358236, 'validation/num_examples': 43793, 'test/accuracy': 0.9847704172134399, 'test/loss': 0.0521637499332428, 'test/mean_average_precision': 0.17599197125757965, 'test/num_examples': 43793, 'score': 6013.775787115097, 'total_duration': 8743.120332956314, 'accumulated_submission_time': 6013.775787115097, 'accumulated_eval_time': 2728.1255388259888, 'accumulated_logging_time': 0.7215981483459473}
I0210 02:29:39.148262 139818172847872 logging_writer.py:48] [18728] accumulated_eval_time=2728.125539, accumulated_logging_time=0.721598, accumulated_submission_time=6013.775787, global_step=18728, preemption_count=0, score=6013.775787, test/accuracy=0.984770, test/loss=0.052164, test/mean_average_precision=0.175992, test/num_examples=43793, total_duration=8743.120333, train/accuracy=0.988779, train/loss=0.038340, train/mean_average_precision=0.224223, validation/accuracy=0.985748, validation/loss=0.049099, validation/mean_average_precision=0.182950, validation/num_examples=43793
I0210 02:30:02.680168 139836244932352 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.05645905062556267, loss=0.039435792714357376
I0210 02:30:34.721638 139818172847872 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.040560122579336166, loss=0.040703434497117996
I0210 02:31:06.653255 139836244932352 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.04150032252073288, loss=0.04278632253408432
I0210 02:31:38.452438 139818172847872 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.030191244557499886, loss=0.03886302560567856
I0210 02:32:10.254668 139836244932352 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.048404864966869354, loss=0.04086778312921524
I0210 02:32:42.141463 139818172847872 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.05506768450140953, loss=0.041848666965961456
I0210 02:33:14.073956 139836244932352 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.036438919603824615, loss=0.04070465266704559
I0210 02:33:39.365368 140039251117888 spec.py:321] Evaluating on the training split.
I0210 02:35:17.842634 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 02:35:20.917826 140039251117888 spec.py:349] Evaluating on the test split.
I0210 02:35:23.919133 140039251117888 submission_runner.py:408] Time since start: 9087.91s, 	Step: 19480, 	{'train/accuracy': 0.988776445388794, 'train/loss': 0.03847422078251839, 'train/mean_average_precision': 0.22298679010206465, 'validation/accuracy': 0.9857891798019409, 'validation/loss': 0.04893574118614197, 'validation/mean_average_precision': 0.18490211072678567, 'validation/num_examples': 43793, 'test/accuracy': 0.9848179817199707, 'test/loss': 0.05209505558013916, 'test/mean_average_precision': 0.1765644162673381, 'test/num_examples': 43793, 'score': 6253.961903810501, 'total_duration': 9087.911134004593, 'accumulated_submission_time': 6253.961903810501, 'accumulated_eval_time': 2832.679259777069, 'accumulated_logging_time': 0.7523925304412842}
I0210 02:35:23.938514 139836634785536 logging_writer.py:48] [19480] accumulated_eval_time=2832.679260, accumulated_logging_time=0.752393, accumulated_submission_time=6253.961904, global_step=19480, preemption_count=0, score=6253.961904, test/accuracy=0.984818, test/loss=0.052095, test/mean_average_precision=0.176564, test/num_examples=43793, total_duration=9087.911134, train/accuracy=0.988776, train/loss=0.038474, train/mean_average_precision=0.222987, validation/accuracy=0.985789, validation/loss=0.048936, validation/mean_average_precision=0.184902, validation/num_examples=43793
I0210 02:35:30.859536 139878398215936 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.048332761973142624, loss=0.03826676681637764
I0210 02:36:03.739703 139836634785536 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.03154533728957176, loss=0.037920862436294556
I0210 02:36:35.445711 139878398215936 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.04158569499850273, loss=0.038183920085430145
I0210 02:37:07.612700 139836634785536 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.08938062936067581, loss=0.039688438177108765
I0210 02:37:39.803816 139878398215936 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.03320637345314026, loss=0.0352233350276947
I0210 02:38:11.917156 139836634785536 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.035724442452192307, loss=0.040561068803071976
I0210 02:38:44.127985 139878398215936 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.04819732904434204, loss=0.039753254503011703
I0210 02:39:15.923777 139836634785536 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.06569825857877731, loss=0.037983257323503494
I0210 02:39:24.030290 140039251117888 spec.py:321] Evaluating on the training split.
I0210 02:41:00.236792 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 02:41:03.474879 140039251117888 spec.py:349] Evaluating on the test split.
I0210 02:41:06.481742 140039251117888 submission_runner.py:408] Time since start: 9430.47s, 	Step: 20226, 	{'train/accuracy': 0.9885373115539551, 'train/loss': 0.03903643786907196, 'train/mean_average_precision': 0.21376707881150128, 'validation/accuracy': 0.9857355952262878, 'validation/loss': 0.04907619208097458, 'validation/mean_average_precision': 0.1825315784667147, 'validation/num_examples': 43793, 'test/accuracy': 0.9848251342773438, 'test/loss': 0.052028656005859375, 'test/mean_average_precision': 0.17495912351054604, 'test/num_examples': 43793, 'score': 6494.021353006363, 'total_duration': 9430.473743200302, 'accumulated_submission_time': 6494.021353006363, 'accumulated_eval_time': 2935.130667924881, 'accumulated_logging_time': 0.7841694355010986}
I0210 02:41:06.500981 139813965014784 logging_writer.py:48] [20226] accumulated_eval_time=2935.130668, accumulated_logging_time=0.784169, accumulated_submission_time=6494.021353, global_step=20226, preemption_count=0, score=6494.021353, test/accuracy=0.984825, test/loss=0.052029, test/mean_average_precision=0.174959, test/num_examples=43793, total_duration=9430.473743, train/accuracy=0.988537, train/loss=0.039036, train/mean_average_precision=0.213767, validation/accuracy=0.985736, validation/loss=0.049076, validation/mean_average_precision=0.182532, validation/num_examples=43793
I0210 02:41:30.588375 139836244932352 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.08346763253211975, loss=0.041366010904312134
I0210 02:42:02.765832 139813965014784 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.029535558074712753, loss=0.03554638475179672
I0210 02:42:34.719422 139836244932352 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.052366290241479874, loss=0.040690768510103226
I0210 02:43:06.699966 139813965014784 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.0419299341738224, loss=0.04260756075382233
I0210 02:43:38.552504 139836244932352 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.05453220009803772, loss=0.03375735506415367
I0210 02:44:10.331932 139813965014784 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.022361673414707184, loss=0.03471783548593521
I0210 02:44:41.730876 139836244932352 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.03593679144978523, loss=0.04121338948607445
I0210 02:45:06.798170 140039251117888 spec.py:321] Evaluating on the training split.
I0210 02:46:46.695242 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 02:46:49.716675 140039251117888 spec.py:349] Evaluating on the test split.
I0210 02:46:52.801319 140039251117888 submission_runner.py:408] Time since start: 9776.79s, 	Step: 20980, 	{'train/accuracy': 0.988884449005127, 'train/loss': 0.03870683163404465, 'train/mean_average_precision': 0.21985198387675192, 'validation/accuracy': 0.9858046174049377, 'validation/loss': 0.04974351450800896, 'validation/mean_average_precision': 0.18310630590458274, 'validation/num_examples': 43793, 'test/accuracy': 0.984847903251648, 'test/loss': 0.05284995585680008, 'test/mean_average_precision': 0.17790897236115488, 'test/num_examples': 43793, 'score': 6734.287105321884, 'total_duration': 9776.793322563171, 'accumulated_submission_time': 6734.287105321884, 'accumulated_eval_time': 3041.1337745189667, 'accumulated_logging_time': 0.8143470287322998}
I0210 02:46:52.821135 139818172847872 logging_writer.py:48] [20980] accumulated_eval_time=3041.133775, accumulated_logging_time=0.814347, accumulated_submission_time=6734.287105, global_step=20980, preemption_count=0, score=6734.287105, test/accuracy=0.984848, test/loss=0.052850, test/mean_average_precision=0.177909, test/num_examples=43793, total_duration=9776.793323, train/accuracy=0.988884, train/loss=0.038707, train/mean_average_precision=0.219852, validation/accuracy=0.985805, validation/loss=0.049744, validation/mean_average_precision=0.183106, validation/num_examples=43793
I0210 02:46:59.540085 139878398215936 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.047317203134298325, loss=0.036166802048683167
I0210 02:47:31.885748 139818172847872 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.04306242614984512, loss=0.03684864938259125
I0210 02:48:04.150076 139878398215936 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.056007884442806244, loss=0.03657842427492142
I0210 02:48:36.403454 139818172847872 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.06821819394826889, loss=0.0389055535197258
I0210 02:49:08.853859 139878398215936 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.04045158997178078, loss=0.03744456544518471
I0210 02:49:41.114543 139818172847872 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.041514717042446136, loss=0.034753646701574326
I0210 02:50:13.123304 139878398215936 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.042948104441165924, loss=0.0388166680932045
I0210 02:50:45.076314 139818172847872 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.058646176010370255, loss=0.037503696978092194
I0210 02:50:53.133780 140039251117888 spec.py:321] Evaluating on the training split.
I0210 02:52:35.689960 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 02:52:38.899403 140039251117888 spec.py:349] Evaluating on the test split.
I0210 02:52:42.008412 140039251117888 submission_runner.py:408] Time since start: 10126.00s, 	Step: 21726, 	{'train/accuracy': 0.9888972640037537, 'train/loss': 0.038502927869558334, 'train/mean_average_precision': 0.20883904450703217, 'validation/accuracy': 0.9857494235038757, 'validation/loss': 0.04828091710805893, 'validation/mean_average_precision': 0.1844016397267828, 'validation/num_examples': 43793, 'test/accuracy': 0.9848167300224304, 'test/loss': 0.05139738321304321, 'test/mean_average_precision': 0.17765463798209252, 'test/num_examples': 43793, 'score': 6974.567994594574, 'total_duration': 10126.000288248062, 'accumulated_submission_time': 6974.567994594574, 'accumulated_eval_time': 3150.0082376003265, 'accumulated_logging_time': 0.8462753295898438}
I0210 02:52:42.028384 139836244932352 logging_writer.py:48] [21726] accumulated_eval_time=3150.008238, accumulated_logging_time=0.846275, accumulated_submission_time=6974.567995, global_step=21726, preemption_count=0, score=6974.567995, test/accuracy=0.984817, test/loss=0.051397, test/mean_average_precision=0.177655, test/num_examples=43793, total_duration=10126.000288, train/accuracy=0.988897, train/loss=0.038503, train/mean_average_precision=0.208839, validation/accuracy=0.985749, validation/loss=0.048281, validation/mean_average_precision=0.184402, validation/num_examples=43793
I0210 02:53:06.724990 139836634785536 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.025442464277148247, loss=0.034038178622722626
I0210 02:53:38.716635 139836244932352 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.026397012174129486, loss=0.0431068055331707
I0210 02:54:10.513489 139836634785536 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.030370764434337616, loss=0.037935324013233185
I0210 02:54:42.309251 139836244932352 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.05213334411382675, loss=0.03960629552602768
I0210 02:55:14.239504 139836634785536 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.04877275601029396, loss=0.0357709638774395
I0210 02:55:46.003743 139836244932352 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.06783106178045273, loss=0.040805917233228683
I0210 02:56:17.840217 139836634785536 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.04070612043142319, loss=0.0337182842195034
I0210 02:56:42.119547 140039251117888 spec.py:321] Evaluating on the training split.
I0210 02:58:19.264187 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 02:58:22.453156 140039251117888 spec.py:349] Evaluating on the test split.
I0210 02:58:25.525885 140039251117888 submission_runner.py:408] Time since start: 10469.52s, 	Step: 22477, 	{'train/accuracy': 0.9887509942054749, 'train/loss': 0.03849947452545166, 'train/mean_average_precision': 0.21631777584403558, 'validation/accuracy': 0.9858188033103943, 'validation/loss': 0.04855337366461754, 'validation/mean_average_precision': 0.19513874586251742, 'validation/num_examples': 43793, 'test/accuracy': 0.984839916229248, 'test/loss': 0.051818352192640305, 'test/mean_average_precision': 0.18568779328349783, 'test/num_examples': 43793, 'score': 7214.62762594223, 'total_duration': 10469.517782449722, 'accumulated_submission_time': 7214.62762594223, 'accumulated_eval_time': 3253.414433002472, 'accumulated_logging_time': 0.877194881439209}
I0210 02:58:25.545500 139813965014784 logging_writer.py:48] [22477] accumulated_eval_time=3253.414433, accumulated_logging_time=0.877195, accumulated_submission_time=7214.627626, global_step=22477, preemption_count=0, score=7214.627626, test/accuracy=0.984840, test/loss=0.051818, test/mean_average_precision=0.185688, test/num_examples=43793, total_duration=10469.517782, train/accuracy=0.988751, train/loss=0.038499, train/mean_average_precision=0.216318, validation/accuracy=0.985819, validation/loss=0.048553, validation/mean_average_precision=0.195139, validation/num_examples=43793
I0210 02:58:33.244582 139878398215936 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.027977388352155685, loss=0.033541299402713776
I0210 02:59:05.315685 139813965014784 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.03478396683931351, loss=0.039947304874658585
I0210 02:59:37.250995 139878398215936 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.03450495004653931, loss=0.03372092545032501
I0210 03:00:09.537328 139813965014784 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.04854704439640045, loss=0.03549065440893173
I0210 03:00:41.671290 139878398215936 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.0492875911295414, loss=0.03825843706727028
I0210 03:01:14.096896 139813965014784 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.04534713178873062, loss=0.04002164676785469
I0210 03:01:46.178615 139878398215936 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.03944411128759384, loss=0.03307299688458443
I0210 03:02:18.574508 139813965014784 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.03001650609076023, loss=0.036262448877096176
I0210 03:02:25.619993 140039251117888 spec.py:321] Evaluating on the training split.
I0210 03:04:07.881478 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 03:04:11.022732 140039251117888 spec.py:349] Evaluating on the test split.
I0210 03:04:14.209762 140039251117888 submission_runner.py:408] Time since start: 10818.20s, 	Step: 23223, 	{'train/accuracy': 0.9887223243713379, 'train/loss': 0.03865669295191765, 'train/mean_average_precision': 0.21402641052660315, 'validation/accuracy': 0.9857449531555176, 'validation/loss': 0.04908564314246178, 'validation/mean_average_precision': 0.18002393601332625, 'validation/num_examples': 43793, 'test/accuracy': 0.9848597049713135, 'test/loss': 0.05204010382294655, 'test/mean_average_precision': 0.17371921432040302, 'test/num_examples': 43793, 'score': 7454.670845508575, 'total_duration': 10818.201761245728, 'accumulated_submission_time': 7454.670845508575, 'accumulated_eval_time': 3362.0041534900665, 'accumulated_logging_time': 0.9081747531890869}
I0210 03:04:14.230273 139818172847872 logging_writer.py:48] [23223] accumulated_eval_time=3362.004153, accumulated_logging_time=0.908175, accumulated_submission_time=7454.670846, global_step=23223, preemption_count=0, score=7454.670846, test/accuracy=0.984860, test/loss=0.052040, test/mean_average_precision=0.173719, test/num_examples=43793, total_duration=10818.201761, train/accuracy=0.988722, train/loss=0.038657, train/mean_average_precision=0.214026, validation/accuracy=0.985745, validation/loss=0.049086, validation/mean_average_precision=0.180024, validation/num_examples=43793
I0210 03:04:39.556565 139836634785536 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.04108750820159912, loss=0.044560790061950684
I0210 03:05:12.362988 139818172847872 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.07010898739099503, loss=0.04069382697343826
I0210 03:05:44.821047 139836634785536 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.053426649421453476, loss=0.037468355149030685
I0210 03:06:17.358724 139818172847872 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.040098126977682114, loss=0.036609847098588943
I0210 03:06:49.679526 139836634785536 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.06171266362071037, loss=0.034933969378471375
I0210 03:07:22.336006 139818172847872 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.032357484102249146, loss=0.04397473856806755
I0210 03:07:54.650437 139836634785536 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.028340328484773636, loss=0.0388161726295948
I0210 03:08:14.362582 140039251117888 spec.py:321] Evaluating on the training split.
I0210 03:09:52.129434 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 03:09:55.372039 140039251117888 spec.py:349] Evaluating on the test split.
I0210 03:09:58.495455 140039251117888 submission_runner.py:408] Time since start: 11162.49s, 	Step: 23962, 	{'train/accuracy': 0.988548755645752, 'train/loss': 0.03936893865466118, 'train/mean_average_precision': 0.2090295690843694, 'validation/accuracy': 0.9855188131332397, 'validation/loss': 0.04907645285129547, 'validation/mean_average_precision': 0.17879016202597145, 'validation/num_examples': 43793, 'test/accuracy': 0.9845973253250122, 'test/loss': 0.05218444764614105, 'test/mean_average_precision': 0.1705859883723574, 'test/num_examples': 43793, 'score': 7694.770886421204, 'total_duration': 11162.487454891205, 'accumulated_submission_time': 7694.770886421204, 'accumulated_eval_time': 3466.1369805336, 'accumulated_logging_time': 0.9395201206207275}
I0210 03:09:58.515894 139836244932352 logging_writer.py:48] [23962] accumulated_eval_time=3466.136981, accumulated_logging_time=0.939520, accumulated_submission_time=7694.770886, global_step=23962, preemption_count=0, score=7694.770886, test/accuracy=0.984597, test/loss=0.052184, test/mean_average_precision=0.170586, test/num_examples=43793, total_duration=11162.487455, train/accuracy=0.988549, train/loss=0.039369, train/mean_average_precision=0.209030, validation/accuracy=0.985519, validation/loss=0.049076, validation/mean_average_precision=0.178790, validation/num_examples=43793
I0210 03:10:11.390140 139878398215936 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.041614800691604614, loss=0.039972517639398575
I0210 03:10:43.575291 139836244932352 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.0347018726170063, loss=0.03423798456788063
I0210 03:11:15.814311 139878398215936 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.06448648869991302, loss=0.03595832735300064
I0210 03:11:47.891258 139836244932352 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.034610405564308167, loss=0.037929240614175797
I0210 03:12:20.254977 139878398215936 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.036781176924705505, loss=0.04112434759736061
I0210 03:12:52.189700 139836244932352 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.06339631974697113, loss=0.039747271686792374
I0210 03:13:24.500724 139878398215936 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.03526000678539276, loss=0.040471065789461136
I0210 03:13:56.350357 139836244932352 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.03049188107252121, loss=0.03647453710436821
I0210 03:13:58.568685 140039251117888 spec.py:321] Evaluating on the training split.
I0210 03:15:36.830758 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 03:15:39.949378 140039251117888 spec.py:349] Evaluating on the test split.
I0210 03:15:43.046450 140039251117888 submission_runner.py:408] Time since start: 11507.04s, 	Step: 24708, 	{'train/accuracy': 0.9888287782669067, 'train/loss': 0.03828553482890129, 'train/mean_average_precision': 0.22730054165800737, 'validation/accuracy': 0.9858277440071106, 'validation/loss': 0.04814917594194412, 'validation/mean_average_precision': 0.19149159963752463, 'validation/num_examples': 43793, 'test/accuracy': 0.9849161505699158, 'test/loss': 0.05093801021575928, 'test/mean_average_precision': 0.1800086315183968, 'test/num_examples': 43793, 'score': 7934.792539834976, 'total_duration': 11507.038450717926, 'accumulated_submission_time': 7934.792539834976, 'accumulated_eval_time': 3570.6146986484528, 'accumulated_logging_time': 0.9710242748260498}
I0210 03:15:43.067545 139818172847872 logging_writer.py:48] [24708] accumulated_eval_time=3570.614699, accumulated_logging_time=0.971024, accumulated_submission_time=7934.792540, global_step=24708, preemption_count=0, score=7934.792540, test/accuracy=0.984916, test/loss=0.050938, test/mean_average_precision=0.180009, test/num_examples=43793, total_duration=11507.038451, train/accuracy=0.988829, train/loss=0.038286, train/mean_average_precision=0.227301, validation/accuracy=0.985828, validation/loss=0.048149, validation/mean_average_precision=0.191492, validation/num_examples=43793
I0210 03:16:13.818839 139836634785536 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.049334436655044556, loss=0.04054928198456764
I0210 03:16:47.484610 139818172847872 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.03222667798399925, loss=0.035212792456150055
I0210 03:17:19.972389 139836634785536 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.02422318421304226, loss=0.03875557333230972
I0210 03:17:52.326267 139818172847872 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.04612605273723602, loss=0.03675369545817375
I0210 03:18:24.503039 139836634785536 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.0806417390704155, loss=0.03855237737298012
I0210 03:18:56.171245 139818172847872 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.07618261128664017, loss=0.04440863057971001
I0210 03:19:28.457182 139836634785536 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.06698975712060928, loss=0.03825622424483299
I0210 03:19:43.212482 140039251117888 spec.py:321] Evaluating on the training split.
I0210 03:21:22.326366 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 03:21:25.831547 140039251117888 spec.py:349] Evaluating on the test split.
I0210 03:21:29.196646 140039251117888 submission_runner.py:408] Time since start: 11853.19s, 	Step: 25447, 	{'train/accuracy': 0.9889517426490784, 'train/loss': 0.03764249011874199, 'train/mean_average_precision': 0.24037670319066257, 'validation/accuracy': 0.9858801364898682, 'validation/loss': 0.047933947294950485, 'validation/mean_average_precision': 0.19393603199913673, 'validation/num_examples': 43793, 'test/accuracy': 0.9848967790603638, 'test/loss': 0.05094367638230324, 'test/mean_average_precision': 0.18389132112044393, 'test/num_examples': 43793, 'score': 8174.904351234436, 'total_duration': 11853.188624620438, 'accumulated_submission_time': 8174.904351234436, 'accumulated_eval_time': 3676.598792552948, 'accumulated_logging_time': 1.0045819282531738}
I0210 03:21:29.220804 139836244932352 logging_writer.py:48] [25447] accumulated_eval_time=3676.598793, accumulated_logging_time=1.004582, accumulated_submission_time=8174.904351, global_step=25447, preemption_count=0, score=8174.904351, test/accuracy=0.984897, test/loss=0.050944, test/mean_average_precision=0.183891, test/num_examples=43793, total_duration=11853.188625, train/accuracy=0.988952, train/loss=0.037642, train/mean_average_precision=0.240377, validation/accuracy=0.985880, validation/loss=0.047934, validation/mean_average_precision=0.193936, validation/num_examples=43793
I0210 03:21:47.492299 139878398215936 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.04034945368766785, loss=0.0395311564207077
I0210 03:22:20.621613 139836244932352 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.0609377846121788, loss=0.03876306116580963
I0210 03:22:53.476008 139878398215936 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.08641692996025085, loss=0.04143598675727844
I0210 03:23:26.499645 139836244932352 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.07218104600906372, loss=0.03404882922768593
I0210 03:23:59.140761 139878398215936 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.04346224665641785, loss=0.03554566577076912
I0210 03:24:31.898219 139836244932352 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.036425963044166565, loss=0.037402499467134476
I0210 03:25:05.337785 139878398215936 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.05232204124331474, loss=0.04608675464987755
I0210 03:25:29.350876 140039251117888 spec.py:321] Evaluating on the training split.
I0210 03:27:06.618081 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 03:27:09.652141 140039251117888 spec.py:349] Evaluating on the test split.
I0210 03:27:12.701082 140039251117888 submission_runner.py:408] Time since start: 12196.69s, 	Step: 26174, 	{'train/accuracy': 0.988925576210022, 'train/loss': 0.037910811603069305, 'train/mean_average_precision': 0.22496314824009483, 'validation/accuracy': 0.9856353402137756, 'validation/loss': 0.04848511889576912, 'validation/mean_average_precision': 0.18464683630427442, 'validation/num_examples': 43793, 'test/accuracy': 0.9846979379653931, 'test/loss': 0.051385655999183655, 'test/mean_average_precision': 0.17690776959130197, 'test/num_examples': 43793, 'score': 8414.594813585281, 'total_duration': 12196.693078517914, 'accumulated_submission_time': 8414.594813585281, 'accumulated_eval_time': 3779.948962688446, 'accumulated_logging_time': 1.4443256855010986}
I0210 03:27:12.722918 139813965014784 logging_writer.py:48] [26174] accumulated_eval_time=3779.948963, accumulated_logging_time=1.444326, accumulated_submission_time=8414.594814, global_step=26174, preemption_count=0, score=8414.594814, test/accuracy=0.984698, test/loss=0.051386, test/mean_average_precision=0.176908, test/num_examples=43793, total_duration=12196.693079, train/accuracy=0.988926, train/loss=0.037911, train/mean_average_precision=0.224963, validation/accuracy=0.985635, validation/loss=0.048485, validation/mean_average_precision=0.184647, validation/num_examples=43793
I0210 03:27:21.695764 139818172847872 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.04914235323667526, loss=0.036506105214357376
I0210 03:27:53.834851 139813965014784 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.05660826340317726, loss=0.03847981616854668
I0210 03:28:25.433403 139818172847872 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.037344206124544144, loss=0.04148919880390167
I0210 03:28:57.421546 139813965014784 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.03435171768069267, loss=0.040141455829143524
I0210 03:29:29.436415 139818172847872 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.031788237392902374, loss=0.039570312947034836
I0210 03:30:02.142079 139813965014784 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.06196581944823265, loss=0.036757681518793106
I0210 03:30:33.841726 139818172847872 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.03835410252213478, loss=0.0426001250743866
I0210 03:31:05.853122 139813965014784 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.042014747858047485, loss=0.0362064354121685
I0210 03:31:12.813839 140039251117888 spec.py:321] Evaluating on the training split.
I0210 03:32:51.604585 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 03:32:54.787552 140039251117888 spec.py:349] Evaluating on the test split.
I0210 03:32:57.821128 140039251117888 submission_runner.py:408] Time since start: 12541.81s, 	Step: 26923, 	{'train/accuracy': 0.9887903332710266, 'train/loss': 0.03832165524363518, 'train/mean_average_precision': 0.21370930130145657, 'validation/accuracy': 0.9858123064041138, 'validation/loss': 0.04862184450030327, 'validation/mean_average_precision': 0.18894703016211614, 'validation/num_examples': 43793, 'test/accuracy': 0.9848226308822632, 'test/loss': 0.05195417255163193, 'test/mean_average_precision': 0.17806965507704928, 'test/num_examples': 43793, 'score': 8654.654390335083, 'total_duration': 12541.8131275177, 'accumulated_submission_time': 8654.654390335083, 'accumulated_eval_time': 3884.9562027454376, 'accumulated_logging_time': 1.477400302886963}
I0210 03:32:57.841974 139836634785536 logging_writer.py:48] [26923] accumulated_eval_time=3884.956203, accumulated_logging_time=1.477400, accumulated_submission_time=8654.654390, global_step=26923, preemption_count=0, score=8654.654390, test/accuracy=0.984823, test/loss=0.051954, test/mean_average_precision=0.178070, test/num_examples=43793, total_duration=12541.813128, train/accuracy=0.988790, train/loss=0.038322, train/mean_average_precision=0.213709, validation/accuracy=0.985812, validation/loss=0.048622, validation/mean_average_precision=0.188947, validation/num_examples=43793
I0210 03:33:23.143622 139878398215936 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.03915811702609062, loss=0.036413345485925674
I0210 03:33:54.914784 139836634785536 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.06068093702197075, loss=0.03970951586961746
I0210 03:34:27.373173 139878398215936 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.04224865511059761, loss=0.037057146430015564
I0210 03:34:59.752834 139836634785536 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.05358150228857994, loss=0.03572015464305878
I0210 03:35:32.640274 139878398215936 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.08018854260444641, loss=0.04388725385069847
I0210 03:36:05.464328 139836634785536 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.03486322611570358, loss=0.03712718188762665
I0210 03:36:37.899042 139878398215936 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.030597101897001266, loss=0.034751858562231064
I0210 03:36:58.046050 140039251117888 spec.py:321] Evaluating on the training split.
I0210 03:38:33.707886 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 03:38:37.130311 140039251117888 spec.py:349] Evaluating on the test split.
I0210 03:38:40.494410 140039251117888 submission_runner.py:408] Time since start: 12884.49s, 	Step: 27663, 	{'train/accuracy': 0.9887416362762451, 'train/loss': 0.03844446316361427, 'train/mean_average_precision': 0.21724267770919028, 'validation/accuracy': 0.9857981204986572, 'validation/loss': 0.04820290207862854, 'validation/mean_average_precision': 0.1893011427508064, 'validation/num_examples': 43793, 'test/accuracy': 0.984944760799408, 'test/loss': 0.05114787071943283, 'test/mean_average_precision': 0.1803156483087207, 'test/num_examples': 43793, 'score': 8894.8265645504, 'total_duration': 12884.48639678955, 'accumulated_submission_time': 8894.8265645504, 'accumulated_eval_time': 3987.4045078754425, 'accumulated_logging_time': 1.5093200206756592}
I0210 03:38:40.517031 139813965014784 logging_writer.py:48] [27663] accumulated_eval_time=3987.404508, accumulated_logging_time=1.509320, accumulated_submission_time=8894.826565, global_step=27663, preemption_count=0, score=8894.826565, test/accuracy=0.984945, test/loss=0.051148, test/mean_average_precision=0.180316, test/num_examples=43793, total_duration=12884.486397, train/accuracy=0.988742, train/loss=0.038444, train/mean_average_precision=0.217243, validation/accuracy=0.985798, validation/loss=0.048203, validation/mean_average_precision=0.189301, validation/num_examples=43793
I0210 03:38:52.865708 139836244932352 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.028484029695391655, loss=0.03331613168120384
I0210 03:39:25.552925 139813965014784 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.06413183361291885, loss=0.040908560156822205
I0210 03:39:58.018346 139836244932352 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.04764213413000107, loss=0.04162455350160599
I0210 03:40:30.931274 139813965014784 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.03283088654279709, loss=0.039946720004081726
I0210 03:41:03.924699 139836244932352 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.0467916876077652, loss=0.03581010177731514
I0210 03:41:36.330392 139813965014784 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.04206249490380287, loss=0.0348040871322155
I0210 03:42:09.148931 139836244932352 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.07162878662347794, loss=0.03748835623264313
I0210 03:42:40.780504 140039251117888 spec.py:321] Evaluating on the training split.
I0210 03:44:25.751923 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 03:44:28.843015 140039251117888 spec.py:349] Evaluating on the test split.
I0210 03:44:31.861037 140039251117888 submission_runner.py:408] Time since start: 13235.85s, 	Step: 28398, 	{'train/accuracy': 0.9888597130775452, 'train/loss': 0.03834989294409752, 'train/mean_average_precision': 0.2187355641821716, 'validation/accuracy': 0.985792875289917, 'validation/loss': 0.048368994146585464, 'validation/mean_average_precision': 0.1914001890482414, 'validation/num_examples': 43793, 'test/accuracy': 0.9848782420158386, 'test/loss': 0.05129092186689377, 'test/mean_average_precision': 0.18150058291692095, 'test/num_examples': 43793, 'score': 9135.05375790596, 'total_duration': 13235.853038072586, 'accumulated_submission_time': 9135.05375790596, 'accumulated_eval_time': 4098.485007286072, 'accumulated_logging_time': 1.5454697608947754}
I0210 03:44:31.882110 139818172847872 logging_writer.py:48] [28398] accumulated_eval_time=4098.485007, accumulated_logging_time=1.545470, accumulated_submission_time=9135.053758, global_step=28398, preemption_count=0, score=9135.053758, test/accuracy=0.984878, test/loss=0.051291, test/mean_average_precision=0.181501, test/num_examples=43793, total_duration=13235.853038, train/accuracy=0.988860, train/loss=0.038350, train/mean_average_precision=0.218736, validation/accuracy=0.985793, validation/loss=0.048369, validation/mean_average_precision=0.191400, validation/num_examples=43793
I0210 03:44:32.894573 139878398215936 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.03243045136332512, loss=0.03984285518527031
I0210 03:45:05.966731 139818172847872 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.047135964035987854, loss=0.03929179906845093
I0210 03:45:37.777694 139878398215936 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.04129879176616669, loss=0.040412306785583496
I0210 03:46:10.281742 139818172847872 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.050313446670770645, loss=0.043207455426454544
I0210 03:46:42.007609 139878398215936 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.09129127115011215, loss=0.03958868980407715
I0210 03:47:14.418731 139818172847872 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.032129596918821335, loss=0.03692254424095154
I0210 03:47:46.744494 139878398215936 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.05106048658490181, loss=0.03694240376353264
I0210 03:48:19.441477 139818172847872 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.037469323724508286, loss=0.03411474823951721
I0210 03:48:31.902923 140039251117888 spec.py:321] Evaluating on the training split.
I0210 03:50:08.823354 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 03:50:11.880215 140039251117888 spec.py:349] Evaluating on the test split.
I0210 03:50:14.876661 140039251117888 submission_runner.py:408] Time since start: 13578.87s, 	Step: 29139, 	{'train/accuracy': 0.9890433549880981, 'train/loss': 0.03766318038105965, 'train/mean_average_precision': 0.2262856218682795, 'validation/accuracy': 0.9858456254005432, 'validation/loss': 0.047857899218797684, 'validation/mean_average_precision': 0.1901224715875739, 'validation/num_examples': 43793, 'test/accuracy': 0.9849388599395752, 'test/loss': 0.05089077353477478, 'test/mean_average_precision': 0.18449894548178364, 'test/num_examples': 43793, 'score': 9375.042990922928, 'total_duration': 13578.868658781052, 'accumulated_submission_time': 9375.042990922928, 'accumulated_eval_time': 4201.458696365356, 'accumulated_logging_time': 1.5778565406799316}
I0210 03:50:14.898029 139813965014784 logging_writer.py:48] [29139] accumulated_eval_time=4201.458696, accumulated_logging_time=1.577857, accumulated_submission_time=9375.042991, global_step=29139, preemption_count=0, score=9375.042991, test/accuracy=0.984939, test/loss=0.050891, test/mean_average_precision=0.184499, test/num_examples=43793, total_duration=13578.868659, train/accuracy=0.989043, train/loss=0.037663, train/mean_average_precision=0.226286, validation/accuracy=0.985846, validation/loss=0.047858, validation/mean_average_precision=0.190122, validation/num_examples=43793
I0210 03:50:34.922759 139836244932352 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.056540027260780334, loss=0.03919709473848343
I0210 03:51:07.398796 139813965014784 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.043879132717847824, loss=0.038040053099393845
I0210 03:51:39.538031 139836244932352 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.057790737599134445, loss=0.03497806563973427
I0210 03:52:11.736329 139813965014784 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.07541624456644058, loss=0.03962065279483795
I0210 03:52:43.888676 139836244932352 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.030134405940771103, loss=0.04036526381969452
I0210 03:53:15.938378 139813965014784 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.06016569957137108, loss=0.03948404639959335
I0210 03:53:47.753186 139836244932352 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.0916144996881485, loss=0.03742821142077446
I0210 03:54:15.141080 140039251117888 spec.py:321] Evaluating on the training split.
I0210 03:55:51.237002 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 03:55:54.357100 140039251117888 spec.py:349] Evaluating on the test split.
I0210 03:55:59.743214 140039251117888 submission_runner.py:408] Time since start: 13923.74s, 	Step: 29886, 	{'train/accuracy': 0.9888697862625122, 'train/loss': 0.038062483072280884, 'train/mean_average_precision': 0.2286901512819851, 'validation/accuracy': 0.9859182834625244, 'validation/loss': 0.04802340269088745, 'validation/mean_average_precision': 0.19513268864816938, 'validation/num_examples': 43793, 'test/accuracy': 0.9849885702133179, 'test/loss': 0.05114302784204483, 'test/mean_average_precision': 0.18174385901222853, 'test/num_examples': 43793, 'score': 9615.254949569702, 'total_duration': 13923.735192537308, 'accumulated_submission_time': 9615.254949569702, 'accumulated_eval_time': 4306.060763597488, 'accumulated_logging_time': 1.6103568077087402}
I0210 03:55:59.765428 139818172847872 logging_writer.py:48] [29886] accumulated_eval_time=4306.060764, accumulated_logging_time=1.610357, accumulated_submission_time=9615.254950, global_step=29886, preemption_count=0, score=9615.254950, test/accuracy=0.984989, test/loss=0.051143, test/mean_average_precision=0.181744, test/num_examples=43793, total_duration=13923.735193, train/accuracy=0.988870, train/loss=0.038062, train/mean_average_precision=0.228690, validation/accuracy=0.985918, validation/loss=0.048023, validation/mean_average_precision=0.195133, validation/num_examples=43793
I0210 03:56:04.710165 139878398215936 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.049680422991514206, loss=0.038169555366039276
I0210 03:56:37.113003 139818172847872 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.037448421120643616, loss=0.03835791349411011
I0210 03:57:09.637675 139878398215936 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.07425706833600998, loss=0.037964798510074615
I0210 03:57:41.511767 139818172847872 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.09569823741912842, loss=0.03960895538330078
I0210 03:58:13.825459 139878398215936 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.049706101417541504, loss=0.040819112211465836
I0210 03:58:46.054729 139818172847872 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.07232214510440826, loss=0.04119299352169037
I0210 03:59:17.991834 139878398215936 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.07356130331754684, loss=0.042097948491573334
I0210 03:59:50.105595 139818172847872 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.07168811559677124, loss=0.03799871355295181
I0210 04:00:00.052850 140039251117888 spec.py:321] Evaluating on the training split.
I0210 04:01:33.468280 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 04:01:36.905597 140039251117888 spec.py:349] Evaluating on the test split.
I0210 04:01:40.293079 140039251117888 submission_runner.py:408] Time since start: 14264.29s, 	Step: 30632, 	{'train/accuracy': 0.9889466166496277, 'train/loss': 0.03792235255241394, 'train/mean_average_precision': 0.22140702156558983, 'validation/accuracy': 0.9857209920883179, 'validation/loss': 0.04887448623776436, 'validation/mean_average_precision': 0.18472380761224852, 'validation/num_examples': 43793, 'test/accuracy': 0.9848508834838867, 'test/loss': 0.05192939564585686, 'test/mean_average_precision': 0.17757909118447807, 'test/num_examples': 43793, 'score': 9855.509615659714, 'total_duration': 14264.285064697266, 'accumulated_submission_time': 9855.509615659714, 'accumulated_eval_time': 4406.300930023193, 'accumulated_logging_time': 1.644965410232544}
I0210 04:01:40.317145 139836244932352 logging_writer.py:48] [30632] accumulated_eval_time=4406.300930, accumulated_logging_time=1.644965, accumulated_submission_time=9855.509616, global_step=30632, preemption_count=0, score=9855.509616, test/accuracy=0.984851, test/loss=0.051929, test/mean_average_precision=0.177579, test/num_examples=43793, total_duration=14264.285065, train/accuracy=0.988947, train/loss=0.037922, train/mean_average_precision=0.221407, validation/accuracy=0.985721, validation/loss=0.048874, validation/mean_average_precision=0.184724, validation/num_examples=43793
I0210 04:02:02.921310 139836634785536 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.04282250627875328, loss=0.03695294260978699
I0210 04:02:35.737133 139836244932352 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.05978597328066826, loss=0.03958019241690636
I0210 04:03:08.228723 139836634785536 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.03391597047448158, loss=0.04025945067405701
I0210 04:03:40.861637 139836244932352 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.07245156168937683, loss=0.0341251939535141
I0210 04:04:13.879406 139836634785536 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.06778984516859055, loss=0.0410277284681797
I0210 04:04:46.831085 139836244932352 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.08850958943367004, loss=0.035264573991298676
I0210 04:05:19.416766 139836634785536 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.06192586198449135, loss=0.03762543573975563
I0210 04:05:40.498718 140039251117888 spec.py:321] Evaluating on the training split.
I0210 04:07:16.619255 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 04:07:19.679826 140039251117888 spec.py:349] Evaluating on the test split.
I0210 04:07:22.691991 140039251117888 submission_runner.py:408] Time since start: 14606.68s, 	Step: 31367, 	{'train/accuracy': 0.9888671636581421, 'train/loss': 0.038255780935287476, 'train/mean_average_precision': 0.2291374449002138, 'validation/accuracy': 0.9856942296028137, 'validation/loss': 0.04822748154401779, 'validation/mean_average_precision': 0.18544683546508053, 'validation/num_examples': 43793, 'test/accuracy': 0.9847914576530457, 'test/loss': 0.05096743628382683, 'test/mean_average_precision': 0.1805531748658989, 'test/num_examples': 43793, 'score': 10095.65578699112, 'total_duration': 14606.683991193771, 'accumulated_submission_time': 10095.65578699112, 'accumulated_eval_time': 4508.494157552719, 'accumulated_logging_time': 1.681239128112793}
I0210 04:07:22.713654 139813965014784 logging_writer.py:48] [31367] accumulated_eval_time=4508.494158, accumulated_logging_time=1.681239, accumulated_submission_time=10095.655787, global_step=31367, preemption_count=0, score=10095.655787, test/accuracy=0.984791, test/loss=0.050967, test/mean_average_precision=0.180553, test/num_examples=43793, total_duration=14606.683991, train/accuracy=0.988867, train/loss=0.038256, train/mean_average_precision=0.229137, validation/accuracy=0.985694, validation/loss=0.048227, validation/mean_average_precision=0.185447, validation/num_examples=43793
I0210 04:07:33.579668 139878398215936 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.07819885015487671, loss=0.036281321197748184
I0210 04:08:05.929700 139813965014784 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.06555846333503723, loss=0.03660174459218979
I0210 04:08:38.134298 139878398215936 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.07124342024326324, loss=0.036226965487003326
I0210 04:09:10.556024 139813965014784 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.04670371487736702, loss=0.0395023375749588
I0210 04:09:42.666940 139878398215936 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.035893555730581284, loss=0.03385800123214722
I0210 04:10:14.846637 139813965014784 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.06469995528459549, loss=0.035431381314992905
I0210 04:10:47.222478 139878398215936 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.04065816104412079, loss=0.03726668283343315
I0210 04:11:19.744054 139813965014784 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.06516263633966446, loss=0.03587440401315689
I0210 04:11:22.985627 140039251117888 spec.py:321] Evaluating on the training split.
I0210 04:13:04.452082 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 04:13:07.870918 140039251117888 spec.py:349] Evaluating on the test split.
I0210 04:13:11.217403 140039251117888 submission_runner.py:408] Time since start: 14955.21s, 	Step: 32111, 	{'train/accuracy': 0.988877534866333, 'train/loss': 0.037788957357406616, 'train/mean_average_precision': 0.2309434431589295, 'validation/accuracy': 0.9858277440071106, 'validation/loss': 0.048646677285432816, 'validation/mean_average_precision': 0.19952700093152717, 'validation/num_examples': 43793, 'test/accuracy': 0.9849165678024292, 'test/loss': 0.05145052820444107, 'test/mean_average_precision': 0.1954261868553897, 'test/num_examples': 43793, 'score': 10335.89645934105, 'total_duration': 14955.209387540817, 'accumulated_submission_time': 10335.89645934105, 'accumulated_eval_time': 4616.725874662399, 'accumulated_logging_time': 1.714320421218872}
I0210 04:13:11.242371 139818172847872 logging_writer.py:48] [32111] accumulated_eval_time=4616.725875, accumulated_logging_time=1.714320, accumulated_submission_time=10335.896459, global_step=32111, preemption_count=0, score=10335.896459, test/accuracy=0.984917, test/loss=0.051451, test/mean_average_precision=0.195426, test/num_examples=43793, total_duration=14955.209388, train/accuracy=0.988878, train/loss=0.037789, train/mean_average_precision=0.230943, validation/accuracy=0.985828, validation/loss=0.048647, validation/mean_average_precision=0.199527, validation/num_examples=43793
I0210 04:13:40.542790 139836634785536 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.047763191163539886, loss=0.03701081499457359
I0210 04:14:13.977960 139818172847872 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.07916556298732758, loss=0.03719445317983627
I0210 04:14:47.600745 139836634785536 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.04442959651350975, loss=0.03683188185095787
I0210 04:15:19.925291 139818172847872 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.03226253390312195, loss=0.03521069511771202
I0210 04:15:51.851266 139836634785536 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.03876228258013725, loss=0.03896475210785866
I0210 04:16:23.907776 139818172847872 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.0833999514579773, loss=0.044255781918764114
I0210 04:16:55.957690 139836634785536 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.04804534465074539, loss=0.03662842512130737
I0210 04:17:11.350308 140039251117888 spec.py:321] Evaluating on the training split.
I0210 04:18:50.714125 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 04:18:54.192939 140039251117888 spec.py:349] Evaluating on the test split.
I0210 04:18:57.562973 140039251117888 submission_runner.py:408] Time since start: 15301.55s, 	Step: 32848, 	{'train/accuracy': 0.9889434576034546, 'train/loss': 0.03779633715748787, 'train/mean_average_precision': 0.2432826322175141, 'validation/accuracy': 0.9857007265090942, 'validation/loss': 0.048318423330783844, 'validation/mean_average_precision': 0.19516267882604674, 'validation/num_examples': 43793, 'test/accuracy': 0.9847506284713745, 'test/loss': 0.051211535930633545, 'test/mean_average_precision': 0.18921546074442697, 'test/num_examples': 43793, 'score': 10575.972088098526, 'total_duration': 15301.55495762825, 'accumulated_submission_time': 10575.972088098526, 'accumulated_eval_time': 4722.9384779930115, 'accumulated_logging_time': 1.7510161399841309}
I0210 04:18:57.586322 139836244932352 logging_writer.py:48] [32848] accumulated_eval_time=4722.938478, accumulated_logging_time=1.751016, accumulated_submission_time=10575.972088, global_step=32848, preemption_count=0, score=10575.972088, test/accuracy=0.984751, test/loss=0.051212, test/mean_average_precision=0.189215, test/num_examples=43793, total_duration=15301.554958, train/accuracy=0.988943, train/loss=0.037796, train/mean_average_precision=0.243283, validation/accuracy=0.985701, validation/loss=0.048318, validation/mean_average_precision=0.195163, validation/num_examples=43793
I0210 04:19:14.802813 139878398215936 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.04453859478235245, loss=0.03348183259367943
I0210 04:19:47.730020 139836244932352 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.040296584367752075, loss=0.03757810592651367
I0210 04:20:20.388139 139878398215936 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.0486866794526577, loss=0.03928151726722717
I0210 04:20:53.049058 139836244932352 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.07610613107681274, loss=0.03495151549577713
I0210 04:21:25.615629 139878398215936 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.06179410219192505, loss=0.039353612810373306
I0210 04:21:58.091187 139836244932352 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.043613236397504807, loss=0.03862006589770317
I0210 04:22:30.677834 139878398215936 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.11195795983076096, loss=0.03878037631511688
I0210 04:22:57.750188 140039251117888 spec.py:321] Evaluating on the training split.
I0210 04:24:37.321343 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 04:24:40.446127 140039251117888 spec.py:349] Evaluating on the test split.
I0210 04:24:43.604696 140039251117888 submission_runner.py:408] Time since start: 15647.60s, 	Step: 33585, 	{'train/accuracy': 0.9891308546066284, 'train/loss': 0.037314917892217636, 'train/mean_average_precision': 0.22602699491282865, 'validation/accuracy': 0.9859727025032043, 'validation/loss': 0.04742559790611267, 'validation/mean_average_precision': 0.19375695681022728, 'validation/num_examples': 43793, 'test/accuracy': 0.9850643873214722, 'test/loss': 0.05009305477142334, 'test/mean_average_precision': 0.1914235613692715, 'test/num_examples': 43793, 'score': 10816.100275278091, 'total_duration': 15647.596687793732, 'accumulated_submission_time': 10816.100275278091, 'accumulated_eval_time': 4828.792944908142, 'accumulated_logging_time': 1.7866477966308594}
I0210 04:24:43.626526 139813965014784 logging_writer.py:48] [33585] accumulated_eval_time=4828.792945, accumulated_logging_time=1.786648, accumulated_submission_time=10816.100275, global_step=33585, preemption_count=0, score=10816.100275, test/accuracy=0.985064, test/loss=0.050093, test/mean_average_precision=0.191424, test/num_examples=43793, total_duration=15647.596688, train/accuracy=0.989131, train/loss=0.037315, train/mean_average_precision=0.226027, validation/accuracy=0.985973, validation/loss=0.047426, validation/mean_average_precision=0.193757, validation/num_examples=43793
I0210 04:24:48.851653 139836634785536 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.08398005366325378, loss=0.04067691043019295
I0210 04:25:21.125711 139813965014784 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.04541349411010742, loss=0.037362076342105865
I0210 04:25:53.508503 139836634785536 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.06411736458539963, loss=0.033156294375658035
I0210 04:26:26.071182 139813965014784 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.04425610974431038, loss=0.0402316153049469
I0210 04:26:58.373866 139836634785536 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.04016009718179703, loss=0.039214592427015305
I0210 04:27:30.326989 139813965014784 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.06416158378124237, loss=0.04252086579799652
I0210 04:28:02.329485 139836634785536 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.041448257863521576, loss=0.037375327199697495
I0210 04:28:34.547034 139813965014784 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.07668566703796387, loss=0.03987418860197067
I0210 04:28:43.727685 140039251117888 spec.py:321] Evaluating on the training split.
I0210 04:30:24.328056 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 04:30:27.417403 140039251117888 spec.py:349] Evaluating on the test split.
I0210 04:30:30.394902 140039251117888 submission_runner.py:408] Time since start: 15994.39s, 	Step: 34329, 	{'train/accuracy': 0.9889580011367798, 'train/loss': 0.03759896010160446, 'train/mean_average_precision': 0.2289441943128746, 'validation/accuracy': 0.9859438538551331, 'validation/loss': 0.04773719608783722, 'validation/mean_average_precision': 0.20205845787736976, 'validation/num_examples': 43793, 'test/accuracy': 0.9850370287895203, 'test/loss': 0.050530947744846344, 'test/mean_average_precision': 0.19075800828155534, 'test/num_examples': 43793, 'score': 11056.169059515, 'total_duration': 15994.386904001236, 'accumulated_submission_time': 11056.169059515, 'accumulated_eval_time': 4935.460122585297, 'accumulated_logging_time': 1.8201825618743896}
I0210 04:30:30.417035 139836244932352 logging_writer.py:48] [34329] accumulated_eval_time=4935.460123, accumulated_logging_time=1.820183, accumulated_submission_time=11056.169060, global_step=34329, preemption_count=0, score=11056.169060, test/accuracy=0.985037, test/loss=0.050531, test/mean_average_precision=0.190758, test/num_examples=43793, total_duration=15994.386904, train/accuracy=0.988958, train/loss=0.037599, train/mean_average_precision=0.228944, validation/accuracy=0.985944, validation/loss=0.047737, validation/mean_average_precision=0.202058, validation/num_examples=43793
I0210 04:30:53.754807 139878398215936 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.04240330308675766, loss=0.04301910102367401
I0210 04:31:25.830124 139836244932352 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.06681445986032486, loss=0.037174440920352936
I0210 04:31:58.194246 139878398215936 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.041735075414180756, loss=0.033603474497795105
I0210 04:32:30.520030 139836244932352 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.03913189470767975, loss=0.04090702533721924
I0210 04:33:02.427697 139878398215936 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.07059885561466217, loss=0.0388268418610096
I0210 04:33:34.291181 139836244932352 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.047302551567554474, loss=0.039842016994953156
I0210 04:34:06.256354 139878398215936 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.03423132374882698, loss=0.038932763040065765
I0210 04:34:30.490289 140039251117888 spec.py:321] Evaluating on the training split.
I0210 04:36:14.018595 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 04:36:17.466049 140039251117888 spec.py:349] Evaluating on the test split.
I0210 04:36:20.841430 140039251117888 submission_runner.py:408] Time since start: 16344.83s, 	Step: 35077, 	{'train/accuracy': 0.9889733791351318, 'train/loss': 0.0374145433306694, 'train/mean_average_precision': 0.23837052534211148, 'validation/accuracy': 0.985992968082428, 'validation/loss': 0.04756581783294678, 'validation/mean_average_precision': 0.19238177482586644, 'validation/num_examples': 43793, 'test/accuracy': 0.9851309657096863, 'test/loss': 0.05031953379511833, 'test/mean_average_precision': 0.18689104401852946, 'test/num_examples': 43793, 'score': 11296.211079359055, 'total_duration': 16344.833291053772, 'accumulated_submission_time': 11296.211079359055, 'accumulated_eval_time': 5045.811078548431, 'accumulated_logging_time': 1.8531954288482666}
I0210 04:36:20.866579 139813965014784 logging_writer.py:48] [35077] accumulated_eval_time=5045.811079, accumulated_logging_time=1.853195, accumulated_submission_time=11296.211079, global_step=35077, preemption_count=0, score=11296.211079, test/accuracy=0.985131, test/loss=0.050320, test/mean_average_precision=0.186891, test/num_examples=43793, total_duration=16344.833291, train/accuracy=0.988973, train/loss=0.037415, train/mean_average_precision=0.238371, validation/accuracy=0.985993, validation/loss=0.047566, validation/mean_average_precision=0.192382, validation/num_examples=43793
I0210 04:36:28.756087 139818172847872 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.06437449157238007, loss=0.04348926246166229
I0210 04:37:00.907017 139813965014784 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.07994372397661209, loss=0.03703807294368744
I0210 04:37:33.325896 139818172847872 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.03453114628791809, loss=0.03611689433455467
I0210 04:38:05.353293 139813965014784 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.048702970147132874, loss=0.040859073400497437
I0210 04:38:37.923358 139818172847872 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.052753474563360214, loss=0.036901652812957764
I0210 04:39:10.822215 139813965014784 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.05360821634531021, loss=0.03793887794017792
I0210 04:39:43.468801 139818172847872 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.04383021593093872, loss=0.03569289296865463
I0210 04:40:16.169053 139813965014784 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.06578527390956879, loss=0.03856443613767624
I0210 04:40:20.932699 140039251117888 spec.py:321] Evaluating on the training split.
I0210 04:41:55.254379 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 04:41:58.341172 140039251117888 spec.py:349] Evaluating on the test split.
I0210 04:42:01.303623 140039251117888 submission_runner.py:408] Time since start: 16685.30s, 	Step: 35816, 	{'train/accuracy': 0.9889889359474182, 'train/loss': 0.03766604885458946, 'train/mean_average_precision': 0.22819317542738163, 'validation/accuracy': 0.9858512878417969, 'validation/loss': 0.04821831360459328, 'validation/mean_average_precision': 0.19119373361529793, 'validation/num_examples': 43793, 'test/accuracy': 0.9848508834838867, 'test/loss': 0.05135584622621536, 'test/mean_average_precision': 0.18605029850991475, 'test/num_examples': 43793, 'score': 11536.243942975998, 'total_duration': 16685.29561161995, 'accumulated_submission_time': 11536.243942975998, 'accumulated_eval_time': 5146.181943178177, 'accumulated_logging_time': 1.8905532360076904}
I0210 04:42:01.326015 139836244932352 logging_writer.py:48] [35816] accumulated_eval_time=5146.181943, accumulated_logging_time=1.890553, accumulated_submission_time=11536.243943, global_step=35816, preemption_count=0, score=11536.243943, test/accuracy=0.984851, test/loss=0.051356, test/mean_average_precision=0.186050, test/num_examples=43793, total_duration=16685.295612, train/accuracy=0.988989, train/loss=0.037666, train/mean_average_precision=0.228193, validation/accuracy=0.985851, validation/loss=0.048218, validation/mean_average_precision=0.191194, validation/num_examples=43793
I0210 04:42:28.696270 139836634785536 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.05422063171863556, loss=0.03912794589996338
I0210 04:43:00.751509 139836244932352 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.055863965302705765, loss=0.04154232516884804
I0210 04:43:32.741184 139836634785536 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.04303969442844391, loss=0.0368034802377224
I0210 04:44:05.002326 139836244932352 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.08296999335289001, loss=0.035682931542396545
I0210 04:44:37.338472 139836634785536 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.04447607323527336, loss=0.038350995630025864
I0210 04:45:09.561772 139836244932352 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.05734169855713844, loss=0.036974746733903885
I0210 04:45:42.024721 139836634785536 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.08218919485807419, loss=0.04161190986633301
I0210 04:46:01.378382 140039251117888 spec.py:321] Evaluating on the training split.
I0210 04:47:37.085854 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 04:47:40.213585 140039251117888 spec.py:349] Evaluating on the test split.
I0210 04:47:43.257826 140039251117888 submission_runner.py:408] Time since start: 17027.25s, 	Step: 36561, 	{'train/accuracy': 0.9889605641365051, 'train/loss': 0.03772250562906265, 'train/mean_average_precision': 0.22952639439431494, 'validation/accuracy': 0.9858009815216064, 'validation/loss': 0.04832683131098747, 'validation/mean_average_precision': 0.1856618333966113, 'validation/num_examples': 43793, 'test/accuracy': 0.984813392162323, 'test/loss': 0.051375459879636765, 'test/mean_average_precision': 0.1791125450303736, 'test/num_examples': 43793, 'score': 11776.2650411129, 'total_duration': 17027.249828100204, 'accumulated_submission_time': 11776.2650411129, 'accumulated_eval_time': 5248.06134390831, 'accumulated_logging_time': 1.9240601062774658}
I0210 04:47:43.280432 139818172847872 logging_writer.py:48] [36561] accumulated_eval_time=5248.061344, accumulated_logging_time=1.924060, accumulated_submission_time=11776.265041, global_step=36561, preemption_count=0, score=11776.265041, test/accuracy=0.984813, test/loss=0.051375, test/mean_average_precision=0.179113, test/num_examples=43793, total_duration=17027.249828, train/accuracy=0.988961, train/loss=0.037723, train/mean_average_precision=0.229526, validation/accuracy=0.985801, validation/loss=0.048327, validation/mean_average_precision=0.185662, validation/num_examples=43793
I0210 04:47:56.178510 139878398215936 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.05565079674124718, loss=0.03707999736070633
I0210 04:48:28.737196 139818172847872 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.04165009409189224, loss=0.03543204441666603
I0210 04:49:01.071474 139878398215936 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.07167457789182663, loss=0.038317691534757614
I0210 04:49:33.387332 139818172847872 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.05551294982433319, loss=0.03810415044426918
I0210 04:50:05.773181 139878398215936 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.04168076440691948, loss=0.034464944154024124
I0210 04:50:37.706312 139818172847872 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.0943816527724266, loss=0.03603117913007736
I0210 04:51:09.718405 139878398215936 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.08134764432907104, loss=0.03716267645359039
I0210 04:51:41.632970 139818172847872 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.044964589178562164, loss=0.0372040756046772
I0210 04:51:43.566702 140039251117888 spec.py:321] Evaluating on the training split.
I0210 04:53:21.443812 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 04:53:24.566043 140039251117888 spec.py:349] Evaluating on the test split.
I0210 04:53:27.679198 140039251117888 submission_runner.py:408] Time since start: 17371.67s, 	Step: 37307, 	{'train/accuracy': 0.9892958998680115, 'train/loss': 0.03638657182455063, 'train/mean_average_precision': 0.2465555241199644, 'validation/accuracy': 0.985998272895813, 'validation/loss': 0.04717489331960678, 'validation/mean_average_precision': 0.20124921395267825, 'validation/num_examples': 43793, 'test/accuracy': 0.9851098656654358, 'test/loss': 0.050088971853256226, 'test/mean_average_precision': 0.19173120152422768, 'test/num_examples': 43793, 'score': 12016.520510673523, 'total_duration': 17371.671184062958, 'accumulated_submission_time': 12016.520510673523, 'accumulated_eval_time': 5352.1737768650055, 'accumulated_logging_time': 1.9577631950378418}
I0210 04:53:27.701758 139813965014784 logging_writer.py:48] [37307] accumulated_eval_time=5352.173777, accumulated_logging_time=1.957763, accumulated_submission_time=12016.520511, global_step=37307, preemption_count=0, score=12016.520511, test/accuracy=0.985110, test/loss=0.050089, test/mean_average_precision=0.191731, test/num_examples=43793, total_duration=17371.671184, train/accuracy=0.989296, train/loss=0.036387, train/mean_average_precision=0.246556, validation/accuracy=0.985998, validation/loss=0.047175, validation/mean_average_precision=0.201249, validation/num_examples=43793
I0210 04:53:58.228388 139836634785536 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.042874615639448166, loss=0.03410514444112778
I0210 04:54:30.916887 139813965014784 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.06348830461502075, loss=0.0363771952688694
I0210 04:55:03.308010 139836634785536 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.05983851104974747, loss=0.036055710166692734
I0210 04:55:35.570144 139813965014784 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.07851480692625046, loss=0.033481571823358536
I0210 04:56:07.716639 139836634785536 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.09047631919384003, loss=0.04137988016009331
I0210 04:56:39.713547 139813965014784 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.06038527935743332, loss=0.03458473086357117
I0210 04:57:11.907909 139836634785536 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.08379629254341125, loss=0.0346335805952549
I0210 04:57:27.996006 140039251117888 spec.py:321] Evaluating on the training split.
I0210 04:59:08.401988 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 04:59:11.572067 140039251117888 spec.py:349] Evaluating on the test split.
I0210 04:59:14.751735 140039251117888 submission_runner.py:408] Time since start: 17718.74s, 	Step: 38051, 	{'train/accuracy': 0.9892456531524658, 'train/loss': 0.03651779517531395, 'train/mean_average_precision': 0.25090958297878485, 'validation/accuracy': 0.9860972762107849, 'validation/loss': 0.04758093133568764, 'validation/mean_average_precision': 0.19894745975519185, 'validation/num_examples': 43793, 'test/accuracy': 0.9851751923561096, 'test/loss': 0.050572469830513, 'test/mean_average_precision': 0.19549311068044276, 'test/num_examples': 43793, 'score': 12256.781912088394, 'total_duration': 17718.74373793602, 'accumulated_submission_time': 12256.781912088394, 'accumulated_eval_time': 5458.929462432861, 'accumulated_logging_time': 1.9929468631744385}
I0210 04:59:14.775277 139818172847872 logging_writer.py:48] [38051] accumulated_eval_time=5458.929462, accumulated_logging_time=1.992947, accumulated_submission_time=12256.781912, global_step=38051, preemption_count=0, score=12256.781912, test/accuracy=0.985175, test/loss=0.050572, test/mean_average_precision=0.195493, test/num_examples=43793, total_duration=17718.743738, train/accuracy=0.989246, train/loss=0.036518, train/mean_average_precision=0.250910, validation/accuracy=0.986097, validation/loss=0.047581, validation/mean_average_precision=0.198947, validation/num_examples=43793
I0210 04:59:30.806441 139878398215936 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.0481080636382103, loss=0.03268896043300629
I0210 05:00:03.213229 139818172847872 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.06568773090839386, loss=0.033856794238090515
I0210 05:00:35.334780 139878398215936 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.09280776977539062, loss=0.04186493530869484
I0210 05:01:07.887439 139818172847872 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.05557208135724068, loss=0.031360577791929245
I0210 05:01:39.937481 139878398215936 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.13004738092422485, loss=0.037107884883880615
I0210 05:02:12.590673 139818172847872 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.06608142703771591, loss=0.03531510755419731
I0210 05:02:45.018956 139878398215936 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.05281080678105354, loss=0.04121999815106392
I0210 05:03:14.993404 140039251117888 spec.py:321] Evaluating on the training split.
I0210 05:04:51.774837 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 05:04:54.867940 140039251117888 spec.py:349] Evaluating on the test split.
I0210 05:04:58.008728 140039251117888 submission_runner.py:408] Time since start: 18062.00s, 	Step: 38792, 	{'train/accuracy': 0.9892786145210266, 'train/loss': 0.036268580704927444, 'train/mean_average_precision': 0.2614758905918543, 'validation/accuracy': 0.9860640168190002, 'validation/loss': 0.04710369557142258, 'validation/mean_average_precision': 0.20225724747870746, 'validation/num_examples': 43793, 'test/accuracy': 0.9852147698402405, 'test/loss': 0.049997612833976746, 'test/mean_average_precision': 0.20277524336345168, 'test/num_examples': 43793, 'score': 12496.968587875366, 'total_duration': 18062.000724554062, 'accumulated_submission_time': 12496.968587875366, 'accumulated_eval_time': 5561.9447453022, 'accumulated_logging_time': 2.0276620388031006}
I0210 05:04:58.031994 139813965014784 logging_writer.py:48] [38792] accumulated_eval_time=5561.944745, accumulated_logging_time=2.027662, accumulated_submission_time=12496.968588, global_step=38792, preemption_count=0, score=12496.968588, test/accuracy=0.985215, test/loss=0.049998, test/mean_average_precision=0.202775, test/num_examples=43793, total_duration=18062.000725, train/accuracy=0.989279, train/loss=0.036269, train/mean_average_precision=0.261476, validation/accuracy=0.986064, validation/loss=0.047104, validation/mean_average_precision=0.202257, validation/num_examples=43793
I0210 05:05:00.921614 139836244932352 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.04489828646183014, loss=0.03595159947872162
I0210 05:05:33.307063 139813965014784 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.05225607752799988, loss=0.03652580454945564
I0210 05:06:06.164548 139836244932352 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.044357672333717346, loss=0.034002162516117096
I0210 05:06:38.146350 139813965014784 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.038965120911598206, loss=0.0338311567902565
I0210 05:07:10.485153 139836244932352 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.0877101793885231, loss=0.03880814462900162
I0210 05:07:42.456567 139813965014784 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.04329829663038254, loss=0.036992404609918594
I0210 05:08:14.482027 139836244932352 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.06486796587705612, loss=0.037643227726221085
I0210 05:08:46.820870 139813965014784 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.0390695184469223, loss=0.039996832609176636
I0210 05:08:58.081938 140039251117888 spec.py:321] Evaluating on the training split.
I0210 05:10:34.152552 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 05:10:37.333558 140039251117888 spec.py:349] Evaluating on the test split.
I0210 05:10:40.364705 140039251117888 submission_runner.py:408] Time since start: 18404.36s, 	Step: 39536, 	{'train/accuracy': 0.9891278743743896, 'train/loss': 0.03699309378862381, 'train/mean_average_precision': 0.24490563183510883, 'validation/accuracy': 0.985833466053009, 'validation/loss': 0.047233082354068756, 'validation/mean_average_precision': 0.196620120614496, 'validation/num_examples': 43793, 'test/accuracy': 0.9850079417228699, 'test/loss': 0.049868013709783554, 'test/mean_average_precision': 0.18901894211007755, 'test/num_examples': 43793, 'score': 12736.987015485764, 'total_duration': 18404.35669374466, 'accumulated_submission_time': 12736.987015485764, 'accumulated_eval_time': 5664.227452039719, 'accumulated_logging_time': 2.0624632835388184}
I0210 05:10:40.389771 139836634785536 logging_writer.py:48] [39536] accumulated_eval_time=5664.227452, accumulated_logging_time=2.062463, accumulated_submission_time=12736.987015, global_step=39536, preemption_count=0, score=12736.987015, test/accuracy=0.985008, test/loss=0.049868, test/mean_average_precision=0.189019, test/num_examples=43793, total_duration=18404.356694, train/accuracy=0.989128, train/loss=0.036993, train/mean_average_precision=0.244906, validation/accuracy=0.985833, validation/loss=0.047233, validation/mean_average_precision=0.196620, validation/num_examples=43793
I0210 05:11:01.388015 139878398215936 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.05459599569439888, loss=0.03983946144580841
I0210 05:11:33.463875 139836634785536 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.05742896348237991, loss=0.03919424116611481
I0210 05:12:05.375787 139878398215936 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.042272426187992096, loss=0.039966776967048645
I0210 05:12:37.564797 139836634785536 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.10495112836360931, loss=0.039301034063100815
I0210 05:13:09.560497 139878398215936 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.04501287639141083, loss=0.03626580163836479
I0210 05:13:42.443774 139836634785536 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.07858941704034805, loss=0.033470284193754196
I0210 05:14:15.003187 139878398215936 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.10194180905818939, loss=0.03360750898718834
I0210 05:14:40.508882 140039251117888 spec.py:321] Evaluating on the training split.
I0210 05:16:20.755697 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 05:16:23.848003 140039251117888 spec.py:349] Evaluating on the test split.
I0210 05:16:26.867882 140039251117888 submission_runner.py:408] Time since start: 18750.86s, 	Step: 40280, 	{'train/accuracy': 0.9891670346260071, 'train/loss': 0.03707946464419365, 'train/mean_average_precision': 0.2359508469546338, 'validation/accuracy': 0.985985279083252, 'validation/loss': 0.04715704917907715, 'validation/mean_average_precision': 0.20417189248316878, 'validation/num_examples': 43793, 'test/accuracy': 0.9851473569869995, 'test/loss': 0.04994756355881691, 'test/mean_average_precision': 0.19444708721248188, 'test/num_examples': 43793, 'score': 12977.07556772232, 'total_duration': 18750.859877347946, 'accumulated_submission_time': 12977.07556772232, 'accumulated_eval_time': 5770.586403608322, 'accumulated_logging_time': 2.098353624343872}
I0210 05:16:26.891788 139818172847872 logging_writer.py:48] [40280] accumulated_eval_time=5770.586404, accumulated_logging_time=2.098354, accumulated_submission_time=12977.075568, global_step=40280, preemption_count=0, score=12977.075568, test/accuracy=0.985147, test/loss=0.049948, test/mean_average_precision=0.194447, test/num_examples=43793, total_duration=18750.859877, train/accuracy=0.989167, train/loss=0.037079, train/mean_average_precision=0.235951, validation/accuracy=0.985985, validation/loss=0.047157, validation/mean_average_precision=0.204172, validation/num_examples=43793
I0210 05:16:33.664736 139836244932352 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.06944602727890015, loss=0.039726246148347855
I0210 05:17:05.761662 139818172847872 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.04646901413798332, loss=0.040053706616163254
I0210 05:17:37.935751 139836244932352 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.06169017404317856, loss=0.039940107613801956
I0210 05:18:10.125394 139818172847872 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.051664769649505615, loss=0.036312054842710495
I0210 05:18:41.966684 139836244932352 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.07204496115446091, loss=0.04034704342484474
I0210 05:19:14.157583 139818172847872 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.05004933103919029, loss=0.03592350706458092
I0210 05:19:45.943021 139836244932352 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.07333658635616302, loss=0.03893823176622391
I0210 05:20:17.725650 139818172847872 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.06237705796957016, loss=0.04216531291604042
I0210 05:20:27.106900 140039251117888 spec.py:321] Evaluating on the training split.
I0210 05:21:59.784716 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 05:22:03.016194 140039251117888 spec.py:349] Evaluating on the test split.
I0210 05:22:05.968249 140039251117888 submission_runner.py:408] Time since start: 19089.96s, 	Step: 41030, 	{'train/accuracy': 0.9892788529396057, 'train/loss': 0.03651939332485199, 'train/mean_average_precision': 0.25827914906327654, 'validation/accuracy': 0.9861382842063904, 'validation/loss': 0.04669969156384468, 'validation/mean_average_precision': 0.20358638501200454, 'validation/num_examples': 43793, 'test/accuracy': 0.9852400422096252, 'test/loss': 0.049561236053705215, 'test/mean_average_precision': 0.19574684937723494, 'test/num_examples': 43793, 'score': 13217.259890556335, 'total_duration': 19089.960252285004, 'accumulated_submission_time': 13217.259890556335, 'accumulated_eval_time': 5869.447708368301, 'accumulated_logging_time': 2.133057117462158}
I0210 05:22:05.991668 139836634785536 logging_writer.py:48] [41030] accumulated_eval_time=5869.447708, accumulated_logging_time=2.133057, accumulated_submission_time=13217.259891, global_step=41030, preemption_count=0, score=13217.259891, test/accuracy=0.985240, test/loss=0.049561, test/mean_average_precision=0.195747, test/num_examples=43793, total_duration=19089.960252, train/accuracy=0.989279, train/loss=0.036519, train/mean_average_precision=0.258279, validation/accuracy=0.986138, validation/loss=0.046700, validation/mean_average_precision=0.203586, validation/num_examples=43793
I0210 05:22:28.243627 139878398215936 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.05874461680650711, loss=0.03616052865982056
I0210 05:23:00.142546 139836634785536 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.05270186439156532, loss=0.036455780267715454
I0210 05:23:32.031848 139878398215936 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.05305952578783035, loss=0.03898345306515694
I0210 05:24:04.265263 139836634785536 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.047952357679605484, loss=0.03480059280991554
I0210 05:24:36.166753 139878398215936 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.1005101427435875, loss=0.03902716562151909
I0210 05:25:08.100859 139836634785536 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.11797533929347992, loss=0.03569715470075607
I0210 05:25:39.931678 139878398215936 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.09428970515727997, loss=0.03677923604846001
I0210 05:26:06.093450 140039251117888 spec.py:321] Evaluating on the training split.
I0210 05:27:38.393658 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 05:27:41.439196 140039251117888 spec.py:349] Evaluating on the test split.
I0210 05:27:44.440217 140039251117888 submission_runner.py:408] Time since start: 19428.43s, 	Step: 41783, 	{'train/accuracy': 0.98917156457901, 'train/loss': 0.036688920110464096, 'train/mean_average_precision': 0.23759855260147475, 'validation/accuracy': 0.9860579371452332, 'validation/loss': 0.04709014296531677, 'validation/mean_average_precision': 0.20033803818553, 'validation/num_examples': 43793, 'test/accuracy': 0.9851107597351074, 'test/loss': 0.05010908097028732, 'test/mean_average_precision': 0.19270607028931946, 'test/num_examples': 43793, 'score': 13457.330917358398, 'total_duration': 19428.43219280243, 'accumulated_submission_time': 13457.330917358398, 'accumulated_eval_time': 5967.794405460358, 'accumulated_logging_time': 2.167390823364258}
I0210 05:27:44.463766 139813965014784 logging_writer.py:48] [41783] accumulated_eval_time=5967.794405, accumulated_logging_time=2.167391, accumulated_submission_time=13457.330917, global_step=41783, preemption_count=0, score=13457.330917, test/accuracy=0.985111, test/loss=0.050109, test/mean_average_precision=0.192706, test/num_examples=43793, total_duration=19428.432193, train/accuracy=0.989172, train/loss=0.036689, train/mean_average_precision=0.237599, validation/accuracy=0.986058, validation/loss=0.047090, validation/mean_average_precision=0.200338, validation/num_examples=43793
I0210 05:27:50.227377 139836244932352 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.06028437241911888, loss=0.03706323727965355
I0210 05:28:22.433853 139813965014784 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.052909787744283676, loss=0.03905624896287918
I0210 05:28:54.643385 139836244932352 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.06280535459518433, loss=0.03440644219517708
I0210 05:29:26.613238 139813965014784 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.05118345469236374, loss=0.03571198880672455
I0210 05:29:59.875447 139836244932352 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.04560151323676109, loss=0.04165508970618248
I0210 05:30:32.226620 139813965014784 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.05303892493247986, loss=0.03822702541947365
I0210 05:31:04.709125 139836244932352 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.07133381813764572, loss=0.04204026237130165
I0210 05:31:37.098136 139813965014784 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.05042922496795654, loss=0.034707438200712204
I0210 05:31:44.460190 140039251117888 spec.py:321] Evaluating on the training split.
I0210 05:33:28.641291 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 05:33:31.699167 140039251117888 spec.py:349] Evaluating on the test split.
I0210 05:33:34.677198 140039251117888 submission_runner.py:408] Time since start: 19778.67s, 	Step: 42524, 	{'train/accuracy': 0.9892622828483582, 'train/loss': 0.03639383241534233, 'train/mean_average_precision': 0.2609484153302455, 'validation/accuracy': 0.9860400557518005, 'validation/loss': 0.04684174433350563, 'validation/mean_average_precision': 0.20936669277164166, 'validation/num_examples': 43793, 'test/accuracy': 0.9851625561714172, 'test/loss': 0.04959564283490181, 'test/mean_average_precision': 0.20294228271802964, 'test/num_examples': 43793, 'score': 13697.295249938965, 'total_duration': 19778.669197797775, 'accumulated_submission_time': 13697.295249938965, 'accumulated_eval_time': 6078.01136302948, 'accumulated_logging_time': 2.2034754753112793}
I0210 05:33:34.700598 139818172847872 logging_writer.py:48] [42524] accumulated_eval_time=6078.011363, accumulated_logging_time=2.203475, accumulated_submission_time=13697.295250, global_step=42524, preemption_count=0, score=13697.295250, test/accuracy=0.985163, test/loss=0.049596, test/mean_average_precision=0.202942, test/num_examples=43793, total_duration=19778.669198, train/accuracy=0.989262, train/loss=0.036394, train/mean_average_precision=0.260948, validation/accuracy=0.986040, validation/loss=0.046842, validation/mean_average_precision=0.209367, validation/num_examples=43793
I0210 05:33:59.218236 139878398215936 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.07471784949302673, loss=0.03382548689842224
I0210 05:34:31.611603 139818172847872 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.06772206723690033, loss=0.03250300511717796
I0210 05:35:03.668412 139878398215936 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.07025923579931259, loss=0.0355219691991806
I0210 05:35:35.488628 139818172847872 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.04925411939620972, loss=0.03328324109315872
I0210 05:36:07.749913 139878398215936 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.053795404732227325, loss=0.0403524786233902
I0210 05:36:39.399801 139818172847872 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.047500744462013245, loss=0.03795922175049782
I0210 05:37:12.090815 139878398215936 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.06061965972185135, loss=0.036733150482177734
I0210 05:37:34.885281 140039251117888 spec.py:321] Evaluating on the training split.
I0210 05:39:10.195398 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 05:39:13.239571 140039251117888 spec.py:349] Evaluating on the test split.
I0210 05:39:16.405776 140039251117888 submission_runner.py:408] Time since start: 20120.40s, 	Step: 43273, 	{'train/accuracy': 0.9893701076507568, 'train/loss': 0.03616950288414955, 'train/mean_average_precision': 0.24883681990520495, 'validation/accuracy': 0.9862012267112732, 'validation/loss': 0.04695441946387291, 'validation/mean_average_precision': 0.20823950554080176, 'validation/num_examples': 43793, 'test/accuracy': 0.985236644744873, 'test/loss': 0.049845922738313675, 'test/mean_average_precision': 0.2022384611718691, 'test/num_examples': 43793, 'score': 13937.44949221611, 'total_duration': 20120.397768735886, 'accumulated_submission_time': 13937.44949221611, 'accumulated_eval_time': 6179.531805515289, 'accumulated_logging_time': 2.237947463989258}
I0210 05:39:16.430206 139813965014784 logging_writer.py:48] [43273] accumulated_eval_time=6179.531806, accumulated_logging_time=2.237947, accumulated_submission_time=13937.449492, global_step=43273, preemption_count=0, score=13937.449492, test/accuracy=0.985237, test/loss=0.049846, test/mean_average_precision=0.202238, test/num_examples=43793, total_duration=20120.397769, train/accuracy=0.989370, train/loss=0.036170, train/mean_average_precision=0.248837, validation/accuracy=0.986201, validation/loss=0.046954, validation/mean_average_precision=0.208240, validation/num_examples=43793
I0210 05:39:25.310961 139836634785536 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.06810283660888672, loss=0.03335665538907051
I0210 05:39:57.427463 139813965014784 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.09084433317184448, loss=0.035424474626779556
I0210 05:40:29.603169 139836634785536 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.045116398483514786, loss=0.035500239580869675
I0210 05:41:01.776750 139813965014784 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.08202210813760757, loss=0.038980308920145035
I0210 05:41:34.165757 139836634785536 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.03883872553706169, loss=0.03724726662039757
I0210 05:42:06.306161 139813965014784 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.07738126814365387, loss=0.03389900177717209
I0210 05:42:38.408381 139836634785536 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.04179796203970909, loss=0.03736994042992592
I0210 05:43:10.655284 139813965014784 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.07354111224412918, loss=0.03539658337831497
I0210 05:43:16.550495 140039251117888 spec.py:321] Evaluating on the training split.
I0210 05:44:54.457756 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 05:44:57.519351 140039251117888 spec.py:349] Evaluating on the test split.
I0210 05:45:00.513378 140039251117888 submission_runner.py:408] Time since start: 20464.51s, 	Step: 44019, 	{'train/accuracy': 0.9893001317977905, 'train/loss': 0.03607473149895668, 'train/mean_average_precision': 0.2637567741698877, 'validation/accuracy': 0.9860668778419495, 'validation/loss': 0.046803850680589676, 'validation/mean_average_precision': 0.20828175219288686, 'validation/num_examples': 43793, 'test/accuracy': 0.9852383732795715, 'test/loss': 0.04968736320734024, 'test/mean_average_precision': 0.20247295465330872, 'test/num_examples': 43793, 'score': 14177.538898229599, 'total_duration': 20464.505380392075, 'accumulated_submission_time': 14177.538898229599, 'accumulated_eval_time': 6283.494655847549, 'accumulated_logging_time': 2.2736637592315674}
I0210 05:45:00.537296 139818172847872 logging_writer.py:48] [44019] accumulated_eval_time=6283.494656, accumulated_logging_time=2.273664, accumulated_submission_time=14177.538898, global_step=44019, preemption_count=0, score=14177.538898, test/accuracy=0.985238, test/loss=0.049687, test/mean_average_precision=0.202473, test/num_examples=43793, total_duration=20464.505380, train/accuracy=0.989300, train/loss=0.036075, train/mean_average_precision=0.263757, validation/accuracy=0.986067, validation/loss=0.046804, validation/mean_average_precision=0.208282, validation/num_examples=43793
I0210 05:45:26.331479 139878398215936 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.04097748175263405, loss=0.037172261625528336
I0210 05:45:57.640365 139818172847872 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.06755874305963516, loss=0.039420727640390396
I0210 05:46:29.459663 139878398215936 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.07139231264591217, loss=0.039577826857566833
I0210 05:47:00.918274 139818172847872 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.046280499547719955, loss=0.03623639792203903
I0210 05:47:32.544486 139878398215936 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.07194995880126953, loss=0.03587090224027634
I0210 05:48:03.986431 139818172847872 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.1016712337732315, loss=0.03696140646934509
I0210 05:48:35.731203 139878398215936 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.04652713984251022, loss=0.03418821841478348
I0210 05:49:00.747380 140039251117888 spec.py:321] Evaluating on the training split.
I0210 05:50:35.944275 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 05:50:39.005720 140039251117888 spec.py:349] Evaluating on the test split.
I0210 05:50:42.001231 140039251117888 submission_runner.py:408] Time since start: 20805.99s, 	Step: 44780, 	{'train/accuracy': 0.9894915223121643, 'train/loss': 0.03542591631412506, 'train/mean_average_precision': 0.26977764927073483, 'validation/accuracy': 0.9861618280410767, 'validation/loss': 0.046834852546453476, 'validation/mean_average_precision': 0.20999691848267893, 'validation/num_examples': 43793, 'test/accuracy': 0.9852139353752136, 'test/loss': 0.04983562231063843, 'test/mean_average_precision': 0.19900461359394145, 'test/num_examples': 43793, 'score': 14417.7179479599, 'total_duration': 20805.993231773376, 'accumulated_submission_time': 14417.7179479599, 'accumulated_eval_time': 6384.748462438583, 'accumulated_logging_time': 2.3086318969726562}
I0210 05:50:42.026486 139836244932352 logging_writer.py:48] [44780] accumulated_eval_time=6384.748462, accumulated_logging_time=2.308632, accumulated_submission_time=14417.717948, global_step=44780, preemption_count=0, score=14417.717948, test/accuracy=0.985214, test/loss=0.049836, test/mean_average_precision=0.199005, test/num_examples=43793, total_duration=20805.993232, train/accuracy=0.989492, train/loss=0.035426, train/mean_average_precision=0.269778, validation/accuracy=0.986162, validation/loss=0.046835, validation/mean_average_precision=0.209997, validation/num_examples=43793
I0210 05:50:48.791559 139836634785536 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.05314880236983299, loss=0.0380316823720932
I0210 05:51:20.768242 139836244932352 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.047990117222070694, loss=0.03605210408568382
I0210 05:51:52.821030 139836634785536 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.056140199303627014, loss=0.03704002499580383
I0210 05:52:24.957610 139836244932352 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.07261752337217331, loss=0.040919072926044464
I0210 05:52:57.277690 139836634785536 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.08508213609457016, loss=0.03700034320354462
I0210 05:53:29.223749 139836244932352 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.06714718043804169, loss=0.03476487845182419
I0210 05:54:01.005127 139836634785536 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.06006478890776634, loss=0.038629088550806046
I0210 05:54:33.602880 139836244932352 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.05022408440709114, loss=0.03656373545527458
I0210 05:54:42.151538 140039251117888 spec.py:321] Evaluating on the training split.
I0210 05:56:21.123605 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 05:56:24.618629 140039251117888 spec.py:349] Evaluating on the test split.
I0210 05:56:28.054359 140039251117888 submission_runner.py:408] Time since start: 21152.05s, 	Step: 45528, 	{'train/accuracy': 0.9893769025802612, 'train/loss': 0.03581002727150917, 'train/mean_average_precision': 0.2722272612931093, 'validation/accuracy': 0.9861736297607422, 'validation/loss': 0.0468456894159317, 'validation/mean_average_precision': 0.20630033965734001, 'validation/num_examples': 43793, 'test/accuracy': 0.9852522611618042, 'test/loss': 0.04980984330177307, 'test/mean_average_precision': 0.19994921522976514, 'test/num_examples': 43793, 'score': 14657.810967445374, 'total_duration': 21152.046332597733, 'accumulated_submission_time': 14657.810967445374, 'accumulated_eval_time': 6490.6512088775635, 'accumulated_logging_time': 2.3464467525482178}
I0210 05:56:28.082108 139813965014784 logging_writer.py:48] [45528] accumulated_eval_time=6490.651209, accumulated_logging_time=2.346447, accumulated_submission_time=14657.810967, global_step=45528, preemption_count=0, score=14657.810967, test/accuracy=0.985252, test/loss=0.049810, test/mean_average_precision=0.199949, test/num_examples=43793, total_duration=21152.046333, train/accuracy=0.989377, train/loss=0.035810, train/mean_average_precision=0.272227, validation/accuracy=0.986174, validation/loss=0.046846, validation/mean_average_precision=0.206300, validation/num_examples=43793
I0210 05:56:52.172198 139878398215936 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.03889325633645058, loss=0.030438710004091263
I0210 05:57:25.077599 139813965014784 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.06453002244234085, loss=0.034749459475278854
I0210 05:57:57.619005 139878398215936 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.0837811678647995, loss=0.03661905229091644
I0210 05:58:30.167233 139813965014784 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.13006485998630524, loss=0.036527808755636215
I0210 05:59:02.146276 139878398215936 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.05706337094306946, loss=0.03563803434371948
I0210 05:59:34.095195 139813965014784 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.052129413932561874, loss=0.03378915414214134
I0210 06:00:06.304074 139878398215936 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.0559404082596302, loss=0.039238326251506805
I0210 06:00:28.215776 140039251117888 spec.py:321] Evaluating on the training split.
I0210 06:02:03.524853 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 06:02:06.571368 140039251117888 spec.py:349] Evaluating on the test split.
I0210 06:02:09.557101 140039251117888 submission_runner.py:408] Time since start: 21493.55s, 	Step: 46269, 	{'train/accuracy': 0.9895589351654053, 'train/loss': 0.035501208156347275, 'train/mean_average_precision': 0.2688084506959136, 'validation/accuracy': 0.9862186908721924, 'validation/loss': 0.04645548760890961, 'validation/mean_average_precision': 0.21455191095003842, 'validation/num_examples': 43793, 'test/accuracy': 0.9853171110153198, 'test/loss': 0.04936239495873451, 'test/mean_average_precision': 0.20562919119766426, 'test/num_examples': 43793, 'score': 14897.910877466202, 'total_duration': 21493.54910182953, 'accumulated_submission_time': 14897.910877466202, 'accumulated_eval_time': 6591.992488861084, 'accumulated_logging_time': 2.38614821434021}
I0210 06:02:09.581668 139818172847872 logging_writer.py:48] [46269] accumulated_eval_time=6591.992489, accumulated_logging_time=2.386148, accumulated_submission_time=14897.910877, global_step=46269, preemption_count=0, score=14897.910877, test/accuracy=0.985317, test/loss=0.049362, test/mean_average_precision=0.205629, test/num_examples=43793, total_duration=21493.549102, train/accuracy=0.989559, train/loss=0.035501, train/mean_average_precision=0.268808, validation/accuracy=0.986219, validation/loss=0.046455, validation/mean_average_precision=0.214552, validation/num_examples=43793
I0210 06:02:19.865188 139836634785536 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.08487459272146225, loss=0.03943350166082382
I0210 06:02:52.095263 139818172847872 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.04763391613960266, loss=0.03510558232665062
I0210 06:03:24.681210 139836634785536 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.06944554299116135, loss=0.03632977604866028
I0210 06:03:57.326942 139818172847872 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.08967555314302444, loss=0.03323676809668541
I0210 06:04:29.934446 139836634785536 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.04576785862445831, loss=0.03874291107058525
I0210 06:05:02.273145 139818172847872 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.04566292092204094, loss=0.03598158061504364
I0210 06:05:34.606932 139836634785536 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.1124323382973671, loss=0.038998015224933624
I0210 06:06:07.212817 139818172847872 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.0581294521689415, loss=0.03338027745485306
I0210 06:06:09.782608 140039251117888 spec.py:321] Evaluating on the training split.
I0210 06:07:45.878952 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 06:07:48.937827 140039251117888 spec.py:349] Evaluating on the test split.
I0210 06:07:54.391173 140039251117888 submission_runner.py:408] Time since start: 21838.38s, 	Step: 47009, 	{'train/accuracy': 0.9891928434371948, 'train/loss': 0.03659552335739136, 'train/mean_average_precision': 0.2510488937477338, 'validation/accuracy': 0.9861078858375549, 'validation/loss': 0.04711566120386124, 'validation/mean_average_precision': 0.20644794906789043, 'validation/num_examples': 43793, 'test/accuracy': 0.9852370619773865, 'test/loss': 0.049996789544820786, 'test/mean_average_precision': 0.1964800375281237, 'test/num_examples': 43793, 'score': 15138.077248096466, 'total_duration': 21838.38315463066, 'accumulated_submission_time': 15138.077248096466, 'accumulated_eval_time': 6696.60099697113, 'accumulated_logging_time': 2.4226717948913574}
I0210 06:07:54.417674 139813965014784 logging_writer.py:48] [47009] accumulated_eval_time=6696.600997, accumulated_logging_time=2.422672, accumulated_submission_time=15138.077248, global_step=47009, preemption_count=0, score=15138.077248, test/accuracy=0.985237, test/loss=0.049997, test/mean_average_precision=0.196480, test/num_examples=43793, total_duration=21838.383155, train/accuracy=0.989193, train/loss=0.036596, train/mean_average_precision=0.251049, validation/accuracy=0.986108, validation/loss=0.047116, validation/mean_average_precision=0.206448, validation/num_examples=43793
I0210 06:08:24.454166 139836244932352 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.09410473704338074, loss=0.037538424134254456
I0210 06:08:56.590972 139813965014784 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.057278383523225784, loss=0.03589899092912674
I0210 06:09:29.108499 139836244932352 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.05295193940401077, loss=0.033532071858644485
I0210 06:10:01.261140 139813965014784 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.05562184005975723, loss=0.03391304239630699
I0210 06:10:33.707378 139836244932352 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.1729804277420044, loss=0.03749573975801468
I0210 06:11:06.397516 139813965014784 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.04397667571902275, loss=0.03329097107052803
I0210 06:11:38.322261 139836244932352 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.07967724651098251, loss=0.03272195905447006
I0210 06:11:54.482600 140039251117888 spec.py:321] Evaluating on the training split.
I0210 06:13:28.488391 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 06:13:31.952052 140039251117888 spec.py:349] Evaluating on the test split.
I0210 06:13:35.272603 140039251117888 submission_runner.py:408] Time since start: 22179.26s, 	Step: 47751, 	{'train/accuracy': 0.9892840385437012, 'train/loss': 0.036285195499658585, 'train/mean_average_precision': 0.2603987360262739, 'validation/accuracy': 0.9862515330314636, 'validation/loss': 0.04682980477809906, 'validation/mean_average_precision': 0.21119415249727339, 'validation/num_examples': 43793, 'test/accuracy': 0.9853697419166565, 'test/loss': 0.04973635822534561, 'test/mean_average_precision': 0.20645110365449323, 'test/num_examples': 43793, 'score': 15378.110914945602, 'total_duration': 22179.26458454132, 'accumulated_submission_time': 15378.110914945602, 'accumulated_eval_time': 6797.390934467316, 'accumulated_logging_time': 2.4604079723358154}
I0210 06:13:35.302957 139818172847872 logging_writer.py:48] [47751] accumulated_eval_time=6797.390934, accumulated_logging_time=2.460408, accumulated_submission_time=15378.110915, global_step=47751, preemption_count=0, score=15378.110915, test/accuracy=0.985370, test/loss=0.049736, test/mean_average_precision=0.206451, test/num_examples=43793, total_duration=22179.264585, train/accuracy=0.989284, train/loss=0.036285, train/mean_average_precision=0.260399, validation/accuracy=0.986252, validation/loss=0.046830, validation/mean_average_precision=0.211194, validation/num_examples=43793
I0210 06:13:51.531348 139836634785536 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.0756048932671547, loss=0.03943512216210365
I0210 06:14:23.616177 139818172847872 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.07020612806081772, loss=0.032839540392160416
I0210 06:14:55.820864 139836634785536 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.16628926992416382, loss=0.03927531838417053
I0210 06:15:28.002259 139818172847872 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.09600493311882019, loss=0.037147387862205505
I0210 06:16:00.063588 139836634785536 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.07023460417985916, loss=0.03657062351703644
I0210 06:16:32.461796 139818172847872 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.06939686834812164, loss=0.037374380975961685
I0210 06:17:04.976719 139836634785536 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.048565853387117386, loss=0.0354742631316185
I0210 06:17:35.544985 140039251117888 spec.py:321] Evaluating on the training split.
I0210 06:19:10.203546 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 06:19:13.218351 140039251117888 spec.py:349] Evaluating on the test split.
I0210 06:19:16.233368 140039251117888 submission_runner.py:408] Time since start: 22520.23s, 	Step: 48497, 	{'train/accuracy': 0.9894214272499084, 'train/loss': 0.03587300330400467, 'train/mean_average_precision': 0.2551420798235866, 'validation/accuracy': 0.9860554933547974, 'validation/loss': 0.046592436730861664, 'validation/mean_average_precision': 0.21204854440704776, 'validation/num_examples': 43793, 'test/accuracy': 0.9850791692733765, 'test/loss': 0.049412429332733154, 'test/mean_average_precision': 0.20297205695318776, 'test/num_examples': 43793, 'score': 15618.3207821846, 'total_duration': 22520.22537112236, 'accumulated_submission_time': 15618.3207821846, 'accumulated_eval_time': 6898.0792760849, 'accumulated_logging_time': 2.502638578414917}
I0210 06:19:16.257754 139836244932352 logging_writer.py:48] [48497] accumulated_eval_time=6898.079276, accumulated_logging_time=2.502639, accumulated_submission_time=15618.320782, global_step=48497, preemption_count=0, score=15618.320782, test/accuracy=0.985079, test/loss=0.049412, test/mean_average_precision=0.202972, test/num_examples=43793, total_duration=22520.225371, train/accuracy=0.989421, train/loss=0.035873, train/mean_average_precision=0.255142, validation/accuracy=0.986055, validation/loss=0.046592, validation/mean_average_precision=0.212049, validation/num_examples=43793
I0210 06:19:17.642704 139878398215936 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.04860435798764229, loss=0.038393471390008926
I0210 06:19:50.003297 139836244932352 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.052393898367881775, loss=0.02984033152461052
I0210 06:20:22.126409 139878398215936 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.12653912603855133, loss=0.034621503204107285
I0210 06:20:53.512524 139836244932352 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.0559626929461956, loss=0.032816413789987564
I0210 06:21:25.343451 139878398215936 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.062005169689655304, loss=0.03917335718870163
I0210 06:21:57.228291 139836244932352 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.0960460677742958, loss=0.037781719118356705
I0210 06:22:29.056910 139878398215936 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.06844121217727661, loss=0.03712649643421173
I0210 06:23:00.609290 139836244932352 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.08309043198823929, loss=0.03874998167157173
I0210 06:23:16.263577 140039251117888 spec.py:321] Evaluating on the training split.
I0210 06:24:52.769604 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 06:24:55.835392 140039251117888 spec.py:349] Evaluating on the test split.
I0210 06:24:58.798356 140039251117888 submission_runner.py:408] Time since start: 22862.79s, 	Step: 49250, 	{'train/accuracy': 0.9894852638244629, 'train/loss': 0.035724472254514694, 'train/mean_average_precision': 0.26698095934131594, 'validation/accuracy': 0.9862223267555237, 'validation/loss': 0.046513985842466354, 'validation/mean_average_precision': 0.2131052753435941, 'validation/num_examples': 43793, 'test/accuracy': 0.9852433800697327, 'test/loss': 0.04969567805528641, 'test/mean_average_precision': 0.20034394022746574, 'test/num_examples': 43793, 'score': 15858.295699596405, 'total_duration': 22862.790357112885, 'accumulated_submission_time': 15858.295699596405, 'accumulated_eval_time': 7000.6140151023865, 'accumulated_logging_time': 2.537923812866211}
I0210 06:24:58.823362 139813965014784 logging_writer.py:48] [49250] accumulated_eval_time=7000.614015, accumulated_logging_time=2.537924, accumulated_submission_time=15858.295700, global_step=49250, preemption_count=0, score=15858.295700, test/accuracy=0.985243, test/loss=0.049696, test/mean_average_precision=0.200344, test/num_examples=43793, total_duration=22862.790357, train/accuracy=0.989485, train/loss=0.035724, train/mean_average_precision=0.266981, validation/accuracy=0.986222, validation/loss=0.046514, validation/mean_average_precision=0.213105, validation/num_examples=43793
I0210 06:25:15.189671 139836634785536 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.07842011749744415, loss=0.03414367884397507
I0210 06:25:47.073470 139813965014784 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.06718012690544128, loss=0.037369173020124435
I0210 06:26:19.320921 139836634785536 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.0649721547961235, loss=0.03648693114519119
I0210 06:26:51.257490 139813965014784 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.0653960108757019, loss=0.03268684819340706
I0210 06:27:23.357079 139836634785536 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.07686462253332138, loss=0.03589686378836632
I0210 06:27:55.090549 139813965014784 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.060217779129743576, loss=0.03426332399249077
I0210 06:28:27.248494 139836634785536 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.0685127004981041, loss=0.03655441477894783
I0210 06:28:58.801071 140039251117888 spec.py:321] Evaluating on the training split.
I0210 06:30:33.715532 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 06:30:36.806213 140039251117888 spec.py:349] Evaluating on the test split.
I0210 06:30:39.843079 140039251117888 submission_runner.py:408] Time since start: 23203.84s, 	Step: 50000, 	{'train/accuracy': 0.9894618391990662, 'train/loss': 0.03557528182864189, 'train/mean_average_precision': 0.2711189138805558, 'validation/accuracy': 0.9861240983009338, 'validation/loss': 0.04661322012543678, 'validation/mean_average_precision': 0.20959399077861243, 'validation/num_examples': 43793, 'test/accuracy': 0.9852758646011353, 'test/loss': 0.04927290603518486, 'test/mean_average_precision': 0.20343352830175473, 'test/num_examples': 43793, 'score': 16098.242151737213, 'total_duration': 23203.83507847786, 'accumulated_submission_time': 16098.242151737213, 'accumulated_eval_time': 7101.655977487564, 'accumulated_logging_time': 2.5741710662841797}
I0210 06:30:39.868879 139818172847872 logging_writer.py:48] [50000] accumulated_eval_time=7101.655977, accumulated_logging_time=2.574171, accumulated_submission_time=16098.242152, global_step=50000, preemption_count=0, score=16098.242152, test/accuracy=0.985276, test/loss=0.049273, test/mean_average_precision=0.203434, test/num_examples=43793, total_duration=23203.835078, train/accuracy=0.989462, train/loss=0.035575, train/mean_average_precision=0.271119, validation/accuracy=0.986124, validation/loss=0.046613, validation/mean_average_precision=0.209594, validation/num_examples=43793
I0210 06:30:40.222718 139836244932352 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.06058753281831741, loss=0.027720050886273384
I0210 06:31:12.706288 139818172847872 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.08969902247190475, loss=0.03629429638385773
I0210 06:31:44.738842 139836244932352 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.10729209333658218, loss=0.033578742295503616
I0210 06:32:17.163269 139818172847872 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.10614249855279922, loss=0.035329997539520264
I0210 06:32:49.633374 139836244932352 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.07282263785600662, loss=0.03177133575081825
I0210 06:33:21.749490 139818172847872 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.05529790744185448, loss=0.034276414662599564
I0210 06:33:53.295403 139836244932352 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.07160060852766037, loss=0.03258746489882469
I0210 06:34:25.296763 139818172847872 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.06298858672380447, loss=0.040379125624895096
I0210 06:34:39.997421 140039251117888 spec.py:321] Evaluating on the training split.
I0210 06:36:18.934427 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 06:36:22.092844 140039251117888 spec.py:349] Evaluating on the test split.
I0210 06:36:25.187335 140039251117888 submission_runner.py:408] Time since start: 23549.18s, 	Step: 50747, 	{'train/accuracy': 0.9896489381790161, 'train/loss': 0.03502275422215462, 'train/mean_average_precision': 0.2890718598209157, 'validation/accuracy': 0.9863201379776001, 'validation/loss': 0.04624200612306595, 'validation/mean_average_precision': 0.21238210352457706, 'validation/num_examples': 43793, 'test/accuracy': 0.9854114651679993, 'test/loss': 0.04919213801622391, 'test/mean_average_precision': 0.20622264976099486, 'test/num_examples': 43793, 'score': 16338.338171482086, 'total_duration': 23549.17933702469, 'accumulated_submission_time': 16338.338171482086, 'accumulated_eval_time': 7206.845845937729, 'accumulated_logging_time': 2.612422466278076}
I0210 06:36:25.212279 139813965014784 logging_writer.py:48] [50747] accumulated_eval_time=7206.845846, accumulated_logging_time=2.612422, accumulated_submission_time=16338.338171, global_step=50747, preemption_count=0, score=16338.338171, test/accuracy=0.985411, test/loss=0.049192, test/mean_average_precision=0.206223, test/num_examples=43793, total_duration=23549.179337, train/accuracy=0.989649, train/loss=0.035023, train/mean_average_precision=0.289072, validation/accuracy=0.986320, validation/loss=0.046242, validation/mean_average_precision=0.212382, validation/num_examples=43793
I0210 06:36:42.638489 139878398215936 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.08001730591058731, loss=0.03720100596547127
I0210 06:37:14.807285 139813965014784 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.11254017055034637, loss=0.03763340041041374
I0210 06:37:46.810073 139878398215936 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.057173434644937515, loss=0.03438844159245491
I0210 06:38:18.948520 139813965014784 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.0746929943561554, loss=0.03298826515674591
I0210 06:38:50.851406 139878398215936 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.089093878865242, loss=0.033151883631944656
I0210 06:39:23.001265 139813965014784 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.05967710167169571, loss=0.03504286706447601
I0210 06:39:54.701245 139878398215936 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.05009303241968155, loss=0.033366672694683075
I0210 06:40:25.429900 140039251117888 spec.py:321] Evaluating on the training split.
I0210 06:42:00.134438 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 06:42:03.475662 140039251117888 spec.py:349] Evaluating on the test split.
I0210 06:42:06.583198 140039251117888 submission_runner.py:408] Time since start: 23890.58s, 	Step: 51497, 	{'train/accuracy': 0.9896401762962341, 'train/loss': 0.03489775210618973, 'train/mean_average_precision': 0.27522383903813774, 'validation/accuracy': 0.9863294959068298, 'validation/loss': 0.04617808014154434, 'validation/mean_average_precision': 0.21516065789567526, 'validation/num_examples': 43793, 'test/accuracy': 0.9853802919387817, 'test/loss': 0.04909180849790573, 'test/mean_average_precision': 0.2061251770082258, 'test/num_examples': 43793, 'score': 16578.522582292557, 'total_duration': 23890.57517528534, 'accumulated_submission_time': 16578.522582292557, 'accumulated_eval_time': 7307.999075889587, 'accumulated_logging_time': 2.64997935295105}
I0210 06:42:06.609371 139836244932352 logging_writer.py:48] [51497] accumulated_eval_time=7307.999076, accumulated_logging_time=2.649979, accumulated_submission_time=16578.522582, global_step=51497, preemption_count=0, score=16578.522582, test/accuracy=0.985380, test/loss=0.049092, test/mean_average_precision=0.206125, test/num_examples=43793, total_duration=23890.575175, train/accuracy=0.989640, train/loss=0.034898, train/mean_average_precision=0.275224, validation/accuracy=0.986329, validation/loss=0.046178, validation/mean_average_precision=0.215161, validation/num_examples=43793
I0210 06:42:08.012065 139836634785536 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.07599084824323654, loss=0.033782828599214554
I0210 06:42:41.049772 139836244932352 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.08023993670940399, loss=0.0339607410132885
I0210 06:43:13.816003 139836634785536 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.055352404713630676, loss=0.0375012569129467
I0210 06:43:45.999711 139836244932352 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.050944119691848755, loss=0.036002546548843384
I0210 06:44:18.462659 139836634785536 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.09463013708591461, loss=0.03782745450735092
I0210 06:44:50.889270 139836244932352 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.08817432820796967, loss=0.03215276077389717
I0210 06:45:22.786206 139836634785536 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.13773280382156372, loss=0.03723777458071709
I0210 06:45:54.788561 139836244932352 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.06975062191486359, loss=0.03319224342703819
I0210 06:46:06.775659 140039251117888 spec.py:321] Evaluating on the training split.
I0210 06:47:39.774088 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 06:47:42.907468 140039251117888 spec.py:349] Evaluating on the test split.
I0210 06:47:45.977692 140039251117888 submission_runner.py:408] Time since start: 24229.97s, 	Step: 52238, 	{'train/accuracy': 0.9896869659423828, 'train/loss': 0.03474186733365059, 'train/mean_average_precision': 0.29000266580794, 'validation/accuracy': 0.9863424897193909, 'validation/loss': 0.046252839267253876, 'validation/mean_average_precision': 0.21700427765967875, 'validation/num_examples': 43793, 'test/accuracy': 0.9854455590248108, 'test/loss': 0.04912613704800606, 'test/mean_average_precision': 0.2123704547099925, 'test/num_examples': 43793, 'score': 16818.65753221512, 'total_duration': 24229.96969652176, 'accumulated_submission_time': 16818.65753221512, 'accumulated_eval_time': 7407.201065540314, 'accumulated_logging_time': 2.6873366832733154}
I0210 06:47:46.003271 139818172847872 logging_writer.py:48] [52238] accumulated_eval_time=7407.201066, accumulated_logging_time=2.687337, accumulated_submission_time=16818.657532, global_step=52238, preemption_count=0, score=16818.657532, test/accuracy=0.985446, test/loss=0.049126, test/mean_average_precision=0.212370, test/num_examples=43793, total_duration=24229.969697, train/accuracy=0.989687, train/loss=0.034742, train/mean_average_precision=0.290003, validation/accuracy=0.986342, validation/loss=0.046253, validation/mean_average_precision=0.217004, validation/num_examples=43793
I0210 06:48:06.508751 139878398215936 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.09272966533899307, loss=0.037122227251529694
I0210 06:48:38.699435 139818172847872 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.08044643700122833, loss=0.03174000233411789
I0210 06:49:11.221785 139878398215936 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.06539487093687057, loss=0.03530970215797424
I0210 06:49:43.875671 139818172847872 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.07069087773561478, loss=0.032963208854198456
I0210 06:50:16.827943 139878398215936 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.1274966597557068, loss=0.03776438161730766
I0210 06:50:49.497263 139818172847872 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.0969255268573761, loss=0.035002823919057846
I0210 06:51:22.003523 139878398215936 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.06067349761724472, loss=0.029215430840849876
I0210 06:51:46.164284 140039251117888 spec.py:321] Evaluating on the training split.
I0210 06:53:22.147659 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 06:53:25.303360 140039251117888 spec.py:349] Evaluating on the test split.
I0210 06:53:28.405041 140039251117888 submission_runner.py:408] Time since start: 24572.40s, 	Step: 52977, 	{'train/accuracy': 0.9897372126579285, 'train/loss': 0.03438626602292061, 'train/mean_average_precision': 0.2859971482991346, 'validation/accuracy': 0.986326277256012, 'validation/loss': 0.04607265815138817, 'validation/mean_average_precision': 0.2227344429919973, 'validation/num_examples': 43793, 'test/accuracy': 0.985417366027832, 'test/loss': 0.04900296777486801, 'test/mean_average_precision': 0.21153648323356808, 'test/num_examples': 43793, 'score': 17058.785735607147, 'total_duration': 24572.39704155922, 'accumulated_submission_time': 17058.785735607147, 'accumulated_eval_time': 7509.441777706146, 'accumulated_logging_time': 2.724439859390259}
I0210 06:53:28.430320 139813965014784 logging_writer.py:48] [52977] accumulated_eval_time=7509.441778, accumulated_logging_time=2.724440, accumulated_submission_time=17058.785736, global_step=52977, preemption_count=0, score=17058.785736, test/accuracy=0.985417, test/loss=0.049003, test/mean_average_precision=0.211536, test/num_examples=43793, total_duration=24572.397042, train/accuracy=0.989737, train/loss=0.034386, train/mean_average_precision=0.285997, validation/accuracy=0.986326, validation/loss=0.046073, validation/mean_average_precision=0.222734, validation/num_examples=43793
I0210 06:53:36.159026 139836244932352 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.0950109213590622, loss=0.034230977296829224
I0210 06:54:08.363480 139813965014784 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.08854492753744125, loss=0.0379009023308754
I0210 06:54:40.217998 139836244932352 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.06481269001960754, loss=0.03721896559000015
I0210 06:55:12.218504 139813965014784 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.06378017365932465, loss=0.0328652523458004
I0210 06:55:43.856140 139836244932352 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.055913474410772324, loss=0.034872423857450485
I0210 06:56:15.637941 139813965014784 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.05432094261050224, loss=0.035837575793266296
I0210 06:56:47.238958 139836244932352 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.08498241007328033, loss=0.035491976886987686
I0210 06:57:18.715394 139813965014784 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.11370263993740082, loss=0.03333551064133644
I0210 06:57:28.594894 140039251117888 spec.py:321] Evaluating on the training split.
I0210 06:59:05.190604 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 06:59:08.207759 140039251117888 spec.py:349] Evaluating on the test split.
I0210 06:59:11.211051 140039251117888 submission_runner.py:408] Time since start: 24915.20s, 	Step: 53732, 	{'train/accuracy': 0.989798903465271, 'train/loss': 0.03463223949074745, 'train/mean_average_precision': 0.27364324378520405, 'validation/accuracy': 0.986401379108429, 'validation/loss': 0.04586976394057274, 'validation/mean_average_precision': 0.21815914793114188, 'validation/num_examples': 43793, 'test/accuracy': 0.9854910969734192, 'test/loss': 0.04870198667049408, 'test/mean_average_precision': 0.2097780509319448, 'test/num_examples': 43793, 'score': 17298.919110774994, 'total_duration': 24915.203050851822, 'accumulated_submission_time': 17298.919110774994, 'accumulated_eval_time': 7612.057886600494, 'accumulated_logging_time': 2.76061749458313}
I0210 06:59:11.236994 139818172847872 logging_writer.py:48] [53732] accumulated_eval_time=7612.057887, accumulated_logging_time=2.760617, accumulated_submission_time=17298.919111, global_step=53732, preemption_count=0, score=17298.919111, test/accuracy=0.985491, test/loss=0.048702, test/mean_average_precision=0.209778, test/num_examples=43793, total_duration=24915.203051, train/accuracy=0.989799, train/loss=0.034632, train/mean_average_precision=0.273643, validation/accuracy=0.986401, validation/loss=0.045870, validation/mean_average_precision=0.218159, validation/num_examples=43793
I0210 06:59:33.005410 139836634785536 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.09704606980085373, loss=0.03630023077130318
I0210 07:00:04.931908 139818172847872 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.10870366543531418, loss=0.02737039513885975
I0210 07:00:36.497221 139836634785536 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.08019307255744934, loss=0.03500734642148018
I0210 07:01:07.967670 139818172847872 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.07560815662145615, loss=0.03449796885251999
I0210 07:01:39.385965 139836634785536 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.07935276627540588, loss=0.031168783083558083
I0210 07:02:11.130484 139818172847872 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.07001659274101257, loss=0.03703630343079567
I0210 07:02:42.379272 139836634785536 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.06722618639469147, loss=0.03307920694351196
I0210 07:03:11.311265 140039251117888 spec.py:321] Evaluating on the training split.
I0210 07:04:47.120544 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 07:04:50.180186 140039251117888 spec.py:349] Evaluating on the test split.
I0210 07:04:53.199865 140039251117888 submission_runner.py:408] Time since start: 25257.19s, 	Step: 54493, 	{'train/accuracy': 0.989654541015625, 'train/loss': 0.03495398908853531, 'train/mean_average_precision': 0.2809974503744279, 'validation/accuracy': 0.9862645268440247, 'validation/loss': 0.04621202498674393, 'validation/mean_average_precision': 0.21551536423348336, 'validation/num_examples': 43793, 'test/accuracy': 0.9853390455245972, 'test/loss': 0.04909416660666466, 'test/mean_average_precision': 0.2090533931014886, 'test/num_examples': 43793, 'score': 17538.96286535263, 'total_duration': 25257.191864728928, 'accumulated_submission_time': 17538.96286535263, 'accumulated_eval_time': 7713.9464428424835, 'accumulated_logging_time': 2.797553777694702}
I0210 07:04:53.226139 139813965014784 logging_writer.py:48] [54493] accumulated_eval_time=7713.946443, accumulated_logging_time=2.797554, accumulated_submission_time=17538.962865, global_step=54493, preemption_count=0, score=17538.962865, test/accuracy=0.985339, test/loss=0.049094, test/mean_average_precision=0.209053, test/num_examples=43793, total_duration=25257.191865, train/accuracy=0.989655, train/loss=0.034954, train/mean_average_precision=0.280997, validation/accuracy=0.986265, validation/loss=0.046212, validation/mean_average_precision=0.215515, validation/num_examples=43793
I0210 07:04:55.822713 139878398215936 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.0694117397069931, loss=0.03408876433968544
I0210 07:05:27.740971 139813965014784 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.06261445581912994, loss=0.034574393182992935
I0210 07:05:59.694267 139878398215936 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.08330313861370087, loss=0.037083156406879425
I0210 07:06:31.513003 139813965014784 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.06366746872663498, loss=0.030902786180377007
I0210 07:07:03.379002 139878398215936 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.10165763646364212, loss=0.03268846869468689
I0210 07:07:35.115361 139813965014784 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.08315710723400116, loss=0.03629938140511513
I0210 07:08:06.665002 139878398215936 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.0904214084148407, loss=0.036745574325323105
I0210 07:08:38.117179 139813965014784 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.12374312430620193, loss=0.03616127744317055
I0210 07:08:53.368251 140039251117888 spec.py:321] Evaluating on the training split.
I0210 07:10:28.317819 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 07:10:31.746964 140039251117888 spec.py:349] Evaluating on the test split.
I0210 07:10:35.122887 140039251117888 submission_runner.py:408] Time since start: 25599.11s, 	Step: 55249, 	{'train/accuracy': 0.989753007888794, 'train/loss': 0.034596964716911316, 'train/mean_average_precision': 0.28548771479706025, 'validation/accuracy': 0.9863563179969788, 'validation/loss': 0.04587423428893089, 'validation/mean_average_precision': 0.2150310554975685, 'validation/num_examples': 43793, 'test/accuracy': 0.9854329228401184, 'test/loss': 0.04876290634274483, 'test/mean_average_precision': 0.2074281320406993, 'test/num_examples': 43793, 'score': 17779.07443547249, 'total_duration': 25599.114872932434, 'accumulated_submission_time': 17779.07443547249, 'accumulated_eval_time': 7815.701034784317, 'accumulated_logging_time': 2.8346564769744873}
I0210 07:10:35.152966 139818172847872 logging_writer.py:48] [55249] accumulated_eval_time=7815.701035, accumulated_logging_time=2.834656, accumulated_submission_time=17779.074435, global_step=55249, preemption_count=0, score=17779.074435, test/accuracy=0.985433, test/loss=0.048763, test/mean_average_precision=0.207428, test/num_examples=43793, total_duration=25599.114873, train/accuracy=0.989753, train/loss=0.034597, train/mean_average_precision=0.285488, validation/accuracy=0.986356, validation/loss=0.045874, validation/mean_average_precision=0.215031, validation/num_examples=43793
I0210 07:10:52.009083 139836634785536 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.06658110022544861, loss=0.03579074144363403
I0210 07:11:24.757821 139818172847872 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.09642958641052246, loss=0.034211624413728714
I0210 07:11:57.167065 139836634785536 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.09999147057533264, loss=0.03592894971370697
I0210 07:12:29.848062 139818172847872 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.13514332473278046, loss=0.03724674880504608
I0210 07:13:02.535379 139836634785536 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.05256573110818863, loss=0.03561941161751747
I0210 07:13:34.704442 139818172847872 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.06922447681427002, loss=0.033275991678237915
I0210 07:14:06.844256 139836634785536 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.07371141016483307, loss=0.03863503783941269
I0210 07:14:35.341114 140039251117888 spec.py:321] Evaluating on the training split.
I0210 07:16:09.459870 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 07:16:12.862468 140039251117888 spec.py:349] Evaluating on the test split.
I0210 07:16:16.174910 140039251117888 submission_runner.py:408] Time since start: 25940.17s, 	Step: 55990, 	{'train/accuracy': 0.9899150133132935, 'train/loss': 0.03416066989302635, 'train/mean_average_precision': 0.29227628198369915, 'validation/accuracy': 0.9864853620529175, 'validation/loss': 0.04559401422739029, 'validation/mean_average_precision': 0.22398801953629738, 'validation/num_examples': 43793, 'test/accuracy': 0.9854813814163208, 'test/loss': 0.0485040582716465, 'test/mean_average_precision': 0.21495641242022187, 'test/num_examples': 43793, 'score': 18019.22664308548, 'total_duration': 25940.16689515114, 'accumulated_submission_time': 18019.22664308548, 'accumulated_eval_time': 7916.534770727158, 'accumulated_logging_time': 2.8772406578063965}
I0210 07:16:16.203386 139813965014784 logging_writer.py:48] [55990] accumulated_eval_time=7916.534771, accumulated_logging_time=2.877241, accumulated_submission_time=18019.226643, global_step=55990, preemption_count=0, score=18019.226643, test/accuracy=0.985481, test/loss=0.048504, test/mean_average_precision=0.214956, test/num_examples=43793, total_duration=25940.166895, train/accuracy=0.989915, train/loss=0.034161, train/mean_average_precision=0.292276, validation/accuracy=0.986485, validation/loss=0.045594, validation/mean_average_precision=0.223988, validation/num_examples=43793
I0210 07:16:19.818978 139836244932352 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.08505921810865402, loss=0.03328893333673477
I0210 07:16:52.197822 139813965014784 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.10222753137350082, loss=0.033131636679172516
I0210 07:17:24.638227 139836244932352 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.071297287940979, loss=0.03460675850510597
I0210 07:17:56.334361 139813965014784 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.10687167942523956, loss=0.03280896693468094
I0210 07:18:28.189569 139836244932352 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.10390359163284302, loss=0.03635328635573387
I0210 07:19:00.012112 139813965014784 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.08734994381666183, loss=0.03402460739016533
I0210 07:19:32.212603 139836244932352 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.0778002068400383, loss=0.03486793488264084
I0210 07:20:04.093620 139813965014784 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.08225388824939728, loss=0.03385255113244057
I0210 07:20:16.180723 140039251117888 spec.py:321] Evaluating on the training split.
I0210 07:21:55.071882 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 07:21:58.178977 140039251117888 spec.py:349] Evaluating on the test split.
I0210 07:22:01.137678 140039251117888 submission_runner.py:408] Time since start: 26285.13s, 	Step: 56739, 	{'train/accuracy': 0.9898921847343445, 'train/loss': 0.03401986509561539, 'train/mean_average_precision': 0.29999873610666594, 'validation/accuracy': 0.986504077911377, 'validation/loss': 0.04590983688831329, 'validation/mean_average_precision': 0.2238110699225643, 'validation/num_examples': 43793, 'test/accuracy': 0.9855051636695862, 'test/loss': 0.048889946192502975, 'test/mean_average_precision': 0.21376523695951066, 'test/num_examples': 43793, 'score': 18259.172052145004, 'total_duration': 26285.1296813488, 'accumulated_submission_time': 18259.172052145004, 'accumulated_eval_time': 8021.491682767868, 'accumulated_logging_time': 2.9176416397094727}
I0210 07:22:01.164139 139818172847872 logging_writer.py:48] [56739] accumulated_eval_time=8021.491683, accumulated_logging_time=2.917642, accumulated_submission_time=18259.172052, global_step=56739, preemption_count=0, score=18259.172052, test/accuracy=0.985505, test/loss=0.048890, test/mean_average_precision=0.213765, test/num_examples=43793, total_duration=26285.129681, train/accuracy=0.989892, train/loss=0.034020, train/mean_average_precision=0.299999, validation/accuracy=0.986504, validation/loss=0.045910, validation/mean_average_precision=0.223811, validation/num_examples=43793
I0210 07:22:21.257754 139878398215936 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.09756491333246231, loss=0.032365161925554276
I0210 07:22:53.704042 139818172847872 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.077852264046669, loss=0.03142505884170532
I0210 07:23:25.890830 139878398215936 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.11293758451938629, loss=0.03546258807182312
I0210 07:23:58.066419 139818172847872 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.1002490371465683, loss=0.035858239978551865
I0210 07:24:30.444762 139878398215936 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.1001734584569931, loss=0.036433883011341095
I0210 07:25:03.087724 139818172847872 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.11027485132217407, loss=0.03171241655945778
I0210 07:25:35.351469 139878398215936 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.09817148745059967, loss=0.030444616451859474
I0210 07:25:39.330314 139818172847872 logging_writer.py:48] [57413] global_step=57413, preemption_count=0, score=18477.291267
I0210 07:25:39.384444 140039251117888 checkpoints.py:490] Saving checkpoint at step: 57413
I0210 07:25:39.502836 140039251117888 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_3/ogbg_jax/trial_4/checkpoint_57413
I0210 07:25:39.504026 140039251117888 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_3/ogbg_jax/trial_4/checkpoint_57413.
I0210 07:25:39.675036 140039251117888 submission_runner.py:583] Tuning trial 4/5
I0210 07:25:39.675355 140039251117888 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.004958460849689891, one_minus_beta1=0.13625575743, beta2=0.6291854735396584, weight_decay=0.1147386261512052, warmup_factor=0.02)
I0210 07:25:39.679947 140039251117888 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5250684022903442, 'train/loss': 0.715176522731781, 'train/mean_average_precision': 0.023795002749508583, 'validation/accuracy': 0.5213832855224609, 'validation/loss': 0.7166012525558472, 'validation/mean_average_precision': 0.026141387117008964, 'validation/num_examples': 43793, 'test/accuracy': 0.5224818587303162, 'test/loss': 0.7161954641342163, 'test/mean_average_precision': 0.027845703932648375, 'test/num_examples': 43793, 'score': 12.220922231674194, 'total_duration': 120.41805958747864, 'accumulated_submission_time': 12.220922231674194, 'accumulated_eval_time': 108.19709205627441, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (742, {'train/accuracy': 0.9868351221084595, 'train/loss': 0.05353858694434166, 'train/mean_average_precision': 0.04192054947942697, 'validation/accuracy': 0.9841658473014832, 'validation/loss': 0.06382942944765091, 'validation/mean_average_precision': 0.04026482508837749, 'validation/num_examples': 43793, 'test/accuracy': 0.9831947088241577, 'test/loss': 0.06701404601335526, 'test/mean_average_precision': 0.04095081648056769, 'test/num_examples': 43793, 'score': 252.46294784545898, 'total_duration': 469.6144058704376, 'accumulated_submission_time': 252.46294784545898, 'accumulated_eval_time': 217.11013174057007, 'accumulated_logging_time': 0.021190404891967773, 'global_step': 742, 'preemption_count': 0}), (1482, {'train/accuracy': 0.9870707988739014, 'train/loss': 0.049404438585042953, 'train/mean_average_precision': 0.07272801721838304, 'validation/accuracy': 0.9842279553413391, 'validation/loss': 0.06012863293290138, 'validation/mean_average_precision': 0.06954331548862047, 'validation/num_examples': 43793, 'test/accuracy': 0.9832208156585693, 'test/loss': 0.06371833384037018, 'test/mean_average_precision': 0.06871401787448847, 'test/num_examples': 43793, 'score': 492.63631868362427, 'total_duration': 815.2754602432251, 'accumulated_submission_time': 492.63631868362427, 'accumulated_eval_time': 322.55107712745667, 'accumulated_logging_time': 0.0476987361907959, 'global_step': 1482, 'preemption_count': 0}), (2230, {'train/accuracy': 0.9873882532119751, 'train/loss': 0.046789973974227905, 'train/mean_average_precision': 0.10480282761530568, 'validation/accuracy': 0.9846578240394592, 'validation/loss': 0.05671948939561844, 'validation/mean_average_precision': 0.09975522593130466, 'validation/num_examples': 43793, 'test/accuracy': 0.9836769700050354, 'test/loss': 0.06019135192036629, 'test/mean_average_precision': 0.10440029994939186, 'test/num_examples': 43793, 'score': 732.8385767936707, 'total_duration': 1164.4848325252533, 'accumulated_submission_time': 732.8385767936707, 'accumulated_eval_time': 431.51141595840454, 'accumulated_logging_time': 0.07473325729370117, 'global_step': 2230, 'preemption_count': 0}), (2978, {'train/accuracy': 0.9875863790512085, 'train/loss': 0.04446644335985184, 'train/mean_average_precision': 0.13305032045933401, 'validation/accuracy': 0.9849082827568054, 'validation/loss': 0.0534515306353569, 'validation/mean_average_precision': 0.12068096396737814, 'validation/num_examples': 43793, 'test/accuracy': 0.9839507341384888, 'test/loss': 0.0563553124666214, 'test/mean_average_precision': 0.1253405511373911, 'test/num_examples': 43793, 'score': 972.8108472824097, 'total_duration': 1509.5015892982483, 'accumulated_submission_time': 972.8108472824097, 'accumulated_eval_time': 536.5080606937408, 'accumulated_logging_time': 0.1023402214050293, 'global_step': 2978, 'preemption_count': 0}), (3728, {'train/accuracy': 0.9877572655677795, 'train/loss': 0.04308393970131874, 'train/mean_average_precision': 0.1463087512056293, 'validation/accuracy': 0.9850422739982605, 'validation/loss': 0.05267150700092316, 'validation/mean_average_precision': 0.12456385688350158, 'validation/num_examples': 43793, 'test/accuracy': 0.9840299487113953, 'test/loss': 0.05582546442747116, 'test/mean_average_precision': 0.12580866930699636, 'test/num_examples': 43793, 'score': 1212.889556646347, 'total_duration': 1859.0164363384247, 'accumulated_submission_time': 1212.889556646347, 'accumulated_eval_time': 645.8973081111908, 'accumulated_logging_time': 0.1297438144683838, 'global_step': 3728, 'preemption_count': 0}), (4478, {'train/accuracy': 0.9880750179290771, 'train/loss': 0.04148411378264427, 'train/mean_average_precision': 0.1616528893787325, 'validation/accuracy': 0.985185980796814, 'validation/loss': 0.051431361585855484, 'validation/mean_average_precision': 0.144405763800987, 'validation/num_examples': 43793, 'test/accuracy': 0.9842784404754639, 'test/loss': 0.054719991981983185, 'test/mean_average_precision': 0.14370305841595316, 'test/num_examples': 43793, 'score': 1452.92014336586, 'total_duration': 2199.8714084625244, 'accumulated_submission_time': 1452.92014336586, 'accumulated_eval_time': 746.6748743057251, 'accumulated_logging_time': 0.15679001808166504, 'global_step': 4478, 'preemption_count': 0}), (5222, {'train/accuracy': 0.9881662726402283, 'train/loss': 0.04132499173283577, 'train/mean_average_precision': 0.1795573077062224, 'validation/accuracy': 0.9852378964424133, 'validation/loss': 0.050925739109516144, 'validation/mean_average_precision': 0.15389470149995021, 'validation/num_examples': 43793, 'test/accuracy': 0.9843176007270813, 'test/loss': 0.05375967174768448, 'test/mean_average_precision': 0.1524632543170833, 'test/num_examples': 43793, 'score': 1692.9912858009338, 'total_duration': 2544.843962907791, 'accumulated_submission_time': 1692.9912858009338, 'accumulated_eval_time': 851.5282611846924, 'accumulated_logging_time': 0.18505358695983887, 'global_step': 5222, 'preemption_count': 0}), (5973, {'train/accuracy': 0.9882667064666748, 'train/loss': 0.04118207469582558, 'train/mean_average_precision': 0.16472526914151106, 'validation/accuracy': 0.9852712154388428, 'validation/loss': 0.05083019286394119, 'validation/mean_average_precision': 0.1485142866215823, 'validation/num_examples': 43793, 'test/accuracy': 0.9843466877937317, 'test/loss': 0.05360308662056923, 'test/mean_average_precision': 0.15322191213047312, 'test/num_examples': 43793, 'score': 1933.113857269287, 'total_duration': 2889.129508972168, 'accumulated_submission_time': 1933.113857269287, 'accumulated_eval_time': 955.6449084281921, 'accumulated_logging_time': 0.21189594268798828, 'global_step': 5973, 'preemption_count': 0}), (6726, {'train/accuracy': 0.9882901310920715, 'train/loss': 0.040386952459812164, 'train/mean_average_precision': 0.18939021860761607, 'validation/accuracy': 0.9854116439819336, 'validation/loss': 0.05021588131785393, 'validation/mean_average_precision': 0.1654686194838573, 'validation/num_examples': 43793, 'test/accuracy': 0.9844532608985901, 'test/loss': 0.053245652467012405, 'test/mean_average_precision': 0.16650507086139726, 'test/num_examples': 43793, 'score': 2173.093494415283, 'total_duration': 3229.888239145279, 'accumulated_submission_time': 2173.093494415283, 'accumulated_eval_time': 1056.376545906067, 'accumulated_logging_time': 0.2399752140045166, 'global_step': 6726, 'preemption_count': 0}), (7478, {'train/accuracy': 0.9882825613021851, 'train/loss': 0.04071137681603432, 'train/mean_average_precision': 0.1801941639881932, 'validation/accuracy': 0.9854328036308289, 'validation/loss': 0.049915019422769547, 'validation/mean_average_precision': 0.1604554826060077, 'validation/num_examples': 43793, 'test/accuracy': 0.9845025539398193, 'test/loss': 0.05256318673491478, 'test/mean_average_precision': 0.16147580187672952, 'test/num_examples': 43793, 'score': 2413.0617480278015, 'total_duration': 3574.727502822876, 'accumulated_submission_time': 2413.0617480278015, 'accumulated_eval_time': 1161.199847459793, 'accumulated_logging_time': 0.2680511474609375, 'global_step': 7478, 'preemption_count': 0}), (8227, {'train/accuracy': 0.9884076118469238, 'train/loss': 0.04007153585553169, 'train/mean_average_precision': 0.18910998079722016, 'validation/accuracy': 0.9855748414993286, 'validation/loss': 0.04978977516293526, 'validation/mean_average_precision': 0.1705180084490086, 'validation/num_examples': 43793, 'test/accuracy': 0.9846537113189697, 'test/loss': 0.052776869386434555, 'test/mean_average_precision': 0.16179037336754432, 'test/num_examples': 43793, 'score': 2653.143779039383, 'total_duration': 3916.7604858875275, 'accumulated_submission_time': 2653.143779039383, 'accumulated_eval_time': 1263.1020185947418, 'accumulated_logging_time': 0.2975277900695801, 'global_step': 8227, 'preemption_count': 0}), (8974, {'train/accuracy': 0.9886220097541809, 'train/loss': 0.039900485426187515, 'train/mean_average_precision': 0.1923071782334979, 'validation/accuracy': 0.9855508804321289, 'validation/loss': 0.04973052814602852, 'validation/mean_average_precision': 0.16555442965631947, 'validation/num_examples': 43793, 'test/accuracy': 0.9845378994941711, 'test/loss': 0.052378538995981216, 'test/mean_average_precision': 0.16635132712470502, 'test/num_examples': 43793, 'score': 2893.2505328655243, 'total_duration': 4262.32323884964, 'accumulated_submission_time': 2893.2505328655243, 'accumulated_eval_time': 1368.509529352188, 'accumulated_logging_time': 0.32607460021972656, 'global_step': 8974, 'preemption_count': 0}), (9727, {'train/accuracy': 0.9885628819465637, 'train/loss': 0.039701081812381744, 'train/mean_average_precision': 0.19980620975853122, 'validation/accuracy': 0.9855829477310181, 'validation/loss': 0.050064969807863235, 'validation/mean_average_precision': 0.17189206866310625, 'validation/num_examples': 43793, 'test/accuracy': 0.9846276044845581, 'test/loss': 0.05322679132223129, 'test/mean_average_precision': 0.1667354992957371, 'test/num_examples': 43793, 'score': 3133.2117640972137, 'total_duration': 4604.147631645203, 'accumulated_submission_time': 3133.2117640972137, 'accumulated_eval_time': 1470.3253271579742, 'accumulated_logging_time': 0.35387539863586426, 'global_step': 9727, 'preemption_count': 0}), (10476, {'train/accuracy': 0.9885133504867554, 'train/loss': 0.03942936658859253, 'train/mean_average_precision': 0.20045229589354108, 'validation/accuracy': 0.9854490160942078, 'validation/loss': 0.04966723546385765, 'validation/mean_average_precision': 0.1666263098371183, 'validation/num_examples': 43793, 'test/accuracy': 0.9844890236854553, 'test/loss': 0.052470430731773376, 'test/mean_average_precision': 0.16691585201323547, 'test/num_examples': 43793, 'score': 3373.1918189525604, 'total_duration': 4950.001894235611, 'accumulated_submission_time': 3373.1918189525604, 'accumulated_eval_time': 1576.149382352829, 'accumulated_logging_time': 0.38416481018066406, 'global_step': 10476, 'preemption_count': 0}), (11226, {'train/accuracy': 0.9886851906776428, 'train/loss': 0.03907548263669014, 'train/mean_average_precision': 0.20898945189280574, 'validation/accuracy': 0.9855192303657532, 'validation/loss': 0.049371156841516495, 'validation/mean_average_precision': 0.17491581954717367, 'validation/num_examples': 43793, 'test/accuracy': 0.9845783710479736, 'test/loss': 0.052384376525878906, 'test/mean_average_precision': 0.1724580365021767, 'test/num_examples': 43793, 'score': 3613.1764261722565, 'total_duration': 5293.536472797394, 'accumulated_submission_time': 3613.1764261722565, 'accumulated_eval_time': 1679.6482055187225, 'accumulated_logging_time': 0.41542959213256836, 'global_step': 11226, 'preemption_count': 0}), (11977, {'train/accuracy': 0.9885552525520325, 'train/loss': 0.039133235812187195, 'train/mean_average_precision': 0.20560365906791717, 'validation/accuracy': 0.9857209920883179, 'validation/loss': 0.04876871407032013, 'validation/mean_average_precision': 0.18172306654698742, 'validation/num_examples': 43793, 'test/accuracy': 0.9847741723060608, 'test/loss': 0.05155624821782112, 'test/mean_average_precision': 0.17483467443549086, 'test/num_examples': 43793, 'score': 3853.230010032654, 'total_duration': 5639.680479049683, 'accumulated_submission_time': 3853.230010032654, 'accumulated_eval_time': 1785.6903955936432, 'accumulated_logging_time': 0.4441530704498291, 'global_step': 11977, 'preemption_count': 0}), (12734, {'train/accuracy': 0.9885872602462769, 'train/loss': 0.03986997902393341, 'train/mean_average_precision': 0.2044354306931992, 'validation/accuracy': 0.9855306148529053, 'validation/loss': 0.049604810774326324, 'validation/mean_average_precision': 0.16906780951342024, 'validation/num_examples': 43793, 'test/accuracy': 0.9846027493476868, 'test/loss': 0.05225344002246857, 'test/mean_average_precision': 0.1657067585304924, 'test/num_examples': 43793, 'score': 4093.308432340622, 'total_duration': 5986.535817146301, 'accumulated_submission_time': 4093.308432340622, 'accumulated_eval_time': 1892.4185304641724, 'accumulated_logging_time': 0.4730062484741211, 'global_step': 12734, 'preemption_count': 0}), (13486, {'train/accuracy': 0.9886804819107056, 'train/loss': 0.03929399326443672, 'train/mean_average_precision': 0.20355045624948978, 'validation/accuracy': 0.9856122136116028, 'validation/loss': 0.04895460233092308, 'validation/mean_average_precision': 0.1798079085889805, 'validation/num_examples': 43793, 'test/accuracy': 0.9846764802932739, 'test/loss': 0.051828641444444656, 'test/mean_average_precision': 0.17550897646591945, 'test/num_examples': 43793, 'score': 4333.41264462471, 'total_duration': 6332.10172867775, 'accumulated_submission_time': 4333.41264462471, 'accumulated_eval_time': 1997.831268787384, 'accumulated_logging_time': 0.5020365715026855, 'global_step': 13486, 'preemption_count': 0}), (14228, {'train/accuracy': 0.9885827898979187, 'train/loss': 0.039349667727947235, 'train/mean_average_precision': 0.1989838706543882, 'validation/accuracy': 0.9857429265975952, 'validation/loss': 0.04903091862797737, 'validation/mean_average_precision': 0.1796446625647285, 'validation/num_examples': 43793, 'test/accuracy': 0.9847055673599243, 'test/loss': 0.05213354900479317, 'test/mean_average_precision': 0.1784581841483761, 'test/num_examples': 43793, 'score': 4573.500229597092, 'total_duration': 6678.234007120132, 'accumulated_submission_time': 4573.500229597092, 'accumulated_eval_time': 2103.8231523036957, 'accumulated_logging_time': 0.5319912433624268, 'global_step': 14228, 'preemption_count': 0}), (14978, {'train/accuracy': 0.9885103106498718, 'train/loss': 0.039425864815711975, 'train/mean_average_precision': 0.21031990673518336, 'validation/accuracy': 0.9857169389724731, 'validation/loss': 0.049071572721004486, 'validation/mean_average_precision': 0.18099567580130638, 'validation/num_examples': 43793, 'test/accuracy': 0.9847737550735474, 'test/loss': 0.051877040416002274, 'test/mean_average_precision': 0.1807104800860751, 'test/num_examples': 43793, 'score': 4813.4798884391785, 'total_duration': 7024.253228664398, 'accumulated_submission_time': 4813.4798884391785, 'accumulated_eval_time': 2209.81316947937, 'accumulated_logging_time': 0.5619616508483887, 'global_step': 14978, 'preemption_count': 0}), (15730, {'train/accuracy': 0.9885491132736206, 'train/loss': 0.03898508846759796, 'train/mean_average_precision': 0.20316993229019112, 'validation/accuracy': 0.985655665397644, 'validation/loss': 0.0487467497587204, 'validation/mean_average_precision': 0.18462462128448548, 'validation/num_examples': 43793, 'test/accuracy': 0.9846811294555664, 'test/loss': 0.0517447367310524, 'test/mean_average_precision': 0.17993505069326723, 'test/num_examples': 43793, 'score': 5053.459381818771, 'total_duration': 7364.3765437603, 'accumulated_submission_time': 5053.459381818771, 'accumulated_eval_time': 2309.904142856598, 'accumulated_logging_time': 0.5947067737579346, 'global_step': 15730, 'preemption_count': 0}), (16481, {'train/accuracy': 0.9886710047721863, 'train/loss': 0.03975606709718704, 'train/mean_average_precision': 0.20017878745560774, 'validation/accuracy': 0.9856000542640686, 'validation/loss': 0.04901471734046936, 'validation/mean_average_precision': 0.17581403247664895, 'validation/num_examples': 43793, 'test/accuracy': 0.9846815466880798, 'test/loss': 0.05170534551143646, 'test/mean_average_precision': 0.16965997870898855, 'test/num_examples': 43793, 'score': 5293.689656734467, 'total_duration': 7708.322807788849, 'accumulated_submission_time': 5293.689656734467, 'accumulated_eval_time': 2413.569778442383, 'accumulated_logging_time': 0.625511884689331, 'global_step': 16481, 'preemption_count': 0}), (17235, {'train/accuracy': 0.988852858543396, 'train/loss': 0.03818158060312271, 'train/mean_average_precision': 0.22472311091833175, 'validation/accuracy': 0.9858115315437317, 'validation/loss': 0.04819267988204956, 'validation/mean_average_precision': 0.19142163577378093, 'validation/num_examples': 43793, 'test/accuracy': 0.9848251342773438, 'test/loss': 0.051109109073877335, 'test/mean_average_precision': 0.17861484881648326, 'test/num_examples': 43793, 'score': 5533.651293992996, 'total_duration': 8051.120588302612, 'accumulated_submission_time': 5533.651293992996, 'accumulated_eval_time': 2516.3563482761383, 'accumulated_logging_time': 0.6553435325622559, 'global_step': 17235, 'preemption_count': 0}), (17987, {'train/accuracy': 0.9886994361877441, 'train/loss': 0.03855183348059654, 'train/mean_average_precision': 0.21943499417505946, 'validation/accuracy': 0.9855996370315552, 'validation/loss': 0.04912377893924713, 'validation/mean_average_precision': 0.17756952417811328, 'validation/num_examples': 43793, 'test/accuracy': 0.9846655130386353, 'test/loss': 0.05189542844891548, 'test/mean_average_precision': 0.17316708913025225, 'test/num_examples': 43793, 'score': 5773.6067237854, 'total_duration': 8394.634209156036, 'accumulated_submission_time': 5773.6067237854, 'accumulated_eval_time': 2619.8639080524445, 'accumulated_logging_time': 0.6865098476409912, 'global_step': 17987, 'preemption_count': 0}), (18728, {'train/accuracy': 0.9887787699699402, 'train/loss': 0.03834012150764465, 'train/mean_average_precision': 0.22422281877283778, 'validation/accuracy': 0.9857478141784668, 'validation/loss': 0.04909900575876236, 'validation/mean_average_precision': 0.18295043848358236, 'validation/num_examples': 43793, 'test/accuracy': 0.9847704172134399, 'test/loss': 0.0521637499332428, 'test/mean_average_precision': 0.17599197125757965, 'test/num_examples': 43793, 'score': 6013.775787115097, 'total_duration': 8743.120332956314, 'accumulated_submission_time': 6013.775787115097, 'accumulated_eval_time': 2728.1255388259888, 'accumulated_logging_time': 0.7215981483459473, 'global_step': 18728, 'preemption_count': 0}), (19480, {'train/accuracy': 0.988776445388794, 'train/loss': 0.03847422078251839, 'train/mean_average_precision': 0.22298679010206465, 'validation/accuracy': 0.9857891798019409, 'validation/loss': 0.04893574118614197, 'validation/mean_average_precision': 0.18490211072678567, 'validation/num_examples': 43793, 'test/accuracy': 0.9848179817199707, 'test/loss': 0.05209505558013916, 'test/mean_average_precision': 0.1765644162673381, 'test/num_examples': 43793, 'score': 6253.961903810501, 'total_duration': 9087.911134004593, 'accumulated_submission_time': 6253.961903810501, 'accumulated_eval_time': 2832.679259777069, 'accumulated_logging_time': 0.7523925304412842, 'global_step': 19480, 'preemption_count': 0}), (20226, {'train/accuracy': 0.9885373115539551, 'train/loss': 0.03903643786907196, 'train/mean_average_precision': 0.21376707881150128, 'validation/accuracy': 0.9857355952262878, 'validation/loss': 0.04907619208097458, 'validation/mean_average_precision': 0.1825315784667147, 'validation/num_examples': 43793, 'test/accuracy': 0.9848251342773438, 'test/loss': 0.052028656005859375, 'test/mean_average_precision': 0.17495912351054604, 'test/num_examples': 43793, 'score': 6494.021353006363, 'total_duration': 9430.473743200302, 'accumulated_submission_time': 6494.021353006363, 'accumulated_eval_time': 2935.130667924881, 'accumulated_logging_time': 0.7841694355010986, 'global_step': 20226, 'preemption_count': 0}), (20980, {'train/accuracy': 0.988884449005127, 'train/loss': 0.03870683163404465, 'train/mean_average_precision': 0.21985198387675192, 'validation/accuracy': 0.9858046174049377, 'validation/loss': 0.04974351450800896, 'validation/mean_average_precision': 0.18310630590458274, 'validation/num_examples': 43793, 'test/accuracy': 0.984847903251648, 'test/loss': 0.05284995585680008, 'test/mean_average_precision': 0.17790897236115488, 'test/num_examples': 43793, 'score': 6734.287105321884, 'total_duration': 9776.793322563171, 'accumulated_submission_time': 6734.287105321884, 'accumulated_eval_time': 3041.1337745189667, 'accumulated_logging_time': 0.8143470287322998, 'global_step': 20980, 'preemption_count': 0}), (21726, {'train/accuracy': 0.9888972640037537, 'train/loss': 0.038502927869558334, 'train/mean_average_precision': 0.20883904450703217, 'validation/accuracy': 0.9857494235038757, 'validation/loss': 0.04828091710805893, 'validation/mean_average_precision': 0.1844016397267828, 'validation/num_examples': 43793, 'test/accuracy': 0.9848167300224304, 'test/loss': 0.05139738321304321, 'test/mean_average_precision': 0.17765463798209252, 'test/num_examples': 43793, 'score': 6974.567994594574, 'total_duration': 10126.000288248062, 'accumulated_submission_time': 6974.567994594574, 'accumulated_eval_time': 3150.0082376003265, 'accumulated_logging_time': 0.8462753295898438, 'global_step': 21726, 'preemption_count': 0}), (22477, {'train/accuracy': 0.9887509942054749, 'train/loss': 0.03849947452545166, 'train/mean_average_precision': 0.21631777584403558, 'validation/accuracy': 0.9858188033103943, 'validation/loss': 0.04855337366461754, 'validation/mean_average_precision': 0.19513874586251742, 'validation/num_examples': 43793, 'test/accuracy': 0.984839916229248, 'test/loss': 0.051818352192640305, 'test/mean_average_precision': 0.18568779328349783, 'test/num_examples': 43793, 'score': 7214.62762594223, 'total_duration': 10469.517782449722, 'accumulated_submission_time': 7214.62762594223, 'accumulated_eval_time': 3253.414433002472, 'accumulated_logging_time': 0.877194881439209, 'global_step': 22477, 'preemption_count': 0}), (23223, {'train/accuracy': 0.9887223243713379, 'train/loss': 0.03865669295191765, 'train/mean_average_precision': 0.21402641052660315, 'validation/accuracy': 0.9857449531555176, 'validation/loss': 0.04908564314246178, 'validation/mean_average_precision': 0.18002393601332625, 'validation/num_examples': 43793, 'test/accuracy': 0.9848597049713135, 'test/loss': 0.05204010382294655, 'test/mean_average_precision': 0.17371921432040302, 'test/num_examples': 43793, 'score': 7454.670845508575, 'total_duration': 10818.201761245728, 'accumulated_submission_time': 7454.670845508575, 'accumulated_eval_time': 3362.0041534900665, 'accumulated_logging_time': 0.9081747531890869, 'global_step': 23223, 'preemption_count': 0}), (23962, {'train/accuracy': 0.988548755645752, 'train/loss': 0.03936893865466118, 'train/mean_average_precision': 0.2090295690843694, 'validation/accuracy': 0.9855188131332397, 'validation/loss': 0.04907645285129547, 'validation/mean_average_precision': 0.17879016202597145, 'validation/num_examples': 43793, 'test/accuracy': 0.9845973253250122, 'test/loss': 0.05218444764614105, 'test/mean_average_precision': 0.1705859883723574, 'test/num_examples': 43793, 'score': 7694.770886421204, 'total_duration': 11162.487454891205, 'accumulated_submission_time': 7694.770886421204, 'accumulated_eval_time': 3466.1369805336, 'accumulated_logging_time': 0.9395201206207275, 'global_step': 23962, 'preemption_count': 0}), (24708, {'train/accuracy': 0.9888287782669067, 'train/loss': 0.03828553482890129, 'train/mean_average_precision': 0.22730054165800737, 'validation/accuracy': 0.9858277440071106, 'validation/loss': 0.04814917594194412, 'validation/mean_average_precision': 0.19149159963752463, 'validation/num_examples': 43793, 'test/accuracy': 0.9849161505699158, 'test/loss': 0.05093801021575928, 'test/mean_average_precision': 0.1800086315183968, 'test/num_examples': 43793, 'score': 7934.792539834976, 'total_duration': 11507.038450717926, 'accumulated_submission_time': 7934.792539834976, 'accumulated_eval_time': 3570.6146986484528, 'accumulated_logging_time': 0.9710242748260498, 'global_step': 24708, 'preemption_count': 0}), (25447, {'train/accuracy': 0.9889517426490784, 'train/loss': 0.03764249011874199, 'train/mean_average_precision': 0.24037670319066257, 'validation/accuracy': 0.9858801364898682, 'validation/loss': 0.047933947294950485, 'validation/mean_average_precision': 0.19393603199913673, 'validation/num_examples': 43793, 'test/accuracy': 0.9848967790603638, 'test/loss': 0.05094367638230324, 'test/mean_average_precision': 0.18389132112044393, 'test/num_examples': 43793, 'score': 8174.904351234436, 'total_duration': 11853.188624620438, 'accumulated_submission_time': 8174.904351234436, 'accumulated_eval_time': 3676.598792552948, 'accumulated_logging_time': 1.0045819282531738, 'global_step': 25447, 'preemption_count': 0}), (26174, {'train/accuracy': 0.988925576210022, 'train/loss': 0.037910811603069305, 'train/mean_average_precision': 0.22496314824009483, 'validation/accuracy': 0.9856353402137756, 'validation/loss': 0.04848511889576912, 'validation/mean_average_precision': 0.18464683630427442, 'validation/num_examples': 43793, 'test/accuracy': 0.9846979379653931, 'test/loss': 0.051385655999183655, 'test/mean_average_precision': 0.17690776959130197, 'test/num_examples': 43793, 'score': 8414.594813585281, 'total_duration': 12196.693078517914, 'accumulated_submission_time': 8414.594813585281, 'accumulated_eval_time': 3779.948962688446, 'accumulated_logging_time': 1.4443256855010986, 'global_step': 26174, 'preemption_count': 0}), (26923, {'train/accuracy': 0.9887903332710266, 'train/loss': 0.03832165524363518, 'train/mean_average_precision': 0.21370930130145657, 'validation/accuracy': 0.9858123064041138, 'validation/loss': 0.04862184450030327, 'validation/mean_average_precision': 0.18894703016211614, 'validation/num_examples': 43793, 'test/accuracy': 0.9848226308822632, 'test/loss': 0.05195417255163193, 'test/mean_average_precision': 0.17806965507704928, 'test/num_examples': 43793, 'score': 8654.654390335083, 'total_duration': 12541.8131275177, 'accumulated_submission_time': 8654.654390335083, 'accumulated_eval_time': 3884.9562027454376, 'accumulated_logging_time': 1.477400302886963, 'global_step': 26923, 'preemption_count': 0}), (27663, {'train/accuracy': 0.9887416362762451, 'train/loss': 0.03844446316361427, 'train/mean_average_precision': 0.21724267770919028, 'validation/accuracy': 0.9857981204986572, 'validation/loss': 0.04820290207862854, 'validation/mean_average_precision': 0.1893011427508064, 'validation/num_examples': 43793, 'test/accuracy': 0.984944760799408, 'test/loss': 0.05114787071943283, 'test/mean_average_precision': 0.1803156483087207, 'test/num_examples': 43793, 'score': 8894.8265645504, 'total_duration': 12884.48639678955, 'accumulated_submission_time': 8894.8265645504, 'accumulated_eval_time': 3987.4045078754425, 'accumulated_logging_time': 1.5093200206756592, 'global_step': 27663, 'preemption_count': 0}), (28398, {'train/accuracy': 0.9888597130775452, 'train/loss': 0.03834989294409752, 'train/mean_average_precision': 0.2187355641821716, 'validation/accuracy': 0.985792875289917, 'validation/loss': 0.048368994146585464, 'validation/mean_average_precision': 0.1914001890482414, 'validation/num_examples': 43793, 'test/accuracy': 0.9848782420158386, 'test/loss': 0.05129092186689377, 'test/mean_average_precision': 0.18150058291692095, 'test/num_examples': 43793, 'score': 9135.05375790596, 'total_duration': 13235.853038072586, 'accumulated_submission_time': 9135.05375790596, 'accumulated_eval_time': 4098.485007286072, 'accumulated_logging_time': 1.5454697608947754, 'global_step': 28398, 'preemption_count': 0}), (29139, {'train/accuracy': 0.9890433549880981, 'train/loss': 0.03766318038105965, 'train/mean_average_precision': 0.2262856218682795, 'validation/accuracy': 0.9858456254005432, 'validation/loss': 0.047857899218797684, 'validation/mean_average_precision': 0.1901224715875739, 'validation/num_examples': 43793, 'test/accuracy': 0.9849388599395752, 'test/loss': 0.05089077353477478, 'test/mean_average_precision': 0.18449894548178364, 'test/num_examples': 43793, 'score': 9375.042990922928, 'total_duration': 13578.868658781052, 'accumulated_submission_time': 9375.042990922928, 'accumulated_eval_time': 4201.458696365356, 'accumulated_logging_time': 1.5778565406799316, 'global_step': 29139, 'preemption_count': 0}), (29886, {'train/accuracy': 0.9888697862625122, 'train/loss': 0.038062483072280884, 'train/mean_average_precision': 0.2286901512819851, 'validation/accuracy': 0.9859182834625244, 'validation/loss': 0.04802340269088745, 'validation/mean_average_precision': 0.19513268864816938, 'validation/num_examples': 43793, 'test/accuracy': 0.9849885702133179, 'test/loss': 0.05114302784204483, 'test/mean_average_precision': 0.18174385901222853, 'test/num_examples': 43793, 'score': 9615.254949569702, 'total_duration': 13923.735192537308, 'accumulated_submission_time': 9615.254949569702, 'accumulated_eval_time': 4306.060763597488, 'accumulated_logging_time': 1.6103568077087402, 'global_step': 29886, 'preemption_count': 0}), (30632, {'train/accuracy': 0.9889466166496277, 'train/loss': 0.03792235255241394, 'train/mean_average_precision': 0.22140702156558983, 'validation/accuracy': 0.9857209920883179, 'validation/loss': 0.04887448623776436, 'validation/mean_average_precision': 0.18472380761224852, 'validation/num_examples': 43793, 'test/accuracy': 0.9848508834838867, 'test/loss': 0.05192939564585686, 'test/mean_average_precision': 0.17757909118447807, 'test/num_examples': 43793, 'score': 9855.509615659714, 'total_duration': 14264.285064697266, 'accumulated_submission_time': 9855.509615659714, 'accumulated_eval_time': 4406.300930023193, 'accumulated_logging_time': 1.644965410232544, 'global_step': 30632, 'preemption_count': 0}), (31367, {'train/accuracy': 0.9888671636581421, 'train/loss': 0.038255780935287476, 'train/mean_average_precision': 0.2291374449002138, 'validation/accuracy': 0.9856942296028137, 'validation/loss': 0.04822748154401779, 'validation/mean_average_precision': 0.18544683546508053, 'validation/num_examples': 43793, 'test/accuracy': 0.9847914576530457, 'test/loss': 0.05096743628382683, 'test/mean_average_precision': 0.1805531748658989, 'test/num_examples': 43793, 'score': 10095.65578699112, 'total_duration': 14606.683991193771, 'accumulated_submission_time': 10095.65578699112, 'accumulated_eval_time': 4508.494157552719, 'accumulated_logging_time': 1.681239128112793, 'global_step': 31367, 'preemption_count': 0}), (32111, {'train/accuracy': 0.988877534866333, 'train/loss': 0.037788957357406616, 'train/mean_average_precision': 0.2309434431589295, 'validation/accuracy': 0.9858277440071106, 'validation/loss': 0.048646677285432816, 'validation/mean_average_precision': 0.19952700093152717, 'validation/num_examples': 43793, 'test/accuracy': 0.9849165678024292, 'test/loss': 0.05145052820444107, 'test/mean_average_precision': 0.1954261868553897, 'test/num_examples': 43793, 'score': 10335.89645934105, 'total_duration': 14955.209387540817, 'accumulated_submission_time': 10335.89645934105, 'accumulated_eval_time': 4616.725874662399, 'accumulated_logging_time': 1.714320421218872, 'global_step': 32111, 'preemption_count': 0}), (32848, {'train/accuracy': 0.9889434576034546, 'train/loss': 0.03779633715748787, 'train/mean_average_precision': 0.2432826322175141, 'validation/accuracy': 0.9857007265090942, 'validation/loss': 0.048318423330783844, 'validation/mean_average_precision': 0.19516267882604674, 'validation/num_examples': 43793, 'test/accuracy': 0.9847506284713745, 'test/loss': 0.051211535930633545, 'test/mean_average_precision': 0.18921546074442697, 'test/num_examples': 43793, 'score': 10575.972088098526, 'total_duration': 15301.55495762825, 'accumulated_submission_time': 10575.972088098526, 'accumulated_eval_time': 4722.9384779930115, 'accumulated_logging_time': 1.7510161399841309, 'global_step': 32848, 'preemption_count': 0}), (33585, {'train/accuracy': 0.9891308546066284, 'train/loss': 0.037314917892217636, 'train/mean_average_precision': 0.22602699491282865, 'validation/accuracy': 0.9859727025032043, 'validation/loss': 0.04742559790611267, 'validation/mean_average_precision': 0.19375695681022728, 'validation/num_examples': 43793, 'test/accuracy': 0.9850643873214722, 'test/loss': 0.05009305477142334, 'test/mean_average_precision': 0.1914235613692715, 'test/num_examples': 43793, 'score': 10816.100275278091, 'total_duration': 15647.596687793732, 'accumulated_submission_time': 10816.100275278091, 'accumulated_eval_time': 4828.792944908142, 'accumulated_logging_time': 1.7866477966308594, 'global_step': 33585, 'preemption_count': 0}), (34329, {'train/accuracy': 0.9889580011367798, 'train/loss': 0.03759896010160446, 'train/mean_average_precision': 0.2289441943128746, 'validation/accuracy': 0.9859438538551331, 'validation/loss': 0.04773719608783722, 'validation/mean_average_precision': 0.20205845787736976, 'validation/num_examples': 43793, 'test/accuracy': 0.9850370287895203, 'test/loss': 0.050530947744846344, 'test/mean_average_precision': 0.19075800828155534, 'test/num_examples': 43793, 'score': 11056.169059515, 'total_duration': 15994.386904001236, 'accumulated_submission_time': 11056.169059515, 'accumulated_eval_time': 4935.460122585297, 'accumulated_logging_time': 1.8201825618743896, 'global_step': 34329, 'preemption_count': 0}), (35077, {'train/accuracy': 0.9889733791351318, 'train/loss': 0.0374145433306694, 'train/mean_average_precision': 0.23837052534211148, 'validation/accuracy': 0.985992968082428, 'validation/loss': 0.04756581783294678, 'validation/mean_average_precision': 0.19238177482586644, 'validation/num_examples': 43793, 'test/accuracy': 0.9851309657096863, 'test/loss': 0.05031953379511833, 'test/mean_average_precision': 0.18689104401852946, 'test/num_examples': 43793, 'score': 11296.211079359055, 'total_duration': 16344.833291053772, 'accumulated_submission_time': 11296.211079359055, 'accumulated_eval_time': 5045.811078548431, 'accumulated_logging_time': 1.8531954288482666, 'global_step': 35077, 'preemption_count': 0}), (35816, {'train/accuracy': 0.9889889359474182, 'train/loss': 0.03766604885458946, 'train/mean_average_precision': 0.22819317542738163, 'validation/accuracy': 0.9858512878417969, 'validation/loss': 0.04821831360459328, 'validation/mean_average_precision': 0.19119373361529793, 'validation/num_examples': 43793, 'test/accuracy': 0.9848508834838867, 'test/loss': 0.05135584622621536, 'test/mean_average_precision': 0.18605029850991475, 'test/num_examples': 43793, 'score': 11536.243942975998, 'total_duration': 16685.29561161995, 'accumulated_submission_time': 11536.243942975998, 'accumulated_eval_time': 5146.181943178177, 'accumulated_logging_time': 1.8905532360076904, 'global_step': 35816, 'preemption_count': 0}), (36561, {'train/accuracy': 0.9889605641365051, 'train/loss': 0.03772250562906265, 'train/mean_average_precision': 0.22952639439431494, 'validation/accuracy': 0.9858009815216064, 'validation/loss': 0.04832683131098747, 'validation/mean_average_precision': 0.1856618333966113, 'validation/num_examples': 43793, 'test/accuracy': 0.984813392162323, 'test/loss': 0.051375459879636765, 'test/mean_average_precision': 0.1791125450303736, 'test/num_examples': 43793, 'score': 11776.2650411129, 'total_duration': 17027.249828100204, 'accumulated_submission_time': 11776.2650411129, 'accumulated_eval_time': 5248.06134390831, 'accumulated_logging_time': 1.9240601062774658, 'global_step': 36561, 'preemption_count': 0}), (37307, {'train/accuracy': 0.9892958998680115, 'train/loss': 0.03638657182455063, 'train/mean_average_precision': 0.2465555241199644, 'validation/accuracy': 0.985998272895813, 'validation/loss': 0.04717489331960678, 'validation/mean_average_precision': 0.20124921395267825, 'validation/num_examples': 43793, 'test/accuracy': 0.9851098656654358, 'test/loss': 0.050088971853256226, 'test/mean_average_precision': 0.19173120152422768, 'test/num_examples': 43793, 'score': 12016.520510673523, 'total_duration': 17371.671184062958, 'accumulated_submission_time': 12016.520510673523, 'accumulated_eval_time': 5352.1737768650055, 'accumulated_logging_time': 1.9577631950378418, 'global_step': 37307, 'preemption_count': 0}), (38051, {'train/accuracy': 0.9892456531524658, 'train/loss': 0.03651779517531395, 'train/mean_average_precision': 0.25090958297878485, 'validation/accuracy': 0.9860972762107849, 'validation/loss': 0.04758093133568764, 'validation/mean_average_precision': 0.19894745975519185, 'validation/num_examples': 43793, 'test/accuracy': 0.9851751923561096, 'test/loss': 0.050572469830513, 'test/mean_average_precision': 0.19549311068044276, 'test/num_examples': 43793, 'score': 12256.781912088394, 'total_duration': 17718.74373793602, 'accumulated_submission_time': 12256.781912088394, 'accumulated_eval_time': 5458.929462432861, 'accumulated_logging_time': 1.9929468631744385, 'global_step': 38051, 'preemption_count': 0}), (38792, {'train/accuracy': 0.9892786145210266, 'train/loss': 0.036268580704927444, 'train/mean_average_precision': 0.2614758905918543, 'validation/accuracy': 0.9860640168190002, 'validation/loss': 0.04710369557142258, 'validation/mean_average_precision': 0.20225724747870746, 'validation/num_examples': 43793, 'test/accuracy': 0.9852147698402405, 'test/loss': 0.049997612833976746, 'test/mean_average_precision': 0.20277524336345168, 'test/num_examples': 43793, 'score': 12496.968587875366, 'total_duration': 18062.000724554062, 'accumulated_submission_time': 12496.968587875366, 'accumulated_eval_time': 5561.9447453022, 'accumulated_logging_time': 2.0276620388031006, 'global_step': 38792, 'preemption_count': 0}), (39536, {'train/accuracy': 0.9891278743743896, 'train/loss': 0.03699309378862381, 'train/mean_average_precision': 0.24490563183510883, 'validation/accuracy': 0.985833466053009, 'validation/loss': 0.047233082354068756, 'validation/mean_average_precision': 0.196620120614496, 'validation/num_examples': 43793, 'test/accuracy': 0.9850079417228699, 'test/loss': 0.049868013709783554, 'test/mean_average_precision': 0.18901894211007755, 'test/num_examples': 43793, 'score': 12736.987015485764, 'total_duration': 18404.35669374466, 'accumulated_submission_time': 12736.987015485764, 'accumulated_eval_time': 5664.227452039719, 'accumulated_logging_time': 2.0624632835388184, 'global_step': 39536, 'preemption_count': 0}), (40280, {'train/accuracy': 0.9891670346260071, 'train/loss': 0.03707946464419365, 'train/mean_average_precision': 0.2359508469546338, 'validation/accuracy': 0.985985279083252, 'validation/loss': 0.04715704917907715, 'validation/mean_average_precision': 0.20417189248316878, 'validation/num_examples': 43793, 'test/accuracy': 0.9851473569869995, 'test/loss': 0.04994756355881691, 'test/mean_average_precision': 0.19444708721248188, 'test/num_examples': 43793, 'score': 12977.07556772232, 'total_duration': 18750.859877347946, 'accumulated_submission_time': 12977.07556772232, 'accumulated_eval_time': 5770.586403608322, 'accumulated_logging_time': 2.098353624343872, 'global_step': 40280, 'preemption_count': 0}), (41030, {'train/accuracy': 0.9892788529396057, 'train/loss': 0.03651939332485199, 'train/mean_average_precision': 0.25827914906327654, 'validation/accuracy': 0.9861382842063904, 'validation/loss': 0.04669969156384468, 'validation/mean_average_precision': 0.20358638501200454, 'validation/num_examples': 43793, 'test/accuracy': 0.9852400422096252, 'test/loss': 0.049561236053705215, 'test/mean_average_precision': 0.19574684937723494, 'test/num_examples': 43793, 'score': 13217.259890556335, 'total_duration': 19089.960252285004, 'accumulated_submission_time': 13217.259890556335, 'accumulated_eval_time': 5869.447708368301, 'accumulated_logging_time': 2.133057117462158, 'global_step': 41030, 'preemption_count': 0}), (41783, {'train/accuracy': 0.98917156457901, 'train/loss': 0.036688920110464096, 'train/mean_average_precision': 0.23759855260147475, 'validation/accuracy': 0.9860579371452332, 'validation/loss': 0.04709014296531677, 'validation/mean_average_precision': 0.20033803818553, 'validation/num_examples': 43793, 'test/accuracy': 0.9851107597351074, 'test/loss': 0.05010908097028732, 'test/mean_average_precision': 0.19270607028931946, 'test/num_examples': 43793, 'score': 13457.330917358398, 'total_duration': 19428.43219280243, 'accumulated_submission_time': 13457.330917358398, 'accumulated_eval_time': 5967.794405460358, 'accumulated_logging_time': 2.167390823364258, 'global_step': 41783, 'preemption_count': 0}), (42524, {'train/accuracy': 0.9892622828483582, 'train/loss': 0.03639383241534233, 'train/mean_average_precision': 0.2609484153302455, 'validation/accuracy': 0.9860400557518005, 'validation/loss': 0.04684174433350563, 'validation/mean_average_precision': 0.20936669277164166, 'validation/num_examples': 43793, 'test/accuracy': 0.9851625561714172, 'test/loss': 0.04959564283490181, 'test/mean_average_precision': 0.20294228271802964, 'test/num_examples': 43793, 'score': 13697.295249938965, 'total_duration': 19778.669197797775, 'accumulated_submission_time': 13697.295249938965, 'accumulated_eval_time': 6078.01136302948, 'accumulated_logging_time': 2.2034754753112793, 'global_step': 42524, 'preemption_count': 0}), (43273, {'train/accuracy': 0.9893701076507568, 'train/loss': 0.03616950288414955, 'train/mean_average_precision': 0.24883681990520495, 'validation/accuracy': 0.9862012267112732, 'validation/loss': 0.04695441946387291, 'validation/mean_average_precision': 0.20823950554080176, 'validation/num_examples': 43793, 'test/accuracy': 0.985236644744873, 'test/loss': 0.049845922738313675, 'test/mean_average_precision': 0.2022384611718691, 'test/num_examples': 43793, 'score': 13937.44949221611, 'total_duration': 20120.397768735886, 'accumulated_submission_time': 13937.44949221611, 'accumulated_eval_time': 6179.531805515289, 'accumulated_logging_time': 2.237947463989258, 'global_step': 43273, 'preemption_count': 0}), (44019, {'train/accuracy': 0.9893001317977905, 'train/loss': 0.03607473149895668, 'train/mean_average_precision': 0.2637567741698877, 'validation/accuracy': 0.9860668778419495, 'validation/loss': 0.046803850680589676, 'validation/mean_average_precision': 0.20828175219288686, 'validation/num_examples': 43793, 'test/accuracy': 0.9852383732795715, 'test/loss': 0.04968736320734024, 'test/mean_average_precision': 0.20247295465330872, 'test/num_examples': 43793, 'score': 14177.538898229599, 'total_duration': 20464.505380392075, 'accumulated_submission_time': 14177.538898229599, 'accumulated_eval_time': 6283.494655847549, 'accumulated_logging_time': 2.2736637592315674, 'global_step': 44019, 'preemption_count': 0}), (44780, {'train/accuracy': 0.9894915223121643, 'train/loss': 0.03542591631412506, 'train/mean_average_precision': 0.26977764927073483, 'validation/accuracy': 0.9861618280410767, 'validation/loss': 0.046834852546453476, 'validation/mean_average_precision': 0.20999691848267893, 'validation/num_examples': 43793, 'test/accuracy': 0.9852139353752136, 'test/loss': 0.04983562231063843, 'test/mean_average_precision': 0.19900461359394145, 'test/num_examples': 43793, 'score': 14417.7179479599, 'total_duration': 20805.993231773376, 'accumulated_submission_time': 14417.7179479599, 'accumulated_eval_time': 6384.748462438583, 'accumulated_logging_time': 2.3086318969726562, 'global_step': 44780, 'preemption_count': 0}), (45528, {'train/accuracy': 0.9893769025802612, 'train/loss': 0.03581002727150917, 'train/mean_average_precision': 0.2722272612931093, 'validation/accuracy': 0.9861736297607422, 'validation/loss': 0.0468456894159317, 'validation/mean_average_precision': 0.20630033965734001, 'validation/num_examples': 43793, 'test/accuracy': 0.9852522611618042, 'test/loss': 0.04980984330177307, 'test/mean_average_precision': 0.19994921522976514, 'test/num_examples': 43793, 'score': 14657.810967445374, 'total_duration': 21152.046332597733, 'accumulated_submission_time': 14657.810967445374, 'accumulated_eval_time': 6490.6512088775635, 'accumulated_logging_time': 2.3464467525482178, 'global_step': 45528, 'preemption_count': 0}), (46269, {'train/accuracy': 0.9895589351654053, 'train/loss': 0.035501208156347275, 'train/mean_average_precision': 0.2688084506959136, 'validation/accuracy': 0.9862186908721924, 'validation/loss': 0.04645548760890961, 'validation/mean_average_precision': 0.21455191095003842, 'validation/num_examples': 43793, 'test/accuracy': 0.9853171110153198, 'test/loss': 0.04936239495873451, 'test/mean_average_precision': 0.20562919119766426, 'test/num_examples': 43793, 'score': 14897.910877466202, 'total_duration': 21493.54910182953, 'accumulated_submission_time': 14897.910877466202, 'accumulated_eval_time': 6591.992488861084, 'accumulated_logging_time': 2.38614821434021, 'global_step': 46269, 'preemption_count': 0}), (47009, {'train/accuracy': 0.9891928434371948, 'train/loss': 0.03659552335739136, 'train/mean_average_precision': 0.2510488937477338, 'validation/accuracy': 0.9861078858375549, 'validation/loss': 0.04711566120386124, 'validation/mean_average_precision': 0.20644794906789043, 'validation/num_examples': 43793, 'test/accuracy': 0.9852370619773865, 'test/loss': 0.049996789544820786, 'test/mean_average_precision': 0.1964800375281237, 'test/num_examples': 43793, 'score': 15138.077248096466, 'total_duration': 21838.38315463066, 'accumulated_submission_time': 15138.077248096466, 'accumulated_eval_time': 6696.60099697113, 'accumulated_logging_time': 2.4226717948913574, 'global_step': 47009, 'preemption_count': 0}), (47751, {'train/accuracy': 0.9892840385437012, 'train/loss': 0.036285195499658585, 'train/mean_average_precision': 0.2603987360262739, 'validation/accuracy': 0.9862515330314636, 'validation/loss': 0.04682980477809906, 'validation/mean_average_precision': 0.21119415249727339, 'validation/num_examples': 43793, 'test/accuracy': 0.9853697419166565, 'test/loss': 0.04973635822534561, 'test/mean_average_precision': 0.20645110365449323, 'test/num_examples': 43793, 'score': 15378.110914945602, 'total_duration': 22179.26458454132, 'accumulated_submission_time': 15378.110914945602, 'accumulated_eval_time': 6797.390934467316, 'accumulated_logging_time': 2.4604079723358154, 'global_step': 47751, 'preemption_count': 0}), (48497, {'train/accuracy': 0.9894214272499084, 'train/loss': 0.03587300330400467, 'train/mean_average_precision': 0.2551420798235866, 'validation/accuracy': 0.9860554933547974, 'validation/loss': 0.046592436730861664, 'validation/mean_average_precision': 0.21204854440704776, 'validation/num_examples': 43793, 'test/accuracy': 0.9850791692733765, 'test/loss': 0.049412429332733154, 'test/mean_average_precision': 0.20297205695318776, 'test/num_examples': 43793, 'score': 15618.3207821846, 'total_duration': 22520.22537112236, 'accumulated_submission_time': 15618.3207821846, 'accumulated_eval_time': 6898.0792760849, 'accumulated_logging_time': 2.502638578414917, 'global_step': 48497, 'preemption_count': 0}), (49250, {'train/accuracy': 0.9894852638244629, 'train/loss': 0.035724472254514694, 'train/mean_average_precision': 0.26698095934131594, 'validation/accuracy': 0.9862223267555237, 'validation/loss': 0.046513985842466354, 'validation/mean_average_precision': 0.2131052753435941, 'validation/num_examples': 43793, 'test/accuracy': 0.9852433800697327, 'test/loss': 0.04969567805528641, 'test/mean_average_precision': 0.20034394022746574, 'test/num_examples': 43793, 'score': 15858.295699596405, 'total_duration': 22862.790357112885, 'accumulated_submission_time': 15858.295699596405, 'accumulated_eval_time': 7000.6140151023865, 'accumulated_logging_time': 2.537923812866211, 'global_step': 49250, 'preemption_count': 0}), (50000, {'train/accuracy': 0.9894618391990662, 'train/loss': 0.03557528182864189, 'train/mean_average_precision': 0.2711189138805558, 'validation/accuracy': 0.9861240983009338, 'validation/loss': 0.04661322012543678, 'validation/mean_average_precision': 0.20959399077861243, 'validation/num_examples': 43793, 'test/accuracy': 0.9852758646011353, 'test/loss': 0.04927290603518486, 'test/mean_average_precision': 0.20343352830175473, 'test/num_examples': 43793, 'score': 16098.242151737213, 'total_duration': 23203.83507847786, 'accumulated_submission_time': 16098.242151737213, 'accumulated_eval_time': 7101.655977487564, 'accumulated_logging_time': 2.5741710662841797, 'global_step': 50000, 'preemption_count': 0}), (50747, {'train/accuracy': 0.9896489381790161, 'train/loss': 0.03502275422215462, 'train/mean_average_precision': 0.2890718598209157, 'validation/accuracy': 0.9863201379776001, 'validation/loss': 0.04624200612306595, 'validation/mean_average_precision': 0.21238210352457706, 'validation/num_examples': 43793, 'test/accuracy': 0.9854114651679993, 'test/loss': 0.04919213801622391, 'test/mean_average_precision': 0.20622264976099486, 'test/num_examples': 43793, 'score': 16338.338171482086, 'total_duration': 23549.17933702469, 'accumulated_submission_time': 16338.338171482086, 'accumulated_eval_time': 7206.845845937729, 'accumulated_logging_time': 2.612422466278076, 'global_step': 50747, 'preemption_count': 0}), (51497, {'train/accuracy': 0.9896401762962341, 'train/loss': 0.03489775210618973, 'train/mean_average_precision': 0.27522383903813774, 'validation/accuracy': 0.9863294959068298, 'validation/loss': 0.04617808014154434, 'validation/mean_average_precision': 0.21516065789567526, 'validation/num_examples': 43793, 'test/accuracy': 0.9853802919387817, 'test/loss': 0.04909180849790573, 'test/mean_average_precision': 0.2061251770082258, 'test/num_examples': 43793, 'score': 16578.522582292557, 'total_duration': 23890.57517528534, 'accumulated_submission_time': 16578.522582292557, 'accumulated_eval_time': 7307.999075889587, 'accumulated_logging_time': 2.64997935295105, 'global_step': 51497, 'preemption_count': 0}), (52238, {'train/accuracy': 0.9896869659423828, 'train/loss': 0.03474186733365059, 'train/mean_average_precision': 0.29000266580794, 'validation/accuracy': 0.9863424897193909, 'validation/loss': 0.046252839267253876, 'validation/mean_average_precision': 0.21700427765967875, 'validation/num_examples': 43793, 'test/accuracy': 0.9854455590248108, 'test/loss': 0.04912613704800606, 'test/mean_average_precision': 0.2123704547099925, 'test/num_examples': 43793, 'score': 16818.65753221512, 'total_duration': 24229.96969652176, 'accumulated_submission_time': 16818.65753221512, 'accumulated_eval_time': 7407.201065540314, 'accumulated_logging_time': 2.6873366832733154, 'global_step': 52238, 'preemption_count': 0}), (52977, {'train/accuracy': 0.9897372126579285, 'train/loss': 0.03438626602292061, 'train/mean_average_precision': 0.2859971482991346, 'validation/accuracy': 0.986326277256012, 'validation/loss': 0.04607265815138817, 'validation/mean_average_precision': 0.2227344429919973, 'validation/num_examples': 43793, 'test/accuracy': 0.985417366027832, 'test/loss': 0.04900296777486801, 'test/mean_average_precision': 0.21153648323356808, 'test/num_examples': 43793, 'score': 17058.785735607147, 'total_duration': 24572.39704155922, 'accumulated_submission_time': 17058.785735607147, 'accumulated_eval_time': 7509.441777706146, 'accumulated_logging_time': 2.724439859390259, 'global_step': 52977, 'preemption_count': 0}), (53732, {'train/accuracy': 0.989798903465271, 'train/loss': 0.03463223949074745, 'train/mean_average_precision': 0.27364324378520405, 'validation/accuracy': 0.986401379108429, 'validation/loss': 0.04586976394057274, 'validation/mean_average_precision': 0.21815914793114188, 'validation/num_examples': 43793, 'test/accuracy': 0.9854910969734192, 'test/loss': 0.04870198667049408, 'test/mean_average_precision': 0.2097780509319448, 'test/num_examples': 43793, 'score': 17298.919110774994, 'total_duration': 24915.203050851822, 'accumulated_submission_time': 17298.919110774994, 'accumulated_eval_time': 7612.057886600494, 'accumulated_logging_time': 2.76061749458313, 'global_step': 53732, 'preemption_count': 0}), (54493, {'train/accuracy': 0.989654541015625, 'train/loss': 0.03495398908853531, 'train/mean_average_precision': 0.2809974503744279, 'validation/accuracy': 0.9862645268440247, 'validation/loss': 0.04621202498674393, 'validation/mean_average_precision': 0.21551536423348336, 'validation/num_examples': 43793, 'test/accuracy': 0.9853390455245972, 'test/loss': 0.04909416660666466, 'test/mean_average_precision': 0.2090533931014886, 'test/num_examples': 43793, 'score': 17538.96286535263, 'total_duration': 25257.191864728928, 'accumulated_submission_time': 17538.96286535263, 'accumulated_eval_time': 7713.9464428424835, 'accumulated_logging_time': 2.797553777694702, 'global_step': 54493, 'preemption_count': 0}), (55249, {'train/accuracy': 0.989753007888794, 'train/loss': 0.034596964716911316, 'train/mean_average_precision': 0.28548771479706025, 'validation/accuracy': 0.9863563179969788, 'validation/loss': 0.04587423428893089, 'validation/mean_average_precision': 0.2150310554975685, 'validation/num_examples': 43793, 'test/accuracy': 0.9854329228401184, 'test/loss': 0.04876290634274483, 'test/mean_average_precision': 0.2074281320406993, 'test/num_examples': 43793, 'score': 17779.07443547249, 'total_duration': 25599.114872932434, 'accumulated_submission_time': 17779.07443547249, 'accumulated_eval_time': 7815.701034784317, 'accumulated_logging_time': 2.8346564769744873, 'global_step': 55249, 'preemption_count': 0}), (55990, {'train/accuracy': 0.9899150133132935, 'train/loss': 0.03416066989302635, 'train/mean_average_precision': 0.29227628198369915, 'validation/accuracy': 0.9864853620529175, 'validation/loss': 0.04559401422739029, 'validation/mean_average_precision': 0.22398801953629738, 'validation/num_examples': 43793, 'test/accuracy': 0.9854813814163208, 'test/loss': 0.0485040582716465, 'test/mean_average_precision': 0.21495641242022187, 'test/num_examples': 43793, 'score': 18019.22664308548, 'total_duration': 25940.16689515114, 'accumulated_submission_time': 18019.22664308548, 'accumulated_eval_time': 7916.534770727158, 'accumulated_logging_time': 2.8772406578063965, 'global_step': 55990, 'preemption_count': 0}), (56739, {'train/accuracy': 0.9898921847343445, 'train/loss': 0.03401986509561539, 'train/mean_average_precision': 0.29999873610666594, 'validation/accuracy': 0.986504077911377, 'validation/loss': 0.04590983688831329, 'validation/mean_average_precision': 0.2238110699225643, 'validation/num_examples': 43793, 'test/accuracy': 0.9855051636695862, 'test/loss': 0.048889946192502975, 'test/mean_average_precision': 0.21376523695951066, 'test/num_examples': 43793, 'score': 18259.172052145004, 'total_duration': 26285.1296813488, 'accumulated_submission_time': 18259.172052145004, 'accumulated_eval_time': 8021.491682767868, 'accumulated_logging_time': 2.9176416397094727, 'global_step': 56739, 'preemption_count': 0})], 'global_step': 57413}
I0210 07:25:39.680151 140039251117888 submission_runner.py:586] Timing: 18477.2912671566
I0210 07:25:39.680218 140039251117888 submission_runner.py:588] Total number of evals: 77
I0210 07:25:39.680269 140039251117888 submission_runner.py:589] ====================
I0210 07:25:39.680324 140039251117888 submission_runner.py:542] Using RNG seed 1895687988
I0210 07:25:39.744912 140039251117888 submission_runner.py:551] --- Tuning run 5/5 ---
I0210 07:25:39.745072 140039251117888 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_3/ogbg_jax/trial_5.
I0210 07:25:39.749661 140039251117888 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_3/ogbg_jax/trial_5/hparams.json.
I0210 07:25:39.885478 140039251117888 submission_runner.py:206] Initializing dataset.
I0210 07:25:39.979267 140039251117888 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0210 07:25:39.983522 140039251117888 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0210 07:25:40.113077 140039251117888 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0210 07:25:40.149326 140039251117888 submission_runner.py:213] Initializing model.
I0210 07:25:42.599230 140039251117888 submission_runner.py:255] Initializing optimizer.
I0210 07:25:43.192096 140039251117888 submission_runner.py:262] Initializing metrics bundle.
I0210 07:25:43.192304 140039251117888 submission_runner.py:280] Initializing checkpoint and logger.
I0210 07:25:43.192979 140039251117888 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_3/ogbg_jax/trial_5 with prefix checkpoint_
I0210 07:25:43.193104 140039251117888 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_3/ogbg_jax/trial_5/meta_data_0.json.
I0210 07:25:43.193314 140039251117888 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0210 07:25:43.193375 140039251117888 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0210 07:25:45.486009 140039251117888 logger_utils.py:220] Unable to record git information. Continuing without it.
I0210 07:25:47.765952 140039251117888 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_3/ogbg_jax/trial_5/flags_0.json.
I0210 07:25:47.770133 140039251117888 submission_runner.py:314] Starting training loop.
I0210 07:26:00.990548 139799361206016 logging_writer.py:48] [0] global_step=0, grad_norm=2.025653839111328, loss=0.7228075265884399
I0210 07:26:01.000177 140039251117888 spec.py:321] Evaluating on the training split.
I0210 07:27:37.948153 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 07:27:41.061050 140039251117888 spec.py:349] Evaluating on the test split.
I0210 07:27:44.070225 140039251117888 submission_runner.py:408] Time since start: 116.30s, 	Step: 1, 	{'train/accuracy': 0.5250520706176758, 'train/loss': 0.7150473594665527, 'train/mean_average_precision': 0.02231542038668717, 'validation/accuracy': 0.5213832855224609, 'validation/loss': 0.7166012525558472, 'validation/mean_average_precision': 0.026142065301455825, 'validation/num_examples': 43793, 'test/accuracy': 0.5224818587303162, 'test/loss': 0.7161954641342163, 'test/mean_average_precision': 0.02783946806769139, 'test/num_examples': 43793, 'score': 13.229990720748901, 'total_duration': 116.30002975463867, 'accumulated_submission_time': 13.229990720748901, 'accumulated_eval_time': 103.06999778747559, 'accumulated_logging_time': 0}
I0210 07:27:44.079230 139799390091008 logging_writer.py:48] [1] accumulated_eval_time=103.069998, accumulated_logging_time=0, accumulated_submission_time=13.229991, global_step=1, preemption_count=0, score=13.229991, test/accuracy=0.522482, test/loss=0.716195, test/mean_average_precision=0.027839, test/num_examples=43793, total_duration=116.300030, train/accuracy=0.525052, train/loss=0.715047, train/mean_average_precision=0.022315, validation/accuracy=0.521383, validation/loss=0.716601, validation/mean_average_precision=0.026142, validation/num_examples=43793
I0210 07:28:15.793685 139799398483712 logging_writer.py:48] [100] global_step=100, grad_norm=0.2694820761680603, loss=0.2504158318042755
I0210 07:28:47.638632 139799390091008 logging_writer.py:48] [200] global_step=200, grad_norm=0.0846104547381401, loss=0.10300187021493912
I0210 07:29:19.299655 139799398483712 logging_writer.py:48] [300] global_step=300, grad_norm=0.028566164895892143, loss=0.06496882438659668
I0210 07:29:51.064473 139799390091008 logging_writer.py:48] [400] global_step=400, grad_norm=0.02370876632630825, loss=0.06121401861310005
I0210 07:30:22.548119 139799398483712 logging_writer.py:48] [500] global_step=500, grad_norm=0.012395625934004784, loss=0.052902571856975555
I0210 07:30:54.191861 139799390091008 logging_writer.py:48] [600] global_step=600, grad_norm=0.03272939845919609, loss=0.057464733719825745
I0210 07:31:26.158922 139799398483712 logging_writer.py:48] [700] global_step=700, grad_norm=0.015620985999703407, loss=0.04603424295783043
I0210 07:31:44.227658 140039251117888 spec.py:321] Evaluating on the training split.
I0210 07:33:19.206114 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 07:33:22.757838 140039251117888 spec.py:349] Evaluating on the test split.
I0210 07:33:26.213291 140039251117888 submission_runner.py:408] Time since start: 458.44s, 	Step: 758, 	{'train/accuracy': 0.986740231513977, 'train/loss': 0.05124962702393532, 'train/mean_average_precision': 0.056202073924971514, 'validation/accuracy': 0.9841589331626892, 'validation/loss': 0.061199311167001724, 'validation/mean_average_precision': 0.05523248937010333, 'validation/num_examples': 43793, 'test/accuracy': 0.9832048416137695, 'test/loss': 0.0645277202129364, 'test/mean_average_precision': 0.0557577787410884, 'test/num_examples': 43793, 'score': 253.348135471344, 'total_duration': 458.4430618286133, 'accumulated_submission_time': 253.348135471344, 'accumulated_eval_time': 205.0555601119995, 'accumulated_logging_time': 0.019795894622802734}
I0210 07:33:26.229911 139798976751360 logging_writer.py:48] [758] accumulated_eval_time=205.055560, accumulated_logging_time=0.019796, accumulated_submission_time=253.348135, global_step=758, preemption_count=0, score=253.348135, test/accuracy=0.983205, test/loss=0.064528, test/mean_average_precision=0.055758, test/num_examples=43793, total_duration=458.443062, train/accuracy=0.986740, train/loss=0.051250, train/mean_average_precision=0.056202, validation/accuracy=0.984159, validation/loss=0.061199, validation/mean_average_precision=0.055232, validation/num_examples=43793
I0210 07:33:40.257651 139818172847872 logging_writer.py:48] [800] global_step=800, grad_norm=0.019420549273490906, loss=0.05411973595619202
I0210 07:34:12.841697 139798976751360 logging_writer.py:48] [900] global_step=900, grad_norm=0.01558664720505476, loss=0.05243484303355217
I0210 07:34:44.862923 139818172847872 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.026044806465506554, loss=0.05201346054673195
I0210 07:35:16.950896 139798976751360 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.016221242025494576, loss=0.04959464818239212
I0210 07:35:48.657515 139818172847872 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.01662210375070572, loss=0.0508902333676815
I0210 07:36:21.203994 139798976751360 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.026223961263895035, loss=0.048248205333948135
I0210 07:36:53.968611 139818172847872 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.01209794357419014, loss=0.04538661614060402
I0210 07:37:26.501361 140039251117888 spec.py:321] Evaluating on the training split.
I0210 07:39:04.596860 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 07:39:07.617439 140039251117888 spec.py:349] Evaluating on the test split.
I0210 07:39:13.046487 140039251117888 submission_runner.py:408] Time since start: 805.28s, 	Step: 1500, 	{'train/accuracy': 0.9867498874664307, 'train/loss': 0.050342004746198654, 'train/mean_average_precision': 0.07422550614128996, 'validation/accuracy': 0.9842255115509033, 'validation/loss': 0.060442958027124405, 'validation/mean_average_precision': 0.07188214537665251, 'validation/num_examples': 43793, 'test/accuracy': 0.9832578897476196, 'test/loss': 0.06376054137945175, 'test/mean_average_precision': 0.07471594251830098, 'test/num_examples': 43793, 'score': 493.5868184566498, 'total_duration': 805.2762854099274, 'accumulated_submission_time': 493.5868184566498, 'accumulated_eval_time': 311.6006577014923, 'accumulated_logging_time': 0.047776222229003906}
I0210 07:39:13.061747 139799390091008 logging_writer.py:48] [1500] accumulated_eval_time=311.600658, accumulated_logging_time=0.047776, accumulated_submission_time=493.586818, global_step=1500, preemption_count=0, score=493.586818, test/accuracy=0.983258, test/loss=0.063761, test/mean_average_precision=0.074716, test/num_examples=43793, total_duration=805.276285, train/accuracy=0.986750, train/loss=0.050342, train/mean_average_precision=0.074226, validation/accuracy=0.984226, validation/loss=0.060443, validation/mean_average_precision=0.071882, validation/num_examples=43793
I0210 07:39:13.726676 139799398483712 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.04487255960702896, loss=0.04499373212456703
I0210 07:39:45.430211 139799390091008 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.05684324726462364, loss=0.048953909426927567
I0210 07:40:17.432958 139799398483712 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.025313012301921844, loss=0.052258796989917755
I0210 07:40:49.147342 139799390091008 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.015197262167930603, loss=0.0493009127676487
I0210 07:41:21.155679 139799398483712 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.014885996468365192, loss=0.04909615218639374
I0210 07:41:52.965818 139799390091008 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.028357386589050293, loss=0.04480978474020958
I0210 07:42:24.859987 139799398483712 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.025620095431804657, loss=0.04163843393325806
I0210 07:42:56.314985 139799390091008 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.016712643206119537, loss=0.04532415792346001
I0210 07:43:13.294066 140039251117888 spec.py:321] Evaluating on the training split.
I0210 07:44:47.978706 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 07:44:51.048992 140039251117888 spec.py:349] Evaluating on the test split.
I0210 07:44:54.053781 140039251117888 submission_runner.py:408] Time since start: 1146.28s, 	Step: 2254, 	{'train/accuracy': 0.9876054525375366, 'train/loss': 0.04437072202563286, 'train/mean_average_precision': 0.13130761852340767, 'validation/accuracy': 0.9849160313606262, 'validation/loss': 0.053602177649736404, 'validation/mean_average_precision': 0.12358903997484055, 'validation/num_examples': 43793, 'test/accuracy': 0.9839398264884949, 'test/loss': 0.05674533173441887, 'test/mean_average_precision': 0.11911557470652198, 'test/num_examples': 43793, 'score': 733.4666738510132, 'total_duration': 1146.283578157425, 'accumulated_submission_time': 733.4666738510132, 'accumulated_eval_time': 412.36032915115356, 'accumulated_logging_time': 0.3961923122406006}
I0210 07:44:54.076569 139798976751360 logging_writer.py:48] [2254] accumulated_eval_time=412.360329, accumulated_logging_time=0.396192, accumulated_submission_time=733.466674, global_step=2254, preemption_count=0, score=733.466674, test/accuracy=0.983940, test/loss=0.056745, test/mean_average_precision=0.119116, test/num_examples=43793, total_duration=1146.283578, train/accuracy=0.987605, train/loss=0.044371, train/mean_average_precision=0.131308, validation/accuracy=0.984916, validation/loss=0.053602, validation/mean_average_precision=0.123589, validation/num_examples=43793
I0210 07:45:09.608643 139818172847872 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.018660590052604675, loss=0.0423651821911335
I0210 07:45:42.087657 139798976751360 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.011688546277582645, loss=0.04470675811171532
I0210 07:46:14.440097 139818172847872 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.011866189539432526, loss=0.04017937555909157
I0210 07:46:46.854392 139798976751360 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.010394438169896603, loss=0.04744505509734154
I0210 07:47:18.822187 139818172847872 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.014613482169806957, loss=0.04063648357987404
I0210 07:47:51.337702 139798976751360 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.013967117294669151, loss=0.04021070897579193
I0210 07:48:24.040897 139818172847872 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.013140153139829636, loss=0.03984306752681732
I0210 07:48:54.136968 140039251117888 spec.py:321] Evaluating on the training split.
I0210 07:50:30.221502 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 07:50:33.323357 140039251117888 spec.py:349] Evaluating on the test split.
I0210 07:50:36.360825 140039251117888 submission_runner.py:408] Time since start: 1488.59s, 	Step: 2992, 	{'train/accuracy': 0.9879937171936035, 'train/loss': 0.04256334528326988, 'train/mean_average_precision': 0.15644186837769805, 'validation/accuracy': 0.9850869178771973, 'validation/loss': 0.051669634878635406, 'validation/mean_average_precision': 0.14192352319117627, 'validation/num_examples': 43793, 'test/accuracy': 0.9841234683990479, 'test/loss': 0.05444411188364029, 'test/mean_average_precision': 0.14019992793623207, 'test/num_examples': 43793, 'score': 973.4950165748596, 'total_duration': 1488.5906157493591, 'accumulated_submission_time': 973.4950165748596, 'accumulated_eval_time': 514.584146976471, 'accumulated_logging_time': 0.43120884895324707}
I0210 07:50:36.377465 139799390091008 logging_writer.py:48] [2992] accumulated_eval_time=514.584147, accumulated_logging_time=0.431209, accumulated_submission_time=973.495017, global_step=2992, preemption_count=0, score=973.495017, test/accuracy=0.984123, test/loss=0.054444, test/mean_average_precision=0.140200, test/num_examples=43793, total_duration=1488.590616, train/accuracy=0.987994, train/loss=0.042563, train/mean_average_precision=0.156442, validation/accuracy=0.985087, validation/loss=0.051670, validation/mean_average_precision=0.141924, validation/num_examples=43793
I0210 07:50:39.303120 139799398483712 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.013300484046339989, loss=0.044944897294044495
I0210 07:51:11.240307 139799390091008 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.011381926946341991, loss=0.04484058916568756
I0210 07:51:43.168912 139799398483712 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.013667951337993145, loss=0.0396144837141037
I0210 07:52:15.868961 139799390091008 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.024984460324048996, loss=0.04238184168934822
I0210 07:52:48.263395 139799398483712 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.02172994427382946, loss=0.04301800578832626
I0210 07:53:21.022048 139799390091008 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.011229767464101315, loss=0.04225217550992966
I0210 07:53:53.313919 139799398483712 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.019297923892736435, loss=0.038870759308338165
I0210 07:54:25.709336 139799390091008 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.019021570682525635, loss=0.0425252728164196
I0210 07:54:36.443110 140039251117888 spec.py:321] Evaluating on the training split.
I0210 07:56:12.109086 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 07:56:15.197780 140039251117888 spec.py:349] Evaluating on the test split.
I0210 07:56:18.236370 140039251117888 submission_runner.py:408] Time since start: 1830.47s, 	Step: 3734, 	{'train/accuracy': 0.9882489442825317, 'train/loss': 0.04114070162177086, 'train/mean_average_precision': 0.18479054694703131, 'validation/accuracy': 0.9853414297103882, 'validation/loss': 0.05053086206316948, 'validation/mean_average_precision': 0.16241611775463913, 'validation/num_examples': 43793, 'test/accuracy': 0.9844486117362976, 'test/loss': 0.053183663636446, 'test/mean_average_precision': 0.16174832188936106, 'test/num_examples': 43793, 'score': 1213.5263085365295, 'total_duration': 1830.4661691188812, 'accumulated_submission_time': 1213.5263085365295, 'accumulated_eval_time': 616.377361536026, 'accumulated_logging_time': 0.4610466957092285}
I0210 07:56:18.252349 139813965014784 logging_writer.py:48] [3734] accumulated_eval_time=616.377362, accumulated_logging_time=0.461047, accumulated_submission_time=1213.526309, global_step=3734, preemption_count=0, score=1213.526309, test/accuracy=0.984449, test/loss=0.053184, test/mean_average_precision=0.161748, test/num_examples=43793, total_duration=1830.466169, train/accuracy=0.988249, train/loss=0.041141, train/mean_average_precision=0.184791, validation/accuracy=0.985341, validation/loss=0.050531, validation/mean_average_precision=0.162416, validation/num_examples=43793
I0210 07:56:39.753402 139818172847872 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.010284366086125374, loss=0.04098379239439964
I0210 07:57:12.111361 139813965014784 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.009597440250217915, loss=0.04295697435736656
I0210 07:57:44.325451 139818172847872 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.024854736402630806, loss=0.04598167911171913
I0210 07:58:16.488949 139813965014784 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.013159732334315777, loss=0.03708019480109215
I0210 07:58:48.227381 139818172847872 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.010189036838710308, loss=0.0434647798538208
I0210 07:59:20.225574 139813965014784 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.01186959445476532, loss=0.04138670489192009
I0210 07:59:51.808941 139818172847872 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.013387328013777733, loss=0.03288698196411133
I0210 08:00:18.501863 140039251117888 spec.py:321] Evaluating on the training split.
I0210 08:01:59.095028 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 08:02:02.335069 140039251117888 spec.py:349] Evaluating on the test split.
I0210 08:02:05.309134 140039251117888 submission_runner.py:408] Time since start: 2177.54s, 	Step: 4485, 	{'train/accuracy': 0.9885805249214172, 'train/loss': 0.03955966234207153, 'train/mean_average_precision': 0.20502891507200674, 'validation/accuracy': 0.9856662154197693, 'validation/loss': 0.049088116735219955, 'validation/mean_average_precision': 0.18354689952738906, 'validation/num_examples': 43793, 'test/accuracy': 0.9847337603569031, 'test/loss': 0.052051421254873276, 'test/mean_average_precision': 0.18946579973570027, 'test/num_examples': 43793, 'score': 1453.7445459365845, 'total_duration': 2177.53892993927, 'accumulated_submission_time': 1453.7445459365845, 'accumulated_eval_time': 723.1845881938934, 'accumulated_logging_time': 0.48903846740722656}
I0210 08:02:05.325916 139798976751360 logging_writer.py:48] [4485] accumulated_eval_time=723.184588, accumulated_logging_time=0.489038, accumulated_submission_time=1453.744546, global_step=4485, preemption_count=0, score=1453.744546, test/accuracy=0.984734, test/loss=0.052051, test/mean_average_precision=0.189466, test/num_examples=43793, total_duration=2177.538930, train/accuracy=0.988581, train/loss=0.039560, train/mean_average_precision=0.205029, validation/accuracy=0.985666, validation/loss=0.049088, validation/mean_average_precision=0.183547, validation/num_examples=43793
I0210 08:02:10.522102 139799398483712 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.01460979413241148, loss=0.040108826011419296
I0210 08:02:42.477515 139798976751360 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.010360191576182842, loss=0.04516131058335304
I0210 08:03:14.618260 139799398483712 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.014120974577963352, loss=0.04291542246937752
I0210 08:03:46.435746 139798976751360 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.015606861561536789, loss=0.040503837168216705
I0210 08:04:18.480859 139799398483712 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.011676624417304993, loss=0.03814532980322838
I0210 08:04:50.051759 139798976751360 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.008952307514846325, loss=0.0369233675301075
I0210 08:05:21.772897 139799398483712 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.018336908891797066, loss=0.039519961923360825
I0210 08:05:53.859442 139798976751360 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.013878049328923225, loss=0.03938572481274605
I0210 08:06:05.574769 140039251117888 spec.py:321] Evaluating on the training split.
I0210 08:07:40.611075 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 08:07:43.635919 140039251117888 spec.py:349] Evaluating on the test split.
I0210 08:07:46.596668 140039251117888 submission_runner.py:408] Time since start: 2518.83s, 	Step: 5237, 	{'train/accuracy': 0.9888032674789429, 'train/loss': 0.03795219212770462, 'train/mean_average_precision': 0.23725617233091223, 'validation/accuracy': 0.985903263092041, 'validation/loss': 0.04772370681166649, 'validation/mean_average_precision': 0.20035285455957202, 'validation/num_examples': 43793, 'test/accuracy': 0.9849599599838257, 'test/loss': 0.05057065561413765, 'test/mean_average_precision': 0.20146415901992373, 'test/num_examples': 43793, 'score': 1693.9610152244568, 'total_duration': 2518.8264665603638, 'accumulated_submission_time': 1693.9610152244568, 'accumulated_eval_time': 824.2064425945282, 'accumulated_logging_time': 0.5192301273345947}
I0210 08:07:46.612968 139813965014784 logging_writer.py:48] [5237] accumulated_eval_time=824.206443, accumulated_logging_time=0.519230, accumulated_submission_time=1693.961015, global_step=5237, preemption_count=0, score=1693.961015, test/accuracy=0.984960, test/loss=0.050571, test/mean_average_precision=0.201464, test/num_examples=43793, total_duration=2518.826467, train/accuracy=0.988803, train/loss=0.037952, train/mean_average_precision=0.237256, validation/accuracy=0.985903, validation/loss=0.047724, validation/mean_average_precision=0.200353, validation/num_examples=43793
I0210 08:08:07.219332 139818172847872 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.012317819520831108, loss=0.03818855434656143
I0210 08:08:38.962872 139813965014784 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.010309090837836266, loss=0.03626551106572151
I0210 08:09:11.108478 139818172847872 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.011549066752195358, loss=0.04120441526174545
I0210 08:09:42.860485 139813965014784 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.012753934599459171, loss=0.035012856125831604
I0210 08:10:14.612333 139818172847872 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.013462727889418602, loss=0.03832922875881195
I0210 08:10:46.156533 139813965014784 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.01639278419315815, loss=0.03557852283120155
I0210 08:11:18.196304 139818172847872 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.018056944012641907, loss=0.038484565913677216
I0210 08:11:46.721972 140039251117888 spec.py:321] Evaluating on the training split.
I0210 08:13:22.961547 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 08:13:26.061941 140039251117888 spec.py:349] Evaluating on the test split.
I0210 08:13:29.083771 140039251117888 submission_runner.py:408] Time since start: 2861.31s, 	Step: 5990, 	{'train/accuracy': 0.9887657761573792, 'train/loss': 0.03785643354058266, 'train/mean_average_precision': 0.24467373526100106, 'validation/accuracy': 0.9857993125915527, 'validation/loss': 0.04756652191281319, 'validation/mean_average_precision': 0.2110000050222093, 'validation/num_examples': 43793, 'test/accuracy': 0.984913170337677, 'test/loss': 0.05036148801445961, 'test/mean_average_precision': 0.2117786355546247, 'test/num_examples': 43793, 'score': 1934.0389623641968, 'total_duration': 2861.313567876816, 'accumulated_submission_time': 1934.0389623641968, 'accumulated_eval_time': 926.5682003498077, 'accumulated_logging_time': 0.5474085807800293}
I0210 08:13:29.100216 139798976751360 logging_writer.py:48] [5990] accumulated_eval_time=926.568200, accumulated_logging_time=0.547409, accumulated_submission_time=1934.038962, global_step=5990, preemption_count=0, score=1934.038962, test/accuracy=0.984913, test/loss=0.050361, test/mean_average_precision=0.211779, test/num_examples=43793, total_duration=2861.313568, train/accuracy=0.988766, train/loss=0.037856, train/mean_average_precision=0.244674, validation/accuracy=0.985799, validation/loss=0.047567, validation/mean_average_precision=0.211000, validation/num_examples=43793
I0210 08:13:32.668089 139799398483712 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.010021586902439594, loss=0.03584062680602074
I0210 08:14:04.932316 139798976751360 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.018432335928082466, loss=0.03960072621703148
I0210 08:14:37.061611 139799398483712 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.02122480235993862, loss=0.03773041442036629
I0210 08:15:09.112263 139798976751360 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.023277629166841507, loss=0.038259442895650864
I0210 08:15:40.813088 139799398483712 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.01207762211561203, loss=0.0350223109126091
I0210 08:16:12.824340 139798976751360 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.012611323967576027, loss=0.034542687237262726
I0210 08:16:44.859128 139799398483712 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.017875773832201958, loss=0.038006510585546494
I0210 08:17:17.158200 139798976751360 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.01989133283495903, loss=0.03920668363571167
I0210 08:17:29.367573 140039251117888 spec.py:321] Evaluating on the training split.
I0210 08:19:04.190385 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 08:19:07.278230 140039251117888 spec.py:349] Evaluating on the test split.
I0210 08:19:10.334645 140039251117888 submission_runner.py:408] Time since start: 3202.56s, 	Step: 6739, 	{'train/accuracy': 0.9891668558120728, 'train/loss': 0.03648771345615387, 'train/mean_average_precision': 0.27751284413558747, 'validation/accuracy': 0.986108660697937, 'validation/loss': 0.0465688593685627, 'validation/mean_average_precision': 0.21951441803696914, 'validation/num_examples': 43793, 'test/accuracy': 0.9852050542831421, 'test/loss': 0.049427032470703125, 'test/mean_average_precision': 0.2250390013924116, 'test/num_examples': 43793, 'score': 2174.274698495865, 'total_duration': 3202.564444541931, 'accumulated_submission_time': 2174.274698495865, 'accumulated_eval_time': 1027.5352380275726, 'accumulated_logging_time': 0.5763721466064453}
I0210 08:19:10.351302 139818172847872 logging_writer.py:48] [6739] accumulated_eval_time=1027.535238, accumulated_logging_time=0.576372, accumulated_submission_time=2174.274698, global_step=6739, preemption_count=0, score=2174.274698, test/accuracy=0.985205, test/loss=0.049427, test/mean_average_precision=0.225039, test/num_examples=43793, total_duration=3202.564445, train/accuracy=0.989167, train/loss=0.036488, train/mean_average_precision=0.277513, validation/accuracy=0.986109, validation/loss=0.046569, validation/mean_average_precision=0.219514, validation/num_examples=43793
I0210 08:19:30.061154 139976524166912 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.01722497120499611, loss=0.039047304540872574
I0210 08:20:02.150986 139818172847872 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.018187619745731354, loss=0.040468353778123856
I0210 08:20:34.033415 139976524166912 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.024617455899715424, loss=0.03462422266602516
I0210 08:21:05.925972 139818172847872 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.016090407967567444, loss=0.037045616656541824
I0210 08:21:37.925591 139976524166912 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.0125763900578022, loss=0.03379787132143974
I0210 08:22:10.069639 139818172847872 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.015927061438560486, loss=0.03733368217945099
I0210 08:22:43.860953 139976524166912 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.015694914385676384, loss=0.03155200183391571
I0210 08:23:10.587108 140039251117888 spec.py:321] Evaluating on the training split.
I0210 08:24:47.497642 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 08:24:50.816305 140039251117888 spec.py:349] Evaluating on the test split.
I0210 08:24:54.066391 140039251117888 submission_runner.py:408] Time since start: 3546.30s, 	Step: 7485, 	{'train/accuracy': 0.9896649718284607, 'train/loss': 0.034851428121328354, 'train/mean_average_precision': 0.3058615825134636, 'validation/accuracy': 0.9863104224205017, 'validation/loss': 0.045855745673179626, 'validation/mean_average_precision': 0.22820115659870607, 'validation/num_examples': 43793, 'test/accuracy': 0.9853411316871643, 'test/loss': 0.0486358180642128, 'test/mean_average_precision': 0.23021534201673358, 'test/num_examples': 43793, 'score': 2414.481337785721, 'total_duration': 3546.296167373657, 'accumulated_submission_time': 2414.481337785721, 'accumulated_eval_time': 1131.0144610404968, 'accumulated_logging_time': 0.6038436889648438}
I0210 08:24:54.085984 139799398483712 logging_writer.py:48] [7485] accumulated_eval_time=1131.014461, accumulated_logging_time=0.603844, accumulated_submission_time=2414.481338, global_step=7485, preemption_count=0, score=2414.481338, test/accuracy=0.985341, test/loss=0.048636, test/mean_average_precision=0.230215, test/num_examples=43793, total_duration=3546.296167, train/accuracy=0.989665, train/loss=0.034851, train/mean_average_precision=0.305862, validation/accuracy=0.986310, validation/loss=0.045856, validation/mean_average_precision=0.228201, validation/num_examples=43793
I0210 08:24:59.522771 139813965014784 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.017396336421370506, loss=0.036257170140743256
I0210 08:25:32.376545 139799398483712 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.013266593217849731, loss=0.03435185179114342
I0210 08:26:04.674619 139813965014784 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.015722794458270073, loss=0.04102468118071556
I0210 08:26:36.975803 139799398483712 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.019772350788116455, loss=0.037604205310344696
I0210 08:27:09.052855 139813965014784 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.011530609801411629, loss=0.034194111824035645
I0210 08:27:41.124204 139799398483712 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.020018909126520157, loss=0.03754148259758949
I0210 08:28:13.149502 139813965014784 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.015019611455500126, loss=0.04011926054954529
I0210 08:28:44.962489 139799398483712 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.016229500994086266, loss=0.034616291522979736
I0210 08:28:54.137817 140039251117888 spec.py:321] Evaluating on the training split.
I0210 08:30:28.202098 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 08:30:31.337803 140039251117888 spec.py:349] Evaluating on the test split.
I0210 08:30:34.398281 140039251117888 submission_runner.py:408] Time since start: 3886.63s, 	Step: 8230, 	{'train/accuracy': 0.9897923469543457, 'train/loss': 0.03433658555150032, 'train/mean_average_precision': 0.3150043600492316, 'validation/accuracy': 0.98642897605896, 'validation/loss': 0.0455535389482975, 'validation/mean_average_precision': 0.23367242995826423, 'validation/num_examples': 43793, 'test/accuracy': 0.9855673313140869, 'test/loss': 0.04825417324900627, 'test/mean_average_precision': 0.23695948493119998, 'test/num_examples': 43793, 'score': 2654.502183198929, 'total_duration': 3886.628069639206, 'accumulated_submission_time': 2654.502183198929, 'accumulated_eval_time': 1231.274868965149, 'accumulated_logging_time': 0.635286808013916}
I0210 08:30:34.415176 139871926159104 logging_writer.py:48] [8230] accumulated_eval_time=1231.274869, accumulated_logging_time=0.635287, accumulated_submission_time=2654.502183, global_step=8230, preemption_count=0, score=2654.502183, test/accuracy=0.985567, test/loss=0.048254, test/mean_average_precision=0.236959, test/num_examples=43793, total_duration=3886.628070, train/accuracy=0.989792, train/loss=0.034337, train/mean_average_precision=0.315004, validation/accuracy=0.986429, validation/loss=0.045554, validation/mean_average_precision=0.233672, validation/num_examples=43793
I0210 08:30:56.942980 139976524166912 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.013116410002112389, loss=0.037831854075193405
I0210 08:31:28.704599 139871926159104 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.014456640928983688, loss=0.03518668934702873
I0210 08:32:00.678781 139976524166912 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.019599907100200653, loss=0.03379036858677864
I0210 08:32:32.716199 139871926159104 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.02624278888106346, loss=0.037347666919231415
I0210 08:33:05.287998 139976524166912 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.016122495755553246, loss=0.03559264540672302
I0210 08:33:37.705120 139871926159104 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.016676444560289383, loss=0.033707186579704285
I0210 08:34:10.190411 139976524166912 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.022311974316835403, loss=0.04008457809686661
I0210 08:34:34.440628 140039251117888 spec.py:321] Evaluating on the training split.
I0210 08:36:10.837830 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 08:36:13.951843 140039251117888 spec.py:349] Evaluating on the test split.
I0210 08:36:17.032011 140039251117888 submission_runner.py:408] Time since start: 4229.26s, 	Step: 8977, 	{'train/accuracy': 0.9900234341621399, 'train/loss': 0.03339506685733795, 'train/mean_average_precision': 0.3414138007447258, 'validation/accuracy': 0.9864451885223389, 'validation/loss': 0.04538137838244438, 'validation/mean_average_precision': 0.23793114613338667, 'validation/num_examples': 43793, 'test/accuracy': 0.9856507182121277, 'test/loss': 0.04776066541671753, 'test/mean_average_precision': 0.24603677236961033, 'test/num_examples': 43793, 'score': 2894.498073577881, 'total_duration': 4229.2618017196655, 'accumulated_submission_time': 2894.498073577881, 'accumulated_eval_time': 1333.866200208664, 'accumulated_logging_time': 0.6630842685699463}
I0210 08:36:17.051959 139799398483712 logging_writer.py:48] [8977] accumulated_eval_time=1333.866200, accumulated_logging_time=0.663084, accumulated_submission_time=2894.498074, global_step=8977, preemption_count=0, score=2894.498074, test/accuracy=0.985651, test/loss=0.047761, test/mean_average_precision=0.246037, test/num_examples=43793, total_duration=4229.261802, train/accuracy=0.990023, train/loss=0.033395, train/mean_average_precision=0.341414, validation/accuracy=0.986445, validation/loss=0.045381, validation/mean_average_precision=0.237931, validation/num_examples=43793
I0210 08:36:24.783510 139813965014784 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.020467976108193398, loss=0.034707359969615936
I0210 08:36:56.368782 139799398483712 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.01811026781797409, loss=0.03602701798081398
I0210 08:37:28.406513 139813965014784 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.016362717375159264, loss=0.030877934768795967
I0210 08:38:00.646238 139799398483712 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.01708127371966839, loss=0.035427484661340714
I0210 08:38:32.716148 139813965014784 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.014904575422406197, loss=0.03326035290956497
I0210 08:39:04.632663 139799398483712 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.01645153947174549, loss=0.029671158641576767
I0210 08:39:36.549839 139813965014784 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.02300010807812214, loss=0.035247959196567535
I0210 08:40:09.112429 139799398483712 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.022527918219566345, loss=0.035424523055553436
I0210 08:40:17.105386 140039251117888 spec.py:321] Evaluating on the training split.
I0210 08:41:55.748216 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 08:41:58.801094 140039251117888 spec.py:349] Evaluating on the test split.
I0210 08:42:01.823120 140039251117888 submission_runner.py:408] Time since start: 4574.05s, 	Step: 9726, 	{'train/accuracy': 0.9900118708610535, 'train/loss': 0.03329336270689964, 'train/mean_average_precision': 0.3366389186653354, 'validation/accuracy': 0.9865231513977051, 'validation/loss': 0.04517379775643349, 'validation/mean_average_precision': 0.24064049963896625, 'validation/num_examples': 43793, 'test/accuracy': 0.9857496619224548, 'test/loss': 0.04780091345310211, 'test/mean_average_precision': 0.24382410873859825, 'test/num_examples': 43793, 'score': 3134.5203773975372, 'total_duration': 4574.052921056747, 'accumulated_submission_time': 3134.5203773975372, 'accumulated_eval_time': 1438.5838916301727, 'accumulated_logging_time': 0.6951918601989746}
I0210 08:42:01.840365 139871926159104 logging_writer.py:48] [9726] accumulated_eval_time=1438.583892, accumulated_logging_time=0.695192, accumulated_submission_time=3134.520377, global_step=9726, preemption_count=0, score=3134.520377, test/accuracy=0.985750, test/loss=0.047801, test/mean_average_precision=0.243824, test/num_examples=43793, total_duration=4574.052921, train/accuracy=0.990012, train/loss=0.033293, train/mean_average_precision=0.336639, validation/accuracy=0.986523, validation/loss=0.045174, validation/mean_average_precision=0.240640, validation/num_examples=43793
I0210 08:42:26.225317 139976524166912 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.027551937848329544, loss=0.03466082364320755
I0210 08:42:58.315386 139871926159104 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.02323153056204319, loss=0.035737618803977966
I0210 08:43:30.971425 139976524166912 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.022850431501865387, loss=0.03233656659722328
I0210 08:44:03.438179 139871926159104 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.019246727228164673, loss=0.038558632135391235
I0210 08:44:35.698113 139976524166912 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.018089383840560913, loss=0.031445324420928955
I0210 08:45:08.461435 139871926159104 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.023752478882670403, loss=0.037496473640203476
I0210 08:45:41.411281 139976524166912 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.03517672047019005, loss=0.03401917219161987
I0210 08:46:01.899204 140039251117888 spec.py:321] Evaluating on the training split.
I0210 08:47:43.100153 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 08:47:46.165359 140039251117888 spec.py:349] Evaluating on the test split.
I0210 08:47:49.161621 140039251117888 submission_runner.py:408] Time since start: 4921.39s, 	Step: 10464, 	{'train/accuracy': 0.9898894429206848, 'train/loss': 0.0337132103741169, 'train/mean_average_precision': 0.32434957025237554, 'validation/accuracy': 0.9865986108779907, 'validation/loss': 0.045117463916540146, 'validation/mean_average_precision': 0.24252620421974527, 'validation/num_examples': 43793, 'test/accuracy': 0.9857842326164246, 'test/loss': 0.04770386964082718, 'test/mean_average_precision': 0.2462669049591603, 'test/num_examples': 43793, 'score': 3374.544422149658, 'total_duration': 4921.391419172287, 'accumulated_submission_time': 3374.544422149658, 'accumulated_eval_time': 1545.8462941646576, 'accumulated_logging_time': 0.7231438159942627}
I0210 08:47:49.179897 139799398483712 logging_writer.py:48] [10464] accumulated_eval_time=1545.846294, accumulated_logging_time=0.723144, accumulated_submission_time=3374.544422, global_step=10464, preemption_count=0, score=3374.544422, test/accuracy=0.985784, test/loss=0.047704, test/mean_average_precision=0.246267, test/num_examples=43793, total_duration=4921.391419, train/accuracy=0.989889, train/loss=0.033713, train/mean_average_precision=0.324350, validation/accuracy=0.986599, validation/loss=0.045117, validation/mean_average_precision=0.242526, validation/num_examples=43793
I0210 08:48:01.156565 139818172847872 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.024665100499987602, loss=0.03451366722583771
I0210 08:48:34.088055 139799398483712 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.03150107339024544, loss=0.036420729011297226
I0210 08:49:06.461377 139818172847872 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.023047208786010742, loss=0.03473924472928047
I0210 08:49:38.138886 139799398483712 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.019209355115890503, loss=0.032559264451265335
I0210 08:50:10.578635 139818172847872 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.03689941018819809, loss=0.0395662747323513
I0210 08:50:42.607022 139799398483712 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.023901576176285744, loss=0.03491412103176117
I0210 08:51:15.246229 139818172847872 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.018407419323921204, loss=0.03162340819835663
I0210 08:51:46.886567 139799398483712 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.027051597833633423, loss=0.034892406314611435
I0210 08:51:49.412261 140039251117888 spec.py:321] Evaluating on the training split.
I0210 08:53:21.764487 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 08:53:24.815282 140039251117888 spec.py:349] Evaluating on the test split.
I0210 08:53:27.854650 140039251117888 submission_runner.py:408] Time since start: 5260.08s, 	Step: 11209, 	{'train/accuracy': 0.9900863766670227, 'train/loss': 0.03292468562722206, 'train/mean_average_precision': 0.3577444428522381, 'validation/accuracy': 0.9866254329681396, 'validation/loss': 0.04499063268303871, 'validation/mean_average_precision': 0.252842845892908, 'validation/num_examples': 43793, 'test/accuracy': 0.9858301281929016, 'test/loss': 0.04760059714317322, 'test/mean_average_precision': 0.25779776291195505, 'test/num_examples': 43793, 'score': 3614.7446529865265, 'total_duration': 5260.084446191788, 'accumulated_submission_time': 3614.7446529865265, 'accumulated_eval_time': 1644.2886338233948, 'accumulated_logging_time': 0.7532014846801758}
I0210 08:53:27.872902 139813965014784 logging_writer.py:48] [11209] accumulated_eval_time=1644.288634, accumulated_logging_time=0.753201, accumulated_submission_time=3614.744653, global_step=11209, preemption_count=0, score=3614.744653, test/accuracy=0.985830, test/loss=0.047601, test/mean_average_precision=0.257798, test/num_examples=43793, total_duration=5260.084446, train/accuracy=0.990086, train/loss=0.032925, train/mean_average_precision=0.357744, validation/accuracy=0.986625, validation/loss=0.044991, validation/mean_average_precision=0.252843, validation/num_examples=43793
I0210 08:53:57.714217 139871926159104 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.0274981539696455, loss=0.036502327769994736
I0210 08:54:29.808138 139813965014784 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.01994919590651989, loss=0.03529789298772812
I0210 08:55:02.156164 139871926159104 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.026471808552742004, loss=0.034469738602638245
I0210 08:55:34.297623 139813965014784 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.021148530766367912, loss=0.032798271626234055
I0210 08:56:06.897329 139871926159104 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.02215643599629402, loss=0.030540844425559044
I0210 08:56:38.991544 139813965014784 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.027478516101837158, loss=0.03144955262541771
I0210 08:57:11.177698 139871926159104 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.027898842468857765, loss=0.03698413446545601
I0210 08:57:27.859234 140039251117888 spec.py:321] Evaluating on the training split.
I0210 08:59:05.305064 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 08:59:08.338577 140039251117888 spec.py:349] Evaluating on the test split.
I0210 08:59:11.290213 140039251117888 submission_runner.py:408] Time since start: 5603.52s, 	Step: 11953, 	{'train/accuracy': 0.9902267456054688, 'train/loss': 0.032570574432611465, 'train/mean_average_precision': 0.35369296343583034, 'validation/accuracy': 0.9866960644721985, 'validation/loss': 0.04454199597239494, 'validation/mean_average_precision': 0.2551666926403044, 'validation/num_examples': 43793, 'test/accuracy': 0.9858154058456421, 'test/loss': 0.04706943780183792, 'test/mean_average_precision': 0.2561756224992291, 'test/num_examples': 43793, 'score': 3854.700940847397, 'total_duration': 5603.52001285553, 'accumulated_submission_time': 3854.700940847397, 'accumulated_eval_time': 1747.719571352005, 'accumulated_logging_time': 0.7825462818145752}
I0210 08:59:11.308769 139799398483712 logging_writer.py:48] [11953] accumulated_eval_time=1747.719571, accumulated_logging_time=0.782546, accumulated_submission_time=3854.700941, global_step=11953, preemption_count=0, score=3854.700941, test/accuracy=0.985815, test/loss=0.047069, test/mean_average_precision=0.256176, test/num_examples=43793, total_duration=5603.520013, train/accuracy=0.990227, train/loss=0.032571, train/mean_average_precision=0.353693, validation/accuracy=0.986696, validation/loss=0.044542, validation/mean_average_precision=0.255167, validation/num_examples=43793
I0210 08:59:27.746882 139818172847872 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.023747505620121956, loss=0.0329519547522068
I0210 08:59:59.724333 139799398483712 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.029534779489040375, loss=0.035157181322574615
I0210 09:00:31.839696 139818172847872 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.02344570681452751, loss=0.03154638782143593
I0210 09:01:04.132134 139799398483712 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.021252671256661415, loss=0.029842020943760872
I0210 09:01:36.446944 139818172847872 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.028649937361478806, loss=0.03151887282729149
I0210 09:02:08.313679 139799398483712 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.022221285849809647, loss=0.03207370266318321
I0210 09:02:40.346567 139818172847872 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.0275688748806715, loss=0.03380366414785385
I0210 09:03:11.408406 140039251117888 spec.py:321] Evaluating on the training split.
I0210 09:04:50.983386 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 09:04:54.034185 140039251117888 spec.py:349] Evaluating on the test split.
I0210 09:04:57.013942 140039251117888 submission_runner.py:408] Time since start: 5949.24s, 	Step: 12698, 	{'train/accuracy': 0.9903358221054077, 'train/loss': 0.031861528754234314, 'train/mean_average_precision': 0.3735765411648799, 'validation/accuracy': 0.9865893125534058, 'validation/loss': 0.044699788093566895, 'validation/mean_average_precision': 0.2553556998427123, 'validation/num_examples': 43793, 'test/accuracy': 0.9857736825942993, 'test/loss': 0.04746128246188164, 'test/mean_average_precision': 0.25380504066264953, 'test/num_examples': 43793, 'score': 4094.7709546089172, 'total_duration': 5949.243740320206, 'accumulated_submission_time': 4094.7709546089172, 'accumulated_eval_time': 1853.3250706195831, 'accumulated_logging_time': 0.812180757522583}
I0210 09:04:57.032605 139813965014784 logging_writer.py:48] [12698] accumulated_eval_time=1853.325071, accumulated_logging_time=0.812181, accumulated_submission_time=4094.770955, global_step=12698, preemption_count=0, score=4094.770955, test/accuracy=0.985774, test/loss=0.047461, test/mean_average_precision=0.253805, test/num_examples=43793, total_duration=5949.243740, train/accuracy=0.990336, train/loss=0.031862, train/mean_average_precision=0.373577, validation/accuracy=0.986589, validation/loss=0.044700, validation/mean_average_precision=0.255356, validation/num_examples=43793
I0210 09:04:58.073432 139871926159104 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.03123418055474758, loss=0.03178795427083969
I0210 09:05:30.049537 139813965014784 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.03329509124159813, loss=0.03529681637883186
I0210 09:06:01.846063 139871926159104 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.029896259307861328, loss=0.032742440700531006
I0210 09:06:33.693994 139813965014784 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.02478528581559658, loss=0.036559008061885834
I0210 09:07:05.664653 139871926159104 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.034832585602998734, loss=0.03575972467660904
I0210 09:07:37.292210 139813965014784 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.05144890025258064, loss=0.039461079984903336
I0210 09:08:09.046603 139871926159104 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.03302067145705223, loss=0.03111683577299118
I0210 09:08:40.859843 139813965014784 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.03546623885631561, loss=0.03723237290978432
I0210 09:08:57.203083 140039251117888 spec.py:321] Evaluating on the training split.
I0210 09:10:31.333312 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 09:10:34.468827 140039251117888 spec.py:349] Evaluating on the test split.
I0210 09:10:37.451203 140039251117888 submission_runner.py:408] Time since start: 6289.68s, 	Step: 13452, 	{'train/accuracy': 0.9905796051025391, 'train/loss': 0.031181801110506058, 'train/mean_average_precision': 0.3965822545995702, 'validation/accuracy': 0.9866871237754822, 'validation/loss': 0.044055573642253876, 'validation/mean_average_precision': 0.265454619956688, 'validation/num_examples': 43793, 'test/accuracy': 0.9858141541481018, 'test/loss': 0.04674358665943146, 'test/mean_average_precision': 0.2679424626869677, 'test/num_examples': 43793, 'score': 4334.911934137344, 'total_duration': 6289.6809866428375, 'accumulated_submission_time': 4334.911934137344, 'accumulated_eval_time': 1953.5731303691864, 'accumulated_logging_time': 0.8416659832000732}
I0210 09:10:37.469206 139799398483712 logging_writer.py:48] [13452] accumulated_eval_time=1953.573130, accumulated_logging_time=0.841666, accumulated_submission_time=4334.911934, global_step=13452, preemption_count=0, score=4334.911934, test/accuracy=0.985814, test/loss=0.046744, test/mean_average_precision=0.267942, test/num_examples=43793, total_duration=6289.680987, train/accuracy=0.990580, train/loss=0.031182, train/mean_average_precision=0.396582, validation/accuracy=0.986687, validation/loss=0.044056, validation/mean_average_precision=0.265455, validation/num_examples=43793
I0210 09:10:52.944928 139976524166912 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.02733353152871132, loss=0.029845498502254486
I0210 09:11:25.020191 139799398483712 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.024909216910600662, loss=0.03293180093169212
I0210 09:11:57.032622 139976524166912 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.030725065618753433, loss=0.03374297171831131
I0210 09:12:29.113196 139799398483712 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.025980446487665176, loss=0.03302362188696861
I0210 09:13:01.134353 139976524166912 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.03555680811405182, loss=0.03329569101333618
I0210 09:13:33.242010 139799398483712 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.03309363126754761, loss=0.0370757095515728
I0210 09:14:05.168060 139976524166912 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.0362851619720459, loss=0.036257434636354446
I0210 09:14:37.342893 139799398483712 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.03322503715753555, loss=0.03201311454176903
I0210 09:14:37.653645 140039251117888 spec.py:321] Evaluating on the training split.
I0210 09:16:15.792155 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 09:16:19.227310 140039251117888 spec.py:349] Evaluating on the test split.
I0210 09:16:22.655564 140039251117888 submission_runner.py:408] Time since start: 6634.89s, 	Step: 14202, 	{'train/accuracy': 0.9905725121498108, 'train/loss': 0.03087206371128559, 'train/mean_average_precision': 0.3942485855207641, 'validation/accuracy': 0.9867480397224426, 'validation/loss': 0.04451712965965271, 'validation/mean_average_precision': 0.2602028697357067, 'validation/num_examples': 43793, 'test/accuracy': 0.9858920574188232, 'test/loss': 0.04740261659026146, 'test/mean_average_precision': 0.2573156301471659, 'test/num_examples': 43793, 'score': 4575.066102266312, 'total_duration': 6634.885343790054, 'accumulated_submission_time': 4575.066102266312, 'accumulated_eval_time': 2058.5749821662903, 'accumulated_logging_time': 0.8709104061126709}
I0210 09:16:22.675892 139818172847872 logging_writer.py:48] [14202] accumulated_eval_time=2058.574982, accumulated_logging_time=0.870910, accumulated_submission_time=4575.066102, global_step=14202, preemption_count=0, score=4575.066102, test/accuracy=0.985892, test/loss=0.047403, test/mean_average_precision=0.257316, test/num_examples=43793, total_duration=6634.885344, train/accuracy=0.990573, train/loss=0.030872, train/mean_average_precision=0.394249, validation/accuracy=0.986748, validation/loss=0.044517, validation/mean_average_precision=0.260203, validation/num_examples=43793
I0210 09:16:55.243034 139871926159104 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.03177837282419205, loss=0.031432248651981354
I0210 09:17:27.284154 139818172847872 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.02814263291656971, loss=0.032353632152080536
I0210 09:17:58.613060 139871926159104 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.03313685208559036, loss=0.032662712037563324
I0210 09:18:30.477918 139818172847872 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.02902328222990036, loss=0.031321931630373
I0210 09:19:02.309674 139871926159104 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.054966650903224945, loss=0.03059798665344715
I0210 09:19:35.340320 139818172847872 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.03512602299451828, loss=0.03415968641638756
I0210 09:20:08.113477 139871926159104 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.026758240535855293, loss=0.02849467471241951
I0210 09:20:22.753529 140039251117888 spec.py:321] Evaluating on the training split.
I0210 09:21:59.256907 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 09:22:02.745661 140039251117888 spec.py:349] Evaluating on the test split.
I0210 09:22:05.747740 140039251117888 submission_runner.py:408] Time since start: 6977.98s, 	Step: 14946, 	{'train/accuracy': 0.9908130168914795, 'train/loss': 0.03027959354221821, 'train/mean_average_precision': 0.4061079881876005, 'validation/accuracy': 0.986812949180603, 'validation/loss': 0.044084303081035614, 'validation/mean_average_precision': 0.2687047924207835, 'validation/num_examples': 43793, 'test/accuracy': 0.9859813451766968, 'test/loss': 0.04696929082274437, 'test/mean_average_precision': 0.2671970450686207, 'test/num_examples': 43793, 'score': 4815.111397981644, 'total_duration': 6977.977539300919, 'accumulated_submission_time': 4815.111397981644, 'accumulated_eval_time': 2161.5691606998444, 'accumulated_logging_time': 0.9022881984710693}
I0210 09:22:05.766792 139813965014784 logging_writer.py:48] [14946] accumulated_eval_time=2161.569161, accumulated_logging_time=0.902288, accumulated_submission_time=4815.111398, global_step=14946, preemption_count=0, score=4815.111398, test/accuracy=0.985981, test/loss=0.046969, test/mean_average_precision=0.267197, test/num_examples=43793, total_duration=6977.977539, train/accuracy=0.990813, train/loss=0.030280, train/mean_average_precision=0.406108, validation/accuracy=0.986813, validation/loss=0.044084, validation/mean_average_precision=0.268705, validation/num_examples=43793
I0210 09:22:23.756261 139976524166912 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.03697819635272026, loss=0.03415669500827789
I0210 09:22:55.297353 139813965014784 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.032228972762823105, loss=0.02901214361190796
I0210 09:23:27.287826 139976524166912 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.03798352926969528, loss=0.03374577686190605
I0210 09:23:59.262790 139813965014784 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.038813501596450806, loss=0.03444841131567955
I0210 09:24:31.362380 139976524166912 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.036894541233778, loss=0.035374678671360016
I0210 09:25:03.280766 139813965014784 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.035410329699516296, loss=0.032526809722185135
I0210 09:25:35.164776 139976524166912 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.04730254039168358, loss=0.029095865786075592
I0210 09:26:06.010528 140039251117888 spec.py:321] Evaluating on the training split.
I0210 09:27:42.742475 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 09:27:45.773341 140039251117888 spec.py:349] Evaluating on the test split.
I0210 09:27:51.238061 140039251117888 submission_runner.py:408] Time since start: 7323.47s, 	Step: 15696, 	{'train/accuracy': 0.9909314513206482, 'train/loss': 0.029750367626547813, 'train/mean_average_precision': 0.4212134215553861, 'validation/accuracy': 0.9868617057800293, 'validation/loss': 0.04417761042714119, 'validation/mean_average_precision': 0.2672141950776982, 'validation/num_examples': 43793, 'test/accuracy': 0.9860504269599915, 'test/loss': 0.047021593898534775, 'test/mean_average_precision': 0.2661189499958922, 'test/num_examples': 43793, 'score': 5055.322886943817, 'total_duration': 7323.467862606049, 'accumulated_submission_time': 5055.322886943817, 'accumulated_eval_time': 2266.796665430069, 'accumulated_logging_time': 0.9336240291595459}
I0210 09:27:51.256420 139799398483712 logging_writer.py:48] [15696] accumulated_eval_time=2266.796665, accumulated_logging_time=0.933624, accumulated_submission_time=5055.322887, global_step=15696, preemption_count=0, score=5055.322887, test/accuracy=0.986050, test/loss=0.047022, test/mean_average_precision=0.266119, test/num_examples=43793, total_duration=7323.467863, train/accuracy=0.990931, train/loss=0.029750, train/mean_average_precision=0.421213, validation/accuracy=0.986862, validation/loss=0.044178, validation/mean_average_precision=0.267214, validation/num_examples=43793
I0210 09:27:52.924293 139818172847872 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.028805425390601158, loss=0.02861906960606575
I0210 09:28:24.956768 139799398483712 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.04013954848051071, loss=0.03127732127904892
I0210 09:28:57.420253 139818172847872 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.037410929799079895, loss=0.035257864743471146
I0210 09:29:29.227292 139799398483712 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.03383374959230423, loss=0.032154422253370285
I0210 09:30:01.204504 139818172847872 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.032971661537885666, loss=0.031361524015665054
I0210 09:30:33.520874 139799398483712 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.05031660571694374, loss=0.03278974071145058
I0210 09:31:06.168016 139818172847872 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.05443894490599632, loss=0.027537871152162552
I0210 09:31:37.858976 139799398483712 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.03824922442436218, loss=0.03379715606570244
I0210 09:31:51.460028 140039251117888 spec.py:321] Evaluating on the training split.
I0210 09:33:25.337298 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 09:33:28.410829 140039251117888 spec.py:349] Evaluating on the test split.
I0210 09:33:31.408337 140039251117888 submission_runner.py:408] Time since start: 7663.64s, 	Step: 16444, 	{'train/accuracy': 0.9908322095870972, 'train/loss': 0.030448731034994125, 'train/mean_average_precision': 0.40309762966671503, 'validation/accuracy': 0.9867277145385742, 'validation/loss': 0.044124990701675415, 'validation/mean_average_precision': 0.26478223635318743, 'validation/num_examples': 43793, 'test/accuracy': 0.9859354496002197, 'test/loss': 0.04659931734204292, 'test/mean_average_precision': 0.2583384105555724, 'test/num_examples': 43793, 'score': 5295.4963212013245, 'total_duration': 7663.6381068229675, 'accumulated_submission_time': 5295.4963212013245, 'accumulated_eval_time': 2366.7448992729187, 'accumulated_logging_time': 0.9629554748535156}
I0210 09:33:31.427784 139813965014784 logging_writer.py:48] [16444] accumulated_eval_time=2366.744899, accumulated_logging_time=0.962955, accumulated_submission_time=5295.496321, global_step=16444, preemption_count=0, score=5295.496321, test/accuracy=0.985935, test/loss=0.046599, test/mean_average_precision=0.258338, test/num_examples=43793, total_duration=7663.638107, train/accuracy=0.990832, train/loss=0.030449, train/mean_average_precision=0.403098, validation/accuracy=0.986728, validation/loss=0.044125, validation/mean_average_precision=0.264782, validation/num_examples=43793
I0210 09:33:49.571130 139871926159104 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.03700167313218117, loss=0.03384784236550331
I0210 09:34:21.314635 139813965014784 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.03787640482187271, loss=0.03351486474275589
I0210 09:34:53.037509 139871926159104 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.04605842009186745, loss=0.03203720599412918
I0210 09:35:24.914933 139813965014784 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.05387749895453453, loss=0.032243404537439346
I0210 09:35:56.753071 139871926159104 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.054315268993377686, loss=0.03290582820773125
I0210 09:36:29.057705 139813965014784 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.03641761839389801, loss=0.03130387142300606
I0210 09:37:01.058657 139871926159104 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.04450603201985359, loss=0.03261403739452362
I0210 09:37:31.638595 140039251117888 spec.py:321] Evaluating on the training split.
I0210 09:39:05.523809 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 09:39:08.564361 140039251117888 spec.py:349] Evaluating on the test split.
I0210 09:39:11.653960 140039251117888 submission_runner.py:408] Time since start: 8003.88s, 	Step: 17196, 	{'train/accuracy': 0.9909189939498901, 'train/loss': 0.03002912551164627, 'train/mean_average_precision': 0.41130995483938027, 'validation/accuracy': 0.9866400361061096, 'validation/loss': 0.04415911063551903, 'validation/mean_average_precision': 0.2626244980078488, 'validation/num_examples': 43793, 'test/accuracy': 0.9858874082565308, 'test/loss': 0.04647881165146828, 'test/mean_average_precision': 0.2605033970119208, 'test/num_examples': 43793, 'score': 5535.677088022232, 'total_duration': 8003.883762598038, 'accumulated_submission_time': 5535.677088022232, 'accumulated_eval_time': 2466.7602257728577, 'accumulated_logging_time': 0.9934120178222656}
I0210 09:39:11.672764 139799398483712 logging_writer.py:48] [17196] accumulated_eval_time=2466.760226, accumulated_logging_time=0.993412, accumulated_submission_time=5535.677088, global_step=17196, preemption_count=0, score=5535.677088, test/accuracy=0.985887, test/loss=0.046479, test/mean_average_precision=0.260503, test/num_examples=43793, total_duration=8003.883763, train/accuracy=0.990919, train/loss=0.030029, train/mean_average_precision=0.411310, validation/accuracy=0.986640, validation/loss=0.044159, validation/mean_average_precision=0.262624, validation/num_examples=43793
I0210 09:39:13.308420 139818172847872 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.0718710646033287, loss=0.03120250813663006
I0210 09:39:45.589834 139799398483712 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.03732601925730705, loss=0.03058060258626938
I0210 09:40:18.015131 139818172847872 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.03697257861495018, loss=0.03224292770028114
I0210 09:40:50.398540 139799398483712 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.04970995709300041, loss=0.03350745886564255
I0210 09:41:22.699389 139818172847872 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.03172214329242706, loss=0.030360419303178787
I0210 09:41:54.478442 139799398483712 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.04900891333818436, loss=0.03044050745666027
I0210 09:42:26.546557 139818172847872 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.047186627984046936, loss=0.03328217566013336
I0210 09:42:58.664953 139799398483712 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.04398620128631592, loss=0.028748922049999237
I0210 09:43:11.843390 140039251117888 spec.py:321] Evaluating on the training split.
I0210 09:44:48.198865 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 09:44:51.247601 140039251117888 spec.py:349] Evaluating on the test split.
I0210 09:44:54.254393 140039251117888 submission_runner.py:408] Time since start: 8346.48s, 	Step: 17942, 	{'train/accuracy': 0.9906976222991943, 'train/loss': 0.030496561899781227, 'train/mean_average_precision': 0.3988523759511924, 'validation/accuracy': 0.9869071245193481, 'validation/loss': 0.0442199669778347, 'validation/mean_average_precision': 0.2648747445150785, 'validation/num_examples': 43793, 'test/accuracy': 0.9860824346542358, 'test/loss': 0.046835239976644516, 'test/mean_average_precision': 0.26844017790901564, 'test/num_examples': 43793, 'score': 5775.816581726074, 'total_duration': 8346.484165668488, 'accumulated_submission_time': 5775.816581726074, 'accumulated_eval_time': 2569.17115855217, 'accumulated_logging_time': 1.0242016315460205}
I0210 09:44:54.273349 139813965014784 logging_writer.py:48] [17942] accumulated_eval_time=2569.171159, accumulated_logging_time=1.024202, accumulated_submission_time=5775.816582, global_step=17942, preemption_count=0, score=5775.816582, test/accuracy=0.986082, test/loss=0.046835, test/mean_average_precision=0.268440, test/num_examples=43793, total_duration=8346.484166, train/accuracy=0.990698, train/loss=0.030497, train/mean_average_precision=0.398852, validation/accuracy=0.986907, validation/loss=0.044220, validation/mean_average_precision=0.264875, validation/num_examples=43793
I0210 09:45:13.225407 139871926159104 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.04245081916451454, loss=0.03605649247765541
I0210 09:45:45.106349 139813965014784 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.03362966328859329, loss=0.02920054830610752
I0210 09:46:16.871603 139871926159104 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.046888336539268494, loss=0.029444629326462746
I0210 09:46:48.853668 139813965014784 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.04570095241069794, loss=0.03503294661641121
I0210 09:47:20.763646 139871926159104 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.04094328731298447, loss=0.03179815039038658
I0210 09:47:52.512811 139813965014784 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.037476055324077606, loss=0.03388717770576477
I0210 09:48:24.543993 139871926159104 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.044862180948257446, loss=0.032944925129413605
I0210 09:48:54.430388 140039251117888 spec.py:321] Evaluating on the training split.
I0210 09:50:27.476378 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 09:50:30.634771 140039251117888 spec.py:349] Evaluating on the test split.
I0210 09:50:33.707719 140039251117888 submission_runner.py:408] Time since start: 8685.94s, 	Step: 18695, 	{'train/accuracy': 0.9908912777900696, 'train/loss': 0.02989709936082363, 'train/mean_average_precision': 0.4184613505153205, 'validation/accuracy': 0.9868507385253906, 'validation/loss': 0.04404584690928459, 'validation/mean_average_precision': 0.2643315627503256, 'validation/num_examples': 43793, 'test/accuracy': 0.9860491752624512, 'test/loss': 0.04678769409656525, 'test/mean_average_precision': 0.27347083800907224, 'test/num_examples': 43793, 'score': 6015.941589832306, 'total_duration': 8685.937515974045, 'accumulated_submission_time': 6015.941589832306, 'accumulated_eval_time': 2668.448446750641, 'accumulated_logging_time': 1.0554554462432861}
I0210 09:50:33.727848 139799398483712 logging_writer.py:48] [18695] accumulated_eval_time=2668.448447, accumulated_logging_time=1.055455, accumulated_submission_time=6015.941590, global_step=18695, preemption_count=0, score=6015.941590, test/accuracy=0.986049, test/loss=0.046788, test/mean_average_precision=0.273471, test/num_examples=43793, total_duration=8685.937516, train/accuracy=0.990891, train/loss=0.029897, train/mean_average_precision=0.418461, validation/accuracy=0.986851, validation/loss=0.044046, validation/mean_average_precision=0.264332, validation/num_examples=43793
I0210 09:50:35.889526 139976524166912 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.04636430740356445, loss=0.03089982457458973
I0210 09:51:07.893640 139799398483712 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.03883236274123192, loss=0.031069792807102203
I0210 09:51:39.501554 139976524166912 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.039849426597356796, loss=0.03430565446615219
I0210 09:52:11.602971 139799398483712 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.04696006327867508, loss=0.036787934601306915
I0210 09:52:43.377580 139976524166912 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.04180360585451126, loss=0.03292366489768028
I0210 09:53:15.441548 139799398483712 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.04560088738799095, loss=0.032505203038454056
I0210 09:53:47.600129 139976524166912 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.03899303451180458, loss=0.032766371965408325
I0210 09:54:19.676429 139799398483712 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.042822375893592834, loss=0.032252296805381775
I0210 09:54:33.829316 140039251117888 spec.py:321] Evaluating on the training split.
I0210 09:56:10.368678 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 09:56:13.408809 140039251117888 spec.py:349] Evaluating on the test split.
I0210 09:56:16.387496 140039251117888 submission_runner.py:408] Time since start: 9028.62s, 	Step: 19445, 	{'train/accuracy': 0.9909371137619019, 'train/loss': 0.02975236065685749, 'train/mean_average_precision': 0.4243669747970852, 'validation/accuracy': 0.9867671132087708, 'validation/loss': 0.04395902156829834, 'validation/mean_average_precision': 0.267616429782502, 'validation/num_examples': 43793, 'test/accuracy': 0.9860230684280396, 'test/loss': 0.04655415937304497, 'test/mean_average_precision': 0.2650854230397668, 'test/num_examples': 43793, 'score': 6256.012636184692, 'total_duration': 9028.61729645729, 'accumulated_submission_time': 6256.012636184692, 'accumulated_eval_time': 2771.006584405899, 'accumulated_logging_time': 1.0866594314575195}
I0210 09:56:16.406475 139813965014784 logging_writer.py:48] [19445] accumulated_eval_time=2771.006584, accumulated_logging_time=1.086659, accumulated_submission_time=6256.012636, global_step=19445, preemption_count=0, score=6256.012636, test/accuracy=0.986023, test/loss=0.046554, test/mean_average_precision=0.265085, test/num_examples=43793, total_duration=9028.617296, train/accuracy=0.990937, train/loss=0.029752, train/mean_average_precision=0.424367, validation/accuracy=0.986767, validation/loss=0.043959, validation/mean_average_precision=0.267616, validation/num_examples=43793
I0210 09:56:34.147211 139818172847872 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.0424140989780426, loss=0.031566597521305084
I0210 09:57:06.144368 139813965014784 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.04420845955610275, loss=0.030906368046998978
I0210 09:57:37.780785 139818172847872 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.03742397949099541, loss=0.030194181948900223
I0210 09:58:09.744240 139813965014784 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.0570475235581398, loss=0.031071698293089867
I0210 09:58:41.264974 139818172847872 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.04213980212807655, loss=0.02807093970477581
I0210 09:59:12.966275 139813965014784 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.05467994511127472, loss=0.03279668837785721
I0210 09:59:44.621107 139818172847872 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.04559653624892235, loss=0.03042161837220192
I0210 10:00:16.152297 139813965014784 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.0533130057156086, loss=0.029209494590759277
I0210 10:00:16.469769 140039251117888 spec.py:321] Evaluating on the training split.
I0210 10:01:48.171411 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 10:01:51.230210 140039251117888 spec.py:349] Evaluating on the test split.
I0210 10:01:54.218717 140039251117888 submission_runner.py:408] Time since start: 9366.45s, 	Step: 20202, 	{'train/accuracy': 0.9910696148872375, 'train/loss': 0.029148319736123085, 'train/mean_average_precision': 0.4278207878860953, 'validation/accuracy': 0.9869027137756348, 'validation/loss': 0.04428491368889809, 'validation/mean_average_precision': 0.2742441528896123, 'validation/num_examples': 43793, 'test/accuracy': 0.9861186742782593, 'test/loss': 0.04707004502415657, 'test/mean_average_precision': 0.26815792306761826, 'test/num_examples': 43793, 'score': 6496.045476913452, 'total_duration': 9366.448516368866, 'accumulated_submission_time': 6496.045476913452, 'accumulated_eval_time': 2868.755485534668, 'accumulated_logging_time': 1.1163904666900635}
I0210 10:01:54.237893 139799398483712 logging_writer.py:48] [20202] accumulated_eval_time=2868.755486, accumulated_logging_time=1.116390, accumulated_submission_time=6496.045477, global_step=20202, preemption_count=0, score=6496.045477, test/accuracy=0.986119, test/loss=0.047070, test/mean_average_precision=0.268158, test/num_examples=43793, total_duration=9366.448516, train/accuracy=0.991070, train/loss=0.029148, train/mean_average_precision=0.427821, validation/accuracy=0.986903, validation/loss=0.044285, validation/mean_average_precision=0.274244, validation/num_examples=43793
I0210 10:02:25.939093 139871926159104 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.0561944805085659, loss=0.03149921074509621
I0210 10:02:57.789276 139799398483712 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.03283924609422684, loss=0.027161171659827232
I0210 10:03:29.542193 139871926159104 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.047009218484163284, loss=0.03196052089333534
I0210 10:04:00.850020 139799398483712 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.046205878257751465, loss=0.031940046697854996
I0210 10:04:33.197252 139871926159104 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.037850379943847656, loss=0.026092959567904472
I0210 10:05:04.810813 139799398483712 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.048873696476221085, loss=0.028988631442189217
I0210 10:05:36.428637 139871926159104 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.0438944511115551, loss=0.03313038498163223
I0210 10:05:54.281574 140039251117888 spec.py:321] Evaluating on the training split.
I0210 10:07:25.692585 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 10:07:28.812581 140039251117888 spec.py:349] Evaluating on the test split.
I0210 10:07:31.819814 140039251117888 submission_runner.py:408] Time since start: 9704.05s, 	Step: 20957, 	{'train/accuracy': 0.9910153150558472, 'train/loss': 0.029240183532238007, 'train/mean_average_precision': 0.4452358825783558, 'validation/accuracy': 0.9868357181549072, 'validation/loss': 0.044326040893793106, 'validation/mean_average_precision': 0.2762801502975392, 'validation/num_examples': 43793, 'test/accuracy': 0.9860588312149048, 'test/loss': 0.04683621600270271, 'test/mean_average_precision': 0.27502654054477293, 'test/num_examples': 43793, 'score': 6736.057872056961, 'total_duration': 9704.049597024918, 'accumulated_submission_time': 6736.057872056961, 'accumulated_eval_time': 2966.293663740158, 'accumulated_logging_time': 1.1478157043457031}
I0210 10:07:31.839310 139818172847872 logging_writer.py:48] [20957] accumulated_eval_time=2966.293664, accumulated_logging_time=1.147816, accumulated_submission_time=6736.057872, global_step=20957, preemption_count=0, score=6736.057872, test/accuracy=0.986059, test/loss=0.046836, test/mean_average_precision=0.275027, test/num_examples=43793, total_duration=9704.049597, train/accuracy=0.991015, train/loss=0.029240, train/mean_average_precision=0.445236, validation/accuracy=0.986836, validation/loss=0.044326, validation/mean_average_precision=0.276280, validation/num_examples=43793
I0210 10:07:45.928563 139976524166912 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.038502633571624756, loss=0.028370214626193047
I0210 10:08:17.906446 139818172847872 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.04986000806093216, loss=0.02990485168993473
I0210 10:08:49.772347 139976524166912 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.0513615719974041, loss=0.02988414280116558
I0210 10:09:21.503464 139818172847872 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.04629652202129364, loss=0.03192604333162308
I0210 10:09:53.232548 139976524166912 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.05577828735113144, loss=0.0317075252532959
I0210 10:10:25.096637 139818172847872 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.057395171374082565, loss=0.028287244960665703
I0210 10:10:56.645582 139976524166912 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.05714261159300804, loss=0.031524598598480225
I0210 10:11:28.625033 139818172847872 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.055166251957416534, loss=0.028660675510764122
I0210 10:11:32.051239 140039251117888 spec.py:321] Evaluating on the training split.
I0210 10:13:10.916265 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 10:13:14.031684 140039251117888 spec.py:349] Evaluating on the test split.
I0210 10:13:17.077693 140039251117888 submission_runner.py:408] Time since start: 10049.31s, 	Step: 21712, 	{'train/accuracy': 0.9912437200546265, 'train/loss': 0.02839926816523075, 'train/mean_average_precision': 0.4604391958469054, 'validation/accuracy': 0.9868929386138916, 'validation/loss': 0.04405832663178444, 'validation/mean_average_precision': 0.2723770620871654, 'validation/num_examples': 43793, 'test/accuracy': 0.9860668182373047, 'test/loss': 0.04659341275691986, 'test/mean_average_precision': 0.273082646921358, 'test/num_examples': 43793, 'score': 6976.239338636398, 'total_duration': 10049.307490348816, 'accumulated_submission_time': 6976.239338636398, 'accumulated_eval_time': 3071.3200681209564, 'accumulated_logging_time': 1.1784136295318604}
I0210 10:13:17.097266 139813965014784 logging_writer.py:48] [21712] accumulated_eval_time=3071.320068, accumulated_logging_time=1.178414, accumulated_submission_time=6976.239339, global_step=21712, preemption_count=0, score=6976.239339, test/accuracy=0.986067, test/loss=0.046593, test/mean_average_precision=0.273083, test/num_examples=43793, total_duration=10049.307490, train/accuracy=0.991244, train/loss=0.028399, train/mean_average_precision=0.460439, validation/accuracy=0.986893, validation/loss=0.044058, validation/mean_average_precision=0.272377, validation/num_examples=43793
I0210 10:13:45.415086 139871926159104 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.04107610136270523, loss=0.02784941904246807
I0210 10:14:17.070956 139813965014784 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.04847380518913269, loss=0.0354478545486927
I0210 10:14:48.845530 139871926159104 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.05301262065768242, loss=0.029886391013860703
I0210 10:15:20.608148 139813965014784 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.060280077159404755, loss=0.03144386038184166
I0210 10:15:52.522500 139871926159104 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.04777287691831589, loss=0.02801770530641079
I0210 10:16:24.360136 139813965014784 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.05345221608877182, loss=0.03179444372653961
I0210 10:16:56.350068 139871926159104 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.06512957066297531, loss=0.027782293036580086
I0210 10:17:17.134398 140039251117888 spec.py:321] Evaluating on the training split.
I0210 10:18:52.119242 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 10:18:55.274326 140039251117888 spec.py:349] Evaluating on the test split.
I0210 10:18:58.424042 140039251117888 submission_runner.py:408] Time since start: 10390.65s, 	Step: 22467, 	{'train/accuracy': 0.991296112537384, 'train/loss': 0.028296872973442078, 'train/mean_average_precision': 0.44887732756269577, 'validation/accuracy': 0.9868758916854858, 'validation/loss': 0.04404003173112869, 'validation/mean_average_precision': 0.2674766798136144, 'validation/num_examples': 43793, 'test/accuracy': 0.98606938123703, 'test/loss': 0.046771008521318436, 'test/mean_average_precision': 0.2726564770947386, 'test/num_examples': 43793, 'score': 7216.246557235718, 'total_duration': 10390.653838157654, 'accumulated_submission_time': 7216.246557235718, 'accumulated_eval_time': 3172.609664440155, 'accumulated_logging_time': 1.208643913269043}
I0210 10:18:58.444508 139799398483712 logging_writer.py:48] [22467] accumulated_eval_time=3172.609664, accumulated_logging_time=1.208644, accumulated_submission_time=7216.246557, global_step=22467, preemption_count=0, score=7216.246557, test/accuracy=0.986069, test/loss=0.046771, test/mean_average_precision=0.272656, test/num_examples=43793, total_duration=10390.653838, train/accuracy=0.991296, train/loss=0.028297, train/mean_average_precision=0.448877, validation/accuracy=0.986876, validation/loss=0.044040, validation/mean_average_precision=0.267477, validation/num_examples=43793
I0210 10:19:09.320492 139976524166912 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.04820739105343819, loss=0.027081644162535667
I0210 10:19:41.224199 139799398483712 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.05359493941068649, loss=0.031516026705503464
I0210 10:20:13.151184 139976524166912 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.045308344066143036, loss=0.027526993304491043
I0210 10:20:44.693609 139799398483712 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.05878307297825813, loss=0.02935594692826271
I0210 10:21:16.148560 139976524166912 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.04422712326049805, loss=0.02990030124783516
I0210 10:21:48.401707 139799398483712 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.05332259088754654, loss=0.03115183860063553
I0210 10:22:20.114090 139976524166912 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.05181952938437462, loss=0.02595461905002594
I0210 10:22:51.825488 139799398483712 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.03875327110290527, loss=0.02706748992204666
I0210 10:22:58.625337 140039251117888 spec.py:321] Evaluating on the training split.
I0210 10:24:36.441394 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 10:24:39.563846 140039251117888 spec.py:349] Evaluating on the test split.
I0210 10:24:42.696133 140039251117888 submission_runner.py:408] Time since start: 10734.93s, 	Step: 23222, 	{'train/accuracy': 0.9915757775306702, 'train/loss': 0.027429115027189255, 'train/mean_average_precision': 0.46926767989383444, 'validation/accuracy': 0.9869286417961121, 'validation/loss': 0.043794091790914536, 'validation/mean_average_precision': 0.2775344659929296, 'validation/num_examples': 43793, 'test/accuracy': 0.9861102104187012, 'test/loss': 0.04683634638786316, 'test/mean_average_precision': 0.2697631826066058, 'test/num_examples': 43793, 'score': 7456.395405292511, 'total_duration': 10734.925931692123, 'accumulated_submission_time': 7456.395405292511, 'accumulated_eval_time': 3276.680414915085, 'accumulated_logging_time': 1.241858959197998}
I0210 10:24:42.715616 139813965014784 logging_writer.py:48] [23222] accumulated_eval_time=3276.680415, accumulated_logging_time=1.241859, accumulated_submission_time=7456.395405, global_step=23222, preemption_count=0, score=7456.395405, test/accuracy=0.986110, test/loss=0.046836, test/mean_average_precision=0.269763, test/num_examples=43793, total_duration=10734.925932, train/accuracy=0.991576, train/loss=0.027429, train/mean_average_precision=0.469268, validation/accuracy=0.986929, validation/loss=0.043794, validation/mean_average_precision=0.277534, validation/num_examples=43793
I0210 10:25:08.306125 139871926159104 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.059288859367370605, loss=0.0355500690639019
I0210 10:25:40.754057 139813965014784 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.05468825623393059, loss=0.03251639008522034
I0210 10:26:13.018141 139871926159104 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.04982610419392586, loss=0.029437487944960594
I0210 10:26:45.353543 139813965014784 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.06688758730888367, loss=0.028798334300518036
I0210 10:27:17.490636 139871926159104 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.060597315430641174, loss=0.028614195063710213
I0210 10:27:49.581771 139813965014784 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.05221966281533241, loss=0.035040441900491714
I0210 10:28:21.513208 139871926159104 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.04605104401707649, loss=0.0317898653447628
I0210 10:28:42.854310 140039251117888 spec.py:321] Evaluating on the training split.
I0210 10:30:16.940355 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 10:30:20.091474 140039251117888 spec.py:349] Evaluating on the test split.
I0210 10:30:23.183272 140039251117888 submission_runner.py:408] Time since start: 11075.41s, 	Step: 23968, 	{'train/accuracy': 0.991378128528595, 'train/loss': 0.028135981410741806, 'train/mean_average_precision': 0.46534005699604974, 'validation/accuracy': 0.9869207739830017, 'validation/loss': 0.04384111613035202, 'validation/mean_average_precision': 0.27828051068360626, 'validation/num_examples': 43793, 'test/accuracy': 0.986065149307251, 'test/loss': 0.046624280512332916, 'test/mean_average_precision': 0.27555730666603345, 'test/num_examples': 43793, 'score': 7696.503025054932, 'total_duration': 11075.413035392761, 'accumulated_submission_time': 7696.503025054932, 'accumulated_eval_time': 3377.009297847748, 'accumulated_logging_time': 1.2736289501190186}
I0210 10:30:23.204155 139798976751360 logging_writer.py:48] [23968] accumulated_eval_time=3377.009298, accumulated_logging_time=1.273629, accumulated_submission_time=7696.503025, global_step=23968, preemption_count=0, score=7696.503025, test/accuracy=0.986065, test/loss=0.046624, test/mean_average_precision=0.275557, test/num_examples=43793, total_duration=11075.413035, train/accuracy=0.991378, train/loss=0.028136, train/mean_average_precision=0.465340, validation/accuracy=0.986921, validation/loss=0.043841, validation/mean_average_precision=0.278281, validation/num_examples=43793
I0210 10:30:33.940986 139818172847872 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.057046011090278625, loss=0.031609103083610535
I0210 10:31:05.922994 139798976751360 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.055794261395931244, loss=0.02736327052116394
I0210 10:31:38.418449 139818172847872 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.045160386711359024, loss=0.027289144694805145
I0210 10:32:10.943993 139798976751360 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.054604656994342804, loss=0.03066503070294857
I0210 10:32:43.052555 139818172847872 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.05470656231045723, loss=0.038378067314624786
I0210 10:33:15.576220 139798976751360 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.05619765445590019, loss=0.03137300908565521
I0210 10:33:47.673544 139818172847872 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.05252150818705559, loss=0.031366486102342606
I0210 10:34:20.331912 139798976751360 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.04970811307430267, loss=0.03042648173868656
I0210 10:34:23.274469 140039251117888 spec.py:321] Evaluating on the training split.
I0210 10:35:59.417844 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 10:36:02.743686 140039251117888 spec.py:349] Evaluating on the test split.
I0210 10:36:05.900439 140039251117888 submission_runner.py:408] Time since start: 11418.13s, 	Step: 24710, 	{'train/accuracy': 0.9912890195846558, 'train/loss': 0.02855648472905159, 'train/mean_average_precision': 0.446825708880746, 'validation/accuracy': 0.9868153929710388, 'validation/loss': 0.04399457201361656, 'validation/mean_average_precision': 0.2730327227108191, 'validation/num_examples': 43793, 'test/accuracy': 0.9860036373138428, 'test/loss': 0.046485982835292816, 'test/mean_average_precision': 0.2684493736634549, 'test/num_examples': 43793, 'score': 7936.5431044101715, 'total_duration': 11418.130235671997, 'accumulated_submission_time': 7936.5431044101715, 'accumulated_eval_time': 3479.635217189789, 'accumulated_logging_time': 1.3058831691741943}
I0210 10:36:05.921323 139799398483712 logging_writer.py:48] [24710] accumulated_eval_time=3479.635217, accumulated_logging_time=1.305883, accumulated_submission_time=7936.543104, global_step=24710, preemption_count=0, score=7936.543104, test/accuracy=0.986004, test/loss=0.046486, test/mean_average_precision=0.268449, test/num_examples=43793, total_duration=11418.130236, train/accuracy=0.991289, train/loss=0.028556, train/mean_average_precision=0.446826, validation/accuracy=0.986815, validation/loss=0.043995, validation/mean_average_precision=0.273033, validation/num_examples=43793
I0210 10:36:35.150937 139813965014784 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.06418632715940475, loss=0.032323144376277924
I0210 10:37:07.618079 139799398483712 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.04553936421871185, loss=0.02750319056212902
I0210 10:37:39.607881 139813965014784 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.0528719536960125, loss=0.03156733512878418
I0210 10:38:12.115639 139799398483712 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.05514013022184372, loss=0.03054068796336651
I0210 10:38:44.290203 139813965014784 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.05315416306257248, loss=0.031533706933259964
I0210 10:39:16.667068 139799398483712 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.052722688764333725, loss=0.03555985540151596
I0210 10:39:48.517731 139813965014784 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.07397325336933136, loss=0.032921209931373596
I0210 10:40:06.164677 140039251117888 spec.py:321] Evaluating on the training split.
I0210 10:41:44.545267 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 10:41:47.598275 140039251117888 spec.py:349] Evaluating on the test split.
I0210 10:41:50.547359 140039251117888 submission_runner.py:408] Time since start: 11762.78s, 	Step: 25457, 	{'train/accuracy': 0.9913530349731445, 'train/loss': 0.028152110055088997, 'train/mean_average_precision': 0.46453359169836816, 'validation/accuracy': 0.9869887232780457, 'validation/loss': 0.04381720349192619, 'validation/mean_average_precision': 0.27789378919818764, 'validation/num_examples': 43793, 'test/accuracy': 0.9860908389091492, 'test/loss': 0.04651831090450287, 'test/mean_average_precision': 0.27900299528478145, 'test/num_examples': 43793, 'score': 8176.7564833164215, 'total_duration': 11762.777160644531, 'accumulated_submission_time': 8176.7564833164215, 'accumulated_eval_time': 3584.0178577899933, 'accumulated_logging_time': 1.3375873565673828}
I0210 10:41:50.567650 139799390091008 logging_writer.py:48] [25457] accumulated_eval_time=3584.017858, accumulated_logging_time=1.337587, accumulated_submission_time=8176.756483, global_step=25457, preemption_count=0, score=8176.756483, test/accuracy=0.986091, test/loss=0.046518, test/mean_average_precision=0.279003, test/num_examples=43793, total_duration=11762.777161, train/accuracy=0.991353, train/loss=0.028152, train/mean_average_precision=0.464534, validation/accuracy=0.986989, validation/loss=0.043817, validation/mean_average_precision=0.277894, validation/num_examples=43793
I0210 10:42:04.795758 139818172847872 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.07510104775428772, loss=0.03202735260128975
I0210 10:42:36.757464 139799390091008 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.06631791591644287, loss=0.03184397146105766
I0210 10:43:08.370440 139818172847872 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.050750959664583206, loss=0.031391408294439316
I0210 10:43:39.957457 139799390091008 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.05489576235413551, loss=0.02690083347260952
I0210 10:44:11.307480 139818172847872 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.0580870546400547, loss=0.028842460364103317
I0210 10:44:42.987358 139799390091008 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.0651625543832779, loss=0.029183056205511093
I0210 10:45:14.356116 139818172847872 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.06096085533499718, loss=0.03599924594163895
I0210 10:45:46.098315 139799390091008 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.05994041636586189, loss=0.027999790385365486
I0210 10:45:50.818080 140039251117888 spec.py:321] Evaluating on the training split.
I0210 10:47:23.177286 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 10:47:26.296452 140039251117888 spec.py:349] Evaluating on the test split.
I0210 10:47:29.328333 140039251117888 submission_runner.py:408] Time since start: 12101.56s, 	Step: 26216, 	{'train/accuracy': 0.9911909699440002, 'train/loss': 0.02844357304275036, 'train/mean_average_precision': 0.44178144617310267, 'validation/accuracy': 0.98688805103302, 'validation/loss': 0.044134438037872314, 'validation/mean_average_precision': 0.2709613871925868, 'validation/num_examples': 43793, 'test/accuracy': 0.9861236810684204, 'test/loss': 0.04700309410691261, 'test/mean_average_precision': 0.26599435220298556, 'test/num_examples': 43793, 'score': 8416.975894927979, 'total_duration': 12101.558113098145, 'accumulated_submission_time': 8416.975894927979, 'accumulated_eval_time': 3682.5280437469482, 'accumulated_logging_time': 1.3701817989349365}
I0210 10:47:29.348776 139799398483712 logging_writer.py:48] [26216] accumulated_eval_time=3682.528044, accumulated_logging_time=1.370182, accumulated_submission_time=8416.975895, global_step=26216, preemption_count=0, score=8416.975895, test/accuracy=0.986124, test/loss=0.047003, test/mean_average_precision=0.265994, test/num_examples=43793, total_duration=12101.558113, train/accuracy=0.991191, train/loss=0.028444, train/mean_average_precision=0.441781, validation/accuracy=0.986888, validation/loss=0.044134, validation/mean_average_precision=0.270961, validation/num_examples=43793
I0210 10:47:56.497684 139871926159104 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.04617755487561226, loss=0.029825616627931595
I0210 10:48:28.181273 139799398483712 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.05226464942097664, loss=0.03237219154834747
I0210 10:49:00.267516 139871926159104 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.04460402578115463, loss=0.0316011868417263
I0210 10:49:32.133015 139799398483712 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.0492999330163002, loss=0.03110458329319954
I0210 10:50:04.198929 139871926159104 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.059276923537254333, loss=0.028466371819376945
I0210 10:50:36.140331 139799398483712 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.061519380658864975, loss=0.032300349324941635
I0210 10:51:07.902351 139871926159104 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.05313441902399063, loss=0.028069134801626205
I0210 10:51:29.463405 140039251117888 spec.py:321] Evaluating on the training split.
I0210 10:53:04.649067 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 10:53:07.662552 140039251117888 spec.py:349] Evaluating on the test split.
I0210 10:53:10.622610 140039251117888 submission_runner.py:408] Time since start: 12442.85s, 	Step: 26968, 	{'train/accuracy': 0.9912831783294678, 'train/loss': 0.02814350835978985, 'train/mean_average_precision': 0.46464755492582693, 'validation/accuracy': 0.9869964718818665, 'validation/loss': 0.04417134448885918, 'validation/mean_average_precision': 0.2806928255369479, 'validation/num_examples': 43793, 'test/accuracy': 0.986162006855011, 'test/loss': 0.04697659984230995, 'test/mean_average_precision': 0.27542822452749105, 'test/num_examples': 43793, 'score': 8657.060805559158, 'total_duration': 12442.852400064468, 'accumulated_submission_time': 8657.060805559158, 'accumulated_eval_time': 3783.687194108963, 'accumulated_logging_time': 1.4013991355895996}
I0210 10:53:10.642831 139813965014784 logging_writer.py:48] [26968] accumulated_eval_time=3783.687194, accumulated_logging_time=1.401399, accumulated_submission_time=8657.060806, global_step=26968, preemption_count=0, score=8657.060806, test/accuracy=0.986162, test/loss=0.046977, test/mean_average_precision=0.275428, test/num_examples=43793, total_duration=12442.852400, train/accuracy=0.991283, train/loss=0.028144, train/mean_average_precision=0.464648, validation/accuracy=0.986996, validation/loss=0.044171, validation/mean_average_precision=0.280693, validation/num_examples=43793
I0210 10:53:21.217539 139818172847872 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.05681022256612778, loss=0.028684720396995544
I0210 10:53:52.686490 139813965014784 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.06068146973848343, loss=0.030690383166074753
I0210 10:54:24.514119 139818172847872 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.051864009350538254, loss=0.029115064069628716
I0210 10:54:56.296086 139813965014784 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.06282773613929749, loss=0.026288717985153198
I0210 10:55:28.268088 139818172847872 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.061427343636751175, loss=0.034386876970529556
I0210 10:55:59.663670 139813965014784 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.048507802188396454, loss=0.028800882399082184
I0210 10:56:31.647001 139818172847872 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.054728057235479355, loss=0.027924885973334312
I0210 10:57:03.599668 139813965014784 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.04144776985049248, loss=0.02581050992012024
I0210 10:57:10.733906 140039251117888 spec.py:321] Evaluating on the training split.
I0210 10:58:46.799631 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 10:58:50.275425 140039251117888 spec.py:349] Evaluating on the test split.
I0210 10:58:53.713133 140039251117888 submission_runner.py:408] Time since start: 12785.94s, 	Step: 27724, 	{'train/accuracy': 0.9916079044342041, 'train/loss': 0.027133068069815636, 'train/mean_average_precision': 0.4827749643581413, 'validation/accuracy': 0.986968457698822, 'validation/loss': 0.0441259928047657, 'validation/mean_average_precision': 0.27225728252014003, 'validation/num_examples': 43793, 'test/accuracy': 0.9861776232719421, 'test/loss': 0.04679429903626442, 'test/mean_average_precision': 0.27168875551218347, 'test/num_examples': 43793, 'score': 8897.122105360031, 'total_duration': 12785.942913293839, 'accumulated_submission_time': 8897.122105360031, 'accumulated_eval_time': 3886.666358947754, 'accumulated_logging_time': 1.4324800968170166}
I0210 10:58:53.737272 139799390091008 logging_writer.py:48] [27724] accumulated_eval_time=3886.666359, accumulated_logging_time=1.432480, accumulated_submission_time=8897.122105, global_step=27724, preemption_count=0, score=8897.122105, test/accuracy=0.986178, test/loss=0.046794, test/mean_average_precision=0.271689, test/num_examples=43793, total_duration=12785.942913, train/accuracy=0.991608, train/loss=0.027133, train/mean_average_precision=0.482775, validation/accuracy=0.986968, validation/loss=0.044126, validation/mean_average_precision=0.272257, validation/num_examples=43793
I0210 10:59:19.110000 139871926159104 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.06933040916919708, loss=0.030514376237988472
I0210 10:59:51.498084 139799390091008 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.05982515215873718, loss=0.03323500603437424
I0210 11:00:23.410392 139871926159104 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.06108449399471283, loss=0.02995258942246437
I0210 11:00:55.334846 139799390091008 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.06035797297954559, loss=0.02596266008913517
I0210 11:01:27.048619 139871926159104 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.05052001029253006, loss=0.027082715183496475
I0210 11:01:58.760581 139799390091008 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.05950542539358139, loss=0.029010815545916557
I0210 11:02:30.606771 139871926159104 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.059080641716718674, loss=0.029740428552031517
I0210 11:02:53.839597 140039251117888 spec.py:321] Evaluating on the training split.
I0210 11:04:32.997434 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 11:04:36.118855 140039251117888 spec.py:349] Evaluating on the test split.
I0210 11:04:39.147259 140039251117888 submission_runner.py:408] Time since start: 13131.38s, 	Step: 28474, 	{'train/accuracy': 0.9916527271270752, 'train/loss': 0.027179455384612083, 'train/mean_average_precision': 0.48076338524130036, 'validation/accuracy': 0.9868791699409485, 'validation/loss': 0.044070225208997726, 'validation/mean_average_precision': 0.27541784470378794, 'validation/num_examples': 43793, 'test/accuracy': 0.986123263835907, 'test/loss': 0.046606939285993576, 'test/mean_average_precision': 0.2744048140787645, 'test/num_examples': 43793, 'score': 9137.191581249237, 'total_duration': 13131.377036809921, 'accumulated_submission_time': 9137.191581249237, 'accumulated_eval_time': 3991.973956346512, 'accumulated_logging_time': 1.4689247608184814}
I0210 11:04:39.168960 139799398483712 logging_writer.py:48] [28474] accumulated_eval_time=3991.973956, accumulated_logging_time=1.468925, accumulated_submission_time=9137.191581, global_step=28474, preemption_count=0, score=9137.191581, test/accuracy=0.986123, test/loss=0.046607, test/mean_average_precision=0.274405, test/num_examples=43793, total_duration=13131.377037, train/accuracy=0.991653, train/loss=0.027179, train/mean_average_precision=0.480763, validation/accuracy=0.986879, validation/loss=0.044070, validation/mean_average_precision=0.275418, validation/num_examples=43793
I0210 11:04:47.851799 139818172847872 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.06766814738512039, loss=0.031241290271282196
I0210 11:05:19.963103 139799398483712 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.053436554968357086, loss=0.030647708103060722
I0210 11:05:52.017244 139818172847872 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.057869888842105865, loss=0.031952865421772
I0210 11:06:24.209975 139799398483712 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.054166171699762344, loss=0.030529851093888283
I0210 11:06:56.424091 139818172847872 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.06677796691656113, loss=0.029214151203632355
I0210 11:07:28.702198 139799398483712 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.05973631888628006, loss=0.03028617799282074
I0210 11:08:00.554451 139818172847872 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.05182632803916931, loss=0.025652145966887474
I0210 11:08:32.420665 139799398483712 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.055079612880945206, loss=0.029576973989605904
I0210 11:08:39.427840 140039251117888 spec.py:321] Evaluating on the training split.
I0210 11:10:15.272493 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 11:10:18.407780 140039251117888 spec.py:349] Evaluating on the test split.
I0210 11:10:21.418405 140039251117888 submission_runner.py:408] Time since start: 13473.65s, 	Step: 29223, 	{'train/accuracy': 0.9917328357696533, 'train/loss': 0.026465030387043953, 'train/mean_average_precision': 0.5178903073222842, 'validation/accuracy': 0.9871068596839905, 'validation/loss': 0.04411550983786583, 'validation/mean_average_precision': 0.2825732089125649, 'validation/num_examples': 43793, 'test/accuracy': 0.986240804195404, 'test/loss': 0.04688546061515808, 'test/mean_average_precision': 0.27499786698549594, 'test/num_examples': 43793, 'score': 9377.420338869095, 'total_duration': 13473.64820098877, 'accumulated_submission_time': 9377.420338869095, 'accumulated_eval_time': 4093.9644737243652, 'accumulated_logging_time': 1.5014734268188477}
I0210 11:10:21.440304 139799390091008 logging_writer.py:48] [29223] accumulated_eval_time=4093.964474, accumulated_logging_time=1.501473, accumulated_submission_time=9377.420339, global_step=29223, preemption_count=0, score=9377.420339, test/accuracy=0.986241, test/loss=0.046885, test/mean_average_precision=0.274998, test/num_examples=43793, total_duration=13473.648201, train/accuracy=0.991733, train/loss=0.026465, train/mean_average_precision=0.517890, validation/accuracy=0.987107, validation/loss=0.044116, validation/mean_average_precision=0.282573, validation/num_examples=43793
I0210 11:10:46.457043 139813965014784 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.06152055412530899, loss=0.02831270545721054
I0210 11:11:18.884225 139799390091008 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.06128045916557312, loss=0.026952585205435753
I0210 11:11:50.902731 139813965014784 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.14767715334892273, loss=0.031093794852495193
I0210 11:12:22.901147 139799390091008 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.05990280956029892, loss=0.030285846441984177
I0210 11:12:54.598065 139813965014784 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.069568932056427, loss=0.032113317400217056
I0210 11:13:26.695399 139799390091008 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.07293671369552612, loss=0.029867947101593018
I0210 11:13:58.346295 139813965014784 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.05536434054374695, loss=0.029916204512119293
I0210 11:14:21.744819 140039251117888 spec.py:321] Evaluating on the training split.
I0210 11:15:58.471481 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 11:16:01.766611 140039251117888 spec.py:349] Evaluating on the test split.
I0210 11:16:04.808687 140039251117888 submission_runner.py:408] Time since start: 13817.04s, 	Step: 29972, 	{'train/accuracy': 0.9918742179870605, 'train/loss': 0.026252735406160355, 'train/mean_average_precision': 0.5005576253532122, 'validation/accuracy': 0.9870269298553467, 'validation/loss': 0.04420791566371918, 'validation/mean_average_precision': 0.28201485491354894, 'validation/num_examples': 43793, 'test/accuracy': 0.9862378239631653, 'test/loss': 0.046826593577861786, 'test/mean_average_precision': 0.28050102651211495, 'test/num_examples': 43793, 'score': 9617.694372415543, 'total_duration': 13817.03848195076, 'accumulated_submission_time': 9617.694372415543, 'accumulated_eval_time': 4197.028309106827, 'accumulated_logging_time': 1.5345180034637451}
I0210 11:16:04.830357 139799398483712 logging_writer.py:48] [29972] accumulated_eval_time=4197.028309, accumulated_logging_time=1.534518, accumulated_submission_time=9617.694372, global_step=29972, preemption_count=0, score=9617.694372, test/accuracy=0.986238, test/loss=0.046827, test/mean_average_precision=0.280501, test/num_examples=43793, total_duration=13817.038482, train/accuracy=0.991874, train/loss=0.026253, train/mean_average_precision=0.500558, validation/accuracy=0.987027, validation/loss=0.044208, validation/mean_average_precision=0.282015, validation/num_examples=43793
I0210 11:16:14.205019 139818172847872 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.05945281311869621, loss=0.0292045995593071
I0210 11:16:46.418041 139799398483712 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.06507816910743713, loss=0.02888031303882599
I0210 11:17:18.676590 139818172847872 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.058651819825172424, loss=0.028312930837273598
I0210 11:17:50.582556 139799398483712 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.0630612000823021, loss=0.030342448502779007
I0210 11:18:23.156090 139818172847872 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.04650043696165085, loss=0.03025021217763424
I0210 11:18:55.556461 139799398483712 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.05967726930975914, loss=0.03129924461245537
I0210 11:19:27.405588 139818172847872 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.05727706849575043, loss=0.028047580271959305
I0210 11:20:00.422027 139799398483712 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.0662364736199379, loss=0.02899962291121483
I0210 11:20:04.820878 140039251117888 spec.py:321] Evaluating on the training split.
I0210 11:21:45.645062 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 11:21:48.684872 140039251117888 spec.py:349] Evaluating on the test split.
I0210 11:21:51.696880 140039251117888 submission_runner.py:408] Time since start: 14163.93s, 	Step: 30714, 	{'train/accuracy': 0.9917741417884827, 'train/loss': 0.026466084644198418, 'train/mean_average_precision': 0.49740525993522233, 'validation/accuracy': 0.9868994355201721, 'validation/loss': 0.04427551105618477, 'validation/mean_average_precision': 0.2783416322284315, 'validation/num_examples': 43793, 'test/accuracy': 0.9860790371894836, 'test/loss': 0.046951211988925934, 'test/mean_average_precision': 0.2753131299952883, 'test/num_examples': 43793, 'score': 9857.652417898178, 'total_duration': 14163.926680326462, 'accumulated_submission_time': 9857.652417898178, 'accumulated_eval_time': 4303.904276847839, 'accumulated_logging_time': 1.5686705112457275}
I0210 11:21:51.718410 139813965014784 logging_writer.py:48] [30714] accumulated_eval_time=4303.904277, accumulated_logging_time=1.568671, accumulated_submission_time=9857.652418, global_step=30714, preemption_count=0, score=9857.652418, test/accuracy=0.986079, test/loss=0.046951, test/mean_average_precision=0.275313, test/num_examples=43793, total_duration=14163.926680, train/accuracy=0.991774, train/loss=0.026466, train/mean_average_precision=0.497405, validation/accuracy=0.986899, validation/loss=0.044276, validation/mean_average_precision=0.278342, validation/num_examples=43793
I0210 11:22:19.409922 139871926159104 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.062478743493556976, loss=0.029348071664571762
I0210 11:22:51.409642 139813965014784 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.05681198462843895, loss=0.030218521133065224
I0210 11:23:22.956334 139871926159104 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.05030055716633797, loss=0.026057593524456024
I0210 11:23:54.910223 139813965014784 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.06111399456858635, loss=0.03167248144745827
I0210 11:24:26.649687 139871926159104 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.06393369287252426, loss=0.026792598888278008
I0210 11:24:58.155357 139813965014784 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.06329189985990524, loss=0.029953043907880783
I0210 11:25:29.916972 139871926159104 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.07270283252000809, loss=0.02795848436653614
I0210 11:25:51.897884 140039251117888 spec.py:321] Evaluating on the training split.
I0210 11:27:25.391241 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 11:27:28.881408 140039251117888 spec.py:349] Evaluating on the test split.
I0210 11:27:32.181027 140039251117888 submission_runner.py:408] Time since start: 14504.41s, 	Step: 31471, 	{'train/accuracy': 0.9916498064994812, 'train/loss': 0.026945652440190315, 'train/mean_average_precision': 0.48888145743807776, 'validation/accuracy': 0.9870484471321106, 'validation/loss': 0.04448457062244415, 'validation/mean_average_precision': 0.2807809220691736, 'validation/num_examples': 43793, 'test/accuracy': 0.9862290024757385, 'test/loss': 0.047175098210573196, 'test/mean_average_precision': 0.2739805160313872, 'test/num_examples': 43793, 'score': 10097.801033735275, 'total_duration': 14504.410804271698, 'accumulated_submission_time': 10097.801033735275, 'accumulated_eval_time': 4404.187355518341, 'accumulated_logging_time': 1.6013495922088623}
I0210 11:27:32.204555 139799398483712 logging_writer.py:48] [31471] accumulated_eval_time=4404.187356, accumulated_logging_time=1.601350, accumulated_submission_time=10097.801034, global_step=31471, preemption_count=0, score=10097.801034, test/accuracy=0.986229, test/loss=0.047175, test/mean_average_precision=0.273981, test/num_examples=43793, total_duration=14504.410804, train/accuracy=0.991650, train/loss=0.026946, train/mean_average_precision=0.488881, validation/accuracy=0.987048, validation/loss=0.044485, validation/mean_average_precision=0.280781, validation/num_examples=43793
I0210 11:27:41.931519 139818172847872 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.05970679223537445, loss=0.02798178605735302
I0210 11:28:14.402988 139799398483712 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.052560918033123016, loss=0.027592789381742477
I0210 11:28:46.214915 139818172847872 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.059370700269937515, loss=0.02840803563594818
I0210 11:29:18.096047 139799398483712 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.052864301949739456, loss=0.02665444277226925
I0210 11:29:49.695077 139818172847872 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.06403560936450958, loss=0.026370014995336533
I0210 11:30:21.557415 139799398483712 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.06867697834968567, loss=0.029148327186703682
I0210 11:30:53.222592 139818172847872 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.05594950541853905, loss=0.027834581211209297
I0210 11:31:25.080392 139799398483712 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.051593419164419174, loss=0.027273017913103104
I0210 11:31:32.309075 140039251117888 spec.py:321] Evaluating on the training split.
I0210 11:33:08.089486 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 11:33:11.129704 140039251117888 spec.py:349] Evaluating on the test split.
I0210 11:33:14.202386 140039251117888 submission_runner.py:408] Time since start: 14846.43s, 	Step: 32224, 	{'train/accuracy': 0.9916934370994568, 'train/loss': 0.026851730421185493, 'train/mean_average_precision': 0.4979061005428039, 'validation/accuracy': 0.9870707392692566, 'validation/loss': 0.0440482571721077, 'validation/mean_average_precision': 0.2837913263385009, 'validation/num_examples': 43793, 'test/accuracy': 0.9862456321716309, 'test/loss': 0.046788159757852554, 'test/mean_average_precision': 0.2771946456173613, 'test/num_examples': 43793, 'score': 10337.873920917511, 'total_duration': 14846.432065725327, 'accumulated_submission_time': 10337.873920917511, 'accumulated_eval_time': 4506.080503463745, 'accumulated_logging_time': 1.637082815170288}
I0210 11:33:14.226387 139813965014784 logging_writer.py:48] [32224] accumulated_eval_time=4506.080503, accumulated_logging_time=1.637083, accumulated_submission_time=10337.873921, global_step=32224, preemption_count=0, score=10337.873921, test/accuracy=0.986246, test/loss=0.046788, test/mean_average_precision=0.277195, test/num_examples=43793, total_duration=14846.432066, train/accuracy=0.991693, train/loss=0.026852, train/mean_average_precision=0.497906, validation/accuracy=0.987071, validation/loss=0.044048, validation/mean_average_precision=0.283791, validation/num_examples=43793
I0210 11:33:38.947272 139871926159104 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.051911599934101105, loss=0.027371330186724663
I0210 11:34:11.258709 139813965014784 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.05748765915632248, loss=0.027160679921507835
I0210 11:34:43.634227 139871926159104 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.04938904196023941, loss=0.028048112988471985
I0210 11:35:15.629875 139813965014784 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.058596279472112656, loss=0.028942324221134186
I0210 11:35:47.552181 139871926159104 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.06960055977106094, loss=0.033269114792346954
I0210 11:36:19.727576 139813965014784 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.05927807837724686, loss=0.027421291917562485
I0210 11:36:51.771540 139871926159104 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.0669182762503624, loss=0.02716296724975109
I0210 11:37:14.343113 140039251117888 spec.py:321] Evaluating on the training split.
I0210 11:38:50.049324 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 11:38:53.160367 140039251117888 spec.py:349] Evaluating on the test split.
I0210 11:38:56.124410 140039251117888 submission_runner.py:408] Time since start: 15188.35s, 	Step: 32971, 	{'train/accuracy': 0.9917108416557312, 'train/loss': 0.026847785338759422, 'train/mean_average_precision': 0.4847077089609162, 'validation/accuracy': 0.9870195984840393, 'validation/loss': 0.04391064494848251, 'validation/mean_average_precision': 0.28105090039840597, 'validation/num_examples': 43793, 'test/accuracy': 0.9862130284309387, 'test/loss': 0.046704407781362534, 'test/mean_average_precision': 0.2748783828748157, 'test/num_examples': 43793, 'score': 10577.960082054138, 'total_duration': 15188.354211330414, 'accumulated_submission_time': 10577.960082054138, 'accumulated_eval_time': 4607.861759901047, 'accumulated_logging_time': 1.6719791889190674}
I0210 11:38:56.145941 139799390091008 logging_writer.py:48] [32971] accumulated_eval_time=4607.861760, accumulated_logging_time=1.671979, accumulated_submission_time=10577.960082, global_step=32971, preemption_count=0, score=10577.960082, test/accuracy=0.986213, test/loss=0.046704, test/mean_average_precision=0.274878, test/num_examples=43793, total_duration=15188.354211, train/accuracy=0.991711, train/loss=0.026848, train/mean_average_precision=0.484708, validation/accuracy=0.987020, validation/loss=0.043911, validation/mean_average_precision=0.281051, validation/num_examples=43793
I0210 11:39:06.235032 139818172847872 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.060553617775440216, loss=0.02876495197415352
I0210 11:39:38.254491 139799390091008 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.05036277323961258, loss=0.029136033728718758
I0210 11:40:10.330127 139818172847872 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.06754487007856369, loss=0.02787259966135025
I0210 11:40:41.988315 139799390091008 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.06337931752204895, loss=0.029077403247356415
I0210 11:41:13.907717 139818172847872 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.06723783910274506, loss=0.0279978159815073
I0210 11:41:45.730408 139799390091008 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.07527603209018707, loss=0.02988622710108757
I0210 11:42:17.625953 139818172847872 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.07909028977155685, loss=0.030676746740937233
I0210 11:42:49.303771 139799390091008 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.060280367732048035, loss=0.02775229886174202
I0210 11:42:56.325921 140039251117888 spec.py:321] Evaluating on the training split.
I0210 11:44:30.972618 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 11:44:34.105324 140039251117888 spec.py:349] Evaluating on the test split.
I0210 11:44:37.104504 140039251117888 submission_runner.py:408] Time since start: 15529.33s, 	Step: 33723, 	{'train/accuracy': 0.9917195439338684, 'train/loss': 0.026451144367456436, 'train/mean_average_precision': 0.5071388805226102, 'validation/accuracy': 0.9871170520782471, 'validation/loss': 0.04401608929038048, 'validation/mean_average_precision': 0.2869082133800527, 'validation/num_examples': 43793, 'test/accuracy': 0.986255943775177, 'test/loss': 0.046712297946214676, 'test/mean_average_precision': 0.2788147279553553, 'test/num_examples': 43793, 'score': 10818.10796546936, 'total_duration': 15529.334302663803, 'accumulated_submission_time': 10818.10796546936, 'accumulated_eval_time': 4708.640298604965, 'accumulated_logging_time': 1.7060413360595703}
I0210 11:44:37.126467 139799398483712 logging_writer.py:48] [33723] accumulated_eval_time=4708.640299, accumulated_logging_time=1.706041, accumulated_submission_time=10818.107965, global_step=33723, preemption_count=0, score=10818.107965, test/accuracy=0.986256, test/loss=0.046712, test/mean_average_precision=0.278815, test/num_examples=43793, total_duration=15529.334303, train/accuracy=0.991720, train/loss=0.026451, train/mean_average_precision=0.507139, validation/accuracy=0.987117, validation/loss=0.044016, validation/mean_average_precision=0.286908, validation/num_examples=43793
I0210 11:45:02.589359 139871926159104 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.053752049803733826, loss=0.024476604536175728
I0210 11:45:34.973576 139799398483712 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.055121924728155136, loss=0.02896498702466488
I0210 11:46:07.117087 139871926159104 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.06029396504163742, loss=0.02886771969497204
I0210 11:46:39.634831 139799398483712 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.0701463520526886, loss=0.03252900019288063
I0210 11:47:11.922019 139871926159104 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.049708686769008636, loss=0.026791315525770187
I0210 11:47:43.508102 139799398483712 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.05121605843305588, loss=0.02962368354201317
I0210 11:48:15.656901 139871926159104 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.06331047415733337, loss=0.033290766179561615
I0210 11:48:37.349327 140039251117888 spec.py:321] Evaluating on the training split.
I0210 11:50:18.563692 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 11:50:21.653831 140039251117888 spec.py:349] Evaluating on the test split.
I0210 11:50:24.617005 140039251117888 submission_runner.py:408] Time since start: 15876.85s, 	Step: 34468, 	{'train/accuracy': 0.9917593002319336, 'train/loss': 0.026479462161660194, 'train/mean_average_precision': 0.4987217774287702, 'validation/accuracy': 0.9870354533195496, 'validation/loss': 0.044570889323949814, 'validation/mean_average_precision': 0.2862611577565212, 'validation/num_examples': 43793, 'test/accuracy': 0.9862926006317139, 'test/loss': 0.04743202030658722, 'test/mean_average_precision': 0.27369947145660745, 'test/num_examples': 43793, 'score': 11058.298412799835, 'total_duration': 15876.846804141998, 'accumulated_submission_time': 11058.298412799835, 'accumulated_eval_time': 4815.907956838608, 'accumulated_logging_time': 1.7401671409606934}
I0210 11:50:24.639568 139813965014784 logging_writer.py:48] [34468] accumulated_eval_time=4815.907957, accumulated_logging_time=1.740167, accumulated_submission_time=11058.298413, global_step=34468, preemption_count=0, score=11058.298413, test/accuracy=0.986293, test/loss=0.047432, test/mean_average_precision=0.273699, test/num_examples=43793, total_duration=15876.846804, train/accuracy=0.991759, train/loss=0.026479, train/mean_average_precision=0.498722, validation/accuracy=0.987035, validation/loss=0.044571, validation/mean_average_precision=0.286261, validation/num_examples=43793
I0210 11:50:35.243731 139818172847872 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.052182529121637344, loss=0.02692912146449089
I0210 11:51:07.561250 139813965014784 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.05646904930472374, loss=0.025003043934702873
I0210 11:51:39.705148 139818172847872 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.06516432017087936, loss=0.029517941176891327
I0210 11:52:11.656971 139813965014784 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.05772140249609947, loss=0.027975352481007576
I0210 11:52:43.714504 139818172847872 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.0719992071390152, loss=0.030089396983385086
I0210 11:53:15.718632 139813965014784 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.05898929387331009, loss=0.029768705368041992
I0210 11:53:47.760328 139818172847872 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.0663149431347847, loss=0.03255527839064598
I0210 11:54:19.563575 139813965014784 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.05897865444421768, loss=0.027313118800520897
I0210 11:54:24.665302 140039251117888 spec.py:321] Evaluating on the training split.
I0210 11:56:00.073166 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 11:56:03.291611 140039251117888 spec.py:349] Evaluating on the test split.
I0210 11:56:06.317227 140039251117888 submission_runner.py:408] Time since start: 16218.55s, 	Step: 35216, 	{'train/accuracy': 0.9919956922531128, 'train/loss': 0.025788769125938416, 'train/mean_average_precision': 0.514730269745362, 'validation/accuracy': 0.9871243238449097, 'validation/loss': 0.0442296601831913, 'validation/mean_average_precision': 0.28767634130287056, 'validation/num_examples': 43793, 'test/accuracy': 0.9862761497497559, 'test/loss': 0.047140274196863174, 'test/mean_average_precision': 0.27782360070679796, 'test/num_examples': 43793, 'score': 11298.292127609253, 'total_duration': 16218.54702448845, 'accumulated_submission_time': 11298.292127609253, 'accumulated_eval_time': 4917.55984044075, 'accumulated_logging_time': 1.7752108573913574}
I0210 11:56:06.339928 139799398483712 logging_writer.py:48] [35216] accumulated_eval_time=4917.559840, accumulated_logging_time=1.775211, accumulated_submission_time=11298.292128, global_step=35216, preemption_count=0, score=11298.292128, test/accuracy=0.986276, test/loss=0.047140, test/mean_average_precision=0.277824, test/num_examples=43793, total_duration=16218.547024, train/accuracy=0.991996, train/loss=0.025789, train/mean_average_precision=0.514730, validation/accuracy=0.987124, validation/loss=0.044230, validation/mean_average_precision=0.287676, validation/num_examples=43793
I0210 11:56:33.904344 139871926159104 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.05310043692588806, loss=0.027822133153676987
I0210 11:57:05.945763 139799398483712 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.10042385756969452, loss=0.03050699643790722
I0210 11:57:37.683619 139871926159104 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.07511813193559647, loss=0.027184970676898956
I0210 11:58:09.518892 139799398483712 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.05838443338871002, loss=0.026410050690174103
I0210 11:58:41.235736 139871926159104 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.062362685799598694, loss=0.027492791414260864
I0210 11:59:13.333600 139799398483712 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.06452862173318863, loss=0.02682921290397644
I0210 11:59:45.379729 139871926159104 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.06838365644216537, loss=0.028689522296190262
I0210 12:00:06.609214 140039251117888 spec.py:321] Evaluating on the training split.
I0210 12:01:42.586955 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 12:01:45.705252 140039251117888 spec.py:349] Evaluating on the test split.
I0210 12:01:48.824810 140039251117888 submission_runner.py:408] Time since start: 16561.05s, 	Step: 35967, 	{'train/accuracy': 0.9921483397483826, 'train/loss': 0.025018077343702316, 'train/mean_average_precision': 0.5371498315246481, 'validation/accuracy': 0.9870285391807556, 'validation/loss': 0.044526178389787674, 'validation/mean_average_precision': 0.2823993135022633, 'validation/num_examples': 43793, 'test/accuracy': 0.9861658215522766, 'test/loss': 0.04742975905537605, 'test/mean_average_precision': 0.2739156328156931, 'test/num_examples': 43793, 'score': 11538.531027317047, 'total_duration': 16561.054585933685, 'accumulated_submission_time': 11538.531027317047, 'accumulated_eval_time': 5019.775372505188, 'accumulated_logging_time': 1.809107780456543}
I0210 12:01:48.846687 139813965014784 logging_writer.py:48] [35967] accumulated_eval_time=5019.775373, accumulated_logging_time=1.809108, accumulated_submission_time=11538.531027, global_step=35967, preemption_count=0, score=11538.531027, test/accuracy=0.986166, test/loss=0.047430, test/mean_average_precision=0.273916, test/num_examples=43793, total_duration=16561.054586, train/accuracy=0.992148, train/loss=0.025018, train/mean_average_precision=0.537150, validation/accuracy=0.987029, validation/loss=0.044526, validation/mean_average_precision=0.282399, validation/num_examples=43793
I0210 12:01:59.643070 139818172847872 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.06873151659965515, loss=0.02896670438349247
I0210 12:02:31.858402 139813965014784 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.060269176959991455, loss=0.027579545974731445
I0210 12:03:03.471420 139818172847872 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.05965518578886986, loss=0.027063198387622833
I0210 12:03:35.565267 139813965014784 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.06672168523073196, loss=0.02937878668308258
I0210 12:04:07.265238 139818172847872 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.054832398891448975, loss=0.027706969529390335
I0210 12:04:39.106891 139813965014784 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.05359135940670967, loss=0.030196432024240494
I0210 12:05:11.104903 139818172847872 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.05857379361987114, loss=0.027101635932922363
I0210 12:05:42.596566 139813965014784 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.05877041444182396, loss=0.026874227449297905
I0210 12:05:48.912619 140039251117888 spec.py:321] Evaluating on the training split.
I0210 12:07:21.083848 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 12:07:24.427302 140039251117888 spec.py:349] Evaluating on the test split.
I0210 12:07:27.741736 140039251117888 submission_runner.py:408] Time since start: 16899.97s, 	Step: 36721, 	{'train/accuracy': 0.9923694133758545, 'train/loss': 0.024611108005046844, 'train/mean_average_precision': 0.5423980058733963, 'validation/accuracy': 0.9868807792663574, 'validation/loss': 0.044240955263376236, 'validation/mean_average_precision': 0.2812444716684876, 'validation/num_examples': 43793, 'test/accuracy': 0.9860352277755737, 'test/loss': 0.046927087008953094, 'test/mean_average_precision': 0.27760796493702045, 'test/num_examples': 43793, 'score': 11778.565999269485, 'total_duration': 16899.971519231796, 'accumulated_submission_time': 11778.565999269485, 'accumulated_eval_time': 5118.604428529739, 'accumulated_logging_time': 1.8418443202972412}
I0210 12:07:27.766336 139799398483712 logging_writer.py:48] [36721] accumulated_eval_time=5118.604429, accumulated_logging_time=1.841844, accumulated_submission_time=11778.565999, global_step=36721, preemption_count=0, score=11778.565999, test/accuracy=0.986035, test/loss=0.046927, test/mean_average_precision=0.277608, test/num_examples=43793, total_duration=16899.971519, train/accuracy=0.992369, train/loss=0.024611, train/mean_average_precision=0.542398, validation/accuracy=0.986881, validation/loss=0.044241, validation/mean_average_precision=0.281244, validation/num_examples=43793
I0210 12:07:53.710249 139871926159104 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.06624184548854828, loss=0.027695201337337494
I0210 12:08:26.145278 139799398483712 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.061397403478622437, loss=0.027994144707918167
I0210 12:08:58.467852 139871926159104 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.05727149546146393, loss=0.02604023925960064
I0210 12:09:30.301983 139799398483712 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.06322215497493744, loss=0.02588212676346302
I0210 12:10:02.872547 139871926159104 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.05621914938092232, loss=0.026911836117506027
I0210 12:10:35.475420 139799398483712 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.06509055197238922, loss=0.027386467903852463
I0210 12:11:07.981306 139871926159104 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.06124796345829964, loss=0.025596963241696358
I0210 12:11:27.800756 140039251117888 spec.py:321] Evaluating on the training split.
I0210 12:13:05.158380 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 12:13:08.373706 140039251117888 spec.py:349] Evaluating on the test split.
I0210 12:13:11.569107 140039251117888 submission_runner.py:408] Time since start: 17243.80s, 	Step: 37462, 	{'train/accuracy': 0.9923901557922363, 'train/loss': 0.02431369200348854, 'train/mean_average_precision': 0.5580372747770705, 'validation/accuracy': 0.9870139360427856, 'validation/loss': 0.044508807361125946, 'validation/mean_average_precision': 0.2837891887851236, 'validation/num_examples': 43793, 'test/accuracy': 0.9861957430839539, 'test/loss': 0.04747257009148598, 'test/mean_average_precision': 0.28282577110993073, 'test/num_examples': 43793, 'score': 12018.567860364914, 'total_duration': 17243.798904657364, 'accumulated_submission_time': 12018.567860364914, 'accumulated_eval_time': 5222.37273478508, 'accumulated_logging_time': 1.8780317306518555}
I0210 12:13:11.591825 139799390091008 logging_writer.py:48] [37462] accumulated_eval_time=5222.372735, accumulated_logging_time=1.878032, accumulated_submission_time=12018.567860, global_step=37462, preemption_count=0, score=12018.567860, test/accuracy=0.986196, test/loss=0.047473, test/mean_average_precision=0.282826, test/num_examples=43793, total_duration=17243.798905, train/accuracy=0.992390, train/loss=0.024314, train/mean_average_precision=0.558037, validation/accuracy=0.987014, validation/loss=0.044509, validation/mean_average_precision=0.283789, validation/num_examples=43793
I0210 12:13:24.047373 139813965014784 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.06470491737127304, loss=0.027632152661681175
I0210 12:13:55.917097 139799390091008 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.06055436655879021, loss=0.02702310122549534
I0210 12:14:27.967388 139813965014784 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.05617828667163849, loss=0.023946542292833328
I0210 12:14:59.889842 139799390091008 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.0823255181312561, loss=0.02935582585632801
I0210 12:15:32.004589 139813965014784 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.053598497062921524, loss=0.0255514457821846
I0210 12:16:03.505100 139799390091008 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.06790928542613983, loss=0.024859992787241936
I0210 12:16:35.612081 139813965014784 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.05495428666472435, loss=0.023702871054410934
I0210 12:17:07.635301 139799390091008 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.05600953847169876, loss=0.02532319724559784
I0210 12:17:11.755848 140039251117888 spec.py:321] Evaluating on the training split.
I0210 12:18:47.387066 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 12:18:50.528547 140039251117888 spec.py:349] Evaluating on the test split.
I0210 12:18:53.636775 140039251117888 submission_runner.py:408] Time since start: 17585.87s, 	Step: 38214, 	{'train/accuracy': 0.9922755360603333, 'train/loss': 0.024770546704530716, 'train/mean_average_precision': 0.5347153345263589, 'validation/accuracy': 0.9870476126670837, 'validation/loss': 0.04480938985943794, 'validation/mean_average_precision': 0.28137809293253063, 'validation/num_examples': 43793, 'test/accuracy': 0.9862033128738403, 'test/loss': 0.04759284481406212, 'test/mean_average_precision': 0.2776924273187139, 'test/num_examples': 43793, 'score': 12258.701553821564, 'total_duration': 17585.86657810211, 'accumulated_submission_time': 12258.701553821564, 'accumulated_eval_time': 5324.253618955612, 'accumulated_logging_time': 1.9119784832000732}
I0210 12:18:53.659709 139818172847872 logging_writer.py:48] [38214] accumulated_eval_time=5324.253619, accumulated_logging_time=1.911978, accumulated_submission_time=12258.701554, global_step=38214, preemption_count=0, score=12258.701554, test/accuracy=0.986203, test/loss=0.047593, test/mean_average_precision=0.277692, test/num_examples=43793, total_duration=17585.866578, train/accuracy=0.992276, train/loss=0.024771, train/mean_average_precision=0.534715, validation/accuracy=0.987048, validation/loss=0.044809, validation/mean_average_precision=0.281378, validation/num_examples=43793
I0210 12:19:21.675400 139871926159104 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.09791027009487152, loss=0.0308743417263031
I0210 12:19:53.073497 139818172847872 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.06181427091360092, loss=0.023577889427542686
I0210 12:20:25.047063 139871926159104 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.07189184427261353, loss=0.027084043249487877
I0210 12:20:57.034450 139818172847872 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.052182912826538086, loss=0.024604525417089462
I0210 12:21:29.117641 139871926159104 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.07350598275661469, loss=0.030710889026522636
I0210 12:22:01.021679 139818172847872 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.06693775206804276, loss=0.025901904329657555
I0210 12:22:32.998726 139871926159104 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.07132795453071594, loss=0.027315029874444008
I0210 12:22:53.844394 140039251117888 spec.py:321] Evaluating on the training split.
I0210 12:24:29.259431 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 12:24:32.372435 140039251117888 spec.py:349] Evaluating on the test split.
I0210 12:24:35.423143 140039251117888 submission_runner.py:408] Time since start: 17927.65s, 	Step: 38966, 	{'train/accuracy': 0.9921236634254456, 'train/loss': 0.02517537586390972, 'train/mean_average_precision': 0.5225601239481771, 'validation/accuracy': 0.9870768189430237, 'validation/loss': 0.04468025639653206, 'validation/mean_average_precision': 0.2864147846605825, 'validation/num_examples': 43793, 'test/accuracy': 0.9862778782844543, 'test/loss': 0.04753851518034935, 'test/mean_average_precision': 0.2776829744498602, 'test/num_examples': 43793, 'score': 12498.43436551094, 'total_duration': 17927.652944803238, 'accumulated_submission_time': 12498.43436551094, 'accumulated_eval_time': 5425.832328796387, 'accumulated_logging_time': 2.3672399520874023}
I0210 12:24:35.446349 139799398483712 logging_writer.py:48] [38966] accumulated_eval_time=5425.832329, accumulated_logging_time=2.367240, accumulated_submission_time=12498.434366, global_step=38966, preemption_count=0, score=12498.434366, test/accuracy=0.986278, test/loss=0.047539, test/mean_average_precision=0.277683, test/num_examples=43793, total_duration=17927.652945, train/accuracy=0.992124, train/loss=0.025175, train/mean_average_precision=0.522560, validation/accuracy=0.987077, validation/loss=0.044680, validation/mean_average_precision=0.286415, validation/num_examples=43793
I0210 12:24:46.562217 139813965014784 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.07140414416790009, loss=0.024865802377462387
I0210 12:25:18.584098 139799398483712 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.06641076505184174, loss=0.02501327358186245
I0210 12:25:50.657991 139813965014784 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.07620762288570404, loss=0.028323683887720108
I0210 12:26:23.011321 139799398483712 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.07102178037166595, loss=0.026707462966442108
I0210 12:26:55.642575 139813965014784 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.0660451203584671, loss=0.028470342978835106
I0210 12:27:28.577059 139799398483712 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.07322624325752258, loss=0.027272557839751244
I0210 12:28:01.204057 139813965014784 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.09571218490600586, loss=0.029587920755147934
I0210 12:28:34.022472 139799398483712 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.07625918835401535, loss=0.029857860878109932
I0210 12:28:35.623883 140039251117888 spec.py:321] Evaluating on the training split.
I0210 12:30:14.502259 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 12:30:17.540490 140039251117888 spec.py:349] Evaluating on the test split.
I0210 12:30:20.536628 140039251117888 submission_runner.py:408] Time since start: 18272.77s, 	Step: 39706, 	{'train/accuracy': 0.9922091364860535, 'train/loss': 0.025052009150385857, 'train/mean_average_precision': 0.534985723044657, 'validation/accuracy': 0.9870837330818176, 'validation/loss': 0.0441632904112339, 'validation/mean_average_precision': 0.2902683111279471, 'validation/num_examples': 43793, 'test/accuracy': 0.9863018989562988, 'test/loss': 0.04700274392962456, 'test/mean_average_precision': 0.2751548149248845, 'test/num_examples': 43793, 'score': 12738.577870845795, 'total_duration': 18272.76641869545, 'accumulated_submission_time': 12738.577870845795, 'accumulated_eval_time': 5530.745029449463, 'accumulated_logging_time': 2.4028241634368896}
I0210 12:30:20.559899 139799390091008 logging_writer.py:48] [39706] accumulated_eval_time=5530.745029, accumulated_logging_time=2.402824, accumulated_submission_time=12738.577871, global_step=39706, preemption_count=0, score=12738.577871, test/accuracy=0.986302, test/loss=0.047003, test/mean_average_precision=0.275155, test/num_examples=43793, total_duration=18272.766419, train/accuracy=0.992209, train/loss=0.025052, train/mean_average_precision=0.534986, validation/accuracy=0.987084, validation/loss=0.044163, validation/mean_average_precision=0.290268, validation/num_examples=43793
I0210 12:30:51.057840 139871926159104 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.06446196883916855, loss=0.028257180005311966
I0210 12:31:23.248778 139799390091008 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.06784486770629883, loss=0.028833327814936638
I0210 12:31:54.897551 139871926159104 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.07732744514942169, loss=0.026970647275447845
I0210 12:32:26.917480 139799390091008 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.07317198067903519, loss=0.02468182146549225
I0210 12:32:58.952120 139871926159104 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.06133349612355232, loss=0.023746665567159653
I0210 12:33:30.783085 139799390091008 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.08107507973909378, loss=0.02961040288209915
I0210 12:34:02.654554 139871926159104 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.08509202301502228, loss=0.031017234548926353
I0210 12:34:20.602359 140039251117888 spec.py:321] Evaluating on the training split.
I0210 12:35:53.657959 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 12:35:56.764740 140039251117888 spec.py:349] Evaluating on the test split.
I0210 12:35:59.767598 140039251117888 submission_runner.py:408] Time since start: 18612.00s, 	Step: 40458, 	{'train/accuracy': 0.9922121167182922, 'train/loss': 0.024899430572986603, 'train/mean_average_precision': 0.5352071481301257, 'validation/accuracy': 0.9870695471763611, 'validation/loss': 0.04427975043654442, 'validation/mean_average_precision': 0.28663532808791076, 'validation/num_examples': 43793, 'test/accuracy': 0.986185610294342, 'test/loss': 0.04728372022509575, 'test/mean_average_precision': 0.27403020649579524, 'test/num_examples': 43793, 'score': 12978.589567184448, 'total_duration': 18611.997394800186, 'accumulated_submission_time': 12978.589567184448, 'accumulated_eval_time': 5629.910222530365, 'accumulated_logging_time': 2.4375898838043213}
I0210 12:35:59.791302 139799398483712 logging_writer.py:48] [40458] accumulated_eval_time=5629.910223, accumulated_logging_time=2.437590, accumulated_submission_time=12978.589567, global_step=40458, preemption_count=0, score=12978.589567, test/accuracy=0.986186, test/loss=0.047284, test/mean_average_precision=0.274030, test/num_examples=43793, total_duration=18611.997395, train/accuracy=0.992212, train/loss=0.024899, train/mean_average_precision=0.535207, validation/accuracy=0.987070, validation/loss=0.044280, validation/mean_average_precision=0.286635, validation/num_examples=43793
I0210 12:36:13.868283 139818172847872 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.07106753438711166, loss=0.029638076201081276
I0210 12:36:45.641479 139799398483712 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.0615743063390255, loss=0.02612590789794922
I0210 12:37:17.446501 139818172847872 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.08844679594039917, loss=0.02977239154279232
I0210 12:37:49.130043 139799398483712 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.07256214320659637, loss=0.026644255965948105
I0210 12:38:21.405815 139818172847872 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.07449714094400406, loss=0.027617797255516052
I0210 12:38:53.232756 139799398483712 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.07980827987194061, loss=0.02953244186937809
I0210 12:39:25.106806 139818172847872 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.06800767779350281, loss=0.026483658701181412
I0210 12:39:56.666960 139799398483712 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.07142988592386246, loss=0.026768136769533157
I0210 12:39:59.856836 140039251117888 spec.py:321] Evaluating on the training split.
I0210 12:41:28.935163 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 12:41:34.646584 140039251117888 spec.py:349] Evaluating on the test split.
I0210 12:41:37.632056 140039251117888 submission_runner.py:408] Time since start: 18949.86s, 	Step: 41211, 	{'train/accuracy': 0.9924872517585754, 'train/loss': 0.024059409275650978, 'train/mean_average_precision': 0.5571624292463508, 'validation/accuracy': 0.9870370626449585, 'validation/loss': 0.044358331710100174, 'validation/mean_average_precision': 0.28416683557907974, 'validation/num_examples': 43793, 'test/accuracy': 0.9861717224121094, 'test/loss': 0.047193001955747604, 'test/mean_average_precision': 0.2720835039963426, 'test/num_examples': 43793, 'score': 13218.623850822449, 'total_duration': 18949.861858844757, 'accumulated_submission_time': 13218.623850822449, 'accumulated_eval_time': 5727.685400247574, 'accumulated_logging_time': 2.4737045764923096}
I0210 12:41:37.656059 139799390091008 logging_writer.py:48] [41211] accumulated_eval_time=5727.685400, accumulated_logging_time=2.473705, accumulated_submission_time=13218.623851, global_step=41211, preemption_count=0, score=13218.623851, test/accuracy=0.986172, test/loss=0.047193, test/mean_average_precision=0.272084, test/num_examples=43793, total_duration=18949.861859, train/accuracy=0.992487, train/loss=0.024059, train/mean_average_precision=0.557162, validation/accuracy=0.987037, validation/loss=0.044358, validation/mean_average_precision=0.284167, validation/num_examples=43793
I0210 12:42:06.318113 139871926159104 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.0706545040011406, loss=0.02905641309916973
I0210 12:42:37.950502 139799390091008 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.08183729648590088, loss=0.026431215927004814
I0210 12:43:09.733728 139871926159104 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.09424235671758652, loss=0.028052804991602898
I0210 12:43:41.182479 139799390091008 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.06254994869232178, loss=0.023798596113920212
I0210 12:44:12.662660 139871926159104 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.0818507969379425, loss=0.02602621167898178
I0210 12:44:44.475645 139799390091008 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.08005119115114212, loss=0.028006181120872498
I0210 12:45:16.248972 139871926159104 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.0695001557469368, loss=0.028973478823900223
I0210 12:45:37.908672 140039251117888 spec.py:321] Evaluating on the training split.
I0210 12:47:11.930244 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 12:47:14.981154 140039251117888 spec.py:349] Evaluating on the test split.
I0210 12:47:18.021122 140039251117888 submission_runner.py:408] Time since start: 19290.25s, 	Step: 41969, 	{'train/accuracy': 0.9924891591072083, 'train/loss': 0.023978086188435555, 'train/mean_average_precision': 0.5498505304311031, 'validation/accuracy': 0.9871364831924438, 'validation/loss': 0.04452793300151825, 'validation/mean_average_precision': 0.292061123135377, 'validation/num_examples': 43793, 'test/accuracy': 0.9861666560173035, 'test/loss': 0.04746491089463234, 'test/mean_average_precision': 0.2750019408692665, 'test/num_examples': 43793, 'score': 13458.846201658249, 'total_duration': 19290.2509264946, 'accumulated_submission_time': 13458.846201658249, 'accumulated_eval_time': 5827.797815561295, 'accumulated_logging_time': 2.508739709854126}
I0210 12:47:18.044960 139799398483712 logging_writer.py:48] [41969] accumulated_eval_time=5827.797816, accumulated_logging_time=2.508740, accumulated_submission_time=13458.846202, global_step=41969, preemption_count=0, score=13458.846202, test/accuracy=0.986167, test/loss=0.047465, test/mean_average_precision=0.275002, test/num_examples=43793, total_duration=19290.250926, train/accuracy=0.992489, train/loss=0.023978, train/mean_average_precision=0.549851, validation/accuracy=0.987136, validation/loss=0.044528, validation/mean_average_precision=0.292061, validation/num_examples=43793
I0210 12:47:28.349378 139813965014784 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.07135629653930664, loss=0.02310621179640293
I0210 12:48:02.274278 139799398483712 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.07358869910240173, loss=0.025961484760046005
I0210 12:48:36.540488 139813965014784 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.0844043716788292, loss=0.030011598020792007
I0210 12:49:08.990658 139799398483712 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.07517272233963013, loss=0.026732461526989937
I0210 12:49:40.509541 139813965014784 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.08241300284862518, loss=0.030350588262081146
I0210 12:50:12.485718 139799398483712 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.07251652330160141, loss=0.025713708251714706
I0210 12:50:44.393057 139813965014784 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.07868822664022446, loss=0.02613023854792118
I0210 12:51:16.023321 139799398483712 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.05980642884969711, loss=0.023557474836707115
I0210 12:51:18.231358 140039251117888 spec.py:321] Evaluating on the training split.
I0210 12:52:52.836700 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 12:52:55.929020 140039251117888 spec.py:349] Evaluating on the test split.
I0210 12:52:58.950838 140039251117888 submission_runner.py:408] Time since start: 19631.18s, 	Step: 42708, 	{'train/accuracy': 0.9924418926239014, 'train/loss': 0.02395336702466011, 'train/mean_average_precision': 0.5589275876659786, 'validation/accuracy': 0.9870520830154419, 'validation/loss': 0.04472799226641655, 'validation/mean_average_precision': 0.28691590283802076, 'validation/num_examples': 43793, 'test/accuracy': 0.9862711429595947, 'test/loss': 0.04723460599780083, 'test/mean_average_precision': 0.27847302337643665, 'test/num_examples': 43793, 'score': 13698.999931812286, 'total_duration': 19631.180638074875, 'accumulated_submission_time': 13698.999931812286, 'accumulated_eval_time': 5928.517250061035, 'accumulated_logging_time': 2.544694185256958}
I0210 12:52:58.974597 139799390091008 logging_writer.py:48] [42708] accumulated_eval_time=5928.517250, accumulated_logging_time=2.544694, accumulated_submission_time=13698.999932, global_step=42708, preemption_count=0, score=13698.999932, test/accuracy=0.986271, test/loss=0.047235, test/mean_average_precision=0.278473, test/num_examples=43793, total_duration=19631.180638, train/accuracy=0.992442, train/loss=0.023953, train/mean_average_precision=0.558928, validation/accuracy=0.987052, validation/loss=0.044728, validation/mean_average_precision=0.286916, validation/num_examples=43793
I0210 12:53:28.721866 139871926159104 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.0687502846121788, loss=0.02615363337099552
I0210 12:54:00.522955 139799390091008 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.06908561289310455, loss=0.02309272065758705
I0210 12:54:32.741099 139871926159104 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.08050788193941116, loss=0.028527002781629562
I0210 12:55:04.777685 139799390091008 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.06910664588212967, loss=0.027609826996922493
I0210 12:55:36.560537 139871926159104 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.08201005309820175, loss=0.025582825765013695
I0210 12:56:08.473328 139799390091008 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.06919972598552704, loss=0.02383546158671379
I0210 12:56:40.308818 139871926159104 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.08695148676633835, loss=0.026820488274097443
I0210 12:56:59.132491 140039251117888 spec.py:321] Evaluating on the training split.
I0210 12:58:34.266815 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 12:58:37.444213 140039251117888 spec.py:349] Evaluating on the test split.
I0210 12:58:40.591953 140039251117888 submission_runner.py:408] Time since start: 19972.82s, 	Step: 43459, 	{'train/accuracy': 0.9928225874900818, 'train/loss': 0.022863321006298065, 'train/mean_average_precision': 0.5811168779417648, 'validation/accuracy': 0.9870573282241821, 'validation/loss': 0.04499298706650734, 'validation/mean_average_precision': 0.2861434822523052, 'validation/num_examples': 43793, 'test/accuracy': 0.9862121343612671, 'test/loss': 0.04791620746254921, 'test/mean_average_precision': 0.27526388680439795, 'test/num_examples': 43793, 'score': 13939.127762079239, 'total_duration': 19972.82164144516, 'accumulated_submission_time': 13939.127762079239, 'accumulated_eval_time': 6029.9765565395355, 'accumulated_logging_time': 2.5795791149139404}
I0210 12:58:40.615547 139799398483712 logging_writer.py:48] [43459] accumulated_eval_time=6029.976557, accumulated_logging_time=2.579579, accumulated_submission_time=13939.127762, global_step=43459, preemption_count=0, score=13939.127762, test/accuracy=0.986212, test/loss=0.047916, test/mean_average_precision=0.275264, test/num_examples=43793, total_duration=19972.821641, train/accuracy=0.992823, train/loss=0.022863, train/mean_average_precision=0.581117, validation/accuracy=0.987057, validation/loss=0.044993, validation/mean_average_precision=0.286143, validation/num_examples=43793
I0210 12:58:54.326812 139813965014784 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.07957009971141815, loss=0.02648150362074375
I0210 12:59:26.747054 139799398483712 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.08995255827903748, loss=0.026712341234087944
I0210 12:59:58.857665 139813965014784 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.08141188323497772, loss=0.026801280677318573
I0210 13:00:31.061859 139799398483712 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.07451096922159195, loss=0.024194035679101944
I0210 13:01:03.149170 139813965014784 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.08610354363918304, loss=0.027340415865182877
I0210 13:01:34.790934 139799398483712 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.06389433890581131, loss=0.02426881529390812
I0210 13:02:06.793228 139813965014784 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.06785759329795837, loss=0.027875391766428947
I0210 13:02:39.086052 139799398483712 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.08222838491201401, loss=0.02736135572195053
I0210 13:02:40.651343 140039251117888 spec.py:321] Evaluating on the training split.
I0210 13:04:12.457196 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 13:04:15.570718 140039251117888 spec.py:349] Evaluating on the test split.
I0210 13:04:18.557748 140039251117888 submission_runner.py:408] Time since start: 20310.79s, 	Step: 44206, 	{'train/accuracy': 0.9930243492126465, 'train/loss': 0.022302931174635887, 'train/mean_average_precision': 0.6021831467185383, 'validation/accuracy': 0.9870301485061646, 'validation/loss': 0.04487265273928642, 'validation/mean_average_precision': 0.2870586605260568, 'validation/num_examples': 43793, 'test/accuracy': 0.9861574172973633, 'test/loss': 0.04790334776043892, 'test/mean_average_precision': 0.27577883781596174, 'test/num_examples': 43793, 'score': 14179.133565664291, 'total_duration': 20310.78754711151, 'accumulated_submission_time': 14179.133565664291, 'accumulated_eval_time': 6127.8829135894775, 'accumulated_logging_time': 2.6141395568847656}
I0210 13:04:18.581639 139799390091008 logging_writer.py:48] [44206] accumulated_eval_time=6127.882914, accumulated_logging_time=2.614140, accumulated_submission_time=14179.133566, global_step=44206, preemption_count=0, score=14179.133566, test/accuracy=0.986157, test/loss=0.047903, test/mean_average_precision=0.275779, test/num_examples=43793, total_duration=20310.787547, train/accuracy=0.993024, train/loss=0.022303, train/mean_average_precision=0.602183, validation/accuracy=0.987030, validation/loss=0.044873, validation/mean_average_precision=0.287059, validation/num_examples=43793
I0210 13:04:48.571216 139871926159104 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.0755046010017395, loss=0.026240481063723564
I0210 13:05:20.256078 139799390091008 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.08103285729885101, loss=0.02477109059691429
I0210 13:05:51.739897 139871926159104 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.07983703911304474, loss=0.02525603584945202
I0210 13:06:23.805621 139799390091008 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.0871535912156105, loss=0.02564389258623123
I0210 13:06:55.459922 139871926159104 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.06810012459754944, loss=0.025308649986982346
I0210 13:07:27.192887 139799390091008 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.07133787870407104, loss=0.027757685631513596
I0210 13:07:58.834740 139871926159104 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.07169096916913986, loss=0.026322100311517715
I0210 13:08:18.597596 140039251117888 spec.py:321] Evaluating on the training split.
I0210 13:09:50.408579 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 13:09:53.427797 140039251117888 spec.py:349] Evaluating on the test split.
I0210 13:09:58.910224 140039251117888 submission_runner.py:408] Time since start: 20651.14s, 	Step: 44963, 	{'train/accuracy': 0.9930409789085388, 'train/loss': 0.022134987637400627, 'train/mean_average_precision': 0.602127596669349, 'validation/accuracy': 0.9871600270271301, 'validation/loss': 0.044910237193107605, 'validation/mean_average_precision': 0.2858967786479935, 'validation/num_examples': 43793, 'test/accuracy': 0.9862648248672485, 'test/loss': 0.04774727299809456, 'test/mean_average_precision': 0.2821340983594425, 'test/num_examples': 43793, 'score': 14419.119592666626, 'total_duration': 20651.140026569366, 'accumulated_submission_time': 14419.119592666626, 'accumulated_eval_time': 6228.195499420166, 'accumulated_logging_time': 2.649040937423706}
I0210 13:09:58.934797 139799398483712 logging_writer.py:48] [44963] accumulated_eval_time=6228.195499, accumulated_logging_time=2.649041, accumulated_submission_time=14419.119593, global_step=44963, preemption_count=0, score=14419.119593, test/accuracy=0.986265, test/loss=0.047747, test/mean_average_precision=0.282134, test/num_examples=43793, total_duration=20651.140027, train/accuracy=0.993041, train/loss=0.022135, train/mean_average_precision=0.602128, validation/accuracy=0.987160, validation/loss=0.044910, validation/mean_average_precision=0.285897, validation/num_examples=43793
I0210 13:10:11.186138 139818172847872 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.07835864275693893, loss=0.026297442615032196
I0210 13:10:42.513971 139799398483712 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.08270327001810074, loss=0.02883519046008587
I0210 13:11:13.810750 139818172847872 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.10295256972312927, loss=0.027120906859636307
I0210 13:11:45.197094 139799398483712 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.07498227059841156, loss=0.02501070126891136
I0210 13:12:16.543015 139818172847872 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.0819888636469841, loss=0.027983607724308968
I0210 13:12:47.819904 139799398483712 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.0766303613781929, loss=0.02692008763551712
I0210 13:13:19.407277 139818172847872 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.06724578142166138, loss=0.02338147908449173
I0210 13:13:51.012964 139799398483712 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.08217275887727737, loss=0.02576689049601555
I0210 13:13:59.181421 140039251117888 spec.py:321] Evaluating on the training split.
I0210 13:15:32.305123 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 13:15:35.353525 140039251117888 spec.py:349] Evaluating on the test split.
I0210 13:15:38.390652 140039251117888 submission_runner.py:408] Time since start: 20990.62s, 	Step: 45727, 	{'train/accuracy': 0.9928721785545349, 'train/loss': 0.02260545827448368, 'train/mean_average_precision': 0.583901784898843, 'validation/accuracy': 0.9871146082878113, 'validation/loss': 0.04508694261312485, 'validation/mean_average_precision': 0.2856805049431565, 'validation/num_examples': 43793, 'test/accuracy': 0.9863384962081909, 'test/loss': 0.04790147393941879, 'test/mean_average_precision': 0.2862644840320328, 'test/num_examples': 43793, 'score': 14659.334668159485, 'total_duration': 20990.620449781418, 'accumulated_submission_time': 14659.334668159485, 'accumulated_eval_time': 6327.404683113098, 'accumulated_logging_time': 2.686012029647827}
I0210 13:15:38.415778 139799390091008 logging_writer.py:48] [45727] accumulated_eval_time=6327.404683, accumulated_logging_time=2.686012, accumulated_submission_time=14659.334668, global_step=45727, preemption_count=0, score=14659.334668, test/accuracy=0.986338, test/loss=0.047901, test/mean_average_precision=0.286264, test/num_examples=43793, total_duration=20990.620450, train/accuracy=0.992872, train/loss=0.022605, train/mean_average_precision=0.583902, validation/accuracy=0.987115, validation/loss=0.045087, validation/mean_average_precision=0.285681, validation/num_examples=43793
I0210 13:16:01.751563 139813965014784 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.09368614107370377, loss=0.02606360614299774
I0210 13:16:33.199227 139799390091008 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.08412539213895798, loss=0.024718787521123886
I0210 13:17:04.738566 139813965014784 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.08749984204769135, loss=0.02379814349114895
I0210 13:17:36.423390 139799390091008 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.07133270800113678, loss=0.025417529046535492
I0210 13:18:08.081529 139813965014784 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.11735500395298004, loss=0.027803568169474602
I0210 13:18:39.496858 139799390091008 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.07956463098526001, loss=0.027058463543653488
I0210 13:19:10.978446 139813965014784 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.09264145791530609, loss=0.025883881375193596
I0210 13:19:38.630496 140039251117888 spec.py:321] Evaluating on the training split.
I0210 13:21:08.922924 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 13:21:11.976684 140039251117888 spec.py:349] Evaluating on the test split.
I0210 13:21:14.980538 140039251117888 submission_runner.py:408] Time since start: 21327.21s, 	Step: 46489, 	{'train/accuracy': 0.9927763342857361, 'train/loss': 0.022976737469434738, 'train/mean_average_precision': 0.5796602909704152, 'validation/accuracy': 0.9870861768722534, 'validation/loss': 0.04499715939164162, 'validation/mean_average_precision': 0.28788446774354765, 'validation/num_examples': 43793, 'test/accuracy': 0.9862883687019348, 'test/loss': 0.04792165756225586, 'test/mean_average_precision': 0.2808426418318341, 'test/num_examples': 43793, 'score': 14899.518951892853, 'total_duration': 21327.210332155228, 'accumulated_submission_time': 14899.518951892853, 'accumulated_eval_time': 6423.754679679871, 'accumulated_logging_time': 2.7223784923553467}
I0210 13:21:15.005336 139799398483712 logging_writer.py:48] [46489] accumulated_eval_time=6423.754680, accumulated_logging_time=2.722378, accumulated_submission_time=14899.518952, global_step=46489, preemption_count=0, score=14899.518952, test/accuracy=0.986288, test/loss=0.047922, test/mean_average_precision=0.280843, test/num_examples=43793, total_duration=21327.210332, train/accuracy=0.992776, train/loss=0.022977, train/mean_average_precision=0.579660, validation/accuracy=0.987086, validation/loss=0.044997, validation/mean_average_precision=0.287884, validation/num_examples=43793
I0210 13:21:18.817203 139871926159104 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.09934445470571518, loss=0.025173379108309746
I0210 13:21:50.711978 139799398483712 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.06695162504911423, loss=0.022813847288489342
I0210 13:22:22.443641 139871926159104 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.09633959829807281, loss=0.027198858559131622
I0210 13:22:54.398332 139799398483712 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.07524824887514114, loss=0.024663981050252914
I0210 13:23:25.965759 139871926159104 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.10166016966104507, loss=0.027873922139406204
I0210 13:23:57.682863 139799398483712 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.0693555623292923, loss=0.023157063871622086
I0210 13:24:29.697654 139871926159104 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.08340441435575485, loss=0.026637626811861992
I0210 13:25:01.118768 139799398483712 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.08410456031560898, loss=0.025392362847924232
I0210 13:25:15.184605 140039251117888 spec.py:321] Evaluating on the training split.
I0210 13:26:54.210655 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 13:26:57.276138 140039251117888 spec.py:349] Evaluating on the test split.
I0210 13:27:00.306824 140039251117888 submission_runner.py:408] Time since start: 21672.54s, 	Step: 47245, 	{'train/accuracy': 0.9926639199256897, 'train/loss': 0.02317655086517334, 'train/mean_average_precision': 0.5689805751294961, 'validation/accuracy': 0.9870666861534119, 'validation/loss': 0.045409902930259705, 'validation/mean_average_precision': 0.2847175279822495, 'validation/num_examples': 43793, 'test/accuracy': 0.9861990809440613, 'test/loss': 0.04832647368311882, 'test/mean_average_precision': 0.2735987649905027, 'test/num_examples': 43793, 'score': 15139.666759252548, 'total_duration': 21672.53662109375, 'accumulated_submission_time': 15139.666759252548, 'accumulated_eval_time': 6528.876852750778, 'accumulated_logging_time': 2.7595677375793457}
I0210 13:27:00.331487 139799390091008 logging_writer.py:48] [47245] accumulated_eval_time=6528.876853, accumulated_logging_time=2.759568, accumulated_submission_time=15139.666759, global_step=47245, preemption_count=0, score=15139.666759, test/accuracy=0.986199, test/loss=0.048326, test/mean_average_precision=0.273599, test/num_examples=43793, total_duration=21672.536621, train/accuracy=0.992664, train/loss=0.023177, train/mean_average_precision=0.568981, validation/accuracy=0.987067, validation/loss=0.045410, validation/mean_average_precision=0.284718, validation/num_examples=43793
I0210 13:27:18.343378 139813965014784 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.07328517735004425, loss=0.023361677303910255
I0210 13:27:50.126772 139799390091008 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.09508553147315979, loss=0.02549937553703785
I0210 13:28:22.257912 139813965014784 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.09012999385595322, loss=0.023446235805749893
I0210 13:28:53.726788 139799390091008 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.08191166818141937, loss=0.02381170354783535
I0210 13:29:25.621793 139813965014784 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.07755101472139359, loss=0.023159503936767578
I0210 13:29:57.778646 139799390091008 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.09004127979278564, loss=0.026123924180865288
I0210 13:30:30.548335 139813965014784 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.09508019685745239, loss=0.023692043498158455
I0210 13:31:00.628598 140039251117888 spec.py:321] Evaluating on the training split.
I0210 13:32:32.729248 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 13:32:35.796319 140039251117888 spec.py:349] Evaluating on the test split.
I0210 13:32:38.779992 140039251117888 submission_runner.py:408] Time since start: 22011.01s, 	Step: 47994, 	{'train/accuracy': 0.9927076697349548, 'train/loss': 0.022998401895165443, 'train/mean_average_precision': 0.5812239180112349, 'validation/accuracy': 0.9869928359985352, 'validation/loss': 0.04547933116555214, 'validation/mean_average_precision': 0.2820994474233479, 'validation/num_examples': 43793, 'test/accuracy': 0.9861843585968018, 'test/loss': 0.048406634479761124, 'test/mean_average_precision': 0.27019014397531693, 'test/num_examples': 43793, 'score': 15379.932568311691, 'total_duration': 22011.009790182114, 'accumulated_submission_time': 15379.932568311691, 'accumulated_eval_time': 6627.028215408325, 'accumulated_logging_time': 2.79514479637146}
I0210 13:32:38.804806 139799398483712 logging_writer.py:48] [47994] accumulated_eval_time=6627.028215, accumulated_logging_time=2.795145, accumulated_submission_time=15379.932568, global_step=47994, preemption_count=0, score=15379.932568, test/accuracy=0.986184, test/loss=0.048407, test/mean_average_precision=0.270190, test/num_examples=43793, total_duration=22011.009790, train/accuracy=0.992708, train/loss=0.022998, train/mean_average_precision=0.581224, validation/accuracy=0.986993, validation/loss=0.045479, validation/mean_average_precision=0.282099, validation/num_examples=43793
I0210 13:32:41.043010 139818172847872 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.08611937612295151, loss=0.026409229263663292
I0210 13:33:13.328072 139799398483712 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.07778509706258774, loss=0.02539665438234806
I0210 13:33:45.124158 139818172847872 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.07506060600280762, loss=0.024860497564077377
I0210 13:34:16.895675 139799398483712 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.08930462598800659, loss=0.025098029524087906
I0210 13:34:48.871997 139818172847872 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.07085008174180984, loss=0.023319324478507042
I0210 13:35:20.784394 139799398483712 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.08543521165847778, loss=0.02723814733326435
I0210 13:35:52.586980 139818172847872 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.08429848402738571, loss=0.022255126386880875
I0210 13:36:24.343425 139799398483712 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.07474230974912643, loss=0.024328352883458138
I0210 13:36:39.031909 140039251117888 spec.py:321] Evaluating on the training split.
I0210 13:38:08.564164 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 13:38:11.607596 140039251117888 spec.py:349] Evaluating on the test split.
I0210 13:38:14.677361 140039251117888 submission_runner.py:408] Time since start: 22346.91s, 	Step: 48748, 	{'train/accuracy': 0.992895781993866, 'train/loss': 0.02250996232032776, 'train/mean_average_precision': 0.6029228174570651, 'validation/accuracy': 0.9870654940605164, 'validation/loss': 0.04532771185040474, 'validation/mean_average_precision': 0.28648249001307446, 'validation/num_examples': 43793, 'test/accuracy': 0.9862820506095886, 'test/loss': 0.048175618052482605, 'test/mean_average_precision': 0.27638856792281613, 'test/num_examples': 43793, 'score': 15620.129272222519, 'total_duration': 22346.907153129578, 'accumulated_submission_time': 15620.129272222519, 'accumulated_eval_time': 6722.673615455627, 'accumulated_logging_time': 2.8310930728912354}
I0210 13:38:14.702014 139799390091008 logging_writer.py:48] [48748] accumulated_eval_time=6722.673615, accumulated_logging_time=2.831093, accumulated_submission_time=15620.129272, global_step=48748, preemption_count=0, score=15620.129272, test/accuracy=0.986282, test/loss=0.048176, test/mean_average_precision=0.276389, test/num_examples=43793, total_duration=22346.907153, train/accuracy=0.992896, train/loss=0.022510, train/mean_average_precision=0.602923, validation/accuracy=0.987065, validation/loss=0.045328, validation/mean_average_precision=0.286482, validation/num_examples=43793
I0210 13:38:31.461465 139871926159104 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.10987845808267593, loss=0.023687168955802917
I0210 13:39:03.386413 139799390091008 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.10420066118240356, loss=0.02592768892645836
I0210 13:39:35.401735 139871926159104 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.08121734857559204, loss=0.025425894185900688
I0210 13:40:07.480240 139799390091008 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.0807635560631752, loss=0.026627175509929657
I0210 13:40:39.182862 139871926159104 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.08769048750400543, loss=0.02659326232969761
I0210 13:41:11.252476 139799390091008 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.0848492905497551, loss=0.02225688472390175
I0210 13:41:42.924710 139871926159104 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.07469917088747025, loss=0.025739632546901703
I0210 13:42:14.680505 140039251117888 spec.py:321] Evaluating on the training split.
I0210 13:43:52.353962 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 13:43:55.445460 140039251117888 spec.py:349] Evaluating on the test split.
I0210 13:43:58.480591 140039251117888 submission_runner.py:408] Time since start: 22690.71s, 	Step: 49500, 	{'train/accuracy': 0.9930621981620789, 'train/loss': 0.021937798708677292, 'train/mean_average_precision': 0.6060850216324132, 'validation/accuracy': 0.9870135188102722, 'validation/loss': 0.04548106715083122, 'validation/mean_average_precision': 0.28988763798470013, 'validation/num_examples': 43793, 'test/accuracy': 0.9862605929374695, 'test/loss': 0.048380717635154724, 'test/mean_average_precision': 0.27548869856047037, 'test/num_examples': 43793, 'score': 15860.07743358612, 'total_duration': 22690.710390090942, 'accumulated_submission_time': 15860.07743358612, 'accumulated_eval_time': 6826.473671674728, 'accumulated_logging_time': 2.8667874336242676}
I0210 13:43:58.505401 139799398483712 logging_writer.py:48] [49500] accumulated_eval_time=6826.473672, accumulated_logging_time=2.866787, accumulated_submission_time=15860.077434, global_step=49500, preemption_count=0, score=15860.077434, test/accuracy=0.986261, test/loss=0.048381, test/mean_average_precision=0.275489, test/num_examples=43793, total_duration=22690.710390, train/accuracy=0.993062, train/loss=0.021938, train/mean_average_precision=0.606085, validation/accuracy=0.987014, validation/loss=0.045481, validation/mean_average_precision=0.289888, validation/num_examples=43793
I0210 13:43:58.871960 139813965014784 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.099669449031353, loss=0.025056976824998856
I0210 13:44:31.170919 139799398483712 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.08728136867284775, loss=0.023140376433730125
I0210 13:45:02.973410 139813965014784 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.08977761119604111, loss=0.0246410071849823
I0210 13:45:34.811348 139799398483712 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.08894263952970505, loss=0.022392693907022476
I0210 13:46:06.735393 139813965014784 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.09619176387786865, loss=0.023357583209872246
I0210 13:46:38.115982 139799398483712 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.08181364834308624, loss=0.020165955647826195
I0210 13:47:09.832967 139813965014784 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.10020241141319275, loss=0.026034396141767502
I0210 13:47:41.546413 139799398483712 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.09703818708658218, loss=0.024418259039521217
I0210 13:47:58.694272 140039251117888 spec.py:321] Evaluating on the training split.
I0210 13:49:28.892540 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 13:49:32.047275 140039251117888 spec.py:349] Evaluating on the test split.
I0210 13:49:35.179492 140039251117888 submission_runner.py:408] Time since start: 23027.41s, 	Step: 50255, 	{'train/accuracy': 0.9931796193122864, 'train/loss': 0.02170548215508461, 'train/mean_average_precision': 0.5990312491287693, 'validation/accuracy': 0.9870184063911438, 'validation/loss': 0.04573266953229904, 'validation/mean_average_precision': 0.2864895013825414, 'validation/num_examples': 43793, 'test/accuracy': 0.9861767888069153, 'test/loss': 0.0484723262488842, 'test/mean_average_precision': 0.27364605936614306, 'test/num_examples': 43793, 'score': 16100.23567533493, 'total_duration': 23027.409290790558, 'accumulated_submission_time': 16100.23567533493, 'accumulated_eval_time': 6922.95885682106, 'accumulated_logging_time': 2.9028890132904053}
I0210 13:49:35.204767 139818172847872 logging_writer.py:48] [50255] accumulated_eval_time=6922.958857, accumulated_logging_time=2.902889, accumulated_submission_time=16100.235675, global_step=50255, preemption_count=0, score=16100.235675, test/accuracy=0.986177, test/loss=0.048472, test/mean_average_precision=0.273646, test/num_examples=43793, total_duration=23027.409291, train/accuracy=0.993180, train/loss=0.021705, train/mean_average_precision=0.599031, validation/accuracy=0.987018, validation/loss=0.045733, validation/mean_average_precision=0.286490, validation/num_examples=43793
I0210 13:49:49.670078 139871926159104 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.09637728333473206, loss=0.02451026439666748
I0210 13:50:21.199423 139818172847872 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.08246828615665436, loss=0.022541334852576256
I0210 13:50:52.754425 139871926159104 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.0918075293302536, loss=0.025044504553079605
I0210 13:51:24.463120 139818172847872 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.08353786170482635, loss=0.022600600495934486
I0210 13:51:55.707431 139871926159104 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.1029287800192833, loss=0.028104180470108986
I0210 13:52:27.542370 139818172847872 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.0820932388305664, loss=0.025344498455524445
I0210 13:52:58.999374 139871926159104 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.08483079075813293, loss=0.02465408854186535
I0210 13:53:30.638377 139818172847872 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.08233547955751419, loss=0.022296642884612083
I0210 13:53:35.418144 140039251117888 spec.py:321] Evaluating on the training split.
I0210 13:55:14.125132 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 13:55:17.211248 140039251117888 spec.py:349] Evaluating on the test split.
I0210 13:55:20.272162 140039251117888 submission_runner.py:408] Time since start: 23372.50s, 	Step: 51016, 	{'train/accuracy': 0.9933199286460876, 'train/loss': 0.021025843918323517, 'train/mean_average_precision': 0.6328068561122213, 'validation/accuracy': 0.9870354533195496, 'validation/loss': 0.04584246128797531, 'validation/mean_average_precision': 0.28707328510616265, 'validation/num_examples': 43793, 'test/accuracy': 0.9862424731254578, 'test/loss': 0.04855342209339142, 'test/mean_average_precision': 0.2766943537149877, 'test/num_examples': 43793, 'score': 16340.418535232544, 'total_duration': 23372.501963615417, 'accumulated_submission_time': 16340.418535232544, 'accumulated_eval_time': 7027.812830686569, 'accumulated_logging_time': 2.9394538402557373}
I0210 13:55:20.297479 139799390091008 logging_writer.py:48] [51016] accumulated_eval_time=7027.812831, accumulated_logging_time=2.939454, accumulated_submission_time=16340.418535, global_step=51016, preemption_count=0, score=16340.418535, test/accuracy=0.986242, test/loss=0.048553, test/mean_average_precision=0.276694, test/num_examples=43793, total_duration=23372.501964, train/accuracy=0.993320, train/loss=0.021026, train/mean_average_precision=0.632807, validation/accuracy=0.987035, validation/loss=0.045842, validation/mean_average_precision=0.287073, validation/num_examples=43793
I0210 13:55:47.823450 139813965014784 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.08697843551635742, loss=0.022048911079764366
I0210 13:56:19.558221 139799390091008 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.08521582186222076, loss=0.022884150967001915
I0210 13:56:50.903271 139813965014784 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.08575107157230377, loss=0.023521026596426964
I0210 13:57:22.383084 139799390091008 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.10600727051496506, loss=0.02462003193795681
I0210 13:57:53.962724 139813965014784 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.10032372176647186, loss=0.023684734478592873
I0210 13:58:25.719835 139799390091008 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.10449954122304916, loss=0.023019757121801376
I0210 13:58:57.498681 139813965014784 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.086270771920681, loss=0.023736972361803055
I0210 13:59:20.456908 140039251117888 spec.py:321] Evaluating on the training split.
I0210 14:00:56.178361 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 14:00:59.344463 140039251117888 spec.py:349] Evaluating on the test split.
I0210 14:01:02.643296 140039251117888 submission_runner.py:408] Time since start: 23714.87s, 	Step: 51773, 	{'train/accuracy': 0.993573784828186, 'train/loss': 0.02038927562534809, 'train/mean_average_precision': 0.6390301099909069, 'validation/accuracy': 0.9870334267616272, 'validation/loss': 0.046002548187971115, 'validation/mean_average_precision': 0.28934933586599526, 'validation/num_examples': 43793, 'test/accuracy': 0.9862707257270813, 'test/loss': 0.048848822712898254, 'test/mean_average_precision': 0.2793308413871986, 'test/num_examples': 43793, 'score': 16580.547554254532, 'total_duration': 23714.87309408188, 'accumulated_submission_time': 16580.547554254532, 'accumulated_eval_time': 7129.999175310135, 'accumulated_logging_time': 2.975675106048584}
I0210 14:01:02.668880 139818172847872 logging_writer.py:48] [51773] accumulated_eval_time=7129.999175, accumulated_logging_time=2.975675, accumulated_submission_time=16580.547554, global_step=51773, preemption_count=0, score=16580.547554, test/accuracy=0.986271, test/loss=0.048849, test/mean_average_precision=0.279331, test/num_examples=43793, total_duration=23714.873094, train/accuracy=0.993574, train/loss=0.020389, train/mean_average_precision=0.639030, validation/accuracy=0.987033, validation/loss=0.046003, validation/mean_average_precision=0.289349, validation/num_examples=43793
I0210 14:01:11.551309 139871926159104 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.08780001848936081, loss=0.02471325732767582
I0210 14:01:43.294763 139818172847872 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.09758488833904266, loss=0.029016243293881416
I0210 14:02:15.074226 139871926159104 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.08204100281000137, loss=0.021359626203775406
I0210 14:02:47.072793 139818172847872 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.11395454406738281, loss=0.025044774636626244
I0210 14:03:19.444999 139871926159104 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.0894143134355545, loss=0.023412423208355904
I0210 14:03:51.448620 139818172847872 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.09442993998527527, loss=0.02377723902463913
I0210 14:04:23.579011 139871926159104 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.08654175698757172, loss=0.0217923354357481
I0210 14:04:55.643794 139818172847872 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.09397732466459274, loss=0.025415528565645218
I0210 14:05:02.838866 140039251117888 spec.py:321] Evaluating on the training split.
I0210 14:06:37.141929 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 14:06:40.531623 140039251117888 spec.py:349] Evaluating on the test split.
I0210 14:06:43.855780 140039251117888 submission_runner.py:408] Time since start: 24056.09s, 	Step: 52523, 	{'train/accuracy': 0.9938708543777466, 'train/loss': 0.019789312034845352, 'train/mean_average_precision': 0.6536397233896962, 'validation/accuracy': 0.9869379997253418, 'validation/loss': 0.04590245708823204, 'validation/mean_average_precision': 0.2885353006882328, 'validation/num_examples': 43793, 'test/accuracy': 0.9861144423484802, 'test/loss': 0.04876953735947609, 'test/mean_average_precision': 0.2781312477674411, 'test/num_examples': 43793, 'score': 16820.687520742416, 'total_duration': 24056.085558652878, 'accumulated_submission_time': 16820.687520742416, 'accumulated_eval_time': 7231.016023159027, 'accumulated_logging_time': 3.0121028423309326}
I0210 14:06:43.884454 139799390091008 logging_writer.py:48] [52523] accumulated_eval_time=7231.016023, accumulated_logging_time=3.012103, accumulated_submission_time=16820.687521, global_step=52523, preemption_count=0, score=16820.687521, test/accuracy=0.986114, test/loss=0.048770, test/mean_average_precision=0.278131, test/num_examples=43793, total_duration=24056.085559, train/accuracy=0.993871, train/loss=0.019789, train/mean_average_precision=0.653640, validation/accuracy=0.986938, validation/loss=0.045902, validation/mean_average_precision=0.288535, validation/num_examples=43793
I0210 14:07:09.349912 139813965014784 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.0889667421579361, loss=0.023418577387928963
I0210 14:07:41.888898 139799390091008 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.10965118557214737, loss=0.025184713304042816
I0210 14:08:14.716011 139813965014784 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.09969417005777359, loss=0.023454686626791954
I0210 14:08:47.319894 139799390091008 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.0886431485414505, loss=0.019883107393980026
I0210 14:09:19.476697 139813965014784 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.09902723878622055, loss=0.023024745285511017
I0210 14:09:51.073821 139799390091008 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.10440881550312042, loss=0.025783194229006767
I0210 14:10:22.489458 139813965014784 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.09482157230377197, loss=0.024617159739136696
I0210 14:10:43.888544 140039251117888 spec.py:321] Evaluating on the training split.
I0210 14:12:17.723494 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 14:12:20.853567 140039251117888 spec.py:349] Evaluating on the test split.
I0210 14:12:23.885136 140039251117888 submission_runner.py:408] Time since start: 24396.11s, 	Step: 53269, 	{'train/accuracy': 0.9936069250106812, 'train/loss': 0.02018664963543415, 'train/mean_average_precision': 0.6379870321649714, 'validation/accuracy': 0.98707115650177, 'validation/loss': 0.046360552310943604, 'validation/mean_average_precision': 0.2920631409937257, 'validation/num_examples': 43793, 'test/accuracy': 0.9862328171730042, 'test/loss': 0.049179110676050186, 'test/mean_average_precision': 0.27610814570712733, 'test/num_examples': 43793, 'score': 17060.65802717209, 'total_duration': 24396.114936590195, 'accumulated_submission_time': 17060.65802717209, 'accumulated_eval_time': 7331.012581825256, 'accumulated_logging_time': 3.0534260272979736}
I0210 14:12:23.910674 139799398483712 logging_writer.py:48] [53269] accumulated_eval_time=7331.012582, accumulated_logging_time=3.053426, accumulated_submission_time=17060.658027, global_step=53269, preemption_count=0, score=17060.658027, test/accuracy=0.986233, test/loss=0.049179, test/mean_average_precision=0.276108, test/num_examples=43793, total_duration=24396.114937, train/accuracy=0.993607, train/loss=0.020187, train/mean_average_precision=0.637987, validation/accuracy=0.987071, validation/loss=0.046361, validation/mean_average_precision=0.292063, validation/num_examples=43793
I0210 14:12:34.164912 139871926159104 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.10829945653676987, loss=0.021711405366659164
I0210 14:13:06.172201 139799398483712 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.09417910873889923, loss=0.024859102442860603
I0210 14:13:37.686837 139871926159104 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.09130998700857162, loss=0.024281825870275497
I0210 14:14:09.535221 139799398483712 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.09201464056968689, loss=0.024090848863124847
I0210 14:14:41.563825 139871926159104 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.10322921723127365, loss=0.022322166711091995
I0210 14:15:13.787527 139799398483712 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.12375880777835846, loss=0.025576967746019363
I0210 14:15:46.612030 139871926159104 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.10302633792161942, loss=0.019137483090162277
I0210 14:16:19.215054 139799398483712 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.08487995713949203, loss=0.02245769277215004
I0210 14:16:24.105574 140039251117888 spec.py:321] Evaluating on the training split.
I0210 14:18:00.478875 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 14:18:03.699119 140039251117888 spec.py:349] Evaluating on the test split.
I0210 14:18:06.692903 140039251117888 submission_runner.py:408] Time since start: 24738.92s, 	Step: 54016, 	{'train/accuracy': 0.9934079051017761, 'train/loss': 0.02068304270505905, 'train/mean_average_precision': 0.6422283249361697, 'validation/accuracy': 0.9870346188545227, 'validation/loss': 0.046364326030015945, 'validation/mean_average_precision': 0.2859237206899969, 'validation/num_examples': 43793, 'test/accuracy': 0.9862012267112732, 'test/loss': 0.049231842160224915, 'test/mean_average_precision': 0.2789847800402106, 'test/num_examples': 43793, 'score': 17300.819898843765, 'total_duration': 24738.922700881958, 'accumulated_submission_time': 17300.819898843765, 'accumulated_eval_time': 7433.599872112274, 'accumulated_logging_time': 3.091221570968628}
I0210 14:18:06.718957 139799390091008 logging_writer.py:48] [54016] accumulated_eval_time=7433.599872, accumulated_logging_time=3.091222, accumulated_submission_time=17300.819899, global_step=54016, preemption_count=0, score=17300.819899, test/accuracy=0.986201, test/loss=0.049232, test/mean_average_precision=0.278985, test/num_examples=43793, total_duration=24738.922701, train/accuracy=0.993408, train/loss=0.020683, train/mean_average_precision=0.642228, validation/accuracy=0.987035, validation/loss=0.046364, validation/mean_average_precision=0.285924, validation/num_examples=43793
I0210 14:18:34.068669 139818172847872 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.08292382955551147, loss=0.02177957072854042
I0210 14:19:06.345715 139799390091008 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.09330321103334427, loss=0.02054457925260067
I0210 14:19:38.672812 139818172847872 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.10902892798185349, loss=0.023853540420532227
I0210 14:20:10.970929 139799390091008 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.09678206592798233, loss=0.023816535249352455
I0210 14:20:43.295639 139818172847872 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.09415441751480103, loss=0.02156306616961956
I0210 14:21:15.552868 139799390091008 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.10971230268478394, loss=0.022425204515457153
I0210 14:21:47.974393 139818172847872 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.09943380951881409, loss=0.02437431924045086
I0210 14:22:06.945489 140039251117888 spec.py:321] Evaluating on the training split.
I0210 14:23:41.968464 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 14:23:45.002085 140039251117888 spec.py:349] Evaluating on the test split.
I0210 14:23:47.974987 140039251117888 submission_runner.py:408] Time since start: 25080.20s, 	Step: 54759, 	{'train/accuracy': 0.9934549331665039, 'train/loss': 0.020581141114234924, 'train/mean_average_precision': 0.6245154233950967, 'validation/accuracy': 0.9870610237121582, 'validation/loss': 0.04653650149703026, 'validation/mean_average_precision': 0.2884550829211076, 'validation/num_examples': 43793, 'test/accuracy': 0.9863005876541138, 'test/loss': 0.049402348697185516, 'test/mean_average_precision': 0.27736942988361163, 'test/num_examples': 43793, 'score': 17541.016096830368, 'total_duration': 25080.204783678055, 'accumulated_submission_time': 17541.016096830368, 'accumulated_eval_time': 7534.629323959351, 'accumulated_logging_time': 3.1281652450561523}
I0210 14:23:48.001668 139799398483712 logging_writer.py:48] [54759] accumulated_eval_time=7534.629324, accumulated_logging_time=3.128165, accumulated_submission_time=17541.016097, global_step=54759, preemption_count=0, score=17541.016097, test/accuracy=0.986301, test/loss=0.049402, test/mean_average_precision=0.277369, test/num_examples=43793, total_duration=25080.204784, train/accuracy=0.993455, train/loss=0.020581, train/mean_average_precision=0.624515, validation/accuracy=0.987061, validation/loss=0.046537, validation/mean_average_precision=0.288455, validation/num_examples=43793
I0210 14:24:01.730025 139813965014784 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.10202427208423615, loss=0.021244974806904793
I0210 14:24:33.989626 139799398483712 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.09135007113218307, loss=0.021060897037386894
I0210 14:25:06.066082 139813965014784 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.09850043058395386, loss=0.023191938176751137
I0210 14:25:37.755564 139799398483712 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.12166063487529755, loss=0.02457965351641178
I0210 14:26:10.115015 139813965014784 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.10799559205770493, loss=0.023484813049435616
I0210 14:26:42.434544 139799398483712 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.1015816330909729, loss=0.02176845632493496
I0210 14:27:14.772612 139813965014784 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.10036112368106842, loss=0.022943265736103058
I0210 14:27:46.897406 139799398483712 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.10245431959629059, loss=0.025170492008328438
I0210 14:27:48.184386 140039251117888 spec.py:321] Evaluating on the training split.
I0210 14:29:18.376462 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 14:29:21.423286 140039251117888 spec.py:349] Evaluating on the test split.
I0210 14:29:24.461945 140039251117888 submission_runner.py:408] Time since start: 25416.69s, 	Step: 55505, 	{'train/accuracy': 0.9934847950935364, 'train/loss': 0.020396802574396133, 'train/mean_average_precision': 0.6366287142374494, 'validation/accuracy': 0.9871174097061157, 'validation/loss': 0.04654433950781822, 'validation/mean_average_precision': 0.2906898791426333, 'validation/num_examples': 43793, 'test/accuracy': 0.9862551093101501, 'test/loss': 0.0496644601225853, 'test/mean_average_precision': 0.2748268458454385, 'test/num_examples': 43793, 'score': 17781.16787624359, 'total_duration': 25416.691744089127, 'accumulated_submission_time': 17781.16787624359, 'accumulated_eval_time': 7630.906837940216, 'accumulated_logging_time': 3.166072130203247}
I0210 14:29:24.487869 139799390091008 logging_writer.py:48] [55505] accumulated_eval_time=7630.906838, accumulated_logging_time=3.166072, accumulated_submission_time=17781.167876, global_step=55505, preemption_count=0, score=17781.167876, test/accuracy=0.986255, test/loss=0.049664, test/mean_average_precision=0.274827, test/num_examples=43793, total_duration=25416.691744, train/accuracy=0.993485, train/loss=0.020397, train/mean_average_precision=0.636629, validation/accuracy=0.987117, validation/loss=0.046544, validation/mean_average_precision=0.290690, validation/num_examples=43793
I0210 14:29:55.715466 139871926159104 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.12821342051029205, loss=0.025188662111759186
I0210 14:30:27.799244 139799390091008 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.10205551236867905, loss=0.023879200220108032
I0210 14:30:59.818180 139871926159104 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.09336458891630173, loss=0.021542780101299286
I0210 14:31:32.024328 139799390091008 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.0954718291759491, loss=0.024935616180300713
I0210 14:32:04.140427 139871926159104 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.0983181819319725, loss=0.022109748795628548
I0210 14:32:36.059761 139799390091008 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.09710565954446793, loss=0.021180110052227974
I0210 14:33:08.381952 139871926159104 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.1035844087600708, loss=0.022997045889496803
I0210 14:33:24.588625 140039251117888 spec.py:321] Evaluating on the training split.
I0210 14:34:58.784150 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 14:35:02.150121 140039251117888 spec.py:349] Evaluating on the test split.
I0210 14:35:07.628170 140039251117888 submission_runner.py:408] Time since start: 25759.86s, 	Step: 56252, 	{'train/accuracy': 0.993678867816925, 'train/loss': 0.01993425376713276, 'train/mean_average_precision': 0.6509323873209593, 'validation/accuracy': 0.9869863390922546, 'validation/loss': 0.04683464393019676, 'validation/mean_average_precision': 0.28849475560347887, 'validation/num_examples': 43793, 'test/accuracy': 0.9861961603164673, 'test/loss': 0.04965412616729736, 'test/mean_average_precision': 0.27707614673272585, 'test/num_examples': 43793, 'score': 18021.238358020782, 'total_duration': 25759.857971429825, 'accumulated_submission_time': 18021.238358020782, 'accumulated_eval_time': 7733.94634437561, 'accumulated_logging_time': 3.202991008758545}
I0210 14:35:07.654896 139799398483712 logging_writer.py:48] [56252] accumulated_eval_time=7733.946344, accumulated_logging_time=3.202991, accumulated_submission_time=18021.238358, global_step=56252, preemption_count=0, score=18021.238358, test/accuracy=0.986196, test/loss=0.049654, test/mean_average_precision=0.277076, test/num_examples=43793, total_duration=25759.857971, train/accuracy=0.993679, train/loss=0.019934, train/mean_average_precision=0.650932, validation/accuracy=0.986986, validation/loss=0.046835, validation/mean_average_precision=0.288495, validation/num_examples=43793
I0210 14:35:23.523190 139813965014784 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.10021104663610458, loss=0.022023433819413185
I0210 14:35:55.345180 139799398483712 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.1161099374294281, loss=0.023570414632558823
I0210 14:36:27.377901 139813965014784 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.09536492079496384, loss=0.022242868319153786
I0210 14:36:59.232988 139799398483712 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.1206812709569931, loss=0.02374962717294693
I0210 14:37:31.402489 139813965014784 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.09877371042966843, loss=0.022140130400657654
I0210 14:38:03.927131 139799398483712 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.12308816611766815, loss=0.020531848073005676
I0210 14:38:35.933593 139813965014784 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.09906330704689026, loss=0.021053437143564224
I0210 14:39:07.920944 139799398483712 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.11889653652906418, loss=0.023783724755048752
I0210 14:39:07.926300 140039251117888 spec.py:321] Evaluating on the training split.
I0210 14:40:39.382439 140039251117888 spec.py:333] Evaluating on the validation split.
I0210 14:40:42.459138 140039251117888 spec.py:349] Evaluating on the test split.
I0210 14:40:45.487982 140039251117888 submission_runner.py:408] Time since start: 26097.72s, 	Step: 57001, 	{'train/accuracy': 0.9936870336532593, 'train/loss': 0.01976962573826313, 'train/mean_average_precision': 0.6579927457781373, 'validation/accuracy': 0.9869254231452942, 'validation/loss': 0.0470384806394577, 'validation/mean_average_precision': 0.2847933420483199, 'validation/num_examples': 43793, 'test/accuracy': 0.9861481189727783, 'test/loss': 0.04986385628581047, 'test/mean_average_precision': 0.2743496012662404, 'test/num_examples': 43793, 'score': 18261.479460000992, 'total_duration': 26097.717761278152, 'accumulated_submission_time': 18261.479460000992, 'accumulated_eval_time': 7831.50794172287, 'accumulated_logging_time': 3.240222930908203}
I0210 14:40:45.513976 139818172847872 logging_writer.py:48] [57001] accumulated_eval_time=7831.507942, accumulated_logging_time=3.240223, accumulated_submission_time=18261.479460, global_step=57001, preemption_count=0, score=18261.479460, test/accuracy=0.986148, test/loss=0.049864, test/mean_average_precision=0.274350, test/num_examples=43793, total_duration=26097.717761, train/accuracy=0.993687, train/loss=0.019770, train/mean_average_precision=0.657993, validation/accuracy=0.986925, validation/loss=0.047038, validation/mean_average_precision=0.284793, validation/num_examples=43793
I0210 14:41:17.467986 139871926159104 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.11055781692266464, loss=0.02258160337805748
I0210 14:41:49.255271 139818172847872 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.12563452124595642, loss=0.023917533457279205
I0210 14:42:21.061161 139871926159104 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.10570807754993439, loss=0.021357091143727303
I0210 14:42:53.033307 139818172847872 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.10419358313083649, loss=0.019407764077186584
I0210 14:43:24.918874 139871926159104 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.10987194627523422, loss=0.02420990727841854
I0210 14:43:56.594151 139818172847872 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.11090895533561707, loss=0.023843899369239807
I0210 14:44:21.337388 139871926159104 logging_writer.py:48] [57678] global_step=57678, preemption_count=0, score=18477.257613
I0210 14:44:21.390421 140039251117888 checkpoints.py:490] Saving checkpoint at step: 57678
I0210 14:44:21.509264 140039251117888 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_3/ogbg_jax/trial_5/checkpoint_57678
I0210 14:44:21.510143 140039251117888 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_3/ogbg_jax/trial_5/checkpoint_57678.
I0210 14:44:21.691988 140039251117888 submission_runner.py:583] Tuning trial 5/5
I0210 14:44:21.692229 140039251117888 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0210 14:44:21.696127 140039251117888 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5250520706176758, 'train/loss': 0.7150473594665527, 'train/mean_average_precision': 0.02231542038668717, 'validation/accuracy': 0.5213832855224609, 'validation/loss': 0.7166012525558472, 'validation/mean_average_precision': 0.026142065301455825, 'validation/num_examples': 43793, 'test/accuracy': 0.5224818587303162, 'test/loss': 0.7161954641342163, 'test/mean_average_precision': 0.02783946806769139, 'test/num_examples': 43793, 'score': 13.229990720748901, 'total_duration': 116.30002975463867, 'accumulated_submission_time': 13.229990720748901, 'accumulated_eval_time': 103.06999778747559, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (758, {'train/accuracy': 0.986740231513977, 'train/loss': 0.05124962702393532, 'train/mean_average_precision': 0.056202073924971514, 'validation/accuracy': 0.9841589331626892, 'validation/loss': 0.061199311167001724, 'validation/mean_average_precision': 0.05523248937010333, 'validation/num_examples': 43793, 'test/accuracy': 0.9832048416137695, 'test/loss': 0.0645277202129364, 'test/mean_average_precision': 0.0557577787410884, 'test/num_examples': 43793, 'score': 253.348135471344, 'total_duration': 458.4430618286133, 'accumulated_submission_time': 253.348135471344, 'accumulated_eval_time': 205.0555601119995, 'accumulated_logging_time': 0.019795894622802734, 'global_step': 758, 'preemption_count': 0}), (1500, {'train/accuracy': 0.9867498874664307, 'train/loss': 0.050342004746198654, 'train/mean_average_precision': 0.07422550614128996, 'validation/accuracy': 0.9842255115509033, 'validation/loss': 0.060442958027124405, 'validation/mean_average_precision': 0.07188214537665251, 'validation/num_examples': 43793, 'test/accuracy': 0.9832578897476196, 'test/loss': 0.06376054137945175, 'test/mean_average_precision': 0.07471594251830098, 'test/num_examples': 43793, 'score': 493.5868184566498, 'total_duration': 805.2762854099274, 'accumulated_submission_time': 493.5868184566498, 'accumulated_eval_time': 311.6006577014923, 'accumulated_logging_time': 0.047776222229003906, 'global_step': 1500, 'preemption_count': 0}), (2254, {'train/accuracy': 0.9876054525375366, 'train/loss': 0.04437072202563286, 'train/mean_average_precision': 0.13130761852340767, 'validation/accuracy': 0.9849160313606262, 'validation/loss': 0.053602177649736404, 'validation/mean_average_precision': 0.12358903997484055, 'validation/num_examples': 43793, 'test/accuracy': 0.9839398264884949, 'test/loss': 0.05674533173441887, 'test/mean_average_precision': 0.11911557470652198, 'test/num_examples': 43793, 'score': 733.4666738510132, 'total_duration': 1146.283578157425, 'accumulated_submission_time': 733.4666738510132, 'accumulated_eval_time': 412.36032915115356, 'accumulated_logging_time': 0.3961923122406006, 'global_step': 2254, 'preemption_count': 0}), (2992, {'train/accuracy': 0.9879937171936035, 'train/loss': 0.04256334528326988, 'train/mean_average_precision': 0.15644186837769805, 'validation/accuracy': 0.9850869178771973, 'validation/loss': 0.051669634878635406, 'validation/mean_average_precision': 0.14192352319117627, 'validation/num_examples': 43793, 'test/accuracy': 0.9841234683990479, 'test/loss': 0.05444411188364029, 'test/mean_average_precision': 0.14019992793623207, 'test/num_examples': 43793, 'score': 973.4950165748596, 'total_duration': 1488.5906157493591, 'accumulated_submission_time': 973.4950165748596, 'accumulated_eval_time': 514.584146976471, 'accumulated_logging_time': 0.43120884895324707, 'global_step': 2992, 'preemption_count': 0}), (3734, {'train/accuracy': 0.9882489442825317, 'train/loss': 0.04114070162177086, 'train/mean_average_precision': 0.18479054694703131, 'validation/accuracy': 0.9853414297103882, 'validation/loss': 0.05053086206316948, 'validation/mean_average_precision': 0.16241611775463913, 'validation/num_examples': 43793, 'test/accuracy': 0.9844486117362976, 'test/loss': 0.053183663636446, 'test/mean_average_precision': 0.16174832188936106, 'test/num_examples': 43793, 'score': 1213.5263085365295, 'total_duration': 1830.4661691188812, 'accumulated_submission_time': 1213.5263085365295, 'accumulated_eval_time': 616.377361536026, 'accumulated_logging_time': 0.4610466957092285, 'global_step': 3734, 'preemption_count': 0}), (4485, {'train/accuracy': 0.9885805249214172, 'train/loss': 0.03955966234207153, 'train/mean_average_precision': 0.20502891507200674, 'validation/accuracy': 0.9856662154197693, 'validation/loss': 0.049088116735219955, 'validation/mean_average_precision': 0.18354689952738906, 'validation/num_examples': 43793, 'test/accuracy': 0.9847337603569031, 'test/loss': 0.052051421254873276, 'test/mean_average_precision': 0.18946579973570027, 'test/num_examples': 43793, 'score': 1453.7445459365845, 'total_duration': 2177.53892993927, 'accumulated_submission_time': 1453.7445459365845, 'accumulated_eval_time': 723.1845881938934, 'accumulated_logging_time': 0.48903846740722656, 'global_step': 4485, 'preemption_count': 0}), (5237, {'train/accuracy': 0.9888032674789429, 'train/loss': 0.03795219212770462, 'train/mean_average_precision': 0.23725617233091223, 'validation/accuracy': 0.985903263092041, 'validation/loss': 0.04772370681166649, 'validation/mean_average_precision': 0.20035285455957202, 'validation/num_examples': 43793, 'test/accuracy': 0.9849599599838257, 'test/loss': 0.05057065561413765, 'test/mean_average_precision': 0.20146415901992373, 'test/num_examples': 43793, 'score': 1693.9610152244568, 'total_duration': 2518.8264665603638, 'accumulated_submission_time': 1693.9610152244568, 'accumulated_eval_time': 824.2064425945282, 'accumulated_logging_time': 0.5192301273345947, 'global_step': 5237, 'preemption_count': 0}), (5990, {'train/accuracy': 0.9887657761573792, 'train/loss': 0.03785643354058266, 'train/mean_average_precision': 0.24467373526100106, 'validation/accuracy': 0.9857993125915527, 'validation/loss': 0.04756652191281319, 'validation/mean_average_precision': 0.2110000050222093, 'validation/num_examples': 43793, 'test/accuracy': 0.984913170337677, 'test/loss': 0.05036148801445961, 'test/mean_average_precision': 0.2117786355546247, 'test/num_examples': 43793, 'score': 1934.0389623641968, 'total_duration': 2861.313567876816, 'accumulated_submission_time': 1934.0389623641968, 'accumulated_eval_time': 926.5682003498077, 'accumulated_logging_time': 0.5474085807800293, 'global_step': 5990, 'preemption_count': 0}), (6739, {'train/accuracy': 0.9891668558120728, 'train/loss': 0.03648771345615387, 'train/mean_average_precision': 0.27751284413558747, 'validation/accuracy': 0.986108660697937, 'validation/loss': 0.0465688593685627, 'validation/mean_average_precision': 0.21951441803696914, 'validation/num_examples': 43793, 'test/accuracy': 0.9852050542831421, 'test/loss': 0.049427032470703125, 'test/mean_average_precision': 0.2250390013924116, 'test/num_examples': 43793, 'score': 2174.274698495865, 'total_duration': 3202.564444541931, 'accumulated_submission_time': 2174.274698495865, 'accumulated_eval_time': 1027.5352380275726, 'accumulated_logging_time': 0.5763721466064453, 'global_step': 6739, 'preemption_count': 0}), (7485, {'train/accuracy': 0.9896649718284607, 'train/loss': 0.034851428121328354, 'train/mean_average_precision': 0.3058615825134636, 'validation/accuracy': 0.9863104224205017, 'validation/loss': 0.045855745673179626, 'validation/mean_average_precision': 0.22820115659870607, 'validation/num_examples': 43793, 'test/accuracy': 0.9853411316871643, 'test/loss': 0.0486358180642128, 'test/mean_average_precision': 0.23021534201673358, 'test/num_examples': 43793, 'score': 2414.481337785721, 'total_duration': 3546.296167373657, 'accumulated_submission_time': 2414.481337785721, 'accumulated_eval_time': 1131.0144610404968, 'accumulated_logging_time': 0.6038436889648438, 'global_step': 7485, 'preemption_count': 0}), (8230, {'train/accuracy': 0.9897923469543457, 'train/loss': 0.03433658555150032, 'train/mean_average_precision': 0.3150043600492316, 'validation/accuracy': 0.98642897605896, 'validation/loss': 0.0455535389482975, 'validation/mean_average_precision': 0.23367242995826423, 'validation/num_examples': 43793, 'test/accuracy': 0.9855673313140869, 'test/loss': 0.04825417324900627, 'test/mean_average_precision': 0.23695948493119998, 'test/num_examples': 43793, 'score': 2654.502183198929, 'total_duration': 3886.628069639206, 'accumulated_submission_time': 2654.502183198929, 'accumulated_eval_time': 1231.274868965149, 'accumulated_logging_time': 0.635286808013916, 'global_step': 8230, 'preemption_count': 0}), (8977, {'train/accuracy': 0.9900234341621399, 'train/loss': 0.03339506685733795, 'train/mean_average_precision': 0.3414138007447258, 'validation/accuracy': 0.9864451885223389, 'validation/loss': 0.04538137838244438, 'validation/mean_average_precision': 0.23793114613338667, 'validation/num_examples': 43793, 'test/accuracy': 0.9856507182121277, 'test/loss': 0.04776066541671753, 'test/mean_average_precision': 0.24603677236961033, 'test/num_examples': 43793, 'score': 2894.498073577881, 'total_duration': 4229.2618017196655, 'accumulated_submission_time': 2894.498073577881, 'accumulated_eval_time': 1333.866200208664, 'accumulated_logging_time': 0.6630842685699463, 'global_step': 8977, 'preemption_count': 0}), (9726, {'train/accuracy': 0.9900118708610535, 'train/loss': 0.03329336270689964, 'train/mean_average_precision': 0.3366389186653354, 'validation/accuracy': 0.9865231513977051, 'validation/loss': 0.04517379775643349, 'validation/mean_average_precision': 0.24064049963896625, 'validation/num_examples': 43793, 'test/accuracy': 0.9857496619224548, 'test/loss': 0.04780091345310211, 'test/mean_average_precision': 0.24382410873859825, 'test/num_examples': 43793, 'score': 3134.5203773975372, 'total_duration': 4574.052921056747, 'accumulated_submission_time': 3134.5203773975372, 'accumulated_eval_time': 1438.5838916301727, 'accumulated_logging_time': 0.6951918601989746, 'global_step': 9726, 'preemption_count': 0}), (10464, {'train/accuracy': 0.9898894429206848, 'train/loss': 0.0337132103741169, 'train/mean_average_precision': 0.32434957025237554, 'validation/accuracy': 0.9865986108779907, 'validation/loss': 0.045117463916540146, 'validation/mean_average_precision': 0.24252620421974527, 'validation/num_examples': 43793, 'test/accuracy': 0.9857842326164246, 'test/loss': 0.04770386964082718, 'test/mean_average_precision': 0.2462669049591603, 'test/num_examples': 43793, 'score': 3374.544422149658, 'total_duration': 4921.391419172287, 'accumulated_submission_time': 3374.544422149658, 'accumulated_eval_time': 1545.8462941646576, 'accumulated_logging_time': 0.7231438159942627, 'global_step': 10464, 'preemption_count': 0}), (11209, {'train/accuracy': 0.9900863766670227, 'train/loss': 0.03292468562722206, 'train/mean_average_precision': 0.3577444428522381, 'validation/accuracy': 0.9866254329681396, 'validation/loss': 0.04499063268303871, 'validation/mean_average_precision': 0.252842845892908, 'validation/num_examples': 43793, 'test/accuracy': 0.9858301281929016, 'test/loss': 0.04760059714317322, 'test/mean_average_precision': 0.25779776291195505, 'test/num_examples': 43793, 'score': 3614.7446529865265, 'total_duration': 5260.084446191788, 'accumulated_submission_time': 3614.7446529865265, 'accumulated_eval_time': 1644.2886338233948, 'accumulated_logging_time': 0.7532014846801758, 'global_step': 11209, 'preemption_count': 0}), (11953, {'train/accuracy': 0.9902267456054688, 'train/loss': 0.032570574432611465, 'train/mean_average_precision': 0.35369296343583034, 'validation/accuracy': 0.9866960644721985, 'validation/loss': 0.04454199597239494, 'validation/mean_average_precision': 0.2551666926403044, 'validation/num_examples': 43793, 'test/accuracy': 0.9858154058456421, 'test/loss': 0.04706943780183792, 'test/mean_average_precision': 0.2561756224992291, 'test/num_examples': 43793, 'score': 3854.700940847397, 'total_duration': 5603.52001285553, 'accumulated_submission_time': 3854.700940847397, 'accumulated_eval_time': 1747.719571352005, 'accumulated_logging_time': 0.7825462818145752, 'global_step': 11953, 'preemption_count': 0}), (12698, {'train/accuracy': 0.9903358221054077, 'train/loss': 0.031861528754234314, 'train/mean_average_precision': 0.3735765411648799, 'validation/accuracy': 0.9865893125534058, 'validation/loss': 0.044699788093566895, 'validation/mean_average_precision': 0.2553556998427123, 'validation/num_examples': 43793, 'test/accuracy': 0.9857736825942993, 'test/loss': 0.04746128246188164, 'test/mean_average_precision': 0.25380504066264953, 'test/num_examples': 43793, 'score': 4094.7709546089172, 'total_duration': 5949.243740320206, 'accumulated_submission_time': 4094.7709546089172, 'accumulated_eval_time': 1853.3250706195831, 'accumulated_logging_time': 0.812180757522583, 'global_step': 12698, 'preemption_count': 0}), (13452, {'train/accuracy': 0.9905796051025391, 'train/loss': 0.031181801110506058, 'train/mean_average_precision': 0.3965822545995702, 'validation/accuracy': 0.9866871237754822, 'validation/loss': 0.044055573642253876, 'validation/mean_average_precision': 0.265454619956688, 'validation/num_examples': 43793, 'test/accuracy': 0.9858141541481018, 'test/loss': 0.04674358665943146, 'test/mean_average_precision': 0.2679424626869677, 'test/num_examples': 43793, 'score': 4334.911934137344, 'total_duration': 6289.6809866428375, 'accumulated_submission_time': 4334.911934137344, 'accumulated_eval_time': 1953.5731303691864, 'accumulated_logging_time': 0.8416659832000732, 'global_step': 13452, 'preemption_count': 0}), (14202, {'train/accuracy': 0.9905725121498108, 'train/loss': 0.03087206371128559, 'train/mean_average_precision': 0.3942485855207641, 'validation/accuracy': 0.9867480397224426, 'validation/loss': 0.04451712965965271, 'validation/mean_average_precision': 0.2602028697357067, 'validation/num_examples': 43793, 'test/accuracy': 0.9858920574188232, 'test/loss': 0.04740261659026146, 'test/mean_average_precision': 0.2573156301471659, 'test/num_examples': 43793, 'score': 4575.066102266312, 'total_duration': 6634.885343790054, 'accumulated_submission_time': 4575.066102266312, 'accumulated_eval_time': 2058.5749821662903, 'accumulated_logging_time': 0.8709104061126709, 'global_step': 14202, 'preemption_count': 0}), (14946, {'train/accuracy': 0.9908130168914795, 'train/loss': 0.03027959354221821, 'train/mean_average_precision': 0.4061079881876005, 'validation/accuracy': 0.986812949180603, 'validation/loss': 0.044084303081035614, 'validation/mean_average_precision': 0.2687047924207835, 'validation/num_examples': 43793, 'test/accuracy': 0.9859813451766968, 'test/loss': 0.04696929082274437, 'test/mean_average_precision': 0.2671970450686207, 'test/num_examples': 43793, 'score': 4815.111397981644, 'total_duration': 6977.977539300919, 'accumulated_submission_time': 4815.111397981644, 'accumulated_eval_time': 2161.5691606998444, 'accumulated_logging_time': 0.9022881984710693, 'global_step': 14946, 'preemption_count': 0}), (15696, {'train/accuracy': 0.9909314513206482, 'train/loss': 0.029750367626547813, 'train/mean_average_precision': 0.4212134215553861, 'validation/accuracy': 0.9868617057800293, 'validation/loss': 0.04417761042714119, 'validation/mean_average_precision': 0.2672141950776982, 'validation/num_examples': 43793, 'test/accuracy': 0.9860504269599915, 'test/loss': 0.047021593898534775, 'test/mean_average_precision': 0.2661189499958922, 'test/num_examples': 43793, 'score': 5055.322886943817, 'total_duration': 7323.467862606049, 'accumulated_submission_time': 5055.322886943817, 'accumulated_eval_time': 2266.796665430069, 'accumulated_logging_time': 0.9336240291595459, 'global_step': 15696, 'preemption_count': 0}), (16444, {'train/accuracy': 0.9908322095870972, 'train/loss': 0.030448731034994125, 'train/mean_average_precision': 0.40309762966671503, 'validation/accuracy': 0.9867277145385742, 'validation/loss': 0.044124990701675415, 'validation/mean_average_precision': 0.26478223635318743, 'validation/num_examples': 43793, 'test/accuracy': 0.9859354496002197, 'test/loss': 0.04659931734204292, 'test/mean_average_precision': 0.2583384105555724, 'test/num_examples': 43793, 'score': 5295.4963212013245, 'total_duration': 7663.6381068229675, 'accumulated_submission_time': 5295.4963212013245, 'accumulated_eval_time': 2366.7448992729187, 'accumulated_logging_time': 0.9629554748535156, 'global_step': 16444, 'preemption_count': 0}), (17196, {'train/accuracy': 0.9909189939498901, 'train/loss': 0.03002912551164627, 'train/mean_average_precision': 0.41130995483938027, 'validation/accuracy': 0.9866400361061096, 'validation/loss': 0.04415911063551903, 'validation/mean_average_precision': 0.2626244980078488, 'validation/num_examples': 43793, 'test/accuracy': 0.9858874082565308, 'test/loss': 0.04647881165146828, 'test/mean_average_precision': 0.2605033970119208, 'test/num_examples': 43793, 'score': 5535.677088022232, 'total_duration': 8003.883762598038, 'accumulated_submission_time': 5535.677088022232, 'accumulated_eval_time': 2466.7602257728577, 'accumulated_logging_time': 0.9934120178222656, 'global_step': 17196, 'preemption_count': 0}), (17942, {'train/accuracy': 0.9906976222991943, 'train/loss': 0.030496561899781227, 'train/mean_average_precision': 0.3988523759511924, 'validation/accuracy': 0.9869071245193481, 'validation/loss': 0.0442199669778347, 'validation/mean_average_precision': 0.2648747445150785, 'validation/num_examples': 43793, 'test/accuracy': 0.9860824346542358, 'test/loss': 0.046835239976644516, 'test/mean_average_precision': 0.26844017790901564, 'test/num_examples': 43793, 'score': 5775.816581726074, 'total_duration': 8346.484165668488, 'accumulated_submission_time': 5775.816581726074, 'accumulated_eval_time': 2569.17115855217, 'accumulated_logging_time': 1.0242016315460205, 'global_step': 17942, 'preemption_count': 0}), (18695, {'train/accuracy': 0.9908912777900696, 'train/loss': 0.02989709936082363, 'train/mean_average_precision': 0.4184613505153205, 'validation/accuracy': 0.9868507385253906, 'validation/loss': 0.04404584690928459, 'validation/mean_average_precision': 0.2643315627503256, 'validation/num_examples': 43793, 'test/accuracy': 0.9860491752624512, 'test/loss': 0.04678769409656525, 'test/mean_average_precision': 0.27347083800907224, 'test/num_examples': 43793, 'score': 6015.941589832306, 'total_duration': 8685.937515974045, 'accumulated_submission_time': 6015.941589832306, 'accumulated_eval_time': 2668.448446750641, 'accumulated_logging_time': 1.0554554462432861, 'global_step': 18695, 'preemption_count': 0}), (19445, {'train/accuracy': 0.9909371137619019, 'train/loss': 0.02975236065685749, 'train/mean_average_precision': 0.4243669747970852, 'validation/accuracy': 0.9867671132087708, 'validation/loss': 0.04395902156829834, 'validation/mean_average_precision': 0.267616429782502, 'validation/num_examples': 43793, 'test/accuracy': 0.9860230684280396, 'test/loss': 0.04655415937304497, 'test/mean_average_precision': 0.2650854230397668, 'test/num_examples': 43793, 'score': 6256.012636184692, 'total_duration': 9028.61729645729, 'accumulated_submission_time': 6256.012636184692, 'accumulated_eval_time': 2771.006584405899, 'accumulated_logging_time': 1.0866594314575195, 'global_step': 19445, 'preemption_count': 0}), (20202, {'train/accuracy': 0.9910696148872375, 'train/loss': 0.029148319736123085, 'train/mean_average_precision': 0.4278207878860953, 'validation/accuracy': 0.9869027137756348, 'validation/loss': 0.04428491368889809, 'validation/mean_average_precision': 0.2742441528896123, 'validation/num_examples': 43793, 'test/accuracy': 0.9861186742782593, 'test/loss': 0.04707004502415657, 'test/mean_average_precision': 0.26815792306761826, 'test/num_examples': 43793, 'score': 6496.045476913452, 'total_duration': 9366.448516368866, 'accumulated_submission_time': 6496.045476913452, 'accumulated_eval_time': 2868.755485534668, 'accumulated_logging_time': 1.1163904666900635, 'global_step': 20202, 'preemption_count': 0}), (20957, {'train/accuracy': 0.9910153150558472, 'train/loss': 0.029240183532238007, 'train/mean_average_precision': 0.4452358825783558, 'validation/accuracy': 0.9868357181549072, 'validation/loss': 0.044326040893793106, 'validation/mean_average_precision': 0.2762801502975392, 'validation/num_examples': 43793, 'test/accuracy': 0.9860588312149048, 'test/loss': 0.04683621600270271, 'test/mean_average_precision': 0.27502654054477293, 'test/num_examples': 43793, 'score': 6736.057872056961, 'total_duration': 9704.049597024918, 'accumulated_submission_time': 6736.057872056961, 'accumulated_eval_time': 2966.293663740158, 'accumulated_logging_time': 1.1478157043457031, 'global_step': 20957, 'preemption_count': 0}), (21712, {'train/accuracy': 0.9912437200546265, 'train/loss': 0.02839926816523075, 'train/mean_average_precision': 0.4604391958469054, 'validation/accuracy': 0.9868929386138916, 'validation/loss': 0.04405832663178444, 'validation/mean_average_precision': 0.2723770620871654, 'validation/num_examples': 43793, 'test/accuracy': 0.9860668182373047, 'test/loss': 0.04659341275691986, 'test/mean_average_precision': 0.273082646921358, 'test/num_examples': 43793, 'score': 6976.239338636398, 'total_duration': 10049.307490348816, 'accumulated_submission_time': 6976.239338636398, 'accumulated_eval_time': 3071.3200681209564, 'accumulated_logging_time': 1.1784136295318604, 'global_step': 21712, 'preemption_count': 0}), (22467, {'train/accuracy': 0.991296112537384, 'train/loss': 0.028296872973442078, 'train/mean_average_precision': 0.44887732756269577, 'validation/accuracy': 0.9868758916854858, 'validation/loss': 0.04404003173112869, 'validation/mean_average_precision': 0.2674766798136144, 'validation/num_examples': 43793, 'test/accuracy': 0.98606938123703, 'test/loss': 0.046771008521318436, 'test/mean_average_precision': 0.2726564770947386, 'test/num_examples': 43793, 'score': 7216.246557235718, 'total_duration': 10390.653838157654, 'accumulated_submission_time': 7216.246557235718, 'accumulated_eval_time': 3172.609664440155, 'accumulated_logging_time': 1.208643913269043, 'global_step': 22467, 'preemption_count': 0}), (23222, {'train/accuracy': 0.9915757775306702, 'train/loss': 0.027429115027189255, 'train/mean_average_precision': 0.46926767989383444, 'validation/accuracy': 0.9869286417961121, 'validation/loss': 0.043794091790914536, 'validation/mean_average_precision': 0.2775344659929296, 'validation/num_examples': 43793, 'test/accuracy': 0.9861102104187012, 'test/loss': 0.04683634638786316, 'test/mean_average_precision': 0.2697631826066058, 'test/num_examples': 43793, 'score': 7456.395405292511, 'total_duration': 10734.925931692123, 'accumulated_submission_time': 7456.395405292511, 'accumulated_eval_time': 3276.680414915085, 'accumulated_logging_time': 1.241858959197998, 'global_step': 23222, 'preemption_count': 0}), (23968, {'train/accuracy': 0.991378128528595, 'train/loss': 0.028135981410741806, 'train/mean_average_precision': 0.46534005699604974, 'validation/accuracy': 0.9869207739830017, 'validation/loss': 0.04384111613035202, 'validation/mean_average_precision': 0.27828051068360626, 'validation/num_examples': 43793, 'test/accuracy': 0.986065149307251, 'test/loss': 0.046624280512332916, 'test/mean_average_precision': 0.27555730666603345, 'test/num_examples': 43793, 'score': 7696.503025054932, 'total_duration': 11075.413035392761, 'accumulated_submission_time': 7696.503025054932, 'accumulated_eval_time': 3377.009297847748, 'accumulated_logging_time': 1.2736289501190186, 'global_step': 23968, 'preemption_count': 0}), (24710, {'train/accuracy': 0.9912890195846558, 'train/loss': 0.02855648472905159, 'train/mean_average_precision': 0.446825708880746, 'validation/accuracy': 0.9868153929710388, 'validation/loss': 0.04399457201361656, 'validation/mean_average_precision': 0.2730327227108191, 'validation/num_examples': 43793, 'test/accuracy': 0.9860036373138428, 'test/loss': 0.046485982835292816, 'test/mean_average_precision': 0.2684493736634549, 'test/num_examples': 43793, 'score': 7936.5431044101715, 'total_duration': 11418.130235671997, 'accumulated_submission_time': 7936.5431044101715, 'accumulated_eval_time': 3479.635217189789, 'accumulated_logging_time': 1.3058831691741943, 'global_step': 24710, 'preemption_count': 0}), (25457, {'train/accuracy': 0.9913530349731445, 'train/loss': 0.028152110055088997, 'train/mean_average_precision': 0.46453359169836816, 'validation/accuracy': 0.9869887232780457, 'validation/loss': 0.04381720349192619, 'validation/mean_average_precision': 0.27789378919818764, 'validation/num_examples': 43793, 'test/accuracy': 0.9860908389091492, 'test/loss': 0.04651831090450287, 'test/mean_average_precision': 0.27900299528478145, 'test/num_examples': 43793, 'score': 8176.7564833164215, 'total_duration': 11762.777160644531, 'accumulated_submission_time': 8176.7564833164215, 'accumulated_eval_time': 3584.0178577899933, 'accumulated_logging_time': 1.3375873565673828, 'global_step': 25457, 'preemption_count': 0}), (26216, {'train/accuracy': 0.9911909699440002, 'train/loss': 0.02844357304275036, 'train/mean_average_precision': 0.44178144617310267, 'validation/accuracy': 0.98688805103302, 'validation/loss': 0.044134438037872314, 'validation/mean_average_precision': 0.2709613871925868, 'validation/num_examples': 43793, 'test/accuracy': 0.9861236810684204, 'test/loss': 0.04700309410691261, 'test/mean_average_precision': 0.26599435220298556, 'test/num_examples': 43793, 'score': 8416.975894927979, 'total_duration': 12101.558113098145, 'accumulated_submission_time': 8416.975894927979, 'accumulated_eval_time': 3682.5280437469482, 'accumulated_logging_time': 1.3701817989349365, 'global_step': 26216, 'preemption_count': 0}), (26968, {'train/accuracy': 0.9912831783294678, 'train/loss': 0.02814350835978985, 'train/mean_average_precision': 0.46464755492582693, 'validation/accuracy': 0.9869964718818665, 'validation/loss': 0.04417134448885918, 'validation/mean_average_precision': 0.2806928255369479, 'validation/num_examples': 43793, 'test/accuracy': 0.986162006855011, 'test/loss': 0.04697659984230995, 'test/mean_average_precision': 0.27542822452749105, 'test/num_examples': 43793, 'score': 8657.060805559158, 'total_duration': 12442.852400064468, 'accumulated_submission_time': 8657.060805559158, 'accumulated_eval_time': 3783.687194108963, 'accumulated_logging_time': 1.4013991355895996, 'global_step': 26968, 'preemption_count': 0}), (27724, {'train/accuracy': 0.9916079044342041, 'train/loss': 0.027133068069815636, 'train/mean_average_precision': 0.4827749643581413, 'validation/accuracy': 0.986968457698822, 'validation/loss': 0.0441259928047657, 'validation/mean_average_precision': 0.27225728252014003, 'validation/num_examples': 43793, 'test/accuracy': 0.9861776232719421, 'test/loss': 0.04679429903626442, 'test/mean_average_precision': 0.27168875551218347, 'test/num_examples': 43793, 'score': 8897.122105360031, 'total_duration': 12785.942913293839, 'accumulated_submission_time': 8897.122105360031, 'accumulated_eval_time': 3886.666358947754, 'accumulated_logging_time': 1.4324800968170166, 'global_step': 27724, 'preemption_count': 0}), (28474, {'train/accuracy': 0.9916527271270752, 'train/loss': 0.027179455384612083, 'train/mean_average_precision': 0.48076338524130036, 'validation/accuracy': 0.9868791699409485, 'validation/loss': 0.044070225208997726, 'validation/mean_average_precision': 0.27541784470378794, 'validation/num_examples': 43793, 'test/accuracy': 0.986123263835907, 'test/loss': 0.046606939285993576, 'test/mean_average_precision': 0.2744048140787645, 'test/num_examples': 43793, 'score': 9137.191581249237, 'total_duration': 13131.377036809921, 'accumulated_submission_time': 9137.191581249237, 'accumulated_eval_time': 3991.973956346512, 'accumulated_logging_time': 1.4689247608184814, 'global_step': 28474, 'preemption_count': 0}), (29223, {'train/accuracy': 0.9917328357696533, 'train/loss': 0.026465030387043953, 'train/mean_average_precision': 0.5178903073222842, 'validation/accuracy': 0.9871068596839905, 'validation/loss': 0.04411550983786583, 'validation/mean_average_precision': 0.2825732089125649, 'validation/num_examples': 43793, 'test/accuracy': 0.986240804195404, 'test/loss': 0.04688546061515808, 'test/mean_average_precision': 0.27499786698549594, 'test/num_examples': 43793, 'score': 9377.420338869095, 'total_duration': 13473.64820098877, 'accumulated_submission_time': 9377.420338869095, 'accumulated_eval_time': 4093.9644737243652, 'accumulated_logging_time': 1.5014734268188477, 'global_step': 29223, 'preemption_count': 0}), (29972, {'train/accuracy': 0.9918742179870605, 'train/loss': 0.026252735406160355, 'train/mean_average_precision': 0.5005576253532122, 'validation/accuracy': 0.9870269298553467, 'validation/loss': 0.04420791566371918, 'validation/mean_average_precision': 0.28201485491354894, 'validation/num_examples': 43793, 'test/accuracy': 0.9862378239631653, 'test/loss': 0.046826593577861786, 'test/mean_average_precision': 0.28050102651211495, 'test/num_examples': 43793, 'score': 9617.694372415543, 'total_duration': 13817.03848195076, 'accumulated_submission_time': 9617.694372415543, 'accumulated_eval_time': 4197.028309106827, 'accumulated_logging_time': 1.5345180034637451, 'global_step': 29972, 'preemption_count': 0}), (30714, {'train/accuracy': 0.9917741417884827, 'train/loss': 0.026466084644198418, 'train/mean_average_precision': 0.49740525993522233, 'validation/accuracy': 0.9868994355201721, 'validation/loss': 0.04427551105618477, 'validation/mean_average_precision': 0.2783416322284315, 'validation/num_examples': 43793, 'test/accuracy': 0.9860790371894836, 'test/loss': 0.046951211988925934, 'test/mean_average_precision': 0.2753131299952883, 'test/num_examples': 43793, 'score': 9857.652417898178, 'total_duration': 14163.926680326462, 'accumulated_submission_time': 9857.652417898178, 'accumulated_eval_time': 4303.904276847839, 'accumulated_logging_time': 1.5686705112457275, 'global_step': 30714, 'preemption_count': 0}), (31471, {'train/accuracy': 0.9916498064994812, 'train/loss': 0.026945652440190315, 'train/mean_average_precision': 0.48888145743807776, 'validation/accuracy': 0.9870484471321106, 'validation/loss': 0.04448457062244415, 'validation/mean_average_precision': 0.2807809220691736, 'validation/num_examples': 43793, 'test/accuracy': 0.9862290024757385, 'test/loss': 0.047175098210573196, 'test/mean_average_precision': 0.2739805160313872, 'test/num_examples': 43793, 'score': 10097.801033735275, 'total_duration': 14504.410804271698, 'accumulated_submission_time': 10097.801033735275, 'accumulated_eval_time': 4404.187355518341, 'accumulated_logging_time': 1.6013495922088623, 'global_step': 31471, 'preemption_count': 0}), (32224, {'train/accuracy': 0.9916934370994568, 'train/loss': 0.026851730421185493, 'train/mean_average_precision': 0.4979061005428039, 'validation/accuracy': 0.9870707392692566, 'validation/loss': 0.0440482571721077, 'validation/mean_average_precision': 0.2837913263385009, 'validation/num_examples': 43793, 'test/accuracy': 0.9862456321716309, 'test/loss': 0.046788159757852554, 'test/mean_average_precision': 0.2771946456173613, 'test/num_examples': 43793, 'score': 10337.873920917511, 'total_duration': 14846.432065725327, 'accumulated_submission_time': 10337.873920917511, 'accumulated_eval_time': 4506.080503463745, 'accumulated_logging_time': 1.637082815170288, 'global_step': 32224, 'preemption_count': 0}), (32971, {'train/accuracy': 0.9917108416557312, 'train/loss': 0.026847785338759422, 'train/mean_average_precision': 0.4847077089609162, 'validation/accuracy': 0.9870195984840393, 'validation/loss': 0.04391064494848251, 'validation/mean_average_precision': 0.28105090039840597, 'validation/num_examples': 43793, 'test/accuracy': 0.9862130284309387, 'test/loss': 0.046704407781362534, 'test/mean_average_precision': 0.2748783828748157, 'test/num_examples': 43793, 'score': 10577.960082054138, 'total_duration': 15188.354211330414, 'accumulated_submission_time': 10577.960082054138, 'accumulated_eval_time': 4607.861759901047, 'accumulated_logging_time': 1.6719791889190674, 'global_step': 32971, 'preemption_count': 0}), (33723, {'train/accuracy': 0.9917195439338684, 'train/loss': 0.026451144367456436, 'train/mean_average_precision': 0.5071388805226102, 'validation/accuracy': 0.9871170520782471, 'validation/loss': 0.04401608929038048, 'validation/mean_average_precision': 0.2869082133800527, 'validation/num_examples': 43793, 'test/accuracy': 0.986255943775177, 'test/loss': 0.046712297946214676, 'test/mean_average_precision': 0.2788147279553553, 'test/num_examples': 43793, 'score': 10818.10796546936, 'total_duration': 15529.334302663803, 'accumulated_submission_time': 10818.10796546936, 'accumulated_eval_time': 4708.640298604965, 'accumulated_logging_time': 1.7060413360595703, 'global_step': 33723, 'preemption_count': 0}), (34468, {'train/accuracy': 0.9917593002319336, 'train/loss': 0.026479462161660194, 'train/mean_average_precision': 0.4987217774287702, 'validation/accuracy': 0.9870354533195496, 'validation/loss': 0.044570889323949814, 'validation/mean_average_precision': 0.2862611577565212, 'validation/num_examples': 43793, 'test/accuracy': 0.9862926006317139, 'test/loss': 0.04743202030658722, 'test/mean_average_precision': 0.27369947145660745, 'test/num_examples': 43793, 'score': 11058.298412799835, 'total_duration': 15876.846804141998, 'accumulated_submission_time': 11058.298412799835, 'accumulated_eval_time': 4815.907956838608, 'accumulated_logging_time': 1.7401671409606934, 'global_step': 34468, 'preemption_count': 0}), (35216, {'train/accuracy': 0.9919956922531128, 'train/loss': 0.025788769125938416, 'train/mean_average_precision': 0.514730269745362, 'validation/accuracy': 0.9871243238449097, 'validation/loss': 0.0442296601831913, 'validation/mean_average_precision': 0.28767634130287056, 'validation/num_examples': 43793, 'test/accuracy': 0.9862761497497559, 'test/loss': 0.047140274196863174, 'test/mean_average_precision': 0.27782360070679796, 'test/num_examples': 43793, 'score': 11298.292127609253, 'total_duration': 16218.54702448845, 'accumulated_submission_time': 11298.292127609253, 'accumulated_eval_time': 4917.55984044075, 'accumulated_logging_time': 1.7752108573913574, 'global_step': 35216, 'preemption_count': 0}), (35967, {'train/accuracy': 0.9921483397483826, 'train/loss': 0.025018077343702316, 'train/mean_average_precision': 0.5371498315246481, 'validation/accuracy': 0.9870285391807556, 'validation/loss': 0.044526178389787674, 'validation/mean_average_precision': 0.2823993135022633, 'validation/num_examples': 43793, 'test/accuracy': 0.9861658215522766, 'test/loss': 0.04742975905537605, 'test/mean_average_precision': 0.2739156328156931, 'test/num_examples': 43793, 'score': 11538.531027317047, 'total_duration': 16561.054585933685, 'accumulated_submission_time': 11538.531027317047, 'accumulated_eval_time': 5019.775372505188, 'accumulated_logging_time': 1.809107780456543, 'global_step': 35967, 'preemption_count': 0}), (36721, {'train/accuracy': 0.9923694133758545, 'train/loss': 0.024611108005046844, 'train/mean_average_precision': 0.5423980058733963, 'validation/accuracy': 0.9868807792663574, 'validation/loss': 0.044240955263376236, 'validation/mean_average_precision': 0.2812444716684876, 'validation/num_examples': 43793, 'test/accuracy': 0.9860352277755737, 'test/loss': 0.046927087008953094, 'test/mean_average_precision': 0.27760796493702045, 'test/num_examples': 43793, 'score': 11778.565999269485, 'total_duration': 16899.971519231796, 'accumulated_submission_time': 11778.565999269485, 'accumulated_eval_time': 5118.604428529739, 'accumulated_logging_time': 1.8418443202972412, 'global_step': 36721, 'preemption_count': 0}), (37462, {'train/accuracy': 0.9923901557922363, 'train/loss': 0.02431369200348854, 'train/mean_average_precision': 0.5580372747770705, 'validation/accuracy': 0.9870139360427856, 'validation/loss': 0.044508807361125946, 'validation/mean_average_precision': 0.2837891887851236, 'validation/num_examples': 43793, 'test/accuracy': 0.9861957430839539, 'test/loss': 0.04747257009148598, 'test/mean_average_precision': 0.28282577110993073, 'test/num_examples': 43793, 'score': 12018.567860364914, 'total_duration': 17243.798904657364, 'accumulated_submission_time': 12018.567860364914, 'accumulated_eval_time': 5222.37273478508, 'accumulated_logging_time': 1.8780317306518555, 'global_step': 37462, 'preemption_count': 0}), (38214, {'train/accuracy': 0.9922755360603333, 'train/loss': 0.024770546704530716, 'train/mean_average_precision': 0.5347153345263589, 'validation/accuracy': 0.9870476126670837, 'validation/loss': 0.04480938985943794, 'validation/mean_average_precision': 0.28137809293253063, 'validation/num_examples': 43793, 'test/accuracy': 0.9862033128738403, 'test/loss': 0.04759284481406212, 'test/mean_average_precision': 0.2776924273187139, 'test/num_examples': 43793, 'score': 12258.701553821564, 'total_duration': 17585.86657810211, 'accumulated_submission_time': 12258.701553821564, 'accumulated_eval_time': 5324.253618955612, 'accumulated_logging_time': 1.9119784832000732, 'global_step': 38214, 'preemption_count': 0}), (38966, {'train/accuracy': 0.9921236634254456, 'train/loss': 0.02517537586390972, 'train/mean_average_precision': 0.5225601239481771, 'validation/accuracy': 0.9870768189430237, 'validation/loss': 0.04468025639653206, 'validation/mean_average_precision': 0.2864147846605825, 'validation/num_examples': 43793, 'test/accuracy': 0.9862778782844543, 'test/loss': 0.04753851518034935, 'test/mean_average_precision': 0.2776829744498602, 'test/num_examples': 43793, 'score': 12498.43436551094, 'total_duration': 17927.652944803238, 'accumulated_submission_time': 12498.43436551094, 'accumulated_eval_time': 5425.832328796387, 'accumulated_logging_time': 2.3672399520874023, 'global_step': 38966, 'preemption_count': 0}), (39706, {'train/accuracy': 0.9922091364860535, 'train/loss': 0.025052009150385857, 'train/mean_average_precision': 0.534985723044657, 'validation/accuracy': 0.9870837330818176, 'validation/loss': 0.0441632904112339, 'validation/mean_average_precision': 0.2902683111279471, 'validation/num_examples': 43793, 'test/accuracy': 0.9863018989562988, 'test/loss': 0.04700274392962456, 'test/mean_average_precision': 0.2751548149248845, 'test/num_examples': 43793, 'score': 12738.577870845795, 'total_duration': 18272.76641869545, 'accumulated_submission_time': 12738.577870845795, 'accumulated_eval_time': 5530.745029449463, 'accumulated_logging_time': 2.4028241634368896, 'global_step': 39706, 'preemption_count': 0}), (40458, {'train/accuracy': 0.9922121167182922, 'train/loss': 0.024899430572986603, 'train/mean_average_precision': 0.5352071481301257, 'validation/accuracy': 0.9870695471763611, 'validation/loss': 0.04427975043654442, 'validation/mean_average_precision': 0.28663532808791076, 'validation/num_examples': 43793, 'test/accuracy': 0.986185610294342, 'test/loss': 0.04728372022509575, 'test/mean_average_precision': 0.27403020649579524, 'test/num_examples': 43793, 'score': 12978.589567184448, 'total_duration': 18611.997394800186, 'accumulated_submission_time': 12978.589567184448, 'accumulated_eval_time': 5629.910222530365, 'accumulated_logging_time': 2.4375898838043213, 'global_step': 40458, 'preemption_count': 0}), (41211, {'train/accuracy': 0.9924872517585754, 'train/loss': 0.024059409275650978, 'train/mean_average_precision': 0.5571624292463508, 'validation/accuracy': 0.9870370626449585, 'validation/loss': 0.044358331710100174, 'validation/mean_average_precision': 0.28416683557907974, 'validation/num_examples': 43793, 'test/accuracy': 0.9861717224121094, 'test/loss': 0.047193001955747604, 'test/mean_average_precision': 0.2720835039963426, 'test/num_examples': 43793, 'score': 13218.623850822449, 'total_duration': 18949.861858844757, 'accumulated_submission_time': 13218.623850822449, 'accumulated_eval_time': 5727.685400247574, 'accumulated_logging_time': 2.4737045764923096, 'global_step': 41211, 'preemption_count': 0}), (41969, {'train/accuracy': 0.9924891591072083, 'train/loss': 0.023978086188435555, 'train/mean_average_precision': 0.5498505304311031, 'validation/accuracy': 0.9871364831924438, 'validation/loss': 0.04452793300151825, 'validation/mean_average_precision': 0.292061123135377, 'validation/num_examples': 43793, 'test/accuracy': 0.9861666560173035, 'test/loss': 0.04746491089463234, 'test/mean_average_precision': 0.2750019408692665, 'test/num_examples': 43793, 'score': 13458.846201658249, 'total_duration': 19290.2509264946, 'accumulated_submission_time': 13458.846201658249, 'accumulated_eval_time': 5827.797815561295, 'accumulated_logging_time': 2.508739709854126, 'global_step': 41969, 'preemption_count': 0}), (42708, {'train/accuracy': 0.9924418926239014, 'train/loss': 0.02395336702466011, 'train/mean_average_precision': 0.5589275876659786, 'validation/accuracy': 0.9870520830154419, 'validation/loss': 0.04472799226641655, 'validation/mean_average_precision': 0.28691590283802076, 'validation/num_examples': 43793, 'test/accuracy': 0.9862711429595947, 'test/loss': 0.04723460599780083, 'test/mean_average_precision': 0.27847302337643665, 'test/num_examples': 43793, 'score': 13698.999931812286, 'total_duration': 19631.180638074875, 'accumulated_submission_time': 13698.999931812286, 'accumulated_eval_time': 5928.517250061035, 'accumulated_logging_time': 2.544694185256958, 'global_step': 42708, 'preemption_count': 0}), (43459, {'train/accuracy': 0.9928225874900818, 'train/loss': 0.022863321006298065, 'train/mean_average_precision': 0.5811168779417648, 'validation/accuracy': 0.9870573282241821, 'validation/loss': 0.04499298706650734, 'validation/mean_average_precision': 0.2861434822523052, 'validation/num_examples': 43793, 'test/accuracy': 0.9862121343612671, 'test/loss': 0.04791620746254921, 'test/mean_average_precision': 0.27526388680439795, 'test/num_examples': 43793, 'score': 13939.127762079239, 'total_duration': 19972.82164144516, 'accumulated_submission_time': 13939.127762079239, 'accumulated_eval_time': 6029.9765565395355, 'accumulated_logging_time': 2.5795791149139404, 'global_step': 43459, 'preemption_count': 0}), (44206, {'train/accuracy': 0.9930243492126465, 'train/loss': 0.022302931174635887, 'train/mean_average_precision': 0.6021831467185383, 'validation/accuracy': 0.9870301485061646, 'validation/loss': 0.04487265273928642, 'validation/mean_average_precision': 0.2870586605260568, 'validation/num_examples': 43793, 'test/accuracy': 0.9861574172973633, 'test/loss': 0.04790334776043892, 'test/mean_average_precision': 0.27577883781596174, 'test/num_examples': 43793, 'score': 14179.133565664291, 'total_duration': 20310.78754711151, 'accumulated_submission_time': 14179.133565664291, 'accumulated_eval_time': 6127.8829135894775, 'accumulated_logging_time': 2.6141395568847656, 'global_step': 44206, 'preemption_count': 0}), (44963, {'train/accuracy': 0.9930409789085388, 'train/loss': 0.022134987637400627, 'train/mean_average_precision': 0.602127596669349, 'validation/accuracy': 0.9871600270271301, 'validation/loss': 0.044910237193107605, 'validation/mean_average_precision': 0.2858967786479935, 'validation/num_examples': 43793, 'test/accuracy': 0.9862648248672485, 'test/loss': 0.04774727299809456, 'test/mean_average_precision': 0.2821340983594425, 'test/num_examples': 43793, 'score': 14419.119592666626, 'total_duration': 20651.140026569366, 'accumulated_submission_time': 14419.119592666626, 'accumulated_eval_time': 6228.195499420166, 'accumulated_logging_time': 2.649040937423706, 'global_step': 44963, 'preemption_count': 0}), (45727, {'train/accuracy': 0.9928721785545349, 'train/loss': 0.02260545827448368, 'train/mean_average_precision': 0.583901784898843, 'validation/accuracy': 0.9871146082878113, 'validation/loss': 0.04508694261312485, 'validation/mean_average_precision': 0.2856805049431565, 'validation/num_examples': 43793, 'test/accuracy': 0.9863384962081909, 'test/loss': 0.04790147393941879, 'test/mean_average_precision': 0.2862644840320328, 'test/num_examples': 43793, 'score': 14659.334668159485, 'total_duration': 20990.620449781418, 'accumulated_submission_time': 14659.334668159485, 'accumulated_eval_time': 6327.404683113098, 'accumulated_logging_time': 2.686012029647827, 'global_step': 45727, 'preemption_count': 0}), (46489, {'train/accuracy': 0.9927763342857361, 'train/loss': 0.022976737469434738, 'train/mean_average_precision': 0.5796602909704152, 'validation/accuracy': 0.9870861768722534, 'validation/loss': 0.04499715939164162, 'validation/mean_average_precision': 0.28788446774354765, 'validation/num_examples': 43793, 'test/accuracy': 0.9862883687019348, 'test/loss': 0.04792165756225586, 'test/mean_average_precision': 0.2808426418318341, 'test/num_examples': 43793, 'score': 14899.518951892853, 'total_duration': 21327.210332155228, 'accumulated_submission_time': 14899.518951892853, 'accumulated_eval_time': 6423.754679679871, 'accumulated_logging_time': 2.7223784923553467, 'global_step': 46489, 'preemption_count': 0}), (47245, {'train/accuracy': 0.9926639199256897, 'train/loss': 0.02317655086517334, 'train/mean_average_precision': 0.5689805751294961, 'validation/accuracy': 0.9870666861534119, 'validation/loss': 0.045409902930259705, 'validation/mean_average_precision': 0.2847175279822495, 'validation/num_examples': 43793, 'test/accuracy': 0.9861990809440613, 'test/loss': 0.04832647368311882, 'test/mean_average_precision': 0.2735987649905027, 'test/num_examples': 43793, 'score': 15139.666759252548, 'total_duration': 21672.53662109375, 'accumulated_submission_time': 15139.666759252548, 'accumulated_eval_time': 6528.876852750778, 'accumulated_logging_time': 2.7595677375793457, 'global_step': 47245, 'preemption_count': 0}), (47994, {'train/accuracy': 0.9927076697349548, 'train/loss': 0.022998401895165443, 'train/mean_average_precision': 0.5812239180112349, 'validation/accuracy': 0.9869928359985352, 'validation/loss': 0.04547933116555214, 'validation/mean_average_precision': 0.2820994474233479, 'validation/num_examples': 43793, 'test/accuracy': 0.9861843585968018, 'test/loss': 0.048406634479761124, 'test/mean_average_precision': 0.27019014397531693, 'test/num_examples': 43793, 'score': 15379.932568311691, 'total_duration': 22011.009790182114, 'accumulated_submission_time': 15379.932568311691, 'accumulated_eval_time': 6627.028215408325, 'accumulated_logging_time': 2.79514479637146, 'global_step': 47994, 'preemption_count': 0}), (48748, {'train/accuracy': 0.992895781993866, 'train/loss': 0.02250996232032776, 'train/mean_average_precision': 0.6029228174570651, 'validation/accuracy': 0.9870654940605164, 'validation/loss': 0.04532771185040474, 'validation/mean_average_precision': 0.28648249001307446, 'validation/num_examples': 43793, 'test/accuracy': 0.9862820506095886, 'test/loss': 0.048175618052482605, 'test/mean_average_precision': 0.27638856792281613, 'test/num_examples': 43793, 'score': 15620.129272222519, 'total_duration': 22346.907153129578, 'accumulated_submission_time': 15620.129272222519, 'accumulated_eval_time': 6722.673615455627, 'accumulated_logging_time': 2.8310930728912354, 'global_step': 48748, 'preemption_count': 0}), (49500, {'train/accuracy': 0.9930621981620789, 'train/loss': 0.021937798708677292, 'train/mean_average_precision': 0.6060850216324132, 'validation/accuracy': 0.9870135188102722, 'validation/loss': 0.04548106715083122, 'validation/mean_average_precision': 0.28988763798470013, 'validation/num_examples': 43793, 'test/accuracy': 0.9862605929374695, 'test/loss': 0.048380717635154724, 'test/mean_average_precision': 0.27548869856047037, 'test/num_examples': 43793, 'score': 15860.07743358612, 'total_duration': 22690.710390090942, 'accumulated_submission_time': 15860.07743358612, 'accumulated_eval_time': 6826.473671674728, 'accumulated_logging_time': 2.8667874336242676, 'global_step': 49500, 'preemption_count': 0}), (50255, {'train/accuracy': 0.9931796193122864, 'train/loss': 0.02170548215508461, 'train/mean_average_precision': 0.5990312491287693, 'validation/accuracy': 0.9870184063911438, 'validation/loss': 0.04573266953229904, 'validation/mean_average_precision': 0.2864895013825414, 'validation/num_examples': 43793, 'test/accuracy': 0.9861767888069153, 'test/loss': 0.0484723262488842, 'test/mean_average_precision': 0.27364605936614306, 'test/num_examples': 43793, 'score': 16100.23567533493, 'total_duration': 23027.409290790558, 'accumulated_submission_time': 16100.23567533493, 'accumulated_eval_time': 6922.95885682106, 'accumulated_logging_time': 2.9028890132904053, 'global_step': 50255, 'preemption_count': 0}), (51016, {'train/accuracy': 0.9933199286460876, 'train/loss': 0.021025843918323517, 'train/mean_average_precision': 0.6328068561122213, 'validation/accuracy': 0.9870354533195496, 'validation/loss': 0.04584246128797531, 'validation/mean_average_precision': 0.28707328510616265, 'validation/num_examples': 43793, 'test/accuracy': 0.9862424731254578, 'test/loss': 0.04855342209339142, 'test/mean_average_precision': 0.2766943537149877, 'test/num_examples': 43793, 'score': 16340.418535232544, 'total_duration': 23372.501963615417, 'accumulated_submission_time': 16340.418535232544, 'accumulated_eval_time': 7027.812830686569, 'accumulated_logging_time': 2.9394538402557373, 'global_step': 51016, 'preemption_count': 0}), (51773, {'train/accuracy': 0.993573784828186, 'train/loss': 0.02038927562534809, 'train/mean_average_precision': 0.6390301099909069, 'validation/accuracy': 0.9870334267616272, 'validation/loss': 0.046002548187971115, 'validation/mean_average_precision': 0.28934933586599526, 'validation/num_examples': 43793, 'test/accuracy': 0.9862707257270813, 'test/loss': 0.048848822712898254, 'test/mean_average_precision': 0.2793308413871986, 'test/num_examples': 43793, 'score': 16580.547554254532, 'total_duration': 23714.87309408188, 'accumulated_submission_time': 16580.547554254532, 'accumulated_eval_time': 7129.999175310135, 'accumulated_logging_time': 2.975675106048584, 'global_step': 51773, 'preemption_count': 0}), (52523, {'train/accuracy': 0.9938708543777466, 'train/loss': 0.019789312034845352, 'train/mean_average_precision': 0.6536397233896962, 'validation/accuracy': 0.9869379997253418, 'validation/loss': 0.04590245708823204, 'validation/mean_average_precision': 0.2885353006882328, 'validation/num_examples': 43793, 'test/accuracy': 0.9861144423484802, 'test/loss': 0.04876953735947609, 'test/mean_average_precision': 0.2781312477674411, 'test/num_examples': 43793, 'score': 16820.687520742416, 'total_duration': 24056.085558652878, 'accumulated_submission_time': 16820.687520742416, 'accumulated_eval_time': 7231.016023159027, 'accumulated_logging_time': 3.0121028423309326, 'global_step': 52523, 'preemption_count': 0}), (53269, {'train/accuracy': 0.9936069250106812, 'train/loss': 0.02018664963543415, 'train/mean_average_precision': 0.6379870321649714, 'validation/accuracy': 0.98707115650177, 'validation/loss': 0.046360552310943604, 'validation/mean_average_precision': 0.2920631409937257, 'validation/num_examples': 43793, 'test/accuracy': 0.9862328171730042, 'test/loss': 0.049179110676050186, 'test/mean_average_precision': 0.27610814570712733, 'test/num_examples': 43793, 'score': 17060.65802717209, 'total_duration': 24396.114936590195, 'accumulated_submission_time': 17060.65802717209, 'accumulated_eval_time': 7331.012581825256, 'accumulated_logging_time': 3.0534260272979736, 'global_step': 53269, 'preemption_count': 0}), (54016, {'train/accuracy': 0.9934079051017761, 'train/loss': 0.02068304270505905, 'train/mean_average_precision': 0.6422283249361697, 'validation/accuracy': 0.9870346188545227, 'validation/loss': 0.046364326030015945, 'validation/mean_average_precision': 0.2859237206899969, 'validation/num_examples': 43793, 'test/accuracy': 0.9862012267112732, 'test/loss': 0.049231842160224915, 'test/mean_average_precision': 0.2789847800402106, 'test/num_examples': 43793, 'score': 17300.819898843765, 'total_duration': 24738.922700881958, 'accumulated_submission_time': 17300.819898843765, 'accumulated_eval_time': 7433.599872112274, 'accumulated_logging_time': 3.091221570968628, 'global_step': 54016, 'preemption_count': 0}), (54759, {'train/accuracy': 0.9934549331665039, 'train/loss': 0.020581141114234924, 'train/mean_average_precision': 0.6245154233950967, 'validation/accuracy': 0.9870610237121582, 'validation/loss': 0.04653650149703026, 'validation/mean_average_precision': 0.2884550829211076, 'validation/num_examples': 43793, 'test/accuracy': 0.9863005876541138, 'test/loss': 0.049402348697185516, 'test/mean_average_precision': 0.27736942988361163, 'test/num_examples': 43793, 'score': 17541.016096830368, 'total_duration': 25080.204783678055, 'accumulated_submission_time': 17541.016096830368, 'accumulated_eval_time': 7534.629323959351, 'accumulated_logging_time': 3.1281652450561523, 'global_step': 54759, 'preemption_count': 0}), (55505, {'train/accuracy': 0.9934847950935364, 'train/loss': 0.020396802574396133, 'train/mean_average_precision': 0.6366287142374494, 'validation/accuracy': 0.9871174097061157, 'validation/loss': 0.04654433950781822, 'validation/mean_average_precision': 0.2906898791426333, 'validation/num_examples': 43793, 'test/accuracy': 0.9862551093101501, 'test/loss': 0.0496644601225853, 'test/mean_average_precision': 0.2748268458454385, 'test/num_examples': 43793, 'score': 17781.16787624359, 'total_duration': 25416.691744089127, 'accumulated_submission_time': 17781.16787624359, 'accumulated_eval_time': 7630.906837940216, 'accumulated_logging_time': 3.166072130203247, 'global_step': 55505, 'preemption_count': 0}), (56252, {'train/accuracy': 0.993678867816925, 'train/loss': 0.01993425376713276, 'train/mean_average_precision': 0.6509323873209593, 'validation/accuracy': 0.9869863390922546, 'validation/loss': 0.04683464393019676, 'validation/mean_average_precision': 0.28849475560347887, 'validation/num_examples': 43793, 'test/accuracy': 0.9861961603164673, 'test/loss': 0.04965412616729736, 'test/mean_average_precision': 0.27707614673272585, 'test/num_examples': 43793, 'score': 18021.238358020782, 'total_duration': 25759.857971429825, 'accumulated_submission_time': 18021.238358020782, 'accumulated_eval_time': 7733.94634437561, 'accumulated_logging_time': 3.202991008758545, 'global_step': 56252, 'preemption_count': 0}), (57001, {'train/accuracy': 0.9936870336532593, 'train/loss': 0.01976962573826313, 'train/mean_average_precision': 0.6579927457781373, 'validation/accuracy': 0.9869254231452942, 'validation/loss': 0.0470384806394577, 'validation/mean_average_precision': 0.2847933420483199, 'validation/num_examples': 43793, 'test/accuracy': 0.9861481189727783, 'test/loss': 0.04986385628581047, 'test/mean_average_precision': 0.2743496012662404, 'test/num_examples': 43793, 'score': 18261.479460000992, 'total_duration': 26097.717761278152, 'accumulated_submission_time': 18261.479460000992, 'accumulated_eval_time': 7831.50794172287, 'accumulated_logging_time': 3.240222930908203, 'global_step': 57001, 'preemption_count': 0})], 'global_step': 57678}
I0210 14:44:21.696338 140039251117888 submission_runner.py:586] Timing: 18477.257613420486
I0210 14:44:21.696413 140039251117888 submission_runner.py:588] Total number of evals: 77
I0210 14:44:21.696460 140039251117888 submission_runner.py:589] ====================
I0210 14:44:21.699000 140039251117888 submission_runner.py:673] Final ogbg score: 18477.04327273369
