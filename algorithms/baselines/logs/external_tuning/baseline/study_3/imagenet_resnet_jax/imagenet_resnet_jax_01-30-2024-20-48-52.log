python3 submission_runner.py --framework=jax --workload=imagenet_resnet --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification/study_3 --overwrite=true --save_checkpoints=false --num_tuning_trials=5 --rng_seed=485521238 --max_global_steps=186666 --imagenet_v2_data_dir=/data/imagenet/jax 2>&1 | tee -a /logs/imagenet_resnet_jax_01-30-2024-20-48-52.log
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
I0130 20:49:14.718778 140070692116288 logger_utils.py:61] Removing existing experiment directory /experiment_runs/prize_qualification/study_3/imagenet_resnet_jax because --overwrite was set.
I0130 20:49:14.721017 140070692116288 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification/study_3/imagenet_resnet_jax.
I0130 20:49:15.714572 140070692116288 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Interpreter Host
I0130 20:49:15.715400 140070692116288 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0130 20:49:15.715521 140070692116288 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0130 20:49:15.716550 140070692116288 submission_runner.py:542] Using RNG seed 485521238
I0130 20:49:16.843114 140070692116288 submission_runner.py:551] --- Tuning run 1/5 ---
I0130 20:49:16.843314 140070692116288 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_3/imagenet_resnet_jax/trial_1.
I0130 20:49:16.843495 140070692116288 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_3/imagenet_resnet_jax/trial_1/hparams.json.
I0130 20:49:17.024303 140070692116288 submission_runner.py:206] Initializing dataset.
I0130 20:49:17.040024 140070692116288 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0130 20:49:17.050107 140070692116288 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0130 20:49:17.426697 140070692116288 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0130 20:49:18.614854 140070692116288 submission_runner.py:213] Initializing model.
I0130 20:49:29.141183 140070692116288 submission_runner.py:255] Initializing optimizer.
I0130 20:49:30.801403 140070692116288 submission_runner.py:262] Initializing metrics bundle.
I0130 20:49:30.801580 140070692116288 submission_runner.py:280] Initializing checkpoint and logger.
I0130 20:49:30.802627 140070692116288 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_3/imagenet_resnet_jax/trial_1 with prefix checkpoint_
I0130 20:49:30.802776 140070692116288 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_3/imagenet_resnet_jax/trial_1/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0130 20:49:31.095557 140070692116288 logger_utils.py:220] Unable to record git information. Continuing without it.
I0130 20:49:31.364221 140070692116288 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_3/imagenet_resnet_jax/trial_1/flags_0.json.
I0130 20:49:31.373069 140070692116288 submission_runner.py:314] Starting training loop.
I0130 20:50:26.099289 139907779520256 logging_writer.py:48] [0] global_step=0, grad_norm=0.6239581108093262, loss=6.925497055053711
I0130 20:50:26.114971 140070692116288 spec.py:321] Evaluating on the training split.
I0130 20:50:27.104016 140070692116288 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0130 20:50:27.113187 140070692116288 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0130 20:50:27.196013 140070692116288 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0130 20:50:40.742518 140070692116288 spec.py:333] Evaluating on the validation split.
I0130 20:50:42.021013 140070692116288 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0130 20:50:42.046290 140070692116288 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0130 20:50:42.113568 140070692116288 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0130 20:50:58.454612 140070692116288 spec.py:349] Evaluating on the test split.
I0130 20:50:59.235005 140070692116288 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0130 20:50:59.240020 140070692116288 dataset_builder.py:528] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0130 20:50:59.276308 140070692116288 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0130 20:51:03.611012 140070692116288 submission_runner.py:408] Time since start: 92.24s, 	Step: 1, 	{'train/accuracy': 0.0013352996902540326, 'train/loss': 6.911616802215576, 'validation/accuracy': 0.0011199999134987593, 'validation/loss': 6.912059783935547, 'validation/num_examples': 50000, 'test/accuracy': 0.0009000000427477062, 'test/loss': 6.912177562713623, 'test/num_examples': 10000, 'score': 54.74183368682861, 'total_duration': 92.23791027069092, 'accumulated_submission_time': 54.74183368682861, 'accumulated_eval_time': 37.495994091033936, 'accumulated_logging_time': 0}
I0130 20:51:03.628597 139886053017344 logging_writer.py:48] [1] accumulated_eval_time=37.495994, accumulated_logging_time=0, accumulated_submission_time=54.741834, global_step=1, preemption_count=0, score=54.741834, test/accuracy=0.000900, test/loss=6.912178, test/num_examples=10000, total_duration=92.237910, train/accuracy=0.001335, train/loss=6.911617, validation/accuracy=0.001120, validation/loss=6.912060, validation/num_examples=50000
I0130 20:51:37.405429 139886044624640 logging_writer.py:48] [100] global_step=100, grad_norm=0.5948771238327026, loss=6.90458345413208
I0130 20:52:11.221388 139886053017344 logging_writer.py:48] [200] global_step=200, grad_norm=0.6011343002319336, loss=6.865135669708252
I0130 20:52:45.096470 139886044624640 logging_writer.py:48] [300] global_step=300, grad_norm=0.6382471919059753, loss=6.788351058959961
I0130 20:53:18.998765 139886053017344 logging_writer.py:48] [400] global_step=400, grad_norm=0.6636415123939514, loss=6.700945854187012
I0130 20:53:52.873347 139886044624640 logging_writer.py:48] [500] global_step=500, grad_norm=0.7348316311836243, loss=6.642312526702881
I0130 20:54:26.752948 139886053017344 logging_writer.py:48] [600] global_step=600, grad_norm=0.7418363094329834, loss=6.521494388580322
I0130 20:55:00.624623 139886044624640 logging_writer.py:48] [700] global_step=700, grad_norm=0.7691767811775208, loss=6.443938255310059
I0130 20:55:34.491764 139886053017344 logging_writer.py:48] [800] global_step=800, grad_norm=0.860856831073761, loss=6.348688125610352
I0130 20:56:08.398032 139886044624640 logging_writer.py:48] [900] global_step=900, grad_norm=1.0663427114486694, loss=6.270645618438721
I0130 20:56:42.287332 139886053017344 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.5191617012023926, loss=6.220330238342285
I0130 20:57:16.233298 139886044624640 logging_writer.py:48] [1100] global_step=1100, grad_norm=2.045318603515625, loss=6.061069011688232
I0130 20:57:50.097008 139886053017344 logging_writer.py:48] [1200] global_step=1200, grad_norm=3.6029767990112305, loss=6.034955024719238
I0130 20:58:24.015034 139886044624640 logging_writer.py:48] [1300] global_step=1300, grad_norm=2.024296522140503, loss=5.970179557800293
I0130 20:58:57.899111 139886053017344 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.0123648643493652, loss=5.8854475021362305
I0130 20:59:31.788059 139886044624640 logging_writer.py:48] [1500] global_step=1500, grad_norm=3.2370662689208984, loss=5.814906120300293
I0130 20:59:33.922716 140070692116288 spec.py:321] Evaluating on the training split.
I0130 20:59:41.227296 140070692116288 spec.py:333] Evaluating on the validation split.
I0130 20:59:49.602498 140070692116288 spec.py:349] Evaluating on the test split.
I0130 20:59:51.908568 140070692116288 submission_runner.py:408] Time since start: 620.54s, 	Step: 1508, 	{'train/accuracy': 0.07099011540412903, 'train/loss': 5.393021583557129, 'validation/accuracy': 0.06667999923229218, 'validation/loss': 5.462131500244141, 'validation/num_examples': 50000, 'test/accuracy': 0.048500001430511475, 'test/loss': 5.668511390686035, 'test/num_examples': 10000, 'score': 564.9765136241913, 'total_duration': 620.535459280014, 'accumulated_submission_time': 564.9765136241913, 'accumulated_eval_time': 55.481812715530396, 'accumulated_logging_time': 0.02588486671447754}
I0130 20:59:51.924233 139886061410048 logging_writer.py:48] [1508] accumulated_eval_time=55.481813, accumulated_logging_time=0.025885, accumulated_submission_time=564.976514, global_step=1508, preemption_count=0, score=564.976514, test/accuracy=0.048500, test/loss=5.668511, test/num_examples=10000, total_duration=620.535459, train/accuracy=0.070990, train/loss=5.393022, validation/accuracy=0.066680, validation/loss=5.462132, validation/num_examples=50000
I0130 21:00:23.390204 139886069802752 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.0486514568328857, loss=5.747714996337891
I0130 21:00:57.238426 139886061410048 logging_writer.py:48] [1700] global_step=1700, grad_norm=3.2883176803588867, loss=5.7365031242370605
I0130 21:01:31.174533 139886069802752 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.5192208290100098, loss=5.636041164398193
I0130 21:02:05.061990 139886061410048 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.402587890625, loss=5.5427775382995605
I0130 21:02:39.000198 139886069802752 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.9796409606933594, loss=5.517243385314941
I0130 21:03:12.892879 139886061410048 logging_writer.py:48] [2100] global_step=2100, grad_norm=3.5545718669891357, loss=5.445000648498535
I0130 21:03:46.808043 139886069802752 logging_writer.py:48] [2200] global_step=2200, grad_norm=3.1048717498779297, loss=5.415953159332275
I0130 21:04:20.706676 139886061410048 logging_writer.py:48] [2300] global_step=2300, grad_norm=5.32972526550293, loss=5.341846942901611
I0130 21:04:54.644006 139886069802752 logging_writer.py:48] [2400] global_step=2400, grad_norm=3.260169506072998, loss=5.3113694190979
I0130 21:05:28.572643 139886061410048 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.301496744155884, loss=5.340569972991943
I0130 21:06:02.487448 139886069802752 logging_writer.py:48] [2600] global_step=2600, grad_norm=3.1894073486328125, loss=5.205324172973633
I0130 21:06:36.383300 139886061410048 logging_writer.py:48] [2700] global_step=2700, grad_norm=4.173537254333496, loss=5.116978168487549
I0130 21:07:10.294897 139886069802752 logging_writer.py:48] [2800] global_step=2800, grad_norm=4.543205738067627, loss=5.105591297149658
I0130 21:07:44.212756 139886061410048 logging_writer.py:48] [2900] global_step=2900, grad_norm=3.4564030170440674, loss=4.982969760894775
I0130 21:08:18.127246 139886069802752 logging_writer.py:48] [3000] global_step=3000, grad_norm=5.331862926483154, loss=5.058995246887207
I0130 21:08:21.957875 140070692116288 spec.py:321] Evaluating on the training split.
I0130 21:08:29.603650 140070692116288 spec.py:333] Evaluating on the validation split.
I0130 21:08:37.981343 140070692116288 spec.py:349] Evaluating on the test split.
I0130 21:08:40.354602 140070692116288 submission_runner.py:408] Time since start: 1148.98s, 	Step: 3013, 	{'train/accuracy': 0.17681759595870972, 'train/loss': 4.263920783996582, 'validation/accuracy': 0.15916000306606293, 'validation/loss': 4.370645523071289, 'validation/num_examples': 50000, 'test/accuracy': 0.1177000030875206, 'test/loss': 4.772252559661865, 'test/num_examples': 10000, 'score': 1074.9497528076172, 'total_duration': 1148.9814901351929, 'accumulated_submission_time': 1074.9497528076172, 'accumulated_eval_time': 73.87850284576416, 'accumulated_logging_time': 0.05106663703918457}
I0130 21:08:40.370801 139908601595648 logging_writer.py:48] [3013] accumulated_eval_time=73.878503, accumulated_logging_time=0.051067, accumulated_submission_time=1074.949753, global_step=3013, preemption_count=0, score=1074.949753, test/accuracy=0.117700, test/loss=4.772253, test/num_examples=10000, total_duration=1148.981490, train/accuracy=0.176818, train/loss=4.263921, validation/accuracy=0.159160, validation/loss=4.370646, validation/num_examples=50000
I0130 21:09:10.169915 139908987463424 logging_writer.py:48] [3100] global_step=3100, grad_norm=3.7323715686798096, loss=5.026122093200684
I0130 21:09:44.046131 139908601595648 logging_writer.py:48] [3200] global_step=3200, grad_norm=6.818744659423828, loss=4.880526542663574
I0130 21:10:17.951089 139908987463424 logging_writer.py:48] [3300] global_step=3300, grad_norm=4.000993728637695, loss=4.832096099853516
I0130 21:10:51.865279 139908601595648 logging_writer.py:48] [3400] global_step=3400, grad_norm=5.346816539764404, loss=4.9675798416137695
I0130 21:11:25.754220 139908987463424 logging_writer.py:48] [3500] global_step=3500, grad_norm=4.462972640991211, loss=4.764492988586426
I0130 21:11:59.672615 139908601595648 logging_writer.py:48] [3600] global_step=3600, grad_norm=3.799473285675049, loss=4.826603412628174
I0130 21:12:33.583309 139908987463424 logging_writer.py:48] [3700] global_step=3700, grad_norm=4.281647205352783, loss=4.7381181716918945
I0130 21:13:07.504953 139908601595648 logging_writer.py:48] [3800] global_step=3800, grad_norm=5.581806182861328, loss=4.662817478179932
I0130 21:13:41.406266 139908987463424 logging_writer.py:48] [3900] global_step=3900, grad_norm=5.742629051208496, loss=4.661163806915283
I0130 21:14:15.334025 139908601595648 logging_writer.py:48] [4000] global_step=4000, grad_norm=5.0144476890563965, loss=4.6588544845581055
I0130 21:14:49.274529 139908987463424 logging_writer.py:48] [4100] global_step=4100, grad_norm=6.208893299102783, loss=4.586007595062256
I0130 21:15:23.169944 139908601595648 logging_writer.py:48] [4200] global_step=4200, grad_norm=6.903799533843994, loss=4.4704060554504395
I0130 21:15:57.100587 139908987463424 logging_writer.py:48] [4300] global_step=4300, grad_norm=3.529606819152832, loss=4.52169942855835
I0130 21:16:30.992367 139908601595648 logging_writer.py:48] [4400] global_step=4400, grad_norm=5.941612243652344, loss=4.273775100708008
I0130 21:17:04.909630 139908987463424 logging_writer.py:48] [4500] global_step=4500, grad_norm=6.329765319824219, loss=4.368678092956543
I0130 21:17:10.429301 140070692116288 spec.py:321] Evaluating on the training split.
I0130 21:17:17.754810 140070692116288 spec.py:333] Evaluating on the validation split.
I0130 21:17:26.167659 140070692116288 spec.py:349] Evaluating on the test split.
I0130 21:17:28.495027 140070692116288 submission_runner.py:408] Time since start: 1677.12s, 	Step: 4518, 	{'train/accuracy': 0.28756776452064514, 'train/loss': 3.482984781265259, 'validation/accuracy': 0.26330000162124634, 'validation/loss': 3.6200780868530273, 'validation/num_examples': 50000, 'test/accuracy': 0.19010001420974731, 'test/loss': 4.155796527862549, 'test/num_examples': 10000, 'score': 1584.9473168849945, 'total_duration': 1677.1219086647034, 'accumulated_submission_time': 1584.9473168849945, 'accumulated_eval_time': 91.94419121742249, 'accumulated_logging_time': 0.07854390144348145}
I0130 21:17:28.512387 139908601595648 logging_writer.py:48] [4518] accumulated_eval_time=91.944191, accumulated_logging_time=0.078544, accumulated_submission_time=1584.947317, global_step=4518, preemption_count=0, score=1584.947317, test/accuracy=0.190100, test/loss=4.155797, test/num_examples=10000, total_duration=1677.121909, train/accuracy=0.287568, train/loss=3.482985, validation/accuracy=0.263300, validation/loss=3.620078, validation/num_examples=50000
I0130 21:17:56.610347 139908609988352 logging_writer.py:48] [4600] global_step=4600, grad_norm=8.229104042053223, loss=4.427536487579346
I0130 21:18:30.501394 139908601595648 logging_writer.py:48] [4700] global_step=4700, grad_norm=5.818325996398926, loss=4.405822277069092
I0130 21:19:04.406075 139908609988352 logging_writer.py:48] [4800] global_step=4800, grad_norm=6.87431526184082, loss=4.289041042327881
I0130 21:19:38.316880 139908601595648 logging_writer.py:48] [4900] global_step=4900, grad_norm=4.172502517700195, loss=4.286367893218994
I0130 21:20:12.209343 139908609988352 logging_writer.py:48] [5000] global_step=5000, grad_norm=5.248867511749268, loss=4.296948432922363
I0130 21:20:46.143907 139908601595648 logging_writer.py:48] [5100] global_step=5100, grad_norm=5.306280612945557, loss=4.200756072998047
I0130 21:21:20.016669 139908609988352 logging_writer.py:48] [5200] global_step=5200, grad_norm=7.0365190505981445, loss=4.233946800231934
I0130 21:21:53.908301 139908601595648 logging_writer.py:48] [5300] global_step=5300, grad_norm=6.42923641204834, loss=4.096972465515137
I0130 21:22:27.794209 139908609988352 logging_writer.py:48] [5400] global_step=5400, grad_norm=5.584322452545166, loss=4.108725547790527
I0130 21:23:01.697627 139908601595648 logging_writer.py:48] [5500] global_step=5500, grad_norm=6.555634498596191, loss=4.210332870483398
I0130 21:23:35.572365 139908609988352 logging_writer.py:48] [5600] global_step=5600, grad_norm=4.871555805206299, loss=4.075652599334717
I0130 21:24:09.454466 139908601595648 logging_writer.py:48] [5700] global_step=5700, grad_norm=6.6978759765625, loss=4.131717205047607
I0130 21:24:43.338905 139908609988352 logging_writer.py:48] [5800] global_step=5800, grad_norm=4.329511642456055, loss=4.000617980957031
I0130 21:25:17.227175 139908601595648 logging_writer.py:48] [5900] global_step=5900, grad_norm=5.465132236480713, loss=4.048159599304199
I0130 21:25:51.092935 139908609988352 logging_writer.py:48] [6000] global_step=6000, grad_norm=4.562321662902832, loss=3.9901010990142822
I0130 21:25:58.648461 140070692116288 spec.py:321] Evaluating on the training split.
I0130 21:26:06.261167 140070692116288 spec.py:333] Evaluating on the validation split.
I0130 21:26:14.587247 140070692116288 spec.py:349] Evaluating on the test split.
I0130 21:26:16.888687 140070692116288 submission_runner.py:408] Time since start: 2205.52s, 	Step: 6024, 	{'train/accuracy': 0.3698580861091614, 'train/loss': 2.9841694831848145, 'validation/accuracy': 0.3408399820327759, 'validation/loss': 3.133474349975586, 'validation/num_examples': 50000, 'test/accuracy': 0.255700021982193, 'test/loss': 3.7140119075775146, 'test/num_examples': 10000, 'score': 2095.0244052410126, 'total_duration': 2205.5155744552612, 'accumulated_submission_time': 2095.0244052410126, 'accumulated_eval_time': 110.18438458442688, 'accumulated_logging_time': 0.10527682304382324}
I0130 21:26:16.906198 139908450592512 logging_writer.py:48] [6024] accumulated_eval_time=110.184385, accumulated_logging_time=0.105277, accumulated_submission_time=2095.024405, global_step=6024, preemption_count=0, score=2095.024405, test/accuracy=0.255700, test/loss=3.714012, test/num_examples=10000, total_duration=2205.515574, train/accuracy=0.369858, train/loss=2.984169, validation/accuracy=0.340840, validation/loss=3.133474, validation/num_examples=50000
I0130 21:26:43.000825 139908458985216 logging_writer.py:48] [6100] global_step=6100, grad_norm=4.729935646057129, loss=3.9544193744659424
I0130 21:27:17.058634 139908450592512 logging_writer.py:48] [6200] global_step=6200, grad_norm=5.6171746253967285, loss=3.8651373386383057
I0130 21:27:50.929877 139908458985216 logging_writer.py:48] [6300] global_step=6300, grad_norm=6.015139579772949, loss=3.9876229763031006
I0130 21:28:24.853635 139908450592512 logging_writer.py:48] [6400] global_step=6400, grad_norm=7.301257610321045, loss=4.010088920593262
I0130 21:28:58.731523 139908458985216 logging_writer.py:48] [6500] global_step=6500, grad_norm=7.8082194328308105, loss=3.8580074310302734
I0130 21:29:32.634295 139908450592512 logging_writer.py:48] [6600] global_step=6600, grad_norm=4.9772162437438965, loss=3.901129961013794
I0130 21:30:06.533480 139908458985216 logging_writer.py:48] [6700] global_step=6700, grad_norm=9.47354793548584, loss=3.801995038986206
I0130 21:30:40.418567 139908450592512 logging_writer.py:48] [6800] global_step=6800, grad_norm=5.313536167144775, loss=3.8093972206115723
I0130 21:31:14.303826 139908458985216 logging_writer.py:48] [6900] global_step=6900, grad_norm=8.786799430847168, loss=3.694005012512207
I0130 21:31:48.202652 139908450592512 logging_writer.py:48] [7000] global_step=7000, grad_norm=4.578554153442383, loss=3.6152560710906982
I0130 21:32:22.087121 139908458985216 logging_writer.py:48] [7100] global_step=7100, grad_norm=7.989756107330322, loss=3.712496280670166
I0130 21:32:55.971561 139908450592512 logging_writer.py:48] [7200] global_step=7200, grad_norm=6.48911190032959, loss=3.691152811050415
I0130 21:33:29.935635 139908458985216 logging_writer.py:48] [7300] global_step=7300, grad_norm=7.190356254577637, loss=3.684753656387329
I0130 21:34:03.831373 139908450592512 logging_writer.py:48] [7400] global_step=7400, grad_norm=5.932214260101318, loss=3.7838075160980225
I0130 21:34:37.738701 139908458985216 logging_writer.py:48] [7500] global_step=7500, grad_norm=4.473159313201904, loss=3.6272823810577393
I0130 21:34:46.971649 140070692116288 spec.py:321] Evaluating on the training split.
I0130 21:34:54.241494 140070692116288 spec.py:333] Evaluating on the validation split.
I0130 21:35:02.848756 140070692116288 spec.py:349] Evaluating on the test split.
I0130 21:35:05.186004 140070692116288 submission_runner.py:408] Time since start: 2733.81s, 	Step: 7529, 	{'train/accuracy': 0.44088807702064514, 'train/loss': 2.5516676902770996, 'validation/accuracy': 0.41152000427246094, 'validation/loss': 2.7000417709350586, 'validation/num_examples': 50000, 'test/accuracy': 0.3109000027179718, 'test/loss': 3.379472017288208, 'test/num_examples': 10000, 'score': 2605.0299191474915, 'total_duration': 2733.8128702640533, 'accumulated_submission_time': 2605.0299191474915, 'accumulated_eval_time': 128.39868831634521, 'accumulated_logging_time': 0.13362383842468262}
I0130 21:35:05.207014 139908987463424 logging_writer.py:48] [7529] accumulated_eval_time=128.398688, accumulated_logging_time=0.133624, accumulated_submission_time=2605.029919, global_step=7529, preemption_count=0, score=2605.029919, test/accuracy=0.310900, test/loss=3.379472, test/num_examples=10000, total_duration=2733.812870, train/accuracy=0.440888, train/loss=2.551668, validation/accuracy=0.411520, validation/loss=2.700042, validation/num_examples=50000
I0130 21:35:29.585941 139908995856128 logging_writer.py:48] [7600] global_step=7600, grad_norm=7.07049036026001, loss=3.6077640056610107
I0130 21:36:03.448750 139908987463424 logging_writer.py:48] [7700] global_step=7700, grad_norm=6.583141326904297, loss=3.6331939697265625
I0130 21:36:37.337740 139908995856128 logging_writer.py:48] [7800] global_step=7800, grad_norm=6.045862674713135, loss=3.5923943519592285
I0130 21:37:11.197813 139908987463424 logging_writer.py:48] [7900] global_step=7900, grad_norm=3.943702220916748, loss=3.604759693145752
I0130 21:37:45.042842 139908995856128 logging_writer.py:48] [8000] global_step=8000, grad_norm=4.955761432647705, loss=3.5063376426696777
I0130 21:38:18.970037 139908987463424 logging_writer.py:48] [8100] global_step=8100, grad_norm=8.155864715576172, loss=3.569632053375244
I0130 21:38:52.838316 139908995856128 logging_writer.py:48] [8200] global_step=8200, grad_norm=5.534674644470215, loss=3.512713670730591
I0130 21:39:26.726951 139908987463424 logging_writer.py:48] [8300] global_step=8300, grad_norm=7.0172343254089355, loss=3.4495036602020264
I0130 21:40:00.615315 139908995856128 logging_writer.py:48] [8400] global_step=8400, grad_norm=6.563126087188721, loss=3.5314159393310547
I0130 21:40:34.493615 139908987463424 logging_writer.py:48] [8500] global_step=8500, grad_norm=4.377130031585693, loss=3.5564043521881104
I0130 21:41:08.349633 139908995856128 logging_writer.py:48] [8600] global_step=8600, grad_norm=8.233680725097656, loss=3.5760180950164795
I0130 21:41:42.224977 139908987463424 logging_writer.py:48] [8700] global_step=8700, grad_norm=3.664294481277466, loss=3.40613055229187
I0130 21:42:16.107229 139908995856128 logging_writer.py:48] [8800] global_step=8800, grad_norm=3.412221670150757, loss=3.4394965171813965
I0130 21:42:49.947677 139908987463424 logging_writer.py:48] [8900] global_step=8900, grad_norm=6.291590213775635, loss=3.3952133655548096
I0130 21:43:23.807303 139908995856128 logging_writer.py:48] [9000] global_step=9000, grad_norm=5.057382583618164, loss=3.477734327316284
I0130 21:43:35.425280 140070692116288 spec.py:321] Evaluating on the training split.
I0130 21:43:42.822896 140070692116288 spec.py:333] Evaluating on the validation split.
I0130 21:43:51.288352 140070692116288 spec.py:349] Evaluating on the test split.
I0130 21:43:53.574402 140070692116288 submission_runner.py:408] Time since start: 3262.20s, 	Step: 9036, 	{'train/accuracy': 0.49230706691741943, 'train/loss': 2.256446599960327, 'validation/accuracy': 0.45865997672080994, 'validation/loss': 2.4324121475219727, 'validation/num_examples': 50000, 'test/accuracy': 0.3530000150203705, 'test/loss': 3.0937793254852295, 'test/num_examples': 10000, 'score': 3115.184098005295, 'total_duration': 3262.2012915611267, 'accumulated_submission_time': 3115.184098005295, 'accumulated_eval_time': 146.5477843284607, 'accumulated_logging_time': 0.1685779094696045}
I0130 21:43:53.591691 139907737556736 logging_writer.py:48] [9036] accumulated_eval_time=146.547784, accumulated_logging_time=0.168578, accumulated_submission_time=3115.184098, global_step=9036, preemption_count=0, score=3115.184098, test/accuracy=0.353000, test/loss=3.093779, test/num_examples=10000, total_duration=3262.201292, train/accuracy=0.492307, train/loss=2.256447, validation/accuracy=0.458660, validation/loss=2.432412, validation/num_examples=50000
I0130 21:44:15.615274 139908576417536 logging_writer.py:48] [9100] global_step=9100, grad_norm=6.850131511688232, loss=3.397693395614624
I0130 21:44:49.430110 139907737556736 logging_writer.py:48] [9200] global_step=9200, grad_norm=5.692830562591553, loss=3.446866989135742
I0130 21:45:23.309084 139908576417536 logging_writer.py:48] [9300] global_step=9300, grad_norm=3.9998843669891357, loss=3.3893837928771973
I0130 21:45:57.148895 139907737556736 logging_writer.py:48] [9400] global_step=9400, grad_norm=5.01154088973999, loss=3.357482433319092
I0130 21:46:30.974365 139908576417536 logging_writer.py:48] [9500] global_step=9500, grad_norm=5.075397491455078, loss=3.301887035369873
I0130 21:47:04.850305 139907737556736 logging_writer.py:48] [9600] global_step=9600, grad_norm=3.920436143875122, loss=3.437958240509033
I0130 21:47:38.649451 139908576417536 logging_writer.py:48] [9700] global_step=9700, grad_norm=5.832958698272705, loss=3.466249465942383
I0130 21:48:12.501281 139907737556736 logging_writer.py:48] [9800] global_step=9800, grad_norm=5.424371242523193, loss=3.3762667179107666
I0130 21:48:46.333472 139908576417536 logging_writer.py:48] [9900] global_step=9900, grad_norm=4.7234930992126465, loss=3.432234287261963
I0130 21:49:20.188887 139907737556736 logging_writer.py:48] [10000] global_step=10000, grad_norm=5.9905877113342285, loss=3.345338821411133
I0130 21:49:54.013096 139908576417536 logging_writer.py:48] [10100] global_step=10100, grad_norm=5.6201324462890625, loss=3.4129133224487305
I0130 21:50:27.871387 139907737556736 logging_writer.py:48] [10200] global_step=10200, grad_norm=7.4234700202941895, loss=3.3709330558776855
I0130 21:51:01.712213 139908576417536 logging_writer.py:48] [10300] global_step=10300, grad_norm=6.289030075073242, loss=3.3990068435668945
I0130 21:51:35.574678 139907737556736 logging_writer.py:48] [10400] global_step=10400, grad_norm=4.550726890563965, loss=3.2812533378601074
I0130 21:52:09.449902 139908576417536 logging_writer.py:48] [10500] global_step=10500, grad_norm=5.268641471862793, loss=3.272977352142334
I0130 21:52:23.772837 140070692116288 spec.py:321] Evaluating on the training split.
I0130 21:52:31.253043 140070692116288 spec.py:333] Evaluating on the validation split.
I0130 21:52:39.781204 140070692116288 spec.py:349] Evaluating on the test split.
I0130 21:52:42.089103 140070692116288 submission_runner.py:408] Time since start: 3790.72s, 	Step: 10544, 	{'train/accuracy': 0.5581353306770325, 'train/loss': 1.9381918907165527, 'validation/accuracy': 0.5031399726867676, 'validation/loss': 2.2289462089538574, 'validation/num_examples': 50000, 'test/accuracy': 0.39180001616477966, 'test/loss': 2.887331008911133, 'test/num_examples': 10000, 'score': 3625.3059175014496, 'total_duration': 3790.7159888744354, 'accumulated_submission_time': 3625.3059175014496, 'accumulated_eval_time': 164.86400961875916, 'accumulated_logging_time': 0.19490265846252441}
I0130 21:52:42.108399 139907729164032 logging_writer.py:48] [10544] accumulated_eval_time=164.864010, accumulated_logging_time=0.194903, accumulated_submission_time=3625.305918, global_step=10544, preemption_count=0, score=3625.305918, test/accuracy=0.391800, test/loss=2.887331, test/num_examples=10000, total_duration=3790.715989, train/accuracy=0.558135, train/loss=1.938192, validation/accuracy=0.503140, validation/loss=2.228946, validation/num_examples=50000
I0130 21:53:01.397091 139907737556736 logging_writer.py:48] [10600] global_step=10600, grad_norm=4.559664726257324, loss=3.2428371906280518
I0130 21:53:35.215076 139907729164032 logging_writer.py:48] [10700] global_step=10700, grad_norm=9.24938678741455, loss=3.2247562408447266
I0130 21:54:09.031705 139907737556736 logging_writer.py:48] [10800] global_step=10800, grad_norm=6.92523193359375, loss=3.2861227989196777
I0130 21:54:42.851548 139907729164032 logging_writer.py:48] [10900] global_step=10900, grad_norm=4.70695161819458, loss=3.1892921924591064
I0130 21:55:16.688884 139907737556736 logging_writer.py:48] [11000] global_step=11000, grad_norm=4.732746124267578, loss=3.2717173099517822
I0130 21:55:50.507587 139907729164032 logging_writer.py:48] [11100] global_step=11100, grad_norm=4.429790496826172, loss=3.269766330718994
I0130 21:56:24.337982 139907737556736 logging_writer.py:48] [11200] global_step=11200, grad_norm=4.363549709320068, loss=3.2583298683166504
I0130 21:56:58.146972 139907729164032 logging_writer.py:48] [11300] global_step=11300, grad_norm=5.559313774108887, loss=3.25878643989563
I0130 21:57:31.986192 139907737556736 logging_writer.py:48] [11400] global_step=11400, grad_norm=6.188143253326416, loss=3.326054334640503
I0130 21:58:05.836492 139907729164032 logging_writer.py:48] [11500] global_step=11500, grad_norm=5.95137882232666, loss=3.2057409286499023
I0130 21:58:39.664283 139907737556736 logging_writer.py:48] [11600] global_step=11600, grad_norm=3.2903099060058594, loss=3.182488441467285
I0130 21:59:13.524903 139907729164032 logging_writer.py:48] [11700] global_step=11700, grad_norm=3.4770333766937256, loss=3.2307488918304443
I0130 21:59:47.379252 139907737556736 logging_writer.py:48] [11800] global_step=11800, grad_norm=5.051454067230225, loss=3.207829475402832
I0130 22:00:21.206677 139907729164032 logging_writer.py:48] [11900] global_step=11900, grad_norm=3.8219499588012695, loss=3.0730128288269043
I0130 22:00:55.048720 139907737556736 logging_writer.py:48] [12000] global_step=12000, grad_norm=5.411558151245117, loss=3.137388229370117
I0130 22:01:12.371417 140070692116288 spec.py:321] Evaluating on the training split.
I0130 22:01:19.919844 140070692116288 spec.py:333] Evaluating on the validation split.
I0130 22:01:28.658097 140070692116288 spec.py:349] Evaluating on the test split.
I0130 22:01:30.962640 140070692116288 submission_runner.py:408] Time since start: 4319.59s, 	Step: 12053, 	{'train/accuracy': 0.5622209906578064, 'train/loss': 1.9551576375961304, 'validation/accuracy': 0.5192399621009827, 'validation/loss': 2.169461488723755, 'validation/num_examples': 50000, 'test/accuracy': 0.40320003032684326, 'test/loss': 2.8256454467773438, 'test/num_examples': 10000, 'score': 4135.503875255585, 'total_duration': 4319.589529514313, 'accumulated_submission_time': 4135.503875255585, 'accumulated_eval_time': 183.4552013874054, 'accumulated_logging_time': 0.22897028923034668}
I0130 22:01:30.981492 139907536246528 logging_writer.py:48] [12053] accumulated_eval_time=183.455201, accumulated_logging_time=0.228970, accumulated_submission_time=4135.503875, global_step=12053, preemption_count=0, score=4135.503875, test/accuracy=0.403200, test/loss=2.825645, test/num_examples=10000, total_duration=4319.589530, train/accuracy=0.562221, train/loss=1.955158, validation/accuracy=0.519240, validation/loss=2.169461, validation/num_examples=50000
I0130 22:01:47.250257 139907553031936 logging_writer.py:48] [12100] global_step=12100, grad_norm=6.894993782043457, loss=3.1875340938568115
I0130 22:02:21.028000 139907536246528 logging_writer.py:48] [12200] global_step=12200, grad_norm=4.2279582023620605, loss=3.171477794647217
I0130 22:02:54.860766 139907553031936 logging_writer.py:48] [12300] global_step=12300, grad_norm=7.870974063873291, loss=3.2573132514953613
I0130 22:03:28.692145 139907536246528 logging_writer.py:48] [12400] global_step=12400, grad_norm=5.1420063972473145, loss=3.139160394668579
I0130 22:04:02.576000 139907553031936 logging_writer.py:48] [12500] global_step=12500, grad_norm=6.056877613067627, loss=3.113656759262085
I0130 22:04:36.393319 139907536246528 logging_writer.py:48] [12600] global_step=12600, grad_norm=4.086940288543701, loss=3.075042247772217
I0130 22:05:10.233643 139907553031936 logging_writer.py:48] [12700] global_step=12700, grad_norm=4.508412837982178, loss=3.2378945350646973
I0130 22:05:44.069096 139907536246528 logging_writer.py:48] [12800] global_step=12800, grad_norm=5.779430389404297, loss=3.055988311767578
I0130 22:06:17.895568 139907553031936 logging_writer.py:48] [12900] global_step=12900, grad_norm=6.229721546173096, loss=3.1127724647521973
I0130 22:06:51.716666 139907536246528 logging_writer.py:48] [13000] global_step=13000, grad_norm=5.3724493980407715, loss=3.186023235321045
I0130 22:07:25.544265 139907553031936 logging_writer.py:48] [13100] global_step=13100, grad_norm=5.1012678146362305, loss=3.1232476234436035
I0130 22:07:59.390325 139907536246528 logging_writer.py:48] [13200] global_step=13200, grad_norm=4.266502857208252, loss=3.1750941276550293
I0130 22:08:33.240469 139907553031936 logging_writer.py:48] [13300] global_step=13300, grad_norm=6.873019695281982, loss=3.1410233974456787
I0130 22:09:07.088232 139907536246528 logging_writer.py:48] [13400] global_step=13400, grad_norm=4.4216814041137695, loss=3.0740151405334473
I0130 22:09:40.899977 139907553031936 logging_writer.py:48] [13500] global_step=13500, grad_norm=5.5570783615112305, loss=3.111912727355957
I0130 22:10:01.293849 140070692116288 spec.py:321] Evaluating on the training split.
I0130 22:10:08.815554 140070692116288 spec.py:333] Evaluating on the validation split.
I0130 22:10:20.430502 140070692116288 spec.py:349] Evaluating on the test split.
I0130 22:10:22.675303 140070692116288 submission_runner.py:408] Time since start: 4851.30s, 	Step: 13562, 	{'train/accuracy': 0.5889070630073547, 'train/loss': 1.8355814218521118, 'validation/accuracy': 0.5423399806022644, 'validation/loss': 2.049694776535034, 'validation/num_examples': 50000, 'test/accuracy': 0.41920003294944763, 'test/loss': 2.7327487468719482, 'test/num_examples': 10000, 'score': 4645.7563996315, 'total_duration': 4851.3021948337555, 'accumulated_submission_time': 4645.7563996315, 'accumulated_eval_time': 204.8366265296936, 'accumulated_logging_time': 0.25846338272094727}
I0130 22:10:22.691142 139907544639232 logging_writer.py:48] [13562] accumulated_eval_time=204.836627, accumulated_logging_time=0.258463, accumulated_submission_time=4645.756400, global_step=13562, preemption_count=0, score=4645.756400, test/accuracy=0.419200, test/loss=2.732749, test/num_examples=10000, total_duration=4851.302195, train/accuracy=0.588907, train/loss=1.835581, validation/accuracy=0.542340, validation/loss=2.049695, validation/num_examples=50000
I0130 22:10:35.879593 139907720771328 logging_writer.py:48] [13600] global_step=13600, grad_norm=3.4790420532226562, loss=3.0655035972595215
I0130 22:11:09.630805 139907544639232 logging_writer.py:48] [13700] global_step=13700, grad_norm=4.998806953430176, loss=3.0449743270874023
I0130 22:11:43.401705 139907720771328 logging_writer.py:48] [13800] global_step=13800, grad_norm=7.172605037689209, loss=3.114809036254883
I0130 22:12:17.157072 139907544639232 logging_writer.py:48] [13900] global_step=13900, grad_norm=4.442071914672852, loss=3.055264711380005
I0130 22:12:51.007237 139907720771328 logging_writer.py:48] [14000] global_step=14000, grad_norm=7.280290603637695, loss=3.106455087661743
I0130 22:13:24.824160 139907544639232 logging_writer.py:48] [14100] global_step=14100, grad_norm=5.726831436157227, loss=3.055286407470703
I0130 22:13:58.606781 139907720771328 logging_writer.py:48] [14200] global_step=14200, grad_norm=6.486490726470947, loss=3.1805551052093506
I0130 22:14:32.358483 139907544639232 logging_writer.py:48] [14300] global_step=14300, grad_norm=6.555810928344727, loss=3.0880374908447266
I0130 22:15:06.143002 139907720771328 logging_writer.py:48] [14400] global_step=14400, grad_norm=5.250153541564941, loss=3.066195011138916
I0130 22:15:39.941939 139907544639232 logging_writer.py:48] [14500] global_step=14500, grad_norm=3.4084837436676025, loss=3.033982276916504
I0130 22:16:13.731730 139907720771328 logging_writer.py:48] [14600] global_step=14600, grad_norm=8.654122352600098, loss=3.0619630813598633
I0130 22:16:47.483061 139907544639232 logging_writer.py:48] [14700] global_step=14700, grad_norm=4.289534568786621, loss=3.100268840789795
I0130 22:17:21.267391 139907720771328 logging_writer.py:48] [14800] global_step=14800, grad_norm=4.912904262542725, loss=2.9348325729370117
I0130 22:17:55.056755 139907544639232 logging_writer.py:48] [14900] global_step=14900, grad_norm=5.974021911621094, loss=3.1295008659362793
I0130 22:18:28.829472 139907720771328 logging_writer.py:48] [15000] global_step=15000, grad_norm=4.452821731567383, loss=3.0711050033569336
I0130 22:18:52.879860 140070692116288 spec.py:321] Evaluating on the training split.
I0130 22:19:01.050118 140070692116288 spec.py:333] Evaluating on the validation split.
I0130 22:19:14.111520 140070692116288 spec.py:349] Evaluating on the test split.
I0130 22:19:16.275948 140070692116288 submission_runner.py:408] Time since start: 5384.90s, 	Step: 15073, 	{'train/accuracy': 0.5912587642669678, 'train/loss': 1.7979090213775635, 'validation/accuracy': 0.551800012588501, 'validation/loss': 1.9871643781661987, 'validation/num_examples': 50000, 'test/accuracy': 0.42600002884864807, 'test/loss': 2.665627956390381, 'test/num_examples': 10000, 'score': 5155.886093854904, 'total_duration': 5384.902832508087, 'accumulated_submission_time': 5155.886093854904, 'accumulated_eval_time': 228.2326774597168, 'accumulated_logging_time': 0.28252625465393066}
I0130 22:19:16.303123 139907762734848 logging_writer.py:48] [15073] accumulated_eval_time=228.232677, accumulated_logging_time=0.282526, accumulated_submission_time=5155.886094, global_step=15073, preemption_count=0, score=5155.886094, test/accuracy=0.426000, test/loss=2.665628, test/num_examples=10000, total_duration=5384.902833, train/accuracy=0.591259, train/loss=1.797909, validation/accuracy=0.551800, validation/loss=1.987164, validation/num_examples=50000
I0130 22:19:25.752846 139908425447168 logging_writer.py:48] [15100] global_step=15100, grad_norm=4.032474040985107, loss=3.0507469177246094
I0130 22:19:59.527611 139907762734848 logging_writer.py:48] [15200] global_step=15200, grad_norm=6.618714332580566, loss=2.9964957237243652
I0130 22:20:33.320124 139908425447168 logging_writer.py:48] [15300] global_step=15300, grad_norm=3.544848918914795, loss=2.9454169273376465
I0130 22:21:07.137580 139907762734848 logging_writer.py:48] [15400] global_step=15400, grad_norm=6.275908470153809, loss=3.1726889610290527
I0130 22:21:40.902593 139908425447168 logging_writer.py:48] [15500] global_step=15500, grad_norm=4.2730021476745605, loss=2.966676950454712
I0130 22:22:14.685655 139907762734848 logging_writer.py:48] [15600] global_step=15600, grad_norm=4.672142028808594, loss=2.9687857627868652
I0130 22:22:48.532407 139908425447168 logging_writer.py:48] [15700] global_step=15700, grad_norm=4.725391864776611, loss=3.06913423538208
I0130 22:23:22.389241 139907762734848 logging_writer.py:48] [15800] global_step=15800, grad_norm=5.662118434906006, loss=3.0671029090881348
I0130 22:23:56.176985 139908425447168 logging_writer.py:48] [15900] global_step=15900, grad_norm=4.494996547698975, loss=2.9855027198791504
I0130 22:24:30.016282 139907762734848 logging_writer.py:48] [16000] global_step=16000, grad_norm=3.7658796310424805, loss=2.9513161182403564
I0130 22:25:03.835546 139908425447168 logging_writer.py:48] [16100] global_step=16100, grad_norm=5.760713577270508, loss=2.947169780731201
I0130 22:25:37.657942 139907762734848 logging_writer.py:48] [16200] global_step=16200, grad_norm=5.480365753173828, loss=3.0875139236450195
I0130 22:26:11.469275 139908425447168 logging_writer.py:48] [16300] global_step=16300, grad_norm=4.971734046936035, loss=3.0695900917053223
I0130 22:26:45.293674 139907762734848 logging_writer.py:48] [16400] global_step=16400, grad_norm=5.58854341506958, loss=2.9882149696350098
I0130 22:27:19.093517 139908425447168 logging_writer.py:48] [16500] global_step=16500, grad_norm=3.1933374404907227, loss=2.9984683990478516
I0130 22:27:46.583886 140070692116288 spec.py:321] Evaluating on the training split.
I0130 22:27:54.236406 140070692116288 spec.py:333] Evaluating on the validation split.
I0130 22:28:07.411755 140070692116288 spec.py:349] Evaluating on the test split.
I0130 22:28:09.528533 140070692116288 submission_runner.py:408] Time since start: 5918.16s, 	Step: 16583, 	{'train/accuracy': 0.6018216013908386, 'train/loss': 1.7547258138656616, 'validation/accuracy': 0.5679000020027161, 'validation/loss': 1.932205080986023, 'validation/num_examples': 50000, 'test/accuracy': 0.44450002908706665, 'test/loss': 2.6144609451293945, 'test/num_examples': 10000, 'score': 5666.106993675232, 'total_duration': 5918.15540766716, 'accumulated_submission_time': 5666.106993675232, 'accumulated_eval_time': 251.17728233337402, 'accumulated_logging_time': 0.3175804615020752}
I0130 22:28:09.547849 139907737556736 logging_writer.py:48] [16583] accumulated_eval_time=251.177282, accumulated_logging_time=0.317580, accumulated_submission_time=5666.106994, global_step=16583, preemption_count=0, score=5666.106994, test/accuracy=0.444500, test/loss=2.614461, test/num_examples=10000, total_duration=5918.155408, train/accuracy=0.601822, train/loss=1.754726, validation/accuracy=0.567900, validation/loss=1.932205, validation/num_examples=50000
I0130 22:28:15.648752 139907745949440 logging_writer.py:48] [16600] global_step=16600, grad_norm=4.440095901489258, loss=3.04055118560791
I0130 22:28:49.363341 139907737556736 logging_writer.py:48] [16700] global_step=16700, grad_norm=4.054500579833984, loss=3.0203938484191895
I0130 22:29:23.059357 139907745949440 logging_writer.py:48] [16800] global_step=16800, grad_norm=8.064936637878418, loss=3.019232988357544
I0130 22:29:56.852205 139907737556736 logging_writer.py:48] [16900] global_step=16900, grad_norm=4.468962669372559, loss=2.989666223526001
I0130 22:30:30.649779 139907745949440 logging_writer.py:48] [17000] global_step=17000, grad_norm=4.923215389251709, loss=3.0928564071655273
I0130 22:31:04.478303 139907737556736 logging_writer.py:48] [17100] global_step=17100, grad_norm=7.397370338439941, loss=3.018669366836548
I0130 22:31:38.264255 139907745949440 logging_writer.py:48] [17200] global_step=17200, grad_norm=7.1789727210998535, loss=3.027968406677246
I0130 22:32:12.031919 139907737556736 logging_writer.py:48] [17300] global_step=17300, grad_norm=4.363795280456543, loss=2.97824764251709
I0130 22:32:45.797986 139907745949440 logging_writer.py:48] [17400] global_step=17400, grad_norm=5.715080738067627, loss=3.0877327919006348
I0130 22:33:19.608469 139907737556736 logging_writer.py:48] [17500] global_step=17500, grad_norm=3.827378511428833, loss=2.978365182876587
I0130 22:33:53.408081 139907745949440 logging_writer.py:48] [17600] global_step=17600, grad_norm=3.7317471504211426, loss=2.9460930824279785
I0130 22:34:27.234900 139907737556736 logging_writer.py:48] [17700] global_step=17700, grad_norm=4.865779876708984, loss=2.8881523609161377
I0130 22:35:01.072579 139907745949440 logging_writer.py:48] [17800] global_step=17800, grad_norm=5.139481544494629, loss=2.994751453399658
I0130 22:35:34.809112 139907737556736 logging_writer.py:48] [17900] global_step=17900, grad_norm=4.04380989074707, loss=3.0642380714416504
I0130 22:36:08.604687 139907745949440 logging_writer.py:48] [18000] global_step=18000, grad_norm=4.07263708114624, loss=3.0197575092315674
I0130 22:36:39.772452 140070692116288 spec.py:321] Evaluating on the training split.
I0130 22:36:48.020900 140070692116288 spec.py:333] Evaluating on the validation split.
I0130 22:37:02.829670 140070692116288 spec.py:349] Evaluating on the test split.
I0130 22:37:04.954715 140070692116288 submission_runner.py:408] Time since start: 6453.58s, 	Step: 18094, 	{'train/accuracy': 0.6030970811843872, 'train/loss': 1.7894777059555054, 'validation/accuracy': 0.5586400032043457, 'validation/loss': 1.985974907875061, 'validation/num_examples': 50000, 'test/accuracy': 0.43950003385543823, 'test/loss': 2.643144369125366, 'test/num_examples': 10000, 'score': 6176.272500514984, 'total_duration': 6453.581561326981, 'accumulated_submission_time': 6176.272500514984, 'accumulated_eval_time': 276.3594694137573, 'accumulated_logging_time': 0.34600257873535156}
I0130 22:37:04.984182 139907737556736 logging_writer.py:48] [18094] accumulated_eval_time=276.359469, accumulated_logging_time=0.346003, accumulated_submission_time=6176.272501, global_step=18094, preemption_count=0, score=6176.272501, test/accuracy=0.439500, test/loss=2.643144, test/num_examples=10000, total_duration=6453.581561, train/accuracy=0.603097, train/loss=1.789478, validation/accuracy=0.558640, validation/loss=1.985975, validation/num_examples=50000
I0130 22:37:07.358828 139907745949440 logging_writer.py:48] [18100] global_step=18100, grad_norm=3.900740146636963, loss=3.0778019428253174
I0130 22:37:41.054409 139907737556736 logging_writer.py:48] [18200] global_step=18200, grad_norm=4.121669769287109, loss=2.96988582611084
I0130 22:38:14.774990 139907745949440 logging_writer.py:48] [18300] global_step=18300, grad_norm=3.117478847503662, loss=3.016904354095459
I0130 22:38:48.559853 139907737556736 logging_writer.py:48] [18400] global_step=18400, grad_norm=3.0446338653564453, loss=3.115323543548584
I0130 22:39:22.294924 139907745949440 logging_writer.py:48] [18500] global_step=18500, grad_norm=4.068877220153809, loss=3.015010118484497
I0130 22:39:56.083085 139907737556736 logging_writer.py:48] [18600] global_step=18600, grad_norm=3.7942259311676025, loss=2.985048532485962
I0130 22:40:29.809281 139907745949440 logging_writer.py:48] [18700] global_step=18700, grad_norm=5.3303542137146, loss=2.95920729637146
I0130 22:41:03.630841 139907737556736 logging_writer.py:48] [18800] global_step=18800, grad_norm=2.964934825897217, loss=2.950697422027588
I0130 22:41:37.422145 139907745949440 logging_writer.py:48] [18900] global_step=18900, grad_norm=3.2232463359832764, loss=2.9608500003814697
I0130 22:42:11.202353 139907737556736 logging_writer.py:48] [19000] global_step=19000, grad_norm=2.9501123428344727, loss=2.9542112350463867
I0130 22:42:44.966910 139907745949440 logging_writer.py:48] [19100] global_step=19100, grad_norm=2.913191556930542, loss=2.9688992500305176
I0130 22:43:18.755922 139907737556736 logging_writer.py:48] [19200] global_step=19200, grad_norm=3.920760154724121, loss=2.96108078956604
I0130 22:43:52.528851 139907745949440 logging_writer.py:48] [19300] global_step=19300, grad_norm=5.4088897705078125, loss=3.018075942993164
I0130 22:44:26.303346 139907737556736 logging_writer.py:48] [19400] global_step=19400, grad_norm=2.8021047115325928, loss=3.038693904876709
I0130 22:45:00.091729 139907745949440 logging_writer.py:48] [19500] global_step=19500, grad_norm=3.14341139793396, loss=2.928044080734253
I0130 22:45:33.891281 139907737556736 logging_writer.py:48] [19600] global_step=19600, grad_norm=5.3591108322143555, loss=2.9592788219451904
I0130 22:45:35.015452 140070692116288 spec.py:321] Evaluating on the training split.
I0130 22:45:43.092456 140070692116288 spec.py:333] Evaluating on the validation split.
I0130 22:45:57.078787 140070692116288 spec.py:349] Evaluating on the test split.
I0130 22:45:59.234029 140070692116288 submission_runner.py:408] Time since start: 6987.86s, 	Step: 19605, 	{'train/accuracy': 0.6378746628761292, 'train/loss': 1.6028066873550415, 'validation/accuracy': 0.5719999670982361, 'validation/loss': 1.902109146118164, 'validation/num_examples': 50000, 'test/accuracy': 0.4588000178337097, 'test/loss': 2.5408897399902344, 'test/num_examples': 10000, 'score': 6686.243889808655, 'total_duration': 6987.860915660858, 'accumulated_submission_time': 6686.243889808655, 'accumulated_eval_time': 300.57800698280334, 'accumulated_logging_time': 0.38440513610839844}
I0130 22:45:59.252294 139908576417536 logging_writer.py:48] [19605] accumulated_eval_time=300.578007, accumulated_logging_time=0.384405, accumulated_submission_time=6686.243890, global_step=19605, preemption_count=0, score=6686.243890, test/accuracy=0.458800, test/loss=2.540890, test/num_examples=10000, total_duration=6987.860916, train/accuracy=0.637875, train/loss=1.602807, validation/accuracy=0.572000, validation/loss=1.902109, validation/num_examples=50000
I0130 22:46:31.563140 139908584810240 logging_writer.py:48] [19700] global_step=19700, grad_norm=3.3465869426727295, loss=3.0850231647491455
I0130 22:47:05.282579 139908576417536 logging_writer.py:48] [19800] global_step=19800, grad_norm=3.3525619506835938, loss=2.849966287612915
I0130 22:47:39.043776 139908584810240 logging_writer.py:48] [19900] global_step=19900, grad_norm=2.731386661529541, loss=2.812202215194702
I0130 22:48:12.775593 139908576417536 logging_writer.py:48] [20000] global_step=20000, grad_norm=2.44033145904541, loss=2.949302911758423
I0130 22:48:46.563485 139908584810240 logging_writer.py:48] [20100] global_step=20100, grad_norm=3.1260979175567627, loss=2.906287670135498
I0130 22:49:20.277929 139908576417536 logging_writer.py:48] [20200] global_step=20200, grad_norm=3.7157938480377197, loss=3.0435218811035156
I0130 22:49:54.047126 139908584810240 logging_writer.py:48] [20300] global_step=20300, grad_norm=3.9955756664276123, loss=2.9255659580230713
I0130 22:50:27.784340 139908576417536 logging_writer.py:48] [20400] global_step=20400, grad_norm=4.242244720458984, loss=3.025564670562744
I0130 22:51:01.552435 139908584810240 logging_writer.py:48] [20500] global_step=20500, grad_norm=3.842137575149536, loss=2.909917116165161
I0130 22:51:35.361760 139908576417536 logging_writer.py:48] [20600] global_step=20600, grad_norm=2.360182285308838, loss=2.871412754058838
I0130 22:52:09.128208 139908584810240 logging_writer.py:48] [20700] global_step=20700, grad_norm=3.3458855152130127, loss=2.844799518585205
I0130 22:52:42.870180 139908576417536 logging_writer.py:48] [20800] global_step=20800, grad_norm=3.6621415615081787, loss=2.9714436531066895
I0130 22:53:16.692492 139908584810240 logging_writer.py:48] [20900] global_step=20900, grad_norm=2.5957717895507812, loss=2.9860925674438477
I0130 22:53:50.493437 139908576417536 logging_writer.py:48] [21000] global_step=21000, grad_norm=4.0944061279296875, loss=2.9613089561462402
I0130 22:54:24.244386 139908584810240 logging_writer.py:48] [21100] global_step=21100, grad_norm=3.7316083908081055, loss=2.950840473175049
I0130 22:54:29.409849 140070692116288 spec.py:321] Evaluating on the training split.
I0130 22:54:37.517931 140070692116288 spec.py:333] Evaluating on the validation split.
I0130 22:54:51.279406 140070692116288 spec.py:349] Evaluating on the test split.
I0130 22:54:53.411047 140070692116288 submission_runner.py:408] Time since start: 7522.04s, 	Step: 21117, 	{'train/accuracy': 0.6107900142669678, 'train/loss': 1.6893270015716553, 'validation/accuracy': 0.5640599727630615, 'validation/loss': 1.925768256187439, 'validation/num_examples': 50000, 'test/accuracy': 0.43890002369880676, 'test/loss': 2.6041297912597656, 'test/num_examples': 10000, 'score': 7196.342430591583, 'total_duration': 7522.037932395935, 'accumulated_submission_time': 7196.342430591583, 'accumulated_eval_time': 324.5791804790497, 'accumulated_logging_time': 0.41138768196105957}
I0130 22:54:53.438701 139907712378624 logging_writer.py:48] [21117] accumulated_eval_time=324.579180, accumulated_logging_time=0.411388, accumulated_submission_time=7196.342431, global_step=21117, preemption_count=0, score=7196.342431, test/accuracy=0.438900, test/loss=2.604130, test/num_examples=10000, total_duration=7522.037932, train/accuracy=0.610790, train/loss=1.689327, validation/accuracy=0.564060, validation/loss=1.925768, validation/num_examples=50000
I0130 22:55:21.762474 139907720771328 logging_writer.py:48] [21200] global_step=21200, grad_norm=3.8900656700134277, loss=2.908250570297241
I0130 22:55:55.474654 139907712378624 logging_writer.py:48] [21300] global_step=21300, grad_norm=4.849586486816406, loss=3.049866199493408
I0130 22:56:29.235621 139907720771328 logging_writer.py:48] [21400] global_step=21400, grad_norm=4.336041450500488, loss=2.841507911682129
I0130 22:57:02.968933 139907712378624 logging_writer.py:48] [21500] global_step=21500, grad_norm=3.280256748199463, loss=2.9121203422546387
I0130 22:57:36.734528 139907720771328 logging_writer.py:48] [21600] global_step=21600, grad_norm=3.0691301822662354, loss=2.893933057785034
I0130 22:58:10.477106 139907712378624 logging_writer.py:48] [21700] global_step=21700, grad_norm=3.2831552028656006, loss=2.903326988220215
I0130 22:58:44.260042 139907720771328 logging_writer.py:48] [21800] global_step=21800, grad_norm=3.5898313522338867, loss=2.956815719604492
I0130 22:59:18.056487 139907712378624 logging_writer.py:48] [21900] global_step=21900, grad_norm=3.0289204120635986, loss=2.967768907546997
I0130 22:59:51.842783 139907720771328 logging_writer.py:48] [22000] global_step=22000, grad_norm=3.3812928199768066, loss=2.903428316116333
I0130 23:00:25.626354 139907712378624 logging_writer.py:48] [22100] global_step=22100, grad_norm=4.456884860992432, loss=2.9044017791748047
I0130 23:00:59.405431 139907720771328 logging_writer.py:48] [22200] global_step=22200, grad_norm=3.4588215351104736, loss=2.997668743133545
I0130 23:01:33.122925 139907712378624 logging_writer.py:48] [22300] global_step=22300, grad_norm=3.628084421157837, loss=2.8300046920776367
I0130 23:02:06.898765 139907720771328 logging_writer.py:48] [22400] global_step=22400, grad_norm=3.227476119995117, loss=2.9723458290100098
I0130 23:02:40.621468 139907712378624 logging_writer.py:48] [22500] global_step=22500, grad_norm=3.4849822521209717, loss=2.8673980236053467
I0130 23:03:14.400567 139907720771328 logging_writer.py:48] [22600] global_step=22600, grad_norm=2.922652244567871, loss=2.899305582046509
I0130 23:03:23.611908 140070692116288 spec.py:321] Evaluating on the training split.
I0130 23:03:31.567049 140070692116288 spec.py:333] Evaluating on the validation split.
I0130 23:03:46.086413 140070692116288 spec.py:349] Evaluating on the test split.
I0130 23:03:48.199765 140070692116288 submission_runner.py:408] Time since start: 8056.83s, 	Step: 22629, 	{'train/accuracy': 0.6161710619926453, 'train/loss': 1.6690157651901245, 'validation/accuracy': 0.5745599865913391, 'validation/loss': 1.873211145401001, 'validation/num_examples': 50000, 'test/accuracy': 0.45810002088546753, 'test/loss': 2.543734312057495, 'test/num_examples': 10000, 'score': 7706.454968452454, 'total_duration': 8056.826657772064, 'accumulated_submission_time': 7706.454968452454, 'accumulated_eval_time': 349.16700863838196, 'accumulated_logging_time': 0.44753503799438477}
I0130 23:03:48.217288 139906886108928 logging_writer.py:48] [22629] accumulated_eval_time=349.167009, accumulated_logging_time=0.447535, accumulated_submission_time=7706.454968, global_step=22629, preemption_count=0, score=7706.454968, test/accuracy=0.458100, test/loss=2.543734, test/num_examples=10000, total_duration=8056.826658, train/accuracy=0.616171, train/loss=1.669016, validation/accuracy=0.574560, validation/loss=1.873211, validation/num_examples=50000
I0130 23:04:12.496658 139906949052160 logging_writer.py:48] [22700] global_step=22700, grad_norm=2.6565356254577637, loss=2.8874330520629883
I0130 23:04:46.185922 139906886108928 logging_writer.py:48] [22800] global_step=22800, grad_norm=2.6048614978790283, loss=2.957247734069824
I0130 23:05:19.899946 139906949052160 logging_writer.py:48] [22900] global_step=22900, grad_norm=3.0502824783325195, loss=2.846381187438965
I0130 23:05:53.846248 139906886108928 logging_writer.py:48] [23000] global_step=23000, grad_norm=2.964165687561035, loss=2.958662509918213
I0130 23:06:27.853197 139906949052160 logging_writer.py:48] [23100] global_step=23100, grad_norm=2.5972886085510254, loss=2.9411299228668213
I0130 23:07:06.126953 139906886108928 logging_writer.py:48] [23200] global_step=23200, grad_norm=4.200104713439941, loss=2.890967845916748
I0130 23:07:39.885738 139906949052160 logging_writer.py:48] [23300] global_step=23300, grad_norm=4.595045566558838, loss=2.9338157176971436
I0130 23:08:13.685205 139906886108928 logging_writer.py:48] [23400] global_step=23400, grad_norm=2.9503228664398193, loss=2.81378173828125
I0130 23:08:47.434373 139906949052160 logging_writer.py:48] [23500] global_step=23500, grad_norm=3.9350192546844482, loss=2.882626533508301
I0130 23:09:21.192059 139906886108928 logging_writer.py:48] [23600] global_step=23600, grad_norm=2.5500166416168213, loss=2.9005613327026367
I0130 23:09:54.966161 139906949052160 logging_writer.py:48] [23700] global_step=23700, grad_norm=3.1841554641723633, loss=2.9634082317352295
I0130 23:10:28.658598 139906886108928 logging_writer.py:48] [23800] global_step=23800, grad_norm=3.6744630336761475, loss=2.8746602535247803
I0130 23:11:02.423241 139906949052160 logging_writer.py:48] [23900] global_step=23900, grad_norm=2.6956677436828613, loss=2.8796842098236084
I0130 23:11:36.153205 139906886108928 logging_writer.py:48] [24000] global_step=24000, grad_norm=3.2989823818206787, loss=2.9617812633514404
I0130 23:12:09.987037 139906949052160 logging_writer.py:48] [24100] global_step=24100, grad_norm=3.3057096004486084, loss=2.9119486808776855
I0130 23:12:18.206673 140070692116288 spec.py:321] Evaluating on the training split.
I0130 23:12:26.249908 140070692116288 spec.py:333] Evaluating on the validation split.
I0130 23:12:40.683965 140070692116288 spec.py:349] Evaluating on the test split.
I0130 23:12:42.854008 140070692116288 submission_runner.py:408] Time since start: 8591.48s, 	Step: 24126, 	{'train/accuracy': 0.634785532951355, 'train/loss': 1.5688326358795166, 'validation/accuracy': 0.5852999687194824, 'validation/loss': 1.8004077672958374, 'validation/num_examples': 50000, 'test/accuracy': 0.46480002999305725, 'test/loss': 2.4927031993865967, 'test/num_examples': 10000, 'score': 8216.385123491287, 'total_duration': 8591.48088502884, 'accumulated_submission_time': 8216.385123491287, 'accumulated_eval_time': 373.81429505348206, 'accumulated_logging_time': 0.4732851982116699}
I0130 23:12:42.872027 139906886108928 logging_writer.py:48] [24126] accumulated_eval_time=373.814295, accumulated_logging_time=0.473285, accumulated_submission_time=8216.385123, global_step=24126, preemption_count=0, score=8216.385123, test/accuracy=0.464800, test/loss=2.492703, test/num_examples=10000, total_duration=8591.480885, train/accuracy=0.634786, train/loss=1.568833, validation/accuracy=0.585300, validation/loss=1.800408, validation/num_examples=50000
I0130 23:13:08.179240 139906949052160 logging_writer.py:48] [24200] global_step=24200, grad_norm=3.0248777866363525, loss=2.896919012069702
I0130 23:13:41.857435 139906886108928 logging_writer.py:48] [24300] global_step=24300, grad_norm=3.23412823677063, loss=2.8234968185424805
I0130 23:14:15.562227 139906949052160 logging_writer.py:48] [24400] global_step=24400, grad_norm=2.889310598373413, loss=2.9112629890441895
I0130 23:14:49.357035 139906886108928 logging_writer.py:48] [24500] global_step=24500, grad_norm=3.4817357063293457, loss=2.9378135204315186
I0130 23:15:23.075285 139906949052160 logging_writer.py:48] [24600] global_step=24600, grad_norm=3.157707929611206, loss=2.888625383377075
I0130 23:15:56.867324 139906886108928 logging_writer.py:48] [24700] global_step=24700, grad_norm=3.047344207763672, loss=2.7515206336975098
I0130 23:16:30.598238 139906949052160 logging_writer.py:48] [24800] global_step=24800, grad_norm=4.1212897300720215, loss=2.9284603595733643
I0130 23:17:04.334195 139906886108928 logging_writer.py:48] [24900] global_step=24900, grad_norm=3.3943185806274414, loss=2.9856674671173096
I0130 23:17:38.111572 139906949052160 logging_writer.py:48] [25000] global_step=25000, grad_norm=3.7116127014160156, loss=2.8929107189178467
I0130 23:18:11.858229 139906886108928 logging_writer.py:48] [25100] global_step=25100, grad_norm=3.065967082977295, loss=2.8305821418762207
I0130 23:18:45.696218 139906949052160 logging_writer.py:48] [25200] global_step=25200, grad_norm=4.1840500831604, loss=2.859692096710205
I0130 23:19:19.441623 139906886108928 logging_writer.py:48] [25300] global_step=25300, grad_norm=3.8640823364257812, loss=2.8443541526794434
I0130 23:19:53.127943 139906949052160 logging_writer.py:48] [25400] global_step=25400, grad_norm=3.5600337982177734, loss=2.888382911682129
I0130 23:20:26.920447 139906886108928 logging_writer.py:48] [25500] global_step=25500, grad_norm=3.419616460800171, loss=2.8932721614837646
I0130 23:21:00.656092 139906949052160 logging_writer.py:48] [25600] global_step=25600, grad_norm=2.967087507247925, loss=2.883894443511963
I0130 23:21:12.909908 140070692116288 spec.py:321] Evaluating on the training split.
I0130 23:21:21.140161 140070692116288 spec.py:333] Evaluating on the validation split.
I0130 23:21:34.785393 140070692116288 spec.py:349] Evaluating on the test split.
I0130 23:21:36.963515 140070692116288 submission_runner.py:408] Time since start: 9125.59s, 	Step: 25638, 	{'train/accuracy': 0.6300820708274841, 'train/loss': 1.6126160621643066, 'validation/accuracy': 0.5874999761581421, 'validation/loss': 1.8088067770004272, 'validation/num_examples': 50000, 'test/accuracy': 0.4674000144004822, 'test/loss': 2.484600067138672, 'test/num_examples': 10000, 'score': 8726.36254477501, 'total_duration': 9125.590382575989, 'accumulated_submission_time': 8726.36254477501, 'accumulated_eval_time': 397.86784505844116, 'accumulated_logging_time': 0.5002624988555908}
I0130 23:21:36.996984 139906886108928 logging_writer.py:48] [25638] accumulated_eval_time=397.867845, accumulated_logging_time=0.500262, accumulated_submission_time=8726.362545, global_step=25638, preemption_count=0, score=8726.362545, test/accuracy=0.467400, test/loss=2.484600, test/num_examples=10000, total_duration=9125.590383, train/accuracy=0.630082, train/loss=1.612616, validation/accuracy=0.587500, validation/loss=1.808807, validation/num_examples=50000
I0130 23:21:58.178811 139907720771328 logging_writer.py:48] [25700] global_step=25700, grad_norm=3.2728774547576904, loss=2.777838706970215
I0130 23:22:31.822307 139906886108928 logging_writer.py:48] [25800] global_step=25800, grad_norm=3.158235549926758, loss=2.8459465503692627
I0130 23:23:05.565347 139907720771328 logging_writer.py:48] [25900] global_step=25900, grad_norm=3.274977684020996, loss=2.8389945030212402
I0130 23:23:39.318207 139906886108928 logging_writer.py:48] [26000] global_step=26000, grad_norm=3.029996395111084, loss=2.96244740486145
I0130 23:24:13.078137 139907720771328 logging_writer.py:48] [26100] global_step=26100, grad_norm=4.630677700042725, loss=2.9041171073913574
I0130 23:24:46.876670 139906886108928 logging_writer.py:48] [26200] global_step=26200, grad_norm=4.095739364624023, loss=2.8167190551757812
I0130 23:25:20.545548 139907720771328 logging_writer.py:48] [26300] global_step=26300, grad_norm=3.170938014984131, loss=2.925973653793335
I0130 23:25:54.298448 139906886108928 logging_writer.py:48] [26400] global_step=26400, grad_norm=3.4882564544677734, loss=2.8583762645721436
I0130 23:26:28.021748 139907720771328 logging_writer.py:48] [26500] global_step=26500, grad_norm=2.910642623901367, loss=2.9250683784484863
I0130 23:27:01.771668 139906886108928 logging_writer.py:48] [26600] global_step=26600, grad_norm=2.960646152496338, loss=2.8382976055145264
I0130 23:27:35.496489 139907720771328 logging_writer.py:48] [26700] global_step=26700, grad_norm=3.125856399536133, loss=2.7545857429504395
I0130 23:28:09.200906 139906886108928 logging_writer.py:48] [26800] global_step=26800, grad_norm=3.647761344909668, loss=2.828968048095703
I0130 23:28:42.980943 139907720771328 logging_writer.py:48] [26900] global_step=26900, grad_norm=2.9465222358703613, loss=2.8274219036102295
I0130 23:29:16.736014 139906886108928 logging_writer.py:48] [27000] global_step=27000, grad_norm=2.957073211669922, loss=2.9149513244628906
I0130 23:29:50.446797 139907720771328 logging_writer.py:48] [27100] global_step=27100, grad_norm=4.3737473487854, loss=2.9901039600372314
I0130 23:30:07.047864 140070692116288 spec.py:321] Evaluating on the training split.
I0130 23:30:15.361106 140070692116288 spec.py:333] Evaluating on the validation split.
I0130 23:30:28.413893 140070692116288 spec.py:349] Evaluating on the test split.
I0130 23:30:30.610031 140070692116288 submission_runner.py:408] Time since start: 9659.24s, 	Step: 27151, 	{'train/accuracy': 0.6246811151504517, 'train/loss': 1.6491903066635132, 'validation/accuracy': 0.5797399878501892, 'validation/loss': 1.8536149263381958, 'validation/num_examples': 50000, 'test/accuracy': 0.4571000337600708, 'test/loss': 2.5513482093811035, 'test/num_examples': 10000, 'score': 9236.353283643723, 'total_duration': 9659.236914873123, 'accumulated_submission_time': 9236.353283643723, 'accumulated_eval_time': 421.4299862384796, 'accumulated_logging_time': 0.5421044826507568}
I0130 23:30:30.627737 139907703985920 logging_writer.py:48] [27151] accumulated_eval_time=421.429986, accumulated_logging_time=0.542104, accumulated_submission_time=9236.353284, global_step=27151, preemption_count=0, score=9236.353284, test/accuracy=0.457100, test/loss=2.551348, test/num_examples=10000, total_duration=9659.236915, train/accuracy=0.624681, train/loss=1.649190, validation/accuracy=0.579740, validation/loss=1.853615, validation/num_examples=50000
I0130 23:30:47.734143 139907712378624 logging_writer.py:48] [27200] global_step=27200, grad_norm=2.8516643047332764, loss=2.784825563430786
I0130 23:31:21.409871 139907703985920 logging_writer.py:48] [27300] global_step=27300, grad_norm=3.3796427249908447, loss=2.8516385555267334
I0130 23:31:55.115356 139907712378624 logging_writer.py:48] [27400] global_step=27400, grad_norm=3.7383034229278564, loss=2.854379415512085
I0130 23:32:28.796845 139907703985920 logging_writer.py:48] [27500] global_step=27500, grad_norm=3.1233444213867188, loss=2.750572443008423
I0130 23:33:02.584676 139907712378624 logging_writer.py:48] [27600] global_step=27600, grad_norm=3.1579160690307617, loss=2.819352626800537
I0130 23:33:36.360281 139907703985920 logging_writer.py:48] [27700] global_step=27700, grad_norm=2.9015560150146484, loss=2.814974069595337
I0130 23:34:10.072352 139907712378624 logging_writer.py:48] [27800] global_step=27800, grad_norm=2.7096259593963623, loss=2.9074738025665283
I0130 23:34:43.819405 139907703985920 logging_writer.py:48] [27900] global_step=27900, grad_norm=2.862985610961914, loss=2.848994493484497
I0130 23:35:17.547344 139907712378624 logging_writer.py:48] [28000] global_step=28000, grad_norm=3.2827672958374023, loss=2.837439775466919
I0130 23:35:51.276107 139907703985920 logging_writer.py:48] [28100] global_step=28100, grad_norm=2.9229111671447754, loss=2.851781129837036
I0130 23:36:25.029926 139907712378624 logging_writer.py:48] [28200] global_step=28200, grad_norm=3.718132734298706, loss=2.8980798721313477
I0130 23:36:58.725875 139907703985920 logging_writer.py:48] [28300] global_step=28300, grad_norm=2.5199944972991943, loss=2.9019031524658203
I0130 23:37:32.481907 139907712378624 logging_writer.py:48] [28400] global_step=28400, grad_norm=3.8677096366882324, loss=2.8365254402160645
I0130 23:38:06.215681 139907703985920 logging_writer.py:48] [28500] global_step=28500, grad_norm=2.927008628845215, loss=2.8414177894592285
I0130 23:38:39.989386 139907712378624 logging_writer.py:48] [28600] global_step=28600, grad_norm=3.168689250946045, loss=2.9271068572998047
I0130 23:39:00.683696 140070692116288 spec.py:321] Evaluating on the training split.
I0130 23:39:09.201446 140070692116288 spec.py:333] Evaluating on the validation split.
I0130 23:39:23.811914 140070692116288 spec.py:349] Evaluating on the test split.
I0130 23:39:25.930611 140070692116288 submission_runner.py:408] Time since start: 10194.56s, 	Step: 28663, 	{'train/accuracy': 0.6515266299247742, 'train/loss': 1.5246480703353882, 'validation/accuracy': 0.590399980545044, 'validation/loss': 1.8066246509552002, 'validation/num_examples': 50000, 'test/accuracy': 0.4710000157356262, 'test/loss': 2.4751579761505127, 'test/num_examples': 10000, 'score': 9746.119905948639, 'total_duration': 10194.557493686676, 'accumulated_submission_time': 9746.119905948639, 'accumulated_eval_time': 446.6768915653229, 'accumulated_logging_time': 0.7980978488922119}
I0130 23:39:25.949435 139907729164032 logging_writer.py:48] [28663] accumulated_eval_time=446.676892, accumulated_logging_time=0.798098, accumulated_submission_time=9746.119906, global_step=28663, preemption_count=0, score=9746.119906, test/accuracy=0.471000, test/loss=2.475158, test/num_examples=10000, total_duration=10194.557494, train/accuracy=0.651527, train/loss=1.524648, validation/accuracy=0.590400, validation/loss=1.806625, validation/num_examples=50000
I0130 23:39:38.774358 139907737556736 logging_writer.py:48] [28700] global_step=28700, grad_norm=4.188605308532715, loss=2.8307406902313232
I0130 23:40:12.417727 139907729164032 logging_writer.py:48] [28800] global_step=28800, grad_norm=2.544713020324707, loss=2.788191795349121
I0130 23:40:46.135366 139907737556736 logging_writer.py:48] [28900] global_step=28900, grad_norm=3.7224652767181396, loss=2.7777557373046875
I0130 23:41:19.833875 139907729164032 logging_writer.py:48] [29000] global_step=29000, grad_norm=3.146005630493164, loss=2.859842300415039
I0130 23:41:53.630957 139907737556736 logging_writer.py:48] [29100] global_step=29100, grad_norm=3.105090618133545, loss=2.878795623779297
I0130 23:42:27.314228 139907729164032 logging_writer.py:48] [29200] global_step=29200, grad_norm=2.9311671257019043, loss=2.869091510772705
I0130 23:43:01.126865 139907737556736 logging_writer.py:48] [29300] global_step=29300, grad_norm=3.1972193717956543, loss=2.884903907775879
I0130 23:43:34.888961 139907729164032 logging_writer.py:48] [29400] global_step=29400, grad_norm=4.2383928298950195, loss=2.820575714111328
I0130 23:44:08.618953 139907737556736 logging_writer.py:48] [29500] global_step=29500, grad_norm=3.426870346069336, loss=2.909870147705078
I0130 23:44:42.288390 139907729164032 logging_writer.py:48] [29600] global_step=29600, grad_norm=3.64418888092041, loss=2.8681998252868652
I0130 23:45:15.984811 139907737556736 logging_writer.py:48] [29700] global_step=29700, grad_norm=2.689291477203369, loss=2.708604335784912
I0130 23:45:49.733235 139907729164032 logging_writer.py:48] [29800] global_step=29800, grad_norm=3.5561070442199707, loss=2.876893997192383
I0130 23:46:23.430226 139907737556736 logging_writer.py:48] [29900] global_step=29900, grad_norm=3.2142152786254883, loss=2.799649238586426
I0130 23:46:57.134406 139907729164032 logging_writer.py:48] [30000] global_step=30000, grad_norm=2.6440021991729736, loss=2.8347997665405273
I0130 23:47:30.874337 139907737556736 logging_writer.py:48] [30100] global_step=30100, grad_norm=2.8583035469055176, loss=2.8125102519989014
I0130 23:47:55.957798 140070692116288 spec.py:321] Evaluating on the training split.
I0130 23:48:04.256006 140070692116288 spec.py:333] Evaluating on the validation split.
I0130 23:48:18.880050 140070692116288 spec.py:349] Evaluating on the test split.
I0130 23:48:21.005886 140070692116288 submission_runner.py:408] Time since start: 10729.63s, 	Step: 30176, 	{'train/accuracy': 0.6483777165412903, 'train/loss': 1.5167217254638672, 'validation/accuracy': 0.5963599681854248, 'validation/loss': 1.7699456214904785, 'validation/num_examples': 50000, 'test/accuracy': 0.47130003571510315, 'test/loss': 2.4669439792633057, 'test/num_examples': 10000, 'score': 10256.066632509232, 'total_duration': 10729.63278746605, 'accumulated_submission_time': 10256.066632509232, 'accumulated_eval_time': 471.7249677181244, 'accumulated_logging_time': 0.8278708457946777}
I0130 23:48:21.025192 139907703985920 logging_writer.py:48] [30176] accumulated_eval_time=471.724968, accumulated_logging_time=0.827871, accumulated_submission_time=10256.066633, global_step=30176, preemption_count=0, score=10256.066633, test/accuracy=0.471300, test/loss=2.466944, test/num_examples=10000, total_duration=10729.632787, train/accuracy=0.648378, train/loss=1.516722, validation/accuracy=0.596360, validation/loss=1.769946, validation/num_examples=50000
I0130 23:48:29.449181 139907712378624 logging_writer.py:48] [30200] global_step=30200, grad_norm=3.3444836139678955, loss=2.8795764446258545
I0130 23:49:03.090080 139907703985920 logging_writer.py:48] [30300] global_step=30300, grad_norm=3.71866512298584, loss=2.8940067291259766
I0130 23:49:36.800262 139907712378624 logging_writer.py:48] [30400] global_step=30400, grad_norm=3.5962131023406982, loss=2.9539685249328613
I0130 23:50:10.541440 139907703985920 logging_writer.py:48] [30500] global_step=30500, grad_norm=3.13427734375, loss=2.861215114593506
I0130 23:50:44.316935 139907712378624 logging_writer.py:48] [30600] global_step=30600, grad_norm=3.1745896339416504, loss=2.746535539627075
I0130 23:51:18.020207 139907703985920 logging_writer.py:48] [30700] global_step=30700, grad_norm=3.269388198852539, loss=2.8031861782073975
I0130 23:51:51.777614 139907712378624 logging_writer.py:48] [30800] global_step=30800, grad_norm=2.6533143520355225, loss=2.890658140182495
I0130 23:52:25.496836 139907703985920 logging_writer.py:48] [30900] global_step=30900, grad_norm=2.967766523361206, loss=2.734750270843506
I0130 23:52:59.270712 139907712378624 logging_writer.py:48] [31000] global_step=31000, grad_norm=2.951876163482666, loss=2.8774843215942383
I0130 23:53:32.965462 139907703985920 logging_writer.py:48] [31100] global_step=31100, grad_norm=2.607673168182373, loss=2.8323731422424316
I0130 23:54:06.757253 139907712378624 logging_writer.py:48] [31200] global_step=31200, grad_norm=3.6144843101501465, loss=2.767277956008911
I0130 23:54:40.472399 139907703985920 logging_writer.py:48] [31300] global_step=31300, grad_norm=3.1666083335876465, loss=2.866546630859375
I0130 23:55:14.258778 139907712378624 logging_writer.py:48] [31400] global_step=31400, grad_norm=3.2862281799316406, loss=2.8765950202941895
I0130 23:55:48.028859 139907703985920 logging_writer.py:48] [31500] global_step=31500, grad_norm=3.4146440029144287, loss=2.8196582794189453
I0130 23:56:21.757762 139907712378624 logging_writer.py:48] [31600] global_step=31600, grad_norm=3.0686116218566895, loss=2.8050951957702637
I0130 23:56:51.224697 140070692116288 spec.py:321] Evaluating on the training split.
I0130 23:56:59.465759 140070692116288 spec.py:333] Evaluating on the validation split.
I0130 23:57:15.044347 140070692116288 spec.py:349] Evaluating on the test split.
I0130 23:57:17.235306 140070692116288 submission_runner.py:408] Time since start: 11265.86s, 	Step: 31689, 	{'train/accuracy': 0.6477000713348389, 'train/loss': 1.5327821969985962, 'validation/accuracy': 0.6036799550056458, 'validation/loss': 1.7479439973831177, 'validation/num_examples': 50000, 'test/accuracy': 0.47690001130104065, 'test/loss': 2.4278860092163086, 'test/num_examples': 10000, 'score': 10766.203356981277, 'total_duration': 11265.862180709839, 'accumulated_submission_time': 10766.203356981277, 'accumulated_eval_time': 497.73553681373596, 'accumulated_logging_time': 0.8581020832061768}
I0130 23:57:17.258732 139906756101888 logging_writer.py:48] [31689] accumulated_eval_time=497.735537, accumulated_logging_time=0.858102, accumulated_submission_time=10766.203357, global_step=31689, preemption_count=0, score=10766.203357, test/accuracy=0.476900, test/loss=2.427886, test/num_examples=10000, total_duration=11265.862181, train/accuracy=0.647700, train/loss=1.532782, validation/accuracy=0.603680, validation/loss=1.747944, validation/num_examples=50000
I0130 23:57:21.306839 139906949052160 logging_writer.py:48] [31700] global_step=31700, grad_norm=3.3540263175964355, loss=2.8904311656951904
I0130 23:57:55.018907 139906756101888 logging_writer.py:48] [31800] global_step=31800, grad_norm=3.787590742111206, loss=2.7794783115386963
I0130 23:58:28.713850 139906949052160 logging_writer.py:48] [31900] global_step=31900, grad_norm=3.7496979236602783, loss=2.9040257930755615
I0130 23:59:02.426716 139906756101888 logging_writer.py:48] [32000] global_step=32000, grad_norm=3.138946533203125, loss=2.8526833057403564
I0130 23:59:36.197007 139906949052160 logging_writer.py:48] [32100] global_step=32100, grad_norm=2.7459335327148438, loss=2.7262864112854004
I0131 00:00:09.907656 139906756101888 logging_writer.py:48] [32200] global_step=32200, grad_norm=3.09063720703125, loss=2.8639464378356934
I0131 00:00:43.678468 139906949052160 logging_writer.py:48] [32300] global_step=32300, grad_norm=2.7809455394744873, loss=2.772470474243164
I0131 00:01:17.398203 139906756101888 logging_writer.py:48] [32400] global_step=32400, grad_norm=2.778878927230835, loss=2.818089723587036
I0131 00:01:51.160377 139906949052160 logging_writer.py:48] [32500] global_step=32500, grad_norm=2.7987546920776367, loss=2.753610134124756
I0131 00:02:24.874333 139906756101888 logging_writer.py:48] [32600] global_step=32600, grad_norm=3.1658239364624023, loss=2.8571929931640625
I0131 00:02:58.640427 139906949052160 logging_writer.py:48] [32700] global_step=32700, grad_norm=2.9218618869781494, loss=2.841836929321289
I0131 00:03:32.329911 139906756101888 logging_writer.py:48] [32800] global_step=32800, grad_norm=2.611124277114868, loss=2.859934091567993
I0131 00:04:06.000825 139906949052160 logging_writer.py:48] [32900] global_step=32900, grad_norm=3.0408670902252197, loss=2.7928504943847656
I0131 00:04:39.742749 139906756101888 logging_writer.py:48] [33000] global_step=33000, grad_norm=2.7129058837890625, loss=2.896934986114502
I0131 00:05:13.412279 139906949052160 logging_writer.py:48] [33100] global_step=33100, grad_norm=3.034688949584961, loss=2.7639143466949463
I0131 00:05:47.091425 139906756101888 logging_writer.py:48] [33200] global_step=33200, grad_norm=2.6711487770080566, loss=2.8122763633728027
I0131 00:05:47.556704 140070692116288 spec.py:321] Evaluating on the training split.
I0131 00:05:55.743068 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 00:06:10.615776 140070692116288 spec.py:349] Evaluating on the test split.
I0131 00:06:12.755204 140070692116288 submission_runner.py:408] Time since start: 11801.38s, 	Step: 33203, 	{'train/accuracy': 0.6440529227256775, 'train/loss': 1.5270545482635498, 'validation/accuracy': 0.6020199656486511, 'validation/loss': 1.7369011640548706, 'validation/num_examples': 50000, 'test/accuracy': 0.4764000177383423, 'test/loss': 2.4197638034820557, 'test/num_examples': 10000, 'score': 11276.440303564072, 'total_duration': 11801.382081270218, 'accumulated_submission_time': 11276.440303564072, 'accumulated_eval_time': 522.9339916706085, 'accumulated_logging_time': 0.8919429779052734}
I0131 00:06:12.775306 139907712378624 logging_writer.py:48] [33203] accumulated_eval_time=522.933992, accumulated_logging_time=0.891943, accumulated_submission_time=11276.440304, global_step=33203, preemption_count=0, score=11276.440304, test/accuracy=0.476400, test/loss=2.419764, test/num_examples=10000, total_duration=11801.382081, train/accuracy=0.644053, train/loss=1.527055, validation/accuracy=0.602020, validation/loss=1.736901, validation/num_examples=50000
I0131 00:06:45.726968 139907737556736 logging_writer.py:48] [33300] global_step=33300, grad_norm=2.859069347381592, loss=2.8227100372314453
I0131 00:07:19.470268 139907712378624 logging_writer.py:48] [33400] global_step=33400, grad_norm=4.197963714599609, loss=2.8251163959503174
I0131 00:07:53.187394 139907737556736 logging_writer.py:48] [33500] global_step=33500, grad_norm=3.6374733448028564, loss=2.7847864627838135
I0131 00:08:26.963284 139907712378624 logging_writer.py:48] [33600] global_step=33600, grad_norm=4.092629909515381, loss=2.855700731277466
I0131 00:09:00.712427 139907737556736 logging_writer.py:48] [33700] global_step=33700, grad_norm=4.143304347991943, loss=2.8188130855560303
I0131 00:09:34.432145 139907712378624 logging_writer.py:48] [33800] global_step=33800, grad_norm=3.477182626724243, loss=2.85849666595459
I0131 00:10:08.200095 139907737556736 logging_writer.py:48] [33900] global_step=33900, grad_norm=3.286526918411255, loss=2.863131523132324
I0131 00:10:41.940208 139907712378624 logging_writer.py:48] [34000] global_step=34000, grad_norm=3.6567041873931885, loss=2.9278407096862793
I0131 00:11:15.683938 139907737556736 logging_writer.py:48] [34100] global_step=34100, grad_norm=3.3159310817718506, loss=2.7694265842437744
I0131 00:11:49.412475 139907712378624 logging_writer.py:48] [34200] global_step=34200, grad_norm=3.2582149505615234, loss=2.7913389205932617
I0131 00:12:23.159447 139907737556736 logging_writer.py:48] [34300] global_step=34300, grad_norm=2.975229501724243, loss=2.779116630554199
I0131 00:12:56.911512 139907712378624 logging_writer.py:48] [34400] global_step=34400, grad_norm=3.23591947555542, loss=2.7297630310058594
I0131 00:13:30.619657 139907737556736 logging_writer.py:48] [34500] global_step=34500, grad_norm=3.3311846256256104, loss=2.8946666717529297
I0131 00:14:04.379567 139907712378624 logging_writer.py:48] [34600] global_step=34600, grad_norm=4.713311672210693, loss=2.8119075298309326
I0131 00:14:38.263592 139907737556736 logging_writer.py:48] [34700] global_step=34700, grad_norm=3.153625011444092, loss=2.778770923614502
I0131 00:14:42.769715 140070692116288 spec.py:321] Evaluating on the training split.
I0131 00:14:51.273917 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 00:15:05.617185 140070692116288 spec.py:349] Evaluating on the test split.
I0131 00:15:07.896325 140070692116288 submission_runner.py:408] Time since start: 12336.52s, 	Step: 34715, 	{'train/accuracy': 0.6390106678009033, 'train/loss': 1.5380759239196777, 'validation/accuracy': 0.5995799899101257, 'validation/loss': 1.7401143312454224, 'validation/num_examples': 50000, 'test/accuracy': 0.4750000238418579, 'test/loss': 2.4321818351745605, 'test/num_examples': 10000, 'score': 11786.374200820923, 'total_duration': 12336.523176193237, 'accumulated_submission_time': 11786.374200820923, 'accumulated_eval_time': 548.0605285167694, 'accumulated_logging_time': 0.9209282398223877}
I0131 00:15:07.918492 139906827380480 logging_writer.py:48] [34715] accumulated_eval_time=548.060529, accumulated_logging_time=0.920928, accumulated_submission_time=11786.374201, global_step=34715, preemption_count=0, score=11786.374201, test/accuracy=0.475000, test/loss=2.432182, test/num_examples=10000, total_duration=12336.523176, train/accuracy=0.639011, train/loss=1.538076, validation/accuracy=0.599580, validation/loss=1.740114, validation/num_examples=50000
I0131 00:15:36.893369 139906835773184 logging_writer.py:48] [34800] global_step=34800, grad_norm=3.0651917457580566, loss=2.786876678466797
I0131 00:16:10.543231 139906827380480 logging_writer.py:48] [34900] global_step=34900, grad_norm=3.300354480743408, loss=2.780242919921875
I0131 00:16:44.260075 139906835773184 logging_writer.py:48] [35000] global_step=35000, grad_norm=3.054365634918213, loss=2.8331222534179688
I0131 00:17:17.966873 139906827380480 logging_writer.py:48] [35100] global_step=35100, grad_norm=3.455122947692871, loss=2.7576241493225098
I0131 00:17:51.743739 139906835773184 logging_writer.py:48] [35200] global_step=35200, grad_norm=2.8699662685394287, loss=2.853860855102539
I0131 00:18:25.412463 139906827380480 logging_writer.py:48] [35300] global_step=35300, grad_norm=2.847607135772705, loss=2.6560001373291016
I0131 00:18:59.180353 139906835773184 logging_writer.py:48] [35400] global_step=35400, grad_norm=2.8624730110168457, loss=2.660238742828369
I0131 00:19:32.909209 139906827380480 logging_writer.py:48] [35500] global_step=35500, grad_norm=3.552618980407715, loss=2.8936707973480225
I0131 00:20:06.673238 139906835773184 logging_writer.py:48] [35600] global_step=35600, grad_norm=3.6301496028900146, loss=2.889465808868408
I0131 00:20:40.465277 139906827380480 logging_writer.py:48] [35700] global_step=35700, grad_norm=3.009786367416382, loss=2.855288028717041
I0131 00:21:14.213466 139906835773184 logging_writer.py:48] [35800] global_step=35800, grad_norm=3.169154167175293, loss=2.805190324783325
I0131 00:21:47.899461 139906827380480 logging_writer.py:48] [35900] global_step=35900, grad_norm=3.1665937900543213, loss=2.7935307025909424
I0131 00:22:21.600001 139906835773184 logging_writer.py:48] [36000] global_step=36000, grad_norm=3.0630080699920654, loss=2.8244106769561768
I0131 00:22:55.306276 139906827380480 logging_writer.py:48] [36100] global_step=36100, grad_norm=3.0446648597717285, loss=2.8243820667266846
I0131 00:23:29.076558 139906835773184 logging_writer.py:48] [36200] global_step=36200, grad_norm=2.7827258110046387, loss=2.819840431213379
I0131 00:23:37.971472 140070692116288 spec.py:321] Evaluating on the training split.
I0131 00:23:46.473070 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 00:24:01.217475 140070692116288 spec.py:349] Evaluating on the test split.
I0131 00:24:03.560370 140070692116288 submission_runner.py:408] Time since start: 12872.19s, 	Step: 36228, 	{'train/accuracy': 0.6710578799247742, 'train/loss': 1.4383693933486938, 'validation/accuracy': 0.6104999780654907, 'validation/loss': 1.7170253992080688, 'validation/num_examples': 50000, 'test/accuracy': 0.4812000095844269, 'test/loss': 2.404179334640503, 'test/num_examples': 10000, 'score': 12296.366559743881, 'total_duration': 12872.187220096588, 'accumulated_submission_time': 12296.366559743881, 'accumulated_eval_time': 573.6493542194366, 'accumulated_logging_time': 0.9530339241027832}
I0131 00:24:03.584511 139906827380480 logging_writer.py:48] [36228] accumulated_eval_time=573.649354, accumulated_logging_time=0.953034, accumulated_submission_time=12296.366560, global_step=36228, preemption_count=0, score=12296.366560, test/accuracy=0.481200, test/loss=2.404179, test/num_examples=10000, total_duration=12872.187220, train/accuracy=0.671058, train/loss=1.438369, validation/accuracy=0.610500, validation/loss=1.717025, validation/num_examples=50000
I0131 00:24:28.220048 139907712378624 logging_writer.py:48] [36300] global_step=36300, grad_norm=2.897223472595215, loss=2.848869562149048
I0131 00:25:01.877932 139906827380480 logging_writer.py:48] [36400] global_step=36400, grad_norm=3.7136776447296143, loss=2.871591567993164
I0131 00:25:35.581097 139907712378624 logging_writer.py:48] [36500] global_step=36500, grad_norm=2.9434573650360107, loss=2.702498197555542
I0131 00:26:09.244179 139906827380480 logging_writer.py:48] [36600] global_step=36600, grad_norm=2.9733948707580566, loss=2.7948036193847656
I0131 00:26:43.064157 139907712378624 logging_writer.py:48] [36700] global_step=36700, grad_norm=3.7032361030578613, loss=2.7803611755371094
I0131 00:27:16.815981 139906827380480 logging_writer.py:48] [36800] global_step=36800, grad_norm=2.714067220687866, loss=2.7537436485290527
I0131 00:27:50.515966 139907712378624 logging_writer.py:48] [36900] global_step=36900, grad_norm=3.872591257095337, loss=2.955965042114258
I0131 00:28:24.261811 139906827380480 logging_writer.py:48] [37000] global_step=37000, grad_norm=2.9456543922424316, loss=2.7818329334259033
I0131 00:28:57.984446 139907712378624 logging_writer.py:48] [37100] global_step=37100, grad_norm=3.192927598953247, loss=2.7413928508758545
I0131 00:29:31.726051 139906827380480 logging_writer.py:48] [37200] global_step=37200, grad_norm=3.338559150695801, loss=2.858083724975586
I0131 00:30:05.498531 139907712378624 logging_writer.py:48] [37300] global_step=37300, grad_norm=3.2245311737060547, loss=2.713738441467285
I0131 00:30:39.169725 139906827380480 logging_writer.py:48] [37400] global_step=37400, grad_norm=3.3356804847717285, loss=2.745234966278076
I0131 00:31:12.872437 139907712378624 logging_writer.py:48] [37500] global_step=37500, grad_norm=2.8371453285217285, loss=2.7389187812805176
I0131 00:31:46.607822 139906827380480 logging_writer.py:48] [37600] global_step=37600, grad_norm=3.0671770572662354, loss=2.8796567916870117
I0131 00:32:20.325587 139907712378624 logging_writer.py:48] [37700] global_step=37700, grad_norm=3.4471435546875, loss=2.76090407371521
I0131 00:32:33.616217 140070692116288 spec.py:321] Evaluating on the training split.
I0131 00:32:41.388787 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 00:32:55.219105 140070692116288 spec.py:349] Evaluating on the test split.
I0131 00:32:57.329735 140070692116288 submission_runner.py:408] Time since start: 13405.96s, 	Step: 37741, 	{'train/accuracy': 0.6660754084587097, 'train/loss': 1.4482871294021606, 'validation/accuracy': 0.6075599789619446, 'validation/loss': 1.733280897140503, 'validation/num_examples': 50000, 'test/accuracy': 0.47540003061294556, 'test/loss': 2.427032232284546, 'test/num_examples': 10000, 'score': 12806.337379455566, 'total_duration': 13405.956619262695, 'accumulated_submission_time': 12806.337379455566, 'accumulated_eval_time': 597.3628311157227, 'accumulated_logging_time': 0.9872357845306396}
I0131 00:32:57.352184 139906827380480 logging_writer.py:48] [37741] accumulated_eval_time=597.362831, accumulated_logging_time=0.987236, accumulated_submission_time=12806.337379, global_step=37741, preemption_count=0, score=12806.337379, test/accuracy=0.475400, test/loss=2.427032, test/num_examples=10000, total_duration=13405.956619, train/accuracy=0.666075, train/loss=1.448287, validation/accuracy=0.607560, validation/loss=1.733281, validation/num_examples=50000
I0131 00:33:17.608101 139907703985920 logging_writer.py:48] [37800] global_step=37800, grad_norm=4.027946472167969, loss=2.7708139419555664
I0131 00:33:51.257597 139906827380480 logging_writer.py:48] [37900] global_step=37900, grad_norm=3.049967050552368, loss=2.857712984085083
I0131 00:34:24.956363 139907703985920 logging_writer.py:48] [38000] global_step=38000, grad_norm=3.08707594871521, loss=2.8035480976104736
I0131 00:34:58.623260 139906827380480 logging_writer.py:48] [38100] global_step=38100, grad_norm=3.4969000816345215, loss=2.7843825817108154
I0131 00:35:32.317753 139907703985920 logging_writer.py:48] [38200] global_step=38200, grad_norm=2.7290191650390625, loss=2.7602880001068115
I0131 00:36:05.983381 139906827380480 logging_writer.py:48] [38300] global_step=38300, grad_norm=3.1108458042144775, loss=2.8133065700531006
I0131 00:36:39.752776 139907703985920 logging_writer.py:48] [38400] global_step=38400, grad_norm=2.59755277633667, loss=2.822683572769165
I0131 00:37:13.449450 139906827380480 logging_writer.py:48] [38500] global_step=38500, grad_norm=2.7984516620635986, loss=2.751971960067749
I0131 00:37:47.149349 139907703985920 logging_writer.py:48] [38600] global_step=38600, grad_norm=3.020340919494629, loss=2.8119616508483887
I0131 00:38:20.817064 139906827380480 logging_writer.py:48] [38700] global_step=38700, grad_norm=2.9927597045898438, loss=2.91091251373291
I0131 00:38:54.512345 139907703985920 logging_writer.py:48] [38800] global_step=38800, grad_norm=2.990642786026001, loss=2.75614333152771
I0131 00:39:28.190966 139906827380480 logging_writer.py:48] [38900] global_step=38900, grad_norm=3.1257541179656982, loss=2.808286666870117
I0131 00:40:01.898247 139907703985920 logging_writer.py:48] [39000] global_step=39000, grad_norm=3.237154245376587, loss=2.7668826580047607
I0131 00:40:35.567276 139906827380480 logging_writer.py:48] [39100] global_step=39100, grad_norm=2.841916084289551, loss=2.830169439315796
I0131 00:41:09.357428 139907703985920 logging_writer.py:48] [39200] global_step=39200, grad_norm=3.0561394691467285, loss=2.794759750366211
I0131 00:41:27.349500 140070692116288 spec.py:321] Evaluating on the training split.
I0131 00:41:34.841332 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 00:41:48.720308 140070692116288 spec.py:349] Evaluating on the test split.
I0131 00:41:50.951520 140070692116288 submission_runner.py:408] Time since start: 13939.58s, 	Step: 39255, 	{'train/accuracy': 0.6478993892669678, 'train/loss': 1.5248180627822876, 'validation/accuracy': 0.5951799750328064, 'validation/loss': 1.7707302570343018, 'validation/num_examples': 50000, 'test/accuracy': 0.47690001130104065, 'test/loss': 2.424974203109741, 'test/num_examples': 10000, 'score': 13316.273628473282, 'total_duration': 13939.578382253647, 'accumulated_submission_time': 13316.273628473282, 'accumulated_eval_time': 620.9647953510284, 'accumulated_logging_time': 1.0185627937316895}
I0131 00:41:50.975463 139906756101888 logging_writer.py:48] [39255] accumulated_eval_time=620.964795, accumulated_logging_time=1.018563, accumulated_submission_time=13316.273628, global_step=39255, preemption_count=0, score=13316.273628, test/accuracy=0.476900, test/loss=2.424974, test/num_examples=10000, total_duration=13939.578382, train/accuracy=0.647899, train/loss=1.524818, validation/accuracy=0.595180, validation/loss=1.770730, validation/num_examples=50000
I0131 00:42:06.511199 139907712378624 logging_writer.py:48] [39300] global_step=39300, grad_norm=3.079083204269409, loss=2.7253568172454834
I0131 00:42:40.219897 139906756101888 logging_writer.py:48] [39400] global_step=39400, grad_norm=2.7902278900146484, loss=2.6804356575012207
I0131 00:43:13.901522 139907712378624 logging_writer.py:48] [39500] global_step=39500, grad_norm=3.515070915222168, loss=2.873551368713379
I0131 00:43:47.590930 139906756101888 logging_writer.py:48] [39600] global_step=39600, grad_norm=3.665800094604492, loss=2.7871153354644775
I0131 00:44:21.280050 139907712378624 logging_writer.py:48] [39700] global_step=39700, grad_norm=3.79380464553833, loss=2.8256750106811523
I0131 00:44:54.992827 139906756101888 logging_writer.py:48] [39800] global_step=39800, grad_norm=2.9990551471710205, loss=2.8349826335906982
I0131 00:45:28.780232 139907712378624 logging_writer.py:48] [39900] global_step=39900, grad_norm=3.779057025909424, loss=2.716623067855835
I0131 00:46:02.467289 139906756101888 logging_writer.py:48] [40000] global_step=40000, grad_norm=3.152714967727661, loss=2.781085729598999
I0131 00:46:36.121281 139907712378624 logging_writer.py:48] [40100] global_step=40100, grad_norm=3.1074163913726807, loss=2.7621688842773438
I0131 00:47:09.815374 139906756101888 logging_writer.py:48] [40200] global_step=40200, grad_norm=3.9202067852020264, loss=2.7449865341186523
I0131 00:47:43.542805 139907712378624 logging_writer.py:48] [40300] global_step=40300, grad_norm=3.3816421031951904, loss=2.8783881664276123
I0131 00:48:17.283847 139906756101888 logging_writer.py:48] [40400] global_step=40400, grad_norm=3.2380049228668213, loss=2.7502713203430176
I0131 00:48:51.052449 139907712378624 logging_writer.py:48] [40500] global_step=40500, grad_norm=2.9090652465820312, loss=2.8368844985961914
I0131 00:49:24.733273 139906756101888 logging_writer.py:48] [40600] global_step=40600, grad_norm=3.2359046936035156, loss=2.7222609519958496
I0131 00:49:58.486750 139907712378624 logging_writer.py:48] [40700] global_step=40700, grad_norm=2.806389331817627, loss=2.783301830291748
I0131 00:50:21.233038 140070692116288 spec.py:321] Evaluating on the training split.
I0131 00:50:28.696092 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 00:50:42.252943 140070692116288 spec.py:349] Evaluating on the test split.
I0131 00:50:44.451342 140070692116288 submission_runner.py:408] Time since start: 14473.08s, 	Step: 40769, 	{'train/accuracy': 0.6623883843421936, 'train/loss': 1.4788810014724731, 'validation/accuracy': 0.6086199879646301, 'validation/loss': 1.71470308303833, 'validation/num_examples': 50000, 'test/accuracy': 0.4877000153064728, 'test/loss': 2.3885064125061035, 'test/num_examples': 10000, 'score': 13826.47026848793, 'total_duration': 14473.0782289505, 'accumulated_submission_time': 13826.47026848793, 'accumulated_eval_time': 644.1830842494965, 'accumulated_logging_time': 1.0521259307861328}
I0131 00:50:44.471637 139907703985920 logging_writer.py:48] [40769] accumulated_eval_time=644.183084, accumulated_logging_time=1.052126, accumulated_submission_time=13826.470268, global_step=40769, preemption_count=0, score=13826.470268, test/accuracy=0.487700, test/loss=2.388506, test/num_examples=10000, total_duration=14473.078229, train/accuracy=0.662388, train/loss=1.478881, validation/accuracy=0.608620, validation/loss=1.714703, validation/num_examples=50000
I0131 00:50:55.256450 139907729164032 logging_writer.py:48] [40800] global_step=40800, grad_norm=3.194148063659668, loss=2.731886386871338
I0131 00:51:28.963959 139907703985920 logging_writer.py:48] [40900] global_step=40900, grad_norm=2.6311848163604736, loss=2.897531270980835
I0131 00:52:02.663250 139907729164032 logging_writer.py:48] [41000] global_step=41000, grad_norm=3.2993533611297607, loss=2.7036256790161133
I0131 00:52:36.373812 139907703985920 logging_writer.py:48] [41100] global_step=41100, grad_norm=2.882418394088745, loss=2.6998696327209473
I0131 00:53:10.037045 139907729164032 logging_writer.py:48] [41200] global_step=41200, grad_norm=3.1786739826202393, loss=2.8368568420410156
I0131 00:53:43.754894 139907703985920 logging_writer.py:48] [41300] global_step=41300, grad_norm=3.0928804874420166, loss=2.8261241912841797
I0131 00:54:17.434800 139907729164032 logging_writer.py:48] [41400] global_step=41400, grad_norm=3.1267614364624023, loss=2.74460768699646
I0131 00:54:51.160188 139907703985920 logging_writer.py:48] [41500] global_step=41500, grad_norm=2.9445204734802246, loss=2.74961256980896
I0131 00:55:24.820743 139907729164032 logging_writer.py:48] [41600] global_step=41600, grad_norm=3.4656472206115723, loss=2.777815341949463
I0131 00:55:58.498384 139907703985920 logging_writer.py:48] [41700] global_step=41700, grad_norm=3.1088733673095703, loss=2.80539608001709
I0131 00:56:32.151946 139907729164032 logging_writer.py:48] [41800] global_step=41800, grad_norm=3.26324200630188, loss=2.7575669288635254
I0131 00:57:05.847840 139907703985920 logging_writer.py:48] [41900] global_step=41900, grad_norm=3.4296600818634033, loss=2.703641891479492
I0131 00:57:39.578810 139907729164032 logging_writer.py:48] [42000] global_step=42000, grad_norm=3.1773157119750977, loss=2.818016290664673
I0131 00:58:13.319730 139907703985920 logging_writer.py:48] [42100] global_step=42100, grad_norm=2.6754205226898193, loss=2.795105457305908
I0131 00:58:46.976427 139907729164032 logging_writer.py:48] [42200] global_step=42200, grad_norm=3.2098593711853027, loss=2.8285844326019287
I0131 00:59:14.749212 140070692116288 spec.py:321] Evaluating on the training split.
I0131 00:59:22.146225 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 00:59:41.485086 140070692116288 spec.py:349] Evaluating on the test split.
I0131 00:59:43.621629 140070692116288 submission_runner.py:408] Time since start: 15012.25s, 	Step: 42284, 	{'train/accuracy': 0.6502909660339355, 'train/loss': 1.5233001708984375, 'validation/accuracy': 0.6042400002479553, 'validation/loss': 1.73208749294281, 'validation/num_examples': 50000, 'test/accuracy': 0.48020002245903015, 'test/loss': 2.396982431411743, 'test/num_examples': 10000, 'score': 14336.686334371567, 'total_duration': 15012.248512983322, 'accumulated_submission_time': 14336.686334371567, 'accumulated_eval_time': 673.0554871559143, 'accumulated_logging_time': 1.0817155838012695}
I0131 00:59:43.642862 139906827380480 logging_writer.py:48] [42284] accumulated_eval_time=673.055487, accumulated_logging_time=1.081716, accumulated_submission_time=14336.686334, global_step=42284, preemption_count=0, score=14336.686334, test/accuracy=0.480200, test/loss=2.396982, test/num_examples=10000, total_duration=15012.248513, train/accuracy=0.650291, train/loss=1.523300, validation/accuracy=0.604240, validation/loss=1.732087, validation/num_examples=50000
I0131 00:59:49.353680 139906835773184 logging_writer.py:48] [42300] global_step=42300, grad_norm=3.0296924114227295, loss=2.6031417846679688
I0131 01:00:23.043363 139906827380480 logging_writer.py:48] [42400] global_step=42400, grad_norm=3.3232719898223877, loss=2.6821813583374023
I0131 01:00:56.685592 139906835773184 logging_writer.py:48] [42500] global_step=42500, grad_norm=3.6942148208618164, loss=2.7852046489715576
I0131 01:01:30.387358 139906827380480 logging_writer.py:48] [42600] global_step=42600, grad_norm=3.0594234466552734, loss=2.7512660026550293
I0131 01:02:04.061661 139906835773184 logging_writer.py:48] [42700] global_step=42700, grad_norm=3.248215436935425, loss=2.6359386444091797
I0131 01:02:37.859177 139906827380480 logging_writer.py:48] [42800] global_step=42800, grad_norm=2.659240961074829, loss=2.7888948917388916
I0131 01:03:11.554886 139906835773184 logging_writer.py:48] [42900] global_step=42900, grad_norm=3.5863630771636963, loss=2.7769646644592285
I0131 01:03:45.319221 139906827380480 logging_writer.py:48] [43000] global_step=43000, grad_norm=3.1561830043792725, loss=2.8212366104125977
I0131 01:04:19.095939 139906835773184 logging_writer.py:48] [43100] global_step=43100, grad_norm=3.458934783935547, loss=2.7976226806640625
I0131 01:04:52.812211 139906827380480 logging_writer.py:48] [43200] global_step=43200, grad_norm=3.1176652908325195, loss=2.741359233856201
I0131 01:05:26.538434 139906835773184 logging_writer.py:48] [43300] global_step=43300, grad_norm=2.897801160812378, loss=2.7997984886169434
I0131 01:06:00.286031 139906827380480 logging_writer.py:48] [43400] global_step=43400, grad_norm=3.6941330432891846, loss=2.725489616394043
I0131 01:06:34.014192 139906835773184 logging_writer.py:48] [43500] global_step=43500, grad_norm=3.130868911743164, loss=2.7831695079803467
I0131 01:07:07.726435 139906827380480 logging_writer.py:48] [43600] global_step=43600, grad_norm=3.434544563293457, loss=2.684178113937378
I0131 01:07:41.470398 139906835773184 logging_writer.py:48] [43700] global_step=43700, grad_norm=3.44545578956604, loss=2.670085906982422
I0131 01:08:13.916932 140070692116288 spec.py:321] Evaluating on the training split.
I0131 01:08:21.147611 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 01:08:35.116919 140070692116288 spec.py:349] Evaluating on the test split.
I0131 01:08:37.240749 140070692116288 submission_runner.py:408] Time since start: 15545.87s, 	Step: 43798, 	{'train/accuracy': 0.6564692258834839, 'train/loss': 1.4994585514068604, 'validation/accuracy': 0.6126999855041504, 'validation/loss': 1.6944860219955444, 'validation/num_examples': 50000, 'test/accuracy': 0.48920002579689026, 'test/loss': 2.3520050048828125, 'test/num_examples': 10000, 'score': 14846.899607419968, 'total_duration': 15545.86763715744, 'accumulated_submission_time': 14846.899607419968, 'accumulated_eval_time': 696.379273891449, 'accumulated_logging_time': 1.1133880615234375}
I0131 01:08:37.261874 139906835773184 logging_writer.py:48] [43798] accumulated_eval_time=696.379274, accumulated_logging_time=1.113388, accumulated_submission_time=14846.899607, global_step=43798, preemption_count=0, score=14846.899607, test/accuracy=0.489200, test/loss=2.352005, test/num_examples=10000, total_duration=15545.867637, train/accuracy=0.656469, train/loss=1.499459, validation/accuracy=0.612700, validation/loss=1.694486, validation/num_examples=50000
I0131 01:08:38.275089 139907712378624 logging_writer.py:48] [43800] global_step=43800, grad_norm=3.3399055004119873, loss=2.7915425300598145
I0131 01:09:11.957307 139906835773184 logging_writer.py:48] [43900] global_step=43900, grad_norm=4.100551605224609, loss=2.7478950023651123
I0131 01:09:45.622065 139907712378624 logging_writer.py:48] [44000] global_step=44000, grad_norm=3.401735305786133, loss=2.8494906425476074
I0131 01:10:19.325862 139906835773184 logging_writer.py:48] [44100] global_step=44100, grad_norm=3.770019292831421, loss=2.692166328430176
I0131 01:10:52.993275 139907712378624 logging_writer.py:48] [44200] global_step=44200, grad_norm=3.2724897861480713, loss=2.776491165161133
I0131 01:11:26.746461 139906835773184 logging_writer.py:48] [44300] global_step=44300, grad_norm=3.5766360759735107, loss=2.7671520709991455
I0131 01:12:00.434095 139907712378624 logging_writer.py:48] [44400] global_step=44400, grad_norm=3.4528276920318604, loss=2.742788076400757
I0131 01:12:34.138046 139906835773184 logging_writer.py:48] [44500] global_step=44500, grad_norm=3.015148401260376, loss=2.820530652999878
I0131 01:13:07.836671 139907712378624 logging_writer.py:48] [44600] global_step=44600, grad_norm=3.2722158432006836, loss=2.8435873985290527
I0131 01:13:41.624644 139906835773184 logging_writer.py:48] [44700] global_step=44700, grad_norm=3.0921082496643066, loss=2.78603458404541
I0131 01:14:15.314855 139907712378624 logging_writer.py:48] [44800] global_step=44800, grad_norm=2.8582639694213867, loss=2.666121482849121
I0131 01:14:49.013604 139906835773184 logging_writer.py:48] [44900] global_step=44900, grad_norm=3.5114943981170654, loss=2.7306134700775146
I0131 01:15:22.731192 139907712378624 logging_writer.py:48] [45000] global_step=45000, grad_norm=3.1307497024536133, loss=2.870497465133667
I0131 01:15:56.405767 139906835773184 logging_writer.py:48] [45100] global_step=45100, grad_norm=3.5397286415100098, loss=2.690321922302246
I0131 01:16:30.228322 139907712378624 logging_writer.py:48] [45200] global_step=45200, grad_norm=3.5859830379486084, loss=2.814790725708008
I0131 01:17:03.903088 139906835773184 logging_writer.py:48] [45300] global_step=45300, grad_norm=3.273857593536377, loss=2.742382049560547
I0131 01:17:07.387397 140070692116288 spec.py:321] Evaluating on the training split.
I0131 01:17:14.564620 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 01:17:27.982242 140070692116288 spec.py:349] Evaluating on the test split.
I0131 01:17:30.202365 140070692116288 submission_runner.py:408] Time since start: 16078.83s, 	Step: 45312, 	{'train/accuracy': 0.6901307106018066, 'train/loss': 1.335003137588501, 'validation/accuracy': 0.60971999168396, 'validation/loss': 1.702130913734436, 'validation/num_examples': 50000, 'test/accuracy': 0.4855000376701355, 'test/loss': 2.367635488510132, 'test/num_examples': 10000, 'score': 15356.965127944946, 'total_duration': 16078.829252958298, 'accumulated_submission_time': 15356.965127944946, 'accumulated_eval_time': 719.1942150592804, 'accumulated_logging_time': 1.1434450149536133}
I0131 01:17:30.222518 139907703985920 logging_writer.py:48] [45312] accumulated_eval_time=719.194215, accumulated_logging_time=1.143445, accumulated_submission_time=15356.965128, global_step=45312, preemption_count=0, score=15356.965128, test/accuracy=0.485500, test/loss=2.367635, test/num_examples=10000, total_duration=16078.829253, train/accuracy=0.690131, train/loss=1.335003, validation/accuracy=0.609720, validation/loss=1.702131, validation/num_examples=50000
I0131 01:18:00.160418 139907729164032 logging_writer.py:48] [45400] global_step=45400, grad_norm=3.3619587421417236, loss=2.7646989822387695
I0131 01:18:33.829204 139907703985920 logging_writer.py:48] [45500] global_step=45500, grad_norm=2.5549330711364746, loss=2.780467987060547
I0131 01:19:07.500231 139907729164032 logging_writer.py:48] [45600] global_step=45600, grad_norm=3.4400885105133057, loss=2.8268635272979736
I0131 01:19:41.172167 139907703985920 logging_writer.py:48] [45700] global_step=45700, grad_norm=3.2777099609375, loss=2.844081163406372
I0131 01:20:14.848167 139907729164032 logging_writer.py:48] [45800] global_step=45800, grad_norm=3.439793109893799, loss=2.776505947113037
I0131 01:20:48.522962 139907703985920 logging_writer.py:48] [45900] global_step=45900, grad_norm=3.4762091636657715, loss=2.7012970447540283
I0131 01:21:22.292872 139907729164032 logging_writer.py:48] [46000] global_step=46000, grad_norm=3.2881553173065186, loss=2.718848943710327
I0131 01:21:56.021898 139907703985920 logging_writer.py:48] [46100] global_step=46100, grad_norm=3.2193329334259033, loss=2.8100099563598633
I0131 01:22:29.914411 139907729164032 logging_writer.py:48] [46200] global_step=46200, grad_norm=3.5578842163085938, loss=2.7569668292999268
I0131 01:23:03.635012 139907703985920 logging_writer.py:48] [46300] global_step=46300, grad_norm=3.7611873149871826, loss=2.6721463203430176
I0131 01:23:37.328694 139907729164032 logging_writer.py:48] [46400] global_step=46400, grad_norm=3.05036997795105, loss=2.844642400741577
I0131 01:24:11.011241 139907703985920 logging_writer.py:48] [46500] global_step=46500, grad_norm=2.924478054046631, loss=2.697323799133301
I0131 01:24:44.772038 139907729164032 logging_writer.py:48] [46600] global_step=46600, grad_norm=2.811737060546875, loss=2.7825875282287598
I0131 01:25:18.476893 139907703985920 logging_writer.py:48] [46700] global_step=46700, grad_norm=2.806929349899292, loss=2.6904001235961914
I0131 01:25:52.228888 139907729164032 logging_writer.py:48] [46800] global_step=46800, grad_norm=2.7392542362213135, loss=2.803119659423828
I0131 01:26:00.432489 140070692116288 spec.py:321] Evaluating on the training split.
I0131 01:26:07.533233 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 01:26:18.538808 140070692116288 spec.py:349] Evaluating on the test split.
I0131 01:26:20.850795 140070692116288 submission_runner.py:408] Time since start: 16609.48s, 	Step: 46826, 	{'train/accuracy': 0.6707788705825806, 'train/loss': 1.4408752918243408, 'validation/accuracy': 0.6131199598312378, 'validation/loss': 1.7045730352401733, 'validation/num_examples': 50000, 'test/accuracy': 0.48520001769065857, 'test/loss': 2.384239435195923, 'test/num_examples': 10000, 'score': 15867.113546609879, 'total_duration': 16609.47766971588, 'accumulated_submission_time': 15867.113546609879, 'accumulated_eval_time': 739.6124730110168, 'accumulated_logging_time': 1.1734073162078857}
I0131 01:26:20.872666 139906835773184 logging_writer.py:48] [46826] accumulated_eval_time=739.612473, accumulated_logging_time=1.173407, accumulated_submission_time=15867.113547, global_step=46826, preemption_count=0, score=15867.113547, test/accuracy=0.485200, test/loss=2.384239, test/num_examples=10000, total_duration=16609.477670, train/accuracy=0.670779, train/loss=1.440875, validation/accuracy=0.613120, validation/loss=1.704573, validation/num_examples=50000
I0131 01:26:46.160174 139906949052160 logging_writer.py:48] [46900] global_step=46900, grad_norm=3.4078874588012695, loss=2.8433244228363037
I0131 01:27:19.874941 139906835773184 logging_writer.py:48] [47000] global_step=47000, grad_norm=3.254673480987549, loss=2.678621768951416
I0131 01:27:53.535389 139906949052160 logging_writer.py:48] [47100] global_step=47100, grad_norm=2.9946866035461426, loss=2.7008345127105713
I0131 01:28:27.266598 139906835773184 logging_writer.py:48] [47200] global_step=47200, grad_norm=2.7802515029907227, loss=2.689987897872925
I0131 01:29:01.086112 139906949052160 logging_writer.py:48] [47300] global_step=47300, grad_norm=3.706530809402466, loss=2.740650177001953
I0131 01:29:34.802494 139906835773184 logging_writer.py:48] [47400] global_step=47400, grad_norm=3.2698042392730713, loss=2.6627755165100098
I0131 01:30:08.452560 139906949052160 logging_writer.py:48] [47500] global_step=47500, grad_norm=3.050513505935669, loss=2.6858274936676025
I0131 01:30:42.164464 139906835773184 logging_writer.py:48] [47600] global_step=47600, grad_norm=3.3056371212005615, loss=2.8609390258789062
I0131 01:31:15.817927 139906949052160 logging_writer.py:48] [47700] global_step=47700, grad_norm=3.868950605392456, loss=2.6850619316101074
I0131 01:31:49.523357 139906835773184 logging_writer.py:48] [47800] global_step=47800, grad_norm=3.4989748001098633, loss=2.73984694480896
I0131 01:32:23.161165 139906949052160 logging_writer.py:48] [47900] global_step=47900, grad_norm=3.3348586559295654, loss=2.7106130123138428
I0131 01:32:56.875095 139906835773184 logging_writer.py:48] [48000] global_step=48000, grad_norm=3.412550926208496, loss=2.7192535400390625
I0131 01:33:30.562105 139906949052160 logging_writer.py:48] [48100] global_step=48100, grad_norm=3.1512961387634277, loss=2.6936142444610596
I0131 01:34:04.322939 139906835773184 logging_writer.py:48] [48200] global_step=48200, grad_norm=3.1771068572998047, loss=2.856677532196045
I0131 01:34:37.983788 139906949052160 logging_writer.py:48] [48300] global_step=48300, grad_norm=2.8760104179382324, loss=2.7723026275634766
I0131 01:34:50.983638 140070692116288 spec.py:321] Evaluating on the training split.
I0131 01:34:57.921072 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 01:35:08.141757 140070692116288 spec.py:349] Evaluating on the test split.
I0131 01:35:10.420907 140070692116288 submission_runner.py:408] Time since start: 17139.05s, 	Step: 48340, 	{'train/accuracy': 0.6635841727256775, 'train/loss': 1.4390766620635986, 'validation/accuracy': 0.6102799773216248, 'validation/loss': 1.6896299123764038, 'validation/num_examples': 50000, 'test/accuracy': 0.4865000247955322, 'test/loss': 2.390916585922241, 'test/num_examples': 10000, 'score': 16377.163805484772, 'total_duration': 17139.047789812088, 'accumulated_submission_time': 16377.163805484772, 'accumulated_eval_time': 759.0497002601624, 'accumulated_logging_time': 1.2045924663543701}
I0131 01:35:10.445648 139906835773184 logging_writer.py:48] [48340] accumulated_eval_time=759.049700, accumulated_logging_time=1.204592, accumulated_submission_time=16377.163805, global_step=48340, preemption_count=0, score=16377.163805, test/accuracy=0.486500, test/loss=2.390917, test/num_examples=10000, total_duration=17139.047790, train/accuracy=0.663584, train/loss=1.439077, validation/accuracy=0.610280, validation/loss=1.689630, validation/num_examples=50000
I0131 01:35:30.960897 139907703985920 logging_writer.py:48] [48400] global_step=48400, grad_norm=3.65616512298584, loss=2.7406978607177734
I0131 01:36:04.632006 139906835773184 logging_writer.py:48] [48500] global_step=48500, grad_norm=3.207976818084717, loss=2.798456907272339
I0131 01:36:38.308982 139907703985920 logging_writer.py:48] [48600] global_step=48600, grad_norm=3.0372493267059326, loss=2.8069908618927
I0131 01:37:11.996881 139906835773184 logging_writer.py:48] [48700] global_step=48700, grad_norm=3.067340612411499, loss=2.7377419471740723
I0131 01:37:45.667367 139907703985920 logging_writer.py:48] [48800] global_step=48800, grad_norm=3.164545774459839, loss=2.8165745735168457
I0131 01:38:19.442715 139906835773184 logging_writer.py:48] [48900] global_step=48900, grad_norm=3.086909294128418, loss=2.644392728805542
I0131 01:38:53.121217 139907703985920 logging_writer.py:48] [49000] global_step=49000, grad_norm=2.79459285736084, loss=2.7126142978668213
I0131 01:39:26.819360 139906835773184 logging_writer.py:48] [49100] global_step=49100, grad_norm=3.4597134590148926, loss=2.7418251037597656
I0131 01:40:00.558940 139907703985920 logging_writer.py:48] [49200] global_step=49200, grad_norm=3.2514748573303223, loss=2.736462354660034
I0131 01:40:34.255255 139906835773184 logging_writer.py:48] [49300] global_step=49300, grad_norm=3.141735076904297, loss=2.692955255508423
I0131 01:41:08.055612 139907703985920 logging_writer.py:48] [49400] global_step=49400, grad_norm=3.6219375133514404, loss=2.7293577194213867
I0131 01:41:41.818994 139906835773184 logging_writer.py:48] [49500] global_step=49500, grad_norm=3.1073334217071533, loss=2.68571138381958
I0131 01:42:15.504101 139907703985920 logging_writer.py:48] [49600] global_step=49600, grad_norm=2.8596696853637695, loss=2.6896398067474365
I0131 01:42:49.187845 139906835773184 logging_writer.py:48] [49700] global_step=49700, grad_norm=3.208090305328369, loss=2.730882406234741
I0131 01:43:22.899057 139907703985920 logging_writer.py:48] [49800] global_step=49800, grad_norm=3.595482110977173, loss=2.621256113052368
I0131 01:43:40.540028 140070692116288 spec.py:321] Evaluating on the training split.
I0131 01:43:47.352985 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 01:43:56.248784 140070692116288 spec.py:349] Evaluating on the test split.
I0131 01:43:58.552199 140070692116288 submission_runner.py:408] Time since start: 17667.18s, 	Step: 49854, 	{'train/accuracy': 0.6621492505073547, 'train/loss': 1.4692184925079346, 'validation/accuracy': 0.6146799921989441, 'validation/loss': 1.694058895111084, 'validation/num_examples': 50000, 'test/accuracy': 0.4975000321865082, 'test/loss': 2.350130081176758, 'test/num_examples': 10000, 'score': 16887.197038412094, 'total_duration': 17667.17898607254, 'accumulated_submission_time': 16887.197038412094, 'accumulated_eval_time': 777.0617418289185, 'accumulated_logging_time': 1.239149808883667}
I0131 01:43:58.578736 139907712378624 logging_writer.py:48] [49854] accumulated_eval_time=777.061742, accumulated_logging_time=1.239150, accumulated_submission_time=16887.197038, global_step=49854, preemption_count=0, score=16887.197038, test/accuracy=0.497500, test/loss=2.350130, test/num_examples=10000, total_duration=17667.178986, train/accuracy=0.662149, train/loss=1.469218, validation/accuracy=0.614680, validation/loss=1.694059, validation/num_examples=50000
I0131 01:44:14.385316 139907737556736 logging_writer.py:48] [49900] global_step=49900, grad_norm=3.0309813022613525, loss=2.699061632156372
I0131 01:44:48.034865 139907712378624 logging_writer.py:48] [50000] global_step=50000, grad_norm=3.2164716720581055, loss=2.7178096771240234
I0131 01:45:21.736188 139907737556736 logging_writer.py:48] [50100] global_step=50100, grad_norm=3.2360265254974365, loss=2.714264392852783
I0131 01:45:55.408967 139907712378624 logging_writer.py:48] [50200] global_step=50200, grad_norm=3.158501625061035, loss=2.789828062057495
I0131 01:46:29.119703 139907737556736 logging_writer.py:48] [50300] global_step=50300, grad_norm=3.000002861022949, loss=2.6850671768188477
I0131 01:47:02.783485 139907712378624 logging_writer.py:48] [50400] global_step=50400, grad_norm=2.990506649017334, loss=2.800795078277588
I0131 01:47:36.465419 139907737556736 logging_writer.py:48] [50500] global_step=50500, grad_norm=3.4031589031219482, loss=2.673999786376953
I0131 01:48:10.127759 139907712378624 logging_writer.py:48] [50600] global_step=50600, grad_norm=3.6538119316101074, loss=2.768756866455078
I0131 01:48:43.859617 139907737556736 logging_writer.py:48] [50700] global_step=50700, grad_norm=3.387186050415039, loss=2.6990203857421875
I0131 01:49:17.591798 139907712378624 logging_writer.py:48] [50800] global_step=50800, grad_norm=3.4408793449401855, loss=2.780237913131714
I0131 01:49:51.326628 139907737556736 logging_writer.py:48] [50900] global_step=50900, grad_norm=3.254807949066162, loss=2.850724220275879
I0131 01:50:24.987269 139907712378624 logging_writer.py:48] [51000] global_step=51000, grad_norm=3.083522081375122, loss=2.766601085662842
I0131 01:50:58.693810 139907737556736 logging_writer.py:48] [51100] global_step=51100, grad_norm=2.833160638809204, loss=2.7579710483551025
I0131 01:51:32.370017 139907712378624 logging_writer.py:48] [51200] global_step=51200, grad_norm=3.4737720489501953, loss=2.6757946014404297
I0131 01:52:06.063933 139907737556736 logging_writer.py:48] [51300] global_step=51300, grad_norm=3.5326199531555176, loss=2.802565813064575
I0131 01:52:28.776816 140070692116288 spec.py:321] Evaluating on the training split.
I0131 01:52:35.569084 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 01:52:44.324565 140070692116288 spec.py:349] Evaluating on the test split.
I0131 01:52:46.590870 140070692116288 submission_runner.py:408] Time since start: 18195.22s, 	Step: 51369, 	{'train/accuracy': 0.6635642647743225, 'train/loss': 1.439455270767212, 'validation/accuracy': 0.6191399693489075, 'validation/loss': 1.646724820137024, 'validation/num_examples': 50000, 'test/accuracy': 0.4969000220298767, 'test/loss': 2.307199478149414, 'test/num_examples': 10000, 'score': 17397.333253145218, 'total_duration': 18195.217745542526, 'accumulated_submission_time': 17397.333253145218, 'accumulated_eval_time': 794.8757519721985, 'accumulated_logging_time': 1.2756733894348145}
I0131 01:52:46.622128 139906949052160 logging_writer.py:48] [51369] accumulated_eval_time=794.875752, accumulated_logging_time=1.275673, accumulated_submission_time=17397.333253, global_step=51369, preemption_count=0, score=17397.333253, test/accuracy=0.496900, test/loss=2.307199, test/num_examples=10000, total_duration=18195.217746, train/accuracy=0.663564, train/loss=1.439455, validation/accuracy=0.619140, validation/loss=1.646725, validation/num_examples=50000
I0131 01:52:57.427191 139907703985920 logging_writer.py:48] [51400] global_step=51400, grad_norm=3.646207571029663, loss=2.693577289581299
I0131 01:53:31.106163 139906949052160 logging_writer.py:48] [51500] global_step=51500, grad_norm=3.35125732421875, loss=2.785386800765991
I0131 01:54:04.812066 139907703985920 logging_writer.py:48] [51600] global_step=51600, grad_norm=3.1454508304595947, loss=2.671475410461426
I0131 01:54:38.492445 139906949052160 logging_writer.py:48] [51700] global_step=51700, grad_norm=2.996837854385376, loss=2.740940570831299
I0131 01:55:12.267785 139907703985920 logging_writer.py:48] [51800] global_step=51800, grad_norm=2.7928578853607178, loss=2.746734619140625
I0131 01:55:45.937501 139906949052160 logging_writer.py:48] [51900] global_step=51900, grad_norm=3.597533702850342, loss=2.727975368499756
I0131 01:56:19.647509 139907703985920 logging_writer.py:48] [52000] global_step=52000, grad_norm=3.49918794631958, loss=2.7363522052764893
I0131 01:56:53.290005 139906949052160 logging_writer.py:48] [52100] global_step=52100, grad_norm=3.0840859413146973, loss=2.746697425842285
I0131 01:57:27.003984 139907703985920 logging_writer.py:48] [52200] global_step=52200, grad_norm=3.719984769821167, loss=2.725644111633301
I0131 01:58:00.676511 139906949052160 logging_writer.py:48] [52300] global_step=52300, grad_norm=2.673457622528076, loss=2.7683279514312744
I0131 01:58:34.377496 139907703985920 logging_writer.py:48] [52400] global_step=52400, grad_norm=2.976433515548706, loss=2.7279229164123535
I0131 01:59:08.033352 139906949052160 logging_writer.py:48] [52500] global_step=52500, grad_norm=3.1757843494415283, loss=2.7667932510375977
I0131 01:59:41.824277 139907703985920 logging_writer.py:48] [52600] global_step=52600, grad_norm=3.30171275138855, loss=2.7982990741729736
I0131 02:00:15.491906 139906949052160 logging_writer.py:48] [52700] global_step=52700, grad_norm=3.2599549293518066, loss=2.784787178039551
I0131 02:00:49.162800 139907703985920 logging_writer.py:48] [52800] global_step=52800, grad_norm=3.2218868732452393, loss=2.7357993125915527
I0131 02:01:16.898633 140070692116288 spec.py:321] Evaluating on the training split.
I0131 02:01:23.586273 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 02:01:32.295275 140070692116288 spec.py:349] Evaluating on the test split.
I0131 02:01:34.570559 140070692116288 submission_runner.py:408] Time since start: 18723.20s, 	Step: 52884, 	{'train/accuracy': 0.6588408946990967, 'train/loss': 1.4924957752227783, 'validation/accuracy': 0.6126799583435059, 'validation/loss': 1.7022305727005005, 'validation/num_examples': 50000, 'test/accuracy': 0.4881000220775604, 'test/loss': 2.357887029647827, 'test/num_examples': 10000, 'score': 17907.54860687256, 'total_duration': 18723.197404146194, 'accumulated_submission_time': 17907.54860687256, 'accumulated_eval_time': 812.5476040840149, 'accumulated_logging_time': 1.3171625137329102}
I0131 02:01:34.605126 139906827380480 logging_writer.py:48] [52884] accumulated_eval_time=812.547604, accumulated_logging_time=1.317163, accumulated_submission_time=17907.548607, global_step=52884, preemption_count=0, score=17907.548607, test/accuracy=0.488100, test/loss=2.357887, test/num_examples=10000, total_duration=18723.197404, train/accuracy=0.658841, train/loss=1.492496, validation/accuracy=0.612680, validation/loss=1.702231, validation/num_examples=50000
I0131 02:01:40.333773 139906835773184 logging_writer.py:48] [52900] global_step=52900, grad_norm=3.1346468925476074, loss=2.702171802520752
I0131 02:02:14.002350 139906827380480 logging_writer.py:48] [53000] global_step=53000, grad_norm=3.1123299598693848, loss=2.7571542263031006
I0131 02:02:47.698095 139906835773184 logging_writer.py:48] [53100] global_step=53100, grad_norm=3.347177028656006, loss=2.7194058895111084
I0131 02:03:21.375992 139906827380480 logging_writer.py:48] [53200] global_step=53200, grad_norm=3.103111982345581, loss=2.724919557571411
I0131 02:03:55.052274 139906835773184 logging_writer.py:48] [53300] global_step=53300, grad_norm=3.003464937210083, loss=2.6606497764587402
I0131 02:04:28.724471 139906827380480 logging_writer.py:48] [53400] global_step=53400, grad_norm=3.3625314235687256, loss=2.679325580596924
I0131 02:05:02.364523 139906835773184 logging_writer.py:48] [53500] global_step=53500, grad_norm=3.0712242126464844, loss=2.6391501426696777
I0131 02:05:36.136770 139906827380480 logging_writer.py:48] [53600] global_step=53600, grad_norm=3.594054698944092, loss=2.588465690612793
I0131 02:06:09.833403 139906835773184 logging_writer.py:48] [53700] global_step=53700, grad_norm=2.973367214202881, loss=2.6641733646392822
I0131 02:06:43.511697 139906827380480 logging_writer.py:48] [53800] global_step=53800, grad_norm=3.745701789855957, loss=2.683248519897461
I0131 02:07:17.174114 139906835773184 logging_writer.py:48] [53900] global_step=53900, grad_norm=3.2133007049560547, loss=2.64617919921875
I0131 02:07:50.863092 139906827380480 logging_writer.py:48] [54000] global_step=54000, grad_norm=3.277155637741089, loss=2.7309489250183105
I0131 02:08:24.518793 139906835773184 logging_writer.py:48] [54100] global_step=54100, grad_norm=3.643815040588379, loss=2.734177350997925
I0131 02:08:58.229398 139906827380480 logging_writer.py:48] [54200] global_step=54200, grad_norm=4.195680141448975, loss=2.7111258506774902
I0131 02:09:31.871249 139906835773184 logging_writer.py:48] [54300] global_step=54300, grad_norm=3.005409002304077, loss=2.6692676544189453
I0131 02:10:04.670451 140070692116288 spec.py:321] Evaluating on the training split.
I0131 02:10:11.319385 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 02:10:20.017120 140070692116288 spec.py:349] Evaluating on the test split.
I0131 02:10:22.312807 140070692116288 submission_runner.py:408] Time since start: 19250.94s, 	Step: 54399, 	{'train/accuracy': 0.7075693607330322, 'train/loss': 1.2901153564453125, 'validation/accuracy': 0.625819981098175, 'validation/loss': 1.6629422903060913, 'validation/num_examples': 50000, 'test/accuracy': 0.5001000165939331, 'test/loss': 2.3184971809387207, 'test/num_examples': 10000, 'score': 18417.55141377449, 'total_duration': 19250.939685344696, 'accumulated_submission_time': 18417.55141377449, 'accumulated_eval_time': 830.1899147033691, 'accumulated_logging_time': 1.362135410308838}
I0131 02:10:22.338274 139906756101888 logging_writer.py:48] [54399] accumulated_eval_time=830.189915, accumulated_logging_time=1.362135, accumulated_submission_time=18417.551414, global_step=54399, preemption_count=0, score=18417.551414, test/accuracy=0.500100, test/loss=2.318497, test/num_examples=10000, total_duration=19250.939685, train/accuracy=0.707569, train/loss=1.290115, validation/accuracy=0.625820, validation/loss=1.662942, validation/num_examples=50000
I0131 02:10:23.027659 139907703985920 logging_writer.py:48] [54400] global_step=54400, grad_norm=3.5492355823516846, loss=2.731466770172119
I0131 02:10:56.751425 139906756101888 logging_writer.py:48] [54500] global_step=54500, grad_norm=3.3542654514312744, loss=2.6605420112609863
I0131 02:11:30.577596 139907703985920 logging_writer.py:48] [54600] global_step=54600, grad_norm=3.306978464126587, loss=2.727202892303467
I0131 02:12:04.281522 139906756101888 logging_writer.py:48] [54700] global_step=54700, grad_norm=3.2084357738494873, loss=2.689440965652466
I0131 02:12:37.918453 139907703985920 logging_writer.py:48] [54800] global_step=54800, grad_norm=3.087738275527954, loss=2.7441036701202393
I0131 02:13:11.610376 139906756101888 logging_writer.py:48] [54900] global_step=54900, grad_norm=3.379969835281372, loss=2.630784034729004
I0131 02:13:45.263952 139907703985920 logging_writer.py:48] [55000] global_step=55000, grad_norm=3.588761806488037, loss=2.726999282836914
I0131 02:14:18.973427 139906756101888 logging_writer.py:48] [55100] global_step=55100, grad_norm=3.1420488357543945, loss=2.8361244201660156
I0131 02:14:52.619611 139907703985920 logging_writer.py:48] [55200] global_step=55200, grad_norm=3.283635377883911, loss=2.6975302696228027
I0131 02:15:26.311678 139906756101888 logging_writer.py:48] [55300] global_step=55300, grad_norm=3.3454506397247314, loss=2.7098217010498047
I0131 02:16:00.011235 139907703985920 logging_writer.py:48] [55400] global_step=55400, grad_norm=2.9097421169281006, loss=2.7267251014709473
I0131 02:16:33.696115 139906756101888 logging_writer.py:48] [55500] global_step=55500, grad_norm=3.4849350452423096, loss=2.7677741050720215
I0131 02:17:07.350932 139907703985920 logging_writer.py:48] [55600] global_step=55600, grad_norm=3.212392568588257, loss=2.6507904529571533
I0131 02:17:41.141052 139906756101888 logging_writer.py:48] [55700] global_step=55700, grad_norm=3.4281458854675293, loss=2.771651029586792
I0131 02:18:14.842600 139907703985920 logging_writer.py:48] [55800] global_step=55800, grad_norm=3.5374858379364014, loss=2.8308768272399902
I0131 02:18:48.508949 139906756101888 logging_writer.py:48] [55900] global_step=55900, grad_norm=3.3481602668762207, loss=2.6511802673339844
I0131 02:18:52.343330 140070692116288 spec.py:321] Evaluating on the training split.
I0131 02:18:58.881872 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 02:19:07.742149 140070692116288 spec.py:349] Evaluating on the test split.
I0131 02:19:10.112113 140070692116288 submission_runner.py:408] Time since start: 19778.74s, 	Step: 55913, 	{'train/accuracy': 0.6686663031578064, 'train/loss': 1.4696898460388184, 'validation/accuracy': 0.6124399900436401, 'validation/loss': 1.7320475578308105, 'validation/num_examples': 50000, 'test/accuracy': 0.4935000240802765, 'test/loss': 2.397702932357788, 'test/num_examples': 10000, 'score': 18927.4927611351, 'total_duration': 19778.73899126053, 'accumulated_submission_time': 18927.4927611351, 'accumulated_eval_time': 847.9586672782898, 'accumulated_logging_time': 1.3999717235565186}
I0131 02:19:10.138155 139906949052160 logging_writer.py:48] [55913] accumulated_eval_time=847.958667, accumulated_logging_time=1.399972, accumulated_submission_time=18927.492761, global_step=55913, preemption_count=0, score=18927.492761, test/accuracy=0.493500, test/loss=2.397703, test/num_examples=10000, total_duration=19778.738991, train/accuracy=0.668666, train/loss=1.469690, validation/accuracy=0.612440, validation/loss=1.732048, validation/num_examples=50000
I0131 02:19:39.747411 139907712378624 logging_writer.py:48] [56000] global_step=56000, grad_norm=4.006570339202881, loss=2.65596342086792
I0131 02:20:13.417891 139906949052160 logging_writer.py:48] [56100] global_step=56100, grad_norm=2.9552974700927734, loss=2.625213146209717
I0131 02:20:47.105950 139907712378624 logging_writer.py:48] [56200] global_step=56200, grad_norm=3.27882719039917, loss=2.7596302032470703
I0131 02:21:20.790688 139906949052160 logging_writer.py:48] [56300] global_step=56300, grad_norm=3.0950655937194824, loss=2.7521109580993652
I0131 02:21:54.477447 139907712378624 logging_writer.py:48] [56400] global_step=56400, grad_norm=2.888583183288574, loss=2.6566615104675293
I0131 02:22:28.148738 139906949052160 logging_writer.py:48] [56500] global_step=56500, grad_norm=3.1595048904418945, loss=2.6633431911468506
I0131 02:23:01.829018 139907712378624 logging_writer.py:48] [56600] global_step=56600, grad_norm=2.990790367126465, loss=2.7480766773223877
I0131 02:23:35.508996 139906949052160 logging_writer.py:48] [56700] global_step=56700, grad_norm=3.087171792984009, loss=2.6509151458740234
I0131 02:24:09.249183 139907712378624 logging_writer.py:48] [56800] global_step=56800, grad_norm=2.90252947807312, loss=2.682373285293579
I0131 02:24:42.984746 139906949052160 logging_writer.py:48] [56900] global_step=56900, grad_norm=3.8924078941345215, loss=2.7750978469848633
I0131 02:25:16.661695 139907712378624 logging_writer.py:48] [57000] global_step=57000, grad_norm=2.9671831130981445, loss=2.7116315364837646
I0131 02:25:50.340441 139906949052160 logging_writer.py:48] [57100] global_step=57100, grad_norm=3.4836621284484863, loss=2.70316743850708
I0131 02:26:24.016921 139907712378624 logging_writer.py:48] [57200] global_step=57200, grad_norm=3.6762797832489014, loss=2.7732486724853516
I0131 02:26:57.673489 139906949052160 logging_writer.py:48] [57300] global_step=57300, grad_norm=3.2831649780273438, loss=2.710113048553467
I0131 02:27:31.340477 139907712378624 logging_writer.py:48] [57400] global_step=57400, grad_norm=3.074045181274414, loss=2.6010921001434326
I0131 02:27:40.241533 140070692116288 spec.py:321] Evaluating on the training split.
I0131 02:27:46.751548 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 02:27:55.339920 140070692116288 spec.py:349] Evaluating on the test split.
I0131 02:27:57.587185 140070692116288 submission_runner.py:408] Time since start: 20306.21s, 	Step: 57428, 	{'train/accuracy': 0.6722536683082581, 'train/loss': 1.435630440711975, 'validation/accuracy': 0.6198399662971497, 'validation/loss': 1.6725577116012573, 'validation/num_examples': 50000, 'test/accuracy': 0.5006000399589539, 'test/loss': 2.3415465354919434, 'test/num_examples': 10000, 'score': 19437.532512187958, 'total_duration': 20306.21404647827, 'accumulated_submission_time': 19437.532512187958, 'accumulated_eval_time': 865.3042812347412, 'accumulated_logging_time': 1.4369032382965088}
I0131 02:27:57.614481 139906756101888 logging_writer.py:48] [57428] accumulated_eval_time=865.304281, accumulated_logging_time=1.436903, accumulated_submission_time=19437.532512, global_step=57428, preemption_count=0, score=19437.532512, test/accuracy=0.500600, test/loss=2.341547, test/num_examples=10000, total_duration=20306.214046, train/accuracy=0.672254, train/loss=1.435630, validation/accuracy=0.619840, validation/loss=1.672558, validation/num_examples=50000
I0131 02:28:22.240585 139906835773184 logging_writer.py:48] [57500] global_step=57500, grad_norm=2.9696171283721924, loss=2.683492660522461
I0131 02:28:55.967356 139906756101888 logging_writer.py:48] [57600] global_step=57600, grad_norm=3.472477912902832, loss=2.7074108123779297
I0131 02:29:29.643750 139906835773184 logging_writer.py:48] [57700] global_step=57700, grad_norm=3.655869722366333, loss=2.7556304931640625
I0131 02:30:03.398573 139906756101888 logging_writer.py:48] [57800] global_step=57800, grad_norm=3.4347856044769287, loss=2.7742042541503906
I0131 02:30:37.030455 139906835773184 logging_writer.py:48] [57900] global_step=57900, grad_norm=3.268176317214966, loss=2.61423659324646
I0131 02:31:10.720705 139906756101888 logging_writer.py:48] [58000] global_step=58000, grad_norm=3.156633138656616, loss=2.7577338218688965
I0131 02:31:44.448484 139906835773184 logging_writer.py:48] [58100] global_step=58100, grad_norm=3.321122646331787, loss=2.755176305770874
I0131 02:32:18.131558 139906756101888 logging_writer.py:48] [58200] global_step=58200, grad_norm=3.0248680114746094, loss=2.6499311923980713
I0131 02:32:51.805372 139906835773184 logging_writer.py:48] [58300] global_step=58300, grad_norm=3.141923189163208, loss=2.6679515838623047
I0131 02:33:25.593696 139906756101888 logging_writer.py:48] [58400] global_step=58400, grad_norm=3.10860538482666, loss=2.7080821990966797
I0131 02:33:59.292513 139906835773184 logging_writer.py:48] [58500] global_step=58500, grad_norm=2.9658095836639404, loss=2.753617763519287
I0131 02:34:33.045341 139906756101888 logging_writer.py:48] [58600] global_step=58600, grad_norm=3.2931535243988037, loss=2.7208304405212402
I0131 02:35:06.727114 139906835773184 logging_writer.py:48] [58700] global_step=58700, grad_norm=3.4915857315063477, loss=2.7079219818115234
I0131 02:35:40.438154 139906756101888 logging_writer.py:48] [58800] global_step=58800, grad_norm=3.203188180923462, loss=2.7576615810394287
I0131 02:36:14.212634 139906835773184 logging_writer.py:48] [58900] global_step=58900, grad_norm=2.8158674240112305, loss=2.7239809036254883
I0131 02:36:27.844878 140070692116288 spec.py:321] Evaluating on the training split.
I0131 02:36:34.326063 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 02:36:42.903726 140070692116288 spec.py:349] Evaluating on the test split.
I0131 02:36:45.188360 140070692116288 submission_runner.py:408] Time since start: 20833.82s, 	Step: 58942, 	{'train/accuracy': 0.6779735088348389, 'train/loss': 1.4131288528442383, 'validation/accuracy': 0.6226599812507629, 'validation/loss': 1.664193868637085, 'validation/num_examples': 50000, 'test/accuracy': 0.5063000321388245, 'test/loss': 2.299123764038086, 'test/num_examples': 10000, 'score': 19947.70263171196, 'total_duration': 20833.81520462036, 'accumulated_submission_time': 19947.70263171196, 'accumulated_eval_time': 882.6476812362671, 'accumulated_logging_time': 1.4736227989196777}
I0131 02:36:45.214242 139906756101888 logging_writer.py:48] [58942] accumulated_eval_time=882.647681, accumulated_logging_time=1.473623, accumulated_submission_time=19947.702632, global_step=58942, preemption_count=0, score=19947.702632, test/accuracy=0.506300, test/loss=2.299124, test/num_examples=10000, total_duration=20833.815205, train/accuracy=0.677974, train/loss=1.413129, validation/accuracy=0.622660, validation/loss=1.664194, validation/num_examples=50000
I0131 02:37:05.054740 139906949052160 logging_writer.py:48] [59000] global_step=59000, grad_norm=4.294057369232178, loss=2.699571132659912
I0131 02:37:38.730330 139906756101888 logging_writer.py:48] [59100] global_step=59100, grad_norm=3.3697593212127686, loss=2.7670016288757324
I0131 02:38:12.393725 139906949052160 logging_writer.py:48] [59200] global_step=59200, grad_norm=2.994976043701172, loss=2.783867359161377
I0131 02:38:46.083261 139906756101888 logging_writer.py:48] [59300] global_step=59300, grad_norm=4.091048240661621, loss=2.7193851470947266
I0131 02:39:19.775686 139906949052160 logging_writer.py:48] [59400] global_step=59400, grad_norm=3.0405192375183105, loss=2.777421474456787
I0131 02:39:53.465787 139906756101888 logging_writer.py:48] [59500] global_step=59500, grad_norm=3.128448963165283, loss=2.6257331371307373
I0131 02:40:27.184442 139906949052160 logging_writer.py:48] [59600] global_step=59600, grad_norm=3.332742214202881, loss=2.6610350608825684
I0131 02:41:00.838045 139906756101888 logging_writer.py:48] [59700] global_step=59700, grad_norm=4.2351393699646, loss=2.7543270587921143
I0131 02:41:34.519897 139906949052160 logging_writer.py:48] [59800] global_step=59800, grad_norm=3.678565502166748, loss=2.759904384613037
I0131 02:42:08.260747 139906756101888 logging_writer.py:48] [59900] global_step=59900, grad_norm=3.1769638061523438, loss=2.642090082168579
I0131 02:42:41.955832 139906949052160 logging_writer.py:48] [60000] global_step=60000, grad_norm=3.5452589988708496, loss=2.700315237045288
I0131 02:43:15.623052 139906756101888 logging_writer.py:48] [60100] global_step=60100, grad_norm=3.091714859008789, loss=2.7617037296295166
I0131 02:43:49.270332 139906949052160 logging_writer.py:48] [60200] global_step=60200, grad_norm=3.149592638015747, loss=2.5931100845336914
I0131 02:44:22.981432 139906756101888 logging_writer.py:48] [60300] global_step=60300, grad_norm=3.182412624359131, loss=2.7732157707214355
I0131 02:44:56.644541 139906949052160 logging_writer.py:48] [60400] global_step=60400, grad_norm=3.144735336303711, loss=2.638728141784668
I0131 02:45:15.316385 140070692116288 spec.py:321] Evaluating on the training split.
I0131 02:45:21.686444 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 02:45:30.520215 140070692116288 spec.py:349] Evaluating on the test split.
I0131 02:45:32.817595 140070692116288 submission_runner.py:408] Time since start: 21361.44s, 	Step: 60457, 	{'train/accuracy': 0.6662946343421936, 'train/loss': 1.4448381662368774, 'validation/accuracy': 0.6218199729919434, 'validation/loss': 1.656185269355774, 'validation/num_examples': 50000, 'test/accuracy': 0.49400001764297485, 'test/loss': 2.3347575664520264, 'test/num_examples': 10000, 'score': 20457.742354631424, 'total_duration': 21361.444463968277, 'accumulated_submission_time': 20457.742354631424, 'accumulated_eval_time': 900.1488373279572, 'accumulated_logging_time': 1.5096936225891113}
I0131 02:45:32.844684 139907703985920 logging_writer.py:48] [60457] accumulated_eval_time=900.148837, accumulated_logging_time=1.509694, accumulated_submission_time=20457.742355, global_step=60457, preemption_count=0, score=20457.742355, test/accuracy=0.494000, test/loss=2.334758, test/num_examples=10000, total_duration=21361.444464, train/accuracy=0.666295, train/loss=1.444838, validation/accuracy=0.621820, validation/loss=1.656185, validation/num_examples=50000
I0131 02:45:47.700185 139907720771328 logging_writer.py:48] [60500] global_step=60500, grad_norm=4.016051769256592, loss=2.6299304962158203
I0131 02:46:21.352347 139907703985920 logging_writer.py:48] [60600] global_step=60600, grad_norm=3.018247365951538, loss=2.6240196228027344
I0131 02:46:55.065983 139907720771328 logging_writer.py:48] [60700] global_step=60700, grad_norm=2.570962905883789, loss=2.490182876586914
I0131 02:47:28.710635 139907703985920 logging_writer.py:48] [60800] global_step=60800, grad_norm=3.338695764541626, loss=2.7553133964538574
I0131 02:48:02.452542 139907720771328 logging_writer.py:48] [60900] global_step=60900, grad_norm=2.8599164485931396, loss=2.6139297485351562
I0131 02:48:36.241835 139907703985920 logging_writer.py:48] [61000] global_step=61000, grad_norm=3.8757565021514893, loss=2.7626285552978516
I0131 02:49:09.955524 139907720771328 logging_writer.py:48] [61100] global_step=61100, grad_norm=3.098395586013794, loss=2.7593674659729004
I0131 02:49:43.693246 139907703985920 logging_writer.py:48] [61200] global_step=61200, grad_norm=3.2372710704803467, loss=2.6030941009521484
I0131 02:50:17.409076 139907720771328 logging_writer.py:48] [61300] global_step=61300, grad_norm=3.5843279361724854, loss=2.6371827125549316
I0131 02:50:51.088332 139907703985920 logging_writer.py:48] [61400] global_step=61400, grad_norm=3.2788500785827637, loss=2.717912197113037
I0131 02:51:24.826620 139907720771328 logging_writer.py:48] [61500] global_step=61500, grad_norm=3.2996320724487305, loss=2.6439716815948486
I0131 02:51:58.560914 139907703985920 logging_writer.py:48] [61600] global_step=61600, grad_norm=3.141185998916626, loss=2.640944004058838
I0131 02:52:32.294833 139907720771328 logging_writer.py:48] [61700] global_step=61700, grad_norm=2.833472490310669, loss=2.6298654079437256
I0131 02:53:06.011306 139907703985920 logging_writer.py:48] [61800] global_step=61800, grad_norm=3.287479877471924, loss=2.678013324737549
I0131 02:53:39.774715 139907720771328 logging_writer.py:48] [61900] global_step=61900, grad_norm=3.6592063903808594, loss=2.7095248699188232
I0131 02:54:03.141614 140070692116288 spec.py:321] Evaluating on the training split.
I0131 02:54:09.496117 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 02:54:18.252785 140070692116288 spec.py:349] Evaluating on the test split.
I0131 02:54:20.542531 140070692116288 submission_runner.py:408] Time since start: 21889.17s, 	Step: 61971, 	{'train/accuracy': 0.6689453125, 'train/loss': 1.4233334064483643, 'validation/accuracy': 0.6264599561691284, 'validation/loss': 1.6362653970718384, 'validation/num_examples': 50000, 'test/accuracy': 0.5004000067710876, 'test/loss': 2.3209445476531982, 'test/num_examples': 10000, 'score': 20967.97685956955, 'total_duration': 21889.169400691986, 'accumulated_submission_time': 20967.97685956955, 'accumulated_eval_time': 917.5497057437897, 'accumulated_logging_time': 1.5463056564331055}
I0131 02:54:20.572017 139907712378624 logging_writer.py:48] [61971] accumulated_eval_time=917.549706, accumulated_logging_time=1.546306, accumulated_submission_time=20967.976860, global_step=61971, preemption_count=0, score=20967.976860, test/accuracy=0.500400, test/loss=2.320945, test/num_examples=10000, total_duration=21889.169401, train/accuracy=0.668945, train/loss=1.423333, validation/accuracy=0.626460, validation/loss=1.636265, validation/num_examples=50000
I0131 02:54:30.789748 139907729164032 logging_writer.py:48] [62000] global_step=62000, grad_norm=2.9864394664764404, loss=2.7581214904785156
I0131 02:55:04.457566 139907712378624 logging_writer.py:48] [62100] global_step=62100, grad_norm=3.186809539794922, loss=2.6366803646087646
I0131 02:55:38.150810 139907729164032 logging_writer.py:48] [62200] global_step=62200, grad_norm=3.4954192638397217, loss=2.722888469696045
I0131 02:56:11.812162 139907712378624 logging_writer.py:48] [62300] global_step=62300, grad_norm=3.044703722000122, loss=2.5839619636535645
I0131 02:56:45.495077 139907729164032 logging_writer.py:48] [62400] global_step=62400, grad_norm=3.660911798477173, loss=2.6643593311309814
I0131 02:57:19.220652 139907712378624 logging_writer.py:48] [62500] global_step=62500, grad_norm=3.588387966156006, loss=2.6501717567443848
I0131 02:57:52.906050 139907729164032 logging_writer.py:48] [62600] global_step=62600, grad_norm=3.633655548095703, loss=2.666447162628174
I0131 02:58:26.575396 139907712378624 logging_writer.py:48] [62700] global_step=62700, grad_norm=4.2665557861328125, loss=2.766599416732788
I0131 02:59:00.278274 139907729164032 logging_writer.py:48] [62800] global_step=62800, grad_norm=2.936225652694702, loss=2.622580051422119
I0131 02:59:33.949254 139907712378624 logging_writer.py:48] [62900] global_step=62900, grad_norm=3.4856879711151123, loss=2.727025032043457
I0131 03:00:07.619628 139907729164032 logging_writer.py:48] [63000] global_step=63000, grad_norm=3.065138816833496, loss=2.5981218814849854
I0131 03:00:41.383391 139907712378624 logging_writer.py:48] [63100] global_step=63100, grad_norm=3.2847259044647217, loss=2.689157724380493
I0131 03:01:15.108469 139907729164032 logging_writer.py:48] [63200] global_step=63200, grad_norm=4.582042694091797, loss=2.6597156524658203
I0131 03:01:48.781326 139907712378624 logging_writer.py:48] [63300] global_step=63300, grad_norm=3.083139181137085, loss=2.70205020904541
I0131 03:02:22.486706 139907729164032 logging_writer.py:48] [63400] global_step=63400, grad_norm=2.953887462615967, loss=2.689980983734131
I0131 03:02:50.558568 140070692116288 spec.py:321] Evaluating on the training split.
I0131 03:02:56.914229 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 03:03:05.884338 140070692116288 spec.py:349] Evaluating on the test split.
I0131 03:03:08.221119 140070692116288 submission_runner.py:408] Time since start: 22416.85s, 	Step: 63485, 	{'train/accuracy': 0.7110969424247742, 'train/loss': 1.2208093404769897, 'validation/accuracy': 0.6271799802780151, 'validation/loss': 1.6009138822555542, 'validation/num_examples': 50000, 'test/accuracy': 0.5062000155448914, 'test/loss': 2.2677626609802246, 'test/num_examples': 10000, 'score': 21477.900118112564, 'total_duration': 22416.847967386246, 'accumulated_submission_time': 21477.900118112564, 'accumulated_eval_time': 935.2121860980988, 'accumulated_logging_time': 1.5857601165771484}
I0131 03:03:08.257252 139906949052160 logging_writer.py:48] [63485] accumulated_eval_time=935.212186, accumulated_logging_time=1.585760, accumulated_submission_time=21477.900118, global_step=63485, preemption_count=0, score=21477.900118, test/accuracy=0.506200, test/loss=2.267763, test/num_examples=10000, total_duration=22416.847967, train/accuracy=0.711097, train/loss=1.220809, validation/accuracy=0.627180, validation/loss=1.600914, validation/num_examples=50000
I0131 03:03:13.653178 139907703985920 logging_writer.py:48] [63500] global_step=63500, grad_norm=3.4313788414001465, loss=2.589128017425537
I0131 03:03:47.271681 139906949052160 logging_writer.py:48] [63600] global_step=63600, grad_norm=3.35790753364563, loss=2.7691478729248047
I0131 03:04:20.989903 139907703985920 logging_writer.py:48] [63700] global_step=63700, grad_norm=3.4703638553619385, loss=2.6677651405334473
I0131 03:04:54.691425 139906949052160 logging_writer.py:48] [63800] global_step=63800, grad_norm=3.4918429851531982, loss=2.7142090797424316
I0131 03:05:28.357457 139907703985920 logging_writer.py:48] [63900] global_step=63900, grad_norm=3.067661762237549, loss=2.637028217315674
I0131 03:06:02.027128 139906949052160 logging_writer.py:48] [64000] global_step=64000, grad_norm=4.201891899108887, loss=2.6423275470733643
I0131 03:06:35.774769 139907703985920 logging_writer.py:48] [64100] global_step=64100, grad_norm=3.8854830265045166, loss=2.656219482421875
I0131 03:07:09.521413 139906949052160 logging_writer.py:48] [64200] global_step=64200, grad_norm=3.416405200958252, loss=2.5936975479125977
I0131 03:07:43.186158 139907703985920 logging_writer.py:48] [64300] global_step=64300, grad_norm=3.1368470191955566, loss=2.736449956893921
I0131 03:08:16.884584 139906949052160 logging_writer.py:48] [64400] global_step=64400, grad_norm=3.107802629470825, loss=2.7321150302886963
I0131 03:08:50.540361 139907703985920 logging_writer.py:48] [64500] global_step=64500, grad_norm=3.529062032699585, loss=2.7441837787628174
I0131 03:09:24.277659 139906949052160 logging_writer.py:48] [64600] global_step=64600, grad_norm=3.5197927951812744, loss=2.637928009033203
I0131 03:09:57.974790 139907703985920 logging_writer.py:48] [64700] global_step=64700, grad_norm=3.181685209274292, loss=2.6513493061065674
I0131 03:10:31.708822 139906949052160 logging_writer.py:48] [64800] global_step=64800, grad_norm=3.2935681343078613, loss=2.7345163822174072
I0131 03:11:05.363860 139907703985920 logging_writer.py:48] [64900] global_step=64900, grad_norm=3.7869980335235596, loss=2.628075122833252
I0131 03:11:38.534816 140070692116288 spec.py:321] Evaluating on the training split.
I0131 03:11:44.979527 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 03:11:53.706026 140070692116288 spec.py:349] Evaluating on the test split.
I0131 03:11:55.959562 140070692116288 submission_runner.py:408] Time since start: 22944.59s, 	Step: 65000, 	{'train/accuracy': 0.6969068646430969, 'train/loss': 1.2933167219161987, 'validation/accuracy': 0.6342200040817261, 'validation/loss': 1.5750954151153564, 'validation/num_examples': 50000, 'test/accuracy': 0.5092000365257263, 'test/loss': 2.258240222930908, 'test/num_examples': 10000, 'score': 21988.11581516266, 'total_duration': 22944.586437940598, 'accumulated_submission_time': 21988.11581516266, 'accumulated_eval_time': 952.6368882656097, 'accumulated_logging_time': 1.6321525573730469}
I0131 03:11:55.988191 139906827380480 logging_writer.py:48] [65000] accumulated_eval_time=952.636888, accumulated_logging_time=1.632153, accumulated_submission_time=21988.115815, global_step=65000, preemption_count=0, score=21988.115815, test/accuracy=0.509200, test/loss=2.258240, test/num_examples=10000, total_duration=22944.586438, train/accuracy=0.696907, train/loss=1.293317, validation/accuracy=0.634220, validation/loss=1.575095, validation/num_examples=50000
I0131 03:11:56.343530 139906835773184 logging_writer.py:48] [65000] global_step=65000, grad_norm=3.156148672103882, loss=2.59173846244812
I0131 03:12:30.038221 139906827380480 logging_writer.py:48] [65100] global_step=65100, grad_norm=3.3614137172698975, loss=2.631598472595215
I0131 03:13:03.791822 139906835773184 logging_writer.py:48] [65200] global_step=65200, grad_norm=3.786738634109497, loss=2.6492667198181152
I0131 03:13:37.524317 139906827380480 logging_writer.py:48] [65300] global_step=65300, grad_norm=3.551054000854492, loss=2.6315908432006836
I0131 03:14:11.175615 139906835773184 logging_writer.py:48] [65400] global_step=65400, grad_norm=4.413917064666748, loss=2.561762571334839
I0131 03:14:44.873683 139906827380480 logging_writer.py:48] [65500] global_step=65500, grad_norm=3.1631722450256348, loss=2.754448890686035
I0131 03:15:18.525053 139906835773184 logging_writer.py:48] [65600] global_step=65600, grad_norm=3.167680025100708, loss=2.5245344638824463
I0131 03:15:52.221318 139906827380480 logging_writer.py:48] [65700] global_step=65700, grad_norm=3.4988596439361572, loss=2.6495234966278076
I0131 03:16:25.880376 139906835773184 logging_writer.py:48] [65800] global_step=65800, grad_norm=3.5971858501434326, loss=2.6186044216156006
I0131 03:16:59.574630 139906827380480 logging_writer.py:48] [65900] global_step=65900, grad_norm=3.6386311054229736, loss=2.645591974258423
I0131 03:17:33.260782 139906835773184 logging_writer.py:48] [66000] global_step=66000, grad_norm=3.299898624420166, loss=2.757268190383911
I0131 03:18:06.961221 139906827380480 logging_writer.py:48] [66100] global_step=66100, grad_norm=3.337803363800049, loss=2.6616110801696777
I0131 03:18:40.614459 139906835773184 logging_writer.py:48] [66200] global_step=66200, grad_norm=2.9374890327453613, loss=2.67156720161438
I0131 03:19:14.308456 139906827380480 logging_writer.py:48] [66300] global_step=66300, grad_norm=3.2623565196990967, loss=2.6135411262512207
I0131 03:19:48.035591 139906835773184 logging_writer.py:48] [66400] global_step=66400, grad_norm=3.537144899368286, loss=2.646169424057007
I0131 03:20:21.767850 139906827380480 logging_writer.py:48] [66500] global_step=66500, grad_norm=3.423166036605835, loss=2.583345413208008
I0131 03:20:26.290146 140070692116288 spec.py:321] Evaluating on the training split.
I0131 03:20:32.748089 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 03:20:41.411555 140070692116288 spec.py:349] Evaluating on the test split.
I0131 03:20:43.698164 140070692116288 submission_runner.py:408] Time since start: 23472.33s, 	Step: 66515, 	{'train/accuracy': 0.6873405575752258, 'train/loss': 1.344896912574768, 'validation/accuracy': 0.632099986076355, 'validation/loss': 1.5866305828094482, 'validation/num_examples': 50000, 'test/accuracy': 0.5099000334739685, 'test/loss': 2.246936559677124, 'test/num_examples': 10000, 'score': 22498.35578727722, 'total_duration': 23472.325043201447, 'accumulated_submission_time': 22498.35578727722, 'accumulated_eval_time': 970.0448710918427, 'accumulated_logging_time': 1.6718873977661133}
I0131 03:20:43.727045 139907720771328 logging_writer.py:48] [66515] accumulated_eval_time=970.044871, accumulated_logging_time=1.671887, accumulated_submission_time=22498.355787, global_step=66515, preemption_count=0, score=22498.355787, test/accuracy=0.509900, test/loss=2.246937, test/num_examples=10000, total_duration=23472.325043, train/accuracy=0.687341, train/loss=1.344897, validation/accuracy=0.632100, validation/loss=1.586631, validation/num_examples=50000
I0131 03:21:12.708725 139907729164032 logging_writer.py:48] [66600] global_step=66600, grad_norm=3.103686809539795, loss=2.7134597301483154
I0131 03:21:46.405369 139907720771328 logging_writer.py:48] [66700] global_step=66700, grad_norm=3.175912857055664, loss=2.6759345531463623
I0131 03:22:20.081770 139907729164032 logging_writer.py:48] [66800] global_step=66800, grad_norm=3.685459852218628, loss=2.5680181980133057
I0131 03:22:53.759749 139907720771328 logging_writer.py:48] [66900] global_step=66900, grad_norm=2.903549909591675, loss=2.6245343685150146
I0131 03:23:27.439047 139907729164032 logging_writer.py:48] [67000] global_step=67000, grad_norm=2.9077844619750977, loss=2.649707794189453
I0131 03:24:01.117990 139907720771328 logging_writer.py:48] [67100] global_step=67100, grad_norm=3.105008840560913, loss=2.612213134765625
I0131 03:24:34.755331 139907729164032 logging_writer.py:48] [67200] global_step=67200, grad_norm=3.0840322971343994, loss=2.6006836891174316
I0131 03:25:08.517522 139907720771328 logging_writer.py:48] [67300] global_step=67300, grad_norm=3.020153760910034, loss=2.721473217010498
I0131 03:25:42.235318 139907729164032 logging_writer.py:48] [67400] global_step=67400, grad_norm=3.0413520336151123, loss=2.674931287765503
I0131 03:26:15.914128 139907720771328 logging_writer.py:48] [67500] global_step=67500, grad_norm=3.95072340965271, loss=2.623457193374634
I0131 03:26:49.579193 139907729164032 logging_writer.py:48] [67600] global_step=67600, grad_norm=3.234285593032837, loss=2.6835713386535645
I0131 03:27:23.246455 139907720771328 logging_writer.py:48] [67700] global_step=67700, grad_norm=4.124347686767578, loss=2.686872959136963
I0131 03:27:56.911591 139907729164032 logging_writer.py:48] [67800] global_step=67800, grad_norm=3.111262559890747, loss=2.625394582748413
I0131 03:28:30.580676 139907720771328 logging_writer.py:48] [67900] global_step=67900, grad_norm=3.0071470737457275, loss=2.6619679927825928
I0131 03:29:04.249861 139907729164032 logging_writer.py:48] [68000] global_step=68000, grad_norm=3.1798765659332275, loss=2.5400304794311523
I0131 03:29:13.839572 140070692116288 spec.py:321] Evaluating on the training split.
I0131 03:29:20.205956 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 03:29:28.861332 140070692116288 spec.py:349] Evaluating on the test split.
I0131 03:29:31.155996 140070692116288 submission_runner.py:408] Time since start: 23999.78s, 	Step: 68030, 	{'train/accuracy': 0.6845503449440002, 'train/loss': 1.392478346824646, 'validation/accuracy': 0.6325799822807312, 'validation/loss': 1.6152527332305908, 'validation/num_examples': 50000, 'test/accuracy': 0.5042000412940979, 'test/loss': 2.301208734512329, 'test/num_examples': 10000, 'score': 23008.404548883438, 'total_duration': 23999.782874584198, 'accumulated_submission_time': 23008.404548883438, 'accumulated_eval_time': 987.3612501621246, 'accumulated_logging_time': 1.7124810218811035}
I0131 03:29:31.185341 139906835773184 logging_writer.py:48] [68030] accumulated_eval_time=987.361250, accumulated_logging_time=1.712481, accumulated_submission_time=23008.404549, global_step=68030, preemption_count=0, score=23008.404549, test/accuracy=0.504200, test/loss=2.301209, test/num_examples=10000, total_duration=23999.782875, train/accuracy=0.684550, train/loss=1.392478, validation/accuracy=0.632580, validation/loss=1.615253, validation/num_examples=50000
I0131 03:29:55.046576 139906949052160 logging_writer.py:48] [68100] global_step=68100, grad_norm=2.9319241046905518, loss=2.7221715450286865
I0131 03:30:28.746831 139906835773184 logging_writer.py:48] [68200] global_step=68200, grad_norm=4.010598182678223, loss=2.638127326965332
I0131 03:31:02.381047 139906949052160 logging_writer.py:48] [68300] global_step=68300, grad_norm=3.2922189235687256, loss=2.7287886142730713
I0131 03:31:36.233880 139906835773184 logging_writer.py:48] [68400] global_step=68400, grad_norm=3.849059820175171, loss=2.6426966190338135
I0131 03:32:09.914508 139906949052160 logging_writer.py:48] [68500] global_step=68500, grad_norm=3.2216155529022217, loss=2.66902232170105
I0131 03:32:43.596777 139906835773184 logging_writer.py:48] [68600] global_step=68600, grad_norm=4.007390975952148, loss=2.585306167602539
I0131 03:33:17.255726 139906949052160 logging_writer.py:48] [68700] global_step=68700, grad_norm=3.916905164718628, loss=2.6335973739624023
I0131 03:33:50.956414 139906835773184 logging_writer.py:48] [68800] global_step=68800, grad_norm=3.3737306594848633, loss=2.6518054008483887
I0131 03:34:24.614294 139906949052160 logging_writer.py:48] [68900] global_step=68900, grad_norm=3.2512712478637695, loss=2.777080774307251
I0131 03:34:58.333197 139906835773184 logging_writer.py:48] [69000] global_step=69000, grad_norm=3.1193366050720215, loss=2.6422786712646484
I0131 03:35:31.983163 139906949052160 logging_writer.py:48] [69100] global_step=69100, grad_norm=3.239755153656006, loss=2.741143226623535
I0131 03:36:05.678390 139906835773184 logging_writer.py:48] [69200] global_step=69200, grad_norm=3.726815938949585, loss=2.6863324642181396
I0131 03:36:39.336303 139906949052160 logging_writer.py:48] [69300] global_step=69300, grad_norm=3.419309616088867, loss=2.716895341873169
I0131 03:37:13.142325 139906835773184 logging_writer.py:48] [69400] global_step=69400, grad_norm=3.9781813621520996, loss=2.598278522491455
I0131 03:37:46.859679 139906949052160 logging_writer.py:48] [69500] global_step=69500, grad_norm=3.6033287048339844, loss=2.675118923187256
I0131 03:38:01.179076 140070692116288 spec.py:321] Evaluating on the training split.
I0131 03:38:07.651199 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 03:38:16.577201 140070692116288 spec.py:349] Evaluating on the test split.
I0131 03:38:18.827325 140070692116288 submission_runner.py:408] Time since start: 24527.45s, 	Step: 69544, 	{'train/accuracy': 0.6779336333274841, 'train/loss': 1.4261982440948486, 'validation/accuracy': 0.6293999552726746, 'validation/loss': 1.6455281972885132, 'validation/num_examples': 50000, 'test/accuracy': 0.5063000321388245, 'test/loss': 2.339195966720581, 'test/num_examples': 10000, 'score': 23518.33584046364, 'total_duration': 24527.454204559326, 'accumulated_submission_time': 23518.33584046364, 'accumulated_eval_time': 1005.0094563961029, 'accumulated_logging_time': 1.7519042491912842}
I0131 03:38:18.855605 139906835773184 logging_writer.py:48] [69544] accumulated_eval_time=1005.009456, accumulated_logging_time=1.751904, accumulated_submission_time=23518.335840, global_step=69544, preemption_count=0, score=23518.335840, test/accuracy=0.506300, test/loss=2.339196, test/num_examples=10000, total_duration=24527.454205, train/accuracy=0.677934, train/loss=1.426198, validation/accuracy=0.629400, validation/loss=1.645528, validation/num_examples=50000
I0131 03:38:38.023550 139907720771328 logging_writer.py:48] [69600] global_step=69600, grad_norm=3.636293649673462, loss=2.5778896808624268
I0131 03:39:11.741080 139906835773184 logging_writer.py:48] [69700] global_step=69700, grad_norm=3.4178853034973145, loss=2.5844452381134033
I0131 03:39:45.450780 139907720771328 logging_writer.py:48] [69800] global_step=69800, grad_norm=3.436985969543457, loss=2.673224687576294
I0131 03:40:19.140618 139906835773184 logging_writer.py:48] [69900] global_step=69900, grad_norm=3.314257860183716, loss=2.6161389350891113
I0131 03:40:52.815728 139907720771328 logging_writer.py:48] [70000] global_step=70000, grad_norm=3.4791650772094727, loss=2.5508828163146973
I0131 03:41:26.487926 139906835773184 logging_writer.py:48] [70100] global_step=70100, grad_norm=3.25803804397583, loss=2.6374716758728027
I0131 03:42:00.220898 139907720771328 logging_writer.py:48] [70200] global_step=70200, grad_norm=3.4591832160949707, loss=2.6640257835388184
I0131 03:42:33.884897 139906835773184 logging_writer.py:48] [70300] global_step=70300, grad_norm=3.3013291358947754, loss=2.7299046516418457
I0131 03:43:07.575325 139907720771328 logging_writer.py:48] [70400] global_step=70400, grad_norm=3.0428640842437744, loss=2.6554009914398193
I0131 03:43:41.342051 139906835773184 logging_writer.py:48] [70500] global_step=70500, grad_norm=3.4261415004730225, loss=2.698741912841797
I0131 03:44:15.058400 139907720771328 logging_writer.py:48] [70600] global_step=70600, grad_norm=3.3495824337005615, loss=2.6774094104766846
I0131 03:44:48.747623 139906835773184 logging_writer.py:48] [70700] global_step=70700, grad_norm=3.623185396194458, loss=2.6505930423736572
I0131 03:45:22.438894 139907720771328 logging_writer.py:48] [70800] global_step=70800, grad_norm=3.329923152923584, loss=2.6799254417419434
I0131 03:45:56.121418 139906835773184 logging_writer.py:48] [70900] global_step=70900, grad_norm=3.489002227783203, loss=2.5995359420776367
I0131 03:46:29.807284 139907720771328 logging_writer.py:48] [71000] global_step=71000, grad_norm=3.0589098930358887, loss=2.659388303756714
I0131 03:46:49.136405 140070692116288 spec.py:321] Evaluating on the training split.
I0131 03:46:55.431066 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 03:47:04.401900 140070692116288 spec.py:349] Evaluating on the test split.
I0131 03:47:06.735340 140070692116288 submission_runner.py:408] Time since start: 25055.36s, 	Step: 71059, 	{'train/accuracy': 0.6732102632522583, 'train/loss': 1.407064437866211, 'validation/accuracy': 0.6301599740982056, 'validation/loss': 1.6089874505996704, 'validation/num_examples': 50000, 'test/accuracy': 0.49820002913475037, 'test/loss': 2.287114143371582, 'test/num_examples': 10000, 'score': 24028.55358481407, 'total_duration': 25055.36220574379, 'accumulated_submission_time': 24028.55358481407, 'accumulated_eval_time': 1022.608335018158, 'accumulated_logging_time': 1.7905707359313965}
I0131 03:47:06.764193 139907703985920 logging_writer.py:48] [71059] accumulated_eval_time=1022.608335, accumulated_logging_time=1.790571, accumulated_submission_time=24028.553585, global_step=71059, preemption_count=0, score=24028.553585, test/accuracy=0.498200, test/loss=2.287114, test/num_examples=10000, total_duration=25055.362206, train/accuracy=0.673210, train/loss=1.407064, validation/accuracy=0.630160, validation/loss=1.608987, validation/num_examples=50000
I0131 03:47:20.921827 139907712378624 logging_writer.py:48] [71100] global_step=71100, grad_norm=3.1321909427642822, loss=2.5982697010040283
I0131 03:47:54.556479 139907703985920 logging_writer.py:48] [71200] global_step=71200, grad_norm=3.5503275394439697, loss=2.6034960746765137
I0131 03:48:28.251767 139907712378624 logging_writer.py:48] [71300] global_step=71300, grad_norm=3.44462251663208, loss=2.62882137298584
I0131 03:49:01.901710 139907703985920 logging_writer.py:48] [71400] global_step=71400, grad_norm=4.378180980682373, loss=2.731116771697998
I0131 03:49:35.689965 139907712378624 logging_writer.py:48] [71500] global_step=71500, grad_norm=3.109004497528076, loss=2.6248183250427246
I0131 03:50:09.402892 139907703985920 logging_writer.py:48] [71600] global_step=71600, grad_norm=3.2790286540985107, loss=2.7028822898864746
I0131 03:50:43.115592 139907712378624 logging_writer.py:48] [71700] global_step=71700, grad_norm=3.3433268070220947, loss=2.678417205810547
I0131 03:51:16.769303 139907703985920 logging_writer.py:48] [71800] global_step=71800, grad_norm=3.2987523078918457, loss=2.671874761581421
I0131 03:51:50.473116 139907712378624 logging_writer.py:48] [71900] global_step=71900, grad_norm=3.9061026573181152, loss=2.5875244140625
I0131 03:52:24.161973 139907703985920 logging_writer.py:48] [72000] global_step=72000, grad_norm=3.1530814170837402, loss=2.5841426849365234
I0131 03:52:57.903733 139907712378624 logging_writer.py:48] [72100] global_step=72100, grad_norm=3.8658840656280518, loss=2.5619454383850098
I0131 03:53:31.575666 139907703985920 logging_writer.py:48] [72200] global_step=72200, grad_norm=3.7728493213653564, loss=2.5975542068481445
I0131 03:54:05.254645 139907712378624 logging_writer.py:48] [72300] global_step=72300, grad_norm=3.288599967956543, loss=2.6487221717834473
I0131 03:54:38.907101 139907703985920 logging_writer.py:48] [72400] global_step=72400, grad_norm=3.3767101764678955, loss=2.664409637451172
I0131 03:55:12.598529 139907712378624 logging_writer.py:48] [72500] global_step=72500, grad_norm=3.309688091278076, loss=2.675750732421875
I0131 03:55:37.031360 140070692116288 spec.py:321] Evaluating on the training split.
I0131 03:55:43.262668 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 03:55:53.570340 140070692116288 spec.py:349] Evaluating on the test split.
I0131 03:55:55.752221 140070692116288 submission_runner.py:408] Time since start: 25584.38s, 	Step: 72574, 	{'train/accuracy': 0.7083266973495483, 'train/loss': 1.2449584007263184, 'validation/accuracy': 0.6342399716377258, 'validation/loss': 1.5873527526855469, 'validation/num_examples': 50000, 'test/accuracy': 0.5004000067710876, 'test/loss': 2.2946672439575195, 'test/num_examples': 10000, 'score': 24538.758437633514, 'total_duration': 25584.37909555435, 'accumulated_submission_time': 24538.758437633514, 'accumulated_eval_time': 1041.3291449546814, 'accumulated_logging_time': 1.8296470642089844}
I0131 03:55:55.781234 139907729164032 logging_writer.py:48] [72574] accumulated_eval_time=1041.329145, accumulated_logging_time=1.829647, accumulated_submission_time=24538.758438, global_step=72574, preemption_count=0, score=24538.758438, test/accuracy=0.500400, test/loss=2.294667, test/num_examples=10000, total_duration=25584.379096, train/accuracy=0.708327, train/loss=1.244958, validation/accuracy=0.634240, validation/loss=1.587353, validation/num_examples=50000
I0131 03:56:04.892249 139907737556736 logging_writer.py:48] [72600] global_step=72600, grad_norm=3.655165195465088, loss=2.7389118671417236
I0131 03:56:38.521010 139907729164032 logging_writer.py:48] [72700] global_step=72700, grad_norm=3.5440335273742676, loss=2.768934726715088
I0131 03:57:12.202631 139907737556736 logging_writer.py:48] [72800] global_step=72800, grad_norm=4.305388927459717, loss=2.730827808380127
I0131 03:57:45.872519 139907729164032 logging_writer.py:48] [72900] global_step=72900, grad_norm=3.5566821098327637, loss=2.676241874694824
I0131 03:58:19.546495 139907737556736 logging_writer.py:48] [73000] global_step=73000, grad_norm=3.6404941082000732, loss=2.6196274757385254
I0131 03:58:53.242854 139907729164032 logging_writer.py:48] [73100] global_step=73100, grad_norm=3.4428584575653076, loss=2.6628854274749756
I0131 03:59:26.926684 139907737556736 logging_writer.py:48] [73200] global_step=73200, grad_norm=3.986642837524414, loss=2.643465280532837
I0131 04:00:00.666336 139907729164032 logging_writer.py:48] [73300] global_step=73300, grad_norm=4.1346540451049805, loss=2.6901488304138184
I0131 04:00:34.353806 139907737556736 logging_writer.py:48] [73400] global_step=73400, grad_norm=4.001355171203613, loss=2.627371311187744
I0131 04:01:08.003644 139907729164032 logging_writer.py:48] [73500] global_step=73500, grad_norm=3.9256467819213867, loss=2.6597390174865723
I0131 04:01:41.688381 139907737556736 logging_writer.py:48] [73600] global_step=73600, grad_norm=3.6002049446105957, loss=2.609691858291626
I0131 04:02:15.432667 139907729164032 logging_writer.py:48] [73700] global_step=73700, grad_norm=3.1111745834350586, loss=2.7039413452148438
I0131 04:02:49.132549 139907737556736 logging_writer.py:48] [73800] global_step=73800, grad_norm=3.8476574420928955, loss=2.6824028491973877
I0131 04:03:22.787054 139907729164032 logging_writer.py:48] [73900] global_step=73900, grad_norm=3.402867078781128, loss=2.628298282623291
I0131 04:03:56.463291 139907737556736 logging_writer.py:48] [74000] global_step=74000, grad_norm=3.262305498123169, loss=2.570101261138916
I0131 04:04:25.903500 140070692116288 spec.py:321] Evaluating on the training split.
I0131 04:04:32.177683 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 04:04:41.066583 140070692116288 spec.py:349] Evaluating on the test split.
I0131 04:04:43.391897 140070692116288 submission_runner.py:408] Time since start: 26112.02s, 	Step: 74089, 	{'train/accuracy': 0.6872608065605164, 'train/loss': 1.3399715423583984, 'validation/accuracy': 0.6256399750709534, 'validation/loss': 1.6160084009170532, 'validation/num_examples': 50000, 'test/accuracy': 0.5052000284194946, 'test/loss': 2.28117036819458, 'test/num_examples': 10000, 'score': 25048.81888151169, 'total_duration': 26112.01877140999, 'accumulated_submission_time': 25048.81888151169, 'accumulated_eval_time': 1058.8174991607666, 'accumulated_logging_time': 1.8682844638824463}
I0131 04:04:43.420452 139906949052160 logging_writer.py:48] [74089] accumulated_eval_time=1058.817499, accumulated_logging_time=1.868284, accumulated_submission_time=25048.818882, global_step=74089, preemption_count=0, score=25048.818882, test/accuracy=0.505200, test/loss=2.281170, test/num_examples=10000, total_duration=26112.018771, train/accuracy=0.687261, train/loss=1.339972, validation/accuracy=0.625640, validation/loss=1.616008, validation/num_examples=50000
I0131 04:04:47.449633 139907703985920 logging_writer.py:48] [74100] global_step=74100, grad_norm=3.4694583415985107, loss=2.6978938579559326
I0131 04:05:21.066592 139906949052160 logging_writer.py:48] [74200] global_step=74200, grad_norm=3.1269052028656006, loss=2.600299119949341
I0131 04:05:54.741285 139907703985920 logging_writer.py:48] [74300] global_step=74300, grad_norm=3.4000773429870605, loss=2.6392297744750977
I0131 04:06:28.450651 139906949052160 logging_writer.py:48] [74400] global_step=74400, grad_norm=3.460752487182617, loss=2.5341365337371826
I0131 04:07:02.124107 139907703985920 logging_writer.py:48] [74500] global_step=74500, grad_norm=3.530233860015869, loss=2.5882456302642822
I0131 04:07:35.814820 139906949052160 logging_writer.py:48] [74600] global_step=74600, grad_norm=3.6048574447631836, loss=2.6790032386779785
I0131 04:08:09.632670 139907703985920 logging_writer.py:48] [74700] global_step=74700, grad_norm=3.2936792373657227, loss=2.6809206008911133
I0131 04:08:43.357067 139906949052160 logging_writer.py:48] [74800] global_step=74800, grad_norm=3.200559139251709, loss=2.682438850402832
I0131 04:09:17.005400 139907703985920 logging_writer.py:48] [74900] global_step=74900, grad_norm=3.891573190689087, loss=2.6095192432403564
I0131 04:09:50.696382 139906949052160 logging_writer.py:48] [75000] global_step=75000, grad_norm=3.3297979831695557, loss=2.758847236633301
I0131 04:10:24.365322 139907703985920 logging_writer.py:48] [75100] global_step=75100, grad_norm=3.5969839096069336, loss=2.7016046047210693
I0131 04:10:58.066026 139906949052160 logging_writer.py:48] [75200] global_step=75200, grad_norm=3.3812568187713623, loss=2.5490012168884277
I0131 04:11:31.726012 139907703985920 logging_writer.py:48] [75300] global_step=75300, grad_norm=3.670440673828125, loss=2.5900096893310547
I0131 04:12:05.412519 139906949052160 logging_writer.py:48] [75400] global_step=75400, grad_norm=3.5436692237854004, loss=2.585956335067749
I0131 04:12:39.069785 139907703985920 logging_writer.py:48] [75500] global_step=75500, grad_norm=4.423444747924805, loss=2.7409157752990723
I0131 04:13:12.746425 139906949052160 logging_writer.py:48] [75600] global_step=75600, grad_norm=3.4130184650421143, loss=2.678225517272949
I0131 04:13:13.568629 140070692116288 spec.py:321] Evaluating on the training split.
I0131 04:13:20.426985 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 04:13:29.272164 140070692116288 spec.py:349] Evaluating on the test split.
I0131 04:13:31.668799 140070692116288 submission_runner.py:408] Time since start: 26640.30s, 	Step: 75604, 	{'train/accuracy': 0.7000358700752258, 'train/loss': 1.2867945432662964, 'validation/accuracy': 0.6447199583053589, 'validation/loss': 1.5424926280975342, 'validation/num_examples': 50000, 'test/accuracy': 0.5152000188827515, 'test/loss': 2.232867956161499, 'test/num_examples': 10000, 'score': 25558.904240608215, 'total_duration': 26640.29567885399, 'accumulated_submission_time': 25558.904240608215, 'accumulated_eval_time': 1076.917620420456, 'accumulated_logging_time': 1.9076271057128906}
I0131 04:13:31.697889 139907720771328 logging_writer.py:48] [75604] accumulated_eval_time=1076.917620, accumulated_logging_time=1.907627, accumulated_submission_time=25558.904241, global_step=75604, preemption_count=0, score=25558.904241, test/accuracy=0.515200, test/loss=2.232868, test/num_examples=10000, total_duration=26640.295679, train/accuracy=0.700036, train/loss=1.286795, validation/accuracy=0.644720, validation/loss=1.542493, validation/num_examples=50000
I0131 04:14:04.427950 139907729164032 logging_writer.py:48] [75700] global_step=75700, grad_norm=3.304251194000244, loss=2.6561732292175293
I0131 04:14:38.128616 139907720771328 logging_writer.py:48] [75800] global_step=75800, grad_norm=3.7116198539733887, loss=2.638620615005493
I0131 04:15:11.833691 139907729164032 logging_writer.py:48] [75900] global_step=75900, grad_norm=4.43892240524292, loss=2.588862895965576
I0131 04:15:45.504802 139907720771328 logging_writer.py:48] [76000] global_step=76000, grad_norm=4.766220569610596, loss=2.668165683746338
I0131 04:16:19.152602 139907729164032 logging_writer.py:48] [76100] global_step=76100, grad_norm=3.54815936088562, loss=2.597644567489624
I0131 04:16:52.821552 139907720771328 logging_writer.py:48] [76200] global_step=76200, grad_norm=3.2663331031799316, loss=2.767540693283081
I0131 04:17:26.507988 139907729164032 logging_writer.py:48] [76300] global_step=76300, grad_norm=3.9793553352355957, loss=2.6187734603881836
I0131 04:18:00.164272 139907720771328 logging_writer.py:48] [76400] global_step=76400, grad_norm=3.2935988903045654, loss=2.591843843460083
I0131 04:18:33.868794 139907729164032 logging_writer.py:48] [76500] global_step=76500, grad_norm=3.9243264198303223, loss=2.6393020153045654
I0131 04:19:07.549441 139907720771328 logging_writer.py:48] [76600] global_step=76600, grad_norm=3.9263346195220947, loss=2.6384830474853516
I0131 04:19:41.206713 139907729164032 logging_writer.py:48] [76700] global_step=76700, grad_norm=3.6574628353118896, loss=2.587226629257202
I0131 04:20:14.965610 139907720771328 logging_writer.py:48] [76800] global_step=76800, grad_norm=3.4057493209838867, loss=2.636942148208618
I0131 04:20:48.701704 139907729164032 logging_writer.py:48] [76900] global_step=76900, grad_norm=3.3976571559906006, loss=2.5825796127319336
I0131 04:21:22.372106 139907720771328 logging_writer.py:48] [77000] global_step=77000, grad_norm=3.7421092987060547, loss=2.63380765914917
I0131 04:21:56.060919 139907729164032 logging_writer.py:48] [77100] global_step=77100, grad_norm=3.382150650024414, loss=2.6541128158569336
I0131 04:22:01.946401 140070692116288 spec.py:321] Evaluating on the training split.
I0131 04:22:08.197717 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 04:22:17.350896 140070692116288 spec.py:349] Evaluating on the test split.
I0131 04:22:19.598027 140070692116288 submission_runner.py:408] Time since start: 27168.22s, 	Step: 77119, 	{'train/accuracy': 0.6952527165412903, 'train/loss': 1.321738600730896, 'validation/accuracy': 0.647599995136261, 'validation/loss': 1.5462092161178589, 'validation/num_examples': 50000, 'test/accuracy': 0.5154000520706177, 'test/loss': 2.2142727375030518, 'test/num_examples': 10000, 'score': 26069.08917927742, 'total_duration': 27168.2249045372, 'accumulated_submission_time': 26069.08917927742, 'accumulated_eval_time': 1094.5691964626312, 'accumulated_logging_time': 1.9482781887054443}
I0131 04:22:19.631317 139907712378624 logging_writer.py:48] [77119] accumulated_eval_time=1094.569196, accumulated_logging_time=1.948278, accumulated_submission_time=26069.089179, global_step=77119, preemption_count=0, score=26069.089179, test/accuracy=0.515400, test/loss=2.214273, test/num_examples=10000, total_duration=27168.224905, train/accuracy=0.695253, train/loss=1.321739, validation/accuracy=0.647600, validation/loss=1.546209, validation/num_examples=50000
I0131 04:22:48.029396 139907737556736 logging_writer.py:48] [77200] global_step=77200, grad_norm=3.142066717147827, loss=2.560467004776001
I0131 04:23:21.706390 139907712378624 logging_writer.py:48] [77300] global_step=77300, grad_norm=3.922870635986328, loss=2.7261433601379395
I0131 04:23:55.367753 139907737556736 logging_writer.py:48] [77400] global_step=77400, grad_norm=4.354031562805176, loss=2.63930082321167
I0131 04:24:29.074489 139907712378624 logging_writer.py:48] [77500] global_step=77500, grad_norm=3.5712311267852783, loss=2.5972228050231934
I0131 04:25:02.733470 139907737556736 logging_writer.py:48] [77600] global_step=77600, grad_norm=3.451854705810547, loss=2.6700003147125244
I0131 04:25:36.418161 139907712378624 logging_writer.py:48] [77700] global_step=77700, grad_norm=3.411755084991455, loss=2.5994458198547363
I0131 04:26:10.081475 139907737556736 logging_writer.py:48] [77800] global_step=77800, grad_norm=3.6770174503326416, loss=2.5450246334075928
I0131 04:26:43.793532 139907712378624 logging_writer.py:48] [77900] global_step=77900, grad_norm=3.71817946434021, loss=2.6423511505126953
I0131 04:27:17.447204 139907737556736 logging_writer.py:48] [78000] global_step=78000, grad_norm=3.5826992988586426, loss=2.537961483001709
I0131 04:27:51.138791 139907712378624 logging_writer.py:48] [78100] global_step=78100, grad_norm=3.1345713138580322, loss=2.635255813598633
I0131 04:28:24.797119 139907737556736 logging_writer.py:48] [78200] global_step=78200, grad_norm=3.30184006690979, loss=2.655085563659668
I0131 04:28:58.476844 139907712378624 logging_writer.py:48] [78300] global_step=78300, grad_norm=3.14223051071167, loss=2.5709736347198486
I0131 04:29:32.137339 139907737556736 logging_writer.py:48] [78400] global_step=78400, grad_norm=3.4606151580810547, loss=2.6553707122802734
I0131 04:30:05.822340 139907712378624 logging_writer.py:48] [78500] global_step=78500, grad_norm=3.732570171356201, loss=2.6370153427124023
I0131 04:30:39.479295 139907737556736 logging_writer.py:48] [78600] global_step=78600, grad_norm=3.1118671894073486, loss=2.56207013130188
I0131 04:30:49.735240 140070692116288 spec.py:321] Evaluating on the training split.
I0131 04:30:56.030771 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 04:31:04.975956 140070692116288 spec.py:349] Evaluating on the test split.
I0131 04:31:07.896819 140070692116288 submission_runner.py:408] Time since start: 27696.52s, 	Step: 78632, 	{'train/accuracy': 0.6901904940605164, 'train/loss': 1.3460441827774048, 'validation/accuracy': 0.6373400092124939, 'validation/loss': 1.582018494606018, 'validation/num_examples': 50000, 'test/accuracy': 0.5111000537872314, 'test/loss': 2.251178503036499, 'test/num_examples': 10000, 'score': 26578.311138153076, 'total_duration': 27696.523701667786, 'accumulated_submission_time': 26578.311138153076, 'accumulated_eval_time': 1112.730740070343, 'accumulated_logging_time': 2.8109002113342285}
I0131 04:31:07.921477 139906949052160 logging_writer.py:48] [78632] accumulated_eval_time=1112.730740, accumulated_logging_time=2.810900, accumulated_submission_time=26578.311138, global_step=78632, preemption_count=0, score=26578.311138, test/accuracy=0.511100, test/loss=2.251179, test/num_examples=10000, total_duration=27696.523702, train/accuracy=0.690190, train/loss=1.346044, validation/accuracy=0.637340, validation/loss=1.582018, validation/num_examples=50000
I0131 04:31:31.133239 139907703985920 logging_writer.py:48] [78700] global_step=78700, grad_norm=4.337728023529053, loss=2.597161054611206
I0131 04:32:04.801449 139906949052160 logging_writer.py:48] [78800] global_step=78800, grad_norm=3.5481009483337402, loss=2.6501376628875732
I0131 04:32:38.481395 139907703985920 logging_writer.py:48] [78900] global_step=78900, grad_norm=3.481992721557617, loss=2.666301965713501
I0131 04:33:12.098774 139906949052160 logging_writer.py:48] [79000] global_step=79000, grad_norm=4.722594738006592, loss=2.6083452701568604
I0131 04:33:45.789215 139907703985920 logging_writer.py:48] [79100] global_step=79100, grad_norm=3.329141616821289, loss=2.560584783554077
I0131 04:34:19.492860 139906949052160 logging_writer.py:48] [79200] global_step=79200, grad_norm=3.639228343963623, loss=2.5083518028259277
I0131 04:34:53.159221 139907703985920 logging_writer.py:48] [79300] global_step=79300, grad_norm=3.2880005836486816, loss=2.7662320137023926
I0131 04:35:26.834504 139906949052160 logging_writer.py:48] [79400] global_step=79400, grad_norm=3.4447128772735596, loss=2.609229564666748
I0131 04:36:00.497215 139907703985920 logging_writer.py:48] [79500] global_step=79500, grad_norm=3.342308282852173, loss=2.5537750720977783
I0131 04:36:34.187037 139906949052160 logging_writer.py:48] [79600] global_step=79600, grad_norm=3.263674259185791, loss=2.53183913230896
I0131 04:37:07.854642 139907703985920 logging_writer.py:48] [79700] global_step=79700, grad_norm=3.655708074569702, loss=2.5760087966918945
I0131 04:37:41.561864 139906949052160 logging_writer.py:48] [79800] global_step=79800, grad_norm=3.745314359664917, loss=2.622753143310547
I0131 04:38:15.236963 139907703985920 logging_writer.py:48] [79900] global_step=79900, grad_norm=3.4449386596679688, loss=2.6391420364379883
I0131 04:38:48.920036 139906949052160 logging_writer.py:48] [80000] global_step=80000, grad_norm=3.4188060760498047, loss=2.6347806453704834
I0131 04:39:22.565294 139907703985920 logging_writer.py:48] [80100] global_step=80100, grad_norm=4.016789436340332, loss=2.6382551193237305
I0131 04:39:38.182151 140070692116288 spec.py:321] Evaluating on the training split.
I0131 04:39:44.492114 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 04:39:53.192130 140070692116288 spec.py:349] Evaluating on the test split.
I0131 04:39:55.450194 140070692116288 submission_runner.py:408] Time since start: 28224.08s, 	Step: 80148, 	{'train/accuracy': 0.6938576102256775, 'train/loss': 1.3477110862731934, 'validation/accuracy': 0.6477599740028381, 'validation/loss': 1.5738341808319092, 'validation/num_examples': 50000, 'test/accuracy': 0.5189000368118286, 'test/loss': 2.2456023693084717, 'test/num_examples': 10000, 'score': 27088.510328292847, 'total_duration': 28224.07705426216, 'accumulated_submission_time': 27088.510328292847, 'accumulated_eval_time': 1129.9987313747406, 'accumulated_logging_time': 2.8450143337249756}
I0131 04:39:55.480891 139906835773184 logging_writer.py:48] [80148] accumulated_eval_time=1129.998731, accumulated_logging_time=2.845014, accumulated_submission_time=27088.510328, global_step=80148, preemption_count=0, score=27088.510328, test/accuracy=0.518900, test/loss=2.245602, test/num_examples=10000, total_duration=28224.077054, train/accuracy=0.693858, train/loss=1.347711, validation/accuracy=0.647760, validation/loss=1.573834, validation/num_examples=50000
I0131 04:40:13.337666 139906949052160 logging_writer.py:48] [80200] global_step=80200, grad_norm=3.044856548309326, loss=2.5853254795074463
I0131 04:40:47.022142 139906835773184 logging_writer.py:48] [80300] global_step=80300, grad_norm=4.113481521606445, loss=2.6057639122009277
I0131 04:41:20.712475 139906949052160 logging_writer.py:48] [80400] global_step=80400, grad_norm=4.133307456970215, loss=2.63336181640625
I0131 04:41:54.373053 139906835773184 logging_writer.py:48] [80500] global_step=80500, grad_norm=4.124088287353516, loss=2.69580340385437
I0131 04:42:28.048936 139906949052160 logging_writer.py:48] [80600] global_step=80600, grad_norm=4.170234203338623, loss=2.541693687438965
I0131 04:43:01.704643 139906835773184 logging_writer.py:48] [80700] global_step=80700, grad_norm=3.6703739166259766, loss=2.583021402359009
I0131 04:43:35.390161 139906949052160 logging_writer.py:48] [80800] global_step=80800, grad_norm=3.968130588531494, loss=2.6509616374969482
I0131 04:44:09.052144 139906835773184 logging_writer.py:48] [80900] global_step=80900, grad_norm=3.921720027923584, loss=2.572704792022705
I0131 04:44:42.826902 139906949052160 logging_writer.py:48] [81000] global_step=81000, grad_norm=3.584346294403076, loss=2.63814640045166
I0131 04:45:16.560337 139906835773184 logging_writer.py:48] [81100] global_step=81100, grad_norm=3.9689741134643555, loss=2.5953598022460938
I0131 04:45:50.237900 139906949052160 logging_writer.py:48] [81200] global_step=81200, grad_norm=3.8873915672302246, loss=2.670518159866333
I0131 04:46:23.896397 139906835773184 logging_writer.py:48] [81300] global_step=81300, grad_norm=3.7046780586242676, loss=2.597837448120117
I0131 04:46:57.581918 139906949052160 logging_writer.py:48] [81400] global_step=81400, grad_norm=3.6403138637542725, loss=2.676530122756958
I0131 04:47:31.229418 139906835773184 logging_writer.py:48] [81500] global_step=81500, grad_norm=4.277108669281006, loss=2.584136486053467
I0131 04:48:04.906165 139906949052160 logging_writer.py:48] [81600] global_step=81600, grad_norm=3.802887201309204, loss=2.552462339401245
I0131 04:48:25.588108 140070692116288 spec.py:321] Evaluating on the training split.
I0131 04:48:31.825250 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 04:48:40.664853 140070692116288 spec.py:349] Evaluating on the test split.
I0131 04:48:43.005557 140070692116288 submission_runner.py:408] Time since start: 28751.63s, 	Step: 81663, 	{'train/accuracy': 0.7150828838348389, 'train/loss': 1.2265141010284424, 'validation/accuracy': 0.6447399854660034, 'validation/loss': 1.5567433834075928, 'validation/num_examples': 50000, 'test/accuracy': 0.5138000249862671, 'test/loss': 2.23746657371521, 'test/num_examples': 10000, 'score': 27598.556184768677, 'total_duration': 28751.632429361343, 'accumulated_submission_time': 27598.556184768677, 'accumulated_eval_time': 1147.4161262512207, 'accumulated_logging_time': 2.885287284851074}
I0131 04:48:43.036929 139906827380480 logging_writer.py:48] [81663] accumulated_eval_time=1147.416126, accumulated_logging_time=2.885287, accumulated_submission_time=27598.556185, global_step=81663, preemption_count=0, score=27598.556185, test/accuracy=0.513800, test/loss=2.237467, test/num_examples=10000, total_duration=28751.632429, train/accuracy=0.715083, train/loss=1.226514, validation/accuracy=0.644740, validation/loss=1.556743, validation/num_examples=50000
I0131 04:48:55.869336 139907703985920 logging_writer.py:48] [81700] global_step=81700, grad_norm=3.5552523136138916, loss=2.710400104522705
I0131 04:49:29.530642 139906827380480 logging_writer.py:48] [81800] global_step=81800, grad_norm=3.838048219680786, loss=2.5853824615478516
I0131 04:50:03.196040 139907703985920 logging_writer.py:48] [81900] global_step=81900, grad_norm=3.458960771560669, loss=2.538433074951172
I0131 04:50:36.833120 139906827380480 logging_writer.py:48] [82000] global_step=82000, grad_norm=3.6874752044677734, loss=2.512197256088257
I0131 04:51:10.589221 139907703985920 logging_writer.py:48] [82100] global_step=82100, grad_norm=3.5332489013671875, loss=2.6749491691589355
I0131 04:51:44.259632 139906827380480 logging_writer.py:48] [82200] global_step=82200, grad_norm=3.686138868331909, loss=2.6070334911346436
I0131 04:52:17.919959 139907703985920 logging_writer.py:48] [82300] global_step=82300, grad_norm=3.150824785232544, loss=2.6253902912139893
I0131 04:52:51.602228 139906827380480 logging_writer.py:48] [82400] global_step=82400, grad_norm=3.8683671951293945, loss=2.5231709480285645
I0131 04:53:25.372374 139907703985920 logging_writer.py:48] [82500] global_step=82500, grad_norm=3.3876304626464844, loss=2.571333646774292
I0131 04:53:59.040756 139906827380480 logging_writer.py:48] [82600] global_step=82600, grad_norm=3.7322542667388916, loss=2.6449575424194336
I0131 04:54:32.737826 139907703985920 logging_writer.py:48] [82700] global_step=82700, grad_norm=3.9305973052978516, loss=2.583563804626465
I0131 04:55:06.404902 139906827380480 logging_writer.py:48] [82800] global_step=82800, grad_norm=3.9449093341827393, loss=2.6679093837738037
I0131 04:55:40.108094 139907703985920 logging_writer.py:48] [82900] global_step=82900, grad_norm=3.9510726928710938, loss=2.539503335952759
I0131 04:56:13.811071 139906827380480 logging_writer.py:48] [83000] global_step=83000, grad_norm=3.3472373485565186, loss=2.689265727996826
I0131 04:56:47.567773 139907703985920 logging_writer.py:48] [83100] global_step=83100, grad_norm=3.9280459880828857, loss=2.6153407096862793
I0131 04:57:13.219899 140070692116288 spec.py:321] Evaluating on the training split.
I0131 04:57:19.770347 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 04:57:28.453092 140070692116288 spec.py:349] Evaluating on the test split.
I0131 04:57:30.720808 140070692116288 submission_runner.py:408] Time since start: 29279.35s, 	Step: 83177, 	{'train/accuracy': 0.7067522406578064, 'train/loss': 1.2656277418136597, 'validation/accuracy': 0.6476799845695496, 'validation/loss': 1.5454624891281128, 'validation/num_examples': 50000, 'test/accuracy': 0.5205000042915344, 'test/loss': 2.2279982566833496, 'test/num_examples': 10000, 'score': 28108.67711853981, 'total_duration': 29279.347688674927, 'accumulated_submission_time': 28108.67711853981, 'accumulated_eval_time': 1164.917008638382, 'accumulated_logging_time': 2.9261295795440674}
I0131 04:57:30.756165 139906835773184 logging_writer.py:48] [83177] accumulated_eval_time=1164.917009, accumulated_logging_time=2.926130, accumulated_submission_time=28108.677119, global_step=83177, preemption_count=0, score=28108.677119, test/accuracy=0.520500, test/loss=2.227998, test/num_examples=10000, total_duration=29279.347689, train/accuracy=0.706752, train/loss=1.265628, validation/accuracy=0.647680, validation/loss=1.545462, validation/num_examples=50000
I0131 04:57:38.822678 139906949052160 logging_writer.py:48] [83200] global_step=83200, grad_norm=3.241657018661499, loss=2.529294729232788
I0131 04:58:12.475348 139906835773184 logging_writer.py:48] [83300] global_step=83300, grad_norm=3.8783857822418213, loss=2.5560452938079834
I0131 04:58:46.168995 139906949052160 logging_writer.py:48] [83400] global_step=83400, grad_norm=3.3997178077697754, loss=2.6044082641601562
I0131 04:59:19.855272 139906835773184 logging_writer.py:48] [83500] global_step=83500, grad_norm=4.068439483642578, loss=2.5756309032440186
I0131 04:59:53.539635 139906949052160 logging_writer.py:48] [83600] global_step=83600, grad_norm=4.206206798553467, loss=2.633040428161621
I0131 05:00:27.214511 139906835773184 logging_writer.py:48] [83700] global_step=83700, grad_norm=3.5425899028778076, loss=2.589242696762085
I0131 05:01:00.891404 139906949052160 logging_writer.py:48] [83800] global_step=83800, grad_norm=3.7510488033294678, loss=2.6183834075927734
I0131 05:01:34.595193 139906835773184 logging_writer.py:48] [83900] global_step=83900, grad_norm=3.6198928356170654, loss=2.5754008293151855
I0131 05:02:08.282252 139906949052160 logging_writer.py:48] [84000] global_step=84000, grad_norm=3.4047391414642334, loss=2.6320242881774902
I0131 05:02:41.935828 139906835773184 logging_writer.py:48] [84100] global_step=84100, grad_norm=3.650528907775879, loss=2.6807308197021484
I0131 05:03:15.740118 139906949052160 logging_writer.py:48] [84200] global_step=84200, grad_norm=5.752957344055176, loss=2.5412447452545166
I0131 05:03:49.452546 139906835773184 logging_writer.py:48] [84300] global_step=84300, grad_norm=3.479133367538452, loss=2.589181900024414
I0131 05:04:23.120290 139906949052160 logging_writer.py:48] [84400] global_step=84400, grad_norm=3.9847412109375, loss=2.586878538131714
I0131 05:04:56.809691 139906835773184 logging_writer.py:48] [84500] global_step=84500, grad_norm=3.3397200107574463, loss=2.5670909881591797
I0131 05:05:30.488609 139906949052160 logging_writer.py:48] [84600] global_step=84600, grad_norm=3.4283981323242188, loss=2.5500149726867676
I0131 05:06:00.955341 140070692116288 spec.py:321] Evaluating on the training split.
I0131 05:06:07.281963 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 05:06:16.138342 140070692116288 spec.py:349] Evaluating on the test split.
I0131 05:06:18.378349 140070692116288 submission_runner.py:408] Time since start: 29807.01s, 	Step: 84692, 	{'train/accuracy': 0.70804762840271, 'train/loss': 1.2592806816101074, 'validation/accuracy': 0.6536399722099304, 'validation/loss': 1.508924961090088, 'validation/num_examples': 50000, 'test/accuracy': 0.526900053024292, 'test/loss': 2.189680576324463, 'test/num_examples': 10000, 'score': 28618.81132388115, 'total_duration': 29807.00520181656, 'accumulated_submission_time': 28618.81132388115, 'accumulated_eval_time': 1182.3399450778961, 'accumulated_logging_time': 2.973907709121704}
I0131 05:06:18.410091 139907720771328 logging_writer.py:48] [84692] accumulated_eval_time=1182.339945, accumulated_logging_time=2.973908, accumulated_submission_time=28618.811324, global_step=84692, preemption_count=0, score=28618.811324, test/accuracy=0.526900, test/loss=2.189681, test/num_examples=10000, total_duration=29807.005202, train/accuracy=0.708048, train/loss=1.259281, validation/accuracy=0.653640, validation/loss=1.508925, validation/num_examples=50000
I0131 05:06:21.438342 139907729164032 logging_writer.py:48] [84700] global_step=84700, grad_norm=4.141604900360107, loss=2.5834908485412598
I0131 05:06:55.120184 139907720771328 logging_writer.py:48] [84800] global_step=84800, grad_norm=4.1220293045043945, loss=2.527615785598755
I0131 05:07:28.788682 139907729164032 logging_writer.py:48] [84900] global_step=84900, grad_norm=4.513105869293213, loss=2.595864772796631
I0131 05:08:02.485323 139907720771328 logging_writer.py:48] [85000] global_step=85000, grad_norm=3.8528454303741455, loss=2.6391115188598633
I0131 05:08:36.168901 139907729164032 logging_writer.py:48] [85100] global_step=85100, grad_norm=3.8771097660064697, loss=2.6037158966064453
I0131 05:09:09.888897 139907720771328 logging_writer.py:48] [85200] global_step=85200, grad_norm=3.9609079360961914, loss=2.5917367935180664
I0131 05:09:43.599338 139907729164032 logging_writer.py:48] [85300] global_step=85300, grad_norm=3.6405720710754395, loss=2.591721534729004
I0131 05:10:17.311187 139907720771328 logging_writer.py:48] [85400] global_step=85400, grad_norm=3.3178186416625977, loss=2.5492870807647705
I0131 05:10:50.971253 139907729164032 logging_writer.py:48] [85500] global_step=85500, grad_norm=3.588723659515381, loss=2.693307399749756
I0131 05:11:24.653201 139907720771328 logging_writer.py:48] [85600] global_step=85600, grad_norm=3.628154993057251, loss=2.6528172492980957
I0131 05:11:58.310066 139907729164032 logging_writer.py:48] [85700] global_step=85700, grad_norm=3.7022812366485596, loss=2.6038429737091064
I0131 05:12:32.000331 139907720771328 logging_writer.py:48] [85800] global_step=85800, grad_norm=4.342007160186768, loss=2.581752061843872
I0131 05:13:05.648121 139907729164032 logging_writer.py:48] [85900] global_step=85900, grad_norm=3.287783622741699, loss=2.5379409790039062
I0131 05:13:39.336062 139907720771328 logging_writer.py:48] [86000] global_step=86000, grad_norm=3.315260648727417, loss=2.5674989223480225
I0131 05:14:12.997108 139907729164032 logging_writer.py:48] [86100] global_step=86100, grad_norm=3.408677339553833, loss=2.6351656913757324
I0131 05:14:46.703975 139907720771328 logging_writer.py:48] [86200] global_step=86200, grad_norm=3.7299606800079346, loss=2.5628602504730225
I0131 05:14:48.531803 140070692116288 spec.py:321] Evaluating on the training split.
I0131 05:14:54.815558 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 05:15:03.850222 140070692116288 spec.py:349] Evaluating on the test split.
I0131 05:15:06.169945 140070692116288 submission_runner.py:408] Time since start: 30334.80s, 	Step: 86207, 	{'train/accuracy': 0.7067123651504517, 'train/loss': 1.260049819946289, 'validation/accuracy': 0.653659999370575, 'validation/loss': 1.5093188285827637, 'validation/num_examples': 50000, 'test/accuracy': 0.5340999960899353, 'test/loss': 2.161987543106079, 'test/num_examples': 10000, 'score': 29128.871512889862, 'total_duration': 30334.796818733215, 'accumulated_submission_time': 29128.871512889862, 'accumulated_eval_time': 1199.9780325889587, 'accumulated_logging_time': 3.0153017044067383}
I0131 05:15:06.204484 139906756101888 logging_writer.py:48] [86207] accumulated_eval_time=1199.978033, accumulated_logging_time=3.015302, accumulated_submission_time=29128.871513, global_step=86207, preemption_count=0, score=29128.871513, test/accuracy=0.534100, test/loss=2.161988, test/num_examples=10000, total_duration=30334.796819, train/accuracy=0.706712, train/loss=1.260050, validation/accuracy=0.653660, validation/loss=1.509319, validation/num_examples=50000
I0131 05:15:38.076772 139906827380480 logging_writer.py:48] [86300] global_step=86300, grad_norm=3.9714996814727783, loss=2.575286388397217
I0131 05:16:11.778207 139906756101888 logging_writer.py:48] [86400] global_step=86400, grad_norm=3.685521364212036, loss=2.5988099575042725
I0131 05:16:45.467698 139906827380480 logging_writer.py:48] [86500] global_step=86500, grad_norm=4.297266483306885, loss=2.5785396099090576
I0131 05:17:19.149407 139906756101888 logging_writer.py:48] [86600] global_step=86600, grad_norm=3.92065167427063, loss=2.531175374984741
I0131 05:17:52.822105 139906827380480 logging_writer.py:48] [86700] global_step=86700, grad_norm=3.8170149326324463, loss=2.585764169692993
I0131 05:18:26.529316 139906756101888 logging_writer.py:48] [86800] global_step=86800, grad_norm=4.3611836433410645, loss=2.615736484527588
I0131 05:19:00.223676 139906827380480 logging_writer.py:48] [86900] global_step=86900, grad_norm=3.6137993335723877, loss=2.565488338470459
I0131 05:19:33.908885 139906756101888 logging_writer.py:48] [87000] global_step=87000, grad_norm=3.658853054046631, loss=2.5562427043914795
I0131 05:20:07.576268 139906827380480 logging_writer.py:48] [87100] global_step=87100, grad_norm=3.7546091079711914, loss=2.627981424331665
I0131 05:20:41.260686 139906756101888 logging_writer.py:48] [87200] global_step=87200, grad_norm=3.804680585861206, loss=2.542437791824341
I0131 05:21:14.924089 139906827380480 logging_writer.py:48] [87300] global_step=87300, grad_norm=3.506967067718506, loss=2.5368876457214355
I0131 05:21:48.742086 139906756101888 logging_writer.py:48] [87400] global_step=87400, grad_norm=3.354691743850708, loss=2.4799747467041016
I0131 05:22:22.429387 139906827380480 logging_writer.py:48] [87500] global_step=87500, grad_norm=3.5055105686187744, loss=2.5200278759002686
I0131 05:22:56.108268 139906756101888 logging_writer.py:48] [87600] global_step=87600, grad_norm=3.8997280597686768, loss=2.574097156524658
I0131 05:23:29.815615 139906827380480 logging_writer.py:48] [87700] global_step=87700, grad_norm=3.637481927871704, loss=2.572636127471924
I0131 05:23:36.365724 140070692116288 spec.py:321] Evaluating on the training split.
I0131 05:23:42.536507 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 05:23:51.348865 140070692116288 spec.py:349] Evaluating on the test split.
I0131 05:23:53.656501 140070692116288 submission_runner.py:408] Time since start: 30862.28s, 	Step: 87721, 	{'train/accuracy': 0.7043008208274841, 'train/loss': 1.2704253196716309, 'validation/accuracy': 0.6558200120925903, 'validation/loss': 1.510108232498169, 'validation/num_examples': 50000, 'test/accuracy': 0.5281000137329102, 'test/loss': 2.171442985534668, 'test/num_examples': 10000, 'score': 29638.966315984726, 'total_duration': 30862.28338265419, 'accumulated_submission_time': 29638.966315984726, 'accumulated_eval_time': 1217.268765926361, 'accumulated_logging_time': 3.0644567012786865}
I0131 05:23:53.688708 139907712378624 logging_writer.py:48] [87721] accumulated_eval_time=1217.268766, accumulated_logging_time=3.064457, accumulated_submission_time=29638.966316, global_step=87721, preemption_count=0, score=29638.966316, test/accuracy=0.528100, test/loss=2.171443, test/num_examples=10000, total_duration=30862.283383, train/accuracy=0.704301, train/loss=1.270425, validation/accuracy=0.655820, validation/loss=1.510108, validation/num_examples=50000
I0131 05:24:20.601564 139907720771328 logging_writer.py:48] [87800] global_step=87800, grad_norm=3.542179822921753, loss=2.660287380218506
I0131 05:24:54.291300 139907712378624 logging_writer.py:48] [87900] global_step=87900, grad_norm=4.644899368286133, loss=2.5786619186401367
I0131 05:25:27.949138 139907720771328 logging_writer.py:48] [88000] global_step=88000, grad_norm=4.456307888031006, loss=2.581237554550171
I0131 05:26:01.658699 139907712378624 logging_writer.py:48] [88100] global_step=88100, grad_norm=3.6653170585632324, loss=2.5017731189727783
I0131 05:26:35.324791 139907720771328 logging_writer.py:48] [88200] global_step=88200, grad_norm=4.121337890625, loss=2.659458875656128
I0131 05:27:09.017838 139907712378624 logging_writer.py:48] [88300] global_step=88300, grad_norm=3.4376680850982666, loss=2.555018663406372
I0131 05:27:42.736379 139907720771328 logging_writer.py:48] [88400] global_step=88400, grad_norm=3.433595657348633, loss=2.5222268104553223
I0131 05:28:16.485225 139907712378624 logging_writer.py:48] [88500] global_step=88500, grad_norm=3.536191701889038, loss=2.5438928604125977
I0131 05:28:50.139561 139907720771328 logging_writer.py:48] [88600] global_step=88600, grad_norm=3.509493827819824, loss=2.522548198699951
I0131 05:29:23.806345 139907712378624 logging_writer.py:48] [88700] global_step=88700, grad_norm=3.6785948276519775, loss=2.5892536640167236
I0131 05:29:57.480558 139907720771328 logging_writer.py:48] [88800] global_step=88800, grad_norm=4.049879550933838, loss=2.5577292442321777
I0131 05:30:31.222670 139907712378624 logging_writer.py:48] [88900] global_step=88900, grad_norm=3.6896159648895264, loss=2.6112427711486816
I0131 05:31:04.942521 139907720771328 logging_writer.py:48] [89000] global_step=89000, grad_norm=3.6739230155944824, loss=2.608884811401367
I0131 05:31:38.682164 139907712378624 logging_writer.py:48] [89100] global_step=89100, grad_norm=4.114251136779785, loss=2.4853978157043457
I0131 05:32:12.330180 139907720771328 logging_writer.py:48] [89200] global_step=89200, grad_norm=3.3874592781066895, loss=2.527661085128784
I0131 05:32:23.942947 140070692116288 spec.py:321] Evaluating on the training split.
I0131 05:32:30.165847 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 05:32:38.988586 140070692116288 spec.py:349] Evaluating on the test split.
I0131 05:32:41.296893 140070692116288 submission_runner.py:408] Time since start: 31389.92s, 	Step: 89236, 	{'train/accuracy': 0.7318239808082581, 'train/loss': 1.1713844537734985, 'validation/accuracy': 0.6634599566459656, 'validation/loss': 1.469886064529419, 'validation/num_examples': 50000, 'test/accuracy': 0.5405000448226929, 'test/loss': 2.117152214050293, 'test/num_examples': 10000, 'score': 30149.158131837845, 'total_duration': 31389.92377448082, 'accumulated_submission_time': 30149.158131837845, 'accumulated_eval_time': 1234.6226682662964, 'accumulated_logging_time': 3.106571912765503}
I0131 05:32:41.331701 139907703985920 logging_writer.py:48] [89236] accumulated_eval_time=1234.622668, accumulated_logging_time=3.106572, accumulated_submission_time=30149.158132, global_step=89236, preemption_count=0, score=30149.158132, test/accuracy=0.540500, test/loss=2.117152, test/num_examples=10000, total_duration=31389.923774, train/accuracy=0.731824, train/loss=1.171384, validation/accuracy=0.663460, validation/loss=1.469886, validation/num_examples=50000
I0131 05:33:03.240486 139907729164032 logging_writer.py:48] [89300] global_step=89300, grad_norm=3.837883710861206, loss=2.5027406215667725
I0131 05:33:36.940077 139907703985920 logging_writer.py:48] [89400] global_step=89400, grad_norm=3.6788060665130615, loss=2.5372071266174316
I0131 05:34:10.584398 139907729164032 logging_writer.py:48] [89500] global_step=89500, grad_norm=3.5228588581085205, loss=2.537541389465332
I0131 05:34:44.256600 139907703985920 logging_writer.py:48] [89600] global_step=89600, grad_norm=3.6302080154418945, loss=2.5669033527374268
I0131 05:35:17.927779 139907729164032 logging_writer.py:48] [89700] global_step=89700, grad_norm=4.571655750274658, loss=2.631497383117676
I0131 05:35:51.612941 139907703985920 logging_writer.py:48] [89800] global_step=89800, grad_norm=3.620277166366577, loss=2.5535097122192383
I0131 05:36:25.277184 139907729164032 logging_writer.py:48] [89900] global_step=89900, grad_norm=3.759549140930176, loss=2.529772996902466
I0131 05:36:58.949620 139907703985920 logging_writer.py:48] [90000] global_step=90000, grad_norm=3.5784754753112793, loss=2.5161592960357666
I0131 05:37:32.606886 139907729164032 logging_writer.py:48] [90100] global_step=90100, grad_norm=3.4538815021514893, loss=2.566084384918213
I0131 05:38:06.258117 139907703985920 logging_writer.py:48] [90200] global_step=90200, grad_norm=3.815483331680298, loss=2.579329013824463
I0131 05:38:39.908984 139907729164032 logging_writer.py:48] [90300] global_step=90300, grad_norm=3.4362857341766357, loss=2.620455265045166
I0131 05:39:13.597973 139907703985920 logging_writer.py:48] [90400] global_step=90400, grad_norm=3.877619504928589, loss=2.628615140914917
I0131 05:39:47.329102 139907729164032 logging_writer.py:48] [90500] global_step=90500, grad_norm=3.4119131565093994, loss=2.624540090560913
I0131 05:40:21.062397 139907703985920 logging_writer.py:48] [90600] global_step=90600, grad_norm=4.196718692779541, loss=2.4585962295532227
I0131 05:40:54.735642 139907729164032 logging_writer.py:48] [90700] global_step=90700, grad_norm=3.50032901763916, loss=2.5075109004974365
I0131 05:41:11.373520 140070692116288 spec.py:321] Evaluating on the training split.
I0131 05:41:17.596882 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 05:41:26.468211 140070692116288 spec.py:349] Evaluating on the test split.
I0131 05:41:28.738601 140070692116288 submission_runner.py:408] Time since start: 31917.37s, 	Step: 90751, 	{'train/accuracy': 0.724609375, 'train/loss': 1.1772572994232178, 'validation/accuracy': 0.6525599956512451, 'validation/loss': 1.5016096830368042, 'validation/num_examples': 50000, 'test/accuracy': 0.5282000303268433, 'test/loss': 2.163940191268921, 'test/num_examples': 10000, 'score': 30659.136559963226, 'total_duration': 31917.365456819534, 'accumulated_submission_time': 30659.136559963226, 'accumulated_eval_time': 1251.9876792430878, 'accumulated_logging_time': 3.152724266052246}
I0131 05:41:28.770443 139906827380480 logging_writer.py:48] [90751] accumulated_eval_time=1251.987679, accumulated_logging_time=3.152724, accumulated_submission_time=30659.136560, global_step=90751, preemption_count=0, score=30659.136560, test/accuracy=0.528200, test/loss=2.163940, test/num_examples=10000, total_duration=31917.365457, train/accuracy=0.724609, train/loss=1.177257, validation/accuracy=0.652560, validation/loss=1.501610, validation/num_examples=50000
I0131 05:41:45.584430 139906835773184 logging_writer.py:48] [90800] global_step=90800, grad_norm=3.7555153369903564, loss=2.6363818645477295
I0131 05:42:19.225426 139906827380480 logging_writer.py:48] [90900] global_step=90900, grad_norm=3.9731523990631104, loss=2.531604290008545
I0131 05:42:52.905272 139906835773184 logging_writer.py:48] [91000] global_step=91000, grad_norm=3.7674713134765625, loss=2.5358710289001465
I0131 05:43:26.561262 139906827380480 logging_writer.py:48] [91100] global_step=91100, grad_norm=3.705688714981079, loss=2.465944290161133
I0131 05:44:00.241063 139906835773184 logging_writer.py:48] [91200] global_step=91200, grad_norm=3.7156524658203125, loss=2.4778780937194824
I0131 05:44:33.905213 139906827380480 logging_writer.py:48] [91300] global_step=91300, grad_norm=3.7467281818389893, loss=2.5336475372314453
I0131 05:45:07.600941 139906835773184 logging_writer.py:48] [91400] global_step=91400, grad_norm=3.6542820930480957, loss=2.3858983516693115
I0131 05:45:41.277680 139906827380480 logging_writer.py:48] [91500] global_step=91500, grad_norm=4.026789665222168, loss=2.6115670204162598
I0131 05:46:15.043258 139906835773184 logging_writer.py:48] [91600] global_step=91600, grad_norm=4.555751323699951, loss=2.5507850646972656
I0131 05:46:48.713892 139906827380480 logging_writer.py:48] [91700] global_step=91700, grad_norm=3.6734611988067627, loss=2.695544481277466
I0131 05:47:22.434480 139906835773184 logging_writer.py:48] [91800] global_step=91800, grad_norm=4.025126934051514, loss=2.549156427383423
I0131 05:47:56.102353 139906827380480 logging_writer.py:48] [91900] global_step=91900, grad_norm=3.669910192489624, loss=2.4373817443847656
I0131 05:48:29.806538 139906835773184 logging_writer.py:48] [92000] global_step=92000, grad_norm=3.9653732776641846, loss=2.483401298522949
I0131 05:49:03.464553 139906827380480 logging_writer.py:48] [92100] global_step=92100, grad_norm=3.6400060653686523, loss=2.475362777709961
I0131 05:49:37.100856 139906835773184 logging_writer.py:48] [92200] global_step=92200, grad_norm=3.4601194858551025, loss=2.4836959838867188
I0131 05:49:58.778702 140070692116288 spec.py:321] Evaluating on the training split.
I0131 05:50:05.121042 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 05:50:14.084322 140070692116288 spec.py:349] Evaluating on the test split.
I0131 05:50:16.312326 140070692116288 submission_runner.py:408] Time since start: 32444.94s, 	Step: 92266, 	{'train/accuracy': 0.7249082922935486, 'train/loss': 1.186362624168396, 'validation/accuracy': 0.6613199710845947, 'validation/loss': 1.461118459701538, 'validation/num_examples': 50000, 'test/accuracy': 0.541700005531311, 'test/loss': 2.0914676189422607, 'test/num_examples': 10000, 'score': 31169.082045078278, 'total_duration': 32444.939204216003, 'accumulated_submission_time': 31169.082045078278, 'accumulated_eval_time': 1269.5212585926056, 'accumulated_logging_time': 3.195277690887451}
I0131 05:50:16.346136 139907703985920 logging_writer.py:48] [92266] accumulated_eval_time=1269.521259, accumulated_logging_time=3.195278, accumulated_submission_time=31169.082045, global_step=92266, preemption_count=0, score=31169.082045, test/accuracy=0.541700, test/loss=2.091468, test/num_examples=10000, total_duration=32444.939204, train/accuracy=0.724908, train/loss=1.186363, validation/accuracy=0.661320, validation/loss=1.461118, validation/num_examples=50000
I0131 05:50:28.132208 139907712378624 logging_writer.py:48] [92300] global_step=92300, grad_norm=3.620211601257324, loss=2.4173004627227783
I0131 05:51:01.812163 139907703985920 logging_writer.py:48] [92400] global_step=92400, grad_norm=3.7416954040527344, loss=2.5139663219451904
I0131 05:51:35.499634 139907712378624 logging_writer.py:48] [92500] global_step=92500, grad_norm=3.4685773849487305, loss=2.4654135704040527
I0131 05:52:09.328417 139907703985920 logging_writer.py:48] [92600] global_step=92600, grad_norm=3.987800121307373, loss=2.515084743499756
I0131 05:52:43.020952 139907712378624 logging_writer.py:48] [92700] global_step=92700, grad_norm=3.7050282955169678, loss=2.508120059967041
I0131 05:53:16.689825 139907703985920 logging_writer.py:48] [92800] global_step=92800, grad_norm=4.149043083190918, loss=2.5811707973480225
I0131 05:53:50.386671 139907712378624 logging_writer.py:48] [92900] global_step=92900, grad_norm=3.9555954933166504, loss=2.5751726627349854
I0131 05:54:24.060480 139907703985920 logging_writer.py:48] [93000] global_step=93000, grad_norm=3.6383657455444336, loss=2.493461847305298
I0131 05:54:57.757315 139907712378624 logging_writer.py:48] [93100] global_step=93100, grad_norm=3.782747745513916, loss=2.61470890045166
I0131 05:55:31.410924 139907703985920 logging_writer.py:48] [93200] global_step=93200, grad_norm=4.067478179931641, loss=2.5076539516448975
I0131 05:56:05.106073 139907712378624 logging_writer.py:48] [93300] global_step=93300, grad_norm=4.325791835784912, loss=2.536055326461792
I0131 05:56:38.763834 139907703985920 logging_writer.py:48] [93400] global_step=93400, grad_norm=3.8070075511932373, loss=2.551614761352539
I0131 05:57:12.466156 139907712378624 logging_writer.py:48] [93500] global_step=93500, grad_norm=3.890709161758423, loss=2.500781536102295
I0131 05:57:46.129868 139907703985920 logging_writer.py:48] [93600] global_step=93600, grad_norm=3.9811794757843018, loss=2.512857675552368
I0131 05:58:19.968877 139907712378624 logging_writer.py:48] [93700] global_step=93700, grad_norm=4.159450054168701, loss=2.5988693237304688
I0131 05:58:46.380524 140070692116288 spec.py:321] Evaluating on the training split.
I0131 05:58:52.640235 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 05:59:01.181980 140070692116288 spec.py:349] Evaluating on the test split.
I0131 05:59:03.589184 140070692116288 submission_runner.py:408] Time since start: 32972.22s, 	Step: 93780, 	{'train/accuracy': 0.7208425998687744, 'train/loss': 1.238020896911621, 'validation/accuracy': 0.6592599749565125, 'validation/loss': 1.504926085472107, 'validation/num_examples': 50000, 'test/accuracy': 0.5371000170707703, 'test/loss': 2.135526180267334, 'test/num_examples': 10000, 'score': 31679.052606105804, 'total_duration': 32972.216052532196, 'accumulated_submission_time': 31679.052606105804, 'accumulated_eval_time': 1286.7298786640167, 'accumulated_logging_time': 3.240107774734497}
I0131 05:59:03.624612 139906827380480 logging_writer.py:48] [93780] accumulated_eval_time=1286.729879, accumulated_logging_time=3.240108, accumulated_submission_time=31679.052606, global_step=93780, preemption_count=0, score=31679.052606, test/accuracy=0.537100, test/loss=2.135526, test/num_examples=10000, total_duration=32972.216053, train/accuracy=0.720843, train/loss=1.238021, validation/accuracy=0.659260, validation/loss=1.504926, validation/num_examples=50000
I0131 05:59:10.711366 139906835773184 logging_writer.py:48] [93800] global_step=93800, grad_norm=4.0750732421875, loss=2.510066032409668
I0131 05:59:44.402650 139906827380480 logging_writer.py:48] [93900] global_step=93900, grad_norm=4.10882043838501, loss=2.632220983505249
I0131 06:00:18.057680 139906835773184 logging_writer.py:48] [94000] global_step=94000, grad_norm=3.8176770210266113, loss=2.4989676475524902
I0131 06:00:51.759895 139906827380480 logging_writer.py:48] [94100] global_step=94100, grad_norm=4.043571472167969, loss=2.590697765350342
I0131 06:01:25.472545 139906835773184 logging_writer.py:48] [94200] global_step=94200, grad_norm=4.038466453552246, loss=2.5303213596343994
I0131 06:01:59.192345 139906827380480 logging_writer.py:48] [94300] global_step=94300, grad_norm=4.731619834899902, loss=2.620609998703003
I0131 06:02:32.859334 139906835773184 logging_writer.py:48] [94400] global_step=94400, grad_norm=4.083166122436523, loss=2.543923854827881
I0131 06:03:06.597488 139906827380480 logging_writer.py:48] [94500] global_step=94500, grad_norm=3.599483013153076, loss=2.485565185546875
I0131 06:03:40.266018 139906835773184 logging_writer.py:48] [94600] global_step=94600, grad_norm=3.882591962814331, loss=2.492870807647705
I0131 06:04:14.157006 139906827380480 logging_writer.py:48] [94700] global_step=94700, grad_norm=3.9739789962768555, loss=2.5217814445495605
I0131 06:04:47.822210 139906835773184 logging_writer.py:48] [94800] global_step=94800, grad_norm=4.75451135635376, loss=2.4833264350891113
I0131 06:05:21.519674 139906827380480 logging_writer.py:48] [94900] global_step=94900, grad_norm=4.052169322967529, loss=2.4686973094940186
I0131 06:05:55.174617 139906835773184 logging_writer.py:48] [95000] global_step=95000, grad_norm=3.680131673812866, loss=2.555081844329834
I0131 06:06:28.834394 139906827380480 logging_writer.py:48] [95100] global_step=95100, grad_norm=3.8332390785217285, loss=2.519878625869751
I0131 06:07:02.513646 139906835773184 logging_writer.py:48] [95200] global_step=95200, grad_norm=4.1644744873046875, loss=2.516103744506836
I0131 06:07:33.686363 140070692116288 spec.py:321] Evaluating on the training split.
I0131 06:07:39.933866 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 06:07:48.573676 140070692116288 spec.py:349] Evaluating on the test split.
I0131 06:07:50.904228 140070692116288 submission_runner.py:408] Time since start: 33499.53s, 	Step: 95294, 	{'train/accuracy': 0.7150231003761292, 'train/loss': 1.20890474319458, 'validation/accuracy': 0.6631799936294556, 'validation/loss': 1.4594324827194214, 'validation/num_examples': 50000, 'test/accuracy': 0.5412000417709351, 'test/loss': 2.117387533187866, 'test/num_examples': 10000, 'score': 32189.051352262497, 'total_duration': 33499.531002283096, 'accumulated_submission_time': 32189.051352262497, 'accumulated_eval_time': 1303.9475963115692, 'accumulated_logging_time': 3.285914421081543}
I0131 06:07:50.938126 139907712378624 logging_writer.py:48] [95294] accumulated_eval_time=1303.947596, accumulated_logging_time=3.285914, accumulated_submission_time=32189.051352, global_step=95294, preemption_count=0, score=32189.051352, test/accuracy=0.541200, test/loss=2.117388, test/num_examples=10000, total_duration=33499.531002, train/accuracy=0.715023, train/loss=1.208905, validation/accuracy=0.663180, validation/loss=1.459432, validation/num_examples=50000
I0131 06:07:53.316834 139907720771328 logging_writer.py:48] [95300] global_step=95300, grad_norm=3.683473825454712, loss=2.5767197608947754
I0131 06:08:27.049793 139907712378624 logging_writer.py:48] [95400] global_step=95400, grad_norm=4.491028308868408, loss=2.488098621368408
I0131 06:09:00.704922 139907720771328 logging_writer.py:48] [95500] global_step=95500, grad_norm=3.731567621231079, loss=2.479740619659424
I0131 06:09:34.393946 139907712378624 logging_writer.py:48] [95600] global_step=95600, grad_norm=4.047667980194092, loss=2.427103042602539
I0131 06:10:08.051047 139907720771328 logging_writer.py:48] [95700] global_step=95700, grad_norm=4.33126974105835, loss=2.518970012664795
I0131 06:10:41.903534 139907712378624 logging_writer.py:48] [95800] global_step=95800, grad_norm=4.5150322914123535, loss=2.5846376419067383
I0131 06:11:15.568592 139907720771328 logging_writer.py:48] [95900] global_step=95900, grad_norm=4.56206750869751, loss=2.538957357406616
I0131 06:11:49.280888 139907712378624 logging_writer.py:48] [96000] global_step=96000, grad_norm=3.295517921447754, loss=2.4813408851623535
I0131 06:12:22.945481 139907720771328 logging_writer.py:48] [96100] global_step=96100, grad_norm=3.6985037326812744, loss=2.5653505325317383
I0131 06:12:56.643452 139907712378624 logging_writer.py:48] [96200] global_step=96200, grad_norm=3.9673397541046143, loss=2.4844486713409424
I0131 06:13:30.295732 139907720771328 logging_writer.py:48] [96300] global_step=96300, grad_norm=4.3430328369140625, loss=2.5664944648742676
I0131 06:14:04.018647 139907712378624 logging_writer.py:48] [96400] global_step=96400, grad_norm=4.06549596786499, loss=2.6114134788513184
I0131 06:14:37.692381 139907720771328 logging_writer.py:48] [96500] global_step=96500, grad_norm=4.463886737823486, loss=2.489760637283325
I0131 06:15:11.386580 139907712378624 logging_writer.py:48] [96600] global_step=96600, grad_norm=3.899017572402954, loss=2.560330629348755
I0131 06:15:45.057117 139907720771328 logging_writer.py:48] [96700] global_step=96700, grad_norm=4.019423007965088, loss=2.4746439456939697
I0131 06:16:18.794297 139907712378624 logging_writer.py:48] [96800] global_step=96800, grad_norm=3.700246810913086, loss=2.5768566131591797
I0131 06:16:20.959563 140070692116288 spec.py:321] Evaluating on the training split.
I0131 06:16:27.240072 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 06:16:36.138188 140070692116288 spec.py:349] Evaluating on the test split.
I0131 06:16:38.421432 140070692116288 submission_runner.py:408] Time since start: 34027.05s, 	Step: 96808, 	{'train/accuracy': 0.721101701259613, 'train/loss': 1.1796294450759888, 'validation/accuracy': 0.6640200018882751, 'validation/loss': 1.435350775718689, 'validation/num_examples': 50000, 'test/accuracy': 0.5457000136375427, 'test/loss': 2.0837349891662598, 'test/num_examples': 10000, 'score': 32699.011062145233, 'total_duration': 34027.04830813408, 'accumulated_submission_time': 32699.011062145233, 'accumulated_eval_time': 1321.4094231128693, 'accumulated_logging_time': 3.3290951251983643}
I0131 06:16:38.454674 139906827380480 logging_writer.py:48] [96808] accumulated_eval_time=1321.409423, accumulated_logging_time=3.329095, accumulated_submission_time=32699.011062, global_step=96808, preemption_count=0, score=32699.011062, test/accuracy=0.545700, test/loss=2.083735, test/num_examples=10000, total_duration=34027.048308, train/accuracy=0.721102, train/loss=1.179629, validation/accuracy=0.664020, validation/loss=1.435351, validation/num_examples=50000
I0131 06:17:09.732009 139906835773184 logging_writer.py:48] [96900] global_step=96900, grad_norm=3.538810968399048, loss=2.5767860412597656
I0131 06:17:43.423533 139906827380480 logging_writer.py:48] [97000] global_step=97000, grad_norm=4.301902770996094, loss=2.5226051807403564
I0131 06:18:17.115350 139906835773184 logging_writer.py:48] [97100] global_step=97100, grad_norm=3.379185914993286, loss=2.5261495113372803
I0131 06:18:50.792652 139906827380480 logging_writer.py:48] [97200] global_step=97200, grad_norm=4.13165807723999, loss=2.4793167114257812
I0131 06:19:24.460366 139906835773184 logging_writer.py:48] [97300] global_step=97300, grad_norm=3.9589905738830566, loss=2.519320011138916
I0131 06:19:58.188878 139906827380480 logging_writer.py:48] [97400] global_step=97400, grad_norm=4.509410858154297, loss=2.4934091567993164
I0131 06:20:31.878036 139906835773184 logging_writer.py:48] [97500] global_step=97500, grad_norm=3.808368682861328, loss=2.4539742469787598
I0131 06:21:05.566198 139906827380480 logging_writer.py:48] [97600] global_step=97600, grad_norm=4.1653265953063965, loss=2.505688428878784
I0131 06:21:39.229155 139906835773184 logging_writer.py:48] [97700] global_step=97700, grad_norm=4.615896701812744, loss=2.4983162879943848
I0131 06:22:12.904489 139906827380480 logging_writer.py:48] [97800] global_step=97800, grad_norm=4.0267014503479, loss=2.623121738433838
I0131 06:22:46.653404 139906835773184 logging_writer.py:48] [97900] global_step=97900, grad_norm=4.630929470062256, loss=2.4884495735168457
I0131 06:23:20.376498 139906827380480 logging_writer.py:48] [98000] global_step=98000, grad_norm=4.662753582000732, loss=2.5120744705200195
I0131 06:23:54.044071 139906835773184 logging_writer.py:48] [98100] global_step=98100, grad_norm=3.4634644985198975, loss=2.503628730773926
I0131 06:24:27.739818 139906827380480 logging_writer.py:48] [98200] global_step=98200, grad_norm=3.8177762031555176, loss=2.4226062297821045
I0131 06:25:01.476293 139906835773184 logging_writer.py:48] [98300] global_step=98300, grad_norm=3.9664454460144043, loss=2.511054039001465
I0131 06:25:08.713948 140070692116288 spec.py:321] Evaluating on the training split.
I0131 06:25:15.018857 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 06:25:23.763039 140070692116288 spec.py:349] Evaluating on the test split.
I0131 06:25:26.070089 140070692116288 submission_runner.py:408] Time since start: 34554.70s, 	Step: 98323, 	{'train/accuracy': 0.7517338991165161, 'train/loss': 1.0737617015838623, 'validation/accuracy': 0.663100004196167, 'validation/loss': 1.4655168056488037, 'validation/num_examples': 50000, 'test/accuracy': 0.5379000306129456, 'test/loss': 2.1180527210235596, 'test/num_examples': 10000, 'score': 33209.20894575119, 'total_duration': 34554.696957588196, 'accumulated_submission_time': 33209.20894575119, 'accumulated_eval_time': 1338.7655036449432, 'accumulated_logging_time': 3.3717434406280518}
I0131 06:25:26.103769 139906756101888 logging_writer.py:48] [98323] accumulated_eval_time=1338.765504, accumulated_logging_time=3.371743, accumulated_submission_time=33209.208946, global_step=98323, preemption_count=0, score=33209.208946, test/accuracy=0.537900, test/loss=2.118053, test/num_examples=10000, total_duration=34554.696958, train/accuracy=0.751734, train/loss=1.073762, validation/accuracy=0.663100, validation/loss=1.465517, validation/num_examples=50000
I0131 06:25:52.348353 139906827380480 logging_writer.py:48] [98400] global_step=98400, grad_norm=4.1744303703308105, loss=2.4579644203186035
I0131 06:26:26.076168 139906756101888 logging_writer.py:48] [98500] global_step=98500, grad_norm=3.646773338317871, loss=2.541865348815918
I0131 06:26:59.734369 139906827380480 logging_writer.py:48] [98600] global_step=98600, grad_norm=3.5614397525787354, loss=2.4971351623535156
I0131 06:27:33.413985 139906756101888 logging_writer.py:48] [98700] global_step=98700, grad_norm=3.6325197219848633, loss=2.4113998413085938
I0131 06:28:07.055179 139906827380480 logging_writer.py:48] [98800] global_step=98800, grad_norm=3.6845335960388184, loss=2.3999416828155518
I0131 06:28:40.818474 139906756101888 logging_writer.py:48] [98900] global_step=98900, grad_norm=3.9686882495880127, loss=2.457855463027954
I0131 06:29:14.539790 139906827380480 logging_writer.py:48] [99000] global_step=99000, grad_norm=4.132779598236084, loss=2.5561363697052
I0131 06:29:48.231301 139906756101888 logging_writer.py:48] [99100] global_step=99100, grad_norm=3.9708988666534424, loss=2.4818553924560547
I0131 06:30:21.895697 139906827380480 logging_writer.py:48] [99200] global_step=99200, grad_norm=4.131904125213623, loss=2.531892776489258
I0131 06:30:55.593932 139906756101888 logging_writer.py:48] [99300] global_step=99300, grad_norm=3.7704648971557617, loss=2.4734668731689453
I0131 06:31:29.246681 139906827380480 logging_writer.py:48] [99400] global_step=99400, grad_norm=4.416468143463135, loss=2.5126397609710693
I0131 06:32:02.934040 139906756101888 logging_writer.py:48] [99500] global_step=99500, grad_norm=4.35202169418335, loss=2.494175672531128
I0131 06:32:36.593312 139906827380480 logging_writer.py:48] [99600] global_step=99600, grad_norm=3.8429555892944336, loss=2.476101875305176
I0131 06:33:10.270075 139906756101888 logging_writer.py:48] [99700] global_step=99700, grad_norm=3.589022636413574, loss=2.3956687450408936
I0131 06:33:43.928943 139906827380480 logging_writer.py:48] [99800] global_step=99800, grad_norm=4.240230560302734, loss=2.493130683898926
I0131 06:33:56.186221 140070692116288 spec.py:321] Evaluating on the training split.
I0131 06:34:02.423215 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 06:34:11.144247 140070692116288 spec.py:349] Evaluating on the test split.
I0131 06:34:13.405446 140070692116288 submission_runner.py:408] Time since start: 35082.03s, 	Step: 99838, 	{'train/accuracy': 0.7379822731018066, 'train/loss': 1.116407036781311, 'validation/accuracy': 0.6669999957084656, 'validation/loss': 1.4401804208755493, 'validation/num_examples': 50000, 'test/accuracy': 0.5509999990463257, 'test/loss': 2.067492961883545, 'test/num_examples': 10000, 'score': 33719.22984600067, 'total_duration': 35082.03232550621, 'accumulated_submission_time': 33719.22984600067, 'accumulated_eval_time': 1355.9846813678741, 'accumulated_logging_time': 3.415469169616699}
I0131 06:34:13.439361 139907729164032 logging_writer.py:48] [99838] accumulated_eval_time=1355.984681, accumulated_logging_time=3.415469, accumulated_submission_time=33719.229846, global_step=99838, preemption_count=0, score=33719.229846, test/accuracy=0.551000, test/loss=2.067493, test/num_examples=10000, total_duration=35082.032326, train/accuracy=0.737982, train/loss=1.116407, validation/accuracy=0.667000, validation/loss=1.440180, validation/num_examples=50000
I0131 06:34:34.643447 139907737556736 logging_writer.py:48] [99900] global_step=99900, grad_norm=4.223364353179932, loss=2.5659658908843994
I0131 06:35:08.366591 139907729164032 logging_writer.py:48] [100000] global_step=100000, grad_norm=4.856065273284912, loss=2.553861618041992
I0131 06:35:42.048552 139907737556736 logging_writer.py:48] [100100] global_step=100100, grad_norm=3.7362356185913086, loss=2.524507761001587
I0131 06:36:15.709828 139907729164032 logging_writer.py:48] [100200] global_step=100200, grad_norm=3.784464120864868, loss=2.3848750591278076
I0131 06:36:49.373951 139907737556736 logging_writer.py:48] [100300] global_step=100300, grad_norm=4.5150041580200195, loss=2.522484540939331
I0131 06:37:23.051110 139907729164032 logging_writer.py:48] [100400] global_step=100400, grad_norm=4.07432222366333, loss=2.4989781379699707
I0131 06:37:56.746931 139907737556736 logging_writer.py:48] [100500] global_step=100500, grad_norm=4.484269618988037, loss=2.536194324493408
I0131 06:38:30.400921 139907729164032 logging_writer.py:48] [100600] global_step=100600, grad_norm=3.825205087661743, loss=2.532099723815918
I0131 06:39:04.127882 139907737556736 logging_writer.py:48] [100700] global_step=100700, grad_norm=4.726371765136719, loss=2.528313159942627
I0131 06:39:37.801203 139907729164032 logging_writer.py:48] [100800] global_step=100800, grad_norm=4.47283935546875, loss=2.440582275390625
I0131 06:40:11.466946 139907737556736 logging_writer.py:48] [100900] global_step=100900, grad_norm=4.7881293296813965, loss=2.450983762741089
I0131 06:40:45.148591 139907729164032 logging_writer.py:48] [101000] global_step=101000, grad_norm=4.000189304351807, loss=2.495325803756714
I0131 06:41:18.925502 139907737556736 logging_writer.py:48] [101100] global_step=101100, grad_norm=3.7780070304870605, loss=2.4831488132476807
I0131 06:41:52.608112 139907729164032 logging_writer.py:48] [101200] global_step=101200, grad_norm=4.218811511993408, loss=2.551236867904663
I0131 06:42:26.299823 139907737556736 logging_writer.py:48] [101300] global_step=101300, grad_norm=3.7729320526123047, loss=2.492314338684082
I0131 06:42:43.605707 140070692116288 spec.py:321] Evaluating on the training split.
I0131 06:42:49.958855 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 06:42:58.705393 140070692116288 spec.py:349] Evaluating on the test split.
I0131 06:43:01.047480 140070692116288 submission_runner.py:408] Time since start: 35609.67s, 	Step: 101353, 	{'train/accuracy': 0.7453961968421936, 'train/loss': 1.0961382389068604, 'validation/accuracy': 0.676099956035614, 'validation/loss': 1.4047099351882935, 'validation/num_examples': 50000, 'test/accuracy': 0.5529000163078308, 'test/loss': 2.0516908168792725, 'test/num_examples': 10000, 'score': 34229.33446264267, 'total_duration': 35609.674355983734, 'accumulated_submission_time': 34229.33446264267, 'accumulated_eval_time': 1373.4264228343964, 'accumulated_logging_time': 3.4589407444000244}
I0131 06:43:01.081729 139906835773184 logging_writer.py:48] [101353] accumulated_eval_time=1373.426423, accumulated_logging_time=3.458941, accumulated_submission_time=34229.334463, global_step=101353, preemption_count=0, score=34229.334463, test/accuracy=0.552900, test/loss=2.051691, test/num_examples=10000, total_duration=35609.674356, train/accuracy=0.745396, train/loss=1.096138, validation/accuracy=0.676100, validation/loss=1.404710, validation/num_examples=50000
I0131 06:43:17.264991 139906949052160 logging_writer.py:48] [101400] global_step=101400, grad_norm=4.73207950592041, loss=2.5092015266418457
I0131 06:43:50.902434 139906835773184 logging_writer.py:48] [101500] global_step=101500, grad_norm=4.056857585906982, loss=2.4196386337280273
I0131 06:44:24.593730 139906949052160 logging_writer.py:48] [101600] global_step=101600, grad_norm=3.8797786235809326, loss=2.5382208824157715
I0131 06:44:58.257002 139906835773184 logging_writer.py:48] [101700] global_step=101700, grad_norm=3.6722028255462646, loss=2.432352304458618
I0131 06:45:31.947378 139906949052160 logging_writer.py:48] [101800] global_step=101800, grad_norm=3.9566431045532227, loss=2.452449083328247
I0131 06:46:05.607886 139906835773184 logging_writer.py:48] [101900] global_step=101900, grad_norm=4.109291076660156, loss=2.3904905319213867
I0131 06:46:39.305822 139906949052160 logging_writer.py:48] [102000] global_step=102000, grad_norm=4.4403910636901855, loss=2.4943320751190186
I0131 06:47:13.059630 139906835773184 logging_writer.py:48] [102100] global_step=102100, grad_norm=4.09539794921875, loss=2.5302486419677734
I0131 06:47:46.793792 139906949052160 logging_writer.py:48] [102200] global_step=102200, grad_norm=3.8115479946136475, loss=2.4488890171051025
I0131 06:48:20.447049 139906835773184 logging_writer.py:48] [102300] global_step=102300, grad_norm=4.405526161193848, loss=2.537909507751465
I0131 06:48:54.149693 139906949052160 logging_writer.py:48] [102400] global_step=102400, grad_norm=4.475647926330566, loss=2.53767991065979
I0131 06:49:27.812745 139906835773184 logging_writer.py:48] [102500] global_step=102500, grad_norm=4.433652400970459, loss=2.460411548614502
I0131 06:50:01.516042 139906949052160 logging_writer.py:48] [102600] global_step=102600, grad_norm=4.237669944763184, loss=2.473022222518921
I0131 06:50:35.155265 139906835773184 logging_writer.py:48] [102700] global_step=102700, grad_norm=4.583588123321533, loss=2.4988255500793457
I0131 06:51:08.842478 139906949052160 logging_writer.py:48] [102800] global_step=102800, grad_norm=3.746006488800049, loss=2.4566614627838135
I0131 06:51:31.217481 140070692116288 spec.py:321] Evaluating on the training split.
I0131 06:51:37.494897 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 06:51:46.375306 140070692116288 spec.py:349] Evaluating on the test split.
I0131 06:51:48.617922 140070692116288 submission_runner.py:408] Time since start: 36137.24s, 	Step: 102868, 	{'train/accuracy': 0.7364476919174194, 'train/loss': 1.114004135131836, 'validation/accuracy': 0.672980010509491, 'validation/loss': 1.399997353553772, 'validation/num_examples': 50000, 'test/accuracy': 0.5488000512123108, 'test/loss': 2.043478488922119, 'test/num_examples': 10000, 'score': 34739.40772628784, 'total_duration': 36137.24479365349, 'accumulated_submission_time': 34739.40772628784, 'accumulated_eval_time': 1390.8268103599548, 'accumulated_logging_time': 3.504430055618286}
I0131 06:51:48.651691 139907720771328 logging_writer.py:48] [102868] accumulated_eval_time=1390.826810, accumulated_logging_time=3.504430, accumulated_submission_time=34739.407726, global_step=102868, preemption_count=0, score=34739.407726, test/accuracy=0.548800, test/loss=2.043478, test/num_examples=10000, total_duration=36137.244794, train/accuracy=0.736448, train/loss=1.114004, validation/accuracy=0.672980, validation/loss=1.399997, validation/num_examples=50000
I0131 06:51:59.798663 139907729164032 logging_writer.py:48] [102900] global_step=102900, grad_norm=3.9935595989227295, loss=2.3746159076690674
I0131 06:52:33.482133 139907720771328 logging_writer.py:48] [103000] global_step=103000, grad_norm=4.130551815032959, loss=2.4489681720733643
I0131 06:53:07.218617 139907729164032 logging_writer.py:48] [103100] global_step=103100, grad_norm=3.5764782428741455, loss=2.4203500747680664
I0131 06:53:40.935270 139907720771328 logging_writer.py:48] [103200] global_step=103200, grad_norm=4.052744388580322, loss=2.5220654010772705
I0131 06:54:14.615792 139907729164032 logging_writer.py:48] [103300] global_step=103300, grad_norm=4.104188919067383, loss=2.444232940673828
I0131 06:54:48.292690 139907720771328 logging_writer.py:48] [103400] global_step=103400, grad_norm=3.766016721725464, loss=2.4338908195495605
I0131 06:55:21.971518 139907729164032 logging_writer.py:48] [103500] global_step=103500, grad_norm=3.695523977279663, loss=2.4195895195007324
I0131 06:55:55.643836 139907720771328 logging_writer.py:48] [103600] global_step=103600, grad_norm=3.887242555618286, loss=2.4382290840148926
I0131 06:56:29.305350 139907729164032 logging_writer.py:48] [103700] global_step=103700, grad_norm=4.233918190002441, loss=2.440133810043335
I0131 06:57:02.980119 139907720771328 logging_writer.py:48] [103800] global_step=103800, grad_norm=4.021418571472168, loss=2.5354185104370117
I0131 06:57:36.669044 139907729164032 logging_writer.py:48] [103900] global_step=103900, grad_norm=3.995100736618042, loss=2.431293249130249
I0131 06:58:10.334199 139907720771328 logging_writer.py:48] [104000] global_step=104000, grad_norm=4.367800235748291, loss=2.49495005607605
I0131 06:58:44.031945 139907729164032 logging_writer.py:48] [104100] global_step=104100, grad_norm=4.284210205078125, loss=2.4695568084716797
I0131 06:59:17.880554 139907720771328 logging_writer.py:48] [104200] global_step=104200, grad_norm=4.508502006530762, loss=2.4106647968292236
I0131 06:59:51.612784 139907729164032 logging_writer.py:48] [104300] global_step=104300, grad_norm=4.0934224128723145, loss=2.420994758605957
I0131 07:00:18.706149 140070692116288 spec.py:321] Evaluating on the training split.
I0131 07:00:24.926191 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 07:00:33.789818 140070692116288 spec.py:349] Evaluating on the test split.
I0131 07:00:36.085077 140070692116288 submission_runner.py:408] Time since start: 36664.71s, 	Step: 104382, 	{'train/accuracy': 0.7351921200752258, 'train/loss': 1.1262609958648682, 'validation/accuracy': 0.6737599968910217, 'validation/loss': 1.4070687294006348, 'validation/num_examples': 50000, 'test/accuracy': 0.5547000169754028, 'test/loss': 2.052401065826416, 'test/num_examples': 10000, 'score': 35249.39946103096, 'total_duration': 36664.71185588837, 'accumulated_submission_time': 35249.39946103096, 'accumulated_eval_time': 1408.2055933475494, 'accumulated_logging_time': 3.548323154449463}
I0131 07:00:36.122281 139907703985920 logging_writer.py:48] [104382] accumulated_eval_time=1408.205593, accumulated_logging_time=3.548323, accumulated_submission_time=35249.399461, global_step=104382, preemption_count=0, score=35249.399461, test/accuracy=0.554700, test/loss=2.052401, test/num_examples=10000, total_duration=36664.711856, train/accuracy=0.735192, train/loss=1.126261, validation/accuracy=0.673760, validation/loss=1.407069, validation/num_examples=50000
I0131 07:00:42.542375 139907712378624 logging_writer.py:48] [104400] global_step=104400, grad_norm=4.1031694412231445, loss=2.3909173011779785
I0131 07:01:16.259968 139907703985920 logging_writer.py:48] [104500] global_step=104500, grad_norm=4.084637641906738, loss=2.4587905406951904
I0131 07:01:49.915305 139907712378624 logging_writer.py:48] [104600] global_step=104600, grad_norm=4.466603755950928, loss=2.4932610988616943
I0131 07:02:23.629636 139907703985920 logging_writer.py:48] [104700] global_step=104700, grad_norm=4.384037971496582, loss=2.436396360397339
I0131 07:02:57.275937 139907712378624 logging_writer.py:48] [104800] global_step=104800, grad_norm=4.478115081787109, loss=2.4999449253082275
I0131 07:03:30.976860 139907703985920 logging_writer.py:48] [104900] global_step=104900, grad_norm=3.9973032474517822, loss=2.5056049823760986
I0131 07:04:04.631785 139907712378624 logging_writer.py:48] [105000] global_step=105000, grad_norm=4.02393913269043, loss=2.5089144706726074
I0131 07:04:38.329553 139907703985920 logging_writer.py:48] [105100] global_step=105100, grad_norm=4.6815361976623535, loss=2.4646549224853516
I0131 07:05:12.000421 139907712378624 logging_writer.py:48] [105200] global_step=105200, grad_norm=4.13779354095459, loss=2.534627914428711
I0131 07:05:45.786170 139907703985920 logging_writer.py:48] [105300] global_step=105300, grad_norm=4.539337158203125, loss=2.557310104370117
I0131 07:06:19.427924 139907712378624 logging_writer.py:48] [105400] global_step=105400, grad_norm=4.053950309753418, loss=2.520197629928589
I0131 07:06:53.133635 139907703985920 logging_writer.py:48] [105500] global_step=105500, grad_norm=4.220632076263428, loss=2.4581105709075928
I0131 07:07:26.797899 139907712378624 logging_writer.py:48] [105600] global_step=105600, grad_norm=4.223706245422363, loss=2.4194626808166504
I0131 07:08:00.477735 139907703985920 logging_writer.py:48] [105700] global_step=105700, grad_norm=4.794189929962158, loss=2.4931375980377197
I0131 07:08:34.123360 139907712378624 logging_writer.py:48] [105800] global_step=105800, grad_norm=3.7091987133026123, loss=2.4052538871765137
I0131 07:09:06.262654 140070692116288 spec.py:321] Evaluating on the training split.
I0131 07:09:12.490080 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 07:09:21.332708 140070692116288 spec.py:349] Evaluating on the test split.
I0131 07:09:23.605507 140070692116288 submission_runner.py:408] Time since start: 37192.23s, 	Step: 105897, 	{'train/accuracy': 0.7310666441917419, 'train/loss': 1.1510498523712158, 'validation/accuracy': 0.6747599840164185, 'validation/loss': 1.4065779447555542, 'validation/num_examples': 50000, 'test/accuracy': 0.5578000545501709, 'test/loss': 2.038433790206909, 'test/num_examples': 10000, 'score': 35759.47870969772, 'total_duration': 37192.232377290726, 'accumulated_submission_time': 35759.47870969772, 'accumulated_eval_time': 1425.5483980178833, 'accumulated_logging_time': 3.5949106216430664}
I0131 07:09:23.640393 139906835773184 logging_writer.py:48] [105897] accumulated_eval_time=1425.548398, accumulated_logging_time=3.594911, accumulated_submission_time=35759.478710, global_step=105897, preemption_count=0, score=35759.478710, test/accuracy=0.557800, test/loss=2.038434, test/num_examples=10000, total_duration=37192.232377, train/accuracy=0.731067, train/loss=1.151050, validation/accuracy=0.674760, validation/loss=1.406578, validation/num_examples=50000
I0131 07:09:24.992948 139906949052160 logging_writer.py:48] [105900] global_step=105900, grad_norm=4.186779022216797, loss=2.410388469696045
I0131 07:09:58.626577 139906835773184 logging_writer.py:48] [106000] global_step=106000, grad_norm=4.233116626739502, loss=2.503817319869995
I0131 07:10:32.314951 139906949052160 logging_writer.py:48] [106100] global_step=106100, grad_norm=4.195087432861328, loss=2.420930862426758
I0131 07:11:06.013187 139906835773184 logging_writer.py:48] [106200] global_step=106200, grad_norm=3.9088170528411865, loss=2.3704957962036133
I0131 07:11:39.734074 139906949052160 logging_writer.py:48] [106300] global_step=106300, grad_norm=3.7464025020599365, loss=2.4048190116882324
I0131 07:12:13.499173 139906835773184 logging_writer.py:48] [106400] global_step=106400, grad_norm=4.277509689331055, loss=2.4292938709259033
I0131 07:12:47.166860 139906949052160 logging_writer.py:48] [106500] global_step=106500, grad_norm=4.305669784545898, loss=2.481147289276123
I0131 07:13:20.885800 139906835773184 logging_writer.py:48] [106600] global_step=106600, grad_norm=4.251619815826416, loss=2.4349536895751953
I0131 07:13:54.589848 139906949052160 logging_writer.py:48] [106700] global_step=106700, grad_norm=3.9647018909454346, loss=2.471346616744995
I0131 07:14:28.251894 139906835773184 logging_writer.py:48] [106800] global_step=106800, grad_norm=3.9552669525146484, loss=2.4683079719543457
I0131 07:15:01.894960 139906949052160 logging_writer.py:48] [106900] global_step=106900, grad_norm=4.1951518058776855, loss=2.3921186923980713
I0131 07:15:35.558624 139906835773184 logging_writer.py:48] [107000] global_step=107000, grad_norm=3.8135087490081787, loss=2.3449151515960693
I0131 07:16:09.206250 139906949052160 logging_writer.py:48] [107100] global_step=107100, grad_norm=4.324061393737793, loss=2.4873769283294678
I0131 07:16:42.903742 139906835773184 logging_writer.py:48] [107200] global_step=107200, grad_norm=4.011002063751221, loss=2.46630859375
I0131 07:17:16.563073 139906949052160 logging_writer.py:48] [107300] global_step=107300, grad_norm=4.2232184410095215, loss=2.486969232559204
I0131 07:17:50.310010 139906835773184 logging_writer.py:48] [107400] global_step=107400, grad_norm=4.400843620300293, loss=2.4660251140594482
I0131 07:17:53.827001 140070692116288 spec.py:321] Evaluating on the training split.
I0131 07:18:00.205339 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 07:18:08.967047 140070692116288 spec.py:349] Evaluating on the test split.
I0131 07:18:11.231981 140070692116288 submission_runner.py:408] Time since start: 37719.86s, 	Step: 107412, 	{'train/accuracy': 0.7695910334587097, 'train/loss': 1.021224856376648, 'validation/accuracy': 0.6710999608039856, 'validation/loss': 1.4482934474945068, 'validation/num_examples': 50000, 'test/accuracy': 0.5497000217437744, 'test/loss': 2.1009111404418945, 'test/num_examples': 10000, 'score': 36269.60155582428, 'total_duration': 37719.8588643074, 'accumulated_submission_time': 36269.60155582428, 'accumulated_eval_time': 1442.9533438682556, 'accumulated_logging_time': 3.6414597034454346}
I0131 07:18:11.266492 139907703985920 logging_writer.py:48] [107412] accumulated_eval_time=1442.953344, accumulated_logging_time=3.641460, accumulated_submission_time=36269.601556, global_step=107412, preemption_count=0, score=36269.601556, test/accuracy=0.549700, test/loss=2.100911, test/num_examples=10000, total_duration=37719.858864, train/accuracy=0.769591, train/loss=1.021225, validation/accuracy=0.671100, validation/loss=1.448293, validation/num_examples=50000
I0131 07:18:41.251845 139907712378624 logging_writer.py:48] [107500] global_step=107500, grad_norm=3.9032890796661377, loss=2.433161497116089
I0131 07:19:14.951801 139907703985920 logging_writer.py:48] [107600] global_step=107600, grad_norm=4.076691627502441, loss=2.382256507873535
I0131 07:19:48.620935 139907712378624 logging_writer.py:48] [107700] global_step=107700, grad_norm=4.089482307434082, loss=2.508462905883789
I0131 07:20:22.335412 139907703985920 logging_writer.py:48] [107800] global_step=107800, grad_norm=4.899813175201416, loss=2.46806263923645
I0131 07:20:55.999835 139907712378624 logging_writer.py:48] [107900] global_step=107900, grad_norm=4.344884872436523, loss=2.5264620780944824
I0131 07:21:29.706362 139907703985920 logging_writer.py:48] [108000] global_step=108000, grad_norm=3.9194247722625732, loss=2.4842286109924316
I0131 07:22:03.355090 139907712378624 logging_writer.py:48] [108100] global_step=108100, grad_norm=4.275765419006348, loss=2.4779632091522217
I0131 07:22:37.050545 139907703985920 logging_writer.py:48] [108200] global_step=108200, grad_norm=4.120179653167725, loss=2.484004020690918
I0131 07:23:10.711935 139907712378624 logging_writer.py:48] [108300] global_step=108300, grad_norm=4.136976718902588, loss=2.4811158180236816
I0131 07:23:44.475578 139907703985920 logging_writer.py:48] [108400] global_step=108400, grad_norm=4.35526704788208, loss=2.450561285018921
I0131 07:24:18.173081 139907712378624 logging_writer.py:48] [108500] global_step=108500, grad_norm=4.226927757263184, loss=2.339141368865967
I0131 07:24:51.880955 139907703985920 logging_writer.py:48] [108600] global_step=108600, grad_norm=4.377087593078613, loss=2.5520272254943848
I0131 07:25:25.531356 139907712378624 logging_writer.py:48] [108700] global_step=108700, grad_norm=4.161393165588379, loss=2.453491687774658
I0131 07:25:59.235253 139907703985920 logging_writer.py:48] [108800] global_step=108800, grad_norm=4.261346340179443, loss=2.478968381881714
I0131 07:26:32.892394 139907712378624 logging_writer.py:48] [108900] global_step=108900, grad_norm=4.424806594848633, loss=2.3998217582702637
I0131 07:26:41.464756 140070692116288 spec.py:321] Evaluating on the training split.
I0131 07:26:47.606114 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 07:26:56.571533 140070692116288 spec.py:349] Evaluating on the test split.
I0131 07:26:58.889035 140070692116288 submission_runner.py:408] Time since start: 38247.52s, 	Step: 108927, 	{'train/accuracy': 0.7567362785339355, 'train/loss': 1.0428543090820312, 'validation/accuracy': 0.6768999695777893, 'validation/loss': 1.38744056224823, 'validation/num_examples': 50000, 'test/accuracy': 0.5545000433921814, 'test/loss': 2.0375123023986816, 'test/num_examples': 10000, 'score': 36779.734280347824, 'total_duration': 38247.51591229439, 'accumulated_submission_time': 36779.734280347824, 'accumulated_eval_time': 1460.3775732517242, 'accumulated_logging_time': 3.689704656600952}
I0131 07:26:58.927842 139907267811072 logging_writer.py:48] [108927] accumulated_eval_time=1460.377573, accumulated_logging_time=3.689705, accumulated_submission_time=36779.734280, global_step=108927, preemption_count=0, score=36779.734280, test/accuracy=0.554500, test/loss=2.037512, test/num_examples=10000, total_duration=38247.515912, train/accuracy=0.756736, train/loss=1.042854, validation/accuracy=0.676900, validation/loss=1.387441, validation/num_examples=50000
I0131 07:27:23.846024 139907276203776 logging_writer.py:48] [109000] global_step=109000, grad_norm=4.3241705894470215, loss=2.4231057167053223
I0131 07:27:57.549663 139907267811072 logging_writer.py:48] [109100] global_step=109100, grad_norm=5.456648826599121, loss=2.480790615081787
I0131 07:28:31.218509 139907276203776 logging_writer.py:48] [109200] global_step=109200, grad_norm=4.077144622802734, loss=2.349933624267578
I0131 07:29:04.916802 139907267811072 logging_writer.py:48] [109300] global_step=109300, grad_norm=4.115015029907227, loss=2.473616361618042
I0131 07:29:38.568977 139907276203776 logging_writer.py:48] [109400] global_step=109400, grad_norm=4.071341037750244, loss=2.4208991527557373
I0131 07:30:12.312996 139907267811072 logging_writer.py:48] [109500] global_step=109500, grad_norm=5.1854448318481445, loss=2.4783194065093994
I0131 07:30:45.998860 139907276203776 logging_writer.py:48] [109600] global_step=109600, grad_norm=4.563271522521973, loss=2.4418375492095947
I0131 07:31:19.668807 139907267811072 logging_writer.py:48] [109700] global_step=109700, grad_norm=3.965754747390747, loss=2.3913371562957764
I0131 07:31:53.318971 139907276203776 logging_writer.py:48] [109800] global_step=109800, grad_norm=4.566623210906982, loss=2.395477294921875
I0131 07:32:27.008322 139907267811072 logging_writer.py:48] [109900] global_step=109900, grad_norm=4.945207595825195, loss=2.433696985244751
I0131 07:33:00.669982 139907276203776 logging_writer.py:48] [110000] global_step=110000, grad_norm=4.163541316986084, loss=2.388997793197632
I0131 07:33:34.355086 139907267811072 logging_writer.py:48] [110100] global_step=110100, grad_norm=4.585495471954346, loss=2.3918566703796387
I0131 07:34:08.015451 139907276203776 logging_writer.py:48] [110200] global_step=110200, grad_norm=4.186434745788574, loss=2.427997589111328
I0131 07:34:41.727555 139907267811072 logging_writer.py:48] [110300] global_step=110300, grad_norm=5.0141072273254395, loss=2.3991212844848633
I0131 07:35:15.410512 139907276203776 logging_writer.py:48] [110400] global_step=110400, grad_norm=4.513448238372803, loss=2.518014669418335
I0131 07:35:29.019349 140070692116288 spec.py:321] Evaluating on the training split.
I0131 07:35:35.102780 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 07:35:45.351490 140070692116288 spec.py:349] Evaluating on the test split.
I0131 07:35:47.654390 140070692116288 submission_runner.py:408] Time since start: 38776.28s, 	Step: 110442, 	{'train/accuracy': 0.7382413744926453, 'train/loss': 1.1036720275878906, 'validation/accuracy': 0.6723799705505371, 'validation/loss': 1.4153038263320923, 'validation/num_examples': 50000, 'test/accuracy': 0.5482000112533569, 'test/loss': 2.050555467605591, 'test/num_examples': 10000, 'score': 37289.76195025444, 'total_duration': 38776.281270504, 'accumulated_submission_time': 37289.76195025444, 'accumulated_eval_time': 1479.0125722885132, 'accumulated_logging_time': 3.740776300430298}
I0131 07:35:47.690634 139907712378624 logging_writer.py:48] [110442] accumulated_eval_time=1479.012572, accumulated_logging_time=3.740776, accumulated_submission_time=37289.761950, global_step=110442, preemption_count=0, score=37289.761950, test/accuracy=0.548200, test/loss=2.050555, test/num_examples=10000, total_duration=38776.281271, train/accuracy=0.738241, train/loss=1.103672, validation/accuracy=0.672380, validation/loss=1.415304, validation/num_examples=50000
I0131 07:36:07.581218 139907720771328 logging_writer.py:48] [110500] global_step=110500, grad_norm=4.006605625152588, loss=2.4052016735076904
I0131 07:36:41.279243 139907712378624 logging_writer.py:48] [110600] global_step=110600, grad_norm=3.782304048538208, loss=2.375199556350708
I0131 07:37:14.976908 139907720771328 logging_writer.py:48] [110700] global_step=110700, grad_norm=4.218128204345703, loss=2.5017857551574707
I0131 07:37:48.631558 139907712378624 logging_writer.py:48] [110800] global_step=110800, grad_norm=4.786860942840576, loss=2.475318431854248
I0131 07:38:22.308650 139907720771328 logging_writer.py:48] [110900] global_step=110900, grad_norm=4.245691299438477, loss=2.4550275802612305
I0131 07:38:55.959791 139907712378624 logging_writer.py:48] [111000] global_step=111000, grad_norm=4.552225589752197, loss=2.463263511657715
I0131 07:39:29.623682 139907720771328 logging_writer.py:48] [111100] global_step=111100, grad_norm=4.765364646911621, loss=2.4577088356018066
I0131 07:40:03.287351 139907712378624 logging_writer.py:48] [111200] global_step=111200, grad_norm=4.240564346313477, loss=2.3532564640045166
I0131 07:40:36.968299 139907720771328 logging_writer.py:48] [111300] global_step=111300, grad_norm=4.416696071624756, loss=2.3918135166168213
I0131 07:41:10.624363 139907712378624 logging_writer.py:48] [111400] global_step=111400, grad_norm=4.2902302742004395, loss=2.376246213912964
I0131 07:41:44.312518 139907720771328 logging_writer.py:48] [111500] global_step=111500, grad_norm=4.648013114929199, loss=2.4494261741638184
I0131 07:42:18.059292 139907712378624 logging_writer.py:48] [111600] global_step=111600, grad_norm=4.613591194152832, loss=2.4019594192504883
I0131 07:42:51.764419 139907720771328 logging_writer.py:48] [111700] global_step=111700, grad_norm=4.536293029785156, loss=2.465308427810669
I0131 07:43:25.435092 139907712378624 logging_writer.py:48] [111800] global_step=111800, grad_norm=4.502883434295654, loss=2.444437265396118
I0131 07:43:59.133771 139907720771328 logging_writer.py:48] [111900] global_step=111900, grad_norm=4.370769023895264, loss=2.390307664871216
I0131 07:44:17.792088 140070692116288 spec.py:321] Evaluating on the training split.
I0131 07:44:23.913933 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 07:44:32.675317 140070692116288 spec.py:349] Evaluating on the test split.
I0131 07:44:34.966217 140070692116288 submission_runner.py:408] Time since start: 39303.59s, 	Step: 111957, 	{'train/accuracy': 0.7502391338348389, 'train/loss': 1.0634797811508179, 'validation/accuracy': 0.6810599565505981, 'validation/loss': 1.3739310503005981, 'validation/num_examples': 50000, 'test/accuracy': 0.5591000318527222, 'test/loss': 2.00235652923584, 'test/num_examples': 10000, 'score': 37799.80076622963, 'total_duration': 39303.59298801422, 'accumulated_submission_time': 37799.80076622963, 'accumulated_eval_time': 1496.186547756195, 'accumulated_logging_time': 3.787414312362671}
I0131 07:44:35.003430 139907267811072 logging_writer.py:48] [111957] accumulated_eval_time=1496.186548, accumulated_logging_time=3.787414, accumulated_submission_time=37799.800766, global_step=111957, preemption_count=0, score=37799.800766, test/accuracy=0.559100, test/loss=2.002357, test/num_examples=10000, total_duration=39303.592988, train/accuracy=0.750239, train/loss=1.063480, validation/accuracy=0.681060, validation/loss=1.373931, validation/num_examples=50000
I0131 07:44:49.818785 139907276203776 logging_writer.py:48] [112000] global_step=112000, grad_norm=4.460752964019775, loss=2.3547685146331787
I0131 07:45:23.485407 139907267811072 logging_writer.py:48] [112100] global_step=112100, grad_norm=4.939575672149658, loss=2.4100711345672607
I0131 07:45:57.171082 139907276203776 logging_writer.py:48] [112200] global_step=112200, grad_norm=3.7810046672821045, loss=2.3948416709899902
I0131 07:46:30.828354 139907267811072 logging_writer.py:48] [112300] global_step=112300, grad_norm=4.35584831237793, loss=2.4319324493408203
I0131 07:47:04.527628 139907276203776 logging_writer.py:48] [112400] global_step=112400, grad_norm=4.826538562774658, loss=2.5155997276306152
I0131 07:47:38.199508 139907267811072 logging_writer.py:48] [112500] global_step=112500, grad_norm=4.379826068878174, loss=2.3555030822753906
I0131 07:48:11.935794 139907276203776 logging_writer.py:48] [112600] global_step=112600, grad_norm=3.872248888015747, loss=2.407878875732422
I0131 07:48:45.657667 139907267811072 logging_writer.py:48] [112700] global_step=112700, grad_norm=4.230675220489502, loss=2.3565943241119385
I0131 07:49:19.368070 139907276203776 logging_writer.py:48] [112800] global_step=112800, grad_norm=4.895418643951416, loss=2.3812806606292725
I0131 07:49:53.018013 139907267811072 logging_writer.py:48] [112900] global_step=112900, grad_norm=4.575686454772949, loss=2.477687358856201
I0131 07:50:26.713395 139907276203776 logging_writer.py:48] [113000] global_step=113000, grad_norm=4.580173015594482, loss=2.430298328399658
I0131 07:51:00.356603 139907267811072 logging_writer.py:48] [113100] global_step=113100, grad_norm=4.1048264503479, loss=2.363779067993164
I0131 07:51:34.049369 139907276203776 logging_writer.py:48] [113200] global_step=113200, grad_norm=4.253017902374268, loss=2.430173635482788
I0131 07:52:07.713447 139907267811072 logging_writer.py:48] [113300] global_step=113300, grad_norm=4.258305072784424, loss=2.381826400756836
I0131 07:52:41.419437 139907276203776 logging_writer.py:48] [113400] global_step=113400, grad_norm=5.016219139099121, loss=2.373143196105957
I0131 07:53:05.117776 140070692116288 spec.py:321] Evaluating on the training split.
I0131 07:53:11.316511 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 07:53:19.969154 140070692116288 spec.py:349] Evaluating on the test split.
I0131 07:53:22.250747 140070692116288 submission_runner.py:408] Time since start: 39830.88s, 	Step: 113472, 	{'train/accuracy': 0.744559109210968, 'train/loss': 1.0819778442382812, 'validation/accuracy': 0.6834200024604797, 'validation/loss': 1.3549482822418213, 'validation/num_examples': 50000, 'test/accuracy': 0.5565000176429749, 'test/loss': 2.0129952430725098, 'test/num_examples': 10000, 'score': 38309.85378551483, 'total_duration': 39830.87760901451, 'accumulated_submission_time': 38309.85378551483, 'accumulated_eval_time': 1513.319462299347, 'accumulated_logging_time': 3.8344779014587402}
I0131 07:53:22.290251 139907712378624 logging_writer.py:48] [113472] accumulated_eval_time=1513.319462, accumulated_logging_time=3.834478, accumulated_submission_time=38309.853786, global_step=113472, preemption_count=0, score=38309.853786, test/accuracy=0.556500, test/loss=2.012995, test/num_examples=10000, total_duration=39830.877609, train/accuracy=0.744559, train/loss=1.081978, validation/accuracy=0.683420, validation/loss=1.354948, validation/num_examples=50000
I0131 07:53:32.050078 139907720771328 logging_writer.py:48] [113500] global_step=113500, grad_norm=4.416343688964844, loss=2.43353533744812
I0131 07:54:05.676662 139907712378624 logging_writer.py:48] [113600] global_step=113600, grad_norm=4.371992111206055, loss=2.381350040435791
I0131 07:54:39.366247 139907720771328 logging_writer.py:48] [113700] global_step=113700, grad_norm=4.134572982788086, loss=2.3923280239105225
I0131 07:55:13.039490 139907712378624 logging_writer.py:48] [113800] global_step=113800, grad_norm=4.403327465057373, loss=2.4493300914764404
I0131 07:55:46.733387 139907720771328 logging_writer.py:48] [113900] global_step=113900, grad_norm=4.428061485290527, loss=2.4409408569335938
I0131 07:56:20.398747 139907712378624 logging_writer.py:48] [114000] global_step=114000, grad_norm=4.428183078765869, loss=2.4126083850860596
I0131 07:56:54.060380 139907720771328 logging_writer.py:48] [114100] global_step=114100, grad_norm=4.597601890563965, loss=2.3636374473571777
I0131 07:57:27.728911 139907712378624 logging_writer.py:48] [114200] global_step=114200, grad_norm=4.798061847686768, loss=2.352478504180908
I0131 07:58:01.395780 139907720771328 logging_writer.py:48] [114300] global_step=114300, grad_norm=4.3619465827941895, loss=2.334916353225708
I0131 07:58:35.064226 139907712378624 logging_writer.py:48] [114400] global_step=114400, grad_norm=4.805878162384033, loss=2.394561529159546
I0131 07:59:08.736392 139907720771328 logging_writer.py:48] [114500] global_step=114500, grad_norm=4.176890850067139, loss=2.3893187046051025
I0131 07:59:42.419343 139907712378624 logging_writer.py:48] [114600] global_step=114600, grad_norm=4.343708038330078, loss=2.4190690517425537
I0131 08:00:16.096885 139907720771328 logging_writer.py:48] [114700] global_step=114700, grad_norm=4.299269676208496, loss=2.355760097503662
I0131 08:00:50.030036 139907712378624 logging_writer.py:48] [114800] global_step=114800, grad_norm=4.807004928588867, loss=2.376608371734619
I0131 08:01:23.716815 139907720771328 logging_writer.py:48] [114900] global_step=114900, grad_norm=4.09487771987915, loss=2.4132211208343506
I0131 08:01:52.491803 140070692116288 spec.py:321] Evaluating on the training split.
I0131 08:01:58.670553 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 08:02:07.705342 140070692116288 spec.py:349] Evaluating on the test split.
I0131 08:02:09.955439 140070692116288 submission_runner.py:408] Time since start: 40358.58s, 	Step: 114987, 	{'train/accuracy': 0.7444595098495483, 'train/loss': 1.1149373054504395, 'validation/accuracy': 0.6838399767875671, 'validation/loss': 1.3911999464035034, 'validation/num_examples': 50000, 'test/accuracy': 0.5561000108718872, 'test/loss': 2.0544209480285645, 'test/num_examples': 10000, 'score': 38819.99199438095, 'total_duration': 40358.582310676575, 'accumulated_submission_time': 38819.99199438095, 'accumulated_eval_time': 1530.7830486297607, 'accumulated_logging_time': 3.8848836421966553}
I0131 08:02:09.990877 139907276203776 logging_writer.py:48] [114987] accumulated_eval_time=1530.783049, accumulated_logging_time=3.884884, accumulated_submission_time=38819.991994, global_step=114987, preemption_count=0, score=38819.991994, test/accuracy=0.556100, test/loss=2.054421, test/num_examples=10000, total_duration=40358.582311, train/accuracy=0.744460, train/loss=1.114937, validation/accuracy=0.683840, validation/loss=1.391200, validation/num_examples=50000
I0131 08:02:14.710356 139907284596480 logging_writer.py:48] [115000] global_step=115000, grad_norm=5.102283954620361, loss=2.349468231201172
I0131 08:02:48.326806 139907276203776 logging_writer.py:48] [115100] global_step=115100, grad_norm=4.368371486663818, loss=2.3876147270202637
I0131 08:03:21.972105 139907284596480 logging_writer.py:48] [115200] global_step=115200, grad_norm=4.244324684143066, loss=2.3860785961151123
I0131 08:03:55.645442 139907276203776 logging_writer.py:48] [115300] global_step=115300, grad_norm=3.9891836643218994, loss=2.374014377593994
I0131 08:04:29.303923 139907284596480 logging_writer.py:48] [115400] global_step=115400, grad_norm=4.095178127288818, loss=2.37477445602417
I0131 08:05:02.985695 139907276203776 logging_writer.py:48] [115500] global_step=115500, grad_norm=4.873423099517822, loss=2.444113254547119
I0131 08:05:36.638754 139907284596480 logging_writer.py:48] [115600] global_step=115600, grad_norm=4.867073059082031, loss=2.32732892036438
I0131 08:06:10.302509 139907276203776 logging_writer.py:48] [115700] global_step=115700, grad_norm=4.345028877258301, loss=2.4590401649475098
I0131 08:06:44.094818 139907284596480 logging_writer.py:48] [115800] global_step=115800, grad_norm=4.916321754455566, loss=2.3370468616485596
I0131 08:07:17.789358 139907276203776 logging_writer.py:48] [115900] global_step=115900, grad_norm=4.202841281890869, loss=2.2792646884918213
I0131 08:07:51.472566 139907284596480 logging_writer.py:48] [116000] global_step=116000, grad_norm=4.497385025024414, loss=2.322477340698242
I0131 08:08:25.169627 139907276203776 logging_writer.py:48] [116100] global_step=116100, grad_norm=4.539150238037109, loss=2.3783092498779297
I0131 08:08:58.866669 139907284596480 logging_writer.py:48] [116200] global_step=116200, grad_norm=4.726343631744385, loss=2.4399075508117676
I0131 08:09:32.558829 139907276203776 logging_writer.py:48] [116300] global_step=116300, grad_norm=4.680694103240967, loss=2.3939478397369385
I0131 08:10:06.211960 139907284596480 logging_writer.py:48] [116400] global_step=116400, grad_norm=4.6318769454956055, loss=2.3422484397888184
I0131 08:10:39.958752 139907276203776 logging_writer.py:48] [116500] global_step=116500, grad_norm=4.763267993927002, loss=2.277243137359619
I0131 08:10:39.966813 140070692116288 spec.py:321] Evaluating on the training split.
I0131 08:10:46.148103 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 08:10:55.059682 140070692116288 spec.py:349] Evaluating on the test split.
I0131 08:10:57.326930 140070692116288 submission_runner.py:408] Time since start: 40885.95s, 	Step: 116501, 	{'train/accuracy': 0.7858139276504517, 'train/loss': 0.9308143854141235, 'validation/accuracy': 0.6909199953079224, 'validation/loss': 1.3370513916015625, 'validation/num_examples': 50000, 'test/accuracy': 0.567300021648407, 'test/loss': 1.9916213750839233, 'test/num_examples': 10000, 'score': 39329.903483867645, 'total_duration': 40885.95380759239, 'accumulated_submission_time': 39329.903483867645, 'accumulated_eval_time': 1548.1430974006653, 'accumulated_logging_time': 3.932539224624634}
I0131 08:10:57.366498 139907720771328 logging_writer.py:48] [116501] accumulated_eval_time=1548.143097, accumulated_logging_time=3.932539, accumulated_submission_time=39329.903484, global_step=116501, preemption_count=0, score=39329.903484, test/accuracy=0.567300, test/loss=1.991621, test/num_examples=10000, total_duration=40885.953808, train/accuracy=0.785814, train/loss=0.930814, validation/accuracy=0.690920, validation/loss=1.337051, validation/num_examples=50000
I0131 08:11:31.078408 139907729164032 logging_writer.py:48] [116600] global_step=116600, grad_norm=4.292185306549072, loss=2.339146375656128
I0131 08:12:04.766261 139907720771328 logging_writer.py:48] [116700] global_step=116700, grad_norm=4.393294811248779, loss=2.4135372638702393
I0131 08:12:38.488900 139907729164032 logging_writer.py:48] [116800] global_step=116800, grad_norm=4.474585056304932, loss=2.318664073944092
I0131 08:13:12.119285 139907720771328 logging_writer.py:48] [116900] global_step=116900, grad_norm=4.363532543182373, loss=2.336099147796631
I0131 08:13:45.808833 139907729164032 logging_writer.py:48] [117000] global_step=117000, grad_norm=4.529862880706787, loss=2.4042422771453857
I0131 08:14:19.487228 139907720771328 logging_writer.py:48] [117100] global_step=117100, grad_norm=3.965494155883789, loss=2.3275134563446045
I0131 08:14:53.166344 139907729164032 logging_writer.py:48] [117200] global_step=117200, grad_norm=4.370762348175049, loss=2.4152584075927734
I0131 08:15:26.849240 139907720771328 logging_writer.py:48] [117300] global_step=117300, grad_norm=4.98616361618042, loss=2.3680763244628906
I0131 08:16:00.512561 139907729164032 logging_writer.py:48] [117400] global_step=117400, grad_norm=4.545725345611572, loss=2.309689998626709
I0131 08:16:34.209665 139907720771328 logging_writer.py:48] [117500] global_step=117500, grad_norm=4.302630424499512, loss=2.3107175827026367
I0131 08:17:07.879235 139907729164032 logging_writer.py:48] [117600] global_step=117600, grad_norm=4.720465183258057, loss=2.456650495529175
I0131 08:17:41.554185 139907720771328 logging_writer.py:48] [117700] global_step=117700, grad_norm=4.845077991485596, loss=2.37161922454834
I0131 08:18:15.211425 139907729164032 logging_writer.py:48] [117800] global_step=117800, grad_norm=4.493023872375488, loss=2.4035487174987793
I0131 08:18:48.942377 139907720771328 logging_writer.py:48] [117900] global_step=117900, grad_norm=4.854752540588379, loss=2.3204469680786133
I0131 08:19:22.675841 139907729164032 logging_writer.py:48] [118000] global_step=118000, grad_norm=4.7384209632873535, loss=2.3650360107421875
I0131 08:19:27.537365 140070692116288 spec.py:321] Evaluating on the training split.
I0131 08:19:33.607014 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 08:19:42.631402 140070692116288 spec.py:349] Evaluating on the test split.
I0131 08:19:44.949540 140070692116288 submission_runner.py:408] Time since start: 41413.58s, 	Step: 118016, 	{'train/accuracy': 0.7719228267669678, 'train/loss': 0.9736966490745544, 'validation/accuracy': 0.692799985408783, 'validation/loss': 1.3256369829177856, 'validation/num_examples': 50000, 'test/accuracy': 0.567300021648407, 'test/loss': 1.984636664390564, 'test/num_examples': 10000, 'score': 39840.01126456261, 'total_duration': 41413.576409339905, 'accumulated_submission_time': 39840.01126456261, 'accumulated_eval_time': 1565.5552134513855, 'accumulated_logging_time': 3.9830572605133057}
I0131 08:19:44.985890 139907267811072 logging_writer.py:48] [118016] accumulated_eval_time=1565.555213, accumulated_logging_time=3.983057, accumulated_submission_time=39840.011265, global_step=118016, preemption_count=0, score=39840.011265, test/accuracy=0.567300, test/loss=1.984637, test/num_examples=10000, total_duration=41413.576409, train/accuracy=0.771923, train/loss=0.973697, validation/accuracy=0.692800, validation/loss=1.325637, validation/num_examples=50000
I0131 08:20:13.631725 139907276203776 logging_writer.py:48] [118100] global_step=118100, grad_norm=4.516946792602539, loss=2.3527939319610596
I0131 08:20:47.337778 139907267811072 logging_writer.py:48] [118200] global_step=118200, grad_norm=4.41060733795166, loss=2.421224594116211
I0131 08:21:21.026909 139907276203776 logging_writer.py:48] [118300] global_step=118300, grad_norm=4.588560581207275, loss=2.4083731174468994
I0131 08:21:54.720073 139907267811072 logging_writer.py:48] [118400] global_step=118400, grad_norm=4.830667495727539, loss=2.2909867763519287
I0131 08:22:28.369300 139907276203776 logging_writer.py:48] [118500] global_step=118500, grad_norm=4.705962181091309, loss=2.370347738265991
I0131 08:23:02.062404 139907267811072 logging_writer.py:48] [118600] global_step=118600, grad_norm=4.382287979125977, loss=2.306173086166382
I0131 08:23:35.722912 139907276203776 logging_writer.py:48] [118700] global_step=118700, grad_norm=4.531647682189941, loss=2.4034762382507324
I0131 08:24:09.485175 139907267811072 logging_writer.py:48] [118800] global_step=118800, grad_norm=5.154679775238037, loss=2.380655288696289
I0131 08:24:43.151984 139907276203776 logging_writer.py:48] [118900] global_step=118900, grad_norm=4.698777198791504, loss=2.368121385574341
I0131 08:25:16.853177 139907267811072 logging_writer.py:48] [119000] global_step=119000, grad_norm=4.243710041046143, loss=2.308641195297241
I0131 08:25:50.570169 139907276203776 logging_writer.py:48] [119100] global_step=119100, grad_norm=4.763708114624023, loss=2.372142791748047
I0131 08:26:24.294324 139907267811072 logging_writer.py:48] [119200] global_step=119200, grad_norm=4.756604194641113, loss=2.400045394897461
I0131 08:26:57.954366 139907276203776 logging_writer.py:48] [119300] global_step=119300, grad_norm=4.927645206451416, loss=2.3019020557403564
I0131 08:27:31.644781 139907267811072 logging_writer.py:48] [119400] global_step=119400, grad_norm=4.823864936828613, loss=2.3389060497283936
I0131 08:28:05.303269 139907276203776 logging_writer.py:48] [119500] global_step=119500, grad_norm=4.434375762939453, loss=2.382579803466797
I0131 08:28:15.208240 140070692116288 spec.py:321] Evaluating on the training split.
I0131 08:28:21.235652 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 08:28:30.262831 140070692116288 spec.py:349] Evaluating on the test split.
I0131 08:28:32.582452 140070692116288 submission_runner.py:408] Time since start: 41941.21s, 	Step: 119531, 	{'train/accuracy': 0.7692123651504517, 'train/loss': 0.9980911016464233, 'validation/accuracy': 0.6963399648666382, 'validation/loss': 1.3163288831710815, 'validation/num_examples': 50000, 'test/accuracy': 0.5732000470161438, 'test/loss': 1.9624512195587158, 'test/num_examples': 10000, 'score': 40350.17130947113, 'total_duration': 41941.20932650566, 'accumulated_submission_time': 40350.17130947113, 'accumulated_eval_time': 1582.9293761253357, 'accumulated_logging_time': 4.029150485992432}
I0131 08:28:32.621023 139907737556736 logging_writer.py:48] [119531] accumulated_eval_time=1582.929376, accumulated_logging_time=4.029150, accumulated_submission_time=40350.171309, global_step=119531, preemption_count=0, score=40350.171309, test/accuracy=0.573200, test/loss=1.962451, test/num_examples=10000, total_duration=41941.209327, train/accuracy=0.769212, train/loss=0.998091, validation/accuracy=0.696340, validation/loss=1.316329, validation/num_examples=50000
I0131 08:28:56.149190 139907745949440 logging_writer.py:48] [119600] global_step=119600, grad_norm=4.512508869171143, loss=2.4881839752197266
I0131 08:29:29.749852 139907737556736 logging_writer.py:48] [119700] global_step=119700, grad_norm=5.04717493057251, loss=2.3539695739746094
I0131 08:30:03.453208 139907745949440 logging_writer.py:48] [119800] global_step=119800, grad_norm=4.869603633880615, loss=2.3707945346832275
I0131 08:30:37.146845 139907737556736 logging_writer.py:48] [119900] global_step=119900, grad_norm=4.640443801879883, loss=2.339714527130127
I0131 08:31:10.889766 139907745949440 logging_writer.py:48] [120000] global_step=120000, grad_norm=4.581476211547852, loss=2.3700809478759766
I0131 08:31:44.640778 139907737556736 logging_writer.py:48] [120100] global_step=120100, grad_norm=4.374421119689941, loss=2.305539846420288
I0131 08:32:18.324656 139907745949440 logging_writer.py:48] [120200] global_step=120200, grad_norm=4.544734477996826, loss=2.4004974365234375
I0131 08:32:52.012506 139907737556736 logging_writer.py:48] [120300] global_step=120300, grad_norm=4.880273818969727, loss=2.3760972023010254
I0131 08:33:25.680301 139907745949440 logging_writer.py:48] [120400] global_step=120400, grad_norm=4.090467929840088, loss=2.3072237968444824
I0131 08:33:59.359348 139907737556736 logging_writer.py:48] [120500] global_step=120500, grad_norm=4.5716986656188965, loss=2.2907180786132812
I0131 08:34:33.023948 139907745949440 logging_writer.py:48] [120600] global_step=120600, grad_norm=4.693550109863281, loss=2.410494804382324
I0131 08:35:06.704315 139907737556736 logging_writer.py:48] [120700] global_step=120700, grad_norm=5.047427177429199, loss=2.4430222511291504
I0131 08:35:40.361099 139907745949440 logging_writer.py:48] [120800] global_step=120800, grad_norm=4.705359935760498, loss=2.359762191772461
I0131 08:36:14.038556 139907737556736 logging_writer.py:48] [120900] global_step=120900, grad_norm=4.8159637451171875, loss=2.3357110023498535
I0131 08:36:47.716669 139907745949440 logging_writer.py:48] [121000] global_step=121000, grad_norm=4.689084529876709, loss=2.3951635360717773
I0131 08:37:02.668541 140070692116288 spec.py:321] Evaluating on the training split.
I0131 08:37:09.010671 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 08:37:18.036875 140070692116288 spec.py:349] Evaluating on the test split.
I0131 08:37:20.311224 140070692116288 submission_runner.py:408] Time since start: 42468.94s, 	Step: 121046, 	{'train/accuracy': 0.7643494606018066, 'train/loss': 1.0025675296783447, 'validation/accuracy': 0.6955400109291077, 'validation/loss': 1.3097865581512451, 'validation/num_examples': 50000, 'test/accuracy': 0.5732000470161438, 'test/loss': 1.9472129344940186, 'test/num_examples': 10000, 'score': 40860.15645599365, 'total_duration': 42468.937943696976, 'accumulated_submission_time': 40860.15645599365, 'accumulated_eval_time': 1600.571870803833, 'accumulated_logging_time': 4.077746868133545}
I0131 08:37:20.349058 139907276203776 logging_writer.py:48] [121046] accumulated_eval_time=1600.571871, accumulated_logging_time=4.077747, accumulated_submission_time=40860.156456, global_step=121046, preemption_count=0, score=40860.156456, test/accuracy=0.573200, test/loss=1.947213, test/num_examples=10000, total_duration=42468.937944, train/accuracy=0.764349, train/loss=1.002568, validation/accuracy=0.695540, validation/loss=1.309787, validation/num_examples=50000
I0131 08:37:38.848181 139907284596480 logging_writer.py:48] [121100] global_step=121100, grad_norm=5.451167106628418, loss=2.2397704124450684
I0131 08:38:12.494524 139907276203776 logging_writer.py:48] [121200] global_step=121200, grad_norm=4.822505474090576, loss=2.3711984157562256
I0131 08:38:46.196276 139907284596480 logging_writer.py:48] [121300] global_step=121300, grad_norm=4.940790176391602, loss=2.242065906524658
I0131 08:39:19.847605 139907276203776 logging_writer.py:48] [121400] global_step=121400, grad_norm=4.901917934417725, loss=2.330536127090454
I0131 08:39:53.522139 139907284596480 logging_writer.py:48] [121500] global_step=121500, grad_norm=4.665106773376465, loss=2.3532466888427734
I0131 08:40:27.182525 139907276203776 logging_writer.py:48] [121600] global_step=121600, grad_norm=4.260833263397217, loss=2.3423752784729004
I0131 08:41:00.862803 139907284596480 logging_writer.py:48] [121700] global_step=121700, grad_norm=4.720088958740234, loss=2.365762233734131
I0131 08:41:34.514857 139907276203776 logging_writer.py:48] [121800] global_step=121800, grad_norm=4.790180683135986, loss=2.281832456588745
I0131 08:42:08.188678 139907284596480 logging_writer.py:48] [121900] global_step=121900, grad_norm=4.677848815917969, loss=2.432854175567627
I0131 08:42:41.822018 139907276203776 logging_writer.py:48] [122000] global_step=122000, grad_norm=4.394254684448242, loss=2.2772903442382812
I0131 08:43:15.571310 139907284596480 logging_writer.py:48] [122100] global_step=122100, grad_norm=5.292672634124756, loss=2.4057397842407227
I0131 08:43:49.302339 139907276203776 logging_writer.py:48] [122200] global_step=122200, grad_norm=5.086952209472656, loss=2.2628045082092285
I0131 08:44:23.000788 139907284596480 logging_writer.py:48] [122300] global_step=122300, grad_norm=4.801326274871826, loss=2.324355363845825
I0131 08:44:56.633967 139907276203776 logging_writer.py:48] [122400] global_step=122400, grad_norm=4.1908674240112305, loss=2.3081138134002686
I0131 08:45:30.323266 139907284596480 logging_writer.py:48] [122500] global_step=122500, grad_norm=4.8711700439453125, loss=2.3450357913970947
I0131 08:45:50.325103 140070692116288 spec.py:321] Evaluating on the training split.
I0131 08:45:56.374289 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 08:46:05.564325 140070692116288 spec.py:349] Evaluating on the test split.
I0131 08:46:07.839970 140070692116288 submission_runner.py:408] Time since start: 42996.47s, 	Step: 122561, 	{'train/accuracy': 0.7720224857330322, 'train/loss': 0.9792758822441101, 'validation/accuracy': 0.7010399699211121, 'validation/loss': 1.279943585395813, 'validation/num_examples': 50000, 'test/accuracy': 0.5808000564575195, 'test/loss': 1.9124568700790405, 'test/num_examples': 10000, 'score': 41370.070234537125, 'total_duration': 42996.46684598923, 'accumulated_submission_time': 41370.070234537125, 'accumulated_eval_time': 1618.0866899490356, 'accumulated_logging_time': 4.125354290008545}
I0131 08:46:07.877793 139907259418368 logging_writer.py:48] [122561] accumulated_eval_time=1618.086690, accumulated_logging_time=4.125354, accumulated_submission_time=41370.070235, global_step=122561, preemption_count=0, score=41370.070235, test/accuracy=0.580800, test/loss=1.912457, test/num_examples=10000, total_duration=42996.466846, train/accuracy=0.772022, train/loss=0.979276, validation/accuracy=0.701040, validation/loss=1.279944, validation/num_examples=50000
I0131 08:46:21.324248 139907267811072 logging_writer.py:48] [122600] global_step=122600, grad_norm=4.489284515380859, loss=2.3620858192443848
I0131 08:46:54.912421 139907259418368 logging_writer.py:48] [122700] global_step=122700, grad_norm=5.854944229125977, loss=2.259974718093872
I0131 08:47:28.649539 139907267811072 logging_writer.py:48] [122800] global_step=122800, grad_norm=4.467734336853027, loss=2.32894229888916
I0131 08:48:02.333340 139907259418368 logging_writer.py:48] [122900] global_step=122900, grad_norm=4.178102493286133, loss=2.2245712280273438
I0131 08:48:36.021713 139907267811072 logging_writer.py:48] [123000] global_step=123000, grad_norm=4.634555816650391, loss=2.3079628944396973
I0131 08:49:09.668963 139907259418368 logging_writer.py:48] [123100] global_step=123100, grad_norm=4.533689975738525, loss=2.2708616256713867
I0131 08:49:43.378609 139907267811072 logging_writer.py:48] [123200] global_step=123200, grad_norm=4.835755825042725, loss=2.361118793487549
I0131 08:50:17.068212 139907259418368 logging_writer.py:48] [123300] global_step=123300, grad_norm=4.642688274383545, loss=2.2706804275512695
I0131 08:50:50.760130 139907267811072 logging_writer.py:48] [123400] global_step=123400, grad_norm=4.733981132507324, loss=2.3572072982788086
I0131 08:51:24.428172 139907259418368 logging_writer.py:48] [123500] global_step=123500, grad_norm=5.123625755310059, loss=2.3722100257873535
I0131 08:51:58.120919 139907267811072 logging_writer.py:48] [123600] global_step=123600, grad_norm=4.75503396987915, loss=2.3212928771972656
I0131 08:52:31.792539 139907259418368 logging_writer.py:48] [123700] global_step=123700, grad_norm=4.525493621826172, loss=2.2533650398254395
I0131 08:53:05.478316 139907267811072 logging_writer.py:48] [123800] global_step=123800, grad_norm=5.254413604736328, loss=2.326620578765869
I0131 08:53:39.153763 139907259418368 logging_writer.py:48] [123900] global_step=123900, grad_norm=4.719845771789551, loss=2.2810113430023193
I0131 08:54:12.835865 139907267811072 logging_writer.py:48] [124000] global_step=124000, grad_norm=4.66758394241333, loss=2.2759182453155518
I0131 08:54:37.904343 140070692116288 spec.py:321] Evaluating on the training split.
I0131 08:54:44.039041 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 08:54:53.001621 140070692116288 spec.py:349] Evaluating on the test split.
I0131 08:54:55.319871 140070692116288 submission_runner.py:408] Time since start: 43523.95s, 	Step: 124076, 	{'train/accuracy': 0.7723413705825806, 'train/loss': 0.971625566482544, 'validation/accuracy': 0.7017599940299988, 'validation/loss': 1.276233196258545, 'validation/num_examples': 50000, 'test/accuracy': 0.5755000114440918, 'test/loss': 1.924700140953064, 'test/num_examples': 10000, 'score': 41880.03378677368, 'total_duration': 43523.94673323631, 'accumulated_submission_time': 41880.03378677368, 'accumulated_eval_time': 1635.502154827118, 'accumulated_logging_time': 4.173144578933716}
I0131 08:54:55.362389 139907284596480 logging_writer.py:48] [124076] accumulated_eval_time=1635.502155, accumulated_logging_time=4.173145, accumulated_submission_time=41880.033787, global_step=124076, preemption_count=0, score=41880.033787, test/accuracy=0.575500, test/loss=1.924700, test/num_examples=10000, total_duration=43523.946733, train/accuracy=0.772341, train/loss=0.971626, validation/accuracy=0.701760, validation/loss=1.276233, validation/num_examples=50000
I0131 08:55:03.794007 139907703985920 logging_writer.py:48] [124100] global_step=124100, grad_norm=4.39830207824707, loss=2.260678291320801
I0131 08:55:37.498199 139907284596480 logging_writer.py:48] [124200] global_step=124200, grad_norm=4.748779773712158, loss=2.353391408920288
I0131 08:56:11.142589 139907703985920 logging_writer.py:48] [124300] global_step=124300, grad_norm=4.821837425231934, loss=2.3979408740997314
I0131 08:56:44.829980 139907284596480 logging_writer.py:48] [124400] global_step=124400, grad_norm=4.677383899688721, loss=2.279332160949707
I0131 08:57:18.489155 139907703985920 logging_writer.py:48] [124500] global_step=124500, grad_norm=5.296476364135742, loss=2.3431925773620605
I0131 08:57:52.175846 139907284596480 logging_writer.py:48] [124600] global_step=124600, grad_norm=4.51179313659668, loss=2.262448310852051
I0131 08:58:25.824164 139907703985920 logging_writer.py:48] [124700] global_step=124700, grad_norm=4.656187057495117, loss=2.2871084213256836
I0131 08:58:59.495101 139907284596480 logging_writer.py:48] [124800] global_step=124800, grad_norm=5.110201358795166, loss=2.320631265640259
I0131 08:59:33.133644 139907703985920 logging_writer.py:48] [124900] global_step=124900, grad_norm=4.972586631774902, loss=2.309086322784424
I0131 09:00:06.836977 139907284596480 logging_writer.py:48] [125000] global_step=125000, grad_norm=4.523313045501709, loss=2.248591184616089
I0131 09:00:40.504855 139907703985920 logging_writer.py:48] [125100] global_step=125100, grad_norm=5.028141975402832, loss=2.3325369358062744
I0131 09:01:14.196168 139907284596480 logging_writer.py:48] [125200] global_step=125200, grad_norm=5.137024879455566, loss=2.3695762157440186
I0131 09:01:47.933450 139907703985920 logging_writer.py:48] [125300] global_step=125300, grad_norm=5.384795665740967, loss=2.3252670764923096
I0131 09:02:21.661399 139907284596480 logging_writer.py:48] [125400] global_step=125400, grad_norm=5.158699989318848, loss=2.344835042953491
I0131 09:02:55.306752 139907703985920 logging_writer.py:48] [125500] global_step=125500, grad_norm=5.41716194152832, loss=2.388126850128174
I0131 09:03:25.432726 140070692116288 spec.py:321] Evaluating on the training split.
I0131 09:03:31.582933 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 09:03:41.298680 140070692116288 spec.py:349] Evaluating on the test split.
I0131 09:03:43.574170 140070692116288 submission_runner.py:408] Time since start: 44052.20s, 	Step: 125591, 	{'train/accuracy': 0.7950015664100647, 'train/loss': 0.878669023513794, 'validation/accuracy': 0.6998400092124939, 'validation/loss': 1.3009834289550781, 'validation/num_examples': 50000, 'test/accuracy': 0.5813000202178955, 'test/loss': 1.9170032739639282, 'test/num_examples': 10000, 'score': 42390.04211616516, 'total_duration': 44052.20104932785, 'accumulated_submission_time': 42390.04211616516, 'accumulated_eval_time': 1653.6435549259186, 'accumulated_logging_time': 4.225467681884766}
I0131 09:03:43.618183 139907712378624 logging_writer.py:48] [125591] accumulated_eval_time=1653.643555, accumulated_logging_time=4.225468, accumulated_submission_time=42390.042116, global_step=125591, preemption_count=0, score=42390.042116, test/accuracy=0.581300, test/loss=1.917003, test/num_examples=10000, total_duration=44052.201049, train/accuracy=0.795002, train/loss=0.878669, validation/accuracy=0.699840, validation/loss=1.300983, validation/num_examples=50000
I0131 09:03:46.991450 139907720771328 logging_writer.py:48] [125600] global_step=125600, grad_norm=5.267796039581299, loss=2.3402836322784424
I0131 09:04:20.589309 139907712378624 logging_writer.py:48] [125700] global_step=125700, grad_norm=4.541416168212891, loss=2.3043158054351807
I0131 09:04:54.244619 139907720771328 logging_writer.py:48] [125800] global_step=125800, grad_norm=5.138378143310547, loss=2.2248682975769043
I0131 09:05:27.936528 139907712378624 logging_writer.py:48] [125900] global_step=125900, grad_norm=4.701931953430176, loss=2.285226345062256
I0131 09:06:01.592439 139907720771328 logging_writer.py:48] [126000] global_step=126000, grad_norm=5.953670501708984, loss=2.403561592102051
I0131 09:06:35.298999 139907712378624 logging_writer.py:48] [126100] global_step=126100, grad_norm=5.16552734375, loss=2.2494254112243652
I0131 09:07:08.972183 139907720771328 logging_writer.py:48] [126200] global_step=126200, grad_norm=4.562232971191406, loss=2.286099433898926
I0131 09:07:42.715695 139907712378624 logging_writer.py:48] [126300] global_step=126300, grad_norm=4.591494083404541, loss=2.239377498626709
I0131 09:08:16.423580 139907720771328 logging_writer.py:48] [126400] global_step=126400, grad_norm=4.860963344573975, loss=2.247527599334717
I0131 09:08:50.115793 139907712378624 logging_writer.py:48] [126500] global_step=126500, grad_norm=4.932956218719482, loss=2.260885715484619
I0131 09:09:23.775498 139907720771328 logging_writer.py:48] [126600] global_step=126600, grad_norm=4.573464870452881, loss=2.182849645614624
I0131 09:09:57.463516 139907712378624 logging_writer.py:48] [126700] global_step=126700, grad_norm=5.412196636199951, loss=2.25392746925354
I0131 09:10:31.120019 139907720771328 logging_writer.py:48] [126800] global_step=126800, grad_norm=4.552134990692139, loss=2.226789951324463
I0131 09:11:04.806556 139907712378624 logging_writer.py:48] [126900] global_step=126900, grad_norm=5.142825603485107, loss=2.2571911811828613
I0131 09:11:38.470140 139907720771328 logging_writer.py:48] [127000] global_step=127000, grad_norm=4.960847854614258, loss=2.2898659706115723
I0131 09:12:12.158997 139907712378624 logging_writer.py:48] [127100] global_step=127100, grad_norm=5.113381862640381, loss=2.256155252456665
I0131 09:12:13.655824 140070692116288 spec.py:321] Evaluating on the training split.
I0131 09:12:19.865038 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 09:12:28.874941 140070692116288 spec.py:349] Evaluating on the test split.
I0131 09:12:31.170925 140070692116288 submission_runner.py:408] Time since start: 44579.80s, 	Step: 127106, 	{'train/accuracy': 0.78714919090271, 'train/loss': 0.9083539247512817, 'validation/accuracy': 0.7061799764633179, 'validation/loss': 1.2675763368606567, 'validation/num_examples': 50000, 'test/accuracy': 0.5827000141143799, 'test/loss': 1.9058300256729126, 'test/num_examples': 10000, 'score': 42900.01652574539, 'total_duration': 44579.797763586044, 'accumulated_submission_time': 42900.01652574539, 'accumulated_eval_time': 1671.1585881710052, 'accumulated_logging_time': 4.279773235321045}
I0131 09:12:31.218242 139907267811072 logging_writer.py:48] [127106] accumulated_eval_time=1671.158588, accumulated_logging_time=4.279773, accumulated_submission_time=42900.016526, global_step=127106, preemption_count=0, score=42900.016526, test/accuracy=0.582700, test/loss=1.905830, test/num_examples=10000, total_duration=44579.797764, train/accuracy=0.787149, train/loss=0.908354, validation/accuracy=0.706180, validation/loss=1.267576, validation/num_examples=50000
I0131 09:13:03.961387 139907276203776 logging_writer.py:48] [127200] global_step=127200, grad_norm=5.285202503204346, loss=2.310096025466919
I0131 09:13:37.651173 139907267811072 logging_writer.py:48] [127300] global_step=127300, grad_norm=4.69027042388916, loss=2.404043674468994
I0131 09:14:11.385754 139907276203776 logging_writer.py:48] [127400] global_step=127400, grad_norm=5.346296310424805, loss=2.320878744125366
I0131 09:14:45.078166 139907267811072 logging_writer.py:48] [127500] global_step=127500, grad_norm=4.7786545753479, loss=2.285567283630371
I0131 09:15:18.739829 139907276203776 logging_writer.py:48] [127600] global_step=127600, grad_norm=5.112433910369873, loss=2.262298345565796
I0131 09:15:52.424976 139907267811072 logging_writer.py:48] [127700] global_step=127700, grad_norm=5.319061279296875, loss=2.274541139602661
I0131 09:16:26.081586 139907276203776 logging_writer.py:48] [127800] global_step=127800, grad_norm=5.255514144897461, loss=2.36417818069458
I0131 09:16:59.783017 139907267811072 logging_writer.py:48] [127900] global_step=127900, grad_norm=5.2206501960754395, loss=2.347227096557617
I0131 09:17:33.433933 139907276203776 logging_writer.py:48] [128000] global_step=128000, grad_norm=5.11041259765625, loss=2.3459606170654297
I0131 09:18:07.158532 139907267811072 logging_writer.py:48] [128100] global_step=128100, grad_norm=5.377942085266113, loss=2.328798294067383
I0131 09:18:40.847960 139907276203776 logging_writer.py:48] [128200] global_step=128200, grad_norm=5.337921142578125, loss=2.320857048034668
I0131 09:19:14.538578 139907267811072 logging_writer.py:48] [128300] global_step=128300, grad_norm=4.372103214263916, loss=2.3549728393554688
I0131 09:19:48.210569 139907276203776 logging_writer.py:48] [128400] global_step=128400, grad_norm=5.143173694610596, loss=2.2698278427124023
I0131 09:20:21.990456 139907267811072 logging_writer.py:48] [128500] global_step=128500, grad_norm=5.4331955909729, loss=2.284773826599121
I0131 09:20:55.659899 139907276203776 logging_writer.py:48] [128600] global_step=128600, grad_norm=4.809323787689209, loss=2.2309699058532715
I0131 09:21:01.191752 140070692116288 spec.py:321] Evaluating on the training split.
I0131 09:21:07.440308 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 09:21:16.175269 140070692116288 spec.py:349] Evaluating on the test split.
I0131 09:21:18.484602 140070692116288 submission_runner.py:408] Time since start: 45107.11s, 	Step: 128618, 	{'train/accuracy': 0.7895806431770325, 'train/loss': 0.9128103852272034, 'validation/accuracy': 0.7077999711036682, 'validation/loss': 1.2611401081085205, 'validation/num_examples': 50000, 'test/accuracy': 0.5888000130653381, 'test/loss': 1.886717438697815, 'test/num_examples': 10000, 'score': 43409.1985976696, 'total_duration': 45107.111483335495, 'accumulated_submission_time': 43409.1985976696, 'accumulated_eval_time': 1688.4513931274414, 'accumulated_logging_time': 5.066087961196899}
I0131 09:21:18.522363 139907712378624 logging_writer.py:48] [128618] accumulated_eval_time=1688.451393, accumulated_logging_time=5.066088, accumulated_submission_time=43409.198598, global_step=128618, preemption_count=0, score=43409.198598, test/accuracy=0.588800, test/loss=1.886717, test/num_examples=10000, total_duration=45107.111483, train/accuracy=0.789581, train/loss=0.912810, validation/accuracy=0.707800, validation/loss=1.261140, validation/num_examples=50000
I0131 09:21:46.448530 139907720771328 logging_writer.py:48] [128700] global_step=128700, grad_norm=4.964395523071289, loss=2.159841299057007
I0131 09:22:20.147564 139907712378624 logging_writer.py:48] [128800] global_step=128800, grad_norm=5.301344394683838, loss=2.2789392471313477
I0131 09:22:53.807442 139907720771328 logging_writer.py:48] [128900] global_step=128900, grad_norm=4.933650970458984, loss=2.2764406204223633
I0131 09:23:27.516832 139907712378624 logging_writer.py:48] [129000] global_step=129000, grad_norm=4.75334358215332, loss=2.220524549484253
I0131 09:24:01.170118 139907720771328 logging_writer.py:48] [129100] global_step=129100, grad_norm=5.148523807525635, loss=2.314249038696289
I0131 09:24:34.869360 139907712378624 logging_writer.py:48] [129200] global_step=129200, grad_norm=5.216090679168701, loss=2.2910537719726562
I0131 09:25:08.515041 139907720771328 logging_writer.py:48] [129300] global_step=129300, grad_norm=5.259037017822266, loss=2.327876329421997
I0131 09:25:42.194561 139907712378624 logging_writer.py:48] [129400] global_step=129400, grad_norm=4.762455463409424, loss=2.156998872756958
I0131 09:26:15.926955 139907720771328 logging_writer.py:48] [129500] global_step=129500, grad_norm=5.132348537445068, loss=2.272146701812744
I0131 09:26:49.673660 139907712378624 logging_writer.py:48] [129600] global_step=129600, grad_norm=5.698916912078857, loss=2.1852147579193115
I0131 09:27:23.339955 139907720771328 logging_writer.py:48] [129700] global_step=129700, grad_norm=5.478888034820557, loss=2.2404143810272217
I0131 09:27:57.056638 139907712378624 logging_writer.py:48] [129800] global_step=129800, grad_norm=4.771590709686279, loss=2.205498456954956
I0131 09:28:30.714549 139907720771328 logging_writer.py:48] [129900] global_step=129900, grad_norm=5.053102493286133, loss=2.2891147136688232
I0131 09:29:04.429110 139907712378624 logging_writer.py:48] [130000] global_step=130000, grad_norm=5.091547966003418, loss=2.349480390548706
I0131 09:29:38.089080 139907720771328 logging_writer.py:48] [130100] global_step=130100, grad_norm=5.868106842041016, loss=2.1874468326568604
I0131 09:29:48.669463 140070692116288 spec.py:321] Evaluating on the training split.
I0131 09:29:54.898613 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 09:30:04.037158 140070692116288 spec.py:349] Evaluating on the test split.
I0131 09:30:06.306092 140070692116288 submission_runner.py:408] Time since start: 45634.93s, 	Step: 130133, 	{'train/accuracy': 0.7902981638908386, 'train/loss': 0.9130910634994507, 'validation/accuracy': 0.711899995803833, 'validation/loss': 1.2529252767562866, 'validation/num_examples': 50000, 'test/accuracy': 0.5927000045776367, 'test/loss': 1.862455129623413, 'test/num_examples': 10000, 'score': 43919.28074002266, 'total_duration': 45634.932916879654, 'accumulated_submission_time': 43919.28074002266, 'accumulated_eval_time': 1706.0879232883453, 'accumulated_logging_time': 5.117692470550537}
I0131 09:30:06.371329 139907276203776 logging_writer.py:48] [130133] accumulated_eval_time=1706.087923, accumulated_logging_time=5.117692, accumulated_submission_time=43919.280740, global_step=130133, preemption_count=0, score=43919.280740, test/accuracy=0.592700, test/loss=1.862455, test/num_examples=10000, total_duration=45634.932917, train/accuracy=0.790298, train/loss=0.913091, validation/accuracy=0.711900, validation/loss=1.252925, validation/num_examples=50000
I0131 09:30:29.254746 139907284596480 logging_writer.py:48] [130200] global_step=130200, grad_norm=5.640390396118164, loss=2.2106127738952637
I0131 09:31:02.901800 139907276203776 logging_writer.py:48] [130300] global_step=130300, grad_norm=5.529541969299316, loss=2.1866326332092285
I0131 09:31:36.588461 139907284596480 logging_writer.py:48] [130400] global_step=130400, grad_norm=4.813979625701904, loss=2.195430278778076
I0131 09:32:10.300571 139907276203776 logging_writer.py:48] [130500] global_step=130500, grad_norm=4.657773017883301, loss=2.159862756729126
I0131 09:32:43.960814 139907284596480 logging_writer.py:48] [130600] global_step=130600, grad_norm=5.550996780395508, loss=2.204557180404663
I0131 09:33:17.628877 139907276203776 logging_writer.py:48] [130700] global_step=130700, grad_norm=4.835667610168457, loss=2.258131504058838
I0131 09:33:51.306408 139907284596480 logging_writer.py:48] [130800] global_step=130800, grad_norm=5.145386695861816, loss=2.2088303565979004
I0131 09:34:24.980579 139907276203776 logging_writer.py:48] [130900] global_step=130900, grad_norm=5.4394659996032715, loss=2.2070491313934326
I0131 09:34:58.669191 139907284596480 logging_writer.py:48] [131000] global_step=131000, grad_norm=4.9643402099609375, loss=2.2296526432037354
I0131 09:35:32.339127 139907276203776 logging_writer.py:48] [131100] global_step=131100, grad_norm=5.191390514373779, loss=2.203400135040283
I0131 09:36:06.018441 139907284596480 logging_writer.py:48] [131200] global_step=131200, grad_norm=4.908767223358154, loss=2.2278475761413574
I0131 09:36:39.702037 139907276203776 logging_writer.py:48] [131300] global_step=131300, grad_norm=5.416501998901367, loss=2.2211766242980957
I0131 09:37:13.384715 139907284596480 logging_writer.py:48] [131400] global_step=131400, grad_norm=4.515653133392334, loss=2.2515621185302734
I0131 09:37:47.059326 139907276203776 logging_writer.py:48] [131500] global_step=131500, grad_norm=5.291280746459961, loss=2.328371286392212
I0131 09:38:20.801104 139907284596480 logging_writer.py:48] [131600] global_step=131600, grad_norm=6.100405216217041, loss=2.242760181427002
I0131 09:38:36.459779 140070692116288 spec.py:321] Evaluating on the training split.
I0131 09:38:42.606678 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 09:38:51.276260 140070692116288 spec.py:349] Evaluating on the test split.
I0131 09:38:53.556867 140070692116288 submission_runner.py:408] Time since start: 46162.18s, 	Step: 131648, 	{'train/accuracy': 0.7849569320678711, 'train/loss': 0.9189361333847046, 'validation/accuracy': 0.7110599875450134, 'validation/loss': 1.247543454170227, 'validation/num_examples': 50000, 'test/accuracy': 0.5861000418663025, 'test/loss': 1.8845510482788086, 'test/num_examples': 10000, 'score': 44429.30247211456, 'total_duration': 46162.18374609947, 'accumulated_submission_time': 44429.30247211456, 'accumulated_eval_time': 1723.184979915619, 'accumulated_logging_time': 5.197498321533203}
I0131 09:38:53.595468 139907276203776 logging_writer.py:48] [131648] accumulated_eval_time=1723.184980, accumulated_logging_time=5.197498, accumulated_submission_time=44429.302472, global_step=131648, preemption_count=0, score=44429.302472, test/accuracy=0.586100, test/loss=1.884551, test/num_examples=10000, total_duration=46162.183746, train/accuracy=0.784957, train/loss=0.918936, validation/accuracy=0.711060, validation/loss=1.247543, validation/num_examples=50000
I0131 09:39:11.466071 139907284596480 logging_writer.py:48] [131700] global_step=131700, grad_norm=5.792852401733398, loss=2.2421979904174805
I0131 09:39:45.123776 139907276203776 logging_writer.py:48] [131800] global_step=131800, grad_norm=5.476200103759766, loss=2.245298147201538
I0131 09:40:18.836444 139907284596480 logging_writer.py:48] [131900] global_step=131900, grad_norm=5.2749409675598145, loss=2.238807439804077
I0131 09:40:52.499478 139907276203776 logging_writer.py:48] [132000] global_step=132000, grad_norm=4.770877838134766, loss=2.1840872764587402
I0131 09:41:26.194669 139907284596480 logging_writer.py:48] [132100] global_step=132100, grad_norm=5.120418071746826, loss=2.243727684020996
I0131 09:41:59.866744 139907276203776 logging_writer.py:48] [132200] global_step=132200, grad_norm=5.166369438171387, loss=2.246739625930786
I0131 09:42:33.570486 139907284596480 logging_writer.py:48] [132300] global_step=132300, grad_norm=5.188446044921875, loss=2.2268617153167725
I0131 09:43:07.231356 139907276203776 logging_writer.py:48] [132400] global_step=132400, grad_norm=5.095048427581787, loss=2.2287864685058594
I0131 09:43:40.934292 139907284596480 logging_writer.py:48] [132500] global_step=132500, grad_norm=5.201696872711182, loss=2.1660468578338623
I0131 09:44:14.586148 139907276203776 logging_writer.py:48] [132600] global_step=132600, grad_norm=5.3339972496032715, loss=2.259934186935425
I0131 09:44:48.357394 139907284596480 logging_writer.py:48] [132700] global_step=132700, grad_norm=5.104486465454102, loss=2.2481131553649902
I0131 09:45:22.028417 139907276203776 logging_writer.py:48] [132800] global_step=132800, grad_norm=5.189178943634033, loss=2.16318416595459
I0131 09:45:55.737733 139907284596480 logging_writer.py:48] [132900] global_step=132900, grad_norm=5.558574199676514, loss=2.298053503036499
I0131 09:46:29.384667 139907276203776 logging_writer.py:48] [133000] global_step=133000, grad_norm=5.3606133460998535, loss=2.3319926261901855
I0131 09:47:03.091659 139907284596480 logging_writer.py:48] [133100] global_step=133100, grad_norm=5.816882610321045, loss=2.1545209884643555
I0131 09:47:23.763627 140070692116288 spec.py:321] Evaluating on the training split.
I0131 09:47:29.966897 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 09:47:38.724647 140070692116288 spec.py:349] Evaluating on the test split.
I0131 09:47:40.946496 140070692116288 submission_runner.py:408] Time since start: 46689.57s, 	Step: 133163, 	{'train/accuracy': 0.7988081574440002, 'train/loss': 0.8635379076004028, 'validation/accuracy': 0.7162399888038635, 'validation/loss': 1.2158832550048828, 'validation/num_examples': 50000, 'test/accuracy': 0.5948000550270081, 'test/loss': 1.8518083095550537, 'test/num_examples': 10000, 'score': 44939.4055583477, 'total_duration': 46689.57337188721, 'accumulated_submission_time': 44939.4055583477, 'accumulated_eval_time': 1740.367802619934, 'accumulated_logging_time': 5.248680830001831}
I0131 09:47:40.985193 139907276203776 logging_writer.py:48] [133163] accumulated_eval_time=1740.367803, accumulated_logging_time=5.248681, accumulated_submission_time=44939.405558, global_step=133163, preemption_count=0, score=44939.405558, test/accuracy=0.594800, test/loss=1.851808, test/num_examples=10000, total_duration=46689.573372, train/accuracy=0.798808, train/loss=0.863538, validation/accuracy=0.716240, validation/loss=1.215883, validation/num_examples=50000
I0131 09:47:53.808132 139907284596480 logging_writer.py:48] [133200] global_step=133200, grad_norm=5.8513264656066895, loss=2.3118488788604736
I0131 09:48:27.492846 139907276203776 logging_writer.py:48] [133300] global_step=133300, grad_norm=5.15981388092041, loss=2.2134006023406982
I0131 09:49:01.154676 139907284596480 logging_writer.py:48] [133400] global_step=133400, grad_norm=5.638075351715088, loss=2.3054771423339844
I0131 09:49:34.817372 139907276203776 logging_writer.py:48] [133500] global_step=133500, grad_norm=5.455971717834473, loss=2.2140305042266846
I0131 09:50:08.483808 139907284596480 logging_writer.py:48] [133600] global_step=133600, grad_norm=5.7318267822265625, loss=2.183788299560547
I0131 09:50:42.254984 139907276203776 logging_writer.py:48] [133700] global_step=133700, grad_norm=5.340155124664307, loss=2.2233145236968994
I0131 09:51:15.965302 139907284596480 logging_writer.py:48] [133800] global_step=133800, grad_norm=6.135325908660889, loss=2.323215961456299
I0131 09:51:49.650837 139907276203776 logging_writer.py:48] [133900] global_step=133900, grad_norm=5.238656520843506, loss=2.1811375617980957
I0131 09:52:23.307218 139907284596480 logging_writer.py:48] [134000] global_step=134000, grad_norm=5.626094818115234, loss=2.195622444152832
I0131 09:52:56.992301 139907276203776 logging_writer.py:48] [134100] global_step=134100, grad_norm=5.241239070892334, loss=2.160641670227051
I0131 09:53:30.702100 139907284596480 logging_writer.py:48] [134200] global_step=134200, grad_norm=4.965615272521973, loss=2.178624153137207
I0131 09:54:04.384627 139907276203776 logging_writer.py:48] [134300] global_step=134300, grad_norm=5.3104634284973145, loss=2.2408080101013184
I0131 09:54:38.068482 139907284596480 logging_writer.py:48] [134400] global_step=134400, grad_norm=5.351666450500488, loss=2.1535003185272217
I0131 09:55:11.752238 139907276203776 logging_writer.py:48] [134500] global_step=134500, grad_norm=5.499332904815674, loss=2.289156913757324
I0131 09:55:45.432234 139907284596480 logging_writer.py:48] [134600] global_step=134600, grad_norm=5.087550640106201, loss=2.1497087478637695
I0131 09:56:11.177049 140070692116288 spec.py:321] Evaluating on the training split.
I0131 09:56:17.306526 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 09:56:26.181395 140070692116288 spec.py:349] Evaluating on the test split.
I0131 09:56:28.495401 140070692116288 submission_runner.py:408] Time since start: 47217.12s, 	Step: 134678, 	{'train/accuracy': 0.8146922588348389, 'train/loss': 0.8167023658752441, 'validation/accuracy': 0.7167999744415283, 'validation/loss': 1.2303428649902344, 'validation/num_examples': 50000, 'test/accuracy': 0.5956000089645386, 'test/loss': 1.8589153289794922, 'test/num_examples': 10000, 'score': 45449.53480505943, 'total_duration': 47217.12211894989, 'accumulated_submission_time': 45449.53480505943, 'accumulated_eval_time': 1757.6859464645386, 'accumulated_logging_time': 5.297849655151367}
I0131 09:56:28.541842 139907729164032 logging_writer.py:48] [134678] accumulated_eval_time=1757.685946, accumulated_logging_time=5.297850, accumulated_submission_time=45449.534805, global_step=134678, preemption_count=0, score=45449.534805, test/accuracy=0.595600, test/loss=1.858915, test/num_examples=10000, total_duration=47217.122119, train/accuracy=0.814692, train/loss=0.816702, validation/accuracy=0.716800, validation/loss=1.230343, validation/num_examples=50000
I0131 09:56:36.341775 139907737556736 logging_writer.py:48] [134700] global_step=134700, grad_norm=5.300811767578125, loss=2.2722465991973877
I0131 09:57:10.065699 139907729164032 logging_writer.py:48] [134800] global_step=134800, grad_norm=4.942920207977295, loss=2.188748836517334
I0131 09:57:43.746502 139907737556736 logging_writer.py:48] [134900] global_step=134900, grad_norm=5.2728471755981445, loss=2.1215550899505615
I0131 09:58:17.423510 139907729164032 logging_writer.py:48] [135000] global_step=135000, grad_norm=5.67952299118042, loss=2.156466007232666
I0131 09:58:51.089639 139907737556736 logging_writer.py:48] [135100] global_step=135100, grad_norm=5.556410789489746, loss=2.167238235473633
I0131 09:59:24.780376 139907729164032 logging_writer.py:48] [135200] global_step=135200, grad_norm=5.7491278648376465, loss=2.2645318508148193
I0131 09:59:58.439096 139907737556736 logging_writer.py:48] [135300] global_step=135300, grad_norm=5.625095367431641, loss=2.2360692024230957
I0131 10:00:32.115237 139907729164032 logging_writer.py:48] [135400] global_step=135400, grad_norm=5.919281482696533, loss=2.307190418243408
I0131 10:01:05.771324 139907737556736 logging_writer.py:48] [135500] global_step=135500, grad_norm=5.0165324211120605, loss=2.2789340019226074
I0131 10:01:39.454535 139907729164032 logging_writer.py:48] [135600] global_step=135600, grad_norm=5.390376091003418, loss=2.1943039894104004
I0131 10:02:13.123479 139907737556736 logging_writer.py:48] [135700] global_step=135700, grad_norm=5.148677825927734, loss=2.146851062774658
I0131 10:02:46.875539 139907729164032 logging_writer.py:48] [135800] global_step=135800, grad_norm=5.515012741088867, loss=2.156287431716919
I0131 10:03:20.583883 139907737556736 logging_writer.py:48] [135900] global_step=135900, grad_norm=5.505014896392822, loss=2.138599157333374
I0131 10:03:54.281343 139907729164032 logging_writer.py:48] [136000] global_step=136000, grad_norm=5.0892510414123535, loss=2.211026906967163
I0131 10:04:27.950113 139907737556736 logging_writer.py:48] [136100] global_step=136100, grad_norm=5.22443962097168, loss=2.2063674926757812
I0131 10:04:58.746124 140070692116288 spec.py:321] Evaluating on the training split.
I0131 10:05:05.061361 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 10:05:14.003721 140070692116288 spec.py:349] Evaluating on the test split.
I0131 10:05:16.252971 140070692116288 submission_runner.py:408] Time since start: 47744.88s, 	Step: 136193, 	{'train/accuracy': 0.812519907951355, 'train/loss': 0.822140634059906, 'validation/accuracy': 0.7186399698257446, 'validation/loss': 1.2230992317199707, 'validation/num_examples': 50000, 'test/accuracy': 0.597000002861023, 'test/loss': 1.8365988731384277, 'test/num_examples': 10000, 'score': 45959.675520420074, 'total_duration': 47744.87985253334, 'accumulated_submission_time': 45959.675520420074, 'accumulated_eval_time': 1775.1927635669708, 'accumulated_logging_time': 5.355523347854614}
I0131 10:05:16.292209 139907267811072 logging_writer.py:48] [136193] accumulated_eval_time=1775.192764, accumulated_logging_time=5.355523, accumulated_submission_time=45959.675520, global_step=136193, preemption_count=0, score=45959.675520, test/accuracy=0.597000, test/loss=1.836599, test/num_examples=10000, total_duration=47744.879853, train/accuracy=0.812520, train/loss=0.822141, validation/accuracy=0.718640, validation/loss=1.223099, validation/num_examples=50000
I0131 10:05:19.001993 139907276203776 logging_writer.py:48] [136200] global_step=136200, grad_norm=5.548081874847412, loss=2.227023124694824
I0131 10:05:52.682566 139907267811072 logging_writer.py:48] [136300] global_step=136300, grad_norm=5.642968654632568, loss=2.1675524711608887
I0131 10:06:26.376506 139907276203776 logging_writer.py:48] [136400] global_step=136400, grad_norm=5.737675189971924, loss=2.151559352874756
I0131 10:07:00.069319 139907267811072 logging_writer.py:48] [136500] global_step=136500, grad_norm=5.372763156890869, loss=2.173449754714966
I0131 10:07:33.755191 139907276203776 logging_writer.py:48] [136600] global_step=136600, grad_norm=6.0920257568359375, loss=2.1805076599121094
I0131 10:08:07.495247 139907267811072 logging_writer.py:48] [136700] global_step=136700, grad_norm=5.120314598083496, loss=2.191103219985962
I0131 10:08:41.167984 139907276203776 logging_writer.py:48] [136800] global_step=136800, grad_norm=5.396114349365234, loss=2.1920814514160156
I0131 10:09:14.961997 139907267811072 logging_writer.py:48] [136900] global_step=136900, grad_norm=5.536356449127197, loss=2.1567728519439697
I0131 10:09:48.653571 139907276203776 logging_writer.py:48] [137000] global_step=137000, grad_norm=5.15170431137085, loss=2.142422676086426
I0131 10:10:22.406602 139907267811072 logging_writer.py:48] [137100] global_step=137100, grad_norm=5.641480445861816, loss=2.2211952209472656
I0131 10:10:56.089754 139907276203776 logging_writer.py:48] [137200] global_step=137200, grad_norm=5.39164400100708, loss=2.1661086082458496
I0131 10:11:29.761781 139907267811072 logging_writer.py:48] [137300] global_step=137300, grad_norm=5.155615329742432, loss=2.18363356590271
I0131 10:12:03.429192 139907276203776 logging_writer.py:48] [137400] global_step=137400, grad_norm=5.57625675201416, loss=2.183555841445923
I0131 10:12:37.116686 139907267811072 logging_writer.py:48] [137500] global_step=137500, grad_norm=5.29716157913208, loss=2.233253240585327
I0131 10:13:10.778286 139907276203776 logging_writer.py:48] [137600] global_step=137600, grad_norm=4.963280200958252, loss=2.126706600189209
I0131 10:13:44.470267 139907267811072 logging_writer.py:48] [137700] global_step=137700, grad_norm=5.86615514755249, loss=2.2618606090545654
I0131 10:13:46.314406 140070692116288 spec.py:321] Evaluating on the training split.
I0131 10:13:52.425325 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 10:14:01.280103 140070692116288 spec.py:349] Evaluating on the test split.
I0131 10:14:03.620766 140070692116288 submission_runner.py:408] Time since start: 48272.25s, 	Step: 137707, 	{'train/accuracy': 0.8054049611091614, 'train/loss': 0.8238815069198608, 'validation/accuracy': 0.7189599871635437, 'validation/loss': 1.202906847000122, 'validation/num_examples': 50000, 'test/accuracy': 0.5964000225067139, 'test/loss': 1.854988932609558, 'test/num_examples': 10000, 'score': 46469.635543346405, 'total_duration': 48272.247631549835, 'accumulated_submission_time': 46469.635543346405, 'accumulated_eval_time': 1792.4990637302399, 'accumulated_logging_time': 5.405432224273682}
I0131 10:14:03.662225 139907267811072 logging_writer.py:48] [137707] accumulated_eval_time=1792.499064, accumulated_logging_time=5.405432, accumulated_submission_time=46469.635543, global_step=137707, preemption_count=0, score=46469.635543, test/accuracy=0.596400, test/loss=1.854989, test/num_examples=10000, total_duration=48272.247632, train/accuracy=0.805405, train/loss=0.823882, validation/accuracy=0.718960, validation/loss=1.202907, validation/num_examples=50000
I0131 10:14:35.295685 139907720771328 logging_writer.py:48] [137800] global_step=137800, grad_norm=4.908048629760742, loss=2.1443333625793457
I0131 10:15:09.032175 139907267811072 logging_writer.py:48] [137900] global_step=137900, grad_norm=6.923530101776123, loss=2.1983699798583984
I0131 10:15:42.679437 139907720771328 logging_writer.py:48] [138000] global_step=138000, grad_norm=6.315634727478027, loss=2.199620246887207
I0131 10:16:16.372094 139907267811072 logging_writer.py:48] [138100] global_step=138100, grad_norm=5.851632118225098, loss=2.1589155197143555
I0131 10:16:50.046042 139907720771328 logging_writer.py:48] [138200] global_step=138200, grad_norm=5.235259056091309, loss=2.1997482776641846
I0131 10:17:23.735590 139907267811072 logging_writer.py:48] [138300] global_step=138300, grad_norm=6.376697540283203, loss=2.1972851753234863
I0131 10:17:57.390133 139907720771328 logging_writer.py:48] [138400] global_step=138400, grad_norm=6.069034099578857, loss=2.2399191856384277
I0131 10:18:31.107777 139907267811072 logging_writer.py:48] [138500] global_step=138500, grad_norm=5.233239650726318, loss=2.1384453773498535
I0131 10:19:04.774358 139907720771328 logging_writer.py:48] [138600] global_step=138600, grad_norm=5.447259426116943, loss=2.2131290435791016
I0131 10:19:38.470251 139907267811072 logging_writer.py:48] [138700] global_step=138700, grad_norm=6.75188684463501, loss=2.2297182083129883
I0131 10:20:12.116665 139907720771328 logging_writer.py:48] [138800] global_step=138800, grad_norm=5.853837966918945, loss=2.127861261367798
I0131 10:20:45.831820 139907267811072 logging_writer.py:48] [138900] global_step=138900, grad_norm=4.988493919372559, loss=2.176154613494873
I0131 10:21:19.609210 139907720771328 logging_writer.py:48] [139000] global_step=139000, grad_norm=5.9460954666137695, loss=2.193903923034668
I0131 10:21:53.297947 139907267811072 logging_writer.py:48] [139100] global_step=139100, grad_norm=5.7336883544921875, loss=2.1439096927642822
I0131 10:22:26.955928 139907720771328 logging_writer.py:48] [139200] global_step=139200, grad_norm=5.6525163650512695, loss=2.160827875137329
I0131 10:22:33.841276 140070692116288 spec.py:321] Evaluating on the training split.
I0131 10:22:39.977055 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 10:22:48.917022 140070692116288 spec.py:349] Evaluating on the test split.
I0131 10:22:51.231749 140070692116288 submission_runner.py:408] Time since start: 48799.86s, 	Step: 139222, 	{'train/accuracy': 0.8106265664100647, 'train/loss': 0.8035197854042053, 'validation/accuracy': 0.7279599905014038, 'validation/loss': 1.174459457397461, 'validation/num_examples': 50000, 'test/accuracy': 0.6018000245094299, 'test/loss': 1.8125733137130737, 'test/num_examples': 10000, 'score': 46979.75103497505, 'total_duration': 48799.85860180855, 'accumulated_submission_time': 46979.75103497505, 'accumulated_eval_time': 1809.889460325241, 'accumulated_logging_time': 5.458134174346924}
I0131 10:22:51.286273 139907284596480 logging_writer.py:48] [139222] accumulated_eval_time=1809.889460, accumulated_logging_time=5.458134, accumulated_submission_time=46979.751035, global_step=139222, preemption_count=0, score=46979.751035, test/accuracy=0.601800, test/loss=1.812573, test/num_examples=10000, total_duration=48799.858602, train/accuracy=0.810627, train/loss=0.803520, validation/accuracy=0.727960, validation/loss=1.174459, validation/num_examples=50000
I0131 10:23:17.912647 139907703985920 logging_writer.py:48] [139300] global_step=139300, grad_norm=5.969715595245361, loss=2.209683418273926
I0131 10:23:51.612180 139907284596480 logging_writer.py:48] [139400] global_step=139400, grad_norm=5.435876846313477, loss=2.0911409854888916
I0131 10:24:25.272084 139907703985920 logging_writer.py:48] [139500] global_step=139500, grad_norm=5.947361469268799, loss=2.22658371925354
I0131 10:24:58.965269 139907284596480 logging_writer.py:48] [139600] global_step=139600, grad_norm=5.688157081604004, loss=2.142366647720337
I0131 10:25:32.635815 139907703985920 logging_writer.py:48] [139700] global_step=139700, grad_norm=5.153337001800537, loss=2.0728952884674072
I0131 10:26:06.284579 139907284596480 logging_writer.py:48] [139800] global_step=139800, grad_norm=6.126055717468262, loss=2.152613401412964
I0131 10:26:39.946143 139907703985920 logging_writer.py:48] [139900] global_step=139900, grad_norm=5.570171356201172, loss=2.1279141902923584
I0131 10:27:13.668414 139907284596480 logging_writer.py:48] [140000] global_step=140000, grad_norm=6.0741753578186035, loss=2.135808229446411
I0131 10:27:47.395924 139907703985920 logging_writer.py:48] [140100] global_step=140100, grad_norm=5.4361186027526855, loss=2.2334043979644775
I0131 10:28:21.087160 139907284596480 logging_writer.py:48] [140200] global_step=140200, grad_norm=5.543342590332031, loss=2.109501600265503
I0131 10:28:54.727665 139907703985920 logging_writer.py:48] [140300] global_step=140300, grad_norm=5.765444755554199, loss=2.137704372406006
I0131 10:29:28.408529 139907284596480 logging_writer.py:48] [140400] global_step=140400, grad_norm=5.732584476470947, loss=2.1165616512298584
I0131 10:30:02.121908 139907703985920 logging_writer.py:48] [140500] global_step=140500, grad_norm=6.463565349578857, loss=2.189108371734619
I0131 10:30:35.819644 139907284596480 logging_writer.py:48] [140600] global_step=140600, grad_norm=5.848639965057373, loss=2.0864315032958984
I0131 10:31:09.488601 139907703985920 logging_writer.py:48] [140700] global_step=140700, grad_norm=5.557837963104248, loss=2.16367244720459
I0131 10:31:21.422645 140070692116288 spec.py:321] Evaluating on the training split.
I0131 10:31:27.610632 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 10:31:36.652852 140070692116288 spec.py:349] Evaluating on the test split.
I0131 10:31:38.951193 140070692116288 submission_runner.py:408] Time since start: 49327.58s, 	Step: 140737, 	{'train/accuracy': 0.8134167790412903, 'train/loss': 0.8105883598327637, 'validation/accuracy': 0.7305999994277954, 'validation/loss': 1.175118327140808, 'validation/num_examples': 50000, 'test/accuracy': 0.5982000231742859, 'test/loss': 1.8039989471435547, 'test/num_examples': 10000, 'score': 47489.82584095001, 'total_duration': 49327.57805490494, 'accumulated_submission_time': 47489.82584095001, 'accumulated_eval_time': 1827.4179458618164, 'accumulated_logging_time': 5.522055149078369}
I0131 10:31:38.991342 139907284596480 logging_writer.py:48] [140737] accumulated_eval_time=1827.417946, accumulated_logging_time=5.522055, accumulated_submission_time=47489.825841, global_step=140737, preemption_count=0, score=47489.825841, test/accuracy=0.598200, test/loss=1.803999, test/num_examples=10000, total_duration=49327.578055, train/accuracy=0.813417, train/loss=0.810588, validation/accuracy=0.730600, validation/loss=1.175118, validation/num_examples=50000
I0131 10:32:00.525670 139907720771328 logging_writer.py:48] [140800] global_step=140800, grad_norm=5.5068230628967285, loss=2.0973434448242188
I0131 10:32:34.202916 139907284596480 logging_writer.py:48] [140900] global_step=140900, grad_norm=5.7490363121032715, loss=2.15940523147583
I0131 10:33:07.903412 139907720771328 logging_writer.py:48] [141000] global_step=141000, grad_norm=5.742147922515869, loss=2.1771321296691895
I0131 10:33:41.695022 139907284596480 logging_writer.py:48] [141100] global_step=141100, grad_norm=5.762780666351318, loss=2.1060431003570557
I0131 10:34:15.395187 139907720771328 logging_writer.py:48] [141200] global_step=141200, grad_norm=5.365157127380371, loss=2.117886543273926
I0131 10:34:49.060956 139907284596480 logging_writer.py:48] [141300] global_step=141300, grad_norm=5.848535060882568, loss=2.1795761585235596
I0131 10:35:22.744643 139907720771328 logging_writer.py:48] [141400] global_step=141400, grad_norm=6.025889873504639, loss=2.1079909801483154
I0131 10:35:56.417359 139907284596480 logging_writer.py:48] [141500] global_step=141500, grad_norm=5.408444404602051, loss=2.176896572113037
I0131 10:36:30.119047 139907720771328 logging_writer.py:48] [141600] global_step=141600, grad_norm=6.444318771362305, loss=2.1985294818878174
I0131 10:37:03.776063 139907284596480 logging_writer.py:48] [141700] global_step=141700, grad_norm=5.663200855255127, loss=2.1598992347717285
I0131 10:37:37.466258 139907720771328 logging_writer.py:48] [141800] global_step=141800, grad_norm=6.169013977050781, loss=2.148252487182617
I0131 10:38:11.132822 139907284596480 logging_writer.py:48] [141900] global_step=141900, grad_norm=6.093029975891113, loss=2.081728935241699
I0131 10:38:44.823207 139907720771328 logging_writer.py:48] [142000] global_step=142000, grad_norm=5.600717544555664, loss=2.145024299621582
I0131 10:39:18.486279 139907284596480 logging_writer.py:48] [142100] global_step=142100, grad_norm=5.594042778015137, loss=2.174180746078491
I0131 10:39:52.280976 139907720771328 logging_writer.py:48] [142200] global_step=142200, grad_norm=5.760766983032227, loss=2.1086623668670654
I0131 10:40:09.266693 140070692116288 spec.py:321] Evaluating on the training split.
I0131 10:40:15.300601 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 10:40:24.066074 140070692116288 spec.py:349] Evaluating on the test split.
I0131 10:40:26.328733 140070692116288 submission_runner.py:408] Time since start: 49854.96s, 	Step: 142252, 	{'train/accuracy': 0.8249959945678711, 'train/loss': 0.7559878826141357, 'validation/accuracy': 0.726419985294342, 'validation/loss': 1.1791678667068481, 'validation/num_examples': 50000, 'test/accuracy': 0.605400025844574, 'test/loss': 1.8135912418365479, 'test/num_examples': 10000, 'score': 48000.03847670555, 'total_duration': 49854.95561385155, 'accumulated_submission_time': 48000.03847670555, 'accumulated_eval_time': 1844.4799404144287, 'accumulated_logging_time': 5.573156356811523}
I0131 10:40:26.373260 139907267811072 logging_writer.py:48] [142252] accumulated_eval_time=1844.479940, accumulated_logging_time=5.573156, accumulated_submission_time=48000.038477, global_step=142252, preemption_count=0, score=48000.038477, test/accuracy=0.605400, test/loss=1.813591, test/num_examples=10000, total_duration=49854.955614, train/accuracy=0.824996, train/loss=0.755988, validation/accuracy=0.726420, validation/loss=1.179168, validation/num_examples=50000
I0131 10:40:42.890334 139907276203776 logging_writer.py:48] [142300] global_step=142300, grad_norm=5.773534774780273, loss=2.133786916732788
I0131 10:41:16.560367 139907267811072 logging_writer.py:48] [142400] global_step=142400, grad_norm=6.203272342681885, loss=2.1816766262054443
I0131 10:41:50.244466 139907276203776 logging_writer.py:48] [142500] global_step=142500, grad_norm=5.788555145263672, loss=2.1819076538085938
I0131 10:42:23.935133 139907267811072 logging_writer.py:48] [142600] global_step=142600, grad_norm=6.264254093170166, loss=2.116525650024414
I0131 10:42:57.606506 139907276203776 logging_writer.py:48] [142700] global_step=142700, grad_norm=5.1193342208862305, loss=2.079852342605591
I0131 10:43:31.332654 139907267811072 logging_writer.py:48] [142800] global_step=142800, grad_norm=5.991487503051758, loss=2.1867048740386963
I0131 10:44:05.016638 139907276203776 logging_writer.py:48] [142900] global_step=142900, grad_norm=6.138715744018555, loss=2.1503491401672363
I0131 10:44:38.674483 139907267811072 logging_writer.py:48] [143000] global_step=143000, grad_norm=5.810458660125732, loss=2.165431261062622
I0131 10:45:12.416226 139907276203776 logging_writer.py:48] [143100] global_step=143100, grad_norm=6.959441661834717, loss=2.1682815551757812
I0131 10:45:46.211888 139907267811072 logging_writer.py:48] [143200] global_step=143200, grad_norm=5.952532768249512, loss=2.0907039642333984
I0131 10:46:19.909938 139907276203776 logging_writer.py:48] [143300] global_step=143300, grad_norm=6.2234063148498535, loss=2.214939594268799
I0131 10:46:53.577817 139907267811072 logging_writer.py:48] [143400] global_step=143400, grad_norm=5.893813610076904, loss=2.1634342670440674
I0131 10:47:27.268531 139907276203776 logging_writer.py:48] [143500] global_step=143500, grad_norm=5.91062068939209, loss=2.077317714691162
I0131 10:48:00.934293 139907267811072 logging_writer.py:48] [143600] global_step=143600, grad_norm=6.139142036437988, loss=2.109990358352661
I0131 10:48:34.621778 139907276203776 logging_writer.py:48] [143700] global_step=143700, grad_norm=5.969907283782959, loss=2.042271375656128
I0131 10:48:56.351991 140070692116288 spec.py:321] Evaluating on the training split.
I0131 10:49:02.728922 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 10:49:11.506866 140070692116288 spec.py:349] Evaluating on the test split.
I0131 10:49:13.794151 140070692116288 submission_runner.py:408] Time since start: 50382.42s, 	Step: 143766, 	{'train/accuracy': 0.8384287357330322, 'train/loss': 0.7113813757896423, 'validation/accuracy': 0.7309799790382385, 'validation/loss': 1.161076545715332, 'validation/num_examples': 50000, 'test/accuracy': 0.6038000583648682, 'test/loss': 1.7907795906066895, 'test/num_examples': 10000, 'score': 48509.954323768616, 'total_duration': 50382.42102813721, 'accumulated_submission_time': 48509.954323768616, 'accumulated_eval_time': 1861.9220707416534, 'accumulated_logging_time': 5.628809690475464}
I0131 10:49:13.835609 139907259418368 logging_writer.py:48] [143766] accumulated_eval_time=1861.922071, accumulated_logging_time=5.628810, accumulated_submission_time=48509.954324, global_step=143766, preemption_count=0, score=48509.954324, test/accuracy=0.603800, test/loss=1.790780, test/num_examples=10000, total_duration=50382.421028, train/accuracy=0.838429, train/loss=0.711381, validation/accuracy=0.730980, validation/loss=1.161077, validation/num_examples=50000
I0131 10:49:25.636661 139907267811072 logging_writer.py:48] [143800] global_step=143800, grad_norm=6.740297317504883, loss=2.2129440307617188
I0131 10:49:59.292957 139907259418368 logging_writer.py:48] [143900] global_step=143900, grad_norm=5.882300853729248, loss=2.1041510105133057
I0131 10:50:32.958255 139907267811072 logging_writer.py:48] [144000] global_step=144000, grad_norm=5.72696590423584, loss=2.1376872062683105
I0131 10:51:06.639270 139907259418368 logging_writer.py:48] [144100] global_step=144100, grad_norm=5.7321062088012695, loss=2.1323354244232178
I0131 10:51:40.349518 139907267811072 logging_writer.py:48] [144200] global_step=144200, grad_norm=5.798093318939209, loss=2.106938600540161
I0131 10:52:14.064515 139907259418368 logging_writer.py:48] [144300] global_step=144300, grad_norm=6.2294840812683105, loss=2.071094274520874
I0131 10:52:47.753389 139907267811072 logging_writer.py:48] [144400] global_step=144400, grad_norm=6.024167060852051, loss=2.122182846069336
I0131 10:53:21.460034 139907259418368 logging_writer.py:48] [144500] global_step=144500, grad_norm=5.622560977935791, loss=2.0429723262786865
I0131 10:53:55.123881 139907267811072 logging_writer.py:48] [144600] global_step=144600, grad_norm=6.444572925567627, loss=2.0992438793182373
I0131 10:54:28.832450 139907259418368 logging_writer.py:48] [144700] global_step=144700, grad_norm=5.71336030960083, loss=2.094747543334961
I0131 10:55:02.501476 139907267811072 logging_writer.py:48] [144800] global_step=144800, grad_norm=6.113256454467773, loss=2.057621717453003
I0131 10:55:36.199090 139907259418368 logging_writer.py:48] [144900] global_step=144900, grad_norm=5.876196384429932, loss=2.0241546630859375
I0131 10:56:09.859469 139907267811072 logging_writer.py:48] [145000] global_step=145000, grad_norm=6.179054260253906, loss=2.10127592086792
I0131 10:56:43.514875 139907259418368 logging_writer.py:48] [145100] global_step=145100, grad_norm=5.878890037536621, loss=2.0785460472106934
I0131 10:57:17.178516 139907267811072 logging_writer.py:48] [145200] global_step=145200, grad_norm=5.801023960113525, loss=2.156308889389038
I0131 10:57:44.003901 140070692116288 spec.py:321] Evaluating on the training split.
I0131 10:57:50.085441 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 10:57:58.901555 140070692116288 spec.py:349] Evaluating on the test split.
I0131 10:58:01.149994 140070692116288 submission_runner.py:408] Time since start: 50909.78s, 	Step: 145281, 	{'train/accuracy': 0.8381098508834839, 'train/loss': 0.7158500552177429, 'validation/accuracy': 0.7337799668312073, 'validation/loss': 1.148126482963562, 'validation/num_examples': 50000, 'test/accuracy': 0.6077000498771667, 'test/loss': 1.781200885772705, 'test/num_examples': 10000, 'score': 49020.05911588669, 'total_duration': 50909.776867866516, 'accumulated_submission_time': 49020.05911588669, 'accumulated_eval_time': 1879.0681087970734, 'accumulated_logging_time': 5.681424856185913}
I0131 10:58:01.200632 139907267811072 logging_writer.py:48] [145281] accumulated_eval_time=1879.068109, accumulated_logging_time=5.681425, accumulated_submission_time=49020.059116, global_step=145281, preemption_count=0, score=49020.059116, test/accuracy=0.607700, test/loss=1.781201, test/num_examples=10000, total_duration=50909.776868, train/accuracy=0.838110, train/loss=0.715850, validation/accuracy=0.733780, validation/loss=1.148126, validation/num_examples=50000
I0131 10:58:07.967016 139907276203776 logging_writer.py:48] [145300] global_step=145300, grad_norm=6.336470603942871, loss=2.123058795928955
I0131 10:58:41.607798 139907267811072 logging_writer.py:48] [145400] global_step=145400, grad_norm=5.853157997131348, loss=2.028635025024414
I0131 10:59:15.268339 139907276203776 logging_writer.py:48] [145500] global_step=145500, grad_norm=5.752536296844482, loss=2.1156492233276367
I0131 10:59:48.977805 139907267811072 logging_writer.py:48] [145600] global_step=145600, grad_norm=6.0949249267578125, loss=2.1116316318511963
I0131 11:00:22.624430 139907276203776 logging_writer.py:48] [145700] global_step=145700, grad_norm=5.949186325073242, loss=2.1238043308258057
I0131 11:00:56.314833 139907267811072 logging_writer.py:48] [145800] global_step=145800, grad_norm=6.09376859664917, loss=2.0735867023468018
I0131 11:01:29.964004 139907276203776 logging_writer.py:48] [145900] global_step=145900, grad_norm=5.717585563659668, loss=2.0808656215667725
I0131 11:02:03.740670 139907267811072 logging_writer.py:48] [146000] global_step=146000, grad_norm=5.865076065063477, loss=2.1828291416168213
I0131 11:02:37.408641 139907276203776 logging_writer.py:48] [146100] global_step=146100, grad_norm=6.251688480377197, loss=2.1501646041870117
I0131 11:03:11.120420 139907267811072 logging_writer.py:48] [146200] global_step=146200, grad_norm=6.285933017730713, loss=2.149106979370117
I0131 11:03:44.765542 139907276203776 logging_writer.py:48] [146300] global_step=146300, grad_norm=5.893344402313232, loss=2.0970611572265625
I0131 11:04:18.577638 139907267811072 logging_writer.py:48] [146400] global_step=146400, grad_norm=5.756746768951416, loss=2.05804443359375
I0131 11:04:52.251709 139907276203776 logging_writer.py:48] [146500] global_step=146500, grad_norm=6.35446310043335, loss=2.151637315750122
I0131 11:05:25.956000 139907267811072 logging_writer.py:48] [146600] global_step=146600, grad_norm=6.162233352661133, loss=2.0309603214263916
I0131 11:05:59.610210 139907276203776 logging_writer.py:48] [146700] global_step=146700, grad_norm=6.2638421058654785, loss=2.0809085369110107
I0131 11:06:31.438843 140070692116288 spec.py:321] Evaluating on the training split.
I0131 11:06:37.607488 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 11:06:46.236521 140070692116288 spec.py:349] Evaluating on the test split.
I0131 11:06:48.560845 140070692116288 submission_runner.py:408] Time since start: 51437.19s, 	Step: 146796, 	{'train/accuracy': 0.8376116156578064, 'train/loss': 0.7040087580680847, 'validation/accuracy': 0.7360000014305115, 'validation/loss': 1.1290984153747559, 'validation/num_examples': 50000, 'test/accuracy': 0.6104000210762024, 'test/loss': 1.7563949823379517, 'test/num_examples': 10000, 'score': 49530.228246212006, 'total_duration': 51437.18770766258, 'accumulated_submission_time': 49530.228246212006, 'accumulated_eval_time': 1896.1900537014008, 'accumulated_logging_time': 5.749187469482422}
I0131 11:06:48.600011 139907720771328 logging_writer.py:48] [146796] accumulated_eval_time=1896.190054, accumulated_logging_time=5.749187, accumulated_submission_time=49530.228246, global_step=146796, preemption_count=0, score=49530.228246, test/accuracy=0.610400, test/loss=1.756395, test/num_examples=10000, total_duration=51437.187708, train/accuracy=0.837612, train/loss=0.704009, validation/accuracy=0.736000, validation/loss=1.129098, validation/num_examples=50000
I0131 11:06:50.292992 139907754342144 logging_writer.py:48] [146800] global_step=146800, grad_norm=6.140422344207764, loss=2.157780170440674
I0131 11:07:23.926124 139907720771328 logging_writer.py:48] [146900] global_step=146900, grad_norm=6.86508321762085, loss=2.1632559299468994
I0131 11:07:57.629804 139907754342144 logging_writer.py:48] [147000] global_step=147000, grad_norm=5.826188087463379, loss=1.9791641235351562
I0131 11:08:31.303386 139907720771328 logging_writer.py:48] [147100] global_step=147100, grad_norm=6.191384792327881, loss=2.0801944732666016
I0131 11:09:04.978845 139907754342144 logging_writer.py:48] [147200] global_step=147200, grad_norm=6.470067977905273, loss=2.137915849685669
I0131 11:09:38.647274 139907720771328 logging_writer.py:48] [147300] global_step=147300, grad_norm=5.947062015533447, loss=2.0693721771240234
I0131 11:10:12.398463 139907754342144 logging_writer.py:48] [147400] global_step=147400, grad_norm=6.7895426750183105, loss=2.1099298000335693
I0131 11:10:46.102892 139907720771328 logging_writer.py:48] [147500] global_step=147500, grad_norm=6.272812843322754, loss=2.153092384338379
I0131 11:11:19.773575 139907754342144 logging_writer.py:48] [147600] global_step=147600, grad_norm=6.747607231140137, loss=2.1189231872558594
I0131 11:11:53.432203 139907720771328 logging_writer.py:48] [147700] global_step=147700, grad_norm=6.373326778411865, loss=2.0861809253692627
I0131 11:12:27.107295 139907754342144 logging_writer.py:48] [147800] global_step=147800, grad_norm=6.3134355545043945, loss=2.0470964908599854
I0131 11:13:00.752067 139907720771328 logging_writer.py:48] [147900] global_step=147900, grad_norm=6.846934795379639, loss=2.0808775424957275
I0131 11:13:34.454573 139907754342144 logging_writer.py:48] [148000] global_step=148000, grad_norm=7.389174938201904, loss=2.1806933879852295
I0131 11:14:08.120805 139907720771328 logging_writer.py:48] [148100] global_step=148100, grad_norm=6.296166896820068, loss=2.0959360599517822
I0131 11:14:41.800943 139907754342144 logging_writer.py:48] [148200] global_step=148200, grad_norm=6.72861909866333, loss=2.055448055267334
I0131 11:15:15.466527 139907720771328 logging_writer.py:48] [148300] global_step=148300, grad_norm=6.095499515533447, loss=2.070157051086426
I0131 11:15:18.647489 140070692116288 spec.py:321] Evaluating on the training split.
I0131 11:15:24.817283 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 11:15:33.461287 140070692116288 spec.py:349] Evaluating on the test split.
I0131 11:15:35.748461 140070692116288 submission_runner.py:408] Time since start: 51964.38s, 	Step: 148311, 	{'train/accuracy': 0.8396643400192261, 'train/loss': 0.7063088417053223, 'validation/accuracy': 0.738319993019104, 'validation/loss': 1.1283575296401978, 'validation/num_examples': 50000, 'test/accuracy': 0.6163000464439392, 'test/loss': 1.7626051902770996, 'test/num_examples': 10000, 'score': 50040.213027477264, 'total_duration': 51964.37534117699, 'accumulated_submission_time': 50040.213027477264, 'accumulated_eval_time': 1913.290997505188, 'accumulated_logging_time': 5.799168109893799}
I0131 11:15:35.790817 139907276203776 logging_writer.py:48] [148311] accumulated_eval_time=1913.290998, accumulated_logging_time=5.799168, accumulated_submission_time=50040.213027, global_step=148311, preemption_count=0, score=50040.213027, test/accuracy=0.616300, test/loss=1.762605, test/num_examples=10000, total_duration=51964.375341, train/accuracy=0.839664, train/loss=0.706309, validation/accuracy=0.738320, validation/loss=1.128358, validation/num_examples=50000
I0131 11:16:06.084397 139907284596480 logging_writer.py:48] [148400] global_step=148400, grad_norm=6.6508002281188965, loss=2.126136064529419
I0131 11:16:39.791044 139907276203776 logging_writer.py:48] [148500] global_step=148500, grad_norm=6.494280815124512, loss=2.044142723083496
I0131 11:17:13.527464 139907284596480 logging_writer.py:48] [148600] global_step=148600, grad_norm=6.001627445220947, loss=2.077883720397949
I0131 11:17:47.208590 139907276203776 logging_writer.py:48] [148700] global_step=148700, grad_norm=6.21083927154541, loss=2.0195798873901367
I0131 11:18:20.906174 139907284596480 logging_writer.py:48] [148800] global_step=148800, grad_norm=6.501619338989258, loss=2.07802414894104
I0131 11:18:54.620024 139907276203776 logging_writer.py:48] [148900] global_step=148900, grad_norm=5.909759044647217, loss=2.067573070526123
I0131 11:19:28.289047 139907284596480 logging_writer.py:48] [149000] global_step=149000, grad_norm=6.606557846069336, loss=2.063197612762451
I0131 11:20:02.030219 139907276203776 logging_writer.py:48] [149100] global_step=149100, grad_norm=5.835898399353027, loss=2.076479911804199
I0131 11:20:35.697400 139907284596480 logging_writer.py:48] [149200] global_step=149200, grad_norm=6.1555657386779785, loss=2.0922744274139404
I0131 11:21:09.391435 139907276203776 logging_writer.py:48] [149300] global_step=149300, grad_norm=6.657260417938232, loss=2.0931649208068848
I0131 11:21:43.064620 139907284596480 logging_writer.py:48] [149400] global_step=149400, grad_norm=6.425254821777344, loss=2.0474085807800293
I0131 11:22:16.888015 139907276203776 logging_writer.py:48] [149500] global_step=149500, grad_norm=6.6759934425354, loss=2.0529096126556396
I0131 11:22:50.599293 139907284596480 logging_writer.py:48] [149600] global_step=149600, grad_norm=5.9482035636901855, loss=2.0024192333221436
I0131 11:23:24.318550 139907276203776 logging_writer.py:48] [149700] global_step=149700, grad_norm=5.882420539855957, loss=2.0291545391082764
I0131 11:23:57.980506 139907284596480 logging_writer.py:48] [149800] global_step=149800, grad_norm=6.644054889678955, loss=2.1105360984802246
I0131 11:24:05.882456 140070692116288 spec.py:321] Evaluating on the training split.
I0131 11:24:12.074350 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 11:24:20.984828 140070692116288 spec.py:349] Evaluating on the test split.
I0131 11:24:23.353560 140070692116288 submission_runner.py:408] Time since start: 52491.98s, 	Step: 149825, 	{'train/accuracy': 0.8420161008834839, 'train/loss': 0.6852911710739136, 'validation/accuracy': 0.7417799830436707, 'validation/loss': 1.1163774728775024, 'validation/num_examples': 50000, 'test/accuracy': 0.6243000030517578, 'test/loss': 1.719955325126648, 'test/num_examples': 10000, 'score': 50550.241391181946, 'total_duration': 52491.98043131828, 'accumulated_submission_time': 50550.241391181946, 'accumulated_eval_time': 1930.7620613574982, 'accumulated_logging_time': 5.853203773498535}
I0131 11:24:23.395437 139907267811072 logging_writer.py:48] [149825] accumulated_eval_time=1930.762061, accumulated_logging_time=5.853204, accumulated_submission_time=50550.241391, global_step=149825, preemption_count=0, score=50550.241391, test/accuracy=0.624300, test/loss=1.719955, test/num_examples=10000, total_duration=52491.980431, train/accuracy=0.842016, train/loss=0.685291, validation/accuracy=0.741780, validation/loss=1.116377, validation/num_examples=50000
I0131 11:24:48.996918 139907276203776 logging_writer.py:48] [149900] global_step=149900, grad_norm=6.161384582519531, loss=2.0654513835906982
I0131 11:25:22.695105 139907267811072 logging_writer.py:48] [150000] global_step=150000, grad_norm=6.171738624572754, loss=2.0241470336914062
I0131 11:25:56.385636 139907276203776 logging_writer.py:48] [150100] global_step=150100, grad_norm=6.377383232116699, loss=1.9753234386444092
I0131 11:26:30.058108 139907267811072 logging_writer.py:48] [150200] global_step=150200, grad_norm=6.438115119934082, loss=2.120232582092285
I0131 11:27:03.726432 139907276203776 logging_writer.py:48] [150300] global_step=150300, grad_norm=7.33420467376709, loss=2.076122522354126
I0131 11:27:37.420896 139907267811072 logging_writer.py:48] [150400] global_step=150400, grad_norm=6.5791120529174805, loss=2.0223751068115234
I0131 11:28:11.083186 139907276203776 logging_writer.py:48] [150500] global_step=150500, grad_norm=6.116204738616943, loss=2.0954883098602295
I0131 11:28:44.876063 139907267811072 logging_writer.py:48] [150600] global_step=150600, grad_norm=6.468465805053711, loss=2.0300846099853516
I0131 11:29:18.579689 139907276203776 logging_writer.py:48] [150700] global_step=150700, grad_norm=6.460126876831055, loss=2.1384174823760986
I0131 11:29:52.252171 139907267811072 logging_writer.py:48] [150800] global_step=150800, grad_norm=6.841116905212402, loss=2.041598081588745
I0131 11:30:25.922982 139907276203776 logging_writer.py:48] [150900] global_step=150900, grad_norm=5.938323974609375, loss=2.0492100715637207
I0131 11:30:59.579116 139907267811072 logging_writer.py:48] [151000] global_step=151000, grad_norm=6.472123622894287, loss=2.0083746910095215
I0131 11:31:33.303405 139907276203776 logging_writer.py:48] [151100] global_step=151100, grad_norm=6.001790523529053, loss=2.02907395362854
I0131 11:32:06.987229 139907267811072 logging_writer.py:48] [151200] global_step=151200, grad_norm=6.289495468139648, loss=2.07322359085083
I0131 11:32:40.675226 139907276203776 logging_writer.py:48] [151300] global_step=151300, grad_norm=6.240553855895996, loss=2.095229148864746
I0131 11:32:53.624488 140070692116288 spec.py:321] Evaluating on the training split.
I0131 11:32:59.768686 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 11:33:08.884854 140070692116288 spec.py:349] Evaluating on the test split.
I0131 11:33:11.170911 140070692116288 submission_runner.py:408] Time since start: 53019.80s, 	Step: 151340, 	{'train/accuracy': 0.86820387840271, 'train/loss': 0.5952945947647095, 'validation/accuracy': 0.7403199672698975, 'validation/loss': 1.1137624979019165, 'validation/num_examples': 50000, 'test/accuracy': 0.6186000108718872, 'test/loss': 1.740875482559204, 'test/num_examples': 10000, 'score': 51060.40700316429, 'total_duration': 53019.797793626785, 'accumulated_submission_time': 51060.40700316429, 'accumulated_eval_time': 1948.308458328247, 'accumulated_logging_time': 5.906131267547607}
I0131 11:33:11.212787 139907720771328 logging_writer.py:48] [151340] accumulated_eval_time=1948.308458, accumulated_logging_time=5.906131, accumulated_submission_time=51060.407003, global_step=151340, preemption_count=0, score=51060.407003, test/accuracy=0.618600, test/loss=1.740875, test/num_examples=10000, total_duration=53019.797794, train/accuracy=0.868204, train/loss=0.595295, validation/accuracy=0.740320, validation/loss=1.113762, validation/num_examples=50000
I0131 11:33:31.783725 139907729164032 logging_writer.py:48] [151400] global_step=151400, grad_norm=7.1593170166015625, loss=1.998348593711853
I0131 11:34:05.435743 139907720771328 logging_writer.py:48] [151500] global_step=151500, grad_norm=5.9313530921936035, loss=2.075829029083252
I0131 11:34:39.172341 139907729164032 logging_writer.py:48] [151600] global_step=151600, grad_norm=6.688138961791992, loss=2.098316192626953
I0131 11:35:12.894531 139907720771328 logging_writer.py:48] [151700] global_step=151700, grad_norm=6.489999294281006, loss=2.0013811588287354
I0131 11:35:46.595285 139907729164032 logging_writer.py:48] [151800] global_step=151800, grad_norm=7.0378522872924805, loss=2.079468250274658
I0131 11:36:20.268572 139907720771328 logging_writer.py:48] [151900] global_step=151900, grad_norm=5.823050498962402, loss=1.986849308013916
I0131 11:36:53.972396 139907729164032 logging_writer.py:48] [152000] global_step=152000, grad_norm=6.27639102935791, loss=2.0513813495635986
I0131 11:37:27.632526 139907720771328 logging_writer.py:48] [152100] global_step=152100, grad_norm=7.253890514373779, loss=1.991246223449707
I0131 11:38:01.305846 139907729164032 logging_writer.py:48] [152200] global_step=152200, grad_norm=6.018932342529297, loss=2.0113582611083984
I0131 11:38:34.961861 139907720771328 logging_writer.py:48] [152300] global_step=152300, grad_norm=6.573118209838867, loss=1.9601930379867554
I0131 11:39:08.670971 139907729164032 logging_writer.py:48] [152400] global_step=152400, grad_norm=6.308222770690918, loss=2.1068220138549805
I0131 11:39:42.344100 139907720771328 logging_writer.py:48] [152500] global_step=152500, grad_norm=6.349735260009766, loss=1.9651269912719727
I0131 11:40:16.037141 139907729164032 logging_writer.py:48] [152600] global_step=152600, grad_norm=6.935632228851318, loss=2.036961555480957
I0131 11:40:49.817483 139907720771328 logging_writer.py:48] [152700] global_step=152700, grad_norm=6.413326740264893, loss=2.0037484169006348
I0131 11:41:23.516454 139907729164032 logging_writer.py:48] [152800] global_step=152800, grad_norm=6.696064472198486, loss=2.056220769882202
I0131 11:41:41.506783 140070692116288 spec.py:321] Evaluating on the training split.
I0131 11:41:48.273629 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 11:41:57.116466 140070692116288 spec.py:349] Evaluating on the test split.
I0131 11:41:59.435980 140070692116288 submission_runner.py:408] Time since start: 53548.06s, 	Step: 152855, 	{'train/accuracy': 0.8589365482330322, 'train/loss': 0.613325297832489, 'validation/accuracy': 0.7432799935340881, 'validation/loss': 1.1003096103668213, 'validation/num_examples': 50000, 'test/accuracy': 0.62090003490448, 'test/loss': 1.735670804977417, 'test/num_examples': 10000, 'score': 51570.638830661774, 'total_duration': 53548.062861442566, 'accumulated_submission_time': 51570.638830661774, 'accumulated_eval_time': 1966.237622976303, 'accumulated_logging_time': 5.958402872085571}
I0131 11:41:59.477753 139907259418368 logging_writer.py:48] [152855] accumulated_eval_time=1966.237623, accumulated_logging_time=5.958403, accumulated_submission_time=51570.638831, global_step=152855, preemption_count=0, score=51570.638831, test/accuracy=0.620900, test/loss=1.735671, test/num_examples=10000, total_duration=53548.062861, train/accuracy=0.858937, train/loss=0.613325, validation/accuracy=0.743280, validation/loss=1.100310, validation/num_examples=50000
I0131 11:42:14.967938 139907267811072 logging_writer.py:48] [152900] global_step=152900, grad_norm=6.239970684051514, loss=2.074113130569458
I0131 11:42:48.607406 139907259418368 logging_writer.py:48] [153000] global_step=153000, grad_norm=7.93061637878418, loss=1.9885469675064087
I0131 11:43:22.290526 139907267811072 logging_writer.py:48] [153100] global_step=153100, grad_norm=6.462928295135498, loss=1.9801908731460571
I0131 11:43:55.962004 139907259418368 logging_writer.py:48] [153200] global_step=153200, grad_norm=6.379546165466309, loss=2.0664875507354736
I0131 11:44:29.644248 139907267811072 logging_writer.py:48] [153300] global_step=153300, grad_norm=7.233450889587402, loss=2.000086784362793
I0131 11:45:03.316569 139907259418368 logging_writer.py:48] [153400] global_step=153400, grad_norm=6.656703472137451, loss=2.051193952560425
I0131 11:45:36.995179 139907267811072 logging_writer.py:48] [153500] global_step=153500, grad_norm=6.617837429046631, loss=2.0157580375671387
I0131 11:46:10.654132 139907259418368 logging_writer.py:48] [153600] global_step=153600, grad_norm=6.852065563201904, loss=2.0823311805725098
I0131 11:46:44.329184 139907267811072 logging_writer.py:48] [153700] global_step=153700, grad_norm=6.544041156768799, loss=2.0398032665252686
I0131 11:47:18.109070 139907259418368 logging_writer.py:48] [153800] global_step=153800, grad_norm=6.494879245758057, loss=2.0005030632019043
I0131 11:47:51.797059 139907267811072 logging_writer.py:48] [153900] global_step=153900, grad_norm=6.519161224365234, loss=2.0061140060424805
I0131 11:48:25.478676 139907259418368 logging_writer.py:48] [154000] global_step=154000, grad_norm=6.9371724128723145, loss=2.080949306488037
I0131 11:48:59.136367 139907267811072 logging_writer.py:48] [154100] global_step=154100, grad_norm=6.7668776512146, loss=2.045274257659912
I0131 11:49:32.793098 139907259418368 logging_writer.py:48] [154200] global_step=154200, grad_norm=7.095303058624268, loss=2.0699329376220703
I0131 11:50:06.490065 139907267811072 logging_writer.py:48] [154300] global_step=154300, grad_norm=6.700210094451904, loss=1.984238862991333
I0131 11:50:29.537071 140070692116288 spec.py:321] Evaluating on the training split.
I0131 11:50:35.714196 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 11:50:44.558778 140070692116288 spec.py:349] Evaluating on the test split.
I0131 11:50:46.847826 140070692116288 submission_runner.py:408] Time since start: 54075.47s, 	Step: 154370, 	{'train/accuracy': 0.8587571382522583, 'train/loss': 0.6176808476448059, 'validation/accuracy': 0.7478799819946289, 'validation/loss': 1.0903000831604004, 'validation/num_examples': 50000, 'test/accuracy': 0.6241000294685364, 'test/loss': 1.7056432962417603, 'test/num_examples': 10000, 'score': 52080.63415670395, 'total_duration': 54075.474705934525, 'accumulated_submission_time': 52080.63415670395, 'accumulated_eval_time': 1983.548333644867, 'accumulated_logging_time': 6.011291027069092}
I0131 11:50:46.891061 139907720771328 logging_writer.py:48] [154370] accumulated_eval_time=1983.548334, accumulated_logging_time=6.011291, accumulated_submission_time=52080.634157, global_step=154370, preemption_count=0, score=52080.634157, test/accuracy=0.624100, test/loss=1.705643, test/num_examples=10000, total_duration=54075.474706, train/accuracy=0.858757, train/loss=0.617681, validation/accuracy=0.747880, validation/loss=1.090300, validation/num_examples=50000
I0131 11:50:57.337305 139907729164032 logging_writer.py:48] [154400] global_step=154400, grad_norm=6.640777111053467, loss=2.0118300914764404
I0131 11:51:30.930673 139907720771328 logging_writer.py:48] [154500] global_step=154500, grad_norm=7.143282413482666, loss=1.9842802286148071
I0131 11:52:04.636675 139907729164032 logging_writer.py:48] [154600] global_step=154600, grad_norm=6.635598182678223, loss=1.946148157119751
I0131 11:52:38.329168 139907720771328 logging_writer.py:48] [154700] global_step=154700, grad_norm=6.858837127685547, loss=2.0111329555511475
I0131 11:53:12.157993 139907729164032 logging_writer.py:48] [154800] global_step=154800, grad_norm=6.688615798950195, loss=1.9831522703170776
I0131 11:53:45.849222 139907720771328 logging_writer.py:48] [154900] global_step=154900, grad_norm=6.21037483215332, loss=1.9149471521377563
I0131 11:54:19.498762 139907729164032 logging_writer.py:48] [155000] global_step=155000, grad_norm=6.978611469268799, loss=1.9920496940612793
I0131 11:54:53.185419 139907720771328 logging_writer.py:48] [155100] global_step=155100, grad_norm=6.858829498291016, loss=2.0477828979492188
I0131 11:55:26.833138 139907729164032 logging_writer.py:48] [155200] global_step=155200, grad_norm=6.317277908325195, loss=1.8917648792266846
I0131 11:56:00.543044 139907720771328 logging_writer.py:48] [155300] global_step=155300, grad_norm=6.850473403930664, loss=2.0168800354003906
I0131 11:56:34.190290 139907729164032 logging_writer.py:48] [155400] global_step=155400, grad_norm=6.765532970428467, loss=1.9797091484069824
I0131 11:57:07.891046 139907720771328 logging_writer.py:48] [155500] global_step=155500, grad_norm=6.762876033782959, loss=2.0183117389678955
I0131 11:57:41.546778 139907729164032 logging_writer.py:48] [155600] global_step=155600, grad_norm=7.157049179077148, loss=2.039011001586914
I0131 11:58:15.236667 139907720771328 logging_writer.py:48] [155700] global_step=155700, grad_norm=6.482504844665527, loss=2.0019893646240234
I0131 11:58:48.876219 139907729164032 logging_writer.py:48] [155800] global_step=155800, grad_norm=6.532858848571777, loss=1.977501392364502
I0131 11:59:17.132596 140070692116288 spec.py:321] Evaluating on the training split.
I0131 11:59:23.249619 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 11:59:32.222032 140070692116288 spec.py:349] Evaluating on the test split.
I0131 11:59:34.526716 140070692116288 submission_runner.py:408] Time since start: 54603.15s, 	Step: 155885, 	{'train/accuracy': 0.8608896732330322, 'train/loss': 0.6201562881469727, 'validation/accuracy': 0.75, 'validation/loss': 1.0886688232421875, 'validation/num_examples': 50000, 'test/accuracy': 0.6253000497817993, 'test/loss': 1.7142834663391113, 'test/num_examples': 10000, 'score': 52590.81240940094, 'total_duration': 54603.15359258652, 'accumulated_submission_time': 52590.81240940094, 'accumulated_eval_time': 2000.9424073696136, 'accumulated_logging_time': 6.064687252044678}
I0131 11:59:34.570174 139907259418368 logging_writer.py:48] [155885] accumulated_eval_time=2000.942407, accumulated_logging_time=6.064687, accumulated_submission_time=52590.812409, global_step=155885, preemption_count=0, score=52590.812409, test/accuracy=0.625300, test/loss=1.714283, test/num_examples=10000, total_duration=54603.153593, train/accuracy=0.860890, train/loss=0.620156, validation/accuracy=0.750000, validation/loss=1.088669, validation/num_examples=50000
I0131 11:59:39.964605 139907267811072 logging_writer.py:48] [155900] global_step=155900, grad_norm=6.365147113800049, loss=1.9561614990234375
I0131 12:00:13.651890 139907259418368 logging_writer.py:48] [156000] global_step=156000, grad_norm=7.5324320793151855, loss=2.041248321533203
I0131 12:00:47.319986 139907267811072 logging_writer.py:48] [156100] global_step=156100, grad_norm=6.704655647277832, loss=1.967734456062317
I0131 12:01:21.016836 139907259418368 logging_writer.py:48] [156200] global_step=156200, grad_norm=6.409012317657471, loss=1.9218422174453735
I0131 12:01:54.683806 139907267811072 logging_writer.py:48] [156300] global_step=156300, grad_norm=7.145963668823242, loss=1.9686681032180786
I0131 12:02:28.402660 139907259418368 logging_writer.py:48] [156400] global_step=156400, grad_norm=6.176244735717773, loss=1.9857062101364136
I0131 12:03:02.065415 139907267811072 logging_writer.py:48] [156500] global_step=156500, grad_norm=6.776340484619141, loss=1.9113924503326416
I0131 12:03:35.758767 139907259418368 logging_writer.py:48] [156600] global_step=156600, grad_norm=7.829400539398193, loss=2.1007232666015625
I0131 12:04:09.425833 139907267811072 logging_writer.py:48] [156700] global_step=156700, grad_norm=6.799709320068359, loss=1.9863402843475342
I0131 12:04:43.099700 139907259418368 logging_writer.py:48] [156800] global_step=156800, grad_norm=6.369138240814209, loss=1.9973335266113281
I0131 12:05:16.845396 139907267811072 logging_writer.py:48] [156900] global_step=156900, grad_norm=6.597700119018555, loss=2.0250890254974365
I0131 12:05:50.587897 139907259418368 logging_writer.py:48] [157000] global_step=157000, grad_norm=6.664844512939453, loss=2.038182020187378
I0131 12:06:24.261846 139907267811072 logging_writer.py:48] [157100] global_step=157100, grad_norm=7.085445880889893, loss=2.0062787532806396
I0131 12:06:57.963968 139907259418368 logging_writer.py:48] [157200] global_step=157200, grad_norm=6.433529853820801, loss=1.9560060501098633
I0131 12:07:31.631384 139907267811072 logging_writer.py:48] [157300] global_step=157300, grad_norm=6.556373596191406, loss=1.954083800315857
I0131 12:08:04.793926 140070692116288 spec.py:321] Evaluating on the training split.
I0131 12:08:10.898175 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 12:08:19.849417 140070692116288 spec.py:349] Evaluating on the test split.
I0131 12:08:22.124233 140070692116288 submission_runner.py:408] Time since start: 55130.75s, 	Step: 157400, 	{'train/accuracy': 0.8644570708274841, 'train/loss': 0.6090182065963745, 'validation/accuracy': 0.7490599751472473, 'validation/loss': 1.0829800367355347, 'validation/num_examples': 50000, 'test/accuracy': 0.6284000277519226, 'test/loss': 1.7018013000488281, 'test/num_examples': 10000, 'score': 53100.97397065163, 'total_duration': 55130.75111031532, 'accumulated_submission_time': 53100.97397065163, 'accumulated_eval_time': 2018.2726693153381, 'accumulated_logging_time': 6.1179773807525635}
I0131 12:08:22.171832 139907712378624 logging_writer.py:48] [157400] accumulated_eval_time=2018.272669, accumulated_logging_time=6.117977, accumulated_submission_time=53100.973971, global_step=157400, preemption_count=0, score=53100.973971, test/accuracy=0.628400, test/loss=1.701801, test/num_examples=10000, total_duration=55130.751110, train/accuracy=0.864457, train/loss=0.609018, validation/accuracy=0.749060, validation/loss=1.082980, validation/num_examples=50000
I0131 12:08:22.523432 139907720771328 logging_writer.py:48] [157400] global_step=157400, grad_norm=6.4428911209106445, loss=1.9928196668624878
I0131 12:08:56.214845 139907712378624 logging_writer.py:48] [157500] global_step=157500, grad_norm=6.82219123840332, loss=2.0019497871398926
I0131 12:09:29.926977 139907720771328 logging_writer.py:48] [157600] global_step=157600, grad_norm=6.350490570068359, loss=1.9262561798095703
I0131 12:10:03.588660 139907712378624 logging_writer.py:48] [157700] global_step=157700, grad_norm=7.457332134246826, loss=1.9237428903579712
I0131 12:10:37.274690 139907720771328 logging_writer.py:48] [157800] global_step=157800, grad_norm=6.8409247398376465, loss=1.9988787174224854
I0131 12:11:10.922201 139907712378624 logging_writer.py:48] [157900] global_step=157900, grad_norm=6.681158542633057, loss=2.007781505584717
I0131 12:11:44.706147 139907720771328 logging_writer.py:48] [158000] global_step=158000, grad_norm=7.442834854125977, loss=2.0039682388305664
I0131 12:12:18.385248 139907712378624 logging_writer.py:48] [158100] global_step=158100, grad_norm=7.0676140785217285, loss=2.0674350261688232
I0131 12:12:52.074131 139907720771328 logging_writer.py:48] [158200] global_step=158200, grad_norm=6.520461559295654, loss=1.9902678728103638
I0131 12:13:25.732681 139907712378624 logging_writer.py:48] [158300] global_step=158300, grad_norm=6.939003944396973, loss=1.9149662256240845
I0131 12:13:59.414885 139907720771328 logging_writer.py:48] [158400] global_step=158400, grad_norm=6.843780040740967, loss=1.9569534063339233
I0131 12:14:33.079492 139907712378624 logging_writer.py:48] [158500] global_step=158500, grad_norm=7.240810394287109, loss=2.0789997577667236
I0131 12:15:06.825419 139907720771328 logging_writer.py:48] [158600] global_step=158600, grad_norm=7.161153316497803, loss=1.9505283832550049
I0131 12:15:40.503839 139907712378624 logging_writer.py:48] [158700] global_step=158700, grad_norm=6.814786911010742, loss=1.9699077606201172
I0131 12:16:14.240264 139907720771328 logging_writer.py:48] [158800] global_step=158800, grad_norm=7.292952537536621, loss=1.9890356063842773
I0131 12:16:47.925215 139907712378624 logging_writer.py:48] [158900] global_step=158900, grad_norm=7.199451923370361, loss=1.9571986198425293
I0131 12:16:52.450014 140070692116288 spec.py:321] Evaluating on the training split.
I0131 12:16:58.556785 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 12:17:07.551017 140070692116288 spec.py:349] Evaluating on the test split.
I0131 12:17:09.863445 140070692116288 submission_runner.py:408] Time since start: 55658.49s, 	Step: 158915, 	{'train/accuracy': 0.871113657951355, 'train/loss': 0.5796046853065491, 'validation/accuracy': 0.7538399696350098, 'validation/loss': 1.0558656454086304, 'validation/num_examples': 50000, 'test/accuracy': 0.6343000531196594, 'test/loss': 1.67049241065979, 'test/num_examples': 10000, 'score': 53611.190331459045, 'total_duration': 55658.49032831192, 'accumulated_submission_time': 53611.190331459045, 'accumulated_eval_time': 2035.686059474945, 'accumulated_logging_time': 6.175013780593872}
I0131 12:17:09.908131 139907267811072 logging_writer.py:48] [158915] accumulated_eval_time=2035.686059, accumulated_logging_time=6.175014, accumulated_submission_time=53611.190331, global_step=158915, preemption_count=0, score=53611.190331, test/accuracy=0.634300, test/loss=1.670492, test/num_examples=10000, total_duration=55658.490328, train/accuracy=0.871114, train/loss=0.579605, validation/accuracy=0.753840, validation/loss=1.055866, validation/num_examples=50000
I0131 12:17:38.911755 139907276203776 logging_writer.py:48] [159000] global_step=159000, grad_norm=7.593190670013428, loss=1.9731862545013428
I0131 12:18:12.569635 139907267811072 logging_writer.py:48] [159100] global_step=159100, grad_norm=6.398245334625244, loss=1.934629201889038
I0131 12:18:46.288050 139907276203776 logging_writer.py:48] [159200] global_step=159200, grad_norm=7.000481128692627, loss=1.9864341020584106
I0131 12:19:20.001487 139907267811072 logging_writer.py:48] [159300] global_step=159300, grad_norm=7.428183555603027, loss=1.8707088232040405
I0131 12:19:53.669930 139907276203776 logging_writer.py:48] [159400] global_step=159400, grad_norm=6.381106853485107, loss=1.915412187576294
I0131 12:20:27.368913 139907267811072 logging_writer.py:48] [159500] global_step=159500, grad_norm=7.618312835693359, loss=2.0200772285461426
I0131 12:21:01.020687 139907276203776 logging_writer.py:48] [159600] global_step=159600, grad_norm=6.692199230194092, loss=1.9655965566635132
I0131 12:21:34.786955 139907267811072 logging_writer.py:48] [159700] global_step=159700, grad_norm=7.111631393432617, loss=1.8994882106781006
I0131 12:22:08.467103 139907276203776 logging_writer.py:48] [159800] global_step=159800, grad_norm=6.906042575836182, loss=1.879654884338379
I0131 12:22:42.159011 139907267811072 logging_writer.py:48] [159900] global_step=159900, grad_norm=6.586863994598389, loss=1.9231009483337402
I0131 12:23:15.809949 139907276203776 logging_writer.py:48] [160000] global_step=160000, grad_norm=7.154613018035889, loss=1.937453269958496
I0131 12:23:49.516232 139907267811072 logging_writer.py:48] [160100] global_step=160100, grad_norm=7.178131103515625, loss=1.9574953317642212
I0131 12:24:23.231883 139907276203776 logging_writer.py:48] [160200] global_step=160200, grad_norm=7.259687423706055, loss=1.9650249481201172
I0131 12:24:56.922868 139907267811072 logging_writer.py:48] [160300] global_step=160300, grad_norm=6.660516738891602, loss=1.9253418445587158
I0131 12:25:30.601072 139907276203776 logging_writer.py:48] [160400] global_step=160400, grad_norm=7.1920485496521, loss=1.9895678758621216
I0131 12:25:40.167832 140070692116288 spec.py:321] Evaluating on the training split.
I0131 12:25:46.258264 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 12:25:55.156951 140070692116288 spec.py:349] Evaluating on the test split.
I0131 12:25:57.446904 140070692116288 submission_runner.py:408] Time since start: 56186.07s, 	Step: 160430, 	{'train/accuracy': 0.8908242583274841, 'train/loss': 0.5088411569595337, 'validation/accuracy': 0.7539199590682983, 'validation/loss': 1.0591936111450195, 'validation/num_examples': 50000, 'test/accuracy': 0.6325000524520874, 'test/loss': 1.6727036237716675, 'test/num_examples': 10000, 'score': 54121.38585519791, 'total_duration': 56186.07378411293, 'accumulated_submission_time': 54121.38585519791, 'accumulated_eval_time': 2052.965085029602, 'accumulated_logging_time': 6.231770277023315}
I0131 12:25:57.490373 139907267811072 logging_writer.py:48] [160430] accumulated_eval_time=2052.965085, accumulated_logging_time=6.231770, accumulated_submission_time=54121.385855, global_step=160430, preemption_count=0, score=54121.385855, test/accuracy=0.632500, test/loss=1.672704, test/num_examples=10000, total_duration=56186.073784, train/accuracy=0.890824, train/loss=0.508841, validation/accuracy=0.753920, validation/loss=1.059194, validation/num_examples=50000
I0131 12:26:21.382491 139907712378624 logging_writer.py:48] [160500] global_step=160500, grad_norm=7.549784183502197, loss=1.9440991878509521
I0131 12:26:55.022220 139907267811072 logging_writer.py:48] [160600] global_step=160600, grad_norm=7.180959701538086, loss=1.9180322885513306
I0131 12:27:28.717927 139907712378624 logging_writer.py:48] [160700] global_step=160700, grad_norm=6.65868616104126, loss=1.9777066707611084
I0131 12:28:02.381145 139907267811072 logging_writer.py:48] [160800] global_step=160800, grad_norm=7.315004825592041, loss=1.9980870485305786
I0131 12:28:36.058357 139907712378624 logging_writer.py:48] [160900] global_step=160900, grad_norm=6.941964626312256, loss=1.9220349788665771
I0131 12:29:09.735327 139907267811072 logging_writer.py:48] [161000] global_step=161000, grad_norm=6.581602573394775, loss=1.9566466808319092
I0131 12:29:43.528295 139907712378624 logging_writer.py:48] [161100] global_step=161100, grad_norm=6.844449520111084, loss=1.9955451488494873
I0131 12:30:17.279456 139907267811072 logging_writer.py:48] [161200] global_step=161200, grad_norm=7.220269680023193, loss=1.8487811088562012
I0131 12:30:50.978888 139907712378624 logging_writer.py:48] [161300] global_step=161300, grad_norm=6.396482467651367, loss=1.8402858972549438
I0131 12:31:24.649638 139907267811072 logging_writer.py:48] [161400] global_step=161400, grad_norm=6.920211315155029, loss=1.8947138786315918
I0131 12:31:58.331552 139907712378624 logging_writer.py:48] [161500] global_step=161500, grad_norm=7.427485466003418, loss=1.9325140714645386
I0131 12:32:31.943786 139907267811072 logging_writer.py:48] [161600] global_step=161600, grad_norm=6.873937606811523, loss=1.9887166023254395
I0131 12:33:05.625619 139907712378624 logging_writer.py:48] [161700] global_step=161700, grad_norm=7.668824672698975, loss=1.8980226516723633
I0131 12:33:39.382342 139907267811072 logging_writer.py:48] [161800] global_step=161800, grad_norm=7.241481304168701, loss=1.8504507541656494
I0131 12:34:13.092227 139907712378624 logging_writer.py:48] [161900] global_step=161900, grad_norm=7.191819190979004, loss=1.915588140487671
I0131 12:34:27.708266 140070692116288 spec.py:321] Evaluating on the training split.
I0131 12:34:33.851035 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 12:34:42.843227 140070692116288 spec.py:349] Evaluating on the test split.
I0131 12:34:45.105066 140070692116288 submission_runner.py:408] Time since start: 56713.73s, 	Step: 161945, 	{'train/accuracy': 0.8883330225944519, 'train/loss': 0.507793128490448, 'validation/accuracy': 0.7577599883079529, 'validation/loss': 1.0418267250061035, 'validation/num_examples': 50000, 'test/accuracy': 0.6374000310897827, 'test/loss': 1.6483393907546997, 'test/num_examples': 10000, 'score': 54631.54222178459, 'total_duration': 56713.73193693161, 'accumulated_submission_time': 54631.54222178459, 'accumulated_eval_time': 2070.361840724945, 'accumulated_logging_time': 6.284480094909668}
I0131 12:34:45.147992 139907276203776 logging_writer.py:48] [161945] accumulated_eval_time=2070.361841, accumulated_logging_time=6.284480, accumulated_submission_time=54631.542222, global_step=161945, preemption_count=0, score=54631.542222, test/accuracy=0.637400, test/loss=1.648339, test/num_examples=10000, total_duration=56713.731937, train/accuracy=0.888333, train/loss=0.507793, validation/accuracy=0.757760, validation/loss=1.041827, validation/num_examples=50000
I0131 12:35:04.040707 139907284596480 logging_writer.py:48] [162000] global_step=162000, grad_norm=6.117192268371582, loss=1.85337233543396
I0131 12:35:37.701889 139907276203776 logging_writer.py:48] [162100] global_step=162100, grad_norm=7.2534613609313965, loss=1.8958444595336914
I0131 12:36:11.455788 139907284596480 logging_writer.py:48] [162200] global_step=162200, grad_norm=7.147542476654053, loss=1.9490681886672974
I0131 12:36:45.166069 139907276203776 logging_writer.py:48] [162300] global_step=162300, grad_norm=6.915839195251465, loss=1.9237306118011475
I0131 12:37:18.873759 139907284596480 logging_writer.py:48] [162400] global_step=162400, grad_norm=6.859375476837158, loss=1.9102351665496826
I0131 12:37:52.530775 139907276203776 logging_writer.py:48] [162500] global_step=162500, grad_norm=7.341441631317139, loss=1.9326519966125488
I0131 12:38:26.227033 139907284596480 logging_writer.py:48] [162600] global_step=162600, grad_norm=6.958901405334473, loss=1.8740572929382324
I0131 12:38:59.896613 139907276203776 logging_writer.py:48] [162700] global_step=162700, grad_norm=7.373176097869873, loss=1.973712682723999
I0131 12:39:33.615173 139907284596480 logging_writer.py:48] [162800] global_step=162800, grad_norm=7.272881507873535, loss=1.9692656993865967
I0131 12:40:07.270744 139907276203776 logging_writer.py:48] [162900] global_step=162900, grad_norm=8.195038795471191, loss=1.9735193252563477
I0131 12:40:40.968621 139907284596480 logging_writer.py:48] [163000] global_step=163000, grad_norm=7.758240222930908, loss=1.9224183559417725
I0131 12:41:14.641407 139907276203776 logging_writer.py:48] [163100] global_step=163100, grad_norm=6.983544826507568, loss=1.956186294555664
I0131 12:41:48.313626 139907284596480 logging_writer.py:48] [163200] global_step=163200, grad_norm=7.055315971374512, loss=1.939613699913025
I0131 12:42:22.079807 139907276203776 logging_writer.py:48] [163300] global_step=163300, grad_norm=6.88157320022583, loss=1.8614040613174438
I0131 12:42:55.800723 139907284596480 logging_writer.py:48] [163400] global_step=163400, grad_norm=7.074707984924316, loss=1.9221251010894775
I0131 12:43:15.114039 140070692116288 spec.py:321] Evaluating on the training split.
I0131 12:43:21.203774 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 12:43:29.919728 140070692116288 spec.py:349] Evaluating on the test split.
I0131 12:43:32.362254 140070692116288 submission_runner.py:408] Time since start: 57240.99s, 	Step: 163459, 	{'train/accuracy': 0.8888113498687744, 'train/loss': 0.508962869644165, 'validation/accuracy': 0.759880006313324, 'validation/loss': 1.0377557277679443, 'validation/num_examples': 50000, 'test/accuracy': 0.6409000158309937, 'test/loss': 1.6533217430114746, 'test/num_examples': 10000, 'score': 55141.445491075516, 'total_duration': 57240.98913478851, 'accumulated_submission_time': 55141.445491075516, 'accumulated_eval_time': 2087.6100096702576, 'accumulated_logging_time': 6.337510824203491}
I0131 12:43:32.406362 139907712378624 logging_writer.py:48] [163459] accumulated_eval_time=2087.610010, accumulated_logging_time=6.337511, accumulated_submission_time=55141.445491, global_step=163459, preemption_count=0, score=55141.445491, test/accuracy=0.640900, test/loss=1.653322, test/num_examples=10000, total_duration=57240.989135, train/accuracy=0.888811, train/loss=0.508963, validation/accuracy=0.759880, validation/loss=1.037756, validation/num_examples=50000
I0131 12:43:46.548405 139907720771328 logging_writer.py:48] [163500] global_step=163500, grad_norm=7.453482627868652, loss=1.8452582359313965
I0131 12:44:20.216666 139907712378624 logging_writer.py:48] [163600] global_step=163600, grad_norm=7.679375171661377, loss=1.9346911907196045
I0131 12:44:53.878945 139907720771328 logging_writer.py:48] [163700] global_step=163700, grad_norm=7.430422782897949, loss=1.9259153604507446
I0131 12:45:27.578252 139907712378624 logging_writer.py:48] [163800] global_step=163800, grad_norm=7.045612812042236, loss=1.8772271871566772
I0131 12:46:01.252017 139907720771328 logging_writer.py:48] [163900] global_step=163900, grad_norm=7.163577079772949, loss=1.9555124044418335
I0131 12:46:34.932183 139907712378624 logging_writer.py:48] [164000] global_step=164000, grad_norm=7.374798774719238, loss=1.8639627695083618
I0131 12:47:08.600031 139907720771328 logging_writer.py:48] [164100] global_step=164100, grad_norm=7.394719123840332, loss=1.9357277154922485
I0131 12:47:42.280220 139907712378624 logging_writer.py:48] [164200] global_step=164200, grad_norm=7.346611499786377, loss=1.9347960948944092
I0131 12:48:15.983371 139907720771328 logging_writer.py:48] [164300] global_step=164300, grad_norm=7.288268566131592, loss=1.8680176734924316
I0131 12:48:49.688047 139907712378624 logging_writer.py:48] [164400] global_step=164400, grad_norm=7.312803268432617, loss=1.9536226987838745
I0131 12:49:23.383362 139907720771328 logging_writer.py:48] [164500] global_step=164500, grad_norm=7.449239253997803, loss=1.956486701965332
I0131 12:49:57.093454 139907712378624 logging_writer.py:48] [164600] global_step=164600, grad_norm=7.791034698486328, loss=1.873734712600708
I0131 12:50:30.775720 139907720771328 logging_writer.py:48] [164700] global_step=164700, grad_norm=7.256157875061035, loss=1.9141359329223633
I0131 12:51:04.443994 139907712378624 logging_writer.py:48] [164800] global_step=164800, grad_norm=6.974534034729004, loss=1.781696081161499
I0131 12:51:38.219470 139907720771328 logging_writer.py:48] [164900] global_step=164900, grad_norm=7.310504913330078, loss=1.8751164674758911
I0131 12:52:02.641168 140070692116288 spec.py:321] Evaluating on the training split.
I0131 12:52:08.724358 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 12:52:17.562202 140070692116288 spec.py:349] Evaluating on the test split.
I0131 12:52:19.843965 140070692116288 submission_runner.py:408] Time since start: 57768.47s, 	Step: 164974, 	{'train/accuracy': 0.8914421200752258, 'train/loss': 0.5064797401428223, 'validation/accuracy': 0.7610999941825867, 'validation/loss': 1.0407962799072266, 'validation/num_examples': 50000, 'test/accuracy': 0.6428000330924988, 'test/loss': 1.644087553024292, 'test/num_examples': 10000, 'score': 55651.616990327835, 'total_duration': 57768.47083735466, 'accumulated_submission_time': 55651.616990327835, 'accumulated_eval_time': 2104.812753200531, 'accumulated_logging_time': 6.391413450241089}
I0131 12:52:19.894720 139907276203776 logging_writer.py:48] [164974] accumulated_eval_time=2104.812753, accumulated_logging_time=6.391413, accumulated_submission_time=55651.616990, global_step=164974, preemption_count=0, score=55651.616990, test/accuracy=0.642800, test/loss=1.644088, test/num_examples=10000, total_duration=57768.470837, train/accuracy=0.891442, train/loss=0.506480, validation/accuracy=0.761100, validation/loss=1.040796, validation/num_examples=50000
I0131 12:52:29.026223 139907284596480 logging_writer.py:48] [165000] global_step=165000, grad_norm=7.167227268218994, loss=1.891051173210144
I0131 12:53:02.724773 139907276203776 logging_writer.py:48] [165100] global_step=165100, grad_norm=6.610793590545654, loss=1.8375515937805176
I0131 12:53:36.408051 139907284596480 logging_writer.py:48] [165200] global_step=165200, grad_norm=7.385254859924316, loss=1.960128903388977
I0131 12:54:10.186133 139907276203776 logging_writer.py:48] [165300] global_step=165300, grad_norm=7.441904067993164, loss=1.8804816007614136
I0131 12:54:43.897735 139907284596480 logging_writer.py:48] [165400] global_step=165400, grad_norm=6.725647449493408, loss=1.8772509098052979
I0131 12:55:17.615999 139907276203776 logging_writer.py:48] [165500] global_step=165500, grad_norm=7.104865074157715, loss=1.8790817260742188
I0131 12:55:51.274034 139907284596480 logging_writer.py:48] [165600] global_step=165600, grad_norm=7.893326759338379, loss=1.819535255432129
I0131 12:56:24.974668 139907276203776 logging_writer.py:48] [165700] global_step=165700, grad_norm=7.8753461837768555, loss=1.909048318862915
I0131 12:56:58.636931 139907284596480 logging_writer.py:48] [165800] global_step=165800, grad_norm=7.2749128341674805, loss=1.899498701095581
I0131 12:57:32.338462 139907276203776 logging_writer.py:48] [165900] global_step=165900, grad_norm=6.84814453125, loss=1.8841650485992432
I0131 12:58:06.023171 139907284596480 logging_writer.py:48] [166000] global_step=166000, grad_norm=7.605445384979248, loss=1.9231798648834229
I0131 12:58:39.701588 139907276203776 logging_writer.py:48] [166100] global_step=166100, grad_norm=6.5113139152526855, loss=1.7847926616668701
I0131 12:59:13.379659 139907284596480 logging_writer.py:48] [166200] global_step=166200, grad_norm=7.98510217666626, loss=1.9643512964248657
I0131 12:59:47.136228 139907276203776 logging_writer.py:48] [166300] global_step=166300, grad_norm=7.383966445922852, loss=1.8951241970062256
I0131 13:00:20.975660 139907284596480 logging_writer.py:48] [166400] global_step=166400, grad_norm=7.450432777404785, loss=1.8848841190338135
I0131 13:00:50.081358 140070692116288 spec.py:321] Evaluating on the training split.
I0131 13:00:56.225378 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 13:01:05.263125 140070692116288 spec.py:349] Evaluating on the test split.
I0131 13:01:07.925256 140070692116288 submission_runner.py:408] Time since start: 58296.55s, 	Step: 166488, 	{'train/accuracy': 0.8913623690605164, 'train/loss': 0.49384015798568726, 'validation/accuracy': 0.761900007724762, 'validation/loss': 1.031830906867981, 'validation/num_examples': 50000, 'test/accuracy': 0.6368000507354736, 'test/loss': 1.654982328414917, 'test/num_examples': 10000, 'score': 56161.74114251137, 'total_duration': 58296.5521376133, 'accumulated_submission_time': 56161.74114251137, 'accumulated_eval_time': 2122.656609773636, 'accumulated_logging_time': 6.452677011489868}
I0131 13:01:07.963207 139907712378624 logging_writer.py:48] [166488] accumulated_eval_time=2122.656610, accumulated_logging_time=6.452677, accumulated_submission_time=56161.741143, global_step=166488, preemption_count=0, score=56161.741143, test/accuracy=0.636800, test/loss=1.654982, test/num_examples=10000, total_duration=58296.552138, train/accuracy=0.891362, train/loss=0.493840, validation/accuracy=0.761900, validation/loss=1.031831, validation/num_examples=50000
I0131 13:01:12.332175 139907720771328 logging_writer.py:48] [166500] global_step=166500, grad_norm=7.2233781814575195, loss=1.8647387027740479
I0131 13:01:45.984094 139907712378624 logging_writer.py:48] [166600] global_step=166600, grad_norm=7.635879993438721, loss=1.91382896900177
I0131 13:02:19.679802 139907720771328 logging_writer.py:48] [166700] global_step=166700, grad_norm=7.939752101898193, loss=1.8148424625396729
I0131 13:02:53.369398 139907712378624 logging_writer.py:48] [166800] global_step=166800, grad_norm=7.510365009307861, loss=1.9027695655822754
I0131 13:03:27.049274 139907720771328 logging_writer.py:48] [166900] global_step=166900, grad_norm=7.4132080078125, loss=1.839573860168457
I0131 13:04:00.702158 139907712378624 logging_writer.py:48] [167000] global_step=167000, grad_norm=7.626990795135498, loss=1.8713734149932861
I0131 13:04:34.389700 139907720771328 logging_writer.py:48] [167100] global_step=167100, grad_norm=8.061141967773438, loss=1.8999830484390259
I0131 13:05:08.077723 139907712378624 logging_writer.py:48] [167200] global_step=167200, grad_norm=7.414119243621826, loss=1.8567183017730713
I0131 13:05:41.770420 139907720771328 logging_writer.py:48] [167300] global_step=167300, grad_norm=7.2673258781433105, loss=1.8738055229187012
I0131 13:06:15.417923 139907712378624 logging_writer.py:48] [167400] global_step=167400, grad_norm=7.147651672363281, loss=1.8047267198562622
I0131 13:06:49.308224 139907720771328 logging_writer.py:48] [167500] global_step=167500, grad_norm=7.516078948974609, loss=1.8600993156433105
I0131 13:07:23.001850 139907712378624 logging_writer.py:48] [167600] global_step=167600, grad_norm=7.4274725914001465, loss=1.7941993474960327
I0131 13:07:56.692820 139907720771328 logging_writer.py:48] [167700] global_step=167700, grad_norm=7.44249153137207, loss=1.8535346984863281
I0131 13:08:30.374650 139907712378624 logging_writer.py:48] [167800] global_step=167800, grad_norm=7.648994445800781, loss=1.8588675260543823
I0131 13:09:04.039300 139907720771328 logging_writer.py:48] [167900] global_step=167900, grad_norm=7.701350688934326, loss=1.9574978351593018
I0131 13:09:37.736476 139907712378624 logging_writer.py:48] [168000] global_step=168000, grad_norm=7.624203681945801, loss=1.901232123374939
I0131 13:09:38.215572 140070692116288 spec.py:321] Evaluating on the training split.
I0131 13:09:44.350298 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 13:09:53.220954 140070692116288 spec.py:349] Evaluating on the test split.
I0131 13:09:55.501474 140070692116288 submission_runner.py:408] Time since start: 58824.13s, 	Step: 168003, 	{'train/accuracy': 0.8980189561843872, 'train/loss': 0.4819498062133789, 'validation/accuracy': 0.7639999985694885, 'validation/loss': 1.017918586730957, 'validation/num_examples': 50000, 'test/accuracy': 0.6433000564575195, 'test/loss': 1.6341732740402222, 'test/num_examples': 10000, 'score': 56671.93290805817, 'total_duration': 58824.12835144997, 'accumulated_submission_time': 56671.93290805817, 'accumulated_eval_time': 2139.942459821701, 'accumulated_logging_time': 6.499396800994873}
I0131 13:09:55.553505 139907703985920 logging_writer.py:48] [168003] accumulated_eval_time=2139.942460, accumulated_logging_time=6.499397, accumulated_submission_time=56671.932908, global_step=168003, preemption_count=0, score=56671.932908, test/accuracy=0.643300, test/loss=1.634173, test/num_examples=10000, total_duration=58824.128351, train/accuracy=0.898019, train/loss=0.481950, validation/accuracy=0.764000, validation/loss=1.017919, validation/num_examples=50000
I0131 13:10:28.534805 139907729164032 logging_writer.py:48] [168100] global_step=168100, grad_norm=7.192138671875, loss=1.8405489921569824
I0131 13:11:02.227142 139907703985920 logging_writer.py:48] [168200] global_step=168200, grad_norm=7.269227504730225, loss=1.8247880935668945
I0131 13:11:35.899477 139907729164032 logging_writer.py:48] [168300] global_step=168300, grad_norm=7.63538122177124, loss=1.879214882850647
I0131 13:12:09.606666 139907703985920 logging_writer.py:48] [168400] global_step=168400, grad_norm=7.194979190826416, loss=1.8285717964172363
I0131 13:12:43.409034 139907729164032 logging_writer.py:48] [168500] global_step=168500, grad_norm=7.227038383483887, loss=1.8486356735229492
I0131 13:13:17.097258 139907703985920 logging_writer.py:48] [168600] global_step=168600, grad_norm=7.229847431182861, loss=1.8572310209274292
I0131 13:13:50.777868 139907729164032 logging_writer.py:48] [168700] global_step=168700, grad_norm=7.017291069030762, loss=1.8439794778823853
I0131 13:14:24.466919 139907703985920 logging_writer.py:48] [168800] global_step=168800, grad_norm=8.152630805969238, loss=1.9088307619094849
I0131 13:14:58.130833 139907729164032 logging_writer.py:48] [168900] global_step=168900, grad_norm=7.941755771636963, loss=1.8492426872253418
I0131 13:15:31.828788 139907703985920 logging_writer.py:48] [169000] global_step=169000, grad_norm=7.785146236419678, loss=1.8512156009674072
I0131 13:16:05.491914 139907729164032 logging_writer.py:48] [169100] global_step=169100, grad_norm=7.531373977661133, loss=1.8769505023956299
I0131 13:16:39.197779 139907703985920 logging_writer.py:48] [169200] global_step=169200, grad_norm=7.948015213012695, loss=1.8727668523788452
I0131 13:17:12.861792 139907729164032 logging_writer.py:48] [169300] global_step=169300, grad_norm=7.885187149047852, loss=1.797092318534851
I0131 13:17:46.568478 139907703985920 logging_writer.py:48] [169400] global_step=169400, grad_norm=6.9840240478515625, loss=1.793406367301941
I0131 13:18:20.220659 139907729164032 logging_writer.py:48] [169500] global_step=169500, grad_norm=7.859589099884033, loss=1.8814754486083984
I0131 13:18:25.753470 140070692116288 spec.py:321] Evaluating on the training split.
I0131 13:18:32.182648 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 13:18:41.219722 140070692116288 spec.py:349] Evaluating on the test split.
I0131 13:18:43.516098 140070692116288 submission_runner.py:408] Time since start: 59352.14s, 	Step: 169518, 	{'train/accuracy': 0.9111128449440002, 'train/loss': 0.43680357933044434, 'validation/accuracy': 0.7679599523544312, 'validation/loss': 1.0132651329040527, 'validation/num_examples': 50000, 'test/accuracy': 0.6438000202178955, 'test/loss': 1.6271370649337769, 'test/num_examples': 10000, 'score': 57182.0711376667, 'total_duration': 59352.142911195755, 'accumulated_submission_time': 57182.0711376667, 'accumulated_eval_time': 2157.704973936081, 'accumulated_logging_time': 6.561173677444458}
I0131 13:18:43.560942 139907284596480 logging_writer.py:48] [169518] accumulated_eval_time=2157.704974, accumulated_logging_time=6.561174, accumulated_submission_time=57182.071138, global_step=169518, preemption_count=0, score=57182.071138, test/accuracy=0.643800, test/loss=1.627137, test/num_examples=10000, total_duration=59352.142911, train/accuracy=0.911113, train/loss=0.436804, validation/accuracy=0.767960, validation/loss=1.013265, validation/num_examples=50000
I0131 13:19:11.493623 139907737556736 logging_writer.py:48] [169600] global_step=169600, grad_norm=7.224221229553223, loss=1.8258004188537598
I0131 13:19:45.180723 139907284596480 logging_writer.py:48] [169700] global_step=169700, grad_norm=7.362010478973389, loss=1.8419785499572754
I0131 13:20:18.852360 139907737556736 logging_writer.py:48] [169800] global_step=169800, grad_norm=7.260504245758057, loss=1.792709231376648
I0131 13:20:52.548453 139907284596480 logging_writer.py:48] [169900] global_step=169900, grad_norm=7.495913982391357, loss=1.813797950744629
I0131 13:21:26.181152 139907737556736 logging_writer.py:48] [170000] global_step=170000, grad_norm=7.719386100769043, loss=1.8507171869277954
I0131 13:21:59.884640 139907284596480 logging_writer.py:48] [170100] global_step=170100, grad_norm=8.813880920410156, loss=1.8332246541976929
I0131 13:22:33.596617 139907737556736 logging_writer.py:48] [170200] global_step=170200, grad_norm=7.439187049865723, loss=1.8776726722717285
I0131 13:23:07.292591 139907284596480 logging_writer.py:48] [170300] global_step=170300, grad_norm=7.62260103225708, loss=1.8150638341903687
I0131 13:23:40.951854 139907737556736 logging_writer.py:48] [170400] global_step=170400, grad_norm=7.04843282699585, loss=1.7336348295211792
I0131 13:24:14.623244 139907284596480 logging_writer.py:48] [170500] global_step=170500, grad_norm=7.631784439086914, loss=1.8421745300292969
I0131 13:24:48.404326 139907737556736 logging_writer.py:48] [170600] global_step=170600, grad_norm=7.91767692565918, loss=1.8684256076812744
I0131 13:25:22.170813 139907284596480 logging_writer.py:48] [170700] global_step=170700, grad_norm=7.617990970611572, loss=1.8421001434326172
I0131 13:25:55.872507 139907737556736 logging_writer.py:48] [170800] global_step=170800, grad_norm=7.514996528625488, loss=1.8524974584579468
I0131 13:26:29.545147 139907284596480 logging_writer.py:48] [170900] global_step=170900, grad_norm=7.447977542877197, loss=1.7914084196090698
I0131 13:27:03.226938 139907737556736 logging_writer.py:48] [171000] global_step=171000, grad_norm=7.781512260437012, loss=1.8158844709396362
I0131 13:27:13.827783 140070692116288 spec.py:321] Evaluating on the training split.
I0131 13:27:19.945213 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 13:27:28.874012 140070692116288 spec.py:349] Evaluating on the test split.
I0131 13:27:31.148973 140070692116288 submission_runner.py:408] Time since start: 59879.78s, 	Step: 171033, 	{'train/accuracy': 0.9100565910339355, 'train/loss': 0.4437120258808136, 'validation/accuracy': 0.768839955329895, 'validation/loss': 1.0136228799819946, 'validation/num_examples': 50000, 'test/accuracy': 0.6476000547409058, 'test/loss': 1.6199846267700195, 'test/num_examples': 10000, 'score': 57692.27556872368, 'total_duration': 59879.77585315704, 'accumulated_submission_time': 57692.27556872368, 'accumulated_eval_time': 2175.0261178016663, 'accumulated_logging_time': 6.616154432296753}
I0131 13:27:31.201935 139907259418368 logging_writer.py:48] [171033] accumulated_eval_time=2175.026118, accumulated_logging_time=6.616154, accumulated_submission_time=57692.275569, global_step=171033, preemption_count=0, score=57692.275569, test/accuracy=0.647600, test/loss=1.619985, test/num_examples=10000, total_duration=59879.775853, train/accuracy=0.910057, train/loss=0.443712, validation/accuracy=0.768840, validation/loss=1.013623, validation/num_examples=50000
I0131 13:27:54.132854 139907267811072 logging_writer.py:48] [171100] global_step=171100, grad_norm=7.448846340179443, loss=1.8287763595581055
I0131 13:28:27.803669 139907259418368 logging_writer.py:48] [171200] global_step=171200, grad_norm=7.468364238739014, loss=1.8021963834762573
I0131 13:29:01.508272 139907267811072 logging_writer.py:48] [171300] global_step=171300, grad_norm=7.2931060791015625, loss=1.8170573711395264
I0131 13:29:35.185170 139907259418368 logging_writer.py:48] [171400] global_step=171400, grad_norm=7.385719299316406, loss=1.8166308403015137
I0131 13:30:08.871807 139907267811072 logging_writer.py:48] [171500] global_step=171500, grad_norm=7.459360599517822, loss=1.8741453886032104
I0131 13:30:42.541410 139907259418368 logging_writer.py:48] [171600] global_step=171600, grad_norm=7.926942825317383, loss=1.8004897832870483
I0131 13:31:16.287316 139907267811072 logging_writer.py:48] [171700] global_step=171700, grad_norm=7.679171085357666, loss=1.911139726638794
I0131 13:31:49.972331 139907259418368 logging_writer.py:48] [171800] global_step=171800, grad_norm=7.7911505699157715, loss=1.8191273212432861
I0131 13:32:23.659732 139907267811072 logging_writer.py:48] [171900] global_step=171900, grad_norm=7.721436977386475, loss=1.804816722869873
I0131 13:32:57.318634 139907259418368 logging_writer.py:48] [172000] global_step=172000, grad_norm=7.8872528076171875, loss=1.8792622089385986
I0131 13:33:31.021425 139907267811072 logging_writer.py:48] [172100] global_step=172100, grad_norm=7.720924377441406, loss=1.7999491691589355
I0131 13:34:04.688900 139907259418368 logging_writer.py:48] [172200] global_step=172200, grad_norm=7.538778305053711, loss=1.8469667434692383
I0131 13:34:38.395569 139907267811072 logging_writer.py:48] [172300] global_step=172300, grad_norm=7.336170196533203, loss=1.8344135284423828
I0131 13:35:12.055854 139907259418368 logging_writer.py:48] [172400] global_step=172400, grad_norm=7.43727445602417, loss=1.7047728300094604
I0131 13:35:45.757463 139907267811072 logging_writer.py:48] [172500] global_step=172500, grad_norm=7.495449542999268, loss=1.793125033378601
I0131 13:36:01.394453 140070692116288 spec.py:321] Evaluating on the training split.
I0131 13:36:07.624165 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 13:36:16.528373 140070692116288 spec.py:349] Evaluating on the test split.
I0131 13:36:18.809892 140070692116288 submission_runner.py:408] Time since start: 60407.44s, 	Step: 172548, 	{'train/accuracy': 0.9079838991165161, 'train/loss': 0.44346883893013, 'validation/accuracy': 0.7683799862861633, 'validation/loss': 1.0122803449630737, 'validation/num_examples': 50000, 'test/accuracy': 0.6457000374794006, 'test/loss': 1.6168798208236694, 'test/num_examples': 10000, 'score': 58202.40605354309, 'total_duration': 60407.436655282974, 'accumulated_submission_time': 58202.40605354309, 'accumulated_eval_time': 2192.441407442093, 'accumulated_logging_time': 6.679574251174927}
I0131 13:36:18.855965 139907259418368 logging_writer.py:48] [172548] accumulated_eval_time=2192.441407, accumulated_logging_time=6.679574, accumulated_submission_time=58202.406054, global_step=172548, preemption_count=0, score=58202.406054, test/accuracy=0.645700, test/loss=1.616880, test/num_examples=10000, total_duration=60407.436655, train/accuracy=0.907984, train/loss=0.443469, validation/accuracy=0.768380, validation/loss=1.012280, validation/num_examples=50000
I0131 13:36:36.720061 139907267811072 logging_writer.py:48] [172600] global_step=172600, grad_norm=8.259105682373047, loss=1.8231719732284546
I0131 13:37:10.507295 139907259418368 logging_writer.py:48] [172700] global_step=172700, grad_norm=7.027181625366211, loss=1.793636441230774
I0131 13:37:44.232892 139907267811072 logging_writer.py:48] [172800] global_step=172800, grad_norm=7.548954486846924, loss=1.7880419492721558
I0131 13:38:17.897732 139907259418368 logging_writer.py:48] [172900] global_step=172900, grad_norm=7.2278571128845215, loss=1.7753647565841675
I0131 13:38:51.570787 139907267811072 logging_writer.py:48] [173000] global_step=173000, grad_norm=8.710442543029785, loss=1.8042562007904053
I0131 13:39:25.227983 139907259418368 logging_writer.py:48] [173100] global_step=173100, grad_norm=8.142005920410156, loss=1.798854112625122
I0131 13:39:58.874244 139907267811072 logging_writer.py:48] [173200] global_step=173200, grad_norm=8.320894241333008, loss=1.86758553981781
I0131 13:40:32.525084 139907259418368 logging_writer.py:48] [173300] global_step=173300, grad_norm=7.449126243591309, loss=1.8017349243164062
I0131 13:41:06.181429 139907267811072 logging_writer.py:48] [173400] global_step=173400, grad_norm=7.7948503494262695, loss=1.784360408782959
I0131 13:41:39.859279 139907259418368 logging_writer.py:48] [173500] global_step=173500, grad_norm=8.088058471679688, loss=1.7628424167633057
I0131 13:42:13.518203 139907267811072 logging_writer.py:48] [173600] global_step=173600, grad_norm=7.3194708824157715, loss=1.7304317951202393
I0131 13:42:47.177338 139907259418368 logging_writer.py:48] [173700] global_step=173700, grad_norm=8.370542526245117, loss=1.8236665725708008
I0131 13:43:20.884383 139907267811072 logging_writer.py:48] [173800] global_step=173800, grad_norm=8.156293869018555, loss=1.8112773895263672
I0131 13:43:54.509044 139907259418368 logging_writer.py:48] [173900] global_step=173900, grad_norm=7.934229850769043, loss=1.7883801460266113
I0131 13:44:28.215905 139907267811072 logging_writer.py:48] [174000] global_step=174000, grad_norm=8.149503707885742, loss=1.7574609518051147
I0131 13:44:48.901577 140070692116288 spec.py:321] Evaluating on the training split.
I0131 13:44:55.026787 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 13:45:04.130440 140070692116288 spec.py:349] Evaluating on the test split.
I0131 13:45:06.495406 140070692116288 submission_runner.py:408] Time since start: 60935.12s, 	Step: 174063, 	{'train/accuracy': 0.9148397445678711, 'train/loss': 0.4309082627296448, 'validation/accuracy': 0.7701599597930908, 'validation/loss': 1.0046439170837402, 'validation/num_examples': 50000, 'test/accuracy': 0.6491000056266785, 'test/loss': 1.6106775999069214, 'test/num_examples': 10000, 'score': 58712.38555598259, 'total_duration': 60935.12227869034, 'accumulated_submission_time': 58712.38555598259, 'accumulated_eval_time': 2210.0351996421814, 'accumulated_logging_time': 6.738846778869629}
I0131 13:45:06.545008 139907284596480 logging_writer.py:48] [174063] accumulated_eval_time=2210.035200, accumulated_logging_time=6.738847, accumulated_submission_time=58712.385556, global_step=174063, preemption_count=0, score=58712.385556, test/accuracy=0.649100, test/loss=1.610678, test/num_examples=10000, total_duration=60935.122279, train/accuracy=0.914840, train/loss=0.430908, validation/accuracy=0.770160, validation/loss=1.004644, validation/num_examples=50000
I0131 13:45:19.374021 139907703985920 logging_writer.py:48] [174100] global_step=174100, grad_norm=7.669536590576172, loss=1.8151026964187622
I0131 13:45:53.040593 139907284596480 logging_writer.py:48] [174200] global_step=174200, grad_norm=7.75718355178833, loss=1.8114159107208252
I0131 13:46:26.714162 139907703985920 logging_writer.py:48] [174300] global_step=174300, grad_norm=7.903013229370117, loss=1.8765740394592285
I0131 13:47:00.409773 139907284596480 logging_writer.py:48] [174400] global_step=174400, grad_norm=7.335986614227295, loss=1.8135576248168945
I0131 13:47:34.052021 139907703985920 logging_writer.py:48] [174500] global_step=174500, grad_norm=7.669249534606934, loss=1.7500405311584473
I0131 13:48:07.755041 139907284596480 logging_writer.py:48] [174600] global_step=174600, grad_norm=7.472527503967285, loss=1.7864370346069336
I0131 13:48:41.415925 139907703985920 logging_writer.py:48] [174700] global_step=174700, grad_norm=6.894510269165039, loss=1.7592909336090088
I0131 13:49:15.203943 139907284596480 logging_writer.py:48] [174800] global_step=174800, grad_norm=7.883026123046875, loss=1.8075014352798462
I0131 13:49:48.917926 139907703985920 logging_writer.py:48] [174900] global_step=174900, grad_norm=7.686400890350342, loss=1.7777488231658936
I0131 13:50:22.620234 139907284596480 logging_writer.py:48] [175000] global_step=175000, grad_norm=8.774070739746094, loss=1.8171995878219604
I0131 13:50:56.267479 139907703985920 logging_writer.py:48] [175100] global_step=175100, grad_norm=7.442971706390381, loss=1.775267481803894
I0131 13:51:29.956477 139907284596480 logging_writer.py:48] [175200] global_step=175200, grad_norm=8.13202953338623, loss=1.80303156375885
I0131 13:52:03.619641 139907703985920 logging_writer.py:48] [175300] global_step=175300, grad_norm=8.04938793182373, loss=1.8396787643432617
I0131 13:52:37.288821 139907284596480 logging_writer.py:48] [175400] global_step=175400, grad_norm=7.65493106842041, loss=1.8096331357955933
I0131 13:53:11.038896 139907703985920 logging_writer.py:48] [175500] global_step=175500, grad_norm=7.284858226776123, loss=1.8071794509887695
I0131 13:53:36.817274 140070692116288 spec.py:321] Evaluating on the training split.
I0131 13:53:43.077829 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 13:53:51.802387 140070692116288 spec.py:349] Evaluating on the test split.
I0131 13:53:54.151496 140070692116288 submission_runner.py:408] Time since start: 61462.78s, 	Step: 175578, 	{'train/accuracy': 0.9133051633834839, 'train/loss': 0.42993512749671936, 'validation/accuracy': 0.77183997631073, 'validation/loss': 1.0019093751907349, 'validation/num_examples': 50000, 'test/accuracy': 0.650700032711029, 'test/loss': 1.6060999631881714, 'test/num_examples': 10000, 'score': 59222.59151554108, 'total_duration': 61462.778339385986, 'accumulated_submission_time': 59222.59151554108, 'accumulated_eval_time': 2227.369342327118, 'accumulated_logging_time': 6.801840305328369}
I0131 13:53:54.198944 139907276203776 logging_writer.py:48] [175578] accumulated_eval_time=2227.369342, accumulated_logging_time=6.801840, accumulated_submission_time=59222.591516, global_step=175578, preemption_count=0, score=59222.591516, test/accuracy=0.650700, test/loss=1.606100, test/num_examples=10000, total_duration=61462.778339, train/accuracy=0.913305, train/loss=0.429935, validation/accuracy=0.771840, validation/loss=1.001909, validation/num_examples=50000
I0131 13:54:01.927024 139907284596480 logging_writer.py:48] [175600] global_step=175600, grad_norm=6.989676475524902, loss=1.723719835281372
I0131 13:54:35.551826 139907276203776 logging_writer.py:48] [175700] global_step=175700, grad_norm=7.803285598754883, loss=1.7455774545669556
I0131 13:55:09.255486 139907284596480 logging_writer.py:48] [175800] global_step=175800, grad_norm=7.956418037414551, loss=1.8361797332763672
I0131 13:55:42.958961 139907276203776 logging_writer.py:48] [175900] global_step=175900, grad_norm=8.337039947509766, loss=1.7788519859313965
I0131 13:56:16.689926 139907284596480 logging_writer.py:48] [176000] global_step=176000, grad_norm=7.28619384765625, loss=1.7690672874450684
I0131 13:56:50.382651 139907276203776 logging_writer.py:48] [176100] global_step=176100, grad_norm=7.631436347961426, loss=1.8313822746276855
I0131 13:57:24.069226 139907284596480 logging_writer.py:48] [176200] global_step=176200, grad_norm=8.174586296081543, loss=1.7711740732192993
I0131 13:57:57.753288 139907276203776 logging_writer.py:48] [176300] global_step=176300, grad_norm=7.490493297576904, loss=1.8212203979492188
I0131 13:58:31.408620 139907284596480 logging_writer.py:48] [176400] global_step=176400, grad_norm=7.459952354431152, loss=1.7719018459320068
I0131 13:59:05.121327 139907276203776 logging_writer.py:48] [176500] global_step=176500, grad_norm=9.083499908447266, loss=1.7947906255722046
I0131 13:59:38.851357 139907284596480 logging_writer.py:48] [176600] global_step=176600, grad_norm=7.188870429992676, loss=1.7324758768081665
I0131 14:00:12.568899 139907276203776 logging_writer.py:48] [176700] global_step=176700, grad_norm=7.641839981079102, loss=1.7793229818344116
I0131 14:00:46.241308 139907284596480 logging_writer.py:48] [176800] global_step=176800, grad_norm=7.170634746551514, loss=1.7040855884552002
I0131 14:01:19.926202 139907276203776 logging_writer.py:48] [176900] global_step=176900, grad_norm=7.713558673858643, loss=1.8162342309951782
I0131 14:01:53.711258 139907284596480 logging_writer.py:48] [177000] global_step=177000, grad_norm=8.005953788757324, loss=1.8059206008911133
I0131 14:02:24.170382 140070692116288 spec.py:321] Evaluating on the training split.
I0131 14:02:30.301097 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 14:02:39.167128 140070692116288 spec.py:349] Evaluating on the test split.
I0131 14:02:41.498406 140070692116288 submission_runner.py:408] Time since start: 61990.13s, 	Step: 177092, 	{'train/accuracy': 0.9144411683082581, 'train/loss': 0.42450007796287537, 'validation/accuracy': 0.7725799679756165, 'validation/loss': 0.9952738881111145, 'validation/num_examples': 50000, 'test/accuracy': 0.65010005235672, 'test/loss': 1.6027166843414307, 'test/num_examples': 10000, 'score': 59732.500272512436, 'total_duration': 61990.1252887249, 'accumulated_submission_time': 59732.500272512436, 'accumulated_eval_time': 2244.6973419189453, 'accumulated_logging_time': 6.859493017196655}
I0131 14:02:41.545483 139907259418368 logging_writer.py:48] [177092] accumulated_eval_time=2244.697342, accumulated_logging_time=6.859493, accumulated_submission_time=59732.500273, global_step=177092, preemption_count=0, score=59732.500273, test/accuracy=0.650100, test/loss=1.602717, test/num_examples=10000, total_duration=61990.125289, train/accuracy=0.914441, train/loss=0.424500, validation/accuracy=0.772580, validation/loss=0.995274, validation/num_examples=50000
I0131 14:02:44.581339 139907267811072 logging_writer.py:48] [177100] global_step=177100, grad_norm=7.347196578979492, loss=1.7771213054656982
I0131 14:03:18.300375 139907259418368 logging_writer.py:48] [177200] global_step=177200, grad_norm=8.283157348632812, loss=1.8555312156677246
I0131 14:03:52.167703 139907267811072 logging_writer.py:48] [177300] global_step=177300, grad_norm=7.800008773803711, loss=1.7876604795455933
I0131 14:04:25.827867 139907259418368 logging_writer.py:48] [177400] global_step=177400, grad_norm=8.22033405303955, loss=1.8119862079620361
I0131 14:04:59.464433 139907267811072 logging_writer.py:48] [177500] global_step=177500, grad_norm=7.366770267486572, loss=1.691244125366211
I0131 14:05:33.128786 139907259418368 logging_writer.py:48] [177600] global_step=177600, grad_norm=7.200551986694336, loss=1.7970284223556519
I0131 14:06:06.849358 139907267811072 logging_writer.py:48] [177700] global_step=177700, grad_norm=8.374346733093262, loss=1.8383594751358032
I0131 14:06:40.550342 139907259418368 logging_writer.py:48] [177800] global_step=177800, grad_norm=7.114278793334961, loss=1.7396385669708252
I0131 14:07:14.244727 139907267811072 logging_writer.py:48] [177900] global_step=177900, grad_norm=7.947005748748779, loss=1.762019157409668
I0131 14:07:48.080909 139907259418368 logging_writer.py:48] [178000] global_step=178000, grad_norm=8.052090644836426, loss=1.7652347087860107
I0131 14:08:21.766226 139907267811072 logging_writer.py:48] [178100] global_step=178100, grad_norm=7.147752285003662, loss=1.717229962348938
I0131 14:08:55.435669 139907259418368 logging_writer.py:48] [178200] global_step=178200, grad_norm=8.145398139953613, loss=1.7507835626602173
I0131 14:09:29.141524 139907267811072 logging_writer.py:48] [178300] global_step=178300, grad_norm=8.165047645568848, loss=1.802912950515747
I0131 14:10:02.810176 139907259418368 logging_writer.py:48] [178400] global_step=178400, grad_norm=8.243778228759766, loss=1.8123019933700562
I0131 14:10:36.502198 139907267811072 logging_writer.py:48] [178500] global_step=178500, grad_norm=7.712591648101807, loss=1.803941011428833
I0131 14:11:10.166082 139907259418368 logging_writer.py:48] [178600] global_step=178600, grad_norm=8.051801681518555, loss=1.765265703201294
I0131 14:11:11.658349 140070692116288 spec.py:321] Evaluating on the training split.
I0131 14:11:17.801058 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 14:11:26.786107 140070692116288 spec.py:349] Evaluating on the test split.
I0131 14:11:29.080356 140070692116288 submission_runner.py:408] Time since start: 62517.71s, 	Step: 178606, 	{'train/accuracy': 0.9225127100944519, 'train/loss': 0.39775556325912476, 'validation/accuracy': 0.7713800072669983, 'validation/loss': 0.9977371096611023, 'validation/num_examples': 50000, 'test/accuracy': 0.6538000106811523, 'test/loss': 1.6024290323257446, 'test/num_examples': 10000, 'score': 60242.55020594597, 'total_duration': 62517.70723223686, 'accumulated_submission_time': 60242.55020594597, 'accumulated_eval_time': 2262.119296312332, 'accumulated_logging_time': 6.916916847229004}
I0131 14:11:29.130944 139907729164032 logging_writer.py:48] [178606] accumulated_eval_time=2262.119296, accumulated_logging_time=6.916917, accumulated_submission_time=60242.550206, global_step=178606, preemption_count=0, score=60242.550206, test/accuracy=0.653800, test/loss=1.602429, test/num_examples=10000, total_duration=62517.707232, train/accuracy=0.922513, train/loss=0.397756, validation/accuracy=0.771380, validation/loss=0.997737, validation/num_examples=50000
I0131 14:12:01.123913 139907737556736 logging_writer.py:48] [178700] global_step=178700, grad_norm=7.716291427612305, loss=1.7653146982192993
I0131 14:12:34.817073 139907729164032 logging_writer.py:48] [178800] global_step=178800, grad_norm=8.431775093078613, loss=1.8273580074310303
I0131 14:13:08.490395 139907737556736 logging_writer.py:48] [178900] global_step=178900, grad_norm=7.394378662109375, loss=1.8273539543151855
I0131 14:13:42.181899 139907729164032 logging_writer.py:48] [179000] global_step=179000, grad_norm=8.094883918762207, loss=1.792931318283081
I0131 14:14:15.996045 139907737556736 logging_writer.py:48] [179100] global_step=179100, grad_norm=7.3356099128723145, loss=1.6812204122543335
I0131 14:14:49.690443 139907729164032 logging_writer.py:48] [179200] global_step=179200, grad_norm=8.406057357788086, loss=1.7791707515716553
I0131 14:15:23.335335 139907737556736 logging_writer.py:48] [179300] global_step=179300, grad_norm=7.459887504577637, loss=1.742480754852295
I0131 14:15:57.027790 139907729164032 logging_writer.py:48] [179400] global_step=179400, grad_norm=7.296416759490967, loss=1.822953224182129
I0131 14:16:30.688381 139907737556736 logging_writer.py:48] [179500] global_step=179500, grad_norm=7.740612506866455, loss=1.8241548538208008
I0131 14:17:04.379304 139907729164032 logging_writer.py:48] [179600] global_step=179600, grad_norm=8.106842994689941, loss=1.8109866380691528
I0131 14:17:38.057106 139907737556736 logging_writer.py:48] [179700] global_step=179700, grad_norm=7.6980791091918945, loss=1.7885663509368896
I0131 14:18:11.728224 139907729164032 logging_writer.py:48] [179800] global_step=179800, grad_norm=7.816958904266357, loss=1.8447445631027222
I0131 14:18:45.432882 139907737556736 logging_writer.py:48] [179900] global_step=179900, grad_norm=7.747223377227783, loss=1.7771409749984741
I0131 14:19:19.167300 139907729164032 logging_writer.py:48] [180000] global_step=180000, grad_norm=7.326665878295898, loss=1.6961328983306885
I0131 14:19:52.905121 139907737556736 logging_writer.py:48] [180100] global_step=180100, grad_norm=7.86131477355957, loss=1.7479475736618042
I0131 14:19:59.146039 140070692116288 spec.py:321] Evaluating on the training split.
I0131 14:20:05.358583 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 14:20:14.158258 140070692116288 spec.py:349] Evaluating on the test split.
I0131 14:20:16.435373 140070692116288 submission_runner.py:408] Time since start: 63045.06s, 	Step: 180120, 	{'train/accuracy': 0.9199019074440002, 'train/loss': 0.4014585316181183, 'validation/accuracy': 0.7723199725151062, 'validation/loss': 0.9948861598968506, 'validation/num_examples': 50000, 'test/accuracy': 0.6515000462532043, 'test/loss': 1.5987427234649658, 'test/num_examples': 10000, 'score': 60752.50250053406, 'total_duration': 63045.0622420311, 'accumulated_submission_time': 60752.50250053406, 'accumulated_eval_time': 2279.408571243286, 'accumulated_logging_time': 6.978010654449463}
I0131 14:20:16.488087 139907276203776 logging_writer.py:48] [180120] accumulated_eval_time=2279.408571, accumulated_logging_time=6.978011, accumulated_submission_time=60752.502501, global_step=180120, preemption_count=0, score=60752.502501, test/accuracy=0.651500, test/loss=1.598743, test/num_examples=10000, total_duration=63045.062242, train/accuracy=0.919902, train/loss=0.401459, validation/accuracy=0.772320, validation/loss=0.994886, validation/num_examples=50000
I0131 14:20:43.796501 139907284596480 logging_writer.py:48] [180200] global_step=180200, grad_norm=7.4434075355529785, loss=1.7638204097747803
I0131 14:21:17.468880 139907276203776 logging_writer.py:48] [180300] global_step=180300, grad_norm=8.132752418518066, loss=1.777501106262207
I0131 14:21:51.136905 139907284596480 logging_writer.py:48] [180400] global_step=180400, grad_norm=7.589956760406494, loss=1.810815453529358
I0131 14:22:24.827364 139907276203776 logging_writer.py:48] [180500] global_step=180500, grad_norm=7.663341999053955, loss=1.8145943880081177
I0131 14:22:58.494415 139907284596480 logging_writer.py:48] [180600] global_step=180600, grad_norm=7.921271800994873, loss=1.7362313270568848
I0131 14:23:32.186222 139907276203776 logging_writer.py:48] [180700] global_step=180700, grad_norm=8.00844669342041, loss=1.7683342695236206
I0131 14:24:05.844722 139907284596480 logging_writer.py:48] [180800] global_step=180800, grad_norm=7.945504665374756, loss=1.8201472759246826
I0131 14:24:39.516290 139907276203776 logging_writer.py:48] [180900] global_step=180900, grad_norm=7.8792548179626465, loss=1.8096891641616821
I0131 14:25:13.252321 139907284596480 logging_writer.py:48] [181000] global_step=181000, grad_norm=7.842169284820557, loss=1.7671761512756348
I0131 14:25:46.942492 139907276203776 logging_writer.py:48] [181100] global_step=181100, grad_norm=7.57103967666626, loss=1.8113903999328613
I0131 14:26:20.746368 139907284596480 logging_writer.py:48] [181200] global_step=181200, grad_norm=8.266312599182129, loss=1.8807190656661987
I0131 14:26:54.427367 139907276203776 logging_writer.py:48] [181300] global_step=181300, grad_norm=7.920151710510254, loss=1.7366924285888672
I0131 14:27:28.124482 139907284596480 logging_writer.py:48] [181400] global_step=181400, grad_norm=8.532148361206055, loss=1.8044507503509521
I0131 14:28:01.811141 139907276203776 logging_writer.py:48] [181500] global_step=181500, grad_norm=8.05745792388916, loss=1.8161526918411255
I0131 14:28:35.473685 139907284596480 logging_writer.py:48] [181600] global_step=181600, grad_norm=8.230963706970215, loss=1.8033413887023926
I0131 14:28:46.726248 140070692116288 spec.py:321] Evaluating on the training split.
I0131 14:28:52.821531 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 14:29:01.729372 140070692116288 spec.py:349] Evaluating on the test split.
I0131 14:29:04.096906 140070692116288 submission_runner.py:408] Time since start: 63572.72s, 	Step: 181635, 	{'train/accuracy': 0.920918345451355, 'train/loss': 0.4040337800979614, 'validation/accuracy': 0.7740799784660339, 'validation/loss': 0.9936750531196594, 'validation/num_examples': 50000, 'test/accuracy': 0.6521000266075134, 'test/loss': 1.5991164445877075, 'test/num_examples': 10000, 'score': 61262.67764997482, 'total_duration': 63572.72377586365, 'accumulated_submission_time': 61262.67764997482, 'accumulated_eval_time': 2296.7791769504547, 'accumulated_logging_time': 7.041255712509155}
I0131 14:29:04.144691 139907720771328 logging_writer.py:48] [181635] accumulated_eval_time=2296.779177, accumulated_logging_time=7.041256, accumulated_submission_time=61262.677650, global_step=181635, preemption_count=0, score=61262.677650, test/accuracy=0.652100, test/loss=1.599116, test/num_examples=10000, total_duration=63572.723776, train/accuracy=0.920918, train/loss=0.404034, validation/accuracy=0.774080, validation/loss=0.993675, validation/num_examples=50000
I0131 14:29:26.371224 139907729164032 logging_writer.py:48] [181700] global_step=181700, grad_norm=7.626605987548828, loss=1.8073759078979492
I0131 14:30:00.044804 139907720771328 logging_writer.py:48] [181800] global_step=181800, grad_norm=6.920311450958252, loss=1.7550990581512451
I0131 14:30:33.740751 139907729164032 logging_writer.py:48] [181900] global_step=181900, grad_norm=8.050138473510742, loss=1.7880603075027466
I0131 14:31:07.403198 139907720771328 logging_writer.py:48] [182000] global_step=182000, grad_norm=7.124270915985107, loss=1.7615679502487183
I0131 14:31:41.116059 139907729164032 logging_writer.py:48] [182100] global_step=182100, grad_norm=8.02560806274414, loss=1.7537109851837158
I0131 14:32:14.891515 139907720771328 logging_writer.py:48] [182200] global_step=182200, grad_norm=7.459174156188965, loss=1.7807615995407104
I0131 14:32:48.643649 139907729164032 logging_writer.py:48] [182300] global_step=182300, grad_norm=7.552229404449463, loss=1.7879514694213867
I0131 14:33:22.347067 139907720771328 logging_writer.py:48] [182400] global_step=182400, grad_norm=8.528654098510742, loss=1.8193151950836182
I0131 14:33:56.094245 139907729164032 logging_writer.py:48] [182500] global_step=182500, grad_norm=7.869606018066406, loss=1.7637524604797363
I0131 14:34:29.767768 139907720771328 logging_writer.py:48] [182600] global_step=182600, grad_norm=7.595462799072266, loss=1.7023563385009766
I0131 14:35:03.460164 139907729164032 logging_writer.py:48] [182700] global_step=182700, grad_norm=8.564876556396484, loss=1.8464685678482056
I0131 14:35:37.125012 139907720771328 logging_writer.py:48] [182800] global_step=182800, grad_norm=7.60538387298584, loss=1.709258794784546
I0131 14:36:10.815024 139907729164032 logging_writer.py:48] [182900] global_step=182900, grad_norm=8.220378875732422, loss=1.7709096670150757
I0131 14:36:44.465693 139907720771328 logging_writer.py:48] [183000] global_step=183000, grad_norm=8.211809158325195, loss=1.8132412433624268
I0131 14:37:18.159937 139907729164032 logging_writer.py:48] [183100] global_step=183100, grad_norm=7.862525939941406, loss=1.8220582008361816
I0131 14:37:34.123847 140070692116288 spec.py:321] Evaluating on the training split.
I0131 14:37:40.343811 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 14:37:48.945169 140070692116288 spec.py:349] Evaluating on the test split.
I0131 14:37:51.211961 140070692116288 submission_runner.py:408] Time since start: 64099.84s, 	Step: 183149, 	{'train/accuracy': 0.9201411008834839, 'train/loss': 0.398114413022995, 'validation/accuracy': 0.7745400071144104, 'validation/loss': 0.9911282062530518, 'validation/num_examples': 50000, 'test/accuracy': 0.6522000432014465, 'test/loss': 1.5940217971801758, 'test/num_examples': 10000, 'score': 61772.59363722801, 'total_duration': 64099.83883333206, 'accumulated_submission_time': 61772.59363722801, 'accumulated_eval_time': 2313.8672394752502, 'accumulated_logging_time': 7.099210262298584}
I0131 14:37:51.259630 139907284596480 logging_writer.py:48] [183149] accumulated_eval_time=2313.867239, accumulated_logging_time=7.099210, accumulated_submission_time=61772.593637, global_step=183149, preemption_count=0, score=61772.593637, test/accuracy=0.652200, test/loss=1.594022, test/num_examples=10000, total_duration=64099.838833, train/accuracy=0.920141, train/loss=0.398114, validation/accuracy=0.774540, validation/loss=0.991128, validation/num_examples=50000
I0131 14:38:08.817250 139907703985920 logging_writer.py:48] [183200] global_step=183200, grad_norm=7.7942891120910645, loss=1.7752814292907715
I0131 14:38:42.467360 139907284596480 logging_writer.py:48] [183300] global_step=183300, grad_norm=7.381013870239258, loss=1.755850076675415
I0131 14:39:16.156079 139907703985920 logging_writer.py:48] [183400] global_step=183400, grad_norm=8.184507369995117, loss=1.742648959159851
I0131 14:39:49.824442 139907284596480 logging_writer.py:48] [183500] global_step=183500, grad_norm=7.773172378540039, loss=1.7192925214767456
I0131 14:40:23.498165 139907703985920 logging_writer.py:48] [183600] global_step=183600, grad_norm=7.639387607574463, loss=1.7578155994415283
I0131 14:40:57.192689 139907284596480 logging_writer.py:48] [183700] global_step=183700, grad_norm=8.16879653930664, loss=1.7916183471679688
I0131 14:41:30.873003 139907703985920 logging_writer.py:48] [183800] global_step=183800, grad_norm=8.000749588012695, loss=1.8229196071624756
I0131 14:42:04.590275 139907284596480 logging_writer.py:48] [183900] global_step=183900, grad_norm=8.075761795043945, loss=1.7404685020446777
I0131 14:42:38.280212 139907703985920 logging_writer.py:48] [184000] global_step=184000, grad_norm=7.727778911590576, loss=1.798178791999817
I0131 14:43:11.959977 139907284596480 logging_writer.py:48] [184100] global_step=184100, grad_norm=8.211248397827148, loss=1.7838879823684692
I0131 14:43:45.655764 139907703985920 logging_writer.py:48] [184200] global_step=184200, grad_norm=7.533268451690674, loss=1.7808558940887451
I0131 14:44:19.503199 139907284596480 logging_writer.py:48] [184300] global_step=184300, grad_norm=7.268650054931641, loss=1.7877190113067627
I0131 14:44:53.169520 139907703985920 logging_writer.py:48] [184400] global_step=184400, grad_norm=7.622912883758545, loss=1.7259310483932495
I0131 14:45:26.834396 139907284596480 logging_writer.py:48] [184500] global_step=184500, grad_norm=7.59812593460083, loss=1.7536929845809937
I0131 14:46:00.526755 139907703985920 logging_writer.py:48] [184600] global_step=184600, grad_norm=7.554960250854492, loss=1.7505078315734863
I0131 14:46:21.222534 140070692116288 spec.py:321] Evaluating on the training split.
I0131 14:46:27.335707 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 14:46:36.244723 140070692116288 spec.py:349] Evaluating on the test split.
I0131 14:46:38.547316 140070692116288 submission_runner.py:408] Time since start: 64627.17s, 	Step: 184663, 	{'train/accuracy': 0.9205795526504517, 'train/loss': 0.4052787721157074, 'validation/accuracy': 0.774399995803833, 'validation/loss': 0.9936506748199463, 'validation/num_examples': 50000, 'test/accuracy': 0.6538000106811523, 'test/loss': 1.596049427986145, 'test/num_examples': 10000, 'score': 62282.494843006134, 'total_duration': 64627.17417168617, 'accumulated_submission_time': 62282.494843006134, 'accumulated_eval_time': 2331.1919524669647, 'accumulated_logging_time': 7.156313896179199}
I0131 14:46:38.592131 139907720771328 logging_writer.py:48] [184663] accumulated_eval_time=2331.191952, accumulated_logging_time=7.156314, accumulated_submission_time=62282.494843, global_step=184663, preemption_count=0, score=62282.494843, test/accuracy=0.653800, test/loss=1.596049, test/num_examples=10000, total_duration=64627.174172, train/accuracy=0.920580, train/loss=0.405279, validation/accuracy=0.774400, validation/loss=0.993651, validation/num_examples=50000
I0131 14:46:51.362807 139907737556736 logging_writer.py:48] [184700] global_step=184700, grad_norm=6.937470436096191, loss=1.7408254146575928
I0131 14:47:24.983433 139907720771328 logging_writer.py:48] [184800] global_step=184800, grad_norm=7.688114166259766, loss=1.7418451309204102
I0131 14:47:58.680643 139907737556736 logging_writer.py:48] [184900] global_step=184900, grad_norm=7.873899936676025, loss=1.8209697008132935
I0131 14:48:32.385764 139907720771328 logging_writer.py:48] [185000] global_step=185000, grad_norm=8.035901069641113, loss=1.7431533336639404
I0131 14:49:06.051782 139907737556736 logging_writer.py:48] [185100] global_step=185100, grad_norm=8.211433410644531, loss=1.7326637506484985
I0131 14:49:39.739878 139907720771328 logging_writer.py:48] [185200] global_step=185200, grad_norm=7.708342552185059, loss=1.7393064498901367
I0131 14:50:13.480465 139907737556736 logging_writer.py:48] [185300] global_step=185300, grad_norm=7.768716335296631, loss=1.6946431398391724
I0131 14:50:47.362053 139907720771328 logging_writer.py:48] [185400] global_step=185400, grad_norm=7.6575846672058105, loss=1.77854585647583
I0131 14:51:21.022430 139907737556736 logging_writer.py:48] [185500] global_step=185500, grad_norm=7.218449115753174, loss=1.750671148300171
I0131 14:51:54.697092 139907720771328 logging_writer.py:48] [185600] global_step=185600, grad_norm=8.645830154418945, loss=1.8101073503494263
I0131 14:52:28.354145 139907737556736 logging_writer.py:48] [185700] global_step=185700, grad_norm=8.083596229553223, loss=1.8170175552368164
I0131 14:53:02.039242 139907720771328 logging_writer.py:48] [185800] global_step=185800, grad_norm=7.208072662353516, loss=1.763372540473938
I0131 14:53:35.762109 139907737556736 logging_writer.py:48] [185900] global_step=185900, grad_norm=7.294958591461182, loss=1.7637797594070435
I0131 14:54:09.512592 139907720771328 logging_writer.py:48] [186000] global_step=186000, grad_norm=7.756521224975586, loss=1.7818549871444702
I0131 14:54:43.180938 139907737556736 logging_writer.py:48] [186100] global_step=186100, grad_norm=7.756319522857666, loss=1.7507922649383545
I0131 14:55:08.589139 140070692116288 spec.py:321] Evaluating on the training split.
I0131 14:55:14.686426 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 14:55:23.583232 140070692116288 spec.py:349] Evaluating on the test split.
I0131 14:55:25.900512 140070692116288 submission_runner.py:408] Time since start: 65154.53s, 	Step: 186177, 	{'train/accuracy': 0.9222536683082581, 'train/loss': 0.4010597765445709, 'validation/accuracy': 0.7745199799537659, 'validation/loss': 0.9941147565841675, 'validation/num_examples': 50000, 'test/accuracy': 0.6533000469207764, 'test/loss': 1.5955201387405396, 'test/num_examples': 10000, 'score': 62792.42883324623, 'total_duration': 65154.52738904953, 'accumulated_submission_time': 62792.42883324623, 'accumulated_eval_time': 2348.503286600113, 'accumulated_logging_time': 7.2113025188446045}
I0131 14:55:25.948301 139907276203776 logging_writer.py:48] [186177] accumulated_eval_time=2348.503287, accumulated_logging_time=7.211303, accumulated_submission_time=62792.428833, global_step=186177, preemption_count=0, score=62792.428833, test/accuracy=0.653300, test/loss=1.595520, test/num_examples=10000, total_duration=65154.527389, train/accuracy=0.922254, train/loss=0.401060, validation/accuracy=0.774520, validation/loss=0.994115, validation/num_examples=50000
I0131 14:55:34.026999 139907284596480 logging_writer.py:48] [186200] global_step=186200, grad_norm=7.914061069488525, loss=1.8221920728683472
I0131 14:56:07.773591 139907276203776 logging_writer.py:48] [186300] global_step=186300, grad_norm=7.822838306427002, loss=1.7438310384750366
I0131 14:56:41.634103 139907284596480 logging_writer.py:48] [186400] global_step=186400, grad_norm=8.285632133483887, loss=1.7448946237564087
I0131 14:57:15.301853 139907276203776 logging_writer.py:48] [186500] global_step=186500, grad_norm=7.633940696716309, loss=1.7352807521820068
I0131 14:57:48.955584 139907284596480 logging_writer.py:48] [186600] global_step=186600, grad_norm=8.026839256286621, loss=1.8062841892242432
I0131 14:58:10.644397 140070692116288 spec.py:321] Evaluating on the training split.
I0131 14:58:16.631643 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 14:58:25.499561 140070692116288 spec.py:349] Evaluating on the test split.
I0131 14:58:27.831237 140070692116288 submission_runner.py:408] Time since start: 65336.46s, 	Step: 186666, 	{'train/accuracy': 0.9229711294174194, 'train/loss': 0.38955602049827576, 'validation/accuracy': 0.774399995803833, 'validation/loss': 0.9909643530845642, 'validation/num_examples': 50000, 'test/accuracy': 0.6532000303268433, 'test/loss': 1.594335675239563, 'test/num_examples': 10000, 'score': 62957.09817099571, 'total_duration': 65336.45809841156, 'accumulated_submission_time': 62957.09817099571, 'accumulated_eval_time': 2365.6900820732117, 'accumulated_logging_time': 7.268607139587402}
I0131 14:58:27.880611 139907720771328 logging_writer.py:48] [186666] accumulated_eval_time=2365.690082, accumulated_logging_time=7.268607, accumulated_submission_time=62957.098171, global_step=186666, preemption_count=0, score=62957.098171, test/accuracy=0.653200, test/loss=1.594336, test/num_examples=10000, total_duration=65336.458098, train/accuracy=0.922971, train/loss=0.389556, validation/accuracy=0.774400, validation/loss=0.990964, validation/num_examples=50000
I0131 14:58:27.921122 139907729164032 logging_writer.py:48] [186666] global_step=186666, preemption_count=0, score=62957.098171
I0131 14:58:28.333238 140070692116288 checkpoints.py:490] Saving checkpoint at step: 186666
I0131 14:58:29.665009 140070692116288 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_3/imagenet_resnet_jax/trial_1/checkpoint_186666
I0131 14:58:29.690936 140070692116288 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_3/imagenet_resnet_jax/trial_1/checkpoint_186666.
I0131 14:58:30.506494 140070692116288 submission_runner.py:583] Tuning trial 1/5
I0131 14:58:30.506761 140070692116288 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0131 14:58:30.511218 140070692116288 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0013352996902540326, 'train/loss': 6.911616802215576, 'validation/accuracy': 0.0011199999134987593, 'validation/loss': 6.912059783935547, 'validation/num_examples': 50000, 'test/accuracy': 0.0009000000427477062, 'test/loss': 6.912177562713623, 'test/num_examples': 10000, 'score': 54.74183368682861, 'total_duration': 92.23791027069092, 'accumulated_submission_time': 54.74183368682861, 'accumulated_eval_time': 37.495994091033936, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1508, {'train/accuracy': 0.07099011540412903, 'train/loss': 5.393021583557129, 'validation/accuracy': 0.06667999923229218, 'validation/loss': 5.462131500244141, 'validation/num_examples': 50000, 'test/accuracy': 0.048500001430511475, 'test/loss': 5.668511390686035, 'test/num_examples': 10000, 'score': 564.9765136241913, 'total_duration': 620.535459280014, 'accumulated_submission_time': 564.9765136241913, 'accumulated_eval_time': 55.481812715530396, 'accumulated_logging_time': 0.02588486671447754, 'global_step': 1508, 'preemption_count': 0}), (3013, {'train/accuracy': 0.17681759595870972, 'train/loss': 4.263920783996582, 'validation/accuracy': 0.15916000306606293, 'validation/loss': 4.370645523071289, 'validation/num_examples': 50000, 'test/accuracy': 0.1177000030875206, 'test/loss': 4.772252559661865, 'test/num_examples': 10000, 'score': 1074.9497528076172, 'total_duration': 1148.9814901351929, 'accumulated_submission_time': 1074.9497528076172, 'accumulated_eval_time': 73.87850284576416, 'accumulated_logging_time': 0.05106663703918457, 'global_step': 3013, 'preemption_count': 0}), (4518, {'train/accuracy': 0.28756776452064514, 'train/loss': 3.482984781265259, 'validation/accuracy': 0.26330000162124634, 'validation/loss': 3.6200780868530273, 'validation/num_examples': 50000, 'test/accuracy': 0.19010001420974731, 'test/loss': 4.155796527862549, 'test/num_examples': 10000, 'score': 1584.9473168849945, 'total_duration': 1677.1219086647034, 'accumulated_submission_time': 1584.9473168849945, 'accumulated_eval_time': 91.94419121742249, 'accumulated_logging_time': 0.07854390144348145, 'global_step': 4518, 'preemption_count': 0}), (6024, {'train/accuracy': 0.3698580861091614, 'train/loss': 2.9841694831848145, 'validation/accuracy': 0.3408399820327759, 'validation/loss': 3.133474349975586, 'validation/num_examples': 50000, 'test/accuracy': 0.255700021982193, 'test/loss': 3.7140119075775146, 'test/num_examples': 10000, 'score': 2095.0244052410126, 'total_duration': 2205.5155744552612, 'accumulated_submission_time': 2095.0244052410126, 'accumulated_eval_time': 110.18438458442688, 'accumulated_logging_time': 0.10527682304382324, 'global_step': 6024, 'preemption_count': 0}), (7529, {'train/accuracy': 0.44088807702064514, 'train/loss': 2.5516676902770996, 'validation/accuracy': 0.41152000427246094, 'validation/loss': 2.7000417709350586, 'validation/num_examples': 50000, 'test/accuracy': 0.3109000027179718, 'test/loss': 3.379472017288208, 'test/num_examples': 10000, 'score': 2605.0299191474915, 'total_duration': 2733.8128702640533, 'accumulated_submission_time': 2605.0299191474915, 'accumulated_eval_time': 128.39868831634521, 'accumulated_logging_time': 0.13362383842468262, 'global_step': 7529, 'preemption_count': 0}), (9036, {'train/accuracy': 0.49230706691741943, 'train/loss': 2.256446599960327, 'validation/accuracy': 0.45865997672080994, 'validation/loss': 2.4324121475219727, 'validation/num_examples': 50000, 'test/accuracy': 0.3530000150203705, 'test/loss': 3.0937793254852295, 'test/num_examples': 10000, 'score': 3115.184098005295, 'total_duration': 3262.2012915611267, 'accumulated_submission_time': 3115.184098005295, 'accumulated_eval_time': 146.5477843284607, 'accumulated_logging_time': 0.1685779094696045, 'global_step': 9036, 'preemption_count': 0}), (10544, {'train/accuracy': 0.5581353306770325, 'train/loss': 1.9381918907165527, 'validation/accuracy': 0.5031399726867676, 'validation/loss': 2.2289462089538574, 'validation/num_examples': 50000, 'test/accuracy': 0.39180001616477966, 'test/loss': 2.887331008911133, 'test/num_examples': 10000, 'score': 3625.3059175014496, 'total_duration': 3790.7159888744354, 'accumulated_submission_time': 3625.3059175014496, 'accumulated_eval_time': 164.86400961875916, 'accumulated_logging_time': 0.19490265846252441, 'global_step': 10544, 'preemption_count': 0}), (12053, {'train/accuracy': 0.5622209906578064, 'train/loss': 1.9551576375961304, 'validation/accuracy': 0.5192399621009827, 'validation/loss': 2.169461488723755, 'validation/num_examples': 50000, 'test/accuracy': 0.40320003032684326, 'test/loss': 2.8256454467773438, 'test/num_examples': 10000, 'score': 4135.503875255585, 'total_duration': 4319.589529514313, 'accumulated_submission_time': 4135.503875255585, 'accumulated_eval_time': 183.4552013874054, 'accumulated_logging_time': 0.22897028923034668, 'global_step': 12053, 'preemption_count': 0}), (13562, {'train/accuracy': 0.5889070630073547, 'train/loss': 1.8355814218521118, 'validation/accuracy': 0.5423399806022644, 'validation/loss': 2.049694776535034, 'validation/num_examples': 50000, 'test/accuracy': 0.41920003294944763, 'test/loss': 2.7327487468719482, 'test/num_examples': 10000, 'score': 4645.7563996315, 'total_duration': 4851.3021948337555, 'accumulated_submission_time': 4645.7563996315, 'accumulated_eval_time': 204.8366265296936, 'accumulated_logging_time': 0.25846338272094727, 'global_step': 13562, 'preemption_count': 0}), (15073, {'train/accuracy': 0.5912587642669678, 'train/loss': 1.7979090213775635, 'validation/accuracy': 0.551800012588501, 'validation/loss': 1.9871643781661987, 'validation/num_examples': 50000, 'test/accuracy': 0.42600002884864807, 'test/loss': 2.665627956390381, 'test/num_examples': 10000, 'score': 5155.886093854904, 'total_duration': 5384.902832508087, 'accumulated_submission_time': 5155.886093854904, 'accumulated_eval_time': 228.2326774597168, 'accumulated_logging_time': 0.28252625465393066, 'global_step': 15073, 'preemption_count': 0}), (16583, {'train/accuracy': 0.6018216013908386, 'train/loss': 1.7547258138656616, 'validation/accuracy': 0.5679000020027161, 'validation/loss': 1.932205080986023, 'validation/num_examples': 50000, 'test/accuracy': 0.44450002908706665, 'test/loss': 2.6144609451293945, 'test/num_examples': 10000, 'score': 5666.106993675232, 'total_duration': 5918.15540766716, 'accumulated_submission_time': 5666.106993675232, 'accumulated_eval_time': 251.17728233337402, 'accumulated_logging_time': 0.3175804615020752, 'global_step': 16583, 'preemption_count': 0}), (18094, {'train/accuracy': 0.6030970811843872, 'train/loss': 1.7894777059555054, 'validation/accuracy': 0.5586400032043457, 'validation/loss': 1.985974907875061, 'validation/num_examples': 50000, 'test/accuracy': 0.43950003385543823, 'test/loss': 2.643144369125366, 'test/num_examples': 10000, 'score': 6176.272500514984, 'total_duration': 6453.581561326981, 'accumulated_submission_time': 6176.272500514984, 'accumulated_eval_time': 276.3594694137573, 'accumulated_logging_time': 0.34600257873535156, 'global_step': 18094, 'preemption_count': 0}), (19605, {'train/accuracy': 0.6378746628761292, 'train/loss': 1.6028066873550415, 'validation/accuracy': 0.5719999670982361, 'validation/loss': 1.902109146118164, 'validation/num_examples': 50000, 'test/accuracy': 0.4588000178337097, 'test/loss': 2.5408897399902344, 'test/num_examples': 10000, 'score': 6686.243889808655, 'total_duration': 6987.860915660858, 'accumulated_submission_time': 6686.243889808655, 'accumulated_eval_time': 300.57800698280334, 'accumulated_logging_time': 0.38440513610839844, 'global_step': 19605, 'preemption_count': 0}), (21117, {'train/accuracy': 0.6107900142669678, 'train/loss': 1.6893270015716553, 'validation/accuracy': 0.5640599727630615, 'validation/loss': 1.925768256187439, 'validation/num_examples': 50000, 'test/accuracy': 0.43890002369880676, 'test/loss': 2.6041297912597656, 'test/num_examples': 10000, 'score': 7196.342430591583, 'total_duration': 7522.037932395935, 'accumulated_submission_time': 7196.342430591583, 'accumulated_eval_time': 324.5791804790497, 'accumulated_logging_time': 0.41138768196105957, 'global_step': 21117, 'preemption_count': 0}), (22629, {'train/accuracy': 0.6161710619926453, 'train/loss': 1.6690157651901245, 'validation/accuracy': 0.5745599865913391, 'validation/loss': 1.873211145401001, 'validation/num_examples': 50000, 'test/accuracy': 0.45810002088546753, 'test/loss': 2.543734312057495, 'test/num_examples': 10000, 'score': 7706.454968452454, 'total_duration': 8056.826657772064, 'accumulated_submission_time': 7706.454968452454, 'accumulated_eval_time': 349.16700863838196, 'accumulated_logging_time': 0.44753503799438477, 'global_step': 22629, 'preemption_count': 0}), (24126, {'train/accuracy': 0.634785532951355, 'train/loss': 1.5688326358795166, 'validation/accuracy': 0.5852999687194824, 'validation/loss': 1.8004077672958374, 'validation/num_examples': 50000, 'test/accuracy': 0.46480002999305725, 'test/loss': 2.4927031993865967, 'test/num_examples': 10000, 'score': 8216.385123491287, 'total_duration': 8591.48088502884, 'accumulated_submission_time': 8216.385123491287, 'accumulated_eval_time': 373.81429505348206, 'accumulated_logging_time': 0.4732851982116699, 'global_step': 24126, 'preemption_count': 0}), (25638, {'train/accuracy': 0.6300820708274841, 'train/loss': 1.6126160621643066, 'validation/accuracy': 0.5874999761581421, 'validation/loss': 1.8088067770004272, 'validation/num_examples': 50000, 'test/accuracy': 0.4674000144004822, 'test/loss': 2.484600067138672, 'test/num_examples': 10000, 'score': 8726.36254477501, 'total_duration': 9125.590382575989, 'accumulated_submission_time': 8726.36254477501, 'accumulated_eval_time': 397.86784505844116, 'accumulated_logging_time': 0.5002624988555908, 'global_step': 25638, 'preemption_count': 0}), (27151, {'train/accuracy': 0.6246811151504517, 'train/loss': 1.6491903066635132, 'validation/accuracy': 0.5797399878501892, 'validation/loss': 1.8536149263381958, 'validation/num_examples': 50000, 'test/accuracy': 0.4571000337600708, 'test/loss': 2.5513482093811035, 'test/num_examples': 10000, 'score': 9236.353283643723, 'total_duration': 9659.236914873123, 'accumulated_submission_time': 9236.353283643723, 'accumulated_eval_time': 421.4299862384796, 'accumulated_logging_time': 0.5421044826507568, 'global_step': 27151, 'preemption_count': 0}), (28663, {'train/accuracy': 0.6515266299247742, 'train/loss': 1.5246480703353882, 'validation/accuracy': 0.590399980545044, 'validation/loss': 1.8066246509552002, 'validation/num_examples': 50000, 'test/accuracy': 0.4710000157356262, 'test/loss': 2.4751579761505127, 'test/num_examples': 10000, 'score': 9746.119905948639, 'total_duration': 10194.557493686676, 'accumulated_submission_time': 9746.119905948639, 'accumulated_eval_time': 446.6768915653229, 'accumulated_logging_time': 0.7980978488922119, 'global_step': 28663, 'preemption_count': 0}), (30176, {'train/accuracy': 0.6483777165412903, 'train/loss': 1.5167217254638672, 'validation/accuracy': 0.5963599681854248, 'validation/loss': 1.7699456214904785, 'validation/num_examples': 50000, 'test/accuracy': 0.47130003571510315, 'test/loss': 2.4669439792633057, 'test/num_examples': 10000, 'score': 10256.066632509232, 'total_duration': 10729.63278746605, 'accumulated_submission_time': 10256.066632509232, 'accumulated_eval_time': 471.7249677181244, 'accumulated_logging_time': 0.8278708457946777, 'global_step': 30176, 'preemption_count': 0}), (31689, {'train/accuracy': 0.6477000713348389, 'train/loss': 1.5327821969985962, 'validation/accuracy': 0.6036799550056458, 'validation/loss': 1.7479439973831177, 'validation/num_examples': 50000, 'test/accuracy': 0.47690001130104065, 'test/loss': 2.4278860092163086, 'test/num_examples': 10000, 'score': 10766.203356981277, 'total_duration': 11265.862180709839, 'accumulated_submission_time': 10766.203356981277, 'accumulated_eval_time': 497.73553681373596, 'accumulated_logging_time': 0.8581020832061768, 'global_step': 31689, 'preemption_count': 0}), (33203, {'train/accuracy': 0.6440529227256775, 'train/loss': 1.5270545482635498, 'validation/accuracy': 0.6020199656486511, 'validation/loss': 1.7369011640548706, 'validation/num_examples': 50000, 'test/accuracy': 0.4764000177383423, 'test/loss': 2.4197638034820557, 'test/num_examples': 10000, 'score': 11276.440303564072, 'total_duration': 11801.382081270218, 'accumulated_submission_time': 11276.440303564072, 'accumulated_eval_time': 522.9339916706085, 'accumulated_logging_time': 0.8919429779052734, 'global_step': 33203, 'preemption_count': 0}), (34715, {'train/accuracy': 0.6390106678009033, 'train/loss': 1.5380759239196777, 'validation/accuracy': 0.5995799899101257, 'validation/loss': 1.7401143312454224, 'validation/num_examples': 50000, 'test/accuracy': 0.4750000238418579, 'test/loss': 2.4321818351745605, 'test/num_examples': 10000, 'score': 11786.374200820923, 'total_duration': 12336.523176193237, 'accumulated_submission_time': 11786.374200820923, 'accumulated_eval_time': 548.0605285167694, 'accumulated_logging_time': 0.9209282398223877, 'global_step': 34715, 'preemption_count': 0}), (36228, {'train/accuracy': 0.6710578799247742, 'train/loss': 1.4383693933486938, 'validation/accuracy': 0.6104999780654907, 'validation/loss': 1.7170253992080688, 'validation/num_examples': 50000, 'test/accuracy': 0.4812000095844269, 'test/loss': 2.404179334640503, 'test/num_examples': 10000, 'score': 12296.366559743881, 'total_duration': 12872.187220096588, 'accumulated_submission_time': 12296.366559743881, 'accumulated_eval_time': 573.6493542194366, 'accumulated_logging_time': 0.9530339241027832, 'global_step': 36228, 'preemption_count': 0}), (37741, {'train/accuracy': 0.6660754084587097, 'train/loss': 1.4482871294021606, 'validation/accuracy': 0.6075599789619446, 'validation/loss': 1.733280897140503, 'validation/num_examples': 50000, 'test/accuracy': 0.47540003061294556, 'test/loss': 2.427032232284546, 'test/num_examples': 10000, 'score': 12806.337379455566, 'total_duration': 13405.956619262695, 'accumulated_submission_time': 12806.337379455566, 'accumulated_eval_time': 597.3628311157227, 'accumulated_logging_time': 0.9872357845306396, 'global_step': 37741, 'preemption_count': 0}), (39255, {'train/accuracy': 0.6478993892669678, 'train/loss': 1.5248180627822876, 'validation/accuracy': 0.5951799750328064, 'validation/loss': 1.7707302570343018, 'validation/num_examples': 50000, 'test/accuracy': 0.47690001130104065, 'test/loss': 2.424974203109741, 'test/num_examples': 10000, 'score': 13316.273628473282, 'total_duration': 13939.578382253647, 'accumulated_submission_time': 13316.273628473282, 'accumulated_eval_time': 620.9647953510284, 'accumulated_logging_time': 1.0185627937316895, 'global_step': 39255, 'preemption_count': 0}), (40769, {'train/accuracy': 0.6623883843421936, 'train/loss': 1.4788810014724731, 'validation/accuracy': 0.6086199879646301, 'validation/loss': 1.71470308303833, 'validation/num_examples': 50000, 'test/accuracy': 0.4877000153064728, 'test/loss': 2.3885064125061035, 'test/num_examples': 10000, 'score': 13826.47026848793, 'total_duration': 14473.0782289505, 'accumulated_submission_time': 13826.47026848793, 'accumulated_eval_time': 644.1830842494965, 'accumulated_logging_time': 1.0521259307861328, 'global_step': 40769, 'preemption_count': 0}), (42284, {'train/accuracy': 0.6502909660339355, 'train/loss': 1.5233001708984375, 'validation/accuracy': 0.6042400002479553, 'validation/loss': 1.73208749294281, 'validation/num_examples': 50000, 'test/accuracy': 0.48020002245903015, 'test/loss': 2.396982431411743, 'test/num_examples': 10000, 'score': 14336.686334371567, 'total_duration': 15012.248512983322, 'accumulated_submission_time': 14336.686334371567, 'accumulated_eval_time': 673.0554871559143, 'accumulated_logging_time': 1.0817155838012695, 'global_step': 42284, 'preemption_count': 0}), (43798, {'train/accuracy': 0.6564692258834839, 'train/loss': 1.4994585514068604, 'validation/accuracy': 0.6126999855041504, 'validation/loss': 1.6944860219955444, 'validation/num_examples': 50000, 'test/accuracy': 0.48920002579689026, 'test/loss': 2.3520050048828125, 'test/num_examples': 10000, 'score': 14846.899607419968, 'total_duration': 15545.86763715744, 'accumulated_submission_time': 14846.899607419968, 'accumulated_eval_time': 696.379273891449, 'accumulated_logging_time': 1.1133880615234375, 'global_step': 43798, 'preemption_count': 0}), (45312, {'train/accuracy': 0.6901307106018066, 'train/loss': 1.335003137588501, 'validation/accuracy': 0.60971999168396, 'validation/loss': 1.702130913734436, 'validation/num_examples': 50000, 'test/accuracy': 0.4855000376701355, 'test/loss': 2.367635488510132, 'test/num_examples': 10000, 'score': 15356.965127944946, 'total_duration': 16078.829252958298, 'accumulated_submission_time': 15356.965127944946, 'accumulated_eval_time': 719.1942150592804, 'accumulated_logging_time': 1.1434450149536133, 'global_step': 45312, 'preemption_count': 0}), (46826, {'train/accuracy': 0.6707788705825806, 'train/loss': 1.4408752918243408, 'validation/accuracy': 0.6131199598312378, 'validation/loss': 1.7045730352401733, 'validation/num_examples': 50000, 'test/accuracy': 0.48520001769065857, 'test/loss': 2.384239435195923, 'test/num_examples': 10000, 'score': 15867.113546609879, 'total_duration': 16609.47766971588, 'accumulated_submission_time': 15867.113546609879, 'accumulated_eval_time': 739.6124730110168, 'accumulated_logging_time': 1.1734073162078857, 'global_step': 46826, 'preemption_count': 0}), (48340, {'train/accuracy': 0.6635841727256775, 'train/loss': 1.4390766620635986, 'validation/accuracy': 0.6102799773216248, 'validation/loss': 1.6896299123764038, 'validation/num_examples': 50000, 'test/accuracy': 0.4865000247955322, 'test/loss': 2.390916585922241, 'test/num_examples': 10000, 'score': 16377.163805484772, 'total_duration': 17139.047789812088, 'accumulated_submission_time': 16377.163805484772, 'accumulated_eval_time': 759.0497002601624, 'accumulated_logging_time': 1.2045924663543701, 'global_step': 48340, 'preemption_count': 0}), (49854, {'train/accuracy': 0.6621492505073547, 'train/loss': 1.4692184925079346, 'validation/accuracy': 0.6146799921989441, 'validation/loss': 1.694058895111084, 'validation/num_examples': 50000, 'test/accuracy': 0.4975000321865082, 'test/loss': 2.350130081176758, 'test/num_examples': 10000, 'score': 16887.197038412094, 'total_duration': 17667.17898607254, 'accumulated_submission_time': 16887.197038412094, 'accumulated_eval_time': 777.0617418289185, 'accumulated_logging_time': 1.239149808883667, 'global_step': 49854, 'preemption_count': 0}), (51369, {'train/accuracy': 0.6635642647743225, 'train/loss': 1.439455270767212, 'validation/accuracy': 0.6191399693489075, 'validation/loss': 1.646724820137024, 'validation/num_examples': 50000, 'test/accuracy': 0.4969000220298767, 'test/loss': 2.307199478149414, 'test/num_examples': 10000, 'score': 17397.333253145218, 'total_duration': 18195.217745542526, 'accumulated_submission_time': 17397.333253145218, 'accumulated_eval_time': 794.8757519721985, 'accumulated_logging_time': 1.2756733894348145, 'global_step': 51369, 'preemption_count': 0}), (52884, {'train/accuracy': 0.6588408946990967, 'train/loss': 1.4924957752227783, 'validation/accuracy': 0.6126799583435059, 'validation/loss': 1.7022305727005005, 'validation/num_examples': 50000, 'test/accuracy': 0.4881000220775604, 'test/loss': 2.357887029647827, 'test/num_examples': 10000, 'score': 17907.54860687256, 'total_duration': 18723.197404146194, 'accumulated_submission_time': 17907.54860687256, 'accumulated_eval_time': 812.5476040840149, 'accumulated_logging_time': 1.3171625137329102, 'global_step': 52884, 'preemption_count': 0}), (54399, {'train/accuracy': 0.7075693607330322, 'train/loss': 1.2901153564453125, 'validation/accuracy': 0.625819981098175, 'validation/loss': 1.6629422903060913, 'validation/num_examples': 50000, 'test/accuracy': 0.5001000165939331, 'test/loss': 2.3184971809387207, 'test/num_examples': 10000, 'score': 18417.55141377449, 'total_duration': 19250.939685344696, 'accumulated_submission_time': 18417.55141377449, 'accumulated_eval_time': 830.1899147033691, 'accumulated_logging_time': 1.362135410308838, 'global_step': 54399, 'preemption_count': 0}), (55913, {'train/accuracy': 0.6686663031578064, 'train/loss': 1.4696898460388184, 'validation/accuracy': 0.6124399900436401, 'validation/loss': 1.7320475578308105, 'validation/num_examples': 50000, 'test/accuracy': 0.4935000240802765, 'test/loss': 2.397702932357788, 'test/num_examples': 10000, 'score': 18927.4927611351, 'total_duration': 19778.73899126053, 'accumulated_submission_time': 18927.4927611351, 'accumulated_eval_time': 847.9586672782898, 'accumulated_logging_time': 1.3999717235565186, 'global_step': 55913, 'preemption_count': 0}), (57428, {'train/accuracy': 0.6722536683082581, 'train/loss': 1.435630440711975, 'validation/accuracy': 0.6198399662971497, 'validation/loss': 1.6725577116012573, 'validation/num_examples': 50000, 'test/accuracy': 0.5006000399589539, 'test/loss': 2.3415465354919434, 'test/num_examples': 10000, 'score': 19437.532512187958, 'total_duration': 20306.21404647827, 'accumulated_submission_time': 19437.532512187958, 'accumulated_eval_time': 865.3042812347412, 'accumulated_logging_time': 1.4369032382965088, 'global_step': 57428, 'preemption_count': 0}), (58942, {'train/accuracy': 0.6779735088348389, 'train/loss': 1.4131288528442383, 'validation/accuracy': 0.6226599812507629, 'validation/loss': 1.664193868637085, 'validation/num_examples': 50000, 'test/accuracy': 0.5063000321388245, 'test/loss': 2.299123764038086, 'test/num_examples': 10000, 'score': 19947.70263171196, 'total_duration': 20833.81520462036, 'accumulated_submission_time': 19947.70263171196, 'accumulated_eval_time': 882.6476812362671, 'accumulated_logging_time': 1.4736227989196777, 'global_step': 58942, 'preemption_count': 0}), (60457, {'train/accuracy': 0.6662946343421936, 'train/loss': 1.4448381662368774, 'validation/accuracy': 0.6218199729919434, 'validation/loss': 1.656185269355774, 'validation/num_examples': 50000, 'test/accuracy': 0.49400001764297485, 'test/loss': 2.3347575664520264, 'test/num_examples': 10000, 'score': 20457.742354631424, 'total_duration': 21361.444463968277, 'accumulated_submission_time': 20457.742354631424, 'accumulated_eval_time': 900.1488373279572, 'accumulated_logging_time': 1.5096936225891113, 'global_step': 60457, 'preemption_count': 0}), (61971, {'train/accuracy': 0.6689453125, 'train/loss': 1.4233334064483643, 'validation/accuracy': 0.6264599561691284, 'validation/loss': 1.6362653970718384, 'validation/num_examples': 50000, 'test/accuracy': 0.5004000067710876, 'test/loss': 2.3209445476531982, 'test/num_examples': 10000, 'score': 20967.97685956955, 'total_duration': 21889.169400691986, 'accumulated_submission_time': 20967.97685956955, 'accumulated_eval_time': 917.5497057437897, 'accumulated_logging_time': 1.5463056564331055, 'global_step': 61971, 'preemption_count': 0}), (63485, {'train/accuracy': 0.7110969424247742, 'train/loss': 1.2208093404769897, 'validation/accuracy': 0.6271799802780151, 'validation/loss': 1.6009138822555542, 'validation/num_examples': 50000, 'test/accuracy': 0.5062000155448914, 'test/loss': 2.2677626609802246, 'test/num_examples': 10000, 'score': 21477.900118112564, 'total_duration': 22416.847967386246, 'accumulated_submission_time': 21477.900118112564, 'accumulated_eval_time': 935.2121860980988, 'accumulated_logging_time': 1.5857601165771484, 'global_step': 63485, 'preemption_count': 0}), (65000, {'train/accuracy': 0.6969068646430969, 'train/loss': 1.2933167219161987, 'validation/accuracy': 0.6342200040817261, 'validation/loss': 1.5750954151153564, 'validation/num_examples': 50000, 'test/accuracy': 0.5092000365257263, 'test/loss': 2.258240222930908, 'test/num_examples': 10000, 'score': 21988.11581516266, 'total_duration': 22944.586437940598, 'accumulated_submission_time': 21988.11581516266, 'accumulated_eval_time': 952.6368882656097, 'accumulated_logging_time': 1.6321525573730469, 'global_step': 65000, 'preemption_count': 0}), (66515, {'train/accuracy': 0.6873405575752258, 'train/loss': 1.344896912574768, 'validation/accuracy': 0.632099986076355, 'validation/loss': 1.5866305828094482, 'validation/num_examples': 50000, 'test/accuracy': 0.5099000334739685, 'test/loss': 2.246936559677124, 'test/num_examples': 10000, 'score': 22498.35578727722, 'total_duration': 23472.325043201447, 'accumulated_submission_time': 22498.35578727722, 'accumulated_eval_time': 970.0448710918427, 'accumulated_logging_time': 1.6718873977661133, 'global_step': 66515, 'preemption_count': 0}), (68030, {'train/accuracy': 0.6845503449440002, 'train/loss': 1.392478346824646, 'validation/accuracy': 0.6325799822807312, 'validation/loss': 1.6152527332305908, 'validation/num_examples': 50000, 'test/accuracy': 0.5042000412940979, 'test/loss': 2.301208734512329, 'test/num_examples': 10000, 'score': 23008.404548883438, 'total_duration': 23999.782874584198, 'accumulated_submission_time': 23008.404548883438, 'accumulated_eval_time': 987.3612501621246, 'accumulated_logging_time': 1.7124810218811035, 'global_step': 68030, 'preemption_count': 0}), (69544, {'train/accuracy': 0.6779336333274841, 'train/loss': 1.4261982440948486, 'validation/accuracy': 0.6293999552726746, 'validation/loss': 1.6455281972885132, 'validation/num_examples': 50000, 'test/accuracy': 0.5063000321388245, 'test/loss': 2.339195966720581, 'test/num_examples': 10000, 'score': 23518.33584046364, 'total_duration': 24527.454204559326, 'accumulated_submission_time': 23518.33584046364, 'accumulated_eval_time': 1005.0094563961029, 'accumulated_logging_time': 1.7519042491912842, 'global_step': 69544, 'preemption_count': 0}), (71059, {'train/accuracy': 0.6732102632522583, 'train/loss': 1.407064437866211, 'validation/accuracy': 0.6301599740982056, 'validation/loss': 1.6089874505996704, 'validation/num_examples': 50000, 'test/accuracy': 0.49820002913475037, 'test/loss': 2.287114143371582, 'test/num_examples': 10000, 'score': 24028.55358481407, 'total_duration': 25055.36220574379, 'accumulated_submission_time': 24028.55358481407, 'accumulated_eval_time': 1022.608335018158, 'accumulated_logging_time': 1.7905707359313965, 'global_step': 71059, 'preemption_count': 0}), (72574, {'train/accuracy': 0.7083266973495483, 'train/loss': 1.2449584007263184, 'validation/accuracy': 0.6342399716377258, 'validation/loss': 1.5873527526855469, 'validation/num_examples': 50000, 'test/accuracy': 0.5004000067710876, 'test/loss': 2.2946672439575195, 'test/num_examples': 10000, 'score': 24538.758437633514, 'total_duration': 25584.37909555435, 'accumulated_submission_time': 24538.758437633514, 'accumulated_eval_time': 1041.3291449546814, 'accumulated_logging_time': 1.8296470642089844, 'global_step': 72574, 'preemption_count': 0}), (74089, {'train/accuracy': 0.6872608065605164, 'train/loss': 1.3399715423583984, 'validation/accuracy': 0.6256399750709534, 'validation/loss': 1.6160084009170532, 'validation/num_examples': 50000, 'test/accuracy': 0.5052000284194946, 'test/loss': 2.28117036819458, 'test/num_examples': 10000, 'score': 25048.81888151169, 'total_duration': 26112.01877140999, 'accumulated_submission_time': 25048.81888151169, 'accumulated_eval_time': 1058.8174991607666, 'accumulated_logging_time': 1.8682844638824463, 'global_step': 74089, 'preemption_count': 0}), (75604, {'train/accuracy': 0.7000358700752258, 'train/loss': 1.2867945432662964, 'validation/accuracy': 0.6447199583053589, 'validation/loss': 1.5424926280975342, 'validation/num_examples': 50000, 'test/accuracy': 0.5152000188827515, 'test/loss': 2.232867956161499, 'test/num_examples': 10000, 'score': 25558.904240608215, 'total_duration': 26640.29567885399, 'accumulated_submission_time': 25558.904240608215, 'accumulated_eval_time': 1076.917620420456, 'accumulated_logging_time': 1.9076271057128906, 'global_step': 75604, 'preemption_count': 0}), (77119, {'train/accuracy': 0.6952527165412903, 'train/loss': 1.321738600730896, 'validation/accuracy': 0.647599995136261, 'validation/loss': 1.5462092161178589, 'validation/num_examples': 50000, 'test/accuracy': 0.5154000520706177, 'test/loss': 2.2142727375030518, 'test/num_examples': 10000, 'score': 26069.08917927742, 'total_duration': 27168.2249045372, 'accumulated_submission_time': 26069.08917927742, 'accumulated_eval_time': 1094.5691964626312, 'accumulated_logging_time': 1.9482781887054443, 'global_step': 77119, 'preemption_count': 0}), (78632, {'train/accuracy': 0.6901904940605164, 'train/loss': 1.3460441827774048, 'validation/accuracy': 0.6373400092124939, 'validation/loss': 1.582018494606018, 'validation/num_examples': 50000, 'test/accuracy': 0.5111000537872314, 'test/loss': 2.251178503036499, 'test/num_examples': 10000, 'score': 26578.311138153076, 'total_duration': 27696.523701667786, 'accumulated_submission_time': 26578.311138153076, 'accumulated_eval_time': 1112.730740070343, 'accumulated_logging_time': 2.8109002113342285, 'global_step': 78632, 'preemption_count': 0}), (80148, {'train/accuracy': 0.6938576102256775, 'train/loss': 1.3477110862731934, 'validation/accuracy': 0.6477599740028381, 'validation/loss': 1.5738341808319092, 'validation/num_examples': 50000, 'test/accuracy': 0.5189000368118286, 'test/loss': 2.2456023693084717, 'test/num_examples': 10000, 'score': 27088.510328292847, 'total_duration': 28224.07705426216, 'accumulated_submission_time': 27088.510328292847, 'accumulated_eval_time': 1129.9987313747406, 'accumulated_logging_time': 2.8450143337249756, 'global_step': 80148, 'preemption_count': 0}), (81663, {'train/accuracy': 0.7150828838348389, 'train/loss': 1.2265141010284424, 'validation/accuracy': 0.6447399854660034, 'validation/loss': 1.5567433834075928, 'validation/num_examples': 50000, 'test/accuracy': 0.5138000249862671, 'test/loss': 2.23746657371521, 'test/num_examples': 10000, 'score': 27598.556184768677, 'total_duration': 28751.632429361343, 'accumulated_submission_time': 27598.556184768677, 'accumulated_eval_time': 1147.4161262512207, 'accumulated_logging_time': 2.885287284851074, 'global_step': 81663, 'preemption_count': 0}), (83177, {'train/accuracy': 0.7067522406578064, 'train/loss': 1.2656277418136597, 'validation/accuracy': 0.6476799845695496, 'validation/loss': 1.5454624891281128, 'validation/num_examples': 50000, 'test/accuracy': 0.5205000042915344, 'test/loss': 2.2279982566833496, 'test/num_examples': 10000, 'score': 28108.67711853981, 'total_duration': 29279.347688674927, 'accumulated_submission_time': 28108.67711853981, 'accumulated_eval_time': 1164.917008638382, 'accumulated_logging_time': 2.9261295795440674, 'global_step': 83177, 'preemption_count': 0}), (84692, {'train/accuracy': 0.70804762840271, 'train/loss': 1.2592806816101074, 'validation/accuracy': 0.6536399722099304, 'validation/loss': 1.508924961090088, 'validation/num_examples': 50000, 'test/accuracy': 0.526900053024292, 'test/loss': 2.189680576324463, 'test/num_examples': 10000, 'score': 28618.81132388115, 'total_duration': 29807.00520181656, 'accumulated_submission_time': 28618.81132388115, 'accumulated_eval_time': 1182.3399450778961, 'accumulated_logging_time': 2.973907709121704, 'global_step': 84692, 'preemption_count': 0}), (86207, {'train/accuracy': 0.7067123651504517, 'train/loss': 1.260049819946289, 'validation/accuracy': 0.653659999370575, 'validation/loss': 1.5093188285827637, 'validation/num_examples': 50000, 'test/accuracy': 0.5340999960899353, 'test/loss': 2.161987543106079, 'test/num_examples': 10000, 'score': 29128.871512889862, 'total_duration': 30334.796818733215, 'accumulated_submission_time': 29128.871512889862, 'accumulated_eval_time': 1199.9780325889587, 'accumulated_logging_time': 3.0153017044067383, 'global_step': 86207, 'preemption_count': 0}), (87721, {'train/accuracy': 0.7043008208274841, 'train/loss': 1.2704253196716309, 'validation/accuracy': 0.6558200120925903, 'validation/loss': 1.510108232498169, 'validation/num_examples': 50000, 'test/accuracy': 0.5281000137329102, 'test/loss': 2.171442985534668, 'test/num_examples': 10000, 'score': 29638.966315984726, 'total_duration': 30862.28338265419, 'accumulated_submission_time': 29638.966315984726, 'accumulated_eval_time': 1217.268765926361, 'accumulated_logging_time': 3.0644567012786865, 'global_step': 87721, 'preemption_count': 0}), (89236, {'train/accuracy': 0.7318239808082581, 'train/loss': 1.1713844537734985, 'validation/accuracy': 0.6634599566459656, 'validation/loss': 1.469886064529419, 'validation/num_examples': 50000, 'test/accuracy': 0.5405000448226929, 'test/loss': 2.117152214050293, 'test/num_examples': 10000, 'score': 30149.158131837845, 'total_duration': 31389.92377448082, 'accumulated_submission_time': 30149.158131837845, 'accumulated_eval_time': 1234.6226682662964, 'accumulated_logging_time': 3.106571912765503, 'global_step': 89236, 'preemption_count': 0}), (90751, {'train/accuracy': 0.724609375, 'train/loss': 1.1772572994232178, 'validation/accuracy': 0.6525599956512451, 'validation/loss': 1.5016096830368042, 'validation/num_examples': 50000, 'test/accuracy': 0.5282000303268433, 'test/loss': 2.163940191268921, 'test/num_examples': 10000, 'score': 30659.136559963226, 'total_duration': 31917.365456819534, 'accumulated_submission_time': 30659.136559963226, 'accumulated_eval_time': 1251.9876792430878, 'accumulated_logging_time': 3.152724266052246, 'global_step': 90751, 'preemption_count': 0}), (92266, {'train/accuracy': 0.7249082922935486, 'train/loss': 1.186362624168396, 'validation/accuracy': 0.6613199710845947, 'validation/loss': 1.461118459701538, 'validation/num_examples': 50000, 'test/accuracy': 0.541700005531311, 'test/loss': 2.0914676189422607, 'test/num_examples': 10000, 'score': 31169.082045078278, 'total_duration': 32444.939204216003, 'accumulated_submission_time': 31169.082045078278, 'accumulated_eval_time': 1269.5212585926056, 'accumulated_logging_time': 3.195277690887451, 'global_step': 92266, 'preemption_count': 0}), (93780, {'train/accuracy': 0.7208425998687744, 'train/loss': 1.238020896911621, 'validation/accuracy': 0.6592599749565125, 'validation/loss': 1.504926085472107, 'validation/num_examples': 50000, 'test/accuracy': 0.5371000170707703, 'test/loss': 2.135526180267334, 'test/num_examples': 10000, 'score': 31679.052606105804, 'total_duration': 32972.216052532196, 'accumulated_submission_time': 31679.052606105804, 'accumulated_eval_time': 1286.7298786640167, 'accumulated_logging_time': 3.240107774734497, 'global_step': 93780, 'preemption_count': 0}), (95294, {'train/accuracy': 0.7150231003761292, 'train/loss': 1.20890474319458, 'validation/accuracy': 0.6631799936294556, 'validation/loss': 1.4594324827194214, 'validation/num_examples': 50000, 'test/accuracy': 0.5412000417709351, 'test/loss': 2.117387533187866, 'test/num_examples': 10000, 'score': 32189.051352262497, 'total_duration': 33499.531002283096, 'accumulated_submission_time': 32189.051352262497, 'accumulated_eval_time': 1303.9475963115692, 'accumulated_logging_time': 3.285914421081543, 'global_step': 95294, 'preemption_count': 0}), (96808, {'train/accuracy': 0.721101701259613, 'train/loss': 1.1796294450759888, 'validation/accuracy': 0.6640200018882751, 'validation/loss': 1.435350775718689, 'validation/num_examples': 50000, 'test/accuracy': 0.5457000136375427, 'test/loss': 2.0837349891662598, 'test/num_examples': 10000, 'score': 32699.011062145233, 'total_duration': 34027.04830813408, 'accumulated_submission_time': 32699.011062145233, 'accumulated_eval_time': 1321.4094231128693, 'accumulated_logging_time': 3.3290951251983643, 'global_step': 96808, 'preemption_count': 0}), (98323, {'train/accuracy': 0.7517338991165161, 'train/loss': 1.0737617015838623, 'validation/accuracy': 0.663100004196167, 'validation/loss': 1.4655168056488037, 'validation/num_examples': 50000, 'test/accuracy': 0.5379000306129456, 'test/loss': 2.1180527210235596, 'test/num_examples': 10000, 'score': 33209.20894575119, 'total_duration': 34554.696957588196, 'accumulated_submission_time': 33209.20894575119, 'accumulated_eval_time': 1338.7655036449432, 'accumulated_logging_time': 3.3717434406280518, 'global_step': 98323, 'preemption_count': 0}), (99838, {'train/accuracy': 0.7379822731018066, 'train/loss': 1.116407036781311, 'validation/accuracy': 0.6669999957084656, 'validation/loss': 1.4401804208755493, 'validation/num_examples': 50000, 'test/accuracy': 0.5509999990463257, 'test/loss': 2.067492961883545, 'test/num_examples': 10000, 'score': 33719.22984600067, 'total_duration': 35082.03232550621, 'accumulated_submission_time': 33719.22984600067, 'accumulated_eval_time': 1355.9846813678741, 'accumulated_logging_time': 3.415469169616699, 'global_step': 99838, 'preemption_count': 0}), (101353, {'train/accuracy': 0.7453961968421936, 'train/loss': 1.0961382389068604, 'validation/accuracy': 0.676099956035614, 'validation/loss': 1.4047099351882935, 'validation/num_examples': 50000, 'test/accuracy': 0.5529000163078308, 'test/loss': 2.0516908168792725, 'test/num_examples': 10000, 'score': 34229.33446264267, 'total_duration': 35609.674355983734, 'accumulated_submission_time': 34229.33446264267, 'accumulated_eval_time': 1373.4264228343964, 'accumulated_logging_time': 3.4589407444000244, 'global_step': 101353, 'preemption_count': 0}), (102868, {'train/accuracy': 0.7364476919174194, 'train/loss': 1.114004135131836, 'validation/accuracy': 0.672980010509491, 'validation/loss': 1.399997353553772, 'validation/num_examples': 50000, 'test/accuracy': 0.5488000512123108, 'test/loss': 2.043478488922119, 'test/num_examples': 10000, 'score': 34739.40772628784, 'total_duration': 36137.24479365349, 'accumulated_submission_time': 34739.40772628784, 'accumulated_eval_time': 1390.8268103599548, 'accumulated_logging_time': 3.504430055618286, 'global_step': 102868, 'preemption_count': 0}), (104382, {'train/accuracy': 0.7351921200752258, 'train/loss': 1.1262609958648682, 'validation/accuracy': 0.6737599968910217, 'validation/loss': 1.4070687294006348, 'validation/num_examples': 50000, 'test/accuracy': 0.5547000169754028, 'test/loss': 2.052401065826416, 'test/num_examples': 10000, 'score': 35249.39946103096, 'total_duration': 36664.71185588837, 'accumulated_submission_time': 35249.39946103096, 'accumulated_eval_time': 1408.2055933475494, 'accumulated_logging_time': 3.548323154449463, 'global_step': 104382, 'preemption_count': 0}), (105897, {'train/accuracy': 0.7310666441917419, 'train/loss': 1.1510498523712158, 'validation/accuracy': 0.6747599840164185, 'validation/loss': 1.4065779447555542, 'validation/num_examples': 50000, 'test/accuracy': 0.5578000545501709, 'test/loss': 2.038433790206909, 'test/num_examples': 10000, 'score': 35759.47870969772, 'total_duration': 37192.232377290726, 'accumulated_submission_time': 35759.47870969772, 'accumulated_eval_time': 1425.5483980178833, 'accumulated_logging_time': 3.5949106216430664, 'global_step': 105897, 'preemption_count': 0}), (107412, {'train/accuracy': 0.7695910334587097, 'train/loss': 1.021224856376648, 'validation/accuracy': 0.6710999608039856, 'validation/loss': 1.4482934474945068, 'validation/num_examples': 50000, 'test/accuracy': 0.5497000217437744, 'test/loss': 2.1009111404418945, 'test/num_examples': 10000, 'score': 36269.60155582428, 'total_duration': 37719.8588643074, 'accumulated_submission_time': 36269.60155582428, 'accumulated_eval_time': 1442.9533438682556, 'accumulated_logging_time': 3.6414597034454346, 'global_step': 107412, 'preemption_count': 0}), (108927, {'train/accuracy': 0.7567362785339355, 'train/loss': 1.0428543090820312, 'validation/accuracy': 0.6768999695777893, 'validation/loss': 1.38744056224823, 'validation/num_examples': 50000, 'test/accuracy': 0.5545000433921814, 'test/loss': 2.0375123023986816, 'test/num_examples': 10000, 'score': 36779.734280347824, 'total_duration': 38247.51591229439, 'accumulated_submission_time': 36779.734280347824, 'accumulated_eval_time': 1460.3775732517242, 'accumulated_logging_time': 3.689704656600952, 'global_step': 108927, 'preemption_count': 0}), (110442, {'train/accuracy': 0.7382413744926453, 'train/loss': 1.1036720275878906, 'validation/accuracy': 0.6723799705505371, 'validation/loss': 1.4153038263320923, 'validation/num_examples': 50000, 'test/accuracy': 0.5482000112533569, 'test/loss': 2.050555467605591, 'test/num_examples': 10000, 'score': 37289.76195025444, 'total_duration': 38776.281270504, 'accumulated_submission_time': 37289.76195025444, 'accumulated_eval_time': 1479.0125722885132, 'accumulated_logging_time': 3.740776300430298, 'global_step': 110442, 'preemption_count': 0}), (111957, {'train/accuracy': 0.7502391338348389, 'train/loss': 1.0634797811508179, 'validation/accuracy': 0.6810599565505981, 'validation/loss': 1.3739310503005981, 'validation/num_examples': 50000, 'test/accuracy': 0.5591000318527222, 'test/loss': 2.00235652923584, 'test/num_examples': 10000, 'score': 37799.80076622963, 'total_duration': 39303.59298801422, 'accumulated_submission_time': 37799.80076622963, 'accumulated_eval_time': 1496.186547756195, 'accumulated_logging_time': 3.787414312362671, 'global_step': 111957, 'preemption_count': 0}), (113472, {'train/accuracy': 0.744559109210968, 'train/loss': 1.0819778442382812, 'validation/accuracy': 0.6834200024604797, 'validation/loss': 1.3549482822418213, 'validation/num_examples': 50000, 'test/accuracy': 0.5565000176429749, 'test/loss': 2.0129952430725098, 'test/num_examples': 10000, 'score': 38309.85378551483, 'total_duration': 39830.87760901451, 'accumulated_submission_time': 38309.85378551483, 'accumulated_eval_time': 1513.319462299347, 'accumulated_logging_time': 3.8344779014587402, 'global_step': 113472, 'preemption_count': 0}), (114987, {'train/accuracy': 0.7444595098495483, 'train/loss': 1.1149373054504395, 'validation/accuracy': 0.6838399767875671, 'validation/loss': 1.3911999464035034, 'validation/num_examples': 50000, 'test/accuracy': 0.5561000108718872, 'test/loss': 2.0544209480285645, 'test/num_examples': 10000, 'score': 38819.99199438095, 'total_duration': 40358.582310676575, 'accumulated_submission_time': 38819.99199438095, 'accumulated_eval_time': 1530.7830486297607, 'accumulated_logging_time': 3.8848836421966553, 'global_step': 114987, 'preemption_count': 0}), (116501, {'train/accuracy': 0.7858139276504517, 'train/loss': 0.9308143854141235, 'validation/accuracy': 0.6909199953079224, 'validation/loss': 1.3370513916015625, 'validation/num_examples': 50000, 'test/accuracy': 0.567300021648407, 'test/loss': 1.9916213750839233, 'test/num_examples': 10000, 'score': 39329.903483867645, 'total_duration': 40885.95380759239, 'accumulated_submission_time': 39329.903483867645, 'accumulated_eval_time': 1548.1430974006653, 'accumulated_logging_time': 3.932539224624634, 'global_step': 116501, 'preemption_count': 0}), (118016, {'train/accuracy': 0.7719228267669678, 'train/loss': 0.9736966490745544, 'validation/accuracy': 0.692799985408783, 'validation/loss': 1.3256369829177856, 'validation/num_examples': 50000, 'test/accuracy': 0.567300021648407, 'test/loss': 1.984636664390564, 'test/num_examples': 10000, 'score': 39840.01126456261, 'total_duration': 41413.576409339905, 'accumulated_submission_time': 39840.01126456261, 'accumulated_eval_time': 1565.5552134513855, 'accumulated_logging_time': 3.9830572605133057, 'global_step': 118016, 'preemption_count': 0}), (119531, {'train/accuracy': 0.7692123651504517, 'train/loss': 0.9980911016464233, 'validation/accuracy': 0.6963399648666382, 'validation/loss': 1.3163288831710815, 'validation/num_examples': 50000, 'test/accuracy': 0.5732000470161438, 'test/loss': 1.9624512195587158, 'test/num_examples': 10000, 'score': 40350.17130947113, 'total_duration': 41941.20932650566, 'accumulated_submission_time': 40350.17130947113, 'accumulated_eval_time': 1582.9293761253357, 'accumulated_logging_time': 4.029150485992432, 'global_step': 119531, 'preemption_count': 0}), (121046, {'train/accuracy': 0.7643494606018066, 'train/loss': 1.0025675296783447, 'validation/accuracy': 0.6955400109291077, 'validation/loss': 1.3097865581512451, 'validation/num_examples': 50000, 'test/accuracy': 0.5732000470161438, 'test/loss': 1.9472129344940186, 'test/num_examples': 10000, 'score': 40860.15645599365, 'total_duration': 42468.937943696976, 'accumulated_submission_time': 40860.15645599365, 'accumulated_eval_time': 1600.571870803833, 'accumulated_logging_time': 4.077746868133545, 'global_step': 121046, 'preemption_count': 0}), (122561, {'train/accuracy': 0.7720224857330322, 'train/loss': 0.9792758822441101, 'validation/accuracy': 0.7010399699211121, 'validation/loss': 1.279943585395813, 'validation/num_examples': 50000, 'test/accuracy': 0.5808000564575195, 'test/loss': 1.9124568700790405, 'test/num_examples': 10000, 'score': 41370.070234537125, 'total_duration': 42996.46684598923, 'accumulated_submission_time': 41370.070234537125, 'accumulated_eval_time': 1618.0866899490356, 'accumulated_logging_time': 4.125354290008545, 'global_step': 122561, 'preemption_count': 0}), (124076, {'train/accuracy': 0.7723413705825806, 'train/loss': 0.971625566482544, 'validation/accuracy': 0.7017599940299988, 'validation/loss': 1.276233196258545, 'validation/num_examples': 50000, 'test/accuracy': 0.5755000114440918, 'test/loss': 1.924700140953064, 'test/num_examples': 10000, 'score': 41880.03378677368, 'total_duration': 43523.94673323631, 'accumulated_submission_time': 41880.03378677368, 'accumulated_eval_time': 1635.502154827118, 'accumulated_logging_time': 4.173144578933716, 'global_step': 124076, 'preemption_count': 0}), (125591, {'train/accuracy': 0.7950015664100647, 'train/loss': 0.878669023513794, 'validation/accuracy': 0.6998400092124939, 'validation/loss': 1.3009834289550781, 'validation/num_examples': 50000, 'test/accuracy': 0.5813000202178955, 'test/loss': 1.9170032739639282, 'test/num_examples': 10000, 'score': 42390.04211616516, 'total_duration': 44052.20104932785, 'accumulated_submission_time': 42390.04211616516, 'accumulated_eval_time': 1653.6435549259186, 'accumulated_logging_time': 4.225467681884766, 'global_step': 125591, 'preemption_count': 0}), (127106, {'train/accuracy': 0.78714919090271, 'train/loss': 0.9083539247512817, 'validation/accuracy': 0.7061799764633179, 'validation/loss': 1.2675763368606567, 'validation/num_examples': 50000, 'test/accuracy': 0.5827000141143799, 'test/loss': 1.9058300256729126, 'test/num_examples': 10000, 'score': 42900.01652574539, 'total_duration': 44579.797763586044, 'accumulated_submission_time': 42900.01652574539, 'accumulated_eval_time': 1671.1585881710052, 'accumulated_logging_time': 4.279773235321045, 'global_step': 127106, 'preemption_count': 0}), (128618, {'train/accuracy': 0.7895806431770325, 'train/loss': 0.9128103852272034, 'validation/accuracy': 0.7077999711036682, 'validation/loss': 1.2611401081085205, 'validation/num_examples': 50000, 'test/accuracy': 0.5888000130653381, 'test/loss': 1.886717438697815, 'test/num_examples': 10000, 'score': 43409.1985976696, 'total_duration': 45107.111483335495, 'accumulated_submission_time': 43409.1985976696, 'accumulated_eval_time': 1688.4513931274414, 'accumulated_logging_time': 5.066087961196899, 'global_step': 128618, 'preemption_count': 0}), (130133, {'train/accuracy': 0.7902981638908386, 'train/loss': 0.9130910634994507, 'validation/accuracy': 0.711899995803833, 'validation/loss': 1.2529252767562866, 'validation/num_examples': 50000, 'test/accuracy': 0.5927000045776367, 'test/loss': 1.862455129623413, 'test/num_examples': 10000, 'score': 43919.28074002266, 'total_duration': 45634.932916879654, 'accumulated_submission_time': 43919.28074002266, 'accumulated_eval_time': 1706.0879232883453, 'accumulated_logging_time': 5.117692470550537, 'global_step': 130133, 'preemption_count': 0}), (131648, {'train/accuracy': 0.7849569320678711, 'train/loss': 0.9189361333847046, 'validation/accuracy': 0.7110599875450134, 'validation/loss': 1.247543454170227, 'validation/num_examples': 50000, 'test/accuracy': 0.5861000418663025, 'test/loss': 1.8845510482788086, 'test/num_examples': 10000, 'score': 44429.30247211456, 'total_duration': 46162.18374609947, 'accumulated_submission_time': 44429.30247211456, 'accumulated_eval_time': 1723.184979915619, 'accumulated_logging_time': 5.197498321533203, 'global_step': 131648, 'preemption_count': 0}), (133163, {'train/accuracy': 0.7988081574440002, 'train/loss': 0.8635379076004028, 'validation/accuracy': 0.7162399888038635, 'validation/loss': 1.2158832550048828, 'validation/num_examples': 50000, 'test/accuracy': 0.5948000550270081, 'test/loss': 1.8518083095550537, 'test/num_examples': 10000, 'score': 44939.4055583477, 'total_duration': 46689.57337188721, 'accumulated_submission_time': 44939.4055583477, 'accumulated_eval_time': 1740.367802619934, 'accumulated_logging_time': 5.248680830001831, 'global_step': 133163, 'preemption_count': 0}), (134678, {'train/accuracy': 0.8146922588348389, 'train/loss': 0.8167023658752441, 'validation/accuracy': 0.7167999744415283, 'validation/loss': 1.2303428649902344, 'validation/num_examples': 50000, 'test/accuracy': 0.5956000089645386, 'test/loss': 1.8589153289794922, 'test/num_examples': 10000, 'score': 45449.53480505943, 'total_duration': 47217.12211894989, 'accumulated_submission_time': 45449.53480505943, 'accumulated_eval_time': 1757.6859464645386, 'accumulated_logging_time': 5.297849655151367, 'global_step': 134678, 'preemption_count': 0}), (136193, {'train/accuracy': 0.812519907951355, 'train/loss': 0.822140634059906, 'validation/accuracy': 0.7186399698257446, 'validation/loss': 1.2230992317199707, 'validation/num_examples': 50000, 'test/accuracy': 0.597000002861023, 'test/loss': 1.8365988731384277, 'test/num_examples': 10000, 'score': 45959.675520420074, 'total_duration': 47744.87985253334, 'accumulated_submission_time': 45959.675520420074, 'accumulated_eval_time': 1775.1927635669708, 'accumulated_logging_time': 5.355523347854614, 'global_step': 136193, 'preemption_count': 0}), (137707, {'train/accuracy': 0.8054049611091614, 'train/loss': 0.8238815069198608, 'validation/accuracy': 0.7189599871635437, 'validation/loss': 1.202906847000122, 'validation/num_examples': 50000, 'test/accuracy': 0.5964000225067139, 'test/loss': 1.854988932609558, 'test/num_examples': 10000, 'score': 46469.635543346405, 'total_duration': 48272.247631549835, 'accumulated_submission_time': 46469.635543346405, 'accumulated_eval_time': 1792.4990637302399, 'accumulated_logging_time': 5.405432224273682, 'global_step': 137707, 'preemption_count': 0}), (139222, {'train/accuracy': 0.8106265664100647, 'train/loss': 0.8035197854042053, 'validation/accuracy': 0.7279599905014038, 'validation/loss': 1.174459457397461, 'validation/num_examples': 50000, 'test/accuracy': 0.6018000245094299, 'test/loss': 1.8125733137130737, 'test/num_examples': 10000, 'score': 46979.75103497505, 'total_duration': 48799.85860180855, 'accumulated_submission_time': 46979.75103497505, 'accumulated_eval_time': 1809.889460325241, 'accumulated_logging_time': 5.458134174346924, 'global_step': 139222, 'preemption_count': 0}), (140737, {'train/accuracy': 0.8134167790412903, 'train/loss': 0.8105883598327637, 'validation/accuracy': 0.7305999994277954, 'validation/loss': 1.175118327140808, 'validation/num_examples': 50000, 'test/accuracy': 0.5982000231742859, 'test/loss': 1.8039989471435547, 'test/num_examples': 10000, 'score': 47489.82584095001, 'total_duration': 49327.57805490494, 'accumulated_submission_time': 47489.82584095001, 'accumulated_eval_time': 1827.4179458618164, 'accumulated_logging_time': 5.522055149078369, 'global_step': 140737, 'preemption_count': 0}), (142252, {'train/accuracy': 0.8249959945678711, 'train/loss': 0.7559878826141357, 'validation/accuracy': 0.726419985294342, 'validation/loss': 1.1791678667068481, 'validation/num_examples': 50000, 'test/accuracy': 0.605400025844574, 'test/loss': 1.8135912418365479, 'test/num_examples': 10000, 'score': 48000.03847670555, 'total_duration': 49854.95561385155, 'accumulated_submission_time': 48000.03847670555, 'accumulated_eval_time': 1844.4799404144287, 'accumulated_logging_time': 5.573156356811523, 'global_step': 142252, 'preemption_count': 0}), (143766, {'train/accuracy': 0.8384287357330322, 'train/loss': 0.7113813757896423, 'validation/accuracy': 0.7309799790382385, 'validation/loss': 1.161076545715332, 'validation/num_examples': 50000, 'test/accuracy': 0.6038000583648682, 'test/loss': 1.7907795906066895, 'test/num_examples': 10000, 'score': 48509.954323768616, 'total_duration': 50382.42102813721, 'accumulated_submission_time': 48509.954323768616, 'accumulated_eval_time': 1861.9220707416534, 'accumulated_logging_time': 5.628809690475464, 'global_step': 143766, 'preemption_count': 0}), (145281, {'train/accuracy': 0.8381098508834839, 'train/loss': 0.7158500552177429, 'validation/accuracy': 0.7337799668312073, 'validation/loss': 1.148126482963562, 'validation/num_examples': 50000, 'test/accuracy': 0.6077000498771667, 'test/loss': 1.781200885772705, 'test/num_examples': 10000, 'score': 49020.05911588669, 'total_duration': 50909.776867866516, 'accumulated_submission_time': 49020.05911588669, 'accumulated_eval_time': 1879.0681087970734, 'accumulated_logging_time': 5.681424856185913, 'global_step': 145281, 'preemption_count': 0}), (146796, {'train/accuracy': 0.8376116156578064, 'train/loss': 0.7040087580680847, 'validation/accuracy': 0.7360000014305115, 'validation/loss': 1.1290984153747559, 'validation/num_examples': 50000, 'test/accuracy': 0.6104000210762024, 'test/loss': 1.7563949823379517, 'test/num_examples': 10000, 'score': 49530.228246212006, 'total_duration': 51437.18770766258, 'accumulated_submission_time': 49530.228246212006, 'accumulated_eval_time': 1896.1900537014008, 'accumulated_logging_time': 5.749187469482422, 'global_step': 146796, 'preemption_count': 0}), (148311, {'train/accuracy': 0.8396643400192261, 'train/loss': 0.7063088417053223, 'validation/accuracy': 0.738319993019104, 'validation/loss': 1.1283575296401978, 'validation/num_examples': 50000, 'test/accuracy': 0.6163000464439392, 'test/loss': 1.7626051902770996, 'test/num_examples': 10000, 'score': 50040.213027477264, 'total_duration': 51964.37534117699, 'accumulated_submission_time': 50040.213027477264, 'accumulated_eval_time': 1913.290997505188, 'accumulated_logging_time': 5.799168109893799, 'global_step': 148311, 'preemption_count': 0}), (149825, {'train/accuracy': 0.8420161008834839, 'train/loss': 0.6852911710739136, 'validation/accuracy': 0.7417799830436707, 'validation/loss': 1.1163774728775024, 'validation/num_examples': 50000, 'test/accuracy': 0.6243000030517578, 'test/loss': 1.719955325126648, 'test/num_examples': 10000, 'score': 50550.241391181946, 'total_duration': 52491.98043131828, 'accumulated_submission_time': 50550.241391181946, 'accumulated_eval_time': 1930.7620613574982, 'accumulated_logging_time': 5.853203773498535, 'global_step': 149825, 'preemption_count': 0}), (151340, {'train/accuracy': 0.86820387840271, 'train/loss': 0.5952945947647095, 'validation/accuracy': 0.7403199672698975, 'validation/loss': 1.1137624979019165, 'validation/num_examples': 50000, 'test/accuracy': 0.6186000108718872, 'test/loss': 1.740875482559204, 'test/num_examples': 10000, 'score': 51060.40700316429, 'total_duration': 53019.797793626785, 'accumulated_submission_time': 51060.40700316429, 'accumulated_eval_time': 1948.308458328247, 'accumulated_logging_time': 5.906131267547607, 'global_step': 151340, 'preemption_count': 0}), (152855, {'train/accuracy': 0.8589365482330322, 'train/loss': 0.613325297832489, 'validation/accuracy': 0.7432799935340881, 'validation/loss': 1.1003096103668213, 'validation/num_examples': 50000, 'test/accuracy': 0.62090003490448, 'test/loss': 1.735670804977417, 'test/num_examples': 10000, 'score': 51570.638830661774, 'total_duration': 53548.062861442566, 'accumulated_submission_time': 51570.638830661774, 'accumulated_eval_time': 1966.237622976303, 'accumulated_logging_time': 5.958402872085571, 'global_step': 152855, 'preemption_count': 0}), (154370, {'train/accuracy': 0.8587571382522583, 'train/loss': 0.6176808476448059, 'validation/accuracy': 0.7478799819946289, 'validation/loss': 1.0903000831604004, 'validation/num_examples': 50000, 'test/accuracy': 0.6241000294685364, 'test/loss': 1.7056432962417603, 'test/num_examples': 10000, 'score': 52080.63415670395, 'total_duration': 54075.474705934525, 'accumulated_submission_time': 52080.63415670395, 'accumulated_eval_time': 1983.548333644867, 'accumulated_logging_time': 6.011291027069092, 'global_step': 154370, 'preemption_count': 0}), (155885, {'train/accuracy': 0.8608896732330322, 'train/loss': 0.6201562881469727, 'validation/accuracy': 0.75, 'validation/loss': 1.0886688232421875, 'validation/num_examples': 50000, 'test/accuracy': 0.6253000497817993, 'test/loss': 1.7142834663391113, 'test/num_examples': 10000, 'score': 52590.81240940094, 'total_duration': 54603.15359258652, 'accumulated_submission_time': 52590.81240940094, 'accumulated_eval_time': 2000.9424073696136, 'accumulated_logging_time': 6.064687252044678, 'global_step': 155885, 'preemption_count': 0}), (157400, {'train/accuracy': 0.8644570708274841, 'train/loss': 0.6090182065963745, 'validation/accuracy': 0.7490599751472473, 'validation/loss': 1.0829800367355347, 'validation/num_examples': 50000, 'test/accuracy': 0.6284000277519226, 'test/loss': 1.7018013000488281, 'test/num_examples': 10000, 'score': 53100.97397065163, 'total_duration': 55130.75111031532, 'accumulated_submission_time': 53100.97397065163, 'accumulated_eval_time': 2018.2726693153381, 'accumulated_logging_time': 6.1179773807525635, 'global_step': 157400, 'preemption_count': 0}), (158915, {'train/accuracy': 0.871113657951355, 'train/loss': 0.5796046853065491, 'validation/accuracy': 0.7538399696350098, 'validation/loss': 1.0558656454086304, 'validation/num_examples': 50000, 'test/accuracy': 0.6343000531196594, 'test/loss': 1.67049241065979, 'test/num_examples': 10000, 'score': 53611.190331459045, 'total_duration': 55658.49032831192, 'accumulated_submission_time': 53611.190331459045, 'accumulated_eval_time': 2035.686059474945, 'accumulated_logging_time': 6.175013780593872, 'global_step': 158915, 'preemption_count': 0}), (160430, {'train/accuracy': 0.8908242583274841, 'train/loss': 0.5088411569595337, 'validation/accuracy': 0.7539199590682983, 'validation/loss': 1.0591936111450195, 'validation/num_examples': 50000, 'test/accuracy': 0.6325000524520874, 'test/loss': 1.6727036237716675, 'test/num_examples': 10000, 'score': 54121.38585519791, 'total_duration': 56186.07378411293, 'accumulated_submission_time': 54121.38585519791, 'accumulated_eval_time': 2052.965085029602, 'accumulated_logging_time': 6.231770277023315, 'global_step': 160430, 'preemption_count': 0}), (161945, {'train/accuracy': 0.8883330225944519, 'train/loss': 0.507793128490448, 'validation/accuracy': 0.7577599883079529, 'validation/loss': 1.0418267250061035, 'validation/num_examples': 50000, 'test/accuracy': 0.6374000310897827, 'test/loss': 1.6483393907546997, 'test/num_examples': 10000, 'score': 54631.54222178459, 'total_duration': 56713.73193693161, 'accumulated_submission_time': 54631.54222178459, 'accumulated_eval_time': 2070.361840724945, 'accumulated_logging_time': 6.284480094909668, 'global_step': 161945, 'preemption_count': 0}), (163459, {'train/accuracy': 0.8888113498687744, 'train/loss': 0.508962869644165, 'validation/accuracy': 0.759880006313324, 'validation/loss': 1.0377557277679443, 'validation/num_examples': 50000, 'test/accuracy': 0.6409000158309937, 'test/loss': 1.6533217430114746, 'test/num_examples': 10000, 'score': 55141.445491075516, 'total_duration': 57240.98913478851, 'accumulated_submission_time': 55141.445491075516, 'accumulated_eval_time': 2087.6100096702576, 'accumulated_logging_time': 6.337510824203491, 'global_step': 163459, 'preemption_count': 0}), (164974, {'train/accuracy': 0.8914421200752258, 'train/loss': 0.5064797401428223, 'validation/accuracy': 0.7610999941825867, 'validation/loss': 1.0407962799072266, 'validation/num_examples': 50000, 'test/accuracy': 0.6428000330924988, 'test/loss': 1.644087553024292, 'test/num_examples': 10000, 'score': 55651.616990327835, 'total_duration': 57768.47083735466, 'accumulated_submission_time': 55651.616990327835, 'accumulated_eval_time': 2104.812753200531, 'accumulated_logging_time': 6.391413450241089, 'global_step': 164974, 'preemption_count': 0}), (166488, {'train/accuracy': 0.8913623690605164, 'train/loss': 0.49384015798568726, 'validation/accuracy': 0.761900007724762, 'validation/loss': 1.031830906867981, 'validation/num_examples': 50000, 'test/accuracy': 0.6368000507354736, 'test/loss': 1.654982328414917, 'test/num_examples': 10000, 'score': 56161.74114251137, 'total_duration': 58296.5521376133, 'accumulated_submission_time': 56161.74114251137, 'accumulated_eval_time': 2122.656609773636, 'accumulated_logging_time': 6.452677011489868, 'global_step': 166488, 'preemption_count': 0}), (168003, {'train/accuracy': 0.8980189561843872, 'train/loss': 0.4819498062133789, 'validation/accuracy': 0.7639999985694885, 'validation/loss': 1.017918586730957, 'validation/num_examples': 50000, 'test/accuracy': 0.6433000564575195, 'test/loss': 1.6341732740402222, 'test/num_examples': 10000, 'score': 56671.93290805817, 'total_duration': 58824.12835144997, 'accumulated_submission_time': 56671.93290805817, 'accumulated_eval_time': 2139.942459821701, 'accumulated_logging_time': 6.499396800994873, 'global_step': 168003, 'preemption_count': 0}), (169518, {'train/accuracy': 0.9111128449440002, 'train/loss': 0.43680357933044434, 'validation/accuracy': 0.7679599523544312, 'validation/loss': 1.0132651329040527, 'validation/num_examples': 50000, 'test/accuracy': 0.6438000202178955, 'test/loss': 1.6271370649337769, 'test/num_examples': 10000, 'score': 57182.0711376667, 'total_duration': 59352.142911195755, 'accumulated_submission_time': 57182.0711376667, 'accumulated_eval_time': 2157.704973936081, 'accumulated_logging_time': 6.561173677444458, 'global_step': 169518, 'preemption_count': 0}), (171033, {'train/accuracy': 0.9100565910339355, 'train/loss': 0.4437120258808136, 'validation/accuracy': 0.768839955329895, 'validation/loss': 1.0136228799819946, 'validation/num_examples': 50000, 'test/accuracy': 0.6476000547409058, 'test/loss': 1.6199846267700195, 'test/num_examples': 10000, 'score': 57692.27556872368, 'total_duration': 59879.77585315704, 'accumulated_submission_time': 57692.27556872368, 'accumulated_eval_time': 2175.0261178016663, 'accumulated_logging_time': 6.616154432296753, 'global_step': 171033, 'preemption_count': 0}), (172548, {'train/accuracy': 0.9079838991165161, 'train/loss': 0.44346883893013, 'validation/accuracy': 0.7683799862861633, 'validation/loss': 1.0122803449630737, 'validation/num_examples': 50000, 'test/accuracy': 0.6457000374794006, 'test/loss': 1.6168798208236694, 'test/num_examples': 10000, 'score': 58202.40605354309, 'total_duration': 60407.436655282974, 'accumulated_submission_time': 58202.40605354309, 'accumulated_eval_time': 2192.441407442093, 'accumulated_logging_time': 6.679574251174927, 'global_step': 172548, 'preemption_count': 0}), (174063, {'train/accuracy': 0.9148397445678711, 'train/loss': 0.4309082627296448, 'validation/accuracy': 0.7701599597930908, 'validation/loss': 1.0046439170837402, 'validation/num_examples': 50000, 'test/accuracy': 0.6491000056266785, 'test/loss': 1.6106775999069214, 'test/num_examples': 10000, 'score': 58712.38555598259, 'total_duration': 60935.12227869034, 'accumulated_submission_time': 58712.38555598259, 'accumulated_eval_time': 2210.0351996421814, 'accumulated_logging_time': 6.738846778869629, 'global_step': 174063, 'preemption_count': 0}), (175578, {'train/accuracy': 0.9133051633834839, 'train/loss': 0.42993512749671936, 'validation/accuracy': 0.77183997631073, 'validation/loss': 1.0019093751907349, 'validation/num_examples': 50000, 'test/accuracy': 0.650700032711029, 'test/loss': 1.6060999631881714, 'test/num_examples': 10000, 'score': 59222.59151554108, 'total_duration': 61462.778339385986, 'accumulated_submission_time': 59222.59151554108, 'accumulated_eval_time': 2227.369342327118, 'accumulated_logging_time': 6.801840305328369, 'global_step': 175578, 'preemption_count': 0}), (177092, {'train/accuracy': 0.9144411683082581, 'train/loss': 0.42450007796287537, 'validation/accuracy': 0.7725799679756165, 'validation/loss': 0.9952738881111145, 'validation/num_examples': 50000, 'test/accuracy': 0.65010005235672, 'test/loss': 1.6027166843414307, 'test/num_examples': 10000, 'score': 59732.500272512436, 'total_duration': 61990.1252887249, 'accumulated_submission_time': 59732.500272512436, 'accumulated_eval_time': 2244.6973419189453, 'accumulated_logging_time': 6.859493017196655, 'global_step': 177092, 'preemption_count': 0}), (178606, {'train/accuracy': 0.9225127100944519, 'train/loss': 0.39775556325912476, 'validation/accuracy': 0.7713800072669983, 'validation/loss': 0.9977371096611023, 'validation/num_examples': 50000, 'test/accuracy': 0.6538000106811523, 'test/loss': 1.6024290323257446, 'test/num_examples': 10000, 'score': 60242.55020594597, 'total_duration': 62517.70723223686, 'accumulated_submission_time': 60242.55020594597, 'accumulated_eval_time': 2262.119296312332, 'accumulated_logging_time': 6.916916847229004, 'global_step': 178606, 'preemption_count': 0}), (180120, {'train/accuracy': 0.9199019074440002, 'train/loss': 0.4014585316181183, 'validation/accuracy': 0.7723199725151062, 'validation/loss': 0.9948861598968506, 'validation/num_examples': 50000, 'test/accuracy': 0.6515000462532043, 'test/loss': 1.5987427234649658, 'test/num_examples': 10000, 'score': 60752.50250053406, 'total_duration': 63045.0622420311, 'accumulated_submission_time': 60752.50250053406, 'accumulated_eval_time': 2279.408571243286, 'accumulated_logging_time': 6.978010654449463, 'global_step': 180120, 'preemption_count': 0}), (181635, {'train/accuracy': 0.920918345451355, 'train/loss': 0.4040337800979614, 'validation/accuracy': 0.7740799784660339, 'validation/loss': 0.9936750531196594, 'validation/num_examples': 50000, 'test/accuracy': 0.6521000266075134, 'test/loss': 1.5991164445877075, 'test/num_examples': 10000, 'score': 61262.67764997482, 'total_duration': 63572.72377586365, 'accumulated_submission_time': 61262.67764997482, 'accumulated_eval_time': 2296.7791769504547, 'accumulated_logging_time': 7.041255712509155, 'global_step': 181635, 'preemption_count': 0}), (183149, {'train/accuracy': 0.9201411008834839, 'train/loss': 0.398114413022995, 'validation/accuracy': 0.7745400071144104, 'validation/loss': 0.9911282062530518, 'validation/num_examples': 50000, 'test/accuracy': 0.6522000432014465, 'test/loss': 1.5940217971801758, 'test/num_examples': 10000, 'score': 61772.59363722801, 'total_duration': 64099.83883333206, 'accumulated_submission_time': 61772.59363722801, 'accumulated_eval_time': 2313.8672394752502, 'accumulated_logging_time': 7.099210262298584, 'global_step': 183149, 'preemption_count': 0}), (184663, {'train/accuracy': 0.9205795526504517, 'train/loss': 0.4052787721157074, 'validation/accuracy': 0.774399995803833, 'validation/loss': 0.9936506748199463, 'validation/num_examples': 50000, 'test/accuracy': 0.6538000106811523, 'test/loss': 1.596049427986145, 'test/num_examples': 10000, 'score': 62282.494843006134, 'total_duration': 64627.17417168617, 'accumulated_submission_time': 62282.494843006134, 'accumulated_eval_time': 2331.1919524669647, 'accumulated_logging_time': 7.156313896179199, 'global_step': 184663, 'preemption_count': 0}), (186177, {'train/accuracy': 0.9222536683082581, 'train/loss': 0.4010597765445709, 'validation/accuracy': 0.7745199799537659, 'validation/loss': 0.9941147565841675, 'validation/num_examples': 50000, 'test/accuracy': 0.6533000469207764, 'test/loss': 1.5955201387405396, 'test/num_examples': 10000, 'score': 62792.42883324623, 'total_duration': 65154.52738904953, 'accumulated_submission_time': 62792.42883324623, 'accumulated_eval_time': 2348.503286600113, 'accumulated_logging_time': 7.2113025188446045, 'global_step': 186177, 'preemption_count': 0}), (186666, {'train/accuracy': 0.9229711294174194, 'train/loss': 0.38955602049827576, 'validation/accuracy': 0.774399995803833, 'validation/loss': 0.9909643530845642, 'validation/num_examples': 50000, 'test/accuracy': 0.6532000303268433, 'test/loss': 1.594335675239563, 'test/num_examples': 10000, 'score': 62957.09817099571, 'total_duration': 65336.45809841156, 'accumulated_submission_time': 62957.09817099571, 'accumulated_eval_time': 2365.6900820732117, 'accumulated_logging_time': 7.268607139587402, 'global_step': 186666, 'preemption_count': 0})], 'global_step': 186666}
I0131 14:58:30.511517 140070692116288 submission_runner.py:586] Timing: 62957.09817099571
I0131 14:58:30.511584 140070692116288 submission_runner.py:588] Total number of evals: 125
I0131 14:58:30.511629 140070692116288 submission_runner.py:589] ====================
I0131 14:58:30.511677 140070692116288 submission_runner.py:542] Using RNG seed 485521238
I0131 14:58:30.513211 140070692116288 submission_runner.py:551] --- Tuning run 2/5 ---
I0131 14:58:30.513342 140070692116288 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_3/imagenet_resnet_jax/trial_2.
I0131 14:58:30.517307 140070692116288 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_3/imagenet_resnet_jax/trial_2/hparams.json.
I0131 14:58:30.519254 140070692116288 submission_runner.py:206] Initializing dataset.
I0131 14:58:30.530676 140070692116288 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0131 14:58:30.541118 140070692116288 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0131 14:58:30.737268 140070692116288 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0131 14:58:31.031898 140070692116288 submission_runner.py:213] Initializing model.
I0131 14:58:36.567758 140070692116288 submission_runner.py:255] Initializing optimizer.
I0131 14:58:36.936284 140070692116288 submission_runner.py:262] Initializing metrics bundle.
I0131 14:58:36.936424 140070692116288 submission_runner.py:280] Initializing checkpoint and logger.
I0131 14:58:37.046895 140070692116288 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_3/imagenet_resnet_jax/trial_2 with prefix checkpoint_
I0131 14:58:37.047002 140070692116288 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_3/imagenet_resnet_jax/trial_2/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0131 14:58:48.835273 140070692116288 logger_utils.py:220] Unable to record git information. Continuing without it.
I0131 14:59:00.216895 140070692116288 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_3/imagenet_resnet_jax/trial_2/flags_0.json.
I0131 14:59:00.227248 140070692116288 submission_runner.py:314] Starting training loop.
I0131 14:59:31.318964 139907745949440 logging_writer.py:48] [0] global_step=0, grad_norm=0.5562551021575928, loss=6.925665378570557
I0131 14:59:31.328117 140070692116288 spec.py:321] Evaluating on the training split.
I0131 14:59:37.489398 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 14:59:46.425372 140070692116288 spec.py:349] Evaluating on the test split.
I0131 14:59:48.892619 140070692116288 submission_runner.py:408] Time since start: 48.67s, 	Step: 1, 	{'train/accuracy': 0.001195790828205645, 'train/loss': 6.911755561828613, 'validation/accuracy': 0.0011199999134987593, 'validation/loss': 6.912059783935547, 'validation/num_examples': 50000, 'test/accuracy': 0.0009000000427477062, 'test/loss': 6.912177562713623, 'test/num_examples': 10000, 'score': 31.10077476501465, 'total_duration': 48.665297985076904, 'accumulated_submission_time': 31.10077476501465, 'accumulated_eval_time': 17.564436674118042, 'accumulated_logging_time': 0}
I0131 14:59:48.902267 139907737556736 logging_writer.py:48] [1] accumulated_eval_time=17.564437, accumulated_logging_time=0, accumulated_submission_time=31.100775, global_step=1, preemption_count=0, score=31.100775, test/accuracy=0.000900, test/loss=6.912178, test/num_examples=10000, total_duration=48.665298, train/accuracy=0.001196, train/loss=6.911756, validation/accuracy=0.001120, validation/loss=6.912060, validation/num_examples=50000
I0131 15:00:22.660438 139908710635264 logging_writer.py:48] [100] global_step=100, grad_norm=0.5287989377975464, loss=6.901110649108887
I0131 15:00:56.464844 139907737556736 logging_writer.py:48] [200] global_step=200, grad_norm=0.5411918759346008, loss=6.856801509857178
I0131 15:01:30.319493 139908710635264 logging_writer.py:48] [300] global_step=300, grad_norm=0.5858916640281677, loss=6.764368534088135
I0131 15:02:04.160442 139907737556736 logging_writer.py:48] [400] global_step=400, grad_norm=0.6113755702972412, loss=6.6849775314331055
I0131 15:02:38.009169 139908710635264 logging_writer.py:48] [500] global_step=500, grad_norm=0.6677273511886597, loss=6.641433238983154
I0131 15:03:11.867872 139907737556736 logging_writer.py:48] [600] global_step=600, grad_norm=0.6808602213859558, loss=6.541276454925537
I0131 15:03:45.730665 139908710635264 logging_writer.py:48] [700] global_step=700, grad_norm=0.7791649699211121, loss=6.4479899406433105
I0131 15:04:19.611277 139907737556736 logging_writer.py:48] [800] global_step=800, grad_norm=1.4667824506759644, loss=6.369887351989746
I0131 15:04:53.453600 139908710635264 logging_writer.py:48] [900] global_step=900, grad_norm=1.5030839443206787, loss=6.30482292175293
I0131 15:05:27.283670 139907737556736 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.820860505104065, loss=6.282628059387207
I0131 15:06:01.154968 139908710635264 logging_writer.py:48] [1100] global_step=1100, grad_norm=2.348893880844116, loss=6.126010894775391
I0131 15:06:35.139445 139907737556736 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.8363744020462036, loss=6.09631872177124
I0131 15:07:09.003152 139908710635264 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.5095796585083008, loss=6.052471160888672
I0131 15:07:42.900095 139907737556736 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.7882401943206787, loss=5.983612060546875
I0131 15:08:16.757119 139908710635264 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.798366069793701, loss=5.937208652496338
I0131 15:08:18.935366 140070692116288 spec.py:321] Evaluating on the training split.
I0131 15:08:25.203333 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 15:08:34.159048 140070692116288 spec.py:349] Evaluating on the test split.
I0131 15:08:36.683750 140070692116288 submission_runner.py:408] Time since start: 576.46s, 	Step: 1508, 	{'train/accuracy': 0.08340641111135483, 'train/loss': 5.276715278625488, 'validation/accuracy': 0.07735999673604965, 'validation/loss': 5.338540077209473, 'validation/num_examples': 50000, 'test/accuracy': 0.05770000442862511, 'test/loss': 5.554655075073242, 'test/num_examples': 10000, 'score': 541.0707614421844, 'total_duration': 576.4564385414124, 'accumulated_submission_time': 541.0707614421844, 'accumulated_eval_time': 35.312798261642456, 'accumulated_logging_time': 0.018644094467163086}
I0131 15:08:36.711908 139907754342144 logging_writer.py:48] [1508] accumulated_eval_time=35.312798, accumulated_logging_time=0.018644, accumulated_submission_time=541.070761, global_step=1508, preemption_count=0, score=541.070761, test/accuracy=0.057700, test/loss=5.554655, test/num_examples=10000, total_duration=576.456439, train/accuracy=0.083406, train/loss=5.276715, validation/accuracy=0.077360, validation/loss=5.338540, validation/num_examples=50000
I0131 15:09:08.149436 139907762734848 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.7695709466934204, loss=5.85582160949707
I0131 15:09:42.005602 139907754342144 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.180795192718506, loss=5.848476409912109
I0131 15:10:15.891775 139907762734848 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.327594757080078, loss=5.779242992401123
I0131 15:10:49.785393 139907754342144 logging_writer.py:48] [1900] global_step=1900, grad_norm=3.4302496910095215, loss=5.711638450622559
I0131 15:11:23.671951 139907762734848 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.582279920578003, loss=5.694633960723877
I0131 15:11:57.536305 139907754342144 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.9231629371643066, loss=5.62406587600708
I0131 15:12:31.497874 139907762734848 logging_writer.py:48] [2200] global_step=2200, grad_norm=2.531993865966797, loss=5.590502738952637
I0131 15:13:05.401895 139907754342144 logging_writer.py:48] [2300] global_step=2300, grad_norm=5.175654411315918, loss=5.563943862915039
I0131 15:13:39.309094 139907762734848 logging_writer.py:48] [2400] global_step=2400, grad_norm=4.269681930541992, loss=5.507047653198242
I0131 15:14:13.200250 139907754342144 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.0944182872772217, loss=5.528146266937256
I0131 15:14:47.094203 139907762734848 logging_writer.py:48] [2600] global_step=2600, grad_norm=2.613779306411743, loss=5.3926520347595215
I0131 15:15:20.961992 139907754342144 logging_writer.py:48] [2700] global_step=2700, grad_norm=4.873204231262207, loss=5.3365631103515625
I0131 15:15:54.882265 139907762734848 logging_writer.py:48] [2800] global_step=2800, grad_norm=3.2162153720855713, loss=5.280333995819092
I0131 15:16:28.769222 139907754342144 logging_writer.py:48] [2900] global_step=2900, grad_norm=5.892422676086426, loss=5.211020469665527
I0131 15:17:02.649644 139907762734848 logging_writer.py:48] [3000] global_step=3000, grad_norm=4.969905376434326, loss=5.271942138671875
I0131 15:17:06.864812 140070692116288 spec.py:321] Evaluating on the training split.
I0131 15:17:13.751717 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 15:17:22.726959 140070692116288 spec.py:349] Evaluating on the test split.
I0131 15:17:25.210472 140070692116288 submission_runner.py:408] Time since start: 1104.98s, 	Step: 3014, 	{'train/accuracy': 0.19947783648967743, 'train/loss': 4.18760871887207, 'validation/accuracy': 0.18081998825073242, 'validation/loss': 4.304879188537598, 'validation/num_examples': 50000, 'test/accuracy': 0.13190001249313354, 'test/loss': 4.727846622467041, 'test/num_examples': 10000, 'score': 1051.159835577011, 'total_duration': 1104.983157634735, 'accumulated_submission_time': 1051.159835577011, 'accumulated_eval_time': 53.65842294692993, 'accumulated_logging_time': 0.058310747146606445}
I0131 15:17:25.227856 139907737556736 logging_writer.py:48] [3014] accumulated_eval_time=53.658423, accumulated_logging_time=0.058311, accumulated_submission_time=1051.159836, global_step=3014, preemption_count=0, score=1051.159836, test/accuracy=0.131900, test/loss=4.727847, test/num_examples=10000, total_duration=1104.983158, train/accuracy=0.199478, train/loss=4.187609, validation/accuracy=0.180820, validation/loss=4.304879, validation/num_examples=50000
I0131 15:17:54.666815 139907745949440 logging_writer.py:48] [3100] global_step=3100, grad_norm=3.5252227783203125, loss=5.2207512855529785
I0131 15:18:28.514948 139907737556736 logging_writer.py:48] [3200] global_step=3200, grad_norm=3.29502272605896, loss=5.119982719421387
I0131 15:19:02.477256 139907745949440 logging_writer.py:48] [3300] global_step=3300, grad_norm=2.5902299880981445, loss=5.0792388916015625
I0131 15:19:36.360252 139907737556736 logging_writer.py:48] [3400] global_step=3400, grad_norm=4.981081485748291, loss=5.17897367477417
I0131 15:20:10.261148 139907745949440 logging_writer.py:48] [3500] global_step=3500, grad_norm=3.5247559547424316, loss=5.025015830993652
I0131 15:20:44.163462 139907737556736 logging_writer.py:48] [3600] global_step=3600, grad_norm=3.285896062850952, loss=5.072305202484131
I0131 15:21:18.066084 139907745949440 logging_writer.py:48] [3700] global_step=3700, grad_norm=2.7614362239837646, loss=4.973146915435791
I0131 15:21:51.968619 139907737556736 logging_writer.py:48] [3800] global_step=3800, grad_norm=2.886082410812378, loss=4.9183430671691895
I0131 15:22:25.853622 139907745949440 logging_writer.py:48] [3900] global_step=3900, grad_norm=4.865738868713379, loss=4.919872283935547
I0131 15:22:59.766121 139907737556736 logging_writer.py:48] [4000] global_step=4000, grad_norm=4.0742645263671875, loss=4.95128059387207
I0131 15:23:33.645925 139907745949440 logging_writer.py:48] [4100] global_step=4100, grad_norm=3.413590431213379, loss=4.844300270080566
I0131 15:24:07.486668 139907737556736 logging_writer.py:48] [4200] global_step=4200, grad_norm=4.570368766784668, loss=4.786763668060303
I0131 15:24:41.352489 139907745949440 logging_writer.py:48] [4300] global_step=4300, grad_norm=3.4934048652648926, loss=4.790903568267822
I0131 15:25:15.248402 139907737556736 logging_writer.py:48] [4400] global_step=4400, grad_norm=4.650233268737793, loss=4.604117393493652
I0131 15:25:49.154070 139907745949440 logging_writer.py:48] [4500] global_step=4500, grad_norm=9.161666870117188, loss=4.69742488861084
I0131 15:25:55.389835 140070692116288 spec.py:321] Evaluating on the training split.
I0131 15:26:01.525213 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 15:26:10.627094 140070692116288 spec.py:349] Evaluating on the test split.
I0131 15:26:13.109277 140070692116288 submission_runner.py:408] Time since start: 1632.88s, 	Step: 4520, 	{'train/accuracy': 0.2977120578289032, 'train/loss': 3.51352596282959, 'validation/accuracy': 0.2717199921607971, 'validation/loss': 3.6509952545166016, 'validation/num_examples': 50000, 'test/accuracy': 0.20490001142024994, 'test/loss': 4.1407623291015625, 'test/num_examples': 10000, 'score': 1561.2604904174805, 'total_duration': 1632.881965637207, 'accumulated_submission_time': 1561.2604904174805, 'accumulated_eval_time': 71.37783074378967, 'accumulated_logging_time': 0.08464241027832031}
I0131 15:26:13.130787 139907762734848 logging_writer.py:48] [4520] accumulated_eval_time=71.377831, accumulated_logging_time=0.084642, accumulated_submission_time=1561.260490, global_step=4520, preemption_count=0, score=1561.260490, test/accuracy=0.204900, test/loss=4.140762, test/num_examples=10000, total_duration=1632.881966, train/accuracy=0.297712, train/loss=3.513526, validation/accuracy=0.271720, validation/loss=3.650995, validation/num_examples=50000
I0131 15:26:40.532709 139908425447168 logging_writer.py:48] [4600] global_step=4600, grad_norm=4.568039417266846, loss=4.738797187805176
I0131 15:27:14.372199 139907762734848 logging_writer.py:48] [4700] global_step=4700, grad_norm=3.8749122619628906, loss=4.750304222106934
I0131 15:27:48.239330 139908425447168 logging_writer.py:48] [4800] global_step=4800, grad_norm=4.491113185882568, loss=4.641872406005859
I0131 15:28:22.098533 139907762734848 logging_writer.py:48] [4900] global_step=4900, grad_norm=2.300830364227295, loss=4.5920586585998535
I0131 15:28:55.936098 139908425447168 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.361727714538574, loss=4.61115837097168
I0131 15:29:29.813605 139907762734848 logging_writer.py:48] [5100] global_step=5100, grad_norm=2.528336763381958, loss=4.524434566497803
I0131 15:30:03.700026 139908425447168 logging_writer.py:48] [5200] global_step=5200, grad_norm=2.9449257850646973, loss=4.58575439453125
I0131 15:30:37.546504 139907762734848 logging_writer.py:48] [5300] global_step=5300, grad_norm=3.0145492553710938, loss=4.458436965942383
I0131 15:31:11.408712 139908425447168 logging_writer.py:48] [5400] global_step=5400, grad_norm=4.197970390319824, loss=4.42903470993042
I0131 15:31:45.355690 139907762734848 logging_writer.py:48] [5500] global_step=5500, grad_norm=3.6430656909942627, loss=4.531069755554199
I0131 15:32:19.216722 139908425447168 logging_writer.py:48] [5600] global_step=5600, grad_norm=2.967087745666504, loss=4.439545154571533
I0131 15:32:53.112657 139907762734848 logging_writer.py:48] [5700] global_step=5700, grad_norm=3.094799518585205, loss=4.482219696044922
I0131 15:33:26.969502 139908425447168 logging_writer.py:48] [5800] global_step=5800, grad_norm=3.134993076324463, loss=4.337958335876465
I0131 15:34:00.830679 139907762734848 logging_writer.py:48] [5900] global_step=5900, grad_norm=3.037424325942993, loss=4.423369407653809
I0131 15:34:34.712813 139908425447168 logging_writer.py:48] [6000] global_step=6000, grad_norm=3.49599027633667, loss=4.393600940704346
I0131 15:34:43.316696 140070692116288 spec.py:321] Evaluating on the training split.
I0131 15:34:49.469408 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 15:34:58.515929 140070692116288 spec.py:349] Evaluating on the test split.
I0131 15:35:00.952080 140070692116288 submission_runner.py:408] Time since start: 2160.72s, 	Step: 6027, 	{'train/accuracy': 0.3806600570678711, 'train/loss': 3.0440993309020996, 'validation/accuracy': 0.3531999886035919, 'validation/loss': 3.1904051303863525, 'validation/num_examples': 50000, 'test/accuracy': 0.26489999890327454, 'test/loss': 3.793429136276245, 'test/num_examples': 10000, 'score': 2071.3847630023956, 'total_duration': 2160.7247705459595, 'accumulated_submission_time': 2071.3847630023956, 'accumulated_eval_time': 89.01321029663086, 'accumulated_logging_time': 0.11512041091918945}
I0131 15:35:00.968607 139907737556736 logging_writer.py:48] [6027] accumulated_eval_time=89.013210, accumulated_logging_time=0.115120, accumulated_submission_time=2071.384763, global_step=6027, preemption_count=0, score=2071.384763, test/accuracy=0.264900, test/loss=3.793429, test/num_examples=10000, total_duration=2160.724771, train/accuracy=0.380660, train/loss=3.044099, validation/accuracy=0.353200, validation/loss=3.190405, validation/num_examples=50000
I0131 15:35:26.046814 139907745949440 logging_writer.py:48] [6100] global_step=6100, grad_norm=3.318894624710083, loss=4.339123249053955
I0131 15:35:59.845776 139907737556736 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.4041085243225098, loss=4.286674976348877
I0131 15:36:33.673113 139907745949440 logging_writer.py:48] [6300] global_step=6300, grad_norm=3.25293231010437, loss=4.397311687469482
I0131 15:37:07.537009 139907737556736 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.783097982406616, loss=4.395253658294678
I0131 15:37:41.450938 139907745949440 logging_writer.py:48] [6500] global_step=6500, grad_norm=4.53137731552124, loss=4.278347969055176
I0131 15:38:15.303169 139907737556736 logging_writer.py:48] [6600] global_step=6600, grad_norm=2.5969793796539307, loss=4.303043842315674
I0131 15:38:49.142580 139907745949440 logging_writer.py:48] [6700] global_step=6700, grad_norm=3.6577091217041016, loss=4.253165245056152
I0131 15:39:23.019395 139907737556736 logging_writer.py:48] [6800] global_step=6800, grad_norm=2.553539276123047, loss=4.233791351318359
I0131 15:39:56.815396 139907745949440 logging_writer.py:48] [6900] global_step=6900, grad_norm=2.8784077167510986, loss=4.178179740905762
I0131 15:40:30.675987 139907737556736 logging_writer.py:48] [7000] global_step=7000, grad_norm=2.8215034008026123, loss=4.093568801879883
I0131 15:41:04.529881 139907745949440 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.8900699615478516, loss=4.178571701049805
I0131 15:41:38.388576 139907737556736 logging_writer.py:48] [7200] global_step=7200, grad_norm=2.570122718811035, loss=4.144044876098633
I0131 15:42:12.217570 139907745949440 logging_writer.py:48] [7300] global_step=7300, grad_norm=2.749410629272461, loss=4.152108192443848
I0131 15:42:46.092437 139907737556736 logging_writer.py:48] [7400] global_step=7400, grad_norm=2.590686559677124, loss=4.195635795593262
I0131 15:43:19.933322 139907745949440 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.988760232925415, loss=4.091312885284424
I0131 15:43:31.241175 140070692116288 spec.py:321] Evaluating on the training split.
I0131 15:43:37.430274 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 15:43:46.296542 140070692116288 spec.py:349] Evaluating on the test split.
I0131 15:43:48.796273 140070692116288 submission_runner.py:408] Time since start: 2688.57s, 	Step: 7535, 	{'train/accuracy': 0.4617745578289032, 'train/loss': 2.5876331329345703, 'validation/accuracy': 0.43479999899864197, 'validation/loss': 2.728386878967285, 'validation/num_examples': 50000, 'test/accuracy': 0.3281000256538391, 'test/loss': 3.381321668624878, 'test/num_examples': 10000, 'score': 2581.59383225441, 'total_duration': 2688.5689568519592, 'accumulated_submission_time': 2581.59383225441, 'accumulated_eval_time': 106.56827187538147, 'accumulated_logging_time': 0.1405167579650879}
I0131 15:43:48.816400 139907762734848 logging_writer.py:48] [7535] accumulated_eval_time=106.568272, accumulated_logging_time=0.140517, accumulated_submission_time=2581.593832, global_step=7535, preemption_count=0, score=2581.593832, test/accuracy=0.328100, test/loss=3.381322, test/num_examples=10000, total_duration=2688.568957, train/accuracy=0.461775, train/loss=2.587633, validation/accuracy=0.434800, validation/loss=2.728387, validation/num_examples=50000
I0131 15:44:11.223733 139908425447168 logging_writer.py:48] [7600] global_step=7600, grad_norm=2.410266876220703, loss=4.092417240142822
I0131 15:44:45.038723 139907762734848 logging_writer.py:48] [7700] global_step=7700, grad_norm=2.1378605365753174, loss=4.118381977081299
I0131 15:45:18.890790 139908425447168 logging_writer.py:48] [7800] global_step=7800, grad_norm=3.2013978958129883, loss=4.104796409606934
I0131 15:45:52.731785 139907762734848 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.7879451513290405, loss=4.040282726287842
I0131 15:46:26.564412 139908425447168 logging_writer.py:48] [8000] global_step=8000, grad_norm=2.2274997234344482, loss=4.0365891456604
I0131 15:47:00.416950 139907762734848 logging_writer.py:48] [8100] global_step=8100, grad_norm=2.9739186763763428, loss=4.024429798126221
I0131 15:47:34.272206 139908425447168 logging_writer.py:48] [8200] global_step=8200, grad_norm=3.348525047302246, loss=3.988468647003174
I0131 15:48:08.131525 139907762734848 logging_writer.py:48] [8300] global_step=8300, grad_norm=3.367971897125244, loss=3.987360954284668
I0131 15:48:41.984568 139908425447168 logging_writer.py:48] [8400] global_step=8400, grad_norm=3.028864860534668, loss=3.9962158203125
I0131 15:49:15.839082 139907762734848 logging_writer.py:48] [8500] global_step=8500, grad_norm=2.3007891178131104, loss=4.048566818237305
I0131 15:49:49.649707 139908425447168 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.9451736211776733, loss=4.037081718444824
I0131 15:50:23.568925 139907762734848 logging_writer.py:48] [8700] global_step=8700, grad_norm=2.5147910118103027, loss=3.9091711044311523
I0131 15:50:57.444645 139908425447168 logging_writer.py:48] [8800] global_step=8800, grad_norm=2.609797239303589, loss=3.927833080291748
I0131 15:51:31.273457 139907762734848 logging_writer.py:48] [8900] global_step=8900, grad_norm=2.575977325439453, loss=3.9243197441101074
I0131 15:52:05.124984 139908425447168 logging_writer.py:48] [9000] global_step=9000, grad_norm=2.3337175846099854, loss=4.024573802947998
I0131 15:52:18.828652 140070692116288 spec.py:321] Evaluating on the training split.
I0131 15:52:24.980693 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 15:52:34.090001 140070692116288 spec.py:349] Evaluating on the test split.
I0131 15:52:36.580845 140070692116288 submission_runner.py:408] Time since start: 3216.35s, 	Step: 9042, 	{'train/accuracy': 0.5261878371238708, 'train/loss': 2.2832539081573486, 'validation/accuracy': 0.4661199748516083, 'validation/loss': 2.5687878131866455, 'validation/num_examples': 50000, 'test/accuracy': 0.35340002179145813, 'test/loss': 3.23469614982605, 'test/num_examples': 10000, 'score': 3091.5421063899994, 'total_duration': 3216.3535330295563, 'accumulated_submission_time': 3091.5421063899994, 'accumulated_eval_time': 124.32044792175293, 'accumulated_logging_time': 0.17194366455078125}
I0131 15:52:36.600134 139907737556736 logging_writer.py:48] [9042] accumulated_eval_time=124.320448, accumulated_logging_time=0.171944, accumulated_submission_time=3091.542106, global_step=9042, preemption_count=0, score=3091.542106, test/accuracy=0.353400, test/loss=3.234696, test/num_examples=10000, total_duration=3216.353533, train/accuracy=0.526188, train/loss=2.283254, validation/accuracy=0.466120, validation/loss=2.568788, validation/num_examples=50000
I0131 15:52:56.544439 139907754342144 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.584546446800232, loss=3.9060633182525635
I0131 15:53:30.317986 139907737556736 logging_writer.py:48] [9200] global_step=9200, grad_norm=2.1251003742218018, loss=3.9335193634033203
I0131 15:54:04.143912 139907754342144 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.9758509397506714, loss=3.8943841457366943
I0131 15:54:37.955345 139907737556736 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.8538832664489746, loss=3.8782105445861816
I0131 15:55:11.769126 139907754342144 logging_writer.py:48] [9500] global_step=9500, grad_norm=2.1673951148986816, loss=3.825308322906494
I0131 15:55:45.613565 139907737556736 logging_writer.py:48] [9600] global_step=9600, grad_norm=2.217988967895508, loss=3.941009521484375
I0131 15:56:19.448741 139907754342144 logging_writer.py:48] [9700] global_step=9700, grad_norm=2.3272225856781006, loss=3.9258861541748047
I0131 15:56:53.315536 139907737556736 logging_writer.py:48] [9800] global_step=9800, grad_norm=2.0117504596710205, loss=3.8723068237304688
I0131 15:57:27.161265 139907754342144 logging_writer.py:48] [9900] global_step=9900, grad_norm=2.4604310989379883, loss=3.8901114463806152
I0131 15:58:00.957928 139907737556736 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.8362518548965454, loss=3.8825254440307617
I0131 15:58:34.784203 139907754342144 logging_writer.py:48] [10100] global_step=10100, grad_norm=1.7170964479446411, loss=3.896603584289551
I0131 15:59:08.617353 139907737556736 logging_writer.py:48] [10200] global_step=10200, grad_norm=2.217782735824585, loss=3.8679325580596924
I0131 15:59:42.406092 139907754342144 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.9294434785842896, loss=3.85648512840271
I0131 16:00:16.243999 139907737556736 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.7017616033554077, loss=3.7931346893310547
I0131 16:00:50.054041 139907754342144 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.8509864807128906, loss=3.7358481884002686
I0131 16:01:06.789053 140070692116288 spec.py:321] Evaluating on the training split.
I0131 16:01:12.967124 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 16:01:22.149180 140070692116288 spec.py:349] Evaluating on the test split.
I0131 16:01:24.672325 140070692116288 submission_runner.py:408] Time since start: 3744.45s, 	Step: 10551, 	{'train/accuracy': 0.5644331574440002, 'train/loss': 2.038395643234253, 'validation/accuracy': 0.5180599689483643, 'validation/loss': 2.2616748809814453, 'validation/num_examples': 50000, 'test/accuracy': 0.4077000319957733, 'test/loss': 2.8990767002105713, 'test/num_examples': 10000, 'score': 3601.6693108081818, 'total_duration': 3744.4450080394745, 'accumulated_submission_time': 3601.6693108081818, 'accumulated_eval_time': 142.20368576049805, 'accumulated_logging_time': 0.20055890083312988}
I0131 16:01:24.691671 139907729164032 logging_writer.py:48] [10551] accumulated_eval_time=142.203686, accumulated_logging_time=0.200559, accumulated_submission_time=3601.669311, global_step=10551, preemption_count=0, score=3601.669311, test/accuracy=0.407700, test/loss=2.899077, test/num_examples=10000, total_duration=3744.445008, train/accuracy=0.564433, train/loss=2.038396, validation/accuracy=0.518060, validation/loss=2.261675, validation/num_examples=50000
I0131 16:01:41.597964 139907737556736 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.9948605298995972, loss=3.7333450317382812
I0131 16:02:15.354381 139907729164032 logging_writer.py:48] [10700] global_step=10700, grad_norm=2.3096537590026855, loss=3.717183828353882
I0131 16:02:49.184433 139907737556736 logging_writer.py:48] [10800] global_step=10800, grad_norm=2.051727056503296, loss=3.792144775390625
I0131 16:03:22.997286 139907729164032 logging_writer.py:48] [10900] global_step=10900, grad_norm=1.8321919441223145, loss=3.707442045211792
I0131 16:03:56.784664 139907737556736 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.9197630882263184, loss=3.7965333461761475
I0131 16:04:30.642136 139907729164032 logging_writer.py:48] [11100] global_step=11100, grad_norm=2.6168806552886963, loss=3.729956865310669
I0131 16:05:04.464393 139907737556736 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.5695492029190063, loss=3.784950017929077
I0131 16:05:38.282941 139907729164032 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.9474477767944336, loss=3.72385573387146
I0131 16:06:12.108376 139907737556736 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.8990769386291504, loss=3.802306652069092
I0131 16:06:45.914940 139907729164032 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.577472448348999, loss=3.720292806625366
I0131 16:07:19.718825 139907737556736 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.8088408708572388, loss=3.701876163482666
I0131 16:07:53.555789 139907729164032 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.8009822368621826, loss=3.717090368270874
I0131 16:08:27.364260 139907737556736 logging_writer.py:48] [11800] global_step=11800, grad_norm=2.3016886711120605, loss=3.7121758460998535
I0131 16:09:01.254645 139907729164032 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.3862727880477905, loss=3.604287624359131
I0131 16:09:35.082056 139907737556736 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.4355952739715576, loss=3.638845205307007
I0131 16:09:54.843392 140070692116288 spec.py:321] Evaluating on the training split.
I0131 16:10:01.033598 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 16:10:10.264513 140070692116288 spec.py:349] Evaluating on the test split.
I0131 16:10:12.698770 140070692116288 submission_runner.py:408] Time since start: 4272.47s, 	Step: 12060, 	{'train/accuracy': 0.5901626348495483, 'train/loss': 1.959301471710205, 'validation/accuracy': 0.5461400151252747, 'validation/loss': 2.174945831298828, 'validation/num_examples': 50000, 'test/accuracy': 0.42480000853538513, 'test/loss': 2.805687189102173, 'test/num_examples': 10000, 'score': 4111.758868694305, 'total_duration': 4272.47144985199, 'accumulated_submission_time': 4111.758868694305, 'accumulated_eval_time': 160.059020280838, 'accumulated_logging_time': 0.22941160202026367}
I0131 16:10:12.717464 139908710635264 logging_writer.py:48] [12060] accumulated_eval_time=160.059020, accumulated_logging_time=0.229412, accumulated_submission_time=4111.758869, global_step=12060, preemption_count=0, score=4111.758869, test/accuracy=0.424800, test/loss=2.805687, test/num_examples=10000, total_duration=4272.471450, train/accuracy=0.590163, train/loss=1.959301, validation/accuracy=0.546140, validation/loss=2.174946, validation/num_examples=50000
I0131 16:10:26.583539 139908719027968 logging_writer.py:48] [12100] global_step=12100, grad_norm=2.033803939819336, loss=3.692047595977783
I0131 16:11:00.332198 139908710635264 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.7749747037887573, loss=3.697747230529785
I0131 16:11:34.105669 139908719027968 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.7760096788406372, loss=3.727555513381958
I0131 16:12:07.882581 139908710635264 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.9330545663833618, loss=3.6943581104278564
I0131 16:12:41.676089 139908719027968 logging_writer.py:48] [12500] global_step=12500, grad_norm=2.9264461994171143, loss=3.6424148082733154
I0131 16:13:15.505512 139908710635264 logging_writer.py:48] [12600] global_step=12600, grad_norm=2.475344657897949, loss=3.5709457397460938
I0131 16:13:49.299640 139908719027968 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.9129626750946045, loss=3.688491106033325
I0131 16:14:23.139809 139908710635264 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.6914266347885132, loss=3.5743191242218018
I0131 16:14:56.945510 139908719027968 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.4747544527053833, loss=3.6398839950561523
I0131 16:15:30.817350 139908710635264 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.9040679931640625, loss=3.6648550033569336
I0131 16:16:04.589517 139908719027968 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.3906753063201904, loss=3.6157946586608887
I0131 16:16:38.417105 139908710635264 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.4076976776123047, loss=3.624464273452759
I0131 16:17:12.232270 139908719027968 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.6600183248519897, loss=3.6470000743865967
I0131 16:17:46.067670 139908710635264 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.7866441011428833, loss=3.575566053390503
I0131 16:18:19.886234 139908719027968 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.3805004358291626, loss=3.59509539604187
I0131 16:18:43.019415 140070692116288 spec.py:321] Evaluating on the training split.
I0131 16:18:49.238674 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 16:18:58.114381 140070692116288 spec.py:349] Evaluating on the test split.
I0131 16:19:00.587476 140070692116288 submission_runner.py:408] Time since start: 4800.36s, 	Step: 13570, 	{'train/accuracy': 0.6247608065605164, 'train/loss': 1.7613424062728882, 'validation/accuracy': 0.5734399557113647, 'validation/loss': 2.0024077892303467, 'validation/num_examples': 50000, 'test/accuracy': 0.44300001859664917, 'test/loss': 2.674421548843384, 'test/num_examples': 10000, 'score': 4621.998073577881, 'total_duration': 4800.3601586818695, 'accumulated_submission_time': 4621.998073577881, 'accumulated_eval_time': 177.62704491615295, 'accumulated_logging_time': 0.2588982582092285}
I0131 16:19:00.607355 139907754342144 logging_writer.py:48] [13570] accumulated_eval_time=177.627045, accumulated_logging_time=0.258898, accumulated_submission_time=4621.998074, global_step=13570, preemption_count=0, score=4621.998074, test/accuracy=0.443000, test/loss=2.674422, test/num_examples=10000, total_duration=4800.360159, train/accuracy=0.624761, train/loss=1.761342, validation/accuracy=0.573440, validation/loss=2.002408, validation/num_examples=50000
I0131 16:19:11.082141 139907762734848 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.406821608543396, loss=3.5600075721740723
I0131 16:19:44.839360 139907754342144 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.4309576749801636, loss=3.5137510299682617
I0131 16:20:18.632587 139907762734848 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.3936424255371094, loss=3.633490800857544
I0131 16:20:52.440980 139907754342144 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.5028021335601807, loss=3.5441107749938965
I0131 16:21:26.287597 139907762734848 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.8630467653274536, loss=3.6026711463928223
I0131 16:22:00.135678 139907754342144 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.4972642660140991, loss=3.537578821182251
I0131 16:22:33.921953 139907762734848 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.7416502237319946, loss=3.6187479496002197
I0131 16:23:07.725217 139907754342144 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.7341713905334473, loss=3.5330238342285156
I0131 16:23:41.558431 139907762734848 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.6921818256378174, loss=3.548124313354492
I0131 16:24:15.350424 139907754342144 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.832565188407898, loss=3.5124623775482178
I0131 16:24:49.156957 139907762734848 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.777292251586914, loss=3.5222251415252686
I0131 16:25:22.952790 139907754342144 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.449975848197937, loss=3.5788369178771973
I0131 16:25:56.753128 139907762734848 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.6380969285964966, loss=3.427617073059082
I0131 16:26:30.567313 139907754342144 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.3396852016448975, loss=3.55061411857605
I0131 16:27:04.317281 139907762734848 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.150686264038086, loss=3.519021987915039
I0131 16:27:30.921858 140070692116288 spec.py:321] Evaluating on the training split.
I0131 16:27:37.062119 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 16:27:45.954978 140070692116288 spec.py:349] Evaluating on the test split.
I0131 16:27:48.426236 140070692116288 submission_runner.py:408] Time since start: 5328.20s, 	Step: 15080, 	{'train/accuracy': 0.6309988498687744, 'train/loss': 1.777909517288208, 'validation/accuracy': 0.5839799642562866, 'validation/loss': 1.9887746572494507, 'validation/num_examples': 50000, 'test/accuracy': 0.4603000283241272, 'test/loss': 2.6590631008148193, 'test/num_examples': 10000, 'score': 5132.248927354813, 'total_duration': 5328.198922872543, 'accumulated_submission_time': 5132.248927354813, 'accumulated_eval_time': 195.13138890266418, 'accumulated_logging_time': 0.28992772102355957}
I0131 16:27:48.445660 139907745949440 logging_writer.py:48] [15080] accumulated_eval_time=195.131389, accumulated_logging_time=0.289928, accumulated_submission_time=5132.248927, global_step=15080, preemption_count=0, score=5132.248927, test/accuracy=0.460300, test/loss=2.659063, test/num_examples=10000, total_duration=5328.198923, train/accuracy=0.630999, train/loss=1.777910, validation/accuracy=0.583980, validation/loss=1.988775, validation/num_examples=50000
I0131 16:27:55.547717 139908710635264 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.25252366065979, loss=3.5284276008605957
I0131 16:28:29.228993 139907745949440 logging_writer.py:48] [15200] global_step=15200, grad_norm=2.0745298862457275, loss=3.479802131652832
I0131 16:29:03.024116 139908710635264 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.4802501201629639, loss=3.4821743965148926
I0131 16:29:36.800621 139907745949440 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.8909225463867188, loss=3.616934299468994
I0131 16:30:10.558669 139908710635264 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.2559622526168823, loss=3.438723564147949
I0131 16:30:44.337674 139907745949440 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.4513367414474487, loss=3.472648859024048
I0131 16:31:18.164528 139908710635264 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.3812599182128906, loss=3.5321831703186035
I0131 16:31:51.918616 139907745949440 logging_writer.py:48] [15800] global_step=15800, grad_norm=2.558289051055908, loss=3.5508389472961426
I0131 16:32:25.688646 139908710635264 logging_writer.py:48] [15900] global_step=15900, grad_norm=2.177682638168335, loss=3.447265386581421
I0131 16:32:59.453060 139907745949440 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.5885030031204224, loss=3.449221134185791
I0131 16:33:33.226222 139908710635264 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.6224733591079712, loss=3.434568166732788
I0131 16:34:07.052443 139907745949440 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.8552546501159668, loss=3.569239616394043
I0131 16:34:40.877891 139908710635264 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.4451371431350708, loss=3.511579751968384
I0131 16:35:14.589748 139907745949440 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.483367919921875, loss=3.4602255821228027
I0131 16:35:48.381299 139908710635264 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.4341347217559814, loss=3.460566282272339
I0131 16:36:18.601774 140070692116288 spec.py:321] Evaluating on the training split.
I0131 16:36:24.901717 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 16:36:33.833984 140070692116288 spec.py:349] Evaluating on the test split.
I0131 16:36:36.399509 140070692116288 submission_runner.py:408] Time since start: 5856.17s, 	Step: 16591, 	{'train/accuracy': 0.6404655575752258, 'train/loss': 1.7349895238876343, 'validation/accuracy': 0.5945599675178528, 'validation/loss': 1.9475181102752686, 'validation/num_examples': 50000, 'test/accuracy': 0.4653000235557556, 'test/loss': 2.612239122390747, 'test/num_examples': 10000, 'score': 5642.341354608536, 'total_duration': 5856.172199487686, 'accumulated_submission_time': 5642.341354608536, 'accumulated_eval_time': 212.92911338806152, 'accumulated_logging_time': 0.31877803802490234}
I0131 16:36:36.420418 139907762734848 logging_writer.py:48] [16591] accumulated_eval_time=212.929113, accumulated_logging_time=0.318778, accumulated_submission_time=5642.341355, global_step=16591, preemption_count=0, score=5642.341355, test/accuracy=0.465300, test/loss=2.612239, test/num_examples=10000, total_duration=5856.172199, train/accuracy=0.640466, train/loss=1.734990, validation/accuracy=0.594560, validation/loss=1.947518, validation/num_examples=50000
I0131 16:36:39.798526 139908425447168 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.2633024454116821, loss=3.4471616744995117
I0131 16:37:13.498960 139907762734848 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.3095141649246216, loss=3.466630697250366
I0131 16:37:47.266742 139908425447168 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.304591178894043, loss=3.450307846069336
I0131 16:38:21.010094 139907762734848 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.21186101436615, loss=3.4363150596618652
I0131 16:38:54.791733 139908425447168 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.9216399192810059, loss=3.567244529724121
I0131 16:39:28.593772 139907762734848 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.6662367582321167, loss=3.4555673599243164
I0131 16:40:02.418690 139908425447168 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.4597891569137573, loss=3.4817841053009033
I0131 16:40:36.209376 139907762734848 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.0481541156768799, loss=3.4002251625061035
I0131 16:41:09.992804 139908425447168 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.5838022232055664, loss=3.5306265354156494
I0131 16:41:43.760903 139907762734848 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.2728164196014404, loss=3.404571533203125
I0131 16:42:17.543016 139908425447168 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.2645106315612793, loss=3.3829691410064697
I0131 16:42:51.347551 139907762734848 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.6246228218078613, loss=3.3566036224365234
I0131 16:43:25.125332 139908425447168 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.368414044380188, loss=3.4048399925231934
I0131 16:43:58.910676 139907762734848 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.3556506633758545, loss=3.4712159633636475
I0131 16:44:32.722770 139908425447168 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.1393991708755493, loss=3.406663417816162
I0131 16:45:06.528280 139907762734848 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.3865774869918823, loss=3.469541311264038
I0131 16:45:06.537606 140070692116288 spec.py:321] Evaluating on the training split.
I0131 16:45:12.884022 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 16:45:21.780638 140070692116288 spec.py:349] Evaluating on the test split.
I0131 16:45:24.255543 140070692116288 submission_runner.py:408] Time since start: 6384.03s, 	Step: 18101, 	{'train/accuracy': 0.6934390664100647, 'train/loss': 1.4368542432785034, 'validation/accuracy': 0.613099992275238, 'validation/loss': 1.7975260019302368, 'validation/num_examples': 50000, 'test/accuracy': 0.4782000184059143, 'test/loss': 2.4677436351776123, 'test/num_examples': 10000, 'score': 6152.393952131271, 'total_duration': 6384.028230428696, 'accumulated_submission_time': 6152.393952131271, 'accumulated_eval_time': 230.6470057964325, 'accumulated_logging_time': 0.35114240646362305}
I0131 16:45:24.275784 139907754342144 logging_writer.py:48] [18101] accumulated_eval_time=230.647006, accumulated_logging_time=0.351142, accumulated_submission_time=6152.393952, global_step=18101, preemption_count=0, score=6152.393952, test/accuracy=0.478200, test/loss=2.467744, test/num_examples=10000, total_duration=6384.028230, train/accuracy=0.693439, train/loss=1.436854, validation/accuracy=0.613100, validation/loss=1.797526, validation/num_examples=50000
I0131 16:45:58.056638 139908710635264 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.435304045677185, loss=3.4372804164886475
I0131 16:46:31.882306 139907754342144 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.4280805587768555, loss=3.468390941619873
I0131 16:47:05.660045 139908710635264 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.4652533531188965, loss=3.554455518722534
I0131 16:47:39.463892 139907754342144 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.3929437398910522, loss=3.4526026248931885
I0131 16:48:13.210101 139908710635264 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.6178956031799316, loss=3.422511577606201
I0131 16:48:46.980798 139907754342144 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.3984508514404297, loss=3.3836557865142822
I0131 16:49:20.707602 139908710635264 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.34342360496521, loss=3.380988836288452
I0131 16:49:54.468821 139907754342144 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.3841129541397095, loss=3.4041178226470947
I0131 16:50:28.266452 139908710635264 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.3605619668960571, loss=3.3959436416625977
I0131 16:51:02.016284 139907754342144 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.5606598854064941, loss=3.404066324234009
I0131 16:51:35.771600 139908710635264 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.1331368684768677, loss=3.398491382598877
I0131 16:52:09.592841 139907754342144 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.3088408708572388, loss=3.404557228088379
I0131 16:52:43.401755 139908710635264 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.2599718570709229, loss=3.4388763904571533
I0131 16:53:17.204102 139907754342144 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.2462021112442017, loss=3.3652846813201904
I0131 16:53:51.001367 139908710635264 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.285079836845398, loss=3.374218463897705
I0131 16:53:54.526914 140070692116288 spec.py:321] Evaluating on the training split.
I0131 16:54:00.846467 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 16:54:09.973499 140070692116288 spec.py:349] Evaluating on the test split.
I0131 16:54:12.339242 140070692116288 submission_runner.py:408] Time since start: 6912.11s, 	Step: 19612, 	{'train/accuracy': 0.6884366869926453, 'train/loss': 1.4951562881469727, 'validation/accuracy': 0.6227799654006958, 'validation/loss': 1.7908552885055542, 'validation/num_examples': 50000, 'test/accuracy': 0.49490001797676086, 'test/loss': 2.4673266410827637, 'test/num_examples': 10000, 'score': 6662.580951213837, 'total_duration': 6912.111881971359, 'accumulated_submission_time': 6662.580951213837, 'accumulated_eval_time': 248.45926570892334, 'accumulated_logging_time': 0.3803071975708008}
I0131 16:54:12.359810 139907737556736 logging_writer.py:48] [19612] accumulated_eval_time=248.459266, accumulated_logging_time=0.380307, accumulated_submission_time=6662.580951, global_step=19612, preemption_count=0, score=6662.580951, test/accuracy=0.494900, test/loss=2.467327, test/num_examples=10000, total_duration=6912.111882, train/accuracy=0.688437, train/loss=1.495156, validation/accuracy=0.622780, validation/loss=1.790855, validation/num_examples=50000
I0131 16:54:42.431407 139907745949440 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.2414665222167969, loss=3.4981613159179688
I0131 16:55:16.192410 139907737556736 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.3898929357528687, loss=3.3002984523773193
I0131 16:55:49.931133 139907745949440 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.0515103340148926, loss=3.2559046745300293
I0131 16:56:23.716371 139907737556736 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.33213210105896, loss=3.3793842792510986
I0131 16:56:57.486183 139907745949440 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.4353828430175781, loss=3.368476152420044
I0131 16:57:31.249312 139907737556736 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.7503693103790283, loss=3.4604525566101074
I0131 16:58:05.044902 139907745949440 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.6021558046340942, loss=3.3370766639709473
I0131 16:58:38.886646 139907737556736 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.4560233354568481, loss=3.411263942718506
I0131 16:59:12.679058 139907745949440 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.540686845779419, loss=3.367979049682617
I0131 16:59:46.440812 139907737556736 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.28903329372406, loss=3.312262535095215
I0131 17:00:20.198895 139907745949440 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.327284336090088, loss=3.2936675548553467
I0131 17:00:53.963385 139907737556736 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.7772557735443115, loss=3.4145352840423584
I0131 17:01:27.756692 139907745949440 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.3874385356903076, loss=3.4341559410095215
I0131 17:02:01.478921 139907737556736 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.259826421737671, loss=3.3635289669036865
I0131 17:02:35.259605 139907745949440 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.2319244146347046, loss=3.3518881797790527
I0131 17:02:42.495267 140070692116288 spec.py:321] Evaluating on the training split.
I0131 17:02:48.686989 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 17:02:57.527014 140070692116288 spec.py:349] Evaluating on the test split.
I0131 17:03:00.039779 140070692116288 submission_runner.py:408] Time since start: 7439.81s, 	Step: 21123, 	{'train/accuracy': 0.6886360049247742, 'train/loss': 1.4495007991790771, 'validation/accuracy': 0.625819981098175, 'validation/loss': 1.7343255281448364, 'validation/num_examples': 50000, 'test/accuracy': 0.49380001425743103, 'test/loss': 2.425893545150757, 'test/num_examples': 10000, 'score': 7172.652762174606, 'total_duration': 7439.812463760376, 'accumulated_submission_time': 7172.652762174606, 'accumulated_eval_time': 266.00374031066895, 'accumulated_logging_time': 0.41115355491638184}
I0131 17:03:00.061223 139907745949440 logging_writer.py:48] [21123] accumulated_eval_time=266.003740, accumulated_logging_time=0.411154, accumulated_submission_time=7172.652762, global_step=21123, preemption_count=0, score=7172.652762, test/accuracy=0.493800, test/loss=2.425894, test/num_examples=10000, total_duration=7439.812464, train/accuracy=0.688636, train/loss=1.449501, validation/accuracy=0.625820, validation/loss=1.734326, validation/num_examples=50000
I0131 17:03:26.371452 139908710635264 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.543332576751709, loss=3.349177837371826
I0131 17:04:00.102346 139907745949440 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.6285083293914795, loss=3.444873094558716
I0131 17:04:33.875419 139908710635264 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.2843594551086426, loss=3.288206100463867
I0131 17:05:07.844662 139907745949440 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.5078463554382324, loss=3.340141773223877
I0131 17:05:41.604103 139908710635264 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.3264085054397583, loss=3.294163703918457
I0131 17:06:15.374678 139907745949440 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.6379311084747314, loss=3.339038372039795
I0131 17:06:49.154901 139908710635264 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.2688772678375244, loss=3.3965039253234863
I0131 17:07:22.898002 139907745949440 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.302695870399475, loss=3.4283666610717773
I0131 17:07:56.669509 139908710635264 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.2460566759109497, loss=3.3545477390289307
I0131 17:08:30.458061 139907745949440 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.1873714923858643, loss=3.3388516902923584
I0131 17:09:04.239964 139908710635264 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.802303671836853, loss=3.4344401359558105
I0131 17:09:37.977815 139907745949440 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.3426145315170288, loss=3.305298089981079
I0131 17:10:11.755985 139908710635264 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.445134162902832, loss=3.3323540687561035
I0131 17:10:45.688909 139907745949440 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.395211100578308, loss=3.305560827255249
I0131 17:11:19.440879 139908710635264 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.5544037818908691, loss=3.3623411655426025
I0131 17:11:30.050457 140070692116288 spec.py:321] Evaluating on the training split.
I0131 17:11:36.258753 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 17:11:45.151824 140070692116288 spec.py:349] Evaluating on the test split.
I0131 17:11:47.723828 140070692116288 submission_runner.py:408] Time since start: 7967.50s, 	Step: 22633, 	{'train/accuracy': 0.6915856003761292, 'train/loss': 1.4739915132522583, 'validation/accuracy': 0.6281999945640564, 'validation/loss': 1.7558631896972656, 'validation/num_examples': 50000, 'test/accuracy': 0.5089000463485718, 'test/loss': 2.38150954246521, 'test/num_examples': 10000, 'score': 7682.5801339149475, 'total_duration': 7967.496514797211, 'accumulated_submission_time': 7682.5801339149475, 'accumulated_eval_time': 283.67709016799927, 'accumulated_logging_time': 0.44150733947753906}
I0131 17:11:47.744589 139907762734848 logging_writer.py:48] [22633] accumulated_eval_time=283.677090, accumulated_logging_time=0.441507, accumulated_submission_time=7682.580134, global_step=22633, preemption_count=0, score=7682.580134, test/accuracy=0.508900, test/loss=2.381510, test/num_examples=10000, total_duration=7967.496515, train/accuracy=0.691586, train/loss=1.473992, validation/accuracy=0.628200, validation/loss=1.755863, validation/num_examples=50000
I0131 17:12:10.681674 139908425447168 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.2618622779846191, loss=3.3565101623535156
I0131 17:12:44.419820 139907762734848 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.2682758569717407, loss=3.38726544380188
I0131 17:13:18.141996 139908425447168 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.3349096775054932, loss=3.3047878742218018
I0131 17:13:51.915624 139907762734848 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.2632678747177124, loss=3.4207286834716797
I0131 17:14:25.703825 139908425447168 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.2680391073226929, loss=3.3515994548797607
I0131 17:14:59.504464 139907762734848 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.3064262866973877, loss=3.295851469039917
I0131 17:15:33.215683 139908425447168 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.2945200204849243, loss=3.3701632022857666
I0131 17:16:06.991705 139907762734848 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.6231509447097778, loss=3.2731447219848633
I0131 17:16:40.749128 139908425447168 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.0447604656219482, loss=3.290489912033081
I0131 17:17:14.652302 139907762734848 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.6800519227981567, loss=3.3734335899353027
I0131 17:17:48.420826 139908425447168 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.4176846742630005, loss=3.3908843994140625
I0131 17:18:22.156414 139907762734848 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.577787160873413, loss=3.270303964614868
I0131 17:18:55.940649 139908425447168 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.4083300828933716, loss=3.331118583679199
I0131 17:19:29.645106 139907762734848 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.6718335151672363, loss=3.3868839740753174
I0131 17:20:03.420736 139908425447168 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.4014430046081543, loss=3.36120867729187
I0131 17:20:17.764427 140070692116288 spec.py:321] Evaluating on the training split.
I0131 17:20:24.039570 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 17:20:33.043529 140070692116288 spec.py:349] Evaluating on the test split.
I0131 17:20:35.610271 140070692116288 submission_runner.py:408] Time since start: 8495.38s, 	Step: 24144, 	{'train/accuracy': 0.6928212642669678, 'train/loss': 1.4294037818908691, 'validation/accuracy': 0.6322399973869324, 'validation/loss': 1.7027853727340698, 'validation/num_examples': 50000, 'test/accuracy': 0.5017000436782837, 'test/loss': 2.382734537124634, 'test/num_examples': 10000, 'score': 8192.537194252014, 'total_duration': 8495.38295841217, 'accumulated_submission_time': 8192.537194252014, 'accumulated_eval_time': 301.5229160785675, 'accumulated_logging_time': 0.4715721607208252}
I0131 17:20:35.632049 139907737556736 logging_writer.py:48] [24144] accumulated_eval_time=301.522916, accumulated_logging_time=0.471572, accumulated_submission_time=8192.537194, global_step=24144, preemption_count=0, score=8192.537194, test/accuracy=0.501700, test/loss=2.382735, test/num_examples=10000, total_duration=8495.382958, train/accuracy=0.692821, train/loss=1.429404, validation/accuracy=0.632240, validation/loss=1.702785, validation/num_examples=50000
I0131 17:20:54.857348 139907745949440 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.3174538612365723, loss=3.366896629333496
I0131 17:21:28.531803 139907737556736 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.4142677783966064, loss=3.295982837677002
I0131 17:22:02.283650 139907745949440 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.5037660598754883, loss=3.379812479019165
I0131 17:22:36.058129 139907737556736 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.3649120330810547, loss=3.352109909057617
I0131 17:23:09.809373 139907745949440 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.7688608169555664, loss=3.3464975357055664
I0131 17:23:43.678911 139907737556736 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.3022878170013428, loss=3.2274703979492188
I0131 17:24:17.429795 139907745949440 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.4157378673553467, loss=3.3398828506469727
I0131 17:24:51.191380 139907737556736 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.2796831130981445, loss=3.376852035522461
I0131 17:25:24.904782 139907745949440 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.4056848287582397, loss=3.3215603828430176
I0131 17:25:58.679061 139907737556736 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.1428908109664917, loss=3.2916150093078613
I0131 17:26:32.419041 139907745949440 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.2283025979995728, loss=3.2845160961151123
I0131 17:27:06.194522 139907737556736 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.5112384557724, loss=3.2659010887145996
I0131 17:27:39.922195 139907745949440 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.2540256977081299, loss=3.3342761993408203
I0131 17:28:13.657668 139907737556736 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.522214412689209, loss=3.3189806938171387
I0131 17:28:47.376508 139907745949440 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.1225804090499878, loss=3.2978971004486084
I0131 17:29:05.745984 140070692116288 spec.py:321] Evaluating on the training split.
I0131 17:29:11.922793 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 17:29:20.784426 140070692116288 spec.py:349] Evaluating on the test split.
I0131 17:29:23.266949 140070692116288 submission_runner.py:408] Time since start: 9023.04s, 	Step: 25656, 	{'train/accuracy': 0.6940768361091614, 'train/loss': 1.4729230403900146, 'validation/accuracy': 0.6380199790000916, 'validation/loss': 1.7212127447128296, 'validation/num_examples': 50000, 'test/accuracy': 0.5093000531196594, 'test/loss': 2.390270709991455, 'test/num_examples': 10000, 'score': 8702.589481115341, 'total_duration': 9023.039636611938, 'accumulated_submission_time': 8702.589481115341, 'accumulated_eval_time': 319.04385137557983, 'accumulated_logging_time': 0.5028097629547119}
I0131 17:29:23.287779 139907737556736 logging_writer.py:48] [25656] accumulated_eval_time=319.043851, accumulated_logging_time=0.502810, accumulated_submission_time=8702.589481, global_step=25656, preemption_count=0, score=8702.589481, test/accuracy=0.509300, test/loss=2.390271, test/num_examples=10000, total_duration=9023.039637, train/accuracy=0.694077, train/loss=1.472923, validation/accuracy=0.638020, validation/loss=1.721213, validation/num_examples=50000
I0131 17:29:38.534906 139908710635264 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.3726626634597778, loss=3.2612533569335938
I0131 17:30:12.277019 139907737556736 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.166928768157959, loss=3.2673933506011963
I0131 17:30:45.982601 139908710635264 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.2734287977218628, loss=3.307034730911255
I0131 17:31:19.732867 139907737556736 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.5772849321365356, loss=3.390897035598755
I0131 17:31:53.500004 139908710635264 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.3267658948898315, loss=3.3161017894744873
I0131 17:32:27.271882 139907737556736 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.6121162176132202, loss=3.2499287128448486
I0131 17:33:00.973933 139908710635264 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.622944712638855, loss=3.365618944168091
I0131 17:33:34.744523 139907737556736 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.4662487506866455, loss=3.2576255798339844
I0131 17:34:08.572938 139908710635264 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.5493104457855225, loss=3.3303163051605225
I0131 17:34:42.357968 139907737556736 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.1564595699310303, loss=3.2609305381774902
I0131 17:35:16.071705 139908710635264 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.4838576316833496, loss=3.211064100265503
I0131 17:35:49.928646 139907737556736 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.2231537103652954, loss=3.2852487564086914
I0131 17:36:23.712783 139908710635264 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.293269157409668, loss=3.243504524230957
I0131 17:36:57.509276 139907737556736 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.154123306274414, loss=3.3387491703033447
I0131 17:37:31.225629 139908710635264 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.4575492143630981, loss=3.358536958694458
I0131 17:37:53.339681 140070692116288 spec.py:321] Evaluating on the training split.
I0131 17:37:59.536818 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 17:38:08.613215 140070692116288 spec.py:349] Evaluating on the test split.
I0131 17:38:11.062480 140070692116288 submission_runner.py:408] Time since start: 9550.84s, 	Step: 27167, 	{'train/accuracy': 0.7210817933082581, 'train/loss': 1.3525702953338623, 'validation/accuracy': 0.6334599852561951, 'validation/loss': 1.7323843240737915, 'validation/num_examples': 50000, 'test/accuracy': 0.5049999952316284, 'test/loss': 2.398658514022827, 'test/num_examples': 10000, 'score': 9212.57828116417, 'total_duration': 9550.835167884827, 'accumulated_submission_time': 9212.57828116417, 'accumulated_eval_time': 336.7666335105896, 'accumulated_logging_time': 0.5334517955780029}
I0131 17:38:11.083799 139907737556736 logging_writer.py:48] [27167] accumulated_eval_time=336.766634, accumulated_logging_time=0.533452, accumulated_submission_time=9212.578281, global_step=27167, preemption_count=0, score=9212.578281, test/accuracy=0.505000, test/loss=2.398659, test/num_examples=10000, total_duration=9550.835168, train/accuracy=0.721082, train/loss=1.352570, validation/accuracy=0.633460, validation/loss=1.732384, validation/num_examples=50000
I0131 17:38:22.563901 139907745949440 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.3556697368621826, loss=3.23884916305542
I0131 17:38:56.306961 139907737556736 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.8609020709991455, loss=3.3228235244750977
I0131 17:39:29.998658 139907745949440 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.599069595336914, loss=3.2469286918640137
I0131 17:40:03.763805 139907737556736 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.3308607339859009, loss=3.2152223587036133
I0131 17:40:37.554478 139907745949440 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.322889804840088, loss=3.2546610832214355
I0131 17:41:11.280330 139907737556736 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.5082738399505615, loss=3.2400760650634766
I0131 17:41:45.030034 139907745949440 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.6890058517456055, loss=3.320127487182617
I0131 17:42:18.890421 139907737556736 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.7488226890563965, loss=3.294499158859253
I0131 17:42:52.673270 139907745949440 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.4313759803771973, loss=3.2567338943481445
I0131 17:43:26.409224 139907737556736 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.4903005361557007, loss=3.2856221199035645
I0131 17:44:00.217975 139907745949440 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.259645700454712, loss=3.294497013092041
I0131 17:44:33.937451 139907737556736 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.9188071489334106, loss=3.339456081390381
I0131 17:45:07.710504 139907745949440 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.4961503744125366, loss=3.223100423812866
I0131 17:45:41.439688 139907737556736 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.1237449645996094, loss=3.250401020050049
I0131 17:46:15.169358 139907745949440 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.2667092084884644, loss=3.3159737586975098
I0131 17:46:41.291815 140070692116288 spec.py:321] Evaluating on the training split.
I0131 17:46:47.479230 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 17:46:56.523022 140070692116288 spec.py:349] Evaluating on the test split.
I0131 17:46:59.092315 140070692116288 submission_runner.py:408] Time since start: 10078.87s, 	Step: 28679, 	{'train/accuracy': 0.7137874364852905, 'train/loss': 1.3917793035507202, 'validation/accuracy': 0.6363599896430969, 'validation/loss': 1.7337682247161865, 'validation/num_examples': 50000, 'test/accuracy': 0.5118000507354736, 'test/loss': 2.377542734146118, 'test/num_examples': 10000, 'score': 9722.724038362503, 'total_duration': 10078.865000724792, 'accumulated_submission_time': 9722.724038362503, 'accumulated_eval_time': 354.5671169757843, 'accumulated_logging_time': 0.5640599727630615}
I0131 17:46:59.113684 139907737556736 logging_writer.py:48] [28679] accumulated_eval_time=354.567117, accumulated_logging_time=0.564060, accumulated_submission_time=9722.724038, global_step=28679, preemption_count=0, score=9722.724038, test/accuracy=0.511800, test/loss=2.377543, test/num_examples=10000, total_duration=10078.865001, train/accuracy=0.713787, train/loss=1.391779, validation/accuracy=0.636360, validation/loss=1.733768, validation/num_examples=50000
I0131 17:47:06.541398 139908710635264 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.6485185623168945, loss=3.2893619537353516
I0131 17:47:40.327657 139907737556736 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.404753565788269, loss=3.2244670391082764
I0131 17:48:14.240880 139908710635264 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.5701289176940918, loss=3.246640205383301
I0131 17:48:47.999690 139907737556736 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.4434090852737427, loss=3.2642011642456055
I0131 17:49:21.738331 139908710635264 logging_writer.py:48] [29100] global_step=29100, grad_norm=1.384783387184143, loss=3.3092236518859863
I0131 17:49:55.477967 139907737556736 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.573415756225586, loss=3.3078739643096924
I0131 17:50:29.255430 139908710635264 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.4680614471435547, loss=3.3550329208374023
I0131 17:51:02.987057 139907737556736 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.422508716583252, loss=3.258777618408203
I0131 17:51:36.743083 139908710635264 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.715695858001709, loss=3.278289318084717
I0131 17:52:10.452505 139907737556736 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.347365379333496, loss=3.320159912109375
I0131 17:52:44.241016 139908710635264 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.2718684673309326, loss=3.1926445960998535
I0131 17:53:17.978541 139907737556736 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.4330012798309326, loss=3.3282153606414795
I0131 17:53:51.767703 139908710635264 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.414962887763977, loss=3.256220579147339
I0131 17:54:25.669102 139907737556736 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.4271526336669922, loss=3.259469985961914
I0131 17:54:59.386761 139908710635264 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.239839792251587, loss=3.2401230335235596
I0131 17:55:29.271708 140070692116288 spec.py:321] Evaluating on the training split.
I0131 17:55:35.517920 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 17:55:44.626734 140070692116288 spec.py:349] Evaluating on the test split.
I0131 17:55:47.028332 140070692116288 submission_runner.py:408] Time since start: 10606.80s, 	Step: 30190, 	{'train/accuracy': 0.7102399468421936, 'train/loss': 1.383233904838562, 'validation/accuracy': 0.6411399841308594, 'validation/loss': 1.6908059120178223, 'validation/num_examples': 50000, 'test/accuracy': 0.5134000182151794, 'test/loss': 2.3572490215301514, 'test/num_examples': 10000, 'score': 10232.818863868713, 'total_duration': 10606.80100107193, 'accumulated_submission_time': 10232.818863868713, 'accumulated_eval_time': 372.32369208335876, 'accumulated_logging_time': 0.5949838161468506}
I0131 17:55:47.049942 139907754342144 logging_writer.py:48] [30190] accumulated_eval_time=372.323692, accumulated_logging_time=0.594984, accumulated_submission_time=10232.818864, global_step=30190, preemption_count=0, score=10232.818864, test/accuracy=0.513400, test/loss=2.357249, test/num_examples=10000, total_duration=10606.801001, train/accuracy=0.710240, train/loss=1.383234, validation/accuracy=0.641140, validation/loss=1.690806, validation/num_examples=50000
I0131 17:55:50.765761 139907762734848 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.2574511766433716, loss=3.3600914478302
I0131 17:56:24.448367 139907754342144 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.2904778718948364, loss=3.268996238708496
I0131 17:56:58.147129 139907762734848 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.468766450881958, loss=3.367347002029419
I0131 17:57:31.946828 139907754342144 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.4264427423477173, loss=3.2753584384918213
I0131 17:58:05.665695 139907762734848 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.5418633222579956, loss=3.2173190116882324
I0131 17:58:39.467322 139907754342144 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.3838176727294922, loss=3.2239973545074463
I0131 17:59:13.188014 139907762734848 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.4162241220474243, loss=3.304187536239624
I0131 17:59:47.303869 139907754342144 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.4528746604919434, loss=3.208613395690918
I0131 18:00:20.991612 139907762734848 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.3305277824401855, loss=3.262439727783203
I0131 18:00:54.806656 139907754342144 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.1933575868606567, loss=3.2582061290740967
I0131 18:01:28.556538 139907762734848 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.2470762729644775, loss=3.228050708770752
I0131 18:02:02.358453 139907754342144 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.5181357860565186, loss=3.313190460205078
I0131 18:02:36.076991 139907762734848 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.72552490234375, loss=3.297610282897949
I0131 18:03:09.844208 139907754342144 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.899846076965332, loss=3.24656081199646
I0131 18:03:43.608273 139907762734848 logging_writer.py:48] [31600] global_step=31600, grad_norm=2.0057013034820557, loss=3.2516326904296875
I0131 18:04:17.363842 139907754342144 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.4243736267089844, loss=3.2607202529907227
I0131 18:04:17.373486 140070692116288 spec.py:321] Evaluating on the training split.
I0131 18:04:23.677107 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 18:04:32.770373 140070692116288 spec.py:349] Evaluating on the test split.
I0131 18:04:35.279171 140070692116288 submission_runner.py:408] Time since start: 11135.05s, 	Step: 31701, 	{'train/accuracy': 0.7122528553009033, 'train/loss': 1.4000403881072998, 'validation/accuracy': 0.6447399854660034, 'validation/loss': 1.6911596059799194, 'validation/num_examples': 50000, 'test/accuracy': 0.515500009059906, 'test/loss': 2.3480465412139893, 'test/num_examples': 10000, 'score': 10743.079125404358, 'total_duration': 11135.051835298538, 'accumulated_submission_time': 10743.079125404358, 'accumulated_eval_time': 390.22929978370667, 'accumulated_logging_time': 0.6257216930389404}
I0131 18:04:35.303003 139907745949440 logging_writer.py:48] [31701] accumulated_eval_time=390.229300, accumulated_logging_time=0.625722, accumulated_submission_time=10743.079125, global_step=31701, preemption_count=0, score=10743.079125, test/accuracy=0.515500, test/loss=2.348047, test/num_examples=10000, total_duration=11135.051835, train/accuracy=0.712253, train/loss=1.400040, validation/accuracy=0.644740, validation/loss=1.691160, validation/num_examples=50000
I0131 18:05:08.997828 139907754342144 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.5870686769485474, loss=3.2172296047210693
I0131 18:05:42.709197 139907745949440 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.4068758487701416, loss=3.2495927810668945
I0131 18:06:16.474184 139907754342144 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.3542237281799316, loss=3.214583396911621
I0131 18:06:50.291010 139907745949440 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.3772916793823242, loss=3.2017910480499268
I0131 18:07:24.032511 139907754342144 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.3580076694488525, loss=3.283184051513672
I0131 18:07:57.839288 139907745949440 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.4480115175247192, loss=3.235333204269409
I0131 18:08:31.609841 139907754342144 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.6823004484176636, loss=3.2197823524475098
I0131 18:09:05.335391 139907745949440 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.5588799715042114, loss=3.1917593479156494
I0131 18:09:39.102115 139907754342144 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.5588290691375732, loss=3.2956418991088867
I0131 18:10:12.838287 139907745949440 logging_writer.py:48] [32700] global_step=32700, grad_norm=1.4363023042678833, loss=3.2563672065734863
I0131 18:10:46.609428 139907754342144 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.333950161933899, loss=3.2732350826263428
I0131 18:11:20.313294 139907745949440 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.3691701889038086, loss=3.209690809249878
I0131 18:11:54.075281 139907754342144 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.2459853887557983, loss=3.3101327419281006
I0131 18:12:27.789455 139907745949440 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.3590716123580933, loss=3.200007200241089
I0131 18:13:01.640363 139907754342144 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.5797306299209595, loss=3.242588520050049
I0131 18:13:05.495667 140070692116288 spec.py:321] Evaluating on the training split.
I0131 18:13:11.734836 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 18:13:20.654286 140070692116288 spec.py:349] Evaluating on the test split.
I0131 18:13:23.128913 140070692116288 submission_runner.py:408] Time since start: 11662.90s, 	Step: 33213, 	{'train/accuracy': 0.6996970772743225, 'train/loss': 1.4557379484176636, 'validation/accuracy': 0.6355999708175659, 'validation/loss': 1.7373415231704712, 'validation/num_examples': 50000, 'test/accuracy': 0.5081000328063965, 'test/loss': 2.3862760066986084, 'test/num_examples': 10000, 'score': 11253.20901465416, 'total_duration': 11662.901600122452, 'accumulated_submission_time': 11253.20901465416, 'accumulated_eval_time': 407.86250853538513, 'accumulated_logging_time': 0.65826416015625}
I0131 18:13:23.151830 139908425447168 logging_writer.py:48] [33213] accumulated_eval_time=407.862509, accumulated_logging_time=0.658264, accumulated_submission_time=11253.209015, global_step=33213, preemption_count=0, score=11253.209015, test/accuracy=0.508100, test/loss=2.386276, test/num_examples=10000, total_duration=11662.901600, train/accuracy=0.699697, train/loss=1.455738, validation/accuracy=0.635600, validation/loss=1.737342, validation/num_examples=50000
I0131 18:13:52.841655 139908719027968 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.530003547668457, loss=3.255610466003418
I0131 18:14:26.541175 139908425447168 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.34014093875885, loss=3.237210988998413
I0131 18:15:00.299571 139908719027968 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.4721736907958984, loss=3.2037551403045654
I0131 18:15:34.021050 139908425447168 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.8805192708969116, loss=3.2426040172576904
I0131 18:16:07.745392 139908719027968 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.4851821660995483, loss=3.2735323905944824
I0131 18:16:41.514200 139908425447168 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.5336494445800781, loss=3.2941439151763916
I0131 18:17:15.259049 139908719027968 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.4543123245239258, loss=3.2700939178466797
I0131 18:17:49.004989 139908425447168 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.8959708213806152, loss=3.3211541175842285
I0131 18:18:22.684627 139908719027968 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.4116592407226562, loss=3.2050464153289795
I0131 18:18:56.457619 139908425447168 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.383005142211914, loss=3.2108571529388428
I0131 18:19:30.273368 139908719027968 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.5699595212936401, loss=3.194749593734741
I0131 18:20:03.984957 139908425447168 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.3968497514724731, loss=3.1743292808532715
I0131 18:20:37.752561 139908719027968 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.433725357055664, loss=3.2779784202575684
I0131 18:21:11.490174 139908425447168 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.4970132112503052, loss=3.238233804702759
I0131 18:21:45.193461 139908719027968 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.34019136428833, loss=3.215198040008545
I0131 18:21:53.452958 140070692116288 spec.py:321] Evaluating on the training split.
I0131 18:21:59.628229 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 18:22:08.750071 140070692116288 spec.py:349] Evaluating on the test split.
I0131 18:22:11.251342 140070692116288 submission_runner.py:408] Time since start: 12191.02s, 	Step: 34726, 	{'train/accuracy': 0.7174744606018066, 'train/loss': 1.3685293197631836, 'validation/accuracy': 0.6532599925994873, 'validation/loss': 1.6530615091323853, 'validation/num_examples': 50000, 'test/accuracy': 0.5192000269889832, 'test/loss': 2.314382553100586, 'test/num_examples': 10000, 'score': 11763.446279764175, 'total_duration': 12191.024013519287, 'accumulated_submission_time': 11763.446279764175, 'accumulated_eval_time': 425.66084122657776, 'accumulated_logging_time': 0.6921558380126953}
I0131 18:22:11.274520 139907754342144 logging_writer.py:48] [34726] accumulated_eval_time=425.660841, accumulated_logging_time=0.692156, accumulated_submission_time=11763.446280, global_step=34726, preemption_count=0, score=11763.446280, test/accuracy=0.519200, test/loss=2.314383, test/num_examples=10000, total_duration=12191.024014, train/accuracy=0.717474, train/loss=1.368529, validation/accuracy=0.653260, validation/loss=1.653062, validation/num_examples=50000
I0131 18:22:36.580374 139907762734848 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.6298151016235352, loss=3.218153476715088
I0131 18:23:10.268433 139907754342144 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.8320324420928955, loss=3.2020645141601562
I0131 18:23:44.044070 139907762734848 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.490033745765686, loss=3.250415325164795
I0131 18:24:17.814497 139907754342144 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.4236048460006714, loss=3.1687233448028564
I0131 18:24:51.561330 139907762734848 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.4732283353805542, loss=3.2530550956726074
I0131 18:25:25.398490 139907754342144 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.6128073930740356, loss=3.1021571159362793
I0131 18:25:59.154160 139907762734848 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.5764847993850708, loss=3.114640235900879
I0131 18:26:32.933696 139907754342144 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.5999747514724731, loss=3.2894654273986816
I0131 18:27:06.629669 139907762734848 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.3450809717178345, loss=3.262647867202759
I0131 18:27:40.429075 139907754342144 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.4416426420211792, loss=3.2898693084716797
I0131 18:28:14.203196 139907762734848 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.5032217502593994, loss=3.2281687259674072
I0131 18:28:48.000644 139907754342144 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.4274744987487793, loss=3.225670576095581
I0131 18:29:21.738114 139907762734848 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.5545188188552856, loss=3.2150070667266846
I0131 18:29:55.511888 139907754342144 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.4911670684814453, loss=3.2214722633361816
I0131 18:30:29.229599 139907762734848 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.345043659210205, loss=3.1965270042419434
I0131 18:30:41.565370 140070692116288 spec.py:321] Evaluating on the training split.
I0131 18:30:47.914512 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 18:30:56.670952 140070692116288 spec.py:349] Evaluating on the test split.
I0131 18:30:59.162831 140070692116288 submission_runner.py:408] Time since start: 12718.94s, 	Step: 36238, 	{'train/accuracy': 0.7438416481018066, 'train/loss': 1.2298567295074463, 'validation/accuracy': 0.6535599827766418, 'validation/loss': 1.6235462427139282, 'validation/num_examples': 50000, 'test/accuracy': 0.5265000462532043, 'test/loss': 2.281526565551758, 'test/num_examples': 10000, 'score': 12273.675210475922, 'total_duration': 12718.935508489609, 'accumulated_submission_time': 12273.675210475922, 'accumulated_eval_time': 443.2582674026489, 'accumulated_logging_time': 0.7246830463409424}
I0131 18:30:59.188641 139907745949440 logging_writer.py:48] [36238] accumulated_eval_time=443.258267, accumulated_logging_time=0.724683, accumulated_submission_time=12273.675210, global_step=36238, preemption_count=0, score=12273.675210, test/accuracy=0.526500, test/loss=2.281527, test/num_examples=10000, total_duration=12718.935508, train/accuracy=0.743842, train/loss=1.229857, validation/accuracy=0.653560, validation/loss=1.623546, validation/num_examples=50000
I0131 18:31:20.444020 139907754342144 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.6114451885223389, loss=3.276353120803833
I0131 18:31:54.216215 139907745949440 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.777561068534851, loss=3.2741539478302
I0131 18:32:27.959427 139907754342144 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.614920973777771, loss=3.1599907875061035
I0131 18:33:01.608262 139907745949440 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.474147081375122, loss=3.211949586868286
I0131 18:33:35.314681 139907754342144 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.4550871849060059, loss=3.2069449424743652
I0131 18:34:09.066512 139907745949440 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.5176095962524414, loss=3.2096362113952637
I0131 18:34:42.780116 139907754342144 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.4496058225631714, loss=3.2927138805389404
I0131 18:35:16.549160 139907745949440 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.6250666379928589, loss=3.2344565391540527
I0131 18:35:50.272210 139907754342144 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.5390974283218384, loss=3.189809799194336
I0131 18:36:24.035947 139907745949440 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.4004160165786743, loss=3.2429444789886475
I0131 18:36:57.739301 139907754342144 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.5682885646820068, loss=3.179461717605591
I0131 18:37:31.496140 139907745949440 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.9138822555541992, loss=3.232699155807495
I0131 18:38:05.268379 139907754342144 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.639122486114502, loss=3.20661997795105
I0131 18:38:39.026210 139907745949440 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.7072969675064087, loss=3.2711124420166016
I0131 18:39:12.753290 139907754342144 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.5455658435821533, loss=3.1863317489624023
I0131 18:39:29.441240 140070692116288 spec.py:321] Evaluating on the training split.
I0131 18:39:35.647014 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 18:39:44.528870 140070692116288 spec.py:349] Evaluating on the test split.
I0131 18:39:47.010539 140070692116288 submission_runner.py:408] Time since start: 13246.78s, 	Step: 37751, 	{'train/accuracy': 0.7174545526504517, 'train/loss': 1.3231170177459717, 'validation/accuracy': 0.6446200013160706, 'validation/loss': 1.666609764099121, 'validation/num_examples': 50000, 'test/accuracy': 0.5151000022888184, 'test/loss': 2.3460588455200195, 'test/num_examples': 10000, 'score': 12783.86525940895, 'total_duration': 13246.783225536346, 'accumulated_submission_time': 12783.86525940895, 'accumulated_eval_time': 460.8275353908539, 'accumulated_logging_time': 0.7599701881408691}
I0131 18:39:47.034151 139908425447168 logging_writer.py:48] [37751] accumulated_eval_time=460.827535, accumulated_logging_time=0.759970, accumulated_submission_time=12783.865259, global_step=37751, preemption_count=0, score=12783.865259, test/accuracy=0.515100, test/loss=2.346059, test/num_examples=10000, total_duration=13246.783226, train/accuracy=0.717455, train/loss=1.323117, validation/accuracy=0.644620, validation/loss=1.666610, validation/num_examples=50000
I0131 18:40:03.909801 139908727420672 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.5052937269210815, loss=3.1902918815612793
I0131 18:40:37.615234 139908425447168 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.5644211769104004, loss=3.242398262023926
I0131 18:41:11.308396 139908727420672 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.4287985563278198, loss=3.2217965126037598
I0131 18:41:45.052651 139908425447168 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.5439320802688599, loss=3.209601879119873
I0131 18:42:18.780436 139908727420672 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.574903130531311, loss=3.2082531452178955
I0131 18:42:52.534585 139908425447168 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.5987821817398071, loss=3.1956398487091064
I0131 18:43:26.251330 139908727420672 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.5848584175109863, loss=3.2417612075805664
I0131 18:44:00.010066 139908425447168 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.4163063764572144, loss=3.157754898071289
I0131 18:44:33.849642 139908727420672 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.597203016281128, loss=3.2335331439971924
I0131 18:45:07.569580 139908425447168 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.5304961204528809, loss=3.290787696838379
I0131 18:45:41.340604 139908727420672 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.6605079174041748, loss=3.199317455291748
I0131 18:46:15.051733 139908425447168 logging_writer.py:48] [38900] global_step=38900, grad_norm=2.0270297527313232, loss=3.204090118408203
I0131 18:46:48.832058 139908727420672 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.5533262491226196, loss=3.213372230529785
I0131 18:47:22.574599 139908425447168 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.6970592737197876, loss=3.2130701541900635
I0131 18:47:56.315694 139908727420672 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.5156352519989014, loss=3.224665403366089
I0131 18:48:17.024539 140070692116288 spec.py:321] Evaluating on the training split.
I0131 18:48:23.305581 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 18:48:32.376349 140070692116288 spec.py:349] Evaluating on the test split.
I0131 18:48:34.832439 140070692116288 submission_runner.py:408] Time since start: 13774.61s, 	Step: 39263, 	{'train/accuracy': 0.7337173223495483, 'train/loss': 1.286712884902954, 'validation/accuracy': 0.6584399938583374, 'validation/loss': 1.6077271699905396, 'validation/num_examples': 50000, 'test/accuracy': 0.5318000316619873, 'test/loss': 2.2832562923431396, 'test/num_examples': 10000, 'score': 13293.79348897934, 'total_duration': 13774.605125188828, 'accumulated_submission_time': 13293.79348897934, 'accumulated_eval_time': 478.63541746139526, 'accumulated_logging_time': 0.7926428318023682}
I0131 18:48:34.856204 139907737556736 logging_writer.py:48] [39263] accumulated_eval_time=478.635417, accumulated_logging_time=0.792643, accumulated_submission_time=13293.793489, global_step=39263, preemption_count=0, score=13293.793489, test/accuracy=0.531800, test/loss=2.283256, test/num_examples=10000, total_duration=13774.605125, train/accuracy=0.733717, train/loss=1.286713, validation/accuracy=0.658440, validation/loss=1.607727, validation/num_examples=50000
I0131 18:48:47.679045 139907745949440 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.5032602548599243, loss=3.1702935695648193
I0131 18:49:21.401179 139907737556736 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.641372799873352, loss=3.1159768104553223
I0131 18:49:55.078544 139907745949440 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.5888733863830566, loss=3.2527239322662354
I0131 18:50:28.885152 139907737556736 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.6359645128250122, loss=3.1784508228302
I0131 18:51:02.568125 139907745949440 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.5025289058685303, loss=3.2318859100341797
I0131 18:51:36.280713 139907737556736 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.6350024938583374, loss=3.24741792678833
I0131 18:52:10.003351 139907745949440 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.501477599143982, loss=3.161785840988159
I0131 18:52:43.761548 139907737556736 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.5446845293045044, loss=3.1883561611175537
I0131 18:53:17.500832 139907745949440 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.539227843284607, loss=3.2066662311553955
I0131 18:53:51.207419 139907737556736 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.5708341598510742, loss=3.213308811187744
I0131 18:54:24.948318 139907745949440 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.6148701906204224, loss=3.276653528213501
I0131 18:54:58.704525 139907737556736 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.4975591897964478, loss=3.1859629154205322
I0131 18:55:32.459126 139907745949440 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.5080510377883911, loss=3.256227970123291
I0131 18:56:06.187890 139907737556736 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.7985072135925293, loss=3.136049270629883
I0131 18:56:40.027122 139907745949440 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.3626171350479126, loss=3.173799991607666
I0131 18:57:04.834163 140070692116288 spec.py:321] Evaluating on the training split.
I0131 18:57:11.828276 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 18:57:20.826008 140070692116288 spec.py:349] Evaluating on the test split.
I0131 18:57:23.297220 140070692116288 submission_runner.py:408] Time since start: 14303.07s, 	Step: 40775, 	{'train/accuracy': 0.7258848547935486, 'train/loss': 1.2848888635635376, 'validation/accuracy': 0.6577799916267395, 'validation/loss': 1.597143530845642, 'validation/num_examples': 50000, 'test/accuracy': 0.5288000106811523, 'test/loss': 2.251645803451538, 'test/num_examples': 10000, 'score': 13803.70843219757, 'total_duration': 14303.069906711578, 'accumulated_submission_time': 13803.70843219757, 'accumulated_eval_time': 497.0984447002411, 'accumulated_logging_time': 0.8266785144805908}
I0131 18:57:23.321449 139907737556736 logging_writer.py:48] [40775] accumulated_eval_time=497.098445, accumulated_logging_time=0.826679, accumulated_submission_time=13803.708432, global_step=40775, preemption_count=0, score=13803.708432, test/accuracy=0.528800, test/loss=2.251646, test/num_examples=10000, total_duration=14303.069907, train/accuracy=0.725885, train/loss=1.284889, validation/accuracy=0.657780, validation/loss=1.597144, validation/num_examples=50000
I0131 18:57:32.096676 139908710635264 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.622757077217102, loss=3.183276653289795
I0131 18:58:05.778399 139907737556736 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.7994238138198853, loss=3.300919532775879
I0131 18:58:39.488845 139908710635264 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.6248468160629272, loss=3.1763038635253906
I0131 18:59:13.297894 139907737556736 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.6011375188827515, loss=3.1323964595794678
I0131 18:59:46.968303 139908710635264 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.706817865371704, loss=3.214972496032715
I0131 19:00:20.755575 139907737556736 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.6019318103790283, loss=3.2119336128234863
I0131 19:00:54.454273 139908710635264 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.543542504310608, loss=3.1531929969787598
I0131 19:01:28.249109 139907737556736 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.4880889654159546, loss=3.1692395210266113
I0131 19:02:01.988704 139908710635264 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.534895896911621, loss=3.204300880432129
I0131 19:02:35.742344 139907737556736 logging_writer.py:48] [41700] global_step=41700, grad_norm=1.7313263416290283, loss=3.211639881134033
I0131 19:03:09.562154 139908710635264 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.7203443050384521, loss=3.144773006439209
I0131 19:03:43.233784 139907737556736 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.612998366355896, loss=3.147975444793701
I0131 19:04:16.968569 139908710635264 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.7133886814117432, loss=3.2077879905700684
I0131 19:04:50.700364 139907737556736 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.812171459197998, loss=3.210508346557617
I0131 19:05:24.385097 139908710635264 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.665669322013855, loss=3.2058682441711426
I0131 19:05:53.606009 140070692116288 spec.py:321] Evaluating on the training split.
I0131 19:05:59.752275 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 19:06:08.991476 140070692116288 spec.py:349] Evaluating on the test split.
I0131 19:06:11.415768 140070692116288 submission_runner.py:408] Time since start: 14831.19s, 	Step: 42288, 	{'train/accuracy': 0.7098811864852905, 'train/loss': 1.3791455030441284, 'validation/accuracy': 0.6428200006484985, 'validation/loss': 1.6814510822296143, 'validation/num_examples': 50000, 'test/accuracy': 0.5174000263214111, 'test/loss': 2.3248226642608643, 'test/num_examples': 10000, 'score': 14313.930534362793, 'total_duration': 14831.188447713852, 'accumulated_submission_time': 14313.930534362793, 'accumulated_eval_time': 514.9081664085388, 'accumulated_logging_time': 0.86090087890625}
I0131 19:06:11.440705 139907754342144 logging_writer.py:48] [42288] accumulated_eval_time=514.908166, accumulated_logging_time=0.860901, accumulated_submission_time=14313.930534, global_step=42288, preemption_count=0, score=14313.930534, test/accuracy=0.517400, test/loss=2.324823, test/num_examples=10000, total_duration=14831.188448, train/accuracy=0.709881, train/loss=1.379146, validation/accuracy=0.642820, validation/loss=1.681451, validation/num_examples=50000
I0131 19:06:15.835508 139907762734848 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.5679339170455933, loss=3.057352304458618
I0131 19:06:49.569458 139907754342144 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.6204348802566528, loss=3.1271016597747803
I0131 19:07:23.304019 139907762734848 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.4410496950149536, loss=3.194228172302246
I0131 19:07:56.981409 139907754342144 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.6934781074523926, loss=3.160172939300537
I0131 19:08:30.705217 139907762734848 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.7915542125701904, loss=3.146157741546631
I0131 19:09:04.457123 139907754342144 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.6880502700805664, loss=3.221970558166504
I0131 19:09:38.218303 139907762734848 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.6031243801116943, loss=3.165149688720703
I0131 19:10:11.895794 139907754342144 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.6583456993103027, loss=3.220829486846924
I0131 19:10:45.628807 139907762734848 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.634138822555542, loss=3.200746536254883
I0131 19:11:19.364820 139907754342144 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.8116592168807983, loss=3.1708924770355225
I0131 19:11:53.121102 139907762734848 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.6724587678909302, loss=3.164576768875122
I0131 19:12:26.871465 139907754342144 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.5677385330200195, loss=3.1500563621520996
I0131 19:13:00.597844 139907762734848 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.7278368473052979, loss=3.1738593578338623
I0131 19:13:34.364668 139907754342144 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.553911805152893, loss=3.095531940460205
I0131 19:14:08.093429 139907762734848 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.722116470336914, loss=3.1274333000183105
I0131 19:14:41.846590 139907754342144 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.599710464477539, loss=3.2037291526794434
I0131 19:14:41.855280 140070692116288 spec.py:321] Evaluating on the training split.
I0131 19:14:48.072768 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 19:14:56.939622 140070692116288 spec.py:349] Evaluating on the test split.
I0131 19:14:59.337919 140070692116288 submission_runner.py:408] Time since start: 15359.11s, 	Step: 43801, 	{'train/accuracy': 0.7302295565605164, 'train/loss': 1.3141027688980103, 'validation/accuracy': 0.6615200042724609, 'validation/loss': 1.6093543767929077, 'validation/num_examples': 50000, 'test/accuracy': 0.5326000452041626, 'test/loss': 2.2799079418182373, 'test/num_examples': 10000, 'score': 14824.281010389328, 'total_duration': 15359.110605239868, 'accumulated_submission_time': 14824.281010389328, 'accumulated_eval_time': 532.3907444477081, 'accumulated_logging_time': 0.8976097106933594}
I0131 19:14:59.362564 139907754342144 logging_writer.py:48] [43801] accumulated_eval_time=532.390744, accumulated_logging_time=0.897610, accumulated_submission_time=14824.281010, global_step=43801, preemption_count=0, score=14824.281010, test/accuracy=0.532600, test/loss=2.279908, test/num_examples=10000, total_duration=15359.110605, train/accuracy=0.730230, train/loss=1.314103, validation/accuracy=0.661520, validation/loss=1.609354, validation/num_examples=50000
I0131 19:15:33.109954 139908710635264 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.9116050004959106, loss=3.1678147315979004
I0131 19:16:06.806591 139907754342144 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.611356496810913, loss=3.2299020290374756
I0131 19:16:40.533954 139908710635264 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.8473399877548218, loss=3.144886016845703
I0131 19:17:14.281900 139907754342144 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.8479948043823242, loss=3.1755144596099854
I0131 19:17:48.019550 139908710635264 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.7534873485565186, loss=3.151975631713867
I0131 19:18:21.754478 139907754342144 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.8509942293167114, loss=3.1643311977386475
I0131 19:18:55.518764 139908710635264 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.7824195623397827, loss=3.2154898643493652
I0131 19:19:29.205417 139907754342144 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.6969964504241943, loss=3.229556083679199
I0131 19:20:02.969851 139908710635264 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.6796468496322632, loss=3.23207950592041
I0131 19:20:36.679297 139907754342144 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.6106849908828735, loss=3.081261396408081
I0131 19:21:10.443399 139908710635264 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.7448612451553345, loss=3.144158124923706
I0131 19:21:44.210105 139907754342144 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.5593372583389282, loss=3.2568318843841553
I0131 19:22:17.958558 139908710635264 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.8548537492752075, loss=3.1334211826324463
I0131 19:22:51.677504 139907754342144 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.9273779392242432, loss=3.15908145904541
I0131 19:23:25.436692 139908710635264 logging_writer.py:48] [45300] global_step=45300, grad_norm=2.045461893081665, loss=3.189362049102783
I0131 19:23:29.629836 140070692116288 spec.py:321] Evaluating on the training split.
I0131 19:23:35.754217 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 19:23:44.754120 140070692116288 spec.py:349] Evaluating on the test split.
I0131 19:23:47.261697 140070692116288 submission_runner.py:408] Time since start: 15887.03s, 	Step: 45314, 	{'train/accuracy': 0.7517538070678711, 'train/loss': 1.200649619102478, 'validation/accuracy': 0.6576399803161621, 'validation/loss': 1.604137659072876, 'validation/num_examples': 50000, 'test/accuracy': 0.5347000360488892, 'test/loss': 2.256589412689209, 'test/num_examples': 10000, 'score': 15334.48612332344, 'total_duration': 15887.034386634827, 'accumulated_submission_time': 15334.48612332344, 'accumulated_eval_time': 550.0225744247437, 'accumulated_logging_time': 0.9313144683837891}
I0131 19:23:47.289056 139907737556736 logging_writer.py:48] [45314] accumulated_eval_time=550.022574, accumulated_logging_time=0.931314, accumulated_submission_time=15334.486123, global_step=45314, preemption_count=0, score=15334.486123, test/accuracy=0.534700, test/loss=2.256589, test/num_examples=10000, total_duration=15887.034387, train/accuracy=0.751754, train/loss=1.200650, validation/accuracy=0.657640, validation/loss=1.604138, validation/num_examples=50000
I0131 19:24:16.593798 139907745949440 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.5783430337905884, loss=3.162729501724243
I0131 19:24:50.266484 139907737556736 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.7131553888320923, loss=3.1896495819091797
I0131 19:25:23.956402 139907745949440 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.701551914215088, loss=3.20727801322937
I0131 19:25:57.740147 139907737556736 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.5882298946380615, loss=3.2048659324645996
I0131 19:26:31.451741 139907745949440 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.6614822149276733, loss=3.17026424407959
I0131 19:27:05.141627 139907737556736 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.784214973449707, loss=3.139648675918579
I0131 19:27:38.857296 139907745949440 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.646396517753601, loss=3.190176486968994
I0131 19:28:12.802323 139907737556736 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.7563475370407104, loss=3.2360024452209473
I0131 19:28:46.491544 139907745949440 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.9128780364990234, loss=3.151933431625366
I0131 19:29:20.263732 139907737556736 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.7412962913513184, loss=3.079026460647583
I0131 19:29:53.978661 139907745949440 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.9292597770690918, loss=3.2774202823638916
I0131 19:30:27.751403 139907737556736 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.9980535507202148, loss=3.14554500579834
I0131 19:31:01.458211 139907745949440 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.7358570098876953, loss=3.2157533168792725
I0131 19:31:35.145845 139907737556736 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.7126305103302002, loss=3.101336717605591
I0131 19:32:08.819352 139907745949440 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.6793880462646484, loss=3.213770866394043
I0131 19:32:17.397367 140070692116288 spec.py:321] Evaluating on the training split.
I0131 19:32:23.578021 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 19:32:32.617220 140070692116288 spec.py:349] Evaluating on the test split.
I0131 19:32:35.124888 140070692116288 submission_runner.py:408] Time since start: 16414.90s, 	Step: 46827, 	{'train/accuracy': 0.7456353306770325, 'train/loss': 1.2059592008590698, 'validation/accuracy': 0.6634799838066101, 'validation/loss': 1.5565850734710693, 'validation/num_examples': 50000, 'test/accuracy': 0.539900004863739, 'test/loss': 2.1983628273010254, 'test/num_examples': 10000, 'score': 15844.531326293945, 'total_duration': 16414.89757823944, 'accumulated_submission_time': 15844.531326293945, 'accumulated_eval_time': 567.7500638961792, 'accumulated_logging_time': 0.9677863121032715}
I0131 19:32:35.149754 139907729164032 logging_writer.py:48] [46827] accumulated_eval_time=567.750064, accumulated_logging_time=0.967786, accumulated_submission_time=15844.531326, global_step=46827, preemption_count=0, score=15844.531326, test/accuracy=0.539900, test/loss=2.198363, test/num_examples=10000, total_duration=16414.897578, train/accuracy=0.745635, train/loss=1.205959, validation/accuracy=0.663480, validation/loss=1.556585, validation/num_examples=50000
I0131 19:33:00.105893 139907737556736 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.6081117391586304, loss=3.2066588401794434
I0131 19:33:33.795310 139907729164032 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.597851037979126, loss=3.1020467281341553
I0131 19:34:07.708925 139907737556736 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.6653656959533691, loss=3.1292898654937744
I0131 19:34:41.423629 139907729164032 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.704846978187561, loss=3.1293065547943115
I0131 19:35:15.182923 139907737556736 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.7409188747406006, loss=3.17207670211792
I0131 19:35:48.942066 139907729164032 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.6577461957931519, loss=3.1095542907714844
I0131 19:36:22.680385 139907737556736 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.6116180419921875, loss=3.1474435329437256
I0131 19:36:56.435014 139907729164032 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.7255600690841675, loss=3.2421810626983643
I0131 19:37:30.162857 139907737556736 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.6251996755599976, loss=3.103837013244629
I0131 19:38:03.935726 139907729164032 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.6686267852783203, loss=3.1690783500671387
I0131 19:38:37.659653 139907737556736 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.823854684829712, loss=3.1083717346191406
I0131 19:39:11.399981 139907729164032 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.7496049404144287, loss=3.132638454437256
I0131 19:39:45.135206 139907737556736 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.7854878902435303, loss=3.1002442836761475
I0131 19:40:18.910190 139907729164032 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.7354916334152222, loss=3.212282657623291
I0131 19:40:52.633107 139907737556736 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.6644113063812256, loss=3.2119195461273193
I0131 19:41:05.282851 140070692116288 spec.py:321] Evaluating on the training split.
I0131 19:41:11.540589 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 19:41:20.252435 140070692116288 spec.py:349] Evaluating on the test split.
I0131 19:41:22.735671 140070692116288 submission_runner.py:408] Time since start: 16942.51s, 	Step: 48339, 	{'train/accuracy': 0.735750138759613, 'train/loss': 1.2548892498016357, 'validation/accuracy': 0.6612600088119507, 'validation/loss': 1.5838819742202759, 'validation/num_examples': 50000, 'test/accuracy': 0.5368000268936157, 'test/loss': 2.2433271408081055, 'test/num_examples': 10000, 'score': 16354.600351333618, 'total_duration': 16942.508352041245, 'accumulated_submission_time': 16354.600351333618, 'accumulated_eval_time': 585.2028439044952, 'accumulated_logging_time': 1.00309419631958}
I0131 19:41:22.760806 139908710635264 logging_writer.py:48] [48339] accumulated_eval_time=585.202844, accumulated_logging_time=1.003094, accumulated_submission_time=16354.600351, global_step=48339, preemption_count=0, score=16354.600351, test/accuracy=0.536800, test/loss=2.243327, test/num_examples=10000, total_duration=16942.508352, train/accuracy=0.735750, train/loss=1.254889, validation/accuracy=0.661260, validation/loss=1.583882, validation/num_examples=50000
I0131 19:41:43.662475 139908719027968 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.7026536464691162, loss=3.191389560699463
I0131 19:42:17.340468 139908710635264 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.7091035842895508, loss=3.201601505279541
I0131 19:42:51.140961 139908719027968 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.6982886791229248, loss=3.213301658630371
I0131 19:43:24.855885 139908710635264 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.6890461444854736, loss=3.095646858215332
I0131 19:43:58.605409 139908719027968 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.8381197452545166, loss=3.202676296234131
I0131 19:44:32.329371 139908710635264 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.6727452278137207, loss=3.1035947799682617
I0131 19:45:06.123993 139908719027968 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.6745190620422363, loss=3.127457857131958
I0131 19:45:39.816817 139908710635264 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.894390344619751, loss=3.1506099700927734
I0131 19:46:13.596213 139908719027968 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.6552037000656128, loss=3.145040273666382
I0131 19:46:47.441334 139908710635264 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.860404372215271, loss=3.080456018447876
I0131 19:47:21.128195 139908719027968 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.7179038524627686, loss=3.1506314277648926
I0131 19:47:54.865980 139908710635264 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.8528578281402588, loss=3.1018972396850586
I0131 19:48:28.595392 139908719027968 logging_writer.py:48] [49600] global_step=49600, grad_norm=2.0740833282470703, loss=3.125267744064331
I0131 19:49:02.277194 139908710635264 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.8202687501907349, loss=3.12532114982605
I0131 19:49:35.970501 139908719027968 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.7132643461227417, loss=3.073240041732788
I0131 19:49:52.949845 140070692116288 spec.py:321] Evaluating on the training split.
I0131 19:49:59.212465 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 19:50:08.289172 140070692116288 spec.py:349] Evaluating on the test split.
I0131 19:50:10.807042 140070692116288 submission_runner.py:408] Time since start: 17470.58s, 	Step: 49852, 	{'train/accuracy': 0.7357302308082581, 'train/loss': 1.2622418403625488, 'validation/accuracy': 0.6646999716758728, 'validation/loss': 1.571779489517212, 'validation/num_examples': 50000, 'test/accuracy': 0.534000039100647, 'test/loss': 2.243760108947754, 'test/num_examples': 10000, 'score': 16864.72789144516, 'total_duration': 17470.579726219177, 'accumulated_submission_time': 16864.72789144516, 'accumulated_eval_time': 603.0600016117096, 'accumulated_logging_time': 1.037532091140747}
I0131 19:50:10.833575 139907754342144 logging_writer.py:48] [49852] accumulated_eval_time=603.060002, accumulated_logging_time=1.037532, accumulated_submission_time=16864.727891, global_step=49852, preemption_count=0, score=16864.727891, test/accuracy=0.534000, test/loss=2.243760, test/num_examples=10000, total_duration=17470.579726, train/accuracy=0.735730, train/loss=1.262242, validation/accuracy=0.664700, validation/loss=1.571779, validation/num_examples=50000
I0131 19:50:27.372494 139907762734848 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.6890143156051636, loss=3.092632293701172
I0131 19:51:01.103295 139907754342144 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.8142215013504028, loss=3.152445077896118
I0131 19:51:34.791467 139907762734848 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.655824899673462, loss=3.129610300064087
I0131 19:52:08.501032 139907754342144 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.7762342691421509, loss=3.1739842891693115
I0131 19:52:42.264860 139907762734848 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.935774803161621, loss=3.1291050910949707
I0131 19:53:16.140617 139907754342144 logging_writer.py:48] [50400] global_step=50400, grad_norm=2.018630027770996, loss=3.189558744430542
I0131 19:53:49.914306 139907762734848 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.717932105064392, loss=3.074039936065674
I0131 19:54:23.636911 139907754342144 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.9394911527633667, loss=3.1647374629974365
I0131 19:54:57.399382 139907762734848 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.7196779251098633, loss=3.111004590988159
I0131 19:55:31.108154 139907754342144 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.8343191146850586, loss=3.1939046382904053
I0131 19:56:04.846586 139907762734848 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.9244987964630127, loss=3.2463340759277344
I0131 19:56:38.581123 139907754342144 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.890641450881958, loss=3.2045204639434814
I0131 19:57:12.305896 139907762734848 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.7768938541412354, loss=3.1911838054656982
I0131 19:57:46.068129 139907754342144 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.7546312808990479, loss=3.125025510787964
I0131 19:58:19.747948 139907762734848 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.9874686002731323, loss=3.177570343017578
I0131 19:58:41.126722 140070692116288 spec.py:321] Evaluating on the training split.
I0131 19:58:47.289701 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 19:58:56.381301 140070692116288 spec.py:349] Evaluating on the test split.
I0131 19:58:59.313567 140070692116288 submission_runner.py:408] Time since start: 17999.09s, 	Step: 51365, 	{'train/accuracy': 0.7381417155265808, 'train/loss': 1.2553119659423828, 'validation/accuracy': 0.6695399880409241, 'validation/loss': 1.566043734550476, 'validation/num_examples': 50000, 'test/accuracy': 0.5473000407218933, 'test/loss': 2.198474884033203, 'test/num_examples': 10000, 'score': 17374.956661462784, 'total_duration': 17999.086223602295, 'accumulated_submission_time': 17374.956661462784, 'accumulated_eval_time': 621.2467834949493, 'accumulated_logging_time': 1.0746331214904785}
I0131 19:58:59.339729 139907737556736 logging_writer.py:48] [51365] accumulated_eval_time=621.246783, accumulated_logging_time=1.074633, accumulated_submission_time=17374.956661, global_step=51365, preemption_count=0, score=17374.956661, test/accuracy=0.547300, test/loss=2.198475, test/num_examples=10000, total_duration=17999.086224, train/accuracy=0.738142, train/loss=1.255312, validation/accuracy=0.669540, validation/loss=1.566044, validation/num_examples=50000
I0131 19:59:11.488690 139907745949440 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.843711495399475, loss=3.1386570930480957
I0131 19:59:45.195882 139907737556736 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.8527899980545044, loss=3.2092249393463135
I0131 20:00:18.915592 139907745949440 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.8242671489715576, loss=3.116401195526123
I0131 20:00:52.614799 139907737556736 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.7946010828018188, loss=3.188837766647339
I0131 20:01:26.343415 139907745949440 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.6818382740020752, loss=3.1893715858459473
I0131 20:02:00.095348 139907737556736 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.936978816986084, loss=3.167797327041626
I0131 20:02:33.846625 139907745949440 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.7766996622085571, loss=3.157048463821411
I0131 20:03:07.576362 139907737556736 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.8102372884750366, loss=3.1672377586364746
I0131 20:03:41.345289 139907745949440 logging_writer.py:48] [52200] global_step=52200, grad_norm=2.0076959133148193, loss=3.151165723800659
I0131 20:04:15.072913 139907737556736 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.6811167001724243, loss=3.1619155406951904
I0131 20:04:48.796368 139907745949440 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.854089617729187, loss=3.1466944217681885
I0131 20:05:22.650733 139907737556736 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.866194725036621, loss=3.164743423461914
I0131 20:05:56.362032 139907745949440 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.802322268486023, loss=3.234164237976074
I0131 20:06:30.149859 139907737556736 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.9137688875198364, loss=3.170145034790039
I0131 20:07:03.849884 139907745949440 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.7903634309768677, loss=3.1248865127563477
I0131 20:07:29.348874 140070692116288 spec.py:321] Evaluating on the training split.
I0131 20:07:35.582906 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 20:07:44.664260 140070692116288 spec.py:349] Evaluating on the test split.
I0131 20:07:47.115633 140070692116288 submission_runner.py:408] Time since start: 18526.89s, 	Step: 52877, 	{'train/accuracy': 0.7241908311843872, 'train/loss': 1.344178318977356, 'validation/accuracy': 0.6525599956512451, 'validation/loss': 1.6665358543395996, 'validation/num_examples': 50000, 'test/accuracy': 0.5195000171661377, 'test/loss': 2.337864875793457, 'test/num_examples': 10000, 'score': 17884.904060840607, 'total_duration': 18526.888315439224, 'accumulated_submission_time': 17884.904060840607, 'accumulated_eval_time': 639.0135197639465, 'accumulated_logging_time': 1.109938621520996}
I0131 20:07:47.142297 139907762734848 logging_writer.py:48] [52877] accumulated_eval_time=639.013520, accumulated_logging_time=1.109939, accumulated_submission_time=17884.904061, global_step=52877, preemption_count=0, score=17884.904061, test/accuracy=0.519500, test/loss=2.337865, test/num_examples=10000, total_duration=18526.888315, train/accuracy=0.724191, train/loss=1.344178, validation/accuracy=0.652560, validation/loss=1.666536, validation/num_examples=50000
I0131 20:07:55.250087 139908425447168 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.7089499235153198, loss=3.120757579803467
I0131 20:08:28.936331 139907762734848 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.9100862741470337, loss=3.16994309425354
I0131 20:09:02.627116 139908425447168 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.873116135597229, loss=3.118528366088867
I0131 20:09:36.392047 139907762734848 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.9478272199630737, loss=3.142726182937622
I0131 20:10:10.136692 139908425447168 logging_writer.py:48] [53300] global_step=53300, grad_norm=2.1646065711975098, loss=3.0856776237487793
I0131 20:10:43.911461 139907762734848 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.7539373636245728, loss=3.1217620372772217
I0131 20:11:17.603276 139908425447168 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.9667339324951172, loss=3.090524673461914
I0131 20:11:51.416943 139907762734848 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.7194291353225708, loss=3.0189316272735596
I0131 20:12:25.113267 139908425447168 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.9489372968673706, loss=3.0723609924316406
I0131 20:12:58.876780 139907762734848 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.8296138048171997, loss=3.135596752166748
I0131 20:13:32.632858 139908425447168 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.8073726892471313, loss=3.1295723915100098
I0131 20:14:06.356407 139907762734848 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.8761066198349, loss=3.1432294845581055
I0131 20:14:40.060206 139908425447168 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.7871681451797485, loss=3.1475725173950195
I0131 20:15:13.835809 139907762734848 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.8483599424362183, loss=3.1243135929107666
I0131 20:15:47.562291 139908425447168 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.7580357789993286, loss=3.0853958129882812
I0131 20:16:17.419281 140070692116288 spec.py:321] Evaluating on the training split.
I0131 20:16:23.576805 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 20:16:32.589707 140070692116288 spec.py:349] Evaluating on the test split.
I0131 20:16:35.077288 140070692116288 submission_runner.py:408] Time since start: 19054.85s, 	Step: 54390, 	{'train/accuracy': 0.750996470451355, 'train/loss': 1.2404024600982666, 'validation/accuracy': 0.6648600101470947, 'validation/loss': 1.6224167346954346, 'validation/num_examples': 50000, 'test/accuracy': 0.5393000245094299, 'test/loss': 2.2847721576690674, 'test/num_examples': 10000, 'score': 18395.117757558823, 'total_duration': 19054.84995698929, 'accumulated_submission_time': 18395.117757558823, 'accumulated_eval_time': 656.6714797019958, 'accumulated_logging_time': 1.147374153137207}
I0131 20:16:35.104136 139907745949440 logging_writer.py:48] [54390] accumulated_eval_time=656.671480, accumulated_logging_time=1.147374, accumulated_submission_time=18395.117758, global_step=54390, preemption_count=0, score=18395.117758, test/accuracy=0.539300, test/loss=2.284772, test/num_examples=10000, total_duration=19054.849957, train/accuracy=0.750996, train/loss=1.240402, validation/accuracy=0.664860, validation/loss=1.622417, validation/num_examples=50000
I0131 20:16:38.818914 139907754342144 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.917893409729004, loss=3.128959894180298
I0131 20:17:12.462218 139907745949440 logging_writer.py:48] [54500] global_step=54500, grad_norm=2.1749541759490967, loss=3.11869215965271
I0131 20:17:46.359559 139907754342144 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.7052972316741943, loss=3.151137351989746
I0131 20:18:20.096491 139907745949440 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.9627575874328613, loss=3.1303558349609375
I0131 20:18:53.849007 139907754342144 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.912767767906189, loss=3.185622215270996
I0131 20:19:27.596400 139907745949440 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.9350910186767578, loss=3.0856454372406006
I0131 20:20:01.324709 139907754342144 logging_writer.py:48] [55000] global_step=55000, grad_norm=2.0523056983947754, loss=3.1446659564971924
I0131 20:20:35.084029 139907745949440 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.8015023469924927, loss=3.2198333740234375
I0131 20:21:08.806944 139907754342144 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.9783291816711426, loss=3.1374075412750244
I0131 20:21:42.499100 139907745949440 logging_writer.py:48] [55300] global_step=55300, grad_norm=2.0091309547424316, loss=3.146867036819458
I0131 20:22:16.265304 139907754342144 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.8741977214813232, loss=3.1291372776031494
I0131 20:22:50.031323 139907745949440 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.7525039911270142, loss=3.1653897762298584
I0131 20:23:23.710012 139907754342144 logging_writer.py:48] [55600] global_step=55600, grad_norm=2.070436477661133, loss=3.0716657638549805
I0131 20:23:57.500238 139907745949440 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.9835401773452759, loss=3.160842180252075
I0131 20:24:31.301601 139907754342144 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.840038776397705, loss=3.232194662094116
I0131 20:25:05.063040 139907745949440 logging_writer.py:48] [55900] global_step=55900, grad_norm=2.0152359008789062, loss=3.0346899032592773
I0131 20:25:05.218882 140070692116288 spec.py:321] Evaluating on the training split.
I0131 20:25:11.451307 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 20:25:20.483911 140070692116288 spec.py:349] Evaluating on the test split.
I0131 20:25:22.971286 140070692116288 submission_runner.py:408] Time since start: 19582.74s, 	Step: 55902, 	{'train/accuracy': 0.7460737824440002, 'train/loss': 1.2308549880981445, 'validation/accuracy': 0.6655600070953369, 'validation/loss': 1.5786885023117065, 'validation/num_examples': 50000, 'test/accuracy': 0.5325000286102295, 'test/loss': 2.248415231704712, 'test/num_examples': 10000, 'score': 18905.16711997986, 'total_duration': 19582.74394917488, 'accumulated_submission_time': 18905.16711997986, 'accumulated_eval_time': 674.4238469600677, 'accumulated_logging_time': 1.1857051849365234}
I0131 20:25:22.998630 139907737556736 logging_writer.py:48] [55902] accumulated_eval_time=674.423847, accumulated_logging_time=1.185705, accumulated_submission_time=18905.167120, global_step=55902, preemption_count=0, score=18905.167120, test/accuracy=0.532500, test/loss=2.248415, test/num_examples=10000, total_duration=19582.743949, train/accuracy=0.746074, train/loss=1.230855, validation/accuracy=0.665560, validation/loss=1.578689, validation/num_examples=50000
I0131 20:25:56.393727 139908710635264 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.9404150247573853, loss=3.0770514011383057
I0131 20:26:30.098363 139907737556736 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.8453162908554077, loss=3.0197594165802
I0131 20:27:03.852221 139908710635264 logging_writer.py:48] [56200] global_step=56200, grad_norm=2.0843167304992676, loss=3.17518949508667
I0131 20:27:37.637582 139907737556736 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.894003987312317, loss=3.1736795902252197
I0131 20:28:11.360608 139908710635264 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.863309383392334, loss=3.117335796356201
I0131 20:28:45.128903 139907737556736 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.8473893404006958, loss=3.0970001220703125
I0131 20:29:18.867841 139908710635264 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.9352271556854248, loss=3.139721632003784
I0131 20:29:52.606028 139907737556736 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.9426361322402954, loss=3.1110501289367676
I0131 20:30:26.500999 139908710635264 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.7310158014297485, loss=3.1400704383850098
I0131 20:31:00.264404 139907737556736 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.8618768453598022, loss=3.1784002780914307
I0131 20:31:34.007546 139908710635264 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.7935879230499268, loss=3.1182737350463867
I0131 20:32:07.754256 139907737556736 logging_writer.py:48] [57100] global_step=57100, grad_norm=2.02335786819458, loss=3.1255645751953125
I0131 20:32:41.463718 139908710635264 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.89125394821167, loss=3.185081958770752
I0131 20:33:15.223326 139907737556736 logging_writer.py:48] [57300] global_step=57300, grad_norm=2.0117266178131104, loss=3.1752548217773438
I0131 20:33:48.949885 139908710635264 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.925127387046814, loss=3.068027973175049
I0131 20:33:53.160147 140070692116288 spec.py:321] Evaluating on the training split.
I0131 20:33:59.510931 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 20:34:08.305488 140070692116288 spec.py:349] Evaluating on the test split.
I0131 20:34:10.800383 140070692116288 submission_runner.py:408] Time since start: 20110.57s, 	Step: 57414, 	{'train/accuracy': 0.7460139989852905, 'train/loss': 1.2035185098648071, 'validation/accuracy': 0.6686199903488159, 'validation/loss': 1.5482203960418701, 'validation/num_examples': 50000, 'test/accuracy': 0.5427000522613525, 'test/loss': 2.203951358795166, 'test/num_examples': 10000, 'score': 19415.267368793488, 'total_duration': 20110.572977542877, 'accumulated_submission_time': 19415.267368793488, 'accumulated_eval_time': 692.0639700889587, 'accumulated_logging_time': 1.2219395637512207}
I0131 20:34:10.826943 139907729164032 logging_writer.py:48] [57414] accumulated_eval_time=692.063970, accumulated_logging_time=1.221940, accumulated_submission_time=19415.267369, global_step=57414, preemption_count=0, score=19415.267369, test/accuracy=0.542700, test/loss=2.203951, test/num_examples=10000, total_duration=20110.572978, train/accuracy=0.746014, train/loss=1.203519, validation/accuracy=0.668620, validation/loss=1.548220, validation/num_examples=50000
I0131 20:34:40.164763 139907737556736 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.8162341117858887, loss=3.077815294265747
I0131 20:35:13.831099 139907729164032 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.9721705913543701, loss=3.1406681537628174
I0131 20:35:47.549744 139907737556736 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.9482395648956299, loss=3.1369290351867676
I0131 20:36:21.299249 139907729164032 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.9682611227035522, loss=3.1583428382873535
I0131 20:36:55.091449 139907737556736 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.9118120670318604, loss=3.0434083938598633
I0131 20:37:28.824971 139907729164032 logging_writer.py:48] [58000] global_step=58000, grad_norm=2.0840485095977783, loss=3.160098075866699
I0131 20:38:02.563471 139907737556736 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.9025955200195312, loss=3.1656875610351562
I0131 20:38:36.237327 139907729164032 logging_writer.py:48] [58200] global_step=58200, grad_norm=2.13566517829895, loss=3.101104736328125
I0131 20:39:10.018034 139907737556736 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.9533319473266602, loss=3.0663130283355713
I0131 20:39:43.769223 139907729164032 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.9076420068740845, loss=3.142160654067993
I0131 20:40:17.517326 139907737556736 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.906704068183899, loss=3.1442995071411133
I0131 20:40:51.285922 139907729164032 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.952866792678833, loss=3.117217540740967
I0131 20:41:25.006791 139907737556736 logging_writer.py:48] [58700] global_step=58700, grad_norm=2.0477075576782227, loss=3.1214263439178467
I0131 20:41:58.752026 139907729164032 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.781565546989441, loss=3.149836778640747
I0131 20:42:32.630370 139907737556736 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.7771793603897095, loss=3.0993080139160156
I0131 20:42:40.869035 140070692116288 spec.py:321] Evaluating on the training split.
I0131 20:42:47.071625 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 20:42:56.052001 140070692116288 spec.py:349] Evaluating on the test split.
I0131 20:42:58.515242 140070692116288 submission_runner.py:408] Time since start: 20638.29s, 	Step: 58926, 	{'train/accuracy': 0.7444595098495483, 'train/loss': 1.2356679439544678, 'validation/accuracy': 0.6713399887084961, 'validation/loss': 1.556386947631836, 'validation/num_examples': 50000, 'test/accuracy': 0.5497000217437744, 'test/loss': 2.2031824588775635, 'test/num_examples': 10000, 'score': 19925.24661397934, 'total_duration': 20638.287900447845, 'accumulated_submission_time': 19925.24661397934, 'accumulated_eval_time': 709.7101130485535, 'accumulated_logging_time': 1.257903814315796}
I0131 20:42:58.546891 139907737556736 logging_writer.py:48] [58926] accumulated_eval_time=709.710113, accumulated_logging_time=1.257904, accumulated_submission_time=19925.246614, global_step=58926, preemption_count=0, score=19925.246614, test/accuracy=0.549700, test/loss=2.203182, test/num_examples=10000, total_duration=20638.287900, train/accuracy=0.744460, train/loss=1.235668, validation/accuracy=0.671340, validation/loss=1.556387, validation/num_examples=50000
I0131 20:43:23.891320 139907754342144 logging_writer.py:48] [59000] global_step=59000, grad_norm=2.1819448471069336, loss=3.1070775985717773
I0131 20:43:57.574756 139907737556736 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.9359849691390991, loss=3.1769604682922363
I0131 20:44:31.285637 139907754342144 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.9010980129241943, loss=3.1824355125427246
I0131 20:45:05.046351 139907737556736 logging_writer.py:48] [59300] global_step=59300, grad_norm=2.090627670288086, loss=3.1419472694396973
I0131 20:45:38.751916 139907754342144 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.9814355373382568, loss=3.2100207805633545
I0131 20:46:12.517893 139907737556736 logging_writer.py:48] [59500] global_step=59500, grad_norm=2.0778510570526123, loss=3.064872980117798
I0131 20:46:46.248610 139907754342144 logging_writer.py:48] [59600] global_step=59600, grad_norm=2.0401411056518555, loss=3.081071376800537
I0131 20:47:19.973546 139907737556736 logging_writer.py:48] [59700] global_step=59700, grad_norm=2.0731003284454346, loss=3.123195171356201
I0131 20:47:53.716533 139907754342144 logging_writer.py:48] [59800] global_step=59800, grad_norm=2.101651906967163, loss=3.1385717391967773
I0131 20:48:27.466580 139907737556736 logging_writer.py:48] [59900] global_step=59900, grad_norm=2.2529489994049072, loss=3.070188522338867
I0131 20:49:01.357306 139907754342144 logging_writer.py:48] [60000] global_step=60000, grad_norm=2.1510396003723145, loss=3.109086751937866
I0131 20:49:35.098231 139907737556736 logging_writer.py:48] [60100] global_step=60100, grad_norm=2.6069400310516357, loss=3.168940544128418
I0131 20:50:08.843965 139907754342144 logging_writer.py:48] [60200] global_step=60200, grad_norm=2.0164716243743896, loss=3.0830929279327393
I0131 20:50:42.530098 139907737556736 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.946195125579834, loss=3.2102415561676025
I0131 20:51:16.318555 139907754342144 logging_writer.py:48] [60400] global_step=60400, grad_norm=2.0155208110809326, loss=3.076853036880493
I0131 20:51:28.601423 140070692116288 spec.py:321] Evaluating on the training split.
I0131 20:51:34.775523 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 20:51:43.781801 140070692116288 spec.py:349] Evaluating on the test split.
I0131 20:51:46.396619 140070692116288 submission_runner.py:408] Time since start: 21166.17s, 	Step: 60438, 	{'train/accuracy': 0.7448979616165161, 'train/loss': 1.266387701034546, 'validation/accuracy': 0.6719599962234497, 'validation/loss': 1.57454514503479, 'validation/num_examples': 50000, 'test/accuracy': 0.5458000302314758, 'test/loss': 2.216935634613037, 'test/num_examples': 10000, 'score': 20435.238516807556, 'total_duration': 21166.16930627823, 'accumulated_submission_time': 20435.238516807556, 'accumulated_eval_time': 727.5052762031555, 'accumulated_logging_time': 1.2989416122436523}
I0131 20:51:46.425111 139907737556736 logging_writer.py:48] [60438] accumulated_eval_time=727.505276, accumulated_logging_time=1.298942, accumulated_submission_time=20435.238517, global_step=60438, preemption_count=0, score=20435.238517, test/accuracy=0.545800, test/loss=2.216936, test/num_examples=10000, total_duration=21166.169306, train/accuracy=0.744898, train/loss=1.266388, validation/accuracy=0.671960, validation/loss=1.574545, validation/num_examples=50000
I0131 20:52:07.702990 139908425447168 logging_writer.py:48] [60500] global_step=60500, grad_norm=2.058478355407715, loss=3.066648006439209
I0131 20:52:41.436363 139907737556736 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.7847495079040527, loss=3.0655901432037354
I0131 20:53:15.176206 139908425447168 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.8088507652282715, loss=2.9845874309539795
I0131 20:53:48.933084 139907737556736 logging_writer.py:48] [60800] global_step=60800, grad_norm=2.0132040977478027, loss=3.162169933319092
I0131 20:54:22.657777 139908425447168 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.9815393686294556, loss=3.0456509590148926
I0131 20:54:56.490755 139907737556736 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.980670690536499, loss=3.190117835998535
I0131 20:55:30.237353 139908425447168 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.8928451538085938, loss=3.1473042964935303
I0131 20:56:03.956114 139907737556736 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.9209147691726685, loss=3.0467731952667236
I0131 20:56:37.723638 139908425447168 logging_writer.py:48] [61300] global_step=61300, grad_norm=2.0066328048706055, loss=3.063786506652832
I0131 20:57:11.433610 139907737556736 logging_writer.py:48] [61400] global_step=61400, grad_norm=2.0160515308380127, loss=3.119961977005005
I0131 20:57:45.124684 139908425447168 logging_writer.py:48] [61500] global_step=61500, grad_norm=2.077554941177368, loss=3.060436487197876
I0131 20:58:18.885287 139907737556736 logging_writer.py:48] [61600] global_step=61600, grad_norm=2.093393564224243, loss=3.0902068614959717
I0131 20:58:52.543207 139908425447168 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.906558871269226, loss=3.070897340774536
I0131 20:59:26.245228 139907737556736 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.9514111280441284, loss=3.0846712589263916
I0131 20:59:59.998338 139908425447168 logging_writer.py:48] [61900] global_step=61900, grad_norm=2.2582485675811768, loss=3.1171464920043945
I0131 21:00:16.664829 140070692116288 spec.py:321] Evaluating on the training split.
I0131 21:00:22.849841 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 21:00:31.847469 140070692116288 spec.py:349] Evaluating on the test split.
I0131 21:00:34.343810 140070692116288 submission_runner.py:408] Time since start: 21694.12s, 	Step: 61951, 	{'train/accuracy': 0.7596260905265808, 'train/loss': 1.151545763015747, 'validation/accuracy': 0.6668800115585327, 'validation/loss': 1.5589600801467896, 'validation/num_examples': 50000, 'test/accuracy': 0.5335000157356262, 'test/loss': 2.219144105911255, 'test/num_examples': 10000, 'score': 20945.41634607315, 'total_duration': 21694.11650276184, 'accumulated_submission_time': 20945.41634607315, 'accumulated_eval_time': 745.1842300891876, 'accumulated_logging_time': 1.3371562957763672}
I0131 21:00:34.370985 139907737556736 logging_writer.py:48] [61951] accumulated_eval_time=745.184230, accumulated_logging_time=1.337156, accumulated_submission_time=20945.416346, global_step=61951, preemption_count=0, score=20945.416346, test/accuracy=0.533500, test/loss=2.219144, test/num_examples=10000, total_duration=21694.116503, train/accuracy=0.759626, train/loss=1.151546, validation/accuracy=0.666880, validation/loss=1.558960, validation/num_examples=50000
I0131 21:00:51.237883 139907762734848 logging_writer.py:48] [62000] global_step=62000, grad_norm=2.0208580493927, loss=3.139876365661621
I0131 21:01:24.996846 139907737556736 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.9666048288345337, loss=3.093169689178467
I0131 21:01:58.676651 139907762734848 logging_writer.py:48] [62200] global_step=62200, grad_norm=2.1956019401550293, loss=3.133737087249756
I0131 21:02:32.456354 139907737556736 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.8556818962097168, loss=3.008955478668213
I0131 21:03:06.145828 139907762734848 logging_writer.py:48] [62400] global_step=62400, grad_norm=2.1328485012054443, loss=3.0874431133270264
I0131 21:03:39.957389 139907737556736 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.9893097877502441, loss=3.10628604888916
I0131 21:04:13.710551 139907762734848 logging_writer.py:48] [62600] global_step=62600, grad_norm=2.1710453033447266, loss=3.0781447887420654
I0131 21:04:47.443692 139907737556736 logging_writer.py:48] [62700] global_step=62700, grad_norm=2.1288206577301025, loss=3.1287336349487305
I0131 21:05:21.206130 139907762734848 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.963286280632019, loss=3.052948474884033
I0131 21:05:54.912429 139907737556736 logging_writer.py:48] [62900] global_step=62900, grad_norm=2.0401649475097656, loss=3.1698856353759766
I0131 21:06:28.672706 139907762734848 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.9134269952774048, loss=3.0470359325408936
I0131 21:07:02.387859 139907737556736 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.9191899299621582, loss=3.11057710647583
I0131 21:07:36.168731 139907762734848 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.8551013469696045, loss=3.0500741004943848
I0131 21:08:09.901553 139907737556736 logging_writer.py:48] [63300] global_step=63300, grad_norm=2.174018144607544, loss=3.1168417930603027
I0131 21:08:43.613607 139907762734848 logging_writer.py:48] [63400] global_step=63400, grad_norm=2.086853504180908, loss=3.0812301635742188
I0131 21:09:04.411139 140070692116288 spec.py:321] Evaluating on the training split.
I0131 21:09:10.622121 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 21:09:19.619697 140070692116288 spec.py:349] Evaluating on the test split.
I0131 21:09:22.234334 140070692116288 submission_runner.py:408] Time since start: 22222.01s, 	Step: 63463, 	{'train/accuracy': 0.7645288705825806, 'train/loss': 1.1519296169281006, 'validation/accuracy': 0.6728799939155579, 'validation/loss': 1.5548346042633057, 'validation/num_examples': 50000, 'test/accuracy': 0.5394999980926514, 'test/loss': 2.243274211883545, 'test/num_examples': 10000, 'score': 21455.393117904663, 'total_duration': 22222.007017850876, 'accumulated_submission_time': 21455.393117904663, 'accumulated_eval_time': 763.0073924064636, 'accumulated_logging_time': 1.3743011951446533}
I0131 21:09:22.263916 139907745949440 logging_writer.py:48] [63463] accumulated_eval_time=763.007392, accumulated_logging_time=1.374301, accumulated_submission_time=21455.393118, global_step=63463, preemption_count=0, score=21455.393118, test/accuracy=0.539500, test/loss=2.243274, test/num_examples=10000, total_duration=22222.007018, train/accuracy=0.764529, train/loss=1.151930, validation/accuracy=0.672880, validation/loss=1.554835, validation/num_examples=50000
I0131 21:09:35.055037 139907754342144 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.9035853147506714, loss=3.0136778354644775
I0131 21:10:08.688144 139907745949440 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.8988261222839355, loss=3.0951080322265625
I0131 21:10:42.409000 139907754342144 logging_writer.py:48] [63700] global_step=63700, grad_norm=2.130038261413574, loss=3.12284517288208
I0131 21:11:16.101513 139907745949440 logging_writer.py:48] [63800] global_step=63800, grad_norm=2.047109603881836, loss=3.096312999725342
I0131 21:11:49.857966 139907754342144 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.9909231662750244, loss=3.069046974182129
I0131 21:12:23.635674 139907745949440 logging_writer.py:48] [64000] global_step=64000, grad_norm=2.2317864894866943, loss=3.0592947006225586
I0131 21:12:57.359005 139907754342144 logging_writer.py:48] [64100] global_step=64100, grad_norm=2.0446813106536865, loss=3.035291910171509
I0131 21:13:31.100280 139907745949440 logging_writer.py:48] [64200] global_step=64200, grad_norm=2.1109399795532227, loss=3.063572406768799
I0131 21:14:04.928324 139907754342144 logging_writer.py:48] [64300] global_step=64300, grad_norm=2.062694787979126, loss=3.16348934173584
I0131 21:14:38.637308 139907745949440 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.987029790878296, loss=3.0935134887695312
I0131 21:15:12.329572 139907754342144 logging_writer.py:48] [64500] global_step=64500, grad_norm=2.205214738845825, loss=3.1319148540496826
I0131 21:15:46.008316 139907745949440 logging_writer.py:48] [64600] global_step=64600, grad_norm=2.054799795150757, loss=3.0608251094818115
I0131 21:16:19.724038 139907754342144 logging_writer.py:48] [64700] global_step=64700, grad_norm=2.157248020172119, loss=3.093475818634033
I0131 21:16:53.496434 139907745949440 logging_writer.py:48] [64800] global_step=64800, grad_norm=2.583674669265747, loss=3.1547462940216064
I0131 21:17:27.201699 139907754342144 logging_writer.py:48] [64900] global_step=64900, grad_norm=2.001345157623291, loss=3.0646417140960693
I0131 21:17:52.322421 140070692116288 spec.py:321] Evaluating on the training split.
I0131 21:17:58.441462 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 21:18:07.535855 140070692116288 spec.py:349] Evaluating on the test split.
I0131 21:18:10.042912 140070692116288 submission_runner.py:408] Time since start: 22749.82s, 	Step: 64976, 	{'train/accuracy': 0.7543845772743225, 'train/loss': 1.1895458698272705, 'validation/accuracy': 0.6688799858093262, 'validation/loss': 1.562997817993164, 'validation/num_examples': 50000, 'test/accuracy': 0.5503000020980835, 'test/loss': 2.204355478286743, 'test/num_examples': 10000, 'score': 21965.388853788376, 'total_duration': 22749.81556200981, 'accumulated_submission_time': 21965.388853788376, 'accumulated_eval_time': 780.7278144359589, 'accumulated_logging_time': 1.4133169651031494}
I0131 21:18:10.072350 139907745949440 logging_writer.py:48] [64976] accumulated_eval_time=780.727814, accumulated_logging_time=1.413317, accumulated_submission_time=21965.388854, global_step=64976, preemption_count=0, score=21965.388854, test/accuracy=0.550300, test/loss=2.204355, test/num_examples=10000, total_duration=22749.815562, train/accuracy=0.754385, train/loss=1.189546, validation/accuracy=0.668880, validation/loss=1.562998, validation/num_examples=50000
I0131 21:18:18.496366 139908710635264 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.9189273118972778, loss=3.0048933029174805
I0131 21:18:52.219153 139907745949440 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.9463810920715332, loss=3.0652449131011963
I0131 21:19:25.940323 139908710635264 logging_writer.py:48] [65200] global_step=65200, grad_norm=2.1711597442626953, loss=3.0836877822875977
I0131 21:19:59.780999 139907745949440 logging_writer.py:48] [65300] global_step=65300, grad_norm=2.2748608589172363, loss=3.0408899784088135
I0131 21:20:33.601161 139908710635264 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.950605034828186, loss=3.027437448501587
I0131 21:21:07.369351 139907745949440 logging_writer.py:48] [65500] global_step=65500, grad_norm=2.04848575592041, loss=3.162306547164917
I0131 21:21:41.084668 139908710635264 logging_writer.py:48] [65600] global_step=65600, grad_norm=2.077972650527954, loss=3.00959849357605
I0131 21:22:14.864729 139907745949440 logging_writer.py:48] [65700] global_step=65700, grad_norm=2.063744068145752, loss=3.0671050548553467
I0131 21:22:48.551544 139908710635264 logging_writer.py:48] [65800] global_step=65800, grad_norm=2.1599676609039307, loss=3.053380012512207
I0131 21:23:22.330144 139907745949440 logging_writer.py:48] [65900] global_step=65900, grad_norm=2.1171610355377197, loss=3.109442949295044
I0131 21:23:56.112766 139908710635264 logging_writer.py:48] [66000] global_step=66000, grad_norm=2.2727127075195312, loss=3.149386167526245
I0131 21:24:29.840572 139907745949440 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.9780277013778687, loss=3.0859451293945312
I0131 21:25:03.613378 139908710635264 logging_writer.py:48] [66200] global_step=66200, grad_norm=2.1321887969970703, loss=3.096376895904541
I0131 21:25:37.285037 139907745949440 logging_writer.py:48] [66300] global_step=66300, grad_norm=2.057093381881714, loss=3.018839120864868
I0131 21:26:11.142679 139908710635264 logging_writer.py:48] [66400] global_step=66400, grad_norm=2.4455184936523438, loss=3.0934839248657227
I0131 21:26:40.292957 140070692116288 spec.py:321] Evaluating on the training split.
I0131 21:26:46.569683 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 21:26:55.453829 140070692116288 spec.py:349] Evaluating on the test split.
I0131 21:26:57.991901 140070692116288 submission_runner.py:408] Time since start: 23277.76s, 	Step: 66488, 	{'train/accuracy': 0.7489436864852905, 'train/loss': 1.2165971994400024, 'validation/accuracy': 0.6738399863243103, 'validation/loss': 1.5674960613250732, 'validation/num_examples': 50000, 'test/accuracy': 0.5478000044822693, 'test/loss': 2.2059414386749268, 'test/num_examples': 10000, 'score': 22475.54665660858, 'total_duration': 23277.764575958252, 'accumulated_submission_time': 22475.54665660858, 'accumulated_eval_time': 798.4267275333405, 'accumulated_logging_time': 1.4519784450531006}
I0131 21:26:58.021265 139907745949440 logging_writer.py:48] [66488] accumulated_eval_time=798.426728, accumulated_logging_time=1.451978, accumulated_submission_time=22475.546657, global_step=66488, preemption_count=0, score=22475.546657, test/accuracy=0.547800, test/loss=2.205941, test/num_examples=10000, total_duration=23277.764576, train/accuracy=0.748944, train/loss=1.216597, validation/accuracy=0.673840, validation/loss=1.567496, validation/num_examples=50000
I0131 21:27:02.394437 139907754342144 logging_writer.py:48] [66500] global_step=66500, grad_norm=2.022808790206909, loss=3.0645973682403564
I0131 21:27:36.095118 139907745949440 logging_writer.py:48] [66600] global_step=66600, grad_norm=2.275697708129883, loss=3.113334894180298
I0131 21:28:09.763295 139907754342144 logging_writer.py:48] [66700] global_step=66700, grad_norm=2.072387933731079, loss=3.0921452045440674
I0131 21:28:43.459466 139907745949440 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.8910932540893555, loss=3.0034046173095703
I0131 21:29:17.143981 139907754342144 logging_writer.py:48] [66900] global_step=66900, grad_norm=2.1237142086029053, loss=3.041515827178955
I0131 21:29:50.820849 139907745949440 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.883466362953186, loss=3.0633254051208496
I0131 21:30:24.503955 139907754342144 logging_writer.py:48] [67100] global_step=67100, grad_norm=2.0889785289764404, loss=3.0688107013702393
I0131 21:30:58.198053 139907745949440 logging_writer.py:48] [67200] global_step=67200, grad_norm=2.26953125, loss=3.048004388809204
I0131 21:31:31.993449 139907754342144 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.9809725284576416, loss=3.1243677139282227
I0131 21:32:05.684498 139907745949440 logging_writer.py:48] [67400] global_step=67400, grad_norm=2.4857723712921143, loss=3.0850915908813477
I0131 21:32:39.428260 139907754342144 logging_writer.py:48] [67500] global_step=67500, grad_norm=2.0390141010284424, loss=3.0332276821136475
I0131 21:33:13.162143 139907745949440 logging_writer.py:48] [67600] global_step=67600, grad_norm=2.0971968173980713, loss=3.049408435821533
I0131 21:33:46.863483 139907754342144 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.9796059131622314, loss=3.1273090839385986
I0131 21:34:20.534239 139907745949440 logging_writer.py:48] [67800] global_step=67800, grad_norm=2.030522346496582, loss=3.087575912475586
I0131 21:34:54.317933 139907754342144 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.905458927154541, loss=3.072396993637085
I0131 21:35:28.020583 139907745949440 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.8980838060379028, loss=3.014615058898926
I0131 21:35:28.027976 140070692116288 spec.py:321] Evaluating on the training split.
I0131 21:35:34.219085 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 21:35:43.131329 140070692116288 spec.py:349] Evaluating on the test split.
I0131 21:35:45.559693 140070692116288 submission_runner.py:408] Time since start: 23805.33s, 	Step: 68001, 	{'train/accuracy': 0.7564173936843872, 'train/loss': 1.1640042066574097, 'validation/accuracy': 0.6771000027656555, 'validation/loss': 1.5072546005249023, 'validation/num_examples': 50000, 'test/accuracy': 0.5573000311851501, 'test/loss': 2.136253595352173, 'test/num_examples': 10000, 'score': 22985.491063833237, 'total_duration': 23805.332373142242, 'accumulated_submission_time': 22985.491063833237, 'accumulated_eval_time': 815.9583787918091, 'accumulated_logging_time': 1.4906814098358154}
I0131 21:35:45.589951 139908719027968 logging_writer.py:48] [68001] accumulated_eval_time=815.958379, accumulated_logging_time=1.490681, accumulated_submission_time=22985.491064, global_step=68001, preemption_count=0, score=22985.491064, test/accuracy=0.557300, test/loss=2.136254, test/num_examples=10000, total_duration=23805.332373, train/accuracy=0.756417, train/loss=1.164004, validation/accuracy=0.677100, validation/loss=1.507255, validation/num_examples=50000
I0131 21:36:19.316477 139908727420672 logging_writer.py:48] [68100] global_step=68100, grad_norm=2.1474719047546387, loss=3.1305510997772217
I0131 21:36:52.994112 139908719027968 logging_writer.py:48] [68200] global_step=68200, grad_norm=2.110452890396118, loss=3.049887180328369
I0131 21:37:26.748726 139908727420672 logging_writer.py:48] [68300] global_step=68300, grad_norm=2.3577048778533936, loss=3.1404035091400146
I0131 21:38:00.504465 139908719027968 logging_writer.py:48] [68400] global_step=68400, grad_norm=2.0568392276763916, loss=3.0727367401123047
I0131 21:38:34.312811 139908727420672 logging_writer.py:48] [68500] global_step=68500, grad_norm=2.111145257949829, loss=3.076415777206421
I0131 21:39:08.020610 139908719027968 logging_writer.py:48] [68600] global_step=68600, grad_norm=2.113658905029297, loss=2.988783836364746
I0131 21:39:41.736042 139908727420672 logging_writer.py:48] [68700] global_step=68700, grad_norm=2.244968891143799, loss=3.0305962562561035
I0131 21:40:15.505064 139908719027968 logging_writer.py:48] [68800] global_step=68800, grad_norm=2.142460346221924, loss=3.076096534729004
I0131 21:40:49.223300 139908727420672 logging_writer.py:48] [68900] global_step=68900, grad_norm=2.182544708251953, loss=3.1906378269195557
I0131 21:41:22.927661 139908719027968 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.9537968635559082, loss=3.073463201522827
I0131 21:41:56.713592 139908727420672 logging_writer.py:48] [69100] global_step=69100, grad_norm=2.205521821975708, loss=3.143955707550049
I0131 21:42:30.447366 139908719027968 logging_writer.py:48] [69200] global_step=69200, grad_norm=2.085070848464966, loss=3.0589091777801514
I0131 21:43:04.207974 139908727420672 logging_writer.py:48] [69300] global_step=69300, grad_norm=2.118511438369751, loss=3.078806161880493
I0131 21:43:37.954572 139908719027968 logging_writer.py:48] [69400] global_step=69400, grad_norm=2.238358497619629, loss=3.047844648361206
I0131 21:44:11.697936 139908727420672 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.9987362623214722, loss=3.084331512451172
I0131 21:44:15.560998 140070692116288 spec.py:321] Evaluating on the training split.
I0131 21:44:21.823883 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 21:44:31.009883 140070692116288 spec.py:349] Evaluating on the test split.
I0131 21:44:33.508394 140070692116288 submission_runner.py:408] Time since start: 24333.28s, 	Step: 69513, 	{'train/accuracy': 0.7506178021430969, 'train/loss': 1.205312728881836, 'validation/accuracy': 0.6768999695777893, 'validation/loss': 1.5448400974273682, 'validation/num_examples': 50000, 'test/accuracy': 0.5519000291824341, 'test/loss': 2.177412986755371, 'test/num_examples': 10000, 'score': 23495.39884543419, 'total_duration': 24333.28107357025, 'accumulated_submission_time': 23495.39884543419, 'accumulated_eval_time': 833.9057495594025, 'accumulated_logging_time': 1.5299150943756104}
I0131 21:44:33.536821 139907737556736 logging_writer.py:48] [69513] accumulated_eval_time=833.905750, accumulated_logging_time=1.529915, accumulated_submission_time=23495.398845, global_step=69513, preemption_count=0, score=23495.398845, test/accuracy=0.551900, test/loss=2.177413, test/num_examples=10000, total_duration=24333.281074, train/accuracy=0.750618, train/loss=1.205313, validation/accuracy=0.676900, validation/loss=1.544840, validation/num_examples=50000
I0131 21:45:03.293164 139907762734848 logging_writer.py:48] [69600] global_step=69600, grad_norm=2.171877145767212, loss=3.030487060546875
I0131 21:45:36.976797 139907737556736 logging_writer.py:48] [69700] global_step=69700, grad_norm=2.007004737854004, loss=3.020930767059326
I0131 21:46:10.669506 139907762734848 logging_writer.py:48] [69800] global_step=69800, grad_norm=2.079882860183716, loss=3.07582950592041
I0131 21:46:44.351783 139907737556736 logging_writer.py:48] [69900] global_step=69900, grad_norm=2.0756568908691406, loss=3.0564332008361816
I0131 21:47:18.151990 139907762734848 logging_writer.py:48] [70000] global_step=70000, grad_norm=2.0263469219207764, loss=3.0072453022003174
I0131 21:47:51.906002 139907737556736 logging_writer.py:48] [70100] global_step=70100, grad_norm=2.0638587474823, loss=3.045729875564575
I0131 21:48:25.614030 139907762734848 logging_writer.py:48] [70200] global_step=70200, grad_norm=2.070708990097046, loss=3.074906349182129
I0131 21:48:59.298818 139907737556736 logging_writer.py:48] [70300] global_step=70300, grad_norm=2.1597163677215576, loss=3.1543545722961426
I0131 21:49:32.985298 139907762734848 logging_writer.py:48] [70400] global_step=70400, grad_norm=2.150017499923706, loss=3.072570562362671
I0131 21:50:06.741991 139907737556736 logging_writer.py:48] [70500] global_step=70500, grad_norm=2.1443660259246826, loss=3.0925254821777344
I0131 21:50:40.471651 139907762734848 logging_writer.py:48] [70600] global_step=70600, grad_norm=2.0337071418762207, loss=3.1198136806488037
I0131 21:51:14.337811 139907737556736 logging_writer.py:48] [70700] global_step=70700, grad_norm=2.1377081871032715, loss=3.1023659706115723
I0131 21:51:48.108330 139907762734848 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.9517136812210083, loss=3.0783379077911377
I0131 21:52:21.801825 139907737556736 logging_writer.py:48] [70900] global_step=70900, grad_norm=2.044940233230591, loss=3.0365352630615234
I0131 21:52:55.548935 139907762734848 logging_writer.py:48] [71000] global_step=71000, grad_norm=2.1329357624053955, loss=3.105865478515625
I0131 21:53:03.802960 140070692116288 spec.py:321] Evaluating on the training split.
I0131 21:53:10.000871 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 21:53:18.753792 140070692116288 spec.py:349] Evaluating on the test split.
I0131 21:53:21.234069 140070692116288 submission_runner.py:408] Time since start: 24861.01s, 	Step: 71026, 	{'train/accuracy': 0.7942841053009033, 'train/loss': 1.016778826713562, 'validation/accuracy': 0.6810799837112427, 'validation/loss': 1.4947758913040161, 'validation/num_examples': 50000, 'test/accuracy': 0.5546000003814697, 'test/loss': 2.1463091373443604, 'test/num_examples': 10000, 'score': 24005.601548433304, 'total_duration': 24861.00675535202, 'accumulated_submission_time': 24005.601548433304, 'accumulated_eval_time': 851.3368241786957, 'accumulated_logging_time': 1.5676684379577637}
I0131 21:53:21.263852 139907754342144 logging_writer.py:48] [71026] accumulated_eval_time=851.336824, accumulated_logging_time=1.567668, accumulated_submission_time=24005.601548, global_step=71026, preemption_count=0, score=24005.601548, test/accuracy=0.554600, test/loss=2.146309, test/num_examples=10000, total_duration=24861.006755, train/accuracy=0.794284, train/loss=1.016779, validation/accuracy=0.681080, validation/loss=1.494776, validation/num_examples=50000
I0131 21:53:46.510257 139908719027968 logging_writer.py:48] [71100] global_step=71100, grad_norm=2.1055757999420166, loss=3.0118703842163086
I0131 21:54:20.224155 139907754342144 logging_writer.py:48] [71200] global_step=71200, grad_norm=2.1070799827575684, loss=3.0434393882751465
I0131 21:54:53.925558 139908719027968 logging_writer.py:48] [71300] global_step=71300, grad_norm=2.066223621368408, loss=3.047468662261963
I0131 21:55:27.678122 139907754342144 logging_writer.py:48] [71400] global_step=71400, grad_norm=2.222440004348755, loss=3.0931849479675293
I0131 21:56:01.447406 139908719027968 logging_writer.py:48] [71500] global_step=71500, grad_norm=2.037221908569336, loss=3.026273727416992
I0131 21:56:35.173022 139907754342144 logging_writer.py:48] [71600] global_step=71600, grad_norm=2.24977707862854, loss=3.124559164047241
I0131 21:57:09.035023 139908719027968 logging_writer.py:48] [71700] global_step=71700, grad_norm=2.1272504329681396, loss=3.0733985900878906
I0131 21:57:42.787438 139907754342144 logging_writer.py:48] [71800] global_step=71800, grad_norm=2.2072596549987793, loss=3.0805819034576416
I0131 21:58:16.556127 139908719027968 logging_writer.py:48] [71900] global_step=71900, grad_norm=2.0450680255889893, loss=3.0117123126983643
I0131 21:58:50.288693 139907754342144 logging_writer.py:48] [72000] global_step=72000, grad_norm=2.0637271404266357, loss=3.041602611541748
I0131 21:59:24.059490 139908719027968 logging_writer.py:48] [72100] global_step=72100, grad_norm=2.092989683151245, loss=3.024020195007324
I0131 21:59:57.784368 139907754342144 logging_writer.py:48] [72200] global_step=72200, grad_norm=2.15215802192688, loss=3.050884246826172
I0131 22:00:31.545306 139908719027968 logging_writer.py:48] [72300] global_step=72300, grad_norm=2.25020432472229, loss=3.0973939895629883
I0131 22:01:05.265020 139907754342144 logging_writer.py:48] [72400] global_step=72400, grad_norm=2.1798744201660156, loss=3.0592403411865234
I0131 22:01:39.017796 139908719027968 logging_writer.py:48] [72500] global_step=72500, grad_norm=2.0734331607818604, loss=3.0611181259155273
I0131 22:01:51.312422 140070692116288 spec.py:321] Evaluating on the training split.
I0131 22:01:57.579599 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 22:02:06.509914 140070692116288 spec.py:349] Evaluating on the test split.
I0131 22:02:09.242631 140070692116288 submission_runner.py:408] Time since start: 25389.02s, 	Step: 72538, 	{'train/accuracy': 0.7712053656578064, 'train/loss': 1.1332015991210938, 'validation/accuracy': 0.6800199747085571, 'validation/loss': 1.5349023342132568, 'validation/num_examples': 50000, 'test/accuracy': 0.5504000186920166, 'test/loss': 2.2045271396636963, 'test/num_examples': 10000, 'score': 24515.58721637726, 'total_duration': 25389.0153260231, 'accumulated_submission_time': 24515.58721637726, 'accumulated_eval_time': 869.2670221328735, 'accumulated_logging_time': 1.6069247722625732}
I0131 22:02:09.267394 139907737556736 logging_writer.py:48] [72538] accumulated_eval_time=869.267022, accumulated_logging_time=1.606925, accumulated_submission_time=24515.587216, global_step=72538, preemption_count=0, score=24515.587216, test/accuracy=0.550400, test/loss=2.204527, test/num_examples=10000, total_duration=25389.015326, train/accuracy=0.771205, train/loss=1.133202, validation/accuracy=0.680020, validation/loss=1.534902, validation/num_examples=50000
I0131 22:02:30.473165 139907745949440 logging_writer.py:48] [72600] global_step=72600, grad_norm=2.2466306686401367, loss=3.1324524879455566
I0131 22:03:04.175107 139907737556736 logging_writer.py:48] [72700] global_step=72700, grad_norm=2.325100898742676, loss=3.135195732116699
I0131 22:03:37.980554 139907745949440 logging_writer.py:48] [72800] global_step=72800, grad_norm=2.2438700199127197, loss=3.143972396850586
I0131 22:04:11.712299 139907737556736 logging_writer.py:48] [72900] global_step=72900, grad_norm=2.109107255935669, loss=3.0700278282165527
I0131 22:04:45.449723 139907745949440 logging_writer.py:48] [73000] global_step=73000, grad_norm=2.131584644317627, loss=3.030012369155884
I0131 22:05:19.185642 139907737556736 logging_writer.py:48] [73100] global_step=73100, grad_norm=2.174536943435669, loss=3.0781402587890625
I0131 22:05:52.932319 139907745949440 logging_writer.py:48] [73200] global_step=73200, grad_norm=2.1107828617095947, loss=3.0640316009521484
I0131 22:06:26.657900 139907737556736 logging_writer.py:48] [73300] global_step=73300, grad_norm=2.1754140853881836, loss=3.1134979724884033
I0131 22:07:00.314233 139907745949440 logging_writer.py:48] [73400] global_step=73400, grad_norm=2.374690532684326, loss=3.0629420280456543
I0131 22:07:34.006013 139907737556736 logging_writer.py:48] [73500] global_step=73500, grad_norm=2.27286696434021, loss=3.0794129371643066
I0131 22:08:07.742151 139907745949440 logging_writer.py:48] [73600] global_step=73600, grad_norm=2.261364221572876, loss=3.0730254650115967
I0131 22:08:41.487141 139907737556736 logging_writer.py:48] [73700] global_step=73700, grad_norm=2.0848138332366943, loss=3.097031354904175
I0131 22:09:15.236337 139907745949440 logging_writer.py:48] [73800] global_step=73800, grad_norm=2.1625382900238037, loss=3.075782060623169
I0131 22:09:49.060195 139907737556736 logging_writer.py:48] [73900] global_step=73900, grad_norm=2.0921528339385986, loss=3.0682742595672607
I0131 22:10:22.773866 139907745949440 logging_writer.py:48] [74000] global_step=74000, grad_norm=2.0314269065856934, loss=3.01737642288208
I0131 22:10:39.438683 140070692116288 spec.py:321] Evaluating on the training split.
I0131 22:10:45.613367 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 22:10:54.608018 140070692116288 spec.py:349] Evaluating on the test split.
I0131 22:10:57.107062 140070692116288 submission_runner.py:408] Time since start: 25916.88s, 	Step: 74051, 	{'train/accuracy': 0.7703882455825806, 'train/loss': 1.119797945022583, 'validation/accuracy': 0.6869999766349792, 'validation/loss': 1.496230125427246, 'validation/num_examples': 50000, 'test/accuracy': 0.5599000453948975, 'test/loss': 2.144322395324707, 'test/num_examples': 10000, 'score': 25025.69666481018, 'total_duration': 25916.87974834442, 'accumulated_submission_time': 25025.69666481018, 'accumulated_eval_time': 886.9353656768799, 'accumulated_logging_time': 1.6402819156646729}
I0131 22:10:57.136404 139908710635264 logging_writer.py:48] [74051] accumulated_eval_time=886.935366, accumulated_logging_time=1.640282, accumulated_submission_time=25025.696665, global_step=74051, preemption_count=0, score=25025.696665, test/accuracy=0.559900, test/loss=2.144322, test/num_examples=10000, total_duration=25916.879748, train/accuracy=0.770388, train/loss=1.119798, validation/accuracy=0.687000, validation/loss=1.496230, validation/num_examples=50000
I0131 22:11:13.945579 139908719027968 logging_writer.py:48] [74100] global_step=74100, grad_norm=2.0589754581451416, loss=3.1170597076416016
I0131 22:11:47.599991 139908710635264 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.9562840461730957, loss=3.017637252807617
I0131 22:12:21.351439 139908719027968 logging_writer.py:48] [74300] global_step=74300, grad_norm=2.2316598892211914, loss=3.0600123405456543
I0131 22:12:55.069618 139908710635264 logging_writer.py:48] [74400] global_step=74400, grad_norm=2.066350221633911, loss=2.983743667602539
I0131 22:13:28.808908 139908719027968 logging_writer.py:48] [74500] global_step=74500, grad_norm=2.1635591983795166, loss=3.0108513832092285
I0131 22:14:02.608738 139908710635264 logging_writer.py:48] [74600] global_step=74600, grad_norm=2.32319712638855, loss=3.1004579067230225
I0131 22:14:36.351891 139908719027968 logging_writer.py:48] [74700] global_step=74700, grad_norm=2.2283568382263184, loss=3.0863912105560303
I0131 22:15:10.070855 139908710635264 logging_writer.py:48] [74800] global_step=74800, grad_norm=2.2185781002044678, loss=3.0970263481140137
I0131 22:15:43.865183 139908719027968 logging_writer.py:48] [74900] global_step=74900, grad_norm=2.178917407989502, loss=3.01986026763916
I0131 22:16:17.656282 139908710635264 logging_writer.py:48] [75000] global_step=75000, grad_norm=2.1294491291046143, loss=3.1267802715301514
I0131 22:16:51.372302 139908719027968 logging_writer.py:48] [75100] global_step=75100, grad_norm=2.0944626331329346, loss=3.057452440261841
I0131 22:17:25.141295 139908710635264 logging_writer.py:48] [75200] global_step=75200, grad_norm=2.260906457901001, loss=3.005253791809082
I0131 22:17:58.832093 139908719027968 logging_writer.py:48] [75300] global_step=75300, grad_norm=2.1869094371795654, loss=3.004361629486084
I0131 22:18:32.619660 139908710635264 logging_writer.py:48] [75400] global_step=75400, grad_norm=2.1974270343780518, loss=3.053682804107666
I0131 22:19:06.355786 139908719027968 logging_writer.py:48] [75500] global_step=75500, grad_norm=2.5704805850982666, loss=3.132786273956299
I0131 22:19:27.405629 140070692116288 spec.py:321] Evaluating on the training split.
I0131 22:19:33.616417 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 22:19:42.392233 140070692116288 spec.py:349] Evaluating on the test split.
I0131 22:19:44.851235 140070692116288 submission_runner.py:408] Time since start: 26444.62s, 	Step: 75564, 	{'train/accuracy': 0.7689333558082581, 'train/loss': 1.1266475915908813, 'validation/accuracy': 0.6871799826622009, 'validation/loss': 1.490763545036316, 'validation/num_examples': 50000, 'test/accuracy': 0.5641000270843506, 'test/loss': 2.1208748817443848, 'test/num_examples': 10000, 'score': 25535.903984308243, 'total_duration': 26444.623901844025, 'accumulated_submission_time': 25535.903984308243, 'accumulated_eval_time': 904.3809192180634, 'accumulated_logging_time': 1.6784143447875977}
I0131 22:19:44.880841 139907745949440 logging_writer.py:48] [75564] accumulated_eval_time=904.380919, accumulated_logging_time=1.678414, accumulated_submission_time=25535.903984, global_step=75564, preemption_count=0, score=25535.903984, test/accuracy=0.564100, test/loss=2.120875, test/num_examples=10000, total_duration=26444.623902, train/accuracy=0.768933, train/loss=1.126648, validation/accuracy=0.687180, validation/loss=1.490764, validation/num_examples=50000
I0131 22:19:57.388370 139907754342144 logging_writer.py:48] [75600] global_step=75600, grad_norm=2.1815385818481445, loss=3.1106467247009277
I0131 22:20:31.046386 139907745949440 logging_writer.py:48] [75700] global_step=75700, grad_norm=2.2289416790008545, loss=3.0637710094451904
I0131 22:21:04.749792 139907754342144 logging_writer.py:48] [75800] global_step=75800, grad_norm=2.2306463718414307, loss=3.0361809730529785
I0131 22:21:38.409201 139907745949440 logging_writer.py:48] [75900] global_step=75900, grad_norm=2.129657506942749, loss=3.003727674484253
I0131 22:22:12.207005 139907754342144 logging_writer.py:48] [76000] global_step=76000, grad_norm=2.4546077251434326, loss=3.0823328495025635
I0131 22:22:45.920514 139907745949440 logging_writer.py:48] [76100] global_step=76100, grad_norm=2.2410619258880615, loss=3.008847713470459
I0131 22:23:19.647598 139907754342144 logging_writer.py:48] [76200] global_step=76200, grad_norm=2.255598545074463, loss=3.1554739475250244
I0131 22:23:53.392340 139907745949440 logging_writer.py:48] [76300] global_step=76300, grad_norm=2.0580031871795654, loss=3.0297060012817383
I0131 22:24:27.125247 139907754342144 logging_writer.py:48] [76400] global_step=76400, grad_norm=2.550128221511841, loss=3.0420777797698975
I0131 22:25:00.840426 139907745949440 logging_writer.py:48] [76500] global_step=76500, grad_norm=2.291818141937256, loss=3.0779237747192383
I0131 22:25:34.605484 139907754342144 logging_writer.py:48] [76600] global_step=76600, grad_norm=2.4470314979553223, loss=3.0553081035614014
I0131 22:26:08.356665 139907745949440 logging_writer.py:48] [76700] global_step=76700, grad_norm=2.3792853355407715, loss=3.032459259033203
I0131 22:26:42.066984 139907754342144 logging_writer.py:48] [76800] global_step=76800, grad_norm=2.127157688140869, loss=3.085326671600342
I0131 22:27:15.721868 139907745949440 logging_writer.py:48] [76900] global_step=76900, grad_norm=2.3075199127197266, loss=3.029751777648926
I0131 22:27:49.436460 139907754342144 logging_writer.py:48] [77000] global_step=77000, grad_norm=2.1252200603485107, loss=3.044635534286499
I0131 22:28:14.935853 140070692116288 spec.py:321] Evaluating on the training split.
I0131 22:28:21.100338 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 22:28:30.017477 140070692116288 spec.py:349] Evaluating on the test split.
I0131 22:28:32.512042 140070692116288 submission_runner.py:408] Time since start: 26972.28s, 	Step: 77077, 	{'train/accuracy': 0.7555803656578064, 'train/loss': 1.252349853515625, 'validation/accuracy': 0.6780799627304077, 'validation/loss': 1.591875433921814, 'validation/num_examples': 50000, 'test/accuracy': 0.5550000071525574, 'test/loss': 2.229222297668457, 'test/num_examples': 10000, 'score': 26045.897315502167, 'total_duration': 26972.284712553024, 'accumulated_submission_time': 26045.897315502167, 'accumulated_eval_time': 921.9570591449738, 'accumulated_logging_time': 1.7172760963439941}
I0131 22:28:32.542319 139907729164032 logging_writer.py:48] [77077] accumulated_eval_time=921.957059, accumulated_logging_time=1.717276, accumulated_submission_time=26045.897316, global_step=77077, preemption_count=0, score=26045.897316, test/accuracy=0.555000, test/loss=2.229222, test/num_examples=10000, total_duration=26972.284713, train/accuracy=0.755580, train/loss=1.252350, validation/accuracy=0.678080, validation/loss=1.591875, validation/num_examples=50000
I0131 22:28:40.651354 139907737556736 logging_writer.py:48] [77100] global_step=77100, grad_norm=2.1581764221191406, loss=3.0802202224731445
I0131 22:29:14.348611 139907729164032 logging_writer.py:48] [77200] global_step=77200, grad_norm=2.106149673461914, loss=2.998278856277466
I0131 22:29:48.061795 139907737556736 logging_writer.py:48] [77300] global_step=77300, grad_norm=2.2590107917785645, loss=3.0906388759613037
I0131 22:30:21.793058 139907729164032 logging_writer.py:48] [77400] global_step=77400, grad_norm=2.0698368549346924, loss=3.040065288543701
I0131 22:30:55.603943 139907737556736 logging_writer.py:48] [77500] global_step=77500, grad_norm=2.3876731395721436, loss=3.0188260078430176
I0131 22:31:29.327803 139907729164032 logging_writer.py:48] [77600] global_step=77600, grad_norm=2.368433713912964, loss=3.069437026977539
I0131 22:32:03.106957 139907737556736 logging_writer.py:48] [77700] global_step=77700, grad_norm=2.289515972137451, loss=3.040436267852783
I0131 22:32:36.793140 139907729164032 logging_writer.py:48] [77800] global_step=77800, grad_norm=2.1328952312469482, loss=2.9876675605773926
I0131 22:33:10.592628 139907737556736 logging_writer.py:48] [77900] global_step=77900, grad_norm=2.2740654945373535, loss=3.0410683155059814
I0131 22:33:44.324062 139907729164032 logging_writer.py:48] [78000] global_step=78000, grad_norm=2.317394733428955, loss=2.969712257385254
I0131 22:34:18.134307 139907737556736 logging_writer.py:48] [78100] global_step=78100, grad_norm=2.289104461669922, loss=3.0616185665130615
I0131 22:34:51.980932 139907729164032 logging_writer.py:48] [78200] global_step=78200, grad_norm=2.1857776641845703, loss=3.058814525604248
I0131 22:35:25.719923 139907737556736 logging_writer.py:48] [78300] global_step=78300, grad_norm=2.3117733001708984, loss=3.0305261611938477
I0131 22:35:59.435328 139907729164032 logging_writer.py:48] [78400] global_step=78400, grad_norm=2.163560390472412, loss=3.0811986923217773
I0131 22:36:33.134460 139907737556736 logging_writer.py:48] [78500] global_step=78500, grad_norm=2.4836511611938477, loss=3.068976640701294
I0131 22:37:02.641313 140070692116288 spec.py:321] Evaluating on the training split.
I0131 22:37:08.883020 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 22:37:17.689895 140070692116288 spec.py:349] Evaluating on the test split.
I0131 22:37:20.276129 140070692116288 submission_runner.py:408] Time since start: 27500.05s, 	Step: 78589, 	{'train/accuracy': 0.7691724896430969, 'train/loss': 1.146484375, 'validation/accuracy': 0.6866399645805359, 'validation/loss': 1.4988430738449097, 'validation/num_examples': 50000, 'test/accuracy': 0.5601000189781189, 'test/loss': 2.1550445556640625, 'test/num_examples': 10000, 'score': 26555.934617996216, 'total_duration': 27500.048808574677, 'accumulated_submission_time': 26555.934617996216, 'accumulated_eval_time': 939.5918412208557, 'accumulated_logging_time': 1.7566168308258057}
I0131 22:37:20.309549 139908425447168 logging_writer.py:48] [78589] accumulated_eval_time=939.591841, accumulated_logging_time=1.756617, accumulated_submission_time=26555.934618, global_step=78589, preemption_count=0, score=26555.934618, test/accuracy=0.560100, test/loss=2.155045, test/num_examples=10000, total_duration=27500.048809, train/accuracy=0.769172, train/loss=1.146484, validation/accuracy=0.686640, validation/loss=1.498843, validation/num_examples=50000
I0131 22:37:24.366793 139908719027968 logging_writer.py:48] [78600] global_step=78600, grad_norm=2.2413837909698486, loss=3.012814521789551
I0131 22:37:58.101728 139908425447168 logging_writer.py:48] [78700] global_step=78700, grad_norm=2.1585824489593506, loss=3.017512321472168
I0131 22:38:31.759654 139908719027968 logging_writer.py:48] [78800] global_step=78800, grad_norm=2.235077142715454, loss=3.039003849029541
I0131 22:39:05.463208 139908425447168 logging_writer.py:48] [78900] global_step=78900, grad_norm=2.169163703918457, loss=3.0567626953125
I0131 22:39:39.217223 139908719027968 logging_writer.py:48] [79000] global_step=79000, grad_norm=2.3552494049072266, loss=3.0493006706237793
I0131 22:40:12.934331 139908425447168 logging_writer.py:48] [79100] global_step=79100, grad_norm=2.139580488204956, loss=3.0107712745666504
I0131 22:40:46.707412 139908719027968 logging_writer.py:48] [79200] global_step=79200, grad_norm=2.0706045627593994, loss=2.9277350902557373
I0131 22:41:20.397375 139908425447168 logging_writer.py:48] [79300] global_step=79300, grad_norm=2.573482036590576, loss=3.181234359741211
I0131 22:41:54.152844 139908719027968 logging_writer.py:48] [79400] global_step=79400, grad_norm=2.138336658477783, loss=3.047849416732788
I0131 22:42:27.894374 139908425447168 logging_writer.py:48] [79500] global_step=79500, grad_norm=2.2044873237609863, loss=2.9657459259033203
I0131 22:43:01.573239 139908719027968 logging_writer.py:48] [79600] global_step=79600, grad_norm=2.234112024307251, loss=2.962536334991455
I0131 22:43:35.286315 139908425447168 logging_writer.py:48] [79700] global_step=79700, grad_norm=2.2492587566375732, loss=2.9968552589416504
I0131 22:44:08.925406 139908719027968 logging_writer.py:48] [79800] global_step=79800, grad_norm=2.2009921073913574, loss=3.0580620765686035
I0131 22:44:42.626751 139908425447168 logging_writer.py:48] [79900] global_step=79900, grad_norm=2.317368984222412, loss=3.06095552444458
I0131 22:45:16.305627 139908719027968 logging_writer.py:48] [80000] global_step=80000, grad_norm=2.413519859313965, loss=3.04245662689209
I0131 22:45:50.039685 139908425447168 logging_writer.py:48] [80100] global_step=80100, grad_norm=2.328585386276245, loss=3.0756938457489014
I0131 22:45:50.529760 140070692116288 spec.py:321] Evaluating on the training split.
I0131 22:45:57.418612 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 22:46:06.515177 140070692116288 spec.py:349] Evaluating on the test split.
I0131 22:46:09.357577 140070692116288 submission_runner.py:408] Time since start: 28029.13s, 	Step: 80103, 	{'train/accuracy': 0.8092314600944519, 'train/loss': 0.9729456305503845, 'validation/accuracy': 0.6866999864578247, 'validation/loss': 1.4889295101165771, 'validation/num_examples': 50000, 'test/accuracy': 0.5586000084877014, 'test/loss': 2.127764940261841, 'test/num_examples': 10000, 'score': 27066.09313583374, 'total_duration': 28029.13027572632, 'accumulated_submission_time': 27066.09313583374, 'accumulated_eval_time': 958.4196372032166, 'accumulated_logging_time': 1.7997441291809082}
I0131 22:46:09.383229 139907754342144 logging_writer.py:48] [80103] accumulated_eval_time=958.419637, accumulated_logging_time=1.799744, accumulated_submission_time=27066.093136, global_step=80103, preemption_count=0, score=27066.093136, test/accuracy=0.558600, test/loss=2.127765, test/num_examples=10000, total_duration=28029.130276, train/accuracy=0.809231, train/loss=0.972946, validation/accuracy=0.686700, validation/loss=1.488930, validation/num_examples=50000
I0131 22:46:42.464189 139907762734848 logging_writer.py:48] [80200] global_step=80200, grad_norm=2.2908613681793213, loss=3.0105650424957275
I0131 22:47:16.270546 139907754342144 logging_writer.py:48] [80300] global_step=80300, grad_norm=2.090702772140503, loss=3.008612632751465
I0131 22:47:49.971865 139907762734848 logging_writer.py:48] [80400] global_step=80400, grad_norm=2.215914726257324, loss=3.091953754425049
I0131 22:48:23.626858 139907754342144 logging_writer.py:48] [80500] global_step=80500, grad_norm=2.3669543266296387, loss=3.094496965408325
I0131 22:48:57.426578 139907762734848 logging_writer.py:48] [80600] global_step=80600, grad_norm=2.490447998046875, loss=3.0098092555999756
I0131 22:49:31.145397 139907754342144 logging_writer.py:48] [80700] global_step=80700, grad_norm=2.4462220668792725, loss=3.0244669914245605
I0131 22:50:04.884900 139907762734848 logging_writer.py:48] [80800] global_step=80800, grad_norm=2.29657244682312, loss=3.0711729526519775
I0131 22:50:38.623848 139907754342144 logging_writer.py:48] [80900] global_step=80900, grad_norm=2.297657012939453, loss=3.0127034187316895
I0131 22:51:12.341423 139907762734848 logging_writer.py:48] [81000] global_step=81000, grad_norm=2.3590173721313477, loss=3.0856235027313232
I0131 22:51:46.424588 139907754342144 logging_writer.py:48] [81100] global_step=81100, grad_norm=2.1788854598999023, loss=3.0912771224975586
I0131 22:52:20.143106 139907762734848 logging_writer.py:48] [81200] global_step=81200, grad_norm=2.2303340435028076, loss=3.0394539833068848
I0131 22:52:53.891425 139907754342144 logging_writer.py:48] [81300] global_step=81300, grad_norm=2.295423746109009, loss=3.0418941974639893
I0131 22:53:27.736697 139907762734848 logging_writer.py:48] [81400] global_step=81400, grad_norm=2.2530953884124756, loss=3.081313133239746
I0131 22:54:01.474523 139907754342144 logging_writer.py:48] [81500] global_step=81500, grad_norm=2.266669750213623, loss=3.0134501457214355
I0131 22:54:35.207559 139907762734848 logging_writer.py:48] [81600] global_step=81600, grad_norm=2.231224298477173, loss=2.9672293663024902
I0131 22:54:39.399603 140070692116288 spec.py:321] Evaluating on the training split.
I0131 22:54:45.607418 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 22:54:54.599558 140070692116288 spec.py:349] Evaluating on the test split.
I0131 22:54:57.082047 140070692116288 submission_runner.py:408] Time since start: 28556.85s, 	Step: 81614, 	{'train/accuracy': 0.7765266299247742, 'train/loss': 1.067360758781433, 'validation/accuracy': 0.6843599677085876, 'validation/loss': 1.4753485918045044, 'validation/num_examples': 50000, 'test/accuracy': 0.5557000041007996, 'test/loss': 2.1231517791748047, 'test/num_examples': 10000, 'score': 27576.048866033554, 'total_duration': 28556.85472536087, 'accumulated_submission_time': 27576.048866033554, 'accumulated_eval_time': 976.1020576953888, 'accumulated_logging_time': 1.8335380554199219}
I0131 22:54:57.112769 139907737556736 logging_writer.py:48] [81614] accumulated_eval_time=976.102058, accumulated_logging_time=1.833538, accumulated_submission_time=27576.048866, global_step=81614, preemption_count=0, score=27576.048866, test/accuracy=0.555700, test/loss=2.123152, test/num_examples=10000, total_duration=28556.854725, train/accuracy=0.776527, train/loss=1.067361, validation/accuracy=0.684360, validation/loss=1.475349, validation/num_examples=50000
I0131 22:55:26.453540 139908710635264 logging_writer.py:48] [81700] global_step=81700, grad_norm=2.460108757019043, loss=3.103424072265625
I0131 22:56:00.146509 139907737556736 logging_writer.py:48] [81800] global_step=81800, grad_norm=2.225921392440796, loss=2.9716854095458984
I0131 22:56:33.866035 139908710635264 logging_writer.py:48] [81900] global_step=81900, grad_norm=2.237921953201294, loss=2.9749884605407715
I0131 22:57:07.600430 139907737556736 logging_writer.py:48] [82000] global_step=82000, grad_norm=2.1877028942108154, loss=2.973930597305298
I0131 22:57:41.377307 139908710635264 logging_writer.py:48] [82100] global_step=82100, grad_norm=2.1972897052764893, loss=3.1157500743865967
I0131 22:58:15.091219 139907737556736 logging_writer.py:48] [82200] global_step=82200, grad_norm=2.3228466510772705, loss=3.01183819770813
I0131 22:58:48.796730 139908710635264 logging_writer.py:48] [82300] global_step=82300, grad_norm=2.3275461196899414, loss=3.022589683532715
I0131 22:59:22.767747 139907737556736 logging_writer.py:48] [82400] global_step=82400, grad_norm=2.246361255645752, loss=2.9702649116516113
I0131 22:59:56.540660 139908710635264 logging_writer.py:48] [82500] global_step=82500, grad_norm=2.274693250656128, loss=3.018237590789795
I0131 23:00:30.250940 139907737556736 logging_writer.py:48] [82600] global_step=82600, grad_norm=2.2761380672454834, loss=3.0373120307922363
I0131 23:01:04.016813 139908710635264 logging_writer.py:48] [82700] global_step=82700, grad_norm=2.2192628383636475, loss=2.9860291481018066
I0131 23:01:37.731274 139907737556736 logging_writer.py:48] [82800] global_step=82800, grad_norm=2.363969087600708, loss=3.08956241607666
I0131 23:02:11.402496 139908710635264 logging_writer.py:48] [82900] global_step=82900, grad_norm=2.401268482208252, loss=2.9794301986694336
I0131 23:02:45.082942 139907737556736 logging_writer.py:48] [83000] global_step=83000, grad_norm=2.596700429916382, loss=3.0721206665039062
I0131 23:03:18.846587 139908710635264 logging_writer.py:48] [83100] global_step=83100, grad_norm=2.393602132797241, loss=3.0373237133026123
I0131 23:03:27.094035 140070692116288 spec.py:321] Evaluating on the training split.
I0131 23:03:33.311421 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 23:03:42.082098 140070692116288 spec.py:349] Evaluating on the test split.
I0131 23:03:44.549689 140070692116288 submission_runner.py:408] Time since start: 29084.32s, 	Step: 83126, 	{'train/accuracy': 0.7771045565605164, 'train/loss': 1.1030161380767822, 'validation/accuracy': 0.6881399750709534, 'validation/loss': 1.486668586730957, 'validation/num_examples': 50000, 'test/accuracy': 0.5627000331878662, 'test/loss': 2.139356851577759, 'test/num_examples': 10000, 'score': 28085.96573448181, 'total_duration': 29084.32238149643, 'accumulated_submission_time': 28085.96573448181, 'accumulated_eval_time': 993.557685136795, 'accumulated_logging_time': 1.8761115074157715}
I0131 23:03:44.583193 139907737556736 logging_writer.py:48] [83126] accumulated_eval_time=993.557685, accumulated_logging_time=1.876112, accumulated_submission_time=28085.965734, global_step=83126, preemption_count=0, score=28085.965734, test/accuracy=0.562700, test/loss=2.139357, test/num_examples=10000, total_duration=29084.322381, train/accuracy=0.777105, train/loss=1.103016, validation/accuracy=0.688140, validation/loss=1.486669, validation/num_examples=50000
I0131 23:04:09.818818 139907745949440 logging_writer.py:48] [83200] global_step=83200, grad_norm=2.2594316005706787, loss=2.959578275680542
I0131 23:04:43.529684 139907737556736 logging_writer.py:48] [83300] global_step=83300, grad_norm=2.337217092514038, loss=3.023524284362793
I0131 23:05:17.276912 139907745949440 logging_writer.py:48] [83400] global_step=83400, grad_norm=2.1787798404693604, loss=3.009937047958374
I0131 23:05:51.138720 139907737556736 logging_writer.py:48] [83500] global_step=83500, grad_norm=2.2763142585754395, loss=3.035881280899048
I0131 23:06:24.883815 139907745949440 logging_writer.py:48] [83600] global_step=83600, grad_norm=2.343909502029419, loss=3.02219557762146
I0131 23:06:58.627412 139907737556736 logging_writer.py:48] [83700] global_step=83700, grad_norm=2.3150362968444824, loss=3.0432443618774414
I0131 23:07:32.365403 139907745949440 logging_writer.py:48] [83800] global_step=83800, grad_norm=2.294752359390259, loss=3.0479111671447754
I0131 23:08:06.110319 139907737556736 logging_writer.py:48] [83900] global_step=83900, grad_norm=2.117157459259033, loss=2.9838144779205322
I0131 23:08:39.870117 139907745949440 logging_writer.py:48] [84000] global_step=84000, grad_norm=2.294384002685547, loss=3.042656660079956
I0131 23:09:13.599456 139907737556736 logging_writer.py:48] [84100] global_step=84100, grad_norm=2.3552794456481934, loss=3.0770633220672607
I0131 23:09:47.349509 139907745949440 logging_writer.py:48] [84200] global_step=84200, grad_norm=2.2671220302581787, loss=2.9797215461730957
I0131 23:10:21.080665 139907737556736 logging_writer.py:48] [84300] global_step=84300, grad_norm=2.2825496196746826, loss=2.96951961517334
I0131 23:10:54.835620 139907745949440 logging_writer.py:48] [84400] global_step=84400, grad_norm=2.5796871185302734, loss=3.017197370529175
I0131 23:11:28.553665 139907737556736 logging_writer.py:48] [84500] global_step=84500, grad_norm=2.2673676013946533, loss=2.974421501159668
I0131 23:12:02.375988 139907745949440 logging_writer.py:48] [84600] global_step=84600, grad_norm=2.2955451011657715, loss=2.999631881713867
I0131 23:12:14.716227 140070692116288 spec.py:321] Evaluating on the training split.
I0131 23:12:21.003198 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 23:12:29.869135 140070692116288 spec.py:349] Evaluating on the test split.
I0131 23:12:32.357197 140070692116288 submission_runner.py:408] Time since start: 29612.13s, 	Step: 84638, 	{'train/accuracy': 0.7768853306770325, 'train/loss': 1.0869734287261963, 'validation/accuracy': 0.693399965763092, 'validation/loss': 1.4536720514297485, 'validation/num_examples': 50000, 'test/accuracy': 0.5644000172615051, 'test/loss': 2.1074471473693848, 'test/num_examples': 10000, 'score': 28596.036551475525, 'total_duration': 29612.1298763752, 'accumulated_submission_time': 28596.036551475525, 'accumulated_eval_time': 1011.1986262798309, 'accumulated_logging_time': 1.9192132949829102}
I0131 23:12:32.392378 139907729164032 logging_writer.py:48] [84638] accumulated_eval_time=1011.198626, accumulated_logging_time=1.919213, accumulated_submission_time=28596.036551, global_step=84638, preemption_count=0, score=28596.036551, test/accuracy=0.564400, test/loss=2.107447, test/num_examples=10000, total_duration=29612.129876, train/accuracy=0.776885, train/loss=1.086973, validation/accuracy=0.693400, validation/loss=1.453672, validation/num_examples=50000
I0131 23:12:53.630193 139907737556736 logging_writer.py:48] [84700] global_step=84700, grad_norm=2.2352349758148193, loss=3.039691925048828
I0131 23:13:27.308756 139907729164032 logging_writer.py:48] [84800] global_step=84800, grad_norm=2.4098408222198486, loss=2.9640402793884277
I0131 23:14:00.999576 139907737556736 logging_writer.py:48] [84900] global_step=84900, grad_norm=2.1804051399230957, loss=3.001413106918335
I0131 23:14:34.778238 139907729164032 logging_writer.py:48] [85000] global_step=85000, grad_norm=2.3870797157287598, loss=3.04948091506958
I0131 23:15:08.459938 139907737556736 logging_writer.py:48] [85100] global_step=85100, grad_norm=2.2533233165740967, loss=2.985548734664917
I0131 23:15:42.160276 139907729164032 logging_writer.py:48] [85200] global_step=85200, grad_norm=2.266753911972046, loss=2.9889907836914062
I0131 23:16:15.854524 139907737556736 logging_writer.py:48] [85300] global_step=85300, grad_norm=2.2481930255889893, loss=3.0210354328155518
I0131 23:16:49.620609 139907729164032 logging_writer.py:48] [85400] global_step=85400, grad_norm=2.2327511310577393, loss=2.962265968322754
I0131 23:17:23.336899 139907737556736 logging_writer.py:48] [85500] global_step=85500, grad_norm=2.5382144451141357, loss=3.086085557937622
I0131 23:17:57.136239 139907729164032 logging_writer.py:48] [85600] global_step=85600, grad_norm=2.508225679397583, loss=3.067988872528076
I0131 23:18:30.958621 139907737556736 logging_writer.py:48] [85700] global_step=85700, grad_norm=2.2934000492095947, loss=3.025418281555176
I0131 23:19:04.658549 139907729164032 logging_writer.py:48] [85800] global_step=85800, grad_norm=2.3282387256622314, loss=3.041116714477539
I0131 23:19:38.374808 139907737556736 logging_writer.py:48] [85900] global_step=85900, grad_norm=2.3426356315612793, loss=2.9701921939849854
I0131 23:20:12.147233 139907729164032 logging_writer.py:48] [86000] global_step=86000, grad_norm=2.2191898822784424, loss=3.019622564315796
I0131 23:20:45.845941 139907737556736 logging_writer.py:48] [86100] global_step=86100, grad_norm=2.4717767238616943, loss=3.0525031089782715
I0131 23:21:02.523816 140070692116288 spec.py:321] Evaluating on the training split.
I0131 23:21:08.737117 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 23:21:17.522134 140070692116288 spec.py:349] Evaluating on the test split.
I0131 23:21:20.041059 140070692116288 submission_runner.py:408] Time since start: 30139.81s, 	Step: 86151, 	{'train/accuracy': 0.7732979655265808, 'train/loss': 1.0890520811080933, 'validation/accuracy': 0.6902599930763245, 'validation/loss': 1.4564690589904785, 'validation/num_examples': 50000, 'test/accuracy': 0.5592000484466553, 'test/loss': 2.1242754459381104, 'test/num_examples': 10000, 'score': 29106.105019569397, 'total_duration': 30139.81373643875, 'accumulated_submission_time': 29106.105019569397, 'accumulated_eval_time': 1028.7158319950104, 'accumulated_logging_time': 1.964526891708374}
I0131 23:21:20.072435 139907762734848 logging_writer.py:48] [86151] accumulated_eval_time=1028.715832, accumulated_logging_time=1.964527, accumulated_submission_time=29106.105020, global_step=86151, preemption_count=0, score=29106.105020, test/accuracy=0.559200, test/loss=2.124275, test/num_examples=10000, total_duration=30139.813736, train/accuracy=0.773298, train/loss=1.089052, validation/accuracy=0.690260, validation/loss=1.456469, validation/num_examples=50000
I0131 23:21:36.919916 139908425447168 logging_writer.py:48] [86200] global_step=86200, grad_norm=2.211261749267578, loss=3.0204648971557617
I0131 23:22:10.575947 139907762734848 logging_writer.py:48] [86300] global_step=86300, grad_norm=2.4932167530059814, loss=3.063750982284546
I0131 23:22:44.301225 139908425447168 logging_writer.py:48] [86400] global_step=86400, grad_norm=2.2077934741973877, loss=3.0461113452911377
I0131 23:23:18.010511 139907762734848 logging_writer.py:48] [86500] global_step=86500, grad_norm=2.419309377670288, loss=3.0394465923309326
I0131 23:23:51.786040 139908425447168 logging_writer.py:48] [86600] global_step=86600, grad_norm=2.2137179374694824, loss=2.9765567779541016
I0131 23:24:25.570235 139907762734848 logging_writer.py:48] [86700] global_step=86700, grad_norm=2.5031471252441406, loss=3.010763645172119
I0131 23:24:59.271917 139908425447168 logging_writer.py:48] [86800] global_step=86800, grad_norm=2.457221508026123, loss=3.0410044193267822
I0131 23:25:33.036992 139907762734848 logging_writer.py:48] [86900] global_step=86900, grad_norm=2.368173837661743, loss=3.009795665740967
I0131 23:26:06.734264 139908425447168 logging_writer.py:48] [87000] global_step=87000, grad_norm=2.285618782043457, loss=2.9608216285705566
I0131 23:26:40.503137 139907762734848 logging_writer.py:48] [87100] global_step=87100, grad_norm=2.3898353576660156, loss=3.0455658435821533
I0131 23:27:14.225656 139908425447168 logging_writer.py:48] [87200] global_step=87200, grad_norm=2.227170705795288, loss=2.974360942840576
I0131 23:27:47.965567 139907762734848 logging_writer.py:48] [87300] global_step=87300, grad_norm=2.225696325302124, loss=2.938632011413574
I0131 23:28:21.711728 139908425447168 logging_writer.py:48] [87400] global_step=87400, grad_norm=2.1626808643341064, loss=2.9608311653137207
I0131 23:28:55.471399 139907762734848 logging_writer.py:48] [87500] global_step=87500, grad_norm=2.4697303771972656, loss=2.992851495742798
I0131 23:29:29.196473 139908425447168 logging_writer.py:48] [87600] global_step=87600, grad_norm=2.5567398071289062, loss=3.008671998977661
I0131 23:29:50.281196 140070692116288 spec.py:321] Evaluating on the training split.
I0131 23:29:56.645701 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 23:30:05.601238 140070692116288 spec.py:349] Evaluating on the test split.
I0131 23:30:08.218949 140070692116288 submission_runner.py:408] Time since start: 30667.99s, 	Step: 87664, 	{'train/accuracy': 0.755281388759613, 'train/loss': 1.1695573329925537, 'validation/accuracy': 0.6784399747848511, 'validation/loss': 1.5213377475738525, 'validation/num_examples': 50000, 'test/accuracy': 0.5514000058174133, 'test/loss': 2.184067964553833, 'test/num_examples': 10000, 'score': 29616.25096130371, 'total_duration': 30667.99164557457, 'accumulated_submission_time': 29616.25096130371, 'accumulated_eval_time': 1046.6535975933075, 'accumulated_logging_time': 2.0054867267608643}
I0131 23:30:08.247058 139907745949440 logging_writer.py:48] [87664] accumulated_eval_time=1046.653598, accumulated_logging_time=2.005487, accumulated_submission_time=29616.250961, global_step=87664, preemption_count=0, score=29616.250961, test/accuracy=0.551400, test/loss=2.184068, test/num_examples=10000, total_duration=30667.991646, train/accuracy=0.755281, train/loss=1.169557, validation/accuracy=0.678440, validation/loss=1.521338, validation/num_examples=50000
I0131 23:30:20.718485 139907754342144 logging_writer.py:48] [87700] global_step=87700, grad_norm=2.387176513671875, loss=2.993952989578247
I0131 23:30:54.459079 139907745949440 logging_writer.py:48] [87800] global_step=87800, grad_norm=2.447938919067383, loss=3.078242301940918
I0131 23:31:28.241724 139907754342144 logging_writer.py:48] [87900] global_step=87900, grad_norm=2.396958112716675, loss=2.998370885848999
I0131 23:32:01.946978 139907745949440 logging_writer.py:48] [88000] global_step=88000, grad_norm=2.553091049194336, loss=3.0259621143341064
I0131 23:32:35.738457 139907754342144 logging_writer.py:48] [88100] global_step=88100, grad_norm=2.3885936737060547, loss=2.976696252822876
I0131 23:33:09.428462 139907745949440 logging_writer.py:48] [88200] global_step=88200, grad_norm=2.2656259536743164, loss=3.0657708644866943
I0131 23:33:43.096258 139907754342144 logging_writer.py:48] [88300] global_step=88300, grad_norm=2.5035288333892822, loss=2.993919849395752
I0131 23:34:16.834344 139907745949440 logging_writer.py:48] [88400] global_step=88400, grad_norm=2.3772058486938477, loss=2.978531837463379
I0131 23:34:50.573046 139907754342144 logging_writer.py:48] [88500] global_step=88500, grad_norm=2.306074857711792, loss=2.9914302825927734
I0131 23:35:24.270750 139907745949440 logging_writer.py:48] [88600] global_step=88600, grad_norm=2.4007925987243652, loss=2.9608523845672607
I0131 23:35:58.036782 139907754342144 logging_writer.py:48] [88700] global_step=88700, grad_norm=2.2703821659088135, loss=3.032449722290039
I0131 23:36:31.707346 139907745949440 logging_writer.py:48] [88800] global_step=88800, grad_norm=2.27646803855896, loss=2.9957642555236816
I0131 23:37:05.525570 139907754342144 logging_writer.py:48] [88900] global_step=88900, grad_norm=2.2829248905181885, loss=3.0169808864593506
I0131 23:37:39.254260 139907745949440 logging_writer.py:48] [89000] global_step=89000, grad_norm=2.357618808746338, loss=3.0142579078674316
I0131 23:38:12.997373 139907754342144 logging_writer.py:48] [89100] global_step=89100, grad_norm=2.291970729827881, loss=2.9207851886749268
I0131 23:38:38.404616 140070692116288 spec.py:321] Evaluating on the training split.
I0131 23:38:44.607628 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 23:38:53.517233 140070692116288 spec.py:349] Evaluating on the test split.
I0131 23:38:55.972957 140070692116288 submission_runner.py:408] Time since start: 31195.75s, 	Step: 89177, 	{'train/accuracy': 0.7955994606018066, 'train/loss': 1.0491045713424683, 'validation/accuracy': 0.6782199740409851, 'validation/loss': 1.5385297536849976, 'validation/num_examples': 50000, 'test/accuracy': 0.5473000407218933, 'test/loss': 2.2165260314941406, 'test/num_examples': 10000, 'score': 30126.34645795822, 'total_duration': 31195.745640039444, 'accumulated_submission_time': 30126.34645795822, 'accumulated_eval_time': 1064.221899986267, 'accumulated_logging_time': 2.043313980102539}
I0131 23:38:56.005280 139907745949440 logging_writer.py:48] [89177] accumulated_eval_time=1064.221900, accumulated_logging_time=2.043314, accumulated_submission_time=30126.346458, global_step=89177, preemption_count=0, score=30126.346458, test/accuracy=0.547300, test/loss=2.216526, test/num_examples=10000, total_duration=31195.745640, train/accuracy=0.795599, train/loss=1.049105, validation/accuracy=0.678220, validation/loss=1.538530, validation/num_examples=50000
I0131 23:39:04.114864 139908425447168 logging_writer.py:48] [89200] global_step=89200, grad_norm=2.4657392501831055, loss=2.9593286514282227
I0131 23:39:37.833775 139907745949440 logging_writer.py:48] [89300] global_step=89300, grad_norm=2.4086179733276367, loss=2.9618866443634033
I0131 23:40:11.504724 139908425447168 logging_writer.py:48] [89400] global_step=89400, grad_norm=2.2891182899475098, loss=2.959627389907837
I0131 23:40:45.206675 139907745949440 logging_writer.py:48] [89500] global_step=89500, grad_norm=2.3556694984436035, loss=2.962629556655884
I0131 23:41:18.957475 139908425447168 logging_writer.py:48] [89600] global_step=89600, grad_norm=2.257323980331421, loss=2.977252960205078
I0131 23:41:52.713449 139907745949440 logging_writer.py:48] [89700] global_step=89700, grad_norm=2.633753776550293, loss=3.038668155670166
I0131 23:42:26.481130 139908425447168 logging_writer.py:48] [89800] global_step=89800, grad_norm=2.5014891624450684, loss=3.0036468505859375
I0131 23:43:00.246010 139907745949440 logging_writer.py:48] [89900] global_step=89900, grad_norm=2.3493752479553223, loss=2.9474563598632812
I0131 23:43:33.980345 139908425447168 logging_writer.py:48] [90000] global_step=90000, grad_norm=2.390913963317871, loss=2.9727933406829834
I0131 23:44:07.675815 139907745949440 logging_writer.py:48] [90100] global_step=90100, grad_norm=2.3642609119415283, loss=3.042829990386963
I0131 23:44:41.434069 139908425447168 logging_writer.py:48] [90200] global_step=90200, grad_norm=2.402697801589966, loss=2.9838926792144775
I0131 23:45:15.143663 139907745949440 logging_writer.py:48] [90300] global_step=90300, grad_norm=2.6850998401641846, loss=3.0390868186950684
I0131 23:45:48.911795 139908425447168 logging_writer.py:48] [90400] global_step=90400, grad_norm=2.4919183254241943, loss=3.0577125549316406
I0131 23:46:22.651320 139907745949440 logging_writer.py:48] [90500] global_step=90500, grad_norm=2.5001728534698486, loss=3.0594770908355713
I0131 23:46:56.387440 139908425447168 logging_writer.py:48] [90600] global_step=90600, grad_norm=2.363353729248047, loss=2.921032428741455
I0131 23:47:26.191210 140070692116288 spec.py:321] Evaluating on the training split.
I0131 23:47:32.378545 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 23:47:41.223617 140070692116288 spec.py:349] Evaluating on the test split.
I0131 23:47:43.758379 140070692116288 submission_runner.py:408] Time since start: 31723.53s, 	Step: 90690, 	{'train/accuracy': 0.8026148080825806, 'train/loss': 0.9901580810546875, 'validation/accuracy': 0.7046200037002563, 'validation/loss': 1.4159032106399536, 'validation/num_examples': 50000, 'test/accuracy': 0.572700023651123, 'test/loss': 2.065574884414673, 'test/num_examples': 10000, 'score': 30636.468733549118, 'total_duration': 31723.53106689453, 'accumulated_submission_time': 30636.468733549118, 'accumulated_eval_time': 1081.7890536785126, 'accumulated_logging_time': 2.085658311843872}
I0131 23:47:43.795795 139907762734848 logging_writer.py:48] [90690] accumulated_eval_time=1081.789054, accumulated_logging_time=2.085658, accumulated_submission_time=30636.468734, global_step=90690, preemption_count=0, score=30636.468734, test/accuracy=0.572700, test/loss=2.065575, test/num_examples=10000, total_duration=31723.531067, train/accuracy=0.802615, train/loss=0.990158, validation/accuracy=0.704620, validation/loss=1.415903, validation/num_examples=50000
I0131 23:47:47.512006 139908710635264 logging_writer.py:48] [90700] global_step=90700, grad_norm=2.325693368911743, loss=2.9498376846313477
I0131 23:48:21.206137 139907762734848 logging_writer.py:48] [90800] global_step=90800, grad_norm=2.395514965057373, loss=3.039264440536499
I0131 23:48:54.873425 139908710635264 logging_writer.py:48] [90900] global_step=90900, grad_norm=2.6391055583953857, loss=2.9800472259521484
I0131 23:49:28.667544 139907762734848 logging_writer.py:48] [91000] global_step=91000, grad_norm=2.4466819763183594, loss=2.951972484588623
I0131 23:50:02.427303 139908710635264 logging_writer.py:48] [91100] global_step=91100, grad_norm=2.4346389770507812, loss=2.936899423599243
I0131 23:50:36.154371 139907762734848 logging_writer.py:48] [91200] global_step=91200, grad_norm=2.447587013244629, loss=2.9097981452941895
I0131 23:51:09.874519 139908710635264 logging_writer.py:48] [91300] global_step=91300, grad_norm=2.28651762008667, loss=2.9677608013153076
I0131 23:51:43.621316 139907762734848 logging_writer.py:48] [91400] global_step=91400, grad_norm=2.304612398147583, loss=2.880134105682373
I0131 23:52:17.345703 139908710635264 logging_writer.py:48] [91500] global_step=91500, grad_norm=2.3710036277770996, loss=3.05645751953125
I0131 23:52:51.089380 139907762734848 logging_writer.py:48] [91600] global_step=91600, grad_norm=2.295713424682617, loss=2.9808807373046875
I0131 23:53:24.838181 139908710635264 logging_writer.py:48] [91700] global_step=91700, grad_norm=2.4186038970947266, loss=3.10103440284729
I0131 23:53:58.561724 139907762734848 logging_writer.py:48] [91800] global_step=91800, grad_norm=2.9725005626678467, loss=2.9939725399017334
I0131 23:54:32.266983 139908710635264 logging_writer.py:48] [91900] global_step=91900, grad_norm=2.3273255825042725, loss=2.914222002029419
I0131 23:55:06.051088 139907762734848 logging_writer.py:48] [92000] global_step=92000, grad_norm=2.347651243209839, loss=2.9200291633605957
I0131 23:55:39.855514 139908710635264 logging_writer.py:48] [92100] global_step=92100, grad_norm=2.4359700679779053, loss=2.9451420307159424
I0131 23:56:13.557414 139907762734848 logging_writer.py:48] [92200] global_step=92200, grad_norm=2.477146625518799, loss=2.9067394733428955
I0131 23:56:14.052888 140070692116288 spec.py:321] Evaluating on the training split.
I0131 23:56:20.223485 140070692116288 spec.py:333] Evaluating on the validation split.
I0131 23:56:29.098828 140070692116288 spec.py:349] Evaluating on the test split.
I0131 23:56:31.515385 140070692116288 submission_runner.py:408] Time since start: 32251.29s, 	Step: 92203, 	{'train/accuracy': 0.7906369566917419, 'train/loss': 1.0359426736831665, 'validation/accuracy': 0.6946600079536438, 'validation/loss': 1.449451208114624, 'validation/num_examples': 50000, 'test/accuracy': 0.5664000511169434, 'test/loss': 2.099205255508423, 'test/num_examples': 10000, 'score': 31146.662981510162, 'total_duration': 32251.288065195084, 'accumulated_submission_time': 31146.662981510162, 'accumulated_eval_time': 1099.251507282257, 'accumulated_logging_time': 2.132451057434082}
I0131 23:56:31.548314 139907754342144 logging_writer.py:48] [92203] accumulated_eval_time=1099.251507, accumulated_logging_time=2.132451, accumulated_submission_time=31146.662982, global_step=92203, preemption_count=0, score=31146.662982, test/accuracy=0.566400, test/loss=2.099205, test/num_examples=10000, total_duration=32251.288065, train/accuracy=0.790637, train/loss=1.035943, validation/accuracy=0.694660, validation/loss=1.449451, validation/num_examples=50000
I0131 23:57:04.605628 139907762734848 logging_writer.py:48] [92300] global_step=92300, grad_norm=2.480309247970581, loss=2.882878303527832
I0131 23:57:38.328338 139907754342144 logging_writer.py:48] [92400] global_step=92400, grad_norm=2.598011016845703, loss=2.9302074909210205
I0131 23:58:12.097945 139907762734848 logging_writer.py:48] [92500] global_step=92500, grad_norm=2.4560418128967285, loss=2.9501307010650635
I0131 23:58:45.828982 139907754342144 logging_writer.py:48] [92600] global_step=92600, grad_norm=2.3844733238220215, loss=2.937040090560913
I0131 23:59:19.590227 139907762734848 logging_writer.py:48] [92700] global_step=92700, grad_norm=2.6172304153442383, loss=2.966966152191162
I0131 23:59:53.322856 139907754342144 logging_writer.py:48] [92800] global_step=92800, grad_norm=2.3974928855895996, loss=2.9990224838256836
I0201 00:00:27.065274 139907762734848 logging_writer.py:48] [92900] global_step=92900, grad_norm=2.5563583374023438, loss=3.0186822414398193
I0201 00:01:00.749824 139907754342144 logging_writer.py:48] [93000] global_step=93000, grad_norm=2.424381732940674, loss=2.9484152793884277
I0201 00:01:34.515219 139907762734848 logging_writer.py:48] [93100] global_step=93100, grad_norm=2.562864303588867, loss=3.0167324542999268
I0201 00:02:08.217190 139907754342144 logging_writer.py:48] [93200] global_step=93200, grad_norm=2.331657886505127, loss=2.919618844985962
I0201 00:02:41.997381 139907762734848 logging_writer.py:48] [93300] global_step=93300, grad_norm=2.525313138961792, loss=2.9678030014038086
I0201 00:03:15.713585 139907754342144 logging_writer.py:48] [93400] global_step=93400, grad_norm=2.4162089824676514, loss=2.9597103595733643
I0201 00:03:49.483080 139907762734848 logging_writer.py:48] [93500] global_step=93500, grad_norm=2.4443066120147705, loss=2.9266748428344727
I0201 00:04:23.181852 139907754342144 logging_writer.py:48] [93600] global_step=93600, grad_norm=2.477151870727539, loss=2.9782345294952393
I0201 00:04:56.976011 139907762734848 logging_writer.py:48] [93700] global_step=93700, grad_norm=2.7750961780548096, loss=3.0258278846740723
I0201 00:05:01.525248 140070692116288 spec.py:321] Evaluating on the training split.
I0201 00:05:07.785665 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 00:05:16.600643 140070692116288 spec.py:349] Evaluating on the test split.
I0201 00:05:19.069282 140070692116288 submission_runner.py:408] Time since start: 32778.84s, 	Step: 93715, 	{'train/accuracy': 0.7890027165412903, 'train/loss': 1.048642635345459, 'validation/accuracy': 0.6960399746894836, 'validation/loss': 1.4489803314208984, 'validation/num_examples': 50000, 'test/accuracy': 0.5732000470161438, 'test/loss': 2.0790419578552246, 'test/num_examples': 10000, 'score': 31656.57655262947, 'total_duration': 32778.84194803238, 'accumulated_submission_time': 31656.57655262947, 'accumulated_eval_time': 1116.7954907417297, 'accumulated_logging_time': 2.174596071243286}
I0201 00:05:19.101396 139908425447168 logging_writer.py:48] [93715] accumulated_eval_time=1116.795491, accumulated_logging_time=2.174596, accumulated_submission_time=31656.576553, global_step=93715, preemption_count=0, score=31656.576553, test/accuracy=0.573200, test/loss=2.079042, test/num_examples=10000, total_duration=32778.841948, train/accuracy=0.789003, train/loss=1.048643, validation/accuracy=0.696040, validation/loss=1.448980, validation/num_examples=50000
I0201 00:05:48.074855 139908710635264 logging_writer.py:48] [93800] global_step=93800, grad_norm=2.445199489593506, loss=2.919316053390503
I0201 00:06:21.773667 139908425447168 logging_writer.py:48] [93900] global_step=93900, grad_norm=2.5020604133605957, loss=3.063786506652832
I0201 00:06:55.431836 139908710635264 logging_writer.py:48] [94000] global_step=94000, grad_norm=2.3640756607055664, loss=2.9433834552764893
I0201 00:07:29.144395 139908425447168 logging_writer.py:48] [94100] global_step=94100, grad_norm=2.562593936920166, loss=3.012514352798462
I0201 00:08:02.955437 139908710635264 logging_writer.py:48] [94200] global_step=94200, grad_norm=2.584876537322998, loss=2.946946382522583
I0201 00:08:36.674237 139908425447168 logging_writer.py:48] [94300] global_step=94300, grad_norm=2.4795315265655518, loss=3.01043963432312
I0201 00:09:10.427111 139908710635264 logging_writer.py:48] [94400] global_step=94400, grad_norm=2.441200017929077, loss=3.0152618885040283
I0201 00:09:44.147055 139908425447168 logging_writer.py:48] [94500] global_step=94500, grad_norm=2.4944519996643066, loss=2.9504752159118652
I0201 00:10:17.806658 139908710635264 logging_writer.py:48] [94600] global_step=94600, grad_norm=2.2975900173187256, loss=2.914501905441284
I0201 00:10:51.535041 139908425447168 logging_writer.py:48] [94700] global_step=94700, grad_norm=2.4893903732299805, loss=2.9529218673706055
I0201 00:11:25.266752 139908710635264 logging_writer.py:48] [94800] global_step=94800, grad_norm=2.4868152141571045, loss=2.9175338745117188
I0201 00:11:58.992317 139908425447168 logging_writer.py:48] [94900] global_step=94900, grad_norm=2.60198974609375, loss=2.961569309234619
I0201 00:12:32.712822 139908710635264 logging_writer.py:48] [95000] global_step=95000, grad_norm=2.3147125244140625, loss=2.9767935276031494
I0201 00:13:06.437161 139908425447168 logging_writer.py:48] [95100] global_step=95100, grad_norm=2.6288869380950928, loss=2.977215528488159
I0201 00:13:40.153592 139908710635264 logging_writer.py:48] [95200] global_step=95200, grad_norm=2.486297845840454, loss=2.9515328407287598
I0201 00:13:49.097379 140070692116288 spec.py:321] Evaluating on the training split.
I0201 00:13:55.264434 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 00:14:04.324829 140070692116288 spec.py:349] Evaluating on the test split.
I0201 00:14:06.811815 140070692116288 submission_runner.py:408] Time since start: 33306.58s, 	Step: 95228, 	{'train/accuracy': 0.7940050959587097, 'train/loss': 1.0202025175094604, 'validation/accuracy': 0.706059992313385, 'validation/loss': 1.4011818170547485, 'validation/num_examples': 50000, 'test/accuracy': 0.5764999985694885, 'test/loss': 2.050769567489624, 'test/num_examples': 10000, 'score': 32166.51022219658, 'total_duration': 33306.58448624611, 'accumulated_submission_time': 32166.51022219658, 'accumulated_eval_time': 1134.5098896026611, 'accumulated_logging_time': 2.2162539958953857}
I0201 00:14:06.854381 139907762734848 logging_writer.py:48] [95228] accumulated_eval_time=1134.509890, accumulated_logging_time=2.216254, accumulated_submission_time=32166.510222, global_step=95228, preemption_count=0, score=32166.510222, test/accuracy=0.576500, test/loss=2.050770, test/num_examples=10000, total_duration=33306.584486, train/accuracy=0.794005, train/loss=1.020203, validation/accuracy=0.706060, validation/loss=1.401182, validation/num_examples=50000
I0201 00:14:31.532388 139908719027968 logging_writer.py:48] [95300] global_step=95300, grad_norm=2.512718915939331, loss=3.0108134746551514
I0201 00:15:05.218726 139907762734848 logging_writer.py:48] [95400] global_step=95400, grad_norm=2.5848429203033447, loss=2.8766748905181885
I0201 00:15:38.915209 139908719027968 logging_writer.py:48] [95500] global_step=95500, grad_norm=2.420330762863159, loss=2.9324324131011963
I0201 00:16:12.621580 139907762734848 logging_writer.py:48] [95600] global_step=95600, grad_norm=2.345191240310669, loss=2.9120867252349854
I0201 00:16:46.353309 139908719027968 logging_writer.py:48] [95700] global_step=95700, grad_norm=2.493006706237793, loss=2.9568557739257812
I0201 00:17:20.032274 139907762734848 logging_writer.py:48] [95800] global_step=95800, grad_norm=2.630675792694092, loss=3.0036211013793945
I0201 00:17:53.743018 139908719027968 logging_writer.py:48] [95900] global_step=95900, grad_norm=2.6805684566497803, loss=2.9594967365264893
I0201 00:18:27.491449 139907762734848 logging_writer.py:48] [96000] global_step=96000, grad_norm=2.5594828128814697, loss=2.941710948944092
I0201 00:19:01.203669 139908719027968 logging_writer.py:48] [96100] global_step=96100, grad_norm=2.482649087905884, loss=2.9832005500793457
I0201 00:19:34.997420 139907762734848 logging_writer.py:48] [96200] global_step=96200, grad_norm=2.618178129196167, loss=2.940253257751465
I0201 00:20:08.699477 139908719027968 logging_writer.py:48] [96300] global_step=96300, grad_norm=2.5722270011901855, loss=2.9910888671875
I0201 00:20:42.621808 139907762734848 logging_writer.py:48] [96400] global_step=96400, grad_norm=2.7825372219085693, loss=3.056802272796631
I0201 00:21:16.395537 139908719027968 logging_writer.py:48] [96500] global_step=96500, grad_norm=2.2693707942962646, loss=2.931694984436035
I0201 00:21:50.094891 139907762734848 logging_writer.py:48] [96600] global_step=96600, grad_norm=2.510960340499878, loss=2.9677014350891113
I0201 00:22:23.792844 139908719027968 logging_writer.py:48] [96700] global_step=96700, grad_norm=2.4563698768615723, loss=2.9158475399017334
I0201 00:22:37.128774 140070692116288 spec.py:321] Evaluating on the training split.
I0201 00:22:43.312595 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 00:22:52.142305 140070692116288 spec.py:349] Evaluating on the test split.
I0201 00:22:54.639565 140070692116288 submission_runner.py:408] Time since start: 33834.41s, 	Step: 96741, 	{'train/accuracy': 0.7877470850944519, 'train/loss': 1.0436723232269287, 'validation/accuracy': 0.6979999542236328, 'validation/loss': 1.427424669265747, 'validation/num_examples': 50000, 'test/accuracy': 0.5685000419616699, 'test/loss': 2.087905168533325, 'test/num_examples': 10000, 'score': 32676.71812939644, 'total_duration': 33834.412241220474, 'accumulated_submission_time': 32676.71812939644, 'accumulated_eval_time': 1152.0206370353699, 'accumulated_logging_time': 2.271630048751831}
I0201 00:22:54.672892 139907754342144 logging_writer.py:48] [96741] accumulated_eval_time=1152.020637, accumulated_logging_time=2.271630, accumulated_submission_time=32676.718129, global_step=96741, preemption_count=0, score=32676.718129, test/accuracy=0.568500, test/loss=2.087905, test/num_examples=10000, total_duration=33834.412241, train/accuracy=0.787747, train/loss=1.043672, validation/accuracy=0.698000, validation/loss=1.427425, validation/num_examples=50000
I0201 00:23:14.879262 139908425447168 logging_writer.py:48] [96800] global_step=96800, grad_norm=2.514723062515259, loss=3.013583183288574
I0201 00:23:48.557677 139907754342144 logging_writer.py:48] [96900] global_step=96900, grad_norm=2.467951536178589, loss=3.007115125656128
I0201 00:24:22.256218 139908425447168 logging_writer.py:48] [97000] global_step=97000, grad_norm=2.3896822929382324, loss=2.9753775596618652
I0201 00:24:56.017475 139907754342144 logging_writer.py:48] [97100] global_step=97100, grad_norm=2.4827487468719482, loss=2.952803611755371
I0201 00:25:29.750469 139908425447168 logging_writer.py:48] [97200] global_step=97200, grad_norm=2.517700433731079, loss=2.9168541431427
I0201 00:26:03.480792 139907754342144 logging_writer.py:48] [97300] global_step=97300, grad_norm=2.5903029441833496, loss=2.976285457611084
I0201 00:26:37.425420 139908425447168 logging_writer.py:48] [97400] global_step=97400, grad_norm=2.364253520965576, loss=2.935112476348877
I0201 00:27:11.187025 139907754342144 logging_writer.py:48] [97500] global_step=97500, grad_norm=2.534475564956665, loss=2.907644748687744
I0201 00:27:44.884629 139908425447168 logging_writer.py:48] [97600] global_step=97600, grad_norm=2.745882272720337, loss=2.942417860031128
I0201 00:28:18.539981 139907754342144 logging_writer.py:48] [97700] global_step=97700, grad_norm=2.3675460815429688, loss=2.9360239505767822
I0201 00:28:52.309834 139908425447168 logging_writer.py:48] [97800] global_step=97800, grad_norm=2.670616865158081, loss=3.0697758197784424
I0201 00:29:26.043717 139907754342144 logging_writer.py:48] [97900] global_step=97900, grad_norm=2.57254958152771, loss=2.961231231689453
I0201 00:29:59.805592 139908425447168 logging_writer.py:48] [98000] global_step=98000, grad_norm=2.3551199436187744, loss=2.9432084560394287
I0201 00:30:33.563626 139907754342144 logging_writer.py:48] [98100] global_step=98100, grad_norm=2.5685653686523438, loss=2.939316511154175
I0201 00:31:07.266531 139908425447168 logging_writer.py:48] [98200] global_step=98200, grad_norm=2.4395885467529297, loss=2.89064359664917
I0201 00:31:24.915565 140070692116288 spec.py:321] Evaluating on the training split.
I0201 00:31:31.149403 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 00:31:39.914422 140070692116288 spec.py:349] Evaluating on the test split.
I0201 00:31:42.399484 140070692116288 submission_runner.py:408] Time since start: 34362.17s, 	Step: 98254, 	{'train/accuracy': 0.8298588991165161, 'train/loss': 0.8838488459587097, 'validation/accuracy': 0.7114599943161011, 'validation/loss': 1.3823779821395874, 'validation/num_examples': 50000, 'test/accuracy': 0.5869000554084778, 'test/loss': 2.0103774070739746, 'test/num_examples': 10000, 'score': 33186.897922992706, 'total_duration': 34362.17217421532, 'accumulated_submission_time': 33186.897922992706, 'accumulated_eval_time': 1169.504544019699, 'accumulated_logging_time': 2.3140299320220947}
I0201 00:31:42.435794 139907745949440 logging_writer.py:48] [98254] accumulated_eval_time=1169.504544, accumulated_logging_time=2.314030, accumulated_submission_time=33186.897923, global_step=98254, preemption_count=0, score=33186.897923, test/accuracy=0.586900, test/loss=2.010377, test/num_examples=10000, total_duration=34362.172174, train/accuracy=0.829859, train/loss=0.883849, validation/accuracy=0.711460, validation/loss=1.382378, validation/num_examples=50000
I0201 00:31:58.240990 139907762734848 logging_writer.py:48] [98300] global_step=98300, grad_norm=2.461271047592163, loss=2.953854560852051
I0201 00:32:31.871983 139907745949440 logging_writer.py:48] [98400] global_step=98400, grad_norm=2.5159122943878174, loss=2.910776376724243
I0201 00:33:05.732224 139907762734848 logging_writer.py:48] [98500] global_step=98500, grad_norm=2.5502374172210693, loss=2.9348201751708984
I0201 00:33:39.473407 139907745949440 logging_writer.py:48] [98600] global_step=98600, grad_norm=2.5604422092437744, loss=2.9166953563690186
I0201 00:34:13.148862 139907762734848 logging_writer.py:48] [98700] global_step=98700, grad_norm=2.491122007369995, loss=2.8845558166503906
I0201 00:34:46.898789 139907745949440 logging_writer.py:48] [98800] global_step=98800, grad_norm=2.4429931640625, loss=2.885201930999756
I0201 00:35:20.617722 139907762734848 logging_writer.py:48] [98900] global_step=98900, grad_norm=2.553950071334839, loss=2.9363515377044678
I0201 00:35:54.375766 139907745949440 logging_writer.py:48] [99000] global_step=99000, grad_norm=2.5055973529815674, loss=2.985774517059326
I0201 00:36:28.099346 139907762734848 logging_writer.py:48] [99100] global_step=99100, grad_norm=2.4204483032226562, loss=2.920654296875
I0201 00:37:01.764704 139907745949440 logging_writer.py:48] [99200] global_step=99200, grad_norm=2.6449053287506104, loss=2.9607503414154053
I0201 00:37:35.530345 139907762734848 logging_writer.py:48] [99300] global_step=99300, grad_norm=2.6601810455322266, loss=2.9443087577819824
I0201 00:38:09.235962 139907745949440 logging_writer.py:48] [99400] global_step=99400, grad_norm=2.5221047401428223, loss=2.9194905757904053
I0201 00:38:42.980559 139907762734848 logging_writer.py:48] [99500] global_step=99500, grad_norm=2.889766216278076, loss=2.929185152053833
I0201 00:39:16.802786 139907745949440 logging_writer.py:48] [99600] global_step=99600, grad_norm=2.5041263103485107, loss=2.924288749694824
I0201 00:39:50.515427 139907762734848 logging_writer.py:48] [99700] global_step=99700, grad_norm=2.3731071949005127, loss=2.8773322105407715
I0201 00:40:12.581583 140070692116288 spec.py:321] Evaluating on the training split.
I0201 00:40:18.819594 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 00:40:27.627802 140070692116288 spec.py:349] Evaluating on the test split.
I0201 00:40:30.137495 140070692116288 submission_runner.py:408] Time since start: 34889.91s, 	Step: 99767, 	{'train/accuracy': 0.8191565275192261, 'train/loss': 0.9219579696655273, 'validation/accuracy': 0.7096199989318848, 'validation/loss': 1.3757820129394531, 'validation/num_examples': 50000, 'test/accuracy': 0.5852000117301941, 'test/loss': 2.0205318927764893, 'test/num_examples': 10000, 'score': 33696.980467796326, 'total_duration': 34889.91017913818, 'accumulated_submission_time': 33696.980467796326, 'accumulated_eval_time': 1187.0604236125946, 'accumulated_logging_time': 2.3605294227600098}
I0201 00:40:30.171721 139907737556736 logging_writer.py:48] [99767] accumulated_eval_time=1187.060424, accumulated_logging_time=2.360529, accumulated_submission_time=33696.980468, global_step=99767, preemption_count=0, score=33696.980468, test/accuracy=0.585200, test/loss=2.020532, test/num_examples=10000, total_duration=34889.910179, train/accuracy=0.819157, train/loss=0.921958, validation/accuracy=0.709620, validation/loss=1.375782, validation/num_examples=50000
I0201 00:40:41.637173 139907754342144 logging_writer.py:48] [99800] global_step=99800, grad_norm=2.4735541343688965, loss=2.940688133239746
I0201 00:41:15.351202 139907737556736 logging_writer.py:48] [99900] global_step=99900, grad_norm=2.3947980403900146, loss=2.953331470489502
I0201 00:41:49.027567 139907754342144 logging_writer.py:48] [100000] global_step=100000, grad_norm=2.6790590286254883, loss=2.9936084747314453
I0201 00:42:22.749659 139907737556736 logging_writer.py:48] [100100] global_step=100100, grad_norm=2.5895066261291504, loss=2.9573237895965576
I0201 00:42:56.438603 139907754342144 logging_writer.py:48] [100200] global_step=100200, grad_norm=2.6802361011505127, loss=2.85494065284729
I0201 00:43:30.189329 139907737556736 logging_writer.py:48] [100300] global_step=100300, grad_norm=2.6288094520568848, loss=2.988123655319214
I0201 00:44:03.875957 139907754342144 logging_writer.py:48] [100400] global_step=100400, grad_norm=2.559450387954712, loss=2.9687793254852295
I0201 00:44:37.618577 139907737556736 logging_writer.py:48] [100500] global_step=100500, grad_norm=2.671443462371826, loss=2.974658966064453
I0201 00:45:11.357735 139907754342144 logging_writer.py:48] [100600] global_step=100600, grad_norm=2.715471029281616, loss=2.9453091621398926
I0201 00:45:45.132088 139907737556736 logging_writer.py:48] [100700] global_step=100700, grad_norm=2.7179644107818604, loss=2.9671475887298584
I0201 00:46:18.891235 139907754342144 logging_writer.py:48] [100800] global_step=100800, grad_norm=2.720254421234131, loss=2.8966474533081055
I0201 00:46:52.582013 139907737556736 logging_writer.py:48] [100900] global_step=100900, grad_norm=2.4546940326690674, loss=2.887958526611328
I0201 00:47:26.255573 139907754342144 logging_writer.py:48] [101000] global_step=101000, grad_norm=2.4358043670654297, loss=2.945857286453247
I0201 00:47:59.961592 139907737556736 logging_writer.py:48] [101100] global_step=101100, grad_norm=2.711137056350708, loss=2.953078269958496
I0201 00:48:33.703181 139907754342144 logging_writer.py:48] [101200] global_step=101200, grad_norm=2.85947585105896, loss=2.997528553009033
I0201 00:49:00.169940 140070692116288 spec.py:321] Evaluating on the training split.
I0201 00:49:06.492779 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 00:49:15.204920 140070692116288 spec.py:349] Evaluating on the test split.
I0201 00:49:17.702813 140070692116288 submission_runner.py:408] Time since start: 35417.48s, 	Step: 101280, 	{'train/accuracy': 0.8095503449440002, 'train/loss': 0.9634189605712891, 'validation/accuracy': 0.7093799710273743, 'validation/loss': 1.3969210386276245, 'validation/num_examples': 50000, 'test/accuracy': 0.5839000344276428, 'test/loss': 2.0435094833374023, 'test/num_examples': 10000, 'score': 34206.91540932655, 'total_duration': 35417.475497722626, 'accumulated_submission_time': 34206.91540932655, 'accumulated_eval_time': 1204.593267440796, 'accumulated_logging_time': 2.404573440551758}
I0201 00:49:17.736715 139908710635264 logging_writer.py:48] [101280] accumulated_eval_time=1204.593267, accumulated_logging_time=2.404573, accumulated_submission_time=34206.915409, global_step=101280, preemption_count=0, score=34206.915409, test/accuracy=0.583900, test/loss=2.043509, test/num_examples=10000, total_duration=35417.475498, train/accuracy=0.809550, train/loss=0.963419, validation/accuracy=0.709380, validation/loss=1.396921, validation/num_examples=50000
I0201 00:49:24.854702 139908719027968 logging_writer.py:48] [101300] global_step=101300, grad_norm=2.5285484790802, loss=2.9617178440093994
I0201 00:49:58.558013 139908710635264 logging_writer.py:48] [101400] global_step=101400, grad_norm=2.645286798477173, loss=2.953944683074951
I0201 00:50:32.225974 139908719027968 logging_writer.py:48] [101500] global_step=101500, grad_norm=2.9229443073272705, loss=2.88142991065979
I0201 00:51:05.929725 139908710635264 logging_writer.py:48] [101600] global_step=101600, grad_norm=2.413264274597168, loss=2.9296274185180664
I0201 00:51:39.711579 139908719027968 logging_writer.py:48] [101700] global_step=101700, grad_norm=2.6137566566467285, loss=2.9376611709594727
I0201 00:52:13.440522 139908710635264 logging_writer.py:48] [101800] global_step=101800, grad_norm=2.7647485733032227, loss=2.9094018936157227
I0201 00:52:47.203534 139908719027968 logging_writer.py:48] [101900] global_step=101900, grad_norm=2.641303062438965, loss=2.853342056274414
I0201 00:53:20.925506 139908710635264 logging_writer.py:48] [102000] global_step=102000, grad_norm=2.6929285526275635, loss=2.9469408988952637
I0201 00:53:54.685289 139908719027968 logging_writer.py:48] [102100] global_step=102100, grad_norm=2.6455941200256348, loss=2.9517931938171387
I0201 00:54:28.371569 139908710635264 logging_writer.py:48] [102200] global_step=102200, grad_norm=2.725893497467041, loss=2.9085121154785156
I0201 00:55:02.126285 139908719027968 logging_writer.py:48] [102300] global_step=102300, grad_norm=2.5728113651275635, loss=2.94128680229187
I0201 00:55:35.857758 139908710635264 logging_writer.py:48] [102400] global_step=102400, grad_norm=2.7193338871002197, loss=2.963514804840088
I0201 00:56:09.579283 139908719027968 logging_writer.py:48] [102500] global_step=102500, grad_norm=2.519815444946289, loss=2.9091224670410156
I0201 00:56:43.343357 139908710635264 logging_writer.py:48] [102600] global_step=102600, grad_norm=2.600234270095825, loss=2.888906240463257
I0201 00:57:17.096578 139908719027968 logging_writer.py:48] [102700] global_step=102700, grad_norm=2.57791805267334, loss=2.9284329414367676
I0201 00:57:47.970838 140070692116288 spec.py:321] Evaluating on the training split.
I0201 00:57:54.174541 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 00:58:03.168358 140070692116288 spec.py:349] Evaluating on the test split.
I0201 00:58:05.606266 140070692116288 submission_runner.py:408] Time since start: 35945.38s, 	Step: 102793, 	{'train/accuracy': 0.8084542155265808, 'train/loss': 0.9739400744438171, 'validation/accuracy': 0.7127400040626526, 'validation/loss': 1.390412449836731, 'validation/num_examples': 50000, 'test/accuracy': 0.5825000405311584, 'test/loss': 2.034860849380493, 'test/num_examples': 10000, 'score': 34717.08581137657, 'total_duration': 35945.378918647766, 'accumulated_submission_time': 34717.08581137657, 'accumulated_eval_time': 1222.228625535965, 'accumulated_logging_time': 2.44966459274292}
I0201 00:58:05.638766 139907754342144 logging_writer.py:48] [102793] accumulated_eval_time=1222.228626, accumulated_logging_time=2.449665, accumulated_submission_time=34717.085811, global_step=102793, preemption_count=0, score=34717.085811, test/accuracy=0.582500, test/loss=2.034861, test/num_examples=10000, total_duration=35945.378919, train/accuracy=0.808454, train/loss=0.973940, validation/accuracy=0.712740, validation/loss=1.390412, validation/num_examples=50000
I0201 00:58:08.354330 139907762734848 logging_writer.py:48] [102800] global_step=102800, grad_norm=2.6412220001220703, loss=2.902320623397827
I0201 00:58:42.036199 139907754342144 logging_writer.py:48] [102900] global_step=102900, grad_norm=2.555948495864868, loss=2.8468942642211914
I0201 00:59:15.743039 139907762734848 logging_writer.py:48] [103000] global_step=103000, grad_norm=2.684420585632324, loss=2.8988189697265625
I0201 00:59:49.443289 139907754342144 logging_writer.py:48] [103100] global_step=103100, grad_norm=2.559183120727539, loss=2.8997535705566406
I0201 01:00:23.183139 139907762734848 logging_writer.py:48] [103200] global_step=103200, grad_norm=2.443864583969116, loss=2.9372220039367676
I0201 01:00:56.840986 139907754342144 logging_writer.py:48] [103300] global_step=103300, grad_norm=2.584804058074951, loss=2.9105050563812256
I0201 01:01:30.527792 139907762734848 logging_writer.py:48] [103400] global_step=103400, grad_norm=2.568284511566162, loss=2.8884027004241943
I0201 01:02:04.206216 139907754342144 logging_writer.py:48] [103500] global_step=103500, grad_norm=2.555483341217041, loss=2.8978073596954346
I0201 01:02:37.910606 139907762734848 logging_writer.py:48] [103600] global_step=103600, grad_norm=2.6037158966064453, loss=2.8926825523376465
I0201 01:03:11.668388 139907754342144 logging_writer.py:48] [103700] global_step=103700, grad_norm=2.8721773624420166, loss=2.9194133281707764
I0201 01:03:45.390322 139907762734848 logging_writer.py:48] [103800] global_step=103800, grad_norm=2.6990253925323486, loss=2.972243309020996
I0201 01:04:19.217712 139907754342144 logging_writer.py:48] [103900] global_step=103900, grad_norm=2.4426071643829346, loss=2.8540303707122803
I0201 01:04:52.908693 139907762734848 logging_writer.py:48] [104000] global_step=104000, grad_norm=2.568615436553955, loss=2.9123117923736572
I0201 01:05:26.669342 139907754342144 logging_writer.py:48] [104100] global_step=104100, grad_norm=2.6920199394226074, loss=2.894590377807617
I0201 01:06:00.369831 139907762734848 logging_writer.py:48] [104200] global_step=104200, grad_norm=2.810420513153076, loss=2.880286455154419
I0201 01:06:34.082946 139907754342144 logging_writer.py:48] [104300] global_step=104300, grad_norm=2.6994950771331787, loss=2.9037020206451416
I0201 01:06:35.924692 140070692116288 spec.py:321] Evaluating on the training split.
I0201 01:06:42.069988 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 01:06:50.813853 140070692116288 spec.py:349] Evaluating on the test split.
I0201 01:06:53.345567 140070692116288 submission_runner.py:408] Time since start: 36473.12s, 	Step: 104307, 	{'train/accuracy': 0.7966358065605164, 'train/loss': 0.990210771560669, 'validation/accuracy': 0.7057600021362305, 'validation/loss': 1.3867374658584595, 'validation/num_examples': 50000, 'test/accuracy': 0.5819000005722046, 'test/loss': 2.023756504058838, 'test/num_examples': 10000, 'score': 35227.30777025223, 'total_duration': 36473.11824512482, 'accumulated_submission_time': 35227.30777025223, 'accumulated_eval_time': 1239.6494569778442, 'accumulated_logging_time': 2.4930479526519775}
I0201 01:06:53.380329 139907729164032 logging_writer.py:48] [104307] accumulated_eval_time=1239.649457, accumulated_logging_time=2.493048, accumulated_submission_time=35227.307770, global_step=104307, preemption_count=0, score=35227.307770, test/accuracy=0.581900, test/loss=2.023757, test/num_examples=10000, total_duration=36473.118245, train/accuracy=0.796636, train/loss=0.990211, validation/accuracy=0.705760, validation/loss=1.386737, validation/num_examples=50000
I0201 01:07:25.085417 139908425447168 logging_writer.py:48] [104400] global_step=104400, grad_norm=2.6946637630462646, loss=2.8829760551452637
I0201 01:07:58.776517 139907729164032 logging_writer.py:48] [104500] global_step=104500, grad_norm=2.8272101879119873, loss=2.916761875152588
I0201 01:08:32.456676 139908425447168 logging_writer.py:48] [104600] global_step=104600, grad_norm=2.5928707122802734, loss=2.908514976501465
I0201 01:09:06.243301 139907729164032 logging_writer.py:48] [104700] global_step=104700, grad_norm=2.7013771533966064, loss=2.880903720855713
I0201 01:09:40.020565 139908425447168 logging_writer.py:48] [104800] global_step=104800, grad_norm=2.7018375396728516, loss=2.930504560470581
I0201 01:10:13.822865 139907729164032 logging_writer.py:48] [104900] global_step=104900, grad_norm=2.8053367137908936, loss=2.9516799449920654
I0201 01:10:47.582398 139908425447168 logging_writer.py:48] [105000] global_step=105000, grad_norm=2.89530611038208, loss=2.9377589225769043
I0201 01:11:21.382136 139907729164032 logging_writer.py:48] [105100] global_step=105100, grad_norm=2.7904016971588135, loss=2.9276123046875
I0201 01:11:55.142548 139908425447168 logging_writer.py:48] [105200] global_step=105200, grad_norm=2.648252010345459, loss=2.947568893432617
I0201 01:12:28.857977 139907729164032 logging_writer.py:48] [105300] global_step=105300, grad_norm=2.957798957824707, loss=3.010277509689331
I0201 01:13:02.619846 139908425447168 logging_writer.py:48] [105400] global_step=105400, grad_norm=2.603599786758423, loss=2.945544719696045
I0201 01:13:36.342601 139907729164032 logging_writer.py:48] [105500] global_step=105500, grad_norm=2.6380116939544678, loss=2.9309866428375244
I0201 01:14:10.109981 139908425447168 logging_writer.py:48] [105600] global_step=105600, grad_norm=2.6676833629608154, loss=2.888340711593628
I0201 01:14:43.817672 139907729164032 logging_writer.py:48] [105700] global_step=105700, grad_norm=2.679614782333374, loss=2.926974296569824
I0201 01:15:17.561594 139908425447168 logging_writer.py:48] [105800] global_step=105800, grad_norm=2.8685238361358643, loss=2.8960094451904297
I0201 01:15:23.447427 140070692116288 spec.py:321] Evaluating on the training split.
I0201 01:15:29.685927 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 01:15:38.376734 140070692116288 spec.py:349] Evaluating on the test split.
I0201 01:15:40.799685 140070692116288 submission_runner.py:408] Time since start: 37000.57s, 	Step: 105819, 	{'train/accuracy': 0.809968888759613, 'train/loss': 0.9250097274780273, 'validation/accuracy': 0.7107399702072144, 'validation/loss': 1.3485785722732544, 'validation/num_examples': 50000, 'test/accuracy': 0.5885000228881836, 'test/loss': 2.001072645187378, 'test/num_examples': 10000, 'score': 35737.31253170967, 'total_duration': 37000.57236742973, 'accumulated_submission_time': 35737.31253170967, 'accumulated_eval_time': 1257.0016777515411, 'accumulated_logging_time': 2.5371103286743164}
I0201 01:15:40.834946 139907762734848 logging_writer.py:48] [105819] accumulated_eval_time=1257.001678, accumulated_logging_time=2.537110, accumulated_submission_time=35737.312532, global_step=105819, preemption_count=0, score=35737.312532, test/accuracy=0.588500, test/loss=2.001073, test/num_examples=10000, total_duration=37000.572367, train/accuracy=0.809969, train/loss=0.925010, validation/accuracy=0.710740, validation/loss=1.348579, validation/num_examples=50000
I0201 01:16:08.498228 139908719027968 logging_writer.py:48] [105900] global_step=105900, grad_norm=2.8286259174346924, loss=2.857799768447876
I0201 01:16:42.285144 139907762734848 logging_writer.py:48] [106000] global_step=106000, grad_norm=2.686922073364258, loss=2.9350650310516357
I0201 01:17:16.027910 139908719027968 logging_writer.py:48] [106100] global_step=106100, grad_norm=2.6770355701446533, loss=2.857480525970459
I0201 01:17:49.728893 139907762734848 logging_writer.py:48] [106200] global_step=106200, grad_norm=2.6757898330688477, loss=2.8610973358154297
I0201 01:18:23.461189 139908719027968 logging_writer.py:48] [106300] global_step=106300, grad_norm=2.6771063804626465, loss=2.8695030212402344
I0201 01:18:57.203292 139907762734848 logging_writer.py:48] [106400] global_step=106400, grad_norm=2.5419387817382812, loss=2.8959662914276123
I0201 01:19:30.895115 139908719027968 logging_writer.py:48] [106500] global_step=106500, grad_norm=2.805102825164795, loss=2.9207115173339844
I0201 01:20:04.606756 139907762734848 logging_writer.py:48] [106600] global_step=106600, grad_norm=2.704859972000122, loss=2.874535322189331
I0201 01:20:38.339279 139908719027968 logging_writer.py:48] [106700] global_step=106700, grad_norm=2.773231267929077, loss=2.8787074089050293
I0201 01:21:12.054518 139907762734848 logging_writer.py:48] [106800] global_step=106800, grad_norm=2.741135597229004, loss=2.9058640003204346
I0201 01:21:45.790510 139908719027968 logging_writer.py:48] [106900] global_step=106900, grad_norm=2.5759775638580322, loss=2.873627185821533
I0201 01:22:19.493016 139907762734848 logging_writer.py:48] [107000] global_step=107000, grad_norm=2.6735057830810547, loss=2.8229308128356934
I0201 01:22:53.431075 139908719027968 logging_writer.py:48] [107100] global_step=107100, grad_norm=2.8327736854553223, loss=2.9389028549194336
I0201 01:23:27.179323 139907762734848 logging_writer.py:48] [107200] global_step=107200, grad_norm=2.666231393814087, loss=2.9754996299743652
I0201 01:24:00.925544 139908719027968 logging_writer.py:48] [107300] global_step=107300, grad_norm=2.863576889038086, loss=2.9204535484313965
I0201 01:24:10.853542 140070692116288 spec.py:321] Evaluating on the training split.
I0201 01:24:17.035878 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 01:24:26.017741 140070692116288 spec.py:349] Evaluating on the test split.
I0201 01:24:28.535050 140070692116288 submission_runner.py:408] Time since start: 37528.31s, 	Step: 107331, 	{'train/accuracy': 0.8362364172935486, 'train/loss': 0.838945746421814, 'validation/accuracy': 0.7157599925994873, 'validation/loss': 1.3426353931427002, 'validation/num_examples': 50000, 'test/accuracy': 0.5887000560760498, 'test/loss': 1.985122561454773, 'test/num_examples': 10000, 'score': 36247.26858711243, 'total_duration': 37528.30771255493, 'accumulated_submission_time': 36247.26858711243, 'accumulated_eval_time': 1274.6831283569336, 'accumulated_logging_time': 2.5816597938537598}
I0201 01:24:28.569747 139907737556736 logging_writer.py:48] [107331] accumulated_eval_time=1274.683128, accumulated_logging_time=2.581660, accumulated_submission_time=36247.268587, global_step=107331, preemption_count=0, score=36247.268587, test/accuracy=0.588700, test/loss=1.985123, test/num_examples=10000, total_duration=37528.307713, train/accuracy=0.836236, train/loss=0.838946, validation/accuracy=0.715760, validation/loss=1.342635, validation/num_examples=50000
I0201 01:24:52.154071 139907745949440 logging_writer.py:48] [107400] global_step=107400, grad_norm=2.6245758533477783, loss=2.8845295906066895
I0201 01:25:25.858474 139907737556736 logging_writer.py:48] [107500] global_step=107500, grad_norm=2.548816442489624, loss=2.894270658493042
I0201 01:25:59.568926 139907745949440 logging_writer.py:48] [107600] global_step=107600, grad_norm=2.741880178451538, loss=2.8678948879241943
I0201 01:26:33.326043 139907737556736 logging_writer.py:48] [107700] global_step=107700, grad_norm=2.6177196502685547, loss=2.923705577850342
I0201 01:27:07.045768 139907745949440 logging_writer.py:48] [107800] global_step=107800, grad_norm=2.8214333057403564, loss=2.8973782062530518
I0201 01:27:40.800490 139907737556736 logging_writer.py:48] [107900] global_step=107900, grad_norm=3.2719573974609375, loss=2.929612159729004
I0201 01:28:14.526254 139907745949440 logging_writer.py:48] [108000] global_step=108000, grad_norm=2.6892096996307373, loss=2.932683229446411
I0201 01:28:48.392080 139907737556736 logging_writer.py:48] [108100] global_step=108100, grad_norm=2.7630879878997803, loss=2.9281833171844482
I0201 01:29:22.155029 139907745949440 logging_writer.py:48] [108200] global_step=108200, grad_norm=2.613579273223877, loss=2.89052677154541
I0201 01:29:55.817119 139907737556736 logging_writer.py:48] [108300] global_step=108300, grad_norm=2.757172107696533, loss=2.9491569995880127
I0201 01:30:29.570693 139907745949440 logging_writer.py:48] [108400] global_step=108400, grad_norm=2.8264613151550293, loss=2.9457151889801025
I0201 01:31:03.321076 139907737556736 logging_writer.py:48] [108500] global_step=108500, grad_norm=2.5464377403259277, loss=2.8052985668182373
I0201 01:31:37.071685 139907745949440 logging_writer.py:48] [108600] global_step=108600, grad_norm=2.83943772315979, loss=2.970642328262329
I0201 01:32:10.725316 139907737556736 logging_writer.py:48] [108700] global_step=108700, grad_norm=2.7144224643707275, loss=2.9107255935668945
I0201 01:32:44.436547 139907745949440 logging_writer.py:48] [108800] global_step=108800, grad_norm=2.7569193840026855, loss=2.954824686050415
I0201 01:32:58.723650 140070692116288 spec.py:321] Evaluating on the training split.
I0201 01:33:05.240378 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 01:33:13.941077 140070692116288 spec.py:349] Evaluating on the test split.
I0201 01:33:16.417379 140070692116288 submission_runner.py:408] Time since start: 38056.19s, 	Step: 108844, 	{'train/accuracy': 0.8233019709587097, 'train/loss': 0.8994101285934448, 'validation/accuracy': 0.7140600085258484, 'validation/loss': 1.3612909317016602, 'validation/num_examples': 50000, 'test/accuracy': 0.5867000222206116, 'test/loss': 2.025434732437134, 'test/num_examples': 10000, 'score': 36757.356506347656, 'total_duration': 38056.19006371498, 'accumulated_submission_time': 36757.356506347656, 'accumulated_eval_time': 1292.3768393993378, 'accumulated_logging_time': 2.6290013790130615}
I0201 01:33:16.452327 139907729164032 logging_writer.py:48] [108844] accumulated_eval_time=1292.376839, accumulated_logging_time=2.629001, accumulated_submission_time=36757.356506, global_step=108844, preemption_count=0, score=36757.356506, test/accuracy=0.586700, test/loss=2.025435, test/num_examples=10000, total_duration=38056.190064, train/accuracy=0.823302, train/loss=0.899410, validation/accuracy=0.714060, validation/loss=1.361291, validation/num_examples=50000
I0201 01:33:35.695447 139907737556736 logging_writer.py:48] [108900] global_step=108900, grad_norm=2.5308268070220947, loss=2.846755266189575
I0201 01:34:09.387655 139907729164032 logging_writer.py:48] [109000] global_step=109000, grad_norm=2.8443233966827393, loss=2.927361488342285
I0201 01:34:43.085821 139907737556736 logging_writer.py:48] [109100] global_step=109100, grad_norm=2.925968885421753, loss=2.889955997467041
I0201 01:35:16.879325 139907729164032 logging_writer.py:48] [109200] global_step=109200, grad_norm=2.861644744873047, loss=2.8258843421936035
I0201 01:35:50.592295 139907737556736 logging_writer.py:48] [109300] global_step=109300, grad_norm=2.954928398132324, loss=2.923922538757324
I0201 01:36:24.257355 139907729164032 logging_writer.py:48] [109400] global_step=109400, grad_norm=2.791452407836914, loss=2.8823459148406982
I0201 01:36:58.007273 139907737556736 logging_writer.py:48] [109500] global_step=109500, grad_norm=3.231560707092285, loss=2.9205336570739746
I0201 01:37:31.705141 139907729164032 logging_writer.py:48] [109600] global_step=109600, grad_norm=2.7746622562408447, loss=2.8848812580108643
I0201 01:38:05.428606 139907737556736 logging_writer.py:48] [109700] global_step=109700, grad_norm=2.7561986446380615, loss=2.8532981872558594
I0201 01:38:39.132318 139907729164032 logging_writer.py:48] [109800] global_step=109800, grad_norm=2.8110125064849854, loss=2.8471999168395996
I0201 01:39:12.909776 139907737556736 logging_writer.py:48] [109900] global_step=109900, grad_norm=2.948410749435425, loss=2.897904396057129
I0201 01:39:46.603129 139907729164032 logging_writer.py:48] [110000] global_step=110000, grad_norm=2.7638354301452637, loss=2.834888219833374
I0201 01:40:20.272769 139907737556736 logging_writer.py:48] [110100] global_step=110100, grad_norm=2.9494595527648926, loss=2.8439583778381348
I0201 01:40:54.018817 139907729164032 logging_writer.py:48] [110200] global_step=110200, grad_norm=2.7439451217651367, loss=2.896219491958618
I0201 01:41:27.844828 139907737556736 logging_writer.py:48] [110300] global_step=110300, grad_norm=3.0242762565612793, loss=2.8588905334472656
I0201 01:41:46.519025 140070692116288 spec.py:321] Evaluating on the training split.
I0201 01:41:52.667175 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 01:42:01.447821 140070692116288 spec.py:349] Evaluating on the test split.
I0201 01:42:04.139971 140070692116288 submission_runner.py:408] Time since start: 38583.91s, 	Step: 110357, 	{'train/accuracy': 0.8215481042861938, 'train/loss': 0.8967534899711609, 'validation/accuracy': 0.7145199775695801, 'validation/loss': 1.3505741357803345, 'validation/num_examples': 50000, 'test/accuracy': 0.5910000205039978, 'test/loss': 1.9762768745422363, 'test/num_examples': 10000, 'score': 37267.36195087433, 'total_duration': 38583.91264152527, 'accumulated_submission_time': 37267.36195087433, 'accumulated_eval_time': 1309.9977324008942, 'accumulated_logging_time': 2.6728272438049316}
I0201 01:42:04.186290 139907737556736 logging_writer.py:48] [110357] accumulated_eval_time=1309.997732, accumulated_logging_time=2.672827, accumulated_submission_time=37267.361951, global_step=110357, preemption_count=0, score=37267.361951, test/accuracy=0.591000, test/loss=1.976277, test/num_examples=10000, total_duration=38583.912642, train/accuracy=0.821548, train/loss=0.896753, validation/accuracy=0.714520, validation/loss=1.350574, validation/num_examples=50000
I0201 01:42:19.067549 139908710635264 logging_writer.py:48] [110400] global_step=110400, grad_norm=2.940554618835449, loss=2.9739062786102295
I0201 01:42:52.788668 139907737556736 logging_writer.py:48] [110500] global_step=110500, grad_norm=2.898284673690796, loss=2.8804702758789062
I0201 01:43:26.472634 139908710635264 logging_writer.py:48] [110600] global_step=110600, grad_norm=2.6676013469696045, loss=2.874074935913086
I0201 01:44:00.222015 139907737556736 logging_writer.py:48] [110700] global_step=110700, grad_norm=2.6786506175994873, loss=2.88411021232605
I0201 01:44:33.917780 139908710635264 logging_writer.py:48] [110800] global_step=110800, grad_norm=2.9868829250335693, loss=2.91268253326416
I0201 01:45:07.650638 139907737556736 logging_writer.py:48] [110900] global_step=110900, grad_norm=2.847071886062622, loss=2.916308879852295
I0201 01:45:41.384098 139908710635264 logging_writer.py:48] [111000] global_step=111000, grad_norm=2.7426517009735107, loss=2.8875224590301514
I0201 01:46:15.118571 139907737556736 logging_writer.py:48] [111100] global_step=111100, grad_norm=2.8107411861419678, loss=2.9379868507385254
I0201 01:46:48.860840 139908710635264 logging_writer.py:48] [111200] global_step=111200, grad_norm=2.6425013542175293, loss=2.7811262607574463
I0201 01:47:22.661642 139907737556736 logging_writer.py:48] [111300] global_step=111300, grad_norm=2.767474412918091, loss=2.874830484390259
I0201 01:47:56.370923 139908710635264 logging_writer.py:48] [111400] global_step=111400, grad_norm=2.8682703971862793, loss=2.8415021896362305
I0201 01:48:30.078397 139907737556736 logging_writer.py:48] [111500] global_step=111500, grad_norm=2.8498783111572266, loss=2.8959479331970215
I0201 01:49:03.832563 139908710635264 logging_writer.py:48] [111600] global_step=111600, grad_norm=2.7958176136016846, loss=2.859025478363037
I0201 01:49:37.563652 139907737556736 logging_writer.py:48] [111700] global_step=111700, grad_norm=2.906834840774536, loss=2.884631395339966
I0201 01:50:11.304492 139908710635264 logging_writer.py:48] [111800] global_step=111800, grad_norm=2.890714406967163, loss=2.8838343620300293
I0201 01:50:34.383573 140070692116288 spec.py:321] Evaluating on the training split.
I0201 01:50:40.541115 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 01:50:49.151974 140070692116288 spec.py:349] Evaluating on the test split.
I0201 01:50:51.666067 140070692116288 submission_runner.py:408] Time since start: 39111.44s, 	Step: 111870, 	{'train/accuracy': 0.8194156289100647, 'train/loss': 0.8783233761787415, 'validation/accuracy': 0.7137799859046936, 'validation/loss': 1.3280168771743774, 'validation/num_examples': 50000, 'test/accuracy': 0.5868000388145447, 'test/loss': 1.9892569780349731, 'test/num_examples': 10000, 'score': 37777.49592471123, 'total_duration': 39111.43874955177, 'accumulated_submission_time': 37777.49592471123, 'accumulated_eval_time': 1327.2801899909973, 'accumulated_logging_time': 2.7286429405212402}
I0201 01:50:51.705020 139907754342144 logging_writer.py:48] [111870] accumulated_eval_time=1327.280190, accumulated_logging_time=2.728643, accumulated_submission_time=37777.495925, global_step=111870, preemption_count=0, score=37777.495925, test/accuracy=0.586800, test/loss=1.989257, test/num_examples=10000, total_duration=39111.438750, train/accuracy=0.819416, train/loss=0.878323, validation/accuracy=0.713780, validation/loss=1.328017, validation/num_examples=50000
I0201 01:51:02.200976 139907762734848 logging_writer.py:48] [111900] global_step=111900, grad_norm=2.8730416297912598, loss=2.8711843490600586
I0201 01:51:35.898225 139907754342144 logging_writer.py:48] [112000] global_step=112000, grad_norm=2.8247528076171875, loss=2.8062233924865723
I0201 01:52:09.574460 139907762734848 logging_writer.py:48] [112100] global_step=112100, grad_norm=2.916506767272949, loss=2.8633952140808105
I0201 01:52:43.271785 139907754342144 logging_writer.py:48] [112200] global_step=112200, grad_norm=2.8351948261260986, loss=2.876258373260498
I0201 01:53:16.987663 139907762734848 logging_writer.py:48] [112300] global_step=112300, grad_norm=2.9071877002716064, loss=2.9341211318969727
I0201 01:53:50.747756 139907754342144 logging_writer.py:48] [112400] global_step=112400, grad_norm=2.9187543392181396, loss=2.9325056076049805
I0201 01:54:24.434675 139907762734848 logging_writer.py:48] [112500] global_step=112500, grad_norm=3.0230727195739746, loss=2.8633244037628174
I0201 01:54:58.217875 139907754342144 logging_writer.py:48] [112600] global_step=112600, grad_norm=2.5485329627990723, loss=2.84676456451416
I0201 01:55:31.895121 139907762734848 logging_writer.py:48] [112700] global_step=112700, grad_norm=2.91450834274292, loss=2.841322660446167
I0201 01:56:05.563215 139907754342144 logging_writer.py:48] [112800] global_step=112800, grad_norm=2.881688356399536, loss=2.8348402976989746
I0201 01:56:39.235053 139907762734848 logging_writer.py:48] [112900] global_step=112900, grad_norm=2.935805559158325, loss=2.925607204437256
I0201 01:57:12.986997 139907754342144 logging_writer.py:48] [113000] global_step=113000, grad_norm=2.9036502838134766, loss=2.844895124435425
I0201 01:57:46.678084 139907762734848 logging_writer.py:48] [113100] global_step=113100, grad_norm=2.851426124572754, loss=2.809563159942627
I0201 01:58:20.410100 139907754342144 logging_writer.py:48] [113200] global_step=113200, grad_norm=2.9320881366729736, loss=2.9002957344055176
I0201 01:58:54.155954 139907762734848 logging_writer.py:48] [113300] global_step=113300, grad_norm=2.7644104957580566, loss=2.8274896144866943
I0201 01:59:21.968018 140070692116288 spec.py:321] Evaluating on the training split.
I0201 01:59:28.136086 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 01:59:36.906651 140070692116288 spec.py:349] Evaluating on the test split.
I0201 01:59:39.397740 140070692116288 submission_runner.py:408] Time since start: 39639.17s, 	Step: 113384, 	{'train/accuracy': 0.8175222873687744, 'train/loss': 0.9152930974960327, 'validation/accuracy': 0.7148399949073792, 'validation/loss': 1.350134015083313, 'validation/num_examples': 50000, 'test/accuracy': 0.5896000266075134, 'test/loss': 1.9782979488372803, 'test/num_examples': 10000, 'score': 38287.695055007935, 'total_duration': 39639.17041897774, 'accumulated_submission_time': 38287.695055007935, 'accumulated_eval_time': 1344.7098760604858, 'accumulated_logging_time': 2.7765440940856934}
I0201 01:59:39.436096 139908710635264 logging_writer.py:48] [113384] accumulated_eval_time=1344.709876, accumulated_logging_time=2.776544, accumulated_submission_time=38287.695055, global_step=113384, preemption_count=0, score=38287.695055, test/accuracy=0.589600, test/loss=1.978298, test/num_examples=10000, total_duration=39639.170419, train/accuracy=0.817522, train/loss=0.915293, validation/accuracy=0.714840, validation/loss=1.350134, validation/num_examples=50000
I0201 01:59:45.187780 139908719027968 logging_writer.py:48] [113400] global_step=113400, grad_norm=2.742351770401001, loss=2.8284127712249756
I0201 02:00:18.979874 139908710635264 logging_writer.py:48] [113500] global_step=113500, grad_norm=2.8969852924346924, loss=2.8990464210510254
I0201 02:00:52.689373 139908719027968 logging_writer.py:48] [113600] global_step=113600, grad_norm=2.9621031284332275, loss=2.86550235748291
I0201 02:01:26.391520 139908710635264 logging_writer.py:48] [113700] global_step=113700, grad_norm=2.807363748550415, loss=2.8534274101257324
I0201 02:02:00.138804 139908719027968 logging_writer.py:48] [113800] global_step=113800, grad_norm=2.914398670196533, loss=2.8970866203308105
I0201 02:02:33.832759 139908710635264 logging_writer.py:48] [113900] global_step=113900, grad_norm=2.9239721298217773, loss=2.8868231773376465
I0201 02:03:07.564088 139908719027968 logging_writer.py:48] [114000] global_step=114000, grad_norm=2.8630146980285645, loss=2.889756441116333
I0201 02:03:41.329979 139908710635264 logging_writer.py:48] [114100] global_step=114100, grad_norm=2.6548216342926025, loss=2.828352928161621
I0201 02:04:15.080359 139908719027968 logging_writer.py:48] [114200] global_step=114200, grad_norm=2.9265520572662354, loss=2.8295071125030518
I0201 02:04:48.821121 139908710635264 logging_writer.py:48] [114300] global_step=114300, grad_norm=2.9813129901885986, loss=2.811744213104248
I0201 02:05:22.501422 139908719027968 logging_writer.py:48] [114400] global_step=114400, grad_norm=2.8239784240722656, loss=2.8498682975769043
I0201 02:05:56.221982 139908710635264 logging_writer.py:48] [114500] global_step=114500, grad_norm=2.666999101638794, loss=2.8543622493743896
I0201 02:06:30.013689 139908719027968 logging_writer.py:48] [114600] global_step=114600, grad_norm=2.9031262397766113, loss=2.878347396850586
I0201 02:07:03.748027 139908710635264 logging_writer.py:48] [114700] global_step=114700, grad_norm=2.6707680225372314, loss=2.849630355834961
I0201 02:07:37.462607 139908719027968 logging_writer.py:48] [114800] global_step=114800, grad_norm=2.79005765914917, loss=2.8517260551452637
I0201 02:08:09.686434 140070692116288 spec.py:321] Evaluating on the training split.
I0201 02:08:15.910292 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 02:08:24.836377 140070692116288 spec.py:349] Evaluating on the test split.
I0201 02:08:27.358743 140070692116288 submission_runner.py:408] Time since start: 40167.13s, 	Step: 114897, 	{'train/accuracy': 0.8275271058082581, 'train/loss': 0.9153132438659668, 'validation/accuracy': 0.7184000015258789, 'validation/loss': 1.3692786693572998, 'validation/num_examples': 50000, 'test/accuracy': 0.5919000506401062, 'test/loss': 2.0019583702087402, 'test/num_examples': 10000, 'score': 38797.88224673271, 'total_duration': 40167.13142871857, 'accumulated_submission_time': 38797.88224673271, 'accumulated_eval_time': 1362.3821530342102, 'accumulated_logging_time': 2.825171709060669}
I0201 02:08:27.398965 139907754342144 logging_writer.py:48] [114897] accumulated_eval_time=1362.382153, accumulated_logging_time=2.825172, accumulated_submission_time=38797.882247, global_step=114897, preemption_count=0, score=38797.882247, test/accuracy=0.591900, test/loss=2.001958, test/num_examples=10000, total_duration=40167.131429, train/accuracy=0.827527, train/loss=0.915313, validation/accuracy=0.718400, validation/loss=1.369279, validation/num_examples=50000
I0201 02:08:28.765010 139907762734848 logging_writer.py:48] [114900] global_step=114900, grad_norm=2.900561809539795, loss=2.8850574493408203
I0201 02:09:02.488315 139907754342144 logging_writer.py:48] [115000] global_step=115000, grad_norm=2.9619269371032715, loss=2.8313448429107666
I0201 02:09:36.210054 139907762734848 logging_writer.py:48] [115100] global_step=115100, grad_norm=2.8665740489959717, loss=2.8440117835998535
I0201 02:10:09.873570 139907754342144 logging_writer.py:48] [115200] global_step=115200, grad_norm=3.0368905067443848, loss=2.8282549381256104
I0201 02:10:43.608084 139907762734848 logging_writer.py:48] [115300] global_step=115300, grad_norm=2.8228843212127686, loss=2.8677523136138916
I0201 02:11:17.318215 139907754342144 logging_writer.py:48] [115400] global_step=115400, grad_norm=2.834770917892456, loss=2.8342952728271484
I0201 02:11:51.095968 139907762734848 logging_writer.py:48] [115500] global_step=115500, grad_norm=2.963456869125366, loss=2.9014780521392822
I0201 02:12:24.899223 139907754342144 logging_writer.py:48] [115600] global_step=115600, grad_norm=2.952258825302124, loss=2.815209150314331
I0201 02:12:58.688175 139907762734848 logging_writer.py:48] [115700] global_step=115700, grad_norm=3.0601582527160645, loss=2.9120163917541504
I0201 02:13:32.386260 139907754342144 logging_writer.py:48] [115800] global_step=115800, grad_norm=3.118657112121582, loss=2.8075482845306396
I0201 02:14:06.169259 139907762734848 logging_writer.py:48] [115900] global_step=115900, grad_norm=2.805633544921875, loss=2.748932123184204
I0201 02:14:39.886520 139907754342144 logging_writer.py:48] [116000] global_step=116000, grad_norm=2.6955618858337402, loss=2.8078770637512207
I0201 02:15:13.655813 139907762734848 logging_writer.py:48] [116100] global_step=116100, grad_norm=2.839258909225464, loss=2.8660049438476562
I0201 02:15:47.415687 139907754342144 logging_writer.py:48] [116200] global_step=116200, grad_norm=2.9668548107147217, loss=2.892087697982788
I0201 02:16:21.118165 139907762734848 logging_writer.py:48] [116300] global_step=116300, grad_norm=2.9238815307617188, loss=2.837130308151245
I0201 02:16:54.895263 139907754342144 logging_writer.py:48] [116400] global_step=116400, grad_norm=3.0422236919403076, loss=2.832958698272705
I0201 02:16:57.417984 140070692116288 spec.py:321] Evaluating on the training split.
I0201 02:17:03.794827 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 02:17:12.516210 140070692116288 spec.py:349] Evaluating on the test split.
I0201 02:17:15.024693 140070692116288 submission_runner.py:408] Time since start: 40694.80s, 	Step: 116409, 	{'train/accuracy': 0.848074734210968, 'train/loss': 0.8165138363838196, 'validation/accuracy': 0.7175999879837036, 'validation/loss': 1.3547149896621704, 'validation/num_examples': 50000, 'test/accuracy': 0.588200032711029, 'test/loss': 2.00327205657959, 'test/num_examples': 10000, 'score': 39307.83862376213, 'total_duration': 40694.7973818779, 'accumulated_submission_time': 39307.83862376213, 'accumulated_eval_time': 1379.9888272285461, 'accumulated_logging_time': 2.8746609687805176}
I0201 02:17:15.062438 139908425447168 logging_writer.py:48] [116409] accumulated_eval_time=1379.988827, accumulated_logging_time=2.874661, accumulated_submission_time=39307.838624, global_step=116409, preemption_count=0, score=39307.838624, test/accuracy=0.588200, test/loss=2.003272, test/num_examples=10000, total_duration=40694.797382, train/accuracy=0.848075, train/loss=0.816514, validation/accuracy=0.717600, validation/loss=1.354715, validation/num_examples=50000
I0201 02:17:46.039233 139908710635264 logging_writer.py:48] [116500] global_step=116500, grad_norm=2.753507375717163, loss=2.7927417755126953
I0201 02:18:19.795364 139908425447168 logging_writer.py:48] [116600] global_step=116600, grad_norm=2.838960647583008, loss=2.8092525005340576
I0201 02:18:53.598956 139908710635264 logging_writer.py:48] [116700] global_step=116700, grad_norm=2.9147660732269287, loss=2.852984666824341
I0201 02:19:27.322188 139908425447168 logging_writer.py:48] [116800] global_step=116800, grad_norm=3.1105759143829346, loss=2.812997579574585
I0201 02:20:00.994652 139908710635264 logging_writer.py:48] [116900] global_step=116900, grad_norm=3.0238258838653564, loss=2.8057971000671387
I0201 02:20:34.779711 139908425447168 logging_writer.py:48] [117000] global_step=117000, grad_norm=2.980211019515991, loss=2.853167772293091
I0201 02:21:08.495371 139908710635264 logging_writer.py:48] [117100] global_step=117100, grad_norm=2.778376579284668, loss=2.814502477645874
I0201 02:21:42.268858 139908425447168 logging_writer.py:48] [117200] global_step=117200, grad_norm=2.9131524562835693, loss=2.8747987747192383
I0201 02:22:16.012017 139908710635264 logging_writer.py:48] [117300] global_step=117300, grad_norm=3.1435599327087402, loss=2.8664987087249756
I0201 02:22:49.749799 139908425447168 logging_writer.py:48] [117400] global_step=117400, grad_norm=2.7467246055603027, loss=2.7907845973968506
I0201 02:23:23.467956 139908710635264 logging_writer.py:48] [117500] global_step=117500, grad_norm=2.9754860401153564, loss=2.832651138305664
I0201 02:23:57.193853 139908425447168 logging_writer.py:48] [117600] global_step=117600, grad_norm=2.895801067352295, loss=2.884047031402588
I0201 02:24:30.920278 139908710635264 logging_writer.py:48] [117700] global_step=117700, grad_norm=3.01339054107666, loss=2.8525028228759766
I0201 02:25:04.760742 139908425447168 logging_writer.py:48] [117800] global_step=117800, grad_norm=3.102069139480591, loss=2.874063491821289
I0201 02:25:38.470218 139908710635264 logging_writer.py:48] [117900] global_step=117900, grad_norm=2.971513032913208, loss=2.804006576538086
I0201 02:25:45.362550 140070692116288 spec.py:321] Evaluating on the training split.
I0201 02:25:52.254135 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 02:26:01.079678 140070692116288 spec.py:349] Evaluating on the test split.
I0201 02:26:03.623070 140070692116288 submission_runner.py:408] Time since start: 41223.40s, 	Step: 117922, 	{'train/accuracy': 0.8465999364852905, 'train/loss': 0.8292368650436401, 'validation/accuracy': 0.7242199778556824, 'validation/loss': 1.3349623680114746, 'validation/num_examples': 50000, 'test/accuracy': 0.5986000299453735, 'test/loss': 1.9471795558929443, 'test/num_examples': 10000, 'score': 39818.07694029808, 'total_duration': 41223.39574432373, 'accumulated_submission_time': 39818.07694029808, 'accumulated_eval_time': 1398.249297618866, 'accumulated_logging_time': 2.9214396476745605}
I0201 02:26:03.665334 139907754342144 logging_writer.py:48] [117922] accumulated_eval_time=1398.249298, accumulated_logging_time=2.921440, accumulated_submission_time=39818.076940, global_step=117922, preemption_count=0, score=39818.076940, test/accuracy=0.598600, test/loss=1.947180, test/num_examples=10000, total_duration=41223.395744, train/accuracy=0.846600, train/loss=0.829237, validation/accuracy=0.724220, validation/loss=1.334962, validation/num_examples=50000
I0201 02:26:30.269865 139907762734848 logging_writer.py:48] [118000] global_step=118000, grad_norm=2.9321985244750977, loss=2.8298916816711426
I0201 02:27:03.916238 139907754342144 logging_writer.py:48] [118100] global_step=118100, grad_norm=2.80338454246521, loss=2.8409841060638428
I0201 02:27:37.620120 139907762734848 logging_writer.py:48] [118200] global_step=118200, grad_norm=2.9327356815338135, loss=2.850433588027954
I0201 02:28:11.290215 139907754342144 logging_writer.py:48] [118300] global_step=118300, grad_norm=3.1078038215637207, loss=2.879483699798584
I0201 02:28:44.992078 139907762734848 logging_writer.py:48] [118400] global_step=118400, grad_norm=2.880356550216675, loss=2.766144275665283
I0201 02:29:18.706782 139907754342144 logging_writer.py:48] [118500] global_step=118500, grad_norm=2.967364549636841, loss=2.8416428565979004
I0201 02:29:52.475306 139907762734848 logging_writer.py:48] [118600] global_step=118600, grad_norm=3.046097993850708, loss=2.823452949523926
I0201 02:30:26.232761 139907754342144 logging_writer.py:48] [118700] global_step=118700, grad_norm=2.9418938159942627, loss=2.8544931411743164
I0201 02:30:59.959841 139907762734848 logging_writer.py:48] [118800] global_step=118800, grad_norm=3.1766161918640137, loss=2.827151298522949
I0201 02:31:33.791428 139907754342144 logging_writer.py:48] [118900] global_step=118900, grad_norm=2.986997127532959, loss=2.8030436038970947
I0201 02:32:07.498214 139907762734848 logging_writer.py:48] [119000] global_step=119000, grad_norm=3.014815330505371, loss=2.827134609222412
I0201 02:32:41.264824 139907754342144 logging_writer.py:48] [119100] global_step=119100, grad_norm=2.8923397064208984, loss=2.8420095443725586
I0201 02:33:14.961227 139907762734848 logging_writer.py:48] [119200] global_step=119200, grad_norm=2.9558005332946777, loss=2.8593215942382812
I0201 02:33:48.607314 139907754342144 logging_writer.py:48] [119300] global_step=119300, grad_norm=2.907708168029785, loss=2.8263378143310547
I0201 02:34:22.302518 139907762734848 logging_writer.py:48] [119400] global_step=119400, grad_norm=2.926140785217285, loss=2.8009727001190186
I0201 02:34:33.905423 140070692116288 spec.py:321] Evaluating on the training split.
I0201 02:34:40.106954 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 02:34:48.833158 140070692116288 spec.py:349] Evaluating on the test split.
I0201 02:34:51.299052 140070692116288 submission_runner.py:408] Time since start: 41751.07s, 	Step: 119436, 	{'train/accuracy': 0.8353993892669678, 'train/loss': 0.8475539088249207, 'validation/accuracy': 0.724399983882904, 'validation/loss': 1.3222719430923462, 'validation/num_examples': 50000, 'test/accuracy': 0.5958999991416931, 'test/loss': 1.961624264717102, 'test/num_examples': 10000, 'score': 40328.25413155556, 'total_duration': 41751.07174015045, 'accumulated_submission_time': 40328.25413155556, 'accumulated_eval_time': 1415.6428937911987, 'accumulated_logging_time': 2.9733994007110596}
I0201 02:34:51.336477 139907737556736 logging_writer.py:48] [119436] accumulated_eval_time=1415.642894, accumulated_logging_time=2.973399, accumulated_submission_time=40328.254132, global_step=119436, preemption_count=0, score=40328.254132, test/accuracy=0.595900, test/loss=1.961624, test/num_examples=10000, total_duration=41751.071740, train/accuracy=0.835399, train/loss=0.847554, validation/accuracy=0.724400, validation/loss=1.322272, validation/num_examples=50000
I0201 02:35:13.225398 139908710635264 logging_writer.py:48] [119500] global_step=119500, grad_norm=2.8439955711364746, loss=2.8442187309265137
I0201 02:35:46.870651 139907737556736 logging_writer.py:48] [119600] global_step=119600, grad_norm=3.2785801887512207, loss=2.91754150390625
I0201 02:36:20.641009 139908710635264 logging_writer.py:48] [119700] global_step=119700, grad_norm=3.0225331783294678, loss=2.8314208984375
I0201 02:36:54.346796 139907737556736 logging_writer.py:48] [119800] global_step=119800, grad_norm=2.997626304626465, loss=2.8368325233459473
I0201 02:37:28.157710 139908710635264 logging_writer.py:48] [119900] global_step=119900, grad_norm=2.9406418800354004, loss=2.7948336601257324
I0201 02:38:01.908533 139907737556736 logging_writer.py:48] [120000] global_step=120000, grad_norm=2.957568883895874, loss=2.8455371856689453
I0201 02:38:35.623839 139908710635264 logging_writer.py:48] [120100] global_step=120100, grad_norm=2.9324793815612793, loss=2.821798324584961
I0201 02:39:09.358471 139907737556736 logging_writer.py:48] [120200] global_step=120200, grad_norm=2.9536619186401367, loss=2.84535551071167
I0201 02:39:43.118192 139908710635264 logging_writer.py:48] [120300] global_step=120300, grad_norm=3.040889024734497, loss=2.8732283115386963
I0201 02:40:16.849587 139907737556736 logging_writer.py:48] [120400] global_step=120400, grad_norm=2.7384181022644043, loss=2.7596092224121094
I0201 02:40:50.632629 139908710635264 logging_writer.py:48] [120500] global_step=120500, grad_norm=2.9642140865325928, loss=2.7568511962890625
I0201 02:41:24.359860 139907737556736 logging_writer.py:48] [120600] global_step=120600, grad_norm=2.984121561050415, loss=2.909327983856201
I0201 02:41:58.113155 139908710635264 logging_writer.py:48] [120700] global_step=120700, grad_norm=3.244140625, loss=2.9275639057159424
I0201 02:42:31.861391 139907737556736 logging_writer.py:48] [120800] global_step=120800, grad_norm=2.979018211364746, loss=2.819486618041992
I0201 02:43:05.596016 139908710635264 logging_writer.py:48] [120900] global_step=120900, grad_norm=3.200010061264038, loss=2.830009698867798
I0201 02:43:21.594267 140070692116288 spec.py:321] Evaluating on the training split.
I0201 02:43:28.172477 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 02:43:37.022411 140070692116288 spec.py:349] Evaluating on the test split.
I0201 02:43:39.546663 140070692116288 submission_runner.py:408] Time since start: 42279.32s, 	Step: 120949, 	{'train/accuracy': 0.8393654227256775, 'train/loss': 0.8338555097579956, 'validation/accuracy': 0.7299799919128418, 'validation/loss': 1.3040684461593628, 'validation/num_examples': 50000, 'test/accuracy': 0.6011000275611877, 'test/loss': 1.9599751234054565, 'test/num_examples': 10000, 'score': 40838.449570417404, 'total_duration': 42279.31935048103, 'accumulated_submission_time': 40838.449570417404, 'accumulated_eval_time': 1433.5952577590942, 'accumulated_logging_time': 3.020388603210449}
I0201 02:43:39.585057 139907762734848 logging_writer.py:48] [120949] accumulated_eval_time=1433.595258, accumulated_logging_time=3.020389, accumulated_submission_time=40838.449570, global_step=120949, preemption_count=0, score=40838.449570, test/accuracy=0.601100, test/loss=1.959975, test/num_examples=10000, total_duration=42279.319350, train/accuracy=0.839365, train/loss=0.833856, validation/accuracy=0.729980, validation/loss=1.304068, validation/num_examples=50000
I0201 02:43:57.126763 139908425447168 logging_writer.py:48] [121000] global_step=121000, grad_norm=3.016836404800415, loss=2.8302974700927734
I0201 02:44:30.841715 139907762734848 logging_writer.py:48] [121100] global_step=121100, grad_norm=3.022975206375122, loss=2.749955177307129
I0201 02:45:04.516751 139908425447168 logging_writer.py:48] [121200] global_step=121200, grad_norm=3.093872308731079, loss=2.8436672687530518
I0201 02:45:38.218838 139907762734848 logging_writer.py:48] [121300] global_step=121300, grad_norm=3.2337450981140137, loss=2.7797889709472656
I0201 02:46:11.883442 139908425447168 logging_writer.py:48] [121400] global_step=121400, grad_norm=3.1744444370269775, loss=2.8072662353515625
I0201 02:46:45.589024 139907762734848 logging_writer.py:48] [121500] global_step=121500, grad_norm=2.995171070098877, loss=2.845679759979248
I0201 02:47:19.305060 139908425447168 logging_writer.py:48] [121600] global_step=121600, grad_norm=3.163978099822998, loss=2.850092887878418
I0201 02:47:53.042114 139907762734848 logging_writer.py:48] [121700] global_step=121700, grad_norm=3.170671224594116, loss=2.856055736541748
I0201 02:48:26.787706 139908425447168 logging_writer.py:48] [121800] global_step=121800, grad_norm=3.110272169113159, loss=2.78456449508667
I0201 02:49:00.502916 139907762734848 logging_writer.py:48] [121900] global_step=121900, grad_norm=3.343254566192627, loss=2.9056174755096436
I0201 02:49:34.182502 139908425447168 logging_writer.py:48] [122000] global_step=122000, grad_norm=2.933518171310425, loss=2.7792577743530273
I0201 02:50:08.016628 139907762734848 logging_writer.py:48] [122100] global_step=122100, grad_norm=3.1238884925842285, loss=2.877058982849121
I0201 02:50:41.702212 139908425447168 logging_writer.py:48] [122200] global_step=122200, grad_norm=2.979309558868408, loss=2.7655391693115234
I0201 02:51:15.405831 139907762734848 logging_writer.py:48] [122300] global_step=122300, grad_norm=2.893016815185547, loss=2.8138082027435303
I0201 02:51:49.061582 139908425447168 logging_writer.py:48] [122400] global_step=122400, grad_norm=2.9887614250183105, loss=2.810152530670166
I0201 02:52:09.765692 140070692116288 spec.py:321] Evaluating on the training split.
I0201 02:52:16.052511 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 02:52:24.771831 140070692116288 spec.py:349] Evaluating on the test split.
I0201 02:52:27.284110 140070692116288 submission_runner.py:408] Time since start: 42807.06s, 	Step: 122463, 	{'train/accuracy': 0.8434908986091614, 'train/loss': 0.8159423470497131, 'validation/accuracy': 0.730459988117218, 'validation/loss': 1.2901736497879028, 'validation/num_examples': 50000, 'test/accuracy': 0.6029000282287598, 'test/loss': 1.9311084747314453, 'test/num_examples': 10000, 'score': 41348.564710855484, 'total_duration': 42807.05680012703, 'accumulated_submission_time': 41348.564710855484, 'accumulated_eval_time': 1451.1136507987976, 'accumulated_logging_time': 3.0704591274261475}
I0201 02:52:27.325745 139907729164032 logging_writer.py:48] [122463] accumulated_eval_time=1451.113651, accumulated_logging_time=3.070459, accumulated_submission_time=41348.564711, global_step=122463, preemption_count=0, score=41348.564711, test/accuracy=0.602900, test/loss=1.931108, test/num_examples=10000, total_duration=42807.056800, train/accuracy=0.843491, train/loss=0.815942, validation/accuracy=0.730460, validation/loss=1.290174, validation/num_examples=50000
I0201 02:52:40.173514 139907754342144 logging_writer.py:48] [122500] global_step=122500, grad_norm=3.057563066482544, loss=2.840205669403076
I0201 02:53:13.872734 139907729164032 logging_writer.py:48] [122600] global_step=122600, grad_norm=3.1072068214416504, loss=2.8428895473480225
I0201 02:53:47.533913 139907754342144 logging_writer.py:48] [122700] global_step=122700, grad_norm=2.9811599254608154, loss=2.7234456539154053
I0201 02:54:21.249982 139907729164032 logging_writer.py:48] [122800] global_step=122800, grad_norm=3.229677200317383, loss=2.8136138916015625
I0201 02:54:54.954872 139907754342144 logging_writer.py:48] [122900] global_step=122900, grad_norm=3.030695676803589, loss=2.729698657989502
I0201 02:55:28.715550 139907729164032 logging_writer.py:48] [123000] global_step=123000, grad_norm=3.1657769680023193, loss=2.8150992393493652
I0201 02:56:02.495946 139907754342144 logging_writer.py:48] [123100] global_step=123100, grad_norm=3.127528667449951, loss=2.806915760040283
I0201 02:56:36.215230 139907729164032 logging_writer.py:48] [123200] global_step=123200, grad_norm=3.1629714965820312, loss=2.8596079349517822
I0201 02:57:09.983569 139907754342144 logging_writer.py:48] [123300] global_step=123300, grad_norm=3.221071481704712, loss=2.7703797817230225
I0201 02:57:43.703340 139907729164032 logging_writer.py:48] [123400] global_step=123400, grad_norm=3.138345956802368, loss=2.835031509399414
I0201 02:58:17.446219 139907754342144 logging_writer.py:48] [123500] global_step=123500, grad_norm=3.1600663661956787, loss=2.8499703407287598
I0201 02:58:51.185463 139907729164032 logging_writer.py:48] [123600] global_step=123600, grad_norm=3.0180325508117676, loss=2.794417142868042
I0201 02:59:24.952181 139907754342144 logging_writer.py:48] [123700] global_step=123700, grad_norm=2.9449429512023926, loss=2.764831066131592
I0201 02:59:58.640883 139907729164032 logging_writer.py:48] [123800] global_step=123800, grad_norm=3.1842007637023926, loss=2.7918503284454346
I0201 03:00:32.296972 139907754342144 logging_writer.py:48] [123900] global_step=123900, grad_norm=3.1116867065429688, loss=2.795325756072998
I0201 03:00:57.398348 140070692116288 spec.py:321] Evaluating on the training split.
I0201 03:01:03.744727 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 03:01:12.450403 140070692116288 spec.py:349] Evaluating on the test split.
I0201 03:01:14.957987 140070692116288 submission_runner.py:408] Time since start: 43334.73s, 	Step: 123976, 	{'train/accuracy': 0.843191921710968, 'train/loss': 0.7751460671424866, 'validation/accuracy': 0.7268799543380737, 'validation/loss': 1.2697910070419312, 'validation/num_examples': 50000, 'test/accuracy': 0.604200005531311, 'test/loss': 1.9074426889419556, 'test/num_examples': 10000, 'score': 41858.57477927208, 'total_duration': 43334.73067235947, 'accumulated_submission_time': 41858.57477927208, 'accumulated_eval_time': 1468.6732609272003, 'accumulated_logging_time': 3.1229352951049805}
I0201 03:01:14.995839 139907729164032 logging_writer.py:48] [123976] accumulated_eval_time=1468.673261, accumulated_logging_time=3.122935, accumulated_submission_time=41858.574779, global_step=123976, preemption_count=0, score=41858.574779, test/accuracy=0.604200, test/loss=1.907443, test/num_examples=10000, total_duration=43334.730672, train/accuracy=0.843192, train/loss=0.775146, validation/accuracy=0.726880, validation/loss=1.269791, validation/num_examples=50000
I0201 03:01:23.438389 139907737556736 logging_writer.py:48] [124000] global_step=124000, grad_norm=3.075005531311035, loss=2.7631406784057617
I0201 03:01:57.117332 139907729164032 logging_writer.py:48] [124100] global_step=124100, grad_norm=3.0268585681915283, loss=2.7739083766937256
I0201 03:02:30.991188 139907737556736 logging_writer.py:48] [124200] global_step=124200, grad_norm=2.9401133060455322, loss=2.8035519123077393
I0201 03:03:04.678472 139907729164032 logging_writer.py:48] [124300] global_step=124300, grad_norm=3.2830209732055664, loss=2.8818206787109375
I0201 03:03:38.357734 139907737556736 logging_writer.py:48] [124400] global_step=124400, grad_norm=3.0655012130737305, loss=2.7680273056030273
I0201 03:04:12.039213 139907729164032 logging_writer.py:48] [124500] global_step=124500, grad_norm=3.240903377532959, loss=2.814027786254883
I0201 03:04:45.808082 139907737556736 logging_writer.py:48] [124600] global_step=124600, grad_norm=3.1240170001983643, loss=2.7807135581970215
I0201 03:05:19.506305 139907729164032 logging_writer.py:48] [124700] global_step=124700, grad_norm=3.3461053371429443, loss=2.8217008113861084
I0201 03:05:53.195325 139907737556736 logging_writer.py:48] [124800] global_step=124800, grad_norm=3.0234127044677734, loss=2.8112800121307373
I0201 03:06:26.979284 139907729164032 logging_writer.py:48] [124900] global_step=124900, grad_norm=3.0107195377349854, loss=2.7946627140045166
I0201 03:07:00.669892 139907737556736 logging_writer.py:48] [125000] global_step=125000, grad_norm=2.9625344276428223, loss=2.765859365463257
I0201 03:07:34.433045 139907729164032 logging_writer.py:48] [125100] global_step=125100, grad_norm=3.226656436920166, loss=2.851651430130005
I0201 03:08:08.131901 139907737556736 logging_writer.py:48] [125200] global_step=125200, grad_norm=3.3091742992401123, loss=2.8521337509155273
I0201 03:08:41.974705 139907729164032 logging_writer.py:48] [125300] global_step=125300, grad_norm=3.3114118576049805, loss=2.798008918762207
I0201 03:09:15.695677 139907737556736 logging_writer.py:48] [125400] global_step=125400, grad_norm=3.1027286052703857, loss=2.795733690261841
I0201 03:09:45.174731 140070692116288 spec.py:321] Evaluating on the training split.
I0201 03:09:51.343183 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 03:10:00.070763 140070692116288 spec.py:349] Evaluating on the test split.
I0201 03:10:02.644850 140070692116288 submission_runner.py:408] Time since start: 43862.42s, 	Step: 125489, 	{'train/accuracy': 0.84574294090271, 'train/loss': 0.8140817880630493, 'validation/accuracy': 0.7204599976539612, 'validation/loss': 1.347158670425415, 'validation/num_examples': 50000, 'test/accuracy': 0.5929000377655029, 'test/loss': 2.000288963317871, 'test/num_examples': 10000, 'score': 42368.690266132355, 'total_duration': 43862.41753697395, 'accumulated_submission_time': 42368.690266132355, 'accumulated_eval_time': 1486.1433503627777, 'accumulated_logging_time': 3.1712000370025635}
I0201 03:10:02.687284 139908719027968 logging_writer.py:48] [125489] accumulated_eval_time=1486.143350, accumulated_logging_time=3.171200, accumulated_submission_time=42368.690266, global_step=125489, preemption_count=0, score=42368.690266, test/accuracy=0.592900, test/loss=2.000289, test/num_examples=10000, total_duration=43862.417537, train/accuracy=0.845743, train/loss=0.814082, validation/accuracy=0.720460, validation/loss=1.347159, validation/num_examples=50000
I0201 03:10:06.769990 139908727420672 logging_writer.py:48] [125500] global_step=125500, grad_norm=3.214657783508301, loss=2.863069534301758
I0201 03:10:40.416742 139908719027968 logging_writer.py:48] [125600] global_step=125600, grad_norm=3.004525899887085, loss=2.790499448776245
I0201 03:11:14.124476 139908727420672 logging_writer.py:48] [125700] global_step=125700, grad_norm=3.1375367641448975, loss=2.792937994003296
I0201 03:11:47.784136 139908719027968 logging_writer.py:48] [125800] global_step=125800, grad_norm=3.1611526012420654, loss=2.735196113586426
I0201 03:12:21.533502 139908727420672 logging_writer.py:48] [125900] global_step=125900, grad_norm=3.2765183448791504, loss=2.792898654937744
I0201 03:12:55.243035 139908719027968 logging_writer.py:48] [126000] global_step=126000, grad_norm=3.292182683944702, loss=2.859799861907959
I0201 03:13:29.018924 139908727420672 logging_writer.py:48] [126100] global_step=126100, grad_norm=2.9272398948669434, loss=2.726459264755249
I0201 03:14:02.712870 139908719027968 logging_writer.py:48] [126200] global_step=126200, grad_norm=3.214200258255005, loss=2.7514429092407227
I0201 03:14:36.626885 139908727420672 logging_writer.py:48] [126300] global_step=126300, grad_norm=2.9742395877838135, loss=2.7516002655029297
I0201 03:15:10.326716 139908719027968 logging_writer.py:48] [126400] global_step=126400, grad_norm=3.144568681716919, loss=2.7736656665802
I0201 03:15:44.045331 139908727420672 logging_writer.py:48] [126500] global_step=126500, grad_norm=3.4885823726654053, loss=2.7728068828582764
I0201 03:16:17.772441 139908719027968 logging_writer.py:48] [126600] global_step=126600, grad_norm=2.9334287643432617, loss=2.7021918296813965
I0201 03:16:51.539062 139908727420672 logging_writer.py:48] [126700] global_step=126700, grad_norm=3.158669948577881, loss=2.736995220184326
I0201 03:17:25.274225 139908719027968 logging_writer.py:48] [126800] global_step=126800, grad_norm=3.1190223693847656, loss=2.74541974067688
I0201 03:17:58.993654 139908727420672 logging_writer.py:48] [126900] global_step=126900, grad_norm=3.186511516571045, loss=2.755950689315796
I0201 03:18:32.740808 139908719027968 logging_writer.py:48] [127000] global_step=127000, grad_norm=3.2588610649108887, loss=2.800431251525879
I0201 03:18:32.749278 140070692116288 spec.py:321] Evaluating on the training split.
I0201 03:18:38.978711 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 03:18:47.671234 140070692116288 spec.py:349] Evaluating on the test split.
I0201 03:18:50.198724 140070692116288 submission_runner.py:408] Time since start: 44389.97s, 	Step: 127001, 	{'train/accuracy': 0.851980984210968, 'train/loss': 0.7569774389266968, 'validation/accuracy': 0.7263799905776978, 'validation/loss': 1.2846840620040894, 'validation/num_examples': 50000, 'test/accuracy': 0.6015000343322754, 'test/loss': 1.922147512435913, 'test/num_examples': 10000, 'score': 42878.689056158066, 'total_duration': 44389.9714012146, 'accumulated_submission_time': 42878.689056158066, 'accumulated_eval_time': 1503.5928556919098, 'accumulated_logging_time': 3.2235565185546875}
I0201 03:18:50.237274 139907754342144 logging_writer.py:48] [127001] accumulated_eval_time=1503.592856, accumulated_logging_time=3.223557, accumulated_submission_time=42878.689056, global_step=127001, preemption_count=0, score=42878.689056, test/accuracy=0.601500, test/loss=1.922148, test/num_examples=10000, total_duration=44389.971401, train/accuracy=0.851981, train/loss=0.756977, validation/accuracy=0.726380, validation/loss=1.284684, validation/num_examples=50000
I0201 03:19:23.911989 139907762734848 logging_writer.py:48] [127100] global_step=127100, grad_norm=3.078606128692627, loss=2.772451877593994
I0201 03:19:57.594158 139907754342144 logging_writer.py:48] [127200] global_step=127200, grad_norm=3.3176910877227783, loss=2.7674150466918945
I0201 03:20:31.259383 139907762734848 logging_writer.py:48] [127300] global_step=127300, grad_norm=3.330644130706787, loss=2.890676259994507
I0201 03:21:05.057030 139907754342144 logging_writer.py:48] [127400] global_step=127400, grad_norm=3.142585277557373, loss=2.785419225692749
I0201 03:21:38.768311 139907762734848 logging_writer.py:48] [127500] global_step=127500, grad_norm=3.007260799407959, loss=2.788609266281128
I0201 03:22:12.484493 139907754342144 logging_writer.py:48] [127600] global_step=127600, grad_norm=2.991339683532715, loss=2.769768476486206
I0201 03:22:46.150854 139907762734848 logging_writer.py:48] [127700] global_step=127700, grad_norm=3.151583194732666, loss=2.7809836864471436
I0201 03:23:19.942115 139907754342144 logging_writer.py:48] [127800] global_step=127800, grad_norm=3.4223344326019287, loss=2.820180892944336
I0201 03:23:53.646116 139907762734848 logging_writer.py:48] [127900] global_step=127900, grad_norm=3.125025510787964, loss=2.82195782661438
I0201 03:24:27.417060 139907754342144 logging_writer.py:48] [128000] global_step=128000, grad_norm=3.177400588989258, loss=2.8435211181640625
I0201 03:25:01.090934 139907762734848 logging_writer.py:48] [128100] global_step=128100, grad_norm=3.264876365661621, loss=2.8149003982543945
I0201 03:25:34.860585 139907754342144 logging_writer.py:48] [128200] global_step=128200, grad_norm=3.347811222076416, loss=2.812502145767212
I0201 03:26:08.568745 139907762734848 logging_writer.py:48] [128300] global_step=128300, grad_norm=3.318525791168213, loss=2.8171403408050537
I0201 03:26:42.331527 139907754342144 logging_writer.py:48] [128400] global_step=128400, grad_norm=3.0156192779541016, loss=2.7752082347869873
I0201 03:27:16.118041 139907762734848 logging_writer.py:48] [128500] global_step=128500, grad_norm=3.3144423961639404, loss=2.774165153503418
I0201 03:27:20.310196 140070692116288 spec.py:321] Evaluating on the training split.
I0201 03:27:26.510725 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 03:27:35.395602 140070692116288 spec.py:349] Evaluating on the test split.
I0201 03:27:37.864379 140070692116288 submission_runner.py:408] Time since start: 44917.64s, 	Step: 128514, 	{'train/accuracy': 0.8525390625, 'train/loss': 0.7666031718254089, 'validation/accuracy': 0.7315399646759033, 'validation/loss': 1.2773808240890503, 'validation/num_examples': 50000, 'test/accuracy': 0.6098000407218933, 'test/loss': 1.9216325283050537, 'test/num_examples': 10000, 'score': 43388.700949430466, 'total_duration': 44917.63706827164, 'accumulated_submission_time': 43388.700949430466, 'accumulated_eval_time': 1521.1470046043396, 'accumulated_logging_time': 3.2711191177368164}
I0201 03:27:37.902787 139907729164032 logging_writer.py:48] [128514] accumulated_eval_time=1521.147005, accumulated_logging_time=3.271119, accumulated_submission_time=43388.700949, global_step=128514, preemption_count=0, score=43388.700949, test/accuracy=0.609800, test/loss=1.921633, test/num_examples=10000, total_duration=44917.637068, train/accuracy=0.852539, train/loss=0.766603, validation/accuracy=0.731540, validation/loss=1.277381, validation/num_examples=50000
I0201 03:28:07.252555 139907737556736 logging_writer.py:48] [128600] global_step=128600, grad_norm=3.1963486671447754, loss=2.7295114994049072
I0201 03:28:40.929756 139907729164032 logging_writer.py:48] [128700] global_step=128700, grad_norm=3.141576051712036, loss=2.6767730712890625
I0201 03:29:14.646396 139907737556736 logging_writer.py:48] [128800] global_step=128800, grad_norm=3.277014970779419, loss=2.792680025100708
I0201 03:29:48.345891 139907729164032 logging_writer.py:48] [128900] global_step=128900, grad_norm=3.207026481628418, loss=2.7765562534332275
I0201 03:30:22.108176 139907737556736 logging_writer.py:48] [129000] global_step=129000, grad_norm=3.158393383026123, loss=2.7388498783111572
I0201 03:30:55.838073 139907729164032 logging_writer.py:48] [129100] global_step=129100, grad_norm=3.3761401176452637, loss=2.8231115341186523
I0201 03:31:29.598173 139907737556736 logging_writer.py:48] [129200] global_step=129200, grad_norm=3.6065726280212402, loss=2.7805910110473633
I0201 03:32:03.370999 139907729164032 logging_writer.py:48] [129300] global_step=129300, grad_norm=3.3190600872039795, loss=2.806501865386963
I0201 03:32:37.089631 139907737556736 logging_writer.py:48] [129400] global_step=129400, grad_norm=2.9326388835906982, loss=2.692147970199585
I0201 03:33:10.845750 139907729164032 logging_writer.py:48] [129500] global_step=129500, grad_norm=3.238380193710327, loss=2.7539899349212646
I0201 03:33:44.659258 139907737556736 logging_writer.py:48] [129600] global_step=129600, grad_norm=3.191824436187744, loss=2.7019429206848145
I0201 03:34:18.361213 139907729164032 logging_writer.py:48] [129700] global_step=129700, grad_norm=3.4772908687591553, loss=2.7720260620117188
I0201 03:34:52.061440 139907737556736 logging_writer.py:48] [129800] global_step=129800, grad_norm=3.190469264984131, loss=2.7337331771850586
I0201 03:35:25.774435 139907729164032 logging_writer.py:48] [129900] global_step=129900, grad_norm=3.1717917919158936, loss=2.79085373878479
I0201 03:35:59.513772 139907737556736 logging_writer.py:48] [130000] global_step=130000, grad_norm=3.1424968242645264, loss=2.8291237354278564
I0201 03:36:08.101099 140070692116288 spec.py:321] Evaluating on the training split.
I0201 03:36:14.314199 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 03:36:23.059370 140070692116288 spec.py:349] Evaluating on the test split.
I0201 03:36:25.521864 140070692116288 submission_runner.py:408] Time since start: 45445.29s, 	Step: 130027, 	{'train/accuracy': 0.8579002022743225, 'train/loss': 0.7536612153053284, 'validation/accuracy': 0.7330600023269653, 'validation/loss': 1.2795575857162476, 'validation/num_examples': 50000, 'test/accuracy': 0.6058000326156616, 'test/loss': 1.9297984838485718, 'test/num_examples': 10000, 'score': 43898.8369243145, 'total_duration': 45445.294552087784, 'accumulated_submission_time': 43898.8369243145, 'accumulated_eval_time': 1538.5677382946014, 'accumulated_logging_time': 3.3185129165649414}
I0201 03:36:25.560667 139907737556736 logging_writer.py:48] [130027] accumulated_eval_time=1538.567738, accumulated_logging_time=3.318513, accumulated_submission_time=43898.836924, global_step=130027, preemption_count=0, score=43898.836924, test/accuracy=0.605800, test/loss=1.929798, test/num_examples=10000, total_duration=45445.294552, train/accuracy=0.857900, train/loss=0.753661, validation/accuracy=0.733060, validation/loss=1.279558, validation/num_examples=50000
I0201 03:36:50.498426 139907745949440 logging_writer.py:48] [130100] global_step=130100, grad_norm=3.211500644683838, loss=2.727759838104248
I0201 03:37:24.201144 139907737556736 logging_writer.py:48] [130200] global_step=130200, grad_norm=3.085947036743164, loss=2.704566478729248
I0201 03:37:57.948861 139907745949440 logging_writer.py:48] [130300] global_step=130300, grad_norm=3.139112710952759, loss=2.7160000801086426
I0201 03:38:31.646679 139907737556736 logging_writer.py:48] [130400] global_step=130400, grad_norm=3.2031350135803223, loss=2.725755453109741
I0201 03:39:05.336847 139907745949440 logging_writer.py:48] [130500] global_step=130500, grad_norm=3.2155990600585938, loss=2.678997278213501
I0201 03:39:39.149621 139907737556736 logging_writer.py:48] [130600] global_step=130600, grad_norm=3.19089412689209, loss=2.689399242401123
I0201 03:40:12.898296 139907745949440 logging_writer.py:48] [130700] global_step=130700, grad_norm=3.294419050216675, loss=2.7761940956115723
I0201 03:40:46.572223 139907737556736 logging_writer.py:48] [130800] global_step=130800, grad_norm=3.1057286262512207, loss=2.7205722332000732
I0201 03:41:20.260346 139907745949440 logging_writer.py:48] [130900] global_step=130900, grad_norm=3.200754404067993, loss=2.727881669998169
I0201 03:41:53.922776 139907737556736 logging_writer.py:48] [131000] global_step=131000, grad_norm=3.2381649017333984, loss=2.722628116607666
I0201 03:42:27.710782 139907745949440 logging_writer.py:48] [131100] global_step=131100, grad_norm=3.2311742305755615, loss=2.705301523208618
I0201 03:43:01.732369 139907737556736 logging_writer.py:48] [131200] global_step=131200, grad_norm=3.2982733249664307, loss=2.7305922508239746
I0201 03:43:35.459025 139907745949440 logging_writer.py:48] [131300] global_step=131300, grad_norm=3.471451997756958, loss=2.722414970397949
I0201 03:44:09.126471 139907737556736 logging_writer.py:48] [131400] global_step=131400, grad_norm=3.2359886169433594, loss=2.731701135635376
I0201 03:44:42.886293 139907745949440 logging_writer.py:48] [131500] global_step=131500, grad_norm=3.1752798557281494, loss=2.7967605590820312
I0201 03:44:55.843620 140070692116288 spec.py:321] Evaluating on the training split.
I0201 03:45:02.157523 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 03:45:11.084876 140070692116288 spec.py:349] Evaluating on the test split.
I0201 03:45:13.493200 140070692116288 submission_runner.py:408] Time since start: 45973.27s, 	Step: 131540, 	{'train/accuracy': 0.8606704473495483, 'train/loss': 0.7800890803337097, 'validation/accuracy': 0.7356199622154236, 'validation/loss': 1.2970260381698608, 'validation/num_examples': 50000, 'test/accuracy': 0.6103000044822693, 'test/loss': 1.926138162612915, 'test/num_examples': 10000, 'score': 44409.057758808136, 'total_duration': 45973.265880823135, 'accumulated_submission_time': 44409.057758808136, 'accumulated_eval_time': 1556.2172808647156, 'accumulated_logging_time': 3.3665919303894043}
I0201 03:45:13.536882 139908719027968 logging_writer.py:48] [131540] accumulated_eval_time=1556.217281, accumulated_logging_time=3.366592, accumulated_submission_time=44409.057759, global_step=131540, preemption_count=0, score=44409.057759, test/accuracy=0.610300, test/loss=1.926138, test/num_examples=10000, total_duration=45973.265881, train/accuracy=0.860670, train/loss=0.780089, validation/accuracy=0.735620, validation/loss=1.297026, validation/num_examples=50000
I0201 03:45:34.074832 139908727420672 logging_writer.py:48] [131600] global_step=131600, grad_norm=3.1343326568603516, loss=2.735736846923828
I0201 03:46:07.809820 139908719027968 logging_writer.py:48] [131700] global_step=131700, grad_norm=3.2147140502929688, loss=2.7641398906707764
I0201 03:46:41.536756 139908727420672 logging_writer.py:48] [131800] global_step=131800, grad_norm=3.3617894649505615, loss=2.7235169410705566
I0201 03:47:15.289379 139908719027968 logging_writer.py:48] [131900] global_step=131900, grad_norm=3.335571050643921, loss=2.7278695106506348
I0201 03:47:49.059240 139908727420672 logging_writer.py:48] [132000] global_step=132000, grad_norm=3.040534257888794, loss=2.699533462524414
I0201 03:48:22.778831 139908719027968 logging_writer.py:48] [132100] global_step=132100, grad_norm=3.362311363220215, loss=2.762920379638672
I0201 03:48:56.508409 139908727420672 logging_writer.py:48] [132200] global_step=132200, grad_norm=3.3143112659454346, loss=2.777007579803467
I0201 03:49:30.237187 139908719027968 logging_writer.py:48] [132300] global_step=132300, grad_norm=3.3016304969787598, loss=2.737622022628784
I0201 03:50:03.964904 139908727420672 logging_writer.py:48] [132400] global_step=132400, grad_norm=3.5343098640441895, loss=2.7520432472229004
I0201 03:50:37.704015 139908719027968 logging_writer.py:48] [132500] global_step=132500, grad_norm=3.472825288772583, loss=2.728801727294922
I0201 03:51:11.386819 139908727420672 logging_writer.py:48] [132600] global_step=132600, grad_norm=3.1142802238464355, loss=2.7349765300750732
I0201 03:51:45.137512 139908719027968 logging_writer.py:48] [132700] global_step=132700, grad_norm=3.344412088394165, loss=2.742462396621704
I0201 03:52:19.097404 139908727420672 logging_writer.py:48] [132800] global_step=132800, grad_norm=3.037968158721924, loss=2.6952531337738037
I0201 03:52:52.816912 139908719027968 logging_writer.py:48] [132900] global_step=132900, grad_norm=3.3038148880004883, loss=2.764145612716675
I0201 03:53:26.579544 139908727420672 logging_writer.py:48] [133000] global_step=133000, grad_norm=3.3287758827209473, loss=2.831815719604492
I0201 03:53:43.567930 140070692116288 spec.py:321] Evaluating on the training split.
I0201 03:53:49.768053 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 03:53:58.556944 140070692116288 spec.py:349] Evaluating on the test split.
I0201 03:54:00.971641 140070692116288 submission_runner.py:408] Time since start: 46500.74s, 	Step: 133052, 	{'train/accuracy': 0.8704559803009033, 'train/loss': 0.7145001888275146, 'validation/accuracy': 0.737060010433197, 'validation/loss': 1.258089542388916, 'validation/num_examples': 50000, 'test/accuracy': 0.615600049495697, 'test/loss': 1.887601613998413, 'test/num_examples': 10000, 'score': 44919.02611017227, 'total_duration': 46500.744307756424, 'accumulated_submission_time': 44919.02611017227, 'accumulated_eval_time': 1573.6209378242493, 'accumulated_logging_time': 3.4200706481933594}
I0201 03:54:01.015549 139907745949440 logging_writer.py:48] [133052] accumulated_eval_time=1573.620938, accumulated_logging_time=3.420071, accumulated_submission_time=44919.026110, global_step=133052, preemption_count=0, score=44919.026110, test/accuracy=0.615600, test/loss=1.887602, test/num_examples=10000, total_duration=46500.744308, train/accuracy=0.870456, train/loss=0.714500, validation/accuracy=0.737060, validation/loss=1.258090, validation/num_examples=50000
I0201 03:54:17.567854 139907754342144 logging_writer.py:48] [133100] global_step=133100, grad_norm=3.211888551712036, loss=2.6852264404296875
I0201 03:54:51.331802 139907745949440 logging_writer.py:48] [133200] global_step=133200, grad_norm=3.4411277770996094, loss=2.8105838298797607
I0201 03:55:25.021798 139907754342144 logging_writer.py:48] [133300] global_step=133300, grad_norm=3.256354808807373, loss=2.6971185207366943
I0201 03:55:58.741783 139907745949440 logging_writer.py:48] [133400] global_step=133400, grad_norm=3.6023499965667725, loss=2.798006057739258
I0201 03:56:32.451456 139907754342144 logging_writer.py:48] [133500] global_step=133500, grad_norm=3.3168351650238037, loss=2.7204177379608154
I0201 03:57:06.178361 139907745949440 logging_writer.py:48] [133600] global_step=133600, grad_norm=3.27805233001709, loss=2.7321081161499023
I0201 03:57:39.860044 139907754342144 logging_writer.py:48] [133700] global_step=133700, grad_norm=3.381728172302246, loss=2.745114326477051
I0201 03:58:13.781050 139907745949440 logging_writer.py:48] [133800] global_step=133800, grad_norm=3.3971991539001465, loss=2.810812473297119
I0201 03:58:47.474595 139907754342144 logging_writer.py:48] [133900] global_step=133900, grad_norm=3.357117176055908, loss=2.723492383956909
I0201 03:59:21.179442 139907745949440 logging_writer.py:48] [134000] global_step=134000, grad_norm=3.3412113189697266, loss=2.729463577270508
I0201 03:59:54.852235 139907754342144 logging_writer.py:48] [134100] global_step=134100, grad_norm=3.339586019515991, loss=2.745887517929077
I0201 04:00:28.568653 139907745949440 logging_writer.py:48] [134200] global_step=134200, grad_norm=3.2085461616516113, loss=2.6965079307556152
I0201 04:01:02.310570 139907754342144 logging_writer.py:48] [134300] global_step=134300, grad_norm=3.272794485092163, loss=2.7669758796691895
I0201 04:01:36.009333 139907745949440 logging_writer.py:48] [134400] global_step=134400, grad_norm=3.2854208946228027, loss=2.687655448913574
I0201 04:02:09.755902 139907754342144 logging_writer.py:48] [134500] global_step=134500, grad_norm=3.4600181579589844, loss=2.772735834121704
I0201 04:02:31.155822 140070692116288 spec.py:321] Evaluating on the training split.
I0201 04:02:37.367042 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 04:02:46.075936 140070692116288 spec.py:349] Evaluating on the test split.
I0201 04:02:48.569024 140070692116288 submission_runner.py:408] Time since start: 47028.34s, 	Step: 134565, 	{'train/accuracy': 0.8752790093421936, 'train/loss': 0.6944555640220642, 'validation/accuracy': 0.7367199659347534, 'validation/loss': 1.2754249572753906, 'validation/num_examples': 50000, 'test/accuracy': 0.6135000586509705, 'test/loss': 1.9048426151275635, 'test/num_examples': 10000, 'score': 45429.102596998215, 'total_duration': 47028.341700553894, 'accumulated_submission_time': 45429.102596998215, 'accumulated_eval_time': 1591.034093618393, 'accumulated_logging_time': 3.4738011360168457}
I0201 04:02:48.611498 139907737556736 logging_writer.py:48] [134565] accumulated_eval_time=1591.034094, accumulated_logging_time=3.473801, accumulated_submission_time=45429.102597, global_step=134565, preemption_count=0, score=45429.102597, test/accuracy=0.613500, test/loss=1.904843, test/num_examples=10000, total_duration=47028.341701, train/accuracy=0.875279, train/loss=0.694456, validation/accuracy=0.736720, validation/loss=1.275425, validation/num_examples=50000
I0201 04:03:00.764370 139908710635264 logging_writer.py:48] [134600] global_step=134600, grad_norm=3.3766257762908936, loss=2.7057552337646484
I0201 04:03:34.417691 139907737556736 logging_writer.py:48] [134700] global_step=134700, grad_norm=3.2531824111938477, loss=2.7658956050872803
I0201 04:04:08.102962 139908710635264 logging_writer.py:48] [134800] global_step=134800, grad_norm=3.2894368171691895, loss=2.7133004665374756
I0201 04:04:42.068276 139907737556736 logging_writer.py:48] [134900] global_step=134900, grad_norm=3.2656784057617188, loss=2.6940650939941406
I0201 04:05:15.764991 139908710635264 logging_writer.py:48] [135000] global_step=135000, grad_norm=3.8120639324188232, loss=2.738767147064209
I0201 04:05:49.531366 139907737556736 logging_writer.py:48] [135100] global_step=135100, grad_norm=3.266181468963623, loss=2.699903964996338
I0201 04:06:23.222330 139908710635264 logging_writer.py:48] [135200] global_step=135200, grad_norm=3.288940668106079, loss=2.789590358734131
I0201 04:06:56.924622 139907737556736 logging_writer.py:48] [135300] global_step=135300, grad_norm=3.334402322769165, loss=2.7612156867980957
I0201 04:07:30.658598 139908710635264 logging_writer.py:48] [135400] global_step=135400, grad_norm=3.6070046424865723, loss=2.7933545112609863
I0201 04:08:04.368862 139907737556736 logging_writer.py:48] [135500] global_step=135500, grad_norm=3.445937395095825, loss=2.785430908203125
I0201 04:08:38.122963 139908710635264 logging_writer.py:48] [135600] global_step=135600, grad_norm=3.479682445526123, loss=2.7213001251220703
I0201 04:09:11.790466 139907737556736 logging_writer.py:48] [135700] global_step=135700, grad_norm=3.278193712234497, loss=2.6985297203063965
I0201 04:09:45.460963 139908710635264 logging_writer.py:48] [135800] global_step=135800, grad_norm=3.28450345993042, loss=2.722257375717163
I0201 04:10:19.248436 139907737556736 logging_writer.py:48] [135900] global_step=135900, grad_norm=3.380687713623047, loss=2.72481632232666
I0201 04:10:53.121659 139908710635264 logging_writer.py:48] [136000] global_step=136000, grad_norm=3.344403028488159, loss=2.753192901611328
I0201 04:11:18.873854 140070692116288 spec.py:321] Evaluating on the training split.
I0201 04:11:25.034399 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 04:11:33.879006 140070692116288 spec.py:349] Evaluating on the test split.
I0201 04:11:36.375288 140070692116288 submission_runner.py:408] Time since start: 47556.15s, 	Step: 136078, 	{'train/accuracy': 0.8797034025192261, 'train/loss': 0.667981743812561, 'validation/accuracy': 0.7418999671936035, 'validation/loss': 1.2350200414657593, 'validation/num_examples': 50000, 'test/accuracy': 0.6200000047683716, 'test/loss': 1.8570964336395264, 'test/num_examples': 10000, 'score': 45939.301439762115, 'total_duration': 47556.14796924591, 'accumulated_submission_time': 45939.301439762115, 'accumulated_eval_time': 1608.5354924201965, 'accumulated_logging_time': 3.5263099670410156}
I0201 04:11:36.414981 139907762734848 logging_writer.py:48] [136078] accumulated_eval_time=1608.535492, accumulated_logging_time=3.526310, accumulated_submission_time=45939.301440, global_step=136078, preemption_count=0, score=45939.301440, test/accuracy=0.620000, test/loss=1.857096, test/num_examples=10000, total_duration=47556.147969, train/accuracy=0.879703, train/loss=0.667982, validation/accuracy=0.741900, validation/loss=1.235020, validation/num_examples=50000
I0201 04:11:44.191231 139908425447168 logging_writer.py:48] [136100] global_step=136100, grad_norm=3.509624481201172, loss=2.7538952827453613
I0201 04:12:17.882163 139907762734848 logging_writer.py:48] [136200] global_step=136200, grad_norm=3.4165852069854736, loss=2.709184408187866
I0201 04:12:51.615359 139908425447168 logging_writer.py:48] [136300] global_step=136300, grad_norm=3.5487382411956787, loss=2.6852550506591797
I0201 04:13:25.278673 139907762734848 logging_writer.py:48] [136400] global_step=136400, grad_norm=3.3317253589630127, loss=2.725566864013672
I0201 04:13:59.081110 139908425447168 logging_writer.py:48] [136500] global_step=136500, grad_norm=3.6488263607025146, loss=2.7208809852600098
I0201 04:14:32.731371 139907762734848 logging_writer.py:48] [136600] global_step=136600, grad_norm=3.5228734016418457, loss=2.697359323501587
I0201 04:15:06.444989 139908425447168 logging_writer.py:48] [136700] global_step=136700, grad_norm=3.479417562484741, loss=2.714160919189453
I0201 04:15:40.195837 139907762734848 logging_writer.py:48] [136800] global_step=136800, grad_norm=3.635883092880249, loss=2.7563974857330322
I0201 04:16:13.926551 139908425447168 logging_writer.py:48] [136900] global_step=136900, grad_norm=3.440950632095337, loss=2.70147967338562
I0201 04:16:47.682729 139907762734848 logging_writer.py:48] [137000] global_step=137000, grad_norm=3.181462049484253, loss=2.631281614303589
I0201 04:17:21.412494 139908425447168 logging_writer.py:48] [137100] global_step=137100, grad_norm=3.561833620071411, loss=2.7555766105651855
I0201 04:17:55.167592 139907762734848 logging_writer.py:48] [137200] global_step=137200, grad_norm=3.4623451232910156, loss=2.688596248626709
I0201 04:18:28.906673 139908425447168 logging_writer.py:48] [137300] global_step=137300, grad_norm=3.3504178524017334, loss=2.7119829654693604
I0201 04:19:02.616342 139907762734848 logging_writer.py:48] [137400] global_step=137400, grad_norm=3.1408348083496094, loss=2.6980221271514893
I0201 04:19:36.336651 139908425447168 logging_writer.py:48] [137500] global_step=137500, grad_norm=3.4064745903015137, loss=2.7617177963256836
I0201 04:20:06.481440 140070692116288 spec.py:321] Evaluating on the training split.
I0201 04:20:12.809899 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 04:20:21.656147 140070692116288 spec.py:349] Evaluating on the test split.
I0201 04:20:24.190415 140070692116288 submission_runner.py:408] Time since start: 48083.96s, 	Step: 137591, 	{'train/accuracy': 0.8743423223495483, 'train/loss': 0.6792070865631104, 'validation/accuracy': 0.7400799989700317, 'validation/loss': 1.2348096370697021, 'validation/num_examples': 50000, 'test/accuracy': 0.6186000108718872, 'test/loss': 1.8752394914627075, 'test/num_examples': 10000, 'score': 46449.305644750595, 'total_duration': 48083.96310710907, 'accumulated_submission_time': 46449.305644750595, 'accumulated_eval_time': 1626.2444546222687, 'accumulated_logging_time': 3.5757358074188232}
I0201 04:20:24.234868 139907754342144 logging_writer.py:48] [137591] accumulated_eval_time=1626.244455, accumulated_logging_time=3.575736, accumulated_submission_time=46449.305645, global_step=137591, preemption_count=0, score=46449.305645, test/accuracy=0.618600, test/loss=1.875239, test/num_examples=10000, total_duration=48083.963107, train/accuracy=0.874342, train/loss=0.679207, validation/accuracy=0.740080, validation/loss=1.234810, validation/num_examples=50000
I0201 04:20:27.610662 139907762734848 logging_writer.py:48] [137600] global_step=137600, grad_norm=3.3843743801116943, loss=2.698934555053711
I0201 04:21:01.259426 139907754342144 logging_writer.py:48] [137700] global_step=137700, grad_norm=3.370882511138916, loss=2.7651679515838623
I0201 04:21:34.962085 139907762734848 logging_writer.py:48] [137800] global_step=137800, grad_norm=3.2991623878479004, loss=2.6912503242492676
I0201 04:22:08.635762 139907754342144 logging_writer.py:48] [137900] global_step=137900, grad_norm=3.6512229442596436, loss=2.7220277786254883
I0201 04:22:42.350741 139907762734848 logging_writer.py:48] [138000] global_step=138000, grad_norm=3.698939800262451, loss=2.7367374897003174
I0201 04:23:16.284882 139907754342144 logging_writer.py:48] [138100] global_step=138100, grad_norm=3.4460060596466064, loss=2.677607774734497
I0201 04:23:50.013750 139907762734848 logging_writer.py:48] [138200] global_step=138200, grad_norm=3.264859914779663, loss=2.7024900913238525
I0201 04:24:23.707792 139907754342144 logging_writer.py:48] [138300] global_step=138300, grad_norm=3.433222532272339, loss=2.7075376510620117
I0201 04:24:57.383064 139907762734848 logging_writer.py:48] [138400] global_step=138400, grad_norm=3.755260705947876, loss=2.749267578125
I0201 04:25:31.044037 139907754342144 logging_writer.py:48] [138500] global_step=138500, grad_norm=3.2750794887542725, loss=2.660676956176758
I0201 04:26:04.838384 139907762734848 logging_writer.py:48] [138600] global_step=138600, grad_norm=3.5668342113494873, loss=2.7238340377807617
I0201 04:26:38.530015 139907754342144 logging_writer.py:48] [138700] global_step=138700, grad_norm=3.5380477905273438, loss=2.7536399364471436
I0201 04:27:12.333927 139907762734848 logging_writer.py:48] [138800] global_step=138800, grad_norm=3.490382432937622, loss=2.677135705947876
I0201 04:27:46.037240 139907754342144 logging_writer.py:48] [138900] global_step=138900, grad_norm=3.2784061431884766, loss=2.69256329536438
I0201 04:28:19.800423 139907762734848 logging_writer.py:48] [139000] global_step=139000, grad_norm=3.4884679317474365, loss=2.7321579456329346
I0201 04:28:53.505829 139907754342144 logging_writer.py:48] [139100] global_step=139100, grad_norm=3.275411605834961, loss=2.694662570953369
I0201 04:28:54.332782 140070692116288 spec.py:321] Evaluating on the training split.
I0201 04:29:00.562621 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 04:29:09.372212 140070692116288 spec.py:349] Evaluating on the test split.
I0201 04:29:12.203936 140070692116288 submission_runner.py:408] Time since start: 48611.98s, 	Step: 139104, 	{'train/accuracy': 0.880301296710968, 'train/loss': 0.6912388801574707, 'validation/accuracy': 0.7443999648094177, 'validation/loss': 1.2525049448013306, 'validation/num_examples': 50000, 'test/accuracy': 0.6166000366210938, 'test/loss': 1.8933311700820923, 'test/num_examples': 10000, 'score': 46959.34210753441, 'total_duration': 48611.97661066055, 'accumulated_submission_time': 46959.34210753441, 'accumulated_eval_time': 1644.1155638694763, 'accumulated_logging_time': 3.6292083263397217}
I0201 04:29:12.245146 139908425447168 logging_writer.py:48] [139104] accumulated_eval_time=1644.115564, accumulated_logging_time=3.629208, accumulated_submission_time=46959.342108, global_step=139104, preemption_count=0, score=46959.342108, test/accuracy=0.616600, test/loss=1.893331, test/num_examples=10000, total_duration=48611.976611, train/accuracy=0.880301, train/loss=0.691239, validation/accuracy=0.744400, validation/loss=1.252505, validation/num_examples=50000
I0201 04:29:44.943408 139908710635264 logging_writer.py:48] [139200] global_step=139200, grad_norm=3.338291645050049, loss=2.714276075363159
I0201 04:30:18.601998 139908425447168 logging_writer.py:48] [139300] global_step=139300, grad_norm=3.5153067111968994, loss=2.758178949356079
I0201 04:30:52.307470 139908710635264 logging_writer.py:48] [139400] global_step=139400, grad_norm=3.2655222415924072, loss=2.6410999298095703
I0201 04:31:25.974641 139908425447168 logging_writer.py:48] [139500] global_step=139500, grad_norm=3.646164894104004, loss=2.7299389839172363
I0201 04:31:59.727670 139908710635264 logging_writer.py:48] [139600] global_step=139600, grad_norm=3.4754068851470947, loss=2.6711549758911133
I0201 04:32:33.396728 139908425447168 logging_writer.py:48] [139700] global_step=139700, grad_norm=3.473440170288086, loss=2.643911838531494
I0201 04:33:07.097923 139908710635264 logging_writer.py:48] [139800] global_step=139800, grad_norm=3.5661239624023438, loss=2.6776487827301025
I0201 04:33:40.738141 139908425447168 logging_writer.py:48] [139900] global_step=139900, grad_norm=3.5064644813537598, loss=2.6747772693634033
I0201 04:34:14.435376 139908710635264 logging_writer.py:48] [140000] global_step=140000, grad_norm=3.964698076248169, loss=2.6792502403259277
I0201 04:34:48.199923 139908425447168 logging_writer.py:48] [140100] global_step=140100, grad_norm=3.730759620666504, loss=2.7760982513427734
I0201 04:35:21.928492 139908710635264 logging_writer.py:48] [140200] global_step=140200, grad_norm=3.418226957321167, loss=2.6524975299835205
I0201 04:35:55.727773 139908425447168 logging_writer.py:48] [140300] global_step=140300, grad_norm=3.2486093044281006, loss=2.705918073654175
I0201 04:36:29.432257 139908710635264 logging_writer.py:48] [140400] global_step=140400, grad_norm=3.4751038551330566, loss=2.6867172718048096
I0201 04:37:03.196300 139908425447168 logging_writer.py:48] [140500] global_step=140500, grad_norm=3.5934152603149414, loss=2.7224814891815186
I0201 04:37:36.932609 139908710635264 logging_writer.py:48] [140600] global_step=140600, grad_norm=3.221229314804077, loss=2.6486105918884277
I0201 04:37:42.467175 140070692116288 spec.py:321] Evaluating on the training split.
I0201 04:37:48.633796 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 04:37:57.364244 140070692116288 spec.py:349] Evaluating on the test split.
I0201 04:37:59.888954 140070692116288 submission_runner.py:408] Time since start: 49139.66s, 	Step: 140618, 	{'train/accuracy': 0.881257951259613, 'train/loss': 0.6726261973381042, 'validation/accuracy': 0.7454599738121033, 'validation/loss': 1.2311464548110962, 'validation/num_examples': 50000, 'test/accuracy': 0.6273000240325928, 'test/loss': 1.8636746406555176, 'test/num_examples': 10000, 'score': 47469.49969315529, 'total_duration': 49139.661640405655, 'accumulated_submission_time': 47469.49969315529, 'accumulated_eval_time': 1661.5373244285583, 'accumulated_logging_time': 3.6814053058624268}
I0201 04:37:59.930263 139907745949440 logging_writer.py:48] [140618] accumulated_eval_time=1661.537324, accumulated_logging_time=3.681405, accumulated_submission_time=47469.499693, global_step=140618, preemption_count=0, score=47469.499693, test/accuracy=0.627300, test/loss=1.863675, test/num_examples=10000, total_duration=49139.661640, train/accuracy=0.881258, train/loss=0.672626, validation/accuracy=0.745460, validation/loss=1.231146, validation/num_examples=50000
I0201 04:38:27.892202 139907754342144 logging_writer.py:48] [140700] global_step=140700, grad_norm=3.6365463733673096, loss=2.6977226734161377
I0201 04:39:01.546953 139907745949440 logging_writer.py:48] [140800] global_step=140800, grad_norm=3.553908348083496, loss=2.641772508621216
I0201 04:39:35.317329 139907754342144 logging_writer.py:48] [140900] global_step=140900, grad_norm=3.359095335006714, loss=2.6909303665161133
I0201 04:40:08.996033 139907745949440 logging_writer.py:48] [141000] global_step=141000, grad_norm=3.286716938018799, loss=2.6933202743530273
I0201 04:40:42.790314 139907754342144 logging_writer.py:48] [141100] global_step=141100, grad_norm=3.581840991973877, loss=2.6837379932403564
I0201 04:41:16.471449 139907745949440 logging_writer.py:48] [141200] global_step=141200, grad_norm=3.5199108123779297, loss=2.6728482246398926
I0201 04:41:50.442970 139907754342144 logging_writer.py:48] [141300] global_step=141300, grad_norm=3.866063117980957, loss=2.7260260581970215
I0201 04:42:24.093197 139907745949440 logging_writer.py:48] [141400] global_step=141400, grad_norm=3.438472270965576, loss=2.6500158309936523
I0201 04:42:57.893560 139907754342144 logging_writer.py:48] [141500] global_step=141500, grad_norm=3.731726884841919, loss=2.7247090339660645
I0201 04:43:31.578230 139907745949440 logging_writer.py:48] [141600] global_step=141600, grad_norm=3.757418632507324, loss=2.7365424633026123
I0201 04:44:05.367257 139907754342144 logging_writer.py:48] [141700] global_step=141700, grad_norm=3.7596166133880615, loss=2.68057918548584
I0201 04:44:39.052167 139907745949440 logging_writer.py:48] [141800] global_step=141800, grad_norm=3.679499387741089, loss=2.69095516204834
I0201 04:45:12.861213 139907754342144 logging_writer.py:48] [141900] global_step=141900, grad_norm=3.454622983932495, loss=2.6654653549194336
I0201 04:45:46.537164 139907745949440 logging_writer.py:48] [142000] global_step=142000, grad_norm=3.5427591800689697, loss=2.6828153133392334
I0201 04:46:20.349021 139907754342144 logging_writer.py:48] [142100] global_step=142100, grad_norm=3.919342517852783, loss=2.7238197326660156
I0201 04:46:29.931075 140070692116288 spec.py:321] Evaluating on the training split.
I0201 04:46:36.085310 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 04:46:44.804293 140070692116288 spec.py:349] Evaluating on the test split.
I0201 04:46:47.337247 140070692116288 submission_runner.py:408] Time since start: 49667.11s, 	Step: 142130, 	{'train/accuracy': 0.8937141299247742, 'train/loss': 0.6258179545402527, 'validation/accuracy': 0.7417399883270264, 'validation/loss': 1.2493900060653687, 'validation/num_examples': 50000, 'test/accuracy': 0.6145000457763672, 'test/loss': 1.894722580909729, 'test/num_examples': 10000, 'score': 47979.43871974945, 'total_duration': 49667.10993528366, 'accumulated_submission_time': 47979.43871974945, 'accumulated_eval_time': 1678.9434888362885, 'accumulated_logging_time': 3.7319438457489014}
I0201 04:46:47.380837 139908425447168 logging_writer.py:48] [142130] accumulated_eval_time=1678.943489, accumulated_logging_time=3.731944, accumulated_submission_time=47979.438720, global_step=142130, preemption_count=0, score=47979.438720, test/accuracy=0.614500, test/loss=1.894723, test/num_examples=10000, total_duration=49667.109935, train/accuracy=0.893714, train/loss=0.625818, validation/accuracy=0.741740, validation/loss=1.249390, validation/num_examples=50000
I0201 04:47:11.297070 139908710635264 logging_writer.py:48] [142200] global_step=142200, grad_norm=3.454648494720459, loss=2.6456377506256104
I0201 04:47:45.014579 139908425447168 logging_writer.py:48] [142300] global_step=142300, grad_norm=3.299217939376831, loss=2.673252582550049
I0201 04:48:18.852235 139908710635264 logging_writer.py:48] [142400] global_step=142400, grad_norm=3.861903667449951, loss=2.704641819000244
I0201 04:48:52.568196 139908425447168 logging_writer.py:48] [142500] global_step=142500, grad_norm=3.421884775161743, loss=2.6876766681671143
I0201 04:49:26.254545 139908710635264 logging_writer.py:48] [142600] global_step=142600, grad_norm=3.7298433780670166, loss=2.687519073486328
I0201 04:49:59.951457 139908425447168 logging_writer.py:48] [142700] global_step=142700, grad_norm=3.436208724975586, loss=2.6553101539611816
I0201 04:50:33.720414 139908710635264 logging_writer.py:48] [142800] global_step=142800, grad_norm=3.565911293029785, loss=2.7398018836975098
I0201 04:51:07.424892 139908425447168 logging_writer.py:48] [142900] global_step=142900, grad_norm=3.825275182723999, loss=2.705564260482788
I0201 04:51:41.188606 139908710635264 logging_writer.py:48] [143000] global_step=143000, grad_norm=3.3338515758514404, loss=2.691128730773926
I0201 04:52:14.894866 139908425447168 logging_writer.py:48] [143100] global_step=143100, grad_norm=3.5720598697662354, loss=2.690952777862549
I0201 04:52:48.634473 139908710635264 logging_writer.py:48] [143200] global_step=143200, grad_norm=3.6520235538482666, loss=2.662893772125244
I0201 04:53:22.325370 139908425447168 logging_writer.py:48] [143300] global_step=143300, grad_norm=3.5501956939697266, loss=2.7556819915771484
I0201 04:53:56.003807 139908710635264 logging_writer.py:48] [143400] global_step=143400, grad_norm=3.662950277328491, loss=2.6760895252227783
I0201 04:54:29.864322 139908425447168 logging_writer.py:48] [143500] global_step=143500, grad_norm=3.8274686336517334, loss=2.6495933532714844
I0201 04:55:03.612184 139908710635264 logging_writer.py:48] [143600] global_step=143600, grad_norm=3.554137945175171, loss=2.6646668910980225
I0201 04:55:17.610225 140070692116288 spec.py:321] Evaluating on the training split.
I0201 04:55:23.778912 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 04:55:32.628804 140070692116288 spec.py:349] Evaluating on the test split.
I0201 04:55:35.040503 140070692116288 submission_runner.py:408] Time since start: 50194.81s, 	Step: 143643, 	{'train/accuracy': 0.8984175324440002, 'train/loss': 0.5999704599380493, 'validation/accuracy': 0.7469199895858765, 'validation/loss': 1.220354676246643, 'validation/num_examples': 50000, 'test/accuracy': 0.6167000532150269, 'test/loss': 1.8685358762741089, 'test/num_examples': 10000, 'score': 48489.606682538986, 'total_duration': 50194.81318330765, 'accumulated_submission_time': 48489.606682538986, 'accumulated_eval_time': 1696.3737258911133, 'accumulated_logging_time': 3.7850804328918457}
I0201 04:55:35.082133 139907754342144 logging_writer.py:48] [143643] accumulated_eval_time=1696.373726, accumulated_logging_time=3.785080, accumulated_submission_time=48489.606683, global_step=143643, preemption_count=0, score=48489.606683, test/accuracy=0.616700, test/loss=1.868536, test/num_examples=10000, total_duration=50194.813183, train/accuracy=0.898418, train/loss=0.599970, validation/accuracy=0.746920, validation/loss=1.220355, validation/num_examples=50000
I0201 04:55:54.636893 139907762734848 logging_writer.py:48] [143700] global_step=143700, grad_norm=3.584270715713501, loss=2.6375792026519775
I0201 04:56:28.327738 139907754342144 logging_writer.py:48] [143800] global_step=143800, grad_norm=4.0158610343933105, loss=2.7432265281677246
I0201 04:57:02.054680 139907762734848 logging_writer.py:48] [143900] global_step=143900, grad_norm=3.2850253582000732, loss=2.669221878051758
I0201 04:57:35.808353 139907754342144 logging_writer.py:48] [144000] global_step=144000, grad_norm=3.995912790298462, loss=2.6823277473449707
I0201 04:58:09.560428 139907762734848 logging_writer.py:48] [144100] global_step=144100, grad_norm=3.577047348022461, loss=2.6660354137420654
I0201 04:58:43.282903 139907754342144 logging_writer.py:48] [144200] global_step=144200, grad_norm=3.516183614730835, loss=2.6602859497070312
I0201 04:59:17.028072 139907762734848 logging_writer.py:48] [144300] global_step=144300, grad_norm=3.4883341789245605, loss=2.6463496685028076
I0201 04:59:50.781327 139907754342144 logging_writer.py:48] [144400] global_step=144400, grad_norm=3.5990498065948486, loss=2.6872568130493164
I0201 05:00:24.609954 139907762734848 logging_writer.py:48] [144500] global_step=144500, grad_norm=3.5872604846954346, loss=2.61802339553833
I0201 05:00:58.336253 139907754342144 logging_writer.py:48] [144600] global_step=144600, grad_norm=3.6928248405456543, loss=2.678283214569092
I0201 05:01:32.003232 139907762734848 logging_writer.py:48] [144700] global_step=144700, grad_norm=3.4328927993774414, loss=2.6442723274230957
I0201 05:02:05.721289 139907754342144 logging_writer.py:48] [144800] global_step=144800, grad_norm=3.6405560970306396, loss=2.649104118347168
I0201 05:02:39.425962 139907762734848 logging_writer.py:48] [144900] global_step=144900, grad_norm=3.4711005687713623, loss=2.639719009399414
I0201 05:03:13.202947 139907754342144 logging_writer.py:48] [145000] global_step=145000, grad_norm=3.668132781982422, loss=2.696366548538208
I0201 05:03:46.900515 139907762734848 logging_writer.py:48] [145100] global_step=145100, grad_norm=3.6001572608947754, loss=2.6472737789154053
I0201 05:04:05.304795 140070692116288 spec.py:321] Evaluating on the training split.
I0201 05:04:11.438913 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 05:04:20.188275 140070692116288 spec.py:349] Evaluating on the test split.
I0201 05:04:22.624239 140070692116288 submission_runner.py:408] Time since start: 50722.40s, 	Step: 145156, 	{'train/accuracy': 0.894551157951355, 'train/loss': 0.6113947629928589, 'validation/accuracy': 0.745959997177124, 'validation/loss': 1.216866374015808, 'validation/num_examples': 50000, 'test/accuracy': 0.6219000220298767, 'test/loss': 1.8563332557678223, 'test/num_examples': 10000, 'score': 48999.76789522171, 'total_duration': 50722.39692115784, 'accumulated_submission_time': 48999.76789522171, 'accumulated_eval_time': 1713.6931321620941, 'accumulated_logging_time': 3.8360297679901123}
I0201 05:04:22.670606 139907745949440 logging_writer.py:48] [145156] accumulated_eval_time=1713.693132, accumulated_logging_time=3.836030, accumulated_submission_time=48999.767895, global_step=145156, preemption_count=0, score=48999.767895, test/accuracy=0.621900, test/loss=1.856333, test/num_examples=10000, total_duration=50722.396921, train/accuracy=0.894551, train/loss=0.611395, validation/accuracy=0.745960, validation/loss=1.216866, validation/num_examples=50000
I0201 05:04:37.814224 139907754342144 logging_writer.py:48] [145200] global_step=145200, grad_norm=3.564178466796875, loss=2.702558994293213
I0201 05:05:11.481714 139907745949440 logging_writer.py:48] [145300] global_step=145300, grad_norm=3.5156729221343994, loss=2.6689789295196533
I0201 05:05:45.159920 139907754342144 logging_writer.py:48] [145400] global_step=145400, grad_norm=3.906067371368408, loss=2.6179676055908203
I0201 05:06:18.851260 139907745949440 logging_writer.py:48] [145500] global_step=145500, grad_norm=3.803487777709961, loss=2.684464454650879
I0201 05:06:52.659946 139907754342144 logging_writer.py:48] [145600] global_step=145600, grad_norm=3.847801923751831, loss=2.702631950378418
I0201 05:07:26.384337 139907745949440 logging_writer.py:48] [145700] global_step=145700, grad_norm=3.9594850540161133, loss=2.7152085304260254
I0201 05:08:00.063738 139907754342144 logging_writer.py:48] [145800] global_step=145800, grad_norm=3.6258325576782227, loss=2.6640784740448
I0201 05:08:33.817749 139907745949440 logging_writer.py:48] [145900] global_step=145900, grad_norm=3.8166933059692383, loss=2.6414220333099365
I0201 05:09:07.517544 139907754342144 logging_writer.py:48] [146000] global_step=146000, grad_norm=3.7297778129577637, loss=2.747321844100952
I0201 05:09:41.237887 139907745949440 logging_writer.py:48] [146100] global_step=146100, grad_norm=4.090029239654541, loss=2.6921048164367676
I0201 05:10:14.973596 139907754342144 logging_writer.py:48] [146200] global_step=146200, grad_norm=3.5346434116363525, loss=2.628710985183716
I0201 05:10:48.655171 139907745949440 logging_writer.py:48] [146300] global_step=146300, grad_norm=3.5867905616760254, loss=2.6768381595611572
I0201 05:11:22.362945 139907754342144 logging_writer.py:48] [146400] global_step=146400, grad_norm=3.7502307891845703, loss=2.6008522510528564
I0201 05:11:56.117180 139907745949440 logging_writer.py:48] [146500] global_step=146500, grad_norm=3.625521659851074, loss=2.695834159851074
I0201 05:12:29.833042 139907754342144 logging_writer.py:48] [146600] global_step=146600, grad_norm=3.6200761795043945, loss=2.570482015609741
I0201 05:12:52.946362 140070692116288 spec.py:321] Evaluating on the training split.
I0201 05:12:59.175145 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 05:13:08.247929 140070692116288 spec.py:349] Evaluating on the test split.
I0201 05:13:10.791951 140070692116288 submission_runner.py:408] Time since start: 51250.56s, 	Step: 146670, 	{'train/accuracy': 0.8981783986091614, 'train/loss': 0.6124516725540161, 'validation/accuracy': 0.748479962348938, 'validation/loss': 1.2270301580429077, 'validation/num_examples': 50000, 'test/accuracy': 0.6224000453948975, 'test/loss': 1.8555512428283691, 'test/num_examples': 10000, 'score': 49509.97759890556, 'total_duration': 51250.564635276794, 'accumulated_submission_time': 49509.97759890556, 'accumulated_eval_time': 1731.5386974811554, 'accumulated_logging_time': 3.894920825958252}
I0201 05:13:10.834882 139908425447168 logging_writer.py:48] [146670] accumulated_eval_time=1731.538697, accumulated_logging_time=3.894921, accumulated_submission_time=49509.977599, global_step=146670, preemption_count=0, score=49509.977599, test/accuracy=0.622400, test/loss=1.855551, test/num_examples=10000, total_duration=51250.564635, train/accuracy=0.898178, train/loss=0.612452, validation/accuracy=0.748480, validation/loss=1.227030, validation/num_examples=50000
I0201 05:13:21.261861 139908710635264 logging_writer.py:48] [146700] global_step=146700, grad_norm=3.8403525352478027, loss=2.6323254108428955
I0201 05:13:54.898820 139908425447168 logging_writer.py:48] [146800] global_step=146800, grad_norm=3.936901330947876, loss=2.704396963119507
I0201 05:14:28.597496 139908710635264 logging_writer.py:48] [146900] global_step=146900, grad_norm=3.917387008666992, loss=2.6853742599487305
I0201 05:15:02.327946 139908425447168 logging_writer.py:48] [147000] global_step=147000, grad_norm=3.7063984870910645, loss=2.573847770690918
I0201 05:15:36.086080 139908710635264 logging_writer.py:48] [147100] global_step=147100, grad_norm=3.7119524478912354, loss=2.6470143795013428
I0201 05:16:09.835920 139908425447168 logging_writer.py:48] [147200] global_step=147200, grad_norm=3.6939287185668945, loss=2.6574809551239014
I0201 05:16:43.552602 139908710635264 logging_writer.py:48] [147300] global_step=147300, grad_norm=3.602593421936035, loss=2.649308681488037
I0201 05:17:17.312919 139908425447168 logging_writer.py:48] [147400] global_step=147400, grad_norm=3.9448108673095703, loss=2.661742687225342
I0201 05:17:51.033431 139908710635264 logging_writer.py:48] [147500] global_step=147500, grad_norm=4.043767929077148, loss=2.7029924392700195
I0201 05:18:24.800658 139908425447168 logging_writer.py:48] [147600] global_step=147600, grad_norm=3.8122119903564453, loss=2.654301404953003
I0201 05:18:58.623451 139908710635264 logging_writer.py:48] [147700] global_step=147700, grad_norm=3.912295341491699, loss=2.665818691253662
I0201 05:19:32.350408 139908425447168 logging_writer.py:48] [147800] global_step=147800, grad_norm=3.7536447048187256, loss=2.6185271739959717
I0201 05:20:06.115025 139908710635264 logging_writer.py:48] [147900] global_step=147900, grad_norm=3.776916742324829, loss=2.6446237564086914
I0201 05:20:39.880009 139908425447168 logging_writer.py:48] [148000] global_step=148000, grad_norm=3.9337382316589355, loss=2.7084298133850098
I0201 05:21:13.604232 139908710635264 logging_writer.py:48] [148100] global_step=148100, grad_norm=3.8573060035705566, loss=2.6948392391204834
I0201 05:21:41.101455 140070692116288 spec.py:321] Evaluating on the training split.
I0201 05:21:47.280540 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 05:21:55.948397 140070692116288 spec.py:349] Evaluating on the test split.
I0201 05:21:58.450922 140070692116288 submission_runner.py:408] Time since start: 51778.22s, 	Step: 148183, 	{'train/accuracy': 0.8929169178009033, 'train/loss': 0.6327682137489319, 'validation/accuracy': 0.746679961681366, 'validation/loss': 1.2398338317871094, 'validation/num_examples': 50000, 'test/accuracy': 0.6222000122070312, 'test/loss': 1.8765029907226562, 'test/num_examples': 10000, 'score': 50020.18068480492, 'total_duration': 51778.22360420227, 'accumulated_submission_time': 50020.18068480492, 'accumulated_eval_time': 1748.8881268501282, 'accumulated_logging_time': 3.9477474689483643}
I0201 05:21:58.495376 139907745949440 logging_writer.py:48] [148183] accumulated_eval_time=1748.888127, accumulated_logging_time=3.947747, accumulated_submission_time=50020.180685, global_step=148183, preemption_count=0, score=50020.180685, test/accuracy=0.622200, test/loss=1.876503, test/num_examples=10000, total_duration=51778.223604, train/accuracy=0.892917, train/loss=0.632768, validation/accuracy=0.746680, validation/loss=1.239834, validation/num_examples=50000
I0201 05:22:04.561959 139907754342144 logging_writer.py:48] [148200] global_step=148200, grad_norm=3.5855326652526855, loss=2.627946138381958
I0201 05:22:38.207466 139907745949440 logging_writer.py:48] [148300] global_step=148300, grad_norm=3.978137731552124, loss=2.6367340087890625
I0201 05:23:11.880532 139907754342144 logging_writer.py:48] [148400] global_step=148400, grad_norm=3.8550214767456055, loss=2.666980266571045
I0201 05:23:45.568353 139907745949440 logging_writer.py:48] [148500] global_step=148500, grad_norm=3.6459801197052, loss=2.6156418323516846
I0201 05:24:19.248913 139907754342144 logging_writer.py:48] [148600] global_step=148600, grad_norm=3.7455859184265137, loss=2.6277987957000732
I0201 05:24:52.977805 139907745949440 logging_writer.py:48] [148700] global_step=148700, grad_norm=3.7139945030212402, loss=2.617475748062134
I0201 05:25:26.888097 139907754342144 logging_writer.py:48] [148800] global_step=148800, grad_norm=3.7884154319763184, loss=2.637969493865967
I0201 05:26:00.649447 139907745949440 logging_writer.py:48] [148900] global_step=148900, grad_norm=3.706387519836426, loss=2.6578876972198486
I0201 05:26:34.377661 139907754342144 logging_writer.py:48] [149000] global_step=149000, grad_norm=3.8557486534118652, loss=2.655111312866211
I0201 05:27:08.054750 139907745949440 logging_writer.py:48] [149100] global_step=149100, grad_norm=3.9845781326293945, loss=2.645620346069336
I0201 05:27:41.784241 139907754342144 logging_writer.py:48] [149200] global_step=149200, grad_norm=3.5334389209747314, loss=2.651155948638916
I0201 05:28:15.519364 139907745949440 logging_writer.py:48] [149300] global_step=149300, grad_norm=3.7373604774475098, loss=2.662735939025879
I0201 05:28:49.218520 139907754342144 logging_writer.py:48] [149400] global_step=149400, grad_norm=3.92168927192688, loss=2.652719020843506
I0201 05:29:22.946367 139907745949440 logging_writer.py:48] [149500] global_step=149500, grad_norm=3.8528859615325928, loss=2.6452860832214355
I0201 05:29:56.685561 139907754342144 logging_writer.py:48] [149600] global_step=149600, grad_norm=3.4743001461029053, loss=2.5855135917663574
I0201 05:30:28.497778 140070692116288 spec.py:321] Evaluating on the training split.
I0201 05:30:34.727217 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 05:30:43.374657 140070692116288 spec.py:349] Evaluating on the test split.
I0201 05:30:45.876619 140070692116288 submission_runner.py:408] Time since start: 52305.65s, 	Step: 149696, 	{'train/accuracy': 0.89652419090271, 'train/loss': 0.6026631593704224, 'validation/accuracy': 0.7511999607086182, 'validation/loss': 1.203833818435669, 'validation/num_examples': 50000, 'test/accuracy': 0.6230000257492065, 'test/loss': 1.8418248891830444, 'test/num_examples': 10000, 'score': 50530.1194422245, 'total_duration': 52305.64930200577, 'accumulated_submission_time': 50530.1194422245, 'accumulated_eval_time': 1766.2669341564178, 'accumulated_logging_time': 4.001991271972656}
I0201 05:30:45.922249 139907745949440 logging_writer.py:48] [149696] accumulated_eval_time=1766.266934, accumulated_logging_time=4.001991, accumulated_submission_time=50530.119442, global_step=149696, preemption_count=0, score=50530.119442, test/accuracy=0.623000, test/loss=1.841825, test/num_examples=10000, total_duration=52305.649302, train/accuracy=0.896524, train/loss=0.602663, validation/accuracy=0.751200, validation/loss=1.203834, validation/num_examples=50000
I0201 05:30:47.600434 139907754342144 logging_writer.py:48] [149700] global_step=149700, grad_norm=3.888343095779419, loss=2.6222798824310303
I0201 05:31:21.275181 139907745949440 logging_writer.py:48] [149800] global_step=149800, grad_norm=4.087747573852539, loss=2.6593539714813232
I0201 05:31:55.089184 139907754342144 logging_writer.py:48] [149900] global_step=149900, grad_norm=3.662741184234619, loss=2.6407506465911865
I0201 05:32:28.825165 139907745949440 logging_writer.py:48] [150000] global_step=150000, grad_norm=3.5443384647369385, loss=2.60720157623291
I0201 05:33:02.531409 139907754342144 logging_writer.py:48] [150100] global_step=150100, grad_norm=3.598398447036743, loss=2.5726747512817383
I0201 05:33:36.295286 139907745949440 logging_writer.py:48] [150200] global_step=150200, grad_norm=3.5549890995025635, loss=2.6893696784973145
I0201 05:34:09.959408 139907754342144 logging_writer.py:48] [150300] global_step=150300, grad_norm=3.5873374938964844, loss=2.6248106956481934
I0201 05:34:43.661854 139907745949440 logging_writer.py:48] [150400] global_step=150400, grad_norm=3.579742431640625, loss=2.631100654602051
I0201 05:35:17.383385 139907754342144 logging_writer.py:48] [150500] global_step=150500, grad_norm=3.9426963329315186, loss=2.6957898139953613
I0201 05:35:51.114324 139907745949440 logging_writer.py:48] [150600] global_step=150600, grad_norm=3.9012224674224854, loss=2.6289520263671875
I0201 05:36:24.873800 139907754342144 logging_writer.py:48] [150700] global_step=150700, grad_norm=3.9860105514526367, loss=2.702967882156372
I0201 05:36:58.595273 139907745949440 logging_writer.py:48] [150800] global_step=150800, grad_norm=3.7680656909942627, loss=2.6098761558532715
I0201 05:37:32.353927 139907754342144 logging_writer.py:48] [150900] global_step=150900, grad_norm=3.7310872077941895, loss=2.6306047439575195
I0201 05:38:06.184526 139907745949440 logging_writer.py:48] [151000] global_step=151000, grad_norm=3.8057620525360107, loss=2.5929973125457764
I0201 05:38:39.879349 139907754342144 logging_writer.py:48] [151100] global_step=151100, grad_norm=3.7026963233947754, loss=2.602048873901367
I0201 05:39:13.641910 139907745949440 logging_writer.py:48] [151200] global_step=151200, grad_norm=3.9128215312957764, loss=2.6678967475891113
I0201 05:39:16.153962 140070692116288 spec.py:321] Evaluating on the training split.
I0201 05:39:22.411855 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 05:39:31.219971 140070692116288 spec.py:349] Evaluating on the test split.
I0201 05:39:33.716740 140070692116288 submission_runner.py:408] Time since start: 52833.49s, 	Step: 151209, 	{'train/accuracy': 0.9214365482330322, 'train/loss': 0.5090224146842957, 'validation/accuracy': 0.7524799704551697, 'validation/loss': 1.1925246715545654, 'validation/num_examples': 50000, 'test/accuracy': 0.6265000104904175, 'test/loss': 1.8258957862854004, 'test/num_examples': 10000, 'score': 51040.28805708885, 'total_duration': 52833.489424705505, 'accumulated_submission_time': 51040.28805708885, 'accumulated_eval_time': 1783.8296740055084, 'accumulated_logging_time': 4.057976961135864}
I0201 05:39:33.758664 139907762734848 logging_writer.py:48] [151209] accumulated_eval_time=1783.829674, accumulated_logging_time=4.057977, accumulated_submission_time=51040.288057, global_step=151209, preemption_count=0, score=51040.288057, test/accuracy=0.626500, test/loss=1.825896, test/num_examples=10000, total_duration=52833.489425, train/accuracy=0.921437, train/loss=0.509022, validation/accuracy=0.752480, validation/loss=1.192525, validation/num_examples=50000
I0201 05:40:04.786273 139908719027968 logging_writer.py:48] [151300] global_step=151300, grad_norm=3.664358139038086, loss=2.6572415828704834
I0201 05:40:38.462995 139907762734848 logging_writer.py:48] [151400] global_step=151400, grad_norm=3.6080896854400635, loss=2.6176395416259766
I0201 05:41:12.169667 139908719027968 logging_writer.py:48] [151500] global_step=151500, grad_norm=3.736518621444702, loss=2.6577444076538086
I0201 05:41:45.841705 139907762734848 logging_writer.py:48] [151600] global_step=151600, grad_norm=3.729253053665161, loss=2.6505355834960938
I0201 05:42:19.567351 139908719027968 logging_writer.py:48] [151700] global_step=151700, grad_norm=3.6709320545196533, loss=2.607027053833008
I0201 05:42:53.233740 139907762734848 logging_writer.py:48] [151800] global_step=151800, grad_norm=4.021372318267822, loss=2.653118133544922
I0201 05:43:26.964352 139908719027968 logging_writer.py:48] [151900] global_step=151900, grad_norm=3.785010576248169, loss=2.611435890197754
I0201 05:44:00.783841 139907762734848 logging_writer.py:48] [152000] global_step=152000, grad_norm=3.7246973514556885, loss=2.6206588745117188
I0201 05:44:34.511734 139908719027968 logging_writer.py:48] [152100] global_step=152100, grad_norm=4.161803245544434, loss=2.6142585277557373
I0201 05:45:08.197634 139907762734848 logging_writer.py:48] [152200] global_step=152200, grad_norm=4.025290489196777, loss=2.608635902404785
I0201 05:45:41.974455 139908719027968 logging_writer.py:48] [152300] global_step=152300, grad_norm=3.7691550254821777, loss=2.5883913040161133
I0201 05:46:15.638600 139907762734848 logging_writer.py:48] [152400] global_step=152400, grad_norm=3.8936221599578857, loss=2.7144806385040283
I0201 05:46:49.342622 139908719027968 logging_writer.py:48] [152500] global_step=152500, grad_norm=4.079853057861328, loss=2.592508554458618
I0201 05:47:23.057114 139907762734848 logging_writer.py:48] [152600] global_step=152600, grad_norm=4.155495643615723, loss=2.6500117778778076
I0201 05:47:56.782747 139908719027968 logging_writer.py:48] [152700] global_step=152700, grad_norm=3.7572145462036133, loss=2.6000945568084717
I0201 05:48:04.006762 140070692116288 spec.py:321] Evaluating on the training split.
I0201 05:48:10.188925 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 05:48:18.894124 140070692116288 spec.py:349] Evaluating on the test split.
I0201 05:48:21.377482 140070692116288 submission_runner.py:408] Time since start: 53361.15s, 	Step: 152723, 	{'train/accuracy': 0.9169324040412903, 'train/loss': 0.526613175868988, 'validation/accuracy': 0.7518599629402161, 'validation/loss': 1.1986668109893799, 'validation/num_examples': 50000, 'test/accuracy': 0.6246000528335571, 'test/loss': 1.8432016372680664, 'test/num_examples': 10000, 'score': 51550.47252988815, 'total_duration': 53361.15016055107, 'accumulated_submission_time': 51550.47252988815, 'accumulated_eval_time': 1801.2003610134125, 'accumulated_logging_time': 4.109041690826416}
I0201 05:48:21.422598 139907745949440 logging_writer.py:48] [152723] accumulated_eval_time=1801.200361, accumulated_logging_time=4.109042, accumulated_submission_time=51550.472530, global_step=152723, preemption_count=0, score=51550.472530, test/accuracy=0.624600, test/loss=1.843202, test/num_examples=10000, total_duration=53361.150161, train/accuracy=0.916932, train/loss=0.526613, validation/accuracy=0.751860, validation/loss=1.198667, validation/num_examples=50000
I0201 05:48:47.700895 139907754342144 logging_writer.py:48] [152800] global_step=152800, grad_norm=3.7970657348632812, loss=2.6436753273010254
I0201 05:49:21.377918 139907745949440 logging_writer.py:48] [152900] global_step=152900, grad_norm=3.83128023147583, loss=2.645448684692383
I0201 05:49:55.124417 139907754342144 logging_writer.py:48] [153000] global_step=153000, grad_norm=3.9747297763824463, loss=2.618246555328369
I0201 05:50:28.961414 139907745949440 logging_writer.py:48] [153100] global_step=153100, grad_norm=3.846677303314209, loss=2.5831637382507324
I0201 05:51:02.658075 139907754342144 logging_writer.py:48] [153200] global_step=153200, grad_norm=4.106427192687988, loss=2.653594493865967
I0201 05:51:36.372246 139907745949440 logging_writer.py:48] [153300] global_step=153300, grad_norm=4.115781784057617, loss=2.6112301349639893
I0201 05:52:10.119194 139907754342144 logging_writer.py:48] [153400] global_step=153400, grad_norm=4.22220516204834, loss=2.6655337810516357
I0201 05:52:43.810385 139907745949440 logging_writer.py:48] [153500] global_step=153500, grad_norm=3.815228223800659, loss=2.6202609539031982
I0201 05:53:17.563200 139907754342144 logging_writer.py:48] [153600] global_step=153600, grad_norm=3.9523375034332275, loss=2.6542911529541016
I0201 05:53:51.298774 139907745949440 logging_writer.py:48] [153700] global_step=153700, grad_norm=3.8583455085754395, loss=2.6301398277282715
I0201 05:54:25.053895 139907754342144 logging_writer.py:48] [153800] global_step=153800, grad_norm=3.8974175453186035, loss=2.643219232559204
I0201 05:54:58.788911 139907745949440 logging_writer.py:48] [153900] global_step=153900, grad_norm=3.819425582885742, loss=2.6033499240875244
I0201 05:55:32.567885 139907754342144 logging_writer.py:48] [154000] global_step=154000, grad_norm=3.7995548248291016, loss=2.6836466789245605
I0201 05:56:06.284300 139907745949440 logging_writer.py:48] [154100] global_step=154100, grad_norm=4.0677289962768555, loss=2.6478919982910156
I0201 05:56:40.193090 139907754342144 logging_writer.py:48] [154200] global_step=154200, grad_norm=4.000971794128418, loss=2.6631717681884766
I0201 05:56:51.459112 140070692116288 spec.py:321] Evaluating on the training split.
I0201 05:56:57.766248 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 05:57:06.514169 140070692116288 spec.py:349] Evaluating on the test split.
I0201 05:57:09.088577 140070692116288 submission_runner.py:408] Time since start: 53888.86s, 	Step: 154235, 	{'train/accuracy': 0.9125677347183228, 'train/loss': 0.5550875067710876, 'validation/accuracy': 0.7528600096702576, 'validation/loss': 1.2101103067398071, 'validation/num_examples': 50000, 'test/accuracy': 0.6273000240325928, 'test/loss': 1.8422486782073975, 'test/num_examples': 10000, 'score': 52060.446773052216, 'total_duration': 53888.86126732826, 'accumulated_submission_time': 52060.446773052216, 'accumulated_eval_time': 1818.8298110961914, 'accumulated_logging_time': 4.163464784622192}
I0201 05:57:09.125413 139908710635264 logging_writer.py:48] [154235] accumulated_eval_time=1818.829811, accumulated_logging_time=4.163465, accumulated_submission_time=52060.446773, global_step=154235, preemption_count=0, score=52060.446773, test/accuracy=0.627300, test/loss=1.842249, test/num_examples=10000, total_duration=53888.861267, train/accuracy=0.912568, train/loss=0.555088, validation/accuracy=0.752860, validation/loss=1.210110, validation/num_examples=50000
I0201 05:57:31.367312 139908719027968 logging_writer.py:48] [154300] global_step=154300, grad_norm=3.873037338256836, loss=2.5754542350769043
I0201 05:58:05.110550 139908710635264 logging_writer.py:48] [154400] global_step=154400, grad_norm=3.6094000339508057, loss=2.6298046112060547
I0201 05:58:38.788650 139908719027968 logging_writer.py:48] [154500] global_step=154500, grad_norm=3.9410293102264404, loss=2.5915684700012207
I0201 05:59:12.540699 139908710635264 logging_writer.py:48] [154600] global_step=154600, grad_norm=3.62070631980896, loss=2.535050868988037
I0201 05:59:46.220541 139908719027968 logging_writer.py:48] [154700] global_step=154700, grad_norm=3.772399663925171, loss=2.597172737121582
I0201 06:00:19.939267 139908710635264 logging_writer.py:48] [154800] global_step=154800, grad_norm=3.8146018981933594, loss=2.581186532974243
I0201 06:00:53.606753 139908719027968 logging_writer.py:48] [154900] global_step=154900, grad_norm=3.675900936126709, loss=2.5481138229370117
I0201 06:01:27.356541 139908710635264 logging_writer.py:48] [155000] global_step=155000, grad_norm=4.11588716506958, loss=2.6107842922210693
I0201 06:02:01.059851 139908719027968 logging_writer.py:48] [155100] global_step=155100, grad_norm=3.94453501701355, loss=2.6116364002227783
I0201 06:02:34.962951 139908710635264 logging_writer.py:48] [155200] global_step=155200, grad_norm=3.9950504302978516, loss=2.5473573207855225
I0201 06:03:08.657926 139908719027968 logging_writer.py:48] [155300] global_step=155300, grad_norm=3.7664334774017334, loss=2.6268718242645264
I0201 06:03:42.377335 139908710635264 logging_writer.py:48] [155400] global_step=155400, grad_norm=4.172379493713379, loss=2.588186740875244
I0201 06:04:16.109947 139908719027968 logging_writer.py:48] [155500] global_step=155500, grad_norm=4.193735122680664, loss=2.6403961181640625
I0201 06:04:49.831473 139908710635264 logging_writer.py:48] [155600] global_step=155600, grad_norm=3.910463333129883, loss=2.6314609050750732
I0201 06:05:23.575077 139908719027968 logging_writer.py:48] [155700] global_step=155700, grad_norm=4.11352014541626, loss=2.6131982803344727
I0201 06:05:39.241027 140070692116288 spec.py:321] Evaluating on the training split.
I0201 06:05:45.488095 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 06:05:54.349408 140070692116288 spec.py:349] Evaluating on the test split.
I0201 06:05:56.889059 140070692116288 submission_runner.py:408] Time since start: 54416.66s, 	Step: 155748, 	{'train/accuracy': 0.9142418503761292, 'train/loss': 0.544491171836853, 'validation/accuracy': 0.7534799575805664, 'validation/loss': 1.1975823640823364, 'validation/num_examples': 50000, 'test/accuracy': 0.6277000308036804, 'test/loss': 1.8363298177719116, 'test/num_examples': 10000, 'score': 52570.49973154068, 'total_duration': 54416.66174650192, 'accumulated_submission_time': 52570.49973154068, 'accumulated_eval_time': 1836.4778108596802, 'accumulated_logging_time': 4.209648609161377}
I0201 06:05:56.933300 139907745949440 logging_writer.py:48] [155748] accumulated_eval_time=1836.477811, accumulated_logging_time=4.209649, accumulated_submission_time=52570.499732, global_step=155748, preemption_count=0, score=52570.499732, test/accuracy=0.627700, test/loss=1.836330, test/num_examples=10000, total_duration=54416.661747, train/accuracy=0.914242, train/loss=0.544491, validation/accuracy=0.753480, validation/loss=1.197582, validation/num_examples=50000
I0201 06:06:14.762546 139907754342144 logging_writer.py:48] [155800] global_step=155800, grad_norm=3.919962167739868, loss=2.58096981048584
I0201 06:06:48.419767 139907745949440 logging_writer.py:48] [155900] global_step=155900, grad_norm=3.8777589797973633, loss=2.588484525680542
I0201 06:07:22.116371 139907754342144 logging_writer.py:48] [156000] global_step=156000, grad_norm=3.880882740020752, loss=2.639038324356079
I0201 06:07:55.896079 139907745949440 logging_writer.py:48] [156100] global_step=156100, grad_norm=3.8300602436065674, loss=2.579342842102051
I0201 06:08:29.580620 139907754342144 logging_writer.py:48] [156200] global_step=156200, grad_norm=3.950474262237549, loss=2.565833568572998
I0201 06:09:03.376422 139907745949440 logging_writer.py:48] [156300] global_step=156300, grad_norm=3.7577531337738037, loss=2.5965235233306885
I0201 06:09:37.056959 139907754342144 logging_writer.py:48] [156400] global_step=156400, grad_norm=4.026468276977539, loss=2.6284019947052
I0201 06:10:10.832363 139907745949440 logging_writer.py:48] [156500] global_step=156500, grad_norm=3.7520954608917236, loss=2.569157123565674
I0201 06:10:44.530078 139907754342144 logging_writer.py:48] [156600] global_step=156600, grad_norm=4.174228191375732, loss=2.683563232421875
I0201 06:11:18.300828 139907745949440 logging_writer.py:48] [156700] global_step=156700, grad_norm=3.811793088912964, loss=2.6088621616363525
I0201 06:11:52.003378 139907754342144 logging_writer.py:48] [156800] global_step=156800, grad_norm=3.911332845687866, loss=2.6022469997406006
I0201 06:12:25.685197 139907745949440 logging_writer.py:48] [156900] global_step=156900, grad_norm=4.068520545959473, loss=2.64422345161438
I0201 06:12:59.439952 139907754342144 logging_writer.py:48] [157000] global_step=157000, grad_norm=4.0504536628723145, loss=2.6328847408294678
I0201 06:13:33.164207 139907745949440 logging_writer.py:48] [157100] global_step=157100, grad_norm=3.9598426818847656, loss=2.6088638305664062
I0201 06:14:06.905130 139907754342144 logging_writer.py:48] [157200] global_step=157200, grad_norm=3.726015567779541, loss=2.5737383365631104
I0201 06:14:26.996118 140070692116288 spec.py:321] Evaluating on the training split.
I0201 06:14:33.824752 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 06:14:42.501807 140070692116288 spec.py:349] Evaluating on the test split.
I0201 06:14:44.996799 140070692116288 submission_runner.py:408] Time since start: 54944.77s, 	Step: 157261, 	{'train/accuracy': 0.9153977632522583, 'train/loss': 0.5491384267807007, 'validation/accuracy': 0.7535799741744995, 'validation/loss': 1.2018764019012451, 'validation/num_examples': 50000, 'test/accuracy': 0.6292000412940979, 'test/loss': 1.8402178287506104, 'test/num_examples': 10000, 'score': 53080.501549720764, 'total_duration': 54944.76948451996, 'accumulated_submission_time': 53080.501549720764, 'accumulated_eval_time': 1854.4784564971924, 'accumulated_logging_time': 4.263177156448364}
I0201 06:14:45.040270 139908719027968 logging_writer.py:48] [157261] accumulated_eval_time=1854.478456, accumulated_logging_time=4.263177, accumulated_submission_time=53080.501550, global_step=157261, preemption_count=0, score=53080.501550, test/accuracy=0.629200, test/loss=1.840218, test/num_examples=10000, total_duration=54944.769485, train/accuracy=0.915398, train/loss=0.549138, validation/accuracy=0.753580, validation/loss=1.201876, validation/num_examples=50000
I0201 06:14:58.517081 139908727420672 logging_writer.py:48] [157300] global_step=157300, grad_norm=4.084689617156982, loss=2.5772757530212402
I0201 06:15:32.438249 139908719027968 logging_writer.py:48] [157400] global_step=157400, grad_norm=3.7221646308898926, loss=2.618596076965332
I0201 06:16:06.154264 139908727420672 logging_writer.py:48] [157500] global_step=157500, grad_norm=4.0218424797058105, loss=2.6071319580078125
I0201 06:16:39.818496 139908719027968 logging_writer.py:48] [157600] global_step=157600, grad_norm=3.655474901199341, loss=2.586451768875122
I0201 06:17:13.504826 139908727420672 logging_writer.py:48] [157700] global_step=157700, grad_norm=3.688436508178711, loss=2.548583507537842
I0201 06:17:47.186298 139908719027968 logging_writer.py:48] [157800] global_step=157800, grad_norm=4.28720760345459, loss=2.6121442317962646
I0201 06:18:20.896749 139908727420672 logging_writer.py:48] [157900] global_step=157900, grad_norm=3.8680388927459717, loss=2.6264283657073975
I0201 06:18:54.658907 139908719027968 logging_writer.py:48] [158000] global_step=158000, grad_norm=4.14276123046875, loss=2.617739200592041
I0201 06:19:28.370422 139908727420672 logging_writer.py:48] [158100] global_step=158100, grad_norm=4.088721752166748, loss=2.6490063667297363
I0201 06:20:02.072791 139908719027968 logging_writer.py:48] [158200] global_step=158200, grad_norm=3.875365972518921, loss=2.628183126449585
I0201 06:20:35.837759 139908727420672 logging_writer.py:48] [158300] global_step=158300, grad_norm=3.9577417373657227, loss=2.576045036315918
I0201 06:21:09.573341 139908719027968 logging_writer.py:48] [158400] global_step=158400, grad_norm=3.9060447216033936, loss=2.5638468265533447
I0201 06:21:43.460468 139908727420672 logging_writer.py:48] [158500] global_step=158500, grad_norm=3.9969561100006104, loss=2.647843599319458
I0201 06:22:17.225202 139908719027968 logging_writer.py:48] [158600] global_step=158600, grad_norm=4.107706546783447, loss=2.5977346897125244
I0201 06:22:50.950335 139908727420672 logging_writer.py:48] [158700] global_step=158700, grad_norm=3.8541388511657715, loss=2.570549964904785
I0201 06:23:15.053069 140070692116288 spec.py:321] Evaluating on the training split.
I0201 06:23:21.297410 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 06:23:30.108390 140070692116288 spec.py:349] Evaluating on the test split.
I0201 06:23:32.520774 140070692116288 submission_runner.py:408] Time since start: 55472.29s, 	Step: 158773, 	{'train/accuracy': 0.9156568646430969, 'train/loss': 0.5355087518692017, 'validation/accuracy': 0.7560999989509583, 'validation/loss': 1.1917190551757812, 'validation/num_examples': 50000, 'test/accuracy': 0.6296000480651855, 'test/loss': 1.8320457935333252, 'test/num_examples': 10000, 'score': 53590.45152449608, 'total_duration': 55472.293454647064, 'accumulated_submission_time': 53590.45152449608, 'accumulated_eval_time': 1871.9461405277252, 'accumulated_logging_time': 4.316278457641602}
I0201 06:23:32.573278 139907745949440 logging_writer.py:48] [158773] accumulated_eval_time=1871.946141, accumulated_logging_time=4.316278, accumulated_submission_time=53590.451524, global_step=158773, preemption_count=0, score=53590.451524, test/accuracy=0.629600, test/loss=1.832046, test/num_examples=10000, total_duration=55472.293455, train/accuracy=0.915657, train/loss=0.535509, validation/accuracy=0.756100, validation/loss=1.191719, validation/num_examples=50000
I0201 06:23:41.999445 139907754342144 logging_writer.py:48] [158800] global_step=158800, grad_norm=3.903646230697632, loss=2.6245603561401367
I0201 06:24:15.700386 139907745949440 logging_writer.py:48] [158900] global_step=158900, grad_norm=4.016567230224609, loss=2.5934548377990723
I0201 06:24:49.444748 139907754342144 logging_writer.py:48] [159000] global_step=159000, grad_norm=4.268784999847412, loss=2.598057746887207
I0201 06:25:23.130208 139907745949440 logging_writer.py:48] [159100] global_step=159100, grad_norm=4.0707502365112305, loss=2.55907940864563
I0201 06:25:56.896279 139907754342144 logging_writer.py:48] [159200] global_step=159200, grad_norm=3.8225104808807373, loss=2.62638258934021
I0201 06:26:30.583498 139907745949440 logging_writer.py:48] [159300] global_step=159300, grad_norm=3.73411226272583, loss=2.543762683868408
I0201 06:27:04.361846 139907754342144 logging_writer.py:48] [159400] global_step=159400, grad_norm=3.729231357574463, loss=2.534498453140259
I0201 06:27:38.188457 139907745949440 logging_writer.py:48] [159500] global_step=159500, grad_norm=4.412643909454346, loss=2.635577917098999
I0201 06:28:11.916419 139907754342144 logging_writer.py:48] [159600] global_step=159600, grad_norm=3.842280864715576, loss=2.591732978820801
I0201 06:28:45.626219 139907745949440 logging_writer.py:48] [159700] global_step=159700, grad_norm=3.846728563308716, loss=2.5289738178253174
I0201 06:29:19.402252 139907754342144 logging_writer.py:48] [159800] global_step=159800, grad_norm=4.046579360961914, loss=2.5250303745269775
I0201 06:29:53.159924 139907745949440 logging_writer.py:48] [159900] global_step=159900, grad_norm=4.189145565032959, loss=2.588902711868286
I0201 06:30:26.879584 139907754342144 logging_writer.py:48] [160000] global_step=160000, grad_norm=3.776986598968506, loss=2.5960545539855957
I0201 06:31:00.592977 139907745949440 logging_writer.py:48] [160100] global_step=160100, grad_norm=4.047554016113281, loss=2.6253793239593506
I0201 06:31:34.349837 139907754342144 logging_writer.py:48] [160200] global_step=160200, grad_norm=4.071959972381592, loss=2.617076873779297
I0201 06:32:02.804372 140070692116288 spec.py:321] Evaluating on the training split.
I0201 06:32:09.029969 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 06:32:17.760559 140070692116288 spec.py:349] Evaluating on the test split.
I0201 06:32:20.252586 140070692116288 submission_runner.py:408] Time since start: 56000.03s, 	Step: 160286, 	{'train/accuracy': 0.9319993257522583, 'train/loss': 0.4888227581977844, 'validation/accuracy': 0.7568399906158447, 'validation/loss': 1.1966514587402344, 'validation/num_examples': 50000, 'test/accuracy': 0.6318000555038452, 'test/loss': 1.8300745487213135, 'test/num_examples': 10000, 'score': 54100.61996221542, 'total_duration': 56000.02527117729, 'accumulated_submission_time': 54100.61996221542, 'accumulated_eval_time': 1889.3943195343018, 'accumulated_logging_time': 4.3780577182769775}
I0201 06:32:20.297417 139908425447168 logging_writer.py:48] [160286] accumulated_eval_time=1889.394320, accumulated_logging_time=4.378058, accumulated_submission_time=54100.619962, global_step=160286, preemption_count=0, score=54100.619962, test/accuracy=0.631800, test/loss=1.830075, test/num_examples=10000, total_duration=56000.025271, train/accuracy=0.931999, train/loss=0.488823, validation/accuracy=0.756840, validation/loss=1.196651, validation/num_examples=50000
I0201 06:32:25.374309 139908710635264 logging_writer.py:48] [160300] global_step=160300, grad_norm=3.974489688873291, loss=2.5678205490112305
I0201 06:32:59.076627 139908425447168 logging_writer.py:48] [160400] global_step=160400, grad_norm=4.200191974639893, loss=2.632794141769409
I0201 06:33:32.751653 139908710635264 logging_writer.py:48] [160500] global_step=160500, grad_norm=4.273584365844727, loss=2.5951921939849854
I0201 06:34:06.547067 139908425447168 logging_writer.py:48] [160600] global_step=160600, grad_norm=4.182641506195068, loss=2.561262369155884
I0201 06:34:40.245263 139908710635264 logging_writer.py:48] [160700] global_step=160700, grad_norm=4.117660045623779, loss=2.6229803562164307
I0201 06:35:13.927422 139908425447168 logging_writer.py:48] [160800] global_step=160800, grad_norm=4.102447032928467, loss=2.642031669616699
I0201 06:35:47.696456 139908710635264 logging_writer.py:48] [160900] global_step=160900, grad_norm=3.884673833847046, loss=2.584125518798828
I0201 06:36:21.402877 139908425447168 logging_writer.py:48] [161000] global_step=161000, grad_norm=3.958951234817505, loss=2.598207473754883
I0201 06:36:55.069558 139908710635264 logging_writer.py:48] [161100] global_step=161100, grad_norm=4.072776794433594, loss=2.6249618530273438
I0201 06:37:28.764131 139908425447168 logging_writer.py:48] [161200] global_step=161200, grad_norm=3.828298807144165, loss=2.5065529346466064
I0201 06:38:02.527474 139908710635264 logging_writer.py:48] [161300] global_step=161300, grad_norm=3.814272403717041, loss=2.5304744243621826
I0201 06:38:36.241494 139908425447168 logging_writer.py:48] [161400] global_step=161400, grad_norm=3.9479851722717285, loss=2.5478813648223877
I0201 06:39:09.984441 139908710635264 logging_writer.py:48] [161500] global_step=161500, grad_norm=4.028604984283447, loss=2.5551159381866455
I0201 06:39:43.691872 139908425447168 logging_writer.py:48] [161600] global_step=161600, grad_norm=3.8565151691436768, loss=2.5953640937805176
I0201 06:40:17.541762 139908710635264 logging_writer.py:48] [161700] global_step=161700, grad_norm=4.186524391174316, loss=2.5466277599334717
I0201 06:40:50.407100 140070692116288 spec.py:321] Evaluating on the training split.
I0201 06:40:56.587962 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 06:41:05.305583 140070692116288 spec.py:349] Evaluating on the test split.
I0201 06:41:07.835062 140070692116288 submission_runner.py:408] Time since start: 56527.61s, 	Step: 161799, 	{'train/accuracy': 0.9300462007522583, 'train/loss': 0.48804351687431335, 'validation/accuracy': 0.7581999897956848, 'validation/loss': 1.1818283796310425, 'validation/num_examples': 50000, 'test/accuracy': 0.6349000334739685, 'test/loss': 1.818547010421753, 'test/num_examples': 10000, 'score': 54610.668021678925, 'total_duration': 56527.60770535469, 'accumulated_submission_time': 54610.668021678925, 'accumulated_eval_time': 1906.8222138881683, 'accumulated_logging_time': 4.4321160316467285}
I0201 06:41:07.899302 139907754342144 logging_writer.py:48] [161799] accumulated_eval_time=1906.822214, accumulated_logging_time=4.432116, accumulated_submission_time=54610.668022, global_step=161799, preemption_count=0, score=54610.668022, test/accuracy=0.634900, test/loss=1.818547, test/num_examples=10000, total_duration=56527.607705, train/accuracy=0.930046, train/loss=0.488044, validation/accuracy=0.758200, validation/loss=1.181828, validation/num_examples=50000
I0201 06:41:08.624917 139907762734848 logging_writer.py:48] [161800] global_step=161800, grad_norm=3.962052583694458, loss=2.513493299484253
I0201 06:41:42.409945 139907754342144 logging_writer.py:48] [161900] global_step=161900, grad_norm=3.935204029083252, loss=2.562908172607422
I0201 06:42:16.063710 139907762734848 logging_writer.py:48] [162000] global_step=162000, grad_norm=3.5554933547973633, loss=2.515166997909546
I0201 06:42:49.771578 139907754342144 logging_writer.py:48] [162100] global_step=162100, grad_norm=3.86116361618042, loss=2.550954818725586
I0201 06:43:23.524147 139907762734848 logging_writer.py:48] [162200] global_step=162200, grad_norm=4.149171352386475, loss=2.584336757659912
I0201 06:43:57.271040 139907754342144 logging_writer.py:48] [162300] global_step=162300, grad_norm=4.000607967376709, loss=2.583763837814331
I0201 06:44:31.028072 139907762734848 logging_writer.py:48] [162400] global_step=162400, grad_norm=3.8032190799713135, loss=2.5547847747802734
I0201 06:45:04.794428 139907754342144 logging_writer.py:48] [162500] global_step=162500, grad_norm=4.264333248138428, loss=2.578299045562744
I0201 06:45:38.502100 139907762734848 logging_writer.py:48] [162600] global_step=162600, grad_norm=4.095882415771484, loss=2.5663907527923584
I0201 06:46:12.302616 139907754342144 logging_writer.py:48] [162700] global_step=162700, grad_norm=4.010540962219238, loss=2.595367670059204
I0201 06:46:46.124722 139907762734848 logging_writer.py:48] [162800] global_step=162800, grad_norm=3.9782371520996094, loss=2.612635374069214
I0201 06:47:19.846572 139907754342144 logging_writer.py:48] [162900] global_step=162900, grad_norm=4.258326530456543, loss=2.6270956993103027
I0201 06:47:53.619602 139907762734848 logging_writer.py:48] [163000] global_step=163000, grad_norm=4.08595085144043, loss=2.593273401260376
I0201 06:48:27.318911 139907754342144 logging_writer.py:48] [163100] global_step=163100, grad_norm=4.0923848152160645, loss=2.6133673191070557
I0201 06:49:01.000553 139907762734848 logging_writer.py:48] [163200] global_step=163200, grad_norm=4.508001804351807, loss=2.5912797451019287
I0201 06:49:34.807586 139907754342144 logging_writer.py:48] [163300] global_step=163300, grad_norm=3.627511739730835, loss=2.5175318717956543
I0201 06:49:37.994756 140070692116288 spec.py:321] Evaluating on the training split.
I0201 06:49:44.214627 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 06:49:52.854304 140070692116288 spec.py:349] Evaluating on the test split.
I0201 06:49:55.298631 140070692116288 submission_runner.py:408] Time since start: 57055.07s, 	Step: 163311, 	{'train/accuracy': 0.9286909699440002, 'train/loss': 0.4972872734069824, 'validation/accuracy': 0.7575799822807312, 'validation/loss': 1.1837843656539917, 'validation/num_examples': 50000, 'test/accuracy': 0.6351000070571899, 'test/loss': 1.817732334136963, 'test/num_examples': 10000, 'score': 55120.694390535355, 'total_duration': 57055.07130908966, 'accumulated_submission_time': 55120.694390535355, 'accumulated_eval_time': 1924.1260414123535, 'accumulated_logging_time': 4.511041164398193}
I0201 06:49:55.343327 139907729164032 logging_writer.py:48] [163311] accumulated_eval_time=1924.126041, accumulated_logging_time=4.511041, accumulated_submission_time=55120.694391, global_step=163311, preemption_count=0, score=55120.694391, test/accuracy=0.635100, test/loss=1.817732, test/num_examples=10000, total_duration=57055.071309, train/accuracy=0.928691, train/loss=0.497287, validation/accuracy=0.757580, validation/loss=1.183784, validation/num_examples=50000
I0201 06:50:25.721191 139908425447168 logging_writer.py:48] [163400] global_step=163400, grad_norm=4.073987007141113, loss=2.587679386138916
I0201 06:50:59.438003 139907729164032 logging_writer.py:48] [163500] global_step=163500, grad_norm=3.796593189239502, loss=2.5181357860565186
I0201 06:51:33.152863 139908425447168 logging_writer.py:48] [163600] global_step=163600, grad_norm=4.053004741668701, loss=2.572577476501465
I0201 06:52:06.900109 139907729164032 logging_writer.py:48] [163700] global_step=163700, grad_norm=4.239174842834473, loss=2.5763611793518066
I0201 06:52:40.688861 139908425447168 logging_writer.py:48] [163800] global_step=163800, grad_norm=3.9506607055664062, loss=2.5425124168395996
I0201 06:53:14.400989 139907729164032 logging_writer.py:48] [163900] global_step=163900, grad_norm=4.302896022796631, loss=2.619767427444458
I0201 06:53:48.161525 139908425447168 logging_writer.py:48] [164000] global_step=164000, grad_norm=3.913691520690918, loss=2.549081802368164
I0201 06:54:21.878883 139907729164032 logging_writer.py:48] [164100] global_step=164100, grad_norm=4.307253360748291, loss=2.6227784156799316
I0201 06:54:55.565593 139908425447168 logging_writer.py:48] [164200] global_step=164200, grad_norm=4.086269378662109, loss=2.5939040184020996
I0201 06:55:29.310457 139907729164032 logging_writer.py:48] [164300] global_step=164300, grad_norm=4.108467102050781, loss=2.5610790252685547
I0201 06:56:03.037948 139908425447168 logging_writer.py:48] [164400] global_step=164400, grad_norm=4.321875095367432, loss=2.6082234382629395
I0201 06:56:36.763721 139907729164032 logging_writer.py:48] [164500] global_step=164500, grad_norm=4.233027935028076, loss=2.605463743209839
I0201 06:57:10.455950 139908425447168 logging_writer.py:48] [164600] global_step=164600, grad_norm=3.9546265602111816, loss=2.5298635959625244
I0201 06:57:44.160748 139907729164032 logging_writer.py:48] [164700] global_step=164700, grad_norm=3.9274322986602783, loss=2.5670835971832275
I0201 06:58:17.935659 139908425447168 logging_writer.py:48] [164800] global_step=164800, grad_norm=3.8052611351013184, loss=2.455040693283081
I0201 06:58:25.519608 140070692116288 spec.py:321] Evaluating on the training split.
I0201 06:58:31.915374 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 06:58:40.623432 140070692116288 spec.py:349] Evaluating on the test split.
I0201 06:58:43.119326 140070692116288 submission_runner.py:408] Time since start: 57582.89s, 	Step: 164824, 	{'train/accuracy': 0.9278140664100647, 'train/loss': 0.496550053358078, 'validation/accuracy': 0.7587599754333496, 'validation/loss': 1.1828312873840332, 'validation/num_examples': 50000, 'test/accuracy': 0.6360000371932983, 'test/loss': 1.8208544254302979, 'test/num_examples': 10000, 'score': 55630.80797767639, 'total_duration': 57582.89201283455, 'accumulated_submission_time': 55630.80797767639, 'accumulated_eval_time': 1941.7257256507874, 'accumulated_logging_time': 4.56522536277771}
I0201 06:58:43.165249 139907729164032 logging_writer.py:48] [164824] accumulated_eval_time=1941.725726, accumulated_logging_time=4.565225, accumulated_submission_time=55630.807978, global_step=164824, preemption_count=0, score=55630.807978, test/accuracy=0.636000, test/loss=1.820854, test/num_examples=10000, total_duration=57582.892013, train/accuracy=0.927814, train/loss=0.496550, validation/accuracy=0.758760, validation/loss=1.182831, validation/num_examples=50000
I0201 06:59:09.252692 139907745949440 logging_writer.py:48] [164900] global_step=164900, grad_norm=4.223896503448486, loss=2.5729174613952637
I0201 06:59:42.965761 139907729164032 logging_writer.py:48] [165000] global_step=165000, grad_norm=4.0352983474731445, loss=2.556182622909546
I0201 07:00:16.697626 139907745949440 logging_writer.py:48] [165100] global_step=165100, grad_norm=3.954488515853882, loss=2.5467119216918945
I0201 07:00:50.417145 139907729164032 logging_writer.py:48] [165200] global_step=165200, grad_norm=4.5264739990234375, loss=2.600473403930664
I0201 07:01:24.177233 139907745949440 logging_writer.py:48] [165300] global_step=165300, grad_norm=4.085918426513672, loss=2.5741662979125977
I0201 07:01:57.893199 139907729164032 logging_writer.py:48] [165400] global_step=165400, grad_norm=3.8634023666381836, loss=2.529264450073242
I0201 07:02:31.628360 139907745949440 logging_writer.py:48] [165500] global_step=165500, grad_norm=4.145786762237549, loss=2.559436082839966
I0201 07:03:05.371617 139907729164032 logging_writer.py:48] [165600] global_step=165600, grad_norm=4.106339931488037, loss=2.503909111022949
I0201 07:03:39.125382 139907745949440 logging_writer.py:48] [165700] global_step=165700, grad_norm=3.948491096496582, loss=2.5631816387176514
I0201 07:04:12.837180 139907729164032 logging_writer.py:48] [165800] global_step=165800, grad_norm=3.933318614959717, loss=2.5823984146118164
I0201 07:04:46.508753 139907745949440 logging_writer.py:48] [165900] global_step=165900, grad_norm=3.917989730834961, loss=2.546752691268921
I0201 07:05:20.339778 139907729164032 logging_writer.py:48] [166000] global_step=166000, grad_norm=4.1369171142578125, loss=2.571678400039673
I0201 07:05:54.071094 139907745949440 logging_writer.py:48] [166100] global_step=166100, grad_norm=4.059383392333984, loss=2.4813239574432373
I0201 07:06:27.777538 139907729164032 logging_writer.py:48] [166200] global_step=166200, grad_norm=4.7088541984558105, loss=2.6216578483581543
I0201 07:07:01.528845 139907745949440 logging_writer.py:48] [166300] global_step=166300, grad_norm=3.98884916305542, loss=2.564704656600952
I0201 07:07:13.153760 140070692116288 spec.py:321] Evaluating on the training split.
I0201 07:07:19.364810 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 07:07:28.233043 140070692116288 spec.py:349] Evaluating on the test split.
I0201 07:07:30.808496 140070692116288 submission_runner.py:408] Time since start: 58110.58s, 	Step: 166336, 	{'train/accuracy': 0.9301857352256775, 'train/loss': 0.48756131529808044, 'validation/accuracy': 0.7597599625587463, 'validation/loss': 1.1802102327346802, 'validation/num_examples': 50000, 'test/accuracy': 0.6350000500679016, 'test/loss': 1.8172756433486938, 'test/num_examples': 10000, 'score': 56140.73363828659, 'total_duration': 58110.581184625626, 'accumulated_submission_time': 56140.73363828659, 'accumulated_eval_time': 1959.3804275989532, 'accumulated_logging_time': 4.620617151260376}
I0201 07:07:30.857582 139908710635264 logging_writer.py:48] [166336] accumulated_eval_time=1959.380428, accumulated_logging_time=4.620617, accumulated_submission_time=56140.733638, global_step=166336, preemption_count=0, score=56140.733638, test/accuracy=0.635000, test/loss=1.817276, test/num_examples=10000, total_duration=58110.581185, train/accuracy=0.930186, train/loss=0.487561, validation/accuracy=0.759760, validation/loss=1.180210, validation/num_examples=50000
I0201 07:07:52.773725 139908719027968 logging_writer.py:48] [166400] global_step=166400, grad_norm=4.040491580963135, loss=2.5587587356567383
I0201 07:08:26.483170 139908710635264 logging_writer.py:48] [166500] global_step=166500, grad_norm=3.9812815189361572, loss=2.550468683242798
I0201 07:09:00.180990 139908719027968 logging_writer.py:48] [166600] global_step=166600, grad_norm=4.489357948303223, loss=2.557612657546997
I0201 07:09:33.877793 139908710635264 logging_writer.py:48] [166700] global_step=166700, grad_norm=4.175983428955078, loss=2.516237735748291
I0201 07:10:07.566945 139908719027968 logging_writer.py:48] [166800] global_step=166800, grad_norm=4.116952419281006, loss=2.565646171569824
I0201 07:10:41.289326 139908710635264 logging_writer.py:48] [166900] global_step=166900, grad_norm=3.872323989868164, loss=2.5328359603881836
I0201 07:11:15.214760 139908719027968 logging_writer.py:48] [167000] global_step=167000, grad_norm=3.975135326385498, loss=2.544410467147827
I0201 07:11:49.007463 139908710635264 logging_writer.py:48] [167100] global_step=167100, grad_norm=4.14160680770874, loss=2.555030107498169
I0201 07:12:22.704542 139908719027968 logging_writer.py:48] [167200] global_step=167200, grad_norm=4.002877235412598, loss=2.5344247817993164
I0201 07:12:56.418275 139908710635264 logging_writer.py:48] [167300] global_step=167300, grad_norm=3.907085657119751, loss=2.539865732192993
I0201 07:13:30.110961 139908719027968 logging_writer.py:48] [167400] global_step=167400, grad_norm=3.849604845046997, loss=2.4945671558380127
I0201 07:14:03.892238 139908710635264 logging_writer.py:48] [167500] global_step=167500, grad_norm=3.9806807041168213, loss=2.5363662242889404
I0201 07:14:37.592561 139908719027968 logging_writer.py:48] [167600] global_step=167600, grad_norm=4.278708457946777, loss=2.5118346214294434
I0201 07:15:11.368313 139908710635264 logging_writer.py:48] [167700] global_step=167700, grad_norm=3.923860549926758, loss=2.5322766304016113
I0201 07:15:45.083587 139908719027968 logging_writer.py:48] [167800] global_step=167800, grad_norm=4.294153213500977, loss=2.5660996437072754
I0201 07:16:01.111397 140070692116288 spec.py:321] Evaluating on the training split.
I0201 07:16:07.502148 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 07:16:16.285903 140070692116288 spec.py:349] Evaluating on the test split.
I0201 07:16:18.786509 140070692116288 submission_runner.py:408] Time since start: 58638.56s, 	Step: 167849, 	{'train/accuracy': 0.9341716766357422, 'train/loss': 0.48081260919570923, 'validation/accuracy': 0.7604199647903442, 'validation/loss': 1.180977702140808, 'validation/num_examples': 50000, 'test/accuracy': 0.6374000310897827, 'test/loss': 1.8157609701156616, 'test/num_examples': 10000, 'score': 56650.924723148346, 'total_duration': 58638.559196949005, 'accumulated_submission_time': 56650.924723148346, 'accumulated_eval_time': 1977.0555260181427, 'accumulated_logging_time': 4.680642366409302}
I0201 07:16:18.836937 139907754342144 logging_writer.py:48] [167849] accumulated_eval_time=1977.055526, accumulated_logging_time=4.680642, accumulated_submission_time=56650.924723, global_step=167849, preemption_count=0, score=56650.924723, test/accuracy=0.637400, test/loss=1.815761, test/num_examples=10000, total_duration=58638.559197, train/accuracy=0.934172, train/loss=0.480813, validation/accuracy=0.760420, validation/loss=1.180978, validation/num_examples=50000
I0201 07:16:36.336586 139907762734848 logging_writer.py:48] [167900] global_step=167900, grad_norm=4.656649112701416, loss=2.629281759262085
I0201 07:17:09.992126 139907754342144 logging_writer.py:48] [168000] global_step=168000, grad_norm=4.429194927215576, loss=2.5674729347229004
I0201 07:17:43.791606 139907762734848 logging_writer.py:48] [168100] global_step=168100, grad_norm=4.007958889007568, loss=2.5422511100769043
I0201 07:18:17.539917 139907754342144 logging_writer.py:48] [168200] global_step=168200, grad_norm=4.035110950469971, loss=2.5140609741210938
I0201 07:18:51.286372 139907762734848 logging_writer.py:48] [168300] global_step=168300, grad_norm=4.435131549835205, loss=2.550663709640503
I0201 07:19:24.996251 139907754342144 logging_writer.py:48] [168400] global_step=168400, grad_norm=4.0516767501831055, loss=2.5114338397979736
I0201 07:19:58.775057 139907762734848 logging_writer.py:48] [168500] global_step=168500, grad_norm=4.438083648681641, loss=2.529789447784424
I0201 07:20:32.427262 139907754342144 logging_writer.py:48] [168600] global_step=168600, grad_norm=4.321619987487793, loss=2.544325828552246
I0201 07:21:06.152535 139907762734848 logging_writer.py:48] [168700] global_step=168700, grad_norm=3.8756542205810547, loss=2.5466396808624268
I0201 07:21:39.906131 139907754342144 logging_writer.py:48] [168800] global_step=168800, grad_norm=4.273745536804199, loss=2.592658042907715
I0201 07:22:13.627545 139907762734848 logging_writer.py:48] [168900] global_step=168900, grad_norm=4.189035892486572, loss=2.5366177558898926
I0201 07:22:47.384708 139907754342144 logging_writer.py:48] [169000] global_step=169000, grad_norm=4.130645275115967, loss=2.5622739791870117
I0201 07:23:21.115458 139907762734848 logging_writer.py:48] [169100] global_step=169100, grad_norm=4.087011814117432, loss=2.555741310119629
I0201 07:23:54.910220 139907754342144 logging_writer.py:48] [169200] global_step=169200, grad_norm=3.9597597122192383, loss=2.5635271072387695
I0201 07:24:28.665420 139907762734848 logging_writer.py:48] [169300] global_step=169300, grad_norm=4.252098083496094, loss=2.4944677352905273
I0201 07:24:49.031694 140070692116288 spec.py:321] Evaluating on the training split.
I0201 07:24:55.261693 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 07:25:04.129741 140070692116288 spec.py:349] Evaluating on the test split.
I0201 07:25:06.656025 140070692116288 submission_runner.py:408] Time since start: 59166.43s, 	Step: 169362, 	{'train/accuracy': 0.9384565949440002, 'train/loss': 0.4624026119709015, 'validation/accuracy': 0.7603200078010559, 'validation/loss': 1.1795930862426758, 'validation/num_examples': 50000, 'test/accuracy': 0.6372000575065613, 'test/loss': 1.8115358352661133, 'test/num_examples': 10000, 'score': 57161.05779337883, 'total_duration': 59166.4287109375, 'accumulated_submission_time': 57161.05779337883, 'accumulated_eval_time': 1994.6798272132874, 'accumulated_logging_time': 4.739821195602417}
I0201 07:25:06.703317 139908710635264 logging_writer.py:48] [169362] accumulated_eval_time=1994.679827, accumulated_logging_time=4.739821, accumulated_submission_time=57161.057793, global_step=169362, preemption_count=0, score=57161.057793, test/accuracy=0.637200, test/loss=1.811536, test/num_examples=10000, total_duration=59166.428711, train/accuracy=0.938457, train/loss=0.462403, validation/accuracy=0.760320, validation/loss=1.179593, validation/num_examples=50000
I0201 07:25:19.960803 139908719027968 logging_writer.py:48] [169400] global_step=169400, grad_norm=4.195857524871826, loss=2.4982075691223145
I0201 07:25:53.631770 139908710635264 logging_writer.py:48] [169500] global_step=169500, grad_norm=4.251157283782959, loss=2.6034746170043945
I0201 07:26:27.344619 139908719027968 logging_writer.py:48] [169600] global_step=169600, grad_norm=3.871796131134033, loss=2.5247883796691895
I0201 07:27:01.019452 139908710635264 logging_writer.py:48] [169700] global_step=169700, grad_norm=3.9955897331237793, loss=2.5576579570770264
I0201 07:27:34.765973 139908719027968 logging_writer.py:48] [169800] global_step=169800, grad_norm=3.964677333831787, loss=2.4909441471099854
I0201 07:28:08.515615 139908710635264 logging_writer.py:48] [169900] global_step=169900, grad_norm=3.600216865539551, loss=2.4972152709960938
I0201 07:28:42.274987 139908719027968 logging_writer.py:48] [170000] global_step=170000, grad_norm=3.9250640869140625, loss=2.551919937133789
I0201 07:29:16.016547 139908710635264 logging_writer.py:48] [170100] global_step=170100, grad_norm=4.364114761352539, loss=2.5317025184631348
I0201 07:29:49.765580 139908719027968 logging_writer.py:48] [170200] global_step=170200, grad_norm=3.9392032623291016, loss=2.5425167083740234
I0201 07:30:23.587693 139908710635264 logging_writer.py:48] [170300] global_step=170300, grad_norm=4.156729221343994, loss=2.5284409523010254
I0201 07:30:57.297749 139908719027968 logging_writer.py:48] [170400] global_step=170400, grad_norm=4.088721752166748, loss=2.4783239364624023
I0201 07:31:30.950560 139908710635264 logging_writer.py:48] [170500] global_step=170500, grad_norm=3.865966320037842, loss=2.531827926635742
I0201 07:32:04.663659 139908719027968 logging_writer.py:48] [170600] global_step=170600, grad_norm=4.019815921783447, loss=2.5717685222625732
I0201 07:32:38.321959 139908710635264 logging_writer.py:48] [170700] global_step=170700, grad_norm=4.114116668701172, loss=2.531139373779297
I0201 07:33:11.995938 139908719027968 logging_writer.py:48] [170800] global_step=170800, grad_norm=4.12210750579834, loss=2.5569612979888916
I0201 07:33:36.799401 140070692116288 spec.py:321] Evaluating on the training split.
I0201 07:33:43.045663 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 07:33:51.733239 140070692116288 spec.py:349] Evaluating on the test split.
I0201 07:33:54.205336 140070692116288 submission_runner.py:408] Time since start: 59693.98s, 	Step: 170875, 	{'train/accuracy': 0.9371013641357422, 'train/loss': 0.4609328508377075, 'validation/accuracy': 0.7616399526596069, 'validation/loss': 1.170788288116455, 'validation/num_examples': 50000, 'test/accuracy': 0.6381000280380249, 'test/loss': 1.8069534301757812, 'test/num_examples': 10000, 'score': 57671.088973522186, 'total_duration': 59693.97801613808, 'accumulated_submission_time': 57671.088973522186, 'accumulated_eval_time': 2012.0857291221619, 'accumulated_logging_time': 4.797400236129761}
I0201 07:33:54.252271 139907745949440 logging_writer.py:48] [170875] accumulated_eval_time=2012.085729, accumulated_logging_time=4.797400, accumulated_submission_time=57671.088974, global_step=170875, preemption_count=0, score=57671.088974, test/accuracy=0.638100, test/loss=1.806953, test/num_examples=10000, total_duration=59693.978016, train/accuracy=0.937101, train/loss=0.460933, validation/accuracy=0.761640, validation/loss=1.170788, validation/num_examples=50000
I0201 07:34:03.034603 139907754342144 logging_writer.py:48] [170900] global_step=170900, grad_norm=3.8087949752807617, loss=2.531611919403076
I0201 07:34:36.727881 139907745949440 logging_writer.py:48] [171000] global_step=171000, grad_norm=3.820967674255371, loss=2.521369457244873
I0201 07:35:10.442073 139907754342144 logging_writer.py:48] [171100] global_step=171100, grad_norm=4.145874500274658, loss=2.539215326309204
I0201 07:35:44.200263 139907745949440 logging_writer.py:48] [171200] global_step=171200, grad_norm=4.002477645874023, loss=2.530438184738159
I0201 07:36:18.100831 139907754342144 logging_writer.py:48] [171300] global_step=171300, grad_norm=3.9656782150268555, loss=2.5287399291992188
I0201 07:36:51.824532 139907745949440 logging_writer.py:48] [171400] global_step=171400, grad_norm=3.879394769668579, loss=2.521756172180176
I0201 07:37:25.596065 139907754342144 logging_writer.py:48] [171500] global_step=171500, grad_norm=4.286840915679932, loss=2.587169647216797
I0201 07:37:59.297115 139907745949440 logging_writer.py:48] [171600] global_step=171600, grad_norm=3.816478729248047, loss=2.4929094314575195
I0201 07:38:33.038583 139907754342144 logging_writer.py:48] [171700] global_step=171700, grad_norm=5.000034809112549, loss=2.6128640174865723
I0201 07:39:06.795160 139907745949440 logging_writer.py:48] [171800] global_step=171800, grad_norm=3.988112688064575, loss=2.520017623901367
I0201 07:39:40.546684 139907754342144 logging_writer.py:48] [171900] global_step=171900, grad_norm=4.453782558441162, loss=2.5313732624053955
I0201 07:40:14.256486 139907745949440 logging_writer.py:48] [172000] global_step=172000, grad_norm=4.1277360916137695, loss=2.576932430267334
I0201 07:40:47.916368 139907754342144 logging_writer.py:48] [172100] global_step=172100, grad_norm=4.050106048583984, loss=2.529284954071045
I0201 07:41:21.641238 139907745949440 logging_writer.py:48] [172200] global_step=172200, grad_norm=4.073254108428955, loss=2.5567779541015625
I0201 07:41:55.386027 139907754342144 logging_writer.py:48] [172300] global_step=172300, grad_norm=4.102936267852783, loss=2.5569708347320557
I0201 07:42:24.503528 140070692116288 spec.py:321] Evaluating on the training split.
I0201 07:42:30.630251 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 07:42:39.492988 140070692116288 spec.py:349] Evaluating on the test split.
I0201 07:42:42.023614 140070692116288 submission_runner.py:408] Time since start: 60221.80s, 	Step: 172387, 	{'train/accuracy': 0.9367625713348389, 'train/loss': 0.4671074450016022, 'validation/accuracy': 0.7630599737167358, 'validation/loss': 1.1761828660964966, 'validation/num_examples': 50000, 'test/accuracy': 0.6375000476837158, 'test/loss': 1.80931556224823, 'test/num_examples': 10000, 'score': 58181.277435302734, 'total_duration': 60221.7962770462, 'accumulated_submission_time': 58181.277435302734, 'accumulated_eval_time': 2029.605746269226, 'accumulated_logging_time': 4.853551864624023}
I0201 07:42:42.075681 139908710635264 logging_writer.py:48] [172387] accumulated_eval_time=2029.605746, accumulated_logging_time=4.853552, accumulated_submission_time=58181.277435, global_step=172387, preemption_count=0, score=58181.277435, test/accuracy=0.637500, test/loss=1.809316, test/num_examples=10000, total_duration=60221.796277, train/accuracy=0.936763, train/loss=0.467107, validation/accuracy=0.763060, validation/loss=1.176183, validation/num_examples=50000
I0201 07:42:46.824075 139908719027968 logging_writer.py:48] [172400] global_step=172400, grad_norm=3.915022134780884, loss=2.455811023712158
I0201 07:43:20.562852 139908710635264 logging_writer.py:48] [172500] global_step=172500, grad_norm=4.165687561035156, loss=2.517085552215576
I0201 07:43:54.261096 139908719027968 logging_writer.py:48] [172600] global_step=172600, grad_norm=4.223719596862793, loss=2.543771743774414
I0201 07:44:28.021823 139908710635264 logging_writer.py:48] [172700] global_step=172700, grad_norm=3.9080970287323, loss=2.5325350761413574
I0201 07:45:01.734790 139908719027968 logging_writer.py:48] [172800] global_step=172800, grad_norm=4.484127044677734, loss=2.5261950492858887
I0201 07:45:35.517931 139908710635264 logging_writer.py:48] [172900] global_step=172900, grad_norm=4.352931499481201, loss=2.5054314136505127
I0201 07:46:09.259284 139908719027968 logging_writer.py:48] [173000] global_step=173000, grad_norm=4.348167419433594, loss=2.518988847732544
I0201 07:46:42.999546 139908710635264 logging_writer.py:48] [173100] global_step=173100, grad_norm=4.046299934387207, loss=2.5323596000671387
I0201 07:47:16.673683 139908719027968 logging_writer.py:48] [173200] global_step=173200, grad_norm=4.086315631866455, loss=2.569227457046509
I0201 07:47:50.430397 139908710635264 logging_writer.py:48] [173300] global_step=173300, grad_norm=4.048491477966309, loss=2.527146100997925
I0201 07:48:24.154853 139908719027968 logging_writer.py:48] [173400] global_step=173400, grad_norm=4.206856727600098, loss=2.520155906677246
I0201 07:48:58.018228 139908710635264 logging_writer.py:48] [173500] global_step=173500, grad_norm=4.122846603393555, loss=2.4959330558776855
I0201 07:49:31.704983 139908719027968 logging_writer.py:48] [173600] global_step=173600, grad_norm=3.9662349224090576, loss=2.487884044647217
I0201 07:50:05.449218 139908710635264 logging_writer.py:48] [173700] global_step=173700, grad_norm=4.070870399475098, loss=2.542894124984741
I0201 07:50:39.153190 139908719027968 logging_writer.py:48] [173800] global_step=173800, grad_norm=4.262625217437744, loss=2.557097911834717
I0201 07:51:12.319448 140070692116288 spec.py:321] Evaluating on the training split.
I0201 07:51:18.483096 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 07:51:27.194910 140070692116288 spec.py:349] Evaluating on the test split.
I0201 07:51:29.676499 140070692116288 submission_runner.py:408] Time since start: 60749.45s, 	Step: 173900, 	{'train/accuracy': 0.9363639950752258, 'train/loss': 0.46477654576301575, 'validation/accuracy': 0.7618199586868286, 'validation/loss': 1.1747890710830688, 'validation/num_examples': 50000, 'test/accuracy': 0.6363000273704529, 'test/loss': 1.8088829517364502, 'test/num_examples': 10000, 'score': 58691.458490133286, 'total_duration': 60749.449186086655, 'accumulated_submission_time': 58691.458490133286, 'accumulated_eval_time': 2046.9627692699432, 'accumulated_logging_time': 4.915642261505127}
I0201 07:51:29.727376 139907737556736 logging_writer.py:48] [173900] accumulated_eval_time=2046.962769, accumulated_logging_time=4.915642, accumulated_submission_time=58691.458490, global_step=173900, preemption_count=0, score=58691.458490, test/accuracy=0.636300, test/loss=1.808883, test/num_examples=10000, total_duration=60749.449186, train/accuracy=0.936364, train/loss=0.464777, validation/accuracy=0.761820, validation/loss=1.174789, validation/num_examples=50000
I0201 07:51:30.078165 139907745949440 logging_writer.py:48] [173900] global_step=173900, grad_norm=4.458064556121826, loss=2.5392608642578125
I0201 07:52:03.749517 139907737556736 logging_writer.py:48] [174000] global_step=174000, grad_norm=4.154514789581299, loss=2.4832653999328613
I0201 07:52:37.463358 139907745949440 logging_writer.py:48] [174100] global_step=174100, grad_norm=4.4981160163879395, loss=2.531726360321045
I0201 07:53:11.217353 139907737556736 logging_writer.py:48] [174200] global_step=174200, grad_norm=4.252041816711426, loss=2.533331871032715
I0201 07:53:44.947487 139907745949440 logging_writer.py:48] [174300] global_step=174300, grad_norm=4.423093795776367, loss=2.5647921562194824
I0201 07:54:18.709095 139907737556736 logging_writer.py:48] [174400] global_step=174400, grad_norm=4.155914783477783, loss=2.541990280151367
I0201 07:54:52.502104 139907745949440 logging_writer.py:48] [174500] global_step=174500, grad_norm=4.211813926696777, loss=2.5109164714813232
I0201 07:55:26.194227 139907737556736 logging_writer.py:48] [174600] global_step=174600, grad_norm=4.310256481170654, loss=2.54685115814209
I0201 07:55:59.915910 139907745949440 logging_writer.py:48] [174700] global_step=174700, grad_norm=4.092504024505615, loss=2.4974327087402344
I0201 07:56:33.692112 139907737556736 logging_writer.py:48] [174800] global_step=174800, grad_norm=4.452942848205566, loss=2.5560052394866943
I0201 07:57:07.428521 139907745949440 logging_writer.py:48] [174900] global_step=174900, grad_norm=3.9802017211914062, loss=2.4809956550598145
I0201 07:57:41.186336 139907737556736 logging_writer.py:48] [175000] global_step=175000, grad_norm=4.230829238891602, loss=2.540294647216797
I0201 07:58:14.869815 139907745949440 logging_writer.py:48] [175100] global_step=175100, grad_norm=4.125289440155029, loss=2.4924445152282715
I0201 07:58:48.652258 139907737556736 logging_writer.py:48] [175200] global_step=175200, grad_norm=4.006460189819336, loss=2.5372517108917236
I0201 07:59:22.370884 139907745949440 logging_writer.py:48] [175300] global_step=175300, grad_norm=4.2220845222473145, loss=2.5240108966827393
I0201 07:59:56.139420 139907737556736 logging_writer.py:48] [175400] global_step=175400, grad_norm=3.9293484687805176, loss=2.544692039489746
I0201 08:00:00.009348 140070692116288 spec.py:321] Evaluating on the training split.
I0201 08:00:06.325344 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 08:00:15.267355 140070692116288 spec.py:349] Evaluating on the test split.
I0201 08:00:17.783103 140070692116288 submission_runner.py:408] Time since start: 61277.56s, 	Step: 175413, 	{'train/accuracy': 0.9368223547935486, 'train/loss': 0.4632803499698639, 'validation/accuracy': 0.7626799941062927, 'validation/loss': 1.174310326576233, 'validation/num_examples': 50000, 'test/accuracy': 0.6386000514030457, 'test/loss': 1.8084932565689087, 'test/num_examples': 10000, 'score': 59201.67768287659, 'total_duration': 61277.555790662766, 'accumulated_submission_time': 59201.67768287659, 'accumulated_eval_time': 2064.736491918564, 'accumulated_logging_time': 4.975682020187378}
I0201 08:00:17.829350 139907737556736 logging_writer.py:48] [175413] accumulated_eval_time=2064.736492, accumulated_logging_time=4.975682, accumulated_submission_time=59201.677683, global_step=175413, preemption_count=0, score=59201.677683, test/accuracy=0.638600, test/loss=1.808493, test/num_examples=10000, total_duration=61277.555791, train/accuracy=0.936822, train/loss=0.463280, validation/accuracy=0.762680, validation/loss=1.174310, validation/num_examples=50000
I0201 08:00:47.444048 139908710635264 logging_writer.py:48] [175500] global_step=175500, grad_norm=4.133634090423584, loss=2.529697895050049
I0201 08:01:21.226654 139907737556736 logging_writer.py:48] [175600] global_step=175600, grad_norm=4.384314060211182, loss=2.4760613441467285
I0201 08:01:54.961293 139908710635264 logging_writer.py:48] [175700] global_step=175700, grad_norm=3.8889198303222656, loss=2.4716644287109375
I0201 08:02:28.686726 139907737556736 logging_writer.py:48] [175800] global_step=175800, grad_norm=4.246718406677246, loss=2.576117753982544
I0201 08:03:02.366093 139908710635264 logging_writer.py:48] [175900] global_step=175900, grad_norm=4.229740142822266, loss=2.5075020790100098
I0201 08:03:36.076118 139907737556736 logging_writer.py:48] [176000] global_step=176000, grad_norm=3.958026885986328, loss=2.5080726146698
I0201 08:04:09.820229 139908710635264 logging_writer.py:48] [176100] global_step=176100, grad_norm=4.057795524597168, loss=2.590078592300415
I0201 08:04:43.554236 139907737556736 logging_writer.py:48] [176200] global_step=176200, grad_norm=4.293761730194092, loss=2.5194015502929688
I0201 08:05:17.211612 139908710635264 logging_writer.py:48] [176300] global_step=176300, grad_norm=4.127480983734131, loss=2.551499366760254
I0201 08:05:50.967689 139907737556736 logging_writer.py:48] [176400] global_step=176400, grad_norm=4.040115833282471, loss=2.5159013271331787
I0201 08:06:24.666126 139908710635264 logging_writer.py:48] [176500] global_step=176500, grad_norm=4.116749286651611, loss=2.5019443035125732
I0201 08:06:58.394084 139907737556736 logging_writer.py:48] [176600] global_step=176600, grad_norm=4.060878276824951, loss=2.4752964973449707
I0201 08:07:32.220073 139908710635264 logging_writer.py:48] [176700] global_step=176700, grad_norm=4.270813465118408, loss=2.521146774291992
I0201 08:08:05.969951 139907737556736 logging_writer.py:48] [176800] global_step=176800, grad_norm=4.044672012329102, loss=2.471944808959961
I0201 08:08:39.627865 139908710635264 logging_writer.py:48] [176900] global_step=176900, grad_norm=4.332269191741943, loss=2.5495824813842773
I0201 08:08:47.872279 140070692116288 spec.py:321] Evaluating on the training split.
I0201 08:08:54.155572 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 08:09:02.973670 140070692116288 spec.py:349] Evaluating on the test split.
I0201 08:09:05.474840 140070692116288 submission_runner.py:408] Time since start: 61805.25s, 	Step: 176926, 	{'train/accuracy': 0.9379583597183228, 'train/loss': 0.46584373712539673, 'validation/accuracy': 0.7636199593544006, 'validation/loss': 1.175073504447937, 'validation/num_examples': 50000, 'test/accuracy': 0.6410000324249268, 'test/loss': 1.8080230951309204, 'test/num_examples': 10000, 'score': 59711.657662153244, 'total_duration': 61805.24752783775, 'accumulated_submission_time': 59711.657662153244, 'accumulated_eval_time': 2082.3390328884125, 'accumulated_logging_time': 5.031782627105713}
I0201 08:09:05.525667 139907729164032 logging_writer.py:48] [176926] accumulated_eval_time=2082.339033, accumulated_logging_time=5.031783, accumulated_submission_time=59711.657662, global_step=176926, preemption_count=0, score=59711.657662, test/accuracy=0.641000, test/loss=1.808023, test/num_examples=10000, total_duration=61805.247528, train/accuracy=0.937958, train/loss=0.465844, validation/accuracy=0.763620, validation/loss=1.175074, validation/num_examples=50000
I0201 08:09:30.808217 139907737556736 logging_writer.py:48] [177000] global_step=177000, grad_norm=4.4690680503845215, loss=2.5366122722625732
I0201 08:10:04.527760 139907729164032 logging_writer.py:48] [177100] global_step=177100, grad_norm=4.073525905609131, loss=2.519169330596924
I0201 08:10:38.226784 139907737556736 logging_writer.py:48] [177200] global_step=177200, grad_norm=4.066258907318115, loss=2.5603933334350586
I0201 08:11:11.980573 139907729164032 logging_writer.py:48] [177300] global_step=177300, grad_norm=4.284317970275879, loss=2.518126964569092
I0201 08:11:45.682919 139907737556736 logging_writer.py:48] [177400] global_step=177400, grad_norm=4.234683036804199, loss=2.537104606628418
I0201 08:12:19.464657 139907729164032 logging_writer.py:48] [177500] global_step=177500, grad_norm=3.8096423149108887, loss=2.4586334228515625
I0201 08:12:53.170878 139907737556736 logging_writer.py:48] [177600] global_step=177600, grad_norm=4.1215925216674805, loss=2.5585761070251465
I0201 08:13:26.954612 139907729164032 logging_writer.py:48] [177700] global_step=177700, grad_norm=4.277324199676514, loss=2.5591139793395996
I0201 08:14:00.891722 139907737556736 logging_writer.py:48] [177800] global_step=177800, grad_norm=4.317801475524902, loss=2.4917938709259033
I0201 08:14:34.615672 139907729164032 logging_writer.py:48] [177900] global_step=177900, grad_norm=4.561380386352539, loss=2.5201144218444824
I0201 08:15:08.292907 139907737556736 logging_writer.py:48] [178000] global_step=178000, grad_norm=4.173391342163086, loss=2.5042104721069336
I0201 08:15:41.998852 139907729164032 logging_writer.py:48] [178100] global_step=178100, grad_norm=4.224569797515869, loss=2.4564642906188965
I0201 08:16:15.715645 139907737556736 logging_writer.py:48] [178200] global_step=178200, grad_norm=3.9229772090911865, loss=2.4783215522766113
I0201 08:16:49.462717 139907729164032 logging_writer.py:48] [178300] global_step=178300, grad_norm=4.063440322875977, loss=2.542612075805664
I0201 08:17:23.174381 139907737556736 logging_writer.py:48] [178400] global_step=178400, grad_norm=4.322175025939941, loss=2.5550644397735596
I0201 08:17:35.810868 140070692116288 spec.py:321] Evaluating on the training split.
I0201 08:17:41.996181 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 08:17:50.817185 140070692116288 spec.py:349] Evaluating on the test split.
I0201 08:17:53.277362 140070692116288 submission_runner.py:408] Time since start: 62333.05s, 	Step: 178439, 	{'train/accuracy': 0.9407883882522583, 'train/loss': 0.4490717649459839, 'validation/accuracy': 0.7630800008773804, 'validation/loss': 1.169806957244873, 'validation/num_examples': 50000, 'test/accuracy': 0.6402000188827515, 'test/loss': 1.8037229776382446, 'test/num_examples': 10000, 'score': 60221.87917423248, 'total_duration': 62333.050040483475, 'accumulated_submission_time': 60221.87917423248, 'accumulated_eval_time': 2099.8054864406586, 'accumulated_logging_time': 5.092525005340576}
I0201 08:17:53.325787 139908719027968 logging_writer.py:48] [178439] accumulated_eval_time=2099.805486, accumulated_logging_time=5.092525, accumulated_submission_time=60221.879174, global_step=178439, preemption_count=0, score=60221.879174, test/accuracy=0.640200, test/loss=1.803723, test/num_examples=10000, total_duration=62333.050040, train/accuracy=0.940788, train/loss=0.449072, validation/accuracy=0.763080, validation/loss=1.169807, validation/num_examples=50000
I0201 08:18:14.201241 139908727420672 logging_writer.py:48] [178500] global_step=178500, grad_norm=4.029507160186768, loss=2.5529439449310303
I0201 08:18:47.838742 139908719027968 logging_writer.py:48] [178600] global_step=178600, grad_norm=4.2303690910339355, loss=2.4977948665618896
I0201 08:19:21.560547 139908727420672 logging_writer.py:48] [178700] global_step=178700, grad_norm=4.168203353881836, loss=2.524030923843384
I0201 08:19:55.270301 139908719027968 logging_writer.py:48] [178800] global_step=178800, grad_norm=4.431842803955078, loss=2.5573079586029053
I0201 08:20:29.148225 139908727420672 logging_writer.py:48] [178900] global_step=178900, grad_norm=4.053436279296875, loss=2.541874408721924
I0201 08:21:02.822097 139908719027968 logging_writer.py:48] [179000] global_step=179000, grad_norm=4.335448265075684, loss=2.5028228759765625
I0201 08:21:36.513836 139908727420672 logging_writer.py:48] [179100] global_step=179100, grad_norm=4.080532550811768, loss=2.4372775554656982
I0201 08:22:10.268603 139908719027968 logging_writer.py:48] [179200] global_step=179200, grad_norm=4.198703289031982, loss=2.5118138790130615
I0201 08:22:43.984568 139908727420672 logging_writer.py:48] [179300] global_step=179300, grad_norm=4.384496212005615, loss=2.483018398284912
I0201 08:23:17.679396 139908719027968 logging_writer.py:48] [179400] global_step=179400, grad_norm=4.278239727020264, loss=2.5525906085968018
I0201 08:23:51.461247 139908727420672 logging_writer.py:48] [179500] global_step=179500, grad_norm=4.237922191619873, loss=2.5506370067596436
I0201 08:24:25.188892 139908719027968 logging_writer.py:48] [179600] global_step=179600, grad_norm=3.8948347568511963, loss=2.5507826805114746
I0201 08:24:58.948284 139908727420672 logging_writer.py:48] [179700] global_step=179700, grad_norm=4.401483058929443, loss=2.533409595489502
I0201 08:25:32.700426 139908719027968 logging_writer.py:48] [179800] global_step=179800, grad_norm=4.246620178222656, loss=2.553225517272949
I0201 08:26:06.432987 139908727420672 logging_writer.py:48] [179900] global_step=179900, grad_norm=3.9937334060668945, loss=2.5267326831817627
I0201 08:26:23.545686 140070692116288 spec.py:321] Evaluating on the training split.
I0201 08:26:29.817856 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 08:26:38.324452 140070692116288 spec.py:349] Evaluating on the test split.
I0201 08:26:40.798902 140070692116288 submission_runner.py:408] Time since start: 62860.57s, 	Step: 179952, 	{'train/accuracy': 0.9408083558082581, 'train/loss': 0.45449861884117126, 'validation/accuracy': 0.7629599571228027, 'validation/loss': 1.1712826490402222, 'validation/num_examples': 50000, 'test/accuracy': 0.640500009059906, 'test/loss': 1.8030924797058105, 'test/num_examples': 10000, 'score': 60732.035746097565, 'total_duration': 62860.571590185165, 'accumulated_submission_time': 60732.035746097565, 'accumulated_eval_time': 2117.058675289154, 'accumulated_logging_time': 5.151561737060547}
I0201 08:26:40.846296 139907745949440 logging_writer.py:48] [179952] accumulated_eval_time=2117.058675, accumulated_logging_time=5.151562, accumulated_submission_time=60732.035746, global_step=179952, preemption_count=0, score=60732.035746, test/accuracy=0.640500, test/loss=1.803092, test/num_examples=10000, total_duration=62860.571590, train/accuracy=0.940808, train/loss=0.454499, validation/accuracy=0.762960, validation/loss=1.171283, validation/num_examples=50000
I0201 08:26:57.403885 139907754342144 logging_writer.py:48] [180000] global_step=180000, grad_norm=3.720043897628784, loss=2.441068172454834
I0201 08:27:31.058558 139907745949440 logging_writer.py:48] [180100] global_step=180100, grad_norm=3.878143787384033, loss=2.516183614730835
I0201 08:28:04.734803 139907754342144 logging_writer.py:48] [180200] global_step=180200, grad_norm=4.103969573974609, loss=2.4949307441711426
I0201 08:28:38.482395 139907745949440 logging_writer.py:48] [180300] global_step=180300, grad_norm=4.258728981018066, loss=2.527395009994507
I0201 08:29:12.190657 139907754342144 logging_writer.py:48] [180400] global_step=180400, grad_norm=4.148865222930908, loss=2.548485279083252
I0201 08:29:45.940279 139907745949440 logging_writer.py:48] [180500] global_step=180500, grad_norm=4.03735876083374, loss=2.544853925704956
I0201 08:30:19.667421 139907754342144 logging_writer.py:48] [180600] global_step=180600, grad_norm=4.254353046417236, loss=2.46565842628479
I0201 08:30:53.395754 139907745949440 logging_writer.py:48] [180700] global_step=180700, grad_norm=4.4892377853393555, loss=2.5210986137390137
I0201 08:31:27.154512 139907754342144 logging_writer.py:48] [180800] global_step=180800, grad_norm=4.62172269821167, loss=2.5495376586914062
I0201 08:32:00.903554 139907745949440 logging_writer.py:48] [180900] global_step=180900, grad_norm=4.047360420227051, loss=2.5471129417419434
I0201 08:32:34.728582 139907754342144 logging_writer.py:48] [181000] global_step=181000, grad_norm=3.9741671085357666, loss=2.530482053756714
I0201 08:33:08.401580 139907745949440 logging_writer.py:48] [181100] global_step=181100, grad_norm=3.920076608657837, loss=2.559812545776367
I0201 08:33:42.112090 139907754342144 logging_writer.py:48] [181200] global_step=181200, grad_norm=4.2896223068237305, loss=2.5905022621154785
I0201 08:34:15.878861 139907745949440 logging_writer.py:48] [181300] global_step=181300, grad_norm=3.9837794303894043, loss=2.469757556915283
I0201 08:34:49.913875 139907754342144 logging_writer.py:48] [181400] global_step=181400, grad_norm=4.224194526672363, loss=2.5236120223999023
I0201 08:35:10.987304 140070692116288 spec.py:321] Evaluating on the training split.
I0201 08:35:17.235320 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 08:35:25.796186 140070692116288 spec.py:349] Evaluating on the test split.
I0201 08:35:28.315622 140070692116288 submission_runner.py:408] Time since start: 63388.09s, 	Step: 181464, 	{'train/accuracy': 0.939871609210968, 'train/loss': 0.4563002586364746, 'validation/accuracy': 0.7633999586105347, 'validation/loss': 1.173967719078064, 'validation/num_examples': 50000, 'test/accuracy': 0.6407000422477722, 'test/loss': 1.8057492971420288, 'test/num_examples': 10000, 'score': 61242.113387584686, 'total_duration': 63388.0883102417, 'accumulated_submission_time': 61242.113387584686, 'accumulated_eval_time': 2134.386967897415, 'accumulated_logging_time': 5.21010160446167}
I0201 08:35:28.364788 139907737556736 logging_writer.py:48] [181464] accumulated_eval_time=2134.386968, accumulated_logging_time=5.210102, accumulated_submission_time=61242.113388, global_step=181464, preemption_count=0, score=61242.113388, test/accuracy=0.640700, test/loss=1.805749, test/num_examples=10000, total_duration=63388.088310, train/accuracy=0.939872, train/loss=0.456300, validation/accuracy=0.763400, validation/loss=1.173968, validation/num_examples=50000
I0201 08:35:40.813122 139907745949440 logging_writer.py:48] [181500] global_step=181500, grad_norm=4.225164413452148, loss=2.535186767578125
I0201 08:36:14.483113 139907737556736 logging_writer.py:48] [181600] global_step=181600, grad_norm=4.104762554168701, loss=2.5227551460266113
I0201 08:36:48.163091 139907745949440 logging_writer.py:48] [181700] global_step=181700, grad_norm=4.219407558441162, loss=2.550593376159668
I0201 08:37:21.864196 139907737556736 logging_writer.py:48] [181800] global_step=181800, grad_norm=3.98964786529541, loss=2.5165793895721436
I0201 08:37:55.544981 139907745949440 logging_writer.py:48] [181900] global_step=181900, grad_norm=3.990295886993408, loss=2.5393006801605225
I0201 08:38:29.309957 139907737556736 logging_writer.py:48] [182000] global_step=182000, grad_norm=4.157528400421143, loss=2.5091419219970703
I0201 08:39:03.123640 139907745949440 logging_writer.py:48] [182100] global_step=182100, grad_norm=4.08980131149292, loss=2.4933722019195557
I0201 08:39:36.832958 139907737556736 logging_writer.py:48] [182200] global_step=182200, grad_norm=4.102038383483887, loss=2.5313260555267334
I0201 08:40:10.499116 139907745949440 logging_writer.py:48] [182300] global_step=182300, grad_norm=4.139544486999512, loss=2.553953170776367
I0201 08:40:44.200270 139907737556736 logging_writer.py:48] [182400] global_step=182400, grad_norm=4.150146007537842, loss=2.5186965465545654
I0201 08:41:17.943306 139907745949440 logging_writer.py:48] [182500] global_step=182500, grad_norm=4.347947120666504, loss=2.518545389175415
I0201 08:41:51.692737 139907737556736 logging_writer.py:48] [182600] global_step=182600, grad_norm=3.859624147415161, loss=2.442643404006958
I0201 08:42:25.441434 139907745949440 logging_writer.py:48] [182700] global_step=182700, grad_norm=4.541087627410889, loss=2.589916944503784
I0201 08:42:59.181860 139907737556736 logging_writer.py:48] [182800] global_step=182800, grad_norm=4.084043025970459, loss=2.472989082336426
I0201 08:43:32.937744 139907745949440 logging_writer.py:48] [182900] global_step=182900, grad_norm=4.167227745056152, loss=2.5121564865112305
I0201 08:43:58.399272 140070692116288 spec.py:321] Evaluating on the training split.
I0201 08:44:04.722774 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 08:44:13.442572 140070692116288 spec.py:349] Evaluating on the test split.
I0201 08:44:15.964532 140070692116288 submission_runner.py:408] Time since start: 63915.74s, 	Step: 182977, 	{'train/accuracy': 0.939851701259613, 'train/loss': 0.45275208353996277, 'validation/accuracy': 0.7633599638938904, 'validation/loss': 1.1705142259597778, 'validation/num_examples': 50000, 'test/accuracy': 0.6399000287055969, 'test/loss': 1.800960659980774, 'test/num_examples': 10000, 'score': 61752.08497405052, 'total_duration': 63915.73721885681, 'accumulated_submission_time': 61752.08497405052, 'accumulated_eval_time': 2151.952211856842, 'accumulated_logging_time': 5.2697319984436035}
I0201 08:44:16.013042 139908425447168 logging_writer.py:48] [182977] accumulated_eval_time=2151.952212, accumulated_logging_time=5.269732, accumulated_submission_time=61752.084974, global_step=182977, preemption_count=0, score=61752.084974, test/accuracy=0.639900, test/loss=1.800961, test/num_examples=10000, total_duration=63915.737219, train/accuracy=0.939852, train/loss=0.452752, validation/accuracy=0.763360, validation/loss=1.170514, validation/num_examples=50000
I0201 08:44:24.115408 139908710635264 logging_writer.py:48] [183000] global_step=183000, grad_norm=4.1859636306762695, loss=2.5606906414031982
I0201 08:44:57.861273 139908425447168 logging_writer.py:48] [183100] global_step=183100, grad_norm=4.15803861618042, loss=2.556079149246216
I0201 08:45:31.588856 139908710635264 logging_writer.py:48] [183200] global_step=183200, grad_norm=4.2794904708862305, loss=2.504608631134033
I0201 08:46:05.292598 139908425447168 logging_writer.py:48] [183300] global_step=183300, grad_norm=3.8245770931243896, loss=2.505033016204834
I0201 08:46:39.058037 139908710635264 logging_writer.py:48] [183400] global_step=183400, grad_norm=4.081712245941162, loss=2.4845049381256104
I0201 08:47:12.760125 139908425447168 logging_writer.py:48] [183500] global_step=183500, grad_norm=4.012052536010742, loss=2.4981470108032227
I0201 08:47:46.515987 139908710635264 logging_writer.py:48] [183600] global_step=183600, grad_norm=4.144421100616455, loss=2.5129449367523193
I0201 08:48:20.254368 139908425447168 logging_writer.py:48] [183700] global_step=183700, grad_norm=3.9309568405151367, loss=2.519340991973877
I0201 08:48:53.973837 139908710635264 logging_writer.py:48] [183800] global_step=183800, grad_norm=4.328236103057861, loss=2.5799400806427
I0201 08:49:27.697925 139908425447168 logging_writer.py:48] [183900] global_step=183900, grad_norm=3.9192187786102295, loss=2.465816020965576
I0201 08:50:01.430549 139908710635264 logging_writer.py:48] [184000] global_step=184000, grad_norm=3.9372317790985107, loss=2.5370934009552
I0201 08:50:35.157693 139908425447168 logging_writer.py:48] [184100] global_step=184100, grad_norm=3.9589617252349854, loss=2.510429620742798
I0201 08:51:09.089160 139908710635264 logging_writer.py:48] [184200] global_step=184200, grad_norm=4.309620380401611, loss=2.513737678527832
I0201 08:51:42.869554 139908425447168 logging_writer.py:48] [184300] global_step=184300, grad_norm=3.9641361236572266, loss=2.5077619552612305
I0201 08:52:16.595348 139908710635264 logging_writer.py:48] [184400] global_step=184400, grad_norm=4.479740142822266, loss=2.492340564727783
I0201 08:52:46.140439 140070692116288 spec.py:321] Evaluating on the training split.
I0201 08:52:52.331626 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 08:53:01.199311 140070692116288 spec.py:349] Evaluating on the test split.
I0201 08:53:03.845457 140070692116288 submission_runner.py:408] Time since start: 64443.62s, 	Step: 184489, 	{'train/accuracy': 0.93949294090271, 'train/loss': 0.45518749952316284, 'validation/accuracy': 0.7637400031089783, 'validation/loss': 1.1731160879135132, 'validation/num_examples': 50000, 'test/accuracy': 0.6402000188827515, 'test/loss': 1.8049848079681396, 'test/num_examples': 10000, 'score': 62262.14775061607, 'total_duration': 64443.61814188957, 'accumulated_submission_time': 62262.14775061607, 'accumulated_eval_time': 2169.6572070121765, 'accumulated_logging_time': 5.3304524421691895}
I0201 08:53:03.898487 139907737556736 logging_writer.py:48] [184489] accumulated_eval_time=2169.657207, accumulated_logging_time=5.330452, accumulated_submission_time=62262.147751, global_step=184489, preemption_count=0, score=62262.147751, test/accuracy=0.640200, test/loss=1.804985, test/num_examples=10000, total_duration=64443.618142, train/accuracy=0.939493, train/loss=0.455187, validation/accuracy=0.763740, validation/loss=1.173116, validation/num_examples=50000
I0201 08:53:07.958226 139907745949440 logging_writer.py:48] [184500] global_step=184500, grad_norm=4.272539138793945, loss=2.5032167434692383
I0201 08:53:41.623416 139907737556736 logging_writer.py:48] [184600] global_step=184600, grad_norm=3.986400842666626, loss=2.4942574501037598
I0201 08:54:15.314172 139907745949440 logging_writer.py:48] [184700] global_step=184700, grad_norm=3.892570972442627, loss=2.5073485374450684
I0201 08:54:48.980063 139907737556736 logging_writer.py:48] [184800] global_step=184800, grad_norm=4.511109352111816, loss=2.509140729904175
I0201 08:55:22.660233 139907745949440 logging_writer.py:48] [184900] global_step=184900, grad_norm=4.1474761962890625, loss=2.5720131397247314
I0201 08:55:56.344954 139907737556736 logging_writer.py:48] [185000] global_step=185000, grad_norm=4.111562728881836, loss=2.505112648010254
I0201 08:56:30.050170 139907745949440 logging_writer.py:48] [185100] global_step=185100, grad_norm=4.0522685050964355, loss=2.468550205230713
I0201 08:57:03.805160 139907737556736 logging_writer.py:48] [185200] global_step=185200, grad_norm=3.836141586303711, loss=2.483368396759033
I0201 08:57:37.688415 139907745949440 logging_writer.py:48] [185300] global_step=185300, grad_norm=4.251298427581787, loss=2.4484305381774902
I0201 08:58:11.369603 139907737556736 logging_writer.py:48] [185400] global_step=185400, grad_norm=3.969208002090454, loss=2.5020265579223633
I0201 08:58:45.092018 139907745949440 logging_writer.py:48] [185500] global_step=185500, grad_norm=4.047455787658691, loss=2.510932445526123
I0201 08:59:18.845967 139907737556736 logging_writer.py:48] [185600] global_step=185600, grad_norm=4.273655891418457, loss=2.554783344268799
I0201 08:59:52.530559 139907745949440 logging_writer.py:48] [185700] global_step=185700, grad_norm=4.066029071807861, loss=2.5451056957244873
I0201 09:00:26.223968 139907737556736 logging_writer.py:48] [185800] global_step=185800, grad_norm=3.9836573600769043, loss=2.509951114654541
I0201 09:00:59.981974 139907745949440 logging_writer.py:48] [185900] global_step=185900, grad_norm=4.194019317626953, loss=2.5133886337280273
I0201 09:01:33.766964 139907737556736 logging_writer.py:48] [186000] global_step=186000, grad_norm=4.128310203552246, loss=2.525822639465332
I0201 09:01:33.932341 140070692116288 spec.py:321] Evaluating on the training split.
I0201 09:01:40.146035 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 09:01:48.773789 140070692116288 spec.py:349] Evaluating on the test split.
I0201 09:01:51.268666 140070692116288 submission_runner.py:408] Time since start: 64971.04s, 	Step: 186002, 	{'train/accuracy': 0.9404894709587097, 'train/loss': 0.45212090015411377, 'validation/accuracy': 0.7636399865150452, 'validation/loss': 1.167830228805542, 'validation/num_examples': 50000, 'test/accuracy': 0.6404000520706177, 'test/loss': 1.8014260530471802, 'test/num_examples': 10000, 'score': 62772.118121147156, 'total_duration': 64971.041343450546, 'accumulated_submission_time': 62772.118121147156, 'accumulated_eval_time': 2186.9935114383698, 'accumulated_logging_time': 5.393200159072876}
I0201 09:01:51.317850 139907737556736 logging_writer.py:48] [186002] accumulated_eval_time=2186.993511, accumulated_logging_time=5.393200, accumulated_submission_time=62772.118121, global_step=186002, preemption_count=0, score=62772.118121, test/accuracy=0.640400, test/loss=1.801426, test/num_examples=10000, total_duration=64971.041343, train/accuracy=0.940489, train/loss=0.452121, validation/accuracy=0.763640, validation/loss=1.167830, validation/num_examples=50000
I0201 09:02:24.691438 139907745949440 logging_writer.py:48] [186100] global_step=186100, grad_norm=3.932441473007202, loss=2.512023448944092
I0201 09:02:58.409364 139907737556736 logging_writer.py:48] [186200] global_step=186200, grad_norm=4.818521976470947, loss=2.544259548187256
I0201 09:03:32.161748 139907745949440 logging_writer.py:48] [186300] global_step=186300, grad_norm=4.013312339782715, loss=2.512970209121704
I0201 09:04:06.058992 139907737556736 logging_writer.py:48] [186400] global_step=186400, grad_norm=3.9852030277252197, loss=2.4913148880004883
I0201 09:04:39.777902 139907745949440 logging_writer.py:48] [186500] global_step=186500, grad_norm=4.1450934410095215, loss=2.4750242233276367
I0201 09:05:13.572767 139907737556736 logging_writer.py:48] [186600] global_step=186600, grad_norm=3.978698253631592, loss=2.548847198486328
I0201 09:05:35.344331 140070692116288 spec.py:321] Evaluating on the training split.
I0201 09:05:41.730687 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 09:05:50.308876 140070692116288 spec.py:349] Evaluating on the test split.
I0201 09:05:52.822277 140070692116288 submission_runner.py:408] Time since start: 65212.59s, 	Step: 186666, 	{'train/accuracy': 0.9403100609779358, 'train/loss': 0.44991639256477356, 'validation/accuracy': 0.763759970664978, 'validation/loss': 1.1691426038742065, 'validation/num_examples': 50000, 'test/accuracy': 0.6404000520706177, 'test/loss': 1.8022942543029785, 'test/num_examples': 10000, 'score': 62996.11162734032, 'total_duration': 65212.594963788986, 'accumulated_submission_time': 62996.11162734032, 'accumulated_eval_time': 2204.471424341202, 'accumulated_logging_time': 5.451512098312378}
I0201 09:05:52.871549 139908425447168 logging_writer.py:48] [186666] accumulated_eval_time=2204.471424, accumulated_logging_time=5.451512, accumulated_submission_time=62996.111627, global_step=186666, preemption_count=0, score=62996.111627, test/accuracy=0.640400, test/loss=1.802294, test/num_examples=10000, total_duration=65212.594964, train/accuracy=0.940310, train/loss=0.449916, validation/accuracy=0.763760, validation/loss=1.169143, validation/num_examples=50000
I0201 09:05:52.916300 139908710635264 logging_writer.py:48] [186666] global_step=186666, preemption_count=0, score=62996.111627
I0201 09:05:53.217884 140070692116288 checkpoints.py:490] Saving checkpoint at step: 186666
I0201 09:05:54.413863 140070692116288 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_3/imagenet_resnet_jax/trial_2/checkpoint_186666
I0201 09:05:54.436167 140070692116288 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_3/imagenet_resnet_jax/trial_2/checkpoint_186666.
I0201 09:05:55.194000 140070692116288 submission_runner.py:583] Tuning trial 2/5
I0201 09:05:55.194211 140070692116288 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.2, learning_rate=0.0008445074561975979, one_minus_beta1=0.11042418465, beta2=0.9978504782314613, weight_decay=0.08135402759553023, warmup_factor=0.05)
I0201 09:05:55.201272 140070692116288 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.001195790828205645, 'train/loss': 6.911755561828613, 'validation/accuracy': 0.0011199999134987593, 'validation/loss': 6.912059783935547, 'validation/num_examples': 50000, 'test/accuracy': 0.0009000000427477062, 'test/loss': 6.912177562713623, 'test/num_examples': 10000, 'score': 31.10077476501465, 'total_duration': 48.665297985076904, 'accumulated_submission_time': 31.10077476501465, 'accumulated_eval_time': 17.564436674118042, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1508, {'train/accuracy': 0.08340641111135483, 'train/loss': 5.276715278625488, 'validation/accuracy': 0.07735999673604965, 'validation/loss': 5.338540077209473, 'validation/num_examples': 50000, 'test/accuracy': 0.05770000442862511, 'test/loss': 5.554655075073242, 'test/num_examples': 10000, 'score': 541.0707614421844, 'total_duration': 576.4564385414124, 'accumulated_submission_time': 541.0707614421844, 'accumulated_eval_time': 35.312798261642456, 'accumulated_logging_time': 0.018644094467163086, 'global_step': 1508, 'preemption_count': 0}), (3014, {'train/accuracy': 0.19947783648967743, 'train/loss': 4.18760871887207, 'validation/accuracy': 0.18081998825073242, 'validation/loss': 4.304879188537598, 'validation/num_examples': 50000, 'test/accuracy': 0.13190001249313354, 'test/loss': 4.727846622467041, 'test/num_examples': 10000, 'score': 1051.159835577011, 'total_duration': 1104.983157634735, 'accumulated_submission_time': 1051.159835577011, 'accumulated_eval_time': 53.65842294692993, 'accumulated_logging_time': 0.058310747146606445, 'global_step': 3014, 'preemption_count': 0}), (4520, {'train/accuracy': 0.2977120578289032, 'train/loss': 3.51352596282959, 'validation/accuracy': 0.2717199921607971, 'validation/loss': 3.6509952545166016, 'validation/num_examples': 50000, 'test/accuracy': 0.20490001142024994, 'test/loss': 4.1407623291015625, 'test/num_examples': 10000, 'score': 1561.2604904174805, 'total_duration': 1632.881965637207, 'accumulated_submission_time': 1561.2604904174805, 'accumulated_eval_time': 71.37783074378967, 'accumulated_logging_time': 0.08464241027832031, 'global_step': 4520, 'preemption_count': 0}), (6027, {'train/accuracy': 0.3806600570678711, 'train/loss': 3.0440993309020996, 'validation/accuracy': 0.3531999886035919, 'validation/loss': 3.1904051303863525, 'validation/num_examples': 50000, 'test/accuracy': 0.26489999890327454, 'test/loss': 3.793429136276245, 'test/num_examples': 10000, 'score': 2071.3847630023956, 'total_duration': 2160.7247705459595, 'accumulated_submission_time': 2071.3847630023956, 'accumulated_eval_time': 89.01321029663086, 'accumulated_logging_time': 0.11512041091918945, 'global_step': 6027, 'preemption_count': 0}), (7535, {'train/accuracy': 0.4617745578289032, 'train/loss': 2.5876331329345703, 'validation/accuracy': 0.43479999899864197, 'validation/loss': 2.728386878967285, 'validation/num_examples': 50000, 'test/accuracy': 0.3281000256538391, 'test/loss': 3.381321668624878, 'test/num_examples': 10000, 'score': 2581.59383225441, 'total_duration': 2688.5689568519592, 'accumulated_submission_time': 2581.59383225441, 'accumulated_eval_time': 106.56827187538147, 'accumulated_logging_time': 0.1405167579650879, 'global_step': 7535, 'preemption_count': 0}), (9042, {'train/accuracy': 0.5261878371238708, 'train/loss': 2.2832539081573486, 'validation/accuracy': 0.4661199748516083, 'validation/loss': 2.5687878131866455, 'validation/num_examples': 50000, 'test/accuracy': 0.35340002179145813, 'test/loss': 3.23469614982605, 'test/num_examples': 10000, 'score': 3091.5421063899994, 'total_duration': 3216.3535330295563, 'accumulated_submission_time': 3091.5421063899994, 'accumulated_eval_time': 124.32044792175293, 'accumulated_logging_time': 0.17194366455078125, 'global_step': 9042, 'preemption_count': 0}), (10551, {'train/accuracy': 0.5644331574440002, 'train/loss': 2.038395643234253, 'validation/accuracy': 0.5180599689483643, 'validation/loss': 2.2616748809814453, 'validation/num_examples': 50000, 'test/accuracy': 0.4077000319957733, 'test/loss': 2.8990767002105713, 'test/num_examples': 10000, 'score': 3601.6693108081818, 'total_duration': 3744.4450080394745, 'accumulated_submission_time': 3601.6693108081818, 'accumulated_eval_time': 142.20368576049805, 'accumulated_logging_time': 0.20055890083312988, 'global_step': 10551, 'preemption_count': 0}), (12060, {'train/accuracy': 0.5901626348495483, 'train/loss': 1.959301471710205, 'validation/accuracy': 0.5461400151252747, 'validation/loss': 2.174945831298828, 'validation/num_examples': 50000, 'test/accuracy': 0.42480000853538513, 'test/loss': 2.805687189102173, 'test/num_examples': 10000, 'score': 4111.758868694305, 'total_duration': 4272.47144985199, 'accumulated_submission_time': 4111.758868694305, 'accumulated_eval_time': 160.059020280838, 'accumulated_logging_time': 0.22941160202026367, 'global_step': 12060, 'preemption_count': 0}), (13570, {'train/accuracy': 0.6247608065605164, 'train/loss': 1.7613424062728882, 'validation/accuracy': 0.5734399557113647, 'validation/loss': 2.0024077892303467, 'validation/num_examples': 50000, 'test/accuracy': 0.44300001859664917, 'test/loss': 2.674421548843384, 'test/num_examples': 10000, 'score': 4621.998073577881, 'total_duration': 4800.3601586818695, 'accumulated_submission_time': 4621.998073577881, 'accumulated_eval_time': 177.62704491615295, 'accumulated_logging_time': 0.2588982582092285, 'global_step': 13570, 'preemption_count': 0}), (15080, {'train/accuracy': 0.6309988498687744, 'train/loss': 1.777909517288208, 'validation/accuracy': 0.5839799642562866, 'validation/loss': 1.9887746572494507, 'validation/num_examples': 50000, 'test/accuracy': 0.4603000283241272, 'test/loss': 2.6590631008148193, 'test/num_examples': 10000, 'score': 5132.248927354813, 'total_duration': 5328.198922872543, 'accumulated_submission_time': 5132.248927354813, 'accumulated_eval_time': 195.13138890266418, 'accumulated_logging_time': 0.28992772102355957, 'global_step': 15080, 'preemption_count': 0}), (16591, {'train/accuracy': 0.6404655575752258, 'train/loss': 1.7349895238876343, 'validation/accuracy': 0.5945599675178528, 'validation/loss': 1.9475181102752686, 'validation/num_examples': 50000, 'test/accuracy': 0.4653000235557556, 'test/loss': 2.612239122390747, 'test/num_examples': 10000, 'score': 5642.341354608536, 'total_duration': 5856.172199487686, 'accumulated_submission_time': 5642.341354608536, 'accumulated_eval_time': 212.92911338806152, 'accumulated_logging_time': 0.31877803802490234, 'global_step': 16591, 'preemption_count': 0}), (18101, {'train/accuracy': 0.6934390664100647, 'train/loss': 1.4368542432785034, 'validation/accuracy': 0.613099992275238, 'validation/loss': 1.7975260019302368, 'validation/num_examples': 50000, 'test/accuracy': 0.4782000184059143, 'test/loss': 2.4677436351776123, 'test/num_examples': 10000, 'score': 6152.393952131271, 'total_duration': 6384.028230428696, 'accumulated_submission_time': 6152.393952131271, 'accumulated_eval_time': 230.6470057964325, 'accumulated_logging_time': 0.35114240646362305, 'global_step': 18101, 'preemption_count': 0}), (19612, {'train/accuracy': 0.6884366869926453, 'train/loss': 1.4951562881469727, 'validation/accuracy': 0.6227799654006958, 'validation/loss': 1.7908552885055542, 'validation/num_examples': 50000, 'test/accuracy': 0.49490001797676086, 'test/loss': 2.4673266410827637, 'test/num_examples': 10000, 'score': 6662.580951213837, 'total_duration': 6912.111881971359, 'accumulated_submission_time': 6662.580951213837, 'accumulated_eval_time': 248.45926570892334, 'accumulated_logging_time': 0.3803071975708008, 'global_step': 19612, 'preemption_count': 0}), (21123, {'train/accuracy': 0.6886360049247742, 'train/loss': 1.4495007991790771, 'validation/accuracy': 0.625819981098175, 'validation/loss': 1.7343255281448364, 'validation/num_examples': 50000, 'test/accuracy': 0.49380001425743103, 'test/loss': 2.425893545150757, 'test/num_examples': 10000, 'score': 7172.652762174606, 'total_duration': 7439.812463760376, 'accumulated_submission_time': 7172.652762174606, 'accumulated_eval_time': 266.00374031066895, 'accumulated_logging_time': 0.41115355491638184, 'global_step': 21123, 'preemption_count': 0}), (22633, {'train/accuracy': 0.6915856003761292, 'train/loss': 1.4739915132522583, 'validation/accuracy': 0.6281999945640564, 'validation/loss': 1.7558631896972656, 'validation/num_examples': 50000, 'test/accuracy': 0.5089000463485718, 'test/loss': 2.38150954246521, 'test/num_examples': 10000, 'score': 7682.5801339149475, 'total_duration': 7967.496514797211, 'accumulated_submission_time': 7682.5801339149475, 'accumulated_eval_time': 283.67709016799927, 'accumulated_logging_time': 0.44150733947753906, 'global_step': 22633, 'preemption_count': 0}), (24144, {'train/accuracy': 0.6928212642669678, 'train/loss': 1.4294037818908691, 'validation/accuracy': 0.6322399973869324, 'validation/loss': 1.7027853727340698, 'validation/num_examples': 50000, 'test/accuracy': 0.5017000436782837, 'test/loss': 2.382734537124634, 'test/num_examples': 10000, 'score': 8192.537194252014, 'total_duration': 8495.38295841217, 'accumulated_submission_time': 8192.537194252014, 'accumulated_eval_time': 301.5229160785675, 'accumulated_logging_time': 0.4715721607208252, 'global_step': 24144, 'preemption_count': 0}), (25656, {'train/accuracy': 0.6940768361091614, 'train/loss': 1.4729230403900146, 'validation/accuracy': 0.6380199790000916, 'validation/loss': 1.7212127447128296, 'validation/num_examples': 50000, 'test/accuracy': 0.5093000531196594, 'test/loss': 2.390270709991455, 'test/num_examples': 10000, 'score': 8702.589481115341, 'total_duration': 9023.039636611938, 'accumulated_submission_time': 8702.589481115341, 'accumulated_eval_time': 319.04385137557983, 'accumulated_logging_time': 0.5028097629547119, 'global_step': 25656, 'preemption_count': 0}), (27167, {'train/accuracy': 0.7210817933082581, 'train/loss': 1.3525702953338623, 'validation/accuracy': 0.6334599852561951, 'validation/loss': 1.7323843240737915, 'validation/num_examples': 50000, 'test/accuracy': 0.5049999952316284, 'test/loss': 2.398658514022827, 'test/num_examples': 10000, 'score': 9212.57828116417, 'total_duration': 9550.835167884827, 'accumulated_submission_time': 9212.57828116417, 'accumulated_eval_time': 336.7666335105896, 'accumulated_logging_time': 0.5334517955780029, 'global_step': 27167, 'preemption_count': 0}), (28679, {'train/accuracy': 0.7137874364852905, 'train/loss': 1.3917793035507202, 'validation/accuracy': 0.6363599896430969, 'validation/loss': 1.7337682247161865, 'validation/num_examples': 50000, 'test/accuracy': 0.5118000507354736, 'test/loss': 2.377542734146118, 'test/num_examples': 10000, 'score': 9722.724038362503, 'total_duration': 10078.865000724792, 'accumulated_submission_time': 9722.724038362503, 'accumulated_eval_time': 354.5671169757843, 'accumulated_logging_time': 0.5640599727630615, 'global_step': 28679, 'preemption_count': 0}), (30190, {'train/accuracy': 0.7102399468421936, 'train/loss': 1.383233904838562, 'validation/accuracy': 0.6411399841308594, 'validation/loss': 1.6908059120178223, 'validation/num_examples': 50000, 'test/accuracy': 0.5134000182151794, 'test/loss': 2.3572490215301514, 'test/num_examples': 10000, 'score': 10232.818863868713, 'total_duration': 10606.80100107193, 'accumulated_submission_time': 10232.818863868713, 'accumulated_eval_time': 372.32369208335876, 'accumulated_logging_time': 0.5949838161468506, 'global_step': 30190, 'preemption_count': 0}), (31701, {'train/accuracy': 0.7122528553009033, 'train/loss': 1.4000403881072998, 'validation/accuracy': 0.6447399854660034, 'validation/loss': 1.6911596059799194, 'validation/num_examples': 50000, 'test/accuracy': 0.515500009059906, 'test/loss': 2.3480465412139893, 'test/num_examples': 10000, 'score': 10743.079125404358, 'total_duration': 11135.051835298538, 'accumulated_submission_time': 10743.079125404358, 'accumulated_eval_time': 390.22929978370667, 'accumulated_logging_time': 0.6257216930389404, 'global_step': 31701, 'preemption_count': 0}), (33213, {'train/accuracy': 0.6996970772743225, 'train/loss': 1.4557379484176636, 'validation/accuracy': 0.6355999708175659, 'validation/loss': 1.7373415231704712, 'validation/num_examples': 50000, 'test/accuracy': 0.5081000328063965, 'test/loss': 2.3862760066986084, 'test/num_examples': 10000, 'score': 11253.20901465416, 'total_duration': 11662.901600122452, 'accumulated_submission_time': 11253.20901465416, 'accumulated_eval_time': 407.86250853538513, 'accumulated_logging_time': 0.65826416015625, 'global_step': 33213, 'preemption_count': 0}), (34726, {'train/accuracy': 0.7174744606018066, 'train/loss': 1.3685293197631836, 'validation/accuracy': 0.6532599925994873, 'validation/loss': 1.6530615091323853, 'validation/num_examples': 50000, 'test/accuracy': 0.5192000269889832, 'test/loss': 2.314382553100586, 'test/num_examples': 10000, 'score': 11763.446279764175, 'total_duration': 12191.024013519287, 'accumulated_submission_time': 11763.446279764175, 'accumulated_eval_time': 425.66084122657776, 'accumulated_logging_time': 0.6921558380126953, 'global_step': 34726, 'preemption_count': 0}), (36238, {'train/accuracy': 0.7438416481018066, 'train/loss': 1.2298567295074463, 'validation/accuracy': 0.6535599827766418, 'validation/loss': 1.6235462427139282, 'validation/num_examples': 50000, 'test/accuracy': 0.5265000462532043, 'test/loss': 2.281526565551758, 'test/num_examples': 10000, 'score': 12273.675210475922, 'total_duration': 12718.935508489609, 'accumulated_submission_time': 12273.675210475922, 'accumulated_eval_time': 443.2582674026489, 'accumulated_logging_time': 0.7246830463409424, 'global_step': 36238, 'preemption_count': 0}), (37751, {'train/accuracy': 0.7174545526504517, 'train/loss': 1.3231170177459717, 'validation/accuracy': 0.6446200013160706, 'validation/loss': 1.666609764099121, 'validation/num_examples': 50000, 'test/accuracy': 0.5151000022888184, 'test/loss': 2.3460588455200195, 'test/num_examples': 10000, 'score': 12783.86525940895, 'total_duration': 13246.783225536346, 'accumulated_submission_time': 12783.86525940895, 'accumulated_eval_time': 460.8275353908539, 'accumulated_logging_time': 0.7599701881408691, 'global_step': 37751, 'preemption_count': 0}), (39263, {'train/accuracy': 0.7337173223495483, 'train/loss': 1.286712884902954, 'validation/accuracy': 0.6584399938583374, 'validation/loss': 1.6077271699905396, 'validation/num_examples': 50000, 'test/accuracy': 0.5318000316619873, 'test/loss': 2.2832562923431396, 'test/num_examples': 10000, 'score': 13293.79348897934, 'total_duration': 13774.605125188828, 'accumulated_submission_time': 13293.79348897934, 'accumulated_eval_time': 478.63541746139526, 'accumulated_logging_time': 0.7926428318023682, 'global_step': 39263, 'preemption_count': 0}), (40775, {'train/accuracy': 0.7258848547935486, 'train/loss': 1.2848888635635376, 'validation/accuracy': 0.6577799916267395, 'validation/loss': 1.597143530845642, 'validation/num_examples': 50000, 'test/accuracy': 0.5288000106811523, 'test/loss': 2.251645803451538, 'test/num_examples': 10000, 'score': 13803.70843219757, 'total_duration': 14303.069906711578, 'accumulated_submission_time': 13803.70843219757, 'accumulated_eval_time': 497.0984447002411, 'accumulated_logging_time': 0.8266785144805908, 'global_step': 40775, 'preemption_count': 0}), (42288, {'train/accuracy': 0.7098811864852905, 'train/loss': 1.3791455030441284, 'validation/accuracy': 0.6428200006484985, 'validation/loss': 1.6814510822296143, 'validation/num_examples': 50000, 'test/accuracy': 0.5174000263214111, 'test/loss': 2.3248226642608643, 'test/num_examples': 10000, 'score': 14313.930534362793, 'total_duration': 14831.188447713852, 'accumulated_submission_time': 14313.930534362793, 'accumulated_eval_time': 514.9081664085388, 'accumulated_logging_time': 0.86090087890625, 'global_step': 42288, 'preemption_count': 0}), (43801, {'train/accuracy': 0.7302295565605164, 'train/loss': 1.3141027688980103, 'validation/accuracy': 0.6615200042724609, 'validation/loss': 1.6093543767929077, 'validation/num_examples': 50000, 'test/accuracy': 0.5326000452041626, 'test/loss': 2.2799079418182373, 'test/num_examples': 10000, 'score': 14824.281010389328, 'total_duration': 15359.110605239868, 'accumulated_submission_time': 14824.281010389328, 'accumulated_eval_time': 532.3907444477081, 'accumulated_logging_time': 0.8976097106933594, 'global_step': 43801, 'preemption_count': 0}), (45314, {'train/accuracy': 0.7517538070678711, 'train/loss': 1.200649619102478, 'validation/accuracy': 0.6576399803161621, 'validation/loss': 1.604137659072876, 'validation/num_examples': 50000, 'test/accuracy': 0.5347000360488892, 'test/loss': 2.256589412689209, 'test/num_examples': 10000, 'score': 15334.48612332344, 'total_duration': 15887.034386634827, 'accumulated_submission_time': 15334.48612332344, 'accumulated_eval_time': 550.0225744247437, 'accumulated_logging_time': 0.9313144683837891, 'global_step': 45314, 'preemption_count': 0}), (46827, {'train/accuracy': 0.7456353306770325, 'train/loss': 1.2059592008590698, 'validation/accuracy': 0.6634799838066101, 'validation/loss': 1.5565850734710693, 'validation/num_examples': 50000, 'test/accuracy': 0.539900004863739, 'test/loss': 2.1983628273010254, 'test/num_examples': 10000, 'score': 15844.531326293945, 'total_duration': 16414.89757823944, 'accumulated_submission_time': 15844.531326293945, 'accumulated_eval_time': 567.7500638961792, 'accumulated_logging_time': 0.9677863121032715, 'global_step': 46827, 'preemption_count': 0}), (48339, {'train/accuracy': 0.735750138759613, 'train/loss': 1.2548892498016357, 'validation/accuracy': 0.6612600088119507, 'validation/loss': 1.5838819742202759, 'validation/num_examples': 50000, 'test/accuracy': 0.5368000268936157, 'test/loss': 2.2433271408081055, 'test/num_examples': 10000, 'score': 16354.600351333618, 'total_duration': 16942.508352041245, 'accumulated_submission_time': 16354.600351333618, 'accumulated_eval_time': 585.2028439044952, 'accumulated_logging_time': 1.00309419631958, 'global_step': 48339, 'preemption_count': 0}), (49852, {'train/accuracy': 0.7357302308082581, 'train/loss': 1.2622418403625488, 'validation/accuracy': 0.6646999716758728, 'validation/loss': 1.571779489517212, 'validation/num_examples': 50000, 'test/accuracy': 0.534000039100647, 'test/loss': 2.243760108947754, 'test/num_examples': 10000, 'score': 16864.72789144516, 'total_duration': 17470.579726219177, 'accumulated_submission_time': 16864.72789144516, 'accumulated_eval_time': 603.0600016117096, 'accumulated_logging_time': 1.037532091140747, 'global_step': 49852, 'preemption_count': 0}), (51365, {'train/accuracy': 0.7381417155265808, 'train/loss': 1.2553119659423828, 'validation/accuracy': 0.6695399880409241, 'validation/loss': 1.566043734550476, 'validation/num_examples': 50000, 'test/accuracy': 0.5473000407218933, 'test/loss': 2.198474884033203, 'test/num_examples': 10000, 'score': 17374.956661462784, 'total_duration': 17999.086223602295, 'accumulated_submission_time': 17374.956661462784, 'accumulated_eval_time': 621.2467834949493, 'accumulated_logging_time': 1.0746331214904785, 'global_step': 51365, 'preemption_count': 0}), (52877, {'train/accuracy': 0.7241908311843872, 'train/loss': 1.344178318977356, 'validation/accuracy': 0.6525599956512451, 'validation/loss': 1.6665358543395996, 'validation/num_examples': 50000, 'test/accuracy': 0.5195000171661377, 'test/loss': 2.337864875793457, 'test/num_examples': 10000, 'score': 17884.904060840607, 'total_duration': 18526.888315439224, 'accumulated_submission_time': 17884.904060840607, 'accumulated_eval_time': 639.0135197639465, 'accumulated_logging_time': 1.109938621520996, 'global_step': 52877, 'preemption_count': 0}), (54390, {'train/accuracy': 0.750996470451355, 'train/loss': 1.2404024600982666, 'validation/accuracy': 0.6648600101470947, 'validation/loss': 1.6224167346954346, 'validation/num_examples': 50000, 'test/accuracy': 0.5393000245094299, 'test/loss': 2.2847721576690674, 'test/num_examples': 10000, 'score': 18395.117757558823, 'total_duration': 19054.84995698929, 'accumulated_submission_time': 18395.117757558823, 'accumulated_eval_time': 656.6714797019958, 'accumulated_logging_time': 1.147374153137207, 'global_step': 54390, 'preemption_count': 0}), (55902, {'train/accuracy': 0.7460737824440002, 'train/loss': 1.2308549880981445, 'validation/accuracy': 0.6655600070953369, 'validation/loss': 1.5786885023117065, 'validation/num_examples': 50000, 'test/accuracy': 0.5325000286102295, 'test/loss': 2.248415231704712, 'test/num_examples': 10000, 'score': 18905.16711997986, 'total_duration': 19582.74394917488, 'accumulated_submission_time': 18905.16711997986, 'accumulated_eval_time': 674.4238469600677, 'accumulated_logging_time': 1.1857051849365234, 'global_step': 55902, 'preemption_count': 0}), (57414, {'train/accuracy': 0.7460139989852905, 'train/loss': 1.2035185098648071, 'validation/accuracy': 0.6686199903488159, 'validation/loss': 1.5482203960418701, 'validation/num_examples': 50000, 'test/accuracy': 0.5427000522613525, 'test/loss': 2.203951358795166, 'test/num_examples': 10000, 'score': 19415.267368793488, 'total_duration': 20110.572977542877, 'accumulated_submission_time': 19415.267368793488, 'accumulated_eval_time': 692.0639700889587, 'accumulated_logging_time': 1.2219395637512207, 'global_step': 57414, 'preemption_count': 0}), (58926, {'train/accuracy': 0.7444595098495483, 'train/loss': 1.2356679439544678, 'validation/accuracy': 0.6713399887084961, 'validation/loss': 1.556386947631836, 'validation/num_examples': 50000, 'test/accuracy': 0.5497000217437744, 'test/loss': 2.2031824588775635, 'test/num_examples': 10000, 'score': 19925.24661397934, 'total_duration': 20638.287900447845, 'accumulated_submission_time': 19925.24661397934, 'accumulated_eval_time': 709.7101130485535, 'accumulated_logging_time': 1.257903814315796, 'global_step': 58926, 'preemption_count': 0}), (60438, {'train/accuracy': 0.7448979616165161, 'train/loss': 1.266387701034546, 'validation/accuracy': 0.6719599962234497, 'validation/loss': 1.57454514503479, 'validation/num_examples': 50000, 'test/accuracy': 0.5458000302314758, 'test/loss': 2.216935634613037, 'test/num_examples': 10000, 'score': 20435.238516807556, 'total_duration': 21166.16930627823, 'accumulated_submission_time': 20435.238516807556, 'accumulated_eval_time': 727.5052762031555, 'accumulated_logging_time': 1.2989416122436523, 'global_step': 60438, 'preemption_count': 0}), (61951, {'train/accuracy': 0.7596260905265808, 'train/loss': 1.151545763015747, 'validation/accuracy': 0.6668800115585327, 'validation/loss': 1.5589600801467896, 'validation/num_examples': 50000, 'test/accuracy': 0.5335000157356262, 'test/loss': 2.219144105911255, 'test/num_examples': 10000, 'score': 20945.41634607315, 'total_duration': 21694.11650276184, 'accumulated_submission_time': 20945.41634607315, 'accumulated_eval_time': 745.1842300891876, 'accumulated_logging_time': 1.3371562957763672, 'global_step': 61951, 'preemption_count': 0}), (63463, {'train/accuracy': 0.7645288705825806, 'train/loss': 1.1519296169281006, 'validation/accuracy': 0.6728799939155579, 'validation/loss': 1.5548346042633057, 'validation/num_examples': 50000, 'test/accuracy': 0.5394999980926514, 'test/loss': 2.243274211883545, 'test/num_examples': 10000, 'score': 21455.393117904663, 'total_duration': 22222.007017850876, 'accumulated_submission_time': 21455.393117904663, 'accumulated_eval_time': 763.0073924064636, 'accumulated_logging_time': 1.3743011951446533, 'global_step': 63463, 'preemption_count': 0}), (64976, {'train/accuracy': 0.7543845772743225, 'train/loss': 1.1895458698272705, 'validation/accuracy': 0.6688799858093262, 'validation/loss': 1.562997817993164, 'validation/num_examples': 50000, 'test/accuracy': 0.5503000020980835, 'test/loss': 2.204355478286743, 'test/num_examples': 10000, 'score': 21965.388853788376, 'total_duration': 22749.81556200981, 'accumulated_submission_time': 21965.388853788376, 'accumulated_eval_time': 780.7278144359589, 'accumulated_logging_time': 1.4133169651031494, 'global_step': 64976, 'preemption_count': 0}), (66488, {'train/accuracy': 0.7489436864852905, 'train/loss': 1.2165971994400024, 'validation/accuracy': 0.6738399863243103, 'validation/loss': 1.5674960613250732, 'validation/num_examples': 50000, 'test/accuracy': 0.5478000044822693, 'test/loss': 2.2059414386749268, 'test/num_examples': 10000, 'score': 22475.54665660858, 'total_duration': 23277.764575958252, 'accumulated_submission_time': 22475.54665660858, 'accumulated_eval_time': 798.4267275333405, 'accumulated_logging_time': 1.4519784450531006, 'global_step': 66488, 'preemption_count': 0}), (68001, {'train/accuracy': 0.7564173936843872, 'train/loss': 1.1640042066574097, 'validation/accuracy': 0.6771000027656555, 'validation/loss': 1.5072546005249023, 'validation/num_examples': 50000, 'test/accuracy': 0.5573000311851501, 'test/loss': 2.136253595352173, 'test/num_examples': 10000, 'score': 22985.491063833237, 'total_duration': 23805.332373142242, 'accumulated_submission_time': 22985.491063833237, 'accumulated_eval_time': 815.9583787918091, 'accumulated_logging_time': 1.4906814098358154, 'global_step': 68001, 'preemption_count': 0}), (69513, {'train/accuracy': 0.7506178021430969, 'train/loss': 1.205312728881836, 'validation/accuracy': 0.6768999695777893, 'validation/loss': 1.5448400974273682, 'validation/num_examples': 50000, 'test/accuracy': 0.5519000291824341, 'test/loss': 2.177412986755371, 'test/num_examples': 10000, 'score': 23495.39884543419, 'total_duration': 24333.28107357025, 'accumulated_submission_time': 23495.39884543419, 'accumulated_eval_time': 833.9057495594025, 'accumulated_logging_time': 1.5299150943756104, 'global_step': 69513, 'preemption_count': 0}), (71026, {'train/accuracy': 0.7942841053009033, 'train/loss': 1.016778826713562, 'validation/accuracy': 0.6810799837112427, 'validation/loss': 1.4947758913040161, 'validation/num_examples': 50000, 'test/accuracy': 0.5546000003814697, 'test/loss': 2.1463091373443604, 'test/num_examples': 10000, 'score': 24005.601548433304, 'total_duration': 24861.00675535202, 'accumulated_submission_time': 24005.601548433304, 'accumulated_eval_time': 851.3368241786957, 'accumulated_logging_time': 1.5676684379577637, 'global_step': 71026, 'preemption_count': 0}), (72538, {'train/accuracy': 0.7712053656578064, 'train/loss': 1.1332015991210938, 'validation/accuracy': 0.6800199747085571, 'validation/loss': 1.5349023342132568, 'validation/num_examples': 50000, 'test/accuracy': 0.5504000186920166, 'test/loss': 2.2045271396636963, 'test/num_examples': 10000, 'score': 24515.58721637726, 'total_duration': 25389.0153260231, 'accumulated_submission_time': 24515.58721637726, 'accumulated_eval_time': 869.2670221328735, 'accumulated_logging_time': 1.6069247722625732, 'global_step': 72538, 'preemption_count': 0}), (74051, {'train/accuracy': 0.7703882455825806, 'train/loss': 1.119797945022583, 'validation/accuracy': 0.6869999766349792, 'validation/loss': 1.496230125427246, 'validation/num_examples': 50000, 'test/accuracy': 0.5599000453948975, 'test/loss': 2.144322395324707, 'test/num_examples': 10000, 'score': 25025.69666481018, 'total_duration': 25916.87974834442, 'accumulated_submission_time': 25025.69666481018, 'accumulated_eval_time': 886.9353656768799, 'accumulated_logging_time': 1.6402819156646729, 'global_step': 74051, 'preemption_count': 0}), (75564, {'train/accuracy': 0.7689333558082581, 'train/loss': 1.1266475915908813, 'validation/accuracy': 0.6871799826622009, 'validation/loss': 1.490763545036316, 'validation/num_examples': 50000, 'test/accuracy': 0.5641000270843506, 'test/loss': 2.1208748817443848, 'test/num_examples': 10000, 'score': 25535.903984308243, 'total_duration': 26444.623901844025, 'accumulated_submission_time': 25535.903984308243, 'accumulated_eval_time': 904.3809192180634, 'accumulated_logging_time': 1.6784143447875977, 'global_step': 75564, 'preemption_count': 0}), (77077, {'train/accuracy': 0.7555803656578064, 'train/loss': 1.252349853515625, 'validation/accuracy': 0.6780799627304077, 'validation/loss': 1.591875433921814, 'validation/num_examples': 50000, 'test/accuracy': 0.5550000071525574, 'test/loss': 2.229222297668457, 'test/num_examples': 10000, 'score': 26045.897315502167, 'total_duration': 26972.284712553024, 'accumulated_submission_time': 26045.897315502167, 'accumulated_eval_time': 921.9570591449738, 'accumulated_logging_time': 1.7172760963439941, 'global_step': 77077, 'preemption_count': 0}), (78589, {'train/accuracy': 0.7691724896430969, 'train/loss': 1.146484375, 'validation/accuracy': 0.6866399645805359, 'validation/loss': 1.4988430738449097, 'validation/num_examples': 50000, 'test/accuracy': 0.5601000189781189, 'test/loss': 2.1550445556640625, 'test/num_examples': 10000, 'score': 26555.934617996216, 'total_duration': 27500.048808574677, 'accumulated_submission_time': 26555.934617996216, 'accumulated_eval_time': 939.5918412208557, 'accumulated_logging_time': 1.7566168308258057, 'global_step': 78589, 'preemption_count': 0}), (80103, {'train/accuracy': 0.8092314600944519, 'train/loss': 0.9729456305503845, 'validation/accuracy': 0.6866999864578247, 'validation/loss': 1.4889295101165771, 'validation/num_examples': 50000, 'test/accuracy': 0.5586000084877014, 'test/loss': 2.127764940261841, 'test/num_examples': 10000, 'score': 27066.09313583374, 'total_duration': 28029.13027572632, 'accumulated_submission_time': 27066.09313583374, 'accumulated_eval_time': 958.4196372032166, 'accumulated_logging_time': 1.7997441291809082, 'global_step': 80103, 'preemption_count': 0}), (81614, {'train/accuracy': 0.7765266299247742, 'train/loss': 1.067360758781433, 'validation/accuracy': 0.6843599677085876, 'validation/loss': 1.4753485918045044, 'validation/num_examples': 50000, 'test/accuracy': 0.5557000041007996, 'test/loss': 2.1231517791748047, 'test/num_examples': 10000, 'score': 27576.048866033554, 'total_duration': 28556.85472536087, 'accumulated_submission_time': 27576.048866033554, 'accumulated_eval_time': 976.1020576953888, 'accumulated_logging_time': 1.8335380554199219, 'global_step': 81614, 'preemption_count': 0}), (83126, {'train/accuracy': 0.7771045565605164, 'train/loss': 1.1030161380767822, 'validation/accuracy': 0.6881399750709534, 'validation/loss': 1.486668586730957, 'validation/num_examples': 50000, 'test/accuracy': 0.5627000331878662, 'test/loss': 2.139356851577759, 'test/num_examples': 10000, 'score': 28085.96573448181, 'total_duration': 29084.32238149643, 'accumulated_submission_time': 28085.96573448181, 'accumulated_eval_time': 993.557685136795, 'accumulated_logging_time': 1.8761115074157715, 'global_step': 83126, 'preemption_count': 0}), (84638, {'train/accuracy': 0.7768853306770325, 'train/loss': 1.0869734287261963, 'validation/accuracy': 0.693399965763092, 'validation/loss': 1.4536720514297485, 'validation/num_examples': 50000, 'test/accuracy': 0.5644000172615051, 'test/loss': 2.1074471473693848, 'test/num_examples': 10000, 'score': 28596.036551475525, 'total_duration': 29612.1298763752, 'accumulated_submission_time': 28596.036551475525, 'accumulated_eval_time': 1011.1986262798309, 'accumulated_logging_time': 1.9192132949829102, 'global_step': 84638, 'preemption_count': 0}), (86151, {'train/accuracy': 0.7732979655265808, 'train/loss': 1.0890520811080933, 'validation/accuracy': 0.6902599930763245, 'validation/loss': 1.4564690589904785, 'validation/num_examples': 50000, 'test/accuracy': 0.5592000484466553, 'test/loss': 2.1242754459381104, 'test/num_examples': 10000, 'score': 29106.105019569397, 'total_duration': 30139.81373643875, 'accumulated_submission_time': 29106.105019569397, 'accumulated_eval_time': 1028.7158319950104, 'accumulated_logging_time': 1.964526891708374, 'global_step': 86151, 'preemption_count': 0}), (87664, {'train/accuracy': 0.755281388759613, 'train/loss': 1.1695573329925537, 'validation/accuracy': 0.6784399747848511, 'validation/loss': 1.5213377475738525, 'validation/num_examples': 50000, 'test/accuracy': 0.5514000058174133, 'test/loss': 2.184067964553833, 'test/num_examples': 10000, 'score': 29616.25096130371, 'total_duration': 30667.99164557457, 'accumulated_submission_time': 29616.25096130371, 'accumulated_eval_time': 1046.6535975933075, 'accumulated_logging_time': 2.0054867267608643, 'global_step': 87664, 'preemption_count': 0}), (89177, {'train/accuracy': 0.7955994606018066, 'train/loss': 1.0491045713424683, 'validation/accuracy': 0.6782199740409851, 'validation/loss': 1.5385297536849976, 'validation/num_examples': 50000, 'test/accuracy': 0.5473000407218933, 'test/loss': 2.2165260314941406, 'test/num_examples': 10000, 'score': 30126.34645795822, 'total_duration': 31195.745640039444, 'accumulated_submission_time': 30126.34645795822, 'accumulated_eval_time': 1064.221899986267, 'accumulated_logging_time': 2.043313980102539, 'global_step': 89177, 'preemption_count': 0}), (90690, {'train/accuracy': 0.8026148080825806, 'train/loss': 0.9901580810546875, 'validation/accuracy': 0.7046200037002563, 'validation/loss': 1.4159032106399536, 'validation/num_examples': 50000, 'test/accuracy': 0.572700023651123, 'test/loss': 2.065574884414673, 'test/num_examples': 10000, 'score': 30636.468733549118, 'total_duration': 31723.53106689453, 'accumulated_submission_time': 30636.468733549118, 'accumulated_eval_time': 1081.7890536785126, 'accumulated_logging_time': 2.085658311843872, 'global_step': 90690, 'preemption_count': 0}), (92203, {'train/accuracy': 0.7906369566917419, 'train/loss': 1.0359426736831665, 'validation/accuracy': 0.6946600079536438, 'validation/loss': 1.449451208114624, 'validation/num_examples': 50000, 'test/accuracy': 0.5664000511169434, 'test/loss': 2.099205255508423, 'test/num_examples': 10000, 'score': 31146.662981510162, 'total_duration': 32251.288065195084, 'accumulated_submission_time': 31146.662981510162, 'accumulated_eval_time': 1099.251507282257, 'accumulated_logging_time': 2.132451057434082, 'global_step': 92203, 'preemption_count': 0}), (93715, {'train/accuracy': 0.7890027165412903, 'train/loss': 1.048642635345459, 'validation/accuracy': 0.6960399746894836, 'validation/loss': 1.4489803314208984, 'validation/num_examples': 50000, 'test/accuracy': 0.5732000470161438, 'test/loss': 2.0790419578552246, 'test/num_examples': 10000, 'score': 31656.57655262947, 'total_duration': 32778.84194803238, 'accumulated_submission_time': 31656.57655262947, 'accumulated_eval_time': 1116.7954907417297, 'accumulated_logging_time': 2.174596071243286, 'global_step': 93715, 'preemption_count': 0}), (95228, {'train/accuracy': 0.7940050959587097, 'train/loss': 1.0202025175094604, 'validation/accuracy': 0.706059992313385, 'validation/loss': 1.4011818170547485, 'validation/num_examples': 50000, 'test/accuracy': 0.5764999985694885, 'test/loss': 2.050769567489624, 'test/num_examples': 10000, 'score': 32166.51022219658, 'total_duration': 33306.58448624611, 'accumulated_submission_time': 32166.51022219658, 'accumulated_eval_time': 1134.5098896026611, 'accumulated_logging_time': 2.2162539958953857, 'global_step': 95228, 'preemption_count': 0}), (96741, {'train/accuracy': 0.7877470850944519, 'train/loss': 1.0436723232269287, 'validation/accuracy': 0.6979999542236328, 'validation/loss': 1.427424669265747, 'validation/num_examples': 50000, 'test/accuracy': 0.5685000419616699, 'test/loss': 2.087905168533325, 'test/num_examples': 10000, 'score': 32676.71812939644, 'total_duration': 33834.412241220474, 'accumulated_submission_time': 32676.71812939644, 'accumulated_eval_time': 1152.0206370353699, 'accumulated_logging_time': 2.271630048751831, 'global_step': 96741, 'preemption_count': 0}), (98254, {'train/accuracy': 0.8298588991165161, 'train/loss': 0.8838488459587097, 'validation/accuracy': 0.7114599943161011, 'validation/loss': 1.3823779821395874, 'validation/num_examples': 50000, 'test/accuracy': 0.5869000554084778, 'test/loss': 2.0103774070739746, 'test/num_examples': 10000, 'score': 33186.897922992706, 'total_duration': 34362.17217421532, 'accumulated_submission_time': 33186.897922992706, 'accumulated_eval_time': 1169.504544019699, 'accumulated_logging_time': 2.3140299320220947, 'global_step': 98254, 'preemption_count': 0}), (99767, {'train/accuracy': 0.8191565275192261, 'train/loss': 0.9219579696655273, 'validation/accuracy': 0.7096199989318848, 'validation/loss': 1.3757820129394531, 'validation/num_examples': 50000, 'test/accuracy': 0.5852000117301941, 'test/loss': 2.0205318927764893, 'test/num_examples': 10000, 'score': 33696.980467796326, 'total_duration': 34889.91017913818, 'accumulated_submission_time': 33696.980467796326, 'accumulated_eval_time': 1187.0604236125946, 'accumulated_logging_time': 2.3605294227600098, 'global_step': 99767, 'preemption_count': 0}), (101280, {'train/accuracy': 0.8095503449440002, 'train/loss': 0.9634189605712891, 'validation/accuracy': 0.7093799710273743, 'validation/loss': 1.3969210386276245, 'validation/num_examples': 50000, 'test/accuracy': 0.5839000344276428, 'test/loss': 2.0435094833374023, 'test/num_examples': 10000, 'score': 34206.91540932655, 'total_duration': 35417.475497722626, 'accumulated_submission_time': 34206.91540932655, 'accumulated_eval_time': 1204.593267440796, 'accumulated_logging_time': 2.404573440551758, 'global_step': 101280, 'preemption_count': 0}), (102793, {'train/accuracy': 0.8084542155265808, 'train/loss': 0.9739400744438171, 'validation/accuracy': 0.7127400040626526, 'validation/loss': 1.390412449836731, 'validation/num_examples': 50000, 'test/accuracy': 0.5825000405311584, 'test/loss': 2.034860849380493, 'test/num_examples': 10000, 'score': 34717.08581137657, 'total_duration': 35945.378918647766, 'accumulated_submission_time': 34717.08581137657, 'accumulated_eval_time': 1222.228625535965, 'accumulated_logging_time': 2.44966459274292, 'global_step': 102793, 'preemption_count': 0}), (104307, {'train/accuracy': 0.7966358065605164, 'train/loss': 0.990210771560669, 'validation/accuracy': 0.7057600021362305, 'validation/loss': 1.3867374658584595, 'validation/num_examples': 50000, 'test/accuracy': 0.5819000005722046, 'test/loss': 2.023756504058838, 'test/num_examples': 10000, 'score': 35227.30777025223, 'total_duration': 36473.11824512482, 'accumulated_submission_time': 35227.30777025223, 'accumulated_eval_time': 1239.6494569778442, 'accumulated_logging_time': 2.4930479526519775, 'global_step': 104307, 'preemption_count': 0}), (105819, {'train/accuracy': 0.809968888759613, 'train/loss': 0.9250097274780273, 'validation/accuracy': 0.7107399702072144, 'validation/loss': 1.3485785722732544, 'validation/num_examples': 50000, 'test/accuracy': 0.5885000228881836, 'test/loss': 2.001072645187378, 'test/num_examples': 10000, 'score': 35737.31253170967, 'total_duration': 37000.57236742973, 'accumulated_submission_time': 35737.31253170967, 'accumulated_eval_time': 1257.0016777515411, 'accumulated_logging_time': 2.5371103286743164, 'global_step': 105819, 'preemption_count': 0}), (107331, {'train/accuracy': 0.8362364172935486, 'train/loss': 0.838945746421814, 'validation/accuracy': 0.7157599925994873, 'validation/loss': 1.3426353931427002, 'validation/num_examples': 50000, 'test/accuracy': 0.5887000560760498, 'test/loss': 1.985122561454773, 'test/num_examples': 10000, 'score': 36247.26858711243, 'total_duration': 37528.30771255493, 'accumulated_submission_time': 36247.26858711243, 'accumulated_eval_time': 1274.6831283569336, 'accumulated_logging_time': 2.5816597938537598, 'global_step': 107331, 'preemption_count': 0}), (108844, {'train/accuracy': 0.8233019709587097, 'train/loss': 0.8994101285934448, 'validation/accuracy': 0.7140600085258484, 'validation/loss': 1.3612909317016602, 'validation/num_examples': 50000, 'test/accuracy': 0.5867000222206116, 'test/loss': 2.025434732437134, 'test/num_examples': 10000, 'score': 36757.356506347656, 'total_duration': 38056.19006371498, 'accumulated_submission_time': 36757.356506347656, 'accumulated_eval_time': 1292.3768393993378, 'accumulated_logging_time': 2.6290013790130615, 'global_step': 108844, 'preemption_count': 0}), (110357, {'train/accuracy': 0.8215481042861938, 'train/loss': 0.8967534899711609, 'validation/accuracy': 0.7145199775695801, 'validation/loss': 1.3505741357803345, 'validation/num_examples': 50000, 'test/accuracy': 0.5910000205039978, 'test/loss': 1.9762768745422363, 'test/num_examples': 10000, 'score': 37267.36195087433, 'total_duration': 38583.91264152527, 'accumulated_submission_time': 37267.36195087433, 'accumulated_eval_time': 1309.9977324008942, 'accumulated_logging_time': 2.6728272438049316, 'global_step': 110357, 'preemption_count': 0}), (111870, {'train/accuracy': 0.8194156289100647, 'train/loss': 0.8783233761787415, 'validation/accuracy': 0.7137799859046936, 'validation/loss': 1.3280168771743774, 'validation/num_examples': 50000, 'test/accuracy': 0.5868000388145447, 'test/loss': 1.9892569780349731, 'test/num_examples': 10000, 'score': 37777.49592471123, 'total_duration': 39111.43874955177, 'accumulated_submission_time': 37777.49592471123, 'accumulated_eval_time': 1327.2801899909973, 'accumulated_logging_time': 2.7286429405212402, 'global_step': 111870, 'preemption_count': 0}), (113384, {'train/accuracy': 0.8175222873687744, 'train/loss': 0.9152930974960327, 'validation/accuracy': 0.7148399949073792, 'validation/loss': 1.350134015083313, 'validation/num_examples': 50000, 'test/accuracy': 0.5896000266075134, 'test/loss': 1.9782979488372803, 'test/num_examples': 10000, 'score': 38287.695055007935, 'total_duration': 39639.17041897774, 'accumulated_submission_time': 38287.695055007935, 'accumulated_eval_time': 1344.7098760604858, 'accumulated_logging_time': 2.7765440940856934, 'global_step': 113384, 'preemption_count': 0}), (114897, {'train/accuracy': 0.8275271058082581, 'train/loss': 0.9153132438659668, 'validation/accuracy': 0.7184000015258789, 'validation/loss': 1.3692786693572998, 'validation/num_examples': 50000, 'test/accuracy': 0.5919000506401062, 'test/loss': 2.0019583702087402, 'test/num_examples': 10000, 'score': 38797.88224673271, 'total_duration': 40167.13142871857, 'accumulated_submission_time': 38797.88224673271, 'accumulated_eval_time': 1362.3821530342102, 'accumulated_logging_time': 2.825171709060669, 'global_step': 114897, 'preemption_count': 0}), (116409, {'train/accuracy': 0.848074734210968, 'train/loss': 0.8165138363838196, 'validation/accuracy': 0.7175999879837036, 'validation/loss': 1.3547149896621704, 'validation/num_examples': 50000, 'test/accuracy': 0.588200032711029, 'test/loss': 2.00327205657959, 'test/num_examples': 10000, 'score': 39307.83862376213, 'total_duration': 40694.7973818779, 'accumulated_submission_time': 39307.83862376213, 'accumulated_eval_time': 1379.9888272285461, 'accumulated_logging_time': 2.8746609687805176, 'global_step': 116409, 'preemption_count': 0}), (117922, {'train/accuracy': 0.8465999364852905, 'train/loss': 0.8292368650436401, 'validation/accuracy': 0.7242199778556824, 'validation/loss': 1.3349623680114746, 'validation/num_examples': 50000, 'test/accuracy': 0.5986000299453735, 'test/loss': 1.9471795558929443, 'test/num_examples': 10000, 'score': 39818.07694029808, 'total_duration': 41223.39574432373, 'accumulated_submission_time': 39818.07694029808, 'accumulated_eval_time': 1398.249297618866, 'accumulated_logging_time': 2.9214396476745605, 'global_step': 117922, 'preemption_count': 0}), (119436, {'train/accuracy': 0.8353993892669678, 'train/loss': 0.8475539088249207, 'validation/accuracy': 0.724399983882904, 'validation/loss': 1.3222719430923462, 'validation/num_examples': 50000, 'test/accuracy': 0.5958999991416931, 'test/loss': 1.961624264717102, 'test/num_examples': 10000, 'score': 40328.25413155556, 'total_duration': 41751.07174015045, 'accumulated_submission_time': 40328.25413155556, 'accumulated_eval_time': 1415.6428937911987, 'accumulated_logging_time': 2.9733994007110596, 'global_step': 119436, 'preemption_count': 0}), (120949, {'train/accuracy': 0.8393654227256775, 'train/loss': 0.8338555097579956, 'validation/accuracy': 0.7299799919128418, 'validation/loss': 1.3040684461593628, 'validation/num_examples': 50000, 'test/accuracy': 0.6011000275611877, 'test/loss': 1.9599751234054565, 'test/num_examples': 10000, 'score': 40838.449570417404, 'total_duration': 42279.31935048103, 'accumulated_submission_time': 40838.449570417404, 'accumulated_eval_time': 1433.5952577590942, 'accumulated_logging_time': 3.020388603210449, 'global_step': 120949, 'preemption_count': 0}), (122463, {'train/accuracy': 0.8434908986091614, 'train/loss': 0.8159423470497131, 'validation/accuracy': 0.730459988117218, 'validation/loss': 1.2901736497879028, 'validation/num_examples': 50000, 'test/accuracy': 0.6029000282287598, 'test/loss': 1.9311084747314453, 'test/num_examples': 10000, 'score': 41348.564710855484, 'total_duration': 42807.05680012703, 'accumulated_submission_time': 41348.564710855484, 'accumulated_eval_time': 1451.1136507987976, 'accumulated_logging_time': 3.0704591274261475, 'global_step': 122463, 'preemption_count': 0}), (123976, {'train/accuracy': 0.843191921710968, 'train/loss': 0.7751460671424866, 'validation/accuracy': 0.7268799543380737, 'validation/loss': 1.2697910070419312, 'validation/num_examples': 50000, 'test/accuracy': 0.604200005531311, 'test/loss': 1.9074426889419556, 'test/num_examples': 10000, 'score': 41858.57477927208, 'total_duration': 43334.73067235947, 'accumulated_submission_time': 41858.57477927208, 'accumulated_eval_time': 1468.6732609272003, 'accumulated_logging_time': 3.1229352951049805, 'global_step': 123976, 'preemption_count': 0}), (125489, {'train/accuracy': 0.84574294090271, 'train/loss': 0.8140817880630493, 'validation/accuracy': 0.7204599976539612, 'validation/loss': 1.347158670425415, 'validation/num_examples': 50000, 'test/accuracy': 0.5929000377655029, 'test/loss': 2.000288963317871, 'test/num_examples': 10000, 'score': 42368.690266132355, 'total_duration': 43862.41753697395, 'accumulated_submission_time': 42368.690266132355, 'accumulated_eval_time': 1486.1433503627777, 'accumulated_logging_time': 3.1712000370025635, 'global_step': 125489, 'preemption_count': 0}), (127001, {'train/accuracy': 0.851980984210968, 'train/loss': 0.7569774389266968, 'validation/accuracy': 0.7263799905776978, 'validation/loss': 1.2846840620040894, 'validation/num_examples': 50000, 'test/accuracy': 0.6015000343322754, 'test/loss': 1.922147512435913, 'test/num_examples': 10000, 'score': 42878.689056158066, 'total_duration': 44389.9714012146, 'accumulated_submission_time': 42878.689056158066, 'accumulated_eval_time': 1503.5928556919098, 'accumulated_logging_time': 3.2235565185546875, 'global_step': 127001, 'preemption_count': 0}), (128514, {'train/accuracy': 0.8525390625, 'train/loss': 0.7666031718254089, 'validation/accuracy': 0.7315399646759033, 'validation/loss': 1.2773808240890503, 'validation/num_examples': 50000, 'test/accuracy': 0.6098000407218933, 'test/loss': 1.9216325283050537, 'test/num_examples': 10000, 'score': 43388.700949430466, 'total_duration': 44917.63706827164, 'accumulated_submission_time': 43388.700949430466, 'accumulated_eval_time': 1521.1470046043396, 'accumulated_logging_time': 3.2711191177368164, 'global_step': 128514, 'preemption_count': 0}), (130027, {'train/accuracy': 0.8579002022743225, 'train/loss': 0.7536612153053284, 'validation/accuracy': 0.7330600023269653, 'validation/loss': 1.2795575857162476, 'validation/num_examples': 50000, 'test/accuracy': 0.6058000326156616, 'test/loss': 1.9297984838485718, 'test/num_examples': 10000, 'score': 43898.8369243145, 'total_duration': 45445.294552087784, 'accumulated_submission_time': 43898.8369243145, 'accumulated_eval_time': 1538.5677382946014, 'accumulated_logging_time': 3.3185129165649414, 'global_step': 130027, 'preemption_count': 0}), (131540, {'train/accuracy': 0.8606704473495483, 'train/loss': 0.7800890803337097, 'validation/accuracy': 0.7356199622154236, 'validation/loss': 1.2970260381698608, 'validation/num_examples': 50000, 'test/accuracy': 0.6103000044822693, 'test/loss': 1.926138162612915, 'test/num_examples': 10000, 'score': 44409.057758808136, 'total_duration': 45973.265880823135, 'accumulated_submission_time': 44409.057758808136, 'accumulated_eval_time': 1556.2172808647156, 'accumulated_logging_time': 3.3665919303894043, 'global_step': 131540, 'preemption_count': 0}), (133052, {'train/accuracy': 0.8704559803009033, 'train/loss': 0.7145001888275146, 'validation/accuracy': 0.737060010433197, 'validation/loss': 1.258089542388916, 'validation/num_examples': 50000, 'test/accuracy': 0.615600049495697, 'test/loss': 1.887601613998413, 'test/num_examples': 10000, 'score': 44919.02611017227, 'total_duration': 46500.744307756424, 'accumulated_submission_time': 44919.02611017227, 'accumulated_eval_time': 1573.6209378242493, 'accumulated_logging_time': 3.4200706481933594, 'global_step': 133052, 'preemption_count': 0}), (134565, {'train/accuracy': 0.8752790093421936, 'train/loss': 0.6944555640220642, 'validation/accuracy': 0.7367199659347534, 'validation/loss': 1.2754249572753906, 'validation/num_examples': 50000, 'test/accuracy': 0.6135000586509705, 'test/loss': 1.9048426151275635, 'test/num_examples': 10000, 'score': 45429.102596998215, 'total_duration': 47028.341700553894, 'accumulated_submission_time': 45429.102596998215, 'accumulated_eval_time': 1591.034093618393, 'accumulated_logging_time': 3.4738011360168457, 'global_step': 134565, 'preemption_count': 0}), (136078, {'train/accuracy': 0.8797034025192261, 'train/loss': 0.667981743812561, 'validation/accuracy': 0.7418999671936035, 'validation/loss': 1.2350200414657593, 'validation/num_examples': 50000, 'test/accuracy': 0.6200000047683716, 'test/loss': 1.8570964336395264, 'test/num_examples': 10000, 'score': 45939.301439762115, 'total_duration': 47556.14796924591, 'accumulated_submission_time': 45939.301439762115, 'accumulated_eval_time': 1608.5354924201965, 'accumulated_logging_time': 3.5263099670410156, 'global_step': 136078, 'preemption_count': 0}), (137591, {'train/accuracy': 0.8743423223495483, 'train/loss': 0.6792070865631104, 'validation/accuracy': 0.7400799989700317, 'validation/loss': 1.2348096370697021, 'validation/num_examples': 50000, 'test/accuracy': 0.6186000108718872, 'test/loss': 1.8752394914627075, 'test/num_examples': 10000, 'score': 46449.305644750595, 'total_duration': 48083.96310710907, 'accumulated_submission_time': 46449.305644750595, 'accumulated_eval_time': 1626.2444546222687, 'accumulated_logging_time': 3.5757358074188232, 'global_step': 137591, 'preemption_count': 0}), (139104, {'train/accuracy': 0.880301296710968, 'train/loss': 0.6912388801574707, 'validation/accuracy': 0.7443999648094177, 'validation/loss': 1.2525049448013306, 'validation/num_examples': 50000, 'test/accuracy': 0.6166000366210938, 'test/loss': 1.8933311700820923, 'test/num_examples': 10000, 'score': 46959.34210753441, 'total_duration': 48611.97661066055, 'accumulated_submission_time': 46959.34210753441, 'accumulated_eval_time': 1644.1155638694763, 'accumulated_logging_time': 3.6292083263397217, 'global_step': 139104, 'preemption_count': 0}), (140618, {'train/accuracy': 0.881257951259613, 'train/loss': 0.6726261973381042, 'validation/accuracy': 0.7454599738121033, 'validation/loss': 1.2311464548110962, 'validation/num_examples': 50000, 'test/accuracy': 0.6273000240325928, 'test/loss': 1.8636746406555176, 'test/num_examples': 10000, 'score': 47469.49969315529, 'total_duration': 49139.661640405655, 'accumulated_submission_time': 47469.49969315529, 'accumulated_eval_time': 1661.5373244285583, 'accumulated_logging_time': 3.6814053058624268, 'global_step': 140618, 'preemption_count': 0}), (142130, {'train/accuracy': 0.8937141299247742, 'train/loss': 0.6258179545402527, 'validation/accuracy': 0.7417399883270264, 'validation/loss': 1.2493900060653687, 'validation/num_examples': 50000, 'test/accuracy': 0.6145000457763672, 'test/loss': 1.894722580909729, 'test/num_examples': 10000, 'score': 47979.43871974945, 'total_duration': 49667.10993528366, 'accumulated_submission_time': 47979.43871974945, 'accumulated_eval_time': 1678.9434888362885, 'accumulated_logging_time': 3.7319438457489014, 'global_step': 142130, 'preemption_count': 0}), (143643, {'train/accuracy': 0.8984175324440002, 'train/loss': 0.5999704599380493, 'validation/accuracy': 0.7469199895858765, 'validation/loss': 1.220354676246643, 'validation/num_examples': 50000, 'test/accuracy': 0.6167000532150269, 'test/loss': 1.8685358762741089, 'test/num_examples': 10000, 'score': 48489.606682538986, 'total_duration': 50194.81318330765, 'accumulated_submission_time': 48489.606682538986, 'accumulated_eval_time': 1696.3737258911133, 'accumulated_logging_time': 3.7850804328918457, 'global_step': 143643, 'preemption_count': 0}), (145156, {'train/accuracy': 0.894551157951355, 'train/loss': 0.6113947629928589, 'validation/accuracy': 0.745959997177124, 'validation/loss': 1.216866374015808, 'validation/num_examples': 50000, 'test/accuracy': 0.6219000220298767, 'test/loss': 1.8563332557678223, 'test/num_examples': 10000, 'score': 48999.76789522171, 'total_duration': 50722.39692115784, 'accumulated_submission_time': 48999.76789522171, 'accumulated_eval_time': 1713.6931321620941, 'accumulated_logging_time': 3.8360297679901123, 'global_step': 145156, 'preemption_count': 0}), (146670, {'train/accuracy': 0.8981783986091614, 'train/loss': 0.6124516725540161, 'validation/accuracy': 0.748479962348938, 'validation/loss': 1.2270301580429077, 'validation/num_examples': 50000, 'test/accuracy': 0.6224000453948975, 'test/loss': 1.8555512428283691, 'test/num_examples': 10000, 'score': 49509.97759890556, 'total_duration': 51250.564635276794, 'accumulated_submission_time': 49509.97759890556, 'accumulated_eval_time': 1731.5386974811554, 'accumulated_logging_time': 3.894920825958252, 'global_step': 146670, 'preemption_count': 0}), (148183, {'train/accuracy': 0.8929169178009033, 'train/loss': 0.6327682137489319, 'validation/accuracy': 0.746679961681366, 'validation/loss': 1.2398338317871094, 'validation/num_examples': 50000, 'test/accuracy': 0.6222000122070312, 'test/loss': 1.8765029907226562, 'test/num_examples': 10000, 'score': 50020.18068480492, 'total_duration': 51778.22360420227, 'accumulated_submission_time': 50020.18068480492, 'accumulated_eval_time': 1748.8881268501282, 'accumulated_logging_time': 3.9477474689483643, 'global_step': 148183, 'preemption_count': 0}), (149696, {'train/accuracy': 0.89652419090271, 'train/loss': 0.6026631593704224, 'validation/accuracy': 0.7511999607086182, 'validation/loss': 1.203833818435669, 'validation/num_examples': 50000, 'test/accuracy': 0.6230000257492065, 'test/loss': 1.8418248891830444, 'test/num_examples': 10000, 'score': 50530.1194422245, 'total_duration': 52305.64930200577, 'accumulated_submission_time': 50530.1194422245, 'accumulated_eval_time': 1766.2669341564178, 'accumulated_logging_time': 4.001991271972656, 'global_step': 149696, 'preemption_count': 0}), (151209, {'train/accuracy': 0.9214365482330322, 'train/loss': 0.5090224146842957, 'validation/accuracy': 0.7524799704551697, 'validation/loss': 1.1925246715545654, 'validation/num_examples': 50000, 'test/accuracy': 0.6265000104904175, 'test/loss': 1.8258957862854004, 'test/num_examples': 10000, 'score': 51040.28805708885, 'total_duration': 52833.489424705505, 'accumulated_submission_time': 51040.28805708885, 'accumulated_eval_time': 1783.8296740055084, 'accumulated_logging_time': 4.057976961135864, 'global_step': 151209, 'preemption_count': 0}), (152723, {'train/accuracy': 0.9169324040412903, 'train/loss': 0.526613175868988, 'validation/accuracy': 0.7518599629402161, 'validation/loss': 1.1986668109893799, 'validation/num_examples': 50000, 'test/accuracy': 0.6246000528335571, 'test/loss': 1.8432016372680664, 'test/num_examples': 10000, 'score': 51550.47252988815, 'total_duration': 53361.15016055107, 'accumulated_submission_time': 51550.47252988815, 'accumulated_eval_time': 1801.2003610134125, 'accumulated_logging_time': 4.109041690826416, 'global_step': 152723, 'preemption_count': 0}), (154235, {'train/accuracy': 0.9125677347183228, 'train/loss': 0.5550875067710876, 'validation/accuracy': 0.7528600096702576, 'validation/loss': 1.2101103067398071, 'validation/num_examples': 50000, 'test/accuracy': 0.6273000240325928, 'test/loss': 1.8422486782073975, 'test/num_examples': 10000, 'score': 52060.446773052216, 'total_duration': 53888.86126732826, 'accumulated_submission_time': 52060.446773052216, 'accumulated_eval_time': 1818.8298110961914, 'accumulated_logging_time': 4.163464784622192, 'global_step': 154235, 'preemption_count': 0}), (155748, {'train/accuracy': 0.9142418503761292, 'train/loss': 0.544491171836853, 'validation/accuracy': 0.7534799575805664, 'validation/loss': 1.1975823640823364, 'validation/num_examples': 50000, 'test/accuracy': 0.6277000308036804, 'test/loss': 1.8363298177719116, 'test/num_examples': 10000, 'score': 52570.49973154068, 'total_duration': 54416.66174650192, 'accumulated_submission_time': 52570.49973154068, 'accumulated_eval_time': 1836.4778108596802, 'accumulated_logging_time': 4.209648609161377, 'global_step': 155748, 'preemption_count': 0}), (157261, {'train/accuracy': 0.9153977632522583, 'train/loss': 0.5491384267807007, 'validation/accuracy': 0.7535799741744995, 'validation/loss': 1.2018764019012451, 'validation/num_examples': 50000, 'test/accuracy': 0.6292000412940979, 'test/loss': 1.8402178287506104, 'test/num_examples': 10000, 'score': 53080.501549720764, 'total_duration': 54944.76948451996, 'accumulated_submission_time': 53080.501549720764, 'accumulated_eval_time': 1854.4784564971924, 'accumulated_logging_time': 4.263177156448364, 'global_step': 157261, 'preemption_count': 0}), (158773, {'train/accuracy': 0.9156568646430969, 'train/loss': 0.5355087518692017, 'validation/accuracy': 0.7560999989509583, 'validation/loss': 1.1917190551757812, 'validation/num_examples': 50000, 'test/accuracy': 0.6296000480651855, 'test/loss': 1.8320457935333252, 'test/num_examples': 10000, 'score': 53590.45152449608, 'total_duration': 55472.293454647064, 'accumulated_submission_time': 53590.45152449608, 'accumulated_eval_time': 1871.9461405277252, 'accumulated_logging_time': 4.316278457641602, 'global_step': 158773, 'preemption_count': 0}), (160286, {'train/accuracy': 0.9319993257522583, 'train/loss': 0.4888227581977844, 'validation/accuracy': 0.7568399906158447, 'validation/loss': 1.1966514587402344, 'validation/num_examples': 50000, 'test/accuracy': 0.6318000555038452, 'test/loss': 1.8300745487213135, 'test/num_examples': 10000, 'score': 54100.61996221542, 'total_duration': 56000.02527117729, 'accumulated_submission_time': 54100.61996221542, 'accumulated_eval_time': 1889.3943195343018, 'accumulated_logging_time': 4.3780577182769775, 'global_step': 160286, 'preemption_count': 0}), (161799, {'train/accuracy': 0.9300462007522583, 'train/loss': 0.48804351687431335, 'validation/accuracy': 0.7581999897956848, 'validation/loss': 1.1818283796310425, 'validation/num_examples': 50000, 'test/accuracy': 0.6349000334739685, 'test/loss': 1.818547010421753, 'test/num_examples': 10000, 'score': 54610.668021678925, 'total_duration': 56527.60770535469, 'accumulated_submission_time': 54610.668021678925, 'accumulated_eval_time': 1906.8222138881683, 'accumulated_logging_time': 4.4321160316467285, 'global_step': 161799, 'preemption_count': 0}), (163311, {'train/accuracy': 0.9286909699440002, 'train/loss': 0.4972872734069824, 'validation/accuracy': 0.7575799822807312, 'validation/loss': 1.1837843656539917, 'validation/num_examples': 50000, 'test/accuracy': 0.6351000070571899, 'test/loss': 1.817732334136963, 'test/num_examples': 10000, 'score': 55120.694390535355, 'total_duration': 57055.07130908966, 'accumulated_submission_time': 55120.694390535355, 'accumulated_eval_time': 1924.1260414123535, 'accumulated_logging_time': 4.511041164398193, 'global_step': 163311, 'preemption_count': 0}), (164824, {'train/accuracy': 0.9278140664100647, 'train/loss': 0.496550053358078, 'validation/accuracy': 0.7587599754333496, 'validation/loss': 1.1828312873840332, 'validation/num_examples': 50000, 'test/accuracy': 0.6360000371932983, 'test/loss': 1.8208544254302979, 'test/num_examples': 10000, 'score': 55630.80797767639, 'total_duration': 57582.89201283455, 'accumulated_submission_time': 55630.80797767639, 'accumulated_eval_time': 1941.7257256507874, 'accumulated_logging_time': 4.56522536277771, 'global_step': 164824, 'preemption_count': 0}), (166336, {'train/accuracy': 0.9301857352256775, 'train/loss': 0.48756131529808044, 'validation/accuracy': 0.7597599625587463, 'validation/loss': 1.1802102327346802, 'validation/num_examples': 50000, 'test/accuracy': 0.6350000500679016, 'test/loss': 1.8172756433486938, 'test/num_examples': 10000, 'score': 56140.73363828659, 'total_duration': 58110.581184625626, 'accumulated_submission_time': 56140.73363828659, 'accumulated_eval_time': 1959.3804275989532, 'accumulated_logging_time': 4.620617151260376, 'global_step': 166336, 'preemption_count': 0}), (167849, {'train/accuracy': 0.9341716766357422, 'train/loss': 0.48081260919570923, 'validation/accuracy': 0.7604199647903442, 'validation/loss': 1.180977702140808, 'validation/num_examples': 50000, 'test/accuracy': 0.6374000310897827, 'test/loss': 1.8157609701156616, 'test/num_examples': 10000, 'score': 56650.924723148346, 'total_duration': 58638.559196949005, 'accumulated_submission_time': 56650.924723148346, 'accumulated_eval_time': 1977.0555260181427, 'accumulated_logging_time': 4.680642366409302, 'global_step': 167849, 'preemption_count': 0}), (169362, {'train/accuracy': 0.9384565949440002, 'train/loss': 0.4624026119709015, 'validation/accuracy': 0.7603200078010559, 'validation/loss': 1.1795930862426758, 'validation/num_examples': 50000, 'test/accuracy': 0.6372000575065613, 'test/loss': 1.8115358352661133, 'test/num_examples': 10000, 'score': 57161.05779337883, 'total_duration': 59166.4287109375, 'accumulated_submission_time': 57161.05779337883, 'accumulated_eval_time': 1994.6798272132874, 'accumulated_logging_time': 4.739821195602417, 'global_step': 169362, 'preemption_count': 0}), (170875, {'train/accuracy': 0.9371013641357422, 'train/loss': 0.4609328508377075, 'validation/accuracy': 0.7616399526596069, 'validation/loss': 1.170788288116455, 'validation/num_examples': 50000, 'test/accuracy': 0.6381000280380249, 'test/loss': 1.8069534301757812, 'test/num_examples': 10000, 'score': 57671.088973522186, 'total_duration': 59693.97801613808, 'accumulated_submission_time': 57671.088973522186, 'accumulated_eval_time': 2012.0857291221619, 'accumulated_logging_time': 4.797400236129761, 'global_step': 170875, 'preemption_count': 0}), (172387, {'train/accuracy': 0.9367625713348389, 'train/loss': 0.4671074450016022, 'validation/accuracy': 0.7630599737167358, 'validation/loss': 1.1761828660964966, 'validation/num_examples': 50000, 'test/accuracy': 0.6375000476837158, 'test/loss': 1.80931556224823, 'test/num_examples': 10000, 'score': 58181.277435302734, 'total_duration': 60221.7962770462, 'accumulated_submission_time': 58181.277435302734, 'accumulated_eval_time': 2029.605746269226, 'accumulated_logging_time': 4.853551864624023, 'global_step': 172387, 'preemption_count': 0}), (173900, {'train/accuracy': 0.9363639950752258, 'train/loss': 0.46477654576301575, 'validation/accuracy': 0.7618199586868286, 'validation/loss': 1.1747890710830688, 'validation/num_examples': 50000, 'test/accuracy': 0.6363000273704529, 'test/loss': 1.8088829517364502, 'test/num_examples': 10000, 'score': 58691.458490133286, 'total_duration': 60749.449186086655, 'accumulated_submission_time': 58691.458490133286, 'accumulated_eval_time': 2046.9627692699432, 'accumulated_logging_time': 4.915642261505127, 'global_step': 173900, 'preemption_count': 0}), (175413, {'train/accuracy': 0.9368223547935486, 'train/loss': 0.4632803499698639, 'validation/accuracy': 0.7626799941062927, 'validation/loss': 1.174310326576233, 'validation/num_examples': 50000, 'test/accuracy': 0.6386000514030457, 'test/loss': 1.8084932565689087, 'test/num_examples': 10000, 'score': 59201.67768287659, 'total_duration': 61277.555790662766, 'accumulated_submission_time': 59201.67768287659, 'accumulated_eval_time': 2064.736491918564, 'accumulated_logging_time': 4.975682020187378, 'global_step': 175413, 'preemption_count': 0}), (176926, {'train/accuracy': 0.9379583597183228, 'train/loss': 0.46584373712539673, 'validation/accuracy': 0.7636199593544006, 'validation/loss': 1.175073504447937, 'validation/num_examples': 50000, 'test/accuracy': 0.6410000324249268, 'test/loss': 1.8080230951309204, 'test/num_examples': 10000, 'score': 59711.657662153244, 'total_duration': 61805.24752783775, 'accumulated_submission_time': 59711.657662153244, 'accumulated_eval_time': 2082.3390328884125, 'accumulated_logging_time': 5.031782627105713, 'global_step': 176926, 'preemption_count': 0}), (178439, {'train/accuracy': 0.9407883882522583, 'train/loss': 0.4490717649459839, 'validation/accuracy': 0.7630800008773804, 'validation/loss': 1.169806957244873, 'validation/num_examples': 50000, 'test/accuracy': 0.6402000188827515, 'test/loss': 1.8037229776382446, 'test/num_examples': 10000, 'score': 60221.87917423248, 'total_duration': 62333.050040483475, 'accumulated_submission_time': 60221.87917423248, 'accumulated_eval_time': 2099.8054864406586, 'accumulated_logging_time': 5.092525005340576, 'global_step': 178439, 'preemption_count': 0}), (179952, {'train/accuracy': 0.9408083558082581, 'train/loss': 0.45449861884117126, 'validation/accuracy': 0.7629599571228027, 'validation/loss': 1.1712826490402222, 'validation/num_examples': 50000, 'test/accuracy': 0.640500009059906, 'test/loss': 1.8030924797058105, 'test/num_examples': 10000, 'score': 60732.035746097565, 'total_duration': 62860.571590185165, 'accumulated_submission_time': 60732.035746097565, 'accumulated_eval_time': 2117.058675289154, 'accumulated_logging_time': 5.151561737060547, 'global_step': 179952, 'preemption_count': 0}), (181464, {'train/accuracy': 0.939871609210968, 'train/loss': 0.4563002586364746, 'validation/accuracy': 0.7633999586105347, 'validation/loss': 1.173967719078064, 'validation/num_examples': 50000, 'test/accuracy': 0.6407000422477722, 'test/loss': 1.8057492971420288, 'test/num_examples': 10000, 'score': 61242.113387584686, 'total_duration': 63388.0883102417, 'accumulated_submission_time': 61242.113387584686, 'accumulated_eval_time': 2134.386967897415, 'accumulated_logging_time': 5.21010160446167, 'global_step': 181464, 'preemption_count': 0}), (182977, {'train/accuracy': 0.939851701259613, 'train/loss': 0.45275208353996277, 'validation/accuracy': 0.7633599638938904, 'validation/loss': 1.1705142259597778, 'validation/num_examples': 50000, 'test/accuracy': 0.6399000287055969, 'test/loss': 1.800960659980774, 'test/num_examples': 10000, 'score': 61752.08497405052, 'total_duration': 63915.73721885681, 'accumulated_submission_time': 61752.08497405052, 'accumulated_eval_time': 2151.952211856842, 'accumulated_logging_time': 5.2697319984436035, 'global_step': 182977, 'preemption_count': 0}), (184489, {'train/accuracy': 0.93949294090271, 'train/loss': 0.45518749952316284, 'validation/accuracy': 0.7637400031089783, 'validation/loss': 1.1731160879135132, 'validation/num_examples': 50000, 'test/accuracy': 0.6402000188827515, 'test/loss': 1.8049848079681396, 'test/num_examples': 10000, 'score': 62262.14775061607, 'total_duration': 64443.61814188957, 'accumulated_submission_time': 62262.14775061607, 'accumulated_eval_time': 2169.6572070121765, 'accumulated_logging_time': 5.3304524421691895, 'global_step': 184489, 'preemption_count': 0}), (186002, {'train/accuracy': 0.9404894709587097, 'train/loss': 0.45212090015411377, 'validation/accuracy': 0.7636399865150452, 'validation/loss': 1.167830228805542, 'validation/num_examples': 50000, 'test/accuracy': 0.6404000520706177, 'test/loss': 1.8014260530471802, 'test/num_examples': 10000, 'score': 62772.118121147156, 'total_duration': 64971.041343450546, 'accumulated_submission_time': 62772.118121147156, 'accumulated_eval_time': 2186.9935114383698, 'accumulated_logging_time': 5.393200159072876, 'global_step': 186002, 'preemption_count': 0}), (186666, {'train/accuracy': 0.9403100609779358, 'train/loss': 0.44991639256477356, 'validation/accuracy': 0.763759970664978, 'validation/loss': 1.1691426038742065, 'validation/num_examples': 50000, 'test/accuracy': 0.6404000520706177, 'test/loss': 1.8022942543029785, 'test/num_examples': 10000, 'score': 62996.11162734032, 'total_duration': 65212.594963788986, 'accumulated_submission_time': 62996.11162734032, 'accumulated_eval_time': 2204.471424341202, 'accumulated_logging_time': 5.451512098312378, 'global_step': 186666, 'preemption_count': 0})], 'global_step': 186666}
I0201 09:05:55.201545 140070692116288 submission_runner.py:586] Timing: 62996.11162734032
I0201 09:05:55.201609 140070692116288 submission_runner.py:588] Total number of evals: 125
I0201 09:05:55.201652 140070692116288 submission_runner.py:589] ====================
I0201 09:05:55.201696 140070692116288 submission_runner.py:542] Using RNG seed 485521238
I0201 09:05:55.203034 140070692116288 submission_runner.py:551] --- Tuning run 3/5 ---
I0201 09:05:55.203138 140070692116288 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_3/imagenet_resnet_jax/trial_3.
I0201 09:05:55.206289 140070692116288 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_3/imagenet_resnet_jax/trial_3/hparams.json.
I0201 09:05:55.207263 140070692116288 submission_runner.py:206] Initializing dataset.
I0201 09:05:55.216933 140070692116288 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0201 09:05:55.226455 140070692116288 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0201 09:05:55.416325 140070692116288 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0201 09:05:55.638441 140070692116288 submission_runner.py:213] Initializing model.
I0201 09:06:01.062335 140070692116288 submission_runner.py:255] Initializing optimizer.
I0201 09:06:01.466633 140070692116288 submission_runner.py:262] Initializing metrics bundle.
I0201 09:06:01.466912 140070692116288 submission_runner.py:280] Initializing checkpoint and logger.
I0201 09:06:01.487938 140070692116288 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_3/imagenet_resnet_jax/trial_3 with prefix checkpoint_
I0201 09:06:01.488204 140070692116288 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_3/imagenet_resnet_jax/trial_3/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0201 09:06:12.961575 140070692116288 logger_utils.py:220] Unable to record git information. Continuing without it.
I0201 09:06:23.542283 140070692116288 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_3/imagenet_resnet_jax/trial_3/flags_0.json.
I0201 09:06:23.547281 140070692116288 submission_runner.py:314] Starting training loop.
I0201 09:06:58.514124 139907762734848 logging_writer.py:48] [0] global_step=0, grad_norm=0.6918318271636963, loss=6.925329208374023
I0201 09:06:58.528538 140070692116288 spec.py:321] Evaluating on the training split.
I0201 09:07:04.997779 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 09:07:13.857127 140070692116288 spec.py:349] Evaluating on the test split.
I0201 09:07:16.429479 140070692116288 submission_runner.py:408] Time since start: 52.88s, 	Step: 1, 	{'train/accuracy': 0.0011360011994838715, 'train/loss': 6.912071704864502, 'validation/accuracy': 0.0011199999134987593, 'validation/loss': 6.912059783935547, 'validation/num_examples': 50000, 'test/accuracy': 0.0009000000427477062, 'test/loss': 6.912177562713623, 'test/num_examples': 10000, 'score': 34.98110485076904, 'total_duration': 52.88213634490967, 'accumulated_submission_time': 34.98110485076904, 'accumulated_eval_time': 17.900901556015015, 'accumulated_logging_time': 0}
I0201 09:07:16.439892 139907745949440 logging_writer.py:48] [1] accumulated_eval_time=17.900902, accumulated_logging_time=0, accumulated_submission_time=34.981105, global_step=1, preemption_count=0, score=34.981105, test/accuracy=0.000900, test/loss=6.912178, test/num_examples=10000, total_duration=52.882136, train/accuracy=0.001136, train/loss=6.912072, validation/accuracy=0.001120, validation/loss=6.912060, validation/num_examples=50000
I0201 09:07:50.198220 139907754342144 logging_writer.py:48] [100] global_step=100, grad_norm=0.6605497002601624, loss=6.903188705444336
I0201 09:08:24.000584 139907745949440 logging_writer.py:48] [200] global_step=200, grad_norm=0.6683810353279114, loss=6.859374046325684
I0201 09:08:57.840704 139907754342144 logging_writer.py:48] [300] global_step=300, grad_norm=0.7106596827507019, loss=6.771359443664551
I0201 09:09:31.696548 139907745949440 logging_writer.py:48] [400] global_step=400, grad_norm=0.7413580417633057, loss=6.667791366577148
I0201 09:10:05.560279 139907754342144 logging_writer.py:48] [500] global_step=500, grad_norm=0.8236008286476135, loss=6.592644214630127
I0201 09:10:39.428051 139907745949440 logging_writer.py:48] [600] global_step=600, grad_norm=0.8350692391395569, loss=6.444119930267334
I0201 09:11:13.285107 139907754342144 logging_writer.py:48] [700] global_step=700, grad_norm=0.8717645406723022, loss=6.343482494354248
I0201 09:11:47.159254 139907745949440 logging_writer.py:48] [800] global_step=800, grad_norm=1.021999716758728, loss=6.221237659454346
I0201 09:12:21.009217 139907754342144 logging_writer.py:48] [900] global_step=900, grad_norm=1.2797404527664185, loss=6.1169867515563965
I0201 09:12:54.867518 139907745949440 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.829453706741333, loss=6.05127477645874
I0201 09:13:28.744439 139907754342144 logging_writer.py:48] [1100] global_step=1100, grad_norm=2.4579572677612305, loss=5.852109909057617
I0201 09:14:02.759244 139907745949440 logging_writer.py:48] [1200] global_step=1200, grad_norm=4.756921291351318, loss=5.821584701538086
I0201 09:14:36.609355 139907754342144 logging_writer.py:48] [1300] global_step=1300, grad_norm=2.426664113998413, loss=5.733708381652832
I0201 09:15:10.486312 139907745949440 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.4091556072235107, loss=5.630807876586914
I0201 09:15:44.384584 139907754342144 logging_writer.py:48] [1500] global_step=1500, grad_norm=3.9885478019714355, loss=5.542215347290039
I0201 09:15:46.573322 140070692116288 spec.py:321] Evaluating on the training split.
I0201 09:15:52.865068 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 09:16:01.876681 140070692116288 spec.py:349] Evaluating on the test split.
I0201 09:16:04.501933 140070692116288 submission_runner.py:408] Time since start: 580.95s, 	Step: 1508, 	{'train/accuracy': 0.0686383917927742, 'train/loss': 5.3621506690979, 'validation/accuracy': 0.0652799978852272, 'validation/loss': 5.418907165527344, 'validation/num_examples': 50000, 'test/accuracy': 0.046000003814697266, 'test/loss': 5.649470329284668, 'test/num_examples': 10000, 'score': 545.0515990257263, 'total_duration': 580.9545924663544, 'accumulated_submission_time': 545.0515990257263, 'accumulated_eval_time': 35.82946801185608, 'accumulated_logging_time': 0.02124810218811035}
I0201 09:16:04.527128 139907729164032 logging_writer.py:48] [1508] accumulated_eval_time=35.829468, accumulated_logging_time=0.021248, accumulated_submission_time=545.051599, global_step=1508, preemption_count=0, score=545.051599, test/accuracy=0.046000, test/loss=5.649470, test/num_examples=10000, total_duration=580.954592, train/accuracy=0.068638, train/loss=5.362151, validation/accuracy=0.065280, validation/loss=5.418907, validation/num_examples=50000
I0201 09:16:35.947715 139907737556736 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.537209987640381, loss=5.455859184265137
I0201 09:17:09.780221 139907729164032 logging_writer.py:48] [1700] global_step=1700, grad_norm=3.9927825927734375, loss=5.450523376464844
I0201 09:17:43.686828 139907737556736 logging_writer.py:48] [1800] global_step=1800, grad_norm=3.0819146633148193, loss=5.318718910217285
I0201 09:18:17.541607 139907729164032 logging_writer.py:48] [1900] global_step=1900, grad_norm=3.270634174346924, loss=5.214097499847412
I0201 09:18:51.403941 139907737556736 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.9137959480285645, loss=5.171114921569824
I0201 09:19:25.239764 139907729164032 logging_writer.py:48] [2100] global_step=2100, grad_norm=4.667455673217773, loss=5.074687957763672
I0201 09:19:59.081591 139907737556736 logging_writer.py:48] [2200] global_step=2200, grad_norm=4.212650299072266, loss=5.050261497497559
I0201 09:20:33.034452 139907729164032 logging_writer.py:48] [2300] global_step=2300, grad_norm=6.200363636016846, loss=4.950806617736816
I0201 09:21:06.889183 139907737556736 logging_writer.py:48] [2400] global_step=2400, grad_norm=4.526266098022461, loss=4.933108329772949
I0201 09:21:40.744484 139907729164032 logging_writer.py:48] [2500] global_step=2500, grad_norm=4.225498676300049, loss=4.962958335876465
I0201 09:22:14.612771 139907737556736 logging_writer.py:48] [2600] global_step=2600, grad_norm=3.5820367336273193, loss=4.816555976867676
I0201 09:22:48.452083 139907729164032 logging_writer.py:48] [2700] global_step=2700, grad_norm=5.140907287597656, loss=4.700915336608887
I0201 09:23:22.339051 139907737556736 logging_writer.py:48] [2800] global_step=2800, grad_norm=5.981386661529541, loss=4.684200763702393
I0201 09:23:56.218957 139907729164032 logging_writer.py:48] [2900] global_step=2900, grad_norm=4.908966541290283, loss=4.524417877197266
I0201 09:24:30.055257 139907737556736 logging_writer.py:48] [3000] global_step=3000, grad_norm=6.237407207489014, loss=4.633034706115723
I0201 09:24:34.606392 140070692116288 spec.py:321] Evaluating on the training split.
I0201 09:24:40.994126 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 09:24:49.616936 140070692116288 spec.py:349] Evaluating on the test split.
I0201 09:24:52.300075 140070692116288 submission_runner.py:408] Time since start: 1108.75s, 	Step: 3015, 	{'train/accuracy': 0.16733099520206451, 'train/loss': 4.278256893157959, 'validation/accuracy': 0.15369999408721924, 'validation/loss': 4.376176357269287, 'validation/num_examples': 50000, 'test/accuracy': 0.11080000549554825, 'test/loss': 4.82637882232666, 'test/num_examples': 10000, 'score': 1055.0695431232452, 'total_duration': 1108.7527313232422, 'accumulated_submission_time': 1055.0695431232452, 'accumulated_eval_time': 53.52310228347778, 'accumulated_logging_time': 0.056015729904174805}
I0201 09:24:52.316863 139907762734848 logging_writer.py:48] [3015] accumulated_eval_time=53.523102, accumulated_logging_time=0.056016, accumulated_submission_time=1055.069543, global_step=3015, preemption_count=0, score=1055.069543, test/accuracy=0.110800, test/loss=4.826379, test/num_examples=10000, total_duration=1108.752731, train/accuracy=0.167331, train/loss=4.278257, validation/accuracy=0.153700, validation/loss=4.376176, validation/num_examples=50000
I0201 09:25:21.364970 139908425447168 logging_writer.py:48] [3100] global_step=3100, grad_norm=5.013000965118408, loss=4.597109794616699
I0201 09:25:55.201693 139907762734848 logging_writer.py:48] [3200] global_step=3200, grad_norm=8.406709671020508, loss=4.42641544342041
I0201 09:26:29.070744 139908425447168 logging_writer.py:48] [3300] global_step=3300, grad_norm=4.627994060516357, loss=4.352778434753418
I0201 09:27:03.036666 139907762734848 logging_writer.py:48] [3400] global_step=3400, grad_norm=6.604888916015625, loss=4.533374786376953
I0201 09:27:36.898511 139908425447168 logging_writer.py:48] [3500] global_step=3500, grad_norm=5.391112804412842, loss=4.289287567138672
I0201 09:28:10.758172 139907762734848 logging_writer.py:48] [3600] global_step=3600, grad_norm=4.532129764556885, loss=4.345493793487549
I0201 09:28:44.667610 139908425447168 logging_writer.py:48] [3700] global_step=3700, grad_norm=4.59982967376709, loss=4.267833709716797
I0201 09:29:18.552454 139907762734848 logging_writer.py:48] [3800] global_step=3800, grad_norm=7.635234355926514, loss=4.166813850402832
I0201 09:29:52.436443 139908425447168 logging_writer.py:48] [3900] global_step=3900, grad_norm=6.898202896118164, loss=4.174036979675293
I0201 09:30:26.319299 139907762734848 logging_writer.py:48] [4000] global_step=4000, grad_norm=7.079672813415527, loss=4.158999443054199
I0201 09:31:00.193278 139908425447168 logging_writer.py:48] [4100] global_step=4100, grad_norm=7.877776622772217, loss=4.068376064300537
I0201 09:31:34.069581 139907762734848 logging_writer.py:48] [4200] global_step=4200, grad_norm=7.34678840637207, loss=3.916015625
I0201 09:32:07.914231 139908425447168 logging_writer.py:48] [4300] global_step=4300, grad_norm=4.465527534484863, loss=3.9996864795684814
I0201 09:32:41.781049 139907762734848 logging_writer.py:48] [4400] global_step=4400, grad_norm=7.338870525360107, loss=3.6758365631103516
I0201 09:33:15.767910 139908425447168 logging_writer.py:48] [4500] global_step=4500, grad_norm=8.104231834411621, loss=3.790756940841675
I0201 09:33:22.348005 140070692116288 spec.py:321] Evaluating on the training split.
I0201 09:33:28.665321 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 09:33:37.333946 140070692116288 spec.py:349] Evaluating on the test split.
I0201 09:33:39.997614 140070692116288 submission_runner.py:408] Time since start: 1636.45s, 	Step: 4521, 	{'train/accuracy': 0.2669403553009033, 'train/loss': 3.5287725925445557, 'validation/accuracy': 0.24573999643325806, 'validation/loss': 3.66829776763916, 'validation/num_examples': 50000, 'test/accuracy': 0.17920000851154327, 'test/loss': 4.221713066101074, 'test/num_examples': 10000, 'score': 1565.034935951233, 'total_duration': 1636.450201511383, 'accumulated_submission_time': 1565.034935951233, 'accumulated_eval_time': 71.17259407043457, 'accumulated_logging_time': 0.0849447250366211}
I0201 09:33:40.016565 139907737556736 logging_writer.py:48] [4521] accumulated_eval_time=71.172594, accumulated_logging_time=0.084945, accumulated_submission_time=1565.034936, global_step=4521, preemption_count=0, score=1565.034936, test/accuracy=0.179200, test/loss=4.221713, test/num_examples=10000, total_duration=1636.450202, train/accuracy=0.266940, train/loss=3.528773, validation/accuracy=0.245740, validation/loss=3.668298, validation/num_examples=50000
I0201 09:34:07.088463 139907745949440 logging_writer.py:48] [4600] global_step=4600, grad_norm=12.099804878234863, loss=3.8777313232421875
I0201 09:34:40.910147 139907737556736 logging_writer.py:48] [4700] global_step=4700, grad_norm=7.259272575378418, loss=3.851555824279785
I0201 09:35:14.796447 139907745949440 logging_writer.py:48] [4800] global_step=4800, grad_norm=7.803293228149414, loss=3.7328929901123047
I0201 09:35:48.650999 139907737556736 logging_writer.py:48] [4900] global_step=4900, grad_norm=5.875464916229248, loss=3.715123414993286
I0201 09:36:22.504807 139907745949440 logging_writer.py:48] [5000] global_step=5000, grad_norm=6.434008598327637, loss=3.713931083679199
I0201 09:36:56.371732 139907737556736 logging_writer.py:48] [5100] global_step=5100, grad_norm=9.19091796875, loss=3.627798080444336
I0201 09:37:30.246977 139907745949440 logging_writer.py:48] [5200] global_step=5200, grad_norm=8.083565711975098, loss=3.654160499572754
I0201 09:38:04.122729 139907737556736 logging_writer.py:48] [5300] global_step=5300, grad_norm=8.566787719726562, loss=3.515376329421997
I0201 09:38:37.981927 139907745949440 logging_writer.py:48] [5400] global_step=5400, grad_norm=6.340847969055176, loss=3.4977948665618896
I0201 09:39:11.889532 139907737556736 logging_writer.py:48] [5500] global_step=5500, grad_norm=8.530107498168945, loss=3.643641948699951
I0201 09:39:45.824956 139907745949440 logging_writer.py:48] [5600] global_step=5600, grad_norm=6.19802713394165, loss=3.43049955368042
I0201 09:40:19.721910 139907737556736 logging_writer.py:48] [5700] global_step=5700, grad_norm=8.770251274108887, loss=3.545062780380249
I0201 09:40:53.614757 139907745949440 logging_writer.py:48] [5800] global_step=5800, grad_norm=6.535552501678467, loss=3.3984222412109375
I0201 09:41:27.498623 139907737556736 logging_writer.py:48] [5900] global_step=5900, grad_norm=8.115816116333008, loss=3.4771125316619873
I0201 09:42:01.354634 139907745949440 logging_writer.py:48] [6000] global_step=6000, grad_norm=5.055197715759277, loss=3.3328399658203125
I0201 09:42:10.306084 140070692116288 spec.py:321] Evaluating on the training split.
I0201 09:42:17.330718 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 09:42:26.352086 140070692116288 spec.py:349] Evaluating on the test split.
I0201 09:42:28.960512 140070692116288 submission_runner.py:408] Time since start: 2165.41s, 	Step: 6028, 	{'train/accuracy': 0.36387914419174194, 'train/loss': 2.9160544872283936, 'validation/accuracy': 0.33461999893188477, 'validation/loss': 3.083531141281128, 'validation/num_examples': 50000, 'test/accuracy': 0.249300017952919, 'test/loss': 3.7188665866851807, 'test/num_examples': 10000, 'score': 2075.2632479667664, 'total_duration': 2165.413183450699, 'accumulated_submission_time': 2075.2632479667664, 'accumulated_eval_time': 89.82699704170227, 'accumulated_logging_time': 0.11361503601074219}
I0201 09:42:28.981622 139907729164032 logging_writer.py:48] [6028] accumulated_eval_time=89.826997, accumulated_logging_time=0.113615, accumulated_submission_time=2075.263248, global_step=6028, preemption_count=0, score=2075.263248, test/accuracy=0.249300, test/loss=3.718867, test/num_examples=10000, total_duration=2165.413183, train/accuracy=0.363879, train/loss=2.916054, validation/accuracy=0.334620, validation/loss=3.083531, validation/num_examples=50000
I0201 09:42:53.660409 139907737556736 logging_writer.py:48] [6100] global_step=6100, grad_norm=5.531590461730957, loss=3.328334093093872
I0201 09:43:27.472845 139907729164032 logging_writer.py:48] [6200] global_step=6200, grad_norm=6.547687530517578, loss=3.234320640563965
I0201 09:44:01.346655 139907737556736 logging_writer.py:48] [6300] global_step=6300, grad_norm=10.031163215637207, loss=3.4289307594299316
I0201 09:44:35.232553 139907729164032 logging_writer.py:48] [6400] global_step=6400, grad_norm=6.795985698699951, loss=3.3897252082824707
I0201 09:45:09.087998 139907737556736 logging_writer.py:48] [6500] global_step=6500, grad_norm=9.299928665161133, loss=3.1957592964172363
I0201 09:45:43.040949 139907729164032 logging_writer.py:48] [6600] global_step=6600, grad_norm=6.222101211547852, loss=3.2700276374816895
I0201 09:46:16.901630 139907737556736 logging_writer.py:48] [6700] global_step=6700, grad_norm=6.613223075866699, loss=3.1213877201080322
I0201 09:46:50.771289 139907729164032 logging_writer.py:48] [6800] global_step=6800, grad_norm=4.90502405166626, loss=3.157130718231201
I0201 09:47:24.639900 139907737556736 logging_writer.py:48] [6900] global_step=6900, grad_norm=9.618084907531738, loss=3.0230050086975098
I0201 09:47:58.517927 139907729164032 logging_writer.py:48] [7000] global_step=7000, grad_norm=5.172972679138184, loss=2.8959858417510986
I0201 09:48:32.383821 139907737556736 logging_writer.py:48] [7100] global_step=7100, grad_norm=10.577727317810059, loss=3.0257537364959717
I0201 09:49:06.256652 139907729164032 logging_writer.py:48] [7200] global_step=7200, grad_norm=8.897513389587402, loss=2.9858875274658203
I0201 09:49:40.128301 139907737556736 logging_writer.py:48] [7300] global_step=7300, grad_norm=8.265870094299316, loss=3.0266809463500977
I0201 09:50:13.995918 139907729164032 logging_writer.py:48] [7400] global_step=7400, grad_norm=7.7105278968811035, loss=3.1140670776367188
I0201 09:50:47.836037 139907737556736 logging_writer.py:48] [7500] global_step=7500, grad_norm=5.821416854858398, loss=2.9316084384918213
I0201 09:50:59.136179 140070692116288 spec.py:321] Evaluating on the training split.
I0201 09:51:05.695115 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 09:51:14.607575 140070692116288 spec.py:349] Evaluating on the test split.
I0201 09:51:17.209619 140070692116288 submission_runner.py:408] Time since start: 2693.66s, 	Step: 7535, 	{'train/accuracy': 0.4528658986091614, 'train/loss': 2.3958868980407715, 'validation/accuracy': 0.40101999044418335, 'validation/loss': 2.6911749839782715, 'validation/num_examples': 50000, 'test/accuracy': 0.30090001225471497, 'test/loss': 3.3908586502075195, 'test/num_examples': 10000, 'score': 2585.3535408973694, 'total_duration': 2693.6622858047485, 'accumulated_submission_time': 2585.3535408973694, 'accumulated_eval_time': 107.90042018890381, 'accumulated_logging_time': 0.14592361450195312}
I0201 09:51:17.230694 139907737556736 logging_writer.py:48] [7535] accumulated_eval_time=107.900420, accumulated_logging_time=0.145924, accumulated_submission_time=2585.353541, global_step=7535, preemption_count=0, score=2585.353541, test/accuracy=0.300900, test/loss=3.390859, test/num_examples=10000, total_duration=2693.662286, train/accuracy=0.452866, train/loss=2.395887, validation/accuracy=0.401020, validation/loss=2.691175, validation/num_examples=50000
I0201 09:51:39.477422 139908710635264 logging_writer.py:48] [7600] global_step=7600, grad_norm=7.210198402404785, loss=2.882683277130127
I0201 09:52:13.402148 139907737556736 logging_writer.py:48] [7700] global_step=7700, grad_norm=7.163815498352051, loss=2.959956645965576
I0201 09:52:47.239082 139908710635264 logging_writer.py:48] [7800] global_step=7800, grad_norm=8.931111335754395, loss=2.923712968826294
I0201 09:53:21.045069 139907737556736 logging_writer.py:48] [7900] global_step=7900, grad_norm=9.562929153442383, loss=2.9043428897857666
I0201 09:53:54.869940 139908710635264 logging_writer.py:48] [8000] global_step=8000, grad_norm=7.83793830871582, loss=2.7829208374023438
I0201 09:54:28.753300 139907737556736 logging_writer.py:48] [8100] global_step=8100, grad_norm=5.395839214324951, loss=2.8775999546051025
I0201 09:55:02.607385 139908710635264 logging_writer.py:48] [8200] global_step=8200, grad_norm=7.907201290130615, loss=2.7894105911254883
I0201 09:55:36.416403 139907737556736 logging_writer.py:48] [8300] global_step=8300, grad_norm=12.501922607421875, loss=2.749183177947998
I0201 09:56:10.253745 139908710635264 logging_writer.py:48] [8400] global_step=8400, grad_norm=7.335916519165039, loss=2.8245344161987305
I0201 09:56:44.081374 139907737556736 logging_writer.py:48] [8500] global_step=8500, grad_norm=6.561498165130615, loss=2.873129367828369
I0201 09:57:17.913813 139908710635264 logging_writer.py:48] [8600] global_step=8600, grad_norm=7.316079616546631, loss=2.8945980072021484
I0201 09:57:51.746335 139907737556736 logging_writer.py:48] [8700] global_step=8700, grad_norm=5.714138507843018, loss=2.6452724933624268
I0201 09:58:25.669992 139908710635264 logging_writer.py:48] [8800] global_step=8800, grad_norm=6.203461647033691, loss=2.688836097717285
I0201 09:58:59.465135 139907737556736 logging_writer.py:48] [8900] global_step=8900, grad_norm=8.929107666015625, loss=2.64844012260437
I0201 09:59:33.322792 139908710635264 logging_writer.py:48] [9000] global_step=9000, grad_norm=6.2237372398376465, loss=2.7607264518737793
I0201 09:59:47.327918 140070692116288 spec.py:321] Evaluating on the training split.
I0201 09:59:53.689579 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 10:00:02.829917 140070692116288 spec.py:349] Evaluating on the test split.
I0201 10:00:05.440617 140070692116288 submission_runner.py:408] Time since start: 3221.89s, 	Step: 9043, 	{'train/accuracy': 0.4929049611091614, 'train/loss': 2.1659739017486572, 'validation/accuracy': 0.438539981842041, 'validation/loss': 2.4691154956817627, 'validation/num_examples': 50000, 'test/accuracy': 0.33480000495910645, 'test/loss': 3.1936228275299072, 'test/num_examples': 10000, 'score': 3095.388578891754, 'total_duration': 3221.8932802677155, 'accumulated_submission_time': 3095.388578891754, 'accumulated_eval_time': 126.01309251785278, 'accumulated_logging_time': 0.1767423152923584}
I0201 10:00:05.460319 139907729164032 logging_writer.py:48] [9043] accumulated_eval_time=126.013093, accumulated_logging_time=0.176742, accumulated_submission_time=3095.388579, global_step=9043, preemption_count=0, score=3095.388579, test/accuracy=0.334800, test/loss=3.193623, test/num_examples=10000, total_duration=3221.893280, train/accuracy=0.492905, train/loss=2.165974, validation/accuracy=0.438540, validation/loss=2.469115, validation/num_examples=50000
I0201 10:00:25.048511 139907737556736 logging_writer.py:48] [9100] global_step=9100, grad_norm=5.910811901092529, loss=2.651038885116577
I0201 10:00:58.863596 139907729164032 logging_writer.py:48] [9200] global_step=9200, grad_norm=5.630504131317139, loss=2.6815853118896484
I0201 10:01:32.682878 139907737556736 logging_writer.py:48] [9300] global_step=9300, grad_norm=6.167248249053955, loss=2.6673951148986816
I0201 10:02:06.517692 139907729164032 logging_writer.py:48] [9400] global_step=9400, grad_norm=5.948907852172852, loss=2.5611534118652344
I0201 10:02:40.379660 139907737556736 logging_writer.py:48] [9500] global_step=9500, grad_norm=8.340653419494629, loss=2.5500969886779785
I0201 10:03:14.219158 139907729164032 logging_writer.py:48] [9600] global_step=9600, grad_norm=6.629983425140381, loss=2.6879019737243652
I0201 10:03:48.070796 139907737556736 logging_writer.py:48] [9700] global_step=9700, grad_norm=8.407662391662598, loss=2.7546496391296387
I0201 10:04:21.923659 139907729164032 logging_writer.py:48] [9800] global_step=9800, grad_norm=6.604456901550293, loss=2.6349856853485107
I0201 10:04:55.843743 139907737556736 logging_writer.py:48] [9900] global_step=9900, grad_norm=4.460180282592773, loss=2.6720328330993652
I0201 10:05:29.686934 139907729164032 logging_writer.py:48] [10000] global_step=10000, grad_norm=7.858814239501953, loss=2.5956649780273438
I0201 10:06:03.497978 139907737556736 logging_writer.py:48] [10100] global_step=10100, grad_norm=7.3966474533081055, loss=2.703733444213867
I0201 10:06:37.341024 139907729164032 logging_writer.py:48] [10200] global_step=10200, grad_norm=7.062016010284424, loss=2.6334798336029053
I0201 10:07:11.164708 139907737556736 logging_writer.py:48] [10300] global_step=10300, grad_norm=7.28587007522583, loss=2.597990036010742
I0201 10:07:44.999037 139907729164032 logging_writer.py:48] [10400] global_step=10400, grad_norm=8.448896408081055, loss=2.4888229370117188
I0201 10:08:18.822800 139907737556736 logging_writer.py:48] [10500] global_step=10500, grad_norm=5.771392345428467, loss=2.500880241394043
I0201 10:08:35.555260 140070692116288 spec.py:321] Evaluating on the training split.
I0201 10:08:41.805554 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 10:08:50.485852 140070692116288 spec.py:349] Evaluating on the test split.
I0201 10:08:53.076842 140070692116288 submission_runner.py:408] Time since start: 3749.53s, 	Step: 10551, 	{'train/accuracy': 0.5285794138908386, 'train/loss': 1.9861119985580444, 'validation/accuracy': 0.49087998270988464, 'validation/loss': 2.2068440914154053, 'validation/num_examples': 50000, 'test/accuracy': 0.3759000301361084, 'test/loss': 2.9420087337493896, 'test/num_examples': 10000, 'score': 3605.421551465988, 'total_duration': 3749.529512166977, 'accumulated_submission_time': 3605.421551465988, 'accumulated_eval_time': 143.53465509414673, 'accumulated_logging_time': 0.2061150074005127}
I0201 10:08:53.095270 139908425447168 logging_writer.py:48] [10551] accumulated_eval_time=143.534655, accumulated_logging_time=0.206115, accumulated_submission_time=3605.421551, global_step=10551, preemption_count=0, score=3605.421551, test/accuracy=0.375900, test/loss=2.942009, test/num_examples=10000, total_duration=3749.529512, train/accuracy=0.528579, train/loss=1.986112, validation/accuracy=0.490880, validation/loss=2.206844, validation/num_examples=50000
I0201 10:09:09.935284 139908710635264 logging_writer.py:48] [10600] global_step=10600, grad_norm=5.661561489105225, loss=2.4640419483184814
I0201 10:09:43.707776 139908425447168 logging_writer.py:48] [10700] global_step=10700, grad_norm=7.260134220123291, loss=2.4267261028289795
I0201 10:10:17.455119 139908710635264 logging_writer.py:48] [10800] global_step=10800, grad_norm=8.782193183898926, loss=2.5566940307617188
I0201 10:10:51.265849 139908425447168 logging_writer.py:48] [10900] global_step=10900, grad_norm=6.862388610839844, loss=2.3973801136016846
I0201 10:11:25.149655 139908710635264 logging_writer.py:48] [11000] global_step=11000, grad_norm=8.206084251403809, loss=2.5723633766174316
I0201 10:11:58.956715 139908425447168 logging_writer.py:48] [11100] global_step=11100, grad_norm=4.750550746917725, loss=2.448058843612671
I0201 10:12:32.701388 139908710635264 logging_writer.py:48] [11200] global_step=11200, grad_norm=6.670444965362549, loss=2.5362536907196045
I0201 10:13:06.505333 139908425447168 logging_writer.py:48] [11300] global_step=11300, grad_norm=8.840716361999512, loss=2.4740548133850098
I0201 10:13:40.313313 139908710635264 logging_writer.py:48] [11400] global_step=11400, grad_norm=6.5731048583984375, loss=2.580087661743164
I0201 10:14:14.105653 139908425447168 logging_writer.py:48] [11500] global_step=11500, grad_norm=6.281918048858643, loss=2.4587314128875732
I0201 10:14:47.875092 139908710635264 logging_writer.py:48] [11600] global_step=11600, grad_norm=6.147467136383057, loss=2.451899290084839
I0201 10:15:21.654991 139908425447168 logging_writer.py:48] [11700] global_step=11700, grad_norm=6.001475811004639, loss=2.491279125213623
I0201 10:15:55.454236 139908710635264 logging_writer.py:48] [11800] global_step=11800, grad_norm=6.2510085105896, loss=2.375293016433716
I0201 10:16:29.246564 139908425447168 logging_writer.py:48] [11900] global_step=11900, grad_norm=7.091833114624023, loss=2.2640743255615234
I0201 10:17:03.004265 139908710635264 logging_writer.py:48] [12000] global_step=12000, grad_norm=6.6647725105285645, loss=2.3540408611297607
I0201 10:17:23.294960 140070692116288 spec.py:321] Evaluating on the training split.
I0201 10:17:29.548116 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 10:17:38.258854 140070692116288 spec.py:349] Evaluating on the test split.
I0201 10:17:40.846327 140070692116288 submission_runner.py:408] Time since start: 4277.30s, 	Step: 12061, 	{'train/accuracy': 0.549226701259613, 'train/loss': 1.8957043886184692, 'validation/accuracy': 0.5112400054931641, 'validation/loss': 2.1058874130249023, 'validation/num_examples': 50000, 'test/accuracy': 0.3946000039577484, 'test/loss': 2.823363780975342, 'test/num_examples': 10000, 'score': 4115.560308218002, 'total_duration': 4277.2989773750305, 'accumulated_submission_time': 4115.560308218002, 'accumulated_eval_time': 161.0859730243683, 'accumulated_logging_time': 0.23340106010437012}
I0201 10:17:40.865374 139907754342144 logging_writer.py:48] [12061] accumulated_eval_time=161.085973, accumulated_logging_time=0.233401, accumulated_submission_time=4115.560308, global_step=12061, preemption_count=0, score=4115.560308, test/accuracy=0.394600, test/loss=2.823364, test/num_examples=10000, total_duration=4277.298977, train/accuracy=0.549227, train/loss=1.895704, validation/accuracy=0.511240, validation/loss=2.105887, validation/num_examples=50000
I0201 10:17:54.390025 139907762734848 logging_writer.py:48] [12100] global_step=12100, grad_norm=8.72542667388916, loss=2.3951964378356934
I0201 10:18:28.156316 139907754342144 logging_writer.py:48] [12200] global_step=12200, grad_norm=6.17670202255249, loss=2.4206349849700928
I0201 10:19:01.902293 139907762734848 logging_writer.py:48] [12300] global_step=12300, grad_norm=6.6772780418396, loss=2.50144624710083
I0201 10:19:35.682919 139907754342144 logging_writer.py:48] [12400] global_step=12400, grad_norm=5.398664474487305, loss=2.385523557662964
I0201 10:20:09.471729 139907762734848 logging_writer.py:48] [12500] global_step=12500, grad_norm=7.539214134216309, loss=2.330880641937256
I0201 10:20:43.270793 139907754342144 logging_writer.py:48] [12600] global_step=12600, grad_norm=8.460099220275879, loss=2.2717387676239014
I0201 10:21:17.072643 139907762734848 logging_writer.py:48] [12700] global_step=12700, grad_norm=4.800795078277588, loss=2.4645557403564453
I0201 10:21:50.882837 139907754342144 logging_writer.py:48] [12800] global_step=12800, grad_norm=5.011255264282227, loss=2.279397487640381
I0201 10:22:24.719596 139907762734848 logging_writer.py:48] [12900] global_step=12900, grad_norm=6.506320953369141, loss=2.2963790893554688
I0201 10:22:58.522024 139907754342144 logging_writer.py:48] [13000] global_step=13000, grad_norm=4.9706902503967285, loss=2.4162111282348633
I0201 10:23:32.358501 139907762734848 logging_writer.py:48] [13100] global_step=13100, grad_norm=7.19435453414917, loss=2.330216884613037
I0201 10:24:06.281707 139907754342144 logging_writer.py:48] [13200] global_step=13200, grad_norm=6.392324447631836, loss=2.394402503967285
I0201 10:24:40.011220 139907762734848 logging_writer.py:48] [13300] global_step=13300, grad_norm=6.466413974761963, loss=2.3446602821350098
I0201 10:25:13.805921 139907754342144 logging_writer.py:48] [13400] global_step=13400, grad_norm=6.963465690612793, loss=2.3066346645355225
I0201 10:25:47.630539 139907762734848 logging_writer.py:48] [13500] global_step=13500, grad_norm=4.780236721038818, loss=2.3003194332122803
I0201 10:26:11.039891 140070692116288 spec.py:321] Evaluating on the training split.
I0201 10:26:17.307734 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 10:26:26.362000 140070692116288 spec.py:349] Evaluating on the test split.
I0201 10:26:28.840116 140070692116288 submission_runner.py:408] Time since start: 4805.29s, 	Step: 13571, 	{'train/accuracy': 0.5690967440605164, 'train/loss': 1.7911511659622192, 'validation/accuracy': 0.5332599878311157, 'validation/loss': 1.9902153015136719, 'validation/num_examples': 50000, 'test/accuracy': 0.41440001130104065, 'test/loss': 2.7417922019958496, 'test/num_examples': 10000, 'score': 4625.673633098602, 'total_duration': 4805.292775630951, 'accumulated_submission_time': 4625.673633098602, 'accumulated_eval_time': 178.88615822792053, 'accumulated_logging_time': 0.26222872734069824}
I0201 10:26:28.859850 139907745949440 logging_writer.py:48] [13571] accumulated_eval_time=178.886158, accumulated_logging_time=0.262229, accumulated_submission_time=4625.673633, global_step=13571, preemption_count=0, score=4625.673633, test/accuracy=0.414400, test/loss=2.741792, test/num_examples=10000, total_duration=4805.292776, train/accuracy=0.569097, train/loss=1.791151, validation/accuracy=0.533260, validation/loss=1.990215, validation/num_examples=50000
I0201 10:26:39.026998 139907754342144 logging_writer.py:48] [13600] global_step=13600, grad_norm=5.0940117835998535, loss=2.263201951980591
I0201 10:27:12.762742 139907745949440 logging_writer.py:48] [13700] global_step=13700, grad_norm=4.992976188659668, loss=2.2400944232940674
I0201 10:27:46.540135 139907754342144 logging_writer.py:48] [13800] global_step=13800, grad_norm=6.789854526519775, loss=2.3785109519958496
I0201 10:28:20.300943 139907745949440 logging_writer.py:48] [13900] global_step=13900, grad_norm=5.27995491027832, loss=2.247488260269165
I0201 10:28:54.077455 139907754342144 logging_writer.py:48] [14000] global_step=14000, grad_norm=5.9348931312561035, loss=2.3091330528259277
I0201 10:29:27.795909 139907745949440 logging_writer.py:48] [14100] global_step=14100, grad_norm=5.338581562042236, loss=2.22471284866333
I0201 10:30:01.680286 139907754342144 logging_writer.py:48] [14200] global_step=14200, grad_norm=6.802419662475586, loss=2.3778204917907715
I0201 10:30:35.466591 139907745949440 logging_writer.py:48] [14300] global_step=14300, grad_norm=7.658504009246826, loss=2.284335136413574
I0201 10:31:09.268052 139907754342144 logging_writer.py:48] [14400] global_step=14400, grad_norm=5.6198930740356445, loss=2.2925357818603516
I0201 10:31:42.968778 139907745949440 logging_writer.py:48] [14500] global_step=14500, grad_norm=5.209236145019531, loss=2.177621364593506
I0201 10:32:16.773504 139907754342144 logging_writer.py:48] [14600] global_step=14600, grad_norm=4.2567830085754395, loss=2.2428364753723145
I0201 10:32:50.525851 139907745949440 logging_writer.py:48] [14700] global_step=14700, grad_norm=6.292847633361816, loss=2.2963974475860596
I0201 10:33:24.261365 139907754342144 logging_writer.py:48] [14800] global_step=14800, grad_norm=4.3204474449157715, loss=2.1129677295684814
I0201 10:33:57.998406 139907745949440 logging_writer.py:48] [14900] global_step=14900, grad_norm=7.969369411468506, loss=2.330460548400879
I0201 10:34:31.757264 139907754342144 logging_writer.py:48] [15000] global_step=15000, grad_norm=5.509945392608643, loss=2.2827210426330566
I0201 10:34:58.883290 140070692116288 spec.py:321] Evaluating on the training split.
I0201 10:35:05.299052 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 10:35:14.165667 140070692116288 spec.py:349] Evaluating on the test split.
I0201 10:35:16.783668 140070692116288 submission_runner.py:408] Time since start: 5333.24s, 	Step: 15082, 	{'train/accuracy': 0.5841637253761292, 'train/loss': 1.741123914718628, 'validation/accuracy': 0.5440999865531921, 'validation/loss': 1.943701148033142, 'validation/num_examples': 50000, 'test/accuracy': 0.4231000244617462, 'test/loss': 2.674654245376587, 'test/num_examples': 10000, 'score': 5135.629874706268, 'total_duration': 5333.236327886581, 'accumulated_submission_time': 5135.629874706268, 'accumulated_eval_time': 196.78649830818176, 'accumulated_logging_time': 0.29550814628601074}
I0201 10:35:16.808745 139907737556736 logging_writer.py:48] [15082] accumulated_eval_time=196.786498, accumulated_logging_time=0.295508, accumulated_submission_time=5135.629875, global_step=15082, preemption_count=0, score=5135.629875, test/accuracy=0.423100, test/loss=2.674654, test/num_examples=10000, total_duration=5333.236328, train/accuracy=0.584164, train/loss=1.741124, validation/accuracy=0.544100, validation/loss=1.943701, validation/num_examples=50000
I0201 10:35:23.209818 139908425447168 logging_writer.py:48] [15100] global_step=15100, grad_norm=7.409799575805664, loss=2.240262746810913
I0201 10:35:56.866992 139907737556736 logging_writer.py:48] [15200] global_step=15200, grad_norm=11.093629837036133, loss=2.1544203758239746
I0201 10:36:30.643834 139908425447168 logging_writer.py:48] [15300] global_step=15300, grad_norm=5.706386566162109, loss=2.155395984649658
I0201 10:37:04.394866 139907737556736 logging_writer.py:48] [15400] global_step=15400, grad_norm=9.553391456604004, loss=2.413719654083252
I0201 10:37:38.163605 139908425447168 logging_writer.py:48] [15500] global_step=15500, grad_norm=5.670063018798828, loss=2.0940005779266357
I0201 10:38:11.944337 139907737556736 logging_writer.py:48] [15600] global_step=15600, grad_norm=7.481932163238525, loss=2.2080671787261963
I0201 10:38:45.707589 139908425447168 logging_writer.py:48] [15700] global_step=15700, grad_norm=6.228719711303711, loss=2.287106990814209
I0201 10:39:19.469731 139907737556736 logging_writer.py:48] [15800] global_step=15800, grad_norm=7.236441135406494, loss=2.3081750869750977
I0201 10:39:53.231042 139908425447168 logging_writer.py:48] [15900] global_step=15900, grad_norm=10.304725646972656, loss=2.160612106323242
I0201 10:40:26.951981 139907737556736 logging_writer.py:48] [16000] global_step=16000, grad_norm=8.199586868286133, loss=2.0987389087677
I0201 10:41:00.728062 139908425447168 logging_writer.py:48] [16100] global_step=16100, grad_norm=5.077242374420166, loss=2.118701219558716
I0201 10:41:34.499610 139907737556736 logging_writer.py:48] [16200] global_step=16200, grad_norm=6.42869758605957, loss=2.285088539123535
I0201 10:42:08.264229 139908425447168 logging_writer.py:48] [16300] global_step=16300, grad_norm=5.2650628089904785, loss=2.2440295219421387
I0201 10:42:42.213747 139907737556736 logging_writer.py:48] [16400] global_step=16400, grad_norm=5.7579545974731445, loss=2.1458423137664795
I0201 10:43:15.956603 139908425447168 logging_writer.py:48] [16500] global_step=16500, grad_norm=4.775984764099121, loss=2.2238526344299316
I0201 10:43:46.796652 140070692116288 spec.py:321] Evaluating on the training split.
I0201 10:43:53.059982 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 10:44:01.605136 140070692116288 spec.py:349] Evaluating on the test split.
I0201 10:44:04.320238 140070692116288 submission_runner.py:408] Time since start: 5860.77s, 	Step: 16593, 	{'train/accuracy': 0.6349848508834839, 'train/loss': 1.4669139385223389, 'validation/accuracy': 0.557200014591217, 'validation/loss': 1.8688576221466064, 'validation/num_examples': 50000, 'test/accuracy': 0.4305000305175781, 'test/loss': 2.623843193054199, 'test/num_examples': 10000, 'score': 5645.554906845093, 'total_duration': 5860.77290892601, 'accumulated_submission_time': 5645.554906845093, 'accumulated_eval_time': 214.31005549430847, 'accumulated_logging_time': 0.3311014175415039}
I0201 10:44:04.339636 139908710635264 logging_writer.py:48] [16593] accumulated_eval_time=214.310055, accumulated_logging_time=0.331101, accumulated_submission_time=5645.554907, global_step=16593, preemption_count=0, score=5645.554907, test/accuracy=0.430500, test/loss=2.623843, test/num_examples=10000, total_duration=5860.772909, train/accuracy=0.634985, train/loss=1.466914, validation/accuracy=0.557200, validation/loss=1.868858, validation/num_examples=50000
I0201 10:44:07.066157 139908719027968 logging_writer.py:48] [16600] global_step=16600, grad_norm=7.848126411437988, loss=2.248680591583252
I0201 10:44:40.807367 139908710635264 logging_writer.py:48] [16700] global_step=16700, grad_norm=5.270316123962402, loss=2.1663601398468018
I0201 10:45:14.493334 139908719027968 logging_writer.py:48] [16800] global_step=16800, grad_norm=6.521854400634766, loss=2.192770004272461
I0201 10:45:48.197420 139908710635264 logging_writer.py:48] [16900] global_step=16900, grad_norm=5.097383975982666, loss=2.1222290992736816
I0201 10:46:21.958214 139908719027968 logging_writer.py:48] [17000] global_step=17000, grad_norm=4.472500324249268, loss=2.3463621139526367
I0201 10:46:55.713733 139908710635264 logging_writer.py:48] [17100] global_step=17100, grad_norm=7.796491622924805, loss=2.160304546356201
I0201 10:47:29.459595 139908719027968 logging_writer.py:48] [17200] global_step=17200, grad_norm=5.6468729972839355, loss=2.2669036388397217
I0201 10:48:03.267432 139908710635264 logging_writer.py:48] [17300] global_step=17300, grad_norm=4.242426872253418, loss=2.1574788093566895
I0201 10:48:37.015861 139908719027968 logging_writer.py:48] [17400] global_step=17400, grad_norm=8.255228996276855, loss=2.306156635284424
I0201 10:49:10.929560 139908710635264 logging_writer.py:48] [17500] global_step=17500, grad_norm=6.869631290435791, loss=2.1868720054626465
I0201 10:49:44.616517 139908719027968 logging_writer.py:48] [17600] global_step=17600, grad_norm=4.344733715057373, loss=2.1406760215759277
I0201 10:50:18.371496 139908710635264 logging_writer.py:48] [17700] global_step=17700, grad_norm=6.921165466308594, loss=2.0525126457214355
I0201 10:50:52.098499 139908719027968 logging_writer.py:48] [17800] global_step=17800, grad_norm=4.459743976593018, loss=2.1449339389801025
I0201 10:51:25.904611 139908710635264 logging_writer.py:48] [17900] global_step=17900, grad_norm=4.867574691772461, loss=2.2166895866394043
I0201 10:51:59.656970 139908719027968 logging_writer.py:48] [18000] global_step=18000, grad_norm=4.968900680541992, loss=2.1700568199157715
I0201 10:52:33.405007 139908710635264 logging_writer.py:48] [18100] global_step=18100, grad_norm=4.364200592041016, loss=2.2792844772338867
I0201 10:52:34.575711 140070692116288 spec.py:321] Evaluating on the training split.
I0201 10:52:40.856756 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 10:52:49.549201 140070692116288 spec.py:349] Evaluating on the test split.
I0201 10:52:52.177644 140070692116288 submission_runner.py:408] Time since start: 6388.63s, 	Step: 18105, 	{'train/accuracy': 0.6109095811843872, 'train/loss': 1.5742591619491577, 'validation/accuracy': 0.5535399913787842, 'validation/loss': 1.891344428062439, 'validation/num_examples': 50000, 'test/accuracy': 0.4300000071525574, 'test/loss': 2.6250672340393066, 'test/num_examples': 10000, 'score': 6155.727502822876, 'total_duration': 6388.630298376083, 'accumulated_submission_time': 6155.727502822876, 'accumulated_eval_time': 231.91193890571594, 'accumulated_logging_time': 0.36240530014038086}
I0201 10:52:52.200674 139907745949440 logging_writer.py:48] [18105] accumulated_eval_time=231.911939, accumulated_logging_time=0.362405, accumulated_submission_time=6155.727503, global_step=18105, preemption_count=0, score=6155.727503, test/accuracy=0.430000, test/loss=2.625067, test/num_examples=10000, total_duration=6388.630298, train/accuracy=0.610910, train/loss=1.574259, validation/accuracy=0.553540, validation/loss=1.891344, validation/num_examples=50000
I0201 10:53:24.539682 139907754342144 logging_writer.py:48] [18200] global_step=18200, grad_norm=6.549831390380859, loss=2.1705031394958496
I0201 10:53:58.274601 139907745949440 logging_writer.py:48] [18300] global_step=18300, grad_norm=5.417239189147949, loss=2.2415075302124023
I0201 10:54:32.035981 139907754342144 logging_writer.py:48] [18400] global_step=18400, grad_norm=4.379926681518555, loss=2.3574869632720947
I0201 10:55:05.785278 139907745949440 logging_writer.py:48] [18500] global_step=18500, grad_norm=4.80797004699707, loss=2.2352752685546875
I0201 10:55:39.661965 139907754342144 logging_writer.py:48] [18600] global_step=18600, grad_norm=5.296478748321533, loss=2.146754264831543
I0201 10:56:13.425983 139907745949440 logging_writer.py:48] [18700] global_step=18700, grad_norm=4.712474822998047, loss=2.1038999557495117
I0201 10:56:47.199106 139907754342144 logging_writer.py:48] [18800] global_step=18800, grad_norm=3.383828639984131, loss=2.13875675201416
I0201 10:57:20.921319 139907745949440 logging_writer.py:48] [18900] global_step=18900, grad_norm=5.211409568786621, loss=2.174032211303711
I0201 10:57:54.689564 139907754342144 logging_writer.py:48] [19000] global_step=19000, grad_norm=4.470532417297363, loss=2.1600329875946045
I0201 10:58:28.398460 139907745949440 logging_writer.py:48] [19100] global_step=19100, grad_norm=6.02723503112793, loss=2.1471118927001953
I0201 10:59:02.158078 139907754342144 logging_writer.py:48] [19200] global_step=19200, grad_norm=4.54077672958374, loss=2.153634548187256
I0201 10:59:35.879479 139907745949440 logging_writer.py:48] [19300] global_step=19300, grad_norm=5.693173408508301, loss=2.2166647911071777
I0201 11:00:09.656064 139907754342144 logging_writer.py:48] [19400] global_step=19400, grad_norm=4.192084312438965, loss=2.2240350246429443
I0201 11:00:43.391007 139907745949440 logging_writer.py:48] [19500] global_step=19500, grad_norm=4.959330081939697, loss=2.0709118843078613
I0201 11:01:17.156339 139907754342144 logging_writer.py:48] [19600] global_step=19600, grad_norm=5.256694316864014, loss=2.1638665199279785
I0201 11:01:22.376844 140070692116288 spec.py:321] Evaluating on the training split.
I0201 11:01:28.706836 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 11:01:37.371643 140070692116288 spec.py:349] Evaluating on the test split.
I0201 11:01:39.974666 140070692116288 submission_runner.py:408] Time since start: 6916.43s, 	Step: 19617, 	{'train/accuracy': 0.6103914380073547, 'train/loss': 1.5961946249008179, 'validation/accuracy': 0.5658800005912781, 'validation/loss': 1.8382683992385864, 'validation/num_examples': 50000, 'test/accuracy': 0.43970000743865967, 'test/loss': 2.6097118854522705, 'test/num_examples': 10000, 'score': 6665.842526435852, 'total_duration': 6916.42732667923, 'accumulated_submission_time': 6665.842526435852, 'accumulated_eval_time': 249.50973081588745, 'accumulated_logging_time': 0.394974946975708}
I0201 11:01:39.994667 139907745949440 logging_writer.py:48] [19617] accumulated_eval_time=249.509731, accumulated_logging_time=0.394975, accumulated_submission_time=6665.842526, global_step=19617, preemption_count=0, score=6665.842526, test/accuracy=0.439700, test/loss=2.609712, test/num_examples=10000, total_duration=6916.427327, train/accuracy=0.610391, train/loss=1.596195, validation/accuracy=0.565880, validation/loss=1.838268, validation/num_examples=50000
I0201 11:02:08.410824 139907754342144 logging_writer.py:48] [19700] global_step=19700, grad_norm=4.66217565536499, loss=2.241405487060547
I0201 11:02:42.112591 139907745949440 logging_writer.py:48] [19800] global_step=19800, grad_norm=3.407278060913086, loss=1.9687913656234741
I0201 11:03:15.774603 139907754342144 logging_writer.py:48] [19900] global_step=19900, grad_norm=6.236395359039307, loss=1.9451904296875
I0201 11:03:49.503247 139907745949440 logging_writer.py:48] [20000] global_step=20000, grad_norm=3.851501226425171, loss=2.119220018386841
I0201 11:04:23.243960 139907754342144 logging_writer.py:48] [20100] global_step=20100, grad_norm=4.482935428619385, loss=2.0286874771118164
I0201 11:04:56.977658 139907745949440 logging_writer.py:48] [20200] global_step=20200, grad_norm=3.9209678173065186, loss=2.216242790222168
I0201 11:05:30.644700 139907754342144 logging_writer.py:48] [20300] global_step=20300, grad_norm=3.5085034370422363, loss=2.040501594543457
I0201 11:06:04.356604 139907745949440 logging_writer.py:48] [20400] global_step=20400, grad_norm=5.4125518798828125, loss=2.2063753604888916
I0201 11:06:38.108531 139907754342144 logging_writer.py:48] [20500] global_step=20500, grad_norm=4.321043014526367, loss=2.102022647857666
I0201 11:07:11.839308 139907745949440 logging_writer.py:48] [20600] global_step=20600, grad_norm=4.100398540496826, loss=2.02766752243042
I0201 11:07:45.587504 139907754342144 logging_writer.py:48] [20700] global_step=20700, grad_norm=5.089968204498291, loss=2.0228688716888428
I0201 11:08:19.370479 139907745949440 logging_writer.py:48] [20800] global_step=20800, grad_norm=4.43333101272583, loss=2.1360347270965576
I0201 11:08:53.098356 139907754342144 logging_writer.py:48] [20900] global_step=20900, grad_norm=3.402143955230713, loss=2.206059455871582
I0201 11:09:26.791685 139907745949440 logging_writer.py:48] [21000] global_step=21000, grad_norm=6.477739334106445, loss=2.143497943878174
I0201 11:10:00.462379 139907754342144 logging_writer.py:48] [21100] global_step=21100, grad_norm=3.4289870262145996, loss=2.087256908416748
I0201 11:10:10.043068 140070692116288 spec.py:321] Evaluating on the training split.
I0201 11:10:16.301597 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 11:10:24.945647 140070692116288 spec.py:349] Evaluating on the test split.
I0201 11:10:27.555349 140070692116288 submission_runner.py:408] Time since start: 7444.01s, 	Step: 21130, 	{'train/accuracy': 0.6112683415412903, 'train/loss': 1.5985033512115479, 'validation/accuracy': 0.5659999847412109, 'validation/loss': 1.8424259424209595, 'validation/num_examples': 50000, 'test/accuracy': 0.4401000142097473, 'test/loss': 2.5855188369750977, 'test/num_examples': 10000, 'score': 7175.8285439014435, 'total_duration': 7444.008017539978, 'accumulated_submission_time': 7175.8285439014435, 'accumulated_eval_time': 267.0219874382019, 'accumulated_logging_time': 0.42498087882995605}
I0201 11:10:27.576071 139907762734848 logging_writer.py:48] [21130] accumulated_eval_time=267.021987, accumulated_logging_time=0.424981, accumulated_submission_time=7175.828544, global_step=21130, preemption_count=0, score=7175.828544, test/accuracy=0.440100, test/loss=2.585519, test/num_examples=10000, total_duration=7444.008018, train/accuracy=0.611268, train/loss=1.598503, validation/accuracy=0.566000, validation/loss=1.842426, validation/num_examples=50000
I0201 11:10:51.464051 139908425447168 logging_writer.py:48] [21200] global_step=21200, grad_norm=5.026702404022217, loss=2.0776302814483643
I0201 11:11:25.215407 139907762734848 logging_writer.py:48] [21300] global_step=21300, grad_norm=4.381139278411865, loss=2.1924619674682617
I0201 11:11:58.935087 139908425447168 logging_writer.py:48] [21400] global_step=21400, grad_norm=4.425209045410156, loss=2.0389766693115234
I0201 11:12:32.697881 139907762734848 logging_writer.py:48] [21500] global_step=21500, grad_norm=3.5318164825439453, loss=2.033402919769287
I0201 11:13:06.406860 139908425447168 logging_writer.py:48] [21600] global_step=21600, grad_norm=4.449843406677246, loss=2.0604889392852783
I0201 11:13:40.066745 139907762734848 logging_writer.py:48] [21700] global_step=21700, grad_norm=4.12268590927124, loss=2.0800490379333496
I0201 11:14:13.783183 139908425447168 logging_writer.py:48] [21800] global_step=21800, grad_norm=3.7876193523406982, loss=2.108140468597412
I0201 11:14:47.573718 139907762734848 logging_writer.py:48] [21900] global_step=21900, grad_norm=4.468055725097656, loss=2.1715548038482666
I0201 11:15:21.287477 139908425447168 logging_writer.py:48] [22000] global_step=22000, grad_norm=4.411651611328125, loss=2.0335710048675537
I0201 11:15:55.052257 139907762734848 logging_writer.py:48] [22100] global_step=22100, grad_norm=3.716315984725952, loss=2.0175936222076416
I0201 11:16:28.774766 139908425447168 logging_writer.py:48] [22200] global_step=22200, grad_norm=3.8625664710998535, loss=2.1692092418670654
I0201 11:17:02.523196 139907762734848 logging_writer.py:48] [22300] global_step=22300, grad_norm=4.006656646728516, loss=2.016779899597168
I0201 11:17:36.236911 139908425447168 logging_writer.py:48] [22400] global_step=22400, grad_norm=4.496334075927734, loss=2.1222031116485596
I0201 11:18:09.988700 139907762734848 logging_writer.py:48] [22500] global_step=22500, grad_norm=4.250924110412598, loss=2.030794858932495
I0201 11:18:43.701244 139908425447168 logging_writer.py:48] [22600] global_step=22600, grad_norm=4.678791522979736, loss=2.0081677436828613
I0201 11:18:57.703606 140070692116288 spec.py:321] Evaluating on the training split.
I0201 11:19:04.294946 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 11:19:12.988735 140070692116288 spec.py:349] Evaluating on the test split.
I0201 11:19:15.566600 140070692116288 submission_runner.py:408] Time since start: 7972.02s, 	Step: 22643, 	{'train/accuracy': 0.6199776530265808, 'train/loss': 1.546796441078186, 'validation/accuracy': 0.5753399729728699, 'validation/loss': 1.773450255393982, 'validation/num_examples': 50000, 'test/accuracy': 0.45100003480911255, 'test/loss': 2.5334506034851074, 'test/num_examples': 10000, 'score': 7685.893748044968, 'total_duration': 7972.019269227982, 'accumulated_submission_time': 7685.893748044968, 'accumulated_eval_time': 284.88498640060425, 'accumulated_logging_time': 0.45513081550598145}
I0201 11:19:15.587279 139907754342144 logging_writer.py:48] [22643] accumulated_eval_time=284.884986, accumulated_logging_time=0.455131, accumulated_submission_time=7685.893748, global_step=22643, preemption_count=0, score=7685.893748, test/accuracy=0.451000, test/loss=2.533451, test/num_examples=10000, total_duration=7972.019269, train/accuracy=0.619978, train/loss=1.546796, validation/accuracy=0.575340, validation/loss=1.773450, validation/num_examples=50000
I0201 11:19:35.129907 139907762734848 logging_writer.py:48] [22700] global_step=22700, grad_norm=3.587606430053711, loss=2.0549633502960205
I0201 11:20:08.786411 139907754342144 logging_writer.py:48] [22800] global_step=22800, grad_norm=3.5026955604553223, loss=2.1387898921966553
I0201 11:20:42.514053 139907762734848 logging_writer.py:48] [22900] global_step=22900, grad_norm=5.4021430015563965, loss=2.012517213821411
I0201 11:21:16.338659 139907754342144 logging_writer.py:48] [23000] global_step=23000, grad_norm=3.9112210273742676, loss=2.084979295730591
I0201 11:21:50.058710 139907762734848 logging_writer.py:48] [23100] global_step=23100, grad_norm=5.324692249298096, loss=2.169717788696289
I0201 11:22:23.802439 139907754342144 logging_writer.py:48] [23200] global_step=23200, grad_norm=4.430755138397217, loss=2.0617125034332275
I0201 11:22:57.523031 139907762734848 logging_writer.py:48] [23300] global_step=23300, grad_norm=4.255810260772705, loss=2.1083710193634033
I0201 11:23:31.272661 139907754342144 logging_writer.py:48] [23400] global_step=23400, grad_norm=3.6772139072418213, loss=1.9520275592803955
I0201 11:24:05.010415 139907762734848 logging_writer.py:48] [23500] global_step=23500, grad_norm=4.024404525756836, loss=2.057356595993042
I0201 11:24:38.683959 139907754342144 logging_writer.py:48] [23600] global_step=23600, grad_norm=5.188469409942627, loss=2.0617129802703857
I0201 11:25:12.481259 139907762734848 logging_writer.py:48] [23700] global_step=23700, grad_norm=4.460898399353027, loss=2.106682538986206
I0201 11:25:46.161689 139907754342144 logging_writer.py:48] [23800] global_step=23800, grad_norm=4.376012802124023, loss=2.01245379447937
I0201 11:26:19.930289 139907762734848 logging_writer.py:48] [23900] global_step=23900, grad_norm=3.6879658699035645, loss=2.0308847427368164
I0201 11:26:53.627388 139907754342144 logging_writer.py:48] [24000] global_step=24000, grad_norm=4.625599384307861, loss=2.1311333179473877
I0201 11:27:27.441692 139907762734848 logging_writer.py:48] [24100] global_step=24100, grad_norm=4.343982219696045, loss=2.0606517791748047
I0201 11:27:45.790912 140070692116288 spec.py:321] Evaluating on the training split.
I0201 11:27:52.053308 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 11:28:00.737579 140070692116288 spec.py:349] Evaluating on the test split.
I0201 11:28:03.481188 140070692116288 submission_runner.py:408] Time since start: 8499.93s, 	Step: 24156, 	{'train/accuracy': 0.6190210580825806, 'train/loss': 1.5578505992889404, 'validation/accuracy': 0.5760399699211121, 'validation/loss': 1.7675529718399048, 'validation/num_examples': 50000, 'test/accuracy': 0.45330002903938293, 'test/loss': 2.5400712490081787, 'test/num_examples': 10000, 'score': 8196.035193443298, 'total_duration': 8499.933848619461, 'accumulated_submission_time': 8196.035193443298, 'accumulated_eval_time': 302.57521986961365, 'accumulated_logging_time': 0.48557424545288086}
I0201 11:28:03.502386 139907745949440 logging_writer.py:48] [24156] accumulated_eval_time=302.575220, accumulated_logging_time=0.485574, accumulated_submission_time=8196.035193, global_step=24156, preemption_count=0, score=8196.035193, test/accuracy=0.453300, test/loss=2.540071, test/num_examples=10000, total_duration=8499.933849, train/accuracy=0.619021, train/loss=1.557851, validation/accuracy=0.576040, validation/loss=1.767553, validation/num_examples=50000
I0201 11:28:18.679338 139907754342144 logging_writer.py:48] [24200] global_step=24200, grad_norm=4.854485034942627, loss=2.064887285232544
I0201 11:28:52.359966 139907745949440 logging_writer.py:48] [24300] global_step=24300, grad_norm=4.042715549468994, loss=1.9933325052261353
I0201 11:29:26.075492 139907754342144 logging_writer.py:48] [24400] global_step=24400, grad_norm=5.194443225860596, loss=2.1014256477355957
I0201 11:29:59.813234 139907745949440 logging_writer.py:48] [24500] global_step=24500, grad_norm=5.056919097900391, loss=2.078704357147217
I0201 11:30:33.598699 139907754342144 logging_writer.py:48] [24600] global_step=24600, grad_norm=5.993542671203613, loss=2.0420427322387695
I0201 11:31:07.288429 139907745949440 logging_writer.py:48] [24700] global_step=24700, grad_norm=5.054775238037109, loss=1.8842500448226929
I0201 11:31:41.081216 139907754342144 logging_writer.py:48] [24800] global_step=24800, grad_norm=4.514476299285889, loss=2.129359245300293
I0201 11:32:14.827790 139907745949440 logging_writer.py:48] [24900] global_step=24900, grad_norm=4.040369987487793, loss=2.107820749282837
I0201 11:32:48.568199 139907754342144 logging_writer.py:48] [25000] global_step=25000, grad_norm=3.1948211193084717, loss=2.054009437561035
I0201 11:33:22.363665 139907745949440 logging_writer.py:48] [25100] global_step=25100, grad_norm=3.2010014057159424, loss=1.993098497390747
I0201 11:33:56.075590 139907754342144 logging_writer.py:48] [25200] global_step=25200, grad_norm=4.770113945007324, loss=1.9975640773773193
I0201 11:34:29.767979 139907745949440 logging_writer.py:48] [25300] global_step=25300, grad_norm=4.4849629402160645, loss=1.993525505065918
I0201 11:35:03.548008 139907754342144 logging_writer.py:48] [25400] global_step=25400, grad_norm=3.1309187412261963, loss=2.0854671001434326
I0201 11:35:37.279034 139907745949440 logging_writer.py:48] [25500] global_step=25500, grad_norm=4.906482219696045, loss=2.0452375411987305
I0201 11:36:11.038744 139907754342144 logging_writer.py:48] [25600] global_step=25600, grad_norm=3.0697238445281982, loss=2.070385456085205
I0201 11:36:33.787315 140070692116288 spec.py:321] Evaluating on the training split.
I0201 11:36:40.135091 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 11:36:48.911300 140070692116288 spec.py:349] Evaluating on the test split.
I0201 11:36:51.420514 140070692116288 submission_runner.py:408] Time since start: 9027.87s, 	Step: 25669, 	{'train/accuracy': 0.6695631146430969, 'train/loss': 1.318595290184021, 'validation/accuracy': 0.5854399800300598, 'validation/loss': 1.7338712215423584, 'validation/num_examples': 50000, 'test/accuracy': 0.4646000266075134, 'test/loss': 2.441926956176758, 'test/num_examples': 10000, 'score': 8706.256494283676, 'total_duration': 9027.873177528381, 'accumulated_submission_time': 8706.256494283676, 'accumulated_eval_time': 320.2084016799927, 'accumulated_logging_time': 0.51666259765625}
I0201 11:36:51.442352 139907762734848 logging_writer.py:48] [25669] accumulated_eval_time=320.208402, accumulated_logging_time=0.516663, accumulated_submission_time=8706.256494, global_step=25669, preemption_count=0, score=8706.256494, test/accuracy=0.464600, test/loss=2.441927, test/num_examples=10000, total_duration=9027.873178, train/accuracy=0.669563, train/loss=1.318595, validation/accuracy=0.585440, validation/loss=1.733871, validation/num_examples=50000
I0201 11:37:02.231842 139908425447168 logging_writer.py:48] [25700] global_step=25700, grad_norm=3.884436845779419, loss=1.9360488653182983
I0201 11:37:36.009063 139907762734848 logging_writer.py:48] [25800] global_step=25800, grad_norm=3.463456630706787, loss=1.9690091609954834
I0201 11:38:09.675054 139908425447168 logging_writer.py:48] [25900] global_step=25900, grad_norm=3.605319023132324, loss=2.021104097366333
I0201 11:38:43.395597 139907762734848 logging_writer.py:48] [26000] global_step=26000, grad_norm=3.70154070854187, loss=2.0991151332855225
I0201 11:39:17.093090 139908425447168 logging_writer.py:48] [26100] global_step=26100, grad_norm=4.317074775695801, loss=2.0067384243011475
I0201 11:39:50.952440 139907762734848 logging_writer.py:48] [26200] global_step=26200, grad_norm=5.017609119415283, loss=2.014725923538208
I0201 11:40:24.684752 139908425447168 logging_writer.py:48] [26300] global_step=26300, grad_norm=3.5866076946258545, loss=2.0870108604431152
I0201 11:40:58.393405 139907762734848 logging_writer.py:48] [26400] global_step=26400, grad_norm=4.167263031005859, loss=2.0338492393493652
I0201 11:41:32.068139 139908425447168 logging_writer.py:48] [26500] global_step=26500, grad_norm=4.329430103302002, loss=2.0404820442199707
I0201 11:42:05.789424 139907762734848 logging_writer.py:48] [26600] global_step=26600, grad_norm=4.222240447998047, loss=1.9501105546951294
I0201 11:42:39.533998 139908425447168 logging_writer.py:48] [26700] global_step=26700, grad_norm=2.9109151363372803, loss=1.8435001373291016
I0201 11:43:13.267929 139907762734848 logging_writer.py:48] [26800] global_step=26800, grad_norm=3.4955391883850098, loss=1.9812511205673218
I0201 11:43:47.012795 139908425447168 logging_writer.py:48] [26900] global_step=26900, grad_norm=4.440279006958008, loss=2.0231101512908936
I0201 11:44:20.745102 139907762734848 logging_writer.py:48] [27000] global_step=27000, grad_norm=3.6850998401641846, loss=2.0687952041625977
I0201 11:44:54.430341 139908425447168 logging_writer.py:48] [27100] global_step=27100, grad_norm=3.9936881065368652, loss=2.11098575592041
I0201 11:45:21.563355 140070692116288 spec.py:321] Evaluating on the training split.
I0201 11:45:27.893793 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 11:45:36.730273 140070692116288 spec.py:349] Evaluating on the test split.
I0201 11:45:39.323457 140070692116288 submission_runner.py:408] Time since start: 9555.78s, 	Step: 27182, 	{'train/accuracy': 0.6529615521430969, 'train/loss': 1.4048078060150146, 'validation/accuracy': 0.5888800024986267, 'validation/loss': 1.7101259231567383, 'validation/num_examples': 50000, 'test/accuracy': 0.46560001373291016, 'test/loss': 2.442213535308838, 'test/num_examples': 10000, 'score': 9216.314534425735, 'total_duration': 9555.776126384735, 'accumulated_submission_time': 9216.314534425735, 'accumulated_eval_time': 337.96847558021545, 'accumulated_logging_time': 0.549079179763794}
I0201 11:45:39.345103 139908719027968 logging_writer.py:48] [27182] accumulated_eval_time=337.968476, accumulated_logging_time=0.549079, accumulated_submission_time=9216.314534, global_step=27182, preemption_count=0, score=9216.314534, test/accuracy=0.465600, test/loss=2.442214, test/num_examples=10000, total_duration=9555.776126, train/accuracy=0.652962, train/loss=1.404808, validation/accuracy=0.588880, validation/loss=1.710126, validation/num_examples=50000
I0201 11:45:45.746184 139908727420672 logging_writer.py:48] [27200] global_step=27200, grad_norm=3.907175064086914, loss=1.9865529537200928
I0201 11:46:19.451388 139908719027968 logging_writer.py:48] [27300] global_step=27300, grad_norm=6.102222919464111, loss=2.056142568588257
I0201 11:46:53.116192 139908727420672 logging_writer.py:48] [27400] global_step=27400, grad_norm=3.7747933864593506, loss=1.9677863121032715
I0201 11:47:26.806151 139908719027968 logging_writer.py:48] [27500] global_step=27500, grad_norm=3.715096950531006, loss=1.8840711116790771
I0201 11:48:00.537036 139908727420672 logging_writer.py:48] [27600] global_step=27600, grad_norm=3.806934356689453, loss=1.9092600345611572
I0201 11:48:34.244826 139908719027968 logging_writer.py:48] [27700] global_step=27700, grad_norm=3.381298542022705, loss=2.0192480087280273
I0201 11:49:07.892729 139908727420672 logging_writer.py:48] [27800] global_step=27800, grad_norm=4.24634313583374, loss=2.072272300720215
I0201 11:49:41.592721 139908719027968 logging_writer.py:48] [27900] global_step=27900, grad_norm=4.056918144226074, loss=1.9990050792694092
I0201 11:50:15.325590 139908727420672 logging_writer.py:48] [28000] global_step=28000, grad_norm=4.155706882476807, loss=1.9858362674713135
I0201 11:50:49.112518 139908719027968 logging_writer.py:48] [28100] global_step=28100, grad_norm=4.153609275817871, loss=2.023775100708008
I0201 11:51:22.803205 139908727420672 logging_writer.py:48] [28200] global_step=28200, grad_norm=4.622993469238281, loss=2.0557773113250732
I0201 11:51:56.548787 139908719027968 logging_writer.py:48] [28300] global_step=28300, grad_norm=4.139328956604004, loss=2.098243474960327
I0201 11:52:30.355669 139908727420672 logging_writer.py:48] [28400] global_step=28400, grad_norm=5.29429292678833, loss=1.9450476169586182
I0201 11:53:04.112392 139908719027968 logging_writer.py:48] [28500] global_step=28500, grad_norm=3.9842097759246826, loss=1.9918761253356934
I0201 11:53:37.804668 139908727420672 logging_writer.py:48] [28600] global_step=28600, grad_norm=3.913539409637451, loss=2.057629108428955
I0201 11:54:09.359271 140070692116288 spec.py:321] Evaluating on the training split.
I0201 11:54:15.592736 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 11:54:24.212285 140070692116288 spec.py:349] Evaluating on the test split.
I0201 11:54:26.839537 140070692116288 submission_runner.py:408] Time since start: 10083.29s, 	Step: 28695, 	{'train/accuracy': 0.6298230290412903, 'train/loss': 1.5027976036071777, 'validation/accuracy': 0.5792199969291687, 'validation/loss': 1.772684931755066, 'validation/num_examples': 50000, 'test/accuracy': 0.4537000358104706, 'test/loss': 2.5030179023742676, 'test/num_examples': 10000, 'score': 9726.26763677597, 'total_duration': 10083.292204618454, 'accumulated_submission_time': 9726.26763677597, 'accumulated_eval_time': 355.44871044158936, 'accumulated_logging_time': 0.5797502994537354}
I0201 11:54:26.862249 139907745949440 logging_writer.py:48] [28695] accumulated_eval_time=355.448710, accumulated_logging_time=0.579750, accumulated_submission_time=9726.267637, global_step=28695, preemption_count=0, score=9726.267637, test/accuracy=0.453700, test/loss=2.503018, test/num_examples=10000, total_duration=10083.292205, train/accuracy=0.629823, train/loss=1.502798, validation/accuracy=0.579220, validation/loss=1.772685, validation/num_examples=50000
I0201 11:54:28.896655 139907754342144 logging_writer.py:48] [28700] global_step=28700, grad_norm=3.1936352252960205, loss=1.9689292907714844
I0201 11:55:02.570263 139907745949440 logging_writer.py:48] [28800] global_step=28800, grad_norm=4.229060173034668, loss=1.962792158126831
I0201 11:55:36.272675 139907754342144 logging_writer.py:48] [28900] global_step=28900, grad_norm=4.454586029052734, loss=1.892876148223877
I0201 11:56:09.943950 139907745949440 logging_writer.py:48] [29000] global_step=29000, grad_norm=3.2302658557891846, loss=2.0243771076202393
I0201 11:56:43.660739 139907754342144 logging_writer.py:48] [29100] global_step=29100, grad_norm=4.088079452514648, loss=2.0149178504943848
I0201 11:57:17.335558 139907745949440 logging_writer.py:48] [29200] global_step=29200, grad_norm=3.974929094314575, loss=2.0174951553344727
I0201 11:57:51.092148 139907754342144 logging_writer.py:48] [29300] global_step=29300, grad_norm=4.228001117706299, loss=2.1035618782043457
I0201 11:58:24.769623 139907745949440 logging_writer.py:48] [29400] global_step=29400, grad_norm=3.954432249069214, loss=1.9742602109909058
I0201 11:58:58.567577 139907754342144 logging_writer.py:48] [29500] global_step=29500, grad_norm=3.732701539993286, loss=2.059008836746216
I0201 11:59:32.264819 139907745949440 logging_writer.py:48] [29600] global_step=29600, grad_norm=3.859999179840088, loss=2.0552687644958496
I0201 12:00:05.988596 139907754342144 logging_writer.py:48] [29700] global_step=29700, grad_norm=3.730151653289795, loss=1.9012163877487183
I0201 12:00:39.657145 139907745949440 logging_writer.py:48] [29800] global_step=29800, grad_norm=5.250282287597656, loss=2.059849739074707
I0201 12:01:13.370489 139907754342144 logging_writer.py:48] [29900] global_step=29900, grad_norm=3.7761075496673584, loss=1.9760897159576416
I0201 12:01:47.038024 139907745949440 logging_writer.py:48] [30000] global_step=30000, grad_norm=3.811154842376709, loss=1.9538546800613403
I0201 12:02:20.751715 139907754342144 logging_writer.py:48] [30100] global_step=30100, grad_norm=3.516937017440796, loss=1.9569168090820312
I0201 12:02:54.436838 139907745949440 logging_writer.py:48] [30200] global_step=30200, grad_norm=3.582998275756836, loss=2.084066867828369
I0201 12:02:56.947436 140070692116288 spec.py:321] Evaluating on the training split.
I0201 12:03:03.438742 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 12:03:12.057793 140070692116288 spec.py:349] Evaluating on the test split.
I0201 12:03:14.656550 140070692116288 submission_runner.py:408] Time since start: 10611.11s, 	Step: 30209, 	{'train/accuracy': 0.6353435516357422, 'train/loss': 1.4675039052963257, 'validation/accuracy': 0.5884400010108948, 'validation/loss': 1.714426040649414, 'validation/num_examples': 50000, 'test/accuracy': 0.46700000762939453, 'test/loss': 2.455329656600952, 'test/num_examples': 10000, 'score': 10236.290426254272, 'total_duration': 10611.109208583832, 'accumulated_submission_time': 10236.290426254272, 'accumulated_eval_time': 373.15779161453247, 'accumulated_logging_time': 0.6124565601348877}
I0201 12:03:14.678384 139908719027968 logging_writer.py:48] [30209] accumulated_eval_time=373.157792, accumulated_logging_time=0.612457, accumulated_submission_time=10236.290426, global_step=30209, preemption_count=0, score=10236.290426, test/accuracy=0.467000, test/loss=2.455330, test/num_examples=10000, total_duration=10611.109209, train/accuracy=0.635344, train/loss=1.467504, validation/accuracy=0.588440, validation/loss=1.714426, validation/num_examples=50000
I0201 12:03:45.690769 139908727420672 logging_writer.py:48] [30300] global_step=30300, grad_norm=4.550554275512695, loss=1.994443416595459
I0201 12:04:19.397497 139908719027968 logging_writer.py:48] [30400] global_step=30400, grad_norm=3.873713493347168, loss=2.0855863094329834
I0201 12:04:53.082568 139908727420672 logging_writer.py:48] [30500] global_step=30500, grad_norm=4.13145637512207, loss=1.9740711450576782
I0201 12:05:26.866807 139908719027968 logging_writer.py:48] [30600] global_step=30600, grad_norm=3.479973316192627, loss=1.9313102960586548
I0201 12:06:00.543959 139908727420672 logging_writer.py:48] [30700] global_step=30700, grad_norm=4.544430732727051, loss=1.8903173208236694
I0201 12:06:34.258191 139908719027968 logging_writer.py:48] [30800] global_step=30800, grad_norm=3.5860471725463867, loss=2.0747885704040527
I0201 12:07:07.971757 139908727420672 logging_writer.py:48] [30900] global_step=30900, grad_norm=4.127335548400879, loss=1.8989207744598389
I0201 12:07:41.657458 139908719027968 logging_writer.py:48] [31000] global_step=31000, grad_norm=3.2306020259857178, loss=1.9581822156906128
I0201 12:08:15.329486 139908727420672 logging_writer.py:48] [31100] global_step=31100, grad_norm=4.005913734436035, loss=1.9869152307510376
I0201 12:08:49.016222 139908719027968 logging_writer.py:48] [31200] global_step=31200, grad_norm=4.0869832038879395, loss=1.9044543504714966
I0201 12:09:22.694565 139908727420672 logging_writer.py:48] [31300] global_step=31300, grad_norm=3.9975290298461914, loss=1.9984822273254395
I0201 12:09:56.464429 139908719027968 logging_writer.py:48] [31400] global_step=31400, grad_norm=4.180761814117432, loss=2.028853416442871
I0201 12:10:30.140429 139908727420672 logging_writer.py:48] [31500] global_step=31500, grad_norm=3.4508562088012695, loss=1.8882699012756348
I0201 12:11:03.942362 139908719027968 logging_writer.py:48] [31600] global_step=31600, grad_norm=3.42639422416687, loss=1.9489657878875732
I0201 12:11:37.738928 139908727420672 logging_writer.py:48] [31700] global_step=31700, grad_norm=4.0054755210876465, loss=2.0185348987579346
I0201 12:11:44.969651 140070692116288 spec.py:321] Evaluating on the training split.
I0201 12:11:51.213269 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 12:11:59.895156 140070692116288 spec.py:349] Evaluating on the test split.
I0201 12:12:02.524649 140070692116288 submission_runner.py:408] Time since start: 11138.98s, 	Step: 31723, 	{'train/accuracy': 0.6358418464660645, 'train/loss': 1.4786940813064575, 'validation/accuracy': 0.5914999842643738, 'validation/loss': 1.7083582878112793, 'validation/num_examples': 50000, 'test/accuracy': 0.46640002727508545, 'test/loss': 2.412381172180176, 'test/num_examples': 10000, 'score': 10746.51861667633, 'total_duration': 11138.977312088013, 'accumulated_submission_time': 10746.51861667633, 'accumulated_eval_time': 390.7127459049225, 'accumulated_logging_time': 0.6449933052062988}
I0201 12:12:02.550439 139907745949440 logging_writer.py:48] [31723] accumulated_eval_time=390.712746, accumulated_logging_time=0.644993, accumulated_submission_time=10746.518617, global_step=31723, preemption_count=0, score=10746.518617, test/accuracy=0.466400, test/loss=2.412381, test/num_examples=10000, total_duration=11138.977312, train/accuracy=0.635842, train/loss=1.478694, validation/accuracy=0.591500, validation/loss=1.708358, validation/num_examples=50000
I0201 12:12:28.799465 139907754342144 logging_writer.py:48] [31800] global_step=31800, grad_norm=3.5503692626953125, loss=1.8996877670288086
I0201 12:13:02.523259 139907745949440 logging_writer.py:48] [31900] global_step=31900, grad_norm=3.9775002002716064, loss=1.9928650856018066
I0201 12:13:36.239699 139907754342144 logging_writer.py:48] [32000] global_step=32000, grad_norm=4.401782989501953, loss=1.9885473251342773
I0201 12:14:09.904760 139907745949440 logging_writer.py:48] [32100] global_step=32100, grad_norm=4.423062324523926, loss=1.8451080322265625
I0201 12:14:43.615466 139907754342144 logging_writer.py:48] [32200] global_step=32200, grad_norm=3.9935669898986816, loss=2.0092101097106934
I0201 12:15:17.274210 139907745949440 logging_writer.py:48] [32300] global_step=32300, grad_norm=3.8040077686309814, loss=1.940122127532959
I0201 12:15:50.969641 139907754342144 logging_writer.py:48] [32400] global_step=32400, grad_norm=3.4923577308654785, loss=1.8933154344558716
I0201 12:16:24.649385 139907745949440 logging_writer.py:48] [32500] global_step=32500, grad_norm=3.494248867034912, loss=1.9073292016983032
I0201 12:16:58.340841 139907754342144 logging_writer.py:48] [32600] global_step=32600, grad_norm=3.636927366256714, loss=2.0207855701446533
I0201 12:17:32.071286 139907745949440 logging_writer.py:48] [32700] global_step=32700, grad_norm=4.844222068786621, loss=1.9983267784118652
I0201 12:18:05.877221 139907754342144 logging_writer.py:48] [32800] global_step=32800, grad_norm=3.5800294876098633, loss=2.046874523162842
I0201 12:18:39.572881 139907745949440 logging_writer.py:48] [32900] global_step=32900, grad_norm=3.9702203273773193, loss=1.9431962966918945
I0201 12:19:13.266713 139907754342144 logging_writer.py:48] [33000] global_step=33000, grad_norm=3.7151551246643066, loss=2.0609426498413086
I0201 12:19:46.970239 139907745949440 logging_writer.py:48] [33100] global_step=33100, grad_norm=4.004558086395264, loss=1.839976191520691
I0201 12:20:20.712116 139907754342144 logging_writer.py:48] [33200] global_step=33200, grad_norm=3.1357831954956055, loss=1.9158917665481567
I0201 12:20:32.650258 140070692116288 spec.py:321] Evaluating on the training split.
I0201 12:20:38.940974 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 12:20:47.768296 140070692116288 spec.py:349] Evaluating on the test split.
I0201 12:20:50.385798 140070692116288 submission_runner.py:408] Time since start: 11666.84s, 	Step: 33237, 	{'train/accuracy': 0.639668345451355, 'train/loss': 1.453246831893921, 'validation/accuracy': 0.5940999984741211, 'validation/loss': 1.6839145421981812, 'validation/num_examples': 50000, 'test/accuracy': 0.4757000207901001, 'test/loss': 2.400609254837036, 'test/num_examples': 10000, 'score': 11256.555840015411, 'total_duration': 11666.838463544846, 'accumulated_submission_time': 11256.555840015411, 'accumulated_eval_time': 408.448246717453, 'accumulated_logging_time': 0.680262565612793}
I0201 12:20:50.411925 139907745949440 logging_writer.py:48] [33237] accumulated_eval_time=408.448247, accumulated_logging_time=0.680263, accumulated_submission_time=11256.555840, global_step=33237, preemption_count=0, score=11256.555840, test/accuracy=0.475700, test/loss=2.400609, test/num_examples=10000, total_duration=11666.838464, train/accuracy=0.639668, train/loss=1.453247, validation/accuracy=0.594100, validation/loss=1.683915, validation/num_examples=50000
I0201 12:21:11.997704 139908719027968 logging_writer.py:48] [33300] global_step=33300, grad_norm=3.3218135833740234, loss=2.0192675590515137
I0201 12:21:45.650519 139907745949440 logging_writer.py:48] [33400] global_step=33400, grad_norm=3.9465348720550537, loss=1.9473425149917603
I0201 12:22:19.368958 139908719027968 logging_writer.py:48] [33500] global_step=33500, grad_norm=4.285711288452148, loss=1.9656920433044434
I0201 12:22:53.062845 139907745949440 logging_writer.py:48] [33600] global_step=33600, grad_norm=4.234598159790039, loss=2.0232696533203125
I0201 12:23:26.784650 139908719027968 logging_writer.py:48] [33700] global_step=33700, grad_norm=4.290810585021973, loss=2.002002716064453
I0201 12:24:00.476541 139907745949440 logging_writer.py:48] [33800] global_step=33800, grad_norm=3.77492094039917, loss=2.007599353790283
I0201 12:24:34.279067 139908719027968 logging_writer.py:48] [33900] global_step=33900, grad_norm=3.9369919300079346, loss=1.973869800567627
I0201 12:25:07.957322 139907745949440 logging_writer.py:48] [34000] global_step=34000, grad_norm=4.108068466186523, loss=2.098722457885742
I0201 12:25:41.647097 139908719027968 logging_writer.py:48] [34100] global_step=34100, grad_norm=3.5754871368408203, loss=1.8962290287017822
I0201 12:26:15.393027 139907745949440 logging_writer.py:48] [34200] global_step=34200, grad_norm=4.103382110595703, loss=1.9420005083084106
I0201 12:26:49.087330 139908719027968 logging_writer.py:48] [34300] global_step=34300, grad_norm=3.654188632965088, loss=1.8868097066879272
I0201 12:27:22.755869 139907745949440 logging_writer.py:48] [34400] global_step=34400, grad_norm=5.115415573120117, loss=1.8663620948791504
I0201 12:27:56.456999 139908719027968 logging_writer.py:48] [34500] global_step=34500, grad_norm=4.745734214782715, loss=2.050466299057007
I0201 12:28:30.130292 139907745949440 logging_writer.py:48] [34600] global_step=34600, grad_norm=4.393838882446289, loss=1.9428070783615112
I0201 12:29:03.855536 139908719027968 logging_writer.py:48] [34700] global_step=34700, grad_norm=3.750946044921875, loss=1.9227410554885864
I0201 12:29:20.521436 140070692116288 spec.py:321] Evaluating on the training split.
I0201 12:29:26.915607 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 12:29:35.557314 140070692116288 spec.py:349] Evaluating on the test split.
I0201 12:29:38.110892 140070692116288 submission_runner.py:408] Time since start: 12194.56s, 	Step: 34751, 	{'train/accuracy': 0.6583824753761292, 'train/loss': 1.3591587543487549, 'validation/accuracy': 0.5783599615097046, 'validation/loss': 1.7592555284500122, 'validation/num_examples': 50000, 'test/accuracy': 0.4595000147819519, 'test/loss': 2.4588377475738525, 'test/num_examples': 10000, 'score': 11766.602644443512, 'total_duration': 12194.563556909561, 'accumulated_submission_time': 11766.602644443512, 'accumulated_eval_time': 426.0376763343811, 'accumulated_logging_time': 0.717193603515625}
I0201 12:29:38.133801 139907754342144 logging_writer.py:48] [34751] accumulated_eval_time=426.037676, accumulated_logging_time=0.717194, accumulated_submission_time=11766.602644, global_step=34751, preemption_count=0, score=11766.602644, test/accuracy=0.459500, test/loss=2.458838, test/num_examples=10000, total_duration=12194.563557, train/accuracy=0.658382, train/loss=1.359159, validation/accuracy=0.578360, validation/loss=1.759256, validation/num_examples=50000
I0201 12:29:56.034772 139907762734848 logging_writer.py:48] [34800] global_step=34800, grad_norm=3.8959109783172607, loss=1.8971302509307861
I0201 12:30:29.756752 139907754342144 logging_writer.py:48] [34900] global_step=34900, grad_norm=3.859133243560791, loss=1.9557448625564575
I0201 12:31:03.500611 139907762734848 logging_writer.py:48] [35000] global_step=35000, grad_norm=3.9929304122924805, loss=2.001948833465576
I0201 12:31:37.173785 139907754342144 logging_writer.py:48] [35100] global_step=35100, grad_norm=4.756110668182373, loss=1.9139331579208374
I0201 12:32:10.866032 139907762734848 logging_writer.py:48] [35200] global_step=35200, grad_norm=3.8979427814483643, loss=2.0609853267669678
I0201 12:32:44.561323 139907754342144 logging_writer.py:48] [35300] global_step=35300, grad_norm=3.715961456298828, loss=1.7936893701553345
I0201 12:33:18.245007 139907762734848 logging_writer.py:48] [35400] global_step=35400, grad_norm=4.0666680335998535, loss=1.7480642795562744
I0201 12:33:51.935168 139907754342144 logging_writer.py:48] [35500] global_step=35500, grad_norm=4.357566833496094, loss=2.0510215759277344
I0201 12:34:25.616804 139907762734848 logging_writer.py:48] [35600] global_step=35600, grad_norm=3.9665541648864746, loss=1.9942502975463867
I0201 12:34:59.299933 139907754342144 logging_writer.py:48] [35700] global_step=35700, grad_norm=3.4923455715179443, loss=2.0361342430114746
I0201 12:35:32.983186 139907762734848 logging_writer.py:48] [35800] global_step=35800, grad_norm=4.895355224609375, loss=1.935760498046875
I0201 12:36:06.665223 139907754342144 logging_writer.py:48] [35900] global_step=35900, grad_norm=3.4539952278137207, loss=1.9329317808151245
I0201 12:36:40.334896 139907762734848 logging_writer.py:48] [36000] global_step=36000, grad_norm=3.6746103763580322, loss=1.9844818115234375
I0201 12:37:14.153668 139907754342144 logging_writer.py:48] [36100] global_step=36100, grad_norm=3.2174620628356934, loss=2.0235438346862793
I0201 12:37:47.850275 139907762734848 logging_writer.py:48] [36200] global_step=36200, grad_norm=3.6912758350372314, loss=1.9641486406326294
I0201 12:38:08.222095 140070692116288 spec.py:321] Evaluating on the training split.
I0201 12:38:14.492089 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 12:38:23.203742 140070692116288 spec.py:349] Evaluating on the test split.
I0201 12:38:25.818194 140070692116288 submission_runner.py:408] Time since start: 12722.27s, 	Step: 36262, 	{'train/accuracy': 0.6530413031578064, 'train/loss': 1.3820711374282837, 'validation/accuracy': 0.5939399600028992, 'validation/loss': 1.7025057077407837, 'validation/num_examples': 50000, 'test/accuracy': 0.468500018119812, 'test/loss': 2.4308671951293945, 'test/num_examples': 10000, 'score': 12275.55195569992, 'total_duration': 12722.270855426788, 'accumulated_submission_time': 12275.55195569992, 'accumulated_eval_time': 443.6337375640869, 'accumulated_logging_time': 1.826355218887329}
I0201 12:38:25.841576 139907745949440 logging_writer.py:48] [36262] accumulated_eval_time=443.633738, accumulated_logging_time=1.826355, accumulated_submission_time=12275.551956, global_step=36262, preemption_count=0, score=12275.551956, test/accuracy=0.468500, test/loss=2.430867, test/num_examples=10000, total_duration=12722.270855, train/accuracy=0.653041, train/loss=1.382071, validation/accuracy=0.593940, validation/loss=1.702506, validation/num_examples=50000
I0201 12:38:38.981519 139908719027968 logging_writer.py:48] [36300] global_step=36300, grad_norm=4.251750469207764, loss=2.0072696208953857
I0201 12:39:12.666526 139907745949440 logging_writer.py:48] [36400] global_step=36400, grad_norm=3.9002597332000732, loss=2.016296148300171
I0201 12:39:46.317846 139908719027968 logging_writer.py:48] [36500] global_step=36500, grad_norm=3.5038068294525146, loss=1.8300933837890625
I0201 12:40:20.014198 139907745949440 logging_writer.py:48] [36600] global_step=36600, grad_norm=3.700082540512085, loss=1.9642640352249146
I0201 12:40:53.692326 139908719027968 logging_writer.py:48] [36700] global_step=36700, grad_norm=4.309473514556885, loss=1.8841638565063477
I0201 12:41:27.412903 139907745949440 logging_writer.py:48] [36800] global_step=36800, grad_norm=3.4130280017852783, loss=1.9259231090545654
I0201 12:42:01.072883 139908719027968 logging_writer.py:48] [36900] global_step=36900, grad_norm=4.235228538513184, loss=2.1193313598632812
I0201 12:42:34.781229 139907745949440 logging_writer.py:48] [37000] global_step=37000, grad_norm=3.914015054702759, loss=1.923194408416748
I0201 12:43:08.516184 139908719027968 logging_writer.py:48] [37100] global_step=37100, grad_norm=3.8298027515411377, loss=1.905846357345581
I0201 12:43:42.275551 139907745949440 logging_writer.py:48] [37200] global_step=37200, grad_norm=3.751603364944458, loss=2.021850824356079
I0201 12:44:15.938857 139908719027968 logging_writer.py:48] [37300] global_step=37300, grad_norm=3.527787446975708, loss=1.8554069995880127
I0201 12:44:49.639527 139907745949440 logging_writer.py:48] [37400] global_step=37400, grad_norm=4.4837260246276855, loss=1.871867299079895
I0201 12:45:23.292833 139908719027968 logging_writer.py:48] [37500] global_step=37500, grad_norm=4.984503269195557, loss=1.9500352144241333
I0201 12:45:57.008261 139907745949440 logging_writer.py:48] [37600] global_step=37600, grad_norm=3.8840301036834717, loss=2.0564184188842773
I0201 12:46:30.715891 139908719027968 logging_writer.py:48] [37700] global_step=37700, grad_norm=3.6275644302368164, loss=1.9083189964294434
I0201 12:46:56.134796 140070692116288 spec.py:321] Evaluating on the training split.
I0201 12:47:02.543489 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 12:47:11.247848 140070692116288 spec.py:349] Evaluating on the test split.
I0201 12:47:13.871527 140070692116288 submission_runner.py:408] Time since start: 13250.32s, 	Step: 37777, 	{'train/accuracy': 0.6506098508834839, 'train/loss': 1.3960210084915161, 'validation/accuracy': 0.5997999906539917, 'validation/loss': 1.6665048599243164, 'validation/num_examples': 50000, 'test/accuracy': 0.4780000150203705, 'test/loss': 2.3967301845550537, 'test/num_examples': 10000, 'score': 12785.783144235611, 'total_duration': 13250.32419705391, 'accumulated_submission_time': 12785.783144235611, 'accumulated_eval_time': 461.3704402446747, 'accumulated_logging_time': 1.8594939708709717}
I0201 12:47:13.896674 139907762734848 logging_writer.py:48] [37777] accumulated_eval_time=461.370440, accumulated_logging_time=1.859494, accumulated_submission_time=12785.783144, global_step=37777, preemption_count=0, score=12785.783144, test/accuracy=0.478000, test/loss=2.396730, test/num_examples=10000, total_duration=13250.324197, train/accuracy=0.650610, train/loss=1.396021, validation/accuracy=0.599800, validation/loss=1.666505, validation/num_examples=50000
I0201 12:47:21.992965 139908425447168 logging_writer.py:48] [37800] global_step=37800, grad_norm=3.9839699268341064, loss=1.9589471817016602
I0201 12:47:55.701342 139907762734848 logging_writer.py:48] [37900] global_step=37900, grad_norm=3.6851468086242676, loss=2.0293779373168945
I0201 12:48:29.388532 139908425447168 logging_writer.py:48] [38000] global_step=38000, grad_norm=4.417576313018799, loss=1.9554929733276367
I0201 12:49:03.061809 139907762734848 logging_writer.py:48] [38100] global_step=38100, grad_norm=3.850634813308716, loss=1.9563261270523071
I0201 12:49:36.837389 139908425447168 logging_writer.py:48] [38200] global_step=38200, grad_norm=4.343625545501709, loss=1.9230015277862549
I0201 12:50:10.564786 139907762734848 logging_writer.py:48] [38300] global_step=38300, grad_norm=4.008450031280518, loss=1.9169024229049683
I0201 12:50:44.246476 139908425447168 logging_writer.py:48] [38400] global_step=38400, grad_norm=3.7873144149780273, loss=1.9825363159179688
I0201 12:51:17.925026 139907762734848 logging_writer.py:48] [38500] global_step=38500, grad_norm=3.486712694168091, loss=1.8703469038009644
I0201 12:51:51.615958 139908425447168 logging_writer.py:48] [38600] global_step=38600, grad_norm=3.2138614654541016, loss=1.9557985067367554
I0201 12:52:25.374734 139907762734848 logging_writer.py:48] [38700] global_step=38700, grad_norm=3.956429958343506, loss=2.0646846294403076
I0201 12:52:59.066934 139908425447168 logging_writer.py:48] [38800] global_step=38800, grad_norm=3.780130386352539, loss=1.9499411582946777
I0201 12:53:32.753096 139907762734848 logging_writer.py:48] [38900] global_step=38900, grad_norm=3.2844080924987793, loss=1.920411229133606
I0201 12:54:06.417887 139908425447168 logging_writer.py:48] [39000] global_step=39000, grad_norm=4.20451021194458, loss=1.8975180387496948
I0201 12:54:40.117311 139907762734848 logging_writer.py:48] [39100] global_step=39100, grad_norm=3.6433427333831787, loss=1.9694215059280396
I0201 12:55:13.766540 139908425447168 logging_writer.py:48] [39200] global_step=39200, grad_norm=4.133368015289307, loss=1.954293966293335
I0201 12:55:44.097745 140070692116288 spec.py:321] Evaluating on the training split.
I0201 12:55:50.343774 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 12:55:58.978942 140070692116288 spec.py:349] Evaluating on the test split.
I0201 12:56:01.570868 140070692116288 submission_runner.py:408] Time since start: 13778.02s, 	Step: 39291, 	{'train/accuracy': 0.6446109414100647, 'train/loss': 1.427964210510254, 'validation/accuracy': 0.5927000045776367, 'validation/loss': 1.6880282163619995, 'validation/num_examples': 50000, 'test/accuracy': 0.46950003504753113, 'test/loss': 2.4148316383361816, 'test/num_examples': 10000, 'score': 13295.923071146011, 'total_duration': 13778.023537874222, 'accumulated_submission_time': 13295.923071146011, 'accumulated_eval_time': 478.8435335159302, 'accumulated_logging_time': 1.8939027786254883}
I0201 12:56:01.594648 139908719027968 logging_writer.py:48] [39291] accumulated_eval_time=478.843534, accumulated_logging_time=1.893903, accumulated_submission_time=13295.923071, global_step=39291, preemption_count=0, score=13295.923071, test/accuracy=0.469500, test/loss=2.414832, test/num_examples=10000, total_duration=13778.023538, train/accuracy=0.644611, train/loss=1.427964, validation/accuracy=0.592700, validation/loss=1.688028, validation/num_examples=50000
I0201 12:56:04.958317 139908727420672 logging_writer.py:48] [39300] global_step=39300, grad_norm=4.336007118225098, loss=1.8864362239837646
I0201 12:56:38.597214 139908719027968 logging_writer.py:48] [39400] global_step=39400, grad_norm=3.7056009769439697, loss=1.8015024662017822
I0201 12:57:12.348108 139908727420672 logging_writer.py:48] [39500] global_step=39500, grad_norm=3.9179980754852295, loss=2.0367469787597656
I0201 12:57:46.028471 139908719027968 logging_writer.py:48] [39600] global_step=39600, grad_norm=4.2629289627075195, loss=1.9535775184631348
I0201 12:58:19.740542 139908727420672 logging_writer.py:48] [39700] global_step=39700, grad_norm=3.838921308517456, loss=1.9820064306259155
I0201 12:58:53.407068 139908719027968 logging_writer.py:48] [39800] global_step=39800, grad_norm=3.566784143447876, loss=1.9916045665740967
I0201 12:59:27.078894 139908727420672 logging_writer.py:48] [39900] global_step=39900, grad_norm=4.110283374786377, loss=1.846431851387024
I0201 13:00:00.738909 139908719027968 logging_writer.py:48] [40000] global_step=40000, grad_norm=3.8715403079986572, loss=1.9115936756134033
I0201 13:00:34.437720 139908727420672 logging_writer.py:48] [40100] global_step=40100, grad_norm=3.4179556369781494, loss=1.9474135637283325
I0201 13:01:08.101756 139908719027968 logging_writer.py:48] [40200] global_step=40200, grad_norm=3.759169101715088, loss=1.8796783685684204
I0201 13:01:41.803351 139908727420672 logging_writer.py:48] [40300] global_step=40300, grad_norm=3.635763168334961, loss=1.9968981742858887
I0201 13:02:15.599929 139908719027968 logging_writer.py:48] [40400] global_step=40400, grad_norm=4.307305335998535, loss=1.8791749477386475
I0201 13:02:49.308946 139908727420672 logging_writer.py:48] [40500] global_step=40500, grad_norm=3.9566919803619385, loss=2.039020299911499
I0201 13:03:22.959931 139908719027968 logging_writer.py:48] [40600] global_step=40600, grad_norm=4.1065568923950195, loss=1.8303732872009277
I0201 13:03:56.666397 139908727420672 logging_writer.py:48] [40700] global_step=40700, grad_norm=3.284418821334839, loss=1.9089263677597046
I0201 13:04:30.285891 139908719027968 logging_writer.py:48] [40800] global_step=40800, grad_norm=4.045273303985596, loss=1.9033154249191284
I0201 13:04:31.786252 140070692116288 spec.py:321] Evaluating on the training split.
I0201 13:04:38.259658 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 13:04:47.161301 140070692116288 spec.py:349] Evaluating on the test split.
I0201 13:04:49.625738 140070692116288 submission_runner.py:408] Time since start: 14306.08s, 	Step: 40806, 	{'train/accuracy': 0.6479392647743225, 'train/loss': 1.4033961296081543, 'validation/accuracy': 0.6071599721908569, 'validation/loss': 1.6209561824798584, 'validation/num_examples': 50000, 'test/accuracy': 0.48750001192092896, 'test/loss': 2.322809934616089, 'test/num_examples': 10000, 'score': 13806.05341053009, 'total_duration': 14306.078384160995, 'accumulated_submission_time': 13806.05341053009, 'accumulated_eval_time': 496.6829800605774, 'accumulated_logging_time': 1.9265995025634766}
I0201 13:04:49.649296 139907737556736 logging_writer.py:48] [40806] accumulated_eval_time=496.682980, accumulated_logging_time=1.926600, accumulated_submission_time=13806.053411, global_step=40806, preemption_count=0, score=13806.053411, test/accuracy=0.487500, test/loss=2.322810, test/num_examples=10000, total_duration=14306.078384, train/accuracy=0.647939, train/loss=1.403396, validation/accuracy=0.607160, validation/loss=1.620956, validation/num_examples=50000
I0201 13:05:21.554406 139907745949440 logging_writer.py:48] [40900] global_step=40900, grad_norm=3.8237810134887695, loss=2.0789389610290527
I0201 13:05:55.211072 139907737556736 logging_writer.py:48] [41000] global_step=41000, grad_norm=4.286013126373291, loss=1.8673734664916992
I0201 13:06:28.882117 139907745949440 logging_writer.py:48] [41100] global_step=41100, grad_norm=3.0443227291107178, loss=1.8451417684555054
I0201 13:07:02.562603 139907737556736 logging_writer.py:48] [41200] global_step=41200, grad_norm=3.938558578491211, loss=1.975325107574463
I0201 13:07:36.274840 139907745949440 logging_writer.py:48] [41300] global_step=41300, grad_norm=3.3543148040771484, loss=1.9435982704162598
I0201 13:08:09.953021 139907737556736 logging_writer.py:48] [41400] global_step=41400, grad_norm=3.3509490489959717, loss=1.8321545124053955
I0201 13:08:43.731923 139907745949440 logging_writer.py:48] [41500] global_step=41500, grad_norm=3.652773380279541, loss=1.8991243839263916
I0201 13:09:17.456783 139907737556736 logging_writer.py:48] [41600] global_step=41600, grad_norm=3.521821975708008, loss=1.9246563911437988
I0201 13:09:51.114715 139907745949440 logging_writer.py:48] [41700] global_step=41700, grad_norm=3.9910151958465576, loss=1.9736347198486328
I0201 13:10:24.794431 139907737556736 logging_writer.py:48] [41800] global_step=41800, grad_norm=4.527648448944092, loss=1.93349027633667
I0201 13:10:58.464884 139907745949440 logging_writer.py:48] [41900] global_step=41900, grad_norm=4.071700572967529, loss=1.8197474479675293
I0201 13:11:32.140234 139907737556736 logging_writer.py:48] [42000] global_step=42000, grad_norm=3.8329646587371826, loss=1.9887118339538574
I0201 13:12:05.817187 139907745949440 logging_writer.py:48] [42100] global_step=42100, grad_norm=3.6848690509796143, loss=2.007718563079834
I0201 13:12:39.515116 139907737556736 logging_writer.py:48] [42200] global_step=42200, grad_norm=4.353548049926758, loss=1.9654228687286377
I0201 13:13:13.190332 139907745949440 logging_writer.py:48] [42300] global_step=42300, grad_norm=3.607365131378174, loss=1.751373529434204
I0201 13:13:19.739314 140070692116288 spec.py:321] Evaluating on the training split.
I0201 13:13:25.952456 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 13:13:34.612535 140070692116288 spec.py:349] Evaluating on the test split.
I0201 13:13:37.194420 140070692116288 submission_runner.py:408] Time since start: 14833.65s, 	Step: 42321, 	{'train/accuracy': 0.6422193646430969, 'train/loss': 1.4444411993026733, 'validation/accuracy': 0.5982199907302856, 'validation/loss': 1.6778781414031982, 'validation/num_examples': 50000, 'test/accuracy': 0.4732000231742859, 'test/loss': 2.43147611618042, 'test/num_examples': 10000, 'score': 14316.080854415894, 'total_duration': 14833.64709019661, 'accumulated_submission_time': 14316.080854415894, 'accumulated_eval_time': 514.1380536556244, 'accumulated_logging_time': 1.9594926834106445}
I0201 13:13:37.218559 139907737556736 logging_writer.py:48] [42321] accumulated_eval_time=514.138054, accumulated_logging_time=1.959493, accumulated_submission_time=14316.080854, global_step=42321, preemption_count=0, score=14316.080854, test/accuracy=0.473200, test/loss=2.431476, test/num_examples=10000, total_duration=14833.647090, train/accuracy=0.642219, train/loss=1.444441, validation/accuracy=0.598220, validation/loss=1.677878, validation/num_examples=50000
I0201 13:14:04.147655 139908710635264 logging_writer.py:48] [42400] global_step=42400, grad_norm=4.155340671539307, loss=1.8542380332946777
I0201 13:14:37.786278 139907737556736 logging_writer.py:48] [42500] global_step=42500, grad_norm=3.6925899982452393, loss=1.8986870050430298
I0201 13:15:11.536781 139908710635264 logging_writer.py:48] [42600] global_step=42600, grad_norm=3.527167797088623, loss=1.8428168296813965
I0201 13:15:45.261502 139907737556736 logging_writer.py:48] [42700] global_step=42700, grad_norm=3.838346481323242, loss=1.786410927772522
I0201 13:16:18.963453 139908710635264 logging_writer.py:48] [42800] global_step=42800, grad_norm=3.9696907997131348, loss=1.943453311920166
I0201 13:16:52.635442 139907737556736 logging_writer.py:48] [42900] global_step=42900, grad_norm=4.375095367431641, loss=1.9565712213516235
I0201 13:17:26.335824 139908710635264 logging_writer.py:48] [43000] global_step=43000, grad_norm=3.455519676208496, loss=2.0056467056274414
I0201 13:17:59.977881 139907737556736 logging_writer.py:48] [43100] global_step=43100, grad_norm=3.7983222007751465, loss=1.9207561016082764
I0201 13:18:33.687533 139908710635264 logging_writer.py:48] [43200] global_step=43200, grad_norm=3.470741033554077, loss=1.817110538482666
I0201 13:19:07.350746 139907737556736 logging_writer.py:48] [43300] global_step=43300, grad_norm=3.2692699432373047, loss=1.9238061904907227
I0201 13:19:41.036226 139908710635264 logging_writer.py:48] [43400] global_step=43400, grad_norm=3.596890687942505, loss=1.8669841289520264
I0201 13:20:14.699549 139907737556736 logging_writer.py:48] [43500] global_step=43500, grad_norm=3.8879153728485107, loss=1.9199464321136475
I0201 13:20:48.408212 139908710635264 logging_writer.py:48] [43600] global_step=43600, grad_norm=3.164534330368042, loss=1.8099498748779297
I0201 13:21:22.261936 139907737556736 logging_writer.py:48] [43700] global_step=43700, grad_norm=3.3433139324188232, loss=1.8055720329284668
I0201 13:21:55.970641 139908710635264 logging_writer.py:48] [43800] global_step=43800, grad_norm=3.4916181564331055, loss=1.915327548980713
I0201 13:22:07.226435 140070692116288 spec.py:321] Evaluating on the training split.
I0201 13:22:13.464303 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 13:22:22.276795 140070692116288 spec.py:349] Evaluating on the test split.
I0201 13:22:24.855909 140070692116288 submission_runner.py:408] Time since start: 15361.31s, 	Step: 43835, 	{'train/accuracy': 0.6764788031578064, 'train/loss': 1.2786322832107544, 'validation/accuracy': 0.5990399718284607, 'validation/loss': 1.6587549448013306, 'validation/num_examples': 50000, 'test/accuracy': 0.47610002756118774, 'test/loss': 2.4039862155914307, 'test/num_examples': 10000, 'score': 14826.026376008987, 'total_duration': 15361.308577775955, 'accumulated_submission_time': 14826.026376008987, 'accumulated_eval_time': 531.7674918174744, 'accumulated_logging_time': 1.9935684204101562}
I0201 13:22:24.880939 139907737556736 logging_writer.py:48] [43835] accumulated_eval_time=531.767492, accumulated_logging_time=1.993568, accumulated_submission_time=14826.026376, global_step=43835, preemption_count=0, score=14826.026376, test/accuracy=0.476100, test/loss=2.403986, test/num_examples=10000, total_duration=15361.308578, train/accuracy=0.676479, train/loss=1.278632, validation/accuracy=0.599040, validation/loss=1.658755, validation/num_examples=50000
I0201 13:22:47.101307 139907754342144 logging_writer.py:48] [43900] global_step=43900, grad_norm=3.7283527851104736, loss=1.908969759941101
I0201 13:23:20.776278 139907737556736 logging_writer.py:48] [44000] global_step=44000, grad_norm=4.62645149230957, loss=2.0024733543395996
I0201 13:23:54.501947 139907754342144 logging_writer.py:48] [44100] global_step=44100, grad_norm=3.5564186573028564, loss=1.8542550802230835
I0201 13:24:28.171627 139907737556736 logging_writer.py:48] [44200] global_step=44200, grad_norm=4.663781642913818, loss=1.8834253549575806
I0201 13:25:01.846139 139907754342144 logging_writer.py:48] [44300] global_step=44300, grad_norm=3.750133514404297, loss=1.9311505556106567
I0201 13:25:35.501139 139907737556736 logging_writer.py:48] [44400] global_step=44400, grad_norm=3.786207914352417, loss=1.890802025794983
I0201 13:26:09.181647 139907754342144 logging_writer.py:48] [44500] global_step=44500, grad_norm=3.938817262649536, loss=1.9861516952514648
I0201 13:26:42.858119 139907737556736 logging_writer.py:48] [44600] global_step=44600, grad_norm=4.341872215270996, loss=2.0094685554504395
I0201 13:27:16.535635 139907754342144 logging_writer.py:48] [44700] global_step=44700, grad_norm=4.342687606811523, loss=1.9881024360656738
I0201 13:27:50.339925 139907737556736 logging_writer.py:48] [44800] global_step=44800, grad_norm=3.60028076171875, loss=1.7757296562194824
I0201 13:28:24.017628 139907754342144 logging_writer.py:48] [44900] global_step=44900, grad_norm=3.1832714080810547, loss=1.8369436264038086
I0201 13:28:57.689447 139907737556736 logging_writer.py:48] [45000] global_step=45000, grad_norm=4.242706298828125, loss=2.0116939544677734
I0201 13:29:31.391307 139907754342144 logging_writer.py:48] [45100] global_step=45100, grad_norm=5.411911964416504, loss=1.8410210609436035
I0201 13:30:05.061392 139907737556736 logging_writer.py:48] [45200] global_step=45200, grad_norm=3.9257993698120117, loss=1.8944697380065918
I0201 13:30:38.739690 139907754342144 logging_writer.py:48] [45300] global_step=45300, grad_norm=3.4685981273651123, loss=1.8948757648468018
I0201 13:30:55.052638 140070692116288 spec.py:321] Evaluating on the training split.
I0201 13:31:02.103682 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 13:31:10.592345 140070692116288 spec.py:349] Evaluating on the test split.
I0201 13:31:13.198372 140070692116288 submission_runner.py:408] Time since start: 15889.65s, 	Step: 45350, 	{'train/accuracy': 0.6533800959587097, 'train/loss': 1.3688466548919678, 'validation/accuracy': 0.602180004119873, 'validation/loss': 1.6588586568832397, 'validation/num_examples': 50000, 'test/accuracy': 0.47930002212524414, 'test/loss': 2.3855392932891846, 'test/num_examples': 10000, 'score': 15336.135235071182, 'total_duration': 15889.651034116745, 'accumulated_submission_time': 15336.135235071182, 'accumulated_eval_time': 549.9132053852081, 'accumulated_logging_time': 2.0285537242889404}
I0201 13:31:13.223879 139907729164032 logging_writer.py:48] [45350] accumulated_eval_time=549.913205, accumulated_logging_time=2.028554, accumulated_submission_time=15336.135235, global_step=45350, preemption_count=0, score=15336.135235, test/accuracy=0.479300, test/loss=2.385539, test/num_examples=10000, total_duration=15889.651034, train/accuracy=0.653380, train/loss=1.368847, validation/accuracy=0.602180, validation/loss=1.658859, validation/num_examples=50000
I0201 13:31:30.391488 139907745949440 logging_writer.py:48] [45400] global_step=45400, grad_norm=4.053126811981201, loss=1.954496145248413
I0201 13:32:03.999731 139907729164032 logging_writer.py:48] [45500] global_step=45500, grad_norm=3.343289375305176, loss=1.903956413269043
I0201 13:32:37.714311 139907745949440 logging_writer.py:48] [45600] global_step=45600, grad_norm=3.5509703159332275, loss=1.9210268259048462
I0201 13:33:11.406198 139907729164032 logging_writer.py:48] [45700] global_step=45700, grad_norm=3.208662509918213, loss=1.9432919025421143
I0201 13:33:45.068190 139907745949440 logging_writer.py:48] [45800] global_step=45800, grad_norm=4.051259517669678, loss=1.9332505464553833
I0201 13:34:18.845063 139907729164032 logging_writer.py:48] [45900] global_step=45900, grad_norm=3.805617332458496, loss=1.797200322151184
I0201 13:34:52.522034 139907745949440 logging_writer.py:48] [46000] global_step=46000, grad_norm=3.881247043609619, loss=1.8902885913848877
I0201 13:35:26.204045 139907729164032 logging_writer.py:48] [46100] global_step=46100, grad_norm=3.9305005073547363, loss=1.9682307243347168
I0201 13:35:59.863780 139907745949440 logging_writer.py:48] [46200] global_step=46200, grad_norm=4.094945430755615, loss=1.8751314878463745
I0201 13:36:33.563057 139907729164032 logging_writer.py:48] [46300] global_step=46300, grad_norm=4.074359893798828, loss=1.7643520832061768
I0201 13:37:07.228459 139907745949440 logging_writer.py:48] [46400] global_step=46400, grad_norm=3.764657497406006, loss=2.05305814743042
I0201 13:37:40.901603 139907729164032 logging_writer.py:48] [46500] global_step=46500, grad_norm=3.3986656665802, loss=1.8248976469039917
I0201 13:38:14.556395 139907745949440 logging_writer.py:48] [46600] global_step=46600, grad_norm=4.918180465698242, loss=1.960641622543335
I0201 13:38:48.240090 139907729164032 logging_writer.py:48] [46700] global_step=46700, grad_norm=3.358403444290161, loss=1.842458724975586
I0201 13:39:21.903537 139907745949440 logging_writer.py:48] [46800] global_step=46800, grad_norm=3.5851120948791504, loss=2.003162384033203
I0201 13:39:43.286400 140070692116288 spec.py:321] Evaluating on the training split.
I0201 13:39:49.585969 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 13:39:58.165567 140070692116288 spec.py:349] Evaluating on the test split.
I0201 13:40:00.772108 140070692116288 submission_runner.py:408] Time since start: 16417.22s, 	Step: 46865, 	{'train/accuracy': 0.6542569994926453, 'train/loss': 1.370741367340088, 'validation/accuracy': 0.6079999804496765, 'validation/loss': 1.6286641359329224, 'validation/num_examples': 50000, 'test/accuracy': 0.48090001940727234, 'test/loss': 2.3713834285736084, 'test/num_examples': 10000, 'score': 15846.135519742966, 'total_duration': 16417.224779605865, 'accumulated_submission_time': 15846.135519742966, 'accumulated_eval_time': 567.3988988399506, 'accumulated_logging_time': 2.063908100128174}
I0201 13:40:00.800683 139908719027968 logging_writer.py:48] [46865] accumulated_eval_time=567.398899, accumulated_logging_time=2.063908, accumulated_submission_time=15846.135520, global_step=46865, preemption_count=0, score=15846.135520, test/accuracy=0.480900, test/loss=2.371383, test/num_examples=10000, total_duration=16417.224780, train/accuracy=0.654257, train/loss=1.370741, validation/accuracy=0.608000, validation/loss=1.628664, validation/num_examples=50000
I0201 13:40:13.115216 139908727420672 logging_writer.py:48] [46900] global_step=46900, grad_norm=3.9318668842315674, loss=1.9350546598434448
I0201 13:40:46.826042 139908719027968 logging_writer.py:48] [47000] global_step=47000, grad_norm=3.719724416732788, loss=1.8205022811889648
I0201 13:41:20.506810 139908727420672 logging_writer.py:48] [47100] global_step=47100, grad_norm=3.993945598602295, loss=1.82000732421875
I0201 13:41:54.212960 139908719027968 logging_writer.py:48] [47200] global_step=47200, grad_norm=3.3116464614868164, loss=1.8300899267196655
I0201 13:42:27.888411 139908727420672 logging_writer.py:48] [47300] global_step=47300, grad_norm=3.5684516429901123, loss=1.9354443550109863
I0201 13:43:01.566751 139908719027968 logging_writer.py:48] [47400] global_step=47400, grad_norm=3.4521901607513428, loss=1.8039259910583496
I0201 13:43:35.245858 139908727420672 logging_writer.py:48] [47500] global_step=47500, grad_norm=3.14400577545166, loss=1.7968682050704956
I0201 13:44:08.933548 139908719027968 logging_writer.py:48] [47600] global_step=47600, grad_norm=3.6067593097686768, loss=1.987939476966858
I0201 13:44:42.597377 139908727420672 logging_writer.py:48] [47700] global_step=47700, grad_norm=4.176506519317627, loss=1.8029136657714844
I0201 13:45:16.292277 139908719027968 logging_writer.py:48] [47800] global_step=47800, grad_norm=4.150970935821533, loss=1.885752558708191
I0201 13:45:49.949333 139908727420672 logging_writer.py:48] [47900] global_step=47900, grad_norm=4.661057949066162, loss=1.8251476287841797
I0201 13:46:23.652443 139908719027968 logging_writer.py:48] [48000] global_step=48000, grad_norm=3.9434711933135986, loss=1.882009506225586
I0201 13:46:57.469349 139908727420672 logging_writer.py:48] [48100] global_step=48100, grad_norm=4.028844356536865, loss=1.845323920249939
I0201 13:47:31.166532 139908719027968 logging_writer.py:48] [48200] global_step=48200, grad_norm=4.386270046234131, loss=2.025376796722412
I0201 13:48:04.833717 139908727420672 logging_writer.py:48] [48300] global_step=48300, grad_norm=3.758756160736084, loss=1.9725863933563232
I0201 13:48:30.916002 140070692116288 spec.py:321] Evaluating on the training split.
I0201 13:48:37.165925 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 13:48:45.784467 140070692116288 spec.py:349] Evaluating on the test split.
I0201 13:48:48.430874 140070692116288 submission_runner.py:408] Time since start: 16944.88s, 	Step: 48379, 	{'train/accuracy': 0.6592593789100647, 'train/loss': 1.3680092096328735, 'validation/accuracy': 0.6098399758338928, 'validation/loss': 1.604357361793518, 'validation/num_examples': 50000, 'test/accuracy': 0.4847000241279602, 'test/loss': 2.338550329208374, 'test/num_examples': 10000, 'score': 16356.188809633255, 'total_duration': 16944.88347172737, 'accumulated_submission_time': 16356.188809633255, 'accumulated_eval_time': 584.9136664867401, 'accumulated_logging_time': 2.1026611328125}
I0201 13:48:48.460021 139907745949440 logging_writer.py:48] [48379] accumulated_eval_time=584.913666, accumulated_logging_time=2.102661, accumulated_submission_time=16356.188810, global_step=48379, preemption_count=0, score=16356.188810, test/accuracy=0.484700, test/loss=2.338550, test/num_examples=10000, total_duration=16944.883472, train/accuracy=0.659259, train/loss=1.368009, validation/accuracy=0.609840, validation/loss=1.604357, validation/num_examples=50000
I0201 13:48:55.868301 139907754342144 logging_writer.py:48] [48400] global_step=48400, grad_norm=4.569739818572998, loss=1.9367384910583496
I0201 13:49:29.602535 139907745949440 logging_writer.py:48] [48500] global_step=48500, grad_norm=3.9984211921691895, loss=1.9236595630645752
I0201 13:50:03.287604 139907754342144 logging_writer.py:48] [48600] global_step=48600, grad_norm=3.683717727661133, loss=1.972018837928772
I0201 13:50:36.955785 139907745949440 logging_writer.py:48] [48700] global_step=48700, grad_norm=3.6302857398986816, loss=1.8495055437088013
I0201 13:51:10.650308 139907754342144 logging_writer.py:48] [48800] global_step=48800, grad_norm=3.9350123405456543, loss=1.9782687425613403
I0201 13:51:44.310120 139907745949440 logging_writer.py:48] [48900] global_step=48900, grad_norm=3.31231951713562, loss=1.8086146116256714
I0201 13:52:17.984293 139907754342144 logging_writer.py:48] [49000] global_step=49000, grad_norm=3.673617362976074, loss=1.807744026184082
I0201 13:52:51.721544 139907745949440 logging_writer.py:48] [49100] global_step=49100, grad_norm=3.8740627765655518, loss=1.8689074516296387
I0201 13:53:25.430341 139907754342144 logging_writer.py:48] [49200] global_step=49200, grad_norm=3.778621196746826, loss=1.8409662246704102
I0201 13:53:59.124442 139907745949440 logging_writer.py:48] [49300] global_step=49300, grad_norm=3.7744569778442383, loss=1.8511314392089844
I0201 13:54:32.806101 139907754342144 logging_writer.py:48] [49400] global_step=49400, grad_norm=3.407884359359741, loss=1.9082216024398804
I0201 13:55:06.468947 139907745949440 logging_writer.py:48] [49500] global_step=49500, grad_norm=3.5747780799865723, loss=1.8192408084869385
I0201 13:55:40.134790 139907754342144 logging_writer.py:48] [49600] global_step=49600, grad_norm=3.4173247814178467, loss=1.8164607286453247
I0201 13:56:13.820234 139907745949440 logging_writer.py:48] [49700] global_step=49700, grad_norm=3.64203143119812, loss=1.8493108749389648
I0201 13:56:47.528733 139907754342144 logging_writer.py:48] [49800] global_step=49800, grad_norm=3.713165044784546, loss=1.7477164268493652
I0201 13:57:18.647749 140070692116288 spec.py:321] Evaluating on the training split.
I0201 13:57:24.865337 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 13:57:33.591589 140070692116288 spec.py:349] Evaluating on the test split.
I0201 13:57:36.070398 140070692116288 submission_runner.py:408] Time since start: 17472.52s, 	Step: 49894, 	{'train/accuracy': 0.6473612785339355, 'train/loss': 1.4344884157180786, 'validation/accuracy': 0.5996400117874146, 'validation/loss': 1.6758544445037842, 'validation/num_examples': 50000, 'test/accuracy': 0.4773000180721283, 'test/loss': 2.3852531909942627, 'test/num_examples': 10000, 'score': 16866.31578350067, 'total_duration': 17472.523061990738, 'accumulated_submission_time': 16866.31578350067, 'accumulated_eval_time': 602.3362815380096, 'accumulated_logging_time': 2.1408157348632812}
I0201 13:57:36.096294 139908719027968 logging_writer.py:48] [49894] accumulated_eval_time=602.336282, accumulated_logging_time=2.140816, accumulated_submission_time=16866.315784, global_step=49894, preemption_count=0, score=16866.315784, test/accuracy=0.477300, test/loss=2.385253, test/num_examples=10000, total_duration=17472.523062, train/accuracy=0.647361, train/loss=1.434488, validation/accuracy=0.599640, validation/loss=1.675854, validation/num_examples=50000
I0201 13:57:38.452429 139908727420672 logging_writer.py:48] [49900] global_step=49900, grad_norm=3.513007164001465, loss=1.8481268882751465
I0201 13:58:12.134683 139908719027968 logging_writer.py:48] [50000] global_step=50000, grad_norm=3.3140392303466797, loss=1.8270479440689087
I0201 13:58:45.852869 139908727420672 logging_writer.py:48] [50100] global_step=50100, grad_norm=3.680079460144043, loss=1.848054051399231
I0201 13:59:19.740492 139908719027968 logging_writer.py:48] [50200] global_step=50200, grad_norm=3.2395198345184326, loss=1.8933748006820679
I0201 13:59:53.385246 139908727420672 logging_writer.py:48] [50300] global_step=50300, grad_norm=3.6032395362854004, loss=1.8332715034484863
I0201 14:00:27.094173 139908719027968 logging_writer.py:48] [50400] global_step=50400, grad_norm=4.072157859802246, loss=1.9537553787231445
I0201 14:01:00.802173 139908727420672 logging_writer.py:48] [50500] global_step=50500, grad_norm=3.4879350662231445, loss=1.8172882795333862
I0201 14:01:34.462929 139908719027968 logging_writer.py:48] [50600] global_step=50600, grad_norm=4.348412036895752, loss=1.911932110786438
I0201 14:02:08.151902 139908727420672 logging_writer.py:48] [50700] global_step=50700, grad_norm=3.954050302505493, loss=1.8258432149887085
I0201 14:02:41.828220 139908719027968 logging_writer.py:48] [50800] global_step=50800, grad_norm=3.6798269748687744, loss=1.9362118244171143
I0201 14:03:15.533959 139908727420672 logging_writer.py:48] [50900] global_step=50900, grad_norm=3.1599366664886475, loss=1.9777313470840454
I0201 14:03:49.190097 139908719027968 logging_writer.py:48] [51000] global_step=51000, grad_norm=3.898969888687134, loss=1.9557068347930908
I0201 14:04:22.895448 139908727420672 logging_writer.py:48] [51100] global_step=51100, grad_norm=3.787670373916626, loss=1.9471333026885986
I0201 14:04:56.544182 139908719027968 logging_writer.py:48] [51200] global_step=51200, grad_norm=3.956155776977539, loss=1.8107554912567139
I0201 14:05:30.354962 139908727420672 logging_writer.py:48] [51300] global_step=51300, grad_norm=3.766374349594116, loss=1.907476782798767
I0201 14:06:04.058165 139908719027968 logging_writer.py:48] [51400] global_step=51400, grad_norm=4.227278709411621, loss=1.7977231740951538
I0201 14:06:06.229837 140070692116288 spec.py:321] Evaluating on the training split.
I0201 14:06:12.681879 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 14:06:21.466207 140070692116288 spec.py:349] Evaluating on the test split.
I0201 14:06:23.963559 140070692116288 submission_runner.py:408] Time since start: 18000.42s, 	Step: 51408, 	{'train/accuracy': 0.6523237824440002, 'train/loss': 1.3949483633041382, 'validation/accuracy': 0.6103799939155579, 'validation/loss': 1.6124364137649536, 'validation/num_examples': 50000, 'test/accuracy': 0.48900002241134644, 'test/loss': 2.2980523109436035, 'test/num_examples': 10000, 'score': 17376.387938022614, 'total_duration': 18000.41622543335, 'accumulated_submission_time': 17376.387938022614, 'accumulated_eval_time': 620.0699634552002, 'accumulated_logging_time': 2.1757097244262695}
I0201 14:06:23.991886 139907762734848 logging_writer.py:48] [51408] accumulated_eval_time=620.069963, accumulated_logging_time=2.175710, accumulated_submission_time=17376.387938, global_step=51408, preemption_count=0, score=17376.387938, test/accuracy=0.489000, test/loss=2.298052, test/num_examples=10000, total_duration=18000.416225, train/accuracy=0.652324, train/loss=1.394948, validation/accuracy=0.610380, validation/loss=1.612436, validation/num_examples=50000
I0201 14:06:55.320073 139908425447168 logging_writer.py:48] [51500] global_step=51500, grad_norm=3.7672064304351807, loss=1.9346338510513306
I0201 14:07:28.989994 139907762734848 logging_writer.py:48] [51600] global_step=51600, grad_norm=4.139328479766846, loss=1.865853190422058
I0201 14:08:02.674507 139908425447168 logging_writer.py:48] [51700] global_step=51700, grad_norm=3.8888676166534424, loss=1.9166936874389648
I0201 14:08:36.346765 139907762734848 logging_writer.py:48] [51800] global_step=51800, grad_norm=3.6840405464172363, loss=1.935071349143982
I0201 14:09:10.030377 139908425447168 logging_writer.py:48] [51900] global_step=51900, grad_norm=3.8969247341156006, loss=1.8842947483062744
I0201 14:09:43.706042 139907762734848 logging_writer.py:48] [52000] global_step=52000, grad_norm=4.9260969161987305, loss=1.8855605125427246
I0201 14:10:17.372590 139908425447168 logging_writer.py:48] [52100] global_step=52100, grad_norm=3.750476121902466, loss=1.8514922857284546
I0201 14:10:51.055235 139907762734848 logging_writer.py:48] [52200] global_step=52200, grad_norm=3.861833333969116, loss=1.8380860090255737
I0201 14:11:24.730896 139908425447168 logging_writer.py:48] [52300] global_step=52300, grad_norm=4.053106307983398, loss=1.9413360357284546
I0201 14:11:58.443754 139907762734848 logging_writer.py:48] [52400] global_step=52400, grad_norm=3.8037140369415283, loss=1.9266433715820312
I0201 14:12:32.161641 139908425447168 logging_writer.py:48] [52500] global_step=52500, grad_norm=3.863328456878662, loss=1.9476583003997803
I0201 14:13:05.821350 139907762734848 logging_writer.py:48] [52600] global_step=52600, grad_norm=4.129244804382324, loss=1.9757969379425049
I0201 14:13:39.491809 139908425447168 logging_writer.py:48] [52700] global_step=52700, grad_norm=3.586031913757324, loss=1.8764219284057617
I0201 14:14:13.162740 139907762734848 logging_writer.py:48] [52800] global_step=52800, grad_norm=4.907466888427734, loss=1.8934824466705322
I0201 14:14:46.840573 139908425447168 logging_writer.py:48] [52900] global_step=52900, grad_norm=4.7456536293029785, loss=1.8497143983840942
I0201 14:14:54.054050 140070692116288 spec.py:321] Evaluating on the training split.
I0201 14:15:00.310539 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 14:15:09.121550 140070692116288 spec.py:349] Evaluating on the test split.
I0201 14:15:11.626612 140070692116288 submission_runner.py:408] Time since start: 18528.08s, 	Step: 52923, 	{'train/accuracy': 0.6839325428009033, 'train/loss': 1.2379963397979736, 'validation/accuracy': 0.6139999628067017, 'validation/loss': 1.5940256118774414, 'validation/num_examples': 50000, 'test/accuracy': 0.49060001969337463, 'test/loss': 2.3267288208007812, 'test/num_examples': 10000, 'score': 17886.385818719864, 'total_duration': 18528.079265117645, 'accumulated_submission_time': 17886.385818719864, 'accumulated_eval_time': 637.6424875259399, 'accumulated_logging_time': 2.215566873550415}
I0201 14:15:11.655322 139907745949440 logging_writer.py:48] [52923] accumulated_eval_time=637.642488, accumulated_logging_time=2.215567, accumulated_submission_time=17886.385819, global_step=52923, preemption_count=0, score=17886.385819, test/accuracy=0.490600, test/loss=2.326729, test/num_examples=10000, total_duration=18528.079265, train/accuracy=0.683933, train/loss=1.237996, validation/accuracy=0.614000, validation/loss=1.594026, validation/num_examples=50000
I0201 14:15:37.890702 139907754342144 logging_writer.py:48] [53000] global_step=53000, grad_norm=4.027454853057861, loss=1.916977047920227
I0201 14:16:11.584013 139907745949440 logging_writer.py:48] [53100] global_step=53100, grad_norm=4.2189226150512695, loss=1.8806161880493164
I0201 14:16:45.290134 139907754342144 logging_writer.py:48] [53200] global_step=53200, grad_norm=3.8390378952026367, loss=1.8686233758926392
I0201 14:17:18.947107 139907745949440 logging_writer.py:48] [53300] global_step=53300, grad_norm=3.9411404132843018, loss=1.7670377492904663
I0201 14:17:52.628498 139907754342144 logging_writer.py:48] [53400] global_step=53400, grad_norm=3.920788049697876, loss=1.8482155799865723
I0201 14:18:26.484279 139907745949440 logging_writer.py:48] [53500] global_step=53500, grad_norm=3.910630226135254, loss=1.7879376411437988
I0201 14:19:00.200610 139907754342144 logging_writer.py:48] [53600] global_step=53600, grad_norm=3.3874502182006836, loss=1.7293304204940796
I0201 14:19:33.852164 139907745949440 logging_writer.py:48] [53700] global_step=53700, grad_norm=5.0374650955200195, loss=1.7787526845932007
I0201 14:20:07.534219 139907754342144 logging_writer.py:48] [53800] global_step=53800, grad_norm=3.758829116821289, loss=1.851094365119934
I0201 14:20:41.196116 139907745949440 logging_writer.py:48] [53900] global_step=53900, grad_norm=3.916935682296753, loss=1.83173406124115
I0201 14:21:14.887751 139907754342144 logging_writer.py:48] [54000] global_step=54000, grad_norm=3.372983694076538, loss=1.8674887418746948
I0201 14:21:48.558566 139907745949440 logging_writer.py:48] [54100] global_step=54100, grad_norm=3.656635284423828, loss=1.8817954063415527
I0201 14:22:22.258113 139907754342144 logging_writer.py:48] [54200] global_step=54200, grad_norm=4.187256813049316, loss=1.8204418420791626
I0201 14:22:55.913253 139907745949440 logging_writer.py:48] [54300] global_step=54300, grad_norm=4.003305435180664, loss=1.8202089071273804
I0201 14:23:29.621804 139907754342144 logging_writer.py:48] [54400] global_step=54400, grad_norm=3.4950757026672363, loss=1.8937740325927734
I0201 14:23:41.882298 140070692116288 spec.py:321] Evaluating on the training split.
I0201 14:23:48.208530 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 14:23:56.903047 140070692116288 spec.py:349] Evaluating on the test split.
I0201 14:23:59.503678 140070692116288 submission_runner.py:408] Time since start: 19055.96s, 	Step: 54438, 	{'train/accuracy': 0.6634446382522583, 'train/loss': 1.333989143371582, 'validation/accuracy': 0.608020007610321, 'validation/loss': 1.6208534240722656, 'validation/num_examples': 50000, 'test/accuracy': 0.48430001735687256, 'test/loss': 2.368727922439575, 'test/num_examples': 10000, 'score': 18396.55052471161, 'total_duration': 19055.956347703934, 'accumulated_submission_time': 18396.55052471161, 'accumulated_eval_time': 655.2638325691223, 'accumulated_logging_time': 2.253763198852539}
I0201 14:23:59.531971 139907762734848 logging_writer.py:48] [54438] accumulated_eval_time=655.263833, accumulated_logging_time=2.253763, accumulated_submission_time=18396.550525, global_step=54438, preemption_count=0, score=18396.550525, test/accuracy=0.484300, test/loss=2.368728, test/num_examples=10000, total_duration=19055.956348, train/accuracy=0.663445, train/loss=1.333989, validation/accuracy=0.608020, validation/loss=1.620853, validation/num_examples=50000
I0201 14:24:20.745453 139908425447168 logging_writer.py:48] [54500] global_step=54500, grad_norm=4.103140830993652, loss=1.778688907623291
I0201 14:24:54.557374 139907762734848 logging_writer.py:48] [54600] global_step=54600, grad_norm=3.6021225452423096, loss=1.8745371103286743
I0201 14:25:28.236840 139908425447168 logging_writer.py:48] [54700] global_step=54700, grad_norm=4.269860744476318, loss=1.8705675601959229
I0201 14:26:01.912655 139907762734848 logging_writer.py:48] [54800] global_step=54800, grad_norm=3.4724626541137695, loss=1.8844709396362305
I0201 14:26:35.568517 139908425447168 logging_writer.py:48] [54900] global_step=54900, grad_norm=3.40198016166687, loss=1.770717978477478
I0201 14:27:09.216710 139907762734848 logging_writer.py:48] [55000] global_step=55000, grad_norm=3.9953689575195312, loss=1.9098865985870361
I0201 14:27:42.896379 139908425447168 logging_writer.py:48] [55100] global_step=55100, grad_norm=3.5400397777557373, loss=1.9773972034454346
I0201 14:28:16.550087 139907762734848 logging_writer.py:48] [55200] global_step=55200, grad_norm=4.282717227935791, loss=1.8757457733154297
I0201 14:28:50.222823 139908425447168 logging_writer.py:48] [55300] global_step=55300, grad_norm=5.274156093597412, loss=1.89987313747406
I0201 14:29:23.900310 139907762734848 logging_writer.py:48] [55400] global_step=55400, grad_norm=3.6817574501037598, loss=1.8069753646850586
I0201 14:29:57.579169 139908425447168 logging_writer.py:48] [55500] global_step=55500, grad_norm=3.956559896469116, loss=1.8960578441619873
I0201 14:30:31.244845 139907762734848 logging_writer.py:48] [55600] global_step=55600, grad_norm=4.656852722167969, loss=1.8045552968978882
I0201 14:31:04.966425 139908425447168 logging_writer.py:48] [55700] global_step=55700, grad_norm=5.4762959480285645, loss=1.9445996284484863
I0201 14:31:38.671271 139907762734848 logging_writer.py:48] [55800] global_step=55800, grad_norm=4.181934833526611, loss=1.9832007884979248
I0201 14:32:12.342198 139908425447168 logging_writer.py:48] [55900] global_step=55900, grad_norm=4.679861068725586, loss=1.7579602003097534
I0201 14:32:29.676898 140070692116288 spec.py:321] Evaluating on the training split.
I0201 14:32:35.909050 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 14:32:44.508153 140070692116288 spec.py:349] Evaluating on the test split.
I0201 14:32:47.187227 140070692116288 submission_runner.py:408] Time since start: 19583.64s, 	Step: 55953, 	{'train/accuracy': 0.6671316623687744, 'train/loss': 1.318600058555603, 'validation/accuracy': 0.6162999868392944, 'validation/loss': 1.5899615287780762, 'validation/num_examples': 50000, 'test/accuracy': 0.49410003423690796, 'test/loss': 2.28857421875, 'test/num_examples': 10000, 'score': 18906.632444381714, 'total_duration': 19583.63987517357, 'accumulated_submission_time': 18906.632444381714, 'accumulated_eval_time': 672.7741053104401, 'accumulated_logging_time': 2.2918620109558105}
I0201 14:32:47.213823 139907745949440 logging_writer.py:48] [55953] accumulated_eval_time=672.774105, accumulated_logging_time=2.291862, accumulated_submission_time=18906.632444, global_step=55953, preemption_count=0, score=18906.632444, test/accuracy=0.494100, test/loss=2.288574, test/num_examples=10000, total_duration=19583.639875, train/accuracy=0.667132, train/loss=1.318600, validation/accuracy=0.616300, validation/loss=1.589962, validation/num_examples=50000
I0201 14:33:03.378384 139907754342144 logging_writer.py:48] [56000] global_step=56000, grad_norm=4.390674114227295, loss=1.762495517730713
I0201 14:33:37.073694 139907745949440 logging_writer.py:48] [56100] global_step=56100, grad_norm=3.6646618843078613, loss=1.7469412088394165
I0201 14:34:10.738441 139907754342144 logging_writer.py:48] [56200] global_step=56200, grad_norm=3.629284620285034, loss=1.8876316547393799
I0201 14:34:44.422370 139907745949440 logging_writer.py:48] [56300] global_step=56300, grad_norm=3.926177978515625, loss=1.930347204208374
I0201 14:35:18.062697 139907754342144 logging_writer.py:48] [56400] global_step=56400, grad_norm=3.7052791118621826, loss=1.7500871419906616
I0201 14:35:51.764600 139907745949440 logging_writer.py:48] [56500] global_step=56500, grad_norm=3.9601778984069824, loss=1.8160688877105713
I0201 14:36:25.434765 139907754342144 logging_writer.py:48] [56600] global_step=56600, grad_norm=4.476593971252441, loss=1.9084712266921997
I0201 14:36:59.120830 139907745949440 logging_writer.py:48] [56700] global_step=56700, grad_norm=4.107562065124512, loss=1.7711570262908936
I0201 14:37:32.914696 139907754342144 logging_writer.py:48] [56800] global_step=56800, grad_norm=4.601777076721191, loss=1.8416564464569092
I0201 14:38:06.638440 139907745949440 logging_writer.py:48] [56900] global_step=56900, grad_norm=3.7083113193511963, loss=1.926567554473877
I0201 14:38:40.298984 139907754342144 logging_writer.py:48] [57000] global_step=57000, grad_norm=4.782879829406738, loss=1.821397304534912
I0201 14:39:13.994111 139907745949440 logging_writer.py:48] [57100] global_step=57100, grad_norm=3.90449595451355, loss=1.8403346538543701
I0201 14:39:47.648380 139907754342144 logging_writer.py:48] [57200] global_step=57200, grad_norm=4.366998195648193, loss=1.9241416454315186
I0201 14:40:21.342797 139907745949440 logging_writer.py:48] [57300] global_step=57300, grad_norm=3.5553815364837646, loss=1.8792836666107178
I0201 14:40:54.998458 139907754342144 logging_writer.py:48] [57400] global_step=57400, grad_norm=3.7030835151672363, loss=1.7602235078811646
I0201 14:41:17.385415 140070692116288 spec.py:321] Evaluating on the training split.
I0201 14:41:23.608928 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 14:41:32.363823 140070692116288 spec.py:349] Evaluating on the test split.
I0201 14:41:34.961725 140070692116288 submission_runner.py:408] Time since start: 20111.41s, 	Step: 57468, 	{'train/accuracy': 0.6590401530265808, 'train/loss': 1.3535881042480469, 'validation/accuracy': 0.6110000014305115, 'validation/loss': 1.6046415567398071, 'validation/num_examples': 50000, 'test/accuracy': 0.48990002274513245, 'test/loss': 2.360703468322754, 'test/num_examples': 10000, 'score': 19416.74183535576, 'total_duration': 20111.414390802383, 'accumulated_submission_time': 19416.74183535576, 'accumulated_eval_time': 690.3503816127777, 'accumulated_logging_time': 2.3279449939727783}
I0201 14:41:34.992320 139908425447168 logging_writer.py:48] [57468] accumulated_eval_time=690.350382, accumulated_logging_time=2.327945, accumulated_submission_time=19416.741835, global_step=57468, preemption_count=0, score=19416.741835, test/accuracy=0.489900, test/loss=2.360703, test/num_examples=10000, total_duration=20111.414391, train/accuracy=0.659040, train/loss=1.353588, validation/accuracy=0.611000, validation/loss=1.604642, validation/num_examples=50000
I0201 14:41:46.109703 139908719027968 logging_writer.py:48] [57500] global_step=57500, grad_norm=4.133106231689453, loss=1.9021590948104858
I0201 14:42:19.762105 139908425447168 logging_writer.py:48] [57600] global_step=57600, grad_norm=4.498066425323486, loss=1.8607531785964966
I0201 14:42:53.428336 139908719027968 logging_writer.py:48] [57700] global_step=57700, grad_norm=3.693957805633545, loss=1.8983869552612305
I0201 14:43:27.113249 139908425447168 logging_writer.py:48] [57800] global_step=57800, grad_norm=3.2547481060028076, loss=1.9399789571762085
I0201 14:44:00.940884 139908719027968 logging_writer.py:48] [57900] global_step=57900, grad_norm=4.0049967765808105, loss=1.7112462520599365
I0201 14:44:34.625532 139908425447168 logging_writer.py:48] [58000] global_step=58000, grad_norm=4.478393077850342, loss=1.914684772491455
I0201 14:45:08.287391 139908719027968 logging_writer.py:48] [58100] global_step=58100, grad_norm=4.226706504821777, loss=1.9410029649734497
I0201 14:45:41.957980 139908425447168 logging_writer.py:48] [58200] global_step=58200, grad_norm=3.2082250118255615, loss=1.7657856941223145
I0201 14:46:15.616739 139908719027968 logging_writer.py:48] [58300] global_step=58300, grad_norm=3.2649171352386475, loss=1.7986267805099487
I0201 14:46:49.290763 139908425447168 logging_writer.py:48] [58400] global_step=58400, grad_norm=4.145215034484863, loss=1.8709303140640259
I0201 14:47:22.958757 139908719027968 logging_writer.py:48] [58500] global_step=58500, grad_norm=4.1045732498168945, loss=1.885740876197815
I0201 14:47:56.639759 139908425447168 logging_writer.py:48] [58600] global_step=58600, grad_norm=3.789606809616089, loss=1.8793625831604004
I0201 14:48:30.296664 139908719027968 logging_writer.py:48] [58700] global_step=58700, grad_norm=4.916805267333984, loss=1.8062329292297363
I0201 14:49:03.983943 139908425447168 logging_writer.py:48] [58800] global_step=58800, grad_norm=3.7608728408813477, loss=1.8944859504699707
I0201 14:49:37.640938 139908719027968 logging_writer.py:48] [58900] global_step=58900, grad_norm=4.21340274810791, loss=1.8478001356124878
I0201 14:50:05.202355 140070692116288 spec.py:321] Evaluating on the training split.
I0201 14:50:11.432777 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 14:50:20.016736 140070692116288 spec.py:349] Evaluating on the test split.
I0201 14:50:22.648845 140070692116288 submission_runner.py:408] Time since start: 20639.10s, 	Step: 58983, 	{'train/accuracy': 0.6589404940605164, 'train/loss': 1.3625762462615967, 'validation/accuracy': 0.6136400103569031, 'validation/loss': 1.6028468608856201, 'validation/num_examples': 50000, 'test/accuracy': 0.49250003695487976, 'test/loss': 2.331923246383667, 'test/num_examples': 10000, 'score': 19926.887871026993, 'total_duration': 20639.101494312286, 'accumulated_submission_time': 19926.887871026993, 'accumulated_eval_time': 707.7968149185181, 'accumulated_logging_time': 2.370288133621216}
I0201 14:50:22.684598 139907762734848 logging_writer.py:48] [58983] accumulated_eval_time=707.796815, accumulated_logging_time=2.370288, accumulated_submission_time=19926.887871, global_step=58983, preemption_count=0, score=19926.887871, test/accuracy=0.492500, test/loss=2.331923, test/num_examples=10000, total_duration=20639.101494, train/accuracy=0.658940, train/loss=1.362576, validation/accuracy=0.613640, validation/loss=1.602847, validation/num_examples=50000
I0201 14:50:28.762242 139908710635264 logging_writer.py:48] [59000] global_step=59000, grad_norm=4.637153625488281, loss=1.864805817604065
I0201 14:51:02.436321 139907762734848 logging_writer.py:48] [59100] global_step=59100, grad_norm=3.829573154449463, loss=1.9409971237182617
I0201 14:51:36.143011 139908710635264 logging_writer.py:48] [59200] global_step=59200, grad_norm=4.663274765014648, loss=2.025329113006592
I0201 14:52:09.808431 139907762734848 logging_writer.py:48] [59300] global_step=59300, grad_norm=4.179550647735596, loss=1.9146174192428589
I0201 14:52:43.479703 139908710635264 logging_writer.py:48] [59400] global_step=59400, grad_norm=3.8051397800445557, loss=1.9951303005218506
I0201 14:53:17.130472 139907762734848 logging_writer.py:48] [59500] global_step=59500, grad_norm=4.916173934936523, loss=1.7747740745544434
I0201 14:53:50.811071 139908710635264 logging_writer.py:48] [59600] global_step=59600, grad_norm=5.094639301300049, loss=1.7922773361206055
I0201 14:54:24.463897 139907762734848 logging_writer.py:48] [59700] global_step=59700, grad_norm=4.0282392501831055, loss=1.902174472808838
I0201 14:54:58.163968 139908710635264 logging_writer.py:48] [59800] global_step=59800, grad_norm=3.9687507152557373, loss=1.942754864692688
I0201 14:55:31.810440 139907762734848 logging_writer.py:48] [59900] global_step=59900, grad_norm=4.124354839324951, loss=1.7792832851409912
I0201 14:56:05.588477 139908710635264 logging_writer.py:48] [60000] global_step=60000, grad_norm=5.224400043487549, loss=1.817784309387207
I0201 14:56:39.316055 139907762734848 logging_writer.py:48] [60100] global_step=60100, grad_norm=3.448148250579834, loss=1.9044063091278076
I0201 14:57:13.008168 139908710635264 logging_writer.py:48] [60200] global_step=60200, grad_norm=4.192609786987305, loss=1.7116612195968628
I0201 14:57:46.663983 139907762734848 logging_writer.py:48] [60300] global_step=60300, grad_norm=3.888916254043579, loss=1.9205659627914429
I0201 14:58:20.347704 139908710635264 logging_writer.py:48] [60400] global_step=60400, grad_norm=3.493478298187256, loss=1.7927453517913818
I0201 14:58:52.807618 140070692116288 spec.py:321] Evaluating on the training split.
I0201 14:58:59.107065 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 14:59:07.823154 140070692116288 spec.py:349] Evaluating on the test split.
I0201 14:59:10.975220 140070692116288 submission_runner.py:408] Time since start: 21167.43s, 	Step: 60498, 	{'train/accuracy': 0.6735491156578064, 'train/loss': 1.2887380123138428, 'validation/accuracy': 0.620639979839325, 'validation/loss': 1.5495282411575317, 'validation/num_examples': 50000, 'test/accuracy': 0.4991000294685364, 'test/loss': 2.270559787750244, 'test/num_examples': 10000, 'score': 20436.946853160858, 'total_duration': 21167.427884340286, 'accumulated_submission_time': 20436.946853160858, 'accumulated_eval_time': 725.9643821716309, 'accumulated_logging_time': 2.4173500537872314}
I0201 14:59:10.997687 139907745949440 logging_writer.py:48] [60498] accumulated_eval_time=725.964382, accumulated_logging_time=2.417350, accumulated_submission_time=20436.946853, global_step=60498, preemption_count=0, score=20436.946853, test/accuracy=0.499100, test/loss=2.270560, test/num_examples=10000, total_duration=21167.427884, train/accuracy=0.673549, train/loss=1.288738, validation/accuracy=0.620640, validation/loss=1.549528, validation/num_examples=50000
I0201 14:59:12.008638 139907754342144 logging_writer.py:48] [60500] global_step=60500, grad_norm=3.963822603225708, loss=1.738756537437439
I0201 14:59:45.626928 139907745949440 logging_writer.py:48] [60600] global_step=60600, grad_norm=3.897918701171875, loss=1.8062326908111572
I0201 15:00:19.298263 139907754342144 logging_writer.py:48] [60700] global_step=60700, grad_norm=3.600842237472534, loss=1.6525574922561646
I0201 15:00:52.941278 139907745949440 logging_writer.py:48] [60800] global_step=60800, grad_norm=4.197591781616211, loss=1.904090166091919
I0201 15:01:26.637923 139907754342144 logging_writer.py:48] [60900] global_step=60900, grad_norm=3.834193468093872, loss=1.727867603302002
I0201 15:02:00.283009 139907745949440 logging_writer.py:48] [61000] global_step=61000, grad_norm=4.584933280944824, loss=1.9090383052825928
I0201 15:02:33.970619 139907754342144 logging_writer.py:48] [61100] global_step=61100, grad_norm=4.2310566902160645, loss=1.9079800844192505
I0201 15:03:07.585872 139907745949440 logging_writer.py:48] [61200] global_step=61200, grad_norm=4.392971515655518, loss=1.7049309015274048
I0201 15:03:41.281573 139907754342144 logging_writer.py:48] [61300] global_step=61300, grad_norm=3.966472625732422, loss=1.7433503866195679
I0201 15:04:14.925263 139907745949440 logging_writer.py:48] [61400] global_step=61400, grad_norm=4.386988162994385, loss=1.807742953300476
I0201 15:04:48.628421 139907754342144 logging_writer.py:48] [61500] global_step=61500, grad_norm=4.329443454742432, loss=1.8090046644210815
I0201 15:05:22.289109 139907745949440 logging_writer.py:48] [61600] global_step=61600, grad_norm=4.141831874847412, loss=1.7845268249511719
I0201 15:05:55.953148 139907754342144 logging_writer.py:48] [61700] global_step=61700, grad_norm=4.071753025054932, loss=1.738908052444458
I0201 15:06:29.623723 139907745949440 logging_writer.py:48] [61800] global_step=61800, grad_norm=3.6307923793792725, loss=1.8381680250167847
I0201 15:07:03.307901 139907754342144 logging_writer.py:48] [61900] global_step=61900, grad_norm=4.097952842712402, loss=1.8291269540786743
I0201 15:07:36.975295 139907745949440 logging_writer.py:48] [62000] global_step=62000, grad_norm=3.8401434421539307, loss=1.9186407327651978
I0201 15:07:41.163309 140070692116288 spec.py:321] Evaluating on the training split.
I0201 15:07:47.435804 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 15:07:56.173282 140070692116288 spec.py:349] Evaluating on the test split.
I0201 15:07:58.769299 140070692116288 submission_runner.py:408] Time since start: 21695.22s, 	Step: 62014, 	{'train/accuracy': 0.6741669178009033, 'train/loss': 1.2919275760650635, 'validation/accuracy': 0.6080399751663208, 'validation/loss': 1.623947024345398, 'validation/num_examples': 50000, 'test/accuracy': 0.4821000099182129, 'test/loss': 2.346834421157837, 'test/num_examples': 10000, 'score': 20947.052046775818, 'total_duration': 21695.2219684124, 'accumulated_submission_time': 20947.052046775818, 'accumulated_eval_time': 743.5703382492065, 'accumulated_logging_time': 2.4481093883514404}
I0201 15:07:58.799143 139908719027968 logging_writer.py:48] [62014] accumulated_eval_time=743.570338, accumulated_logging_time=2.448109, accumulated_submission_time=20947.052047, global_step=62014, preemption_count=0, score=20947.052047, test/accuracy=0.482100, test/loss=2.346834, test/num_examples=10000, total_duration=21695.221968, train/accuracy=0.674167, train/loss=1.291928, validation/accuracy=0.608040, validation/loss=1.623947, validation/num_examples=50000
I0201 15:08:28.067102 139908727420672 logging_writer.py:48] [62100] global_step=62100, grad_norm=3.930817127227783, loss=1.7488306760787964
I0201 15:09:01.732927 139908719027968 logging_writer.py:48] [62200] global_step=62200, grad_norm=4.976370811462402, loss=1.8162554502487183
I0201 15:09:35.411259 139908727420672 logging_writer.py:48] [62300] global_step=62300, grad_norm=4.21971321105957, loss=1.7301194667816162
I0201 15:10:09.076505 139908719027968 logging_writer.py:48] [62400] global_step=62400, grad_norm=5.100790023803711, loss=1.8140419721603394
I0201 15:10:42.762117 139908727420672 logging_writer.py:48] [62500] global_step=62500, grad_norm=4.523493766784668, loss=1.8048222064971924
I0201 15:11:16.414165 139908719027968 logging_writer.py:48] [62600] global_step=62600, grad_norm=3.965191125869751, loss=1.8071234226226807
I0201 15:11:50.105484 139908727420672 logging_writer.py:48] [62700] global_step=62700, grad_norm=4.9975128173828125, loss=1.9036694765090942
I0201 15:12:23.756252 139908719027968 logging_writer.py:48] [62800] global_step=62800, grad_norm=3.3941433429718018, loss=1.707960844039917
I0201 15:12:57.429526 139908727420672 logging_writer.py:48] [62900] global_step=62900, grad_norm=4.569104194641113, loss=1.888169288635254
I0201 15:13:31.087030 139908719027968 logging_writer.py:48] [63000] global_step=63000, grad_norm=4.608440399169922, loss=1.746177077293396
I0201 15:14:04.760784 139908727420672 logging_writer.py:48] [63100] global_step=63100, grad_norm=4.1116414070129395, loss=1.8472363948822021
I0201 15:14:38.408201 139908719027968 logging_writer.py:48] [63200] global_step=63200, grad_norm=3.7439804077148438, loss=1.7460458278656006
I0201 15:15:12.185074 139908727420672 logging_writer.py:48] [63300] global_step=63300, grad_norm=3.7646377086639404, loss=1.8494350910186768
I0201 15:15:45.879091 139908719027968 logging_writer.py:48] [63400] global_step=63400, grad_norm=5.5215044021606445, loss=1.8357716798782349
I0201 15:16:19.582478 139908727420672 logging_writer.py:48] [63500] global_step=63500, grad_norm=3.8037679195404053, loss=1.6994774341583252
I0201 15:16:28.829628 140070692116288 spec.py:321] Evaluating on the training split.
I0201 15:16:35.074114 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 15:16:43.713174 140070692116288 spec.py:349] Evaluating on the test split.
I0201 15:16:46.231948 140070692116288 submission_runner.py:408] Time since start: 22222.68s, 	Step: 63529, 	{'train/accuracy': 0.6745057106018066, 'train/loss': 1.275795817375183, 'validation/accuracy': 0.6182000041007996, 'validation/loss': 1.5899293422698975, 'validation/num_examples': 50000, 'test/accuracy': 0.49710002541542053, 'test/loss': 2.292905330657959, 'test/num_examples': 10000, 'score': 21457.020793676376, 'total_duration': 22222.684602499008, 'accumulated_submission_time': 21457.020793676376, 'accumulated_eval_time': 760.9726111888885, 'accumulated_logging_time': 2.4869983196258545}
I0201 15:16:46.266304 139907754342144 logging_writer.py:48] [63529] accumulated_eval_time=760.972611, accumulated_logging_time=2.486998, accumulated_submission_time=21457.020794, global_step=63529, preemption_count=0, score=21457.020794, test/accuracy=0.497100, test/loss=2.292905, test/num_examples=10000, total_duration=22222.684602, train/accuracy=0.674506, train/loss=1.275796, validation/accuracy=0.618200, validation/loss=1.589929, validation/num_examples=50000
I0201 15:17:10.473068 139907762734848 logging_writer.py:48] [63600] global_step=63600, grad_norm=4.131134033203125, loss=1.8859400749206543
I0201 15:17:44.159911 139907754342144 logging_writer.py:48] [63700] global_step=63700, grad_norm=4.029731273651123, loss=1.8362390995025635
I0201 15:18:17.855810 139907762734848 logging_writer.py:48] [63800] global_step=63800, grad_norm=3.7394492626190186, loss=1.8422553539276123
I0201 15:18:51.515863 139907754342144 logging_writer.py:48] [63900] global_step=63900, grad_norm=3.759087085723877, loss=1.8037265539169312
I0201 15:19:25.207379 139907762734848 logging_writer.py:48] [64000] global_step=64000, grad_norm=3.9279844760894775, loss=1.7698789834976196
I0201 15:19:58.855923 139907754342144 logging_writer.py:48] [64100] global_step=64100, grad_norm=4.197653293609619, loss=1.766310453414917
I0201 15:20:32.533928 139907762734848 logging_writer.py:48] [64200] global_step=64200, grad_norm=4.047369480133057, loss=1.6753766536712646
I0201 15:21:06.194030 139907754342144 logging_writer.py:48] [64300] global_step=64300, grad_norm=3.840740919113159, loss=1.8638038635253906
I0201 15:21:39.908750 139907762734848 logging_writer.py:48] [64400] global_step=64400, grad_norm=4.568819046020508, loss=1.8355059623718262
I0201 15:22:13.625004 139907754342144 logging_writer.py:48] [64500] global_step=64500, grad_norm=4.0071210861206055, loss=1.9376044273376465
I0201 15:22:47.321434 139907762734848 logging_writer.py:48] [64600] global_step=64600, grad_norm=3.9370675086975098, loss=1.770258903503418
I0201 15:23:20.980336 139907754342144 logging_writer.py:48] [64700] global_step=64700, grad_norm=4.146255016326904, loss=1.7692102193832397
I0201 15:23:54.659357 139907762734848 logging_writer.py:48] [64800] global_step=64800, grad_norm=4.164078235626221, loss=1.850406289100647
I0201 15:24:28.316366 139907754342144 logging_writer.py:48] [64900] global_step=64900, grad_norm=3.590761423110962, loss=1.768512487411499
I0201 15:25:01.983647 139907762734848 logging_writer.py:48] [65000] global_step=65000, grad_norm=4.181295871734619, loss=1.708524227142334
I0201 15:25:16.262028 140070692116288 spec.py:321] Evaluating on the training split.
I0201 15:25:22.538637 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 15:25:31.368611 140070692116288 spec.py:349] Evaluating on the test split.
I0201 15:25:33.856258 140070692116288 submission_runner.py:408] Time since start: 22750.31s, 	Step: 65044, 	{'train/accuracy': 0.6870814561843872, 'train/loss': 1.2384870052337646, 'validation/accuracy': 0.6372599601745605, 'validation/loss': 1.5065011978149414, 'validation/num_examples': 50000, 'test/accuracy': 0.5083000063896179, 'test/loss': 2.2321155071258545, 'test/num_examples': 10000, 'score': 21966.95255088806, 'total_duration': 22750.308904886246, 'accumulated_submission_time': 21966.95255088806, 'accumulated_eval_time': 778.5667836666107, 'accumulated_logging_time': 2.531611204147339}
I0201 15:25:33.884644 139908710635264 logging_writer.py:48] [65044] accumulated_eval_time=778.566784, accumulated_logging_time=2.531611, accumulated_submission_time=21966.952551, global_step=65044, preemption_count=0, score=21966.952551, test/accuracy=0.508300, test/loss=2.232116, test/num_examples=10000, total_duration=22750.308905, train/accuracy=0.687081, train/loss=1.238487, validation/accuracy=0.637260, validation/loss=1.506501, validation/num_examples=50000
I0201 15:25:53.082037 139908719027968 logging_writer.py:48] [65100] global_step=65100, grad_norm=3.9936845302581787, loss=1.76398766040802
I0201 15:26:26.752104 139908710635264 logging_writer.py:48] [65200] global_step=65200, grad_norm=3.834440231323242, loss=1.762596607208252
I0201 15:27:00.436290 139908719027968 logging_writer.py:48] [65300] global_step=65300, grad_norm=3.629570484161377, loss=1.7025485038757324
I0201 15:27:34.137535 139908710635264 logging_writer.py:48] [65400] global_step=65400, grad_norm=4.0880327224731445, loss=1.740077257156372
I0201 15:28:08.002539 139908719027968 logging_writer.py:48] [65500] global_step=65500, grad_norm=3.7041187286376953, loss=1.8758776187896729
I0201 15:28:41.689523 139908710635264 logging_writer.py:48] [65600] global_step=65600, grad_norm=4.710566520690918, loss=1.6897239685058594
I0201 15:29:15.379004 139908719027968 logging_writer.py:48] [65700] global_step=65700, grad_norm=4.180932998657227, loss=1.7771342992782593
I0201 15:29:49.049802 139908710635264 logging_writer.py:48] [65800] global_step=65800, grad_norm=4.244616985321045, loss=1.8114583492279053
I0201 15:30:22.718866 139908719027968 logging_writer.py:48] [65900] global_step=65900, grad_norm=3.9305930137634277, loss=1.7551084756851196
I0201 15:30:56.379589 139908710635264 logging_writer.py:48] [66000] global_step=66000, grad_norm=4.321260929107666, loss=1.9228816032409668
I0201 15:31:30.041523 139908719027968 logging_writer.py:48] [66100] global_step=66100, grad_norm=3.956782102584839, loss=1.7866559028625488
I0201 15:32:03.726996 139908710635264 logging_writer.py:48] [66200] global_step=66200, grad_norm=4.348387718200684, loss=1.7940616607666016
I0201 15:32:37.391966 139908719027968 logging_writer.py:48] [66300] global_step=66300, grad_norm=4.285783767700195, loss=1.7037107944488525
I0201 15:33:11.084694 139908710635264 logging_writer.py:48] [66400] global_step=66400, grad_norm=3.897214889526367, loss=1.7805697917938232
I0201 15:33:44.736446 139908719027968 logging_writer.py:48] [66500] global_step=66500, grad_norm=3.740056276321411, loss=1.7270721197128296
I0201 15:34:03.949002 140070692116288 spec.py:321] Evaluating on the training split.
I0201 15:34:10.191197 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 15:34:18.783482 140070692116288 spec.py:349] Evaluating on the test split.
I0201 15:34:21.395299 140070692116288 submission_runner.py:408] Time since start: 23277.85s, 	Step: 66558, 	{'train/accuracy': 0.6863042116165161, 'train/loss': 1.2401232719421387, 'validation/accuracy': 0.6317600011825562, 'validation/loss': 1.520139217376709, 'validation/num_examples': 50000, 'test/accuracy': 0.5033000111579895, 'test/loss': 2.2496683597564697, 'test/num_examples': 10000, 'score': 22476.95581459999, 'total_duration': 23277.84796833992, 'accumulated_submission_time': 22476.95581459999, 'accumulated_eval_time': 796.0130662918091, 'accumulated_logging_time': 2.5694398880004883}
I0201 15:34:21.428524 139907737556736 logging_writer.py:48] [66558] accumulated_eval_time=796.013066, accumulated_logging_time=2.569440, accumulated_submission_time=22476.955815, global_step=66558, preemption_count=0, score=22476.955815, test/accuracy=0.503300, test/loss=2.249668, test/num_examples=10000, total_duration=23277.847968, train/accuracy=0.686304, train/loss=1.240123, validation/accuracy=0.631760, validation/loss=1.520139, validation/num_examples=50000
I0201 15:34:35.878391 139907745949440 logging_writer.py:48] [66600] global_step=66600, grad_norm=4.255701541900635, loss=1.8049225807189941
I0201 15:35:09.554358 139907737556736 logging_writer.py:48] [66700] global_step=66700, grad_norm=4.015451908111572, loss=1.802878499031067
I0201 15:35:43.213524 139907745949440 logging_writer.py:48] [66800] global_step=66800, grad_norm=4.1540327072143555, loss=1.677807092666626
I0201 15:36:16.901128 139907737556736 logging_writer.py:48] [66900] global_step=66900, grad_norm=3.77124285697937, loss=1.730506420135498
I0201 15:36:50.542564 139907745949440 logging_writer.py:48] [67000] global_step=67000, grad_norm=4.177480220794678, loss=1.8358662128448486
I0201 15:37:24.215308 139907737556736 logging_writer.py:48] [67100] global_step=67100, grad_norm=3.6857376098632812, loss=1.7517333030700684
I0201 15:37:57.873581 139907745949440 logging_writer.py:48] [67200] global_step=67200, grad_norm=4.516510963439941, loss=1.7196029424667358
I0201 15:38:31.570611 139907737556736 logging_writer.py:48] [67300] global_step=67300, grad_norm=4.235347270965576, loss=1.8401634693145752
I0201 15:39:05.223006 139907745949440 logging_writer.py:48] [67400] global_step=67400, grad_norm=4.036756992340088, loss=1.8062469959259033
I0201 15:39:38.927000 139907737556736 logging_writer.py:48] [67500] global_step=67500, grad_norm=4.729454517364502, loss=1.8026140928268433
I0201 15:40:12.697236 139907745949440 logging_writer.py:48] [67600] global_step=67600, grad_norm=4.00757360458374, loss=1.7863038778305054
I0201 15:40:46.428859 139907737556736 logging_writer.py:48] [67700] global_step=67700, grad_norm=4.817930221557617, loss=1.9030730724334717
I0201 15:41:20.074099 139907745949440 logging_writer.py:48] [67800] global_step=67800, grad_norm=4.09850549697876, loss=1.772024393081665
I0201 15:41:53.767076 139907737556736 logging_writer.py:48] [67900] global_step=67900, grad_norm=4.00065803527832, loss=1.8327456712722778
I0201 15:42:27.416701 139907745949440 logging_writer.py:48] [68000] global_step=68000, grad_norm=4.399011611938477, loss=1.738403081893921
I0201 15:42:51.488155 140070692116288 spec.py:321] Evaluating on the training split.
I0201 15:42:57.960864 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 15:43:06.688625 140070692116288 spec.py:349] Evaluating on the test split.
I0201 15:43:09.377061 140070692116288 submission_runner.py:408] Time since start: 23805.83s, 	Step: 68073, 	{'train/accuracy': 0.675203263759613, 'train/loss': 1.2782618999481201, 'validation/accuracy': 0.6322000026702881, 'validation/loss': 1.5258268117904663, 'validation/num_examples': 50000, 'test/accuracy': 0.5074000358581543, 'test/loss': 2.2572550773620605, 'test/num_examples': 10000, 'score': 22986.95196557045, 'total_duration': 23805.829738855362, 'accumulated_submission_time': 22986.95196557045, 'accumulated_eval_time': 813.9019508361816, 'accumulated_logging_time': 2.613524913787842}
I0201 15:43:09.402022 139908710635264 logging_writer.py:48] [68073] accumulated_eval_time=813.901951, accumulated_logging_time=2.613525, accumulated_submission_time=22986.951966, global_step=68073, preemption_count=0, score=22986.951966, test/accuracy=0.507400, test/loss=2.257255, test/num_examples=10000, total_duration=23805.829739, train/accuracy=0.675203, train/loss=1.278262, validation/accuracy=0.632200, validation/loss=1.525827, validation/num_examples=50000
I0201 15:43:18.836389 139908719027968 logging_writer.py:48] [68100] global_step=68100, grad_norm=4.586103439331055, loss=1.9252464771270752
I0201 15:43:52.523944 139908710635264 logging_writer.py:48] [68200] global_step=68200, grad_norm=4.101780891418457, loss=1.7434968948364258
I0201 15:44:26.203714 139908719027968 logging_writer.py:48] [68300] global_step=68300, grad_norm=4.080649375915527, loss=1.8844006061553955
I0201 15:44:59.881790 139908710635264 logging_writer.py:48] [68400] global_step=68400, grad_norm=3.8721680641174316, loss=1.7540979385375977
I0201 15:45:33.540504 139908719027968 logging_writer.py:48] [68500] global_step=68500, grad_norm=4.062082290649414, loss=1.8208602666854858
I0201 15:46:07.202516 139908710635264 logging_writer.py:48] [68600] global_step=68600, grad_norm=3.60302472114563, loss=1.6436169147491455
I0201 15:46:40.974272 139908719027968 logging_writer.py:48] [68700] global_step=68700, grad_norm=4.6061201095581055, loss=1.698048710823059
I0201 15:47:14.662172 139908710635264 logging_writer.py:48] [68800] global_step=68800, grad_norm=4.912476062774658, loss=1.7657960653305054
I0201 15:47:48.320093 139908719027968 logging_writer.py:48] [68900] global_step=68900, grad_norm=4.7533464431762695, loss=1.9336165189743042
I0201 15:48:21.983493 139908710635264 logging_writer.py:48] [69000] global_step=69000, grad_norm=3.603224515914917, loss=1.8101565837860107
I0201 15:48:55.654540 139908719027968 logging_writer.py:48] [69100] global_step=69100, grad_norm=4.127538204193115, loss=1.8842942714691162
I0201 15:49:29.318723 139908710635264 logging_writer.py:48] [69200] global_step=69200, grad_norm=4.139777660369873, loss=1.8233411312103271
I0201 15:50:02.989787 139908719027968 logging_writer.py:48] [69300] global_step=69300, grad_norm=4.439967155456543, loss=1.8149073123931885
I0201 15:50:36.670376 139908710635264 logging_writer.py:48] [69400] global_step=69400, grad_norm=4.050889015197754, loss=1.7486034631729126
I0201 15:51:10.330356 139908719027968 logging_writer.py:48] [69500] global_step=69500, grad_norm=4.633551120758057, loss=1.7572050094604492
I0201 15:51:39.436221 140070692116288 spec.py:321] Evaluating on the training split.
I0201 15:51:45.711590 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 15:51:54.601854 140070692116288 spec.py:349] Evaluating on the test split.
I0201 15:51:57.197367 140070692116288 submission_runner.py:408] Time since start: 24333.65s, 	Step: 69588, 	{'train/accuracy': 0.6990393400192261, 'train/loss': 1.1675734519958496, 'validation/accuracy': 0.6238600015640259, 'validation/loss': 1.5474979877471924, 'validation/num_examples': 50000, 'test/accuracy': 0.4936000108718872, 'test/loss': 2.2800233364105225, 'test/num_examples': 10000, 'score': 23496.924777507782, 'total_duration': 24333.650037050247, 'accumulated_submission_time': 23496.924777507782, 'accumulated_eval_time': 831.6630780696869, 'accumulated_logging_time': 2.6471753120422363}
I0201 15:51:57.226182 139907737556736 logging_writer.py:48] [69588] accumulated_eval_time=831.663078, accumulated_logging_time=2.647175, accumulated_submission_time=23496.924778, global_step=69588, preemption_count=0, score=23496.924778, test/accuracy=0.493600, test/loss=2.280023, test/num_examples=10000, total_duration=24333.650037, train/accuracy=0.699039, train/loss=1.167573, validation/accuracy=0.623860, validation/loss=1.547498, validation/num_examples=50000
I0201 15:52:01.610329 139907745949440 logging_writer.py:48] [69600] global_step=69600, grad_norm=4.621696472167969, loss=1.7257001399993896
I0201 15:52:35.283842 139907737556736 logging_writer.py:48] [69700] global_step=69700, grad_norm=4.248839855194092, loss=1.712229609489441
I0201 15:53:09.155483 139907745949440 logging_writer.py:48] [69800] global_step=69800, grad_norm=3.6340413093566895, loss=1.7759110927581787
I0201 15:53:42.799321 139907737556736 logging_writer.py:48] [69900] global_step=69900, grad_norm=4.507107257843018, loss=1.7407090663909912
I0201 15:54:16.481787 139907745949440 logging_writer.py:48] [70000] global_step=70000, grad_norm=3.9787559509277344, loss=1.7067959308624268
I0201 15:54:50.141208 139907737556736 logging_writer.py:48] [70100] global_step=70100, grad_norm=3.5856332778930664, loss=1.7329342365264893
I0201 15:55:23.846951 139907745949440 logging_writer.py:48] [70200] global_step=70200, grad_norm=4.021901607513428, loss=1.8231699466705322
I0201 15:55:57.499840 139907737556736 logging_writer.py:48] [70300] global_step=70300, grad_norm=4.192436695098877, loss=1.8717429637908936
I0201 15:56:31.193656 139907745949440 logging_writer.py:48] [70400] global_step=70400, grad_norm=4.191050052642822, loss=1.7959721088409424
I0201 15:57:04.849177 139907737556736 logging_writer.py:48] [70500] global_step=70500, grad_norm=4.0348734855651855, loss=1.8207364082336426
I0201 15:57:38.544342 139907745949440 logging_writer.py:48] [70600] global_step=70600, grad_norm=3.811476707458496, loss=1.7833023071289062
I0201 15:58:12.183878 139907737556736 logging_writer.py:48] [70700] global_step=70700, grad_norm=4.622041702270508, loss=1.8090261220932007
I0201 15:58:45.878347 139907745949440 logging_writer.py:48] [70800] global_step=70800, grad_norm=4.035845756530762, loss=1.8001502752304077
I0201 15:59:19.662358 139907737556736 logging_writer.py:48] [70900] global_step=70900, grad_norm=4.2645697593688965, loss=1.765625
I0201 15:59:53.402646 139907745949440 logging_writer.py:48] [71000] global_step=71000, grad_norm=3.7406723499298096, loss=1.7752983570098877
I0201 16:00:27.045578 139907737556736 logging_writer.py:48] [71100] global_step=71100, grad_norm=3.6479485034942627, loss=1.6952769756317139
I0201 16:00:27.216538 140070692116288 spec.py:321] Evaluating on the training split.
I0201 16:00:33.553476 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 16:00:42.312749 140070692116288 spec.py:349] Evaluating on the test split.
I0201 16:00:44.889822 140070692116288 submission_runner.py:408] Time since start: 24861.34s, 	Step: 71102, 	{'train/accuracy': 0.70023512840271, 'train/loss': 1.153869867324829, 'validation/accuracy': 0.632420003414154, 'validation/loss': 1.5144561529159546, 'validation/num_examples': 50000, 'test/accuracy': 0.5120000243186951, 'test/loss': 2.2258431911468506, 'test/num_examples': 10000, 'score': 24006.85396337509, 'total_duration': 24861.34249138832, 'accumulated_submission_time': 24006.85396337509, 'accumulated_eval_time': 849.3363344669342, 'accumulated_logging_time': 2.685119152069092}
I0201 16:00:44.922178 139908710635264 logging_writer.py:48] [71102] accumulated_eval_time=849.336334, accumulated_logging_time=2.685119, accumulated_submission_time=24006.853963, global_step=71102, preemption_count=0, score=24006.853963, test/accuracy=0.512000, test/loss=2.225843, test/num_examples=10000, total_duration=24861.342491, train/accuracy=0.700235, train/loss=1.153870, validation/accuracy=0.632420, validation/loss=1.514456, validation/num_examples=50000
I0201 16:01:18.186152 139908719027968 logging_writer.py:48] [71200] global_step=71200, grad_norm=3.950967311859131, loss=1.7678226232528687
I0201 16:01:51.844775 139908710635264 logging_writer.py:48] [71300] global_step=71300, grad_norm=4.37095308303833, loss=1.725412130355835
I0201 16:02:25.533849 139908719027968 logging_writer.py:48] [71400] global_step=71400, grad_norm=4.7904791831970215, loss=1.8058043718338013
I0201 16:02:59.196559 139908710635264 logging_writer.py:48] [71500] global_step=71500, grad_norm=4.281838893890381, loss=1.726257085800171
I0201 16:03:32.878938 139908719027968 logging_writer.py:48] [71600] global_step=71600, grad_norm=4.105118274688721, loss=1.8503165245056152
I0201 16:04:06.558788 139908710635264 logging_writer.py:48] [71700] global_step=71700, grad_norm=3.8882033824920654, loss=1.7560194730758667
I0201 16:04:40.219362 139908719027968 logging_writer.py:48] [71800] global_step=71800, grad_norm=4.2464118003845215, loss=1.8316411972045898
I0201 16:05:13.907301 139908710635264 logging_writer.py:48] [71900] global_step=71900, grad_norm=3.982926845550537, loss=1.6761474609375
I0201 16:05:47.705981 139908719027968 logging_writer.py:48] [72000] global_step=72000, grad_norm=4.199317455291748, loss=1.7684333324432373
I0201 16:06:21.402333 139908710635264 logging_writer.py:48] [72100] global_step=72100, grad_norm=4.890565395355225, loss=1.6987651586532593
I0201 16:06:55.074214 139908719027968 logging_writer.py:48] [72200] global_step=72200, grad_norm=4.291840553283691, loss=1.732839584350586
I0201 16:07:28.734577 139908710635264 logging_writer.py:48] [72300] global_step=72300, grad_norm=4.102774143218994, loss=1.8221744298934937
I0201 16:08:02.415341 139908719027968 logging_writer.py:48] [72400] global_step=72400, grad_norm=4.838420867919922, loss=1.7896618843078613
I0201 16:08:36.074241 139908710635264 logging_writer.py:48] [72500] global_step=72500, grad_norm=3.6503868103027344, loss=1.7951138019561768
I0201 16:09:09.749498 139908719027968 logging_writer.py:48] [72600] global_step=72600, grad_norm=4.392019748687744, loss=1.8558177947998047
I0201 16:09:14.938537 140070692116288 spec.py:321] Evaluating on the training split.
I0201 16:09:21.217081 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 16:09:29.851287 140070692116288 spec.py:349] Evaluating on the test split.
I0201 16:09:32.437766 140070692116288 submission_runner.py:408] Time since start: 25388.89s, 	Step: 72617, 	{'train/accuracy': 0.6888153553009033, 'train/loss': 1.204163670539856, 'validation/accuracy': 0.6315199732780457, 'validation/loss': 1.5182421207427979, 'validation/num_examples': 50000, 'test/accuracy': 0.5139000415802002, 'test/loss': 2.2352921962738037, 'test/num_examples': 10000, 'score': 24516.805390119553, 'total_duration': 25388.890427350998, 'accumulated_submission_time': 24516.805390119553, 'accumulated_eval_time': 866.8355195522308, 'accumulated_logging_time': 2.7290549278259277}
I0201 16:09:32.470302 139907754342144 logging_writer.py:48] [72617] accumulated_eval_time=866.835520, accumulated_logging_time=2.729055, accumulated_submission_time=24516.805390, global_step=72617, preemption_count=0, score=24516.805390, test/accuracy=0.513900, test/loss=2.235292, test/num_examples=10000, total_duration=25388.890427, train/accuracy=0.688815, train/loss=1.204164, validation/accuracy=0.631520, validation/loss=1.518242, validation/num_examples=50000
I0201 16:10:00.705982 139907762734848 logging_writer.py:48] [72700] global_step=72700, grad_norm=4.632140636444092, loss=1.9199860095977783
I0201 16:10:34.329858 139907754342144 logging_writer.py:48] [72800] global_step=72800, grad_norm=3.9802122116088867, loss=1.8481961488723755
I0201 16:11:08.022107 139907762734848 logging_writer.py:48] [72900] global_step=72900, grad_norm=3.773277997970581, loss=1.7742493152618408
I0201 16:11:41.758253 139907754342144 logging_writer.py:48] [73000] global_step=73000, grad_norm=3.826972007751465, loss=1.7368309497833252
I0201 16:12:15.458076 139907762734848 logging_writer.py:48] [73100] global_step=73100, grad_norm=4.737924575805664, loss=1.8491184711456299
I0201 16:12:49.121449 139907754342144 logging_writer.py:48] [73200] global_step=73200, grad_norm=3.862886905670166, loss=1.7733805179595947
I0201 16:13:22.831271 139907762734848 logging_writer.py:48] [73300] global_step=73300, grad_norm=4.1342549324035645, loss=1.8693342208862305
I0201 16:13:56.487829 139907754342144 logging_writer.py:48] [73400] global_step=73400, grad_norm=5.080748558044434, loss=1.7156621217727661
I0201 16:14:30.159418 139907762734848 logging_writer.py:48] [73500] global_step=73500, grad_norm=4.518148422241211, loss=1.7970049381256104
I0201 16:15:03.805217 139907754342144 logging_writer.py:48] [73600] global_step=73600, grad_norm=4.2793049812316895, loss=1.7437372207641602
I0201 16:15:37.504611 139907762734848 logging_writer.py:48] [73700] global_step=73700, grad_norm=4.134973049163818, loss=1.8442381620407104
I0201 16:16:11.136178 139907754342144 logging_writer.py:48] [73800] global_step=73800, grad_norm=4.1554131507873535, loss=1.854041337966919
I0201 16:16:44.816702 139907762734848 logging_writer.py:48] [73900] global_step=73900, grad_norm=4.489658832550049, loss=1.8104748725891113
I0201 16:17:18.462239 139907754342144 logging_writer.py:48] [74000] global_step=74000, grad_norm=3.597270965576172, loss=1.73280930519104
I0201 16:17:52.163717 139907762734848 logging_writer.py:48] [74100] global_step=74100, grad_norm=3.7580416202545166, loss=1.8920478820800781
I0201 16:18:02.493679 140070692116288 spec.py:321] Evaluating on the training split.
I0201 16:18:08.787455 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 16:18:17.417185 140070692116288 spec.py:349] Evaluating on the test split.
I0201 16:18:19.996911 140070692116288 submission_runner.py:408] Time since start: 25916.45s, 	Step: 74132, 	{'train/accuracy': 0.6875796914100647, 'train/loss': 1.2316724061965942, 'validation/accuracy': 0.6329799890518188, 'validation/loss': 1.49374258518219, 'validation/num_examples': 50000, 'test/accuracy': 0.5053000450134277, 'test/loss': 2.2189886569976807, 'test/num_examples': 10000, 'score': 25026.766946792603, 'total_duration': 25916.44958114624, 'accumulated_submission_time': 25026.766946792603, 'accumulated_eval_time': 884.3387162685394, 'accumulated_logging_time': 2.771080732345581}
I0201 16:18:20.028468 139908710635264 logging_writer.py:48] [74132] accumulated_eval_time=884.338716, accumulated_logging_time=2.771081, accumulated_submission_time=25026.766947, global_step=74132, preemption_count=0, score=25026.766947, test/accuracy=0.505300, test/loss=2.218989, test/num_examples=10000, total_duration=25916.449581, train/accuracy=0.687580, train/loss=1.231672, validation/accuracy=0.632980, validation/loss=1.493743, validation/num_examples=50000
I0201 16:18:43.267458 139908719027968 logging_writer.py:48] [74200] global_step=74200, grad_norm=3.9901304244995117, loss=1.7219138145446777
I0201 16:19:16.924066 139908710635264 logging_writer.py:48] [74300] global_step=74300, grad_norm=4.780741214752197, loss=1.7788524627685547
I0201 16:19:50.613669 139908719027968 logging_writer.py:48] [74400] global_step=74400, grad_norm=3.9080793857574463, loss=1.6175363063812256
I0201 16:20:24.291257 139908710635264 logging_writer.py:48] [74500] global_step=74500, grad_norm=5.254029273986816, loss=1.7199864387512207
I0201 16:20:57.959450 139908719027968 logging_writer.py:48] [74600] global_step=74600, grad_norm=4.35777473449707, loss=1.785304069519043
I0201 16:21:31.622138 139908710635264 logging_writer.py:48] [74700] global_step=74700, grad_norm=4.553264141082764, loss=1.8215337991714478
I0201 16:22:05.298537 139908719027968 logging_writer.py:48] [74800] global_step=74800, grad_norm=4.173799514770508, loss=1.855676531791687
I0201 16:22:38.983757 139908710635264 logging_writer.py:48] [74900] global_step=74900, grad_norm=4.094174385070801, loss=1.730513334274292
I0201 16:23:12.648835 139908719027968 logging_writer.py:48] [75000] global_step=75000, grad_norm=4.2950119972229, loss=1.8875921964645386
I0201 16:23:46.306917 139908710635264 logging_writer.py:48] [75100] global_step=75100, grad_norm=4.134335041046143, loss=1.8209054470062256
I0201 16:24:20.059725 139908719027968 logging_writer.py:48] [75200] global_step=75200, grad_norm=4.3234052658081055, loss=1.6812077760696411
I0201 16:24:53.752914 139908710635264 logging_writer.py:48] [75300] global_step=75300, grad_norm=4.823652267456055, loss=1.6992183923721313
I0201 16:25:27.444925 139908719027968 logging_writer.py:48] [75400] global_step=75400, grad_norm=4.017220973968506, loss=1.7082436084747314
I0201 16:26:01.104471 139908710635264 logging_writer.py:48] [75500] global_step=75500, grad_norm=4.242379665374756, loss=1.8907698392868042
I0201 16:26:34.793910 139908719027968 logging_writer.py:48] [75600] global_step=75600, grad_norm=4.698177814483643, loss=1.8393182754516602
I0201 16:26:50.093181 140070692116288 spec.py:321] Evaluating on the training split.
I0201 16:26:56.374761 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 16:27:05.117491 140070692116288 spec.py:349] Evaluating on the test split.
I0201 16:27:07.738543 140070692116288 submission_runner.py:408] Time since start: 26444.19s, 	Step: 75647, 	{'train/accuracy': 0.6935586333274841, 'train/loss': 1.2051820755004883, 'validation/accuracy': 0.6389399766921997, 'validation/loss': 1.4845311641693115, 'validation/num_examples': 50000, 'test/accuracy': 0.5136000514030457, 'test/loss': 2.221004009246826, 'test/num_examples': 10000, 'score': 25536.77026438713, 'total_duration': 26444.191212654114, 'accumulated_submission_time': 25536.77026438713, 'accumulated_eval_time': 901.9840440750122, 'accumulated_logging_time': 2.8116519451141357}
I0201 16:27:07.768063 139907754342144 logging_writer.py:48] [75647] accumulated_eval_time=901.984044, accumulated_logging_time=2.811652, accumulated_submission_time=25536.770264, global_step=75647, preemption_count=0, score=25536.770264, test/accuracy=0.513600, test/loss=2.221004, test/num_examples=10000, total_duration=26444.191213, train/accuracy=0.693559, train/loss=1.205182, validation/accuracy=0.638940, validation/loss=1.484531, validation/num_examples=50000
I0201 16:27:25.906895 139907762734848 logging_writer.py:48] [75700] global_step=75700, grad_norm=3.953083038330078, loss=1.814159631729126
I0201 16:27:59.567549 139907754342144 logging_writer.py:48] [75800] global_step=75800, grad_norm=6.167158603668213, loss=1.7571808099746704
I0201 16:28:33.236031 139907762734848 logging_writer.py:48] [75900] global_step=75900, grad_norm=4.108558177947998, loss=1.6725059747695923
I0201 16:29:06.916302 139907754342144 logging_writer.py:48] [76000] global_step=76000, grad_norm=4.782526969909668, loss=1.8082866668701172
I0201 16:29:40.577991 139907762734848 logging_writer.py:48] [76100] global_step=76100, grad_norm=4.0070481300354, loss=1.7288464307785034
I0201 16:30:14.245092 139907754342144 logging_writer.py:48] [76200] global_step=76200, grad_norm=4.106376647949219, loss=1.906853437423706
I0201 16:30:47.971199 139907762734848 logging_writer.py:48] [76300] global_step=76300, grad_norm=4.279028415679932, loss=1.7203150987625122
I0201 16:31:21.663495 139907754342144 logging_writer.py:48] [76400] global_step=76400, grad_norm=5.554087162017822, loss=1.7671585083007812
I0201 16:31:55.341022 139907762734848 logging_writer.py:48] [76500] global_step=76500, grad_norm=4.232010841369629, loss=1.7932432889938354
I0201 16:32:29.003879 139907754342144 logging_writer.py:48] [76600] global_step=76600, grad_norm=4.689270973205566, loss=1.7468732595443726
I0201 16:33:02.652303 139907762734848 logging_writer.py:48] [76700] global_step=76700, grad_norm=3.5824124813079834, loss=1.7008967399597168
I0201 16:33:36.331940 139907754342144 logging_writer.py:48] [76800] global_step=76800, grad_norm=4.321381568908691, loss=1.7776820659637451
I0201 16:34:09.991891 139907762734848 logging_writer.py:48] [76900] global_step=76900, grad_norm=3.994493246078491, loss=1.7078578472137451
I0201 16:34:43.651995 139907754342144 logging_writer.py:48] [77000] global_step=77000, grad_norm=4.058593273162842, loss=1.7431262731552124
I0201 16:35:17.313687 139907762734848 logging_writer.py:48] [77100] global_step=77100, grad_norm=4.044625282287598, loss=1.8014872074127197
I0201 16:35:38.015350 140070692116288 spec.py:321] Evaluating on the training split.
I0201 16:35:44.324148 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 16:35:53.156330 140070692116288 spec.py:349] Evaluating on the test split.
I0201 16:35:55.773195 140070692116288 submission_runner.py:408] Time since start: 26972.23s, 	Step: 77163, 	{'train/accuracy': 0.6889349222183228, 'train/loss': 1.2334457635879517, 'validation/accuracy': 0.6387199759483337, 'validation/loss': 1.485427737236023, 'validation/num_examples': 50000, 'test/accuracy': 0.5131000280380249, 'test/loss': 2.2043116092681885, 'test/num_examples': 10000, 'score': 26046.955763578415, 'total_duration': 26972.225853919983, 'accumulated_submission_time': 26046.955763578415, 'accumulated_eval_time': 919.7418501377106, 'accumulated_logging_time': 2.8506574630737305}
I0201 16:35:55.810774 139907729164032 logging_writer.py:48] [77163] accumulated_eval_time=919.741850, accumulated_logging_time=2.850657, accumulated_submission_time=26046.955764, global_step=77163, preemption_count=0, score=26046.955764, test/accuracy=0.513100, test/loss=2.204312, test/num_examples=10000, total_duration=26972.225854, train/accuracy=0.688935, train/loss=1.233446, validation/accuracy=0.638720, validation/loss=1.485428, validation/num_examples=50000
I0201 16:36:08.600652 139907737556736 logging_writer.py:48] [77200] global_step=77200, grad_norm=3.9870975017547607, loss=1.711574673652649
I0201 16:36:42.259094 139907729164032 logging_writer.py:48] [77300] global_step=77300, grad_norm=4.557324409484863, loss=1.8518823385238647
I0201 16:37:15.974396 139907737556736 logging_writer.py:48] [77400] global_step=77400, grad_norm=4.480950832366943, loss=1.7433140277862549
I0201 16:37:49.648965 139907729164032 logging_writer.py:48] [77500] global_step=77500, grad_norm=4.1592278480529785, loss=1.7235933542251587
I0201 16:38:23.298395 139907737556736 logging_writer.py:48] [77600] global_step=77600, grad_norm=4.0906829833984375, loss=1.8333311080932617
I0201 16:38:56.964509 139907729164032 logging_writer.py:48] [77700] global_step=77700, grad_norm=4.372147083282471, loss=1.7149232625961304
I0201 16:39:30.621400 139907737556736 logging_writer.py:48] [77800] global_step=77800, grad_norm=3.6692326068878174, loss=1.6492438316345215
I0201 16:40:04.310957 139907729164032 logging_writer.py:48] [77900] global_step=77900, grad_norm=4.058997631072998, loss=1.7313002347946167
I0201 16:40:37.954162 139907737556736 logging_writer.py:48] [78000] global_step=78000, grad_norm=4.040206432342529, loss=1.6650933027267456
I0201 16:41:11.640332 139907729164032 logging_writer.py:48] [78100] global_step=78100, grad_norm=4.139091491699219, loss=1.789087176322937
I0201 16:41:45.293049 139907737556736 logging_writer.py:48] [78200] global_step=78200, grad_norm=4.1354289054870605, loss=1.7926450967788696
I0201 16:42:18.984620 139907729164032 logging_writer.py:48] [78300] global_step=78300, grad_norm=3.776564359664917, loss=1.6747794151306152
I0201 16:42:52.637246 139907737556736 logging_writer.py:48] [78400] global_step=78400, grad_norm=4.120289325714111, loss=1.7515610456466675
I0201 16:43:26.375967 139907729164032 logging_writer.py:48] [78500] global_step=78500, grad_norm=4.1796417236328125, loss=1.7425377368927002
I0201 16:43:59.983086 139907737556736 logging_writer.py:48] [78600] global_step=78600, grad_norm=4.608006000518799, loss=1.7052736282348633
I0201 16:44:26.061215 140070692116288 spec.py:321] Evaluating on the training split.
I0201 16:44:32.395831 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 16:44:41.001348 140070692116288 spec.py:349] Evaluating on the test split.
I0201 16:44:43.697899 140070692116288 submission_runner.py:408] Time since start: 27500.15s, 	Step: 78679, 	{'train/accuracy': 0.721121609210968, 'train/loss': 1.0787429809570312, 'validation/accuracy': 0.6339199542999268, 'validation/loss': 1.5018742084503174, 'validation/num_examples': 50000, 'test/accuracy': 0.5126000046730042, 'test/loss': 2.209225654602051, 'test/num_examples': 10000, 'score': 26557.14439845085, 'total_duration': 27500.150496721268, 'accumulated_submission_time': 26557.14439845085, 'accumulated_eval_time': 937.3784308433533, 'accumulated_logging_time': 2.8979365825653076}
I0201 16:44:43.728154 139907754342144 logging_writer.py:48] [78679] accumulated_eval_time=937.378431, accumulated_logging_time=2.897937, accumulated_submission_time=26557.144398, global_step=78679, preemption_count=0, score=26557.144398, test/accuracy=0.512600, test/loss=2.209226, test/num_examples=10000, total_duration=27500.150497, train/accuracy=0.721122, train/loss=1.078743, validation/accuracy=0.633920, validation/loss=1.501874, validation/num_examples=50000
I0201 16:44:51.130654 139907762734848 logging_writer.py:48] [78700] global_step=78700, grad_norm=4.3491435050964355, loss=1.721187949180603
I0201 16:45:24.768173 139907754342144 logging_writer.py:48] [78800] global_step=78800, grad_norm=4.727345943450928, loss=1.78401780128479
I0201 16:45:58.452066 139907762734848 logging_writer.py:48] [78900] global_step=78900, grad_norm=4.137964248657227, loss=1.7946053743362427
I0201 16:46:32.111034 139907754342144 logging_writer.py:48] [79000] global_step=79000, grad_norm=4.526200771331787, loss=1.809514045715332
I0201 16:47:05.780333 139907762734848 logging_writer.py:48] [79100] global_step=79100, grad_norm=3.6492652893066406, loss=1.6426610946655273
I0201 16:47:39.433709 139907754342144 logging_writer.py:48] [79200] global_step=79200, grad_norm=4.080785751342773, loss=1.623837947845459
I0201 16:48:13.106561 139907762734848 logging_writer.py:48] [79300] global_step=79300, grad_norm=4.524228572845459, loss=1.924051284790039
I0201 16:48:46.766537 139907754342144 logging_writer.py:48] [79400] global_step=79400, grad_norm=4.0823869705200195, loss=1.7181510925292969
I0201 16:49:20.434355 139907762734848 logging_writer.py:48] [79500] global_step=79500, grad_norm=4.133951187133789, loss=1.5926363468170166
I0201 16:49:54.155085 139907754342144 logging_writer.py:48] [79600] global_step=79600, grad_norm=3.4867756366729736, loss=1.5938175916671753
I0201 16:50:27.859744 139907762734848 logging_writer.py:48] [79700] global_step=79700, grad_norm=4.199111461639404, loss=1.687644600868225
I0201 16:51:01.525904 139907754342144 logging_writer.py:48] [79800] global_step=79800, grad_norm=4.207174301147461, loss=1.7268579006195068
I0201 16:51:35.212565 139907762734848 logging_writer.py:48] [79900] global_step=79900, grad_norm=3.6719753742218018, loss=1.7358342409133911
I0201 16:52:08.869354 139907754342144 logging_writer.py:48] [80000] global_step=80000, grad_norm=3.8885276317596436, loss=1.696869134902954
I0201 16:52:42.559508 139907762734848 logging_writer.py:48] [80100] global_step=80100, grad_norm=3.704624652862549, loss=1.7421674728393555
I0201 16:53:13.983997 140070692116288 spec.py:321] Evaluating on the training split.
I0201 16:53:20.218924 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 16:53:28.882153 140070692116288 spec.py:349] Evaluating on the test split.
I0201 16:53:31.527700 140070692116288 submission_runner.py:408] Time since start: 28027.98s, 	Step: 80195, 	{'train/accuracy': 0.7099210619926453, 'train/loss': 1.1163970232009888, 'validation/accuracy': 0.6413599848747253, 'validation/loss': 1.4619232416152954, 'validation/num_examples': 50000, 'test/accuracy': 0.5139999985694885, 'test/loss': 2.207616090774536, 'test/num_examples': 10000, 'score': 27067.337899446487, 'total_duration': 28027.980364322662, 'accumulated_submission_time': 27067.337899446487, 'accumulated_eval_time': 954.9220995903015, 'accumulated_logging_time': 2.937781572341919}
I0201 16:53:31.560281 139907754342144 logging_writer.py:48] [80195] accumulated_eval_time=954.922100, accumulated_logging_time=2.937782, accumulated_submission_time=27067.337899, global_step=80195, preemption_count=0, score=27067.337899, test/accuracy=0.514000, test/loss=2.207616, test/num_examples=10000, total_duration=28027.980364, train/accuracy=0.709921, train/loss=1.116397, validation/accuracy=0.641360, validation/loss=1.461923, validation/num_examples=50000
I0201 16:53:33.595321 139908710635264 logging_writer.py:48] [80200] global_step=80200, grad_norm=4.224364757537842, loss=1.6954619884490967
I0201 16:54:07.212338 139907754342144 logging_writer.py:48] [80300] global_step=80300, grad_norm=4.514423370361328, loss=1.6929646730422974
I0201 16:54:40.924090 139908710635264 logging_writer.py:48] [80400] global_step=80400, grad_norm=5.367366790771484, loss=1.809936761856079
I0201 16:55:14.584357 139907754342144 logging_writer.py:48] [80500] global_step=80500, grad_norm=4.248471736907959, loss=1.7740650177001953
I0201 16:55:48.281469 139908710635264 logging_writer.py:48] [80600] global_step=80600, grad_norm=4.413666248321533, loss=1.7157469987869263
I0201 16:56:22.050811 139907754342144 logging_writer.py:48] [80700] global_step=80700, grad_norm=4.316939353942871, loss=1.6978967189788818
I0201 16:56:55.760335 139908710635264 logging_writer.py:48] [80800] global_step=80800, grad_norm=4.991605758666992, loss=1.8078687191009521
I0201 16:57:29.440112 139907754342144 logging_writer.py:48] [80900] global_step=80900, grad_norm=4.4128193855285645, loss=1.7086539268493652
I0201 16:58:03.135553 139908710635264 logging_writer.py:48] [81000] global_step=81000, grad_norm=4.134921073913574, loss=1.7928454875946045
I0201 16:58:36.789758 139907754342144 logging_writer.py:48] [81100] global_step=81100, grad_norm=5.311637878417969, loss=1.7973601818084717
I0201 16:59:10.476576 139908710635264 logging_writer.py:48] [81200] global_step=81200, grad_norm=4.529269218444824, loss=1.7565020322799683
I0201 16:59:44.148011 139907754342144 logging_writer.py:48] [81300] global_step=81300, grad_norm=6.583836555480957, loss=1.7605592012405396
I0201 17:00:17.830141 139908710635264 logging_writer.py:48] [81400] global_step=81400, grad_norm=3.9619977474212646, loss=1.8155217170715332
I0201 17:00:51.471342 139907754342144 logging_writer.py:48] [81500] global_step=81500, grad_norm=4.69464635848999, loss=1.6932973861694336
I0201 17:01:25.151319 139908710635264 logging_writer.py:48] [81600] global_step=81600, grad_norm=5.049510478973389, loss=1.652187705039978
I0201 17:01:58.816851 139907754342144 logging_writer.py:48] [81700] global_step=81700, grad_norm=4.151739597320557, loss=1.819313883781433
I0201 17:02:01.670096 140070692116288 spec.py:321] Evaluating on the training split.
I0201 17:02:08.060371 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 17:02:16.612785 140070692116288 spec.py:349] Evaluating on the test split.
I0201 17:02:19.537756 140070692116288 submission_runner.py:408] Time since start: 28555.99s, 	Step: 81710, 	{'train/accuracy': 0.7057955861091614, 'train/loss': 1.155988097190857, 'validation/accuracy': 0.6452400088310242, 'validation/loss': 1.4618548154830933, 'validation/num_examples': 50000, 'test/accuracy': 0.5199000239372253, 'test/loss': 2.1657211780548096, 'test/num_examples': 10000, 'score': 27577.38576722145, 'total_duration': 28555.9904255867, 'accumulated_submission_time': 27577.38576722145, 'accumulated_eval_time': 972.7897419929504, 'accumulated_logging_time': 2.9804928302764893}
I0201 17:02:19.570199 139907745949440 logging_writer.py:48] [81710] accumulated_eval_time=972.789742, accumulated_logging_time=2.980493, accumulated_submission_time=27577.385767, global_step=81710, preemption_count=0, score=27577.385767, test/accuracy=0.519900, test/loss=2.165721, test/num_examples=10000, total_duration=28555.990426, train/accuracy=0.705796, train/loss=1.155988, validation/accuracy=0.645240, validation/loss=1.461855, validation/num_examples=50000
I0201 17:02:50.147645 139907762734848 logging_writer.py:48] [81800] global_step=81800, grad_norm=4.586506366729736, loss=1.6723084449768066
I0201 17:03:23.798036 139907745949440 logging_writer.py:48] [81900] global_step=81900, grad_norm=4.17957878112793, loss=1.6542787551879883
I0201 17:03:57.482587 139907762734848 logging_writer.py:48] [82000] global_step=82000, grad_norm=4.631239414215088, loss=1.6319457292556763
I0201 17:04:31.130644 139907745949440 logging_writer.py:48] [82100] global_step=82100, grad_norm=4.695829391479492, loss=1.7859208583831787
I0201 17:05:04.806381 139907762734848 logging_writer.py:48] [82200] global_step=82200, grad_norm=4.6063714027404785, loss=1.7240536212921143
I0201 17:05:38.485050 139907745949440 logging_writer.py:48] [82300] global_step=82300, grad_norm=4.067366123199463, loss=1.7653510570526123
I0201 17:06:12.160227 139907762734848 logging_writer.py:48] [82400] global_step=82400, grad_norm=4.806756496429443, loss=1.636033296585083
I0201 17:06:45.819056 139907745949440 logging_writer.py:48] [82500] global_step=82500, grad_norm=3.81839656829834, loss=1.6616090536117554
I0201 17:07:19.465266 139907762734848 logging_writer.py:48] [82600] global_step=82600, grad_norm=4.149588584899902, loss=1.7881550788879395
I0201 17:07:53.149214 139907745949440 logging_writer.py:48] [82700] global_step=82700, grad_norm=3.9601855278015137, loss=1.7016652822494507
I0201 17:08:26.818705 139907762734848 logging_writer.py:48] [82800] global_step=82800, grad_norm=4.10014533996582, loss=1.8110178709030151
I0201 17:09:00.585050 139907745949440 logging_writer.py:48] [82900] global_step=82900, grad_norm=4.7408013343811035, loss=1.6459951400756836
I0201 17:09:34.266642 139907762734848 logging_writer.py:48] [83000] global_step=83000, grad_norm=4.414911270141602, loss=1.8098140954971313
I0201 17:10:07.921757 139907745949440 logging_writer.py:48] [83100] global_step=83100, grad_norm=4.3020219802856445, loss=1.7668700218200684
I0201 17:10:41.610422 139907762734848 logging_writer.py:48] [83200] global_step=83200, grad_norm=4.453429222106934, loss=1.6144057512283325
I0201 17:10:49.833686 140070692116288 spec.py:321] Evaluating on the training split.
I0201 17:10:56.065304 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 17:11:05.279123 140070692116288 spec.py:349] Evaluating on the test split.
I0201 17:11:07.905220 140070692116288 submission_runner.py:408] Time since start: 29084.36s, 	Step: 83226, 	{'train/accuracy': 0.6995774507522583, 'train/loss': 1.173744559288025, 'validation/accuracy': 0.6433599591255188, 'validation/loss': 1.4629638195037842, 'validation/num_examples': 50000, 'test/accuracy': 0.5162000060081482, 'test/loss': 2.1647984981536865, 'test/num_examples': 10000, 'score': 28087.586535692215, 'total_duration': 29084.35788321495, 'accumulated_submission_time': 28087.586535692215, 'accumulated_eval_time': 990.8612344264984, 'accumulated_logging_time': 3.0235769748687744}
I0201 17:11:07.937604 139908710635264 logging_writer.py:48] [83226] accumulated_eval_time=990.861234, accumulated_logging_time=3.023577, accumulated_submission_time=28087.586536, global_step=83226, preemption_count=0, score=28087.586536, test/accuracy=0.516200, test/loss=2.164798, test/num_examples=10000, total_duration=29084.357883, train/accuracy=0.699577, train/loss=1.173745, validation/accuracy=0.643360, validation/loss=1.462964, validation/num_examples=50000
I0201 17:11:33.224782 139908727420672 logging_writer.py:48] [83300] global_step=83300, grad_norm=4.157793045043945, loss=1.6840304136276245
I0201 17:12:06.884798 139908710635264 logging_writer.py:48] [83400] global_step=83400, grad_norm=3.8612260818481445, loss=1.6656465530395508
I0201 17:12:40.606590 139908727420672 logging_writer.py:48] [83500] global_step=83500, grad_norm=4.500246524810791, loss=1.680377721786499
I0201 17:13:14.272800 139908710635264 logging_writer.py:48] [83600] global_step=83600, grad_norm=3.891223907470703, loss=1.745782494544983
I0201 17:13:47.956193 139908727420672 logging_writer.py:48] [83700] global_step=83700, grad_norm=4.693080425262451, loss=1.7117341756820679
I0201 17:14:21.616644 139908710635264 logging_writer.py:48] [83800] global_step=83800, grad_norm=4.347626209259033, loss=1.757239818572998
I0201 17:14:55.320314 139908727420672 logging_writer.py:48] [83900] global_step=83900, grad_norm=4.426828861236572, loss=1.6671664714813232
I0201 17:15:29.074954 139908710635264 logging_writer.py:48] [84000] global_step=84000, grad_norm=3.8715884685516357, loss=1.8095316886901855
I0201 17:16:02.767467 139908727420672 logging_writer.py:48] [84100] global_step=84100, grad_norm=4.354244709014893, loss=1.8194407224655151
I0201 17:16:36.427240 139908710635264 logging_writer.py:48] [84200] global_step=84200, grad_norm=4.429484844207764, loss=1.636786699295044
I0201 17:17:10.127369 139908727420672 logging_writer.py:48] [84300] global_step=84300, grad_norm=3.642115831375122, loss=1.6503487825393677
I0201 17:17:43.779936 139908710635264 logging_writer.py:48] [84400] global_step=84400, grad_norm=4.166665077209473, loss=1.6944109201431274
I0201 17:18:17.473313 139908727420672 logging_writer.py:48] [84500] global_step=84500, grad_norm=3.9042513370513916, loss=1.6884695291519165
I0201 17:18:51.112001 139908710635264 logging_writer.py:48] [84600] global_step=84600, grad_norm=4.14456033706665, loss=1.6588703393936157
I0201 17:19:24.807431 139908727420672 logging_writer.py:48] [84700] global_step=84700, grad_norm=4.539700031280518, loss=1.7079862356185913
I0201 17:19:38.097759 140070692116288 spec.py:321] Evaluating on the training split.
I0201 17:19:44.364113 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 17:19:53.282805 140070692116288 spec.py:349] Evaluating on the test split.
I0201 17:19:55.853138 140070692116288 submission_runner.py:408] Time since start: 29612.31s, 	Step: 84741, 	{'train/accuracy': 0.6940369606018066, 'train/loss': 1.209545612335205, 'validation/accuracy': 0.6385200023651123, 'validation/loss': 1.4816385507583618, 'validation/num_examples': 50000, 'test/accuracy': 0.5128000378608704, 'test/loss': 2.1985490322113037, 'test/num_examples': 10000, 'score': 28597.68378353119, 'total_duration': 29612.305801153183, 'accumulated_submission_time': 28597.68378353119, 'accumulated_eval_time': 1008.6165888309479, 'accumulated_logging_time': 3.065901756286621}
I0201 17:19:55.894367 139907762734848 logging_writer.py:48] [84741] accumulated_eval_time=1008.616589, accumulated_logging_time=3.065902, accumulated_submission_time=28597.683784, global_step=84741, preemption_count=0, score=28597.683784, test/accuracy=0.512800, test/loss=2.198549, test/num_examples=10000, total_duration=29612.305801, train/accuracy=0.694037, train/loss=1.209546, validation/accuracy=0.638520, validation/loss=1.481639, validation/num_examples=50000
I0201 17:20:17.043615 139908425447168 logging_writer.py:48] [84800] global_step=84800, grad_norm=4.502203941345215, loss=1.6673821210861206
I0201 17:20:50.727073 139907762734848 logging_writer.py:48] [84900] global_step=84900, grad_norm=4.654326438903809, loss=1.7529213428497314
I0201 17:21:24.474760 139908425447168 logging_writer.py:48] [85000] global_step=85000, grad_norm=3.8866477012634277, loss=1.7588510513305664
I0201 17:21:58.122734 139907762734848 logging_writer.py:48] [85100] global_step=85100, grad_norm=4.449528694152832, loss=1.7199162244796753
I0201 17:22:31.813515 139908425447168 logging_writer.py:48] [85200] global_step=85200, grad_norm=5.819408893585205, loss=1.714617371559143
I0201 17:23:05.458380 139907762734848 logging_writer.py:48] [85300] global_step=85300, grad_norm=4.0774126052856445, loss=1.7005221843719482
I0201 17:23:39.131627 139908425447168 logging_writer.py:48] [85400] global_step=85400, grad_norm=3.9485855102539062, loss=1.6545624732971191
I0201 17:24:12.794219 139907762734848 logging_writer.py:48] [85500] global_step=85500, grad_norm=5.711665153503418, loss=1.8006188869476318
I0201 17:24:46.463196 139908425447168 logging_writer.py:48] [85600] global_step=85600, grad_norm=5.440627098083496, loss=1.7364641427993774
I0201 17:25:20.142460 139907762734848 logging_writer.py:48] [85700] global_step=85700, grad_norm=4.402862071990967, loss=1.7378636598587036
I0201 17:25:53.808053 139908425447168 logging_writer.py:48] [85800] global_step=85800, grad_norm=4.411390781402588, loss=1.683136224746704
I0201 17:26:27.471474 139907762734848 logging_writer.py:48] [85900] global_step=85900, grad_norm=4.660726547241211, loss=1.650093674659729
I0201 17:27:01.137213 139908425447168 logging_writer.py:48] [86000] global_step=86000, grad_norm=4.065703868865967, loss=1.715226650238037
I0201 17:27:34.804883 139907762734848 logging_writer.py:48] [86100] global_step=86100, grad_norm=4.545862197875977, loss=1.7928475141525269
I0201 17:28:08.526437 139908425447168 logging_writer.py:48] [86200] global_step=86200, grad_norm=4.982448101043701, loss=1.6937978267669678
I0201 17:28:25.882504 140070692116288 spec.py:321] Evaluating on the training split.
I0201 17:28:32.262139 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 17:28:41.157436 140070692116288 spec.py:349] Evaluating on the test split.
I0201 17:28:43.789740 140070692116288 submission_runner.py:408] Time since start: 30140.24s, 	Step: 86253, 	{'train/accuracy': 0.6893335580825806, 'train/loss': 1.2253234386444092, 'validation/accuracy': 0.6370799541473389, 'validation/loss': 1.4862534999847412, 'validation/num_examples': 50000, 'test/accuracy': 0.5186000466346741, 'test/loss': 2.212566614151001, 'test/num_examples': 10000, 'score': 29106.643271446228, 'total_duration': 30140.242399692535, 'accumulated_submission_time': 29106.643271446228, 'accumulated_eval_time': 1026.5237970352173, 'accumulated_logging_time': 4.082794904708862}
I0201 17:28:43.821712 139907737556736 logging_writer.py:48] [86253] accumulated_eval_time=1026.523797, accumulated_logging_time=4.082795, accumulated_submission_time=29106.643271, global_step=86253, preemption_count=0, score=29106.643271, test/accuracy=0.518600, test/loss=2.212567, test/num_examples=10000, total_duration=30140.242400, train/accuracy=0.689334, train/loss=1.225323, validation/accuracy=0.637080, validation/loss=1.486253, validation/num_examples=50000
I0201 17:28:59.987919 139907745949440 logging_writer.py:48] [86300] global_step=86300, grad_norm=4.4790425300598145, loss=1.7528693675994873
I0201 17:29:33.678539 139907737556736 logging_writer.py:48] [86400] global_step=86400, grad_norm=4.884271621704102, loss=1.7680202722549438
I0201 17:30:07.332659 139907745949440 logging_writer.py:48] [86500] global_step=86500, grad_norm=4.634054183959961, loss=1.7256686687469482
I0201 17:30:41.038864 139907737556736 logging_writer.py:48] [86600] global_step=86600, grad_norm=3.9590694904327393, loss=1.61701500415802
I0201 17:31:14.686250 139907745949440 logging_writer.py:48] [86700] global_step=86700, grad_norm=4.625003814697266, loss=1.6700334548950195
I0201 17:31:48.396981 139907737556736 logging_writer.py:48] [86800] global_step=86800, grad_norm=4.425586223602295, loss=1.7479794025421143
I0201 17:32:22.055592 139907745949440 logging_writer.py:48] [86900] global_step=86900, grad_norm=4.179195404052734, loss=1.7034928798675537
I0201 17:32:55.727665 139907737556736 logging_writer.py:48] [87000] global_step=87000, grad_norm=4.309010028839111, loss=1.621929407119751
I0201 17:33:29.367131 139907745949440 logging_writer.py:48] [87100] global_step=87100, grad_norm=4.387655258178711, loss=1.8406695127487183
I0201 17:34:03.100616 139907737556736 logging_writer.py:48] [87200] global_step=87200, grad_norm=3.891125440597534, loss=1.6542387008666992
I0201 17:34:36.804657 139907745949440 logging_writer.py:48] [87300] global_step=87300, grad_norm=3.8415050506591797, loss=1.6632335186004639
I0201 17:35:10.504396 139907737556736 logging_writer.py:48] [87400] global_step=87400, grad_norm=4.002966403961182, loss=1.6159381866455078
I0201 17:35:44.157270 139907745949440 logging_writer.py:48] [87500] global_step=87500, grad_norm=4.198643207550049, loss=1.6888306140899658
I0201 17:36:17.852556 139907737556736 logging_writer.py:48] [87600] global_step=87600, grad_norm=4.480259418487549, loss=1.6906548738479614
I0201 17:36:51.500856 139907745949440 logging_writer.py:48] [87700] global_step=87700, grad_norm=4.219952583312988, loss=1.7489655017852783
I0201 17:37:13.880113 140070692116288 spec.py:321] Evaluating on the training split.
I0201 17:37:20.169924 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 17:37:28.954883 140070692116288 spec.py:349] Evaluating on the test split.
I0201 17:37:31.578622 140070692116288 submission_runner.py:408] Time since start: 30668.03s, 	Step: 87768, 	{'train/accuracy': 0.7394770383834839, 'train/loss': 1.004119873046875, 'validation/accuracy': 0.6474599838256836, 'validation/loss': 1.4325826168060303, 'validation/num_examples': 50000, 'test/accuracy': 0.5189000368118286, 'test/loss': 2.1587750911712646, 'test/num_examples': 10000, 'score': 29616.63855457306, 'total_duration': 30668.031265974045, 'accumulated_submission_time': 29616.63855457306, 'accumulated_eval_time': 1044.2222499847412, 'accumulated_logging_time': 4.124652862548828}
I0201 17:37:31.611000 139908710635264 logging_writer.py:48] [87768] accumulated_eval_time=1044.222250, accumulated_logging_time=4.124653, accumulated_submission_time=29616.638555, global_step=87768, preemption_count=0, score=29616.638555, test/accuracy=0.518900, test/loss=2.158775, test/num_examples=10000, total_duration=30668.031266, train/accuracy=0.739477, train/loss=1.004120, validation/accuracy=0.647460, validation/loss=1.432583, validation/num_examples=50000
I0201 17:37:42.741388 139908719027968 logging_writer.py:48] [87800] global_step=87800, grad_norm=4.3439621925354, loss=1.7927539348602295
I0201 17:38:16.384298 139908710635264 logging_writer.py:48] [87900] global_step=87900, grad_norm=4.626500606536865, loss=1.6767216920852661
I0201 17:38:50.069895 139908719027968 logging_writer.py:48] [88000] global_step=88000, grad_norm=4.549987316131592, loss=1.6978974342346191
I0201 17:39:23.762915 139908710635264 logging_writer.py:48] [88100] global_step=88100, grad_norm=3.886263847351074, loss=1.625920295715332
I0201 17:39:57.441860 139908719027968 logging_writer.py:48] [88200] global_step=88200, grad_norm=4.46222448348999, loss=1.775286316871643
I0201 17:40:31.141986 139908710635264 logging_writer.py:48] [88300] global_step=88300, grad_norm=3.8671083450317383, loss=1.6769137382507324
I0201 17:41:04.786973 139908719027968 logging_writer.py:48] [88400] global_step=88400, grad_norm=4.661489486694336, loss=1.6220957040786743
I0201 17:41:38.468408 139908710635264 logging_writer.py:48] [88500] global_step=88500, grad_norm=4.626904487609863, loss=1.648144245147705
I0201 17:42:12.125235 139908719027968 logging_writer.py:48] [88600] global_step=88600, grad_norm=4.384669780731201, loss=1.6403813362121582
I0201 17:42:45.783703 139908710635264 logging_writer.py:48] [88700] global_step=88700, grad_norm=4.560548305511475, loss=1.7027454376220703
I0201 17:43:19.448735 139908719027968 logging_writer.py:48] [88800] global_step=88800, grad_norm=4.570484161376953, loss=1.6835932731628418
I0201 17:43:53.131711 139908710635264 logging_writer.py:48] [88900] global_step=88900, grad_norm=4.173080921173096, loss=1.7209035158157349
I0201 17:44:26.777275 139908719027968 logging_writer.py:48] [89000] global_step=89000, grad_norm=4.436875343322754, loss=1.7337141036987305
I0201 17:45:00.457808 139908710635264 logging_writer.py:48] [89100] global_step=89100, grad_norm=4.897724628448486, loss=1.601923942565918
I0201 17:45:34.126052 139908719027968 logging_writer.py:48] [89200] global_step=89200, grad_norm=5.021671295166016, loss=1.6391029357910156
I0201 17:46:01.580763 140070692116288 spec.py:321] Evaluating on the training split.
I0201 17:46:07.869703 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 17:46:16.407927 140070692116288 spec.py:349] Evaluating on the test split.
I0201 17:46:18.983075 140070692116288 submission_runner.py:408] Time since start: 31195.44s, 	Step: 89283, 	{'train/accuracy': 0.7106584906578064, 'train/loss': 1.1320785284042358, 'validation/accuracy': 0.644819974899292, 'validation/loss': 1.4565682411193848, 'validation/num_examples': 50000, 'test/accuracy': 0.5223000049591064, 'test/loss': 2.1549034118652344, 'test/num_examples': 10000, 'score': 30126.547045707703, 'total_duration': 31195.43574333191, 'accumulated_submission_time': 30126.547045707703, 'accumulated_eval_time': 1061.6245312690735, 'accumulated_logging_time': 4.165873289108276}
I0201 17:46:19.015067 139907754342144 logging_writer.py:48] [89283] accumulated_eval_time=1061.624531, accumulated_logging_time=4.165873, accumulated_submission_time=30126.547046, global_step=89283, preemption_count=0, score=30126.547046, test/accuracy=0.522300, test/loss=2.154903, test/num_examples=10000, total_duration=31195.435743, train/accuracy=0.710658, train/loss=1.132079, validation/accuracy=0.644820, validation/loss=1.456568, validation/num_examples=50000
I0201 17:46:25.085000 139907762734848 logging_writer.py:48] [89300] global_step=89300, grad_norm=4.312596797943115, loss=1.5772428512573242
I0201 17:46:58.763102 139907754342144 logging_writer.py:48] [89400] global_step=89400, grad_norm=4.998669624328613, loss=1.6555330753326416
I0201 17:47:32.459310 139907762734848 logging_writer.py:48] [89500] global_step=89500, grad_norm=4.335521697998047, loss=1.6637709140777588
I0201 17:48:06.127259 139907754342144 logging_writer.py:48] [89600] global_step=89600, grad_norm=4.440581798553467, loss=1.6639854907989502
I0201 17:48:39.824423 139907762734848 logging_writer.py:48] [89700] global_step=89700, grad_norm=3.830894947052002, loss=1.7324742078781128
I0201 17:49:13.465398 139907754342144 logging_writer.py:48] [89800] global_step=89800, grad_norm=5.130395889282227, loss=1.7082468271255493
I0201 17:49:47.160852 139907762734848 logging_writer.py:48] [89900] global_step=89900, grad_norm=4.685492992401123, loss=1.6483677625656128
I0201 17:50:20.818488 139907754342144 logging_writer.py:48] [90000] global_step=90000, grad_norm=4.6559600830078125, loss=1.6283680200576782
I0201 17:50:54.497340 139907762734848 logging_writer.py:48] [90100] global_step=90100, grad_norm=4.151954174041748, loss=1.7094700336456299
I0201 17:51:28.146699 139907754342144 logging_writer.py:48] [90200] global_step=90200, grad_norm=4.564761638641357, loss=1.6708478927612305
I0201 17:52:01.823350 139907762734848 logging_writer.py:48] [90300] global_step=90300, grad_norm=4.5380330085754395, loss=1.7658605575561523
I0201 17:52:35.489697 139907754342144 logging_writer.py:48] [90400] global_step=90400, grad_norm=4.331417560577393, loss=1.761252999305725
I0201 17:53:09.218199 139907762734848 logging_writer.py:48] [90500] global_step=90500, grad_norm=5.863470077514648, loss=1.750698208808899
I0201 17:53:42.914198 139907754342144 logging_writer.py:48] [90600] global_step=90600, grad_norm=4.202385425567627, loss=1.5700081586837769
I0201 17:54:16.644967 139907762734848 logging_writer.py:48] [90700] global_step=90700, grad_norm=5.395526885986328, loss=1.6556062698364258
I0201 17:54:49.090526 140070692116288 spec.py:321] Evaluating on the training split.
I0201 17:54:55.407479 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 17:55:04.120201 140070692116288 spec.py:349] Evaluating on the test split.
I0201 17:55:06.629019 140070692116288 submission_runner.py:408] Time since start: 31723.08s, 	Step: 90798, 	{'train/accuracy': 0.7100805044174194, 'train/loss': 1.1253926753997803, 'validation/accuracy': 0.6494799852371216, 'validation/loss': 1.432708501815796, 'validation/num_examples': 50000, 'test/accuracy': 0.5230000019073486, 'test/loss': 2.1359827518463135, 'test/num_examples': 10000, 'score': 30636.560704946518, 'total_duration': 31723.081677913666, 'accumulated_submission_time': 30636.560704946518, 'accumulated_eval_time': 1079.1629874706268, 'accumulated_logging_time': 4.2079079151153564}
I0201 17:55:06.663419 139908425447168 logging_writer.py:48] [90798] accumulated_eval_time=1079.162987, accumulated_logging_time=4.207908, accumulated_submission_time=30636.560705, global_step=90798, preemption_count=0, score=30636.560705, test/accuracy=0.523000, test/loss=2.135983, test/num_examples=10000, total_duration=31723.081678, train/accuracy=0.710081, train/loss=1.125393, validation/accuracy=0.649480, validation/loss=1.432709, validation/num_examples=50000
I0201 17:55:07.682058 139908710635264 logging_writer.py:48] [90800] global_step=90800, grad_norm=4.367516994476318, loss=1.7341723442077637
I0201 17:55:41.260634 139908425447168 logging_writer.py:48] [90900] global_step=90900, grad_norm=5.16224479675293, loss=1.6519157886505127
I0201 17:56:14.985171 139908710635264 logging_writer.py:48] [91000] global_step=91000, grad_norm=4.562749862670898, loss=1.6048790216445923
I0201 17:56:48.655904 139908425447168 logging_writer.py:48] [91100] global_step=91100, grad_norm=4.2373948097229, loss=1.5767556428909302
I0201 17:57:22.342779 139908710635264 logging_writer.py:48] [91200] global_step=91200, grad_norm=4.822725296020508, loss=1.669950246810913
I0201 17:57:56.000182 139908425447168 logging_writer.py:48] [91300] global_step=91300, grad_norm=5.846114158630371, loss=1.6443926095962524
I0201 17:58:29.695881 139908710635264 logging_writer.py:48] [91400] global_step=91400, grad_norm=4.491710662841797, loss=1.481221318244934
I0201 17:59:03.351986 139908425447168 logging_writer.py:48] [91500] global_step=91500, grad_norm=4.68438196182251, loss=1.728674054145813
I0201 17:59:37.040110 139908710635264 logging_writer.py:48] [91600] global_step=91600, grad_norm=4.473269939422607, loss=1.6657886505126953
I0201 18:00:10.734533 139908425447168 logging_writer.py:48] [91700] global_step=91700, grad_norm=4.335239887237549, loss=1.8446158170700073
I0201 18:00:44.419201 139908710635264 logging_writer.py:48] [91800] global_step=91800, grad_norm=5.630787372589111, loss=1.652112364768982
I0201 18:01:18.101594 139908425447168 logging_writer.py:48] [91900] global_step=91900, grad_norm=4.6249189376831055, loss=1.5638628005981445
I0201 18:01:51.773557 139908710635264 logging_writer.py:48] [92000] global_step=92000, grad_norm=4.697747707366943, loss=1.575204849243164
I0201 18:02:25.427908 139908425447168 logging_writer.py:48] [92100] global_step=92100, grad_norm=4.348297595977783, loss=1.5694725513458252
I0201 18:02:59.114006 139908710635264 logging_writer.py:48] [92200] global_step=92200, grad_norm=4.576447486877441, loss=1.5537199974060059
I0201 18:03:32.759337 139908425447168 logging_writer.py:48] [92300] global_step=92300, grad_norm=5.334766864776611, loss=1.5180219411849976
I0201 18:03:36.947003 140070692116288 spec.py:321] Evaluating on the training split.
I0201 18:03:43.371512 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 18:03:51.976480 140070692116288 spec.py:349] Evaluating on the test split.
I0201 18:03:54.464254 140070692116288 submission_runner.py:408] Time since start: 32250.92s, 	Step: 92314, 	{'train/accuracy': 0.7146045565605164, 'train/loss': 1.1162675619125366, 'validation/accuracy': 0.6528800129890442, 'validation/loss': 1.4155189990997314, 'validation/num_examples': 50000, 'test/accuracy': 0.5293000340461731, 'test/loss': 2.1161322593688965, 'test/num_examples': 10000, 'score': 31146.7821393013, 'total_duration': 32250.916907072067, 'accumulated_submission_time': 31146.7821393013, 'accumulated_eval_time': 1096.6802098751068, 'accumulated_logging_time': 4.251695394515991}
I0201 18:03:54.501002 139907745949440 logging_writer.py:48] [92314] accumulated_eval_time=1096.680210, accumulated_logging_time=4.251695, accumulated_submission_time=31146.782139, global_step=92314, preemption_count=0, score=31146.782139, test/accuracy=0.529300, test/loss=2.116132, test/num_examples=10000, total_duration=32250.916907, train/accuracy=0.714605, train/loss=1.116268, validation/accuracy=0.652880, validation/loss=1.415519, validation/num_examples=50000
I0201 18:04:23.720237 139907754342144 logging_writer.py:48] [92400] global_step=92400, grad_norm=4.840389728546143, loss=1.6228809356689453
I0201 18:04:57.353194 139907745949440 logging_writer.py:48] [92500] global_step=92500, grad_norm=3.9356772899627686, loss=1.6037706136703491
I0201 18:05:31.116901 139907754342144 logging_writer.py:48] [92600] global_step=92600, grad_norm=4.6978840827941895, loss=1.6319140195846558
I0201 18:06:04.841962 139907745949440 logging_writer.py:48] [92700] global_step=92700, grad_norm=4.6406707763671875, loss=1.6445221900939941
I0201 18:06:38.528501 139907754342144 logging_writer.py:48] [92800] global_step=92800, grad_norm=4.379903316497803, loss=1.7071540355682373
I0201 18:07:12.194736 139907745949440 logging_writer.py:48] [92900] global_step=92900, grad_norm=4.696798324584961, loss=1.7089436054229736
I0201 18:07:45.886417 139907754342144 logging_writer.py:48] [93000] global_step=93000, grad_norm=4.25033712387085, loss=1.6086986064910889
I0201 18:08:19.539939 139907745949440 logging_writer.py:48] [93100] global_step=93100, grad_norm=4.432296276092529, loss=1.747927188873291
I0201 18:08:53.226804 139907754342144 logging_writer.py:48] [93200] global_step=93200, grad_norm=3.9728314876556396, loss=1.57840096950531
I0201 18:09:26.874147 139907745949440 logging_writer.py:48] [93300] global_step=93300, grad_norm=4.767332077026367, loss=1.6840033531188965
I0201 18:10:00.564330 139907754342144 logging_writer.py:48] [93400] global_step=93400, grad_norm=4.438677787780762, loss=1.7008978128433228
I0201 18:10:34.217095 139907745949440 logging_writer.py:48] [93500] global_step=93500, grad_norm=4.609375953674316, loss=1.636231780052185
I0201 18:11:07.914669 139907754342144 logging_writer.py:48] [93600] global_step=93600, grad_norm=4.809985637664795, loss=1.6247522830963135
I0201 18:11:41.576388 139907745949440 logging_writer.py:48] [93700] global_step=93700, grad_norm=5.386240482330322, loss=1.698702096939087
I0201 18:12:15.320970 139907754342144 logging_writer.py:48] [93800] global_step=93800, grad_norm=5.666016578674316, loss=1.6007486581802368
I0201 18:12:24.569091 140070692116288 spec.py:321] Evaluating on the training split.
I0201 18:12:30.810775 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 18:12:39.274087 140070692116288 spec.py:349] Evaluating on the test split.
I0201 18:12:41.953832 140070692116288 submission_runner.py:408] Time since start: 32778.41s, 	Step: 93829, 	{'train/accuracy': 0.7053571343421936, 'train/loss': 1.1537963151931763, 'validation/accuracy': 0.649679958820343, 'validation/loss': 1.422189474105835, 'validation/num_examples': 50000, 'test/accuracy': 0.5184000134468079, 'test/loss': 2.1597468852996826, 'test/num_examples': 10000, 'score': 31656.7867538929, 'total_duration': 32778.40650200844, 'accumulated_submission_time': 31656.7867538929, 'accumulated_eval_time': 1114.0649182796478, 'accumulated_logging_time': 4.299462080001831}
I0201 18:12:41.990509 139908425447168 logging_writer.py:48] [93829] accumulated_eval_time=1114.064918, accumulated_logging_time=4.299462, accumulated_submission_time=31656.786754, global_step=93829, preemption_count=0, score=31656.786754, test/accuracy=0.518400, test/loss=2.159747, test/num_examples=10000, total_duration=32778.406502, train/accuracy=0.705357, train/loss=1.153796, validation/accuracy=0.649680, validation/loss=1.422189, validation/num_examples=50000
I0201 18:13:06.183269 139908710635264 logging_writer.py:48] [93900] global_step=93900, grad_norm=4.52739143371582, loss=1.7760331630706787
I0201 18:13:39.884447 139908425447168 logging_writer.py:48] [94000] global_step=94000, grad_norm=3.863701820373535, loss=1.581514835357666
I0201 18:14:13.571619 139908710635264 logging_writer.py:48] [94100] global_step=94100, grad_norm=4.73348331451416, loss=1.6734135150909424
I0201 18:14:47.238340 139908425447168 logging_writer.py:48] [94200] global_step=94200, grad_norm=4.623351097106934, loss=1.6568504571914673
I0201 18:15:20.907689 139908710635264 logging_writer.py:48] [94300] global_step=94300, grad_norm=4.513363838195801, loss=1.7401819229125977
I0201 18:15:54.581357 139908425447168 logging_writer.py:48] [94400] global_step=94400, grad_norm=5.021273136138916, loss=1.6483490467071533
I0201 18:16:28.252244 139908710635264 logging_writer.py:48] [94500] global_step=94500, grad_norm=4.06455135345459, loss=1.6044741868972778
I0201 18:17:01.897971 139908425447168 logging_writer.py:48] [94600] global_step=94600, grad_norm=4.730434417724609, loss=1.5955125093460083
I0201 18:17:35.592097 139908710635264 logging_writer.py:48] [94700] global_step=94700, grad_norm=4.665809154510498, loss=1.631653070449829
I0201 18:18:09.329886 139908425447168 logging_writer.py:48] [94800] global_step=94800, grad_norm=5.209061145782471, loss=1.613603115081787
I0201 18:18:43.058497 139908710635264 logging_writer.py:48] [94900] global_step=94900, grad_norm=4.5840582847595215, loss=1.5940614938735962
I0201 18:19:16.724376 139908425447168 logging_writer.py:48] [95000] global_step=95000, grad_norm=4.871530055999756, loss=1.6977612972259521
I0201 18:19:50.402530 139908710635264 logging_writer.py:48] [95100] global_step=95100, grad_norm=4.742665767669678, loss=1.6067732572555542
I0201 18:20:24.044454 139908425447168 logging_writer.py:48] [95200] global_step=95200, grad_norm=4.58025598526001, loss=1.6053701639175415
I0201 18:20:57.728093 139908710635264 logging_writer.py:48] [95300] global_step=95300, grad_norm=5.0658793449401855, loss=1.7555630207061768
I0201 18:21:12.012688 140070692116288 spec.py:321] Evaluating on the training split.
I0201 18:21:18.286534 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 18:21:26.858799 140070692116288 spec.py:349] Evaluating on the test split.
I0201 18:21:29.358677 140070692116288 submission_runner.py:408] Time since start: 33305.81s, 	Step: 95344, 	{'train/accuracy': 0.7147639989852905, 'train/loss': 1.101299524307251, 'validation/accuracy': 0.6574400067329407, 'validation/loss': 1.3877466917037964, 'validation/num_examples': 50000, 'test/accuracy': 0.5294000506401062, 'test/loss': 2.091960906982422, 'test/num_examples': 10000, 'score': 32166.745491981506, 'total_duration': 33305.81133413315, 'accumulated_submission_time': 32166.745491981506, 'accumulated_eval_time': 1131.4108610153198, 'accumulated_logging_time': 4.345837831497192}
I0201 18:21:29.394409 139907745949440 logging_writer.py:48] [95344] accumulated_eval_time=1131.410861, accumulated_logging_time=4.345838, accumulated_submission_time=32166.745492, global_step=95344, preemption_count=0, score=32166.745492, test/accuracy=0.529400, test/loss=2.091961, test/num_examples=10000, total_duration=33305.811334, train/accuracy=0.714764, train/loss=1.101300, validation/accuracy=0.657440, validation/loss=1.387747, validation/num_examples=50000
I0201 18:21:48.540177 139907754342144 logging_writer.py:48] [95400] global_step=95400, grad_norm=5.065703392028809, loss=1.4977846145629883
I0201 18:22:22.150800 139907745949440 logging_writer.py:48] [95500] global_step=95500, grad_norm=4.112254619598389, loss=1.5779727697372437
I0201 18:22:55.816092 139907754342144 logging_writer.py:48] [95600] global_step=95600, grad_norm=4.207618713378906, loss=1.5420398712158203
I0201 18:23:29.492655 139907745949440 logging_writer.py:48] [95700] global_step=95700, grad_norm=4.354444980621338, loss=1.5969254970550537
I0201 18:24:03.149099 139907754342144 logging_writer.py:48] [95800] global_step=95800, grad_norm=4.773923873901367, loss=1.690456748008728
I0201 18:24:36.909350 139907745949440 logging_writer.py:48] [95900] global_step=95900, grad_norm=4.190127372741699, loss=1.6406550407409668
I0201 18:25:10.620139 139907754342144 logging_writer.py:48] [96000] global_step=96000, grad_norm=5.360097885131836, loss=1.5513206720352173
I0201 18:25:44.315643 139907745949440 logging_writer.py:48] [96100] global_step=96100, grad_norm=4.05802059173584, loss=1.6333857774734497
I0201 18:26:17.972238 139907754342144 logging_writer.py:48] [96200] global_step=96200, grad_norm=4.424493312835693, loss=1.592581033706665
I0201 18:26:51.643485 139907745949440 logging_writer.py:48] [96300] global_step=96300, grad_norm=6.844723224639893, loss=1.6805003881454468
I0201 18:27:25.309997 139907754342144 logging_writer.py:48] [96400] global_step=96400, grad_norm=5.0413641929626465, loss=1.7173678874969482
I0201 18:27:58.980000 139907745949440 logging_writer.py:48] [96500] global_step=96500, grad_norm=4.812560081481934, loss=1.6145641803741455
I0201 18:28:32.658592 139907754342144 logging_writer.py:48] [96600] global_step=96600, grad_norm=5.308955669403076, loss=1.6858577728271484
I0201 18:29:06.313147 139907745949440 logging_writer.py:48] [96700] global_step=96700, grad_norm=4.556672096252441, loss=1.5473754405975342
I0201 18:29:39.973387 139907754342144 logging_writer.py:48] [96800] global_step=96800, grad_norm=4.511096954345703, loss=1.7305684089660645
I0201 18:29:59.657562 140070692116288 spec.py:321] Evaluating on the training split.
I0201 18:30:06.111345 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 18:30:14.620011 140070692116288 spec.py:349] Evaluating on the test split.
I0201 18:30:17.232967 140070692116288 submission_runner.py:408] Time since start: 33833.69s, 	Step: 96860, 	{'train/accuracy': 0.7434829473495483, 'train/loss': 0.9642866849899292, 'validation/accuracy': 0.6609599590301514, 'validation/loss': 1.386073112487793, 'validation/num_examples': 50000, 'test/accuracy': 0.5304000377655029, 'test/loss': 2.108454704284668, 'test/num_examples': 10000, 'score': 32676.94704413414, 'total_duration': 33833.68562602997, 'accumulated_submission_time': 32676.94704413414, 'accumulated_eval_time': 1148.9862267971039, 'accumulated_logging_time': 4.391420841217041}
I0201 18:30:17.266188 139907729164032 logging_writer.py:48] [96860] accumulated_eval_time=1148.986227, accumulated_logging_time=4.391421, accumulated_submission_time=32676.947044, global_step=96860, preemption_count=0, score=32676.947044, test/accuracy=0.530400, test/loss=2.108455, test/num_examples=10000, total_duration=33833.685626, train/accuracy=0.743483, train/loss=0.964287, validation/accuracy=0.660960, validation/loss=1.386073, validation/num_examples=50000
I0201 18:30:31.072509 139907737556736 logging_writer.py:48] [96900] global_step=96900, grad_norm=5.025156497955322, loss=1.7184784412384033
I0201 18:31:04.756881 139907729164032 logging_writer.py:48] [97000] global_step=97000, grad_norm=5.211225986480713, loss=1.6520447731018066
I0201 18:31:38.396355 139907737556736 logging_writer.py:48] [97100] global_step=97100, grad_norm=5.543167591094971, loss=1.5574862957000732
I0201 18:32:12.085794 139907729164032 logging_writer.py:48] [97200] global_step=97200, grad_norm=4.457960605621338, loss=1.6244902610778809
I0201 18:32:45.740448 139907737556736 logging_writer.py:48] [97300] global_step=97300, grad_norm=4.807801246643066, loss=1.6510560512542725
I0201 18:33:19.425409 139907729164032 logging_writer.py:48] [97400] global_step=97400, grad_norm=4.339105129241943, loss=1.5979894399642944
I0201 18:33:53.073841 139907737556736 logging_writer.py:48] [97500] global_step=97500, grad_norm=5.090649604797363, loss=1.535447359085083
I0201 18:34:26.742619 139907729164032 logging_writer.py:48] [97600] global_step=97600, grad_norm=4.559535026550293, loss=1.5930370092391968
I0201 18:35:00.399259 139907737556736 logging_writer.py:48] [97700] global_step=97700, grad_norm=4.677558898925781, loss=1.6374766826629639
I0201 18:35:34.094292 139907729164032 logging_writer.py:48] [97800] global_step=97800, grad_norm=4.897370338439941, loss=1.7393043041229248
I0201 18:36:07.747171 139907737556736 logging_writer.py:48] [97900] global_step=97900, grad_norm=5.516732215881348, loss=1.6083906888961792
I0201 18:36:41.452217 139907729164032 logging_writer.py:48] [98000] global_step=98000, grad_norm=5.549135684967041, loss=1.5881024599075317
I0201 18:37:15.188277 139907737556736 logging_writer.py:48] [98100] global_step=98100, grad_norm=4.830056667327881, loss=1.5998380184173584
I0201 18:37:48.927031 139907729164032 logging_writer.py:48] [98200] global_step=98200, grad_norm=4.004777908325195, loss=1.566771388053894
I0201 18:38:22.588883 139907737556736 logging_writer.py:48] [98300] global_step=98300, grad_norm=4.504765510559082, loss=1.595221996307373
I0201 18:38:47.327851 140070692116288 spec.py:321] Evaluating on the training split.
I0201 18:38:53.748265 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 18:39:02.356284 140070692116288 spec.py:349] Evaluating on the test split.
I0201 18:39:04.949436 140070692116288 submission_runner.py:408] Time since start: 34361.40s, 	Step: 98375, 	{'train/accuracy': 0.7309271097183228, 'train/loss': 1.0284843444824219, 'validation/accuracy': 0.6595799922943115, 'validation/loss': 1.3712352514266968, 'validation/num_examples': 50000, 'test/accuracy': 0.5261000394821167, 'test/loss': 2.065514326095581, 'test/num_examples': 10000, 'score': 33186.945125579834, 'total_duration': 34361.40209579468, 'accumulated_submission_time': 33186.945125579834, 'accumulated_eval_time': 1166.6077721118927, 'accumulated_logging_time': 4.4350080490112305}
I0201 18:39:04.985874 139908719027968 logging_writer.py:48] [98375] accumulated_eval_time=1166.607772, accumulated_logging_time=4.435008, accumulated_submission_time=33186.945126, global_step=98375, preemption_count=0, score=33186.945126, test/accuracy=0.526100, test/loss=2.065514, test/num_examples=10000, total_duration=34361.402096, train/accuracy=0.730927, train/loss=1.028484, validation/accuracy=0.659580, validation/loss=1.371235, validation/num_examples=50000
I0201 18:39:13.718605 139908727420672 logging_writer.py:48] [98400] global_step=98400, grad_norm=4.6327290534973145, loss=1.5002189874649048
I0201 18:39:47.367672 139908719027968 logging_writer.py:48] [98500] global_step=98500, grad_norm=4.944031715393066, loss=1.6093727350234985
I0201 18:40:21.049202 139908727420672 logging_writer.py:48] [98600] global_step=98600, grad_norm=5.373897075653076, loss=1.6306676864624023
I0201 18:40:54.731398 139908719027968 logging_writer.py:48] [98700] global_step=98700, grad_norm=4.853512287139893, loss=1.5821231603622437
I0201 18:41:28.404373 139908727420672 logging_writer.py:48] [98800] global_step=98800, grad_norm=5.029026985168457, loss=1.5085076093673706
I0201 18:42:02.077208 139908719027968 logging_writer.py:48] [98900] global_step=98900, grad_norm=5.822055816650391, loss=1.6073204278945923
I0201 18:42:35.744754 139908727420672 logging_writer.py:48] [99000] global_step=99000, grad_norm=4.895613193511963, loss=1.631839394569397
I0201 18:43:09.403539 139908719027968 logging_writer.py:48] [99100] global_step=99100, grad_norm=4.044727325439453, loss=1.5536880493164062
I0201 18:43:43.132763 139908727420672 logging_writer.py:48] [99200] global_step=99200, grad_norm=5.656313419342041, loss=1.6312625408172607
I0201 18:44:16.857322 139908719027968 logging_writer.py:48] [99300] global_step=99300, grad_norm=5.649529933929443, loss=1.6027462482452393
I0201 18:44:50.531446 139908727420672 logging_writer.py:48] [99400] global_step=99400, grad_norm=5.214446067810059, loss=1.684199571609497
I0201 18:45:24.197343 139908719027968 logging_writer.py:48] [99500] global_step=99500, grad_norm=4.593749046325684, loss=1.5774695873260498
I0201 18:45:57.861297 139908727420672 logging_writer.py:48] [99600] global_step=99600, grad_norm=5.136510372161865, loss=1.5959718227386475
I0201 18:46:31.541256 139908719027968 logging_writer.py:48] [99700] global_step=99700, grad_norm=4.65817928314209, loss=1.527238130569458
I0201 18:47:05.193101 139908727420672 logging_writer.py:48] [99800] global_step=99800, grad_norm=4.731206893920898, loss=1.5988528728485107
I0201 18:47:34.951348 140070692116288 spec.py:321] Evaluating on the training split.
I0201 18:47:41.318753 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 18:47:49.850068 140070692116288 spec.py:349] Evaluating on the test split.
I0201 18:47:52.451404 140070692116288 submission_runner.py:408] Time since start: 34888.90s, 	Step: 99890, 	{'train/accuracy': 0.7154615521430969, 'train/loss': 1.0986077785491943, 'validation/accuracy': 0.6500999927520752, 'validation/loss': 1.425310730934143, 'validation/num_examples': 50000, 'test/accuracy': 0.5289000272750854, 'test/loss': 2.132866382598877, 'test/num_examples': 10000, 'score': 33696.84750413895, 'total_duration': 34888.904074430466, 'accumulated_submission_time': 33696.84750413895, 'accumulated_eval_time': 1184.1078248023987, 'accumulated_logging_time': 4.481401681900024}
I0201 18:47:52.485297 139907754342144 logging_writer.py:48] [99890] accumulated_eval_time=1184.107825, accumulated_logging_time=4.481402, accumulated_submission_time=33696.847504, global_step=99890, preemption_count=0, score=33696.847504, test/accuracy=0.528900, test/loss=2.132866, test/num_examples=10000, total_duration=34888.904074, train/accuracy=0.715462, train/loss=1.098608, validation/accuracy=0.650100, validation/loss=1.425311, validation/num_examples=50000
I0201 18:47:56.184790 139907762734848 logging_writer.py:48] [99900] global_step=99900, grad_norm=4.570704936981201, loss=1.684464454650879
I0201 18:48:29.779416 139907754342144 logging_writer.py:48] [100000] global_step=100000, grad_norm=4.813002109527588, loss=1.6742734909057617
I0201 18:49:03.374014 139907762734848 logging_writer.py:48] [100100] global_step=100100, grad_norm=4.811169147491455, loss=1.649490237236023
I0201 18:49:37.071712 139907754342144 logging_writer.py:48] [100200] global_step=100200, grad_norm=4.749483585357666, loss=1.4780123233795166
I0201 18:50:10.811444 139907762734848 logging_writer.py:48] [100300] global_step=100300, grad_norm=4.309311866760254, loss=1.6651794910430908
I0201 18:50:44.440890 139907754342144 logging_writer.py:48] [100400] global_step=100400, grad_norm=5.949261665344238, loss=1.5952463150024414
I0201 18:51:18.125912 139907762734848 logging_writer.py:48] [100500] global_step=100500, grad_norm=5.060298442840576, loss=1.6624983549118042
I0201 18:51:51.779319 139907754342144 logging_writer.py:48] [100600] global_step=100600, grad_norm=4.817809104919434, loss=1.6252316236495972
I0201 18:52:25.465462 139907762734848 logging_writer.py:48] [100700] global_step=100700, grad_norm=4.6709065437316895, loss=1.6439173221588135
I0201 18:52:59.113254 139907754342144 logging_writer.py:48] [100800] global_step=100800, grad_norm=5.204155445098877, loss=1.5453637838363647
I0201 18:53:32.810136 139907762734848 logging_writer.py:48] [100900] global_step=100900, grad_norm=4.932740688323975, loss=1.548666000366211
I0201 18:54:06.465046 139907754342144 logging_writer.py:48] [101000] global_step=101000, grad_norm=4.928099632263184, loss=1.5528249740600586
I0201 18:54:40.175891 139907762734848 logging_writer.py:48] [101100] global_step=101100, grad_norm=4.607863903045654, loss=1.5941579341888428
I0201 18:55:13.828135 139907754342144 logging_writer.py:48] [101200] global_step=101200, grad_norm=4.434480667114258, loss=1.701629877090454
I0201 18:55:47.514990 139907762734848 logging_writer.py:48] [101300] global_step=101300, grad_norm=5.130916595458984, loss=1.5871435403823853
I0201 18:56:21.254631 139907754342144 logging_writer.py:48] [101400] global_step=101400, grad_norm=4.321988582611084, loss=1.6650227308273315
I0201 18:56:22.760780 140070692116288 spec.py:321] Evaluating on the training split.
I0201 18:56:29.061047 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 18:56:37.587006 140070692116288 spec.py:349] Evaluating on the test split.
I0201 18:56:40.172257 140070692116288 submission_runner.py:408] Time since start: 35416.62s, 	Step: 101406, 	{'train/accuracy': 0.7203643321990967, 'train/loss': 1.0835435390472412, 'validation/accuracy': 0.6618399620056152, 'validation/loss': 1.3780913352966309, 'validation/num_examples': 50000, 'test/accuracy': 0.525700032711029, 'test/loss': 2.14704966545105, 'test/num_examples': 10000, 'score': 34207.06012392044, 'total_duration': 35416.624915361404, 'accumulated_submission_time': 34207.06012392044, 'accumulated_eval_time': 1201.5192544460297, 'accumulated_logging_time': 4.524857044219971}
I0201 18:56:40.215143 139908719027968 logging_writer.py:48] [101406] accumulated_eval_time=1201.519254, accumulated_logging_time=4.524857, accumulated_submission_time=34207.060124, global_step=101406, preemption_count=0, score=34207.060124, test/accuracy=0.525700, test/loss=2.147050, test/num_examples=10000, total_duration=35416.624915, train/accuracy=0.720364, train/loss=1.083544, validation/accuracy=0.661840, validation/loss=1.378091, validation/num_examples=50000
I0201 18:57:12.127865 139908727420672 logging_writer.py:48] [101500] global_step=101500, grad_norm=4.760774612426758, loss=1.5396060943603516
I0201 18:57:45.758605 139908719027968 logging_writer.py:48] [101600] global_step=101600, grad_norm=4.854062557220459, loss=1.615373969078064
I0201 18:58:19.454843 139908727420672 logging_writer.py:48] [101700] global_step=101700, grad_norm=5.490744113922119, loss=1.5834167003631592
I0201 18:58:53.123668 139908719027968 logging_writer.py:48] [101800] global_step=101800, grad_norm=5.043094635009766, loss=1.5852071046829224
I0201 18:59:26.796672 139908727420672 logging_writer.py:48] [101900] global_step=101900, grad_norm=4.5048298835754395, loss=1.4850891828536987
I0201 19:00:00.460604 139908719027968 logging_writer.py:48] [102000] global_step=102000, grad_norm=5.0556793212890625, loss=1.6547785997390747
I0201 19:00:34.122566 139908727420672 logging_writer.py:48] [102100] global_step=102100, grad_norm=4.381438732147217, loss=1.6075397729873657
I0201 19:01:07.793633 139908719027968 logging_writer.py:48] [102200] global_step=102200, grad_norm=4.596679210662842, loss=1.574454665184021
I0201 19:01:41.462339 139908727420672 logging_writer.py:48] [102300] global_step=102300, grad_norm=5.454852104187012, loss=1.64998197555542
I0201 19:02:15.155355 139908719027968 logging_writer.py:48] [102400] global_step=102400, grad_norm=5.439565658569336, loss=1.672651767730713
I0201 19:02:48.915351 139908727420672 logging_writer.py:48] [102500] global_step=102500, grad_norm=4.845244407653809, loss=1.5905461311340332
I0201 19:03:22.597965 139908719027968 logging_writer.py:48] [102600] global_step=102600, grad_norm=6.3318562507629395, loss=1.5511977672576904
I0201 19:03:56.271524 139908727420672 logging_writer.py:48] [102700] global_step=102700, grad_norm=6.142036437988281, loss=1.6048266887664795
I0201 19:04:29.956221 139908719027968 logging_writer.py:48] [102800] global_step=102800, grad_norm=4.392989635467529, loss=1.5642446279525757
I0201 19:05:03.616489 139908727420672 logging_writer.py:48] [102900] global_step=102900, grad_norm=4.809490203857422, loss=1.4261746406555176
I0201 19:05:10.502051 140070692116288 spec.py:321] Evaluating on the training split.
I0201 19:05:16.779975 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 19:05:25.549789 140070692116288 spec.py:349] Evaluating on the test split.
I0201 19:05:28.147065 140070692116288 submission_runner.py:408] Time since start: 35944.60s, 	Step: 102922, 	{'train/accuracy': 0.7308474183082581, 'train/loss': 1.019730567932129, 'validation/accuracy': 0.6702199578285217, 'validation/loss': 1.3424674272537231, 'validation/num_examples': 50000, 'test/accuracy': 0.5365000367164612, 'test/loss': 2.069563865661621, 'test/num_examples': 10000, 'score': 34717.284263134, 'total_duration': 35944.59972167015, 'accumulated_submission_time': 34717.284263134, 'accumulated_eval_time': 1219.1642200946808, 'accumulated_logging_time': 4.576931715011597}
I0201 19:05:28.180853 139907737556736 logging_writer.py:48] [102922] accumulated_eval_time=1219.164220, accumulated_logging_time=4.576932, accumulated_submission_time=34717.284263, global_step=102922, preemption_count=0, score=34717.284263, test/accuracy=0.536500, test/loss=2.069564, test/num_examples=10000, total_duration=35944.599722, train/accuracy=0.730847, train/loss=1.019731, validation/accuracy=0.670220, validation/loss=1.342467, validation/num_examples=50000
I0201 19:05:54.753516 139907745949440 logging_writer.py:48] [103000] global_step=103000, grad_norm=5.454014301300049, loss=1.625941514968872
I0201 19:06:28.394962 139907737556736 logging_writer.py:48] [103100] global_step=103100, grad_norm=5.402917861938477, loss=1.5384286642074585
I0201 19:07:02.083990 139907745949440 logging_writer.py:48] [103200] global_step=103200, grad_norm=5.08245325088501, loss=1.5841764211654663
I0201 19:07:35.752370 139907737556736 logging_writer.py:48] [103300] global_step=103300, grad_norm=5.535980701446533, loss=1.556929588317871
I0201 19:08:09.444346 139907745949440 logging_writer.py:48] [103400] global_step=103400, grad_norm=4.277846813201904, loss=1.5359500646591187
I0201 19:08:43.114384 139907737556736 logging_writer.py:48] [103500] global_step=103500, grad_norm=4.587794303894043, loss=1.522560715675354
I0201 19:09:16.834181 139907745949440 logging_writer.py:48] [103600] global_step=103600, grad_norm=4.7774434089660645, loss=1.547634482383728
I0201 19:09:50.516062 139907737556736 logging_writer.py:48] [103700] global_step=103700, grad_norm=5.235186576843262, loss=1.5289404392242432
I0201 19:10:24.211409 139907745949440 logging_writer.py:48] [103800] global_step=103800, grad_norm=4.604494094848633, loss=1.6439294815063477
I0201 19:10:57.866740 139907737556736 logging_writer.py:48] [103900] global_step=103900, grad_norm=4.9643425941467285, loss=1.5430235862731934
I0201 19:11:31.548039 139907745949440 logging_writer.py:48] [104000] global_step=104000, grad_norm=5.020052909851074, loss=1.5606752634048462
I0201 19:12:05.193264 139907737556736 logging_writer.py:48] [104100] global_step=104100, grad_norm=5.505103588104248, loss=1.5562727451324463
I0201 19:12:38.900655 139907745949440 logging_writer.py:48] [104200] global_step=104200, grad_norm=4.666419982910156, loss=1.533312439918518
I0201 19:13:12.558228 139907737556736 logging_writer.py:48] [104300] global_step=104300, grad_norm=4.726571083068848, loss=1.534932255744934
I0201 19:13:46.257023 139907745949440 logging_writer.py:48] [104400] global_step=104400, grad_norm=4.405843257904053, loss=1.4804682731628418
I0201 19:13:58.177836 140070692116288 spec.py:321] Evaluating on the training split.
I0201 19:14:04.582447 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 19:14:13.370039 140070692116288 spec.py:349] Evaluating on the test split.
I0201 19:14:16.011905 140070692116288 submission_runner.py:408] Time since start: 36472.46s, 	Step: 104437, 	{'train/accuracy': 0.7296117544174194, 'train/loss': 1.0509129762649536, 'validation/accuracy': 0.6679199934005737, 'validation/loss': 1.3478890657424927, 'validation/num_examples': 50000, 'test/accuracy': 0.5321000218391418, 'test/loss': 2.0829954147338867, 'test/num_examples': 10000, 'score': 35227.21813130379, 'total_duration': 36472.46457672119, 'accumulated_submission_time': 35227.21813130379, 'accumulated_eval_time': 1236.998259305954, 'accumulated_logging_time': 4.621797323226929}
I0201 19:14:16.060174 139908425447168 logging_writer.py:48] [104437] accumulated_eval_time=1236.998259, accumulated_logging_time=4.621797, accumulated_submission_time=35227.218131, global_step=104437, preemption_count=0, score=35227.218131, test/accuracy=0.532100, test/loss=2.082995, test/num_examples=10000, total_duration=36472.464577, train/accuracy=0.729612, train/loss=1.050913, validation/accuracy=0.667920, validation/loss=1.347889, validation/num_examples=50000
I0201 19:14:37.547207 139908710635264 logging_writer.py:48] [104500] global_step=104500, grad_norm=5.084396839141846, loss=1.5956019163131714
I0201 19:15:11.159460 139908425447168 logging_writer.py:48] [104600] global_step=104600, grad_norm=4.7689900398254395, loss=1.5672866106033325
I0201 19:15:44.877243 139908710635264 logging_writer.py:48] [104700] global_step=104700, grad_norm=4.658149719238281, loss=1.5252230167388916
I0201 19:16:18.582550 139908425447168 logging_writer.py:48] [104800] global_step=104800, grad_norm=4.511840343475342, loss=1.5510427951812744
I0201 19:16:52.286301 139908710635264 logging_writer.py:48] [104900] global_step=104900, grad_norm=5.218423366546631, loss=1.5872911214828491
I0201 19:17:25.938995 139908425447168 logging_writer.py:48] [105000] global_step=105000, grad_norm=5.675511837005615, loss=1.604387879371643
I0201 19:17:59.630294 139908710635264 logging_writer.py:48] [105100] global_step=105100, grad_norm=5.221899509429932, loss=1.546358346939087
I0201 19:18:33.291831 139908425447168 logging_writer.py:48] [105200] global_step=105200, grad_norm=4.352924823760986, loss=1.657084345817566
I0201 19:19:06.978538 139908710635264 logging_writer.py:48] [105300] global_step=105300, grad_norm=4.783991813659668, loss=1.6768933534622192
I0201 19:19:40.650931 139908425447168 logging_writer.py:48] [105400] global_step=105400, grad_norm=4.1960015296936035, loss=1.6183562278747559
I0201 19:20:14.332872 139908710635264 logging_writer.py:48] [105500] global_step=105500, grad_norm=5.10711145401001, loss=1.550215482711792
I0201 19:20:47.993192 139908425447168 logging_writer.py:48] [105600] global_step=105600, grad_norm=6.01948356628418, loss=1.5417835712432861
I0201 19:21:21.669778 139908710635264 logging_writer.py:48] [105700] global_step=105700, grad_norm=5.309206485748291, loss=1.6245025396347046
I0201 19:21:55.450706 139908425447168 logging_writer.py:48] [105800] global_step=105800, grad_norm=4.86673641204834, loss=1.5444813966751099
I0201 19:22:29.142740 139908710635264 logging_writer.py:48] [105900] global_step=105900, grad_norm=5.632713317871094, loss=1.4881223440170288
I0201 19:22:46.129563 140070692116288 spec.py:321] Evaluating on the training split.
I0201 19:22:52.499897 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 19:23:00.999250 140070692116288 spec.py:349] Evaluating on the test split.
I0201 19:23:03.835498 140070692116288 submission_runner.py:408] Time since start: 37000.29s, 	Step: 105952, 	{'train/accuracy': 0.76175856590271, 'train/loss': 0.9061012864112854, 'validation/accuracy': 0.674560010433197, 'validation/loss': 1.3246057033538818, 'validation/num_examples': 50000, 'test/accuracy': 0.5433000326156616, 'test/loss': 2.0417113304138184, 'test/num_examples': 10000, 'score': 35737.22608041763, 'total_duration': 37000.28816699982, 'accumulated_submission_time': 35737.22608041763, 'accumulated_eval_time': 1254.704176902771, 'accumulated_logging_time': 4.679151773452759}
I0201 19:23:03.878905 139907754342144 logging_writer.py:48] [105952] accumulated_eval_time=1254.704177, accumulated_logging_time=4.679152, accumulated_submission_time=35737.226080, global_step=105952, preemption_count=0, score=35737.226080, test/accuracy=0.543300, test/loss=2.041711, test/num_examples=10000, total_duration=37000.288167, train/accuracy=0.761759, train/loss=0.906101, validation/accuracy=0.674560, validation/loss=1.324606, validation/num_examples=50000
I0201 19:23:20.389709 139907762734848 logging_writer.py:48] [106000] global_step=106000, grad_norm=6.220914363861084, loss=1.619248390197754
I0201 19:23:53.997411 139907754342144 logging_writer.py:48] [106100] global_step=106100, grad_norm=4.586578845977783, loss=1.45930016040802
I0201 19:24:27.690824 139907762734848 logging_writer.py:48] [106200] global_step=106200, grad_norm=4.873659133911133, loss=1.4705095291137695
I0201 19:25:01.387541 139907754342144 logging_writer.py:48] [106300] global_step=106300, grad_norm=4.944671630859375, loss=1.5010759830474854
I0201 19:25:35.043766 139907762734848 logging_writer.py:48] [106400] global_step=106400, grad_norm=4.621944904327393, loss=1.5417205095291138
I0201 19:26:08.717035 139907754342144 logging_writer.py:48] [106500] global_step=106500, grad_norm=5.118388652801514, loss=1.5760562419891357
I0201 19:26:42.366834 139907762734848 logging_writer.py:48] [106600] global_step=106600, grad_norm=5.253081798553467, loss=1.5094168186187744
I0201 19:27:16.038223 139907754342144 logging_writer.py:48] [106700] global_step=106700, grad_norm=5.118196964263916, loss=1.5683794021606445
I0201 19:27:49.695178 139907762734848 logging_writer.py:48] [106800] global_step=106800, grad_norm=6.526732444763184, loss=1.5939126014709473
I0201 19:28:23.384403 139907754342144 logging_writer.py:48] [106900] global_step=106900, grad_norm=4.67752742767334, loss=1.4786858558654785
I0201 19:28:57.087996 139907762734848 logging_writer.py:48] [107000] global_step=107000, grad_norm=4.834518909454346, loss=1.425113558769226
I0201 19:29:30.782639 139907754342144 logging_writer.py:48] [107100] global_step=107100, grad_norm=5.302796363830566, loss=1.5889443159103394
I0201 19:30:04.432540 139907762734848 logging_writer.py:48] [107200] global_step=107200, grad_norm=4.822324275970459, loss=1.6415469646453857
I0201 19:30:38.115037 139907754342144 logging_writer.py:48] [107300] global_step=107300, grad_norm=5.056654453277588, loss=1.606501579284668
I0201 19:31:11.771824 139907762734848 logging_writer.py:48] [107400] global_step=107400, grad_norm=4.729137420654297, loss=1.5449479818344116
I0201 19:31:34.150630 140070692116288 spec.py:321] Evaluating on the training split.
I0201 19:31:40.410050 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 19:31:49.022300 140070692116288 spec.py:349] Evaluating on the test split.
I0201 19:31:51.776613 140070692116288 submission_runner.py:408] Time since start: 37528.23s, 	Step: 107468, 	{'train/accuracy': 0.7491828799247742, 'train/loss': 0.9538630843162537, 'validation/accuracy': 0.675819993019104, 'validation/loss': 1.3117072582244873, 'validation/num_examples': 50000, 'test/accuracy': 0.546500027179718, 'test/loss': 2.02262544631958, 'test/num_examples': 10000, 'score': 36247.436046123505, 'total_duration': 37528.2292740345, 'accumulated_submission_time': 36247.436046123505, 'accumulated_eval_time': 1272.3301212787628, 'accumulated_logging_time': 4.732055902481079}
I0201 19:31:51.811832 139907745949440 logging_writer.py:48] [107468] accumulated_eval_time=1272.330121, accumulated_logging_time=4.732056, accumulated_submission_time=36247.436046, global_step=107468, preemption_count=0, score=36247.436046, test/accuracy=0.546500, test/loss=2.022625, test/num_examples=10000, total_duration=37528.229274, train/accuracy=0.749183, train/loss=0.953863, validation/accuracy=0.675820, validation/loss=1.311707, validation/num_examples=50000
I0201 19:32:02.901114 139908425447168 logging_writer.py:48] [107500] global_step=107500, grad_norm=4.396630764007568, loss=1.53837251663208
I0201 19:32:36.547959 139907745949440 logging_writer.py:48] [107600] global_step=107600, grad_norm=6.311528205871582, loss=1.485436201095581
I0201 19:33:10.203839 139908425447168 logging_writer.py:48] [107700] global_step=107700, grad_norm=4.651989936828613, loss=1.5855728387832642
I0201 19:33:43.895483 139907745949440 logging_writer.py:48] [107800] global_step=107800, grad_norm=4.996708869934082, loss=1.5326215028762817
I0201 19:34:17.610424 139908425447168 logging_writer.py:48] [107900] global_step=107900, grad_norm=5.334300518035889, loss=1.6160848140716553
I0201 19:34:51.354521 139907745949440 logging_writer.py:48] [108000] global_step=108000, grad_norm=5.768885135650635, loss=1.648410677909851
I0201 19:35:25.016398 139908425447168 logging_writer.py:48] [108100] global_step=108100, grad_norm=5.163650035858154, loss=1.6463491916656494
I0201 19:35:58.691794 139907745949440 logging_writer.py:48] [108200] global_step=108200, grad_norm=5.0041985511779785, loss=1.559689998626709
I0201 19:36:32.344145 139908425447168 logging_writer.py:48] [108300] global_step=108300, grad_norm=5.631543159484863, loss=1.5777599811553955
I0201 19:37:06.018171 139907745949440 logging_writer.py:48] [108400] global_step=108400, grad_norm=4.93949031829834, loss=1.5811411142349243
I0201 19:37:39.678363 139908425447168 logging_writer.py:48] [108500] global_step=108500, grad_norm=5.1753716468811035, loss=1.430570363998413
I0201 19:38:13.356987 139907745949440 logging_writer.py:48] [108600] global_step=108600, grad_norm=5.543901443481445, loss=1.6825237274169922
I0201 19:38:47.011128 139908425447168 logging_writer.py:48] [108700] global_step=108700, grad_norm=4.702125549316406, loss=1.5513588190078735
I0201 19:39:20.703734 139907745949440 logging_writer.py:48] [108800] global_step=108800, grad_norm=5.196305751800537, loss=1.5903514623641968
I0201 19:39:54.358036 139908425447168 logging_writer.py:48] [108900] global_step=108900, grad_norm=5.248639106750488, loss=1.5300006866455078
I0201 19:40:21.790148 140070692116288 spec.py:321] Evaluating on the training split.
I0201 19:40:28.109625 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 19:40:36.827209 140070692116288 spec.py:349] Evaluating on the test split.
I0201 19:40:39.732131 140070692116288 submission_runner.py:408] Time since start: 38056.18s, 	Step: 108983, 	{'train/accuracy': 0.7424266338348389, 'train/loss': 0.9738861918449402, 'validation/accuracy': 0.6760599613189697, 'validation/loss': 1.3137764930725098, 'validation/num_examples': 50000, 'test/accuracy': 0.541700005531311, 'test/loss': 2.0563201904296875, 'test/num_examples': 10000, 'score': 36757.352653265, 'total_duration': 38056.18480205536, 'accumulated_submission_time': 36757.352653265, 'accumulated_eval_time': 1290.2720756530762, 'accumulated_logging_time': 4.7767369747161865}
I0201 19:40:39.771958 139908719027968 logging_writer.py:48] [108983] accumulated_eval_time=1290.272076, accumulated_logging_time=4.776737, accumulated_submission_time=36757.352653, global_step=108983, preemption_count=0, score=36757.352653, test/accuracy=0.541700, test/loss=2.056320, test/num_examples=10000, total_duration=38056.184802, train/accuracy=0.742427, train/loss=0.973886, validation/accuracy=0.676060, validation/loss=1.313776, validation/num_examples=50000
I0201 19:40:45.826565 139908727420672 logging_writer.py:48] [109000] global_step=109000, grad_norm=5.014316082000732, loss=1.5156545639038086
I0201 19:41:19.472856 139908719027968 logging_writer.py:48] [109100] global_step=109100, grad_norm=5.684048652648926, loss=1.5674114227294922
I0201 19:41:53.157876 139908727420672 logging_writer.py:48] [109200] global_step=109200, grad_norm=5.55339241027832, loss=1.4430383443832397
I0201 19:42:26.808295 139908719027968 logging_writer.py:48] [109300] global_step=109300, grad_norm=5.196016788482666, loss=1.5544772148132324
I0201 19:43:00.489889 139908727420672 logging_writer.py:48] [109400] global_step=109400, grad_norm=4.784289360046387, loss=1.5336928367614746
I0201 19:43:34.150816 139908719027968 logging_writer.py:48] [109500] global_step=109500, grad_norm=4.817356586456299, loss=1.5836087465286255
I0201 19:44:07.835649 139908727420672 logging_writer.py:48] [109600] global_step=109600, grad_norm=4.9894819259643555, loss=1.518384575843811
I0201 19:44:41.460213 139908719027968 logging_writer.py:48] [109700] global_step=109700, grad_norm=4.84623384475708, loss=1.4855338335037231
I0201 19:45:15.115601 139908727420672 logging_writer.py:48] [109800] global_step=109800, grad_norm=5.592121601104736, loss=1.462929368019104
I0201 19:45:48.783754 139908719027968 logging_writer.py:48] [109900] global_step=109900, grad_norm=5.8224873542785645, loss=1.5362727642059326
I0201 19:46:22.431730 139908727420672 logging_writer.py:48] [110000] global_step=110000, grad_norm=5.432146072387695, loss=1.5121732950210571
I0201 19:46:56.173332 139908719027968 logging_writer.py:48] [110100] global_step=110100, grad_norm=4.552069187164307, loss=1.4690333604812622
I0201 19:47:29.787185 139908727420672 logging_writer.py:48] [110200] global_step=110200, grad_norm=5.035555839538574, loss=1.5376640558242798
I0201 19:48:03.506185 139908719027968 logging_writer.py:48] [110300] global_step=110300, grad_norm=4.588935852050781, loss=1.4568872451782227
I0201 19:48:37.210094 139908727420672 logging_writer.py:48] [110400] global_step=110400, grad_norm=5.164334774017334, loss=1.6364362239837646
I0201 19:49:09.988998 140070692116288 spec.py:321] Evaluating on the training split.
I0201 19:49:16.230716 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 19:49:24.774309 140070692116288 spec.py:349] Evaluating on the test split.
I0201 19:49:27.431236 140070692116288 submission_runner.py:408] Time since start: 38583.88s, 	Step: 110499, 	{'train/accuracy': 0.7394770383834839, 'train/loss': 0.996335506439209, 'validation/accuracy': 0.6743599772453308, 'validation/loss': 1.319318413734436, 'validation/num_examples': 50000, 'test/accuracy': 0.5463000535964966, 'test/loss': 2.017503261566162, 'test/num_examples': 10000, 'score': 37267.507370471954, 'total_duration': 38583.883883714676, 'accumulated_submission_time': 37267.507370471954, 'accumulated_eval_time': 1307.7142674922943, 'accumulated_logging_time': 4.826080322265625}
I0201 19:49:27.469336 139907762734848 logging_writer.py:48] [110499] accumulated_eval_time=1307.714267, accumulated_logging_time=4.826080, accumulated_submission_time=37267.507370, global_step=110499, preemption_count=0, score=37267.507370, test/accuracy=0.546300, test/loss=2.017503, test/num_examples=10000, total_duration=38583.883884, train/accuracy=0.739477, train/loss=0.996336, validation/accuracy=0.674360, validation/loss=1.319318, validation/num_examples=50000
I0201 19:49:28.147679 139908425447168 logging_writer.py:48] [110500] global_step=110500, grad_norm=5.1800031661987305, loss=1.5223023891448975
I0201 19:50:01.823402 139907762734848 logging_writer.py:48] [110600] global_step=110600, grad_norm=4.504807472229004, loss=1.488447666168213
I0201 19:50:35.515468 139908425447168 logging_writer.py:48] [110700] global_step=110700, grad_norm=5.013238906860352, loss=1.5248942375183105
I0201 19:51:09.165323 139907762734848 logging_writer.py:48] [110800] global_step=110800, grad_norm=5.005399227142334, loss=1.551559329032898
I0201 19:51:42.871861 139908425447168 logging_writer.py:48] [110900] global_step=110900, grad_norm=4.907118797302246, loss=1.5584486722946167
I0201 19:52:16.518587 139907762734848 logging_writer.py:48] [111000] global_step=111000, grad_norm=4.702987194061279, loss=1.5623379945755005
I0201 19:52:50.204176 139908425447168 logging_writer.py:48] [111100] global_step=111100, grad_norm=5.865046977996826, loss=1.5664565563201904
I0201 19:53:23.916561 139907762734848 logging_writer.py:48] [111200] global_step=111200, grad_norm=5.99766731262207, loss=1.4020051956176758
I0201 19:53:57.573067 139908425447168 logging_writer.py:48] [111300] global_step=111300, grad_norm=5.011124134063721, loss=1.4700937271118164
I0201 19:54:31.266374 139907762734848 logging_writer.py:48] [111400] global_step=111400, grad_norm=4.985495090484619, loss=1.4787580966949463
I0201 19:55:04.957689 139908425447168 logging_writer.py:48] [111500] global_step=111500, grad_norm=5.888216495513916, loss=1.5686023235321045
I0201 19:55:38.602447 139907762734848 logging_writer.py:48] [111600] global_step=111600, grad_norm=4.895120143890381, loss=1.5054502487182617
I0201 19:56:12.287313 139908425447168 logging_writer.py:48] [111700] global_step=111700, grad_norm=5.477424144744873, loss=1.5247451066970825
I0201 19:56:45.940333 139907762734848 logging_writer.py:48] [111800] global_step=111800, grad_norm=5.252787113189697, loss=1.5206966400146484
I0201 19:57:19.629937 139908425447168 logging_writer.py:48] [111900] global_step=111900, grad_norm=5.474020481109619, loss=1.4654390811920166
I0201 19:57:53.281999 139907762734848 logging_writer.py:48] [112000] global_step=112000, grad_norm=5.162662029266357, loss=1.434594988822937
I0201 19:57:57.469397 140070692116288 spec.py:321] Evaluating on the training split.
I0201 19:58:03.872508 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 19:58:12.412435 140070692116288 spec.py:349] Evaluating on the test split.
I0201 19:58:14.911005 140070692116288 submission_runner.py:408] Time since start: 39111.36s, 	Step: 112014, 	{'train/accuracy': 0.7409917116165161, 'train/loss': 0.985767662525177, 'validation/accuracy': 0.6783599853515625, 'validation/loss': 1.3062725067138672, 'validation/num_examples': 50000, 'test/accuracy': 0.5491999983787537, 'test/loss': 2.0260019302368164, 'test/num_examples': 10000, 'score': 37777.445001125336, 'total_duration': 39111.363669633865, 'accumulated_submission_time': 37777.445001125336, 'accumulated_eval_time': 1325.155839920044, 'accumulated_logging_time': 4.8738861083984375}
I0201 19:58:14.946652 139907754342144 logging_writer.py:48] [112014] accumulated_eval_time=1325.155840, accumulated_logging_time=4.873886, accumulated_submission_time=37777.445001, global_step=112014, preemption_count=0, score=37777.445001, test/accuracy=0.549200, test/loss=2.026002, test/num_examples=10000, total_duration=39111.363670, train/accuracy=0.740992, train/loss=0.985768, validation/accuracy=0.678360, validation/loss=1.306273, validation/num_examples=50000
I0201 19:58:44.248608 139908710635264 logging_writer.py:48] [112100] global_step=112100, grad_norm=5.308674335479736, loss=1.472557544708252
I0201 19:59:17.921462 139907754342144 logging_writer.py:48] [112200] global_step=112200, grad_norm=5.150899410247803, loss=1.493444800376892
I0201 19:59:51.660320 139908710635264 logging_writer.py:48] [112300] global_step=112300, grad_norm=5.111743450164795, loss=1.54823637008667
I0201 20:00:25.356142 139907754342144 logging_writer.py:48] [112400] global_step=112400, grad_norm=5.9640212059021, loss=1.5504496097564697
I0201 20:00:59.068598 139908710635264 logging_writer.py:48] [112500] global_step=112500, grad_norm=4.972198009490967, loss=1.4016096591949463
I0201 20:01:32.730108 139907754342144 logging_writer.py:48] [112600] global_step=112600, grad_norm=4.844171524047852, loss=1.5352579355239868
I0201 20:02:06.400144 139908710635264 logging_writer.py:48] [112700] global_step=112700, grad_norm=5.520930290222168, loss=1.4250982999801636
I0201 20:02:40.055398 139907754342144 logging_writer.py:48] [112800] global_step=112800, grad_norm=5.014440536499023, loss=1.427453875541687
I0201 20:03:13.738704 139908710635264 logging_writer.py:48] [112900] global_step=112900, grad_norm=4.831171989440918, loss=1.527956247329712
I0201 20:03:47.400777 139907754342144 logging_writer.py:48] [113000] global_step=113000, grad_norm=4.741323471069336, loss=1.4936162233352661
I0201 20:04:21.063134 139908710635264 logging_writer.py:48] [113100] global_step=113100, grad_norm=4.837883472442627, loss=1.4680845737457275
I0201 20:04:54.732392 139907754342144 logging_writer.py:48] [113200] global_step=113200, grad_norm=4.593509197235107, loss=1.4662781953811646
I0201 20:05:28.388992 139908710635264 logging_writer.py:48] [113300] global_step=113300, grad_norm=5.410572052001953, loss=1.4299041032791138
I0201 20:06:02.121817 139907754342144 logging_writer.py:48] [113400] global_step=113400, grad_norm=6.228627681732178, loss=1.4695558547973633
I0201 20:06:35.830118 139908710635264 logging_writer.py:48] [113500] global_step=113500, grad_norm=5.290262222290039, loss=1.5479000806808472
I0201 20:06:45.078934 140070692116288 spec.py:321] Evaluating on the training split.
I0201 20:06:51.441250 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 20:06:59.877985 140070692116288 spec.py:349] Evaluating on the test split.
I0201 20:07:02.489607 140070692116288 submission_runner.py:408] Time since start: 39638.94s, 	Step: 113529, 	{'train/accuracy': 0.7640305757522583, 'train/loss': 0.9003487825393677, 'validation/accuracy': 0.6823399662971497, 'validation/loss': 1.285157561302185, 'validation/num_examples': 50000, 'test/accuracy': 0.5555000305175781, 'test/loss': 1.9852403402328491, 'test/num_examples': 10000, 'score': 38287.514525175095, 'total_duration': 39638.94228172302, 'accumulated_submission_time': 38287.514525175095, 'accumulated_eval_time': 1342.566482782364, 'accumulated_logging_time': 4.919671297073364}
I0201 20:07:02.528747 139907762734848 logging_writer.py:48] [113529] accumulated_eval_time=1342.566483, accumulated_logging_time=4.919671, accumulated_submission_time=38287.514525, global_step=113529, preemption_count=0, score=38287.514525, test/accuracy=0.555500, test/loss=1.985240, test/num_examples=10000, total_duration=39638.942282, train/accuracy=0.764031, train/loss=0.900349, validation/accuracy=0.682340, validation/loss=1.285158, validation/num_examples=50000
I0201 20:07:26.755757 139908425447168 logging_writer.py:48] [113600] global_step=113600, grad_norm=4.529034614562988, loss=1.4657046794891357
I0201 20:08:00.419268 139907762734848 logging_writer.py:48] [113700] global_step=113700, grad_norm=5.233360290527344, loss=1.4654008150100708
I0201 20:08:34.099849 139908425447168 logging_writer.py:48] [113800] global_step=113800, grad_norm=5.289907932281494, loss=1.5357778072357178
I0201 20:09:07.762314 139907762734848 logging_writer.py:48] [113900] global_step=113900, grad_norm=5.039062976837158, loss=1.5506927967071533
I0201 20:09:41.454016 139908425447168 logging_writer.py:48] [114000] global_step=114000, grad_norm=5.782368183135986, loss=1.5373899936676025
I0201 20:10:15.115850 139907762734848 logging_writer.py:48] [114100] global_step=114100, grad_norm=5.2513275146484375, loss=1.4253475666046143
I0201 20:10:48.780125 139908425447168 logging_writer.py:48] [114200] global_step=114200, grad_norm=5.371837615966797, loss=1.4646066427230835
I0201 20:11:22.441041 139907762734848 logging_writer.py:48] [114300] global_step=114300, grad_norm=5.7092509269714355, loss=1.4373235702514648
I0201 20:11:56.114712 139908425447168 logging_writer.py:48] [114400] global_step=114400, grad_norm=5.367743015289307, loss=1.479742407798767
I0201 20:12:29.858029 139907762734848 logging_writer.py:48] [114500] global_step=114500, grad_norm=5.487207889556885, loss=1.5125243663787842
I0201 20:13:03.573444 139908425447168 logging_writer.py:48] [114600] global_step=114600, grad_norm=5.546781063079834, loss=1.519546627998352
I0201 20:13:37.223641 139907762734848 logging_writer.py:48] [114700] global_step=114700, grad_norm=4.7511444091796875, loss=1.4265943765640259
I0201 20:14:10.911892 139908425447168 logging_writer.py:48] [114800] global_step=114800, grad_norm=5.292702674865723, loss=1.4672586917877197
I0201 20:14:44.569663 139907762734848 logging_writer.py:48] [114900] global_step=114900, grad_norm=4.935975074768066, loss=1.5143482685089111
I0201 20:15:18.257478 139908425447168 logging_writer.py:48] [115000] global_step=115000, grad_norm=5.577952861785889, loss=1.4508627653121948
I0201 20:15:32.549108 140070692116288 spec.py:321] Evaluating on the training split.
I0201 20:15:38.881690 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 20:15:47.404475 140070692116288 spec.py:349] Evaluating on the test split.
I0201 20:15:50.019682 140070692116288 submission_runner.py:408] Time since start: 40166.47s, 	Step: 115044, 	{'train/accuracy': 0.772859513759613, 'train/loss': 0.8542397618293762, 'validation/accuracy': 0.6820399761199951, 'validation/loss': 1.2807635068893433, 'validation/num_examples': 50000, 'test/accuracy': 0.553600013256073, 'test/loss': 1.981478214263916, 'test/num_examples': 10000, 'score': 38797.47217464447, 'total_duration': 40166.47235298157, 'accumulated_submission_time': 38797.47217464447, 'accumulated_eval_time': 1360.0370292663574, 'accumulated_logging_time': 4.967864036560059}
I0201 20:15:50.058766 139908425447168 logging_writer.py:48] [115044] accumulated_eval_time=1360.037029, accumulated_logging_time=4.967864, accumulated_submission_time=38797.472175, global_step=115044, preemption_count=0, score=38797.472175, test/accuracy=0.553600, test/loss=1.981478, test/num_examples=10000, total_duration=40166.472353, train/accuracy=0.772860, train/loss=0.854240, validation/accuracy=0.682040, validation/loss=1.280764, validation/num_examples=50000
I0201 20:16:09.223571 139908710635264 logging_writer.py:48] [115100] global_step=115100, grad_norm=4.952839374542236, loss=1.4502339363098145
I0201 20:16:42.839083 139908425447168 logging_writer.py:48] [115200] global_step=115200, grad_norm=5.465085983276367, loss=1.5031764507293701
I0201 20:17:16.504808 139908710635264 logging_writer.py:48] [115300] global_step=115300, grad_norm=5.183104515075684, loss=1.4567694664001465
I0201 20:17:50.167907 139908425447168 logging_writer.py:48] [115400] global_step=115400, grad_norm=5.0929765701293945, loss=1.4643211364746094
I0201 20:18:23.842584 139908710635264 logging_writer.py:48] [115500] global_step=115500, grad_norm=5.0995049476623535, loss=1.5264689922332764
I0201 20:18:57.591297 139908425447168 logging_writer.py:48] [115600] global_step=115600, grad_norm=5.950099468231201, loss=1.398438572883606
I0201 20:19:31.305832 139908710635264 logging_writer.py:48] [115700] global_step=115700, grad_norm=5.229259490966797, loss=1.5441782474517822
I0201 20:20:04.969698 139908425447168 logging_writer.py:48] [115800] global_step=115800, grad_norm=5.153935432434082, loss=1.422960638999939
I0201 20:20:38.656078 139908710635264 logging_writer.py:48] [115900] global_step=115900, grad_norm=5.040157794952393, loss=1.345321774482727
I0201 20:21:12.316637 139908425447168 logging_writer.py:48] [116000] global_step=116000, grad_norm=4.4159040451049805, loss=1.420326590538025
I0201 20:21:45.997282 139908710635264 logging_writer.py:48] [116100] global_step=116100, grad_norm=5.710685729980469, loss=1.5114245414733887
I0201 20:22:19.651731 139908425447168 logging_writer.py:48] [116200] global_step=116200, grad_norm=6.159189701080322, loss=1.5250523090362549
I0201 20:22:53.342594 139908710635264 logging_writer.py:48] [116300] global_step=116300, grad_norm=5.30801248550415, loss=1.4479337930679321
I0201 20:23:27.001254 139908425447168 logging_writer.py:48] [116400] global_step=116400, grad_norm=5.100286960601807, loss=1.4597171545028687
I0201 20:24:00.678071 139908710635264 logging_writer.py:48] [116500] global_step=116500, grad_norm=5.709794521331787, loss=1.3322358131408691
I0201 20:24:20.039114 140070692116288 spec.py:321] Evaluating on the training split.
I0201 20:24:26.458216 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 20:24:35.314692 140070692116288 spec.py:349] Evaluating on the test split.
I0201 20:24:37.964846 140070692116288 submission_runner.py:408] Time since start: 40694.42s, 	Step: 116559, 	{'train/accuracy': 0.7597456574440002, 'train/loss': 0.911958634853363, 'validation/accuracy': 0.681939959526062, 'validation/loss': 1.299109697341919, 'validation/num_examples': 50000, 'test/accuracy': 0.5580000281333923, 'test/loss': 1.988004207611084, 'test/num_examples': 10000, 'score': 39307.38990712166, 'total_duration': 40694.417508125305, 'accumulated_submission_time': 39307.38990712166, 'accumulated_eval_time': 1377.9627270698547, 'accumulated_logging_time': 5.016592741012573}
I0201 20:24:38.001696 139907745949440 logging_writer.py:48] [116559] accumulated_eval_time=1377.962727, accumulated_logging_time=5.016593, accumulated_submission_time=39307.389907, global_step=116559, preemption_count=0, score=39307.389907, test/accuracy=0.558000, test/loss=1.988004, test/num_examples=10000, total_duration=40694.417508, train/accuracy=0.759746, train/loss=0.911959, validation/accuracy=0.681940, validation/loss=1.299110, validation/num_examples=50000
I0201 20:24:52.125993 139907754342144 logging_writer.py:48] [116600] global_step=116600, grad_norm=5.857494354248047, loss=1.4177701473236084
I0201 20:25:25.964979 139907745949440 logging_writer.py:48] [116700] global_step=116700, grad_norm=6.315600395202637, loss=1.5054962635040283
I0201 20:25:59.653202 139907754342144 logging_writer.py:48] [116800] global_step=116800, grad_norm=5.159129619598389, loss=1.426772117614746
I0201 20:26:33.349764 139907745949440 logging_writer.py:48] [116900] global_step=116900, grad_norm=5.52519416809082, loss=1.4398959875106812
I0201 20:27:07.015495 139907754342144 logging_writer.py:48] [117000] global_step=117000, grad_norm=4.876454830169678, loss=1.41536283493042
I0201 20:27:40.696447 139907745949440 logging_writer.py:48] [117100] global_step=117100, grad_norm=5.189067840576172, loss=1.4315012693405151
I0201 20:28:14.343209 139907754342144 logging_writer.py:48] [117200] global_step=117200, grad_norm=5.370707035064697, loss=1.5273884534835815
I0201 20:28:48.029848 139907745949440 logging_writer.py:48] [117300] global_step=117300, grad_norm=5.59404993057251, loss=1.4395971298217773
I0201 20:29:21.685429 139907754342144 logging_writer.py:48] [117400] global_step=117400, grad_norm=4.725177764892578, loss=1.3823498487472534
I0201 20:29:55.369180 139907745949440 logging_writer.py:48] [117500] global_step=117500, grad_norm=6.1179728507995605, loss=1.4132614135742188
I0201 20:30:29.015371 139907754342144 logging_writer.py:48] [117600] global_step=117600, grad_norm=5.499570369720459, loss=1.5148762464523315
I0201 20:31:02.707377 139907745949440 logging_writer.py:48] [117700] global_step=117700, grad_norm=5.44780969619751, loss=1.435605764389038
I0201 20:31:36.427022 139907754342144 logging_writer.py:48] [117800] global_step=117800, grad_norm=5.7431535720825195, loss=1.4697506427764893
I0201 20:32:10.101302 139907745949440 logging_writer.py:48] [117900] global_step=117900, grad_norm=4.7655720710754395, loss=1.35853111743927
I0201 20:32:43.777631 139907754342144 logging_writer.py:48] [118000] global_step=118000, grad_norm=4.806803226470947, loss=1.4491747617721558
I0201 20:33:08.188734 140070692116288 spec.py:321] Evaluating on the training split.
I0201 20:33:14.476010 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 20:33:23.081667 140070692116288 spec.py:349] Evaluating on the test split.
I0201 20:33:25.672456 140070692116288 submission_runner.py:408] Time since start: 41222.13s, 	Step: 118074, 	{'train/accuracy': 0.7604631781578064, 'train/loss': 0.8970020413398743, 'validation/accuracy': 0.6898199915885925, 'validation/loss': 1.2545552253723145, 'validation/num_examples': 50000, 'test/accuracy': 0.5614000558853149, 'test/loss': 1.9666774272918701, 'test/num_examples': 10000, 'score': 39817.51469898224, 'total_duration': 41222.125121593475, 'accumulated_submission_time': 39817.51469898224, 'accumulated_eval_time': 1395.4464178085327, 'accumulated_logging_time': 5.063026189804077}
I0201 20:33:25.709478 139907754342144 logging_writer.py:48] [118074] accumulated_eval_time=1395.446418, accumulated_logging_time=5.063026, accumulated_submission_time=39817.514699, global_step=118074, preemption_count=0, score=39817.514699, test/accuracy=0.561400, test/loss=1.966677, test/num_examples=10000, total_duration=41222.125122, train/accuracy=0.760463, train/loss=0.897002, validation/accuracy=0.689820, validation/loss=1.254555, validation/num_examples=50000
I0201 20:33:34.773048 139908719027968 logging_writer.py:48] [118100] global_step=118100, grad_norm=4.99850606918335, loss=1.4385271072387695
I0201 20:34:08.368296 139907754342144 logging_writer.py:48] [118200] global_step=118200, grad_norm=5.23715353012085, loss=1.4507513046264648
I0201 20:34:42.053129 139908719027968 logging_writer.py:48] [118300] global_step=118300, grad_norm=5.079739570617676, loss=1.50139582157135
I0201 20:35:15.728586 139907754342144 logging_writer.py:48] [118400] global_step=118400, grad_norm=5.326623916625977, loss=1.3294458389282227
I0201 20:35:49.389296 139908719027968 logging_writer.py:48] [118500] global_step=118500, grad_norm=5.466673374176025, loss=1.469576120376587
I0201 20:36:23.063786 139907754342144 logging_writer.py:48] [118600] global_step=118600, grad_norm=5.015721797943115, loss=1.3936561346054077
I0201 20:36:56.723888 139908719027968 logging_writer.py:48] [118700] global_step=118700, grad_norm=5.511504650115967, loss=1.4562960863113403
I0201 20:37:30.411638 139907754342144 logging_writer.py:48] [118800] global_step=118800, grad_norm=5.330419540405273, loss=1.424482822418213
I0201 20:38:04.191514 139908719027968 logging_writer.py:48] [118900] global_step=118900, grad_norm=5.117476940155029, loss=1.4055936336517334
I0201 20:38:37.883339 139907754342144 logging_writer.py:48] [119000] global_step=119000, grad_norm=5.160010814666748, loss=1.4104174375534058
I0201 20:39:11.550517 139908719027968 logging_writer.py:48] [119100] global_step=119100, grad_norm=5.602569580078125, loss=1.5025341510772705
I0201 20:39:45.233305 139907754342144 logging_writer.py:48] [119200] global_step=119200, grad_norm=5.697258949279785, loss=1.4534637928009033
I0201 20:40:18.885514 139908719027968 logging_writer.py:48] [119300] global_step=119300, grad_norm=5.33085823059082, loss=1.3955851793289185
I0201 20:40:52.559800 139907754342144 logging_writer.py:48] [119400] global_step=119400, grad_norm=5.054938793182373, loss=1.3879189491271973
I0201 20:41:26.231129 139908719027968 logging_writer.py:48] [119500] global_step=119500, grad_norm=4.94392728805542, loss=1.4182186126708984
I0201 20:41:55.676915 140070692116288 spec.py:321] Evaluating on the training split.
I0201 20:42:01.929717 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 20:42:10.626566 140070692116288 spec.py:349] Evaluating on the test split.
I0201 20:42:13.300044 140070692116288 submission_runner.py:408] Time since start: 41749.75s, 	Step: 119589, 	{'train/accuracy': 0.7599050998687744, 'train/loss': 0.9128103256225586, 'validation/accuracy': 0.6882799863815308, 'validation/loss': 1.2635095119476318, 'validation/num_examples': 50000, 'test/accuracy': 0.5593000054359436, 'test/loss': 1.9771336317062378, 'test/num_examples': 10000, 'score': 40327.41791152954, 'total_duration': 41749.75271129608, 'accumulated_submission_time': 40327.41791152954, 'accumulated_eval_time': 1413.069516658783, 'accumulated_logging_time': 5.112490177154541}
I0201 20:42:13.341983 139907745949440 logging_writer.py:48] [119589] accumulated_eval_time=1413.069517, accumulated_logging_time=5.112490, accumulated_submission_time=40327.417912, global_step=119589, preemption_count=0, score=40327.417912, test/accuracy=0.559300, test/loss=1.977134, test/num_examples=10000, total_duration=41749.752711, train/accuracy=0.759905, train/loss=0.912810, validation/accuracy=0.688280, validation/loss=1.263510, validation/num_examples=50000
I0201 20:42:17.387722 139907754342144 logging_writer.py:48] [119600] global_step=119600, grad_norm=5.518733978271484, loss=1.5370595455169678
I0201 20:42:51.025659 139907745949440 logging_writer.py:48] [119700] global_step=119700, grad_norm=6.118070125579834, loss=1.458463191986084
I0201 20:43:24.718236 139907754342144 logging_writer.py:48] [119800] global_step=119800, grad_norm=5.819408416748047, loss=1.4202302694320679
I0201 20:43:58.376330 139907745949440 logging_writer.py:48] [119900] global_step=119900, grad_norm=4.966916084289551, loss=1.3793585300445557
I0201 20:44:32.229678 139907754342144 logging_writer.py:48] [120000] global_step=120000, grad_norm=5.561344623565674, loss=1.4072035551071167
I0201 20:45:05.862148 139907745949440 logging_writer.py:48] [120100] global_step=120100, grad_norm=5.423315048217773, loss=1.4046422243118286
I0201 20:45:39.548565 139907754342144 logging_writer.py:48] [120200] global_step=120200, grad_norm=5.5241498947143555, loss=1.4731889963150024
I0201 20:46:13.208077 139907745949440 logging_writer.py:48] [120300] global_step=120300, grad_norm=5.770434379577637, loss=1.4954869747161865
I0201 20:46:46.903816 139907754342144 logging_writer.py:48] [120400] global_step=120400, grad_norm=5.5018181800842285, loss=1.3637006282806396
I0201 20:47:20.561462 139907745949440 logging_writer.py:48] [120500] global_step=120500, grad_norm=5.383137226104736, loss=1.346774935722351
I0201 20:47:54.241318 139907754342144 logging_writer.py:48] [120600] global_step=120600, grad_norm=5.7156195640563965, loss=1.564605474472046
I0201 20:48:27.894081 139907745949440 logging_writer.py:48] [120700] global_step=120700, grad_norm=6.42744255065918, loss=1.549009084701538
I0201 20:49:01.559873 139907754342144 logging_writer.py:48] [120800] global_step=120800, grad_norm=6.01145601272583, loss=1.4142084121704102
I0201 20:49:35.221867 139907745949440 logging_writer.py:48] [120900] global_step=120900, grad_norm=4.971096515655518, loss=1.4037262201309204
I0201 20:50:08.903442 139907754342144 logging_writer.py:48] [121000] global_step=121000, grad_norm=6.24415397644043, loss=1.4945354461669922
I0201 20:50:42.703343 139907745949440 logging_writer.py:48] [121100] global_step=121100, grad_norm=5.697475433349609, loss=1.3320311307907104
I0201 20:50:43.532428 140070692116288 spec.py:321] Evaluating on the training split.
I0201 20:50:49.860658 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 20:50:58.381252 140070692116288 spec.py:349] Evaluating on the test split.
I0201 20:51:00.817929 140070692116288 submission_runner.py:408] Time since start: 42277.27s, 	Step: 121104, 	{'train/accuracy': 0.7638512253761292, 'train/loss': 0.8878701329231262, 'validation/accuracy': 0.6916399598121643, 'validation/loss': 1.2425378561019897, 'validation/num_examples': 50000, 'test/accuracy': 0.5651000142097473, 'test/loss': 1.9462604522705078, 'test/num_examples': 10000, 'score': 40837.543076753616, 'total_duration': 42277.27058959007, 'accumulated_submission_time': 40837.543076753616, 'accumulated_eval_time': 1430.3549864292145, 'accumulated_logging_time': 5.1657538414001465}
I0201 20:51:00.859268 139907729164032 logging_writer.py:48] [121104] accumulated_eval_time=1430.354986, accumulated_logging_time=5.165754, accumulated_submission_time=40837.543077, global_step=121104, preemption_count=0, score=40837.543077, test/accuracy=0.565100, test/loss=1.946260, test/num_examples=10000, total_duration=42277.270590, train/accuracy=0.763851, train/loss=0.887870, validation/accuracy=0.691640, validation/loss=1.242538, validation/num_examples=50000
I0201 20:51:33.508370 139907737556736 logging_writer.py:48] [121200] global_step=121200, grad_norm=6.020562648773193, loss=1.4199256896972656
I0201 20:52:07.195738 139907729164032 logging_writer.py:48] [121300] global_step=121300, grad_norm=5.521292686462402, loss=1.322951078414917
I0201 20:52:40.865156 139907737556736 logging_writer.py:48] [121400] global_step=121400, grad_norm=5.869204044342041, loss=1.4292057752609253
I0201 20:53:14.544329 139907729164032 logging_writer.py:48] [121500] global_step=121500, grad_norm=5.118605136871338, loss=1.4275131225585938
I0201 20:53:48.203264 139907737556736 logging_writer.py:48] [121600] global_step=121600, grad_norm=5.969365119934082, loss=1.4400243759155273
I0201 20:54:21.874987 139907729164032 logging_writer.py:48] [121700] global_step=121700, grad_norm=5.627349853515625, loss=1.4850882291793823
I0201 20:54:55.538406 139907737556736 logging_writer.py:48] [121800] global_step=121800, grad_norm=5.78316593170166, loss=1.4226480722427368
I0201 20:55:29.200648 139907729164032 logging_writer.py:48] [121900] global_step=121900, grad_norm=6.194716930389404, loss=1.5872281789779663
I0201 20:56:02.850336 139907737556736 logging_writer.py:48] [122000] global_step=122000, grad_norm=5.217189311981201, loss=1.3446661233901978
I0201 20:56:36.521124 139907729164032 logging_writer.py:48] [122100] global_step=122100, grad_norm=6.423165321350098, loss=1.495670199394226
I0201 20:57:10.301855 139907737556736 logging_writer.py:48] [122200] global_step=122200, grad_norm=5.8285698890686035, loss=1.3384692668914795
I0201 20:57:43.989472 139907729164032 logging_writer.py:48] [122300] global_step=122300, grad_norm=5.246294021606445, loss=1.4201058149337769
I0201 20:58:17.655745 139907737556736 logging_writer.py:48] [122400] global_step=122400, grad_norm=5.505137920379639, loss=1.3855674266815186
I0201 20:58:51.356988 139907729164032 logging_writer.py:48] [122500] global_step=122500, grad_norm=5.4685468673706055, loss=1.4194344282150269
I0201 20:59:25.031730 139907737556736 logging_writer.py:48] [122600] global_step=122600, grad_norm=6.40659236907959, loss=1.4059191942214966
I0201 20:59:30.900420 140070692116288 spec.py:321] Evaluating on the training split.
I0201 20:59:37.933763 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 20:59:46.676890 140070692116288 spec.py:349] Evaluating on the test split.
I0201 20:59:49.266914 140070692116288 submission_runner.py:408] Time since start: 42805.72s, 	Step: 122619, 	{'train/accuracy': 0.8024553656578064, 'train/loss': 0.7302818298339844, 'validation/accuracy': 0.693619966506958, 'validation/loss': 1.2337989807128906, 'validation/num_examples': 50000, 'test/accuracy': 0.5672000050544739, 'test/loss': 1.9245952367782593, 'test/num_examples': 10000, 'score': 41347.52177858353, 'total_duration': 42805.71956944466, 'accumulated_submission_time': 41347.52177858353, 'accumulated_eval_time': 1448.7214317321777, 'accumulated_logging_time': 5.217383623123169}
I0201 20:59:49.308061 139908719027968 logging_writer.py:48] [122619] accumulated_eval_time=1448.721432, accumulated_logging_time=5.217384, accumulated_submission_time=41347.521779, global_step=122619, preemption_count=0, score=41347.521779, test/accuracy=0.567200, test/loss=1.924595, test/num_examples=10000, total_duration=42805.719569, train/accuracy=0.802455, train/loss=0.730282, validation/accuracy=0.693620, validation/loss=1.233799, validation/num_examples=50000
I0201 21:00:16.932206 139908727420672 logging_writer.py:48] [122700] global_step=122700, grad_norm=4.839665412902832, loss=1.3207499980926514
I0201 21:00:50.596099 139908719027968 logging_writer.py:48] [122800] global_step=122800, grad_norm=6.39567232131958, loss=1.4331084489822388
I0201 21:01:24.303107 139908727420672 logging_writer.py:48] [122900] global_step=122900, grad_norm=5.704229354858398, loss=1.249133825302124
I0201 21:01:57.967241 139908719027968 logging_writer.py:48] [123000] global_step=123000, grad_norm=6.0785040855407715, loss=1.3827769756317139
I0201 21:02:31.648125 139908727420672 logging_writer.py:48] [123100] global_step=123100, grad_norm=6.178412437438965, loss=1.3986968994140625
I0201 21:03:05.361390 139908719027968 logging_writer.py:48] [123200] global_step=123200, grad_norm=5.675750255584717, loss=1.4721821546554565
I0201 21:03:38.949799 139908727420672 logging_writer.py:48] [123300] global_step=123300, grad_norm=5.2117390632629395, loss=1.3421859741210938
I0201 21:04:12.599219 139908719027968 logging_writer.py:48] [123400] global_step=123400, grad_norm=5.918817043304443, loss=1.4415209293365479
I0201 21:04:46.277195 139908727420672 logging_writer.py:48] [123500] global_step=123500, grad_norm=6.096278190612793, loss=1.4722784757614136
I0201 21:05:19.939683 139908719027968 logging_writer.py:48] [123600] global_step=123600, grad_norm=6.084981918334961, loss=1.3511439561843872
I0201 21:05:53.604688 139908727420672 logging_writer.py:48] [123700] global_step=123700, grad_norm=4.757109642028809, loss=1.3421330451965332
I0201 21:06:27.255750 139908719027968 logging_writer.py:48] [123800] global_step=123800, grad_norm=5.608222007751465, loss=1.3514137268066406
I0201 21:07:00.922433 139908727420672 logging_writer.py:48] [123900] global_step=123900, grad_norm=5.744372844696045, loss=1.3200079202651978
I0201 21:07:34.587677 139908719027968 logging_writer.py:48] [124000] global_step=124000, grad_norm=5.1639485359191895, loss=1.309786081314087
I0201 21:08:08.267332 139908727420672 logging_writer.py:48] [124100] global_step=124100, grad_norm=5.326410293579102, loss=1.3126156330108643
I0201 21:08:19.522656 140070692116288 spec.py:321] Evaluating on the training split.
I0201 21:08:25.773742 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 21:08:34.245526 140070692116288 spec.py:349] Evaluating on the test split.
I0201 21:08:36.870384 140070692116288 submission_runner.py:408] Time since start: 43333.32s, 	Step: 124135, 	{'train/accuracy': 0.7833027839660645, 'train/loss': 0.8094884753227234, 'validation/accuracy': 0.6947399973869324, 'validation/loss': 1.2455179691314697, 'validation/num_examples': 50000, 'test/accuracy': 0.5654000043869019, 'test/loss': 1.947636365890503, 'test/num_examples': 10000, 'score': 41857.67465591431, 'total_duration': 43333.32304549217, 'accumulated_submission_time': 41857.67465591431, 'accumulated_eval_time': 1466.0691316127777, 'accumulated_logging_time': 5.268237113952637}
I0201 21:08:36.911106 139907729164032 logging_writer.py:48] [124135] accumulated_eval_time=1466.069132, accumulated_logging_time=5.268237, accumulated_submission_time=41857.674656, global_step=124135, preemption_count=0, score=41857.674656, test/accuracy=0.565400, test/loss=1.947636, test/num_examples=10000, total_duration=43333.323045, train/accuracy=0.783303, train/loss=0.809488, validation/accuracy=0.694740, validation/loss=1.245518, validation/num_examples=50000
I0201 21:08:59.122855 139907737556736 logging_writer.py:48] [124200] global_step=124200, grad_norm=5.347890853881836, loss=1.3836960792541504
I0201 21:09:32.836670 139907729164032 logging_writer.py:48] [124300] global_step=124300, grad_norm=6.0148515701293945, loss=1.4988651275634766
I0201 21:10:06.442527 139907737556736 logging_writer.py:48] [124400] global_step=124400, grad_norm=5.148182392120361, loss=1.39609956741333
I0201 21:10:40.085917 139907729164032 logging_writer.py:48] [124500] global_step=124500, grad_norm=5.536540985107422, loss=1.4175106287002563
I0201 21:11:13.770650 139907737556736 logging_writer.py:48] [124600] global_step=124600, grad_norm=5.495622634887695, loss=1.302152395248413
I0201 21:11:47.424249 139907729164032 logging_writer.py:48] [124700] global_step=124700, grad_norm=5.814621448516846, loss=1.3720051050186157
I0201 21:12:21.119247 139907737556736 logging_writer.py:48] [124800] global_step=124800, grad_norm=6.2638163566589355, loss=1.4084441661834717
I0201 21:12:54.781357 139907729164032 logging_writer.py:48] [124900] global_step=124900, grad_norm=5.729034423828125, loss=1.3874396085739136
I0201 21:13:28.455849 139907737556736 logging_writer.py:48] [125000] global_step=125000, grad_norm=5.933060646057129, loss=1.3685412406921387
I0201 21:14:02.108851 139907729164032 logging_writer.py:48] [125100] global_step=125100, grad_norm=5.880980491638184, loss=1.4298293590545654
I0201 21:14:35.803357 139907737556736 logging_writer.py:48] [125200] global_step=125200, grad_norm=5.723941326141357, loss=1.458184003829956
I0201 21:15:09.465574 139907729164032 logging_writer.py:48] [125300] global_step=125300, grad_norm=5.865568161010742, loss=1.3614559173583984
I0201 21:15:43.236508 139907737556736 logging_writer.py:48] [125400] global_step=125400, grad_norm=6.031179904937744, loss=1.3727030754089355
I0201 21:16:16.935688 139907729164032 logging_writer.py:48] [125500] global_step=125500, grad_norm=5.890716552734375, loss=1.511544108390808
I0201 21:16:50.642126 139907737556736 logging_writer.py:48] [125600] global_step=125600, grad_norm=6.041395664215088, loss=1.4053922891616821
I0201 21:17:06.946063 140070692116288 spec.py:321] Evaluating on the training split.
I0201 21:17:13.424444 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 21:17:22.075549 140070692116288 spec.py:349] Evaluating on the test split.
I0201 21:17:24.679643 140070692116288 submission_runner.py:408] Time since start: 43861.13s, 	Step: 125650, 	{'train/accuracy': 0.7840800285339355, 'train/loss': 0.8092227578163147, 'validation/accuracy': 0.6946600079536438, 'validation/loss': 1.2339342832565308, 'validation/num_examples': 50000, 'test/accuracy': 0.572700023651123, 'test/loss': 1.9287670850753784, 'test/num_examples': 10000, 'score': 42367.645431280136, 'total_duration': 43861.1323056221, 'accumulated_submission_time': 42367.645431280136, 'accumulated_eval_time': 1483.8026728630066, 'accumulated_logging_time': 5.32036828994751}
I0201 21:17:24.722867 139907737556736 logging_writer.py:48] [125650] accumulated_eval_time=1483.802673, accumulated_logging_time=5.320368, accumulated_submission_time=42367.645431, global_step=125650, preemption_count=0, score=42367.645431, test/accuracy=0.572700, test/loss=1.928767, test/num_examples=10000, total_duration=43861.132306, train/accuracy=0.784080, train/loss=0.809223, validation/accuracy=0.694660, validation/loss=1.233934, validation/num_examples=50000
I0201 21:17:41.884288 139908710635264 logging_writer.py:48] [125700] global_step=125700, grad_norm=5.654390335083008, loss=1.3785290718078613
I0201 21:18:15.568046 139907737556736 logging_writer.py:48] [125800] global_step=125800, grad_norm=5.784019947052002, loss=1.3029807806015015
I0201 21:18:49.257084 139908710635264 logging_writer.py:48] [125900] global_step=125900, grad_norm=5.496156692504883, loss=1.3829765319824219
I0201 21:19:22.950087 139907737556736 logging_writer.py:48] [126000] global_step=126000, grad_norm=5.902841567993164, loss=1.4335300922393799
I0201 21:19:56.617179 139908710635264 logging_writer.py:48] [126100] global_step=126100, grad_norm=5.569332599639893, loss=1.2672502994537354
I0201 21:20:30.295125 139907737556736 logging_writer.py:48] [126200] global_step=126200, grad_norm=5.7656731605529785, loss=1.346121907234192
I0201 21:21:03.949445 139908710635264 logging_writer.py:48] [126300] global_step=126300, grad_norm=6.409089088439941, loss=1.3802690505981445
I0201 21:21:37.637540 139907737556736 logging_writer.py:48] [126400] global_step=126400, grad_norm=5.385068893432617, loss=1.3112363815307617
I0201 21:22:11.361564 139908710635264 logging_writer.py:48] [126500] global_step=126500, grad_norm=7.000145435333252, loss=1.328683614730835
I0201 21:22:45.014896 139907737556736 logging_writer.py:48] [126600] global_step=126600, grad_norm=5.738199234008789, loss=1.2396881580352783
I0201 21:23:18.702241 139908710635264 logging_writer.py:48] [126700] global_step=126700, grad_norm=5.448823928833008, loss=1.2886027097702026
I0201 21:23:52.372437 139907737556736 logging_writer.py:48] [126800] global_step=126800, grad_norm=6.770575523376465, loss=1.289308786392212
I0201 21:24:26.033365 139908710635264 logging_writer.py:48] [126900] global_step=126900, grad_norm=5.701718807220459, loss=1.3040013313293457
I0201 21:24:59.727336 139907737556736 logging_writer.py:48] [127000] global_step=127000, grad_norm=5.733895301818848, loss=1.3747148513793945
I0201 21:25:33.366723 139908710635264 logging_writer.py:48] [127100] global_step=127100, grad_norm=5.726292133331299, loss=1.3322983980178833
I0201 21:25:54.746942 140070692116288 spec.py:321] Evaluating on the training split.
I0201 21:26:00.983446 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 21:26:09.716944 140070692116288 spec.py:349] Evaluating on the test split.
I0201 21:26:12.367403 140070692116288 submission_runner.py:408] Time since start: 44388.82s, 	Step: 127165, 	{'train/accuracy': 0.778340220451355, 'train/loss': 0.8295766711235046, 'validation/accuracy': 0.7000399827957153, 'validation/loss': 1.2225801944732666, 'validation/num_examples': 50000, 'test/accuracy': 0.5708000063896179, 'test/loss': 1.908912181854248, 'test/num_examples': 10000, 'score': 42877.60603952408, 'total_duration': 44388.82006406784, 'accumulated_submission_time': 42877.60603952408, 'accumulated_eval_time': 1501.4231088161469, 'accumulated_logging_time': 5.373712062835693}
I0201 21:26:12.409872 139907745949440 logging_writer.py:48] [127165] accumulated_eval_time=1501.423109, accumulated_logging_time=5.373712, accumulated_submission_time=42877.606040, global_step=127165, preemption_count=0, score=42877.606040, test/accuracy=0.570800, test/loss=1.908912, test/num_examples=10000, total_duration=44388.820064, train/accuracy=0.778340, train/loss=0.829577, validation/accuracy=0.700040, validation/loss=1.222580, validation/num_examples=50000
I0201 21:26:24.505882 139907754342144 logging_writer.py:48] [127200] global_step=127200, grad_norm=6.257546424865723, loss=1.3889358043670654
I0201 21:26:58.160797 139907745949440 logging_writer.py:48] [127300] global_step=127300, grad_norm=5.724813461303711, loss=1.5244417190551758
I0201 21:27:31.804954 139907754342144 logging_writer.py:48] [127400] global_step=127400, grad_norm=7.3605170249938965, loss=1.4032334089279175
I0201 21:28:05.486754 139907745949440 logging_writer.py:48] [127500] global_step=127500, grad_norm=5.967270374298096, loss=1.334703803062439
I0201 21:28:39.243109 139907754342144 logging_writer.py:48] [127600] global_step=127600, grad_norm=6.448019504547119, loss=1.3155183792114258
I0201 21:29:12.960377 139907745949440 logging_writer.py:48] [127700] global_step=127700, grad_norm=6.291741371154785, loss=1.3726954460144043
I0201 21:29:46.621783 139907754342144 logging_writer.py:48] [127800] global_step=127800, grad_norm=6.328939437866211, loss=1.3957582712173462
I0201 21:30:20.294918 139907745949440 logging_writer.py:48] [127900] global_step=127900, grad_norm=6.125749111175537, loss=1.448981523513794
I0201 21:30:53.952395 139907754342144 logging_writer.py:48] [128000] global_step=128000, grad_norm=6.199615478515625, loss=1.3916889429092407
I0201 21:31:27.643911 139907745949440 logging_writer.py:48] [128100] global_step=128100, grad_norm=7.186184406280518, loss=1.4291728734970093
I0201 21:32:01.290292 139907754342144 logging_writer.py:48] [128200] global_step=128200, grad_norm=5.966763496398926, loss=1.317618489265442
I0201 21:32:34.996528 139907745949440 logging_writer.py:48] [128300] global_step=128300, grad_norm=5.981594562530518, loss=1.4220726490020752
I0201 21:33:08.629298 139907754342144 logging_writer.py:48] [128400] global_step=128400, grad_norm=6.203113555908203, loss=1.3626149892807007
I0201 21:33:42.327242 139907745949440 logging_writer.py:48] [128500] global_step=128500, grad_norm=5.931863784790039, loss=1.3758213520050049
I0201 21:34:15.966347 139907754342144 logging_writer.py:48] [128600] global_step=128600, grad_norm=5.611818313598633, loss=1.2922579050064087
I0201 21:34:42.399555 140070692116288 spec.py:321] Evaluating on the training split.
I0201 21:34:49.022300 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 21:34:57.654211 140070692116288 spec.py:349] Evaluating on the test split.
I0201 21:35:00.241222 140070692116288 submission_runner.py:408] Time since start: 44916.69s, 	Step: 128680, 	{'train/accuracy': 0.78324294090271, 'train/loss': 0.8075078129768372, 'validation/accuracy': 0.7049199938774109, 'validation/loss': 1.190179467201233, 'validation/num_examples': 50000, 'test/accuracy': 0.5818000435829163, 'test/loss': 1.8730347156524658, 'test/num_examples': 10000, 'score': 43387.53185915947, 'total_duration': 44916.69388747215, 'accumulated_submission_time': 43387.53185915947, 'accumulated_eval_time': 1519.2647440433502, 'accumulated_logging_time': 5.426731824874878}
I0201 21:35:00.280923 139908719027968 logging_writer.py:48] [128680] accumulated_eval_time=1519.264744, accumulated_logging_time=5.426732, accumulated_submission_time=43387.531859, global_step=128680, preemption_count=0, score=43387.531859, test/accuracy=0.581800, test/loss=1.873035, test/num_examples=10000, total_duration=44916.693887, train/accuracy=0.783243, train/loss=0.807508, validation/accuracy=0.704920, validation/loss=1.190179, validation/num_examples=50000
I0201 21:35:07.314395 139908727420672 logging_writer.py:48] [128700] global_step=128700, grad_norm=6.5253143310546875, loss=1.2077311277389526
I0201 21:35:40.890885 139908719027968 logging_writer.py:48] [128800] global_step=128800, grad_norm=5.913821697235107, loss=1.3199248313903809
I0201 21:36:14.551174 139908727420672 logging_writer.py:48] [128900] global_step=128900, grad_norm=5.503734111785889, loss=1.2994930744171143
I0201 21:36:48.224807 139908719027968 logging_writer.py:48] [129000] global_step=129000, grad_norm=5.700748920440674, loss=1.2577643394470215
I0201 21:37:21.923428 139908727420672 logging_writer.py:48] [129100] global_step=129100, grad_norm=5.2935471534729, loss=1.3708921670913696
I0201 21:37:55.591469 139908719027968 logging_writer.py:48] [129200] global_step=129200, grad_norm=6.177609443664551, loss=1.3793786764144897
I0201 21:38:29.265281 139908727420672 logging_writer.py:48] [129300] global_step=129300, grad_norm=6.626722812652588, loss=1.418049931526184
I0201 21:39:02.925983 139908719027968 logging_writer.py:48] [129400] global_step=129400, grad_norm=5.893884658813477, loss=1.2042797803878784
I0201 21:39:36.601297 139908727420672 logging_writer.py:48] [129500] global_step=129500, grad_norm=6.150693416595459, loss=1.3041311502456665
I0201 21:40:10.265831 139908719027968 logging_writer.py:48] [129600] global_step=129600, grad_norm=5.750687599182129, loss=1.2042417526245117
I0201 21:40:43.934894 139908727420672 logging_writer.py:48] [129700] global_step=129700, grad_norm=6.872608661651611, loss=1.3238452672958374
I0201 21:41:17.641081 139908719027968 logging_writer.py:48] [129800] global_step=129800, grad_norm=5.867978096008301, loss=1.2450491189956665
I0201 21:41:51.283159 139908727420672 logging_writer.py:48] [129900] global_step=129900, grad_norm=6.068450927734375, loss=1.3485642671585083
I0201 21:42:24.939709 139908719027968 logging_writer.py:48] [130000] global_step=130000, grad_norm=6.542207717895508, loss=1.4255034923553467
I0201 21:42:58.627870 139908727420672 logging_writer.py:48] [130100] global_step=130100, grad_norm=6.036626815795898, loss=1.2233227491378784
I0201 21:43:30.421728 140070692116288 spec.py:321] Evaluating on the training split.
I0201 21:43:36.869871 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 21:43:45.195747 140070692116288 spec.py:349] Evaluating on the test split.
I0201 21:43:47.801131 140070692116288 submission_runner.py:408] Time since start: 45444.25s, 	Step: 130196, 	{'train/accuracy': 0.7864915132522583, 'train/loss': 0.789760947227478, 'validation/accuracy': 0.7076999545097351, 'validation/loss': 1.1743167638778687, 'validation/num_examples': 50000, 'test/accuracy': 0.5768000483512878, 'test/loss': 1.8925039768218994, 'test/num_examples': 10000, 'score': 43897.6113114357, 'total_duration': 45444.25380349159, 'accumulated_submission_time': 43897.6113114357, 'accumulated_eval_time': 1536.644121170044, 'accumulated_logging_time': 5.475257635116577}
I0201 21:43:47.841590 139907745949440 logging_writer.py:48] [130196] accumulated_eval_time=1536.644121, accumulated_logging_time=5.475258, accumulated_submission_time=43897.611311, global_step=130196, preemption_count=0, score=43897.611311, test/accuracy=0.576800, test/loss=1.892504, test/num_examples=10000, total_duration=45444.253803, train/accuracy=0.786492, train/loss=0.789761, validation/accuracy=0.707700, validation/loss=1.174317, validation/num_examples=50000
I0201 21:43:49.545696 139907754342144 logging_writer.py:48] [130200] global_step=130200, grad_norm=6.4555888175964355, loss=1.2674106359481812
I0201 21:44:23.208204 139907745949440 logging_writer.py:48] [130300] global_step=130300, grad_norm=5.271183967590332, loss=1.2297064065933228
I0201 21:44:56.916410 139907754342144 logging_writer.py:48] [130400] global_step=130400, grad_norm=6.609095573425293, loss=1.2880202531814575
I0201 21:45:30.574805 139907745949440 logging_writer.py:48] [130500] global_step=130500, grad_norm=6.372915267944336, loss=1.2195473909378052
I0201 21:46:04.256278 139907754342144 logging_writer.py:48] [130600] global_step=130600, grad_norm=5.769291400909424, loss=1.2614896297454834
I0201 21:46:37.911450 139907745949440 logging_writer.py:48] [130700] global_step=130700, grad_norm=6.423443794250488, loss=1.336463212966919
I0201 21:47:11.604474 139907754342144 logging_writer.py:48] [130800] global_step=130800, grad_norm=5.723543167114258, loss=1.278295874595642
I0201 21:47:45.370286 139907745949440 logging_writer.py:48] [130900] global_step=130900, grad_norm=5.913618087768555, loss=1.266732931137085
I0201 21:48:19.128243 139907754342144 logging_writer.py:48] [131000] global_step=131000, grad_norm=5.9626922607421875, loss=1.242204189300537
I0201 21:48:52.769400 139907745949440 logging_writer.py:48] [131100] global_step=131100, grad_norm=5.504871368408203, loss=1.218479871749878
I0201 21:49:26.473094 139907754342144 logging_writer.py:48] [131200] global_step=131200, grad_norm=6.206666469573975, loss=1.3269011974334717
I0201 21:50:00.121723 139907745949440 logging_writer.py:48] [131300] global_step=131300, grad_norm=5.576992034912109, loss=1.248763084411621
I0201 21:50:33.816009 139907754342144 logging_writer.py:48] [131400] global_step=131400, grad_norm=6.011138916015625, loss=1.3198392391204834
I0201 21:51:07.466032 139907745949440 logging_writer.py:48] [131500] global_step=131500, grad_norm=7.082121849060059, loss=1.4171442985534668
I0201 21:51:41.169651 139907754342144 logging_writer.py:48] [131600] global_step=131600, grad_norm=5.968186855316162, loss=1.349724531173706
I0201 21:52:14.816828 139907745949440 logging_writer.py:48] [131700] global_step=131700, grad_norm=6.871318817138672, loss=1.32044517993927
I0201 21:52:17.998532 140070692116288 spec.py:321] Evaluating on the training split.
I0201 21:52:24.416165 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 21:52:33.041209 140070692116288 spec.py:349] Evaluating on the test split.
I0201 21:52:35.516416 140070692116288 submission_runner.py:408] Time since start: 45971.97s, 	Step: 131711, 	{'train/accuracy': 0.8223453164100647, 'train/loss': 0.6572174429893494, 'validation/accuracy': 0.7078799605369568, 'validation/loss': 1.1762797832489014, 'validation/num_examples': 50000, 'test/accuracy': 0.5764000415802002, 'test/loss': 1.8879553079605103, 'test/num_examples': 10000, 'score': 44407.7055516243, 'total_duration': 45971.96899843216, 'accumulated_submission_time': 44407.7055516243, 'accumulated_eval_time': 1554.1618838310242, 'accumulated_logging_time': 5.526075601577759}
I0201 21:52:35.553602 139908710635264 logging_writer.py:48] [131711] accumulated_eval_time=1554.161884, accumulated_logging_time=5.526076, accumulated_submission_time=44407.705552, global_step=131711, preemption_count=0, score=44407.705552, test/accuracy=0.576400, test/loss=1.887955, test/num_examples=10000, total_duration=45971.968998, train/accuracy=0.822345, train/loss=0.657217, validation/accuracy=0.707880, validation/loss=1.176280, validation/num_examples=50000
I0201 21:53:05.788071 139908719027968 logging_writer.py:48] [131800] global_step=131800, grad_norm=6.552671909332275, loss=1.3057106733322144
I0201 21:53:39.438677 139908710635264 logging_writer.py:48] [131900] global_step=131900, grad_norm=5.838247776031494, loss=1.293389081954956
I0201 21:54:13.137224 139908719027968 logging_writer.py:48] [132000] global_step=132000, grad_norm=5.417843341827393, loss=1.206752061843872
I0201 21:54:46.805030 139908710635264 logging_writer.py:48] [132100] global_step=132100, grad_norm=6.182950019836426, loss=1.3078995943069458
I0201 21:55:20.460180 139908719027968 logging_writer.py:48] [132200] global_step=132200, grad_norm=5.7964887619018555, loss=1.3228658437728882
I0201 21:55:54.139284 139908710635264 logging_writer.py:48] [132300] global_step=132300, grad_norm=6.927787780761719, loss=1.2604095935821533
I0201 21:56:27.810949 139908719027968 logging_writer.py:48] [132400] global_step=132400, grad_norm=6.1954498291015625, loss=1.2690191268920898
I0201 21:57:01.482648 139908710635264 logging_writer.py:48] [132500] global_step=132500, grad_norm=5.496326923370361, loss=1.1718937158584595
I0201 21:57:35.168447 139908719027968 logging_writer.py:48] [132600] global_step=132600, grad_norm=6.962198257446289, loss=1.3205132484436035
I0201 21:58:08.850817 139908710635264 logging_writer.py:48] [132700] global_step=132700, grad_norm=6.394963264465332, loss=1.3200488090515137
I0201 21:58:42.526054 139908719027968 logging_writer.py:48] [132800] global_step=132800, grad_norm=6.106956481933594, loss=1.2262275218963623
I0201 21:59:16.200258 139908710635264 logging_writer.py:48] [132900] global_step=132900, grad_norm=5.6968560218811035, loss=1.306388258934021
I0201 21:59:49.855319 139908719027968 logging_writer.py:48] [133000] global_step=133000, grad_norm=6.633882999420166, loss=1.4183984994888306
I0201 22:00:23.590808 139908710635264 logging_writer.py:48] [133100] global_step=133100, grad_norm=6.209236145019531, loss=1.2458240985870361
I0201 22:00:57.288141 139908719027968 logging_writer.py:48] [133200] global_step=133200, grad_norm=6.207973003387451, loss=1.4043152332305908
I0201 22:01:05.520295 140070692116288 spec.py:321] Evaluating on the training split.
I0201 22:01:11.818745 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 22:01:20.474098 140070692116288 spec.py:349] Evaluating on the test split.
I0201 22:01:23.105760 140070692116288 submission_runner.py:408] Time since start: 46499.56s, 	Step: 133226, 	{'train/accuracy': 0.8046476244926453, 'train/loss': 0.7072620391845703, 'validation/accuracy': 0.7079600095748901, 'validation/loss': 1.179681420326233, 'validation/num_examples': 50000, 'test/accuracy': 0.581000030040741, 'test/loss': 1.8739051818847656, 'test/num_examples': 10000, 'score': 44917.60682630539, 'total_duration': 46499.558420181274, 'accumulated_submission_time': 44917.60682630539, 'accumulated_eval_time': 1571.7473032474518, 'accumulated_logging_time': 5.575676918029785}
I0201 22:01:23.147972 139907762734848 logging_writer.py:48] [133226] accumulated_eval_time=1571.747303, accumulated_logging_time=5.575677, accumulated_submission_time=44917.606826, global_step=133226, preemption_count=0, score=44917.606826, test/accuracy=0.581000, test/loss=1.873905, test/num_examples=10000, total_duration=46499.558420, train/accuracy=0.804648, train/loss=0.707262, validation/accuracy=0.707960, validation/loss=1.179681, validation/num_examples=50000
I0201 22:01:48.405265 139908425447168 logging_writer.py:48] [133300] global_step=133300, grad_norm=5.89507532119751, loss=1.2352992296218872
I0201 22:02:22.057800 139907762734848 logging_writer.py:48] [133400] global_step=133400, grad_norm=6.665472507476807, loss=1.352313756942749
I0201 22:02:55.769717 139908425447168 logging_writer.py:48] [133500] global_step=133500, grad_norm=6.114542007446289, loss=1.2977795600891113
I0201 22:03:29.421343 139907762734848 logging_writer.py:48] [133600] global_step=133600, grad_norm=6.463685989379883, loss=1.2532581090927124
I0201 22:04:03.127083 139908425447168 logging_writer.py:48] [133700] global_step=133700, grad_norm=6.270256519317627, loss=1.2502753734588623
I0201 22:04:36.785015 139907762734848 logging_writer.py:48] [133800] global_step=133800, grad_norm=6.515168190002441, loss=1.3818608522415161
I0201 22:05:10.479829 139908425447168 logging_writer.py:48] [133900] global_step=133900, grad_norm=5.733111381530762, loss=1.262974500656128
I0201 22:05:44.143955 139907762734848 logging_writer.py:48] [134000] global_step=134000, grad_norm=6.08299446105957, loss=1.2509491443634033
I0201 22:06:17.841499 139908425447168 logging_writer.py:48] [134100] global_step=134100, grad_norm=6.086215496063232, loss=1.2638764381408691
I0201 22:06:51.601324 139907762734848 logging_writer.py:48] [134200] global_step=134200, grad_norm=6.082556247711182, loss=1.241799235343933
I0201 22:07:25.321106 139908425447168 logging_writer.py:48] [134300] global_step=134300, grad_norm=7.1617350578308105, loss=1.2754532098770142
I0201 22:07:58.982298 139907762734848 logging_writer.py:48] [134400] global_step=134400, grad_norm=5.8424577713012695, loss=1.1917834281921387
I0201 22:08:32.655695 139908425447168 logging_writer.py:48] [134500] global_step=134500, grad_norm=6.5749101638793945, loss=1.3359135389328003
I0201 22:09:06.309164 139907762734848 logging_writer.py:48] [134600] global_step=134600, grad_norm=6.421022415161133, loss=1.174843668937683
I0201 22:09:39.996160 139908425447168 logging_writer.py:48] [134700] global_step=134700, grad_norm=6.49591588973999, loss=1.334829568862915
I0201 22:09:53.269056 140070692116288 spec.py:321] Evaluating on the training split.
I0201 22:09:59.558070 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 22:10:08.408842 140070692116288 spec.py:349] Evaluating on the test split.
I0201 22:10:11.506074 140070692116288 submission_runner.py:408] Time since start: 47027.96s, 	Step: 134741, 	{'train/accuracy': 0.8073381781578064, 'train/loss': 0.7013617157936096, 'validation/accuracy': 0.7153399586677551, 'validation/loss': 1.1508142948150635, 'validation/num_examples': 50000, 'test/accuracy': 0.5949000120162964, 'test/loss': 1.822237253189087, 'test/num_examples': 10000, 'score': 45427.66591835022, 'total_duration': 47027.958698511124, 'accumulated_submission_time': 45427.66591835022, 'accumulated_eval_time': 1589.98424077034, 'accumulated_logging_time': 5.627666711807251}
I0201 22:10:11.541987 139907745949440 logging_writer.py:48] [134741] accumulated_eval_time=1589.984241, accumulated_logging_time=5.627667, accumulated_submission_time=45427.665918, global_step=134741, preemption_count=0, score=45427.665918, test/accuracy=0.594900, test/loss=1.822237, test/num_examples=10000, total_duration=47027.958699, train/accuracy=0.807338, train/loss=0.701362, validation/accuracy=0.715340, validation/loss=1.150814, validation/num_examples=50000
I0201 22:10:32.076120 139907754342144 logging_writer.py:48] [134800] global_step=134800, grad_norm=5.885552883148193, loss=1.2204253673553467
I0201 22:11:05.746871 139907745949440 logging_writer.py:48] [134900] global_step=134900, grad_norm=6.143280506134033, loss=1.1577435731887817
I0201 22:11:39.457721 139907754342144 logging_writer.py:48] [135000] global_step=135000, grad_norm=6.755051136016846, loss=1.2393380403518677
I0201 22:12:13.111945 139907745949440 logging_writer.py:48] [135100] global_step=135100, grad_norm=6.4554290771484375, loss=1.2229063510894775
I0201 22:12:46.795914 139907754342144 logging_writer.py:48] [135200] global_step=135200, grad_norm=6.462695598602295, loss=1.3616793155670166
I0201 22:13:20.564740 139907745949440 logging_writer.py:48] [135300] global_step=135300, grad_norm=6.392474174499512, loss=1.3153284788131714
I0201 22:13:54.254153 139907754342144 logging_writer.py:48] [135400] global_step=135400, grad_norm=7.2639641761779785, loss=1.3661253452301025
I0201 22:14:27.934254 139907745949440 logging_writer.py:48] [135500] global_step=135500, grad_norm=6.187065124511719, loss=1.3139140605926514
I0201 22:15:01.620039 139907754342144 logging_writer.py:48] [135600] global_step=135600, grad_norm=6.587935447692871, loss=1.2587330341339111
I0201 22:15:35.279950 139907745949440 logging_writer.py:48] [135700] global_step=135700, grad_norm=7.00932502746582, loss=1.2229827642440796
I0201 22:16:08.954624 139907754342144 logging_writer.py:48] [135800] global_step=135800, grad_norm=5.921555519104004, loss=1.254554033279419
I0201 22:16:42.620067 139907745949440 logging_writer.py:48] [135900] global_step=135900, grad_norm=6.027334690093994, loss=1.1813156604766846
I0201 22:17:16.306394 139907754342144 logging_writer.py:48] [136000] global_step=136000, grad_norm=6.221286296844482, loss=1.2667527198791504
I0201 22:17:49.953694 139907745949440 logging_writer.py:48] [136100] global_step=136100, grad_norm=7.479290008544922, loss=1.3056505918502808
I0201 22:18:23.653469 139907754342144 logging_writer.py:48] [136200] global_step=136200, grad_norm=6.3552093505859375, loss=1.2835158109664917
I0201 22:18:41.648942 140070692116288 spec.py:321] Evaluating on the training split.
I0201 22:18:47.926897 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 22:18:56.412338 140070692116288 spec.py:349] Evaluating on the test split.
I0201 22:18:58.998582 140070692116288 submission_runner.py:408] Time since start: 47555.45s, 	Step: 136255, 	{'train/accuracy': 0.8015385866165161, 'train/loss': 0.7381904721260071, 'validation/accuracy': 0.7129799723625183, 'validation/loss': 1.1590455770492554, 'validation/num_examples': 50000, 'test/accuracy': 0.5845000147819519, 'test/loss': 1.8409979343414307, 'test/num_examples': 10000, 'score': 45937.36339020729, 'total_duration': 47555.45125055313, 'accumulated_submission_time': 45937.36339020729, 'accumulated_eval_time': 1607.3338513374329, 'accumulated_logging_time': 6.020332336425781}
I0201 22:18:59.045593 139908425447168 logging_writer.py:48] [136255] accumulated_eval_time=1607.333851, accumulated_logging_time=6.020332, accumulated_submission_time=45937.363390, global_step=136255, preemption_count=0, score=45937.363390, test/accuracy=0.584500, test/loss=1.840998, test/num_examples=10000, total_duration=47555.451251, train/accuracy=0.801539, train/loss=0.738190, validation/accuracy=0.712980, validation/loss=1.159046, validation/num_examples=50000
I0201 22:19:14.579123 139908710635264 logging_writer.py:48] [136300] global_step=136300, grad_norm=6.0701985359191895, loss=1.175936222076416
I0201 22:19:48.282840 139908425447168 logging_writer.py:48] [136400] global_step=136400, grad_norm=6.3042683601379395, loss=1.2085493803024292
I0201 22:20:21.935625 139908710635264 logging_writer.py:48] [136500] global_step=136500, grad_norm=6.894683361053467, loss=1.2319364547729492
I0201 22:20:55.632471 139908425447168 logging_writer.py:48] [136600] global_step=136600, grad_norm=5.555335521697998, loss=1.1718581914901733
I0201 22:21:29.280162 139908710635264 logging_writer.py:48] [136700] global_step=136700, grad_norm=6.39702033996582, loss=1.2635780572891235
I0201 22:22:02.956894 139908425447168 logging_writer.py:48] [136800] global_step=136800, grad_norm=6.879404544830322, loss=1.2423856258392334
I0201 22:22:36.620685 139908710635264 logging_writer.py:48] [136900] global_step=136900, grad_norm=5.636392116546631, loss=1.1987638473510742
I0201 22:23:10.294450 139908425447168 logging_writer.py:48] [137000] global_step=137000, grad_norm=6.640323162078857, loss=1.20962655544281
I0201 22:23:43.953519 139908710635264 logging_writer.py:48] [137100] global_step=137100, grad_norm=6.8822150230407715, loss=1.3257286548614502
I0201 22:24:17.631492 139908425447168 logging_writer.py:48] [137200] global_step=137200, grad_norm=6.628620147705078, loss=1.2243574857711792
I0201 22:24:51.278254 139908710635264 logging_writer.py:48] [137300] global_step=137300, grad_norm=6.14363956451416, loss=1.2549604177474976
I0201 22:25:24.951944 139908425447168 logging_writer.py:48] [137400] global_step=137400, grad_norm=6.19988489151001, loss=1.207924485206604
I0201 22:25:58.804690 139908710635264 logging_writer.py:48] [137500] global_step=137500, grad_norm=6.059950828552246, loss=1.271608829498291
I0201 22:26:32.454430 139908425447168 logging_writer.py:48] [137600] global_step=137600, grad_norm=5.836740493774414, loss=1.155049204826355
I0201 22:27:06.134115 139908710635264 logging_writer.py:48] [137700] global_step=137700, grad_norm=6.37459659576416, loss=1.3189502954483032
I0201 22:27:29.231438 140070692116288 spec.py:321] Evaluating on the training split.
I0201 22:27:35.461232 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 22:27:44.275346 140070692116288 spec.py:349] Evaluating on the test split.
I0201 22:27:46.881188 140070692116288 submission_runner.py:408] Time since start: 48083.33s, 	Step: 137770, 	{'train/accuracy': 0.8052256107330322, 'train/loss': 0.7155845761299133, 'validation/accuracy': 0.7154799699783325, 'validation/loss': 1.1480318307876587, 'validation/num_examples': 50000, 'test/accuracy': 0.5848000049591064, 'test/loss': 1.8547910451889038, 'test/num_examples': 10000, 'score': 46447.486229896545, 'total_duration': 48083.33385229111, 'accumulated_submission_time': 46447.486229896545, 'accumulated_eval_time': 1624.9835669994354, 'accumulated_logging_time': 6.077698230743408}
I0201 22:27:46.922042 139907762734848 logging_writer.py:48] [137770] accumulated_eval_time=1624.983567, accumulated_logging_time=6.077698, accumulated_submission_time=46447.486230, global_step=137770, preemption_count=0, score=46447.486230, test/accuracy=0.584800, test/loss=1.854791, test/num_examples=10000, total_duration=48083.333852, train/accuracy=0.805226, train/loss=0.715585, validation/accuracy=0.715480, validation/loss=1.148032, validation/num_examples=50000
I0201 22:27:57.333674 139908719027968 logging_writer.py:48] [137800] global_step=137800, grad_norm=6.007528305053711, loss=1.2361723184585571
I0201 22:28:30.963686 139907762734848 logging_writer.py:48] [137900] global_step=137900, grad_norm=6.4994893074035645, loss=1.2386417388916016
I0201 22:29:04.626351 139908719027968 logging_writer.py:48] [138000] global_step=138000, grad_norm=6.394931793212891, loss=1.2308194637298584
I0201 22:29:38.299529 139907762734848 logging_writer.py:48] [138100] global_step=138100, grad_norm=6.387252330780029, loss=1.2281289100646973
I0201 22:30:11.960892 139908719027968 logging_writer.py:48] [138200] global_step=138200, grad_norm=6.475555419921875, loss=1.2506529092788696
I0201 22:30:45.645207 139907762734848 logging_writer.py:48] [138300] global_step=138300, grad_norm=6.9613261222839355, loss=1.220587968826294
I0201 22:31:19.296587 139908719027968 logging_writer.py:48] [138400] global_step=138400, grad_norm=6.496265888214111, loss=1.3109170198440552
I0201 22:31:53.085187 139907762734848 logging_writer.py:48] [138500] global_step=138500, grad_norm=6.145486831665039, loss=1.1572070121765137
I0201 22:32:26.798550 139908719027968 logging_writer.py:48] [138600] global_step=138600, grad_norm=7.175370216369629, loss=1.25081467628479
I0201 22:33:00.468511 139907762734848 logging_writer.py:48] [138700] global_step=138700, grad_norm=6.481269836425781, loss=1.2861254215240479
I0201 22:33:34.128506 139908719027968 logging_writer.py:48] [138800] global_step=138800, grad_norm=6.352991104125977, loss=1.1432836055755615
I0201 22:34:07.805389 139907762734848 logging_writer.py:48] [138900] global_step=138900, grad_norm=6.902617454528809, loss=1.2222355604171753
I0201 22:34:41.458361 139908719027968 logging_writer.py:48] [139000] global_step=139000, grad_norm=6.337747573852539, loss=1.292022466659546
I0201 22:35:15.144196 139907762734848 logging_writer.py:48] [139100] global_step=139100, grad_norm=6.743939399719238, loss=1.195257544517517
I0201 22:35:48.805703 139908719027968 logging_writer.py:48] [139200] global_step=139200, grad_norm=6.255166053771973, loss=1.2113120555877686
I0201 22:36:16.913323 140070692116288 spec.py:321] Evaluating on the training split.
I0201 22:36:23.250001 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 22:36:31.965661 140070692116288 spec.py:349] Evaluating on the test split.
I0201 22:36:34.563899 140070692116288 submission_runner.py:408] Time since start: 48611.02s, 	Step: 139285, 	{'train/accuracy': 0.8102080225944519, 'train/loss': 0.6994613409042358, 'validation/accuracy': 0.7164799571037292, 'validation/loss': 1.1391528844833374, 'validation/num_examples': 50000, 'test/accuracy': 0.5924000144004822, 'test/loss': 1.8475563526153564, 'test/num_examples': 10000, 'score': 46957.41595888138, 'total_duration': 48611.016573905945, 'accumulated_submission_time': 46957.41595888138, 'accumulated_eval_time': 1642.6341168880463, 'accumulated_logging_time': 6.127922534942627}
I0201 22:36:34.604323 139907737556736 logging_writer.py:48] [139285] accumulated_eval_time=1642.634117, accumulated_logging_time=6.127923, accumulated_submission_time=46957.415959, global_step=139285, preemption_count=0, score=46957.415959, test/accuracy=0.592400, test/loss=1.847556, test/num_examples=10000, total_duration=48611.016574, train/accuracy=0.810208, train/loss=0.699461, validation/accuracy=0.716480, validation/loss=1.139153, validation/num_examples=50000
I0201 22:36:39.970221 139907754342144 logging_writer.py:48] [139300] global_step=139300, grad_norm=6.450213432312012, loss=1.2952218055725098
I0201 22:37:13.593238 139907737556736 logging_writer.py:48] [139400] global_step=139400, grad_norm=6.1168293952941895, loss=1.1547582149505615
I0201 22:37:47.309766 139907754342144 logging_writer.py:48] [139500] global_step=139500, grad_norm=6.829983711242676, loss=1.2417855262756348
I0201 22:38:21.044278 139907737556736 logging_writer.py:48] [139600] global_step=139600, grad_norm=6.102762699127197, loss=1.1623598337173462
I0201 22:38:54.700294 139907754342144 logging_writer.py:48] [139700] global_step=139700, grad_norm=6.682015419006348, loss=1.117003321647644
I0201 22:39:28.364538 139907737556736 logging_writer.py:48] [139800] global_step=139800, grad_norm=7.269779682159424, loss=1.224935531616211
I0201 22:40:02.056790 139907754342144 logging_writer.py:48] [139900] global_step=139900, grad_norm=6.766761779785156, loss=1.186105728149414
I0201 22:40:35.714692 139907737556736 logging_writer.py:48] [140000] global_step=140000, grad_norm=7.287099838256836, loss=1.166993498802185
I0201 22:41:09.400434 139907754342144 logging_writer.py:48] [140100] global_step=140100, grad_norm=6.86851692199707, loss=1.2920504808425903
I0201 22:41:43.043681 139907737556736 logging_writer.py:48] [140200] global_step=140200, grad_norm=6.436941623687744, loss=1.116736650466919
I0201 22:42:16.704121 139907754342144 logging_writer.py:48] [140300] global_step=140300, grad_norm=6.254219055175781, loss=1.191266417503357
I0201 22:42:50.346878 139907737556736 logging_writer.py:48] [140400] global_step=140400, grad_norm=7.067525863647461, loss=1.161643624305725
I0201 22:43:24.032896 139907754342144 logging_writer.py:48] [140500] global_step=140500, grad_norm=7.061564922332764, loss=1.275825023651123
I0201 22:43:57.693217 139907737556736 logging_writer.py:48] [140600] global_step=140600, grad_norm=6.476032257080078, loss=1.1580151319503784
I0201 22:44:31.433547 139907754342144 logging_writer.py:48] [140700] global_step=140700, grad_norm=6.5972442626953125, loss=1.1607633829116821
I0201 22:45:04.625977 140070692116288 spec.py:321] Evaluating on the training split.
I0201 22:45:10.871606 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 22:45:19.477270 140070692116288 spec.py:349] Evaluating on the test split.
I0201 22:45:22.120051 140070692116288 submission_runner.py:408] Time since start: 49138.57s, 	Step: 140800, 	{'train/accuracy': 0.8451849222183228, 'train/loss': 0.5710806846618652, 'validation/accuracy': 0.7232999801635742, 'validation/loss': 1.1173813343048096, 'validation/num_examples': 50000, 'test/accuracy': 0.5952000021934509, 'test/loss': 1.7925559282302856, 'test/num_examples': 10000, 'score': 47467.37585401535, 'total_duration': 49138.57271814346, 'accumulated_submission_time': 47467.37585401535, 'accumulated_eval_time': 1660.1281578540802, 'accumulated_logging_time': 6.1773834228515625}
I0201 22:45:22.161181 139907745949440 logging_writer.py:48] [140800] accumulated_eval_time=1660.128158, accumulated_logging_time=6.177383, accumulated_submission_time=47467.375854, global_step=140800, preemption_count=0, score=47467.375854, test/accuracy=0.595200, test/loss=1.792556, test/num_examples=10000, total_duration=49138.572718, train/accuracy=0.845185, train/loss=0.571081, validation/accuracy=0.723300, validation/loss=1.117381, validation/num_examples=50000
I0201 22:45:22.508151 139907754342144 logging_writer.py:48] [140800] global_step=140800, grad_norm=6.409684181213379, loss=1.1264421939849854
I0201 22:45:56.118216 139907745949440 logging_writer.py:48] [140900] global_step=140900, grad_norm=7.095038414001465, loss=1.1946829557418823
I0201 22:46:29.780420 139907754342144 logging_writer.py:48] [141000] global_step=141000, grad_norm=6.898777008056641, loss=1.1946642398834229
I0201 22:47:03.449855 139907745949440 logging_writer.py:48] [141100] global_step=141100, grad_norm=6.857469081878662, loss=1.1376944780349731
I0201 22:47:37.134442 139907754342144 logging_writer.py:48] [141200] global_step=141200, grad_norm=6.578549385070801, loss=1.1411397457122803
I0201 22:48:10.785822 139907745949440 logging_writer.py:48] [141300] global_step=141300, grad_norm=7.674073696136475, loss=1.2500479221343994
I0201 22:48:44.472237 139907754342144 logging_writer.py:48] [141400] global_step=141400, grad_norm=6.7160749435424805, loss=1.1672430038452148
I0201 22:49:18.134936 139907745949440 logging_writer.py:48] [141500] global_step=141500, grad_norm=7.327019214630127, loss=1.2060084342956543
I0201 22:49:51.834761 139907754342144 logging_writer.py:48] [141600] global_step=141600, grad_norm=7.173079967498779, loss=1.2789325714111328
I0201 22:50:25.480960 139907745949440 logging_writer.py:48] [141700] global_step=141700, grad_norm=7.003271102905273, loss=1.1790339946746826
I0201 22:50:59.240504 139907754342144 logging_writer.py:48] [141800] global_step=141800, grad_norm=6.594554901123047, loss=1.212518334388733
I0201 22:51:32.949803 139907745949440 logging_writer.py:48] [141900] global_step=141900, grad_norm=6.41424560546875, loss=1.1380698680877686
I0201 22:52:06.660544 139907754342144 logging_writer.py:48] [142000] global_step=142000, grad_norm=7.156624794006348, loss=1.1713945865631104
I0201 22:52:40.316288 139907745949440 logging_writer.py:48] [142100] global_step=142100, grad_norm=7.835354328155518, loss=1.2439088821411133
I0201 22:53:13.987962 139907754342144 logging_writer.py:48] [142200] global_step=142200, grad_norm=6.818039417266846, loss=1.095698356628418
I0201 22:53:47.638092 139907745949440 logging_writer.py:48] [142300] global_step=142300, grad_norm=6.492810249328613, loss=1.193121075630188
I0201 22:53:52.165904 140070692116288 spec.py:321] Evaluating on the training split.
I0201 22:53:58.433425 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 22:54:07.108284 140070692116288 spec.py:349] Evaluating on the test split.
I0201 22:54:09.818140 140070692116288 submission_runner.py:408] Time since start: 49666.27s, 	Step: 142315, 	{'train/accuracy': 0.8337252736091614, 'train/loss': 0.6066722273826599, 'validation/accuracy': 0.7225199937820435, 'validation/loss': 1.1123135089874268, 'validation/num_examples': 50000, 'test/accuracy': 0.5986000299453735, 'test/loss': 1.7970387935638428, 'test/num_examples': 10000, 'score': 47977.319326639175, 'total_duration': 49666.27079510689, 'accumulated_submission_time': 47977.319326639175, 'accumulated_eval_time': 1677.7803509235382, 'accumulated_logging_time': 6.227893829345703}
I0201 22:54:09.852854 139907737556736 logging_writer.py:48] [142315] accumulated_eval_time=1677.780351, accumulated_logging_time=6.227894, accumulated_submission_time=47977.319327, global_step=142315, preemption_count=0, score=47977.319327, test/accuracy=0.598600, test/loss=1.797039, test/num_examples=10000, total_duration=49666.270795, train/accuracy=0.833725, train/loss=0.606672, validation/accuracy=0.722520, validation/loss=1.112314, validation/num_examples=50000
I0201 22:54:38.775653 139907745949440 logging_writer.py:48] [142400] global_step=142400, grad_norm=6.774091720581055, loss=1.2095224857330322
I0201 22:55:12.430686 139907737556736 logging_writer.py:48] [142500] global_step=142500, grad_norm=8.419297218322754, loss=1.2259687185287476
I0201 22:55:46.121133 139907745949440 logging_writer.py:48] [142600] global_step=142600, grad_norm=7.250378131866455, loss=1.1389305591583252
I0201 22:56:19.786818 139907737556736 logging_writer.py:48] [142700] global_step=142700, grad_norm=6.561239242553711, loss=1.1225593090057373
I0201 22:56:53.456844 139907745949440 logging_writer.py:48] [142800] global_step=142800, grad_norm=7.040931701660156, loss=1.2476264238357544
I0201 22:57:27.172930 139907737556736 logging_writer.py:48] [142900] global_step=142900, grad_norm=7.464378833770752, loss=1.2093126773834229
I0201 22:58:00.876620 139907745949440 logging_writer.py:48] [143000] global_step=143000, grad_norm=6.2118120193481445, loss=1.1807291507720947
I0201 22:58:34.558917 139907737556736 logging_writer.py:48] [143100] global_step=143100, grad_norm=6.93636417388916, loss=1.2074494361877441
I0201 22:59:08.224623 139907745949440 logging_writer.py:48] [143200] global_step=143200, grad_norm=6.7981181144714355, loss=1.1549713611602783
I0201 22:59:41.898404 139907737556736 logging_writer.py:48] [143300] global_step=143300, grad_norm=6.530179977416992, loss=1.220527172088623
I0201 23:00:15.577909 139907745949440 logging_writer.py:48] [143400] global_step=143400, grad_norm=6.973165988922119, loss=1.1763546466827393
I0201 23:00:49.248507 139907737556736 logging_writer.py:48] [143500] global_step=143500, grad_norm=8.468186378479004, loss=1.0772409439086914
I0201 23:01:22.945305 139907745949440 logging_writer.py:48] [143600] global_step=143600, grad_norm=7.290090084075928, loss=1.1421289443969727
I0201 23:01:56.615832 139907737556736 logging_writer.py:48] [143700] global_step=143700, grad_norm=7.077284812927246, loss=1.0519745349884033
I0201 23:02:30.307034 139907745949440 logging_writer.py:48] [143800] global_step=143800, grad_norm=7.838365077972412, loss=1.2776085138320923
I0201 23:02:39.881383 140070692116288 spec.py:321] Evaluating on the training split.
I0201 23:02:46.165426 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 23:02:54.792276 140070692116288 spec.py:349] Evaluating on the test split.
I0201 23:02:57.426771 140070692116288 submission_runner.py:408] Time since start: 50193.88s, 	Step: 143830, 	{'train/accuracy': 0.8341039419174194, 'train/loss': 0.6040483713150024, 'validation/accuracy': 0.7270999550819397, 'validation/loss': 1.1015565395355225, 'validation/num_examples': 50000, 'test/accuracy': 0.600100040435791, 'test/loss': 1.7974672317504883, 'test/num_examples': 10000, 'score': 48487.28637838364, 'total_duration': 50193.8794400692, 'accumulated_submission_time': 48487.28637838364, 'accumulated_eval_time': 1695.3257067203522, 'accumulated_logging_time': 6.27121639251709}
I0201 23:02:57.467780 139907762734848 logging_writer.py:48] [143830] accumulated_eval_time=1695.325707, accumulated_logging_time=6.271216, accumulated_submission_time=48487.286378, global_step=143830, preemption_count=0, score=48487.286378, test/accuracy=0.600100, test/loss=1.797467, test/num_examples=10000, total_duration=50193.879440, train/accuracy=0.834104, train/loss=0.604048, validation/accuracy=0.727100, validation/loss=1.101557, validation/num_examples=50000
I0201 23:03:21.327003 139908710635264 logging_writer.py:48] [143900] global_step=143900, grad_norm=6.166937828063965, loss=1.1089438199996948
I0201 23:03:55.030495 139907762734848 logging_writer.py:48] [144000] global_step=144000, grad_norm=7.437199592590332, loss=1.1633723974227905
I0201 23:04:28.692829 139908710635264 logging_writer.py:48] [144100] global_step=144100, grad_norm=7.145003318786621, loss=1.1752960681915283
I0201 23:05:02.361977 139907762734848 logging_writer.py:48] [144200] global_step=144200, grad_norm=8.048636436462402, loss=1.1610075235366821
I0201 23:05:36.051377 139908710635264 logging_writer.py:48] [144300] global_step=144300, grad_norm=6.8272600173950195, loss=1.1058528423309326
I0201 23:06:09.702702 139907762734848 logging_writer.py:48] [144400] global_step=144400, grad_norm=7.270549774169922, loss=1.156213641166687
I0201 23:06:43.390503 139908710635264 logging_writer.py:48] [144500] global_step=144500, grad_norm=6.750357627868652, loss=1.0796486139297485
I0201 23:07:17.038932 139907762734848 logging_writer.py:48] [144600] global_step=144600, grad_norm=7.242042541503906, loss=1.1813733577728271
I0201 23:07:50.710881 139908710635264 logging_writer.py:48] [144700] global_step=144700, grad_norm=7.247583866119385, loss=1.136272668838501
I0201 23:08:24.363396 139907762734848 logging_writer.py:48] [144800] global_step=144800, grad_norm=6.4016218185424805, loss=1.0614768266677856
I0201 23:08:58.051507 139908710635264 logging_writer.py:48] [144900] global_step=144900, grad_norm=6.269927024841309, loss=1.0805542469024658
I0201 23:09:31.699135 139907762734848 logging_writer.py:48] [145000] global_step=145000, grad_norm=7.560077667236328, loss=1.2016410827636719
I0201 23:10:05.422481 139908710635264 logging_writer.py:48] [145100] global_step=145100, grad_norm=6.963134765625, loss=1.1000139713287354
I0201 23:10:39.078988 139907762734848 logging_writer.py:48] [145200] global_step=145200, grad_norm=7.2046966552734375, loss=1.2054548263549805
I0201 23:11:12.768190 139908710635264 logging_writer.py:48] [145300] global_step=145300, grad_norm=6.271477222442627, loss=1.1349436044692993
I0201 23:11:27.752519 140070692116288 spec.py:321] Evaluating on the training split.
I0201 23:11:34.052503 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 23:11:42.817873 140070692116288 spec.py:349] Evaluating on the test split.
I0201 23:11:45.380407 140070692116288 submission_runner.py:408] Time since start: 50721.83s, 	Step: 145346, 	{'train/accuracy': 0.8286631107330322, 'train/loss': 0.6169041991233826, 'validation/accuracy': 0.727840006351471, 'validation/loss': 1.1030001640319824, 'validation/num_examples': 50000, 'test/accuracy': 0.5995000004768372, 'test/loss': 1.8142592906951904, 'test/num_examples': 10000, 'score': 48997.5089635849, 'total_duration': 50721.83307147026, 'accumulated_submission_time': 48997.5089635849, 'accumulated_eval_time': 1712.9535655975342, 'accumulated_logging_time': 6.321614980697632}
I0201 23:11:45.425604 139907754342144 logging_writer.py:48] [145346] accumulated_eval_time=1712.953566, accumulated_logging_time=6.321615, accumulated_submission_time=48997.508964, global_step=145346, preemption_count=0, score=48997.508964, test/accuracy=0.599500, test/loss=1.814259, test/num_examples=10000, total_duration=50721.833071, train/accuracy=0.828663, train/loss=0.616904, validation/accuracy=0.727840, validation/loss=1.103000, validation/num_examples=50000
I0201 23:12:03.902377 139908425447168 logging_writer.py:48] [145400] global_step=145400, grad_norm=8.096638679504395, loss=1.0170069932937622
I0201 23:12:37.522326 139907754342144 logging_writer.py:48] [145500] global_step=145500, grad_norm=6.636042594909668, loss=1.1403305530548096
I0201 23:13:11.179713 139908425447168 logging_writer.py:48] [145600] global_step=145600, grad_norm=6.468932628631592, loss=1.1113590002059937
I0201 23:13:44.849801 139907754342144 logging_writer.py:48] [145700] global_step=145700, grad_norm=7.752715110778809, loss=1.1958036422729492
I0201 23:14:18.503337 139908425447168 logging_writer.py:48] [145800] global_step=145800, grad_norm=6.4513840675354, loss=1.1218693256378174
I0201 23:14:52.195502 139907754342144 logging_writer.py:48] [145900] global_step=145900, grad_norm=7.851685047149658, loss=1.1416469812393188
I0201 23:15:25.847289 139908425447168 logging_writer.py:48] [146000] global_step=146000, grad_norm=6.953183650970459, loss=1.2437489032745361
I0201 23:15:59.522607 139907754342144 logging_writer.py:48] [146100] global_step=146100, grad_norm=7.049882888793945, loss=1.165985345840454
I0201 23:16:33.276215 139908425447168 logging_writer.py:48] [146200] global_step=146200, grad_norm=6.728024005889893, loss=1.158979892730713
I0201 23:17:06.978562 139907754342144 logging_writer.py:48] [146300] global_step=146300, grad_norm=6.5330963134765625, loss=1.1379947662353516
I0201 23:17:40.657083 139908425447168 logging_writer.py:48] [146400] global_step=146400, grad_norm=7.610319137573242, loss=1.0895767211914062
I0201 23:18:14.330689 139907754342144 logging_writer.py:48] [146500] global_step=146500, grad_norm=7.9274749755859375, loss=1.1992796659469604
I0201 23:18:48.018991 139908425447168 logging_writer.py:48] [146600] global_step=146600, grad_norm=6.736200332641602, loss=0.9902486801147461
I0201 23:19:21.697981 139907754342144 logging_writer.py:48] [146700] global_step=146700, grad_norm=8.37178897857666, loss=1.0979814529418945
I0201 23:19:55.365856 139908425447168 logging_writer.py:48] [146800] global_step=146800, grad_norm=7.6371917724609375, loss=1.2285068035125732
I0201 23:20:15.715778 140070692116288 spec.py:321] Evaluating on the training split.
I0201 23:20:21.942454 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 23:20:30.296540 140070692116288 spec.py:349] Evaluating on the test split.
I0201 23:20:32.802827 140070692116288 submission_runner.py:408] Time since start: 51249.26s, 	Step: 146862, 	{'train/accuracy': 0.832051157951355, 'train/loss': 0.6025562286376953, 'validation/accuracy': 0.7324999570846558, 'validation/loss': 1.0817550420761108, 'validation/num_examples': 50000, 'test/accuracy': 0.6061000227928162, 'test/loss': 1.7760010957717896, 'test/num_examples': 10000, 'score': 49507.73680782318, 'total_duration': 51249.255494356155, 'accumulated_submission_time': 49507.73680782318, 'accumulated_eval_time': 1730.040581703186, 'accumulated_logging_time': 6.375980854034424}
I0201 23:20:32.847039 139908719027968 logging_writer.py:48] [146862] accumulated_eval_time=1730.040582, accumulated_logging_time=6.375981, accumulated_submission_time=49507.736808, global_step=146862, preemption_count=0, score=49507.736808, test/accuracy=0.606100, test/loss=1.776001, test/num_examples=10000, total_duration=51249.255494, train/accuracy=0.832051, train/loss=0.602556, validation/accuracy=0.732500, validation/loss=1.081755, validation/num_examples=50000
I0201 23:20:45.967911 139908727420672 logging_writer.py:48] [146900] global_step=146900, grad_norm=7.209442615509033, loss=1.1989564895629883
I0201 23:21:19.645945 139908719027968 logging_writer.py:48] [147000] global_step=147000, grad_norm=7.075984954833984, loss=0.9542251825332642
I0201 23:21:53.295994 139908727420672 logging_writer.py:48] [147100] global_step=147100, grad_norm=7.271926403045654, loss=1.0551910400390625
I0201 23:22:26.988191 139908719027968 logging_writer.py:48] [147200] global_step=147200, grad_norm=7.177359580993652, loss=1.1387330293655396
I0201 23:23:00.775061 139908727420672 logging_writer.py:48] [147300] global_step=147300, grad_norm=8.624690055847168, loss=1.0958199501037598
I0201 23:23:34.492933 139908719027968 logging_writer.py:48] [147400] global_step=147400, grad_norm=6.867430686950684, loss=1.1326748132705688
I0201 23:24:08.158120 139908727420672 logging_writer.py:48] [147500] global_step=147500, grad_norm=7.302957534790039, loss=1.1853853464126587
I0201 23:24:41.832425 139908719027968 logging_writer.py:48] [147600] global_step=147600, grad_norm=7.834985733032227, loss=1.158776044845581
I0201 23:25:15.482010 139908727420672 logging_writer.py:48] [147700] global_step=147700, grad_norm=8.116922378540039, loss=1.1169931888580322
I0201 23:25:49.179689 139908719027968 logging_writer.py:48] [147800] global_step=147800, grad_norm=6.773387908935547, loss=1.0542598962783813
I0201 23:26:22.835305 139908727420672 logging_writer.py:48] [147900] global_step=147900, grad_norm=6.789922714233398, loss=1.0994833707809448
I0201 23:26:56.508824 139908719027968 logging_writer.py:48] [148000] global_step=148000, grad_norm=7.1184797286987305, loss=1.1752489805221558
I0201 23:27:30.160758 139908727420672 logging_writer.py:48] [148100] global_step=148100, grad_norm=7.705239772796631, loss=1.154902458190918
I0201 23:28:03.871383 139908719027968 logging_writer.py:48] [148200] global_step=148200, grad_norm=7.437172889709473, loss=1.0938951969146729
I0201 23:28:37.538193 139908727420672 logging_writer.py:48] [148300] global_step=148300, grad_norm=7.554641246795654, loss=1.0664128065109253
I0201 23:29:02.986253 140070692116288 spec.py:321] Evaluating on the training split.
I0201 23:29:09.388400 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 23:29:18.087546 140070692116288 spec.py:349] Evaluating on the test split.
I0201 23:29:20.610554 140070692116288 submission_runner.py:408] Time since start: 51777.06s, 	Step: 148377, 	{'train/accuracy': 0.840840220451355, 'train/loss': 0.5678520202636719, 'validation/accuracy': 0.732759952545166, 'validation/loss': 1.0800023078918457, 'validation/num_examples': 50000, 'test/accuracy': 0.6103000044822693, 'test/loss': 1.7833529710769653, 'test/num_examples': 10000, 'score': 50017.81396985054, 'total_duration': 51777.063213825226, 'accumulated_submission_time': 50017.81396985054, 'accumulated_eval_time': 1747.6648552417755, 'accumulated_logging_time': 6.429377555847168}
I0201 23:29:20.651242 139907754342144 logging_writer.py:48] [148377] accumulated_eval_time=1747.664855, accumulated_logging_time=6.429378, accumulated_submission_time=50017.813970, global_step=148377, preemption_count=0, score=50017.813970, test/accuracy=0.610300, test/loss=1.783353, test/num_examples=10000, total_duration=51777.063214, train/accuracy=0.840840, train/loss=0.567852, validation/accuracy=0.732760, validation/loss=1.080002, validation/num_examples=50000
I0201 23:29:28.714776 139907762734848 logging_writer.py:48] [148400] global_step=148400, grad_norm=7.304529190063477, loss=1.1570760011672974
I0201 23:30:02.305508 139907754342144 logging_writer.py:48] [148500] global_step=148500, grad_norm=7.548202991485596, loss=1.016340970993042
I0201 23:30:35.997195 139907762734848 logging_writer.py:48] [148600] global_step=148600, grad_norm=7.457178592681885, loss=1.114420771598816
I0201 23:31:09.681930 139907754342144 logging_writer.py:48] [148700] global_step=148700, grad_norm=7.238865852355957, loss=1.021622657775879
I0201 23:31:43.354916 139907762734848 logging_writer.py:48] [148800] global_step=148800, grad_norm=7.4063262939453125, loss=1.1101871728897095
I0201 23:32:17.014495 139907754342144 logging_writer.py:48] [148900] global_step=148900, grad_norm=7.160582542419434, loss=1.111570954322815
I0201 23:32:50.671272 139907762734848 logging_writer.py:48] [149000] global_step=149000, grad_norm=8.687685012817383, loss=1.1266793012619019
I0201 23:33:24.346288 139907754342144 logging_writer.py:48] [149100] global_step=149100, grad_norm=8.384318351745605, loss=1.0939264297485352
I0201 23:33:58.023905 139907762734848 logging_writer.py:48] [149200] global_step=149200, grad_norm=8.895827293395996, loss=1.133514165878296
I0201 23:34:31.691859 139907754342144 logging_writer.py:48] [149300] global_step=149300, grad_norm=7.1780829429626465, loss=1.1240935325622559
I0201 23:35:05.354097 139907762734848 logging_writer.py:48] [149400] global_step=149400, grad_norm=8.482303619384766, loss=1.121155023574829
I0201 23:35:39.081006 139907754342144 logging_writer.py:48] [149500] global_step=149500, grad_norm=7.8862528800964355, loss=1.117543339729309
I0201 23:36:12.786597 139907762734848 logging_writer.py:48] [149600] global_step=149600, grad_norm=6.731092929840088, loss=1.014419674873352
I0201 23:36:46.468323 139907754342144 logging_writer.py:48] [149700] global_step=149700, grad_norm=7.531520843505859, loss=1.05540132522583
I0201 23:37:20.130702 139907762734848 logging_writer.py:48] [149800] global_step=149800, grad_norm=7.901175498962402, loss=1.1469389200210571
I0201 23:37:50.892485 140070692116288 spec.py:321] Evaluating on the training split.
I0201 23:37:57.204977 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 23:38:05.791047 140070692116288 spec.py:349] Evaluating on the test split.
I0201 23:38:08.455810 140070692116288 submission_runner.py:408] Time since start: 52304.91s, 	Step: 149893, 	{'train/accuracy': 0.8672671914100647, 'train/loss': 0.4734492003917694, 'validation/accuracy': 0.7351399660110474, 'validation/loss': 1.0676651000976562, 'validation/num_examples': 50000, 'test/accuracy': 0.6116000413894653, 'test/loss': 1.751431941986084, 'test/num_examples': 10000, 'score': 50527.99266386032, 'total_duration': 52304.908478975296, 'accumulated_submission_time': 50527.99266386032, 'accumulated_eval_time': 1765.2281498908997, 'accumulated_logging_time': 6.480208396911621}
I0201 23:38:08.502346 139907729164032 logging_writer.py:48] [149893] accumulated_eval_time=1765.228150, accumulated_logging_time=6.480208, accumulated_submission_time=50527.992664, global_step=149893, preemption_count=0, score=50527.992664, test/accuracy=0.611600, test/loss=1.751432, test/num_examples=10000, total_duration=52304.908479, train/accuracy=0.867267, train/loss=0.473449, validation/accuracy=0.735140, validation/loss=1.067665, validation/num_examples=50000
I0201 23:38:11.217039 139907745949440 logging_writer.py:48] [149900] global_step=149900, grad_norm=7.151448726654053, loss=1.0835896730422974
I0201 23:38:44.899445 139907729164032 logging_writer.py:48] [150000] global_step=150000, grad_norm=7.404746055603027, loss=1.0304840803146362
I0201 23:39:18.518641 139907745949440 logging_writer.py:48] [150100] global_step=150100, grad_norm=7.905450344085693, loss=0.9650074243545532
I0201 23:39:52.231381 139907729164032 logging_writer.py:48] [150200] global_step=150200, grad_norm=7.258018493652344, loss=1.1491070985794067
I0201 23:40:25.933286 139907745949440 logging_writer.py:48] [150300] global_step=150300, grad_norm=7.041936874389648, loss=1.0570067167282104
I0201 23:40:59.572217 139907729164032 logging_writer.py:48] [150400] global_step=150400, grad_norm=7.061559677124023, loss=1.0368772745132446
I0201 23:41:33.274197 139907745949440 logging_writer.py:48] [150500] global_step=150500, grad_norm=7.810748100280762, loss=1.1333682537078857
I0201 23:42:07.048703 139907729164032 logging_writer.py:48] [150600] global_step=150600, grad_norm=7.58434534072876, loss=1.04176926612854
I0201 23:42:40.767634 139907745949440 logging_writer.py:48] [150700] global_step=150700, grad_norm=7.389875888824463, loss=1.1670490503311157
I0201 23:43:14.428069 139907729164032 logging_writer.py:48] [150800] global_step=150800, grad_norm=7.891757488250732, loss=1.0552911758422852
I0201 23:43:48.100321 139907745949440 logging_writer.py:48] [150900] global_step=150900, grad_norm=7.396493911743164, loss=1.0492784976959229
I0201 23:44:21.760129 139907729164032 logging_writer.py:48] [151000] global_step=151000, grad_norm=7.536453723907471, loss=1.0109754800796509
I0201 23:44:55.457230 139907745949440 logging_writer.py:48] [151100] global_step=151100, grad_norm=7.117860794067383, loss=1.0038840770721436
I0201 23:45:29.105242 139907729164032 logging_writer.py:48] [151200] global_step=151200, grad_norm=7.7859392166137695, loss=1.1202852725982666
I0201 23:46:02.789958 139907745949440 logging_writer.py:48] [151300] global_step=151300, grad_norm=7.334327697753906, loss=1.1026453971862793
I0201 23:46:36.445797 139907729164032 logging_writer.py:48] [151400] global_step=151400, grad_norm=6.7545294761657715, loss=1.0011746883392334
I0201 23:46:38.613448 140070692116288 spec.py:321] Evaluating on the training split.
I0201 23:46:44.852251 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 23:46:53.446857 140070692116288 spec.py:349] Evaluating on the test split.
I0201 23:46:56.039435 140070692116288 submission_runner.py:408] Time since start: 52832.49s, 	Step: 151408, 	{'train/accuracy': 0.8615473508834839, 'train/loss': 0.48896026611328125, 'validation/accuracy': 0.7380799651145935, 'validation/loss': 1.0503398180007935, 'validation/num_examples': 50000, 'test/accuracy': 0.6101000308990479, 'test/loss': 1.7489426136016846, 'test/num_examples': 10000, 'score': 51038.04159331322, 'total_duration': 52832.49210214615, 'accumulated_submission_time': 51038.04159331322, 'accumulated_eval_time': 1782.6540973186493, 'accumulated_logging_time': 6.537206172943115}
I0201 23:46:56.082430 139907762734848 logging_writer.py:48] [151408] accumulated_eval_time=1782.654097, accumulated_logging_time=6.537206, accumulated_submission_time=51038.041593, global_step=151408, preemption_count=0, score=51038.041593, test/accuracy=0.610100, test/loss=1.748943, test/num_examples=10000, total_duration=52832.492102, train/accuracy=0.861547, train/loss=0.488960, validation/accuracy=0.738080, validation/loss=1.050340, validation/num_examples=50000
I0201 23:47:27.388956 139908425447168 logging_writer.py:48] [151500] global_step=151500, grad_norm=8.879286766052246, loss=1.0790749788284302
I0201 23:48:01.161531 139907762734848 logging_writer.py:48] [151600] global_step=151600, grad_norm=7.743918418884277, loss=1.1070126295089722
I0201 23:48:34.806455 139908425447168 logging_writer.py:48] [151700] global_step=151700, grad_norm=7.511059284210205, loss=1.0510828495025635
I0201 23:49:08.519405 139907762734848 logging_writer.py:48] [151800] global_step=151800, grad_norm=8.08821964263916, loss=1.1086077690124512
I0201 23:49:42.187941 139908425447168 logging_writer.py:48] [151900] global_step=151900, grad_norm=6.982537746429443, loss=1.0005767345428467
I0201 23:50:15.870136 139907762734848 logging_writer.py:48] [152000] global_step=152000, grad_norm=7.553754806518555, loss=1.0469335317611694
I0201 23:50:49.533216 139908425447168 logging_writer.py:48] [152100] global_step=152100, grad_norm=8.749520301818848, loss=0.9910839796066284
I0201 23:51:23.200755 139907762734848 logging_writer.py:48] [152200] global_step=152200, grad_norm=7.040511608123779, loss=1.0358872413635254
I0201 23:51:56.855298 139908425447168 logging_writer.py:48] [152300] global_step=152300, grad_norm=7.054737091064453, loss=1.0243651866912842
I0201 23:52:30.537832 139907762734848 logging_writer.py:48] [152400] global_step=152400, grad_norm=7.545109272003174, loss=1.1884894371032715
I0201 23:53:04.206896 139908425447168 logging_writer.py:48] [152500] global_step=152500, grad_norm=7.546627521514893, loss=0.9693047404289246
I0201 23:53:37.900432 139907762734848 logging_writer.py:48] [152600] global_step=152600, grad_norm=7.636569976806641, loss=1.074106216430664
I0201 23:54:11.552172 139908425447168 logging_writer.py:48] [152700] global_step=152700, grad_norm=7.479586601257324, loss=1.0053372383117676
I0201 23:54:45.294707 139907762734848 logging_writer.py:48] [152800] global_step=152800, grad_norm=7.493004322052002, loss=1.0553444623947144
I0201 23:55:19.006856 139908425447168 logging_writer.py:48] [152900] global_step=152900, grad_norm=8.307243347167969, loss=1.0955628156661987
I0201 23:55:26.223519 140070692116288 spec.py:321] Evaluating on the training split.
I0201 23:55:32.538723 140070692116288 spec.py:333] Evaluating on the validation split.
I0201 23:55:40.912469 140070692116288 spec.py:349] Evaluating on the test split.
I0201 23:55:43.499748 140070692116288 submission_runner.py:408] Time since start: 53359.95s, 	Step: 152923, 	{'train/accuracy': 0.8637993931770325, 'train/loss': 0.4847530722618103, 'validation/accuracy': 0.7389400005340576, 'validation/loss': 1.0541657209396362, 'validation/num_examples': 50000, 'test/accuracy': 0.6165000200271606, 'test/loss': 1.755265712738037, 'test/num_examples': 10000, 'score': 51548.12100839615, 'total_duration': 53359.95241069794, 'accumulated_submission_time': 51548.12100839615, 'accumulated_eval_time': 1799.930284500122, 'accumulated_logging_time': 6.58967399597168}
I0201 23:55:43.545055 139907745949440 logging_writer.py:48] [152923] accumulated_eval_time=1799.930285, accumulated_logging_time=6.589674, accumulated_submission_time=51548.121008, global_step=152923, preemption_count=0, score=51548.121008, test/accuracy=0.616500, test/loss=1.755266, test/num_examples=10000, total_duration=53359.952411, train/accuracy=0.863799, train/loss=0.484753, validation/accuracy=0.738940, validation/loss=1.054166, validation/num_examples=50000
I0201 23:56:09.784974 139907754342144 logging_writer.py:48] [153000] global_step=153000, grad_norm=7.514670372009277, loss=1.0474978685379028
I0201 23:56:43.449643 139907745949440 logging_writer.py:48] [153100] global_step=153100, grad_norm=7.772221088409424, loss=0.9865819215774536
I0201 23:57:17.160501 139907754342144 logging_writer.py:48] [153200] global_step=153200, grad_norm=7.771872043609619, loss=1.1116877794265747
I0201 23:57:50.825731 139907745949440 logging_writer.py:48] [153300] global_step=153300, grad_norm=8.962422370910645, loss=1.017495036125183
I0201 23:58:24.525341 139907754342144 logging_writer.py:48] [153400] global_step=153400, grad_norm=8.181093215942383, loss=1.0534007549285889
I0201 23:58:58.186042 139907745949440 logging_writer.py:48] [153500] global_step=153500, grad_norm=7.825671672821045, loss=0.9946354031562805
I0201 23:59:31.885611 139907754342144 logging_writer.py:48] [153600] global_step=153600, grad_norm=8.49691104888916, loss=1.0615642070770264
I0202 00:00:05.539014 139907745949440 logging_writer.py:48] [153700] global_step=153700, grad_norm=7.583607196807861, loss=1.0851502418518066
I0202 00:00:39.294756 139907754342144 logging_writer.py:48] [153800] global_step=153800, grad_norm=7.464393138885498, loss=1.0179731845855713
I0202 00:01:13.004325 139907745949440 logging_writer.py:48] [153900] global_step=153900, grad_norm=7.319270133972168, loss=1.005601167678833
I0202 00:01:46.709878 139907754342144 logging_writer.py:48] [154000] global_step=154000, grad_norm=7.439705848693848, loss=1.1020609140396118
I0202 00:02:20.371090 139907745949440 logging_writer.py:48] [154100] global_step=154100, grad_norm=8.186277389526367, loss=1.0610562562942505
I0202 00:02:54.061956 139907754342144 logging_writer.py:48] [154200] global_step=154200, grad_norm=7.696191310882568, loss=1.0741804838180542
I0202 00:03:27.735503 139907745949440 logging_writer.py:48] [154300] global_step=154300, grad_norm=8.08090877532959, loss=0.98091059923172
I0202 00:04:01.423231 139907754342144 logging_writer.py:48] [154400] global_step=154400, grad_norm=8.145694732666016, loss=1.0579324960708618
I0202 00:04:13.688474 140070692116288 spec.py:321] Evaluating on the training split.
I0202 00:04:19.901613 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 00:04:28.709871 140070692116288 spec.py:349] Evaluating on the test split.
I0202 00:04:31.327602 140070692116288 submission_runner.py:408] Time since start: 53887.78s, 	Step: 154438, 	{'train/accuracy': 0.8626036047935486, 'train/loss': 0.4872516095638275, 'validation/accuracy': 0.7405999898910522, 'validation/loss': 1.0530531406402588, 'validation/num_examples': 50000, 'test/accuracy': 0.6126000285148621, 'test/loss': 1.7592875957489014, 'test/num_examples': 10000, 'score': 52058.202897787094, 'total_duration': 53887.78027367592, 'accumulated_submission_time': 52058.202897787094, 'accumulated_eval_time': 1817.5693821907043, 'accumulated_logging_time': 6.6446380615234375}
I0202 00:04:31.371194 139907737556736 logging_writer.py:48] [154438] accumulated_eval_time=1817.569382, accumulated_logging_time=6.644638, accumulated_submission_time=52058.202898, global_step=154438, preemption_count=0, score=52058.202898, test/accuracy=0.612600, test/loss=1.759288, test/num_examples=10000, total_duration=53887.780274, train/accuracy=0.862604, train/loss=0.487252, validation/accuracy=0.740600, validation/loss=1.053053, validation/num_examples=50000
I0202 00:04:52.546123 139907762734848 logging_writer.py:48] [154500] global_step=154500, grad_norm=7.8540358543396, loss=0.9495188593864441
I0202 00:05:26.175433 139907737556736 logging_writer.py:48] [154600] global_step=154600, grad_norm=6.878677845001221, loss=0.9078800678253174
I0202 00:05:59.841306 139907762734848 logging_writer.py:48] [154700] global_step=154700, grad_norm=7.420093059539795, loss=0.9933704733848572
I0202 00:06:33.522300 139907737556736 logging_writer.py:48] [154800] global_step=154800, grad_norm=8.061728477478027, loss=0.9619102478027344
I0202 00:07:07.230990 139907762734848 logging_writer.py:48] [154900] global_step=154900, grad_norm=7.365410327911377, loss=0.899114727973938
I0202 00:07:40.860884 139907737556736 logging_writer.py:48] [155000] global_step=155000, grad_norm=8.581038475036621, loss=1.0002830028533936
I0202 00:08:14.560162 139907762734848 logging_writer.py:48] [155100] global_step=155100, grad_norm=10.128252029418945, loss=1.0641957521438599
I0202 00:08:48.208269 139907737556736 logging_writer.py:48] [155200] global_step=155200, grad_norm=6.8682074546813965, loss=0.8999282121658325
I0202 00:09:21.904857 139907762734848 logging_writer.py:48] [155300] global_step=155300, grad_norm=7.9765119552612305, loss=1.0103495121002197
I0202 00:09:55.567828 139907737556736 logging_writer.py:48] [155400] global_step=155400, grad_norm=7.832035064697266, loss=0.9874734282493591
I0202 00:10:29.270592 139907762734848 logging_writer.py:48] [155500] global_step=155500, grad_norm=7.775321006774902, loss=1.030623435974121
I0202 00:11:02.926189 139907737556736 logging_writer.py:48] [155600] global_step=155600, grad_norm=7.3882293701171875, loss=1.024553894996643
I0202 00:11:36.618252 139907762734848 logging_writer.py:48] [155700] global_step=155700, grad_norm=7.620934963226318, loss=1.0147160291671753
I0202 00:12:10.274514 139907737556736 logging_writer.py:48] [155800] global_step=155800, grad_norm=7.480995178222656, loss=0.972295880317688
I0202 00:12:43.964545 139907762734848 logging_writer.py:48] [155900] global_step=155900, grad_norm=7.6288862228393555, loss=0.9737623333930969
I0202 00:13:01.630877 140070692116288 spec.py:321] Evaluating on the training split.
I0202 00:13:08.236840 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 00:13:16.990213 140070692116288 spec.py:349] Evaluating on the test split.
I0202 00:13:19.539294 140070692116288 submission_runner.py:408] Time since start: 54415.99s, 	Step: 155954, 	{'train/accuracy': 0.8659518361091614, 'train/loss': 0.4726797938346863, 'validation/accuracy': 0.7437599897384644, 'validation/loss': 1.0325734615325928, 'validation/num_examples': 50000, 'test/accuracy': 0.6145000457763672, 'test/loss': 1.7565077543258667, 'test/num_examples': 10000, 'score': 52568.401344537735, 'total_duration': 54415.99195933342, 'accumulated_submission_time': 52568.401344537735, 'accumulated_eval_time': 1835.477769613266, 'accumulated_logging_time': 6.697588682174683}
I0202 00:13:19.583082 139908710635264 logging_writer.py:48] [155954] accumulated_eval_time=1835.477770, accumulated_logging_time=6.697589, accumulated_submission_time=52568.401345, global_step=155954, preemption_count=0, score=52568.401345, test/accuracy=0.614500, test/loss=1.756508, test/num_examples=10000, total_duration=54415.991959, train/accuracy=0.865952, train/loss=0.472680, validation/accuracy=0.743760, validation/loss=1.032573, validation/num_examples=50000
I0202 00:13:35.386468 139908719027968 logging_writer.py:48] [156000] global_step=156000, grad_norm=8.520894050598145, loss=1.0378221273422241
I0202 00:14:09.063350 139908710635264 logging_writer.py:48] [156100] global_step=156100, grad_norm=8.19204330444336, loss=0.9391236305236816
I0202 00:14:42.726389 139908719027968 logging_writer.py:48] [156200] global_step=156200, grad_norm=7.703023910522461, loss=0.9116058349609375
I0202 00:15:16.410566 139908710635264 logging_writer.py:48] [156300] global_step=156300, grad_norm=7.4858174324035645, loss=0.9555826783180237
I0202 00:15:50.059230 139908719027968 logging_writer.py:48] [156400] global_step=156400, grad_norm=8.43460464477539, loss=1.0289102792739868
I0202 00:16:23.757310 139908710635264 logging_writer.py:48] [156500] global_step=156500, grad_norm=8.090131759643555, loss=0.9224961400032043
I0202 00:16:57.427463 139908719027968 logging_writer.py:48] [156600] global_step=156600, grad_norm=8.920047760009766, loss=1.1357120275497437
I0202 00:17:31.119565 139908710635264 logging_writer.py:48] [156700] global_step=156700, grad_norm=8.159201622009277, loss=1.0131280422210693
I0202 00:18:04.792153 139908719027968 logging_writer.py:48] [156800] global_step=156800, grad_norm=7.569634914398193, loss=0.9829834699630737
I0202 00:18:38.496350 139908710635264 logging_writer.py:48] [156900] global_step=156900, grad_norm=7.552818775177002, loss=1.0444152355194092
I0202 00:19:12.166651 139908719027968 logging_writer.py:48] [157000] global_step=157000, grad_norm=8.394048690795898, loss=1.0762468576431274
I0202 00:19:46.037614 139908710635264 logging_writer.py:48] [157100] global_step=157100, grad_norm=7.869607925415039, loss=1.000984787940979
I0202 00:20:19.673814 139908719027968 logging_writer.py:48] [157200] global_step=157200, grad_norm=7.722204685211182, loss=0.9571245312690735
I0202 00:20:53.379885 139908710635264 logging_writer.py:48] [157300] global_step=157300, grad_norm=8.375470161437988, loss=0.9756864309310913
I0202 00:21:27.038136 139908719027968 logging_writer.py:48] [157400] global_step=157400, grad_norm=7.534640789031982, loss=0.9951680898666382
I0202 00:21:49.769532 140070692116288 spec.py:321] Evaluating on the training split.
I0202 00:21:55.970622 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 00:22:04.603448 140070692116288 spec.py:349] Evaluating on the test split.
I0202 00:22:07.372981 140070692116288 submission_runner.py:408] Time since start: 54943.83s, 	Step: 157469, 	{'train/accuracy': 0.875996470451355, 'train/loss': 0.4355567395687103, 'validation/accuracy': 0.7448999881744385, 'validation/loss': 1.034436821937561, 'validation/num_examples': 50000, 'test/accuracy': 0.6205000281333923, 'test/loss': 1.7346514463424683, 'test/num_examples': 10000, 'score': 53078.52582502365, 'total_duration': 54943.82565164566, 'accumulated_submission_time': 53078.52582502365, 'accumulated_eval_time': 1853.0811932086945, 'accumulated_logging_time': 6.750476598739624}
I0202 00:22:07.420328 139907737556736 logging_writer.py:48] [157469] accumulated_eval_time=1853.081193, accumulated_logging_time=6.750477, accumulated_submission_time=53078.525825, global_step=157469, preemption_count=0, score=53078.525825, test/accuracy=0.620500, test/loss=1.734651, test/num_examples=10000, total_duration=54943.825652, train/accuracy=0.875996, train/loss=0.435557, validation/accuracy=0.744900, validation/loss=1.034437, validation/num_examples=50000
I0202 00:22:18.197744 139907754342144 logging_writer.py:48] [157500] global_step=157500, grad_norm=8.134492874145508, loss=0.9779413938522339
I0202 00:22:51.858773 139907737556736 logging_writer.py:48] [157600] global_step=157600, grad_norm=7.605058670043945, loss=0.952865481376648
I0202 00:23:25.511788 139907754342144 logging_writer.py:48] [157700] global_step=157700, grad_norm=8.289617538452148, loss=0.9570556282997131
I0202 00:23:59.210969 139907737556736 logging_writer.py:48] [157800] global_step=157800, grad_norm=8.061911582946777, loss=1.0185221433639526
I0202 00:24:32.862061 139907754342144 logging_writer.py:48] [157900] global_step=157900, grad_norm=8.276007652282715, loss=1.0328869819641113
I0202 00:25:06.543709 139907737556736 logging_writer.py:48] [158000] global_step=158000, grad_norm=8.037399291992188, loss=0.999207615852356
I0202 00:25:40.195994 139907754342144 logging_writer.py:48] [158100] global_step=158100, grad_norm=8.4210205078125, loss=1.0677367448806763
I0202 00:26:14.029850 139907737556736 logging_writer.py:48] [158200] global_step=158200, grad_norm=7.83561372756958, loss=1.0272784233093262
I0202 00:26:47.705609 139907754342144 logging_writer.py:48] [158300] global_step=158300, grad_norm=7.609214782714844, loss=0.9237210154533386
I0202 00:27:21.388926 139907737556736 logging_writer.py:48] [158400] global_step=158400, grad_norm=7.860540866851807, loss=0.9426878690719604
I0202 00:27:55.047895 139907754342144 logging_writer.py:48] [158500] global_step=158500, grad_norm=8.694022178649902, loss=1.0478938817977905
I0202 00:28:28.746124 139907737556736 logging_writer.py:48] [158600] global_step=158600, grad_norm=8.563919067382812, loss=0.9706982374191284
I0202 00:29:02.406856 139907754342144 logging_writer.py:48] [158700] global_step=158700, grad_norm=8.394091606140137, loss=0.9635924100875854
I0202 00:29:36.102533 139907737556736 logging_writer.py:48] [158800] global_step=158800, grad_norm=7.717932224273682, loss=0.9375353455543518
I0202 00:30:09.761170 139907754342144 logging_writer.py:48] [158900] global_step=158900, grad_norm=7.768921375274658, loss=0.9773427248001099
I0202 00:30:37.532849 140070692116288 spec.py:321] Evaluating on the training split.
I0202 00:30:43.823540 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 00:30:52.271228 140070692116288 spec.py:349] Evaluating on the test split.
I0202 00:30:54.878708 140070692116288 submission_runner.py:408] Time since start: 55471.33s, 	Step: 158984, 	{'train/accuracy': 0.8933752775192261, 'train/loss': 0.3799366354942322, 'validation/accuracy': 0.7441399693489075, 'validation/loss': 1.0344130992889404, 'validation/num_examples': 50000, 'test/accuracy': 0.6225000023841858, 'test/loss': 1.7522176504135132, 'test/num_examples': 10000, 'score': 53588.57729744911, 'total_duration': 55471.33137130737, 'accumulated_submission_time': 53588.57729744911, 'accumulated_eval_time': 1870.427015542984, 'accumulated_logging_time': 6.8072190284729}
I0202 00:30:54.928876 139907754342144 logging_writer.py:48] [158984] accumulated_eval_time=1870.427016, accumulated_logging_time=6.807219, accumulated_submission_time=53588.577297, global_step=158984, preemption_count=0, score=53588.577297, test/accuracy=0.622500, test/loss=1.752218, test/num_examples=10000, total_duration=55471.331371, train/accuracy=0.893375, train/loss=0.379937, validation/accuracy=0.744140, validation/loss=1.034413, validation/num_examples=50000
I0202 00:31:00.641981 139908710635264 logging_writer.py:48] [159000] global_step=159000, grad_norm=8.593945503234863, loss=0.9772964715957642
I0202 00:31:34.224622 139907754342144 logging_writer.py:48] [159100] global_step=159100, grad_norm=7.825218677520752, loss=0.9010434746742249
I0202 00:32:07.905155 139908710635264 logging_writer.py:48] [159200] global_step=159200, grad_norm=8.13436508178711, loss=1.0226389169692993
I0202 00:32:41.559062 139907754342144 logging_writer.py:48] [159300] global_step=159300, grad_norm=7.879523754119873, loss=0.8884395956993103
I0202 00:33:15.242986 139908710635264 logging_writer.py:48] [159400] global_step=159400, grad_norm=8.070164680480957, loss=0.912647545337677
I0202 00:33:48.907026 139907754342144 logging_writer.py:48] [159500] global_step=159500, grad_norm=10.217535018920898, loss=1.0444730520248413
I0202 00:34:22.574892 139908710635264 logging_writer.py:48] [159600] global_step=159600, grad_norm=8.333320617675781, loss=0.9552270174026489
I0202 00:34:56.235193 139907754342144 logging_writer.py:48] [159700] global_step=159700, grad_norm=8.159805297851562, loss=0.8799023032188416
I0202 00:35:29.928112 139908710635264 logging_writer.py:48] [159800] global_step=159800, grad_norm=8.187227249145508, loss=0.8839654326438904
I0202 00:36:03.580825 139907754342144 logging_writer.py:48] [159900] global_step=159900, grad_norm=7.968828201293945, loss=0.9188023209571838
I0202 00:36:37.263588 139908710635264 logging_writer.py:48] [160000] global_step=160000, grad_norm=8.35726261138916, loss=0.8967283368110657
I0202 00:37:10.911025 139907754342144 logging_writer.py:48] [160100] global_step=160100, grad_norm=8.745509147644043, loss=1.0072481632232666
I0202 00:37:44.598216 139908710635264 logging_writer.py:48] [160200] global_step=160200, grad_norm=8.556872367858887, loss=0.9757781624794006
I0202 00:38:18.258919 139907754342144 logging_writer.py:48] [160300] global_step=160300, grad_norm=7.327989101409912, loss=0.8853306770324707
I0202 00:38:52.033559 139908710635264 logging_writer.py:48] [160400] global_step=160400, grad_norm=7.87919807434082, loss=0.9702430963516235
I0202 00:39:25.180520 140070692116288 spec.py:321] Evaluating on the training split.
I0202 00:39:31.470325 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 00:39:40.428787 140070692116288 spec.py:349] Evaluating on the test split.
I0202 00:39:43.039112 140070692116288 submission_runner.py:408] Time since start: 55999.49s, 	Step: 160500, 	{'train/accuracy': 0.8909438848495483, 'train/loss': 0.38730230927467346, 'validation/accuracy': 0.750499963760376, 'validation/loss': 1.014423131942749, 'validation/num_examples': 50000, 'test/accuracy': 0.6256000399589539, 'test/loss': 1.7291738986968994, 'test/num_examples': 10000, 'score': 54098.76694107056, 'total_duration': 55999.49178338051, 'accumulated_submission_time': 54098.76694107056, 'accumulated_eval_time': 1888.285579442978, 'accumulated_logging_time': 6.8666510581970215}
I0202 00:39:43.084352 139907762734848 logging_writer.py:48] [160500] accumulated_eval_time=1888.285579, accumulated_logging_time=6.866651, accumulated_submission_time=54098.766941, global_step=160500, preemption_count=0, score=54098.766941, test/accuracy=0.625600, test/loss=1.729174, test/num_examples=10000, total_duration=55999.491783, train/accuracy=0.890944, train/loss=0.387302, validation/accuracy=0.750500, validation/loss=1.014423, validation/num_examples=50000
I0202 00:39:43.430661 139908425447168 logging_writer.py:48] [160500] global_step=160500, grad_norm=9.336315155029297, loss=0.9588287472724915
I0202 00:40:16.989994 139907762734848 logging_writer.py:48] [160600] global_step=160600, grad_norm=8.221399307250977, loss=0.8588178753852844
I0202 00:40:50.670817 139908425447168 logging_writer.py:48] [160700] global_step=160700, grad_norm=7.960641384124756, loss=0.9332090020179749
I0202 00:41:24.330648 139907762734848 logging_writer.py:48] [160800] global_step=160800, grad_norm=8.489043235778809, loss=0.9557454586029053
I0202 00:41:58.013658 139908425447168 logging_writer.py:48] [160900] global_step=160900, grad_norm=8.5774564743042, loss=0.9200224280357361
I0202 00:42:31.668547 139907762734848 logging_writer.py:48] [161000] global_step=161000, grad_norm=7.934178829193115, loss=0.9255765676498413
I0202 00:43:05.350016 139908425447168 logging_writer.py:48] [161100] global_step=161100, grad_norm=8.584678649902344, loss=0.9788985848426819
I0202 00:43:39.019953 139907762734848 logging_writer.py:48] [161200] global_step=161200, grad_norm=9.170045852661133, loss=0.8694464564323425
I0202 00:44:12.711956 139908425447168 logging_writer.py:48] [161300] global_step=161300, grad_norm=8.536702156066895, loss=0.8573261499404907
I0202 00:44:46.459298 139907762734848 logging_writer.py:48] [161400] global_step=161400, grad_norm=8.138797760009766, loss=0.9228998422622681
I0202 00:45:20.154483 139908425447168 logging_writer.py:48] [161500] global_step=161500, grad_norm=9.855734825134277, loss=0.9212971925735474
I0202 00:45:53.842916 139907762734848 logging_writer.py:48] [161600] global_step=161600, grad_norm=10.21368408203125, loss=0.9357922077178955
I0202 00:46:27.552842 139908425447168 logging_writer.py:48] [161700] global_step=161700, grad_norm=8.31814193725586, loss=0.8846867680549622
I0202 00:47:01.222538 139907762734848 logging_writer.py:48] [161800] global_step=161800, grad_norm=8.77273941040039, loss=0.8251101970672607
I0202 00:47:34.912341 139908425447168 logging_writer.py:48] [161900] global_step=161900, grad_norm=7.982069969177246, loss=0.877598762512207
I0202 00:48:08.547924 139907762734848 logging_writer.py:48] [162000] global_step=162000, grad_norm=7.634674549102783, loss=0.8288887739181519
I0202 00:48:13.076447 140070692116288 spec.py:321] Evaluating on the training split.
I0202 00:48:19.307238 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 00:48:27.942676 140070692116288 spec.py:349] Evaluating on the test split.
I0202 00:48:30.559655 140070692116288 submission_runner.py:408] Time since start: 56527.01s, 	Step: 162015, 	{'train/accuracy': 0.8907844424247742, 'train/loss': 0.3792414665222168, 'validation/accuracy': 0.7509599924087524, 'validation/loss': 1.014463186264038, 'validation/num_examples': 50000, 'test/accuracy': 0.6287000179290771, 'test/loss': 1.735564112663269, 'test/num_examples': 10000, 'score': 54608.69594120979, 'total_duration': 56527.012323856354, 'accumulated_submission_time': 54608.69594120979, 'accumulated_eval_time': 1905.7687640190125, 'accumulated_logging_time': 6.921643972396851}
I0202 00:48:30.604735 139907745949440 logging_writer.py:48] [162015] accumulated_eval_time=1905.768764, accumulated_logging_time=6.921644, accumulated_submission_time=54608.695941, global_step=162015, preemption_count=0, score=54608.695941, test/accuracy=0.628700, test/loss=1.735564, test/num_examples=10000, total_duration=56527.012324, train/accuracy=0.890784, train/loss=0.379241, validation/accuracy=0.750960, validation/loss=1.014463, validation/num_examples=50000
I0202 00:48:59.504307 139907754342144 logging_writer.py:48] [162100] global_step=162100, grad_norm=7.717128276824951, loss=0.8870936632156372
I0202 00:49:33.158687 139907745949440 logging_writer.py:48] [162200] global_step=162200, grad_norm=8.104619026184082, loss=0.9284690022468567
I0202 00:50:06.829840 139907754342144 logging_writer.py:48] [162300] global_step=162300, grad_norm=8.038992881774902, loss=0.906713604927063
I0202 00:50:40.495917 139907745949440 logging_writer.py:48] [162400] global_step=162400, grad_norm=8.163187026977539, loss=0.8871391415596008
I0202 00:51:14.244426 139907754342144 logging_writer.py:48] [162500] global_step=162500, grad_norm=8.249397277832031, loss=0.9039618372917175
I0202 00:51:47.967845 139907745949440 logging_writer.py:48] [162600] global_step=162600, grad_norm=7.572281360626221, loss=0.8419678807258606
I0202 00:52:21.658425 139907754342144 logging_writer.py:48] [162700] global_step=162700, grad_norm=7.970053195953369, loss=0.914289653301239
I0202 00:52:55.339858 139907745949440 logging_writer.py:48] [162800] global_step=162800, grad_norm=8.169525146484375, loss=0.9596901535987854
I0202 00:53:29.003861 139907754342144 logging_writer.py:48] [162900] global_step=162900, grad_norm=9.679586410522461, loss=1.0081422328948975
I0202 00:54:02.698307 139907745949440 logging_writer.py:48] [163000] global_step=163000, grad_norm=7.989173889160156, loss=0.8978707194328308
I0202 00:54:36.378356 139907754342144 logging_writer.py:48] [163100] global_step=163100, grad_norm=8.510611534118652, loss=0.9700847864151001
I0202 00:55:10.053409 139907745949440 logging_writer.py:48] [163200] global_step=163200, grad_norm=9.39085865020752, loss=0.9141530394554138
I0202 00:55:43.736344 139907754342144 logging_writer.py:48] [163300] global_step=163300, grad_norm=8.51343059539795, loss=0.8385128974914551
I0202 00:56:17.393086 139907745949440 logging_writer.py:48] [163400] global_step=163400, grad_norm=8.617528915405273, loss=0.9156602025032043
I0202 00:56:51.076699 139907754342144 logging_writer.py:48] [163500] global_step=163500, grad_norm=8.16594409942627, loss=0.8142182230949402
I0202 00:57:00.647598 140070692116288 spec.py:321] Evaluating on the training split.
I0202 00:57:07.155757 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 00:57:15.622686 140070692116288 spec.py:349] Evaluating on the test split.
I0202 00:57:18.250777 140070692116288 submission_runner.py:408] Time since start: 57054.70s, 	Step: 163530, 	{'train/accuracy': 0.8932557106018066, 'train/loss': 0.3726956844329834, 'validation/accuracy': 0.7510600090026855, 'validation/loss': 1.0063519477844238, 'validation/num_examples': 50000, 'test/accuracy': 0.6272000074386597, 'test/loss': 1.7141947746276855, 'test/num_examples': 10000, 'score': 55118.67601776123, 'total_duration': 57054.70344829559, 'accumulated_submission_time': 55118.67601776123, 'accumulated_eval_time': 1923.371912240982, 'accumulated_logging_time': 6.976501703262329}
I0202 00:57:18.298608 139908425447168 logging_writer.py:48] [163530] accumulated_eval_time=1923.371912, accumulated_logging_time=6.976502, accumulated_submission_time=55118.676018, global_step=163530, preemption_count=0, score=55118.676018, test/accuracy=0.627200, test/loss=1.714195, test/num_examples=10000, total_duration=57054.703448, train/accuracy=0.893256, train/loss=0.372696, validation/accuracy=0.751060, validation/loss=1.006352, validation/num_examples=50000
I0202 00:57:42.234503 139908719027968 logging_writer.py:48] [163600] global_step=163600, grad_norm=7.9259114265441895, loss=0.9081038236618042
I0202 00:58:15.919842 139908425447168 logging_writer.py:48] [163700] global_step=163700, grad_norm=8.41285514831543, loss=0.9358906149864197
I0202 00:58:49.608014 139908719027968 logging_writer.py:48] [163800] global_step=163800, grad_norm=8.454626083374023, loss=0.8676044344902039
I0202 00:59:23.267745 139908425447168 logging_writer.py:48] [163900] global_step=163900, grad_norm=8.347265243530273, loss=0.905562162399292
I0202 00:59:56.959081 139908719027968 logging_writer.py:48] [164000] global_step=164000, grad_norm=8.180652618408203, loss=0.8365744352340698
I0202 01:00:30.620760 139908425447168 logging_writer.py:48] [164100] global_step=164100, grad_norm=9.133013725280762, loss=0.9430598616600037
I0202 01:01:04.319252 139908719027968 logging_writer.py:48] [164200] global_step=164200, grad_norm=8.255481719970703, loss=0.9140894412994385
I0202 01:01:37.972120 139908425447168 logging_writer.py:48] [164300] global_step=164300, grad_norm=8.232921600341797, loss=0.8449679017066956
I0202 01:02:11.666674 139908719027968 logging_writer.py:48] [164400] global_step=164400, grad_norm=8.52584457397461, loss=0.8942458629608154
I0202 01:02:45.335610 139908425447168 logging_writer.py:48] [164500] global_step=164500, grad_norm=8.665775299072266, loss=0.9312687516212463
I0202 01:03:19.043394 139908719027968 logging_writer.py:48] [164600] global_step=164600, grad_norm=8.529867172241211, loss=0.8584308624267578
I0202 01:03:52.776875 139908425447168 logging_writer.py:48] [164700] global_step=164700, grad_norm=8.159675598144531, loss=0.8615258932113647
I0202 01:04:26.518642 139908719027968 logging_writer.py:48] [164800] global_step=164800, grad_norm=7.7734055519104, loss=0.7078307271003723
I0202 01:05:00.175491 139908425447168 logging_writer.py:48] [164900] global_step=164900, grad_norm=8.260198593139648, loss=0.8645796179771423
I0202 01:05:33.877106 139908719027968 logging_writer.py:48] [165000] global_step=165000, grad_norm=9.39285945892334, loss=0.8489894866943359
I0202 01:05:48.477387 140070692116288 spec.py:321] Evaluating on the training split.
I0202 01:05:54.743623 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 01:06:03.622067 140070692116288 spec.py:349] Evaluating on the test split.
I0202 01:06:06.239344 140070692116288 submission_runner.py:408] Time since start: 57582.69s, 	Step: 165045, 	{'train/accuracy': 0.8986766338348389, 'train/loss': 0.3579636812210083, 'validation/accuracy': 0.7545199990272522, 'validation/loss': 0.9984259009361267, 'validation/num_examples': 50000, 'test/accuracy': 0.6301000118255615, 'test/loss': 1.7019473314285278, 'test/num_examples': 10000, 'score': 55628.793186903, 'total_duration': 57582.69201374054, 'accumulated_submission_time': 55628.793186903, 'accumulated_eval_time': 1941.1338379383087, 'accumulated_logging_time': 7.033837080001831}
I0202 01:06:06.284668 139907729164032 logging_writer.py:48] [165045] accumulated_eval_time=1941.133838, accumulated_logging_time=7.033837, accumulated_submission_time=55628.793187, global_step=165045, preemption_count=0, score=55628.793187, test/accuracy=0.630100, test/loss=1.701947, test/num_examples=10000, total_duration=57582.692014, train/accuracy=0.898677, train/loss=0.357964, validation/accuracy=0.754520, validation/loss=0.998426, validation/num_examples=50000
I0202 01:06:25.132026 139907737556736 logging_writer.py:48] [165100] global_step=165100, grad_norm=7.894866943359375, loss=0.8568714261054993
I0202 01:06:58.789115 139907729164032 logging_writer.py:48] [165200] global_step=165200, grad_norm=9.307608604431152, loss=0.9539433121681213
I0202 01:07:32.463619 139907737556736 logging_writer.py:48] [165300] global_step=165300, grad_norm=8.551812171936035, loss=0.8844443559646606
I0202 01:08:06.111238 139907729164032 logging_writer.py:48] [165400] global_step=165400, grad_norm=8.187314987182617, loss=0.8512312173843384
I0202 01:08:39.777098 139907737556736 logging_writer.py:48] [165500] global_step=165500, grad_norm=8.055869102478027, loss=0.8106553554534912
I0202 01:09:13.441820 139907729164032 logging_writer.py:48] [165600] global_step=165600, grad_norm=7.470773696899414, loss=0.7712337374687195
I0202 01:09:47.125941 139907737556736 logging_writer.py:48] [165700] global_step=165700, grad_norm=9.106224060058594, loss=0.9139999151229858
I0202 01:10:20.855251 139907729164032 logging_writer.py:48] [165800] global_step=165800, grad_norm=9.523957252502441, loss=0.8895912170410156
I0202 01:10:54.585445 139907737556736 logging_writer.py:48] [165900] global_step=165900, grad_norm=8.407870292663574, loss=0.8841685056686401
I0202 01:11:28.260733 139907729164032 logging_writer.py:48] [166000] global_step=166000, grad_norm=8.493634223937988, loss=0.889724850654602
I0202 01:12:01.942099 139907737556736 logging_writer.py:48] [166100] global_step=166100, grad_norm=7.957158088684082, loss=0.7485265731811523
I0202 01:12:35.609978 139907729164032 logging_writer.py:48] [166200] global_step=166200, grad_norm=9.151053428649902, loss=0.9374315738677979
I0202 01:13:09.285743 139907737556736 logging_writer.py:48] [166300] global_step=166300, grad_norm=9.512849807739258, loss=0.9213078022003174
I0202 01:13:42.943952 139907729164032 logging_writer.py:48] [166400] global_step=166400, grad_norm=8.055466651916504, loss=0.8764803409576416
I0202 01:14:16.626944 139907737556736 logging_writer.py:48] [166500] global_step=166500, grad_norm=8.001605033874512, loss=0.8260526657104492
I0202 01:14:36.314470 140070692116288 spec.py:321] Evaluating on the training split.
I0202 01:14:42.737132 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 01:14:51.074542 140070692116288 spec.py:349] Evaluating on the test split.
I0202 01:14:53.609773 140070692116288 submission_runner.py:408] Time since start: 58110.06s, 	Step: 166560, 	{'train/accuracy': 0.9122488498687744, 'train/loss': 0.3140309453010559, 'validation/accuracy': 0.7552599906921387, 'validation/loss': 0.9961941838264465, 'validation/num_examples': 50000, 'test/accuracy': 0.633400022983551, 'test/loss': 1.6993948221206665, 'test/num_examples': 10000, 'score': 56138.758283376694, 'total_duration': 58110.06244254112, 'accumulated_submission_time': 56138.758283376694, 'accumulated_eval_time': 1958.429122209549, 'accumulated_logging_time': 7.088637590408325}
I0202 01:14:53.658215 139908425447168 logging_writer.py:48] [166560] accumulated_eval_time=1958.429122, accumulated_logging_time=7.088638, accumulated_submission_time=56138.758283, global_step=166560, preemption_count=0, score=56138.758283, test/accuracy=0.633400, test/loss=1.699395, test/num_examples=10000, total_duration=58110.062443, train/accuracy=0.912249, train/loss=0.314031, validation/accuracy=0.755260, validation/loss=0.996194, validation/num_examples=50000
I0202 01:15:07.451850 139908710635264 logging_writer.py:48] [166600] global_step=166600, grad_norm=9.267807960510254, loss=0.8778339624404907
I0202 01:15:41.019968 139908425447168 logging_writer.py:48] [166700] global_step=166700, grad_norm=9.379432678222656, loss=0.8188566565513611
I0202 01:16:14.661534 139908710635264 logging_writer.py:48] [166800] global_step=166800, grad_norm=8.531744003295898, loss=0.8536813855171204
I0202 01:16:48.522938 139908425447168 logging_writer.py:48] [166900] global_step=166900, grad_norm=8.491409301757812, loss=0.8171502351760864
I0202 01:17:22.178719 139908710635264 logging_writer.py:48] [167000] global_step=167000, grad_norm=8.05250358581543, loss=0.8765814304351807
I0202 01:17:55.866612 139908425447168 logging_writer.py:48] [167100] global_step=167100, grad_norm=9.335342407226562, loss=0.879135012626648
I0202 01:18:29.534855 139908710635264 logging_writer.py:48] [167200] global_step=167200, grad_norm=8.335119247436523, loss=0.8235670328140259
I0202 01:19:03.231331 139908425447168 logging_writer.py:48] [167300] global_step=167300, grad_norm=8.752915382385254, loss=0.8610498309135437
I0202 01:19:36.890460 139908710635264 logging_writer.py:48] [167400] global_step=167400, grad_norm=8.401613235473633, loss=0.7958304286003113
I0202 01:20:10.560821 139908425447168 logging_writer.py:48] [167500] global_step=167500, grad_norm=8.639758110046387, loss=0.8185315132141113
I0202 01:20:44.235228 139908710635264 logging_writer.py:48] [167600] global_step=167600, grad_norm=8.920403480529785, loss=0.7750184535980225
I0202 01:21:17.927752 139908425447168 logging_writer.py:48] [167700] global_step=167700, grad_norm=8.414388656616211, loss=0.845249593257904
I0202 01:21:51.556153 139908710635264 logging_writer.py:48] [167800] global_step=167800, grad_norm=8.383339881896973, loss=0.8407257199287415
I0202 01:22:25.249964 139908425447168 logging_writer.py:48] [167900] global_step=167900, grad_norm=10.017843246459961, loss=0.9501468539237976
I0202 01:22:59.051831 139908710635264 logging_writer.py:48] [168000] global_step=168000, grad_norm=9.602777481079102, loss=0.8567430377006531
I0202 01:23:23.786488 140070692116288 spec.py:321] Evaluating on the training split.
I0202 01:23:30.082645 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 01:23:38.778321 140070692116288 spec.py:349] Evaluating on the test split.
I0202 01:23:41.385311 140070692116288 submission_runner.py:408] Time since start: 58637.84s, 	Step: 168075, 	{'train/accuracy': 0.9167729616165161, 'train/loss': 0.29545295238494873, 'validation/accuracy': 0.7545599937438965, 'validation/loss': 0.992598831653595, 'validation/num_examples': 50000, 'test/accuracy': 0.6340000033378601, 'test/loss': 1.6936455965042114, 'test/num_examples': 10000, 'score': 56648.82356977463, 'total_duration': 58637.83798003197, 'accumulated_submission_time': 56648.82356977463, 'accumulated_eval_time': 1976.0279388427734, 'accumulated_logging_time': 7.146857023239136}
I0202 01:23:41.436777 139907754342144 logging_writer.py:48] [168075] accumulated_eval_time=1976.027939, accumulated_logging_time=7.146857, accumulated_submission_time=56648.823570, global_step=168075, preemption_count=0, score=56648.823570, test/accuracy=0.634000, test/loss=1.693646, test/num_examples=10000, total_duration=58637.837980, train/accuracy=0.916773, train/loss=0.295453, validation/accuracy=0.754560, validation/loss=0.992599, validation/num_examples=50000
I0202 01:23:50.182060 139907762734848 logging_writer.py:48] [168100] global_step=168100, grad_norm=9.557328224182129, loss=0.8203845024108887
I0202 01:24:23.823121 139907754342144 logging_writer.py:48] [168200] global_step=168200, grad_norm=8.841368675231934, loss=0.8068124651908875
I0202 01:24:57.501001 139907762734848 logging_writer.py:48] [168300] global_step=168300, grad_norm=9.716835021972656, loss=0.8596850633621216
I0202 01:25:31.191439 139907754342144 logging_writer.py:48] [168400] global_step=168400, grad_norm=8.843524932861328, loss=0.8268427848815918
I0202 01:26:04.845446 139907762734848 logging_writer.py:48] [168500] global_step=168500, grad_norm=9.439659118652344, loss=0.8090205192565918
I0202 01:26:38.520613 139907754342144 logging_writer.py:48] [168600] global_step=168600, grad_norm=8.727481842041016, loss=0.8274070620536804
I0202 01:27:12.185902 139907762734848 logging_writer.py:48] [168700] global_step=168700, grad_norm=9.864733695983887, loss=0.8525590300559998
I0202 01:27:45.877463 139907754342144 logging_writer.py:48] [168800] global_step=168800, grad_norm=9.078551292419434, loss=0.9022482633590698
I0202 01:28:19.548441 139907762734848 logging_writer.py:48] [168900] global_step=168900, grad_norm=8.993971824645996, loss=0.7895560264587402
I0202 01:28:53.241182 139907754342144 logging_writer.py:48] [169000] global_step=169000, grad_norm=8.889262199401855, loss=0.8511989116668701
I0202 01:29:27.011346 139907762734848 logging_writer.py:48] [169100] global_step=169100, grad_norm=9.226008415222168, loss=0.8194145560264587
I0202 01:30:00.718357 139907754342144 logging_writer.py:48] [169200] global_step=169200, grad_norm=8.407913208007812, loss=0.8686480522155762
I0202 01:30:34.386293 139907762734848 logging_writer.py:48] [169300] global_step=169300, grad_norm=9.382454872131348, loss=0.8022952079772949
I0202 01:31:08.068121 139907754342144 logging_writer.py:48] [169400] global_step=169400, grad_norm=8.272961616516113, loss=0.7604345679283142
I0202 01:31:41.741642 139907762734848 logging_writer.py:48] [169500] global_step=169500, grad_norm=8.67625904083252, loss=0.8753247857093811
I0202 01:32:11.537715 140070692116288 spec.py:321] Evaluating on the training split.
I0202 01:32:17.789293 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 01:32:26.260815 140070692116288 spec.py:349] Evaluating on the test split.
I0202 01:32:28.900685 140070692116288 submission_runner.py:408] Time since start: 59165.35s, 	Step: 169590, 	{'train/accuracy': 0.9133848547935486, 'train/loss': 0.3045446276664734, 'validation/accuracy': 0.7583999633789062, 'validation/loss': 0.9829720854759216, 'validation/num_examples': 50000, 'test/accuracy': 0.6345000267028809, 'test/loss': 1.6890100240707397, 'test/num_examples': 10000, 'score': 57158.8615398407, 'total_duration': 59165.35334587097, 'accumulated_submission_time': 57158.8615398407, 'accumulated_eval_time': 1993.3908696174622, 'accumulated_logging_time': 7.208467960357666}
I0202 01:32:28.945334 139908710635264 logging_writer.py:48] [169590] accumulated_eval_time=1993.390870, accumulated_logging_time=7.208468, accumulated_submission_time=57158.861540, global_step=169590, preemption_count=0, score=57158.861540, test/accuracy=0.634500, test/loss=1.689010, test/num_examples=10000, total_duration=59165.353346, train/accuracy=0.913385, train/loss=0.304545, validation/accuracy=0.758400, validation/loss=0.982972, validation/num_examples=50000
I0202 01:32:32.642116 139908719027968 logging_writer.py:48] [169600] global_step=169600, grad_norm=9.268549919128418, loss=0.7681277394294739
I0202 01:33:06.275447 139908710635264 logging_writer.py:48] [169700] global_step=169700, grad_norm=8.88174819946289, loss=0.8509317636489868
I0202 01:33:39.951136 139908719027968 logging_writer.py:48] [169800] global_step=169800, grad_norm=8.934759140014648, loss=0.7265774011611938
I0202 01:34:13.620804 139908710635264 logging_writer.py:48] [169900] global_step=169900, grad_norm=8.288650512695312, loss=0.7811077833175659
I0202 01:34:47.320904 139908719027968 logging_writer.py:48] [170000] global_step=170000, grad_norm=9.179733276367188, loss=0.8590856790542603
I0202 01:35:20.978959 139908710635264 logging_writer.py:48] [170100] global_step=170100, grad_norm=9.0971040725708, loss=0.7993181943893433
I0202 01:35:54.750503 139908719027968 logging_writer.py:48] [170200] global_step=170200, grad_norm=8.85981559753418, loss=0.871382474899292
I0202 01:36:28.431277 139908710635264 logging_writer.py:48] [170300] global_step=170300, grad_norm=8.91751766204834, loss=0.775413990020752
I0202 01:37:02.117818 139908719027968 logging_writer.py:48] [170400] global_step=170400, grad_norm=9.039725303649902, loss=0.7216106057167053
I0202 01:37:35.763753 139908710635264 logging_writer.py:48] [170500] global_step=170500, grad_norm=9.111180305480957, loss=0.8142952919006348
I0202 01:38:09.446840 139908719027968 logging_writer.py:48] [170600] global_step=170600, grad_norm=8.900299072265625, loss=0.8846587538719177
I0202 01:38:43.099557 139908710635264 logging_writer.py:48] [170700] global_step=170700, grad_norm=8.560218811035156, loss=0.7879550457000732
I0202 01:39:16.771525 139908719027968 logging_writer.py:48] [170800] global_step=170800, grad_norm=8.762824058532715, loss=0.8132851719856262
I0202 01:39:50.436043 139908710635264 logging_writer.py:48] [170900] global_step=170900, grad_norm=8.688429832458496, loss=0.7906683087348938
I0202 01:40:24.122272 139908719027968 logging_writer.py:48] [171000] global_step=171000, grad_norm=8.163290977478027, loss=0.7641589045524597
I0202 01:40:57.782563 139908710635264 logging_writer.py:48] [171100] global_step=171100, grad_norm=9.226521492004395, loss=0.7711715698242188
I0202 01:40:58.945463 140070692116288 spec.py:321] Evaluating on the training split.
I0202 01:41:05.459865 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 01:41:13.880514 140070692116288 spec.py:349] Evaluating on the test split.
I0202 01:41:16.469800 140070692116288 submission_runner.py:408] Time since start: 59692.92s, 	Step: 171105, 	{'train/accuracy': 0.9176697731018066, 'train/loss': 0.2919151186943054, 'validation/accuracy': 0.7594999670982361, 'validation/loss': 0.9838279485702515, 'validation/num_examples': 50000, 'test/accuracy': 0.6374000310897827, 'test/loss': 1.6809219121932983, 'test/num_examples': 10000, 'score': 57668.79889130592, 'total_duration': 59692.92246937752, 'accumulated_submission_time': 57668.79889130592, 'accumulated_eval_time': 2010.9151842594147, 'accumulated_logging_time': 7.263584613800049}
I0202 01:41:16.516129 139907737556736 logging_writer.py:48] [171105] accumulated_eval_time=2010.915184, accumulated_logging_time=7.263585, accumulated_submission_time=57668.798891, global_step=171105, preemption_count=0, score=57668.798891, test/accuracy=0.637400, test/loss=1.680922, test/num_examples=10000, total_duration=59692.922469, train/accuracy=0.917670, train/loss=0.291915, validation/accuracy=0.759500, validation/loss=0.983828, validation/num_examples=50000
I0202 01:41:48.821686 139907745949440 logging_writer.py:48] [171200] global_step=171200, grad_norm=8.719589233398438, loss=0.7823224067687988
I0202 01:42:22.503392 139907737556736 logging_writer.py:48] [171300] global_step=171300, grad_norm=8.671648979187012, loss=0.7685701251029968
I0202 01:42:56.192430 139907745949440 logging_writer.py:48] [171400] global_step=171400, grad_norm=9.54719352722168, loss=0.7929567694664001
I0202 01:43:29.875975 139907737556736 logging_writer.py:48] [171500] global_step=171500, grad_norm=8.864623069763184, loss=0.8678210377693176
I0202 01:44:03.529924 139907745949440 logging_writer.py:48] [171600] global_step=171600, grad_norm=8.972545623779297, loss=0.7749371528625488
I0202 01:44:37.214457 139907737556736 logging_writer.py:48] [171700] global_step=171700, grad_norm=9.414689064025879, loss=0.8905814290046692
I0202 01:45:10.870828 139907745949440 logging_writer.py:48] [171800] global_step=171800, grad_norm=9.121129989624023, loss=0.7940284013748169
I0202 01:45:44.556915 139907737556736 logging_writer.py:48] [171900] global_step=171900, grad_norm=9.224499702453613, loss=0.7819780111312866
I0202 01:46:18.224699 139907745949440 logging_writer.py:48] [172000] global_step=172000, grad_norm=9.401636123657227, loss=0.8704463243484497
I0202 01:46:51.899737 139907737556736 logging_writer.py:48] [172100] global_step=172100, grad_norm=8.31032657623291, loss=0.7373194694519043
I0202 01:47:25.559425 139907745949440 logging_writer.py:48] [172200] global_step=172200, grad_norm=9.20382022857666, loss=0.8029146790504456
I0202 01:47:59.347662 139907737556736 logging_writer.py:48] [172300] global_step=172300, grad_norm=9.054218292236328, loss=0.8278492093086243
I0202 01:48:33.069886 139907745949440 logging_writer.py:48] [172400] global_step=172400, grad_norm=8.205153465270996, loss=0.6907491683959961
I0202 01:49:06.756026 139907737556736 logging_writer.py:48] [172500] global_step=172500, grad_norm=9.755433082580566, loss=0.7774525880813599
I0202 01:49:40.423583 139907745949440 logging_writer.py:48] [172600] global_step=172600, grad_norm=8.57890510559082, loss=0.7967426776885986
I0202 01:49:46.630029 140070692116288 spec.py:321] Evaluating on the training split.
I0202 01:49:52.916105 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 01:50:01.391526 140070692116288 spec.py:349] Evaluating on the test split.
I0202 01:50:04.253115 140070692116288 submission_runner.py:408] Time since start: 60220.71s, 	Step: 172620, 	{'train/accuracy': 0.9196228981018066, 'train/loss': 0.279366672039032, 'validation/accuracy': 0.7603200078010559, 'validation/loss': 0.9797834157943726, 'validation/num_examples': 50000, 'test/accuracy': 0.6355000138282776, 'test/loss': 1.6834412813186646, 'test/num_examples': 10000, 'score': 58178.850531578064, 'total_duration': 60220.705787181854, 'accumulated_submission_time': 58178.850531578064, 'accumulated_eval_time': 2028.5382385253906, 'accumulated_logging_time': 7.319125175476074}
I0202 01:50:04.302719 139908719027968 logging_writer.py:48] [172620] accumulated_eval_time=2028.538239, accumulated_logging_time=7.319125, accumulated_submission_time=58178.850532, global_step=172620, preemption_count=0, score=58178.850532, test/accuracy=0.635500, test/loss=1.683441, test/num_examples=10000, total_duration=60220.705787, train/accuracy=0.919623, train/loss=0.279367, validation/accuracy=0.760320, validation/loss=0.979783, validation/num_examples=50000
I0202 01:50:31.569746 139908727420672 logging_writer.py:48] [172700] global_step=172700, grad_norm=8.900402069091797, loss=0.7630106806755066
I0202 01:51:05.216772 139908719027968 logging_writer.py:48] [172800] global_step=172800, grad_norm=8.859331130981445, loss=0.7430009245872498
I0202 01:51:38.920317 139908727420672 logging_writer.py:48] [172900] global_step=172900, grad_norm=9.98369026184082, loss=0.743327796459198
I0202 01:52:12.578572 139908719027968 logging_writer.py:48] [173000] global_step=173000, grad_norm=9.501487731933594, loss=0.8110021352767944
I0202 01:52:46.278780 139908727420672 logging_writer.py:48] [173100] global_step=173100, grad_norm=8.900623321533203, loss=0.7951812744140625
I0202 01:53:19.937079 139908719027968 logging_writer.py:48] [173200] global_step=173200, grad_norm=8.887990951538086, loss=0.8221765756607056
I0202 01:53:53.632766 139908727420672 logging_writer.py:48] [173300] global_step=173300, grad_norm=9.192184448242188, loss=0.8226589560508728
I0202 01:54:27.349646 139908719027968 logging_writer.py:48] [173400] global_step=173400, grad_norm=8.179908752441406, loss=0.7604398131370544
I0202 01:55:01.052404 139908727420672 logging_writer.py:48] [173500] global_step=173500, grad_norm=8.793846130371094, loss=0.7133040428161621
I0202 01:55:34.718544 139908719027968 logging_writer.py:48] [173600] global_step=173600, grad_norm=8.954936981201172, loss=0.6763935089111328
I0202 01:56:08.414699 139908727420672 logging_writer.py:48] [173700] global_step=173700, grad_norm=9.1022367477417, loss=0.7674009799957275
I0202 01:56:42.064069 139908719027968 logging_writer.py:48] [173800] global_step=173800, grad_norm=9.658056259155273, loss=0.7697113156318665
I0202 01:57:15.780159 139908727420672 logging_writer.py:48] [173900] global_step=173900, grad_norm=10.166398048400879, loss=0.7813913822174072
I0202 01:57:49.446987 139908719027968 logging_writer.py:48] [174000] global_step=174000, grad_norm=9.093481063842773, loss=0.7209469676017761
I0202 01:58:23.154539 139908727420672 logging_writer.py:48] [174100] global_step=174100, grad_norm=9.560715675354004, loss=0.7758186459541321
I0202 01:58:34.408791 140070692116288 spec.py:321] Evaluating on the training split.
I0202 01:58:40.685860 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 01:58:49.315433 140070692116288 spec.py:349] Evaluating on the test split.
I0202 01:58:51.943112 140070692116288 submission_runner.py:408] Time since start: 60748.40s, 	Step: 174135, 	{'train/accuracy': 0.9229113459587097, 'train/loss': 0.2737523913383484, 'validation/accuracy': 0.7621399760246277, 'validation/loss': 0.9760602712631226, 'validation/num_examples': 50000, 'test/accuracy': 0.6358000040054321, 'test/loss': 1.688401460647583, 'test/num_examples': 10000, 'score': 58688.894991636276, 'total_duration': 60748.39577841759, 'accumulated_submission_time': 58688.894991636276, 'accumulated_eval_time': 2046.0725243091583, 'accumulated_logging_time': 7.3778181076049805}
I0202 01:58:51.989080 139907737556736 logging_writer.py:48] [174135] accumulated_eval_time=2046.072524, accumulated_logging_time=7.377818, accumulated_submission_time=58688.894992, global_step=174135, preemption_count=0, score=58688.894992, test/accuracy=0.635800, test/loss=1.688401, test/num_examples=10000, total_duration=60748.395778, train/accuracy=0.922911, train/loss=0.273752, validation/accuracy=0.762140, validation/loss=0.976060, validation/num_examples=50000
I0202 01:59:14.164602 139907745949440 logging_writer.py:48] [174200] global_step=174200, grad_norm=9.298103332519531, loss=0.8014614582061768
I0202 01:59:47.791448 139907737556736 logging_writer.py:48] [174300] global_step=174300, grad_norm=8.717120170593262, loss=0.8230697512626648
I0202 02:00:21.505818 139907745949440 logging_writer.py:48] [174400] global_step=174400, grad_norm=9.867437362670898, loss=0.7822409868240356
I0202 02:00:55.272722 139907737556736 logging_writer.py:48] [174500] global_step=174500, grad_norm=9.732571601867676, loss=0.7280987501144409
I0202 02:01:29.005384 139907745949440 logging_writer.py:48] [174600] global_step=174600, grad_norm=8.67731761932373, loss=0.7232348322868347
I0202 02:02:02.668259 139907737556736 logging_writer.py:48] [174700] global_step=174700, grad_norm=8.767121315002441, loss=0.7261427640914917
I0202 02:02:36.357358 139907745949440 logging_writer.py:48] [174800] global_step=174800, grad_norm=8.970044136047363, loss=0.7965810298919678
I0202 02:03:10.016662 139907737556736 logging_writer.py:48] [174900] global_step=174900, grad_norm=8.079980850219727, loss=0.7135429978370667
I0202 02:03:43.686660 139907745949440 logging_writer.py:48] [175000] global_step=175000, grad_norm=9.212592124938965, loss=0.7883875370025635
I0202 02:04:17.355016 139907737556736 logging_writer.py:48] [175100] global_step=175100, grad_norm=8.776740074157715, loss=0.7832142114639282
I0202 02:04:51.055755 139907745949440 logging_writer.py:48] [175200] global_step=175200, grad_norm=9.009466171264648, loss=0.7712428569793701
I0202 02:05:24.709020 139907737556736 logging_writer.py:48] [175300] global_step=175300, grad_norm=9.687728881835938, loss=0.769716739654541
I0202 02:05:58.398160 139907745949440 logging_writer.py:48] [175400] global_step=175400, grad_norm=8.47598934173584, loss=0.7876309156417847
I0202 02:06:32.066931 139907737556736 logging_writer.py:48] [175500] global_step=175500, grad_norm=8.685445785522461, loss=0.8009461164474487
I0202 02:07:05.806992 139907745949440 logging_writer.py:48] [175600] global_step=175600, grad_norm=8.588200569152832, loss=0.7005916833877563
I0202 02:07:22.147223 140070692116288 spec.py:321] Evaluating on the training split.
I0202 02:07:28.487983 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 02:07:37.187334 140070692116288 spec.py:349] Evaluating on the test split.
I0202 02:07:39.816219 140070692116288 submission_runner.py:408] Time since start: 61276.27s, 	Step: 175650, 	{'train/accuracy': 0.9290696382522583, 'train/loss': 0.25206708908081055, 'validation/accuracy': 0.7624199986457825, 'validation/loss': 0.9706498980522156, 'validation/num_examples': 50000, 'test/accuracy': 0.6411000490188599, 'test/loss': 1.676186203956604, 'test/num_examples': 10000, 'score': 59198.99043011665, 'total_duration': 61276.268881082535, 'accumulated_submission_time': 59198.99043011665, 'accumulated_eval_time': 2063.7414784431458, 'accumulated_logging_time': 7.4339611530303955}
I0202 02:07:39.862205 139907729164032 logging_writer.py:48] [175650] accumulated_eval_time=2063.741478, accumulated_logging_time=7.433961, accumulated_submission_time=59198.990430, global_step=175650, preemption_count=0, score=59198.990430, test/accuracy=0.641100, test/loss=1.676186, test/num_examples=10000, total_duration=61276.268881, train/accuracy=0.929070, train/loss=0.252067, validation/accuracy=0.762420, validation/loss=0.970650, validation/num_examples=50000
I0202 02:07:57.011903 139907737556736 logging_writer.py:48] [175700] global_step=175700, grad_norm=9.564629554748535, loss=0.7269386053085327
I0202 02:08:30.607300 139907729164032 logging_writer.py:48] [175800] global_step=175800, grad_norm=9.16122055053711, loss=0.80684894323349
I0202 02:09:04.274883 139907737556736 logging_writer.py:48] [175900] global_step=175900, grad_norm=8.954997062683105, loss=0.7181689739227295
I0202 02:09:37.978983 139907729164032 logging_writer.py:48] [176000] global_step=176000, grad_norm=7.87210750579834, loss=0.7567841410636902
I0202 02:10:11.635764 139907737556736 logging_writer.py:48] [176100] global_step=176100, grad_norm=9.212569236755371, loss=0.8206939697265625
I0202 02:10:45.316835 139907729164032 logging_writer.py:48] [176200] global_step=176200, grad_norm=9.359414100646973, loss=0.7609543800354004
I0202 02:11:18.977898 139907737556736 logging_writer.py:48] [176300] global_step=176300, grad_norm=8.749191284179688, loss=0.7877758145332336
I0202 02:11:52.674031 139907729164032 logging_writer.py:48] [176400] global_step=176400, grad_norm=9.924484252929688, loss=0.7469557523727417
I0202 02:12:26.354257 139907737556736 logging_writer.py:48] [176500] global_step=176500, grad_norm=9.258834838867188, loss=0.7823473811149597
I0202 02:13:00.035722 139907729164032 logging_writer.py:48] [176600] global_step=176600, grad_norm=8.325841903686523, loss=0.6907356977462769
I0202 02:13:33.793433 139907737556736 logging_writer.py:48] [176700] global_step=176700, grad_norm=9.129409790039062, loss=0.777165412902832
I0202 02:14:07.523182 139907729164032 logging_writer.py:48] [176800] global_step=176800, grad_norm=7.804750442504883, loss=0.6558897495269775
I0202 02:14:41.195069 139907737556736 logging_writer.py:48] [176900] global_step=176900, grad_norm=8.900273323059082, loss=0.7898052930831909
I0202 02:15:14.885367 139907729164032 logging_writer.py:48] [177000] global_step=177000, grad_norm=9.365643501281738, loss=0.7585951685905457
I0202 02:15:48.555287 139907737556736 logging_writer.py:48] [177100] global_step=177100, grad_norm=8.826104164123535, loss=0.7261931896209717
I0202 02:16:09.919770 140070692116288 spec.py:321] Evaluating on the training split.
I0202 02:16:16.271128 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 02:16:24.923764 140070692116288 spec.py:349] Evaluating on the test split.
I0202 02:16:27.535111 140070692116288 submission_runner.py:408] Time since start: 61803.99s, 	Step: 177165, 	{'train/accuracy': 0.9302256107330322, 'train/loss': 0.24820855259895325, 'validation/accuracy': 0.7620399594306946, 'validation/loss': 0.9679412245750427, 'validation/num_examples': 50000, 'test/accuracy': 0.6412000060081482, 'test/loss': 1.673362374305725, 'test/num_examples': 10000, 'score': 59708.983047008514, 'total_duration': 61803.98777985573, 'accumulated_submission_time': 59708.983047008514, 'accumulated_eval_time': 2081.356790304184, 'accumulated_logging_time': 7.493084192276001}
I0202 02:16:27.583487 139907737556736 logging_writer.py:48] [177165] accumulated_eval_time=2081.356790, accumulated_logging_time=7.493084, accumulated_submission_time=59708.983047, global_step=177165, preemption_count=0, score=59708.983047, test/accuracy=0.641200, test/loss=1.673362, test/num_examples=10000, total_duration=61803.987780, train/accuracy=0.930226, train/loss=0.248209, validation/accuracy=0.762040, validation/loss=0.967941, validation/num_examples=50000
I0202 02:16:39.697812 139908425447168 logging_writer.py:48] [177200] global_step=177200, grad_norm=10.565101623535156, loss=0.8210322260856628
I0202 02:17:13.302880 139907737556736 logging_writer.py:48] [177300] global_step=177300, grad_norm=9.420682907104492, loss=0.7658438086509705
I0202 02:17:46.991683 139908425447168 logging_writer.py:48] [177400] global_step=177400, grad_norm=9.92307186126709, loss=0.7692701816558838
I0202 02:18:20.688901 139907737556736 logging_writer.py:48] [177500] global_step=177500, grad_norm=9.541068077087402, loss=0.6766928434371948
I0202 02:18:54.349656 139908425447168 logging_writer.py:48] [177600] global_step=177600, grad_norm=9.167642593383789, loss=0.7795053124427795
I0202 02:19:28.028907 139907737556736 logging_writer.py:48] [177700] global_step=177700, grad_norm=9.87714672088623, loss=0.813411295413971
I0202 02:20:01.790937 139908425447168 logging_writer.py:48] [177800] global_step=177800, grad_norm=9.144311904907227, loss=0.7274144291877747
I0202 02:20:35.517452 139907737556736 logging_writer.py:48] [177900] global_step=177900, grad_norm=9.806835174560547, loss=0.7347439527511597
I0202 02:21:09.180284 139908425447168 logging_writer.py:48] [178000] global_step=178000, grad_norm=8.702977180480957, loss=0.712268054485321
I0202 02:21:42.884105 139907737556736 logging_writer.py:48] [178100] global_step=178100, grad_norm=8.47649097442627, loss=0.6710186004638672
I0202 02:22:16.533523 139908425447168 logging_writer.py:48] [178200] global_step=178200, grad_norm=8.015768051147461, loss=0.6953146457672119
I0202 02:22:50.228224 139907737556736 logging_writer.py:48] [178300] global_step=178300, grad_norm=9.003297805786133, loss=0.7634978294372559
I0202 02:23:23.873171 139908425447168 logging_writer.py:48] [178400] global_step=178400, grad_norm=9.495012283325195, loss=0.7370252013206482
I0202 02:23:57.559387 139907737556736 logging_writer.py:48] [178500] global_step=178500, grad_norm=8.479387283325195, loss=0.7806254029273987
I0202 02:24:31.222304 139908425447168 logging_writer.py:48] [178600] global_step=178600, grad_norm=9.79837703704834, loss=0.7443733215332031
I0202 02:24:57.649001 140070692116288 spec.py:321] Evaluating on the training split.
I0202 02:25:04.099421 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 02:25:12.499171 140070692116288 spec.py:349] Evaluating on the test split.
I0202 02:25:15.017737 140070692116288 submission_runner.py:408] Time since start: 62331.47s, 	Step: 178680, 	{'train/accuracy': 0.9304248690605164, 'train/loss': 0.24876093864440918, 'validation/accuracy': 0.7635599970817566, 'validation/loss': 0.9660710692405701, 'validation/num_examples': 50000, 'test/accuracy': 0.6420000195503235, 'test/loss': 1.6618832349777222, 'test/num_examples': 10000, 'score': 60218.987303733826, 'total_duration': 62331.470396757126, 'accumulated_submission_time': 60218.987303733826, 'accumulated_eval_time': 2098.7254860401154, 'accumulated_logging_time': 7.550928115844727}
I0202 02:25:15.066816 139907745949440 logging_writer.py:48] [178680] accumulated_eval_time=2098.725486, accumulated_logging_time=7.550928, accumulated_submission_time=60218.987304, global_step=178680, preemption_count=0, score=60218.987304, test/accuracy=0.642000, test/loss=1.661883, test/num_examples=10000, total_duration=62331.470397, train/accuracy=0.930425, train/loss=0.248761, validation/accuracy=0.763560, validation/loss=0.966071, validation/num_examples=50000
I0202 02:25:22.122872 139907754342144 logging_writer.py:48] [178700] global_step=178700, grad_norm=8.662737846374512, loss=0.7092625498771667
I0202 02:25:55.810143 139907745949440 logging_writer.py:48] [178800] global_step=178800, grad_norm=9.105496406555176, loss=0.836045503616333
I0202 02:26:29.542529 139907754342144 logging_writer.py:48] [178900] global_step=178900, grad_norm=9.538928031921387, loss=0.7792143821716309
I0202 02:27:03.270181 139907745949440 logging_writer.py:48] [179000] global_step=179000, grad_norm=9.215129852294922, loss=0.7459802031517029
I0202 02:27:36.962745 139907754342144 logging_writer.py:48] [179100] global_step=179100, grad_norm=8.57172966003418, loss=0.6303161978721619
I0202 02:28:10.646221 139907745949440 logging_writer.py:48] [179200] global_step=179200, grad_norm=9.996711730957031, loss=0.7467833757400513
I0202 02:28:44.316430 139907754342144 logging_writer.py:48] [179300] global_step=179300, grad_norm=9.653631210327148, loss=0.6777435541152954
I0202 02:29:17.993594 139907745949440 logging_writer.py:48] [179400] global_step=179400, grad_norm=10.120901107788086, loss=0.7945722341537476
I0202 02:29:51.643413 139907754342144 logging_writer.py:48] [179500] global_step=179500, grad_norm=9.153059959411621, loss=0.7579786777496338
I0202 02:30:25.328549 139907745949440 logging_writer.py:48] [179600] global_step=179600, grad_norm=9.673506736755371, loss=0.7722338438034058
I0202 02:30:59.009605 139907754342144 logging_writer.py:48] [179700] global_step=179700, grad_norm=9.951258659362793, loss=0.7360787391662598
I0202 02:31:32.700021 139907745949440 logging_writer.py:48] [179800] global_step=179800, grad_norm=10.128620147705078, loss=0.7591407299041748
I0202 02:32:06.369977 139907754342144 logging_writer.py:48] [179900] global_step=179900, grad_norm=9.0907564163208, loss=0.7244225740432739
I0202 02:32:40.102296 139907745949440 logging_writer.py:48] [180000] global_step=180000, grad_norm=8.995548248291016, loss=0.6682438850402832
I0202 02:33:13.821717 139907754342144 logging_writer.py:48] [180100] global_step=180100, grad_norm=8.855891227722168, loss=0.698086678981781
I0202 02:33:45.284136 140070692116288 spec.py:321] Evaluating on the training split.
I0202 02:33:51.595095 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 02:33:59.968276 140070692116288 spec.py:349] Evaluating on the test split.
I0202 02:34:02.643751 140070692116288 submission_runner.py:408] Time since start: 62859.10s, 	Step: 180195, 	{'train/accuracy': 0.9312220811843872, 'train/loss': 0.24766357243061066, 'validation/accuracy': 0.7628799676895142, 'validation/loss': 0.9658904671669006, 'validation/num_examples': 50000, 'test/accuracy': 0.6408000588417053, 'test/loss': 1.6687637567520142, 'test/num_examples': 10000, 'score': 60729.14320731163, 'total_duration': 62859.096420288086, 'accumulated_submission_time': 60729.14320731163, 'accumulated_eval_time': 2116.0850701332092, 'accumulated_logging_time': 7.6090943813323975}
I0202 02:34:02.691743 139908710635264 logging_writer.py:48] [180195] accumulated_eval_time=2116.085070, accumulated_logging_time=7.609094, accumulated_submission_time=60729.143207, global_step=180195, preemption_count=0, score=60729.143207, test/accuracy=0.640800, test/loss=1.668764, test/num_examples=10000, total_duration=62859.096420, train/accuracy=0.931222, train/loss=0.247664, validation/accuracy=0.762880, validation/loss=0.965890, validation/num_examples=50000
I0202 02:34:04.715262 139908719027968 logging_writer.py:48] [180200] global_step=180200, grad_norm=8.863712310791016, loss=0.7328910231590271
I0202 02:34:38.306023 139908710635264 logging_writer.py:48] [180300] global_step=180300, grad_norm=8.76148509979248, loss=0.7761570811271667
I0202 02:35:11.896479 139908719027968 logging_writer.py:48] [180400] global_step=180400, grad_norm=9.672246932983398, loss=0.7718275189399719
I0202 02:35:45.521210 139908710635264 logging_writer.py:48] [180500] global_step=180500, grad_norm=8.438275337219238, loss=0.7703920006752014
I0202 02:36:19.212014 139908719027968 logging_writer.py:48] [180600] global_step=180600, grad_norm=9.522768020629883, loss=0.6547093391418457
I0202 02:36:52.871410 139908710635264 logging_writer.py:48] [180700] global_step=180700, grad_norm=9.7611665725708, loss=0.7430691719055176
I0202 02:37:26.571773 139908719027968 logging_writer.py:48] [180800] global_step=180800, grad_norm=9.303650856018066, loss=0.8025314807891846
I0202 02:38:00.211882 139908710635264 logging_writer.py:48] [180900] global_step=180900, grad_norm=10.000038146972656, loss=0.7894020676612854
I0202 02:38:33.905796 139908719027968 logging_writer.py:48] [181000] global_step=181000, grad_norm=8.792645454406738, loss=0.7857776284217834
I0202 02:39:07.670549 139908710635264 logging_writer.py:48] [181100] global_step=181100, grad_norm=8.667562484741211, loss=0.7777881622314453
I0202 02:39:41.391686 139908719027968 logging_writer.py:48] [181200] global_step=181200, grad_norm=9.178239822387695, loss=0.8030968904495239
I0202 02:40:15.057410 139908710635264 logging_writer.py:48] [181300] global_step=181300, grad_norm=8.821431159973145, loss=0.6724933385848999
I0202 02:40:48.742810 139908719027968 logging_writer.py:48] [181400] global_step=181400, grad_norm=9.093202590942383, loss=0.7621325850486755
I0202 02:41:22.400481 139908710635264 logging_writer.py:48] [181500] global_step=181500, grad_norm=8.290939331054688, loss=0.7481480240821838
I0202 02:41:56.086050 139908719027968 logging_writer.py:48] [181600] global_step=181600, grad_norm=8.922345161437988, loss=0.7710564136505127
I0202 02:42:29.745718 139908710635264 logging_writer.py:48] [181700] global_step=181700, grad_norm=8.992146492004395, loss=0.7541316151618958
I0202 02:42:32.927058 140070692116288 spec.py:321] Evaluating on the training split.
I0202 02:42:39.171275 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 02:42:47.599916 140070692116288 spec.py:349] Evaluating on the test split.
I0202 02:42:50.212471 140070692116288 submission_runner.py:408] Time since start: 63386.67s, 	Step: 181711, 	{'train/accuracy': 0.9324377775192261, 'train/loss': 0.24604295194149017, 'validation/accuracy': 0.7635599970817566, 'validation/loss': 0.963376522064209, 'validation/num_examples': 50000, 'test/accuracy': 0.6413000226020813, 'test/loss': 1.6692149639129639, 'test/num_examples': 10000, 'score': 61239.31663656235, 'total_duration': 63386.6651391983, 'accumulated_submission_time': 61239.31663656235, 'accumulated_eval_time': 2133.3704464435577, 'accumulated_logging_time': 7.666547775268555}
I0202 02:42:50.260607 139907754342144 logging_writer.py:48] [181711] accumulated_eval_time=2133.370446, accumulated_logging_time=7.666548, accumulated_submission_time=61239.316637, global_step=181711, preemption_count=0, score=61239.316637, test/accuracy=0.641300, test/loss=1.669215, test/num_examples=10000, total_duration=63386.665139, train/accuracy=0.932438, train/loss=0.246043, validation/accuracy=0.763560, validation/loss=0.963377, validation/num_examples=50000
I0202 02:43:20.550026 139907762734848 logging_writer.py:48] [181800] global_step=181800, grad_norm=9.61976146697998, loss=0.7662721872329712
I0202 02:43:54.229195 139907754342144 logging_writer.py:48] [181900] global_step=181900, grad_norm=8.233973503112793, loss=0.7406728267669678
I0202 02:44:27.911761 139907762734848 logging_writer.py:48] [182000] global_step=182000, grad_norm=8.538262367248535, loss=0.7521013021469116
I0202 02:45:01.580939 139907754342144 logging_writer.py:48] [182100] global_step=182100, grad_norm=9.258753776550293, loss=0.7250109910964966
I0202 02:45:35.305655 139907762734848 logging_writer.py:48] [182200] global_step=182200, grad_norm=9.378170013427734, loss=0.7607078552246094
I0202 02:46:09.008454 139907754342144 logging_writer.py:48] [182300] global_step=182300, grad_norm=8.660069465637207, loss=0.7794448733329773
I0202 02:46:42.698965 139907762734848 logging_writer.py:48] [182400] global_step=182400, grad_norm=8.968239784240723, loss=0.77444988489151
I0202 02:47:16.378662 139907754342144 logging_writer.py:48] [182500] global_step=182500, grad_norm=9.163713455200195, loss=0.7337281107902527
I0202 02:47:50.050921 139907762734848 logging_writer.py:48] [182600] global_step=182600, grad_norm=8.48416805267334, loss=0.6770491003990173
I0202 02:48:23.720916 139907754342144 logging_writer.py:48] [182700] global_step=182700, grad_norm=9.361971855163574, loss=0.7996265888214111
I0202 02:48:57.379770 139907762734848 logging_writer.py:48] [182800] global_step=182800, grad_norm=8.51469898223877, loss=0.6859619617462158
I0202 02:49:31.058756 139907754342144 logging_writer.py:48] [182900] global_step=182900, grad_norm=9.173876762390137, loss=0.7315225601196289
I0202 02:50:04.727539 139907762734848 logging_writer.py:48] [183000] global_step=183000, grad_norm=9.755932807922363, loss=0.7648699879646301
I0202 02:50:38.410490 139907754342144 logging_writer.py:48] [183100] global_step=183100, grad_norm=10.997653007507324, loss=0.7706875205039978
I0202 02:51:12.090954 139907762734848 logging_writer.py:48] [183200] global_step=183200, grad_norm=8.880193710327148, loss=0.7279663681983948
I0202 02:51:20.317220 140070692116288 spec.py:321] Evaluating on the training split.
I0202 02:51:26.587363 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 02:51:35.387730 140070692116288 spec.py:349] Evaluating on the test split.
I0202 02:51:38.119331 140070692116288 submission_runner.py:408] Time since start: 63914.57s, 	Step: 183226, 	{'train/accuracy': 0.9330556392669678, 'train/loss': 0.24356487393379211, 'validation/accuracy': 0.763700008392334, 'validation/loss': 0.9627121686935425, 'validation/num_examples': 50000, 'test/accuracy': 0.6414000391960144, 'test/loss': 1.668165683746338, 'test/num_examples': 10000, 'score': 61749.31013250351, 'total_duration': 63914.571994781494, 'accumulated_submission_time': 61749.31013250351, 'accumulated_eval_time': 2151.172516822815, 'accumulated_logging_time': 7.724971771240234}
I0202 02:51:38.174500 139907737556736 logging_writer.py:48] [183226] accumulated_eval_time=2151.172517, accumulated_logging_time=7.724972, accumulated_submission_time=61749.310133, global_step=183226, preemption_count=0, score=61749.310133, test/accuracy=0.641400, test/loss=1.668166, test/num_examples=10000, total_duration=63914.571995, train/accuracy=0.933056, train/loss=0.243565, validation/accuracy=0.763700, validation/loss=0.962712, validation/num_examples=50000
I0202 02:52:03.346797 139907745949440 logging_writer.py:48] [183300] global_step=183300, grad_norm=9.092880249023438, loss=0.7146033644676208
I0202 02:52:36.979272 139907737556736 logging_writer.py:48] [183400] global_step=183400, grad_norm=9.245564460754395, loss=0.6969231367111206
I0202 02:53:10.671476 139907745949440 logging_writer.py:48] [183500] global_step=183500, grad_norm=8.632017135620117, loss=0.6987537145614624
I0202 02:53:44.330775 139907737556736 logging_writer.py:48] [183600] global_step=183600, grad_norm=9.343677520751953, loss=0.7166475653648376
I0202 02:54:18.022687 139907745949440 logging_writer.py:48] [183700] global_step=183700, grad_norm=9.50264835357666, loss=0.7208459973335266
I0202 02:54:51.674428 139907737556736 logging_writer.py:48] [183800] global_step=183800, grad_norm=9.392647743225098, loss=0.7899062633514404
I0202 02:55:25.360940 139907745949440 logging_writer.py:48] [183900] global_step=183900, grad_norm=8.630329132080078, loss=0.6950402855873108
I0202 02:55:59.005028 139907737556736 logging_writer.py:48] [184000] global_step=184000, grad_norm=8.540486335754395, loss=0.7416160106658936
I0202 02:56:32.697891 139907745949440 logging_writer.py:48] [184100] global_step=184100, grad_norm=8.854293823242188, loss=0.7416185140609741
I0202 02:57:06.339311 139907737556736 logging_writer.py:48] [184200] global_step=184200, grad_norm=8.94709587097168, loss=0.7443515062332153
I0202 02:57:40.041846 139907745949440 logging_writer.py:48] [184300] global_step=184300, grad_norm=8.86485481262207, loss=0.728874921798706
I0202 02:58:13.858609 139907737556736 logging_writer.py:48] [184400] global_step=184400, grad_norm=9.944519996643066, loss=0.6795295476913452
I0202 02:58:47.565833 139907745949440 logging_writer.py:48] [184500] global_step=184500, grad_norm=10.040203094482422, loss=0.7372080087661743
I0202 02:59:21.233004 139907737556736 logging_writer.py:48] [184600] global_step=184600, grad_norm=8.120382308959961, loss=0.6871630549430847
I0202 02:59:54.923783 139907745949440 logging_writer.py:48] [184700] global_step=184700, grad_norm=8.890819549560547, loss=0.6656057238578796
I0202 03:00:08.188191 140070692116288 spec.py:321] Evaluating on the training split.
I0202 03:00:14.407450 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 03:00:22.877290 140070692116288 spec.py:349] Evaluating on the test split.
I0202 03:00:25.445269 140070692116288 submission_runner.py:408] Time since start: 64441.90s, 	Step: 184741, 	{'train/accuracy': 0.9329758882522583, 'train/loss': 0.24010393023490906, 'validation/accuracy': 0.7639399766921997, 'validation/loss': 0.962593674659729, 'validation/num_examples': 50000, 'test/accuracy': 0.6416000127792358, 'test/loss': 1.6681299209594727, 'test/num_examples': 10000, 'score': 62259.26232099533, 'total_duration': 64441.897938489914, 'accumulated_submission_time': 62259.26232099533, 'accumulated_eval_time': 2168.4295752048492, 'accumulated_logging_time': 7.789133071899414}
I0202 03:00:25.494181 139908710635264 logging_writer.py:48] [184741] accumulated_eval_time=2168.429575, accumulated_logging_time=7.789133, accumulated_submission_time=62259.262321, global_step=184741, preemption_count=0, score=62259.262321, test/accuracy=0.641600, test/loss=1.668130, test/num_examples=10000, total_duration=64441.897938, train/accuracy=0.932976, train/loss=0.240104, validation/accuracy=0.763940, validation/loss=0.962594, validation/num_examples=50000
I0202 03:00:45.701036 139908719027968 logging_writer.py:48] [184800] global_step=184800, grad_norm=8.680597305297852, loss=0.7080442309379578
I0202 03:01:19.364702 139908710635264 logging_writer.py:48] [184900] global_step=184900, grad_norm=8.56938362121582, loss=0.7932783961296082
I0202 03:01:53.339327 139908719027968 logging_writer.py:48] [185000] global_step=185000, grad_norm=9.805930137634277, loss=0.7006134390830994
I0202 03:02:26.982529 139908710635264 logging_writer.py:48] [185100] global_step=185100, grad_norm=9.0662841796875, loss=0.7234753370285034
I0202 03:03:00.655196 139908719027968 logging_writer.py:48] [185200] global_step=185200, grad_norm=8.13372802734375, loss=0.7069669961929321
I0202 03:03:34.351747 139908710635264 logging_writer.py:48] [185300] global_step=185300, grad_norm=8.838181495666504, loss=0.6419440507888794
I0202 03:04:08.021546 139908719027968 logging_writer.py:48] [185400] global_step=185400, grad_norm=9.52253532409668, loss=0.708591878414154
I0202 03:04:41.791916 139908710635264 logging_writer.py:48] [185500] global_step=185500, grad_norm=8.41829776763916, loss=0.7191296815872192
I0202 03:05:15.487598 139908719027968 logging_writer.py:48] [185600] global_step=185600, grad_norm=9.076842308044434, loss=0.7875981330871582
I0202 03:05:49.138845 139908710635264 logging_writer.py:48] [185700] global_step=185700, grad_norm=9.114384651184082, loss=0.7962474226951599
I0202 03:06:22.849059 139908719027968 logging_writer.py:48] [185800] global_step=185800, grad_norm=8.81877613067627, loss=0.7194654941558838
I0202 03:06:56.529671 139908710635264 logging_writer.py:48] [185900] global_step=185900, grad_norm=9.510980606079102, loss=0.774161696434021
I0202 03:07:30.217012 139908719027968 logging_writer.py:48] [186000] global_step=186000, grad_norm=9.288877487182617, loss=0.7277500033378601
I0202 03:08:03.883556 139908710635264 logging_writer.py:48] [186100] global_step=186100, grad_norm=10.013639450073242, loss=0.746761679649353
I0202 03:08:37.560273 139908719027968 logging_writer.py:48] [186200] global_step=186200, grad_norm=9.197070121765137, loss=0.7750126123428345
I0202 03:08:55.561906 140070692116288 spec.py:321] Evaluating on the training split.
I0202 03:09:01.922846 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 03:09:10.629537 140070692116288 spec.py:349] Evaluating on the test split.
I0202 03:09:13.277521 140070692116288 submission_runner.py:408] Time since start: 64969.73s, 	Step: 186255, 	{'train/accuracy': 0.9334343075752258, 'train/loss': 0.2402811050415039, 'validation/accuracy': 0.7638399600982666, 'validation/loss': 0.9630224704742432, 'validation/num_examples': 50000, 'test/accuracy': 0.6425000429153442, 'test/loss': 1.6677354574203491, 'test/num_examples': 10000, 'score': 62769.26744389534, 'total_duration': 64969.73010158539, 'accumulated_submission_time': 62769.26744389534, 'accumulated_eval_time': 2186.145072221756, 'accumulated_logging_time': 7.848384857177734}
I0202 03:09:13.327012 139907745949440 logging_writer.py:48] [186255] accumulated_eval_time=2186.145072, accumulated_logging_time=7.848385, accumulated_submission_time=62769.267444, global_step=186255, preemption_count=0, score=62769.267444, test/accuracy=0.642500, test/loss=1.667735, test/num_examples=10000, total_duration=64969.730102, train/accuracy=0.933434, train/loss=0.240281, validation/accuracy=0.763840, validation/loss=0.963022, validation/num_examples=50000
I0202 03:09:28.824120 139907754342144 logging_writer.py:48] [186300] global_step=186300, grad_norm=8.823419570922852, loss=0.694055438041687
I0202 03:10:02.485124 139907745949440 logging_writer.py:48] [186400] global_step=186400, grad_norm=9.020872116088867, loss=0.6992201209068298
I0202 03:10:36.151998 139907754342144 logging_writer.py:48] [186500] global_step=186500, grad_norm=9.118972778320312, loss=0.6683260798454285
I0202 03:11:09.880739 139907745949440 logging_writer.py:48] [186600] global_step=186600, grad_norm=8.805377006530762, loss=0.7692965865135193
I0202 03:11:31.569225 140070692116288 spec.py:321] Evaluating on the training split.
I0202 03:11:37.816976 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 03:11:46.283074 140070692116288 spec.py:349] Evaluating on the test split.
I0202 03:11:48.933550 140070692116288 submission_runner.py:408] Time since start: 65125.39s, 	Step: 186666, 	{'train/accuracy': 0.9338129758834839, 'train/loss': 0.23659060895442963, 'validation/accuracy': 0.7639399766921997, 'validation/loss': 0.9623273015022278, 'validation/num_examples': 50000, 'test/accuracy': 0.6413000226020813, 'test/loss': 1.6673704385757446, 'test/num_examples': 10000, 'score': 62907.485946178436, 'total_duration': 65125.386219739914, 'accumulated_submission_time': 62907.485946178436, 'accumulated_eval_time': 2203.5093677043915, 'accumulated_logging_time': 7.9071362018585205}
I0202 03:11:48.982474 139907737556736 logging_writer.py:48] [186666] accumulated_eval_time=2203.509368, accumulated_logging_time=7.907136, accumulated_submission_time=62907.485946, global_step=186666, preemption_count=0, score=62907.485946, test/accuracy=0.641300, test/loss=1.667370, test/num_examples=10000, total_duration=65125.386220, train/accuracy=0.933813, train/loss=0.236591, validation/accuracy=0.763940, validation/loss=0.962327, validation/num_examples=50000
I0202 03:11:49.030908 139908710635264 logging_writer.py:48] [186666] global_step=186666, preemption_count=0, score=62907.485946
I0202 03:11:49.336835 140070692116288 checkpoints.py:490] Saving checkpoint at step: 186666
I0202 03:11:50.518326 140070692116288 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_3/imagenet_resnet_jax/trial_3/checkpoint_186666
I0202 03:11:50.539488 140070692116288 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_3/imagenet_resnet_jax/trial_3/checkpoint_186666.
I0202 03:11:51.343756 140070692116288 submission_runner.py:583] Tuning trial 3/5
I0202 03:11:51.343970 140070692116288 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0202 03:11:51.352109 140070692116288 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0011360011994838715, 'train/loss': 6.912071704864502, 'validation/accuracy': 0.0011199999134987593, 'validation/loss': 6.912059783935547, 'validation/num_examples': 50000, 'test/accuracy': 0.0009000000427477062, 'test/loss': 6.912177562713623, 'test/num_examples': 10000, 'score': 34.98110485076904, 'total_duration': 52.88213634490967, 'accumulated_submission_time': 34.98110485076904, 'accumulated_eval_time': 17.900901556015015, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1508, {'train/accuracy': 0.0686383917927742, 'train/loss': 5.3621506690979, 'validation/accuracy': 0.0652799978852272, 'validation/loss': 5.418907165527344, 'validation/num_examples': 50000, 'test/accuracy': 0.046000003814697266, 'test/loss': 5.649470329284668, 'test/num_examples': 10000, 'score': 545.0515990257263, 'total_duration': 580.9545924663544, 'accumulated_submission_time': 545.0515990257263, 'accumulated_eval_time': 35.82946801185608, 'accumulated_logging_time': 0.02124810218811035, 'global_step': 1508, 'preemption_count': 0}), (3015, {'train/accuracy': 0.16733099520206451, 'train/loss': 4.278256893157959, 'validation/accuracy': 0.15369999408721924, 'validation/loss': 4.376176357269287, 'validation/num_examples': 50000, 'test/accuracy': 0.11080000549554825, 'test/loss': 4.82637882232666, 'test/num_examples': 10000, 'score': 1055.0695431232452, 'total_duration': 1108.7527313232422, 'accumulated_submission_time': 1055.0695431232452, 'accumulated_eval_time': 53.52310228347778, 'accumulated_logging_time': 0.056015729904174805, 'global_step': 3015, 'preemption_count': 0}), (4521, {'train/accuracy': 0.2669403553009033, 'train/loss': 3.5287725925445557, 'validation/accuracy': 0.24573999643325806, 'validation/loss': 3.66829776763916, 'validation/num_examples': 50000, 'test/accuracy': 0.17920000851154327, 'test/loss': 4.221713066101074, 'test/num_examples': 10000, 'score': 1565.034935951233, 'total_duration': 1636.450201511383, 'accumulated_submission_time': 1565.034935951233, 'accumulated_eval_time': 71.17259407043457, 'accumulated_logging_time': 0.0849447250366211, 'global_step': 4521, 'preemption_count': 0}), (6028, {'train/accuracy': 0.36387914419174194, 'train/loss': 2.9160544872283936, 'validation/accuracy': 0.33461999893188477, 'validation/loss': 3.083531141281128, 'validation/num_examples': 50000, 'test/accuracy': 0.249300017952919, 'test/loss': 3.7188665866851807, 'test/num_examples': 10000, 'score': 2075.2632479667664, 'total_duration': 2165.413183450699, 'accumulated_submission_time': 2075.2632479667664, 'accumulated_eval_time': 89.82699704170227, 'accumulated_logging_time': 0.11361503601074219, 'global_step': 6028, 'preemption_count': 0}), (7535, {'train/accuracy': 0.4528658986091614, 'train/loss': 2.3958868980407715, 'validation/accuracy': 0.40101999044418335, 'validation/loss': 2.6911749839782715, 'validation/num_examples': 50000, 'test/accuracy': 0.30090001225471497, 'test/loss': 3.3908586502075195, 'test/num_examples': 10000, 'score': 2585.3535408973694, 'total_duration': 2693.6622858047485, 'accumulated_submission_time': 2585.3535408973694, 'accumulated_eval_time': 107.90042018890381, 'accumulated_logging_time': 0.14592361450195312, 'global_step': 7535, 'preemption_count': 0}), (9043, {'train/accuracy': 0.4929049611091614, 'train/loss': 2.1659739017486572, 'validation/accuracy': 0.438539981842041, 'validation/loss': 2.4691154956817627, 'validation/num_examples': 50000, 'test/accuracy': 0.33480000495910645, 'test/loss': 3.1936228275299072, 'test/num_examples': 10000, 'score': 3095.388578891754, 'total_duration': 3221.8932802677155, 'accumulated_submission_time': 3095.388578891754, 'accumulated_eval_time': 126.01309251785278, 'accumulated_logging_time': 0.1767423152923584, 'global_step': 9043, 'preemption_count': 0}), (10551, {'train/accuracy': 0.5285794138908386, 'train/loss': 1.9861119985580444, 'validation/accuracy': 0.49087998270988464, 'validation/loss': 2.2068440914154053, 'validation/num_examples': 50000, 'test/accuracy': 0.3759000301361084, 'test/loss': 2.9420087337493896, 'test/num_examples': 10000, 'score': 3605.421551465988, 'total_duration': 3749.529512166977, 'accumulated_submission_time': 3605.421551465988, 'accumulated_eval_time': 143.53465509414673, 'accumulated_logging_time': 0.2061150074005127, 'global_step': 10551, 'preemption_count': 0}), (12061, {'train/accuracy': 0.549226701259613, 'train/loss': 1.8957043886184692, 'validation/accuracy': 0.5112400054931641, 'validation/loss': 2.1058874130249023, 'validation/num_examples': 50000, 'test/accuracy': 0.3946000039577484, 'test/loss': 2.823363780975342, 'test/num_examples': 10000, 'score': 4115.560308218002, 'total_duration': 4277.2989773750305, 'accumulated_submission_time': 4115.560308218002, 'accumulated_eval_time': 161.0859730243683, 'accumulated_logging_time': 0.23340106010437012, 'global_step': 12061, 'preemption_count': 0}), (13571, {'train/accuracy': 0.5690967440605164, 'train/loss': 1.7911511659622192, 'validation/accuracy': 0.5332599878311157, 'validation/loss': 1.9902153015136719, 'validation/num_examples': 50000, 'test/accuracy': 0.41440001130104065, 'test/loss': 2.7417922019958496, 'test/num_examples': 10000, 'score': 4625.673633098602, 'total_duration': 4805.292775630951, 'accumulated_submission_time': 4625.673633098602, 'accumulated_eval_time': 178.88615822792053, 'accumulated_logging_time': 0.26222872734069824, 'global_step': 13571, 'preemption_count': 0}), (15082, {'train/accuracy': 0.5841637253761292, 'train/loss': 1.741123914718628, 'validation/accuracy': 0.5440999865531921, 'validation/loss': 1.943701148033142, 'validation/num_examples': 50000, 'test/accuracy': 0.4231000244617462, 'test/loss': 2.674654245376587, 'test/num_examples': 10000, 'score': 5135.629874706268, 'total_duration': 5333.236327886581, 'accumulated_submission_time': 5135.629874706268, 'accumulated_eval_time': 196.78649830818176, 'accumulated_logging_time': 0.29550814628601074, 'global_step': 15082, 'preemption_count': 0}), (16593, {'train/accuracy': 0.6349848508834839, 'train/loss': 1.4669139385223389, 'validation/accuracy': 0.557200014591217, 'validation/loss': 1.8688576221466064, 'validation/num_examples': 50000, 'test/accuracy': 0.4305000305175781, 'test/loss': 2.623843193054199, 'test/num_examples': 10000, 'score': 5645.554906845093, 'total_duration': 5860.77290892601, 'accumulated_submission_time': 5645.554906845093, 'accumulated_eval_time': 214.31005549430847, 'accumulated_logging_time': 0.3311014175415039, 'global_step': 16593, 'preemption_count': 0}), (18105, {'train/accuracy': 0.6109095811843872, 'train/loss': 1.5742591619491577, 'validation/accuracy': 0.5535399913787842, 'validation/loss': 1.891344428062439, 'validation/num_examples': 50000, 'test/accuracy': 0.4300000071525574, 'test/loss': 2.6250672340393066, 'test/num_examples': 10000, 'score': 6155.727502822876, 'total_duration': 6388.630298376083, 'accumulated_submission_time': 6155.727502822876, 'accumulated_eval_time': 231.91193890571594, 'accumulated_logging_time': 0.36240530014038086, 'global_step': 18105, 'preemption_count': 0}), (19617, {'train/accuracy': 0.6103914380073547, 'train/loss': 1.5961946249008179, 'validation/accuracy': 0.5658800005912781, 'validation/loss': 1.8382683992385864, 'validation/num_examples': 50000, 'test/accuracy': 0.43970000743865967, 'test/loss': 2.6097118854522705, 'test/num_examples': 10000, 'score': 6665.842526435852, 'total_duration': 6916.42732667923, 'accumulated_submission_time': 6665.842526435852, 'accumulated_eval_time': 249.50973081588745, 'accumulated_logging_time': 0.394974946975708, 'global_step': 19617, 'preemption_count': 0}), (21130, {'train/accuracy': 0.6112683415412903, 'train/loss': 1.5985033512115479, 'validation/accuracy': 0.5659999847412109, 'validation/loss': 1.8424259424209595, 'validation/num_examples': 50000, 'test/accuracy': 0.4401000142097473, 'test/loss': 2.5855188369750977, 'test/num_examples': 10000, 'score': 7175.8285439014435, 'total_duration': 7444.008017539978, 'accumulated_submission_time': 7175.8285439014435, 'accumulated_eval_time': 267.0219874382019, 'accumulated_logging_time': 0.42498087882995605, 'global_step': 21130, 'preemption_count': 0}), (22643, {'train/accuracy': 0.6199776530265808, 'train/loss': 1.546796441078186, 'validation/accuracy': 0.5753399729728699, 'validation/loss': 1.773450255393982, 'validation/num_examples': 50000, 'test/accuracy': 0.45100003480911255, 'test/loss': 2.5334506034851074, 'test/num_examples': 10000, 'score': 7685.893748044968, 'total_duration': 7972.019269227982, 'accumulated_submission_time': 7685.893748044968, 'accumulated_eval_time': 284.88498640060425, 'accumulated_logging_time': 0.45513081550598145, 'global_step': 22643, 'preemption_count': 0}), (24156, {'train/accuracy': 0.6190210580825806, 'train/loss': 1.5578505992889404, 'validation/accuracy': 0.5760399699211121, 'validation/loss': 1.7675529718399048, 'validation/num_examples': 50000, 'test/accuracy': 0.45330002903938293, 'test/loss': 2.5400712490081787, 'test/num_examples': 10000, 'score': 8196.035193443298, 'total_duration': 8499.933848619461, 'accumulated_submission_time': 8196.035193443298, 'accumulated_eval_time': 302.57521986961365, 'accumulated_logging_time': 0.48557424545288086, 'global_step': 24156, 'preemption_count': 0}), (25669, {'train/accuracy': 0.6695631146430969, 'train/loss': 1.318595290184021, 'validation/accuracy': 0.5854399800300598, 'validation/loss': 1.7338712215423584, 'validation/num_examples': 50000, 'test/accuracy': 0.4646000266075134, 'test/loss': 2.441926956176758, 'test/num_examples': 10000, 'score': 8706.256494283676, 'total_duration': 9027.873177528381, 'accumulated_submission_time': 8706.256494283676, 'accumulated_eval_time': 320.2084016799927, 'accumulated_logging_time': 0.51666259765625, 'global_step': 25669, 'preemption_count': 0}), (27182, {'train/accuracy': 0.6529615521430969, 'train/loss': 1.4048078060150146, 'validation/accuracy': 0.5888800024986267, 'validation/loss': 1.7101259231567383, 'validation/num_examples': 50000, 'test/accuracy': 0.46560001373291016, 'test/loss': 2.442213535308838, 'test/num_examples': 10000, 'score': 9216.314534425735, 'total_duration': 9555.776126384735, 'accumulated_submission_time': 9216.314534425735, 'accumulated_eval_time': 337.96847558021545, 'accumulated_logging_time': 0.549079179763794, 'global_step': 27182, 'preemption_count': 0}), (28695, {'train/accuracy': 0.6298230290412903, 'train/loss': 1.5027976036071777, 'validation/accuracy': 0.5792199969291687, 'validation/loss': 1.772684931755066, 'validation/num_examples': 50000, 'test/accuracy': 0.4537000358104706, 'test/loss': 2.5030179023742676, 'test/num_examples': 10000, 'score': 9726.26763677597, 'total_duration': 10083.292204618454, 'accumulated_submission_time': 9726.26763677597, 'accumulated_eval_time': 355.44871044158936, 'accumulated_logging_time': 0.5797502994537354, 'global_step': 28695, 'preemption_count': 0}), (30209, {'train/accuracy': 0.6353435516357422, 'train/loss': 1.4675039052963257, 'validation/accuracy': 0.5884400010108948, 'validation/loss': 1.714426040649414, 'validation/num_examples': 50000, 'test/accuracy': 0.46700000762939453, 'test/loss': 2.455329656600952, 'test/num_examples': 10000, 'score': 10236.290426254272, 'total_duration': 10611.109208583832, 'accumulated_submission_time': 10236.290426254272, 'accumulated_eval_time': 373.15779161453247, 'accumulated_logging_time': 0.6124565601348877, 'global_step': 30209, 'preemption_count': 0}), (31723, {'train/accuracy': 0.6358418464660645, 'train/loss': 1.4786940813064575, 'validation/accuracy': 0.5914999842643738, 'validation/loss': 1.7083582878112793, 'validation/num_examples': 50000, 'test/accuracy': 0.46640002727508545, 'test/loss': 2.412381172180176, 'test/num_examples': 10000, 'score': 10746.51861667633, 'total_duration': 11138.977312088013, 'accumulated_submission_time': 10746.51861667633, 'accumulated_eval_time': 390.7127459049225, 'accumulated_logging_time': 0.6449933052062988, 'global_step': 31723, 'preemption_count': 0}), (33237, {'train/accuracy': 0.639668345451355, 'train/loss': 1.453246831893921, 'validation/accuracy': 0.5940999984741211, 'validation/loss': 1.6839145421981812, 'validation/num_examples': 50000, 'test/accuracy': 0.4757000207901001, 'test/loss': 2.400609254837036, 'test/num_examples': 10000, 'score': 11256.555840015411, 'total_duration': 11666.838463544846, 'accumulated_submission_time': 11256.555840015411, 'accumulated_eval_time': 408.448246717453, 'accumulated_logging_time': 0.680262565612793, 'global_step': 33237, 'preemption_count': 0}), (34751, {'train/accuracy': 0.6583824753761292, 'train/loss': 1.3591587543487549, 'validation/accuracy': 0.5783599615097046, 'validation/loss': 1.7592555284500122, 'validation/num_examples': 50000, 'test/accuracy': 0.4595000147819519, 'test/loss': 2.4588377475738525, 'test/num_examples': 10000, 'score': 11766.602644443512, 'total_duration': 12194.563556909561, 'accumulated_submission_time': 11766.602644443512, 'accumulated_eval_time': 426.0376763343811, 'accumulated_logging_time': 0.717193603515625, 'global_step': 34751, 'preemption_count': 0}), (36262, {'train/accuracy': 0.6530413031578064, 'train/loss': 1.3820711374282837, 'validation/accuracy': 0.5939399600028992, 'validation/loss': 1.7025057077407837, 'validation/num_examples': 50000, 'test/accuracy': 0.468500018119812, 'test/loss': 2.4308671951293945, 'test/num_examples': 10000, 'score': 12275.55195569992, 'total_duration': 12722.270855426788, 'accumulated_submission_time': 12275.55195569992, 'accumulated_eval_time': 443.6337375640869, 'accumulated_logging_time': 1.826355218887329, 'global_step': 36262, 'preemption_count': 0}), (37777, {'train/accuracy': 0.6506098508834839, 'train/loss': 1.3960210084915161, 'validation/accuracy': 0.5997999906539917, 'validation/loss': 1.6665048599243164, 'validation/num_examples': 50000, 'test/accuracy': 0.4780000150203705, 'test/loss': 2.3967301845550537, 'test/num_examples': 10000, 'score': 12785.783144235611, 'total_duration': 13250.32419705391, 'accumulated_submission_time': 12785.783144235611, 'accumulated_eval_time': 461.3704402446747, 'accumulated_logging_time': 1.8594939708709717, 'global_step': 37777, 'preemption_count': 0}), (39291, {'train/accuracy': 0.6446109414100647, 'train/loss': 1.427964210510254, 'validation/accuracy': 0.5927000045776367, 'validation/loss': 1.6880282163619995, 'validation/num_examples': 50000, 'test/accuracy': 0.46950003504753113, 'test/loss': 2.4148316383361816, 'test/num_examples': 10000, 'score': 13295.923071146011, 'total_duration': 13778.023537874222, 'accumulated_submission_time': 13295.923071146011, 'accumulated_eval_time': 478.8435335159302, 'accumulated_logging_time': 1.8939027786254883, 'global_step': 39291, 'preemption_count': 0}), (40806, {'train/accuracy': 0.6479392647743225, 'train/loss': 1.4033961296081543, 'validation/accuracy': 0.6071599721908569, 'validation/loss': 1.6209561824798584, 'validation/num_examples': 50000, 'test/accuracy': 0.48750001192092896, 'test/loss': 2.322809934616089, 'test/num_examples': 10000, 'score': 13806.05341053009, 'total_duration': 14306.078384160995, 'accumulated_submission_time': 13806.05341053009, 'accumulated_eval_time': 496.6829800605774, 'accumulated_logging_time': 1.9265995025634766, 'global_step': 40806, 'preemption_count': 0}), (42321, {'train/accuracy': 0.6422193646430969, 'train/loss': 1.4444411993026733, 'validation/accuracy': 0.5982199907302856, 'validation/loss': 1.6778781414031982, 'validation/num_examples': 50000, 'test/accuracy': 0.4732000231742859, 'test/loss': 2.43147611618042, 'test/num_examples': 10000, 'score': 14316.080854415894, 'total_duration': 14833.64709019661, 'accumulated_submission_time': 14316.080854415894, 'accumulated_eval_time': 514.1380536556244, 'accumulated_logging_time': 1.9594926834106445, 'global_step': 42321, 'preemption_count': 0}), (43835, {'train/accuracy': 0.6764788031578064, 'train/loss': 1.2786322832107544, 'validation/accuracy': 0.5990399718284607, 'validation/loss': 1.6587549448013306, 'validation/num_examples': 50000, 'test/accuracy': 0.47610002756118774, 'test/loss': 2.4039862155914307, 'test/num_examples': 10000, 'score': 14826.026376008987, 'total_duration': 15361.308577775955, 'accumulated_submission_time': 14826.026376008987, 'accumulated_eval_time': 531.7674918174744, 'accumulated_logging_time': 1.9935684204101562, 'global_step': 43835, 'preemption_count': 0}), (45350, {'train/accuracy': 0.6533800959587097, 'train/loss': 1.3688466548919678, 'validation/accuracy': 0.602180004119873, 'validation/loss': 1.6588586568832397, 'validation/num_examples': 50000, 'test/accuracy': 0.47930002212524414, 'test/loss': 2.3855392932891846, 'test/num_examples': 10000, 'score': 15336.135235071182, 'total_duration': 15889.651034116745, 'accumulated_submission_time': 15336.135235071182, 'accumulated_eval_time': 549.9132053852081, 'accumulated_logging_time': 2.0285537242889404, 'global_step': 45350, 'preemption_count': 0}), (46865, {'train/accuracy': 0.6542569994926453, 'train/loss': 1.370741367340088, 'validation/accuracy': 0.6079999804496765, 'validation/loss': 1.6286641359329224, 'validation/num_examples': 50000, 'test/accuracy': 0.48090001940727234, 'test/loss': 2.3713834285736084, 'test/num_examples': 10000, 'score': 15846.135519742966, 'total_duration': 16417.224779605865, 'accumulated_submission_time': 15846.135519742966, 'accumulated_eval_time': 567.3988988399506, 'accumulated_logging_time': 2.063908100128174, 'global_step': 46865, 'preemption_count': 0}), (48379, {'train/accuracy': 0.6592593789100647, 'train/loss': 1.3680092096328735, 'validation/accuracy': 0.6098399758338928, 'validation/loss': 1.604357361793518, 'validation/num_examples': 50000, 'test/accuracy': 0.4847000241279602, 'test/loss': 2.338550329208374, 'test/num_examples': 10000, 'score': 16356.188809633255, 'total_duration': 16944.88347172737, 'accumulated_submission_time': 16356.188809633255, 'accumulated_eval_time': 584.9136664867401, 'accumulated_logging_time': 2.1026611328125, 'global_step': 48379, 'preemption_count': 0}), (49894, {'train/accuracy': 0.6473612785339355, 'train/loss': 1.4344884157180786, 'validation/accuracy': 0.5996400117874146, 'validation/loss': 1.6758544445037842, 'validation/num_examples': 50000, 'test/accuracy': 0.4773000180721283, 'test/loss': 2.3852531909942627, 'test/num_examples': 10000, 'score': 16866.31578350067, 'total_duration': 17472.523061990738, 'accumulated_submission_time': 16866.31578350067, 'accumulated_eval_time': 602.3362815380096, 'accumulated_logging_time': 2.1408157348632812, 'global_step': 49894, 'preemption_count': 0}), (51408, {'train/accuracy': 0.6523237824440002, 'train/loss': 1.3949483633041382, 'validation/accuracy': 0.6103799939155579, 'validation/loss': 1.6124364137649536, 'validation/num_examples': 50000, 'test/accuracy': 0.48900002241134644, 'test/loss': 2.2980523109436035, 'test/num_examples': 10000, 'score': 17376.387938022614, 'total_duration': 18000.41622543335, 'accumulated_submission_time': 17376.387938022614, 'accumulated_eval_time': 620.0699634552002, 'accumulated_logging_time': 2.1757097244262695, 'global_step': 51408, 'preemption_count': 0}), (52923, {'train/accuracy': 0.6839325428009033, 'train/loss': 1.2379963397979736, 'validation/accuracy': 0.6139999628067017, 'validation/loss': 1.5940256118774414, 'validation/num_examples': 50000, 'test/accuracy': 0.49060001969337463, 'test/loss': 2.3267288208007812, 'test/num_examples': 10000, 'score': 17886.385818719864, 'total_duration': 18528.079265117645, 'accumulated_submission_time': 17886.385818719864, 'accumulated_eval_time': 637.6424875259399, 'accumulated_logging_time': 2.215566873550415, 'global_step': 52923, 'preemption_count': 0}), (54438, {'train/accuracy': 0.6634446382522583, 'train/loss': 1.333989143371582, 'validation/accuracy': 0.608020007610321, 'validation/loss': 1.6208534240722656, 'validation/num_examples': 50000, 'test/accuracy': 0.48430001735687256, 'test/loss': 2.368727922439575, 'test/num_examples': 10000, 'score': 18396.55052471161, 'total_duration': 19055.956347703934, 'accumulated_submission_time': 18396.55052471161, 'accumulated_eval_time': 655.2638325691223, 'accumulated_logging_time': 2.253763198852539, 'global_step': 54438, 'preemption_count': 0}), (55953, {'train/accuracy': 0.6671316623687744, 'train/loss': 1.318600058555603, 'validation/accuracy': 0.6162999868392944, 'validation/loss': 1.5899615287780762, 'validation/num_examples': 50000, 'test/accuracy': 0.49410003423690796, 'test/loss': 2.28857421875, 'test/num_examples': 10000, 'score': 18906.632444381714, 'total_duration': 19583.63987517357, 'accumulated_submission_time': 18906.632444381714, 'accumulated_eval_time': 672.7741053104401, 'accumulated_logging_time': 2.2918620109558105, 'global_step': 55953, 'preemption_count': 0}), (57468, {'train/accuracy': 0.6590401530265808, 'train/loss': 1.3535881042480469, 'validation/accuracy': 0.6110000014305115, 'validation/loss': 1.6046415567398071, 'validation/num_examples': 50000, 'test/accuracy': 0.48990002274513245, 'test/loss': 2.360703468322754, 'test/num_examples': 10000, 'score': 19416.74183535576, 'total_duration': 20111.414390802383, 'accumulated_submission_time': 19416.74183535576, 'accumulated_eval_time': 690.3503816127777, 'accumulated_logging_time': 2.3279449939727783, 'global_step': 57468, 'preemption_count': 0}), (58983, {'train/accuracy': 0.6589404940605164, 'train/loss': 1.3625762462615967, 'validation/accuracy': 0.6136400103569031, 'validation/loss': 1.6028468608856201, 'validation/num_examples': 50000, 'test/accuracy': 0.49250003695487976, 'test/loss': 2.331923246383667, 'test/num_examples': 10000, 'score': 19926.887871026993, 'total_duration': 20639.101494312286, 'accumulated_submission_time': 19926.887871026993, 'accumulated_eval_time': 707.7968149185181, 'accumulated_logging_time': 2.370288133621216, 'global_step': 58983, 'preemption_count': 0}), (60498, {'train/accuracy': 0.6735491156578064, 'train/loss': 1.2887380123138428, 'validation/accuracy': 0.620639979839325, 'validation/loss': 1.5495282411575317, 'validation/num_examples': 50000, 'test/accuracy': 0.4991000294685364, 'test/loss': 2.270559787750244, 'test/num_examples': 10000, 'score': 20436.946853160858, 'total_duration': 21167.427884340286, 'accumulated_submission_time': 20436.946853160858, 'accumulated_eval_time': 725.9643821716309, 'accumulated_logging_time': 2.4173500537872314, 'global_step': 60498, 'preemption_count': 0}), (62014, {'train/accuracy': 0.6741669178009033, 'train/loss': 1.2919275760650635, 'validation/accuracy': 0.6080399751663208, 'validation/loss': 1.623947024345398, 'validation/num_examples': 50000, 'test/accuracy': 0.4821000099182129, 'test/loss': 2.346834421157837, 'test/num_examples': 10000, 'score': 20947.052046775818, 'total_duration': 21695.2219684124, 'accumulated_submission_time': 20947.052046775818, 'accumulated_eval_time': 743.5703382492065, 'accumulated_logging_time': 2.4481093883514404, 'global_step': 62014, 'preemption_count': 0}), (63529, {'train/accuracy': 0.6745057106018066, 'train/loss': 1.275795817375183, 'validation/accuracy': 0.6182000041007996, 'validation/loss': 1.5899293422698975, 'validation/num_examples': 50000, 'test/accuracy': 0.49710002541542053, 'test/loss': 2.292905330657959, 'test/num_examples': 10000, 'score': 21457.020793676376, 'total_duration': 22222.684602499008, 'accumulated_submission_time': 21457.020793676376, 'accumulated_eval_time': 760.9726111888885, 'accumulated_logging_time': 2.4869983196258545, 'global_step': 63529, 'preemption_count': 0}), (65044, {'train/accuracy': 0.6870814561843872, 'train/loss': 1.2384870052337646, 'validation/accuracy': 0.6372599601745605, 'validation/loss': 1.5065011978149414, 'validation/num_examples': 50000, 'test/accuracy': 0.5083000063896179, 'test/loss': 2.2321155071258545, 'test/num_examples': 10000, 'score': 21966.95255088806, 'total_duration': 22750.308904886246, 'accumulated_submission_time': 21966.95255088806, 'accumulated_eval_time': 778.5667836666107, 'accumulated_logging_time': 2.531611204147339, 'global_step': 65044, 'preemption_count': 0}), (66558, {'train/accuracy': 0.6863042116165161, 'train/loss': 1.2401232719421387, 'validation/accuracy': 0.6317600011825562, 'validation/loss': 1.520139217376709, 'validation/num_examples': 50000, 'test/accuracy': 0.5033000111579895, 'test/loss': 2.2496683597564697, 'test/num_examples': 10000, 'score': 22476.95581459999, 'total_duration': 23277.84796833992, 'accumulated_submission_time': 22476.95581459999, 'accumulated_eval_time': 796.0130662918091, 'accumulated_logging_time': 2.5694398880004883, 'global_step': 66558, 'preemption_count': 0}), (68073, {'train/accuracy': 0.675203263759613, 'train/loss': 1.2782618999481201, 'validation/accuracy': 0.6322000026702881, 'validation/loss': 1.5258268117904663, 'validation/num_examples': 50000, 'test/accuracy': 0.5074000358581543, 'test/loss': 2.2572550773620605, 'test/num_examples': 10000, 'score': 22986.95196557045, 'total_duration': 23805.829738855362, 'accumulated_submission_time': 22986.95196557045, 'accumulated_eval_time': 813.9019508361816, 'accumulated_logging_time': 2.613524913787842, 'global_step': 68073, 'preemption_count': 0}), (69588, {'train/accuracy': 0.6990393400192261, 'train/loss': 1.1675734519958496, 'validation/accuracy': 0.6238600015640259, 'validation/loss': 1.5474979877471924, 'validation/num_examples': 50000, 'test/accuracy': 0.4936000108718872, 'test/loss': 2.2800233364105225, 'test/num_examples': 10000, 'score': 23496.924777507782, 'total_duration': 24333.650037050247, 'accumulated_submission_time': 23496.924777507782, 'accumulated_eval_time': 831.6630780696869, 'accumulated_logging_time': 2.6471753120422363, 'global_step': 69588, 'preemption_count': 0}), (71102, {'train/accuracy': 0.70023512840271, 'train/loss': 1.153869867324829, 'validation/accuracy': 0.632420003414154, 'validation/loss': 1.5144561529159546, 'validation/num_examples': 50000, 'test/accuracy': 0.5120000243186951, 'test/loss': 2.2258431911468506, 'test/num_examples': 10000, 'score': 24006.85396337509, 'total_duration': 24861.34249138832, 'accumulated_submission_time': 24006.85396337509, 'accumulated_eval_time': 849.3363344669342, 'accumulated_logging_time': 2.685119152069092, 'global_step': 71102, 'preemption_count': 0}), (72617, {'train/accuracy': 0.6888153553009033, 'train/loss': 1.204163670539856, 'validation/accuracy': 0.6315199732780457, 'validation/loss': 1.5182421207427979, 'validation/num_examples': 50000, 'test/accuracy': 0.5139000415802002, 'test/loss': 2.2352921962738037, 'test/num_examples': 10000, 'score': 24516.805390119553, 'total_duration': 25388.890427350998, 'accumulated_submission_time': 24516.805390119553, 'accumulated_eval_time': 866.8355195522308, 'accumulated_logging_time': 2.7290549278259277, 'global_step': 72617, 'preemption_count': 0}), (74132, {'train/accuracy': 0.6875796914100647, 'train/loss': 1.2316724061965942, 'validation/accuracy': 0.6329799890518188, 'validation/loss': 1.49374258518219, 'validation/num_examples': 50000, 'test/accuracy': 0.5053000450134277, 'test/loss': 2.2189886569976807, 'test/num_examples': 10000, 'score': 25026.766946792603, 'total_duration': 25916.44958114624, 'accumulated_submission_time': 25026.766946792603, 'accumulated_eval_time': 884.3387162685394, 'accumulated_logging_time': 2.771080732345581, 'global_step': 74132, 'preemption_count': 0}), (75647, {'train/accuracy': 0.6935586333274841, 'train/loss': 1.2051820755004883, 'validation/accuracy': 0.6389399766921997, 'validation/loss': 1.4845311641693115, 'validation/num_examples': 50000, 'test/accuracy': 0.5136000514030457, 'test/loss': 2.221004009246826, 'test/num_examples': 10000, 'score': 25536.77026438713, 'total_duration': 26444.191212654114, 'accumulated_submission_time': 25536.77026438713, 'accumulated_eval_time': 901.9840440750122, 'accumulated_logging_time': 2.8116519451141357, 'global_step': 75647, 'preemption_count': 0}), (77163, {'train/accuracy': 0.6889349222183228, 'train/loss': 1.2334457635879517, 'validation/accuracy': 0.6387199759483337, 'validation/loss': 1.485427737236023, 'validation/num_examples': 50000, 'test/accuracy': 0.5131000280380249, 'test/loss': 2.2043116092681885, 'test/num_examples': 10000, 'score': 26046.955763578415, 'total_duration': 26972.225853919983, 'accumulated_submission_time': 26046.955763578415, 'accumulated_eval_time': 919.7418501377106, 'accumulated_logging_time': 2.8506574630737305, 'global_step': 77163, 'preemption_count': 0}), (78679, {'train/accuracy': 0.721121609210968, 'train/loss': 1.0787429809570312, 'validation/accuracy': 0.6339199542999268, 'validation/loss': 1.5018742084503174, 'validation/num_examples': 50000, 'test/accuracy': 0.5126000046730042, 'test/loss': 2.209225654602051, 'test/num_examples': 10000, 'score': 26557.14439845085, 'total_duration': 27500.150496721268, 'accumulated_submission_time': 26557.14439845085, 'accumulated_eval_time': 937.3784308433533, 'accumulated_logging_time': 2.8979365825653076, 'global_step': 78679, 'preemption_count': 0}), (80195, {'train/accuracy': 0.7099210619926453, 'train/loss': 1.1163970232009888, 'validation/accuracy': 0.6413599848747253, 'validation/loss': 1.4619232416152954, 'validation/num_examples': 50000, 'test/accuracy': 0.5139999985694885, 'test/loss': 2.207616090774536, 'test/num_examples': 10000, 'score': 27067.337899446487, 'total_duration': 28027.980364322662, 'accumulated_submission_time': 27067.337899446487, 'accumulated_eval_time': 954.9220995903015, 'accumulated_logging_time': 2.937781572341919, 'global_step': 80195, 'preemption_count': 0}), (81710, {'train/accuracy': 0.7057955861091614, 'train/loss': 1.155988097190857, 'validation/accuracy': 0.6452400088310242, 'validation/loss': 1.4618548154830933, 'validation/num_examples': 50000, 'test/accuracy': 0.5199000239372253, 'test/loss': 2.1657211780548096, 'test/num_examples': 10000, 'score': 27577.38576722145, 'total_duration': 28555.9904255867, 'accumulated_submission_time': 27577.38576722145, 'accumulated_eval_time': 972.7897419929504, 'accumulated_logging_time': 2.9804928302764893, 'global_step': 81710, 'preemption_count': 0}), (83226, {'train/accuracy': 0.6995774507522583, 'train/loss': 1.173744559288025, 'validation/accuracy': 0.6433599591255188, 'validation/loss': 1.4629638195037842, 'validation/num_examples': 50000, 'test/accuracy': 0.5162000060081482, 'test/loss': 2.1647984981536865, 'test/num_examples': 10000, 'score': 28087.586535692215, 'total_duration': 29084.35788321495, 'accumulated_submission_time': 28087.586535692215, 'accumulated_eval_time': 990.8612344264984, 'accumulated_logging_time': 3.0235769748687744, 'global_step': 83226, 'preemption_count': 0}), (84741, {'train/accuracy': 0.6940369606018066, 'train/loss': 1.209545612335205, 'validation/accuracy': 0.6385200023651123, 'validation/loss': 1.4816385507583618, 'validation/num_examples': 50000, 'test/accuracy': 0.5128000378608704, 'test/loss': 2.1985490322113037, 'test/num_examples': 10000, 'score': 28597.68378353119, 'total_duration': 29612.305801153183, 'accumulated_submission_time': 28597.68378353119, 'accumulated_eval_time': 1008.6165888309479, 'accumulated_logging_time': 3.065901756286621, 'global_step': 84741, 'preemption_count': 0}), (86253, {'train/accuracy': 0.6893335580825806, 'train/loss': 1.2253234386444092, 'validation/accuracy': 0.6370799541473389, 'validation/loss': 1.4862534999847412, 'validation/num_examples': 50000, 'test/accuracy': 0.5186000466346741, 'test/loss': 2.212566614151001, 'test/num_examples': 10000, 'score': 29106.643271446228, 'total_duration': 30140.242399692535, 'accumulated_submission_time': 29106.643271446228, 'accumulated_eval_time': 1026.5237970352173, 'accumulated_logging_time': 4.082794904708862, 'global_step': 86253, 'preemption_count': 0}), (87768, {'train/accuracy': 0.7394770383834839, 'train/loss': 1.004119873046875, 'validation/accuracy': 0.6474599838256836, 'validation/loss': 1.4325826168060303, 'validation/num_examples': 50000, 'test/accuracy': 0.5189000368118286, 'test/loss': 2.1587750911712646, 'test/num_examples': 10000, 'score': 29616.63855457306, 'total_duration': 30668.031265974045, 'accumulated_submission_time': 29616.63855457306, 'accumulated_eval_time': 1044.2222499847412, 'accumulated_logging_time': 4.124652862548828, 'global_step': 87768, 'preemption_count': 0}), (89283, {'train/accuracy': 0.7106584906578064, 'train/loss': 1.1320785284042358, 'validation/accuracy': 0.644819974899292, 'validation/loss': 1.4565682411193848, 'validation/num_examples': 50000, 'test/accuracy': 0.5223000049591064, 'test/loss': 2.1549034118652344, 'test/num_examples': 10000, 'score': 30126.547045707703, 'total_duration': 31195.43574333191, 'accumulated_submission_time': 30126.547045707703, 'accumulated_eval_time': 1061.6245312690735, 'accumulated_logging_time': 4.165873289108276, 'global_step': 89283, 'preemption_count': 0}), (90798, {'train/accuracy': 0.7100805044174194, 'train/loss': 1.1253926753997803, 'validation/accuracy': 0.6494799852371216, 'validation/loss': 1.432708501815796, 'validation/num_examples': 50000, 'test/accuracy': 0.5230000019073486, 'test/loss': 2.1359827518463135, 'test/num_examples': 10000, 'score': 30636.560704946518, 'total_duration': 31723.081677913666, 'accumulated_submission_time': 30636.560704946518, 'accumulated_eval_time': 1079.1629874706268, 'accumulated_logging_time': 4.2079079151153564, 'global_step': 90798, 'preemption_count': 0}), (92314, {'train/accuracy': 0.7146045565605164, 'train/loss': 1.1162675619125366, 'validation/accuracy': 0.6528800129890442, 'validation/loss': 1.4155189990997314, 'validation/num_examples': 50000, 'test/accuracy': 0.5293000340461731, 'test/loss': 2.1161322593688965, 'test/num_examples': 10000, 'score': 31146.7821393013, 'total_duration': 32250.916907072067, 'accumulated_submission_time': 31146.7821393013, 'accumulated_eval_time': 1096.6802098751068, 'accumulated_logging_time': 4.251695394515991, 'global_step': 92314, 'preemption_count': 0}), (93829, {'train/accuracy': 0.7053571343421936, 'train/loss': 1.1537963151931763, 'validation/accuracy': 0.649679958820343, 'validation/loss': 1.422189474105835, 'validation/num_examples': 50000, 'test/accuracy': 0.5184000134468079, 'test/loss': 2.1597468852996826, 'test/num_examples': 10000, 'score': 31656.7867538929, 'total_duration': 32778.40650200844, 'accumulated_submission_time': 31656.7867538929, 'accumulated_eval_time': 1114.0649182796478, 'accumulated_logging_time': 4.299462080001831, 'global_step': 93829, 'preemption_count': 0}), (95344, {'train/accuracy': 0.7147639989852905, 'train/loss': 1.101299524307251, 'validation/accuracy': 0.6574400067329407, 'validation/loss': 1.3877466917037964, 'validation/num_examples': 50000, 'test/accuracy': 0.5294000506401062, 'test/loss': 2.091960906982422, 'test/num_examples': 10000, 'score': 32166.745491981506, 'total_duration': 33305.81133413315, 'accumulated_submission_time': 32166.745491981506, 'accumulated_eval_time': 1131.4108610153198, 'accumulated_logging_time': 4.345837831497192, 'global_step': 95344, 'preemption_count': 0}), (96860, {'train/accuracy': 0.7434829473495483, 'train/loss': 0.9642866849899292, 'validation/accuracy': 0.6609599590301514, 'validation/loss': 1.386073112487793, 'validation/num_examples': 50000, 'test/accuracy': 0.5304000377655029, 'test/loss': 2.108454704284668, 'test/num_examples': 10000, 'score': 32676.94704413414, 'total_duration': 33833.68562602997, 'accumulated_submission_time': 32676.94704413414, 'accumulated_eval_time': 1148.9862267971039, 'accumulated_logging_time': 4.391420841217041, 'global_step': 96860, 'preemption_count': 0}), (98375, {'train/accuracy': 0.7309271097183228, 'train/loss': 1.0284843444824219, 'validation/accuracy': 0.6595799922943115, 'validation/loss': 1.3712352514266968, 'validation/num_examples': 50000, 'test/accuracy': 0.5261000394821167, 'test/loss': 2.065514326095581, 'test/num_examples': 10000, 'score': 33186.945125579834, 'total_duration': 34361.40209579468, 'accumulated_submission_time': 33186.945125579834, 'accumulated_eval_time': 1166.6077721118927, 'accumulated_logging_time': 4.4350080490112305, 'global_step': 98375, 'preemption_count': 0}), (99890, {'train/accuracy': 0.7154615521430969, 'train/loss': 1.0986077785491943, 'validation/accuracy': 0.6500999927520752, 'validation/loss': 1.425310730934143, 'validation/num_examples': 50000, 'test/accuracy': 0.5289000272750854, 'test/loss': 2.132866382598877, 'test/num_examples': 10000, 'score': 33696.84750413895, 'total_duration': 34888.904074430466, 'accumulated_submission_time': 33696.84750413895, 'accumulated_eval_time': 1184.1078248023987, 'accumulated_logging_time': 4.481401681900024, 'global_step': 99890, 'preemption_count': 0}), (101406, {'train/accuracy': 0.7203643321990967, 'train/loss': 1.0835435390472412, 'validation/accuracy': 0.6618399620056152, 'validation/loss': 1.3780913352966309, 'validation/num_examples': 50000, 'test/accuracy': 0.525700032711029, 'test/loss': 2.14704966545105, 'test/num_examples': 10000, 'score': 34207.06012392044, 'total_duration': 35416.624915361404, 'accumulated_submission_time': 34207.06012392044, 'accumulated_eval_time': 1201.5192544460297, 'accumulated_logging_time': 4.524857044219971, 'global_step': 101406, 'preemption_count': 0}), (102922, {'train/accuracy': 0.7308474183082581, 'train/loss': 1.019730567932129, 'validation/accuracy': 0.6702199578285217, 'validation/loss': 1.3424674272537231, 'validation/num_examples': 50000, 'test/accuracy': 0.5365000367164612, 'test/loss': 2.069563865661621, 'test/num_examples': 10000, 'score': 34717.284263134, 'total_duration': 35944.59972167015, 'accumulated_submission_time': 34717.284263134, 'accumulated_eval_time': 1219.1642200946808, 'accumulated_logging_time': 4.576931715011597, 'global_step': 102922, 'preemption_count': 0}), (104437, {'train/accuracy': 0.7296117544174194, 'train/loss': 1.0509129762649536, 'validation/accuracy': 0.6679199934005737, 'validation/loss': 1.3478890657424927, 'validation/num_examples': 50000, 'test/accuracy': 0.5321000218391418, 'test/loss': 2.0829954147338867, 'test/num_examples': 10000, 'score': 35227.21813130379, 'total_duration': 36472.46457672119, 'accumulated_submission_time': 35227.21813130379, 'accumulated_eval_time': 1236.998259305954, 'accumulated_logging_time': 4.621797323226929, 'global_step': 104437, 'preemption_count': 0}), (105952, {'train/accuracy': 0.76175856590271, 'train/loss': 0.9061012864112854, 'validation/accuracy': 0.674560010433197, 'validation/loss': 1.3246057033538818, 'validation/num_examples': 50000, 'test/accuracy': 0.5433000326156616, 'test/loss': 2.0417113304138184, 'test/num_examples': 10000, 'score': 35737.22608041763, 'total_duration': 37000.28816699982, 'accumulated_submission_time': 35737.22608041763, 'accumulated_eval_time': 1254.704176902771, 'accumulated_logging_time': 4.679151773452759, 'global_step': 105952, 'preemption_count': 0}), (107468, {'train/accuracy': 0.7491828799247742, 'train/loss': 0.9538630843162537, 'validation/accuracy': 0.675819993019104, 'validation/loss': 1.3117072582244873, 'validation/num_examples': 50000, 'test/accuracy': 0.546500027179718, 'test/loss': 2.02262544631958, 'test/num_examples': 10000, 'score': 36247.436046123505, 'total_duration': 37528.2292740345, 'accumulated_submission_time': 36247.436046123505, 'accumulated_eval_time': 1272.3301212787628, 'accumulated_logging_time': 4.732055902481079, 'global_step': 107468, 'preemption_count': 0}), (108983, {'train/accuracy': 0.7424266338348389, 'train/loss': 0.9738861918449402, 'validation/accuracy': 0.6760599613189697, 'validation/loss': 1.3137764930725098, 'validation/num_examples': 50000, 'test/accuracy': 0.541700005531311, 'test/loss': 2.0563201904296875, 'test/num_examples': 10000, 'score': 36757.352653265, 'total_duration': 38056.18480205536, 'accumulated_submission_time': 36757.352653265, 'accumulated_eval_time': 1290.2720756530762, 'accumulated_logging_time': 4.7767369747161865, 'global_step': 108983, 'preemption_count': 0}), (110499, {'train/accuracy': 0.7394770383834839, 'train/loss': 0.996335506439209, 'validation/accuracy': 0.6743599772453308, 'validation/loss': 1.319318413734436, 'validation/num_examples': 50000, 'test/accuracy': 0.5463000535964966, 'test/loss': 2.017503261566162, 'test/num_examples': 10000, 'score': 37267.507370471954, 'total_duration': 38583.883883714676, 'accumulated_submission_time': 37267.507370471954, 'accumulated_eval_time': 1307.7142674922943, 'accumulated_logging_time': 4.826080322265625, 'global_step': 110499, 'preemption_count': 0}), (112014, {'train/accuracy': 0.7409917116165161, 'train/loss': 0.985767662525177, 'validation/accuracy': 0.6783599853515625, 'validation/loss': 1.3062725067138672, 'validation/num_examples': 50000, 'test/accuracy': 0.5491999983787537, 'test/loss': 2.0260019302368164, 'test/num_examples': 10000, 'score': 37777.445001125336, 'total_duration': 39111.363669633865, 'accumulated_submission_time': 37777.445001125336, 'accumulated_eval_time': 1325.155839920044, 'accumulated_logging_time': 4.8738861083984375, 'global_step': 112014, 'preemption_count': 0}), (113529, {'train/accuracy': 0.7640305757522583, 'train/loss': 0.9003487825393677, 'validation/accuracy': 0.6823399662971497, 'validation/loss': 1.285157561302185, 'validation/num_examples': 50000, 'test/accuracy': 0.5555000305175781, 'test/loss': 1.9852403402328491, 'test/num_examples': 10000, 'score': 38287.514525175095, 'total_duration': 39638.94228172302, 'accumulated_submission_time': 38287.514525175095, 'accumulated_eval_time': 1342.566482782364, 'accumulated_logging_time': 4.919671297073364, 'global_step': 113529, 'preemption_count': 0}), (115044, {'train/accuracy': 0.772859513759613, 'train/loss': 0.8542397618293762, 'validation/accuracy': 0.6820399761199951, 'validation/loss': 1.2807635068893433, 'validation/num_examples': 50000, 'test/accuracy': 0.553600013256073, 'test/loss': 1.981478214263916, 'test/num_examples': 10000, 'score': 38797.47217464447, 'total_duration': 40166.47235298157, 'accumulated_submission_time': 38797.47217464447, 'accumulated_eval_time': 1360.0370292663574, 'accumulated_logging_time': 4.967864036560059, 'global_step': 115044, 'preemption_count': 0}), (116559, {'train/accuracy': 0.7597456574440002, 'train/loss': 0.911958634853363, 'validation/accuracy': 0.681939959526062, 'validation/loss': 1.299109697341919, 'validation/num_examples': 50000, 'test/accuracy': 0.5580000281333923, 'test/loss': 1.988004207611084, 'test/num_examples': 10000, 'score': 39307.38990712166, 'total_duration': 40694.417508125305, 'accumulated_submission_time': 39307.38990712166, 'accumulated_eval_time': 1377.9627270698547, 'accumulated_logging_time': 5.016592741012573, 'global_step': 116559, 'preemption_count': 0}), (118074, {'train/accuracy': 0.7604631781578064, 'train/loss': 0.8970020413398743, 'validation/accuracy': 0.6898199915885925, 'validation/loss': 1.2545552253723145, 'validation/num_examples': 50000, 'test/accuracy': 0.5614000558853149, 'test/loss': 1.9666774272918701, 'test/num_examples': 10000, 'score': 39817.51469898224, 'total_duration': 41222.125121593475, 'accumulated_submission_time': 39817.51469898224, 'accumulated_eval_time': 1395.4464178085327, 'accumulated_logging_time': 5.063026189804077, 'global_step': 118074, 'preemption_count': 0}), (119589, {'train/accuracy': 0.7599050998687744, 'train/loss': 0.9128103256225586, 'validation/accuracy': 0.6882799863815308, 'validation/loss': 1.2635095119476318, 'validation/num_examples': 50000, 'test/accuracy': 0.5593000054359436, 'test/loss': 1.9771336317062378, 'test/num_examples': 10000, 'score': 40327.41791152954, 'total_duration': 41749.75271129608, 'accumulated_submission_time': 40327.41791152954, 'accumulated_eval_time': 1413.069516658783, 'accumulated_logging_time': 5.112490177154541, 'global_step': 119589, 'preemption_count': 0}), (121104, {'train/accuracy': 0.7638512253761292, 'train/loss': 0.8878701329231262, 'validation/accuracy': 0.6916399598121643, 'validation/loss': 1.2425378561019897, 'validation/num_examples': 50000, 'test/accuracy': 0.5651000142097473, 'test/loss': 1.9462604522705078, 'test/num_examples': 10000, 'score': 40837.543076753616, 'total_duration': 42277.27058959007, 'accumulated_submission_time': 40837.543076753616, 'accumulated_eval_time': 1430.3549864292145, 'accumulated_logging_time': 5.1657538414001465, 'global_step': 121104, 'preemption_count': 0}), (122619, {'train/accuracy': 0.8024553656578064, 'train/loss': 0.7302818298339844, 'validation/accuracy': 0.693619966506958, 'validation/loss': 1.2337989807128906, 'validation/num_examples': 50000, 'test/accuracy': 0.5672000050544739, 'test/loss': 1.9245952367782593, 'test/num_examples': 10000, 'score': 41347.52177858353, 'total_duration': 42805.71956944466, 'accumulated_submission_time': 41347.52177858353, 'accumulated_eval_time': 1448.7214317321777, 'accumulated_logging_time': 5.217383623123169, 'global_step': 122619, 'preemption_count': 0}), (124135, {'train/accuracy': 0.7833027839660645, 'train/loss': 0.8094884753227234, 'validation/accuracy': 0.6947399973869324, 'validation/loss': 1.2455179691314697, 'validation/num_examples': 50000, 'test/accuracy': 0.5654000043869019, 'test/loss': 1.947636365890503, 'test/num_examples': 10000, 'score': 41857.67465591431, 'total_duration': 43333.32304549217, 'accumulated_submission_time': 41857.67465591431, 'accumulated_eval_time': 1466.0691316127777, 'accumulated_logging_time': 5.268237113952637, 'global_step': 124135, 'preemption_count': 0}), (125650, {'train/accuracy': 0.7840800285339355, 'train/loss': 0.8092227578163147, 'validation/accuracy': 0.6946600079536438, 'validation/loss': 1.2339342832565308, 'validation/num_examples': 50000, 'test/accuracy': 0.572700023651123, 'test/loss': 1.9287670850753784, 'test/num_examples': 10000, 'score': 42367.645431280136, 'total_duration': 43861.1323056221, 'accumulated_submission_time': 42367.645431280136, 'accumulated_eval_time': 1483.8026728630066, 'accumulated_logging_time': 5.32036828994751, 'global_step': 125650, 'preemption_count': 0}), (127165, {'train/accuracy': 0.778340220451355, 'train/loss': 0.8295766711235046, 'validation/accuracy': 0.7000399827957153, 'validation/loss': 1.2225801944732666, 'validation/num_examples': 50000, 'test/accuracy': 0.5708000063896179, 'test/loss': 1.908912181854248, 'test/num_examples': 10000, 'score': 42877.60603952408, 'total_duration': 44388.82006406784, 'accumulated_submission_time': 42877.60603952408, 'accumulated_eval_time': 1501.4231088161469, 'accumulated_logging_time': 5.373712062835693, 'global_step': 127165, 'preemption_count': 0}), (128680, {'train/accuracy': 0.78324294090271, 'train/loss': 0.8075078129768372, 'validation/accuracy': 0.7049199938774109, 'validation/loss': 1.190179467201233, 'validation/num_examples': 50000, 'test/accuracy': 0.5818000435829163, 'test/loss': 1.8730347156524658, 'test/num_examples': 10000, 'score': 43387.53185915947, 'total_duration': 44916.69388747215, 'accumulated_submission_time': 43387.53185915947, 'accumulated_eval_time': 1519.2647440433502, 'accumulated_logging_time': 5.426731824874878, 'global_step': 128680, 'preemption_count': 0}), (130196, {'train/accuracy': 0.7864915132522583, 'train/loss': 0.789760947227478, 'validation/accuracy': 0.7076999545097351, 'validation/loss': 1.1743167638778687, 'validation/num_examples': 50000, 'test/accuracy': 0.5768000483512878, 'test/loss': 1.8925039768218994, 'test/num_examples': 10000, 'score': 43897.6113114357, 'total_duration': 45444.25380349159, 'accumulated_submission_time': 43897.6113114357, 'accumulated_eval_time': 1536.644121170044, 'accumulated_logging_time': 5.475257635116577, 'global_step': 130196, 'preemption_count': 0}), (131711, {'train/accuracy': 0.8223453164100647, 'train/loss': 0.6572174429893494, 'validation/accuracy': 0.7078799605369568, 'validation/loss': 1.1762797832489014, 'validation/num_examples': 50000, 'test/accuracy': 0.5764000415802002, 'test/loss': 1.8879553079605103, 'test/num_examples': 10000, 'score': 44407.7055516243, 'total_duration': 45971.96899843216, 'accumulated_submission_time': 44407.7055516243, 'accumulated_eval_time': 1554.1618838310242, 'accumulated_logging_time': 5.526075601577759, 'global_step': 131711, 'preemption_count': 0}), (133226, {'train/accuracy': 0.8046476244926453, 'train/loss': 0.7072620391845703, 'validation/accuracy': 0.7079600095748901, 'validation/loss': 1.179681420326233, 'validation/num_examples': 50000, 'test/accuracy': 0.581000030040741, 'test/loss': 1.8739051818847656, 'test/num_examples': 10000, 'score': 44917.60682630539, 'total_duration': 46499.558420181274, 'accumulated_submission_time': 44917.60682630539, 'accumulated_eval_time': 1571.7473032474518, 'accumulated_logging_time': 5.575676918029785, 'global_step': 133226, 'preemption_count': 0}), (134741, {'train/accuracy': 0.8073381781578064, 'train/loss': 0.7013617157936096, 'validation/accuracy': 0.7153399586677551, 'validation/loss': 1.1508142948150635, 'validation/num_examples': 50000, 'test/accuracy': 0.5949000120162964, 'test/loss': 1.822237253189087, 'test/num_examples': 10000, 'score': 45427.66591835022, 'total_duration': 47027.958698511124, 'accumulated_submission_time': 45427.66591835022, 'accumulated_eval_time': 1589.98424077034, 'accumulated_logging_time': 5.627666711807251, 'global_step': 134741, 'preemption_count': 0}), (136255, {'train/accuracy': 0.8015385866165161, 'train/loss': 0.7381904721260071, 'validation/accuracy': 0.7129799723625183, 'validation/loss': 1.1590455770492554, 'validation/num_examples': 50000, 'test/accuracy': 0.5845000147819519, 'test/loss': 1.8409979343414307, 'test/num_examples': 10000, 'score': 45937.36339020729, 'total_duration': 47555.45125055313, 'accumulated_submission_time': 45937.36339020729, 'accumulated_eval_time': 1607.3338513374329, 'accumulated_logging_time': 6.020332336425781, 'global_step': 136255, 'preemption_count': 0}), (137770, {'train/accuracy': 0.8052256107330322, 'train/loss': 0.7155845761299133, 'validation/accuracy': 0.7154799699783325, 'validation/loss': 1.1480318307876587, 'validation/num_examples': 50000, 'test/accuracy': 0.5848000049591064, 'test/loss': 1.8547910451889038, 'test/num_examples': 10000, 'score': 46447.486229896545, 'total_duration': 48083.33385229111, 'accumulated_submission_time': 46447.486229896545, 'accumulated_eval_time': 1624.9835669994354, 'accumulated_logging_time': 6.077698230743408, 'global_step': 137770, 'preemption_count': 0}), (139285, {'train/accuracy': 0.8102080225944519, 'train/loss': 0.6994613409042358, 'validation/accuracy': 0.7164799571037292, 'validation/loss': 1.1391528844833374, 'validation/num_examples': 50000, 'test/accuracy': 0.5924000144004822, 'test/loss': 1.8475563526153564, 'test/num_examples': 10000, 'score': 46957.41595888138, 'total_duration': 48611.016573905945, 'accumulated_submission_time': 46957.41595888138, 'accumulated_eval_time': 1642.6341168880463, 'accumulated_logging_time': 6.127922534942627, 'global_step': 139285, 'preemption_count': 0}), (140800, {'train/accuracy': 0.8451849222183228, 'train/loss': 0.5710806846618652, 'validation/accuracy': 0.7232999801635742, 'validation/loss': 1.1173813343048096, 'validation/num_examples': 50000, 'test/accuracy': 0.5952000021934509, 'test/loss': 1.7925559282302856, 'test/num_examples': 10000, 'score': 47467.37585401535, 'total_duration': 49138.57271814346, 'accumulated_submission_time': 47467.37585401535, 'accumulated_eval_time': 1660.1281578540802, 'accumulated_logging_time': 6.1773834228515625, 'global_step': 140800, 'preemption_count': 0}), (142315, {'train/accuracy': 0.8337252736091614, 'train/loss': 0.6066722273826599, 'validation/accuracy': 0.7225199937820435, 'validation/loss': 1.1123135089874268, 'validation/num_examples': 50000, 'test/accuracy': 0.5986000299453735, 'test/loss': 1.7970387935638428, 'test/num_examples': 10000, 'score': 47977.319326639175, 'total_duration': 49666.27079510689, 'accumulated_submission_time': 47977.319326639175, 'accumulated_eval_time': 1677.7803509235382, 'accumulated_logging_time': 6.227893829345703, 'global_step': 142315, 'preemption_count': 0}), (143830, {'train/accuracy': 0.8341039419174194, 'train/loss': 0.6040483713150024, 'validation/accuracy': 0.7270999550819397, 'validation/loss': 1.1015565395355225, 'validation/num_examples': 50000, 'test/accuracy': 0.600100040435791, 'test/loss': 1.7974672317504883, 'test/num_examples': 10000, 'score': 48487.28637838364, 'total_duration': 50193.8794400692, 'accumulated_submission_time': 48487.28637838364, 'accumulated_eval_time': 1695.3257067203522, 'accumulated_logging_time': 6.27121639251709, 'global_step': 143830, 'preemption_count': 0}), (145346, {'train/accuracy': 0.8286631107330322, 'train/loss': 0.6169041991233826, 'validation/accuracy': 0.727840006351471, 'validation/loss': 1.1030001640319824, 'validation/num_examples': 50000, 'test/accuracy': 0.5995000004768372, 'test/loss': 1.8142592906951904, 'test/num_examples': 10000, 'score': 48997.5089635849, 'total_duration': 50721.83307147026, 'accumulated_submission_time': 48997.5089635849, 'accumulated_eval_time': 1712.9535655975342, 'accumulated_logging_time': 6.321614980697632, 'global_step': 145346, 'preemption_count': 0}), (146862, {'train/accuracy': 0.832051157951355, 'train/loss': 0.6025562286376953, 'validation/accuracy': 0.7324999570846558, 'validation/loss': 1.0817550420761108, 'validation/num_examples': 50000, 'test/accuracy': 0.6061000227928162, 'test/loss': 1.7760010957717896, 'test/num_examples': 10000, 'score': 49507.73680782318, 'total_duration': 51249.255494356155, 'accumulated_submission_time': 49507.73680782318, 'accumulated_eval_time': 1730.040581703186, 'accumulated_logging_time': 6.375980854034424, 'global_step': 146862, 'preemption_count': 0}), (148377, {'train/accuracy': 0.840840220451355, 'train/loss': 0.5678520202636719, 'validation/accuracy': 0.732759952545166, 'validation/loss': 1.0800023078918457, 'validation/num_examples': 50000, 'test/accuracy': 0.6103000044822693, 'test/loss': 1.7833529710769653, 'test/num_examples': 10000, 'score': 50017.81396985054, 'total_duration': 51777.063213825226, 'accumulated_submission_time': 50017.81396985054, 'accumulated_eval_time': 1747.6648552417755, 'accumulated_logging_time': 6.429377555847168, 'global_step': 148377, 'preemption_count': 0}), (149893, {'train/accuracy': 0.8672671914100647, 'train/loss': 0.4734492003917694, 'validation/accuracy': 0.7351399660110474, 'validation/loss': 1.0676651000976562, 'validation/num_examples': 50000, 'test/accuracy': 0.6116000413894653, 'test/loss': 1.751431941986084, 'test/num_examples': 10000, 'score': 50527.99266386032, 'total_duration': 52304.908478975296, 'accumulated_submission_time': 50527.99266386032, 'accumulated_eval_time': 1765.2281498908997, 'accumulated_logging_time': 6.480208396911621, 'global_step': 149893, 'preemption_count': 0}), (151408, {'train/accuracy': 0.8615473508834839, 'train/loss': 0.48896026611328125, 'validation/accuracy': 0.7380799651145935, 'validation/loss': 1.0503398180007935, 'validation/num_examples': 50000, 'test/accuracy': 0.6101000308990479, 'test/loss': 1.7489426136016846, 'test/num_examples': 10000, 'score': 51038.04159331322, 'total_duration': 52832.49210214615, 'accumulated_submission_time': 51038.04159331322, 'accumulated_eval_time': 1782.6540973186493, 'accumulated_logging_time': 6.537206172943115, 'global_step': 151408, 'preemption_count': 0}), (152923, {'train/accuracy': 0.8637993931770325, 'train/loss': 0.4847530722618103, 'validation/accuracy': 0.7389400005340576, 'validation/loss': 1.0541657209396362, 'validation/num_examples': 50000, 'test/accuracy': 0.6165000200271606, 'test/loss': 1.755265712738037, 'test/num_examples': 10000, 'score': 51548.12100839615, 'total_duration': 53359.95241069794, 'accumulated_submission_time': 51548.12100839615, 'accumulated_eval_time': 1799.930284500122, 'accumulated_logging_time': 6.58967399597168, 'global_step': 152923, 'preemption_count': 0}), (154438, {'train/accuracy': 0.8626036047935486, 'train/loss': 0.4872516095638275, 'validation/accuracy': 0.7405999898910522, 'validation/loss': 1.0530531406402588, 'validation/num_examples': 50000, 'test/accuracy': 0.6126000285148621, 'test/loss': 1.7592875957489014, 'test/num_examples': 10000, 'score': 52058.202897787094, 'total_duration': 53887.78027367592, 'accumulated_submission_time': 52058.202897787094, 'accumulated_eval_time': 1817.5693821907043, 'accumulated_logging_time': 6.6446380615234375, 'global_step': 154438, 'preemption_count': 0}), (155954, {'train/accuracy': 0.8659518361091614, 'train/loss': 0.4726797938346863, 'validation/accuracy': 0.7437599897384644, 'validation/loss': 1.0325734615325928, 'validation/num_examples': 50000, 'test/accuracy': 0.6145000457763672, 'test/loss': 1.7565077543258667, 'test/num_examples': 10000, 'score': 52568.401344537735, 'total_duration': 54415.99195933342, 'accumulated_submission_time': 52568.401344537735, 'accumulated_eval_time': 1835.477769613266, 'accumulated_logging_time': 6.697588682174683, 'global_step': 155954, 'preemption_count': 0}), (157469, {'train/accuracy': 0.875996470451355, 'train/loss': 0.4355567395687103, 'validation/accuracy': 0.7448999881744385, 'validation/loss': 1.034436821937561, 'validation/num_examples': 50000, 'test/accuracy': 0.6205000281333923, 'test/loss': 1.7346514463424683, 'test/num_examples': 10000, 'score': 53078.52582502365, 'total_duration': 54943.82565164566, 'accumulated_submission_time': 53078.52582502365, 'accumulated_eval_time': 1853.0811932086945, 'accumulated_logging_time': 6.750476598739624, 'global_step': 157469, 'preemption_count': 0}), (158984, {'train/accuracy': 0.8933752775192261, 'train/loss': 0.3799366354942322, 'validation/accuracy': 0.7441399693489075, 'validation/loss': 1.0344130992889404, 'validation/num_examples': 50000, 'test/accuracy': 0.6225000023841858, 'test/loss': 1.7522176504135132, 'test/num_examples': 10000, 'score': 53588.57729744911, 'total_duration': 55471.33137130737, 'accumulated_submission_time': 53588.57729744911, 'accumulated_eval_time': 1870.427015542984, 'accumulated_logging_time': 6.8072190284729, 'global_step': 158984, 'preemption_count': 0}), (160500, {'train/accuracy': 0.8909438848495483, 'train/loss': 0.38730230927467346, 'validation/accuracy': 0.750499963760376, 'validation/loss': 1.014423131942749, 'validation/num_examples': 50000, 'test/accuracy': 0.6256000399589539, 'test/loss': 1.7291738986968994, 'test/num_examples': 10000, 'score': 54098.76694107056, 'total_duration': 55999.49178338051, 'accumulated_submission_time': 54098.76694107056, 'accumulated_eval_time': 1888.285579442978, 'accumulated_logging_time': 6.8666510581970215, 'global_step': 160500, 'preemption_count': 0}), (162015, {'train/accuracy': 0.8907844424247742, 'train/loss': 0.3792414665222168, 'validation/accuracy': 0.7509599924087524, 'validation/loss': 1.014463186264038, 'validation/num_examples': 50000, 'test/accuracy': 0.6287000179290771, 'test/loss': 1.735564112663269, 'test/num_examples': 10000, 'score': 54608.69594120979, 'total_duration': 56527.012323856354, 'accumulated_submission_time': 54608.69594120979, 'accumulated_eval_time': 1905.7687640190125, 'accumulated_logging_time': 6.921643972396851, 'global_step': 162015, 'preemption_count': 0}), (163530, {'train/accuracy': 0.8932557106018066, 'train/loss': 0.3726956844329834, 'validation/accuracy': 0.7510600090026855, 'validation/loss': 1.0063519477844238, 'validation/num_examples': 50000, 'test/accuracy': 0.6272000074386597, 'test/loss': 1.7141947746276855, 'test/num_examples': 10000, 'score': 55118.67601776123, 'total_duration': 57054.70344829559, 'accumulated_submission_time': 55118.67601776123, 'accumulated_eval_time': 1923.371912240982, 'accumulated_logging_time': 6.976501703262329, 'global_step': 163530, 'preemption_count': 0}), (165045, {'train/accuracy': 0.8986766338348389, 'train/loss': 0.3579636812210083, 'validation/accuracy': 0.7545199990272522, 'validation/loss': 0.9984259009361267, 'validation/num_examples': 50000, 'test/accuracy': 0.6301000118255615, 'test/loss': 1.7019473314285278, 'test/num_examples': 10000, 'score': 55628.793186903, 'total_duration': 57582.69201374054, 'accumulated_submission_time': 55628.793186903, 'accumulated_eval_time': 1941.1338379383087, 'accumulated_logging_time': 7.033837080001831, 'global_step': 165045, 'preemption_count': 0}), (166560, {'train/accuracy': 0.9122488498687744, 'train/loss': 0.3140309453010559, 'validation/accuracy': 0.7552599906921387, 'validation/loss': 0.9961941838264465, 'validation/num_examples': 50000, 'test/accuracy': 0.633400022983551, 'test/loss': 1.6993948221206665, 'test/num_examples': 10000, 'score': 56138.758283376694, 'total_duration': 58110.06244254112, 'accumulated_submission_time': 56138.758283376694, 'accumulated_eval_time': 1958.429122209549, 'accumulated_logging_time': 7.088637590408325, 'global_step': 166560, 'preemption_count': 0}), (168075, {'train/accuracy': 0.9167729616165161, 'train/loss': 0.29545295238494873, 'validation/accuracy': 0.7545599937438965, 'validation/loss': 0.992598831653595, 'validation/num_examples': 50000, 'test/accuracy': 0.6340000033378601, 'test/loss': 1.6936455965042114, 'test/num_examples': 10000, 'score': 56648.82356977463, 'total_duration': 58637.83798003197, 'accumulated_submission_time': 56648.82356977463, 'accumulated_eval_time': 1976.0279388427734, 'accumulated_logging_time': 7.146857023239136, 'global_step': 168075, 'preemption_count': 0}), (169590, {'train/accuracy': 0.9133848547935486, 'train/loss': 0.3045446276664734, 'validation/accuracy': 0.7583999633789062, 'validation/loss': 0.9829720854759216, 'validation/num_examples': 50000, 'test/accuracy': 0.6345000267028809, 'test/loss': 1.6890100240707397, 'test/num_examples': 10000, 'score': 57158.8615398407, 'total_duration': 59165.35334587097, 'accumulated_submission_time': 57158.8615398407, 'accumulated_eval_time': 1993.3908696174622, 'accumulated_logging_time': 7.208467960357666, 'global_step': 169590, 'preemption_count': 0}), (171105, {'train/accuracy': 0.9176697731018066, 'train/loss': 0.2919151186943054, 'validation/accuracy': 0.7594999670982361, 'validation/loss': 0.9838279485702515, 'validation/num_examples': 50000, 'test/accuracy': 0.6374000310897827, 'test/loss': 1.6809219121932983, 'test/num_examples': 10000, 'score': 57668.79889130592, 'total_duration': 59692.92246937752, 'accumulated_submission_time': 57668.79889130592, 'accumulated_eval_time': 2010.9151842594147, 'accumulated_logging_time': 7.263584613800049, 'global_step': 171105, 'preemption_count': 0}), (172620, {'train/accuracy': 0.9196228981018066, 'train/loss': 0.279366672039032, 'validation/accuracy': 0.7603200078010559, 'validation/loss': 0.9797834157943726, 'validation/num_examples': 50000, 'test/accuracy': 0.6355000138282776, 'test/loss': 1.6834412813186646, 'test/num_examples': 10000, 'score': 58178.850531578064, 'total_duration': 60220.705787181854, 'accumulated_submission_time': 58178.850531578064, 'accumulated_eval_time': 2028.5382385253906, 'accumulated_logging_time': 7.319125175476074, 'global_step': 172620, 'preemption_count': 0}), (174135, {'train/accuracy': 0.9229113459587097, 'train/loss': 0.2737523913383484, 'validation/accuracy': 0.7621399760246277, 'validation/loss': 0.9760602712631226, 'validation/num_examples': 50000, 'test/accuracy': 0.6358000040054321, 'test/loss': 1.688401460647583, 'test/num_examples': 10000, 'score': 58688.894991636276, 'total_duration': 60748.39577841759, 'accumulated_submission_time': 58688.894991636276, 'accumulated_eval_time': 2046.0725243091583, 'accumulated_logging_time': 7.3778181076049805, 'global_step': 174135, 'preemption_count': 0}), (175650, {'train/accuracy': 0.9290696382522583, 'train/loss': 0.25206708908081055, 'validation/accuracy': 0.7624199986457825, 'validation/loss': 0.9706498980522156, 'validation/num_examples': 50000, 'test/accuracy': 0.6411000490188599, 'test/loss': 1.676186203956604, 'test/num_examples': 10000, 'score': 59198.99043011665, 'total_duration': 61276.268881082535, 'accumulated_submission_time': 59198.99043011665, 'accumulated_eval_time': 2063.7414784431458, 'accumulated_logging_time': 7.4339611530303955, 'global_step': 175650, 'preemption_count': 0}), (177165, {'train/accuracy': 0.9302256107330322, 'train/loss': 0.24820855259895325, 'validation/accuracy': 0.7620399594306946, 'validation/loss': 0.9679412245750427, 'validation/num_examples': 50000, 'test/accuracy': 0.6412000060081482, 'test/loss': 1.673362374305725, 'test/num_examples': 10000, 'score': 59708.983047008514, 'total_duration': 61803.98777985573, 'accumulated_submission_time': 59708.983047008514, 'accumulated_eval_time': 2081.356790304184, 'accumulated_logging_time': 7.493084192276001, 'global_step': 177165, 'preemption_count': 0}), (178680, {'train/accuracy': 0.9304248690605164, 'train/loss': 0.24876093864440918, 'validation/accuracy': 0.7635599970817566, 'validation/loss': 0.9660710692405701, 'validation/num_examples': 50000, 'test/accuracy': 0.6420000195503235, 'test/loss': 1.6618832349777222, 'test/num_examples': 10000, 'score': 60218.987303733826, 'total_duration': 62331.470396757126, 'accumulated_submission_time': 60218.987303733826, 'accumulated_eval_time': 2098.7254860401154, 'accumulated_logging_time': 7.550928115844727, 'global_step': 178680, 'preemption_count': 0}), (180195, {'train/accuracy': 0.9312220811843872, 'train/loss': 0.24766357243061066, 'validation/accuracy': 0.7628799676895142, 'validation/loss': 0.9658904671669006, 'validation/num_examples': 50000, 'test/accuracy': 0.6408000588417053, 'test/loss': 1.6687637567520142, 'test/num_examples': 10000, 'score': 60729.14320731163, 'total_duration': 62859.096420288086, 'accumulated_submission_time': 60729.14320731163, 'accumulated_eval_time': 2116.0850701332092, 'accumulated_logging_time': 7.6090943813323975, 'global_step': 180195, 'preemption_count': 0}), (181711, {'train/accuracy': 0.9324377775192261, 'train/loss': 0.24604295194149017, 'validation/accuracy': 0.7635599970817566, 'validation/loss': 0.963376522064209, 'validation/num_examples': 50000, 'test/accuracy': 0.6413000226020813, 'test/loss': 1.6692149639129639, 'test/num_examples': 10000, 'score': 61239.31663656235, 'total_duration': 63386.6651391983, 'accumulated_submission_time': 61239.31663656235, 'accumulated_eval_time': 2133.3704464435577, 'accumulated_logging_time': 7.666547775268555, 'global_step': 181711, 'preemption_count': 0}), (183226, {'train/accuracy': 0.9330556392669678, 'train/loss': 0.24356487393379211, 'validation/accuracy': 0.763700008392334, 'validation/loss': 0.9627121686935425, 'validation/num_examples': 50000, 'test/accuracy': 0.6414000391960144, 'test/loss': 1.668165683746338, 'test/num_examples': 10000, 'score': 61749.31013250351, 'total_duration': 63914.571994781494, 'accumulated_submission_time': 61749.31013250351, 'accumulated_eval_time': 2151.172516822815, 'accumulated_logging_time': 7.724971771240234, 'global_step': 183226, 'preemption_count': 0}), (184741, {'train/accuracy': 0.9329758882522583, 'train/loss': 0.24010393023490906, 'validation/accuracy': 0.7639399766921997, 'validation/loss': 0.962593674659729, 'validation/num_examples': 50000, 'test/accuracy': 0.6416000127792358, 'test/loss': 1.6681299209594727, 'test/num_examples': 10000, 'score': 62259.26232099533, 'total_duration': 64441.897938489914, 'accumulated_submission_time': 62259.26232099533, 'accumulated_eval_time': 2168.4295752048492, 'accumulated_logging_time': 7.789133071899414, 'global_step': 184741, 'preemption_count': 0}), (186255, {'train/accuracy': 0.9334343075752258, 'train/loss': 0.2402811050415039, 'validation/accuracy': 0.7638399600982666, 'validation/loss': 0.9630224704742432, 'validation/num_examples': 50000, 'test/accuracy': 0.6425000429153442, 'test/loss': 1.6677354574203491, 'test/num_examples': 10000, 'score': 62769.26744389534, 'total_duration': 64969.73010158539, 'accumulated_submission_time': 62769.26744389534, 'accumulated_eval_time': 2186.145072221756, 'accumulated_logging_time': 7.848384857177734, 'global_step': 186255, 'preemption_count': 0}), (186666, {'train/accuracy': 0.9338129758834839, 'train/loss': 0.23659060895442963, 'validation/accuracy': 0.7639399766921997, 'validation/loss': 0.9623273015022278, 'validation/num_examples': 50000, 'test/accuracy': 0.6413000226020813, 'test/loss': 1.6673704385757446, 'test/num_examples': 10000, 'score': 62907.485946178436, 'total_duration': 65125.386219739914, 'accumulated_submission_time': 62907.485946178436, 'accumulated_eval_time': 2203.5093677043915, 'accumulated_logging_time': 7.9071362018585205, 'global_step': 186666, 'preemption_count': 0})], 'global_step': 186666}
I0202 03:11:51.352414 140070692116288 submission_runner.py:586] Timing: 62907.485946178436
I0202 03:11:51.352479 140070692116288 submission_runner.py:588] Total number of evals: 125
I0202 03:11:51.352521 140070692116288 submission_runner.py:589] ====================
I0202 03:11:51.352565 140070692116288 submission_runner.py:542] Using RNG seed 485521238
I0202 03:11:51.353935 140070692116288 submission_runner.py:551] --- Tuning run 4/5 ---
I0202 03:11:51.354045 140070692116288 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_3/imagenet_resnet_jax/trial_4.
I0202 03:11:51.355136 140070692116288 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_3/imagenet_resnet_jax/trial_4/hparams.json.
I0202 03:11:51.355908 140070692116288 submission_runner.py:206] Initializing dataset.
I0202 03:11:51.365482 140070692116288 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0202 03:11:51.375176 140070692116288 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0202 03:11:51.573819 140070692116288 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0202 03:11:51.800634 140070692116288 submission_runner.py:213] Initializing model.
I0202 03:11:57.351130 140070692116288 submission_runner.py:255] Initializing optimizer.
I0202 03:11:57.747804 140070692116288 submission_runner.py:262] Initializing metrics bundle.
I0202 03:11:57.747964 140070692116288 submission_runner.py:280] Initializing checkpoint and logger.
I0202 03:11:57.765116 140070692116288 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_3/imagenet_resnet_jax/trial_4 with prefix checkpoint_
I0202 03:11:57.765250 140070692116288 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_3/imagenet_resnet_jax/trial_4/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0202 03:12:10.174535 140070692116288 logger_utils.py:220] Unable to record git information. Continuing without it.
I0202 03:12:22.397938 140070692116288 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_3/imagenet_resnet_jax/trial_4/flags_0.json.
I0202 03:12:22.402271 140070692116288 submission_runner.py:314] Starting training loop.
I0202 03:12:54.958833 139907762734848 logging_writer.py:48] [0] global_step=0, grad_norm=0.6918318271636963, loss=6.925329208374023
I0202 03:12:54.971990 140070692116288 spec.py:321] Evaluating on the training split.
I0202 03:13:01.282092 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 03:13:09.956196 140070692116288 spec.py:349] Evaluating on the test split.
I0202 03:13:12.610574 140070692116288 submission_runner.py:408] Time since start: 50.21s, 	Step: 1, 	{'train/accuracy': 0.0011160713620483875, 'train/loss': 6.912219524383545, 'validation/accuracy': 0.0011199999134987593, 'validation/loss': 6.912059783935547, 'validation/num_examples': 50000, 'test/accuracy': 0.0009000000427477062, 'test/loss': 6.912177562713623, 'test/num_examples': 10000, 'score': 32.56960868835449, 'total_duration': 50.20822787284851, 'accumulated_submission_time': 32.56960868835449, 'accumulated_eval_time': 17.638524532318115, 'accumulated_logging_time': 0}
I0202 03:13:12.620878 139907754342144 logging_writer.py:48] [1] accumulated_eval_time=17.638525, accumulated_logging_time=0, accumulated_submission_time=32.569609, global_step=1, preemption_count=0, score=32.569609, test/accuracy=0.000900, test/loss=6.912178, test/num_examples=10000, total_duration=50.208228, train/accuracy=0.001116, train/loss=6.912220, validation/accuracy=0.001120, validation/loss=6.912060, validation/num_examples=50000
I0202 03:13:46.457007 139907762734848 logging_writer.py:48] [100] global_step=100, grad_norm=0.7586772441864014, loss=6.623493194580078
I0202 03:14:20.289740 139907754342144 logging_writer.py:48] [200] global_step=200, grad_norm=0.9301058053970337, loss=6.306859970092773
I0202 03:14:54.104200 139907762734848 logging_writer.py:48] [300] global_step=300, grad_norm=2.960355758666992, loss=6.025938034057617
I0202 03:15:27.963896 139907754342144 logging_writer.py:48] [400] global_step=400, grad_norm=2.381793260574341, loss=5.7379841804504395
I0202 03:16:01.825145 139907762734848 logging_writer.py:48] [500] global_step=500, grad_norm=4.3303303718566895, loss=5.600715637207031
I0202 03:16:35.683111 139907754342144 logging_writer.py:48] [600] global_step=600, grad_norm=3.1323816776275635, loss=5.490607261657715
I0202 03:17:09.553099 139907762734848 logging_writer.py:48] [700] global_step=700, grad_norm=3.8981688022613525, loss=5.1535539627075195
I0202 03:17:43.403107 139907754342144 logging_writer.py:48] [800] global_step=800, grad_norm=3.006945848464966, loss=5.09702730178833
I0202 03:18:17.220561 139907762734848 logging_writer.py:48] [900] global_step=900, grad_norm=2.6323955059051514, loss=4.83797025680542
I0202 03:18:51.080325 139907754342144 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.781022071838379, loss=4.836050510406494
I0202 03:19:24.924181 139907762734848 logging_writer.py:48] [1100] global_step=1100, grad_norm=3.0997629165649414, loss=4.420494556427002
I0202 03:19:58.803617 139907754342144 logging_writer.py:48] [1200] global_step=1200, grad_norm=2.9360713958740234, loss=4.360840797424316
I0202 03:20:32.726114 139907762734848 logging_writer.py:48] [1300] global_step=1300, grad_norm=4.130003929138184, loss=4.240389347076416
I0202 03:21:06.580079 139907754342144 logging_writer.py:48] [1400] global_step=1400, grad_norm=3.432987689971924, loss=4.105844974517822
I0202 03:21:40.419545 139907762734848 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.085366725921631, loss=4.113501071929932
I0202 03:21:42.943748 140070692116288 spec.py:321] Evaluating on the training split.
I0202 03:21:49.264731 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 03:21:57.772610 140070692116288 spec.py:349] Evaluating on the test split.
I0202 03:22:00.332454 140070692116288 submission_runner.py:408] Time since start: 577.93s, 	Step: 1509, 	{'train/accuracy': 0.21974648535251617, 'train/loss': 3.89326810836792, 'validation/accuracy': 0.20191998779773712, 'validation/loss': 4.026599407196045, 'validation/num_examples': 50000, 'test/accuracy': 0.1509000062942505, 'test/loss': 4.556992053985596, 'test/num_examples': 10000, 'score': 542.8288688659668, 'total_duration': 577.9301149845123, 'accumulated_submission_time': 542.8288688659668, 'accumulated_eval_time': 35.02720069885254, 'accumulated_logging_time': 0.02122807502746582}
I0202 03:22:00.350060 139907737556736 logging_writer.py:48] [1509] accumulated_eval_time=35.027201, accumulated_logging_time=0.021228, accumulated_submission_time=542.828869, global_step=1509, preemption_count=0, score=542.828869, test/accuracy=0.150900, test/loss=4.556992, test/num_examples=10000, total_duration=577.930115, train/accuracy=0.219746, train/loss=3.893268, validation/accuracy=0.201920, validation/loss=4.026599, validation/num_examples=50000
I0202 03:22:31.442385 139907745949440 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.3858425617218018, loss=3.871476411819458
I0202 03:23:05.272085 139907737556736 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.3589160442352295, loss=3.9363832473754883
I0202 03:23:39.083677 139907745949440 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.4065418243408203, loss=3.6828501224517822
I0202 03:24:12.945417 139907737556736 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.9262757301330566, loss=3.664855480194092
I0202 03:24:46.744477 139907745949440 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.8136885166168213, loss=3.5356805324554443
I0202 03:25:20.551821 139907737556736 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.7582776546478271, loss=3.529825448989868
I0202 03:25:54.318209 139907745949440 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.1673322916030884, loss=3.4409008026123047
I0202 03:26:28.152476 139907737556736 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.835013508796692, loss=3.4004924297332764
I0202 03:27:01.915005 139907745949440 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.390268087387085, loss=3.4100606441497803
I0202 03:27:35.666148 139907737556736 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.362130045890808, loss=3.290894031524658
I0202 03:28:09.451102 139907745949440 logging_writer.py:48] [2600] global_step=2600, grad_norm=2.085824489593506, loss=3.247453451156616
I0202 03:28:43.172616 139907737556736 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.0480296611785889, loss=3.2610983848571777
I0202 03:29:16.947005 139907745949440 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.1775792837142944, loss=3.104562997817993
I0202 03:29:50.754916 139907737556736 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.3345752954483032, loss=3.1019415855407715
I0202 03:30:24.530703 139907745949440 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.165317416191101, loss=3.1591296195983887
I0202 03:30:30.414324 140070692116288 spec.py:321] Evaluating on the training split.
I0202 03:30:37.053858 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 03:30:45.438373 140070692116288 spec.py:349] Evaluating on the test split.
I0202 03:30:48.066511 140070692116288 submission_runner.py:408] Time since start: 1105.66s, 	Step: 3019, 	{'train/accuracy': 0.3487125337123871, 'train/loss': 3.0133767127990723, 'validation/accuracy': 0.3220599889755249, 'validation/loss': 3.1563539505004883, 'validation/num_examples': 50000, 'test/accuracy': 0.24180001020431519, 'test/loss': 3.88297438621521, 'test/num_examples': 10000, 'score': 1052.8306775093079, 'total_duration': 1105.6641788482666, 'accumulated_submission_time': 1052.8306775093079, 'accumulated_eval_time': 52.679352045059204, 'accumulated_logging_time': 0.0487828254699707}
I0202 03:30:48.083054 139907737556736 logging_writer.py:48] [3019] accumulated_eval_time=52.679352, accumulated_logging_time=0.048783, accumulated_submission_time=1052.830678, global_step=3019, preemption_count=0, score=1052.830678, test/accuracy=0.241800, test/loss=3.882974, test/num_examples=10000, total_duration=1105.664179, train/accuracy=0.348713, train/loss=3.013377, validation/accuracy=0.322060, validation/loss=3.156354, validation/num_examples=50000
I0202 03:31:15.733378 139908710635264 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.2123970985412598, loss=3.082789421081543
I0202 03:31:49.427582 139907737556736 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.2792179584503174, loss=2.915649652481079
I0202 03:32:23.192590 139908710635264 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.0950171947479248, loss=3.051586627960205
I0202 03:32:57.050405 139907737556736 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.8306333422660828, loss=3.0650992393493652
I0202 03:33:30.831980 139908710635264 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.8995214700698853, loss=2.995802402496338
I0202 03:34:04.604767 139907737556736 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.9823649525642395, loss=3.0642638206481934
I0202 03:34:38.338555 139908710635264 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.0125346183776855, loss=2.9678642749786377
I0202 03:35:12.125263 139907737556736 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.8205798268318176, loss=2.9506654739379883
I0202 03:35:45.858850 139908710635264 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.8005396127700806, loss=2.905457019805908
I0202 03:36:19.611496 139907737556736 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.9789663553237915, loss=3.012160301208496
I0202 03:36:53.362809 139908710635264 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.9985595941543579, loss=2.8313796520233154
I0202 03:37:27.115871 139907737556736 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.9822568297386169, loss=2.7696919441223145
I0202 03:38:00.842319 139908710635264 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.7232151627540588, loss=2.7834692001342773
I0202 03:38:34.584344 139907737556736 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.8383162617683411, loss=2.6256535053253174
I0202 03:39:08.346905 139908710635264 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.9943323135375977, loss=2.6692452430725098
I0202 03:39:18.374127 140070692116288 spec.py:321] Evaluating on the training split.
I0202 03:39:24.620057 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 03:39:33.063473 140070692116288 spec.py:349] Evaluating on the test split.
I0202 03:39:35.704559 140070692116288 submission_runner.py:408] Time since start: 1633.30s, 	Step: 4531, 	{'train/accuracy': 0.3955078125, 'train/loss': 2.769458055496216, 'validation/accuracy': 0.37136000394821167, 'validation/loss': 2.910965919494629, 'validation/num_examples': 50000, 'test/accuracy': 0.27790001034736633, 'test/loss': 3.6371009349823, 'test/num_examples': 10000, 'score': 1563.0581607818604, 'total_duration': 1633.3022310733795, 'accumulated_submission_time': 1563.0581607818604, 'accumulated_eval_time': 70.0097508430481, 'accumulated_logging_time': 0.0756216049194336}
I0202 03:39:35.722328 139907762734848 logging_writer.py:48] [4531] accumulated_eval_time=70.009751, accumulated_logging_time=0.075622, accumulated_submission_time=1563.058161, global_step=4531, preemption_count=0, score=1563.058161, test/accuracy=0.277900, test/loss=3.637101, test/num_examples=10000, total_duration=1633.302231, train/accuracy=0.395508, train/loss=2.769458, validation/accuracy=0.371360, validation/loss=2.910966, validation/num_examples=50000
I0202 03:39:59.312846 139908425447168 logging_writer.py:48] [4600] global_step=4600, grad_norm=1.1658110618591309, loss=2.788501501083374
I0202 03:40:33.044810 139907762734848 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.8371235728263855, loss=2.8539748191833496
I0202 03:41:06.743992 139908425447168 logging_writer.py:48] [4800] global_step=4800, grad_norm=1.0150600671768188, loss=2.757194995880127
I0202 03:41:40.466710 139907762734848 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.8501878976821899, loss=2.7005488872528076
I0202 03:42:14.203068 139908425447168 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.8787015080451965, loss=2.765214443206787
I0202 03:42:47.908962 139907762734848 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.9660843014717102, loss=2.6625001430511475
I0202 03:43:21.681673 139908425447168 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.8330589532852173, loss=2.821242570877075
I0202 03:43:55.380548 139907762734848 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.807445228099823, loss=2.6512181758880615
I0202 03:44:29.101381 139908425447168 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.9877769351005554, loss=2.63015079498291
I0202 03:45:02.851381 139907762734848 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.8262503147125244, loss=2.6927683353424072
I0202 03:45:36.651576 139908425447168 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.8255710005760193, loss=2.5901083946228027
I0202 03:46:10.375194 139907762734848 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.0419825315475464, loss=2.7025694847106934
I0202 03:46:44.074198 139908425447168 logging_writer.py:48] [5800] global_step=5800, grad_norm=1.0477381944656372, loss=2.5708515644073486
I0202 03:47:17.730487 139907762734848 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.9926294088363647, loss=2.6418614387512207
I0202 03:47:51.469764 139908425447168 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.9722750186920166, loss=2.6293320655822754
I0202 03:48:05.789949 140070692116288 spec.py:321] Evaluating on the training split.
I0202 03:48:12.078309 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 03:48:20.755875 140070692116288 spec.py:349] Evaluating on the test split.
I0202 03:48:23.374779 140070692116288 submission_runner.py:408] Time since start: 2160.97s, 	Step: 6044, 	{'train/accuracy': 0.337890625, 'train/loss': 3.187882423400879, 'validation/accuracy': 0.30987998843193054, 'validation/loss': 3.3979156017303467, 'validation/num_examples': 50000, 'test/accuracy': 0.24710001051425934, 'test/loss': 4.004795551300049, 'test/num_examples': 10000, 'score': 2073.061323404312, 'total_duration': 2160.9724485874176, 'accumulated_submission_time': 2073.061323404312, 'accumulated_eval_time': 87.59454846382141, 'accumulated_logging_time': 0.1046609878540039}
I0202 03:48:23.392402 139907745949440 logging_writer.py:48] [6044] accumulated_eval_time=87.594548, accumulated_logging_time=0.104661, accumulated_submission_time=2073.061323, global_step=6044, preemption_count=0, score=2073.061323, test/accuracy=0.247100, test/loss=4.004796, test/num_examples=10000, total_duration=2160.972449, train/accuracy=0.337891, train/loss=3.187882, validation/accuracy=0.309880, validation/loss=3.397916, validation/num_examples=50000
I0202 03:48:42.592004 139907754342144 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.8736070394515991, loss=2.542238712310791
I0202 03:49:16.270505 139907745949440 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.9652312994003296, loss=2.5861616134643555
I0202 03:49:49.954663 139907754342144 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.2345128059387207, loss=2.7309913635253906
I0202 03:50:23.643532 139907745949440 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.9226760268211365, loss=2.705622673034668
I0202 03:50:57.298446 139907754342144 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.8974433541297913, loss=2.5720388889312744
I0202 03:51:31.006284 139907745949440 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.2017501592636108, loss=2.7657532691955566
I0202 03:52:04.752166 139907754342144 logging_writer.py:48] [6700] global_step=6700, grad_norm=1.032665491104126, loss=2.585893392562866
I0202 03:52:38.491278 139907745949440 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.1503733396530151, loss=2.5700247287750244
I0202 03:53:12.172756 139907754342144 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.3572810888290405, loss=2.508693218231201
I0202 03:53:45.884489 139907745949440 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.9282428026199341, loss=2.3502750396728516
I0202 03:54:19.549429 139907754342144 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.9344136118888855, loss=2.532010078430176
I0202 03:54:53.248732 139907745949440 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.8699520230293274, loss=2.4932496547698975
I0202 03:55:26.915699 139907754342144 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.0135812759399414, loss=2.48856258392334
I0202 03:56:00.629571 139907745949440 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.052233338356018, loss=2.642328977584839
I0202 03:56:34.321699 139907754342144 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.2061187028884888, loss=2.506516218185425
I0202 03:56:53.679351 140070692116288 spec.py:321] Evaluating on the training split.
I0202 03:57:00.050464 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 03:57:08.909622 140070692116288 spec.py:349] Evaluating on the test split.
I0202 03:57:11.911080 140070692116288 submission_runner.py:408] Time since start: 2689.51s, 	Step: 7559, 	{'train/accuracy': 0.24878427386283875, 'train/loss': 3.9911692142486572, 'validation/accuracy': 0.22015999257564545, 'validation/loss': 4.308241844177246, 'validation/num_examples': 50000, 'test/accuracy': 0.1762000024318695, 'test/loss': 4.848935604095459, 'test/num_examples': 10000, 'score': 2583.286673307419, 'total_duration': 2689.5087604522705, 'accumulated_submission_time': 2583.286673307419, 'accumulated_eval_time': 105.82629466056824, 'accumulated_logging_time': 0.13178706169128418}
I0202 03:57:11.926344 139907737556736 logging_writer.py:48] [7559] accumulated_eval_time=105.826295, accumulated_logging_time=0.131787, accumulated_submission_time=2583.286673, global_step=7559, preemption_count=0, score=2583.286673, test/accuracy=0.176200, test/loss=4.848936, test/num_examples=10000, total_duration=2689.508760, train/accuracy=0.248784, train/loss=3.991169, validation/accuracy=0.220160, validation/loss=4.308242, validation/num_examples=50000
I0202 03:57:26.101918 139907745949440 logging_writer.py:48] [7600] global_step=7600, grad_norm=1.0380834341049194, loss=2.5072662830352783
I0202 03:57:59.781408 139907737556736 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.0544191598892212, loss=2.5441319942474365
I0202 03:58:33.687021 139907745949440 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.196181058883667, loss=2.5292863845825195
I0202 03:59:07.473952 139907737556736 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.8769747018814087, loss=2.5066142082214355
I0202 03:59:41.146015 139907745949440 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.9878864288330078, loss=2.45412278175354
I0202 04:00:14.917474 139907737556736 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.9377816319465637, loss=2.546754837036133
I0202 04:00:48.592909 139907745949440 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.9338936805725098, loss=2.4542815685272217
I0202 04:01:22.262670 139907737556736 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.9240605235099792, loss=2.420076608657837
I0202 04:01:55.920306 139907745949440 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.8860188126564026, loss=2.3977694511413574
I0202 04:02:29.613437 139907737556736 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.0844162702560425, loss=2.5790982246398926
I0202 04:03:03.288123 139907745949440 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.9689347147941589, loss=2.5368072986602783
I0202 04:03:36.977022 139907737556736 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.9509432911872864, loss=2.401546001434326
I0202 04:04:10.655842 139907745949440 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.9000288844108582, loss=2.4495317935943604
I0202 04:04:44.358706 139907737556736 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.9540205001831055, loss=2.3873519897460938
I0202 04:05:18.196604 139907745949440 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.9406375288963318, loss=2.551759719848633
I0202 04:05:41.925980 140070692116288 spec.py:321] Evaluating on the training split.
I0202 04:05:48.255633 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 04:05:56.870254 140070692116288 spec.py:349] Evaluating on the test split.
I0202 04:05:59.496992 140070692116288 submission_runner.py:408] Time since start: 3217.09s, 	Step: 9072, 	{'train/accuracy': 0.40692761540412903, 'train/loss': 2.715965986251831, 'validation/accuracy': 0.3807999789714813, 'validation/loss': 2.8986012935638428, 'validation/num_examples': 50000, 'test/accuracy': 0.28280001878738403, 'test/loss': 3.630770444869995, 'test/num_examples': 10000, 'score': 3093.225257396698, 'total_duration': 3217.094662427902, 'accumulated_submission_time': 3093.225257396698, 'accumulated_eval_time': 123.39727687835693, 'accumulated_logging_time': 0.15583324432373047}
I0202 04:05:59.515226 139908710635264 logging_writer.py:48] [9072] accumulated_eval_time=123.397277, accumulated_logging_time=0.155833, accumulated_submission_time=3093.225257, global_step=9072, preemption_count=0, score=3093.225257, test/accuracy=0.282800, test/loss=3.630770, test/num_examples=10000, total_duration=3217.094662, train/accuracy=0.406928, train/loss=2.715966, validation/accuracy=0.380800, validation/loss=2.898601, validation/num_examples=50000
I0202 04:06:09.300843 139908719027968 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.961952805519104, loss=2.406409502029419
I0202 04:06:43.007302 139908710635264 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.0610994100570679, loss=2.530755043029785
I0202 04:07:16.724535 139908719027968 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.9405792951583862, loss=2.4350013732910156
I0202 04:07:50.403470 139908710635264 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.8827111124992371, loss=2.354003667831421
I0202 04:08:24.095955 139908719027968 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.8734927177429199, loss=2.3675007820129395
I0202 04:08:57.771701 139908710635264 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.9349823594093323, loss=2.5134198665618896
I0202 04:09:31.464493 139908719027968 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.9857858419418335, loss=2.5220746994018555
I0202 04:10:05.126479 139908710635264 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.9781460165977478, loss=2.4772801399230957
I0202 04:10:38.821130 139908719027968 logging_writer.py:48] [9900] global_step=9900, grad_norm=1.127079963684082, loss=2.548694610595703
I0202 04:11:12.558321 139908710635264 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.9571018815040588, loss=2.434682607650757
I0202 04:11:46.275584 139908719027968 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.9765482544898987, loss=2.529559373855591
I0202 04:12:19.941275 139908710635264 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.027497410774231, loss=2.523958444595337
I0202 04:12:53.637238 139908719027968 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.9487050771713257, loss=2.5227742195129395
I0202 04:13:27.301581 139908710635264 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.1297050714492798, loss=2.399170398712158
I0202 04:14:01.004527 139908719027968 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.9322482347488403, loss=2.3667263984680176
I0202 04:14:29.768526 140070692116288 spec.py:321] Evaluating on the training split.
I0202 04:14:36.754503 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 04:14:45.334736 140070692116288 spec.py:349] Evaluating on the test split.
I0202 04:14:48.014028 140070692116288 submission_runner.py:408] Time since start: 3745.61s, 	Step: 10587, 	{'train/accuracy': 0.4227718412876129, 'train/loss': 2.5902175903320312, 'validation/accuracy': 0.3854199945926666, 'validation/loss': 2.8245723247528076, 'validation/num_examples': 50000, 'test/accuracy': 0.2955000102519989, 'test/loss': 3.5228676795959473, 'test/num_examples': 10000, 'score': 3603.417446374893, 'total_duration': 3745.611699104309, 'accumulated_submission_time': 3603.417446374893, 'accumulated_eval_time': 141.6427493095398, 'accumulated_logging_time': 0.183197021484375}
I0202 04:14:48.032567 139907745949440 logging_writer.py:48] [10587] accumulated_eval_time=141.642749, accumulated_logging_time=0.183197, accumulated_submission_time=3603.417446, global_step=10587, preemption_count=0, score=3603.417446, test/accuracy=0.295500, test/loss=3.522868, test/num_examples=10000, total_duration=3745.611699, train/accuracy=0.422772, train/loss=2.590218, validation/accuracy=0.385420, validation/loss=2.824572, validation/num_examples=50000
I0202 04:14:52.738426 139907754342144 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.9278644919395447, loss=2.3667502403259277
I0202 04:15:26.421284 139907745949440 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.1212129592895508, loss=2.3825855255126953
I0202 04:16:00.109500 139907754342144 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.96053546667099, loss=2.4693171977996826
I0202 04:16:33.797562 139907745949440 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.9323013424873352, loss=2.339517831802368
I0202 04:17:07.465613 139907754342144 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.9578837156295776, loss=2.51560115814209
I0202 04:17:41.147335 139907745949440 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.0025423765182495, loss=2.3783984184265137
I0202 04:18:15.008673 139907754342144 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.9395114779472351, loss=2.507216453552246
I0202 04:18:48.668417 139907745949440 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.158406376838684, loss=2.41051983833313
I0202 04:19:22.360082 139907754342144 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.9794442653656006, loss=2.5132646560668945
I0202 04:19:56.009680 139907745949440 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.1738026142120361, loss=2.4480016231536865
I0202 04:20:29.710599 139907754342144 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.9758061766624451, loss=2.3774266242980957
I0202 04:21:03.378190 139907745949440 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.0833039283752441, loss=2.4666221141815186
I0202 04:21:37.081851 139907754342144 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.9901466369628906, loss=2.400132656097412
I0202 04:22:10.745508 139907745949440 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.0614855289459229, loss=2.2414186000823975
I0202 04:22:44.442285 139907754342144 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.0841052532196045, loss=2.3199305534362793
I0202 04:23:18.101795 139907745949440 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.9763647317886353, loss=2.4054901599884033
I0202 04:23:18.109932 140070692116288 spec.py:321] Evaluating on the training split.
I0202 04:23:24.390852 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 04:23:32.953514 140070692116288 spec.py:349] Evaluating on the test split.
I0202 04:23:35.680883 140070692116288 submission_runner.py:408] Time since start: 4273.28s, 	Step: 12101, 	{'train/accuracy': 0.3722098171710968, 'train/loss': 2.9450957775115967, 'validation/accuracy': 0.3542799949645996, 'validation/loss': 3.0856668949127197, 'validation/num_examples': 50000, 'test/accuracy': 0.26660001277923584, 'test/loss': 3.802535057067871, 'test/num_examples': 10000, 'score': 4113.432627916336, 'total_duration': 4273.278554916382, 'accumulated_submission_time': 4113.432627916336, 'accumulated_eval_time': 159.21364331245422, 'accumulated_logging_time': 0.21118879318237305}
I0202 04:23:35.702718 139907745949440 logging_writer.py:48] [12101] accumulated_eval_time=159.213643, accumulated_logging_time=0.211189, accumulated_submission_time=4113.432628, global_step=12101, preemption_count=0, score=4113.432628, test/accuracy=0.266600, test/loss=3.802535, test/num_examples=10000, total_duration=4273.278555, train/accuracy=0.372210, train/loss=2.945096, validation/accuracy=0.354280, validation/loss=3.085667, validation/num_examples=50000
I0202 04:24:09.357456 139908719027968 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.9699472188949585, loss=2.4615566730499268
I0202 04:24:43.230764 139907745949440 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.9575458765029907, loss=2.486360788345337
I0202 04:25:16.929823 139908719027968 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.931597113609314, loss=2.4661290645599365
I0202 04:25:50.589967 139907745949440 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.957460880279541, loss=2.325221300125122
I0202 04:26:24.263979 139908719027968 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.019047498703003, loss=2.2910144329071045
I0202 04:26:57.938662 139907745949440 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.1068216562271118, loss=2.5340828895568848
I0202 04:27:31.613580 139908719027968 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.0146323442459106, loss=2.3299930095672607
I0202 04:28:05.292484 139907745949440 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.015376329421997, loss=2.4527535438537598
I0202 04:28:38.975683 139908719027968 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.297888994216919, loss=2.448751211166382
I0202 04:29:12.644314 139907745949440 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.0133700370788574, loss=2.3547744750976562
I0202 04:29:46.329122 139908719027968 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.079503059387207, loss=2.5312931537628174
I0202 04:30:20.008020 139907745949440 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.1148245334625244, loss=2.4550247192382812
I0202 04:30:53.822596 139908719027968 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.1224225759506226, loss=2.3857195377349854
I0202 04:31:27.493010 139907745949440 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.092305064201355, loss=2.351696491241455
I0202 04:32:01.167928 139908719027968 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.8947402834892273, loss=2.31868314743042
I0202 04:32:05.689331 140070692116288 spec.py:321] Evaluating on the training split.
I0202 04:32:11.958946 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 04:32:20.468929 140070692116288 spec.py:349] Evaluating on the test split.
I0202 04:32:23.138465 140070692116288 submission_runner.py:408] Time since start: 4800.74s, 	Step: 13615, 	{'train/accuracy': 0.2633330523967743, 'train/loss': 3.873202085494995, 'validation/accuracy': 0.24587999284267426, 'validation/loss': 3.999222993850708, 'validation/num_examples': 50000, 'test/accuracy': 0.18560001254081726, 'test/loss': 4.675883769989014, 'test/num_examples': 10000, 'score': 4623.356384038925, 'total_duration': 4800.736140012741, 'accumulated_submission_time': 4623.356384038925, 'accumulated_eval_time': 176.6627459526062, 'accumulated_logging_time': 0.24370288848876953}
I0202 04:32:23.157480 139907762734848 logging_writer.py:48] [13615] accumulated_eval_time=176.662746, accumulated_logging_time=0.243703, accumulated_submission_time=4623.356384, global_step=13615, preemption_count=0, score=4623.356384, test/accuracy=0.185600, test/loss=4.675884, test/num_examples=10000, total_duration=4800.736140, train/accuracy=0.263333, train/loss=3.873202, validation/accuracy=0.245880, validation/loss=3.999223, validation/num_examples=50000
I0202 04:32:52.123006 139908425447168 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.002989649772644, loss=2.360342502593994
I0202 04:33:25.781735 139907762734848 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.0235002040863037, loss=2.4540116786956787
I0202 04:33:59.494607 139908425447168 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.1258758306503296, loss=2.325110912322998
I0202 04:34:33.137658 139907762734848 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.0225809812545776, loss=2.4096765518188477
I0202 04:35:06.833337 139908425447168 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.9567215442657471, loss=2.25130558013916
I0202 04:35:40.478725 139907762734848 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.9377505779266357, loss=2.469486951828003
I0202 04:36:14.170840 139908425447168 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.0770318508148193, loss=2.3734986782073975
I0202 04:36:47.823928 139907762734848 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.1738803386688232, loss=2.408722400665283
I0202 04:37:21.541963 139908425447168 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.094749927520752, loss=2.365429401397705
I0202 04:37:55.186593 139907762734848 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.1228277683258057, loss=2.3145461082458496
I0202 04:38:28.884237 139908425447168 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.9806299209594727, loss=2.436532497406006
I0202 04:39:02.518472 139907762734848 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.0699349641799927, loss=2.2636876106262207
I0202 04:39:36.216334 139908425447168 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.0048152208328247, loss=2.443857192993164
I0202 04:40:09.864040 139907762734848 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.0193030834197998, loss=2.4071543216705322
I0202 04:40:43.548352 139908425447168 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.1611769199371338, loss=2.4266366958618164
I0202 04:40:53.459534 140070692116288 spec.py:321] Evaluating on the training split.
I0202 04:40:59.713776 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 04:41:08.251237 140070692116288 spec.py:349] Evaluating on the test split.
I0202 04:41:10.895946 140070692116288 submission_runner.py:408] Time since start: 5328.49s, 	Step: 15131, 	{'train/accuracy': 0.43538743257522583, 'train/loss': 2.5323400497436523, 'validation/accuracy': 0.39135998487472534, 'validation/loss': 2.8394031524658203, 'validation/num_examples': 50000, 'test/accuracy': 0.30390000343322754, 'test/loss': 3.497375249862671, 'test/num_examples': 10000, 'score': 5133.596252441406, 'total_duration': 5328.493609189987, 'accumulated_submission_time': 5133.596252441406, 'accumulated_eval_time': 194.09912204742432, 'accumulated_logging_time': 0.27188539505004883}
I0202 04:41:10.915883 139908710635264 logging_writer.py:48] [15131] accumulated_eval_time=194.099122, accumulated_logging_time=0.271885, accumulated_submission_time=5133.596252, global_step=15131, preemption_count=0, score=5133.596252, test/accuracy=0.303900, test/loss=3.497375, test/num_examples=10000, total_duration=5328.493609, train/accuracy=0.435387, train/loss=2.532340, validation/accuracy=0.391360, validation/loss=2.839403, validation/num_examples=50000
I0202 04:41:34.495549 139908719027968 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.004277229309082, loss=2.3108112812042236
I0202 04:42:08.142246 139908710635264 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.0570179224014282, loss=2.333547353744507
I0202 04:42:41.807467 139908719027968 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.1829839944839478, loss=2.513767719268799
I0202 04:43:15.475702 139908710635264 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.9829981327056885, loss=2.1900417804718018
I0202 04:43:49.174203 139908719027968 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.1022979021072388, loss=2.321773052215576
I0202 04:44:22.862334 139908710635264 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.0550851821899414, loss=2.4136741161346436
I0202 04:44:56.541030 139908719027968 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.0413762331008911, loss=2.4048750400543213
I0202 04:45:30.209038 139908710635264 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.0457419157028198, loss=2.324188232421875
I0202 04:46:03.887506 139908719027968 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.0535420179367065, loss=2.259160041809082
I0202 04:46:37.558591 139908710635264 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.045634388923645, loss=2.2633919715881348
I0202 04:47:11.241633 139908719027968 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.3739253282546997, loss=2.428757667541504
I0202 04:47:44.906778 139908710635264 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.0931123495101929, loss=2.3998353481292725
I0202 04:48:18.583974 139908719027968 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.0283418893814087, loss=2.2806239128112793
I0202 04:48:52.252369 139908710635264 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.2486977577209473, loss=2.408522367477417
I0202 04:49:25.915212 139908719027968 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.005445122718811, loss=2.3887510299682617
I0202 04:49:41.216841 140070692116288 spec.py:321] Evaluating on the training split.
I0202 04:49:47.804890 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 04:49:56.530298 140070692116288 spec.py:349] Evaluating on the test split.
I0202 04:49:59.146378 140070692116288 submission_runner.py:408] Time since start: 5856.74s, 	Step: 16647, 	{'train/accuracy': 0.30745774507522583, 'train/loss': 3.5628976821899414, 'validation/accuracy': 0.2874000072479248, 'validation/loss': 3.7463135719299316, 'validation/num_examples': 50000, 'test/accuracy': 0.22050000727176666, 'test/loss': 4.4452009201049805, 'test/num_examples': 10000, 'score': 5643.8343007564545, 'total_duration': 5856.744036912918, 'accumulated_submission_time': 5643.8343007564545, 'accumulated_eval_time': 212.02861714363098, 'accumulated_logging_time': 0.3007538318634033}
I0202 04:49:59.165885 139907737556736 logging_writer.py:48] [16647] accumulated_eval_time=212.028617, accumulated_logging_time=0.300754, accumulated_submission_time=5643.834301, global_step=16647, preemption_count=0, score=5643.834301, test/accuracy=0.220500, test/loss=4.445201, test/num_examples=10000, total_duration=5856.744037, train/accuracy=0.307458, train/loss=3.562898, validation/accuracy=0.287400, validation/loss=3.746314, validation/num_examples=50000
I0202 04:50:17.347048 139907745949440 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.0336917638778687, loss=2.3101859092712402
I0202 04:50:51.023314 139907737556736 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.1276365518569946, loss=2.3759524822235107
I0202 04:51:24.679319 139907745949440 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.0029040575027466, loss=2.3013200759887695
I0202 04:51:58.361341 139907737556736 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.1168402433395386, loss=2.508635997772217
I0202 04:52:32.032742 139907745949440 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.104241967201233, loss=2.2994370460510254
I0202 04:53:05.720537 139907737556736 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.2715730667114258, loss=2.436635732650757
I0202 04:53:39.370114 139907745949440 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.9928857684135437, loss=2.3339622020721436
I0202 04:54:13.057951 139907737556736 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.0300027132034302, loss=2.4509410858154297
I0202 04:54:46.714361 139907745949440 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.0951173305511475, loss=2.307462215423584
I0202 04:55:20.394409 139907737556736 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.1526618003845215, loss=2.291080951690674
I0202 04:55:54.050046 139907745949440 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.9455925822257996, loss=2.2555246353149414
I0202 04:56:27.749649 139907737556736 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.0443403720855713, loss=2.299294948577881
I0202 04:57:01.377012 139907745949440 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.069395661354065, loss=2.374075412750244
I0202 04:57:35.055741 139907737556736 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.0656944513320923, loss=2.314517021179199
I0202 04:58:08.704640 139907745949440 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.1020108461380005, loss=2.406949996948242
I0202 04:58:29.402924 140070692116288 spec.py:321] Evaluating on the training split.
I0202 04:58:35.662481 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 04:58:44.311647 140070692116288 spec.py:349] Evaluating on the test split.
I0202 04:58:46.854484 140070692116288 submission_runner.py:408] Time since start: 6384.45s, 	Step: 18163, 	{'train/accuracy': 0.24236686527729034, 'train/loss': 3.9114837646484375, 'validation/accuracy': 0.22723999619483948, 'validation/loss': 4.067636013031006, 'validation/num_examples': 50000, 'test/accuracy': 0.16140000522136688, 'test/loss': 4.841095924377441, 'test/num_examples': 10000, 'score': 6154.009658336639, 'total_duration': 6384.4521453380585, 'accumulated_submission_time': 6154.009658336639, 'accumulated_eval_time': 229.48013925552368, 'accumulated_logging_time': 0.3294222354888916}
I0202 04:58:46.873575 139908719027968 logging_writer.py:48] [18163] accumulated_eval_time=229.480139, accumulated_logging_time=0.329422, accumulated_submission_time=6154.009658, global_step=18163, preemption_count=0, score=6154.009658, test/accuracy=0.161400, test/loss=4.841096, test/num_examples=10000, total_duration=6384.452145, train/accuracy=0.242367, train/loss=3.911484, validation/accuracy=0.227240, validation/loss=4.067636, validation/num_examples=50000
I0202 04:58:59.668968 139908727420672 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.0712754726409912, loss=2.337069034576416
I0202 04:59:33.290341 139908719027968 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.09797203540802, loss=2.438204288482666
I0202 05:00:06.968739 139908727420672 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.1598210334777832, loss=2.551008939743042
I0202 05:00:40.649055 139908719027968 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.126175045967102, loss=2.368422746658325
I0202 05:01:14.303881 139908727420672 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.0900832414627075, loss=2.3437232971191406
I0202 05:01:47.984320 139908719027968 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.963492214679718, loss=2.24865460395813
I0202 05:02:21.638493 139908727420672 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.9697412252426147, loss=2.30709171295166
I0202 05:02:55.312650 139908719027968 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.1990247964859009, loss=2.356818914413452
I0202 05:03:28.942720 139908727420672 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.2652467489242554, loss=2.3792881965637207
I0202 05:04:02.604751 139908719027968 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.9588513970375061, loss=2.3091416358947754
I0202 05:04:36.261599 139908727420672 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.021167278289795, loss=2.321997880935669
I0202 05:05:09.913534 139908719027968 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.0423494577407837, loss=2.37247371673584
I0202 05:05:43.577631 139908727420672 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.0922844409942627, loss=2.424466609954834
I0202 05:06:17.246325 139908719027968 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.0260928869247437, loss=2.3134751319885254
I0202 05:06:50.906323 139908727420672 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.085801124572754, loss=2.3833742141723633
I0202 05:07:16.971333 140070692116288 spec.py:321] Evaluating on the training split.
I0202 05:07:23.384332 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 05:07:31.670731 140070692116288 spec.py:349] Evaluating on the test split.
I0202 05:07:34.399409 140070692116288 submission_runner.py:408] Time since start: 6912.00s, 	Step: 19679, 	{'train/accuracy': 0.34693875908851624, 'train/loss': 3.0987894535064697, 'validation/accuracy': 0.3242200016975403, 'validation/loss': 3.262202739715576, 'validation/num_examples': 50000, 'test/accuracy': 0.2403000146150589, 'test/loss': 3.9924895763397217, 'test/num_examples': 10000, 'score': 6664.045799255371, 'total_duration': 6911.997058391571, 'accumulated_submission_time': 6664.045799255371, 'accumulated_eval_time': 246.9081676006317, 'accumulated_logging_time': 0.35784459114074707}
I0202 05:07:34.426742 139907754342144 logging_writer.py:48] [19679] accumulated_eval_time=246.908168, accumulated_logging_time=0.357845, accumulated_submission_time=6664.045799, global_step=19679, preemption_count=0, score=6664.045799, test/accuracy=0.240300, test/loss=3.992490, test/num_examples=10000, total_duration=6911.997058, train/accuracy=0.346939, train/loss=3.098789, validation/accuracy=0.324220, validation/loss=3.262203, validation/num_examples=50000
I0202 05:07:41.840413 139907762734848 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.1898210048675537, loss=2.53144907951355
I0202 05:08:15.497191 139907754342144 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.1595683097839355, loss=2.2229790687561035
I0202 05:08:49.144012 139907762734848 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.0934895277023315, loss=2.14544939994812
I0202 05:09:22.870361 139907754342144 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.2484921216964722, loss=2.362457275390625
I0202 05:09:56.484897 139907762734848 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.1462478637695312, loss=2.3233895301818848
I0202 05:10:30.175640 139907754342144 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.023650884628296, loss=2.386835813522339
I0202 05:11:03.839952 139907762734848 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.00371253490448, loss=2.2841949462890625
I0202 05:11:37.500365 139907754342144 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.028497576713562, loss=2.3895835876464844
I0202 05:12:11.168420 139907762734848 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.0500906705856323, loss=2.2462430000305176
I0202 05:12:44.817229 139907754342144 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.16114342212677, loss=2.266195774078369
I0202 05:13:18.494167 139907762734848 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.2783570289611816, loss=2.2216975688934326
I0202 05:13:52.137282 139907754342144 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.1835323572158813, loss=2.381477117538452
I0202 05:14:25.809567 139907762734848 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.0292179584503174, loss=2.43519926071167
I0202 05:14:59.460390 139907754342144 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.0022742748260498, loss=2.3250949382781982
I0202 05:15:33.219773 139907762734848 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.0664277076721191, loss=2.316770553588867
I0202 05:16:04.656938 140070692116288 spec.py:321] Evaluating on the training split.
I0202 05:16:10.916700 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 05:16:19.632876 140070692116288 spec.py:349] Evaluating on the test split.
I0202 05:16:22.259977 140070692116288 submission_runner.py:408] Time since start: 7439.86s, 	Step: 21195, 	{'train/accuracy': 0.28358179330825806, 'train/loss': 3.587366819381714, 'validation/accuracy': 0.262800008058548, 'validation/loss': 3.7317724227905273, 'validation/num_examples': 50000, 'test/accuracy': 0.18440000712871552, 'test/loss': 4.544374465942383, 'test/num_examples': 10000, 'score': 7174.211032867432, 'total_duration': 7439.85765004158, 'accumulated_submission_time': 7174.211032867432, 'accumulated_eval_time': 264.5111756324768, 'accumulated_logging_time': 0.39708518981933594}
I0202 05:16:22.280470 139907745949440 logging_writer.py:48] [21195] accumulated_eval_time=264.511176, accumulated_logging_time=0.397085, accumulated_submission_time=7174.211033, global_step=21195, preemption_count=0, score=7174.211033, test/accuracy=0.184400, test/loss=4.544374, test/num_examples=10000, total_duration=7439.857650, train/accuracy=0.283582, train/loss=3.587367, validation/accuracy=0.262800, validation/loss=3.731772, validation/num_examples=50000
I0202 05:16:24.299775 139908710635264 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.1162652969360352, loss=2.2807445526123047
I0202 05:16:57.839227 139907745949440 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.1983510255813599, loss=2.488110303878784
I0202 05:17:31.405112 139908710635264 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.1312965154647827, loss=2.2395286560058594
I0202 05:18:05.025373 139907745949440 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.0946192741394043, loss=2.3415393829345703
I0202 05:18:38.717896 139908710635264 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.081950306892395, loss=2.310365676879883
I0202 05:19:12.380124 139907745949440 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.2756707668304443, loss=2.3311827182769775
I0202 05:19:46.040107 139908710635264 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.1622084379196167, loss=2.391265630722046
I0202 05:20:19.704512 139907745949440 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.2358739376068115, loss=2.440384864807129
I0202 05:20:53.363002 139908710635264 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.0425083637237549, loss=2.2759222984313965
I0202 05:21:27.022009 139907745949440 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.0538462400436401, loss=2.318568229675293
I0202 05:22:00.780827 139908710635264 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.0530339479446411, loss=2.4430646896362305
I0202 05:22:34.368279 139907745949440 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.0664825439453125, loss=2.2943520545959473
I0202 05:23:08.014399 139908710635264 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.1652756929397583, loss=2.4100351333618164
I0202 05:23:41.687302 139907745949440 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.0797244310379028, loss=2.2818408012390137
I0202 05:24:15.303736 139908710635264 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.0551499128341675, loss=2.27828049659729
I0202 05:24:48.966388 139907745949440 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.9430981874465942, loss=2.3482582569122314
I0202 05:24:52.485445 140070692116288 spec.py:321] Evaluating on the training split.
I0202 05:24:58.714572 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 05:25:07.321284 140070692116288 spec.py:349] Evaluating on the test split.
I0202 05:25:09.973858 140070692116288 submission_runner.py:408] Time since start: 7967.57s, 	Step: 22712, 	{'train/accuracy': 0.20091278851032257, 'train/loss': 4.426466464996338, 'validation/accuracy': 0.18501999974250793, 'validation/loss': 4.574308395385742, 'validation/num_examples': 50000, 'test/accuracy': 0.14190000295639038, 'test/loss': 5.220768451690674, 'test/num_examples': 10000, 'score': 7684.350539445877, 'total_duration': 7967.571515083313, 'accumulated_submission_time': 7684.350539445877, 'accumulated_eval_time': 281.99954295158386, 'accumulated_logging_time': 0.43106698989868164}
I0202 05:25:09.999648 139907737556736 logging_writer.py:48] [22712] accumulated_eval_time=281.999543, accumulated_logging_time=0.431067, accumulated_submission_time=7684.350539, global_step=22712, preemption_count=0, score=7684.350539, test/accuracy=0.141900, test/loss=5.220768, test/num_examples=10000, total_duration=7967.571515, train/accuracy=0.200913, train/loss=4.426466, validation/accuracy=0.185020, validation/loss=4.574308, validation/num_examples=50000
I0202 05:25:39.930271 139907745949440 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.0267406702041626, loss=2.3604907989501953
I0202 05:26:13.573770 139907737556736 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.0610523223876953, loss=2.276343584060669
I0202 05:26:47.186052 139907745949440 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.0752685070037842, loss=2.4068541526794434
I0202 05:27:20.838384 139907737556736 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.1519466638565063, loss=2.423623561859131
I0202 05:27:54.496778 139907745949440 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.0167192220687866, loss=2.3478479385375977
I0202 05:28:28.204397 139907737556736 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.0526313781738281, loss=2.3665409088134766
I0202 05:29:01.836990 139907745949440 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.0131821632385254, loss=2.2126121520996094
I0202 05:29:35.477668 139907737556736 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.081384539604187, loss=2.3130197525024414
I0202 05:30:09.123329 139907745949440 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.0606372356414795, loss=2.3920583724975586
I0202 05:30:42.776915 139907737556736 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.3088449239730835, loss=2.4266302585601807
I0202 05:31:16.424040 139907745949440 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.086043357849121, loss=2.3176045417785645
I0202 05:31:50.091834 139907737556736 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.1838784217834473, loss=2.2765913009643555
I0202 05:32:23.748858 139907745949440 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.0022871494293213, loss=2.3904147148132324
I0202 05:32:57.411488 139907737556736 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.0880053043365479, loss=2.4066975116729736
I0202 05:33:31.007233 139907745949440 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.0297828912734985, loss=2.3835086822509766
I0202 05:33:40.251968 140070692116288 spec.py:321] Evaluating on the training split.
I0202 05:33:46.561346 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 05:33:55.305937 140070692116288 spec.py:349] Evaluating on the test split.
I0202 05:33:57.819404 140070692116288 submission_runner.py:408] Time since start: 8495.42s, 	Step: 24229, 	{'train/accuracy': 0.20440050959587097, 'train/loss': 4.545769691467285, 'validation/accuracy': 0.1773199886083603, 'validation/loss': 4.824890613555908, 'validation/num_examples': 50000, 'test/accuracy': 0.13830000162124634, 'test/loss': 5.4493255615234375, 'test/num_examples': 10000, 'score': 8194.534126281738, 'total_duration': 8495.416982650757, 'accumulated_submission_time': 8194.534126281738, 'accumulated_eval_time': 299.5668559074402, 'accumulated_logging_time': 0.4717752933502197}
I0202 05:33:57.840715 139908710635264 logging_writer.py:48] [24229] accumulated_eval_time=299.566856, accumulated_logging_time=0.471775, accumulated_submission_time=8194.534126, global_step=24229, preemption_count=0, score=8194.534126, test/accuracy=0.138300, test/loss=5.449326, test/num_examples=10000, total_duration=8495.416983, train/accuracy=0.204401, train/loss=4.545770, validation/accuracy=0.177320, validation/loss=4.824891, validation/num_examples=50000
I0202 05:34:21.994387 139908719027968 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.0169438123703003, loss=2.2907421588897705
I0202 05:34:55.706642 139908710635264 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.1858216524124146, loss=2.3922901153564453
I0202 05:35:29.291836 139908719027968 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.1878654956817627, loss=2.4195024967193604
I0202 05:36:02.877774 139908710635264 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.0666370391845703, loss=2.320263385772705
I0202 05:36:36.519704 139908719027968 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.9917488694190979, loss=2.126847267150879
I0202 05:37:10.182460 139908710635264 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.0468626022338867, loss=2.3937337398529053
I0202 05:37:43.773669 139908719027968 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.0917541980743408, loss=2.3658640384674072
I0202 05:38:17.327670 139908710635264 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.1243709325790405, loss=2.3516228199005127
I0202 05:38:50.978356 139908719027968 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.0836900472640991, loss=2.2777929306030273
I0202 05:39:24.619722 139908710635264 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.239234209060669, loss=2.300097942352295
I0202 05:39:58.246739 139908719027968 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.9863757491111755, loss=2.326897382736206
I0202 05:40:31.904518 139908710635264 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.0638889074325562, loss=2.3926916122436523
I0202 05:41:05.537230 139908719027968 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.1376402378082275, loss=2.353579044342041
I0202 05:41:39.217554 139908710635264 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.066681981086731, loss=2.335744619369507
I0202 05:42:12.855894 139908719027968 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.9973880648612976, loss=2.2759549617767334
I0202 05:42:28.141139 140070692116288 spec.py:321] Evaluating on the training split.
I0202 05:42:34.412260 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 05:42:42.840689 140070692116288 spec.py:349] Evaluating on the test split.
I0202 05:42:45.466653 140070692116288 submission_runner.py:408] Time since start: 9023.06s, 	Step: 25747, 	{'train/accuracy': 0.27180325984954834, 'train/loss': 3.84205961227417, 'validation/accuracy': 0.2549799978733063, 'validation/loss': 4.00461483001709, 'validation/num_examples': 50000, 'test/accuracy': 0.19200000166893005, 'test/loss': 4.759922981262207, 'test/num_examples': 10000, 'score': 8704.772486686707, 'total_duration': 9023.064319849014, 'accumulated_submission_time': 8704.772486686707, 'accumulated_eval_time': 316.89233231544495, 'accumulated_logging_time': 0.50246262550354}
I0202 05:42:45.487920 139907729164032 logging_writer.py:48] [25747] accumulated_eval_time=316.892332, accumulated_logging_time=0.502463, accumulated_submission_time=8704.772487, global_step=25747, preemption_count=0, score=8704.772487, test/accuracy=0.192000, test/loss=4.759923, test/num_examples=10000, total_duration=9023.064320, train/accuracy=0.271803, train/loss=3.842060, validation/accuracy=0.254980, validation/loss=4.004615, validation/num_examples=50000
I0202 05:43:03.607537 139907737556736 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.0730113983154297, loss=2.2615034580230713
I0202 05:43:37.157078 139907729164032 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.1026220321655273, loss=2.3120479583740234
I0202 05:44:10.711024 139907737556736 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.1645281314849854, loss=2.4641194343566895
I0202 05:44:44.292437 139907729164032 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.1549558639526367, loss=2.300800085067749
I0202 05:45:17.919104 139907737556736 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.2744064331054688, loss=2.294869899749756
I0202 05:45:51.571709 139907729164032 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.0931230783462524, loss=2.375577449798584
I0202 05:46:25.231009 139907737556736 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.1086199283599854, loss=2.3545432090759277
I0202 05:46:58.843523 139907729164032 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.0016993284225464, loss=2.350640058517456
I0202 05:47:32.488181 139907737556736 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.0894962549209595, loss=2.277679204940796
I0202 05:48:06.171788 139907729164032 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.9560970067977905, loss=2.1222994327545166
I0202 05:48:39.784631 139907737556736 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.2583298683166504, loss=2.306565761566162
I0202 05:49:13.448716 139907729164032 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.195892095565796, loss=2.281076431274414
I0202 05:49:47.084425 139907737556736 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.0205187797546387, loss=2.394423723220825
I0202 05:50:20.713460 139907729164032 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.0968462228775024, loss=2.3693623542785645
I0202 05:50:54.347363 139907737556736 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.2443137168884277, loss=2.308770179748535
I0202 05:51:15.663066 140070692116288 spec.py:321] Evaluating on the training split.
I0202 05:51:21.966378 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 05:51:30.322273 140070692116288 spec.py:349] Evaluating on the test split.
I0202 05:51:32.981933 140070692116288 submission_runner.py:408] Time since start: 9550.58s, 	Step: 27265, 	{'train/accuracy': 0.32912150025367737, 'train/loss': 3.240143060684204, 'validation/accuracy': 0.30559998750686646, 'validation/loss': 3.3980844020843506, 'validation/num_examples': 50000, 'test/accuracy': 0.22770000994205475, 'test/loss': 4.118068218231201, 'test/num_examples': 10000, 'score': 9214.88447213173, 'total_duration': 9550.579601049423, 'accumulated_submission_time': 9214.88447213173, 'accumulated_eval_time': 334.2111656665802, 'accumulated_logging_time': 0.5347170829772949}
I0202 05:51:33.003330 139908425447168 logging_writer.py:48] [27265] accumulated_eval_time=334.211166, accumulated_logging_time=0.534717, accumulated_submission_time=9214.884472, global_step=27265, preemption_count=0, score=9214.884472, test/accuracy=0.227700, test/loss=4.118068, test/num_examples=10000, total_duration=9550.579601, train/accuracy=0.329122, train/loss=3.240143, validation/accuracy=0.305600, validation/loss=3.398084, validation/num_examples=50000
I0202 05:51:45.097836 139908710635264 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.0829877853393555, loss=2.3627185821533203
I0202 05:52:18.673940 139908425447168 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.2091233730316162, loss=2.248063087463379
I0202 05:52:52.255683 139908710635264 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.0589267015457153, loss=2.1893186569213867
I0202 05:53:25.825503 139908425447168 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.1575976610183716, loss=2.2346153259277344
I0202 05:53:59.436598 139908710635264 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.1876527070999146, loss=2.3635220527648926
I0202 05:54:33.109578 139908425447168 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.210081934928894, loss=2.3914670944213867
I0202 05:55:06.726711 139908710635264 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.2167249917984009, loss=2.331669807434082
I0202 05:55:40.332280 139908425447168 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.1604963541030884, loss=2.331899404525757
I0202 05:56:13.998608 139908710635264 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.2479482889175415, loss=2.3217103481292725
I0202 05:56:47.600624 139908425447168 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.1206505298614502, loss=2.351609468460083
I0202 05:57:21.222691 139908710635264 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.2434322834014893, loss=2.398545980453491
I0202 05:57:54.810359 139908425447168 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.1651275157928467, loss=2.3018603324890137
I0202 05:58:28.419265 139908710635264 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.0924869775772095, loss=2.272045612335205
I0202 05:59:02.062552 139908425447168 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.0600537061691284, loss=2.3575968742370605
I0202 05:59:35.707185 139908710635264 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.109586477279663, loss=2.317821979522705
I0202 06:00:03.084184 140070692116288 spec.py:321] Evaluating on the training split.
I0202 06:00:09.384657 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 06:00:18.052266 140070692116288 spec.py:349] Evaluating on the test split.
I0202 06:00:20.734650 140070692116288 submission_runner.py:408] Time since start: 10078.33s, 	Step: 28783, 	{'train/accuracy': 0.2516741156578064, 'train/loss': 3.8752613067626953, 'validation/accuracy': 0.23507998883724213, 'validation/loss': 4.017209053039551, 'validation/num_examples': 50000, 'test/accuracy': 0.16670000553131104, 'test/loss': 4.732311725616455, 'test/num_examples': 10000, 'score': 9724.903431653976, 'total_duration': 10078.332313776016, 'accumulated_submission_time': 9724.903431653976, 'accumulated_eval_time': 351.86159229278564, 'accumulated_logging_time': 0.5654406547546387}
I0202 06:00:20.756297 139907729164032 logging_writer.py:48] [28783] accumulated_eval_time=351.861592, accumulated_logging_time=0.565441, accumulated_submission_time=9724.903432, global_step=28783, preemption_count=0, score=9724.903432, test/accuracy=0.166700, test/loss=4.732312, test/num_examples=10000, total_duration=10078.332314, train/accuracy=0.251674, train/loss=3.875261, validation/accuracy=0.235080, validation/loss=4.017209, validation/num_examples=50000
I0202 06:00:26.816493 139907737556736 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.0544638633728027, loss=2.2864065170288086
I0202 06:01:00.456317 139907729164032 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.071900725364685, loss=2.2571840286254883
I0202 06:01:33.997842 139907737556736 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.176591396331787, loss=2.338503837585449
I0202 06:02:07.547943 139907729164032 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.9851120114326477, loss=2.3097829818725586
I0202 06:02:41.130138 139907737556736 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.0838358402252197, loss=2.3023433685302734
I0202 06:03:14.743457 139907729164032 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.1086701154708862, loss=2.428189754486084
I0202 06:03:48.397201 139907737556736 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.2132906913757324, loss=2.3485381603240967
I0202 06:04:22.032534 139907729164032 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.1570385694503784, loss=2.3635096549987793
I0202 06:04:55.684447 139907737556736 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.2102949619293213, loss=2.380324363708496
I0202 06:05:29.319912 139907729164032 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.143790364265442, loss=2.167926788330078
I0202 06:06:02.948627 139907737556736 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.1750433444976807, loss=2.4130778312683105
I0202 06:06:36.568526 139907729164032 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.125826358795166, loss=2.2972865104675293
I0202 06:07:10.221795 139907737556736 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.0140081644058228, loss=2.2519164085388184
I0202 06:07:43.831379 139907729164032 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.0078909397125244, loss=2.274013042449951
I0202 06:08:17.477977 139907737556736 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.0922796726226807, loss=2.448050022125244
I0202 06:08:51.095829 139907729164032 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.14629328250885, loss=2.3123021125793457
I0202 06:08:51.105945 140070692116288 spec.py:321] Evaluating on the training split.
I0202 06:08:57.548663 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 06:09:06.202742 140070692116288 spec.py:349] Evaluating on the test split.
I0202 06:09:08.848416 140070692116288 submission_runner.py:408] Time since start: 10606.45s, 	Step: 30301, 	{'train/accuracy': 0.24918286502361298, 'train/loss': 4.077645778656006, 'validation/accuracy': 0.2342199981212616, 'validation/loss': 4.213882923126221, 'validation/num_examples': 50000, 'test/accuracy': 0.171300008893013, 'test/loss': 4.892067909240723, 'test/num_examples': 10000, 'score': 10235.189134597778, 'total_duration': 10606.446083307266, 'accumulated_submission_time': 10235.189134597778, 'accumulated_eval_time': 369.6040177345276, 'accumulated_logging_time': 0.5961604118347168}
I0202 06:09:08.871132 139907762734848 logging_writer.py:48] [30301] accumulated_eval_time=369.604018, accumulated_logging_time=0.596160, accumulated_submission_time=10235.189135, global_step=30301, preemption_count=0, score=10235.189135, test/accuracy=0.171300, test/loss=4.892068, test/num_examples=10000, total_duration=10606.446083, train/accuracy=0.249183, train/loss=4.077646, validation/accuracy=0.234220, validation/loss=4.213883, validation/num_examples=50000
I0202 06:09:42.369157 139908425447168 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.2461031675338745, loss=2.4747509956359863
I0202 06:10:15.955766 139907762734848 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.0421154499053955, loss=2.3137528896331787
I0202 06:10:49.594034 139908425447168 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.164694905281067, loss=2.258195400238037
I0202 06:11:23.160936 139907762734848 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.1074148416519165, loss=2.237515926361084
I0202 06:11:56.716188 139908425447168 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.1618280410766602, loss=2.3800768852233887
I0202 06:12:30.268813 139907762734848 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.1985434293746948, loss=2.222107172012329
I0202 06:13:03.862717 139908425447168 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.0940778255462646, loss=2.286245346069336
I0202 06:13:37.521176 139907762734848 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.104028344154358, loss=2.2570173740386963
I0202 06:14:11.161172 139908425447168 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.0312693119049072, loss=2.2233920097351074
I0202 06:14:44.748785 139907762734848 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.0605093240737915, loss=2.2524914741516113
I0202 06:15:18.372493 139908425447168 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.160601258277893, loss=2.3658950328826904
I0202 06:15:51.965179 139907762734848 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.1447813510894775, loss=2.255436897277832
I0202 06:16:25.615693 139908425447168 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.3123868703842163, loss=2.3390486240386963
I0202 06:16:59.230350 139907762734848 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.0627413988113403, loss=2.2840933799743652
I0202 06:17:32.871094 139908425447168 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.1822853088378906, loss=2.263920545578003
I0202 06:17:39.081751 140070692116288 spec.py:321] Evaluating on the training split.
I0202 06:17:45.336263 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 06:17:54.009087 140070692116288 spec.py:349] Evaluating on the test split.
I0202 06:17:56.704430 140070692116288 submission_runner.py:408] Time since start: 11134.30s, 	Step: 31820, 	{'train/accuracy': 0.23429527878761292, 'train/loss': 4.242311954498291, 'validation/accuracy': 0.22317999601364136, 'validation/loss': 4.331212520599365, 'validation/num_examples': 50000, 'test/accuracy': 0.16250000894069672, 'test/loss': 5.108082294464111, 'test/num_examples': 10000, 'score': 10745.33808541298, 'total_duration': 11134.302097797394, 'accumulated_submission_time': 10745.33808541298, 'accumulated_eval_time': 387.2266595363617, 'accumulated_logging_time': 0.6278092861175537}
I0202 06:17:56.726816 139907745949440 logging_writer.py:48] [31820] accumulated_eval_time=387.226660, accumulated_logging_time=0.627809, accumulated_submission_time=10745.338085, global_step=31820, preemption_count=0, score=10745.338085, test/accuracy=0.162500, test/loss=5.108082, test/num_examples=10000, total_duration=11134.302098, train/accuracy=0.234295, train/loss=4.242312, validation/accuracy=0.223180, validation/loss=4.331213, validation/num_examples=50000
I0202 06:18:23.891025 139907754342144 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.2413780689239502, loss=2.3372905254364014
I0202 06:18:57.407367 139907745949440 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.119558334350586, loss=2.3215107917785645
I0202 06:19:31.039571 139907754342144 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.0346965789794922, loss=2.176170825958252
I0202 06:20:04.741978 139907745949440 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.1369491815567017, loss=2.3616786003112793
I0202 06:20:38.350344 139907754342144 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.0695581436157227, loss=2.2096996307373047
I0202 06:21:11.980477 139907745949440 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.06525719165802, loss=2.237077236175537
I0202 06:21:45.516106 139907754342144 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.1101447343826294, loss=2.2255940437316895
I0202 06:22:19.090715 139907745949440 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.1369788646697998, loss=2.3282084465026855
I0202 06:22:52.686527 139907754342144 logging_writer.py:48] [32700] global_step=32700, grad_norm=1.024053692817688, loss=2.341935873031616
I0202 06:23:26.305271 139907745949440 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.0832781791687012, loss=2.292142152786255
I0202 06:23:59.912767 139907754342144 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.2610276937484741, loss=2.252500534057617
I0202 06:24:33.549427 139907745949440 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.0320968627929688, loss=2.3837146759033203
I0202 06:25:07.135787 139907754342144 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.275587558746338, loss=2.2035200595855713
I0202 06:25:40.761346 139907745949440 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.0389496088027954, loss=2.277299642562866
I0202 06:26:14.390779 139907754342144 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.126903772354126, loss=2.305692195892334
I0202 06:26:26.712162 140070692116288 spec.py:321] Evaluating on the training split.
I0202 06:26:32.981542 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 06:26:41.431251 140070692116288 spec.py:349] Evaluating on the test split.
I0202 06:26:43.965434 140070692116288 submission_runner.py:408] Time since start: 11661.56s, 	Step: 33338, 	{'train/accuracy': 0.3989955186843872, 'train/loss': 2.7934722900390625, 'validation/accuracy': 0.35819998383522034, 'validation/loss': 3.0654101371765137, 'validation/num_examples': 50000, 'test/accuracy': 0.263700008392334, 'test/loss': 3.8857994079589844, 'test/num_examples': 10000, 'score': 11255.258068799973, 'total_duration': 11661.563098192215, 'accumulated_submission_time': 11255.258068799973, 'accumulated_eval_time': 404.47988963127136, 'accumulated_logging_time': 0.6608669757843018}
I0202 06:26:43.988566 139908719027968 logging_writer.py:48] [33338] accumulated_eval_time=404.479890, accumulated_logging_time=0.660867, accumulated_submission_time=11255.258069, global_step=33338, preemption_count=0, score=11255.258069, test/accuracy=0.263700, test/loss=3.885799, test/num_examples=10000, total_duration=11661.563098, train/accuracy=0.398996, train/loss=2.793472, validation/accuracy=0.358200, validation/loss=3.065410, validation/num_examples=50000
I0202 06:27:05.158995 139908727420672 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.1728099584579468, loss=2.273756265640259
I0202 06:27:38.740051 139908719027968 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.3294134140014648, loss=2.261119842529297
I0202 06:28:12.251858 139908727420672 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.2864389419555664, loss=2.318758487701416
I0202 06:28:45.854722 139908719027968 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.087397813796997, loss=2.3333075046539307
I0202 06:29:19.436304 139908727420672 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.1849766969680786, loss=2.4159302711486816
I0202 06:29:53.042188 139908719027968 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.1696338653564453, loss=2.374546766281128
I0202 06:30:26.616048 139908727420672 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.0773085355758667, loss=2.3804128170013428
I0202 06:31:00.162648 139908719027968 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.2386627197265625, loss=2.256211757659912
I0202 06:31:33.725887 139908727420672 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.0412123203277588, loss=2.271723985671997
I0202 06:32:07.339734 139908719027968 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.146446704864502, loss=2.223554849624634
I0202 06:32:40.914859 139908727420672 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.1338887214660645, loss=2.177163600921631
I0202 06:33:14.564500 139908719027968 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.1887270212173462, loss=2.4620096683502197
I0202 06:33:48.205395 139908727420672 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.13180410861969, loss=2.261977434158325
I0202 06:34:21.842061 139908719027968 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.0454416275024414, loss=2.2682321071624756
I0202 06:34:55.406747 139908727420672 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.0487016439437866, loss=2.213620662689209
I0202 06:35:13.977929 140070692116288 spec.py:321] Evaluating on the training split.
I0202 06:35:20.324106 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 06:35:28.647284 140070692116288 spec.py:349] Evaluating on the test split.
I0202 06:35:31.287469 140070692116288 submission_runner.py:408] Time since start: 12188.89s, 	Step: 34857, 	{'train/accuracy': 0.29085618257522583, 'train/loss': 3.6698713302612305, 'validation/accuracy': 0.271479994058609, 'validation/loss': 3.857929229736328, 'validation/num_examples': 50000, 'test/accuracy': 0.19920000433921814, 'test/loss': 4.656971454620361, 'test/num_examples': 10000, 'score': 11765.183889389038, 'total_duration': 12188.885138511658, 'accumulated_submission_time': 11765.183889389038, 'accumulated_eval_time': 421.78941106796265, 'accumulated_logging_time': 0.6944155693054199}
I0202 06:35:31.310149 139907745949440 logging_writer.py:48] [34857] accumulated_eval_time=421.789411, accumulated_logging_time=0.694416, accumulated_submission_time=11765.183889, global_step=34857, preemption_count=0, score=11765.183889, test/accuracy=0.199200, test/loss=4.656971, test/num_examples=10000, total_duration=12188.885139, train/accuracy=0.290856, train/loss=3.669871, validation/accuracy=0.271480, validation/loss=3.857929, validation/num_examples=50000
I0202 06:35:46.073936 139907754342144 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.1122099161148071, loss=2.2644407749176025
I0202 06:36:19.623736 139907745949440 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.0778135061264038, loss=2.3547556400299072
I0202 06:36:53.242424 139907754342144 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.1978669166564941, loss=2.2559187412261963
I0202 06:37:26.841713 139907745949440 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.097151756286621, loss=2.391779899597168
I0202 06:38:00.467241 139907754342144 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.113235592842102, loss=2.172133207321167
I0202 06:38:34.022925 139907745949440 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.060616135597229, loss=2.15195369720459
I0202 06:39:07.652826 139907754342144 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.1770881414413452, loss=2.379080057144165
I0202 06:39:41.333166 139907745949440 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.0726302862167358, loss=2.447460412979126
I0202 06:40:14.927408 139907754342144 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.318294882774353, loss=2.402602195739746
I0202 06:40:48.445394 139907745949440 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.1208631992340088, loss=2.2567567825317383
I0202 06:41:21.988235 139907754342144 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.3591351509094238, loss=2.3166773319244385
I0202 06:41:55.576737 139907745949440 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.1031556129455566, loss=2.243170976638794
I0202 06:42:29.192804 139907754342144 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.104873538017273, loss=2.277280807495117
I0202 06:43:02.801482 139907745949440 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.1984591484069824, loss=2.3567962646484375
I0202 06:43:36.438887 139907754342144 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.0948412418365479, loss=2.351412534713745
I0202 06:44:01.496424 140070692116288 spec.py:321] Evaluating on the training split.
I0202 06:44:07.890074 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 06:44:16.304064 140070692116288 spec.py:349] Evaluating on the test split.
I0202 06:44:18.929416 140070692116288 submission_runner.py:408] Time since start: 12716.53s, 	Step: 36376, 	{'train/accuracy': 0.3069993555545807, 'train/loss': 3.6135473251342773, 'validation/accuracy': 0.2903600037097931, 'validation/loss': 3.7695767879486084, 'validation/num_examples': 50000, 'test/accuracy': 0.2078000158071518, 'test/loss': 4.676905632019043, 'test/num_examples': 10000, 'score': 12275.307540178299, 'total_duration': 12716.527085065842, 'accumulated_submission_time': 12275.307540178299, 'accumulated_eval_time': 439.2223870754242, 'accumulated_logging_time': 0.727224588394165}
I0202 06:44:18.952494 139908710635264 logging_writer.py:48] [36376] accumulated_eval_time=439.222387, accumulated_logging_time=0.727225, accumulated_submission_time=12275.307540, global_step=36376, preemption_count=0, score=12275.307540, test/accuracy=0.207800, test/loss=4.676906, test/num_examples=10000, total_duration=12716.527085, train/accuracy=0.306999, train/loss=3.613547, validation/accuracy=0.290360, validation/loss=3.769577, validation/num_examples=50000
I0202 06:44:27.356445 139908719027968 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.1665875911712646, loss=2.3945822715759277
I0202 06:45:00.865037 139908710635264 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.2274996042251587, loss=2.2191219329833984
I0202 06:45:34.536575 139908719027968 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.0375990867614746, loss=2.2919180393218994
I0202 06:46:08.114342 139908710635264 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.1519088745117188, loss=2.2090728282928467
I0202 06:46:41.751522 139908719027968 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.1844576597213745, loss=2.2484281063079834
I0202 06:47:15.309024 139908710635264 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.1487441062927246, loss=2.4504973888397217
I0202 06:47:48.880382 139908719027968 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.3352546691894531, loss=2.31491756439209
I0202 06:48:22.464719 139908710635264 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.238288164138794, loss=2.207571268081665
I0202 06:48:56.099846 139908719027968 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.1553069353103638, loss=2.2916648387908936
I0202 06:49:29.662817 139908710635264 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.1986923217773438, loss=2.2259931564331055
I0202 06:50:03.208766 139908719027968 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.208914875984192, loss=2.24749755859375
I0202 06:50:36.744986 139908710635264 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.1196073293685913, loss=2.2650060653686523
I0202 06:51:10.375677 139908719027968 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.2162245512008667, loss=2.406911849975586
I0202 06:51:43.955590 139908710635264 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.1410404443740845, loss=2.2848618030548096
I0202 06:52:17.614851 139908719027968 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.1556106805801392, loss=2.3236851692199707
I0202 06:52:49.041802 140070692116288 spec.py:321] Evaluating on the training split.
I0202 06:52:55.326583 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 06:53:04.131051 140070692116288 spec.py:349] Evaluating on the test split.
I0202 06:53:06.766664 140070692116288 submission_runner.py:408] Time since start: 13244.36s, 	Step: 37895, 	{'train/accuracy': 0.31865832209587097, 'train/loss': 3.3709821701049805, 'validation/accuracy': 0.29760000109672546, 'validation/loss': 3.534862756729126, 'validation/num_examples': 50000, 'test/accuracy': 0.22430001199245453, 'test/loss': 4.2304182052612305, 'test/num_examples': 10000, 'score': 12785.335270166397, 'total_duration': 13244.364332437515, 'accumulated_submission_time': 12785.335270166397, 'accumulated_eval_time': 456.94721508026123, 'accumulated_logging_time': 0.7594432830810547}
I0202 06:53:06.790139 139907745949440 logging_writer.py:48] [37895] accumulated_eval_time=456.947215, accumulated_logging_time=0.759443, accumulated_submission_time=12785.335270, global_step=37895, preemption_count=0, score=12785.335270, test/accuracy=0.224300, test/loss=4.230418, test/num_examples=10000, total_duration=13244.364332, train/accuracy=0.318658, train/loss=3.370982, validation/accuracy=0.297600, validation/loss=3.534863, validation/num_examples=50000
I0202 06:53:08.815755 139907754342144 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.1504840850830078, loss=2.3916072845458984
I0202 06:53:42.318625 139907745949440 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.225532054901123, loss=2.2961153984069824
I0202 06:54:15.942909 139907754342144 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.1121429204940796, loss=2.236185073852539
I0202 06:54:49.556842 139907745949440 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.089685320854187, loss=2.292659282684326
I0202 06:55:23.098586 139907754342144 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.1671943664550781, loss=2.2752578258514404
I0202 06:55:56.700478 139907745949440 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.2263175249099731, loss=2.3515071868896484
I0202 06:56:30.322491 139907754342144 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.102430820465088, loss=2.225489854812622
I0202 06:57:04.268277 139907745949440 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.122449517250061, loss=2.2794570922851562
I0202 06:57:37.829136 139907754342144 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.1831032037734985, loss=2.4178836345672607
I0202 06:58:11.433273 139907745949440 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.2323036193847656, loss=2.309847116470337
I0202 06:58:45.085796 139907754342144 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.2078801393508911, loss=2.289653778076172
I0202 06:59:18.672462 139907745949440 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.1721984148025513, loss=2.3091025352478027
I0202 06:59:52.290025 139907754342144 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.1611723899841309, loss=2.3156118392944336
I0202 07:00:25.885730 139907745949440 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.1984221935272217, loss=2.280796766281128
I0202 07:00:59.429659 139907754342144 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.119284749031067, loss=2.2450060844421387
I0202 07:01:32.968396 139907745949440 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.117603063583374, loss=2.0985519886016846
I0202 07:01:36.819864 140070692116288 spec.py:321] Evaluating on the training split.
I0202 07:01:43.087682 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 07:01:51.472428 140070692116288 spec.py:349] Evaluating on the test split.
I0202 07:01:54.149061 140070692116288 submission_runner.py:408] Time since start: 13771.75s, 	Step: 39413, 	{'train/accuracy': 0.25938695669174194, 'train/loss': 3.792681932449341, 'validation/accuracy': 0.24493999779224396, 'validation/loss': 3.898503065109253, 'validation/num_examples': 50000, 'test/accuracy': 0.17670001089572906, 'test/loss': 4.5571184158325195, 'test/num_examples': 10000, 'score': 13295.302300453186, 'total_duration': 13771.746730804443, 'accumulated_submission_time': 13295.302300453186, 'accumulated_eval_time': 474.27638053894043, 'accumulated_logging_time': 0.7922139167785645}
I0202 07:01:54.173345 139907737556736 logging_writer.py:48] [39413] accumulated_eval_time=474.276381, accumulated_logging_time=0.792214, accumulated_submission_time=13295.302300, global_step=39413, preemption_count=0, score=13295.302300, test/accuracy=0.176700, test/loss=4.557118, test/num_examples=10000, total_duration=13771.746731, train/accuracy=0.259387, train/loss=3.792682, validation/accuracy=0.244940, validation/loss=3.898503, validation/num_examples=50000
I0202 07:02:23.697359 139907745949440 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.1416820287704468, loss=2.3994438648223877
I0202 07:02:57.217159 139907737556736 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.1598715782165527, loss=2.294070243835449
I0202 07:03:30.751039 139907745949440 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.1973304748535156, loss=2.2917978763580322
I0202 07:04:04.283140 139907737556736 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.2906992435455322, loss=2.393298387527466
I0202 07:04:37.903406 139907745949440 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.172698736190796, loss=2.204080820083618
I0202 07:05:11.577577 139907737556736 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.1923236846923828, loss=2.2427823543548584
I0202 07:05:45.196073 139907745949440 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.299939513206482, loss=2.284515142440796
I0202 07:06:18.741138 139907737556736 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.1358610391616821, loss=2.25870943069458
I0202 07:06:52.274581 139907745949440 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.214062213897705, loss=2.358468770980835
I0202 07:07:25.861852 139907737556736 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.1014761924743652, loss=2.2422103881835938
I0202 07:07:59.458441 139907745949440 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.2757203578948975, loss=2.4420764446258545
I0202 07:08:33.014980 139907737556736 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.256831169128418, loss=2.1869113445281982
I0202 07:09:06.589433 139907745949440 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.0730608701705933, loss=2.2227530479431152
I0202 07:09:40.108448 139907737556736 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.0714975595474243, loss=2.2382941246032715
I0202 07:10:13.712470 139907745949440 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.1932116746902466, loss=2.3796069622039795
I0202 07:10:24.279193 140070692116288 spec.py:321] Evaluating on the training split.
I0202 07:10:30.602369 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 07:10:39.201566 140070692116288 spec.py:349] Evaluating on the test split.
I0202 07:10:41.864307 140070692116288 submission_runner.py:408] Time since start: 14299.46s, 	Step: 40933, 	{'train/accuracy': 0.22833624482154846, 'train/loss': 3.997976064682007, 'validation/accuracy': 0.21911999583244324, 'validation/loss': 4.096891403198242, 'validation/num_examples': 50000, 'test/accuracy': 0.16600000858306885, 'test/loss': 4.765476703643799, 'test/num_examples': 10000, 'score': 13805.345292568207, 'total_duration': 14299.461974859238, 'accumulated_submission_time': 13805.345292568207, 'accumulated_eval_time': 491.86146664619446, 'accumulated_logging_time': 0.8255927562713623}
I0202 07:10:41.894113 139908425447168 logging_writer.py:48] [40933] accumulated_eval_time=491.861467, accumulated_logging_time=0.825593, accumulated_submission_time=13805.345293, global_step=40933, preemption_count=0, score=13805.345293, test/accuracy=0.166000, test/loss=4.765477, test/num_examples=10000, total_duration=14299.461975, train/accuracy=0.228336, train/loss=3.997976, validation/accuracy=0.219120, validation/loss=4.096891, validation/num_examples=50000
I0202 07:11:04.673360 139908719027968 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.079581618309021, loss=2.2177469730377197
I0202 07:11:38.231529 139908425447168 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.0919950008392334, loss=2.1695175170898438
I0202 07:12:11.827357 139908719027968 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.1599079370498657, loss=2.354555606842041
I0202 07:12:45.415986 139908425447168 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.1831014156341553, loss=2.287053108215332
I0202 07:13:18.949221 139908719027968 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.151991605758667, loss=2.2158894538879395
I0202 07:13:52.489073 139908425447168 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.2348637580871582, loss=2.2342326641082764
I0202 07:14:26.119315 139908719027968 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.2700344324111938, loss=2.362149238586426
I0202 07:14:59.726977 139908425447168 logging_writer.py:48] [41700] global_step=41700, grad_norm=1.1219885349273682, loss=2.3080945014953613
I0202 07:15:33.340437 139908719027968 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.2021514177322388, loss=2.265993356704712
I0202 07:16:06.931731 139908425447168 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.1419920921325684, loss=2.116197109222412
I0202 07:16:40.472995 139908719027968 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.1354159116744995, loss=2.3273398876190186
I0202 07:17:14.007893 139908425447168 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.1585116386413574, loss=2.268092632293701
I0202 07:17:47.656282 139908719027968 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.1217131614685059, loss=2.282759428024292
I0202 07:18:21.225606 139908425447168 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.1504610776901245, loss=2.1023566722869873
I0202 07:18:54.779939 139908719027968 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.1769906282424927, loss=2.1709773540496826
I0202 07:19:12.037275 140070692116288 spec.py:321] Evaluating on the training split.
I0202 07:19:18.328144 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 07:19:26.991849 140070692116288 spec.py:349] Evaluating on the test split.
I0202 07:19:29.510937 140070692116288 submission_runner.py:408] Time since start: 14827.11s, 	Step: 42453, 	{'train/accuracy': 0.3529775142669678, 'train/loss': 3.090325355529785, 'validation/accuracy': 0.3198799788951874, 'validation/loss': 3.338887929916382, 'validation/num_examples': 50000, 'test/accuracy': 0.2339000105857849, 'test/loss': 4.0103983879089355, 'test/num_examples': 10000, 'score': 14315.425469398499, 'total_duration': 14827.108598709106, 'accumulated_submission_time': 14315.425469398499, 'accumulated_eval_time': 509.33509039878845, 'accumulated_logging_time': 0.8656878471374512}
I0202 07:19:29.536741 139907754342144 logging_writer.py:48] [42453] accumulated_eval_time=509.335090, accumulated_logging_time=0.865688, accumulated_submission_time=14315.425469, global_step=42453, preemption_count=0, score=14315.425469, test/accuracy=0.233900, test/loss=4.010398, test/num_examples=10000, total_duration=14827.108599, train/accuracy=0.352978, train/loss=3.090325, validation/accuracy=0.319880, validation/loss=3.338888, validation/num_examples=50000
I0202 07:19:45.622464 139907762734848 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.321593165397644, loss=2.252918243408203
I0202 07:20:19.120705 139907754342144 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.1152836084365845, loss=2.2044875621795654
I0202 07:20:52.665087 139907762734848 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.2277851104736328, loss=2.2233428955078125
I0202 07:21:26.293128 139907754342144 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.1473740339279175, loss=2.3378307819366455
I0202 07:21:59.826904 139907762734848 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.1153899431228638, loss=2.2670321464538574
I0202 07:22:33.372718 139907754342144 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.1629880666732788, loss=2.3146162033081055
I0202 07:23:06.921238 139907762734848 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.2507119178771973, loss=2.2873971462249756
I0202 07:23:40.540206 139907754342144 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.1531381607055664, loss=2.181979179382324
I0202 07:24:14.181558 139907762734848 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.2377305030822754, loss=2.254166603088379
I0202 07:24:47.778367 139907754342144 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.2168633937835693, loss=2.275947332382202
I0202 07:25:21.331559 139907762734848 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.1892528533935547, loss=2.2977006435394287
I0202 07:25:54.867801 139907754342144 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.1379642486572266, loss=2.1314504146575928
I0202 07:26:28.432224 139907762734848 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.1871323585510254, loss=2.1173863410949707
I0202 07:27:02.038847 139907754342144 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.3170491456985474, loss=2.349226236343384
I0202 07:27:35.594852 139907762734848 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.1707662343978882, loss=2.2060494422912598
I0202 07:27:59.540373 140070692116288 spec.py:321] Evaluating on the training split.
I0202 07:28:05.970250 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 07:28:14.551131 140070692116288 spec.py:349] Evaluating on the test split.
I0202 07:28:17.115936 140070692116288 submission_runner.py:408] Time since start: 15354.71s, 	Step: 43973, 	{'train/accuracy': 0.1209542378783226, 'train/loss': 5.900317192077637, 'validation/accuracy': 0.11394000053405762, 'validation/loss': 6.015324115753174, 'validation/num_examples': 50000, 'test/accuracy': 0.08650000393390656, 'test/loss': 6.511098384857178, 'test/num_examples': 10000, 'score': 14825.367683410645, 'total_duration': 15354.713600158691, 'accumulated_submission_time': 14825.367683410645, 'accumulated_eval_time': 526.9106154441833, 'accumulated_logging_time': 0.9007658958435059}
I0202 07:28:17.142567 139907737556736 logging_writer.py:48] [43973] accumulated_eval_time=526.910615, accumulated_logging_time=0.900766, accumulated_submission_time=14825.367683, global_step=43973, preemption_count=0, score=14825.367683, test/accuracy=0.086500, test/loss=6.511098, test/num_examples=10000, total_duration=15354.713600, train/accuracy=0.120954, train/loss=5.900317, validation/accuracy=0.113940, validation/loss=6.015324, validation/num_examples=50000
I0202 07:28:26.544234 139907745949440 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.2746193408966064, loss=2.336066722869873
I0202 07:29:00.090364 139907737556736 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.1387560367584229, loss=2.2043204307556152
I0202 07:29:33.605237 139907745949440 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.3097879886627197, loss=2.307128429412842
I0202 07:30:07.214892 139907737556736 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.143319845199585, loss=2.199204921722412
I0202 07:30:40.899901 139907745949440 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.1639446020126343, loss=2.242825508117676
I0202 07:31:14.508970 139907737556736 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.1187728643417358, loss=2.279492139816284
I0202 07:31:48.068252 139907745949440 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.3050572872161865, loss=2.405202865600586
I0202 07:32:21.611873 139907737556736 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.1379426717758179, loss=2.3473715782165527
I0202 07:32:55.173346 139907745949440 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.2274168729782104, loss=2.0937225818634033
I0202 07:33:28.733026 139907737556736 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.1329551935195923, loss=2.202341079711914
I0202 07:34:02.299312 139907745949440 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.1648989915847778, loss=2.365154504776001
I0202 07:34:35.864333 139907737556736 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.1314525604248047, loss=2.1743836402893066
I0202 07:35:09.464187 139907745949440 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.1089980602264404, loss=2.2271854877471924
I0202 07:35:42.984765 139907737556736 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.1965107917785645, loss=2.290839910507202
I0202 07:36:16.536115 139907745949440 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.2047042846679688, loss=2.2070531845092773
I0202 07:36:47.254873 140070692116288 spec.py:321] Evaluating on the training split.
I0202 07:36:53.529614 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 07:37:02.340105 140070692116288 spec.py:349] Evaluating on the test split.
I0202 07:37:04.832614 140070692116288 submission_runner.py:408] Time since start: 15882.43s, 	Step: 45493, 	{'train/accuracy': 0.17273198068141937, 'train/loss': 4.6645426750183105, 'validation/accuracy': 0.1613999903202057, 'validation/loss': 4.75282621383667, 'validation/num_examples': 50000, 'test/accuracy': 0.11970000714063644, 'test/loss': 5.4320783615112305, 'test/num_examples': 10000, 'score': 15335.417038440704, 'total_duration': 15882.430266857147, 'accumulated_submission_time': 15335.417038440704, 'accumulated_eval_time': 544.4883089065552, 'accumulated_logging_time': 0.9369504451751709}
I0202 07:37:04.858063 139907754342144 logging_writer.py:48] [45493] accumulated_eval_time=544.488309, accumulated_logging_time=0.936950, accumulated_submission_time=15335.417038, global_step=45493, preemption_count=0, score=15335.417038, test/accuracy=0.119700, test/loss=5.432078, test/num_examples=10000, total_duration=15882.430267, train/accuracy=0.172732, train/loss=4.664543, validation/accuracy=0.161400, validation/loss=4.752826, validation/num_examples=50000
I0202 07:37:07.567800 139907762734848 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.2284088134765625, loss=2.2565388679504395
I0202 07:37:41.070652 139907754342144 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.108994483947754, loss=2.3113088607788086
I0202 07:38:14.599398 139907762734848 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.145024299621582, loss=2.291527032852173
I0202 07:38:48.145412 139907754342144 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.1474417448043823, loss=2.224325656890869
I0202 07:39:21.699195 139907762734848 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.2021485567092896, loss=2.15272855758667
I0202 07:39:55.252994 139907754342144 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.1265190839767456, loss=2.217512845993042
I0202 07:40:28.864863 139907762734848 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.2151992321014404, loss=2.334726333618164
I0202 07:41:02.405473 139907754342144 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.4563937187194824, loss=2.21520733833313
I0202 07:41:35.940982 139907762734848 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.120143175125122, loss=2.129464864730835
I0202 07:42:09.504856 139907754342144 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.1139354705810547, loss=2.317148208618164
I0202 07:42:43.104222 139907762734848 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.1960089206695557, loss=2.1710610389709473
I0202 07:43:16.659385 139907754342144 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.1531498432159424, loss=2.291128396987915
I0202 07:43:50.278090 139907762734848 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.201109528541565, loss=2.133613348007202
I0202 07:44:23.819289 139907754342144 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.2532769441604614, loss=2.3737998008728027
I0202 07:44:57.390723 139907762734848 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.2589068412780762, loss=2.3533780574798584
I0202 07:45:30.917824 139907754342144 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.2413756847381592, loss=2.164324998855591
I0202 07:45:35.105743 140070692116288 spec.py:321] Evaluating on the training split.
I0202 07:45:41.410468 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 07:45:49.867701 140070692116288 spec.py:349] Evaluating on the test split.
I0202 07:45:52.509842 140070692116288 submission_runner.py:408] Time since start: 16410.11s, 	Step: 47014, 	{'train/accuracy': 0.1693638414144516, 'train/loss': 4.89009428024292, 'validation/accuracy': 0.16245999932289124, 'validation/loss': 4.977695465087891, 'validation/num_examples': 50000, 'test/accuracy': 0.11620000749826431, 'test/loss': 5.645530700683594, 'test/num_examples': 10000, 'score': 15845.601004362106, 'total_duration': 16410.10750222206, 'accumulated_submission_time': 15845.601004362106, 'accumulated_eval_time': 561.8923692703247, 'accumulated_logging_time': 0.9734163284301758}
I0202 07:45:52.538749 139908710635264 logging_writer.py:48] [47014] accumulated_eval_time=561.892369, accumulated_logging_time=0.973416, accumulated_submission_time=15845.601004, global_step=47014, preemption_count=0, score=15845.601004, test/accuracy=0.116200, test/loss=5.645531, test/num_examples=10000, total_duration=16410.107502, train/accuracy=0.169364, train/loss=4.890094, validation/accuracy=0.162460, validation/loss=4.977695, validation/num_examples=50000
I0202 07:46:21.694790 139908719027968 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.1505879163742065, loss=2.173495292663574
I0202 07:46:55.218563 139908710635264 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.1774550676345825, loss=2.1781046390533447
I0202 07:47:28.750535 139908719027968 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.2178837060928345, loss=2.295374870300293
I0202 07:48:02.353234 139908710635264 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.2208352088928223, loss=2.1506223678588867
I0202 07:48:35.945877 139908719027968 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.1005902290344238, loss=2.199695348739624
I0202 07:49:09.489245 139908710635264 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.133935570716858, loss=2.339478015899658
I0202 07:49:43.038478 139908719027968 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.2352230548858643, loss=2.1152379512786865
I0202 07:50:16.668581 139908710635264 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.1935697793960571, loss=2.2697372436523438
I0202 07:50:50.218035 139908719027968 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.3116936683654785, loss=2.224980354309082
I0202 07:51:23.814840 139908710635264 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.3003602027893066, loss=2.266306161880493
I0202 07:51:57.415791 139908719027968 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.2493648529052734, loss=2.2637600898742676
I0202 07:52:30.983881 139908710635264 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.224361777305603, loss=2.343339443206787
I0202 07:53:04.601177 139908719027968 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.2481504678726196, loss=2.314737319946289
I0202 07:53:38.140187 139908710635264 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.2293263673782349, loss=2.2951908111572266
I0202 07:54:11.674052 139908719027968 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.195165991783142, loss=2.2750046253204346
I0202 07:54:22.564325 140070692116288 spec.py:321] Evaluating on the training split.
I0202 07:54:28.973554 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 07:54:37.833593 140070692116288 spec.py:349] Evaluating on the test split.
I0202 07:54:40.584779 140070692116288 submission_runner.py:408] Time since start: 16938.18s, 	Step: 48534, 	{'train/accuracy': 0.18690210580825806, 'train/loss': 4.8144307136535645, 'validation/accuracy': 0.17753998935222626, 'validation/loss': 4.926436901092529, 'validation/num_examples': 50000, 'test/accuracy': 0.1300000101327896, 'test/loss': 5.572903156280518, 'test/num_examples': 10000, 'score': 16355.56469798088, 'total_duration': 16938.182448148727, 'accumulated_submission_time': 16355.56469798088, 'accumulated_eval_time': 579.9127895832062, 'accumulated_logging_time': 1.0119578838348389}
I0202 07:54:40.609707 139907737556736 logging_writer.py:48] [48534] accumulated_eval_time=579.912790, accumulated_logging_time=1.011958, accumulated_submission_time=16355.564698, global_step=48534, preemption_count=0, score=16355.564698, test/accuracy=0.130000, test/loss=5.572903, test/num_examples=10000, total_duration=16938.182448, train/accuracy=0.186902, train/loss=4.814431, validation/accuracy=0.177540, validation/loss=4.926437, validation/num_examples=50000
I0202 07:55:03.061686 139907754342144 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.0925804376602173, loss=2.2685155868530273
I0202 07:55:36.587192 139907737556736 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.2200772762298584, loss=2.133695125579834
I0202 07:56:10.152374 139907754342144 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.1787049770355225, loss=2.343526840209961
I0202 07:56:43.833027 139907737556736 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.0688105821609497, loss=2.143564224243164
I0202 07:57:17.444562 139907754342144 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.1420201063156128, loss=2.1481728553771973
I0202 07:57:50.996277 139907737556736 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.203860878944397, loss=2.2073042392730713
I0202 07:58:24.545197 139907754342144 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.1227614879608154, loss=2.204023838043213
I0202 07:58:58.103956 139907737556736 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.1884890794754028, loss=2.136167526245117
I0202 07:59:31.714350 139907754342144 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.1952146291732788, loss=2.231602668762207
I0202 08:00:05.256924 139907737556736 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.2194069623947144, loss=2.17507266998291
I0202 08:00:38.801322 139907754342144 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.2376033067703247, loss=2.1870713233947754
I0202 08:01:12.351230 139907737556736 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.2832037210464478, loss=2.247267246246338
I0202 08:01:45.972552 139907754342144 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.1559165716171265, loss=2.1232779026031494
I0202 08:02:19.520250 139907737556736 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.3625917434692383, loss=2.217738628387451
I0202 08:02:53.195381 139907754342144 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.1770803928375244, loss=2.233130693435669
I0202 08:03:10.828530 140070692116288 spec.py:321] Evaluating on the training split.
I0202 08:03:17.077185 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 08:03:25.667250 140070692116288 spec.py:349] Evaluating on the test split.
I0202 08:03:28.273975 140070692116288 submission_runner.py:408] Time since start: 17465.87s, 	Step: 50054, 	{'train/accuracy': 0.2864915430545807, 'train/loss': 3.610297918319702, 'validation/accuracy': 0.2510800063610077, 'validation/loss': 3.9188473224639893, 'validation/num_examples': 50000, 'test/accuracy': 0.1940000057220459, 'test/loss': 4.523690700531006, 'test/num_examples': 10000, 'score': 16865.721137285233, 'total_duration': 17465.871644973755, 'accumulated_submission_time': 16865.721137285233, 'accumulated_eval_time': 597.3582053184509, 'accumulated_logging_time': 1.0468201637268066}
I0202 08:03:28.299430 139907737556736 logging_writer.py:48] [50054] accumulated_eval_time=597.358205, accumulated_logging_time=1.046820, accumulated_submission_time=16865.721137, global_step=50054, preemption_count=0, score=16865.721137, test/accuracy=0.194000, test/loss=4.523691, test/num_examples=10000, total_duration=17465.871645, train/accuracy=0.286492, train/loss=3.610298, validation/accuracy=0.251080, validation/loss=3.918847, validation/num_examples=50000
I0202 08:03:44.070700 139908710635264 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.1613538265228271, loss=2.2271862030029297
I0202 08:04:17.588326 139907737556736 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.1863816976547241, loss=2.3344907760620117
I0202 08:04:51.202613 139908710635264 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.1252403259277344, loss=2.1800200939178467
I0202 08:05:24.781300 139907737556736 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.255414605140686, loss=2.3422651290893555
I0202 08:05:58.336682 139908710635264 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.108718991279602, loss=2.2086639404296875
I0202 08:06:31.860694 139907737556736 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.300175666809082, loss=2.2825770378112793
I0202 08:07:05.474287 139908710635264 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.1989825963974, loss=2.218210458755493
I0202 08:07:39.053663 139907737556736 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.218973159790039, loss=2.2976250648498535
I0202 08:08:12.576609 139908710635264 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.2977100610733032, loss=2.350759983062744
I0202 08:08:46.137762 139907737556736 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.3385745286941528, loss=2.2948458194732666
I0202 08:09:19.778827 139908710635264 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.249258279800415, loss=2.2652206420898438
I0202 08:09:53.331767 139907737556736 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.1684143543243408, loss=2.195993423461914
I0202 08:10:26.950543 139908710635264 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.2610687017440796, loss=2.3353660106658936
I0202 08:11:00.527870 139907737556736 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.113328218460083, loss=2.145306348800659
I0202 08:11:34.061662 139908710635264 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.1603035926818848, loss=2.2882134914398193
I0202 08:11:58.353954 140070692116288 spec.py:321] Evaluating on the training split.
I0202 08:12:04.749275 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 08:12:13.530858 140070692116288 spec.py:349] Evaluating on the test split.
I0202 08:12:16.180058 140070692116288 submission_runner.py:408] Time since start: 17993.78s, 	Step: 51574, 	{'train/accuracy': 0.24388153851032257, 'train/loss': 4.0654730796813965, 'validation/accuracy': 0.2254599928855896, 'validation/loss': 4.252574920654297, 'validation/num_examples': 50000, 'test/accuracy': 0.17340001463890076, 'test/loss': 4.7864227294921875, 'test/num_examples': 10000, 'score': 17375.71496105194, 'total_duration': 17993.777713537216, 'accumulated_submission_time': 17375.71496105194, 'accumulated_eval_time': 615.1842617988586, 'accumulated_logging_time': 1.081218957901001}
I0202 08:12:16.206251 139907762734848 logging_writer.py:48] [51574] accumulated_eval_time=615.184262, accumulated_logging_time=1.081219, accumulated_submission_time=17375.714961, global_step=51574, preemption_count=0, score=17375.714961, test/accuracy=0.173400, test/loss=4.786423, test/num_examples=10000, total_duration=17993.777714, train/accuracy=0.243882, train/loss=4.065473, validation/accuracy=0.225460, validation/loss=4.252575, validation/num_examples=50000
I0202 08:12:25.238358 139908425447168 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.2600431442260742, loss=2.1830921173095703
I0202 08:12:58.749007 139907762734848 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.4588836431503296, loss=2.275428295135498
I0202 08:13:32.270347 139908425447168 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.2583589553833008, loss=2.273256778717041
I0202 08:14:05.847580 139907762734848 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.334877371788025, loss=2.2881288528442383
I0202 08:14:39.377259 139908425447168 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.2215509414672852, loss=2.177151918411255
I0202 08:15:12.996245 139907762734848 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.321655511856079, loss=2.256495952606201
I0202 08:15:46.627450 139908425447168 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.305221676826477, loss=2.225308895111084
I0202 08:16:20.233523 139907762734848 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.1710504293441772, loss=2.292595386505127
I0202 08:16:53.787635 139908425447168 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.1638942956924438, loss=2.2430789470672607
I0202 08:17:27.293775 139907762734848 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.239817500114441, loss=2.2348036766052246
I0202 08:18:00.922351 139908425447168 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.2970774173736572, loss=2.363541603088379
I0202 08:18:34.522534 139907762734848 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.3153018951416016, loss=2.3050479888916016
I0202 08:19:08.099925 139908425447168 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.2637072801589966, loss=2.2476158142089844
I0202 08:19:41.706176 139907762734848 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.1415884494781494, loss=2.143409252166748
I0202 08:20:15.262608 139908425447168 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.141408085823059, loss=2.233309745788574
I0202 08:20:46.322669 140070692116288 spec.py:321] Evaluating on the training split.
I0202 08:20:52.577145 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 08:21:01.051034 140070692116288 spec.py:349] Evaluating on the test split.
I0202 08:21:03.884756 140070692116288 submission_runner.py:408] Time since start: 18521.48s, 	Step: 53094, 	{'train/accuracy': 0.24870455265045166, 'train/loss': 4.032920837402344, 'validation/accuracy': 0.22831998765468597, 'validation/loss': 4.205615043640137, 'validation/num_examples': 50000, 'test/accuracy': 0.17580001056194305, 'test/loss': 4.857440948486328, 'test/num_examples': 10000, 'score': 17885.76908183098, 'total_duration': 18521.48242330551, 'accumulated_submission_time': 17885.76908183098, 'accumulated_eval_time': 632.7463145256042, 'accumulated_logging_time': 1.1168007850646973}
I0202 08:21:03.910837 139908710635264 logging_writer.py:48] [53094] accumulated_eval_time=632.746315, accumulated_logging_time=1.116801, accumulated_submission_time=17885.769082, global_step=53094, preemption_count=0, score=17885.769082, test/accuracy=0.175800, test/loss=4.857441, test/num_examples=10000, total_duration=18521.482423, train/accuracy=0.248705, train/loss=4.032921, validation/accuracy=0.228320, validation/loss=4.205615, validation/num_examples=50000
I0202 08:21:06.274946 139908719027968 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.1775641441345215, loss=2.1930158138275146
I0202 08:21:39.822582 139908710635264 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.2577778100967407, loss=2.2181475162506104
I0202 08:22:13.572038 139908719027968 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.2755166292190552, loss=2.2253952026367188
I0202 08:22:47.117525 139908710635264 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.175726056098938, loss=2.180716037750244
I0202 08:23:20.659190 139908719027968 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.2941195964813232, loss=2.15466570854187
I0202 08:23:54.277716 139908710635264 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.3104513883590698, loss=2.048250436782837
I0202 08:24:27.833163 139908719027968 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.408697485923767, loss=2.1207339763641357
I0202 08:25:01.324692 139908710635264 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.2824151515960693, loss=2.190033197402954
I0202 08:25:34.873894 139908719027968 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.145393967628479, loss=2.1066465377807617
I0202 08:26:08.387985 139908710635264 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.1807146072387695, loss=2.2265841960906982
I0202 08:26:41.931902 139908719027968 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.164971113204956, loss=2.2251381874084473
I0202 08:27:15.541794 139908710635264 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.1733808517456055, loss=2.161557674407959
I0202 08:27:49.125206 139908719027968 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.1359903812408447, loss=2.1907958984375
I0202 08:28:22.768369 139908710635264 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.2389353513717651, loss=2.1982600688934326
I0202 08:28:56.341335 139908719027968 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.3577790260314941, loss=2.2264318466186523
I0202 08:29:29.970312 139908710635264 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.2092628479003906, loss=2.2510008811950684
I0202 08:29:34.168604 140070692116288 spec.py:321] Evaluating on the training split.
I0202 08:29:40.422406 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 08:29:49.132659 140070692116288 spec.py:349] Evaluating on the test split.
I0202 08:29:51.768401 140070692116288 submission_runner.py:408] Time since start: 19049.37s, 	Step: 54614, 	{'train/accuracy': 0.3034518361091614, 'train/loss': 3.569441318511963, 'validation/accuracy': 0.28283998370170593, 'validation/loss': 3.7012898921966553, 'validation/num_examples': 50000, 'test/accuracy': 0.2151000052690506, 'test/loss': 4.410431385040283, 'test/num_examples': 10000, 'score': 18395.96377325058, 'total_duration': 19049.366036891937, 'accumulated_submission_time': 18395.96377325058, 'accumulated_eval_time': 650.3460447788239, 'accumulated_logging_time': 1.1529819965362549}
I0202 08:29:51.795823 139907737556736 logging_writer.py:48] [54614] accumulated_eval_time=650.346045, accumulated_logging_time=1.152982, accumulated_submission_time=18395.963773, global_step=54614, preemption_count=0, score=18395.963773, test/accuracy=0.215100, test/loss=4.410431, test/num_examples=10000, total_duration=19049.366037, train/accuracy=0.303452, train/loss=3.569441, validation/accuracy=0.282840, validation/loss=3.701290, validation/num_examples=50000
I0202 08:30:20.933651 139907754342144 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.2962762117385864, loss=2.1914031505584717
I0202 08:30:54.523597 139907737556736 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.2010492086410522, loss=2.272822856903076
I0202 08:31:28.059564 139907754342144 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.2505825757980347, loss=2.114227294921875
I0202 08:32:01.590820 139907737556736 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.2025983333587646, loss=2.256314516067505
I0202 08:32:35.174824 139907754342144 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.2900439500808716, loss=2.358938217163086
I0202 08:33:08.781356 139907737556736 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.2220218181610107, loss=2.278217077255249
I0202 08:33:42.323496 139907754342144 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.2712172269821167, loss=2.1741559505462646
I0202 08:34:15.847109 139907737556736 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.4453984498977661, loss=2.252089023590088
I0202 08:34:49.519864 139907754342144 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.1665319204330444, loss=2.2080559730529785
I0202 08:35:23.072510 139907737556736 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.2033545970916748, loss=2.2290713787078857
I0202 08:35:56.594933 139907754342144 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.2209855318069458, loss=2.3159520626068115
I0202 08:36:30.110555 139907737556736 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.237683653831482, loss=2.3660173416137695
I0202 08:37:03.706254 139907754342144 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.3187367916107178, loss=2.11921763420105
I0202 08:37:37.307021 139907737556736 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.1735491752624512, loss=2.1120452880859375
I0202 08:38:10.827732 139907754342144 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.2159767150878906, loss=2.0004022121429443
I0202 08:38:22.038639 140070692116288 spec.py:321] Evaluating on the training split.
I0202 08:38:28.324153 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 08:38:36.867119 140070692116288 spec.py:349] Evaluating on the test split.
I0202 08:38:39.488515 140070692116288 submission_runner.py:408] Time since start: 19577.09s, 	Step: 56135, 	{'train/accuracy': 0.2638711631298065, 'train/loss': 3.8232076168060303, 'validation/accuracy': 0.24963998794555664, 'validation/loss': 3.983182907104492, 'validation/num_examples': 50000, 'test/accuracy': 0.18380001187324524, 'test/loss': 4.662926197052002, 'test/num_examples': 10000, 'score': 18906.143671512604, 'total_duration': 19577.08617591858, 'accumulated_submission_time': 18906.143671512604, 'accumulated_eval_time': 667.7958898544312, 'accumulated_logging_time': 1.19061279296875}
I0202 08:38:39.519199 139908710635264 logging_writer.py:48] [56135] accumulated_eval_time=667.795890, accumulated_logging_time=1.190613, accumulated_submission_time=18906.143672, global_step=56135, preemption_count=0, score=18906.143672, test/accuracy=0.183800, test/loss=4.662926, test/num_examples=10000, total_duration=19577.086176, train/accuracy=0.263871, train/loss=3.823208, validation/accuracy=0.249640, validation/loss=3.983183, validation/num_examples=50000
I0202 08:39:01.658922 139908719027968 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.358278512954712, loss=2.250319480895996
I0202 08:39:35.173181 139908710635264 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.1754497289657593, loss=2.2685627937316895
I0202 08:40:08.708589 139908719027968 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.2814972400665283, loss=2.116631507873535
I0202 08:40:42.317199 139908710635264 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.3143521547317505, loss=2.142806053161621
I0202 08:41:15.884293 139908719027968 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.3325287103652954, loss=2.3160319328308105
I0202 08:41:49.549777 139908710635264 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.4857953786849976, loss=2.0938849449157715
I0202 08:42:23.119223 139908719027968 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.2929871082305908, loss=2.18695068359375
I0202 08:42:56.657797 139908710635264 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.4527348279953003, loss=2.2967092990875244
I0202 08:43:30.195133 139908719027968 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.1576769351959229, loss=2.224273443222046
I0202 08:44:03.807528 139908710635264 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.231033205986023, loss=2.2284932136535645
I0202 08:44:37.356688 139908719027968 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.3654448986053467, loss=2.3122825622558594
I0202 08:45:10.866466 139908710635264 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.2419220209121704, loss=2.2367963790893555
I0202 08:45:44.400948 139908719027968 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.2299455404281616, loss=2.1397950649261475
I0202 08:46:17.917768 139908710635264 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.2479143142700195, loss=2.187120199203491
I0202 08:46:51.456631 139908719027968 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.2815743684768677, loss=2.198788642883301
I0202 08:47:09.758045 140070692116288 spec.py:321] Evaluating on the training split.
I0202 08:47:16.036882 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 08:47:24.463887 140070692116288 spec.py:349] Evaluating on the test split.
I0202 08:47:27.101849 140070692116288 submission_runner.py:408] Time since start: 20104.70s, 	Step: 57656, 	{'train/accuracy': 0.18769928812980652, 'train/loss': 4.809234619140625, 'validation/accuracy': 0.17725999653339386, 'validation/loss': 4.956613540649414, 'validation/num_examples': 50000, 'test/accuracy': 0.13450001180171967, 'test/loss': 5.564505100250244, 'test/num_examples': 10000, 'score': 19416.319585561752, 'total_duration': 20104.69949555397, 'accumulated_submission_time': 19416.319585561752, 'accumulated_eval_time': 685.1396398544312, 'accumulated_logging_time': 1.230280876159668}
I0202 08:47:27.129309 139907754342144 logging_writer.py:48] [57656] accumulated_eval_time=685.139640, accumulated_logging_time=1.230281, accumulated_submission_time=19416.319586, global_step=57656, preemption_count=0, score=19416.319586, test/accuracy=0.134500, test/loss=5.564505, test/num_examples=10000, total_duration=20104.699496, train/accuracy=0.187699, train/loss=4.809235, validation/accuracy=0.177260, validation/loss=4.956614, validation/num_examples=50000
I0202 08:47:42.312002 139907762734848 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.2781274318695068, loss=2.2313549518585205
I0202 08:48:15.819689 139907754342144 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.3305715322494507, loss=2.279660940170288
I0202 08:48:49.315075 139907762734848 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.2822867631912231, loss=2.071398973464966
I0202 08:49:22.915570 139907754342144 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.1766698360443115, loss=2.2101335525512695
I0202 08:49:56.498941 139907762734848 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.2698708772659302, loss=2.2916932106018066
I0202 08:50:30.034880 139907754342144 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.222350001335144, loss=2.1726722717285156
I0202 08:51:03.570830 139907762734848 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.268881916999817, loss=2.161992073059082
I0202 08:51:37.180862 139907754342144 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.2419267892837524, loss=2.2735283374786377
I0202 08:52:10.766789 139907762734848 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.237726092338562, loss=2.296032190322876
I0202 08:52:44.298787 139907754342144 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.2569907903671265, loss=2.229262113571167
I0202 08:53:17.882820 139907762734848 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.370805263519287, loss=2.161924362182617
I0202 08:53:51.436923 139907754342144 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.2595486640930176, loss=2.2433860301971436
I0202 08:54:25.038781 139907762734848 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.21687912940979, loss=2.23067045211792
I0202 08:54:58.559331 139907754342144 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.5498206615447998, loss=2.2741189002990723
I0202 08:55:32.061830 139907762734848 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.340531349182129, loss=2.2992124557495117
I0202 08:55:57.323786 140070692116288 spec.py:321] Evaluating on the training split.
I0202 08:56:03.808037 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 08:56:12.268204 140070692116288 spec.py:349] Evaluating on the test split.
I0202 08:56:14.902100 140070692116288 submission_runner.py:408] Time since start: 20632.50s, 	Step: 59177, 	{'train/accuracy': 0.18211893737316132, 'train/loss': 4.6406402587890625, 'validation/accuracy': 0.1666799932718277, 'validation/loss': 4.858196258544922, 'validation/num_examples': 50000, 'test/accuracy': 0.1234000027179718, 'test/loss': 5.476072311401367, 'test/num_examples': 10000, 'score': 19926.450991630554, 'total_duration': 20632.49976873398, 'accumulated_submission_time': 19926.450991630554, 'accumulated_eval_time': 702.7179343700409, 'accumulated_logging_time': 1.2668704986572266}
I0202 08:56:14.933044 139907745949440 logging_writer.py:48] [59177] accumulated_eval_time=702.717934, accumulated_logging_time=1.266870, accumulated_submission_time=19926.450992, global_step=59177, preemption_count=0, score=19926.450992, test/accuracy=0.123400, test/loss=5.476072, test/num_examples=10000, total_duration=20632.499769, train/accuracy=0.182119, train/loss=4.640640, validation/accuracy=0.166680, validation/loss=4.858196, validation/num_examples=50000
I0202 08:56:22.986647 139908710635264 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.1869940757751465, loss=2.3369929790496826
I0202 08:56:56.472357 139907745949440 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.2048548460006714, loss=2.225144386291504
I0202 08:57:29.937605 139908710635264 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.2467901706695557, loss=2.360832929611206
I0202 08:58:03.438534 139907745949440 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.3031679391860962, loss=2.13427734375
I0202 08:58:37.004163 139908710635264 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.2553521394729614, loss=2.114654779434204
I0202 08:59:10.578506 139907745949440 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.403265357017517, loss=2.233764410018921
I0202 08:59:44.110697 139908710635264 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.2951112985610962, loss=2.282121419906616
I0202 09:00:17.645987 139907745949440 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.2244982719421387, loss=2.0846359729766846
I0202 09:00:51.289753 139908710635264 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.394142746925354, loss=2.211864471435547
I0202 09:01:24.848736 139907745949440 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.2471799850463867, loss=2.2881593704223633
I0202 09:01:58.387096 139908710635264 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.300355315208435, loss=2.0585005283355713
I0202 09:02:31.906666 139907745949440 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.258589267730713, loss=2.3013079166412354
I0202 09:03:05.534834 139908710635264 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.391657829284668, loss=2.160330057144165
I0202 09:03:39.074098 139907745949440 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.242229700088501, loss=2.112384557723999
I0202 09:04:12.643326 139908710635264 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.2131872177124023, loss=2.132756233215332
I0202 09:04:44.984104 140070692116288 spec.py:321] Evaluating on the training split.
I0202 09:04:51.268053 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 09:04:59.634489 140070692116288 spec.py:349] Evaluating on the test split.
I0202 09:05:02.193449 140070692116288 submission_runner.py:408] Time since start: 21159.79s, 	Step: 60698, 	{'train/accuracy': 0.34323182702064514, 'train/loss': 3.1367218494415283, 'validation/accuracy': 0.32367998361587524, 'validation/loss': 3.3062968254089355, 'validation/num_examples': 50000, 'test/accuracy': 0.226500004529953, 'test/loss': 4.168323040008545, 'test/num_examples': 10000, 'score': 20436.43869829178, 'total_duration': 21159.79110598564, 'accumulated_submission_time': 20436.43869829178, 'accumulated_eval_time': 719.9272334575653, 'accumulated_logging_time': 1.3086819648742676}
I0202 09:05:02.223739 139907754342144 logging_writer.py:48] [60698] accumulated_eval_time=719.927233, accumulated_logging_time=1.308682, accumulated_submission_time=20436.438698, global_step=60698, preemption_count=0, score=20436.438698, test/accuracy=0.226500, test/loss=4.168323, test/num_examples=10000, total_duration=21159.791106, train/accuracy=0.343232, train/loss=3.136722, validation/accuracy=0.323680, validation/loss=3.306297, validation/num_examples=50000
I0202 09:05:03.251821 139907762734848 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.222352147102356, loss=1.9583901166915894
I0202 09:05:36.707931 139907754342144 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.3181264400482178, loss=2.270573377609253
I0202 09:06:10.258751 139907762734848 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.221318006515503, loss=2.1097259521484375
I0202 09:06:43.851731 139907754342144 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.2805917263031006, loss=2.3436951637268066
I0202 09:07:17.486185 139907762734848 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.3703879117965698, loss=2.28804874420166
I0202 09:07:51.007113 139907754342144 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.3603965044021606, loss=2.020271062850952
I0202 09:08:24.572715 139907762734848 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.1651880741119385, loss=2.1391842365264893
I0202 09:08:58.178424 139907754342144 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.1840466260910034, loss=2.231863498687744
I0202 09:09:31.696859 139907762734848 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.4145430326461792, loss=2.1515567302703857
I0202 09:10:05.221469 139907754342144 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.1264203786849976, loss=2.151553153991699
I0202 09:10:38.747816 139907762734848 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.298508882522583, loss=2.1358437538146973
I0202 09:11:12.264988 139907754342144 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.1813108921051025, loss=2.1046018600463867
I0202 09:11:45.825166 139907762734848 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.2076025009155273, loss=2.2072696685791016
I0202 09:12:19.393476 139907754342144 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.1788443326950073, loss=2.265798568725586
I0202 09:12:52.955116 139907762734848 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.206182837486267, loss=2.1022846698760986
I0202 09:13:26.639086 139907754342144 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.4446312189102173, loss=2.218275547027588
I0202 09:13:32.502387 140070692116288 spec.py:321] Evaluating on the training split.
I0202 09:13:38.890717 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 09:13:47.323312 140070692116288 spec.py:349] Evaluating on the test split.
I0202 09:13:49.969535 140070692116288 submission_runner.py:408] Time since start: 21687.57s, 	Step: 62219, 	{'train/accuracy': 0.2721022069454193, 'train/loss': 3.687821388244629, 'validation/accuracy': 0.25543999671936035, 'validation/loss': 3.826228618621826, 'validation/num_examples': 50000, 'test/accuracy': 0.18440000712871552, 'test/loss': 4.584669589996338, 'test/num_examples': 10000, 'score': 20946.65442109108, 'total_duration': 21687.56720471382, 'accumulated_submission_time': 20946.65442109108, 'accumulated_eval_time': 737.3943648338318, 'accumulated_logging_time': 1.3493919372558594}
I0202 09:13:49.998251 139907729164032 logging_writer.py:48] [62219] accumulated_eval_time=737.394365, accumulated_logging_time=1.349392, accumulated_submission_time=20946.654421, global_step=62219, preemption_count=0, score=20946.654421, test/accuracy=0.184400, test/loss=4.584670, test/num_examples=10000, total_duration=21687.567205, train/accuracy=0.272102, train/loss=3.687821, validation/accuracy=0.255440, validation/loss=3.826229, validation/num_examples=50000
I0202 09:14:17.478803 139907737556736 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.359922170639038, loss=2.0623698234558105
I0202 09:14:51.004612 139907729164032 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.4721978902816772, loss=2.214650869369507
I0202 09:15:24.556716 139907737556736 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.2991657257080078, loss=2.15520977973938
I0202 09:15:58.092546 139907729164032 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.2954620122909546, loss=2.163566827774048
I0202 09:16:31.704682 139907737556736 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.3054291009902954, loss=2.252439498901367
I0202 09:17:05.264231 139907729164032 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.164050579071045, loss=2.1035351753234863
I0202 09:17:38.781952 139907737556736 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.4374322891235352, loss=2.278602123260498
I0202 09:18:12.327821 139907729164032 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.5023698806762695, loss=2.154677629470825
I0202 09:18:45.936764 139907737556736 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.1921193599700928, loss=2.1964821815490723
I0202 09:19:19.471824 139907729164032 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.2920758724212646, loss=2.0979995727539062
I0202 09:19:53.060043 139907737556736 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.445000410079956, loss=2.241415500640869
I0202 09:20:26.814400 139907729164032 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.3190460205078125, loss=2.1327130794525146
I0202 09:21:00.327525 139907737556736 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.214557409286499, loss=2.032506227493286
I0202 09:21:33.867097 139907729164032 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.3989989757537842, loss=2.279710054397583
I0202 09:22:07.471510 139907737556736 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.397929072380066, loss=2.259443759918213
I0202 09:22:20.021251 140070692116288 spec.py:321] Evaluating on the training split.
I0202 09:22:26.351892 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 09:22:34.951423 140070692116288 spec.py:349] Evaluating on the test split.
I0202 09:22:37.574174 140070692116288 submission_runner.py:408] Time since start: 22215.17s, 	Step: 63739, 	{'train/accuracy': 0.26971060037612915, 'train/loss': 3.7855687141418457, 'validation/accuracy': 0.25672000646591187, 'validation/loss': 3.8716468811035156, 'validation/num_examples': 50000, 'test/accuracy': 0.18940000236034393, 'test/loss': 4.6342620849609375, 'test/num_examples': 10000, 'score': 21456.615305900574, 'total_duration': 22215.171835184097, 'accumulated_submission_time': 21456.615305900574, 'accumulated_eval_time': 754.9472448825836, 'accumulated_logging_time': 1.3876104354858398}
I0202 09:22:37.605300 139908710635264 logging_writer.py:48] [63739] accumulated_eval_time=754.947245, accumulated_logging_time=1.387610, accumulated_submission_time=21456.615306, global_step=63739, preemption_count=0, score=21456.615306, test/accuracy=0.189400, test/loss=4.634262, test/num_examples=10000, total_duration=22215.171835, train/accuracy=0.269711, train/loss=3.785569, validation/accuracy=0.256720, validation/loss=3.871647, validation/num_examples=50000
I0202 09:22:58.398760 139908719027968 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.1688988208770752, loss=2.2190310955047607
I0202 09:23:31.882247 139908710635264 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.2958722114562988, loss=2.1541833877563477
I0202 09:24:05.469483 139908719027968 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.3845077753067017, loss=2.089541435241699
I0202 09:24:39.082964 139908710635264 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.2993189096450806, loss=2.096097469329834
I0202 09:25:12.619125 139908719027968 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.1900224685668945, loss=2.1268608570098877
I0202 09:25:46.149544 139908710635264 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.2540571689605713, loss=2.226593494415283
I0202 09:26:19.704666 139908719027968 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.3742111921310425, loss=2.284754991531372
I0202 09:26:53.368686 139908710635264 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.3000119924545288, loss=2.267432928085327
I0202 09:27:26.953729 139908719027968 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.3484699726104736, loss=2.1260054111480713
I0202 09:28:00.560479 139908710635264 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.2084994316101074, loss=2.095961570739746
I0202 09:28:34.083510 139908719027968 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.3398702144622803, loss=2.2281787395477295
I0202 09:29:07.648514 139908710635264 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.4172658920288086, loss=2.1706864833831787
I0202 09:29:41.167853 139908719027968 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.2698103189468384, loss=2.0703821182250977
I0202 09:30:14.685446 139908710635264 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.483339786529541, loss=2.1646785736083984
I0202 09:30:48.226451 139908719027968 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.3191618919372559, loss=2.141420841217041
I0202 09:31:07.814911 140070692116288 spec.py:321] Evaluating on the training split.
I0202 09:31:14.054969 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 09:31:22.526541 140070692116288 spec.py:349] Evaluating on the test split.
I0202 09:31:25.013508 140070692116288 submission_runner.py:408] Time since start: 22742.61s, 	Step: 65260, 	{'train/accuracy': 0.22847576439380646, 'train/loss': 4.340119361877441, 'validation/accuracy': 0.2192399948835373, 'validation/loss': 4.4335126876831055, 'validation/num_examples': 50000, 'test/accuracy': 0.16040000319480896, 'test/loss': 5.25397253036499, 'test/num_examples': 10000, 'score': 21966.762639045715, 'total_duration': 22742.611171722412, 'accumulated_submission_time': 21966.762639045715, 'accumulated_eval_time': 772.145806312561, 'accumulated_logging_time': 1.428035020828247}
I0202 09:31:25.045452 139907754342144 logging_writer.py:48] [65260] accumulated_eval_time=772.145806, accumulated_logging_time=1.428035, accumulated_submission_time=21966.762639, global_step=65260, preemption_count=0, score=21966.762639, test/accuracy=0.160400, test/loss=5.253973, test/num_examples=10000, total_duration=22742.611172, train/accuracy=0.228476, train/loss=4.340119, validation/accuracy=0.219240, validation/loss=4.433513, validation/num_examples=50000
I0202 09:31:38.835374 139907762734848 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.1584702730178833, loss=2.079707145690918
I0202 09:32:12.369126 139907754342144 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.252102255821228, loss=2.1205031871795654
I0202 09:32:45.907567 139907762734848 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.2641057968139648, loss=2.224318027496338
I0202 09:33:19.515505 139907754342144 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.2801250219345093, loss=2.0651865005493164
I0202 09:33:53.054134 139907762734848 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.2905170917510986, loss=2.191403865814209
I0202 09:34:26.581940 139907754342144 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.2370165586471558, loss=2.1655495166778564
I0202 09:35:00.132769 139907762734848 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.357319951057434, loss=2.1613991260528564
I0202 09:35:33.692490 139907754342144 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.2750625610351562, loss=2.2889745235443115
I0202 09:36:07.228228 139907762734848 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.203477144241333, loss=2.1221773624420166
I0202 09:36:40.784231 139907754342144 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.4512892961502075, loss=2.1506576538085938
I0202 09:37:14.318163 139907762734848 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.3507014513015747, loss=2.066817045211792
I0202 09:37:47.822903 139907754342144 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.3795859813690186, loss=2.1471450328826904
I0202 09:38:21.371530 139907762734848 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.2963995933532715, loss=2.0949110984802246
I0202 09:38:54.894289 139907754342144 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.4208623170852661, loss=2.2102348804473877
I0202 09:39:28.521240 139907762734848 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.3346508741378784, loss=2.20080304145813
I0202 09:39:55.150350 140070692116288 spec.py:321] Evaluating on the training split.
I0202 09:40:01.571012 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 09:40:10.110663 140070692116288 spec.py:349] Evaluating on the test split.
I0202 09:40:12.763445 140070692116288 submission_runner.py:408] Time since start: 23270.36s, 	Step: 66781, 	{'train/accuracy': 0.3475964665412903, 'train/loss': 3.1049602031707764, 'validation/accuracy': 0.33611997961997986, 'validation/loss': 3.2092509269714355, 'validation/num_examples': 50000, 'test/accuracy': 0.25780001282691956, 'test/loss': 3.982248544692993, 'test/num_examples': 10000, 'score': 22476.804877758026, 'total_duration': 23270.36111807823, 'accumulated_submission_time': 22476.804877758026, 'accumulated_eval_time': 789.7588932514191, 'accumulated_logging_time': 1.4698281288146973}
I0202 09:40:12.794503 139907745949440 logging_writer.py:48] [66781] accumulated_eval_time=789.758893, accumulated_logging_time=1.469828, accumulated_submission_time=22476.804878, global_step=66781, preemption_count=0, score=22476.804878, test/accuracy=0.257800, test/loss=3.982249, test/num_examples=10000, total_duration=23270.361118, train/accuracy=0.347596, train/loss=3.104960, validation/accuracy=0.336120, validation/loss=3.209251, validation/num_examples=50000
I0202 09:40:19.492324 139908710635264 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.30837881565094, loss=2.0615687370300293
I0202 09:40:53.005415 139907745949440 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.2909884452819824, loss=2.142148733139038
I0202 09:41:26.513070 139908710635264 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.4364380836486816, loss=2.197770357131958
I0202 09:42:00.101708 139907745949440 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.242397665977478, loss=2.1128108501434326
I0202 09:42:33.674968 139908710635264 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.3421260118484497, loss=2.106710910797119
I0202 09:43:07.184642 139907745949440 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.5425236225128174, loss=2.2110586166381836
I0202 09:43:40.725221 139908710635264 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.392175316810608, loss=2.1555747985839844
I0202 09:44:14.263679 139907745949440 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.3271106481552124, loss=2.1539173126220703
I0202 09:44:47.847392 139908710635264 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.209228515625, loss=2.088864803314209
I0202 09:45:21.354261 139907745949440 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.339205265045166, loss=2.1939632892608643
I0202 09:45:54.960671 139908710635264 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.5660905838012695, loss=2.1548995971679688
I0202 09:46:28.527291 139907745949440 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.3881237506866455, loss=2.2195217609405518
I0202 09:47:02.115690 139908710635264 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.2626012563705444, loss=2.074751377105713
I0202 09:47:35.621069 139907745949440 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.345119595527649, loss=2.2508373260498047
I0202 09:48:09.191525 139908710635264 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.2794793844223022, loss=2.144406318664551
I0202 09:48:42.700100 139907745949440 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.4637340307235718, loss=2.2278237342834473
I0202 09:48:42.863868 140070692116288 spec.py:321] Evaluating on the training split.
I0202 09:48:49.192122 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 09:48:57.769802 140070692116288 spec.py:349] Evaluating on the test split.
I0202 09:49:00.498842 140070692116288 submission_runner.py:408] Time since start: 23798.10s, 	Step: 68302, 	{'train/accuracy': 0.351283460855484, 'train/loss': 3.137704372406006, 'validation/accuracy': 0.32259997725486755, 'validation/loss': 3.393313407897949, 'validation/num_examples': 50000, 'test/accuracy': 0.23590001463890076, 'test/loss': 4.1841864585876465, 'test/num_examples': 10000, 'score': 22986.811591625214, 'total_duration': 23798.096511363983, 'accumulated_submission_time': 22986.811591625214, 'accumulated_eval_time': 807.3938567638397, 'accumulated_logging_time': 1.5102369785308838}
I0202 09:49:00.527755 139907729164032 logging_writer.py:48] [68302] accumulated_eval_time=807.393857, accumulated_logging_time=1.510237, accumulated_submission_time=22986.811592, global_step=68302, preemption_count=0, score=22986.811592, test/accuracy=0.235900, test/loss=4.184186, test/num_examples=10000, total_duration=23798.096511, train/accuracy=0.351283, train/loss=3.137704, validation/accuracy=0.322600, validation/loss=3.393313, validation/num_examples=50000
I0202 09:49:33.698026 139907754342144 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.285274863243103, loss=2.0906684398651123
I0202 09:50:07.237155 139907729164032 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.395111083984375, loss=2.210967540740967
I0202 09:50:40.780458 139907754342144 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.326431155204773, loss=2.0542306900024414
I0202 09:51:14.382133 139907729164032 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.3812215328216553, loss=2.0802087783813477
I0202 09:51:47.945329 139907754342144 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.2979810237884521, loss=2.136079788208008
I0202 09:52:21.569407 139907729164032 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.3102998733520508, loss=2.2561802864074707
I0202 09:52:55.118130 139907754342144 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.4532195329666138, loss=2.2518043518066406
I0202 09:53:28.641686 139907729164032 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.3269394636154175, loss=2.29789662361145
I0202 09:54:02.211527 139907754342144 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.4098176956176758, loss=2.156414270401001
I0202 09:54:35.828743 139907729164032 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.389827013015747, loss=2.221827268600464
I0202 09:55:09.394346 139907754342144 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.419478178024292, loss=2.14144229888916
I0202 09:55:42.920333 139907729164032 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.263400673866272, loss=2.1165294647216797
I0202 09:56:16.461979 139907754342144 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.306628704071045, loss=2.0218160152435303
I0202 09:56:50.050935 139907729164032 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.3016928434371948, loss=2.051375150680542
I0202 09:57:23.574457 139907754342144 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.5344946384429932, loss=2.1307661533355713
I0202 09:57:30.771918 140070692116288 spec.py:321] Evaluating on the training split.
I0202 09:57:37.114157 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 09:57:45.607681 140070692116288 spec.py:349] Evaluating on the test split.
I0202 09:57:48.244019 140070692116288 submission_runner.py:408] Time since start: 24325.84s, 	Step: 69823, 	{'train/accuracy': 0.19616948068141937, 'train/loss': 4.434426784515381, 'validation/accuracy': 0.18423999845981598, 'validation/loss': 4.5618109703063965, 'validation/num_examples': 50000, 'test/accuracy': 0.13050000369548798, 'test/loss': 5.301344871520996, 'test/num_examples': 10000, 'score': 23496.993092536926, 'total_duration': 24325.841685056686, 'accumulated_submission_time': 23496.993092536926, 'accumulated_eval_time': 824.8659207820892, 'accumulated_logging_time': 1.5485587120056152}
I0202 09:57:48.272985 139908710635264 logging_writer.py:48] [69823] accumulated_eval_time=824.865921, accumulated_logging_time=1.548559, accumulated_submission_time=23496.993093, global_step=69823, preemption_count=0, score=23496.993093, test/accuracy=0.130500, test/loss=5.301345, test/num_examples=10000, total_duration=24325.841685, train/accuracy=0.196169, train/loss=4.434427, validation/accuracy=0.184240, validation/loss=4.561811, validation/num_examples=50000
I0202 09:58:14.388172 139908719027968 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.3709322214126587, loss=2.1514482498168945
I0202 09:58:47.963128 139908710635264 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.2112319469451904, loss=2.0297141075134277
I0202 09:59:21.545259 139908719027968 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.3268038034439087, loss=2.1195905208587646
I0202 09:59:55.041336 139908710635264 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.3208528757095337, loss=2.1411004066467285
I0202 10:00:28.581467 139908719027968 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.2685177326202393, loss=2.19140887260437
I0202 10:01:02.130922 139908710635264 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.264501929283142, loss=2.10170578956604
I0202 10:01:35.687084 139908719027968 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.4890280961990356, loss=2.180509567260742
I0202 10:02:09.229989 139908710635264 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.3353190422058105, loss=2.1832334995269775
I0202 10:02:42.762676 139908719027968 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.3259490728378296, loss=2.128024101257324
I0202 10:03:16.301537 139908710635264 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.2491518259048462, loss=2.164736270904541
I0202 10:03:49.858141 139908719027968 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.4098464250564575, loss=2.0715231895446777
I0202 10:04:23.418498 139908710635264 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.367838740348816, loss=2.1905899047851562
I0202 10:04:57.055409 139908719027968 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.3101731538772583, loss=2.0287106037139893
I0202 10:05:30.672647 139908710635264 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.2587581872940063, loss=2.091280698776245
I0202 10:06:04.202990 139908719027968 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.4220569133758545, loss=2.0445313453674316
I0202 10:06:18.435998 140070692116288 spec.py:321] Evaluating on the training split.
I0202 10:06:24.703596 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 10:06:33.207167 140070692116288 spec.py:349] Evaluating on the test split.
I0202 10:06:35.850965 140070692116288 submission_runner.py:408] Time since start: 24853.45s, 	Step: 71344, 	{'train/accuracy': 0.30813536047935486, 'train/loss': 3.636435031890869, 'validation/accuracy': 0.27733999490737915, 'validation/loss': 3.8866045475006104, 'validation/num_examples': 50000, 'test/accuracy': 0.21320000290870667, 'test/loss': 4.583950042724609, 'test/num_examples': 10000, 'score': 24007.092417240143, 'total_duration': 24853.448628664017, 'accumulated_submission_time': 24007.092417240143, 'accumulated_eval_time': 842.2808480262756, 'accumulated_logging_time': 1.5876126289367676}
I0202 10:06:35.880329 139907754342144 logging_writer.py:48] [71344] accumulated_eval_time=842.280848, accumulated_logging_time=1.587613, accumulated_submission_time=24007.092417, global_step=71344, preemption_count=0, score=24007.092417, test/accuracy=0.213200, test/loss=4.583950, test/num_examples=10000, total_duration=24853.448629, train/accuracy=0.308135, train/loss=3.636435, validation/accuracy=0.277340, validation/loss=3.886605, validation/num_examples=50000
I0202 10:06:54.980800 139907762734848 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.320290446281433, loss=2.2148349285125732
I0202 10:07:28.498353 139907754342144 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.3484480381011963, loss=2.0698647499084473
I0202 10:08:02.006313 139907762734848 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.4231712818145752, loss=2.2718818187713623
I0202 10:08:35.539260 139907754342144 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.385202169418335, loss=2.1348135471343994
I0202 10:09:09.120855 139907762734848 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.4719655513763428, loss=2.193101167678833
I0202 10:09:42.638293 139907754342144 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.3386292457580566, loss=2.1079330444335938
I0202 10:10:16.229847 139907762734848 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.4188039302825928, loss=2.2081222534179688
I0202 10:10:49.751759 139907754342144 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.3396638631820679, loss=2.064422607421875
I0202 10:11:23.363887 139907762734848 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.3547474145889282, loss=2.0346341133117676
I0202 10:11:56.899204 139907754342144 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.4273321628570557, loss=2.1661059856414795
I0202 10:12:30.422807 139907762734848 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.4070944786071777, loss=2.1512362957000732
I0202 10:13:03.956008 139907754342144 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.3251112699508667, loss=2.107830286026001
I0202 10:13:37.537341 139907762734848 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.4054943323135376, loss=2.22015643119812
I0202 10:14:11.064927 139907754342144 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.3931927680969238, loss=2.315504550933838
I0202 10:14:44.659398 139907762734848 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.314474105834961, loss=2.1902194023132324
I0202 10:15:05.951560 140070692116288 spec.py:321] Evaluating on the training split.
I0202 10:15:12.183287 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 10:15:20.577669 140070692116288 spec.py:349] Evaluating on the test split.
I0202 10:15:23.235582 140070692116288 submission_runner.py:408] Time since start: 25380.83s, 	Step: 72865, 	{'train/accuracy': 0.27214205265045166, 'train/loss': 3.783998489379883, 'validation/accuracy': 0.25863999128341675, 'validation/loss': 3.9203364849090576, 'validation/num_examples': 50000, 'test/accuracy': 0.1842000037431717, 'test/loss': 4.706700325012207, 'test/num_examples': 10000, 'score': 24517.101637125015, 'total_duration': 25380.833248138428, 'accumulated_submission_time': 24517.101637125015, 'accumulated_eval_time': 859.5648393630981, 'accumulated_logging_time': 1.626474142074585}
I0202 10:15:23.265641 139908710635264 logging_writer.py:48] [72865] accumulated_eval_time=859.564839, accumulated_logging_time=1.626474, accumulated_submission_time=24517.101637, global_step=72865, preemption_count=0, score=24517.101637, test/accuracy=0.184200, test/loss=4.706700, test/num_examples=10000, total_duration=25380.833248, train/accuracy=0.272142, train/loss=3.783998, validation/accuracy=0.258640, validation/loss=3.920336, validation/num_examples=50000
I0202 10:15:35.332236 139908719027968 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.5091931819915771, loss=2.163633346557617
I0202 10:16:08.810705 139908710635264 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.3596466779708862, loss=2.0669138431549072
I0202 10:16:42.316588 139908719027968 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.4177258014678955, loss=2.139383316040039
I0202 10:17:15.837056 139908710635264 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.4088988304138184, loss=2.076772451400757
I0202 10:17:49.451648 139908719027968 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.3442119359970093, loss=2.1638622283935547
I0202 10:18:23.044361 139908710635264 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.4413459300994873, loss=2.070244073867798
I0202 10:18:56.534531 139908719027968 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.5427582263946533, loss=2.1452231407165527
I0202 10:19:30.077877 139908710635264 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.375596523284912, loss=2.1072940826416016
I0202 10:20:03.621500 139908719027968 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.320261001586914, loss=2.1070659160614014
I0202 10:20:37.145142 139908710635264 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.3287056684494019, loss=2.1957898139953613
I0202 10:21:10.718316 139908719027968 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.3073111772537231, loss=2.138066530227661
I0202 10:21:44.235270 139908710635264 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.4837603569030762, loss=2.0449578762054443
I0202 10:22:17.767620 139908719027968 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.4511213302612305, loss=2.2627458572387695
I0202 10:22:51.283447 139908710635264 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.3396544456481934, loss=2.0072860717773438
I0202 10:23:24.816321 139908719027968 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.4406684637069702, loss=2.100004196166992
I0202 10:23:53.489859 140070692116288 spec.py:321] Evaluating on the training split.
I0202 10:23:59.730339 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 10:24:08.553542 140070692116288 spec.py:349] Evaluating on the test split.
I0202 10:24:12.138785 140070692116288 submission_runner.py:408] Time since start: 25909.74s, 	Step: 74387, 	{'train/accuracy': 0.40790417790412903, 'train/loss': 2.6916909217834473, 'validation/accuracy': 0.38402000069618225, 'validation/loss': 2.868121385574341, 'validation/num_examples': 50000, 'test/accuracy': 0.28690001368522644, 'test/loss': 3.6172990798950195, 'test/num_examples': 10000, 'score': 25027.26301074028, 'total_duration': 25909.73645925522, 'accumulated_submission_time': 25027.26301074028, 'accumulated_eval_time': 878.2137405872345, 'accumulated_logging_time': 1.6661872863769531}
I0202 10:24:12.164135 139907745949440 logging_writer.py:48] [74387] accumulated_eval_time=878.213741, accumulated_logging_time=1.666187, accumulated_submission_time=25027.263011, global_step=74387, preemption_count=0, score=25027.263011, test/accuracy=0.286900, test/loss=3.617299, test/num_examples=10000, total_duration=25909.736459, train/accuracy=0.407904, train/loss=2.691691, validation/accuracy=0.384020, validation/loss=2.868121, validation/num_examples=50000
I0202 10:24:16.856904 139907754342144 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.3436284065246582, loss=1.9887583255767822
I0202 10:24:50.342571 139907745949440 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.3217114210128784, loss=2.1085917949676514
I0202 10:25:23.860657 139907754342144 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.5579066276550293, loss=2.190063238143921
I0202 10:25:57.462030 139907745949440 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.3907924890518188, loss=2.200427532196045
I0202 10:26:31.004070 139907754342144 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.3712546825408936, loss=2.233715057373047
I0202 10:27:04.605201 139907745949440 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.4220858812332153, loss=2.125544786453247
I0202 10:27:38.144519 139907754342144 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.3314858675003052, loss=2.257962465286255
I0202 10:28:11.673063 139907745949440 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.3769468069076538, loss=2.1818976402282715
I0202 10:28:45.218694 139907754342144 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.6036680936813354, loss=2.0944347381591797
I0202 10:29:18.830832 139907745949440 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.4777575731277466, loss=2.131235122680664
I0202 10:29:52.358868 139907754342144 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.4284749031066895, loss=2.1131234169006348
I0202 10:30:25.922686 139907745949440 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.4900120496749878, loss=2.2453742027282715
I0202 10:30:59.570138 139907754342144 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.410388469696045, loss=2.156721591949463
I0202 10:31:33.121845 139907745949440 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.3534409999847412, loss=2.132554292678833
I0202 10:32:06.641106 139907754342144 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.3828275203704834, loss=2.1255085468292236
I0202 10:32:40.255099 139907745949440 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.2740334272384644, loss=1.9988296031951904
I0202 10:32:42.418747 140070692116288 spec.py:321] Evaluating on the training split.
I0202 10:32:48.789237 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 10:32:57.474214 140070692116288 spec.py:349] Evaluating on the test split.
I0202 10:32:59.989372 140070692116288 submission_runner.py:408] Time since start: 26437.59s, 	Step: 75908, 	{'train/accuracy': 0.4138033986091614, 'train/loss': 2.737895965576172, 'validation/accuracy': 0.3630799949169159, 'validation/loss': 3.0857176780700684, 'validation/num_examples': 50000, 'test/accuracy': 0.27140000462532043, 'test/loss': 3.899897336959839, 'test/num_examples': 10000, 'score': 25537.45592713356, 'total_duration': 26437.587026834488, 'accumulated_submission_time': 25537.45592713356, 'accumulated_eval_time': 895.784318447113, 'accumulated_logging_time': 1.700392484664917}
I0202 10:33:00.019381 139908710635264 logging_writer.py:48] [75908] accumulated_eval_time=895.784318, accumulated_logging_time=1.700392, accumulated_submission_time=25537.455927, global_step=75908, preemption_count=0, score=25537.455927, test/accuracy=0.271400, test/loss=3.899897, test/num_examples=10000, total_duration=26437.587027, train/accuracy=0.413803, train/loss=2.737896, validation/accuracy=0.363080, validation/loss=3.085718, validation/num_examples=50000
I0202 10:33:31.176628 139908719027968 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.4390590190887451, loss=2.1762259006500244
I0202 10:34:04.697041 139908710635264 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.3696988821029663, loss=2.011660575866699
I0202 10:34:38.291095 139908719027968 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.7076319456100464, loss=2.301189661026001
I0202 10:35:11.874219 139908710635264 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.4852720499038696, loss=2.091937780380249
I0202 10:35:45.400453 139908719027968 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.4005464315414429, loss=2.0705318450927734
I0202 10:36:18.930788 139908710635264 logging_writer.py:48] [76500] global_step=76500, grad_norm=1.4681663513183594, loss=2.1474900245666504
I0202 10:36:52.538668 139908719027968 logging_writer.py:48] [76600] global_step=76600, grad_norm=1.5382357835769653, loss=2.149533271789551
I0202 10:37:26.193186 139908710635264 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.357746958732605, loss=2.0650787353515625
I0202 10:37:59.710639 139908719027968 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.3919103145599365, loss=2.194432020187378
I0202 10:38:33.240049 139908710635264 logging_writer.py:48] [76900] global_step=76900, grad_norm=1.4363516569137573, loss=2.098212957382202
I0202 10:39:06.817784 139908719027968 logging_writer.py:48] [77000] global_step=77000, grad_norm=1.3583060503005981, loss=2.0825493335723877
I0202 10:39:40.367737 139908710635264 logging_writer.py:48] [77100] global_step=77100, grad_norm=1.4040008783340454, loss=2.1339879035949707
I0202 10:40:13.899126 139908719027968 logging_writer.py:48] [77200] global_step=77200, grad_norm=1.3625752925872803, loss=2.0124495029449463
I0202 10:40:47.478368 139908710635264 logging_writer.py:48] [77300] global_step=77300, grad_norm=1.3971879482269287, loss=2.203479766845703
I0202 10:41:21.010135 139908719027968 logging_writer.py:48] [77400] global_step=77400, grad_norm=1.338837742805481, loss=2.059337615966797
I0202 10:41:30.210091 140070692116288 spec.py:321] Evaluating on the training split.
I0202 10:41:36.461545 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 10:41:45.114214 140070692116288 spec.py:349] Evaluating on the test split.
I0202 10:41:47.738105 140070692116288 submission_runner.py:408] Time since start: 26965.34s, 	Step: 77429, 	{'train/accuracy': 0.3894292116165161, 'train/loss': 2.835314989089966, 'validation/accuracy': 0.3652399778366089, 'validation/loss': 3.049356460571289, 'validation/num_examples': 50000, 'test/accuracy': 0.2776000201702118, 'test/loss': 3.8139333724975586, 'test/num_examples': 10000, 'score': 26047.585062265396, 'total_duration': 26965.335773706436, 'accumulated_submission_time': 26047.585062265396, 'accumulated_eval_time': 913.3123028278351, 'accumulated_logging_time': 1.7399368286132812}
I0202 10:41:47.768655 139907737556736 logging_writer.py:48] [77429] accumulated_eval_time=913.312303, accumulated_logging_time=1.739937, accumulated_submission_time=26047.585062, global_step=77429, preemption_count=0, score=26047.585062, test/accuracy=0.277600, test/loss=3.813933, test/num_examples=10000, total_duration=26965.335774, train/accuracy=0.389429, train/loss=2.835315, validation/accuracy=0.365240, validation/loss=3.049356, validation/num_examples=50000
I0202 10:42:11.894718 139907745949440 logging_writer.py:48] [77500] global_step=77500, grad_norm=1.293607473373413, loss=2.0436861515045166
I0202 10:42:45.374912 139907737556736 logging_writer.py:48] [77600] global_step=77600, grad_norm=1.423300862312317, loss=2.2204251289367676
I0202 10:43:18.900376 139907745949440 logging_writer.py:48] [77700] global_step=77700, grad_norm=1.4229615926742554, loss=2.0851316452026367
I0202 10:43:52.530262 139907737556736 logging_writer.py:48] [77800] global_step=77800, grad_norm=1.3407057523727417, loss=1.928743600845337
I0202 10:44:26.043581 139907745949440 logging_writer.py:48] [77900] global_step=77900, grad_norm=1.441231608390808, loss=2.125262975692749
I0202 10:44:59.638392 139907737556736 logging_writer.py:48] [78000] global_step=78000, grad_norm=1.378548264503479, loss=2.07462739944458
I0202 10:45:33.172266 139907745949440 logging_writer.py:48] [78100] global_step=78100, grad_norm=1.3662371635437012, loss=2.0846264362335205
I0202 10:46:06.752418 139907737556736 logging_writer.py:48] [78200] global_step=78200, grad_norm=1.4426642656326294, loss=2.1127474308013916
I0202 10:46:40.287732 139907745949440 logging_writer.py:48] [78300] global_step=78300, grad_norm=1.440229058265686, loss=2.0522828102111816
I0202 10:47:13.802555 139907737556736 logging_writer.py:48] [78400] global_step=78400, grad_norm=1.412536859512329, loss=2.16662335395813
I0202 10:47:47.317211 139907745949440 logging_writer.py:48] [78500] global_step=78500, grad_norm=1.363214135169983, loss=2.101109504699707
I0202 10:48:20.889491 139907737556736 logging_writer.py:48] [78600] global_step=78600, grad_norm=1.3943750858306885, loss=2.0234873294830322
I0202 10:48:54.461017 139907745949440 logging_writer.py:48] [78700] global_step=78700, grad_norm=1.4038556814193726, loss=2.047358512878418
I0202 10:49:27.992859 139907737556736 logging_writer.py:48] [78800] global_step=78800, grad_norm=1.3984894752502441, loss=2.1723012924194336
I0202 10:50:01.601058 139907745949440 logging_writer.py:48] [78900] global_step=78900, grad_norm=1.4866598844528198, loss=2.1922152042388916
I0202 10:50:17.859563 140070692116288 spec.py:321] Evaluating on the training split.
I0202 10:50:24.242269 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 10:50:32.721679 140070692116288 spec.py:349] Evaluating on the test split.
I0202 10:50:35.317486 140070692116288 submission_runner.py:408] Time since start: 27492.92s, 	Step: 78950, 	{'train/accuracy': 0.3610690236091614, 'train/loss': 3.041414499282837, 'validation/accuracy': 0.3311599791049957, 'validation/loss': 3.252563238143921, 'validation/num_examples': 50000, 'test/accuracy': 0.24700000882148743, 'test/loss': 3.9893369674682617, 'test/num_examples': 10000, 'score': 26557.61221885681, 'total_duration': 27492.91514992714, 'accumulated_submission_time': 26557.61221885681, 'accumulated_eval_time': 930.7702312469482, 'accumulated_logging_time': 1.7816412448883057}
I0202 10:50:35.356395 139908425447168 logging_writer.py:48] [78950] accumulated_eval_time=930.770231, accumulated_logging_time=1.781641, accumulated_submission_time=26557.612219, global_step=78950, preemption_count=0, score=26557.612219, test/accuracy=0.247000, test/loss=3.989337, test/num_examples=10000, total_duration=27492.915150, train/accuracy=0.361069, train/loss=3.041414, validation/accuracy=0.331160, validation/loss=3.252563, validation/num_examples=50000
I0202 10:50:52.450834 139908710635264 logging_writer.py:48] [79000] global_step=79000, grad_norm=1.3286542892456055, loss=2.094895362854004
I0202 10:51:25.924144 139908425447168 logging_writer.py:48] [79100] global_step=79100, grad_norm=1.21628737449646, loss=1.9832377433776855
I0202 10:51:59.451383 139908710635264 logging_writer.py:48] [79200] global_step=79200, grad_norm=1.4294421672821045, loss=1.9310537576675415
I0202 10:52:32.985122 139908425447168 logging_writer.py:48] [79300] global_step=79300, grad_norm=1.5239932537078857, loss=2.3147406578063965
I0202 10:53:06.533224 139908710635264 logging_writer.py:48] [79400] global_step=79400, grad_norm=1.297311782836914, loss=2.04251766204834
I0202 10:53:40.106678 139908425447168 logging_writer.py:48] [79500] global_step=79500, grad_norm=1.3797829151153564, loss=2.0152993202209473
I0202 10:54:13.686779 139908710635264 logging_writer.py:48] [79600] global_step=79600, grad_norm=1.6445037126541138, loss=1.995730996131897
I0202 10:54:47.193170 139908425447168 logging_writer.py:48] [79700] global_step=79700, grad_norm=1.4715238809585571, loss=2.096501588821411
I0202 10:55:20.741521 139908710635264 logging_writer.py:48] [79800] global_step=79800, grad_norm=1.4554738998413086, loss=2.1195240020751953
I0202 10:55:54.274001 139908425447168 logging_writer.py:48] [79900] global_step=79900, grad_norm=1.4235846996307373, loss=2.1171071529388428
I0202 10:56:27.868075 139908710635264 logging_writer.py:48] [80000] global_step=80000, grad_norm=1.4306687116622925, loss=2.104517698287964
I0202 10:57:01.443364 139908425447168 logging_writer.py:48] [80100] global_step=80100, grad_norm=1.3410301208496094, loss=2.115220069885254
I0202 10:57:34.969681 139908710635264 logging_writer.py:48] [80200] global_step=80200, grad_norm=1.38863205909729, loss=2.1180481910705566
I0202 10:58:08.548758 139908425447168 logging_writer.py:48] [80300] global_step=80300, grad_norm=1.4087189435958862, loss=2.133864641189575
I0202 10:58:42.055087 139908710635264 logging_writer.py:48] [80400] global_step=80400, grad_norm=1.3892191648483276, loss=2.13893985748291
I0202 10:59:05.363986 140070692116288 spec.py:321] Evaluating on the training split.
I0202 10:59:11.879707 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 10:59:20.583142 140070692116288 spec.py:349] Evaluating on the test split.
I0202 10:59:23.100750 140070692116288 submission_runner.py:408] Time since start: 28020.70s, 	Step: 80471, 	{'train/accuracy': 0.3260921537876129, 'train/loss': 3.3770241737365723, 'validation/accuracy': 0.3070800006389618, 'validation/loss': 3.5291993618011475, 'validation/num_examples': 50000, 'test/accuracy': 0.22200001776218414, 'test/loss': 4.42165994644165, 'test/num_examples': 10000, 'score': 27067.556453704834, 'total_duration': 28020.69840979576, 'accumulated_submission_time': 27067.556453704834, 'accumulated_eval_time': 948.5069932937622, 'accumulated_logging_time': 1.830293893814087}
I0202 10:59:23.132774 139907729164032 logging_writer.py:48] [80471] accumulated_eval_time=948.506993, accumulated_logging_time=1.830294, accumulated_submission_time=27067.556454, global_step=80471, preemption_count=0, score=27067.556454, test/accuracy=0.222000, test/loss=4.421660, test/num_examples=10000, total_duration=28020.698410, train/accuracy=0.326092, train/loss=3.377024, validation/accuracy=0.307080, validation/loss=3.529199, validation/num_examples=50000
I0202 10:59:33.214002 139907737556736 logging_writer.py:48] [80500] global_step=80500, grad_norm=1.3626457452774048, loss=2.1293888092041016
I0202 11:00:06.725273 139907729164032 logging_writer.py:48] [80600] global_step=80600, grad_norm=1.432781457901001, loss=2.0079617500305176
I0202 11:00:40.333277 139907737556736 logging_writer.py:48] [80700] global_step=80700, grad_norm=1.406578779220581, loss=2.0517966747283936
I0202 11:01:13.845026 139907729164032 logging_writer.py:48] [80800] global_step=80800, grad_norm=1.5884573459625244, loss=2.1947107315063477
I0202 11:01:47.417691 139907737556736 logging_writer.py:48] [80900] global_step=80900, grad_norm=1.368760108947754, loss=2.0834038257598877
I0202 11:02:20.957201 139907729164032 logging_writer.py:48] [81000] global_step=81000, grad_norm=1.4126685857772827, loss=2.1461901664733887
I0202 11:02:54.742312 139907737556736 logging_writer.py:48] [81100] global_step=81100, grad_norm=1.8260866403579712, loss=2.167985200881958
I0202 11:03:28.313167 139907729164032 logging_writer.py:48] [81200] global_step=81200, grad_norm=1.7200571298599243, loss=2.1359400749206543
I0202 11:04:01.877688 139907737556736 logging_writer.py:48] [81300] global_step=81300, grad_norm=1.623235821723938, loss=2.1350793838500977
I0202 11:04:35.413429 139907729164032 logging_writer.py:48] [81400] global_step=81400, grad_norm=1.4461182355880737, loss=2.150413751602173
I0202 11:05:09.008078 139907737556736 logging_writer.py:48] [81500] global_step=81500, grad_norm=1.524019479751587, loss=2.05336332321167
I0202 11:05:42.524650 139907729164032 logging_writer.py:48] [81600] global_step=81600, grad_norm=1.3881793022155762, loss=1.9693031311035156
I0202 11:06:16.095729 139907737556736 logging_writer.py:48] [81700] global_step=81700, grad_norm=1.4183706045150757, loss=2.1341347694396973
I0202 11:06:49.627231 139907729164032 logging_writer.py:48] [81800] global_step=81800, grad_norm=1.4902753829956055, loss=2.08668851852417
I0202 11:07:23.178659 139907737556736 logging_writer.py:48] [81900] global_step=81900, grad_norm=1.5028882026672363, loss=2.0256078243255615
I0202 11:07:53.180247 140070692116288 spec.py:321] Evaluating on the training split.
I0202 11:07:59.441378 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 11:08:08.350146 140070692116288 spec.py:349] Evaluating on the test split.
I0202 11:08:11.025571 140070692116288 submission_runner.py:408] Time since start: 28548.62s, 	Step: 81991, 	{'train/accuracy': 0.375019907951355, 'train/loss': 2.999455213546753, 'validation/accuracy': 0.35001999139785767, 'validation/loss': 3.140925168991089, 'validation/num_examples': 50000, 'test/accuracy': 0.25780001282691956, 'test/loss': 3.9556169509887695, 'test/num_examples': 10000, 'score': 27577.539540290833, 'total_duration': 28548.623241901398, 'accumulated_submission_time': 27577.539540290833, 'accumulated_eval_time': 966.3523087501526, 'accumulated_logging_time': 1.8731324672698975}
I0202 11:08:11.053175 139908710635264 logging_writer.py:48] [81991] accumulated_eval_time=966.352309, accumulated_logging_time=1.873132, accumulated_submission_time=27577.539540, global_step=81991, preemption_count=0, score=27577.539540, test/accuracy=0.257800, test/loss=3.955617, test/num_examples=10000, total_duration=28548.623242, train/accuracy=0.375020, train/loss=2.999455, validation/accuracy=0.350020, validation/loss=3.140925, validation/num_examples=50000
I0202 11:08:14.413666 139908719027968 logging_writer.py:48] [82000] global_step=82000, grad_norm=1.352858066558838, loss=1.9682185649871826
I0202 11:08:47.914665 139908710635264 logging_writer.py:48] [82100] global_step=82100, grad_norm=1.3914175033569336, loss=2.177615165710449
I0202 11:09:21.607406 139908719027968 logging_writer.py:48] [82200] global_step=82200, grad_norm=1.4130585193634033, loss=2.0812056064605713
I0202 11:09:55.127044 139908710635264 logging_writer.py:48] [82300] global_step=82300, grad_norm=1.4123587608337402, loss=2.081209182739258
I0202 11:10:28.726924 139908719027968 logging_writer.py:48] [82400] global_step=82400, grad_norm=1.6191692352294922, loss=2.0035791397094727
I0202 11:11:02.237433 139908710635264 logging_writer.py:48] [82500] global_step=82500, grad_norm=1.3500500917434692, loss=2.037142515182495
I0202 11:11:35.804156 139908719027968 logging_writer.py:48] [82600] global_step=82600, grad_norm=1.6093676090240479, loss=2.139400005340576
I0202 11:12:09.342796 139908710635264 logging_writer.py:48] [82700] global_step=82700, grad_norm=1.358837604522705, loss=2.0615546703338623
I0202 11:12:42.928653 139908719027968 logging_writer.py:48] [82800] global_step=82800, grad_norm=1.460726022720337, loss=2.184333562850952
I0202 11:13:16.497750 139908710635264 logging_writer.py:48] [82900] global_step=82900, grad_norm=1.771629810333252, loss=2.0797808170318604
I0202 11:13:50.028859 139908719027968 logging_writer.py:48] [83000] global_step=83000, grad_norm=1.5332603454589844, loss=2.1550066471099854
I0202 11:14:23.562425 139908710635264 logging_writer.py:48] [83100] global_step=83100, grad_norm=1.5481888055801392, loss=2.154308795928955
I0202 11:14:57.133703 139908719027968 logging_writer.py:48] [83200] global_step=83200, grad_norm=1.3409475088119507, loss=1.982056975364685
I0202 11:15:30.681407 139908710635264 logging_writer.py:48] [83300] global_step=83300, grad_norm=1.4718374013900757, loss=2.0595412254333496
I0202 11:16:04.273068 139908719027968 logging_writer.py:48] [83400] global_step=83400, grad_norm=1.426052212715149, loss=2.035553455352783
I0202 11:16:37.791448 139908710635264 logging_writer.py:48] [83500] global_step=83500, grad_norm=1.3722715377807617, loss=2.0555319786071777
I0202 11:16:41.299936 140070692116288 spec.py:321] Evaluating on the training split.
I0202 11:16:47.618035 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 11:16:56.092687 140070692116288 spec.py:349] Evaluating on the test split.
I0202 11:16:58.715729 140070692116288 submission_runner.py:408] Time since start: 29076.31s, 	Step: 83512, 	{'train/accuracy': 0.4471859037876129, 'train/loss': 2.4820733070373535, 'validation/accuracy': 0.4222399890422821, 'validation/loss': 2.646937131881714, 'validation/num_examples': 50000, 'test/accuracy': 0.32020002603530884, 'test/loss': 3.3737969398498535, 'test/num_examples': 10000, 'score': 28087.726016521454, 'total_duration': 29076.31339788437, 'accumulated_submission_time': 28087.726016521454, 'accumulated_eval_time': 983.7680652141571, 'accumulated_logging_time': 1.908886432647705}
I0202 11:16:58.747276 139907754342144 logging_writer.py:48] [83512] accumulated_eval_time=983.768065, accumulated_logging_time=1.908886, accumulated_submission_time=28087.726017, global_step=83512, preemption_count=0, score=28087.726017, test/accuracy=0.320200, test/loss=3.373797, test/num_examples=10000, total_duration=29076.313398, train/accuracy=0.447186, train/loss=2.482073, validation/accuracy=0.422240, validation/loss=2.646937, validation/num_examples=50000
I0202 11:17:28.560160 139907762734848 logging_writer.py:48] [83600] global_step=83600, grad_norm=1.4377861022949219, loss=2.093808650970459
I0202 11:18:02.057266 139907754342144 logging_writer.py:48] [83700] global_step=83700, grad_norm=1.3982223272323608, loss=2.1058242321014404
I0202 11:18:35.604927 139907762734848 logging_writer.py:48] [83800] global_step=83800, grad_norm=1.7688781023025513, loss=2.1035242080688477
I0202 11:19:09.151592 139907754342144 logging_writer.py:48] [83900] global_step=83900, grad_norm=1.4693955183029175, loss=2.063584804534912
I0202 11:19:42.727759 139907762734848 logging_writer.py:48] [84000] global_step=84000, grad_norm=1.4126484394073486, loss=2.153388023376465
I0202 11:20:16.247593 139907754342144 logging_writer.py:48] [84100] global_step=84100, grad_norm=1.4003974199295044, loss=2.1397435665130615
I0202 11:20:49.874847 139907762734848 logging_writer.py:48] [84200] global_step=84200, grad_norm=1.5416010618209839, loss=2.0072906017303467
I0202 11:21:23.417640 139907754342144 logging_writer.py:48] [84300] global_step=84300, grad_norm=1.499380111694336, loss=1.9931217432022095
I0202 11:21:56.919222 139907762734848 logging_writer.py:48] [84400] global_step=84400, grad_norm=1.5347421169281006, loss=2.1169650554656982
I0202 11:22:30.533000 139907754342144 logging_writer.py:48] [84500] global_step=84500, grad_norm=1.5241367816925049, loss=2.033360242843628
I0202 11:23:04.133232 139907762734848 logging_writer.py:48] [84600] global_step=84600, grad_norm=1.4675493240356445, loss=2.0709333419799805
I0202 11:23:37.636408 139907754342144 logging_writer.py:48] [84700] global_step=84700, grad_norm=1.3901108503341675, loss=2.005096435546875
I0202 11:24:11.197809 139907762734848 logging_writer.py:48] [84800] global_step=84800, grad_norm=1.540219783782959, loss=2.0346765518188477
I0202 11:24:44.714012 139907754342144 logging_writer.py:48] [84900] global_step=84900, grad_norm=1.5488176345825195, loss=2.0972375869750977
I0202 11:25:18.272121 139907762734848 logging_writer.py:48] [85000] global_step=85000, grad_norm=1.7991150617599487, loss=2.1045522689819336
I0202 11:25:28.799555 140070692116288 spec.py:321] Evaluating on the training split.
I0202 11:25:35.035724 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 11:25:43.400998 140070692116288 spec.py:349] Evaluating on the test split.
I0202 11:25:46.038631 140070692116288 submission_runner.py:408] Time since start: 29603.64s, 	Step: 85033, 	{'train/accuracy': 0.3713129758834839, 'train/loss': 2.982234001159668, 'validation/accuracy': 0.33959999680519104, 'validation/loss': 3.212437391281128, 'validation/num_examples': 50000, 'test/accuracy': 0.2476000189781189, 'test/loss': 4.028741359710693, 'test/num_examples': 10000, 'score': 28597.715670108795, 'total_duration': 29603.636273384094, 'accumulated_submission_time': 28597.715670108795, 'accumulated_eval_time': 1001.0070824623108, 'accumulated_logging_time': 1.9504098892211914}
I0202 11:25:46.075615 139908710635264 logging_writer.py:48] [85033] accumulated_eval_time=1001.007082, accumulated_logging_time=1.950410, accumulated_submission_time=28597.715670, global_step=85033, preemption_count=0, score=28597.715670, test/accuracy=0.247600, test/loss=4.028741, test/num_examples=10000, total_duration=29603.636273, train/accuracy=0.371313, train/loss=2.982234, validation/accuracy=0.339600, validation/loss=3.212437, validation/num_examples=50000
I0202 11:26:08.857253 139908719027968 logging_writer.py:48] [85100] global_step=85100, grad_norm=1.6355762481689453, loss=2.0961227416992188
I0202 11:26:42.352293 139908710635264 logging_writer.py:48] [85200] global_step=85200, grad_norm=1.5058679580688477, loss=2.0606627464294434
I0202 11:27:15.925734 139908719027968 logging_writer.py:48] [85300] global_step=85300, grad_norm=1.456563949584961, loss=2.0793895721435547
I0202 11:27:49.468050 139908710635264 logging_writer.py:48] [85400] global_step=85400, grad_norm=1.6020512580871582, loss=2.0195367336273193
I0202 11:28:23.102745 139908719027968 logging_writer.py:48] [85500] global_step=85500, grad_norm=1.6445415019989014, loss=2.1960251331329346
I0202 11:28:56.638286 139908710635264 logging_writer.py:48] [85600] global_step=85600, grad_norm=1.5032259225845337, loss=2.089395046234131
I0202 11:29:30.166747 139908719027968 logging_writer.py:48] [85700] global_step=85700, grad_norm=1.4376977682113647, loss=2.098658561706543
I0202 11:30:03.703052 139908710635264 logging_writer.py:48] [85800] global_step=85800, grad_norm=1.5394340753555298, loss=2.1032462120056152
I0202 11:30:37.305028 139908719027968 logging_writer.py:48] [85900] global_step=85900, grad_norm=1.5743749141693115, loss=2.0425593852996826
I0202 11:31:10.837058 139908710635264 logging_writer.py:48] [86000] global_step=86000, grad_norm=1.4351726770401, loss=2.0430524349212646
I0202 11:31:44.419843 139908719027968 logging_writer.py:48] [86100] global_step=86100, grad_norm=1.5371290445327759, loss=2.1139936447143555
I0202 11:32:17.970689 139908710635264 logging_writer.py:48] [86200] global_step=86200, grad_norm=1.4809577465057373, loss=2.0076904296875
I0202 11:32:51.491446 139908719027968 logging_writer.py:48] [86300] global_step=86300, grad_norm=1.483731746673584, loss=2.147555351257324
I0202 11:33:25.073241 139908710635264 logging_writer.py:48] [86400] global_step=86400, grad_norm=1.482839584350586, loss=2.117583990097046
I0202 11:33:58.570387 139908719027968 logging_writer.py:48] [86500] global_step=86500, grad_norm=1.7155669927597046, loss=2.1221954822540283
I0202 11:34:16.174424 140070692116288 spec.py:321] Evaluating on the training split.
I0202 11:34:22.494053 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 11:34:31.284427 140070692116288 spec.py:349] Evaluating on the test split.
I0202 11:34:33.946955 140070692116288 submission_runner.py:408] Time since start: 30131.54s, 	Step: 86554, 	{'train/accuracy': 0.3778499662876129, 'train/loss': 3.040504217147827, 'validation/accuracy': 0.3454599976539612, 'validation/loss': 3.2986605167388916, 'validation/num_examples': 50000, 'test/accuracy': 0.26489999890327454, 'test/loss': 4.093916893005371, 'test/num_examples': 10000, 'score': 29107.751658916473, 'total_duration': 30131.544614315033, 'accumulated_submission_time': 29107.751658916473, 'accumulated_eval_time': 1018.7795708179474, 'accumulated_logging_time': 1.9979043006896973}
I0202 11:34:33.978579 139907737556736 logging_writer.py:48] [86554] accumulated_eval_time=1018.779571, accumulated_logging_time=1.997904, accumulated_submission_time=29107.751659, global_step=86554, preemption_count=0, score=29107.751659, test/accuracy=0.264900, test/loss=4.093917, test/num_examples=10000, total_duration=30131.544614, train/accuracy=0.377850, train/loss=3.040504, validation/accuracy=0.345460, validation/loss=3.298661, validation/num_examples=50000
I0202 11:34:49.917511 139907754342144 logging_writer.py:48] [86600] global_step=86600, grad_norm=1.7261854410171509, loss=1.9949190616607666
I0202 11:35:23.425875 139907737556736 logging_writer.py:48] [86700] global_step=86700, grad_norm=1.5420600175857544, loss=2.1301581859588623
I0202 11:35:56.961042 139907754342144 logging_writer.py:48] [86800] global_step=86800, grad_norm=1.554986834526062, loss=2.153538465499878
I0202 11:36:30.485550 139907737556736 logging_writer.py:48] [86900] global_step=86900, grad_norm=1.5601493120193481, loss=2.070474147796631
I0202 11:37:04.027535 139907754342144 logging_writer.py:48] [87000] global_step=87000, grad_norm=1.8888921737670898, loss=2.014138698577881
I0202 11:37:37.557809 139907737556736 logging_writer.py:48] [87100] global_step=87100, grad_norm=1.6389919519424438, loss=2.1912331581115723
I0202 11:38:11.124837 139907754342144 logging_writer.py:48] [87200] global_step=87200, grad_norm=1.5315465927124023, loss=2.05789852142334
I0202 11:38:44.708940 139907737556736 logging_writer.py:48] [87300] global_step=87300, grad_norm=1.5224326848983765, loss=1.9704318046569824
I0202 11:39:18.267572 139907754342144 logging_writer.py:48] [87400] global_step=87400, grad_norm=1.4745235443115234, loss=1.9629039764404297
I0202 11:39:51.794892 139907737556736 logging_writer.py:48] [87500] global_step=87500, grad_norm=1.5595844984054565, loss=1.9810221195220947
I0202 11:40:25.330430 139907754342144 logging_writer.py:48] [87600] global_step=87600, grad_norm=1.5665456056594849, loss=2.0009894371032715
I0202 11:40:58.933423 139907737556736 logging_writer.py:48] [87700] global_step=87700, grad_norm=1.4875320196151733, loss=2.077394962310791
I0202 11:41:32.600723 139907754342144 logging_writer.py:48] [87800] global_step=87800, grad_norm=1.4888241291046143, loss=2.1329545974731445
I0202 11:42:06.200271 139907737556736 logging_writer.py:48] [87900] global_step=87900, grad_norm=1.4964025020599365, loss=2.071284532546997
I0202 11:42:39.733418 139907754342144 logging_writer.py:48] [88000] global_step=88000, grad_norm=1.6078755855560303, loss=2.0940611362457275
I0202 11:43:04.002727 140070692116288 spec.py:321] Evaluating on the training split.
I0202 11:43:10.978167 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 11:43:19.373553 140070692116288 spec.py:349] Evaluating on the test split.
I0202 11:43:22.044564 140070692116288 submission_runner.py:408] Time since start: 30659.64s, 	Step: 88074, 	{'train/accuracy': 0.37448182702064514, 'train/loss': 2.9719016551971436, 'validation/accuracy': 0.34935998916625977, 'validation/loss': 3.141242742538452, 'validation/num_examples': 50000, 'test/accuracy': 0.2532000243663788, 'test/loss': 3.9751439094543457, 'test/num_examples': 10000, 'score': 29617.712538719177, 'total_duration': 30659.6422123909, 'accumulated_submission_time': 29617.712538719177, 'accumulated_eval_time': 1036.8213591575623, 'accumulated_logging_time': 2.039724111557007}
I0202 11:43:22.085455 139907737556736 logging_writer.py:48] [88074] accumulated_eval_time=1036.821359, accumulated_logging_time=2.039724, accumulated_submission_time=29617.712539, global_step=88074, preemption_count=0, score=29617.712539, test/accuracy=0.253200, test/loss=3.975144, test/num_examples=10000, total_duration=30659.642212, train/accuracy=0.374482, train/loss=2.971902, validation/accuracy=0.349360, validation/loss=3.141243, validation/num_examples=50000
I0202 11:43:31.141567 139907745949440 logging_writer.py:48] [88100] global_step=88100, grad_norm=1.485731601715088, loss=1.9922785758972168
I0202 11:44:04.605731 139907737556736 logging_writer.py:48] [88200] global_step=88200, grad_norm=1.4392727613449097, loss=2.1331281661987305
I0202 11:44:38.107032 139907745949440 logging_writer.py:48] [88300] global_step=88300, grad_norm=1.4859331846237183, loss=2.0413334369659424
I0202 11:45:11.640019 139907737556736 logging_writer.py:48] [88400] global_step=88400, grad_norm=1.6621202230453491, loss=2.033719539642334
I0202 11:45:45.169996 139907745949440 logging_writer.py:48] [88500] global_step=88500, grad_norm=1.7188313007354736, loss=2.0555477142333984
I0202 11:46:18.741156 139907737556736 logging_writer.py:48] [88600] global_step=88600, grad_norm=1.449459433555603, loss=1.963271141052246
I0202 11:46:52.654762 139907745949440 logging_writer.py:48] [88700] global_step=88700, grad_norm=1.564917802810669, loss=2.1170239448547363
I0202 11:47:26.165210 139907737556736 logging_writer.py:48] [88800] global_step=88800, grad_norm=1.4352004528045654, loss=2.0642895698547363
I0202 11:47:59.822632 139907745949440 logging_writer.py:48] [88900] global_step=88900, grad_norm=1.4612054824829102, loss=2.056936264038086
I0202 11:48:33.376366 139907737556736 logging_writer.py:48] [89000] global_step=89000, grad_norm=1.5205039978027344, loss=2.1336581707000732
I0202 11:49:06.917213 139907745949440 logging_writer.py:48] [89100] global_step=89100, grad_norm=1.4762766361236572, loss=1.926916480064392
I0202 11:49:40.464781 139907737556736 logging_writer.py:48] [89200] global_step=89200, grad_norm=1.4656249284744263, loss=1.9920475482940674
I0202 11:50:13.986127 139907745949440 logging_writer.py:48] [89300] global_step=89300, grad_norm=1.6209853887557983, loss=1.929728627204895
I0202 11:50:47.560613 139907737556736 logging_writer.py:48] [89400] global_step=89400, grad_norm=1.612840175628662, loss=2.063676118850708
I0202 11:51:21.073649 139907745949440 logging_writer.py:48] [89500] global_step=89500, grad_norm=1.5629563331604004, loss=2.032304525375366
I0202 11:51:52.084444 140070692116288 spec.py:321] Evaluating on the training split.
I0202 11:51:58.344062 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 11:52:07.066284 140070692116288 spec.py:349] Evaluating on the test split.
I0202 11:52:09.716358 140070692116288 submission_runner.py:408] Time since start: 31187.31s, 	Step: 89594, 	{'train/accuracy': 0.4371412396430969, 'train/loss': 2.587308406829834, 'validation/accuracy': 0.40498000383377075, 'validation/loss': 2.8062405586242676, 'validation/num_examples': 50000, 'test/accuracy': 0.31130000948905945, 'test/loss': 3.605199098587036, 'test/num_examples': 10000, 'score': 30127.646330356598, 'total_duration': 31187.31401515007, 'accumulated_submission_time': 30127.646330356598, 'accumulated_eval_time': 1054.453256368637, 'accumulated_logging_time': 2.091838836669922}
I0202 11:52:09.747456 139908425447168 logging_writer.py:48] [89594] accumulated_eval_time=1054.453256, accumulated_logging_time=2.091839, accumulated_submission_time=30127.646330, global_step=89594, preemption_count=0, score=30127.646330, test/accuracy=0.311300, test/loss=3.605199, test/num_examples=10000, total_duration=31187.314015, train/accuracy=0.437141, train/loss=2.587308, validation/accuracy=0.404980, validation/loss=2.806241, validation/num_examples=50000
I0202 11:52:12.120914 139908719027968 logging_writer.py:48] [89600] global_step=89600, grad_norm=1.4962178468704224, loss=2.0443482398986816
I0202 11:52:45.598805 139908425447168 logging_writer.py:48] [89700] global_step=89700, grad_norm=1.6797852516174316, loss=2.136033296585083
I0202 11:53:19.115067 139908719027968 logging_writer.py:48] [89800] global_step=89800, grad_norm=1.5924383401870728, loss=2.0150070190429688
I0202 11:53:52.661660 139908425447168 logging_writer.py:48] [89900] global_step=89900, grad_norm=1.452093243598938, loss=2.0307037830352783
I0202 11:54:26.297680 139908719027968 logging_writer.py:48] [90000] global_step=90000, grad_norm=1.6358462572097778, loss=2.004251003265381
I0202 11:54:59.842065 139908425447168 logging_writer.py:48] [90100] global_step=90100, grad_norm=1.6249948740005493, loss=2.140359401702881
I0202 11:55:33.383660 139908719027968 logging_writer.py:48] [90200] global_step=90200, grad_norm=1.5984734296798706, loss=2.0367002487182617
I0202 11:56:06.931956 139908425447168 logging_writer.py:48] [90300] global_step=90300, grad_norm=1.5205271244049072, loss=2.104780435562134
I0202 11:56:40.525054 139908719027968 logging_writer.py:48] [90400] global_step=90400, grad_norm=1.6659400463104248, loss=2.112791061401367
I0202 11:57:14.068182 139908425447168 logging_writer.py:48] [90500] global_step=90500, grad_norm=1.4493063688278198, loss=2.1299502849578857
I0202 11:57:47.612554 139908719027968 logging_writer.py:48] [90600] global_step=90600, grad_norm=1.6407233476638794, loss=1.9292471408843994
I0202 11:58:21.153683 139908425447168 logging_writer.py:48] [90700] global_step=90700, grad_norm=1.540366530418396, loss=1.9796956777572632
I0202 11:58:54.739447 139908719027968 logging_writer.py:48] [90800] global_step=90800, grad_norm=1.4665837287902832, loss=2.123497247695923
I0202 11:59:28.269852 139908425447168 logging_writer.py:48] [90900] global_step=90900, grad_norm=1.7103867530822754, loss=1.99320650100708
I0202 12:00:01.808090 139908719027968 logging_writer.py:48] [91000] global_step=91000, grad_norm=1.6717115640640259, loss=2.0117268562316895
I0202 12:00:35.584181 139908425447168 logging_writer.py:48] [91100] global_step=91100, grad_norm=1.514685034751892, loss=1.8802984952926636
I0202 12:00:39.760080 140070692116288 spec.py:321] Evaluating on the training split.
I0202 12:00:46.009521 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 12:00:54.394600 140070692116288 spec.py:349] Evaluating on the test split.
I0202 12:00:57.032671 140070692116288 submission_runner.py:408] Time since start: 31714.63s, 	Step: 91114, 	{'train/accuracy': 0.408223032951355, 'train/loss': 2.8384816646575928, 'validation/accuracy': 0.38561999797821045, 'validation/loss': 3.0218026638031006, 'validation/num_examples': 50000, 'test/accuracy': 0.29110002517700195, 'test/loss': 3.8347907066345215, 'test/num_examples': 10000, 'score': 30637.59294629097, 'total_duration': 31714.630335330963, 'accumulated_submission_time': 30637.59294629097, 'accumulated_eval_time': 1071.72580742836, 'accumulated_logging_time': 2.1351735591888428}
I0202 12:00:57.069648 139907737556736 logging_writer.py:48] [91114] accumulated_eval_time=1071.725807, accumulated_logging_time=2.135174, accumulated_submission_time=30637.592946, global_step=91114, preemption_count=0, score=30637.592946, test/accuracy=0.291100, test/loss=3.834791, test/num_examples=10000, total_duration=31714.630335, train/accuracy=0.408223, train/loss=2.838482, validation/accuracy=0.385620, validation/loss=3.021803, validation/num_examples=50000
I0202 12:01:26.244312 139907745949440 logging_writer.py:48] [91200] global_step=91200, grad_norm=1.5808483362197876, loss=1.975416660308838
I0202 12:01:59.776482 139907737556736 logging_writer.py:48] [91300] global_step=91300, grad_norm=1.6796281337738037, loss=2.0100057125091553
I0202 12:02:33.311740 139907745949440 logging_writer.py:48] [91400] global_step=91400, grad_norm=1.5390015840530396, loss=1.8605154752731323
I0202 12:03:06.866783 139907737556736 logging_writer.py:48] [91500] global_step=91500, grad_norm=1.6023696660995483, loss=2.078745126724243
I0202 12:03:40.425220 139907745949440 logging_writer.py:48] [91600] global_step=91600, grad_norm=1.6122218370437622, loss=2.042020082473755
I0202 12:04:13.976474 139907737556736 logging_writer.py:48] [91700] global_step=91700, grad_norm=1.6033276319503784, loss=2.2369344234466553
I0202 12:04:47.511413 139907745949440 logging_writer.py:48] [91800] global_step=91800, grad_norm=1.6861612796783447, loss=1.9534790515899658
I0202 12:05:21.053846 139907737556736 logging_writer.py:48] [91900] global_step=91900, grad_norm=1.622789978981018, loss=1.9038715362548828
I0202 12:05:54.588906 139907745949440 logging_writer.py:48] [92000] global_step=92000, grad_norm=1.5470893383026123, loss=2.02593994140625
I0202 12:06:28.157569 139907737556736 logging_writer.py:48] [92100] global_step=92100, grad_norm=1.5714693069458008, loss=1.924262285232544
I0202 12:07:01.835984 139907745949440 logging_writer.py:48] [92200] global_step=92200, grad_norm=1.492489218711853, loss=1.9246916770935059
I0202 12:07:35.380839 139907737556736 logging_writer.py:48] [92300] global_step=92300, grad_norm=1.6217901706695557, loss=1.9005181789398193
I0202 12:08:08.908057 139907745949440 logging_writer.py:48] [92400] global_step=92400, grad_norm=1.5372689962387085, loss=1.957967758178711
I0202 12:08:42.454696 139907737556736 logging_writer.py:48] [92500] global_step=92500, grad_norm=1.5476642847061157, loss=2.0261099338531494
I0202 12:09:16.005090 139907745949440 logging_writer.py:48] [92600] global_step=92600, grad_norm=2.0378565788269043, loss=2.0159404277801514
I0202 12:09:27.209074 140070692116288 spec.py:321] Evaluating on the training split.
I0202 12:09:33.457782 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 12:09:42.027244 140070692116288 spec.py:349] Evaluating on the test split.
I0202 12:09:44.538263 140070692116288 submission_runner.py:408] Time since start: 32242.14s, 	Step: 92635, 	{'train/accuracy': 0.4818638265132904, 'train/loss': 2.327448844909668, 'validation/accuracy': 0.4464399814605713, 'validation/loss': 2.534179210662842, 'validation/num_examples': 50000, 'test/accuracy': 0.3418000042438507, 'test/loss': 3.3646998405456543, 'test/num_examples': 10000, 'score': 31147.667982816696, 'total_duration': 32242.1359269619, 'accumulated_submission_time': 31147.667982816696, 'accumulated_eval_time': 1089.0549597740173, 'accumulated_logging_time': 2.183558702468872}
I0202 12:09:44.578434 139908710635264 logging_writer.py:48] [92635] accumulated_eval_time=1089.054960, accumulated_logging_time=2.183559, accumulated_submission_time=31147.667983, global_step=92635, preemption_count=0, score=31147.667983, test/accuracy=0.341800, test/loss=3.364700, test/num_examples=10000, total_duration=32242.135927, train/accuracy=0.481864, train/loss=2.327449, validation/accuracy=0.446440, validation/loss=2.534179, validation/num_examples=50000
I0202 12:10:06.690465 139908719027968 logging_writer.py:48] [92700] global_step=92700, grad_norm=1.479622721672058, loss=1.9887192249298096
I0202 12:10:40.204977 139908710635264 logging_writer.py:48] [92800] global_step=92800, grad_norm=1.5542385578155518, loss=2.0927183628082275
I0202 12:11:13.751996 139908719027968 logging_writer.py:48] [92900] global_step=92900, grad_norm=1.6067091226577759, loss=2.053725242614746
I0202 12:11:47.283817 139908710635264 logging_writer.py:48] [93000] global_step=93000, grad_norm=1.4942817687988281, loss=2.005946397781372
I0202 12:12:20.898893 139908719027968 logging_writer.py:48] [93100] global_step=93100, grad_norm=1.6365272998809814, loss=2.0588624477386475
I0202 12:12:54.438796 139908710635264 logging_writer.py:48] [93200] global_step=93200, grad_norm=1.416479229927063, loss=1.960700273513794
I0202 12:13:28.067277 139908719027968 logging_writer.py:48] [93300] global_step=93300, grad_norm=1.6727854013442993, loss=2.068770408630371
I0202 12:14:01.598353 139908710635264 logging_writer.py:48] [93400] global_step=93400, grad_norm=1.657730221748352, loss=2.0982742309570312
I0202 12:14:35.115592 139908719027968 logging_writer.py:48] [93500] global_step=93500, grad_norm=1.609525442123413, loss=1.956573247909546
I0202 12:15:08.636657 139908710635264 logging_writer.py:48] [93600] global_step=93600, grad_norm=1.599557638168335, loss=2.0254178047180176
I0202 12:15:42.170686 139908719027968 logging_writer.py:48] [93700] global_step=93700, grad_norm=1.6251542568206787, loss=2.061278820037842
I0202 12:16:15.727936 139908710635264 logging_writer.py:48] [93800] global_step=93800, grad_norm=1.529335379600525, loss=1.934242844581604
I0202 12:16:49.304575 139908719027968 logging_writer.py:48] [93900] global_step=93900, grad_norm=1.8331549167633057, loss=2.173673152923584
I0202 12:17:22.833863 139908710635264 logging_writer.py:48] [94000] global_step=94000, grad_norm=1.6608659029006958, loss=1.9645124673843384
I0202 12:17:56.412495 139908719027968 logging_writer.py:48] [94100] global_step=94100, grad_norm=1.6943176984786987, loss=2.0741429328918457
I0202 12:18:14.655426 140070692116288 spec.py:321] Evaluating on the training split.
I0202 12:18:20.919063 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 12:18:29.609685 140070692116288 spec.py:349] Evaluating on the test split.
I0202 12:18:32.245793 140070692116288 submission_runner.py:408] Time since start: 32769.84s, 	Step: 94156, 	{'train/accuracy': 0.5026904940605164, 'train/loss': 2.148667812347412, 'validation/accuracy': 0.4616999924182892, 'validation/loss': 2.3995540142059326, 'validation/num_examples': 50000, 'test/accuracy': 0.3456000089645386, 'test/loss': 3.1860923767089844, 'test/num_examples': 10000, 'score': 31657.682291984558, 'total_duration': 32769.84343409538, 'accumulated_submission_time': 31657.682291984558, 'accumulated_eval_time': 1106.6452696323395, 'accumulated_logging_time': 2.233637571334839}
I0202 12:18:32.280447 139907737556736 logging_writer.py:48] [94156] accumulated_eval_time=1106.645270, accumulated_logging_time=2.233638, accumulated_submission_time=31657.682292, global_step=94156, preemption_count=0, score=31657.682292, test/accuracy=0.345600, test/loss=3.186092, test/num_examples=10000, total_duration=32769.843434, train/accuracy=0.502690, train/loss=2.148668, validation/accuracy=0.461700, validation/loss=2.399554, validation/num_examples=50000
I0202 12:18:47.392019 139907745949440 logging_writer.py:48] [94200] global_step=94200, grad_norm=1.6746010780334473, loss=2.013702392578125
I0202 12:19:20.881920 139907737556736 logging_writer.py:48] [94300] global_step=94300, grad_norm=1.6528403759002686, loss=2.0956525802612305
I0202 12:19:54.650343 139907745949440 logging_writer.py:48] [94400] global_step=94400, grad_norm=1.685815691947937, loss=2.031860828399658
I0202 12:20:28.222156 139907737556736 logging_writer.py:48] [94500] global_step=94500, grad_norm=1.4163599014282227, loss=1.956594705581665
I0202 12:21:01.728034 139907745949440 logging_writer.py:48] [94600] global_step=94600, grad_norm=1.6077378988265991, loss=1.9438124895095825
I0202 12:21:35.273143 139907737556736 logging_writer.py:48] [94700] global_step=94700, grad_norm=1.5787876844406128, loss=2.001948118209839
I0202 12:22:08.776469 139907745949440 logging_writer.py:48] [94800] global_step=94800, grad_norm=1.6926705837249756, loss=1.9401733875274658
I0202 12:22:42.335178 139907737556736 logging_writer.py:48] [94900] global_step=94900, grad_norm=1.5586843490600586, loss=1.9114052057266235
I0202 12:23:15.859028 139907745949440 logging_writer.py:48] [95000] global_step=95000, grad_norm=1.5473183393478394, loss=2.0762274265289307
I0202 12:23:49.383543 139907737556736 logging_writer.py:48] [95100] global_step=95100, grad_norm=1.5054843425750732, loss=1.992819905281067
I0202 12:24:22.955365 139907745949440 logging_writer.py:48] [95200] global_step=95200, grad_norm=2.0118799209594727, loss=2.005017042160034
I0202 12:24:56.495026 139907737556736 logging_writer.py:48] [95300] global_step=95300, grad_norm=1.679535150527954, loss=2.1103174686431885
I0202 12:25:30.060848 139907745949440 logging_writer.py:48] [95400] global_step=95400, grad_norm=1.6078382730484009, loss=1.862836241722107
I0202 12:26:03.748776 139907737556736 logging_writer.py:48] [95500] global_step=95500, grad_norm=1.4929430484771729, loss=1.9992679357528687
I0202 12:26:37.303977 139907745949440 logging_writer.py:48] [95600] global_step=95600, grad_norm=1.6118029356002808, loss=1.916656494140625
I0202 12:27:02.566403 140070692116288 spec.py:321] Evaluating on the training split.
I0202 12:27:09.039135 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 12:27:17.642564 140070692116288 spec.py:349] Evaluating on the test split.
I0202 12:27:20.257367 140070692116288 submission_runner.py:408] Time since start: 33297.86s, 	Step: 95677, 	{'train/accuracy': 0.35891661047935486, 'train/loss': 3.0547327995300293, 'validation/accuracy': 0.33701997995376587, 'validation/loss': 3.2362570762634277, 'validation/num_examples': 50000, 'test/accuracy': 0.255700021982193, 'test/loss': 3.914307117462158, 'test/num_examples': 10000, 'score': 32167.90555024147, 'total_duration': 33297.85503602028, 'accumulated_submission_time': 32167.90555024147, 'accumulated_eval_time': 1124.3362169265747, 'accumulated_logging_time': 2.278075695037842}
I0202 12:27:20.292359 139908425447168 logging_writer.py:48] [95677] accumulated_eval_time=1124.336217, accumulated_logging_time=2.278076, accumulated_submission_time=32167.905550, global_step=95677, preemption_count=0, score=32167.905550, test/accuracy=0.255700, test/loss=3.914307, test/num_examples=10000, total_duration=33297.855036, train/accuracy=0.358917, train/loss=3.054733, validation/accuracy=0.337020, validation/loss=3.236257, validation/num_examples=50000
I0202 12:27:28.332957 139908710635264 logging_writer.py:48] [95700] global_step=95700, grad_norm=1.6480880975723267, loss=1.9584741592407227
I0202 12:28:01.826711 139908425447168 logging_writer.py:48] [95800] global_step=95800, grad_norm=1.7671633958816528, loss=2.0894834995269775
I0202 12:28:35.329232 139908710635264 logging_writer.py:48] [95900] global_step=95900, grad_norm=1.8479093313217163, loss=1.9659736156463623
I0202 12:29:08.915425 139908425447168 logging_writer.py:48] [96000] global_step=96000, grad_norm=1.7161304950714111, loss=1.9397177696228027
I0202 12:29:42.449325 139908710635264 logging_writer.py:48] [96100] global_step=96100, grad_norm=1.6188085079193115, loss=1.9688514471054077
I0202 12:30:15.973439 139908425447168 logging_writer.py:48] [96200] global_step=96200, grad_norm=1.6205812692642212, loss=1.9614508152008057
I0202 12:30:49.523098 139908710635264 logging_writer.py:48] [96300] global_step=96300, grad_norm=1.7975705862045288, loss=2.052116632461548
I0202 12:31:23.125214 139908425447168 logging_writer.py:48] [96400] global_step=96400, grad_norm=1.69614577293396, loss=2.0872693061828613
I0202 12:31:56.669111 139908710635264 logging_writer.py:48] [96500] global_step=96500, grad_norm=1.7421504259109497, loss=2.0010433197021484
I0202 12:32:30.273364 139908425447168 logging_writer.py:48] [96600] global_step=96600, grad_norm=1.6558479070663452, loss=2.061680316925049
I0202 12:33:03.766867 139908710635264 logging_writer.py:48] [96700] global_step=96700, grad_norm=1.5692402124404907, loss=1.911081075668335
I0202 12:33:37.325420 139908425447168 logging_writer.py:48] [96800] global_step=96800, grad_norm=1.533355474472046, loss=2.0916786193847656
I0202 12:34:10.856501 139908710635264 logging_writer.py:48] [96900] global_step=96900, grad_norm=1.7998589277267456, loss=2.070951223373413
I0202 12:34:44.359211 139908425447168 logging_writer.py:48] [97000] global_step=97000, grad_norm=1.7224862575531006, loss=2.010626792907715
I0202 12:35:17.961752 139908710635264 logging_writer.py:48] [97100] global_step=97100, grad_norm=1.663361668586731, loss=1.9684919118881226
I0202 12:35:50.339649 140070692116288 spec.py:321] Evaluating on the training split.
I0202 12:35:56.585577 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 12:36:05.397788 140070692116288 spec.py:349] Evaluating on the test split.
I0202 12:36:08.117962 140070692116288 submission_runner.py:408] Time since start: 33825.72s, 	Step: 97198, 	{'train/accuracy': 0.4383370578289032, 'train/loss': 2.5672011375427246, 'validation/accuracy': 0.40639999508857727, 'validation/loss': 2.7606093883514404, 'validation/num_examples': 50000, 'test/accuracy': 0.31540000438690186, 'test/loss': 3.509662628173828, 'test/num_examples': 10000, 'score': 32677.89052796364, 'total_duration': 33825.715623378754, 'accumulated_submission_time': 32677.89052796364, 'accumulated_eval_time': 1142.114492893219, 'accumulated_logging_time': 2.3221030235290527}
I0202 12:36:08.152371 139907754342144 logging_writer.py:48] [97198] accumulated_eval_time=1142.114493, accumulated_logging_time=2.322103, accumulated_submission_time=32677.890528, global_step=97198, preemption_count=0, score=32677.890528, test/accuracy=0.315400, test/loss=3.509663, test/num_examples=10000, total_duration=33825.715623, train/accuracy=0.438337, train/loss=2.567201, validation/accuracy=0.406400, validation/loss=2.760609, validation/num_examples=50000
I0202 12:36:09.163197 139907762734848 logging_writer.py:48] [97200] global_step=97200, grad_norm=1.7166495323181152, loss=1.9913179874420166
I0202 12:36:42.641993 139907754342144 logging_writer.py:48] [97300] global_step=97300, grad_norm=1.7261388301849365, loss=2.022465229034424
I0202 12:37:16.160602 139907762734848 logging_writer.py:48] [97400] global_step=97400, grad_norm=1.6443732976913452, loss=1.945337176322937
I0202 12:37:49.721263 139907754342144 logging_writer.py:48] [97500] global_step=97500, grad_norm=1.627982258796692, loss=1.9103679656982422
I0202 12:38:23.241304 139907762734848 logging_writer.py:48] [97600] global_step=97600, grad_norm=1.6698014736175537, loss=1.937098503112793
I0202 12:38:56.874358 139907754342144 logging_writer.py:48] [97700] global_step=97700, grad_norm=1.7705720663070679, loss=1.9927332401275635
I0202 12:39:30.391310 139907762734848 logging_writer.py:48] [97800] global_step=97800, grad_norm=1.588716983795166, loss=2.119586944580078
I0202 12:40:03.921453 139907754342144 logging_writer.py:48] [97900] global_step=97900, grad_norm=1.6151094436645508, loss=1.9953484535217285
I0202 12:40:37.453630 139907762734848 logging_writer.py:48] [98000] global_step=98000, grad_norm=1.652905821800232, loss=1.9562113285064697
I0202 12:41:11.029730 139907754342144 logging_writer.py:48] [98100] global_step=98100, grad_norm=1.75393545627594, loss=1.943640947341919
I0202 12:41:44.566298 139907762734848 logging_writer.py:48] [98200] global_step=98200, grad_norm=1.5754307508468628, loss=1.932542085647583
I0202 12:42:18.116575 139907754342144 logging_writer.py:48] [98300] global_step=98300, grad_norm=1.5893853902816772, loss=1.981452465057373
I0202 12:42:51.627459 139907762734848 logging_writer.py:48] [98400] global_step=98400, grad_norm=1.6921792030334473, loss=1.888379693031311
I0202 12:43:25.167922 139907754342144 logging_writer.py:48] [98500] global_step=98500, grad_norm=1.6852468252182007, loss=2.0485191345214844
I0202 12:43:58.719882 139907762734848 logging_writer.py:48] [98600] global_step=98600, grad_norm=1.576796531677246, loss=1.955344796180725
I0202 12:44:32.231874 139907754342144 logging_writer.py:48] [98700] global_step=98700, grad_norm=1.645435094833374, loss=1.9169700145721436
I0202 12:44:38.430082 140070692116288 spec.py:321] Evaluating on the training split.
I0202 12:44:44.686937 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 12:44:53.086226 140070692116288 spec.py:349] Evaluating on the test split.
I0202 12:44:55.702116 140070692116288 submission_runner.py:408] Time since start: 34353.30s, 	Step: 98720, 	{'train/accuracy': 0.3068598508834839, 'train/loss': 3.599402904510498, 'validation/accuracy': 0.28567999601364136, 'validation/loss': 3.769570827484131, 'validation/num_examples': 50000, 'test/accuracy': 0.2054000049829483, 'test/loss': 4.586277484893799, 'test/num_examples': 10000, 'score': 33188.105674266815, 'total_duration': 34353.2997841835, 'accumulated_submission_time': 33188.105674266815, 'accumulated_eval_time': 1159.3864908218384, 'accumulated_logging_time': 2.3658483028411865}
I0202 12:44:55.742448 139907737556736 logging_writer.py:48] [98720] accumulated_eval_time=1159.386491, accumulated_logging_time=2.365848, accumulated_submission_time=33188.105674, global_step=98720, preemption_count=0, score=33188.105674, test/accuracy=0.205400, test/loss=4.586277, test/num_examples=10000, total_duration=34353.299784, train/accuracy=0.306860, train/loss=3.599403, validation/accuracy=0.285680, validation/loss=3.769571, validation/num_examples=50000
I0202 12:45:22.858797 139908710635264 logging_writer.py:48] [98800] global_step=98800, grad_norm=1.7020214796066284, loss=1.8742480278015137
I0202 12:45:56.429463 139907737556736 logging_writer.py:48] [98900] global_step=98900, grad_norm=1.7432011365890503, loss=1.9959818124771118
I0202 12:46:29.984003 139908710635264 logging_writer.py:48] [99000] global_step=99000, grad_norm=1.6913882493972778, loss=2.0403072834014893
I0202 12:47:03.569892 139907737556736 logging_writer.py:48] [99100] global_step=99100, grad_norm=1.6040260791778564, loss=1.9454749822616577
I0202 12:47:37.097274 139908710635264 logging_writer.py:48] [99200] global_step=99200, grad_norm=1.7466411590576172, loss=1.966734766960144
I0202 12:48:10.612426 139907737556736 logging_writer.py:48] [99300] global_step=99300, grad_norm=1.718801736831665, loss=1.9535791873931885
I0202 12:48:44.204681 139908710635264 logging_writer.py:48] [99400] global_step=99400, grad_norm=1.6998683214187622, loss=1.9626787900924683
I0202 12:49:17.804083 139907737556736 logging_writer.py:48] [99500] global_step=99500, grad_norm=1.7972837686538696, loss=1.997092604637146
I0202 12:49:51.335823 139908710635264 logging_writer.py:48] [99600] global_step=99600, grad_norm=1.652923345565796, loss=1.8864402770996094
I0202 12:50:24.853878 139907737556736 logging_writer.py:48] [99700] global_step=99700, grad_norm=1.635888695716858, loss=1.8244147300720215
I0202 12:50:58.383056 139908710635264 logging_writer.py:48] [99800] global_step=99800, grad_norm=1.7435935735702515, loss=1.8897188901901245
I0202 12:51:31.897073 139907737556736 logging_writer.py:48] [99900] global_step=99900, grad_norm=1.6844654083251953, loss=2.0345895290374756
I0202 12:52:05.528533 139908710635264 logging_writer.py:48] [100000] global_step=100000, grad_norm=1.7982443571090698, loss=2.033424139022827
I0202 12:52:39.132838 139907737556736 logging_writer.py:48] [100100] global_step=100100, grad_norm=1.7076220512390137, loss=1.9501011371612549
I0202 12:53:12.642935 139908710635264 logging_writer.py:48] [100200] global_step=100200, grad_norm=1.9055455923080444, loss=1.8294202089309692
I0202 12:53:25.886584 140070692116288 spec.py:321] Evaluating on the training split.
I0202 12:53:32.154676 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 12:53:40.783277 140070692116288 spec.py:349] Evaluating on the test split.
I0202 12:53:43.480237 140070692116288 submission_runner.py:408] Time since start: 34881.08s, 	Step: 100241, 	{'train/accuracy': 0.4196627736091614, 'train/loss': 2.642141103744507, 'validation/accuracy': 0.4020799994468689, 'validation/loss': 2.7794859409332275, 'validation/num_examples': 50000, 'test/accuracy': 0.29490000009536743, 'test/loss': 3.5665860176086426, 'test/num_examples': 10000, 'score': 33698.1875834465, 'total_duration': 34881.077904462814, 'accumulated_submission_time': 33698.1875834465, 'accumulated_eval_time': 1176.9801092147827, 'accumulated_logging_time': 2.4154789447784424}
I0202 12:53:43.514878 139907737556736 logging_writer.py:48] [100241] accumulated_eval_time=1176.980109, accumulated_logging_time=2.415479, accumulated_submission_time=33698.187583, global_step=100241, preemption_count=0, score=33698.187583, test/accuracy=0.294900, test/loss=3.566586, test/num_examples=10000, total_duration=34881.077904, train/accuracy=0.419663, train/loss=2.642141, validation/accuracy=0.402080, validation/loss=2.779486, validation/num_examples=50000
I0202 12:54:03.623452 139907754342144 logging_writer.py:48] [100300] global_step=100300, grad_norm=1.8477134704589844, loss=2.1228954792022705
I0202 12:54:37.114785 139907737556736 logging_writer.py:48] [100400] global_step=100400, grad_norm=1.792275309562683, loss=1.9401301145553589
I0202 12:55:10.617472 139907754342144 logging_writer.py:48] [100500] global_step=100500, grad_norm=1.7891974449157715, loss=2.0293643474578857
I0202 12:55:44.199973 139907737556736 logging_writer.py:48] [100600] global_step=100600, grad_norm=1.693030595779419, loss=1.9701805114746094
I0202 12:56:17.753954 139907754342144 logging_writer.py:48] [100700] global_step=100700, grad_norm=1.7954492568969727, loss=2.0229086875915527
I0202 12:56:51.297917 139907737556736 logging_writer.py:48] [100800] global_step=100800, grad_norm=1.844120979309082, loss=1.915730357170105
I0202 12:57:24.834446 139907754342144 logging_writer.py:48] [100900] global_step=100900, grad_norm=1.7094645500183105, loss=1.8599417209625244
I0202 12:57:58.402970 139907737556736 logging_writer.py:48] [101000] global_step=101000, grad_norm=1.7302361726760864, loss=1.9110608100891113
I0202 12:58:32.062958 139907754342144 logging_writer.py:48] [101100] global_step=101100, grad_norm=1.6761218309402466, loss=1.9529461860656738
I0202 12:59:05.575606 139907737556736 logging_writer.py:48] [101200] global_step=101200, grad_norm=1.7070528268814087, loss=2.0140347480773926
I0202 12:59:39.068312 139907754342144 logging_writer.py:48] [101300] global_step=101300, grad_norm=1.7638927698135376, loss=1.9779397249221802
I0202 13:00:12.556475 139907737556736 logging_writer.py:48] [101400] global_step=101400, grad_norm=1.743821144104004, loss=1.998420000076294
I0202 13:00:46.078472 139907754342144 logging_writer.py:48] [101500] global_step=101500, grad_norm=1.795627474784851, loss=1.9038996696472168
I0202 13:01:19.632991 139907737556736 logging_writer.py:48] [101600] global_step=101600, grad_norm=1.6388336420059204, loss=2.0145068168640137
I0202 13:01:53.150898 139907754342144 logging_writer.py:48] [101700] global_step=101700, grad_norm=1.8758145570755005, loss=1.919899821281433
I0202 13:02:13.746431 140070692116288 spec.py:321] Evaluating on the training split.
I0202 13:02:20.140459 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 13:02:28.584300 140070692116288 spec.py:349] Evaluating on the test split.
I0202 13:02:31.234431 140070692116288 submission_runner.py:408] Time since start: 35408.83s, 	Step: 101763, 	{'train/accuracy': 0.4217952787876129, 'train/loss': 2.6498255729675293, 'validation/accuracy': 0.3804999887943268, 'validation/loss': 2.9349420070648193, 'validation/num_examples': 50000, 'test/accuracy': 0.2842999994754791, 'test/loss': 3.691239595413208, 'test/num_examples': 10000, 'score': 34208.35723352432, 'total_duration': 35408.832102775574, 'accumulated_submission_time': 34208.35723352432, 'accumulated_eval_time': 1194.4681041240692, 'accumulated_logging_time': 2.459041118621826}
I0202 13:02:31.273280 139907737556736 logging_writer.py:48] [101763] accumulated_eval_time=1194.468104, accumulated_logging_time=2.459041, accumulated_submission_time=34208.357234, global_step=101763, preemption_count=0, score=34208.357234, test/accuracy=0.284300, test/loss=3.691240, test/num_examples=10000, total_duration=35408.832103, train/accuracy=0.421795, train/loss=2.649826, validation/accuracy=0.380500, validation/loss=2.934942, validation/num_examples=50000
I0202 13:02:44.022457 139907745949440 logging_writer.py:48] [101800] global_step=101800, grad_norm=1.735377311706543, loss=1.9057819843292236
I0202 13:03:17.503835 139907737556736 logging_writer.py:48] [101900] global_step=101900, grad_norm=1.6472012996673584, loss=1.8572814464569092
I0202 13:03:51.061533 139907745949440 logging_writer.py:48] [102000] global_step=102000, grad_norm=1.7091816663742065, loss=1.9339276552200317
I0202 13:04:24.580489 139907737556736 logging_writer.py:48] [102100] global_step=102100, grad_norm=1.6574546098709106, loss=1.9816529750823975
I0202 13:04:58.227894 139907745949440 logging_writer.py:48] [102200] global_step=102200, grad_norm=1.763927936553955, loss=1.9161925315856934
I0202 13:05:31.742266 139907737556736 logging_writer.py:48] [102300] global_step=102300, grad_norm=1.688495397567749, loss=2.0370306968688965
I0202 13:06:05.268998 139907745949440 logging_writer.py:48] [102400] global_step=102400, grad_norm=1.7798810005187988, loss=2.009661912918091
I0202 13:06:38.843998 139907737556736 logging_writer.py:48] [102500] global_step=102500, grad_norm=1.6642532348632812, loss=1.910031795501709
I0202 13:07:12.393461 139907745949440 logging_writer.py:48] [102600] global_step=102600, grad_norm=1.7789899110794067, loss=1.8667802810668945
I0202 13:07:45.963337 139907737556736 logging_writer.py:48] [102700] global_step=102700, grad_norm=1.750274896621704, loss=2.0126118659973145
I0202 13:08:19.529789 139907745949440 logging_writer.py:48] [102800] global_step=102800, grad_norm=1.7944129705429077, loss=1.9151568412780762
I0202 13:08:53.036354 139907737556736 logging_writer.py:48] [102900] global_step=102900, grad_norm=1.8512805700302124, loss=1.8749181032180786
I0202 13:09:26.562946 139907745949440 logging_writer.py:48] [103000] global_step=103000, grad_norm=1.7046650648117065, loss=1.9555901288986206
I0202 13:10:00.103664 139907737556736 logging_writer.py:48] [103100] global_step=103100, grad_norm=1.859121322631836, loss=1.9069286584854126
I0202 13:10:33.678912 139907745949440 logging_writer.py:48] [103200] global_step=103200, grad_norm=1.7983686923980713, loss=1.9890308380126953
I0202 13:11:01.373961 140070692116288 spec.py:321] Evaluating on the training split.
I0202 13:11:07.865253 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 13:11:16.384042 140070692116288 spec.py:349] Evaluating on the test split.
I0202 13:11:18.928000 140070692116288 submission_runner.py:408] Time since start: 35936.53s, 	Step: 103284, 	{'train/accuracy': 0.46273118257522583, 'train/loss': 2.48760986328125, 'validation/accuracy': 0.427979975938797, 'validation/loss': 2.7530295848846436, 'validation/num_examples': 50000, 'test/accuracy': 0.3148000240325928, 'test/loss': 3.6579771041870117, 'test/num_examples': 10000, 'score': 34718.39355421066, 'total_duration': 35936.52565956116, 'accumulated_submission_time': 34718.39355421066, 'accumulated_eval_time': 1212.0220968723297, 'accumulated_logging_time': 2.508146286010742}
I0202 13:11:18.966464 139908710635264 logging_writer.py:48] [103284] accumulated_eval_time=1212.022097, accumulated_logging_time=2.508146, accumulated_submission_time=34718.393554, global_step=103284, preemption_count=0, score=34718.393554, test/accuracy=0.314800, test/loss=3.657977, test/num_examples=10000, total_duration=35936.525660, train/accuracy=0.462731, train/loss=2.487610, validation/accuracy=0.427980, validation/loss=2.753030, validation/num_examples=50000
I0202 13:11:24.672194 139908719027968 logging_writer.py:48] [103300] global_step=103300, grad_norm=1.7643076181411743, loss=1.9316599369049072
I0202 13:11:58.160712 139908710635264 logging_writer.py:48] [103400] global_step=103400, grad_norm=1.7424505949020386, loss=1.8677979707717896
I0202 13:12:31.741159 139908719027968 logging_writer.py:48] [103500] global_step=103500, grad_norm=1.5909126996994019, loss=1.855981707572937
I0202 13:13:05.272386 139908710635264 logging_writer.py:48] [103600] global_step=103600, grad_norm=1.7903335094451904, loss=1.8987557888031006
I0202 13:13:38.790393 139908719027968 logging_writer.py:48] [103700] global_step=103700, grad_norm=1.7843979597091675, loss=1.8621981143951416
I0202 13:14:12.326572 139908710635264 logging_writer.py:48] [103800] global_step=103800, grad_norm=1.8048510551452637, loss=1.9666193723678589
I0202 13:14:45.895821 139908719027968 logging_writer.py:48] [103900] global_step=103900, grad_norm=1.6827268600463867, loss=1.8579185009002686
I0202 13:15:19.465145 139908710635264 logging_writer.py:48] [104000] global_step=104000, grad_norm=1.798660397529602, loss=1.9471993446350098
I0202 13:15:52.961326 139908719027968 logging_writer.py:48] [104100] global_step=104100, grad_norm=1.8307424783706665, loss=1.97523832321167
I0202 13:16:26.494513 139908710635264 logging_writer.py:48] [104200] global_step=104200, grad_norm=1.8315057754516602, loss=1.8719761371612549
I0202 13:17:00.015529 139908719027968 logging_writer.py:48] [104300] global_step=104300, grad_norm=1.8178637027740479, loss=1.8978173732757568
I0202 13:17:33.760901 139908710635264 logging_writer.py:48] [104400] global_step=104400, grad_norm=1.9361647367477417, loss=1.8301172256469727
I0202 13:18:07.259872 139908719027968 logging_writer.py:48] [104500] global_step=104500, grad_norm=1.895961880683899, loss=1.9513705968856812
I0202 13:18:40.778152 139908710635264 logging_writer.py:48] [104600] global_step=104600, grad_norm=1.8939882516860962, loss=1.9647773504257202
I0202 13:19:14.326056 139908719027968 logging_writer.py:48] [104700] global_step=104700, grad_norm=1.8382610082626343, loss=1.9176510572433472
I0202 13:19:47.866918 139908710635264 logging_writer.py:48] [104800] global_step=104800, grad_norm=1.7482990026474, loss=1.8967605829238892
I0202 13:19:49.029125 140070692116288 spec.py:321] Evaluating on the training split.
I0202 13:19:55.300496 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 13:20:03.959020 140070692116288 spec.py:349] Evaluating on the test split.
I0202 13:20:06.507591 140070692116288 submission_runner.py:408] Time since start: 36464.11s, 	Step: 104805, 	{'train/accuracy': 0.5387436151504517, 'train/loss': 1.9647153615951538, 'validation/accuracy': 0.4964599907398224, 'validation/loss': 2.206085205078125, 'validation/num_examples': 50000, 'test/accuracy': 0.3752000033855438, 'test/loss': 3.0397000312805176, 'test/num_examples': 10000, 'score': 35228.39146900177, 'total_duration': 36464.10525536537, 'accumulated_submission_time': 35228.39146900177, 'accumulated_eval_time': 1229.5005240440369, 'accumulated_logging_time': 2.5585193634033203}
I0202 13:20:06.556006 139907754342144 logging_writer.py:48] [104805] accumulated_eval_time=1229.500524, accumulated_logging_time=2.558519, accumulated_submission_time=35228.391469, global_step=104805, preemption_count=0, score=35228.391469, test/accuracy=0.375200, test/loss=3.039700, test/num_examples=10000, total_duration=36464.105255, train/accuracy=0.538744, train/loss=1.964715, validation/accuracy=0.496460, validation/loss=2.206085, validation/num_examples=50000
I0202 13:20:38.688605 139907762734848 logging_writer.py:48] [104900] global_step=104900, grad_norm=1.7781201601028442, loss=1.9691444635391235
I0202 13:21:12.159609 139907754342144 logging_writer.py:48] [105000] global_step=105000, grad_norm=1.767498254776001, loss=1.944951057434082
I0202 13:21:45.746840 139907762734848 logging_writer.py:48] [105100] global_step=105100, grad_norm=1.7483792304992676, loss=1.886933445930481
I0202 13:22:19.283688 139907754342144 logging_writer.py:48] [105200] global_step=105200, grad_norm=1.8192626237869263, loss=1.9476377964019775
I0202 13:22:52.811427 139907762734848 logging_writer.py:48] [105300] global_step=105300, grad_norm=1.727755069732666, loss=2.030564308166504
I0202 13:23:26.343529 139907754342144 logging_writer.py:48] [105400] global_step=105400, grad_norm=1.819023609161377, loss=1.965924859046936
I0202 13:24:00.013400 139907762734848 logging_writer.py:48] [105500] global_step=105500, grad_norm=1.732570767402649, loss=2.0265378952026367
I0202 13:24:33.568692 139907754342144 logging_writer.py:48] [105600] global_step=105600, grad_norm=1.8134180307388306, loss=1.9376858472824097
I0202 13:25:07.166367 139907762734848 logging_writer.py:48] [105700] global_step=105700, grad_norm=1.872681975364685, loss=1.968622088432312
I0202 13:25:40.678269 139907754342144 logging_writer.py:48] [105800] global_step=105800, grad_norm=1.7135250568389893, loss=1.868740200996399
I0202 13:26:14.270388 139907762734848 logging_writer.py:48] [105900] global_step=105900, grad_norm=1.772054672241211, loss=1.833534598350525
I0202 13:26:47.761654 139907754342144 logging_writer.py:48] [106000] global_step=106000, grad_norm=1.8184398412704468, loss=2.0217251777648926
I0202 13:27:21.328487 139907762734848 logging_writer.py:48] [106100] global_step=106100, grad_norm=1.7696380615234375, loss=1.8952946662902832
I0202 13:27:54.846257 139907754342144 logging_writer.py:48] [106200] global_step=106200, grad_norm=1.856549620628357, loss=1.8526368141174316
I0202 13:28:28.450259 139907762734848 logging_writer.py:48] [106300] global_step=106300, grad_norm=1.9097386598587036, loss=1.8430519104003906
I0202 13:28:36.654782 140070692116288 spec.py:321] Evaluating on the training split.
I0202 13:28:43.001804 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 13:28:51.705893 140070692116288 spec.py:349] Evaluating on the test split.
I0202 13:28:54.407401 140070692116288 submission_runner.py:408] Time since start: 36992.01s, 	Step: 106326, 	{'train/accuracy': 0.5285993218421936, 'train/loss': 2.0710136890411377, 'validation/accuracy': 0.4899199903011322, 'validation/loss': 2.3158931732177734, 'validation/num_examples': 50000, 'test/accuracy': 0.3743000030517578, 'test/loss': 3.179978370666504, 'test/num_examples': 10000, 'score': 35738.42715740204, 'total_duration': 36992.00506877899, 'accumulated_submission_time': 35738.42715740204, 'accumulated_eval_time': 1247.2531068325043, 'accumulated_logging_time': 2.616370439529419}
I0202 13:28:54.443013 139907737556736 logging_writer.py:48] [106326] accumulated_eval_time=1247.253107, accumulated_logging_time=2.616370, accumulated_submission_time=35738.427157, global_step=106326, preemption_count=0, score=35738.427157, test/accuracy=0.374300, test/loss=3.179978, test/num_examples=10000, total_duration=36992.005069, train/accuracy=0.528599, train/loss=2.071014, validation/accuracy=0.489920, validation/loss=2.315893, validation/num_examples=50000
I0202 13:29:19.557041 139908710635264 logging_writer.py:48] [106400] global_step=106400, grad_norm=1.7965595722198486, loss=1.9149038791656494
I0202 13:29:53.067625 139907737556736 logging_writer.py:48] [106500] global_step=106500, grad_norm=1.8896745443344116, loss=1.9495412111282349
I0202 13:30:26.684537 139908710635264 logging_writer.py:48] [106600] global_step=106600, grad_norm=1.8196603059768677, loss=1.8957669734954834
I0202 13:31:00.249933 139907737556736 logging_writer.py:48] [106700] global_step=106700, grad_norm=1.8946417570114136, loss=1.9027767181396484
I0202 13:31:33.780574 139908710635264 logging_writer.py:48] [106800] global_step=106800, grad_norm=2.058440685272217, loss=1.9396653175354004
I0202 13:32:07.324618 139907737556736 logging_writer.py:48] [106900] global_step=106900, grad_norm=1.9661917686462402, loss=1.906064510345459
I0202 13:32:40.880556 139908710635264 logging_writer.py:48] [107000] global_step=107000, grad_norm=1.809744119644165, loss=1.756618618965149
I0202 13:33:14.404660 139907737556736 logging_writer.py:48] [107100] global_step=107100, grad_norm=2.015134572982788, loss=2.0137710571289062
I0202 13:33:47.941397 139908710635264 logging_writer.py:48] [107200] global_step=107200, grad_norm=1.9169607162475586, loss=2.0314509868621826
I0202 13:34:21.454088 139907737556736 logging_writer.py:48] [107300] global_step=107300, grad_norm=1.7923306226730347, loss=1.9344145059585571
I0202 13:34:55.053414 139908710635264 logging_writer.py:48] [107400] global_step=107400, grad_norm=1.881078839302063, loss=1.9210000038146973
I0202 13:35:28.582561 139907737556736 logging_writer.py:48] [107500] global_step=107500, grad_norm=1.7399663925170898, loss=1.9172749519348145
I0202 13:36:02.148531 139908710635264 logging_writer.py:48] [107600] global_step=107600, grad_norm=1.8416236639022827, loss=1.79588782787323
I0202 13:36:35.817308 139907737556736 logging_writer.py:48] [107700] global_step=107700, grad_norm=1.7609612941741943, loss=1.969313144683838
I0202 13:37:09.368904 139908710635264 logging_writer.py:48] [107800] global_step=107800, grad_norm=1.7444618940353394, loss=1.8927202224731445
I0202 13:37:24.627368 140070692116288 spec.py:321] Evaluating on the training split.
I0202 13:37:30.903419 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 13:37:39.260725 140070692116288 spec.py:349] Evaluating on the test split.
I0202 13:37:41.983941 140070692116288 submission_runner.py:408] Time since start: 37519.58s, 	Step: 107847, 	{'train/accuracy': 0.4673548936843872, 'train/loss': 2.4306631088256836, 'validation/accuracy': 0.4399999976158142, 'validation/loss': 2.6335792541503906, 'validation/num_examples': 50000, 'test/accuracy': 0.3248000144958496, 'test/loss': 3.541719675064087, 'test/num_examples': 10000, 'score': 36248.5503616333, 'total_duration': 37519.58160948753, 'accumulated_submission_time': 36248.5503616333, 'accumulated_eval_time': 1264.6096456050873, 'accumulated_logging_time': 2.6608407497406006}
I0202 13:37:42.019553 139907754342144 logging_writer.py:48] [107847] accumulated_eval_time=1264.609646, accumulated_logging_time=2.660841, accumulated_submission_time=36248.550362, global_step=107847, preemption_count=0, score=36248.550362, test/accuracy=0.324800, test/loss=3.541720, test/num_examples=10000, total_duration=37519.581609, train/accuracy=0.467355, train/loss=2.430663, validation/accuracy=0.440000, validation/loss=2.633579, validation/num_examples=50000
I0202 13:38:00.123154 139907762734848 logging_writer.py:48] [107900] global_step=107900, grad_norm=1.9224435091018677, loss=1.982827067375183
I0202 13:38:33.596520 139907754342144 logging_writer.py:48] [108000] global_step=108000, grad_norm=1.7618438005447388, loss=1.9546751976013184
I0202 13:39:07.187670 139907762734848 logging_writer.py:48] [108100] global_step=108100, grad_norm=1.862459421157837, loss=1.975987195968628
I0202 13:39:40.785072 139907754342144 logging_writer.py:48] [108200] global_step=108200, grad_norm=1.8290451765060425, loss=1.9207422733306885
I0202 13:40:14.299739 139907762734848 logging_writer.py:48] [108300] global_step=108300, grad_norm=1.8132795095443726, loss=1.9408704042434692
I0202 13:40:47.843135 139907754342144 logging_writer.py:48] [108400] global_step=108400, grad_norm=1.7997084856033325, loss=1.94711434841156
I0202 13:41:21.365195 139907762734848 logging_writer.py:48] [108500] global_step=108500, grad_norm=1.8450431823730469, loss=1.834789752960205
I0202 13:41:54.869613 139907754342144 logging_writer.py:48] [108600] global_step=108600, grad_norm=2.1676089763641357, loss=2.0234761238098145
I0202 13:42:28.462657 139907762734848 logging_writer.py:48] [108700] global_step=108700, grad_norm=2.0059587955474854, loss=1.882750153541565
I0202 13:43:02.135739 139907754342144 logging_writer.py:48] [108800] global_step=108800, grad_norm=1.9849034547805786, loss=2.0018670558929443
I0202 13:43:35.700642 139907762734848 logging_writer.py:48] [108900] global_step=108900, grad_norm=1.8122520446777344, loss=1.876421332359314
I0202 13:44:09.227068 139907754342144 logging_writer.py:48] [109000] global_step=109000, grad_norm=1.756109595298767, loss=1.8712725639343262
I0202 13:44:42.785009 139907762734848 logging_writer.py:48] [109100] global_step=109100, grad_norm=2.0536153316497803, loss=1.9020094871520996
I0202 13:45:16.339375 139907754342144 logging_writer.py:48] [109200] global_step=109200, grad_norm=1.6894599199295044, loss=1.784919261932373
I0202 13:45:49.869634 139907762734848 logging_writer.py:48] [109300] global_step=109300, grad_norm=1.74686861038208, loss=1.8744103908538818
I0202 13:46:12.162633 140070692116288 spec.py:321] Evaluating on the training split.
I0202 13:46:18.531812 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 13:46:26.846019 140070692116288 spec.py:349] Evaluating on the test split.
I0202 13:46:29.492554 140070692116288 submission_runner.py:408] Time since start: 38047.09s, 	Step: 109368, 	{'train/accuracy': 0.5494060516357422, 'train/loss': 1.9401334524154663, 'validation/accuracy': 0.510699987411499, 'validation/loss': 2.1658456325531006, 'validation/num_examples': 50000, 'test/accuracy': 0.388700008392334, 'test/loss': 3.003093719482422, 'test/num_examples': 10000, 'score': 36758.63085794449, 'total_duration': 38047.09022331238, 'accumulated_submission_time': 36758.63085794449, 'accumulated_eval_time': 1281.939534187317, 'accumulated_logging_time': 2.70654034614563}
I0202 13:46:29.532357 139907745949440 logging_writer.py:48] [109368] accumulated_eval_time=1281.939534, accumulated_logging_time=2.706540, accumulated_submission_time=36758.630858, global_step=109368, preemption_count=0, score=36758.630858, test/accuracy=0.388700, test/loss=3.003094, test/num_examples=10000, total_duration=38047.090223, train/accuracy=0.549406, train/loss=1.940133, validation/accuracy=0.510700, validation/loss=2.165846, validation/num_examples=50000
I0202 13:46:40.588856 139908710635264 logging_writer.py:48] [109400] global_step=109400, grad_norm=1.9936603307724, loss=1.919231653213501
I0202 13:47:14.058113 139907745949440 logging_writer.py:48] [109500] global_step=109500, grad_norm=1.8907521963119507, loss=1.9064990282058716
I0202 13:47:47.560585 139908710635264 logging_writer.py:48] [109600] global_step=109600, grad_norm=2.0832769870758057, loss=1.9050683975219727
I0202 13:48:21.101120 139907745949440 logging_writer.py:48] [109700] global_step=109700, grad_norm=1.829795002937317, loss=1.8280370235443115
I0202 13:48:54.615112 139908710635264 logging_writer.py:48] [109800] global_step=109800, grad_norm=1.876150131225586, loss=1.8564289808273315
I0202 13:49:28.219461 139907745949440 logging_writer.py:48] [109900] global_step=109900, grad_norm=2.179454803466797, loss=1.926456332206726
I0202 13:50:01.748218 139908710635264 logging_writer.py:48] [110000] global_step=110000, grad_norm=1.8093122243881226, loss=1.8694109916687012
I0202 13:50:35.313542 139907745949440 logging_writer.py:48] [110100] global_step=110100, grad_norm=1.9435265064239502, loss=1.7995941638946533
I0202 13:51:08.857383 139908710635264 logging_writer.py:48] [110200] global_step=110200, grad_norm=1.841015100479126, loss=1.9052457809448242
I0202 13:51:42.374702 139907745949440 logging_writer.py:48] [110300] global_step=110300, grad_norm=1.9282196760177612, loss=1.868387222290039
I0202 13:52:15.870888 139908710635264 logging_writer.py:48] [110400] global_step=110400, grad_norm=1.8760669231414795, loss=2.0165228843688965
I0202 13:52:49.429243 139907745949440 logging_writer.py:48] [110500] global_step=110500, grad_norm=1.906361699104309, loss=1.9448068141937256
I0202 13:53:22.953712 139908710635264 logging_writer.py:48] [110600] global_step=110600, grad_norm=1.7501980066299438, loss=1.846003532409668
I0202 13:53:56.524086 139907745949440 logging_writer.py:48] [110700] global_step=110700, grad_norm=1.8796825408935547, loss=1.9428150653839111
I0202 13:54:30.050528 139908710635264 logging_writer.py:48] [110800] global_step=110800, grad_norm=1.9578675031661987, loss=1.975363850593567
I0202 13:54:59.707930 140070692116288 spec.py:321] Evaluating on the training split.
I0202 13:55:06.169801 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 13:55:14.549077 140070692116288 spec.py:349] Evaluating on the test split.
I0202 13:55:17.183844 140070692116288 submission_runner.py:408] Time since start: 38574.78s, 	Step: 110890, 	{'train/accuracy': 0.6084582209587097, 'train/loss': 1.6108510494232178, 'validation/accuracy': 0.5411800146102905, 'validation/loss': 1.9688493013381958, 'validation/num_examples': 50000, 'test/accuracy': 0.42670002579689026, 'test/loss': 2.7162270545959473, 'test/num_examples': 10000, 'score': 37268.7436478138, 'total_duration': 38574.781507730484, 'accumulated_submission_time': 37268.7436478138, 'accumulated_eval_time': 1299.4154126644135, 'accumulated_logging_time': 2.756378173828125}
I0202 13:55:17.221675 139907737556736 logging_writer.py:48] [110890] accumulated_eval_time=1299.415413, accumulated_logging_time=2.756378, accumulated_submission_time=37268.743648, global_step=110890, preemption_count=0, score=37268.743648, test/accuracy=0.426700, test/loss=2.716227, test/num_examples=10000, total_duration=38574.781508, train/accuracy=0.608458, train/loss=1.610851, validation/accuracy=0.541180, validation/loss=1.968849, validation/num_examples=50000
I0202 13:55:20.921563 139907745949440 logging_writer.py:48] [110900] global_step=110900, grad_norm=2.168437957763672, loss=1.9064767360687256
I0202 13:55:54.527107 139907737556736 logging_writer.py:48] [111000] global_step=111000, grad_norm=1.9191563129425049, loss=2.0040392875671387
I0202 13:56:28.074798 139907745949440 logging_writer.py:48] [111100] global_step=111100, grad_norm=1.7804253101348877, loss=1.9168169498443604
I0202 13:57:01.603497 139907737556736 logging_writer.py:48] [111200] global_step=111200, grad_norm=1.9371930360794067, loss=1.7640106678009033
I0202 13:57:35.199129 139907745949440 logging_writer.py:48] [111300] global_step=111300, grad_norm=1.873482584953308, loss=1.8604480028152466
I0202 13:58:08.694525 139907737556736 logging_writer.py:48] [111400] global_step=111400, grad_norm=1.859359860420227, loss=1.8253358602523804
I0202 13:58:42.292539 139907745949440 logging_writer.py:48] [111500] global_step=111500, grad_norm=1.9973763227462769, loss=1.9421240091323853
I0202 13:59:15.796391 139907737556736 logging_writer.py:48] [111600] global_step=111600, grad_norm=1.866664171218872, loss=1.8524020910263062
I0202 13:59:49.379518 139907745949440 logging_writer.py:48] [111700] global_step=111700, grad_norm=2.0864689350128174, loss=1.8728139400482178
I0202 14:00:22.901237 139907737556736 logging_writer.py:48] [111800] global_step=111800, grad_norm=1.8901616334915161, loss=1.8885812759399414
I0202 14:00:56.410548 139907745949440 logging_writer.py:48] [111900] global_step=111900, grad_norm=1.9175939559936523, loss=1.7981451749801636
I0202 14:01:29.995674 139907737556736 logging_writer.py:48] [112000] global_step=112000, grad_norm=2.034550905227661, loss=1.82682466506958
I0202 14:02:03.513306 139907745949440 logging_writer.py:48] [112100] global_step=112100, grad_norm=1.931143045425415, loss=1.856722354888916
I0202 14:02:37.118112 139907737556736 logging_writer.py:48] [112200] global_step=112200, grad_norm=1.8926641941070557, loss=1.8040668964385986
I0202 14:03:10.667442 139907745949440 logging_writer.py:48] [112300] global_step=112300, grad_norm=1.8866711854934692, loss=1.8719408512115479
I0202 14:03:44.203860 139907737556736 logging_writer.py:48] [112400] global_step=112400, grad_norm=2.0544567108154297, loss=1.9671196937561035
I0202 14:03:47.367860 140070692116288 spec.py:321] Evaluating on the training split.
I0202 14:03:53.639670 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 14:04:02.247809 140070692116288 spec.py:349] Evaluating on the test split.
I0202 14:04:04.918699 140070692116288 submission_runner.py:408] Time since start: 39102.52s, 	Step: 112411, 	{'train/accuracy': 0.5785833597183228, 'train/loss': 1.7491344213485718, 'validation/accuracy': 0.5329799652099609, 'validation/loss': 2.0025932788848877, 'validation/num_examples': 50000, 'test/accuracy': 0.41670000553131104, 'test/loss': 2.758341073989868, 'test/num_examples': 10000, 'score': 37778.82757425308, 'total_duration': 39102.51637029648, 'accumulated_submission_time': 37778.82757425308, 'accumulated_eval_time': 1316.9662177562714, 'accumulated_logging_time': 2.803908109664917}
I0202 14:04:04.954761 139907745949440 logging_writer.py:48] [112411] accumulated_eval_time=1316.966218, accumulated_logging_time=2.803908, accumulated_submission_time=37778.827574, global_step=112411, preemption_count=0, score=37778.827574, test/accuracy=0.416700, test/loss=2.758341, test/num_examples=10000, total_duration=39102.516370, train/accuracy=0.578583, train/loss=1.749134, validation/accuracy=0.532980, validation/loss=2.002593, validation/num_examples=50000
I0202 14:04:35.072747 139908710635264 logging_writer.py:48] [112500] global_step=112500, grad_norm=1.9803105592727661, loss=1.8336162567138672
I0202 14:05:08.549185 139907745949440 logging_writer.py:48] [112600] global_step=112600, grad_norm=1.8300811052322388, loss=1.9059723615646362
I0202 14:05:42.075811 139908710635264 logging_writer.py:48] [112700] global_step=112700, grad_norm=1.9568393230438232, loss=1.8005832433700562
I0202 14:06:15.585688 139907745949440 logging_writer.py:48] [112800] global_step=112800, grad_norm=1.9402095079421997, loss=1.7782421112060547
I0202 14:06:49.093527 139908710635264 logging_writer.py:48] [112900] global_step=112900, grad_norm=1.9695450067520142, loss=1.8838013410568237
I0202 14:07:22.656010 139907745949440 logging_writer.py:48] [113000] global_step=113000, grad_norm=1.8741999864578247, loss=1.8817379474639893
I0202 14:07:56.195261 139908710635264 logging_writer.py:48] [113100] global_step=113100, grad_norm=2.0276174545288086, loss=1.8286182880401611
I0202 14:08:29.739453 139907745949440 logging_writer.py:48] [113200] global_step=113200, grad_norm=1.9565318822860718, loss=1.9000709056854248
I0202 14:09:03.344601 139908710635264 logging_writer.py:48] [113300] global_step=113300, grad_norm=2.0433852672576904, loss=1.8073807954788208
I0202 14:09:36.841156 139907745949440 logging_writer.py:48] [113400] global_step=113400, grad_norm=2.066441774368286, loss=1.902872085571289
I0202 14:10:10.381108 139908710635264 logging_writer.py:48] [113500] global_step=113500, grad_norm=1.8634746074676514, loss=1.8714394569396973
I0202 14:10:43.892277 139907745949440 logging_writer.py:48] [113600] global_step=113600, grad_norm=1.9453799724578857, loss=1.8268648386001587
I0202 14:11:17.437041 139908710635264 logging_writer.py:48] [113700] global_step=113700, grad_norm=1.9932767152786255, loss=1.8622255325317383
I0202 14:11:51.002279 139907745949440 logging_writer.py:48] [113800] global_step=113800, grad_norm=1.8047707080841064, loss=1.8379138708114624
I0202 14:12:24.526126 139908710635264 logging_writer.py:48] [113900] global_step=113900, grad_norm=2.1812000274658203, loss=1.9687767028808594
I0202 14:12:35.070404 140070692116288 spec.py:321] Evaluating on the training split.
I0202 14:12:41.358392 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 14:12:49.861816 140070692116288 spec.py:349] Evaluating on the test split.
I0202 14:12:52.521975 140070692116288 submission_runner.py:408] Time since start: 39630.12s, 	Step: 113933, 	{'train/accuracy': 0.5904615521430969, 'train/loss': 1.7069220542907715, 'validation/accuracy': 0.5450599789619446, 'validation/loss': 1.9861260652542114, 'validation/num_examples': 50000, 'test/accuracy': 0.41130003333091736, 'test/loss': 2.8443455696105957, 'test/num_examples': 10000, 'score': 38288.882075071335, 'total_duration': 39630.119643211365, 'accumulated_submission_time': 38288.882075071335, 'accumulated_eval_time': 1334.4177539348602, 'accumulated_logging_time': 2.848686695098877}
I0202 14:12:52.563265 139907762734848 logging_writer.py:48] [113933] accumulated_eval_time=1334.417754, accumulated_logging_time=2.848687, accumulated_submission_time=38288.882075, global_step=113933, preemption_count=0, score=38288.882075, test/accuracy=0.411300, test/loss=2.844346, test/num_examples=10000, total_duration=39630.119643, train/accuracy=0.590462, train/loss=1.706922, validation/accuracy=0.545060, validation/loss=1.986126, validation/num_examples=50000
I0202 14:13:15.356331 139908425447168 logging_writer.py:48] [114000] global_step=114000, grad_norm=1.9965577125549316, loss=1.8561760187149048
I0202 14:13:48.861419 139907762734848 logging_writer.py:48] [114100] global_step=114100, grad_norm=1.9279327392578125, loss=1.830165147781372
I0202 14:14:22.423071 139908425447168 logging_writer.py:48] [114200] global_step=114200, grad_norm=2.0212018489837646, loss=1.8305398225784302
I0202 14:14:55.980360 139907762734848 logging_writer.py:48] [114300] global_step=114300, grad_norm=2.1035022735595703, loss=1.7323081493377686
I0202 14:15:29.713463 139908425447168 logging_writer.py:48] [114400] global_step=114400, grad_norm=1.9441182613372803, loss=1.835974931716919
I0202 14:16:03.239783 139907762734848 logging_writer.py:48] [114500] global_step=114500, grad_norm=1.9797751903533936, loss=1.8476290702819824
I0202 14:16:36.821535 139908425447168 logging_writer.py:48] [114600] global_step=114600, grad_norm=1.8494329452514648, loss=1.7733041048049927
I0202 14:17:10.333433 139907762734848 logging_writer.py:48] [114700] global_step=114700, grad_norm=1.9783307313919067, loss=1.8023021221160889
I0202 14:17:43.896234 139908425447168 logging_writer.py:48] [114800] global_step=114800, grad_norm=1.7899351119995117, loss=1.7699124813079834
I0202 14:18:17.393485 139907762734848 logging_writer.py:48] [114900] global_step=114900, grad_norm=2.025998830795288, loss=1.8629183769226074
I0202 14:18:50.977313 139908425447168 logging_writer.py:48] [115000] global_step=115000, grad_norm=2.000710964202881, loss=1.7630624771118164
I0202 14:19:24.503874 139907762734848 logging_writer.py:48] [115100] global_step=115100, grad_norm=2.0182321071624756, loss=1.8268786668777466
I0202 14:19:58.050467 139908425447168 logging_writer.py:48] [115200] global_step=115200, grad_norm=2.1029045581817627, loss=1.9335626363754272
I0202 14:20:31.584496 139907762734848 logging_writer.py:48] [115300] global_step=115300, grad_norm=2.1628241539001465, loss=1.8668955564498901
I0202 14:21:05.170203 139908425447168 logging_writer.py:48] [115400] global_step=115400, grad_norm=2.0963997840881348, loss=1.8795307874679565
I0202 14:21:22.749854 140070692116288 spec.py:321] Evaluating on the training split.
I0202 14:21:29.215677 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 14:21:37.662186 140070692116288 spec.py:349] Evaluating on the test split.
I0202 14:21:40.290568 140070692116288 submission_runner.py:408] Time since start: 40157.89s, 	Step: 115454, 	{'train/accuracy': 0.5925542116165161, 'train/loss': 1.6781911849975586, 'validation/accuracy': 0.5508800148963928, 'validation/loss': 1.918237566947937, 'validation/num_examples': 50000, 'test/accuracy': 0.42600002884864807, 'test/loss': 2.6891703605651855, 'test/num_examples': 10000, 'score': 38799.00455093384, 'total_duration': 40157.88823246956, 'accumulated_submission_time': 38799.00455093384, 'accumulated_eval_time': 1351.9584307670593, 'accumulated_logging_time': 2.899632215499878}
I0202 14:21:40.333208 139907737556736 logging_writer.py:48] [115454] accumulated_eval_time=1351.958431, accumulated_logging_time=2.899632, accumulated_submission_time=38799.004551, global_step=115454, preemption_count=0, score=38799.004551, test/accuracy=0.426000, test/loss=2.689170, test/num_examples=10000, total_duration=40157.888232, train/accuracy=0.592554, train/loss=1.678191, validation/accuracy=0.550880, validation/loss=1.918238, validation/num_examples=50000
I0202 14:21:56.092728 139907745949440 logging_writer.py:48] [115500] global_step=115500, grad_norm=2.0358808040618896, loss=1.9084293842315674
I0202 14:22:29.582839 139907737556736 logging_writer.py:48] [115600] global_step=115600, grad_norm=2.2821712493896484, loss=1.7499291896820068
I0202 14:23:03.166368 139907745949440 logging_writer.py:48] [115700] global_step=115700, grad_norm=1.8983139991760254, loss=1.9280227422714233
I0202 14:23:36.684642 139907737556736 logging_writer.py:48] [115800] global_step=115800, grad_norm=2.1014065742492676, loss=1.8075941801071167
I0202 14:24:10.232109 139907745949440 logging_writer.py:48] [115900] global_step=115900, grad_norm=2.099494218826294, loss=1.6764709949493408
I0202 14:24:43.762786 139907737556736 logging_writer.py:48] [116000] global_step=116000, grad_norm=1.9494708776474, loss=1.749112606048584
I0202 14:25:17.349372 139907745949440 logging_writer.py:48] [116100] global_step=116100, grad_norm=2.119499921798706, loss=1.7892606258392334
I0202 14:25:50.886452 139907737556736 logging_writer.py:48] [116200] global_step=116200, grad_norm=1.9857215881347656, loss=1.9156897068023682
I0202 14:26:24.460691 139907745949440 logging_writer.py:48] [116300] global_step=116300, grad_norm=1.9031842947006226, loss=1.793359398841858
I0202 14:26:58.042001 139907737556736 logging_writer.py:48] [116400] global_step=116400, grad_norm=2.114218235015869, loss=1.8477251529693604
I0202 14:27:31.531421 139907745949440 logging_writer.py:48] [116500] global_step=116500, grad_norm=2.0070998668670654, loss=1.690901756286621
I0202 14:28:05.162051 139907737556736 logging_writer.py:48] [116600] global_step=116600, grad_norm=2.1995105743408203, loss=1.8593621253967285
I0202 14:28:38.752200 139907745949440 logging_writer.py:48] [116700] global_step=116700, grad_norm=2.234398603439331, loss=1.8700639009475708
I0202 14:29:12.299838 139907737556736 logging_writer.py:48] [116800] global_step=116800, grad_norm=1.9723409414291382, loss=1.7073495388031006
I0202 14:29:45.845251 139907745949440 logging_writer.py:48] [116900] global_step=116900, grad_norm=1.9320018291473389, loss=1.737432599067688
I0202 14:30:10.485411 140070692116288 spec.py:321] Evaluating on the training split.
I0202 14:30:16.803071 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 14:30:25.445701 140070692116288 spec.py:349] Evaluating on the test split.
I0202 14:30:28.119241 140070692116288 submission_runner.py:408] Time since start: 40685.72s, 	Step: 116975, 	{'train/accuracy': 0.43536749482154846, 'train/loss': 2.6714303493499756, 'validation/accuracy': 0.41241997480392456, 'validation/loss': 2.853731870651245, 'validation/num_examples': 50000, 'test/accuracy': 0.3038000166416168, 'test/loss': 3.7805306911468506, 'test/num_examples': 10000, 'score': 39309.09029126167, 'total_duration': 40685.7168943882, 'accumulated_submission_time': 39309.09029126167, 'accumulated_eval_time': 1369.5922305583954, 'accumulated_logging_time': 2.9562196731567383}
I0202 14:30:28.156760 139907737556736 logging_writer.py:48] [116975] accumulated_eval_time=1369.592231, accumulated_logging_time=2.956220, accumulated_submission_time=39309.090291, global_step=116975, preemption_count=0, score=39309.090291, test/accuracy=0.303800, test/loss=3.780531, test/num_examples=10000, total_duration=40685.716894, train/accuracy=0.435367, train/loss=2.671430, validation/accuracy=0.412420, validation/loss=2.853732, validation/num_examples=50000
I0202 14:30:36.869808 139908710635264 logging_writer.py:48] [117000] global_step=117000, grad_norm=2.0424654483795166, loss=1.8365373611450195
I0202 14:31:10.346740 139907737556736 logging_writer.py:48] [117100] global_step=117100, grad_norm=2.0866847038269043, loss=1.7562350034713745
I0202 14:31:43.860634 139908710635264 logging_writer.py:48] [117200] global_step=117200, grad_norm=2.1403748989105225, loss=1.9329047203063965
I0202 14:32:17.372706 139907737556736 logging_writer.py:48] [117300] global_step=117300, grad_norm=2.0169694423675537, loss=1.7969779968261719
I0202 14:32:50.947289 139908710635264 logging_writer.py:48] [117400] global_step=117400, grad_norm=2.144756555557251, loss=1.7555217742919922
I0202 14:33:24.480444 139907737556736 logging_writer.py:48] [117500] global_step=117500, grad_norm=1.9945863485336304, loss=1.7918916940689087
I0202 14:33:58.022974 139908710635264 logging_writer.py:48] [117600] global_step=117600, grad_norm=2.2313084602355957, loss=1.882425308227539
I0202 14:34:31.689174 139907737556736 logging_writer.py:48] [117700] global_step=117700, grad_norm=2.1588637828826904, loss=1.8112064599990845
I0202 14:35:05.269301 139908710635264 logging_writer.py:48] [117800] global_step=117800, grad_norm=2.163125991821289, loss=1.8961702585220337
I0202 14:35:38.783132 139907737556736 logging_writer.py:48] [117900] global_step=117900, grad_norm=2.120422124862671, loss=1.8422921895980835
I0202 14:36:12.317522 139908710635264 logging_writer.py:48] [118000] global_step=118000, grad_norm=2.0190584659576416, loss=1.8170018196105957
I0202 14:36:45.850041 139907737556736 logging_writer.py:48] [118100] global_step=118100, grad_norm=2.0665395259857178, loss=1.8403511047363281
I0202 14:37:19.396641 139908710635264 logging_writer.py:48] [118200] global_step=118200, grad_norm=2.323094129562378, loss=1.8303618431091309
I0202 14:37:52.997410 139907737556736 logging_writer.py:48] [118300] global_step=118300, grad_norm=2.1287143230438232, loss=1.8844292163848877
I0202 14:38:26.524210 139908710635264 logging_writer.py:48] [118400] global_step=118400, grad_norm=2.085555076599121, loss=1.7449315786361694
I0202 14:38:58.173372 140070692116288 spec.py:321] Evaluating on the training split.
I0202 14:39:04.682173 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 14:39:13.339698 140070692116288 spec.py:349] Evaluating on the test split.
I0202 14:39:15.976157 140070692116288 submission_runner.py:408] Time since start: 41213.57s, 	Step: 118496, 	{'train/accuracy': 0.5356146097183228, 'train/loss': 2.0111894607543945, 'validation/accuracy': 0.49865999817848206, 'validation/loss': 2.2324469089508057, 'validation/num_examples': 50000, 'test/accuracy': 0.37870001792907715, 'test/loss': 3.0569660663604736, 'test/num_examples': 10000, 'score': 39819.04309177399, 'total_duration': 41213.57382154465, 'accumulated_submission_time': 39819.04309177399, 'accumulated_eval_time': 1387.3949823379517, 'accumulated_logging_time': 3.004948377609253}
I0202 14:39:16.018016 139907762734848 logging_writer.py:48] [118496] accumulated_eval_time=1387.394982, accumulated_logging_time=3.004948, accumulated_submission_time=39819.043092, global_step=118496, preemption_count=0, score=39819.043092, test/accuracy=0.378700, test/loss=3.056966, test/num_examples=10000, total_duration=41213.573822, train/accuracy=0.535615, train/loss=2.011189, validation/accuracy=0.498660, validation/loss=2.232447, validation/num_examples=50000
I0202 14:39:17.703879 139908425447168 logging_writer.py:48] [118500] global_step=118500, grad_norm=2.0426504611968994, loss=1.7853840589523315
I0202 14:39:51.204842 139907762734848 logging_writer.py:48] [118600] global_step=118600, grad_norm=1.9554299116134644, loss=1.7418651580810547
I0202 14:40:24.698893 139908425447168 logging_writer.py:48] [118700] global_step=118700, grad_norm=2.081223964691162, loss=1.8309342861175537
I0202 14:40:58.296605 139907762734848 logging_writer.py:48] [118800] global_step=118800, grad_norm=2.1684799194335938, loss=1.8154438734054565
I0202 14:41:31.824174 139908425447168 logging_writer.py:48] [118900] global_step=118900, grad_norm=1.9434114694595337, loss=1.7506165504455566
I0202 14:42:05.395311 139907762734848 logging_writer.py:48] [119000] global_step=119000, grad_norm=2.087488889694214, loss=1.7546007633209229
I0202 14:42:38.931012 139908425447168 logging_writer.py:48] [119100] global_step=119100, grad_norm=2.1382508277893066, loss=1.8217476606369019
I0202 14:43:12.510009 139907762734848 logging_writer.py:48] [119200] global_step=119200, grad_norm=2.0644068717956543, loss=1.8528462648391724
I0202 14:43:46.071565 139908425447168 logging_writer.py:48] [119300] global_step=119300, grad_norm=1.995768666267395, loss=1.7415887117385864
I0202 14:44:19.588919 139907762734848 logging_writer.py:48] [119400] global_step=119400, grad_norm=2.064718246459961, loss=1.7799874544143677
I0202 14:44:53.132079 139908425447168 logging_writer.py:48] [119500] global_step=119500, grad_norm=1.972542405128479, loss=1.7904324531555176
I0202 14:45:26.676778 139907762734848 logging_writer.py:48] [119600] global_step=119600, grad_norm=2.160970687866211, loss=1.9394371509552002
I0202 14:46:00.210575 139908425447168 logging_writer.py:48] [119700] global_step=119700, grad_norm=2.1041653156280518, loss=1.808831810951233
I0202 14:46:33.792528 139907762734848 logging_writer.py:48] [119800] global_step=119800, grad_norm=2.1551711559295654, loss=1.8058627843856812
I0202 14:47:07.322324 139908425447168 logging_writer.py:48] [119900] global_step=119900, grad_norm=2.1927928924560547, loss=1.7831875085830688
I0202 14:47:40.905219 139907762734848 logging_writer.py:48] [120000] global_step=120000, grad_norm=2.1082401275634766, loss=1.8221769332885742
I0202 14:47:46.083655 140070692116288 spec.py:321] Evaluating on the training split.
I0202 14:47:52.375878 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 14:48:00.842799 140070692116288 spec.py:349] Evaluating on the test split.
I0202 14:48:03.708858 140070692116288 submission_runner.py:408] Time since start: 41741.31s, 	Step: 120017, 	{'train/accuracy': 0.5190728306770325, 'train/loss': 2.124185562133789, 'validation/accuracy': 0.463919997215271, 'validation/loss': 2.485764741897583, 'validation/num_examples': 50000, 'test/accuracy': 0.3614000082015991, 'test/loss': 3.282891273498535, 'test/num_examples': 10000, 'score': 40329.04471373558, 'total_duration': 41741.30652666092, 'accumulated_submission_time': 40329.04471373558, 'accumulated_eval_time': 1405.0201497077942, 'accumulated_logging_time': 3.0573155879974365}
I0202 14:48:03.749134 139907729164032 logging_writer.py:48] [120017] accumulated_eval_time=1405.020150, accumulated_logging_time=3.057316, accumulated_submission_time=40329.044714, global_step=120017, preemption_count=0, score=40329.044714, test/accuracy=0.361400, test/loss=3.282891, test/num_examples=10000, total_duration=41741.306527, train/accuracy=0.519073, train/loss=2.124186, validation/accuracy=0.463920, validation/loss=2.485765, validation/num_examples=50000
I0202 14:48:31.885603 139907737556736 logging_writer.py:48] [120100] global_step=120100, grad_norm=2.094986915588379, loss=1.7447162866592407
I0202 14:49:05.356234 139907729164032 logging_writer.py:48] [120200] global_step=120200, grad_norm=1.892068862915039, loss=1.7995988130569458
I0202 14:49:38.872275 139907737556736 logging_writer.py:48] [120300] global_step=120300, grad_norm=2.0665817260742188, loss=1.8782408237457275
I0202 14:50:12.370053 139907729164032 logging_writer.py:48] [120400] global_step=120400, grad_norm=2.0354902744293213, loss=1.7750446796417236
I0202 14:50:45.961654 139907737556736 logging_writer.py:48] [120500] global_step=120500, grad_norm=2.1717922687530518, loss=1.7184162139892578
I0202 14:51:19.498531 139907729164032 logging_writer.py:48] [120600] global_step=120600, grad_norm=2.147026777267456, loss=1.9579010009765625
I0202 14:51:53.032695 139907737556736 logging_writer.py:48] [120700] global_step=120700, grad_norm=2.153557777404785, loss=1.9500179290771484
I0202 14:52:26.579752 139907729164032 logging_writer.py:48] [120800] global_step=120800, grad_norm=2.2109146118164062, loss=1.8085870742797852
I0202 14:53:00.140420 139907737556736 logging_writer.py:48] [120900] global_step=120900, grad_norm=2.3792343139648438, loss=1.83450448513031
I0202 14:53:33.670404 139907729164032 logging_writer.py:48] [121000] global_step=121000, grad_norm=2.089829921722412, loss=1.8632664680480957
I0202 14:54:07.255028 139907737556736 logging_writer.py:48] [121100] global_step=121100, grad_norm=2.1222076416015625, loss=1.6312278509140015
I0202 14:54:40.750789 139907729164032 logging_writer.py:48] [121200] global_step=121200, grad_norm=2.316450834274292, loss=1.8163392543792725
I0202 14:55:14.314445 139907737556736 logging_writer.py:48] [121300] global_step=121300, grad_norm=2.0297157764434814, loss=1.699097990989685
I0202 14:55:47.828371 139907729164032 logging_writer.py:48] [121400] global_step=121400, grad_norm=2.2207634449005127, loss=1.7794435024261475
I0202 14:56:21.376037 139907737556736 logging_writer.py:48] [121500] global_step=121500, grad_norm=2.056433916091919, loss=1.779746651649475
I0202 14:56:33.958287 140070692116288 spec.py:321] Evaluating on the training split.
I0202 14:56:40.262078 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 14:56:48.895523 140070692116288 spec.py:349] Evaluating on the test split.
I0202 14:56:51.393783 140070692116288 submission_runner.py:408] Time since start: 42268.99s, 	Step: 121539, 	{'train/accuracy': 0.5934311151504517, 'train/loss': 1.6687780618667603, 'validation/accuracy': 0.545740008354187, 'validation/loss': 1.940511703491211, 'validation/num_examples': 50000, 'test/accuracy': 0.4230000078678131, 'test/loss': 2.734823703765869, 'test/num_examples': 10000, 'score': 40839.1926074028, 'total_duration': 42268.991443395615, 'accumulated_submission_time': 40839.1926074028, 'accumulated_eval_time': 1422.4556045532227, 'accumulated_logging_time': 3.1066973209381104}
I0202 14:56:51.434913 139908425447168 logging_writer.py:48] [121539] accumulated_eval_time=1422.455605, accumulated_logging_time=3.106697, accumulated_submission_time=40839.192607, global_step=121539, preemption_count=0, score=40839.192607, test/accuracy=0.423000, test/loss=2.734824, test/num_examples=10000, total_duration=42268.991443, train/accuracy=0.593431, train/loss=1.668778, validation/accuracy=0.545740, validation/loss=1.940512, validation/num_examples=50000
I0202 14:57:12.189699 139908710635264 logging_writer.py:48] [121600] global_step=121600, grad_norm=2.2517101764678955, loss=1.8509533405303955
I0202 14:57:45.694700 139908425447168 logging_writer.py:48] [121700] global_step=121700, grad_norm=2.349381685256958, loss=1.8847863674163818
I0202 14:58:19.230148 139908710635264 logging_writer.py:48] [121800] global_step=121800, grad_norm=2.178818941116333, loss=1.7708255052566528
I0202 14:58:52.820196 139908425447168 logging_writer.py:48] [121900] global_step=121900, grad_norm=2.28401255607605, loss=1.924233317375183
I0202 14:59:26.349632 139908710635264 logging_writer.py:48] [122000] global_step=122000, grad_norm=2.204907178878784, loss=1.6983202695846558
I0202 14:59:59.932791 139908425447168 logging_writer.py:48] [122100] global_step=122100, grad_norm=2.1484146118164062, loss=1.8357841968536377
I0202 15:00:33.520863 139908710635264 logging_writer.py:48] [122200] global_step=122200, grad_norm=2.2729082107543945, loss=1.7232483625411987
I0202 15:01:07.011379 139908425447168 logging_writer.py:48] [122300] global_step=122300, grad_norm=2.1523404121398926, loss=1.7978925704956055
I0202 15:01:40.533233 139908710635264 logging_writer.py:48] [122400] global_step=122400, grad_norm=2.280888557434082, loss=1.7679228782653809
I0202 15:02:14.047809 139908425447168 logging_writer.py:48] [122500] global_step=122500, grad_norm=2.2438864707946777, loss=1.7758853435516357
I0202 15:02:47.537833 139908710635264 logging_writer.py:48] [122600] global_step=122600, grad_norm=2.3872480392456055, loss=1.85369074344635
I0202 15:03:21.040467 139908425447168 logging_writer.py:48] [122700] global_step=122700, grad_norm=2.2390785217285156, loss=1.7143861055374146
I0202 15:03:54.561179 139908710635264 logging_writer.py:48] [122800] global_step=122800, grad_norm=2.188452959060669, loss=1.822535514831543
I0202 15:04:28.058171 139908425447168 logging_writer.py:48] [122900] global_step=122900, grad_norm=2.2860262393951416, loss=1.6756839752197266
I0202 15:05:01.579138 139908710635264 logging_writer.py:48] [123000] global_step=123000, grad_norm=2.5553126335144043, loss=1.8179513216018677
I0202 15:05:21.507525 140070692116288 spec.py:321] Evaluating on the training split.
I0202 15:05:27.813248 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 15:05:36.451130 140070692116288 spec.py:349] Evaluating on the test split.
I0202 15:05:39.077373 140070692116288 submission_runner.py:408] Time since start: 42796.68s, 	Step: 123061, 	{'train/accuracy': 0.5442841053009033, 'train/loss': 1.9463707208633423, 'validation/accuracy': 0.5045599937438965, 'validation/loss': 2.186814069747925, 'validation/num_examples': 50000, 'test/accuracy': 0.3936000168323517, 'test/loss': 2.9241855144500732, 'test/num_examples': 10000, 'score': 41349.20190811157, 'total_duration': 42796.67501139641, 'accumulated_submission_time': 41349.20190811157, 'accumulated_eval_time': 1440.025390625, 'accumulated_logging_time': 3.1579654216766357}
I0202 15:05:39.119966 139907754342144 logging_writer.py:48] [123061] accumulated_eval_time=1440.025391, accumulated_logging_time=3.157965, accumulated_submission_time=41349.201908, global_step=123061, preemption_count=0, score=41349.201908, test/accuracy=0.393600, test/loss=2.924186, test/num_examples=10000, total_duration=42796.675011, train/accuracy=0.544284, train/loss=1.946371, validation/accuracy=0.504560, validation/loss=2.186814, validation/num_examples=50000
I0202 15:05:52.523740 139907762734848 logging_writer.py:48] [123100] global_step=123100, grad_norm=2.1723265647888184, loss=1.7720829248428345
I0202 15:06:26.019249 139907754342144 logging_writer.py:48] [123200] global_step=123200, grad_norm=2.2045657634735107, loss=1.79850435256958
I0202 15:06:59.671839 139907762734848 logging_writer.py:48] [123300] global_step=123300, grad_norm=2.0616841316223145, loss=1.704223394393921
I0202 15:07:33.209199 139907754342144 logging_writer.py:48] [123400] global_step=123400, grad_norm=2.3515498638153076, loss=1.7502177953720093
I0202 15:08:06.807786 139907762734848 logging_writer.py:48] [123500] global_step=123500, grad_norm=2.567528486251831, loss=1.9072074890136719
I0202 15:08:40.331616 139907754342144 logging_writer.py:48] [123600] global_step=123600, grad_norm=2.2345526218414307, loss=1.7123339176177979
I0202 15:09:13.900269 139907762734848 logging_writer.py:48] [123700] global_step=123700, grad_norm=2.1477720737457275, loss=1.7292245626449585
I0202 15:09:47.433468 139907754342144 logging_writer.py:48] [123800] global_step=123800, grad_norm=2.592313766479492, loss=1.8319982290267944
I0202 15:10:21.018679 139907762734848 logging_writer.py:48] [123900] global_step=123900, grad_norm=2.3057656288146973, loss=1.7473536729812622
I0202 15:10:54.542631 139907754342144 logging_writer.py:48] [124000] global_step=124000, grad_norm=2.173288106918335, loss=1.6328237056732178
I0202 15:11:28.101837 139907762734848 logging_writer.py:48] [124100] global_step=124100, grad_norm=2.132288932800293, loss=1.6846930980682373
I0202 15:12:01.632774 139907754342144 logging_writer.py:48] [124200] global_step=124200, grad_norm=2.175291061401367, loss=1.7915623188018799
I0202 15:12:35.142993 139907762734848 logging_writer.py:48] [124300] global_step=124300, grad_norm=2.2013039588928223, loss=1.8230817317962646
I0202 15:13:08.763114 139907754342144 logging_writer.py:48] [124400] global_step=124400, grad_norm=2.1814651489257812, loss=1.7565290927886963
I0202 15:13:42.329607 139907762734848 logging_writer.py:48] [124500] global_step=124500, grad_norm=2.2468249797821045, loss=1.7552695274353027
I0202 15:14:09.267690 140070692116288 spec.py:321] Evaluating on the training split.
I0202 15:14:15.567466 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 15:14:24.079612 140070692116288 spec.py:349] Evaluating on the test split.
I0202 15:14:26.777123 140070692116288 submission_runner.py:408] Time since start: 43324.37s, 	Step: 124582, 	{'train/accuracy': 0.6316764950752258, 'train/loss': 1.5131675004959106, 'validation/accuracy': 0.5785199999809265, 'validation/loss': 1.7937418222427368, 'validation/num_examples': 50000, 'test/accuracy': 0.45840001106262207, 'test/loss': 2.5650177001953125, 'test/num_examples': 10000, 'score': 41859.287153720856, 'total_duration': 43324.37479400635, 'accumulated_submission_time': 41859.287153720856, 'accumulated_eval_time': 1457.5347940921783, 'accumulated_logging_time': 3.2097365856170654}
I0202 15:14:26.815907 139908425447168 logging_writer.py:48] [124582] accumulated_eval_time=1457.534794, accumulated_logging_time=3.209737, accumulated_submission_time=41859.287154, global_step=124582, preemption_count=0, score=41859.287154, test/accuracy=0.458400, test/loss=2.565018, test/num_examples=10000, total_duration=43324.374794, train/accuracy=0.631676, train/loss=1.513168, validation/accuracy=0.578520, validation/loss=1.793742, validation/num_examples=50000
I0202 15:14:33.185895 139908710635264 logging_writer.py:48] [124600] global_step=124600, grad_norm=2.206770896911621, loss=1.6858017444610596
I0202 15:15:06.636141 139908425447168 logging_writer.py:48] [124700] global_step=124700, grad_norm=2.3761091232299805, loss=1.746595859527588
I0202 15:15:40.148499 139908710635264 logging_writer.py:48] [124800] global_step=124800, grad_norm=2.2992544174194336, loss=1.771243691444397
I0202 15:16:13.656183 139908425447168 logging_writer.py:48] [124900] global_step=124900, grad_norm=2.521137237548828, loss=1.816677212715149
I0202 15:16:47.171686 139908710635264 logging_writer.py:48] [125000] global_step=125000, grad_norm=2.4445865154266357, loss=1.7387104034423828
I0202 15:17:20.697446 139908425447168 logging_writer.py:48] [125100] global_step=125100, grad_norm=2.361372947692871, loss=1.8236163854599
I0202 15:17:54.283877 139908710635264 logging_writer.py:48] [125200] global_step=125200, grad_norm=2.27083158493042, loss=1.7875568866729736
I0202 15:18:27.813385 139908425447168 logging_writer.py:48] [125300] global_step=125300, grad_norm=2.3518521785736084, loss=1.7586896419525146
I0202 15:19:01.376948 139908710635264 logging_writer.py:48] [125400] global_step=125400, grad_norm=2.395358085632324, loss=1.7915757894515991
I0202 15:19:35.002352 139908425447168 logging_writer.py:48] [125500] global_step=125500, grad_norm=2.328432559967041, loss=1.835519552230835
I0202 15:20:08.575611 139908710635264 logging_writer.py:48] [125600] global_step=125600, grad_norm=2.318721055984497, loss=1.7792776823043823
I0202 15:20:42.098993 139908425447168 logging_writer.py:48] [125700] global_step=125700, grad_norm=2.324082851409912, loss=1.7551072835922241
I0202 15:21:15.689900 139908710635264 logging_writer.py:48] [125800] global_step=125800, grad_norm=2.1543972492218018, loss=1.5641921758651733
I0202 15:21:49.258024 139908425447168 logging_writer.py:48] [125900] global_step=125900, grad_norm=2.279564380645752, loss=1.7717065811157227
I0202 15:22:22.775052 139908710635264 logging_writer.py:48] [126000] global_step=126000, grad_norm=2.485283613204956, loss=1.9119747877120972
I0202 15:22:56.331127 139908425447168 logging_writer.py:48] [126100] global_step=126100, grad_norm=2.3503220081329346, loss=1.6019166707992554
I0202 15:22:56.818490 140070692116288 spec.py:321] Evaluating on the training split.
I0202 15:23:03.266839 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 15:23:12.047997 140070692116288 spec.py:349] Evaluating on the test split.
I0202 15:23:14.635292 140070692116288 submission_runner.py:408] Time since start: 43852.23s, 	Step: 126103, 	{'train/accuracy': 0.630301296710968, 'train/loss': 1.5103756189346313, 'validation/accuracy': 0.5861600041389465, 'validation/loss': 1.7711243629455566, 'validation/num_examples': 50000, 'test/accuracy': 0.4564000070095062, 'test/loss': 2.564093589782715, 'test/num_examples': 10000, 'score': 42369.22559094429, 'total_duration': 43852.23295521736, 'accumulated_submission_time': 42369.22559094429, 'accumulated_eval_time': 1475.3515548706055, 'accumulated_logging_time': 3.2582929134368896}
I0202 15:23:14.673800 139907729164032 logging_writer.py:48] [126103] accumulated_eval_time=1475.351555, accumulated_logging_time=3.258293, accumulated_submission_time=42369.225591, global_step=126103, preemption_count=0, score=42369.225591, test/accuracy=0.456400, test/loss=2.564094, test/num_examples=10000, total_duration=43852.232955, train/accuracy=0.630301, train/loss=1.510376, validation/accuracy=0.586160, validation/loss=1.771124, validation/num_examples=50000
I0202 15:23:47.524140 139907737556736 logging_writer.py:48] [126200] global_step=126200, grad_norm=2.427062511444092, loss=1.7519193887710571
I0202 15:24:21.063421 139907729164032 logging_writer.py:48] [126300] global_step=126300, grad_norm=2.223351001739502, loss=1.7123628854751587
I0202 15:24:54.589816 139907737556736 logging_writer.py:48] [126400] global_step=126400, grad_norm=2.2332215309143066, loss=1.6751947402954102
I0202 15:25:28.126213 139907729164032 logging_writer.py:48] [126500] global_step=126500, grad_norm=2.259132146835327, loss=1.7018448114395142
I0202 15:26:01.915675 139907737556736 logging_writer.py:48] [126600] global_step=126600, grad_norm=2.1252644062042236, loss=1.5730994939804077
I0202 15:26:35.442034 139907729164032 logging_writer.py:48] [126700] global_step=126700, grad_norm=2.2668373584747314, loss=1.6269879341125488
I0202 15:27:08.960341 139907737556736 logging_writer.py:48] [126800] global_step=126800, grad_norm=2.4031152725219727, loss=1.6755201816558838
I0202 15:27:42.517378 139907729164032 logging_writer.py:48] [126900] global_step=126900, grad_norm=2.297762155532837, loss=1.6438748836517334
I0202 15:28:16.064545 139907737556736 logging_writer.py:48] [127000] global_step=127000, grad_norm=2.3708786964416504, loss=1.7952570915222168
I0202 15:28:49.614052 139907729164032 logging_writer.py:48] [127100] global_step=127100, grad_norm=2.472517251968384, loss=1.7098171710968018
I0202 15:29:23.216670 139907737556736 logging_writer.py:48] [127200] global_step=127200, grad_norm=2.2421133518218994, loss=1.75531804561615
I0202 15:29:56.736022 139907729164032 logging_writer.py:48] [127300] global_step=127300, grad_norm=2.431297779083252, loss=1.9125621318817139
I0202 15:30:30.275074 139907737556736 logging_writer.py:48] [127400] global_step=127400, grad_norm=2.58429217338562, loss=1.7496769428253174
I0202 15:31:03.798810 139907729164032 logging_writer.py:48] [127500] global_step=127500, grad_norm=2.3985488414764404, loss=1.7521113157272339
I0202 15:31:37.302266 139907737556736 logging_writer.py:48] [127600] global_step=127600, grad_norm=2.4409728050231934, loss=1.7016643285751343
I0202 15:31:44.829724 140070692116288 spec.py:321] Evaluating on the training split.
I0202 15:31:51.080591 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 15:31:59.730469 140070692116288 spec.py:349] Evaluating on the test split.
I0202 15:32:02.383110 140070692116288 submission_runner.py:408] Time since start: 44379.98s, 	Step: 127624, 	{'train/accuracy': 0.6471021771430969, 'train/loss': 1.4206527471542358, 'validation/accuracy': 0.5690199732780457, 'validation/loss': 1.836398720741272, 'validation/num_examples': 50000, 'test/accuracy': 0.44850000739097595, 'test/loss': 2.574965000152588, 'test/num_examples': 10000, 'score': 42879.3197491169, 'total_duration': 44379.98077702522, 'accumulated_submission_time': 42879.3197491169, 'accumulated_eval_time': 1492.9049038887024, 'accumulated_logging_time': 3.306204319000244}
I0202 15:32:02.422088 139908710635264 logging_writer.py:48] [127624] accumulated_eval_time=1492.904904, accumulated_logging_time=3.306204, accumulated_submission_time=42879.319749, global_step=127624, preemption_count=0, score=42879.319749, test/accuracy=0.448500, test/loss=2.574965, test/num_examples=10000, total_duration=44379.980777, train/accuracy=0.647102, train/loss=1.420653, validation/accuracy=0.569020, validation/loss=1.836399, validation/num_examples=50000
I0202 15:32:28.338287 139908719027968 logging_writer.py:48] [127700] global_step=127700, grad_norm=2.4546613693237305, loss=1.7866997718811035
I0202 15:33:01.828167 139908710635264 logging_writer.py:48] [127800] global_step=127800, grad_norm=2.4574902057647705, loss=1.782821774482727
I0202 15:33:35.373710 139908719027968 logging_writer.py:48] [127900] global_step=127900, grad_norm=2.591850996017456, loss=1.8504111766815186
I0202 15:34:08.888103 139908710635264 logging_writer.py:48] [128000] global_step=128000, grad_norm=2.3153507709503174, loss=1.7892839908599854
I0202 15:34:42.453844 139908719027968 logging_writer.py:48] [128100] global_step=128100, grad_norm=2.4207587242126465, loss=1.7775750160217285
I0202 15:35:15.963855 139908710635264 logging_writer.py:48] [128200] global_step=128200, grad_norm=2.4057178497314453, loss=1.817888617515564
I0202 15:35:49.501937 139908719027968 logging_writer.py:48] [128300] global_step=128300, grad_norm=2.3626956939697266, loss=1.8161747455596924
I0202 15:36:23.025814 139908710635264 logging_writer.py:48] [128400] global_step=128400, grad_norm=2.3469550609588623, loss=1.6793756484985352
I0202 15:36:56.603039 139908719027968 logging_writer.py:48] [128500] global_step=128500, grad_norm=2.2621676921844482, loss=1.7490732669830322
I0202 15:37:30.145163 139908710635264 logging_writer.py:48] [128600] global_step=128600, grad_norm=2.302457571029663, loss=1.705672264099121
I0202 15:38:03.641656 139908719027968 logging_writer.py:48] [128700] global_step=128700, grad_norm=2.308483600616455, loss=1.5970407724380493
I0202 15:38:37.230727 139908710635264 logging_writer.py:48] [128800] global_step=128800, grad_norm=2.5394954681396484, loss=1.6750767230987549
I0202 15:39:10.728118 139908719027968 logging_writer.py:48] [128900] global_step=128900, grad_norm=2.478844165802002, loss=1.6777652502059937
I0202 15:39:44.248818 139908710635264 logging_writer.py:48] [129000] global_step=129000, grad_norm=2.44069766998291, loss=1.6844267845153809
I0202 15:40:17.821974 139908719027968 logging_writer.py:48] [129100] global_step=129100, grad_norm=2.439822196960449, loss=1.7425625324249268
I0202 15:40:32.403258 140070692116288 spec.py:321] Evaluating on the training split.
I0202 15:40:38.824445 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 15:40:47.340089 140070692116288 spec.py:349] Evaluating on the test split.
I0202 15:40:49.867933 140070692116288 submission_runner.py:408] Time since start: 44907.47s, 	Step: 129145, 	{'train/accuracy': 0.6431162357330322, 'train/loss': 1.4401280879974365, 'validation/accuracy': 0.5803799629211426, 'validation/loss': 1.769958734512329, 'validation/num_examples': 50000, 'test/accuracy': 0.453900009393692, 'test/loss': 2.5460996627807617, 'test/num_examples': 10000, 'score': 43389.23870754242, 'total_duration': 44907.46559214592, 'accumulated_submission_time': 43389.23870754242, 'accumulated_eval_time': 1510.3695363998413, 'accumulated_logging_time': 3.3543436527252197}
I0202 15:40:49.913560 139907745949440 logging_writer.py:48] [129145] accumulated_eval_time=1510.369536, accumulated_logging_time=3.354344, accumulated_submission_time=43389.238708, global_step=129145, preemption_count=0, score=43389.238708, test/accuracy=0.453900, test/loss=2.546100, test/num_examples=10000, total_duration=44907.465592, train/accuracy=0.643116, train/loss=1.440128, validation/accuracy=0.580380, validation/loss=1.769959, validation/num_examples=50000
I0202 15:41:08.665649 139907754342144 logging_writer.py:48] [129200] global_step=129200, grad_norm=2.4728853702545166, loss=1.76705002784729
I0202 15:41:42.169069 139907745949440 logging_writer.py:48] [129300] global_step=129300, grad_norm=2.5703840255737305, loss=1.7768123149871826
I0202 15:42:15.701779 139907754342144 logging_writer.py:48] [129400] global_step=129400, grad_norm=2.349381446838379, loss=1.5648229122161865
I0202 15:42:49.214915 139907745949440 logging_writer.py:48] [129500] global_step=129500, grad_norm=2.4026241302490234, loss=1.6349107027053833
I0202 15:43:22.791330 139907754342144 logging_writer.py:48] [129600] global_step=129600, grad_norm=2.560720920562744, loss=1.5775392055511475
I0202 15:43:56.392956 139907745949440 logging_writer.py:48] [129700] global_step=129700, grad_norm=2.4924628734588623, loss=1.6472442150115967
I0202 15:44:29.906608 139907754342144 logging_writer.py:48] [129800] global_step=129800, grad_norm=2.3505728244781494, loss=1.5764174461364746
I0202 15:45:03.484926 139907745949440 logging_writer.py:48] [129900] global_step=129900, grad_norm=2.4477450847625732, loss=1.741876244544983
I0202 15:45:37.050911 139907754342144 logging_writer.py:48] [130000] global_step=130000, grad_norm=2.447456121444702, loss=1.815274953842163
I0202 15:46:10.639570 139907745949440 logging_writer.py:48] [130100] global_step=130100, grad_norm=2.5956883430480957, loss=1.6306898593902588
I0202 15:46:44.183465 139907754342144 logging_writer.py:48] [130200] global_step=130200, grad_norm=2.402107000350952, loss=1.6688222885131836
I0202 15:47:17.707321 139907745949440 logging_writer.py:48] [130300] global_step=130300, grad_norm=2.4279403686523438, loss=1.6070219278335571
I0202 15:47:51.217542 139907754342144 logging_writer.py:48] [130400] global_step=130400, grad_norm=2.432497501373291, loss=1.580741047859192
I0202 15:48:24.733245 139907745949440 logging_writer.py:48] [130500] global_step=130500, grad_norm=2.438624143600464, loss=1.5810136795043945
I0202 15:48:58.330267 139907754342144 logging_writer.py:48] [130600] global_step=130600, grad_norm=2.4284772872924805, loss=1.6003738641738892
I0202 15:49:19.990679 140070692116288 spec.py:321] Evaluating on the training split.
I0202 15:49:26.271765 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 15:49:34.640439 140070692116288 spec.py:349] Evaluating on the test split.
I0202 15:49:37.277745 140070692116288 submission_runner.py:408] Time since start: 45434.88s, 	Step: 130666, 	{'train/accuracy': 0.6172273755073547, 'train/loss': 1.5718461275100708, 'validation/accuracy': 0.5622000098228455, 'validation/loss': 1.8866841793060303, 'validation/num_examples': 50000, 'test/accuracy': 0.4480000138282776, 'test/loss': 2.622263193130493, 'test/num_examples': 10000, 'score': 43899.253200531006, 'total_duration': 45434.875410079956, 'accumulated_submission_time': 43899.253200531006, 'accumulated_eval_time': 1527.6565673351288, 'accumulated_logging_time': 3.409334897994995}
I0202 15:49:37.317483 139907737556736 logging_writer.py:48] [130666] accumulated_eval_time=1527.656567, accumulated_logging_time=3.409335, accumulated_submission_time=43899.253201, global_step=130666, preemption_count=0, score=43899.253201, test/accuracy=0.448000, test/loss=2.622263, test/num_examples=10000, total_duration=45434.875410, train/accuracy=0.617227, train/loss=1.571846, validation/accuracy=0.562200, validation/loss=1.886684, validation/num_examples=50000
I0202 15:49:49.054716 139907745949440 logging_writer.py:48] [130700] global_step=130700, grad_norm=2.4214460849761963, loss=1.698230266571045
I0202 15:50:22.516495 139907737556736 logging_writer.py:48] [130800] global_step=130800, grad_norm=2.633376121520996, loss=1.6390529870986938
I0202 15:50:56.030656 139907745949440 logging_writer.py:48] [130900] global_step=130900, grad_norm=2.4872546195983887, loss=1.6529195308685303
I0202 15:51:29.587880 139907737556736 logging_writer.py:48] [131000] global_step=131000, grad_norm=2.512253999710083, loss=1.6276706457138062
I0202 15:52:03.100224 139907745949440 logging_writer.py:48] [131100] global_step=131100, grad_norm=2.43346905708313, loss=1.6160551309585571
I0202 15:52:36.629067 139907737556736 logging_writer.py:48] [131200] global_step=131200, grad_norm=2.342317819595337, loss=1.6476560831069946
I0202 15:53:10.131391 139907745949440 logging_writer.py:48] [131300] global_step=131300, grad_norm=2.5181171894073486, loss=1.618234634399414
I0202 15:53:43.706507 139907737556736 logging_writer.py:48] [131400] global_step=131400, grad_norm=2.503316879272461, loss=1.667745590209961
I0202 15:54:17.226649 139907745949440 logging_writer.py:48] [131500] global_step=131500, grad_norm=2.5153989791870117, loss=1.7627266645431519
I0202 15:54:50.744834 139907737556736 logging_writer.py:48] [131600] global_step=131600, grad_norm=2.508232355117798, loss=1.6705431938171387
I0202 15:55:24.257828 139907745949440 logging_writer.py:48] [131700] global_step=131700, grad_norm=2.4074857234954834, loss=1.6485199928283691
I0202 15:55:57.828700 139907737556736 logging_writer.py:48] [131800] global_step=131800, grad_norm=2.496605157852173, loss=1.6937358379364014
I0202 15:56:31.349785 139907745949440 logging_writer.py:48] [131900] global_step=131900, grad_norm=2.44443941116333, loss=1.6242625713348389
I0202 15:57:04.900951 139907737556736 logging_writer.py:48] [132000] global_step=132000, grad_norm=2.405761480331421, loss=1.5990662574768066
I0202 15:57:38.451971 139907745949440 logging_writer.py:48] [132100] global_step=132100, grad_norm=2.5080137252807617, loss=1.6609916687011719
I0202 15:58:07.304682 140070692116288 spec.py:321] Evaluating on the training split.
I0202 15:58:13.574935 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 15:58:22.008391 140070692116288 spec.py:349] Evaluating on the test split.
I0202 15:58:24.647543 140070692116288 submission_runner.py:408] Time since start: 45962.25s, 	Step: 132187, 	{'train/accuracy': 0.5950454473495483, 'train/loss': 1.6977694034576416, 'validation/accuracy': 0.5471599698066711, 'validation/loss': 1.9765727519989014, 'validation/num_examples': 50000, 'test/accuracy': 0.43570002913475037, 'test/loss': 2.7501347064971924, 'test/num_examples': 10000, 'score': 44409.178663253784, 'total_duration': 45962.245206832886, 'accumulated_submission_time': 44409.178663253784, 'accumulated_eval_time': 1544.9993915557861, 'accumulated_logging_time': 3.458484411239624}
I0202 15:58:24.688320 139907737556736 logging_writer.py:48] [132187] accumulated_eval_time=1544.999392, accumulated_logging_time=3.458484, accumulated_submission_time=44409.178663, global_step=132187, preemption_count=0, score=44409.178663, test/accuracy=0.435700, test/loss=2.750135, test/num_examples=10000, total_duration=45962.245207, train/accuracy=0.595045, train/loss=1.697769, validation/accuracy=0.547160, validation/loss=1.976573, validation/num_examples=50000
I0202 15:58:29.384319 139907754342144 logging_writer.py:48] [132200] global_step=132200, grad_norm=2.604743480682373, loss=1.6540381908416748
I0202 15:59:02.867548 139907737556736 logging_writer.py:48] [132300] global_step=132300, grad_norm=2.624730348587036, loss=1.6256535053253174
I0202 15:59:36.377070 139907754342144 logging_writer.py:48] [132400] global_step=132400, grad_norm=2.6127755641937256, loss=1.581581950187683
I0202 16:00:09.898460 139907737556736 logging_writer.py:48] [132500] global_step=132500, grad_norm=2.616471767425537, loss=1.569463849067688
I0202 16:00:43.411996 139907754342144 logging_writer.py:48] [132600] global_step=132600, grad_norm=2.4754574298858643, loss=1.7038991451263428
I0202 16:01:16.942430 139907737556736 logging_writer.py:48] [132700] global_step=132700, grad_norm=2.7588655948638916, loss=1.6583549976348877
I0202 16:01:50.499728 139907754342144 logging_writer.py:48] [132800] global_step=132800, grad_norm=2.6081786155700684, loss=1.5826019048690796
I0202 16:02:24.026310 139907737556736 logging_writer.py:48] [132900] global_step=132900, grad_norm=2.6852500438690186, loss=1.7524147033691406
I0202 16:02:57.580147 139907754342144 logging_writer.py:48] [133000] global_step=133000, grad_norm=2.4539406299591064, loss=1.7257851362228394
I0202 16:03:31.113017 139907737556736 logging_writer.py:48] [133100] global_step=133100, grad_norm=2.50453519821167, loss=1.5490987300872803
I0202 16:04:04.813811 139907754342144 logging_writer.py:48] [133200] global_step=133200, grad_norm=2.5674071311950684, loss=1.7593467235565186
I0202 16:04:38.364248 139907737556736 logging_writer.py:48] [133300] global_step=133300, grad_norm=2.6587390899658203, loss=1.6782526969909668
I0202 16:05:11.950236 139907754342144 logging_writer.py:48] [133400] global_step=133400, grad_norm=2.622237205505371, loss=1.7243874073028564
I0202 16:05:45.479362 139907737556736 logging_writer.py:48] [133500] global_step=133500, grad_norm=2.474262237548828, loss=1.6044479608535767
I0202 16:06:19.005538 139907754342144 logging_writer.py:48] [133600] global_step=133600, grad_norm=2.506140947341919, loss=1.634364128112793
I0202 16:06:52.574168 139907737556736 logging_writer.py:48] [133700] global_step=133700, grad_norm=2.7036445140838623, loss=1.682990312576294
I0202 16:06:54.738990 140070692116288 spec.py:321] Evaluating on the training split.
I0202 16:07:01.119736 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 16:07:09.954364 140070692116288 spec.py:349] Evaluating on the test split.
I0202 16:07:13.253175 140070692116288 submission_runner.py:408] Time since start: 46490.85s, 	Step: 133708, 	{'train/accuracy': 0.6533800959587097, 'train/loss': 1.3872883319854736, 'validation/accuracy': 0.6028199791908264, 'validation/loss': 1.666730284690857, 'validation/num_examples': 50000, 'test/accuracy': 0.48500001430511475, 'test/loss': 2.405172348022461, 'test/num_examples': 10000, 'score': 44919.164951086044, 'total_duration': 46490.850856781006, 'accumulated_submission_time': 44919.164951086044, 'accumulated_eval_time': 1563.513568162918, 'accumulated_logging_time': 3.5099377632141113}
I0202 16:07:13.285769 139908425447168 logging_writer.py:48] [133708] accumulated_eval_time=1563.513568, accumulated_logging_time=3.509938, accumulated_submission_time=44919.164951, global_step=133708, preemption_count=0, score=44919.164951, test/accuracy=0.485000, test/loss=2.405172, test/num_examples=10000, total_duration=46490.850857, train/accuracy=0.653380, train/loss=1.387288, validation/accuracy=0.602820, validation/loss=1.666730, validation/num_examples=50000
I0202 16:07:44.400125 139908710635264 logging_writer.py:48] [133800] global_step=133800, grad_norm=2.762876033782959, loss=1.7511916160583496
I0202 16:08:17.898157 139908425447168 logging_writer.py:48] [133900] global_step=133900, grad_norm=2.698315382003784, loss=1.625614047050476
I0202 16:08:51.436540 139908710635264 logging_writer.py:48] [134000] global_step=134000, grad_norm=2.8296360969543457, loss=1.618129849433899
I0202 16:09:24.994008 139908425447168 logging_writer.py:48] [134100] global_step=134100, grad_norm=2.6704514026641846, loss=1.636106252670288
I0202 16:09:58.513900 139908710635264 logging_writer.py:48] [134200] global_step=134200, grad_norm=2.5242297649383545, loss=1.6097941398620605
I0202 16:10:32.129580 139908425447168 logging_writer.py:48] [134300] global_step=134300, grad_norm=2.5642592906951904, loss=1.6712477207183838
I0202 16:11:05.676804 139908710635264 logging_writer.py:48] [134400] global_step=134400, grad_norm=2.413761854171753, loss=1.5019687414169312
I0202 16:11:39.199208 139908425447168 logging_writer.py:48] [134500] global_step=134500, grad_norm=2.849429130554199, loss=1.7487447261810303
I0202 16:12:12.742026 139908710635264 logging_writer.py:48] [134600] global_step=134600, grad_norm=2.705014705657959, loss=1.5974018573760986
I0202 16:12:46.347556 139908425447168 logging_writer.py:48] [134700] global_step=134700, grad_norm=2.6498401165008545, loss=1.6559300422668457
I0202 16:13:19.916465 139908710635264 logging_writer.py:48] [134800] global_step=134800, grad_norm=2.6391990184783936, loss=1.5963079929351807
I0202 16:13:53.443263 139908425447168 logging_writer.py:48] [134900] global_step=134900, grad_norm=2.571296215057373, loss=1.5530775785446167
I0202 16:14:27.000389 139908710635264 logging_writer.py:48] [135000] global_step=135000, grad_norm=2.5982048511505127, loss=1.5956103801727295
I0202 16:15:00.572130 139908425447168 logging_writer.py:48] [135100] global_step=135100, grad_norm=2.8154008388519287, loss=1.5395045280456543
I0202 16:15:34.114927 139908710635264 logging_writer.py:48] [135200] global_step=135200, grad_norm=2.58247971534729, loss=1.7040799856185913
I0202 16:15:43.329027 140070692116288 spec.py:321] Evaluating on the training split.
I0202 16:15:49.741870 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 16:15:58.198713 140070692116288 spec.py:349] Evaluating on the test split.
I0202 16:16:00.892904 140070692116288 submission_runner.py:408] Time since start: 47018.49s, 	Step: 135229, 	{'train/accuracy': 0.6850087642669678, 'train/loss': 1.247851014137268, 'validation/accuracy': 0.632099986076355, 'validation/loss': 1.5180271863937378, 'validation/num_examples': 50000, 'test/accuracy': 0.5041000247001648, 'test/loss': 2.260892868041992, 'test/num_examples': 10000, 'score': 45429.14738154411, 'total_duration': 47018.49057650566, 'accumulated_submission_time': 45429.14738154411, 'accumulated_eval_time': 1581.0774466991425, 'accumulated_logging_time': 3.5508670806884766}
I0202 16:16:00.938942 139907729164032 logging_writer.py:48] [135229] accumulated_eval_time=1581.077447, accumulated_logging_time=3.550867, accumulated_submission_time=45429.147382, global_step=135229, preemption_count=0, score=45429.147382, test/accuracy=0.504100, test/loss=2.260893, test/num_examples=10000, total_duration=47018.490577, train/accuracy=0.685009, train/loss=1.247851, validation/accuracy=0.632100, validation/loss=1.518027, validation/num_examples=50000
I0202 16:16:25.072213 139907737556736 logging_writer.py:48] [135300] global_step=135300, grad_norm=2.6716227531433105, loss=1.6922129392623901
I0202 16:16:58.655225 139907729164032 logging_writer.py:48] [135400] global_step=135400, grad_norm=2.8748085498809814, loss=1.7631769180297852
I0202 16:17:32.217648 139907737556736 logging_writer.py:48] [135500] global_step=135500, grad_norm=2.5652599334716797, loss=1.6850074529647827
I0202 16:18:05.742130 139907729164032 logging_writer.py:48] [135600] global_step=135600, grad_norm=2.8393025398254395, loss=1.6114749908447266
I0202 16:18:39.271601 139907737556736 logging_writer.py:48] [135700] global_step=135700, grad_norm=2.501988410949707, loss=1.5837408304214478
I0202 16:19:12.814087 139907729164032 logging_writer.py:48] [135800] global_step=135800, grad_norm=2.5358798503875732, loss=1.5990511178970337
I0202 16:19:46.320212 139907737556736 logging_writer.py:48] [135900] global_step=135900, grad_norm=2.6947882175445557, loss=1.5625218152999878
I0202 16:20:19.879299 139907729164032 logging_writer.py:48] [136000] global_step=136000, grad_norm=2.5356147289276123, loss=1.6749815940856934
I0202 16:20:53.387906 139907737556736 logging_writer.py:48] [136100] global_step=136100, grad_norm=3.0696427822113037, loss=1.6983672380447388
I0202 16:21:26.936228 139907729164032 logging_writer.py:48] [136200] global_step=136200, grad_norm=2.681384325027466, loss=1.634521245956421
I0202 16:22:00.467589 139907737556736 logging_writer.py:48] [136300] global_step=136300, grad_norm=2.8344478607177734, loss=1.5967411994934082
I0202 16:22:33.997278 139907729164032 logging_writer.py:48] [136400] global_step=136400, grad_norm=2.959944009780884, loss=1.5892508029937744
I0202 16:23:07.535811 139907737556736 logging_writer.py:48] [136500] global_step=136500, grad_norm=2.9314658641815186, loss=1.6422297954559326
I0202 16:23:41.155977 139907729164032 logging_writer.py:48] [136600] global_step=136600, grad_norm=2.7628378868103027, loss=1.5693608522415161
I0202 16:24:14.716486 139907737556736 logging_writer.py:48] [136700] global_step=136700, grad_norm=2.8098318576812744, loss=1.6017965078353882
I0202 16:24:30.954421 140070692116288 spec.py:321] Evaluating on the training split.
I0202 16:24:37.287585 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 16:24:45.868435 140070692116288 spec.py:349] Evaluating on the test split.
I0202 16:24:48.527497 140070692116288 submission_runner.py:408] Time since start: 47546.13s, 	Step: 136750, 	{'train/accuracy': 0.6704998016357422, 'train/loss': 1.3005253076553345, 'validation/accuracy': 0.5945799946784973, 'validation/loss': 1.7191460132598877, 'validation/num_examples': 50000, 'test/accuracy': 0.46480002999305725, 'test/loss': 2.5056369304656982, 'test/num_examples': 10000, 'score': 45939.10034203529, 'total_duration': 47546.12515926361, 'accumulated_submission_time': 45939.10034203529, 'accumulated_eval_time': 1598.6504790782928, 'accumulated_logging_time': 3.605985164642334}
I0202 16:24:48.565635 139907745949440 logging_writer.py:48] [136750] accumulated_eval_time=1598.650479, accumulated_logging_time=3.605985, accumulated_submission_time=45939.100342, global_step=136750, preemption_count=0, score=45939.100342, test/accuracy=0.464800, test/loss=2.505637, test/num_examples=10000, total_duration=47546.125159, train/accuracy=0.670500, train/loss=1.300525, validation/accuracy=0.594580, validation/loss=1.719146, validation/num_examples=50000
I0202 16:25:05.671761 139908425447168 logging_writer.py:48] [136800] global_step=136800, grad_norm=2.8259263038635254, loss=1.6085546016693115
I0202 16:25:39.167003 139907745949440 logging_writer.py:48] [136900] global_step=136900, grad_norm=2.8016786575317383, loss=1.5609476566314697
I0202 16:26:12.709878 139908425447168 logging_writer.py:48] [137000] global_step=137000, grad_norm=2.8374252319335938, loss=1.564050316810608
I0202 16:26:46.241419 139907745949440 logging_writer.py:48] [137100] global_step=137100, grad_norm=3.0718886852264404, loss=1.6668591499328613
I0202 16:27:19.835212 139908425447168 logging_writer.py:48] [137200] global_step=137200, grad_norm=2.7272989749908447, loss=1.5496011972427368
I0202 16:27:53.358254 139907745949440 logging_writer.py:48] [137300] global_step=137300, grad_norm=2.757359027862549, loss=1.6053884029388428
I0202 16:28:26.937046 139908425447168 logging_writer.py:48] [137400] global_step=137400, grad_norm=3.18068528175354, loss=1.6300100088119507
I0202 16:29:00.454483 139907745949440 logging_writer.py:48] [137500] global_step=137500, grad_norm=2.813826084136963, loss=1.7100275754928589
I0202 16:29:34.034394 139908425447168 logging_writer.py:48] [137600] global_step=137600, grad_norm=2.8971245288848877, loss=1.5823081731796265
I0202 16:30:07.620651 139907745949440 logging_writer.py:48] [137700] global_step=137700, grad_norm=2.878875494003296, loss=1.6919023990631104
I0202 16:30:41.134905 139908425447168 logging_writer.py:48] [137800] global_step=137800, grad_norm=2.8430230617523193, loss=1.6335688829421997
I0202 16:31:14.629104 139907745949440 logging_writer.py:48] [137900] global_step=137900, grad_norm=2.6964573860168457, loss=1.5307857990264893
I0202 16:31:48.176184 139908425447168 logging_writer.py:48] [138000] global_step=138000, grad_norm=2.579946279525757, loss=1.4985322952270508
I0202 16:32:21.731194 139907745949440 logging_writer.py:48] [138100] global_step=138100, grad_norm=2.7916083335876465, loss=1.5570672750473022
I0202 16:32:55.272492 139908425447168 logging_writer.py:48] [138200] global_step=138200, grad_norm=2.952611207962036, loss=1.639677882194519
I0202 16:33:18.541784 140070692116288 spec.py:321] Evaluating on the training split.
I0202 16:33:24.817602 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 16:33:33.510328 140070692116288 spec.py:349] Evaluating on the test split.
I0202 16:33:36.026799 140070692116288 submission_runner.py:408] Time since start: 48073.62s, 	Step: 138271, 	{'train/accuracy': 0.6985809803009033, 'train/loss': 1.16620934009552, 'validation/accuracy': 0.6342599987983704, 'validation/loss': 1.5143696069717407, 'validation/num_examples': 50000, 'test/accuracy': 0.5098000168800354, 'test/loss': 2.2224693298339844, 'test/num_examples': 10000, 'score': 46449.014721632004, 'total_duration': 48073.62446284294, 'accumulated_submission_time': 46449.014721632004, 'accumulated_eval_time': 1616.1354587078094, 'accumulated_logging_time': 3.6534364223480225}
I0202 16:33:36.067053 139907754342144 logging_writer.py:48] [138271] accumulated_eval_time=1616.135459, accumulated_logging_time=3.653436, accumulated_submission_time=46449.014722, global_step=138271, preemption_count=0, score=46449.014722, test/accuracy=0.509800, test/loss=2.222469, test/num_examples=10000, total_duration=48073.624463, train/accuracy=0.698581, train/loss=1.166209, validation/accuracy=0.634260, validation/loss=1.514370, validation/num_examples=50000
I0202 16:33:46.122406 139907762734848 logging_writer.py:48] [138300] global_step=138300, grad_norm=2.729942560195923, loss=1.5591139793395996
I0202 16:34:19.590442 139907754342144 logging_writer.py:48] [138400] global_step=138400, grad_norm=2.9284145832061768, loss=1.68184494972229
I0202 16:34:53.126601 139907762734848 logging_writer.py:48] [138500] global_step=138500, grad_norm=2.843636989593506, loss=1.5519213676452637
I0202 16:35:26.635802 139907754342144 logging_writer.py:48] [138600] global_step=138600, grad_norm=2.8917341232299805, loss=1.6563036441802979
I0202 16:36:00.207909 139907762734848 logging_writer.py:48] [138700] global_step=138700, grad_norm=2.91220760345459, loss=1.6248544454574585
I0202 16:36:33.871441 139907754342144 logging_writer.py:48] [138800] global_step=138800, grad_norm=2.8217508792877197, loss=1.534735918045044
I0202 16:37:07.833831 139907762734848 logging_writer.py:48] [138900] global_step=138900, grad_norm=2.8149971961975098, loss=1.549953579902649
I0202 16:37:41.382434 139907754342144 logging_writer.py:48] [139000] global_step=139000, grad_norm=2.7706871032714844, loss=1.6406521797180176
I0202 16:38:14.894588 139907762734848 logging_writer.py:48] [139100] global_step=139100, grad_norm=3.0639688968658447, loss=1.600464105606079
I0202 16:38:48.419684 139907754342144 logging_writer.py:48] [139200] global_step=139200, grad_norm=2.8349997997283936, loss=1.57948637008667
I0202 16:39:21.971610 139907762734848 logging_writer.py:48] [139300] global_step=139300, grad_norm=3.0206339359283447, loss=1.7103540897369385
I0202 16:39:55.497764 139907754342144 logging_writer.py:48] [139400] global_step=139400, grad_norm=3.0118892192840576, loss=1.5444735288619995
I0202 16:40:29.055927 139907762734848 logging_writer.py:48] [139500] global_step=139500, grad_norm=2.9192721843719482, loss=1.6310349702835083
I0202 16:41:02.652821 139907754342144 logging_writer.py:48] [139600] global_step=139600, grad_norm=2.718179702758789, loss=1.5582237243652344
I0202 16:41:36.177705 139907762734848 logging_writer.py:48] [139700] global_step=139700, grad_norm=2.6778602600097656, loss=1.4239213466644287
I0202 16:42:06.132439 140070692116288 spec.py:321] Evaluating on the training split.
I0202 16:42:12.389942 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 16:42:20.909077 140070692116288 spec.py:349] Evaluating on the test split.
I0202 16:42:23.559170 140070692116288 submission_runner.py:408] Time since start: 48601.16s, 	Step: 139791, 	{'train/accuracy': 0.6724529266357422, 'train/loss': 1.3127702474594116, 'validation/accuracy': 0.6125199794769287, 'validation/loss': 1.6271332502365112, 'validation/num_examples': 50000, 'test/accuracy': 0.4791000187397003, 'test/loss': 2.424319267272949, 'test/num_examples': 10000, 'score': 46959.01626968384, 'total_duration': 48601.15681409836, 'accumulated_submission_time': 46959.01626968384, 'accumulated_eval_time': 1633.5621328353882, 'accumulated_logging_time': 3.705156087875366}
I0202 16:42:23.601460 139908425447168 logging_writer.py:48] [139791] accumulated_eval_time=1633.562133, accumulated_logging_time=3.705156, accumulated_submission_time=46959.016270, global_step=139791, preemption_count=0, score=46959.016270, test/accuracy=0.479100, test/loss=2.424319, test/num_examples=10000, total_duration=48601.156814, train/accuracy=0.672453, train/loss=1.312770, validation/accuracy=0.612520, validation/loss=1.627133, validation/num_examples=50000
I0202 16:42:26.957972 139908710635264 logging_writer.py:48] [139800] global_step=139800, grad_norm=2.9712955951690674, loss=1.575409173965454
I0202 16:43:00.571806 139908425447168 logging_writer.py:48] [139900] global_step=139900, grad_norm=2.9839956760406494, loss=1.5611200332641602
I0202 16:43:34.073689 139908710635264 logging_writer.py:48] [140000] global_step=140000, grad_norm=3.15594220161438, loss=1.5908904075622559
I0202 16:44:07.636918 139908425447168 logging_writer.py:48] [140100] global_step=140100, grad_norm=2.9086663722991943, loss=1.6966190338134766
I0202 16:44:41.156553 139908710635264 logging_writer.py:48] [140200] global_step=140200, grad_norm=2.7782199382781982, loss=1.436586856842041
I0202 16:45:14.675343 139908425447168 logging_writer.py:48] [140300] global_step=140300, grad_norm=3.026498317718506, loss=1.5999844074249268
I0202 16:45:48.232091 139908710635264 logging_writer.py:48] [140400] global_step=140400, grad_norm=2.931211233139038, loss=1.5342955589294434
I0202 16:46:21.779146 139908425447168 logging_writer.py:48] [140500] global_step=140500, grad_norm=2.8944954872131348, loss=1.5597593784332275
I0202 16:46:55.292881 139908710635264 logging_writer.py:48] [140600] global_step=140600, grad_norm=2.8956570625305176, loss=1.5680248737335205
I0202 16:47:28.837495 139908425447168 logging_writer.py:48] [140700] global_step=140700, grad_norm=2.9319076538085938, loss=1.559944987297058
I0202 16:48:02.357556 139908710635264 logging_writer.py:48] [140800] global_step=140800, grad_norm=2.819783926010132, loss=1.4826440811157227
I0202 16:48:35.910426 139908425447168 logging_writer.py:48] [140900] global_step=140900, grad_norm=3.2445833683013916, loss=1.5531976222991943
I0202 16:49:09.500118 139908710635264 logging_writer.py:48] [141000] global_step=141000, grad_norm=2.9839863777160645, loss=1.5778858661651611
I0202 16:49:43.027173 139908425447168 logging_writer.py:48] [141100] global_step=141100, grad_norm=3.0406792163848877, loss=1.5228886604309082
I0202 16:50:16.540114 139908710635264 logging_writer.py:48] [141200] global_step=141200, grad_norm=3.138819456100464, loss=1.5146965980529785
I0202 16:50:50.057968 139908425447168 logging_writer.py:48] [141300] global_step=141300, grad_norm=3.0621166229248047, loss=1.5927066802978516
I0202 16:50:53.566493 140070692116288 spec.py:321] Evaluating on the training split.
I0202 16:50:59.842216 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 16:51:08.367554 140070692116288 spec.py:349] Evaluating on the test split.
I0202 16:51:10.903047 140070692116288 submission_runner.py:408] Time since start: 49128.50s, 	Step: 141312, 	{'train/accuracy': 0.6929009556770325, 'train/loss': 1.2138513326644897, 'validation/accuracy': 0.6319199800491333, 'validation/loss': 1.5169399976730347, 'validation/num_examples': 50000, 'test/accuracy': 0.5, 'test/loss': 2.2908971309661865, 'test/num_examples': 10000, 'score': 47468.91835808754, 'total_duration': 49128.500671863556, 'accumulated_submission_time': 47468.91835808754, 'accumulated_eval_time': 1650.8986172676086, 'accumulated_logging_time': 3.7565438747406006}
I0202 16:51:10.969335 139908719027968 logging_writer.py:48] [141312] accumulated_eval_time=1650.898617, accumulated_logging_time=3.756544, accumulated_submission_time=47468.918358, global_step=141312, preemption_count=0, score=47468.918358, test/accuracy=0.500000, test/loss=2.290897, test/num_examples=10000, total_duration=49128.500672, train/accuracy=0.692901, train/loss=1.213851, validation/accuracy=0.631920, validation/loss=1.516940, validation/num_examples=50000
I0202 16:51:40.817209 139908727420672 logging_writer.py:48] [141400] global_step=141400, grad_norm=2.8309757709503174, loss=1.453078031539917
I0202 16:52:14.305917 139908719027968 logging_writer.py:48] [141500] global_step=141500, grad_norm=2.992168426513672, loss=1.556262731552124
I0202 16:52:47.829282 139908727420672 logging_writer.py:48] [141600] global_step=141600, grad_norm=3.202091693878174, loss=1.647547960281372
I0202 16:53:21.347337 139908719027968 logging_writer.py:48] [141700] global_step=141700, grad_norm=2.9968039989471436, loss=1.5893864631652832
I0202 16:53:54.915974 139908727420672 logging_writer.py:48] [141800] global_step=141800, grad_norm=3.1315441131591797, loss=1.6152926683425903
I0202 16:54:28.500929 139908719027968 logging_writer.py:48] [141900] global_step=141900, grad_norm=2.8606319427490234, loss=1.472276210784912
I0202 16:55:02.021485 139908727420672 logging_writer.py:48] [142000] global_step=142000, grad_norm=2.834129571914673, loss=1.4875332117080688
I0202 16:55:35.678282 139908719027968 logging_writer.py:48] [142100] global_step=142100, grad_norm=3.126420736312866, loss=1.6101373434066772
I0202 16:56:09.241220 139908727420672 logging_writer.py:48] [142200] global_step=142200, grad_norm=2.861110210418701, loss=1.4624704122543335
I0202 16:56:42.838608 139908719027968 logging_writer.py:48] [142300] global_step=142300, grad_norm=3.0142264366149902, loss=1.521521806716919
I0202 16:57:16.338371 139908727420672 logging_writer.py:48] [142400] global_step=142400, grad_norm=2.9737164974212646, loss=1.570654273033142
I0202 16:57:49.877017 139908719027968 logging_writer.py:48] [142500] global_step=142500, grad_norm=3.0449841022491455, loss=1.5669516324996948
I0202 16:58:23.386114 139908727420672 logging_writer.py:48] [142600] global_step=142600, grad_norm=3.1505014896392822, loss=1.513956069946289
I0202 16:58:56.942581 139908719027968 logging_writer.py:48] [142700] global_step=142700, grad_norm=3.04842209815979, loss=1.491638422012329
I0202 16:59:30.454383 139908727420672 logging_writer.py:48] [142800] global_step=142800, grad_norm=3.130652904510498, loss=1.5825130939483643
I0202 16:59:41.004624 140070692116288 spec.py:321] Evaluating on the training split.
I0202 16:59:47.374857 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 16:59:55.992682 140070692116288 spec.py:349] Evaluating on the test split.
I0202 16:59:58.623839 140070692116288 submission_runner.py:408] Time since start: 49656.22s, 	Step: 142833, 	{'train/accuracy': 0.7071707248687744, 'train/loss': 1.1520593166351318, 'validation/accuracy': 0.6445599794387817, 'validation/loss': 1.4677226543426514, 'validation/num_examples': 50000, 'test/accuracy': 0.5177000164985657, 'test/loss': 2.194716215133667, 'test/num_examples': 10000, 'score': 47978.88781452179, 'total_duration': 49656.22150874138, 'accumulated_submission_time': 47978.88781452179, 'accumulated_eval_time': 1668.5178027153015, 'accumulated_logging_time': 3.8354225158691406}
I0202 16:59:58.669283 139907754342144 logging_writer.py:48] [142833] accumulated_eval_time=1668.517803, accumulated_logging_time=3.835423, accumulated_submission_time=47978.887815, global_step=142833, preemption_count=0, score=47978.887815, test/accuracy=0.517700, test/loss=2.194716, test/num_examples=10000, total_duration=49656.221509, train/accuracy=0.707171, train/loss=1.152059, validation/accuracy=0.644560, validation/loss=1.467723, validation/num_examples=50000
I0202 17:00:21.424139 139907762734848 logging_writer.py:48] [142900] global_step=142900, grad_norm=3.104621410369873, loss=1.5270627737045288
I0202 17:00:54.883238 139907754342144 logging_writer.py:48] [143000] global_step=143000, grad_norm=2.9547078609466553, loss=1.5461572408676147
I0202 17:01:28.407780 139907762734848 logging_writer.py:48] [143100] global_step=143100, grad_norm=3.0137531757354736, loss=1.5835633277893066
I0202 17:02:02.092822 139907754342144 logging_writer.py:48] [143200] global_step=143200, grad_norm=3.0684518814086914, loss=1.4929887056350708
I0202 17:02:35.611562 139907762734848 logging_writer.py:48] [143300] global_step=143300, grad_norm=3.06143856048584, loss=1.6408615112304688
I0202 17:03:09.142830 139907754342144 logging_writer.py:48] [143400] global_step=143400, grad_norm=3.113621234893799, loss=1.6001191139221191
I0202 17:03:42.679337 139907762734848 logging_writer.py:48] [143500] global_step=143500, grad_norm=3.137483835220337, loss=1.4528467655181885
I0202 17:04:16.225828 139907754342144 logging_writer.py:48] [143600] global_step=143600, grad_norm=3.080469846725464, loss=1.549950361251831
I0202 17:04:49.747784 139907762734848 logging_writer.py:48] [143700] global_step=143700, grad_norm=3.1130895614624023, loss=1.4490821361541748
I0202 17:05:23.328187 139907754342144 logging_writer.py:48] [143800] global_step=143800, grad_norm=3.3830513954162598, loss=1.664925217628479
I0202 17:05:56.867917 139907762734848 logging_writer.py:48] [143900] global_step=143900, grad_norm=2.903179168701172, loss=1.4817068576812744
I0202 17:06:30.385807 139907754342144 logging_writer.py:48] [144000] global_step=144000, grad_norm=3.059451103210449, loss=1.50453782081604
I0202 17:07:03.932293 139907762734848 logging_writer.py:48] [144100] global_step=144100, grad_norm=3.3115346431732178, loss=1.529746174812317
I0202 17:07:37.506325 139907754342144 logging_writer.py:48] [144200] global_step=144200, grad_norm=2.934291124343872, loss=1.4811537265777588
I0202 17:08:11.058615 139907762734848 logging_writer.py:48] [144300] global_step=144300, grad_norm=3.0257046222686768, loss=1.4547739028930664
I0202 17:08:28.728280 140070692116288 spec.py:321] Evaluating on the training split.
I0202 17:08:35.038501 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 17:08:43.677558 140070692116288 spec.py:349] Evaluating on the test split.
I0202 17:08:46.320055 140070692116288 submission_runner.py:408] Time since start: 50183.92s, 	Step: 144354, 	{'train/accuracy': 0.7137077450752258, 'train/loss': 1.1160260438919067, 'validation/accuracy': 0.65065997838974, 'validation/loss': 1.4377646446228027, 'validation/num_examples': 50000, 'test/accuracy': 0.5253000259399414, 'test/loss': 2.1807503700256348, 'test/num_examples': 10000, 'score': 48488.88462114334, 'total_duration': 50183.9176530838, 'accumulated_submission_time': 48488.88462114334, 'accumulated_eval_time': 1686.1094892024994, 'accumulated_logging_time': 3.890009880065918}
I0202 17:08:46.363583 139908425447168 logging_writer.py:48] [144354] accumulated_eval_time=1686.109489, accumulated_logging_time=3.890010, accumulated_submission_time=48488.884621, global_step=144354, preemption_count=0, score=48488.884621, test/accuracy=0.525300, test/loss=2.180750, test/num_examples=10000, total_duration=50183.917653, train/accuracy=0.713708, train/loss=1.116026, validation/accuracy=0.650660, validation/loss=1.437765, validation/num_examples=50000
I0202 17:09:02.115464 139908710635264 logging_writer.py:48] [144400] global_step=144400, grad_norm=3.1645753383636475, loss=1.5172969102859497
I0202 17:09:35.616909 139908425447168 logging_writer.py:48] [144500] global_step=144500, grad_norm=3.2411694526672363, loss=1.4448301792144775
I0202 17:10:09.126338 139908710635264 logging_writer.py:48] [144600] global_step=144600, grad_norm=3.3305020332336426, loss=1.5327283143997192
I0202 17:10:42.659775 139908425447168 logging_writer.py:48] [144700] global_step=144700, grad_norm=3.0176305770874023, loss=1.4934961795806885
I0202 17:11:16.246972 139908710635264 logging_writer.py:48] [144800] global_step=144800, grad_norm=3.148545503616333, loss=1.431920051574707
I0202 17:11:49.744587 139908425447168 logging_writer.py:48] [144900] global_step=144900, grad_norm=2.9285216331481934, loss=1.3862665891647339
I0202 17:12:23.254712 139908710635264 logging_writer.py:48] [145000] global_step=145000, grad_norm=3.2188761234283447, loss=1.5544577836990356
I0202 17:12:56.828661 139908425447168 logging_writer.py:48] [145100] global_step=145100, grad_norm=3.12484073638916, loss=1.462813138961792
I0202 17:13:30.425348 139908710635264 logging_writer.py:48] [145200] global_step=145200, grad_norm=3.138484001159668, loss=1.5395327806472778
I0202 17:14:03.950218 139908425447168 logging_writer.py:48] [145300] global_step=145300, grad_norm=2.9784789085388184, loss=1.4741263389587402
I0202 17:14:37.456227 139908710635264 logging_writer.py:48] [145400] global_step=145400, grad_norm=3.1519882678985596, loss=1.428407073020935
I0202 17:15:11.088735 139908425447168 logging_writer.py:48] [145500] global_step=145500, grad_norm=3.0166146755218506, loss=1.5066986083984375
I0202 17:15:44.687086 139908710635264 logging_writer.py:48] [145600] global_step=145600, grad_norm=3.0574541091918945, loss=1.4879820346832275
I0202 17:16:18.194458 139908425447168 logging_writer.py:48] [145700] global_step=145700, grad_norm=3.57328200340271, loss=1.566112995147705
I0202 17:16:51.769457 139908710635264 logging_writer.py:48] [145800] global_step=145800, grad_norm=2.9805808067321777, loss=1.4504475593566895
I0202 17:17:16.392732 140070692116288 spec.py:321] Evaluating on the training split.
I0202 17:17:22.638906 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 17:17:31.347833 140070692116288 spec.py:349] Evaluating on the test split.
I0202 17:17:34.000209 140070692116288 submission_runner.py:408] Time since start: 50711.60s, 	Step: 145875, 	{'train/accuracy': 0.7431241869926453, 'train/loss': 0.9743123650550842, 'validation/accuracy': 0.6586199998855591, 'validation/loss': 1.4034175872802734, 'validation/num_examples': 50000, 'test/accuracy': 0.534000039100647, 'test/loss': 2.145510673522949, 'test/num_examples': 10000, 'score': 48998.85201382637, 'total_duration': 50711.597870111465, 'accumulated_submission_time': 48998.85201382637, 'accumulated_eval_time': 1703.7169313430786, 'accumulated_logging_time': 3.9426703453063965}
I0202 17:17:34.044774 139907754342144 logging_writer.py:48] [145875] accumulated_eval_time=1703.716931, accumulated_logging_time=3.942670, accumulated_submission_time=48998.852014, global_step=145875, preemption_count=0, score=48998.852014, test/accuracy=0.534000, test/loss=2.145511, test/num_examples=10000, total_duration=50711.597870, train/accuracy=0.743124, train/loss=0.974312, validation/accuracy=0.658620, validation/loss=1.403418, validation/num_examples=50000
I0202 17:17:42.757753 139907762734848 logging_writer.py:48] [145900] global_step=145900, grad_norm=2.9186344146728516, loss=1.4071236848831177
I0202 17:18:16.245809 139907754342144 logging_writer.py:48] [146000] global_step=146000, grad_norm=3.082995653152466, loss=1.5885088443756104
I0202 17:18:49.729936 139907762734848 logging_writer.py:48] [146100] global_step=146100, grad_norm=3.211634874343872, loss=1.5374641418457031
I0202 17:19:23.255564 139907754342144 logging_writer.py:48] [146200] global_step=146200, grad_norm=3.23346209526062, loss=1.5443363189697266
I0202 17:19:56.807740 139907762734848 logging_writer.py:48] [146300] global_step=146300, grad_norm=3.123652458190918, loss=1.4885772466659546
I0202 17:20:30.320953 139907754342144 logging_writer.py:48] [146400] global_step=146400, grad_norm=3.025388717651367, loss=1.4253264665603638
I0202 17:21:03.845127 139907762734848 logging_writer.py:48] [146500] global_step=146500, grad_norm=3.2878828048706055, loss=1.5349030494689941
I0202 17:21:37.442365 139907754342144 logging_writer.py:48] [146600] global_step=146600, grad_norm=3.268455743789673, loss=1.4044597148895264
I0202 17:22:10.956396 139907762734848 logging_writer.py:48] [146700] global_step=146700, grad_norm=3.273961305618286, loss=1.4492334127426147
I0202 17:22:44.506126 139907754342144 logging_writer.py:48] [146800] global_step=146800, grad_norm=3.312833309173584, loss=1.6209094524383545
I0202 17:23:18.062324 139907762734848 logging_writer.py:48] [146900] global_step=146900, grad_norm=3.193554639816284, loss=1.5125466585159302
I0202 17:23:51.613339 139907754342144 logging_writer.py:48] [147000] global_step=147000, grad_norm=3.114301919937134, loss=1.319707989692688
I0202 17:24:25.191572 139907762734848 logging_writer.py:48] [147100] global_step=147100, grad_norm=3.0687193870544434, loss=1.4258242845535278
I0202 17:24:58.739430 139907754342144 logging_writer.py:48] [147200] global_step=147200, grad_norm=3.53965163230896, loss=1.5072650909423828
I0202 17:25:32.270466 139907762734848 logging_writer.py:48] [147300] global_step=147300, grad_norm=3.3546314239501953, loss=1.4404454231262207
I0202 17:26:04.255838 140070692116288 spec.py:321] Evaluating on the training split.
I0202 17:26:10.704730 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 17:26:19.300106 140070692116288 spec.py:349] Evaluating on the test split.
I0202 17:26:22.060224 140070692116288 submission_runner.py:408] Time since start: 51239.66s, 	Step: 147397, 	{'train/accuracy': 0.7065529227256775, 'train/loss': 1.1428754329681396, 'validation/accuracy': 0.6351799964904785, 'validation/loss': 1.5308077335357666, 'validation/num_examples': 50000, 'test/accuracy': 0.5109000205993652, 'test/loss': 2.299001932144165, 'test/num_examples': 10000, 'score': 49508.99960565567, 'total_duration': 51239.65789651871, 'accumulated_submission_time': 49508.99960565567, 'accumulated_eval_time': 1721.5213029384613, 'accumulated_logging_time': 3.9980673789978027}
I0202 17:26:22.106370 139907754342144 logging_writer.py:48] [147397] accumulated_eval_time=1721.521303, accumulated_logging_time=3.998067, accumulated_submission_time=49508.999606, global_step=147397, preemption_count=0, score=49508.999606, test/accuracy=0.510900, test/loss=2.299002, test/num_examples=10000, total_duration=51239.657897, train/accuracy=0.706553, train/loss=1.142875, validation/accuracy=0.635180, validation/loss=1.530808, validation/num_examples=50000
I0202 17:26:23.460874 139908719027968 logging_writer.py:48] [147400] global_step=147400, grad_norm=3.422053098678589, loss=1.4675010442733765
I0202 17:26:56.932831 139907754342144 logging_writer.py:48] [147500] global_step=147500, grad_norm=3.3955724239349365, loss=1.5407321453094482
I0202 17:27:30.448356 139908719027968 logging_writer.py:48] [147600] global_step=147600, grad_norm=3.323026657104492, loss=1.452770709991455
I0202 17:28:04.074210 139907754342144 logging_writer.py:48] [147700] global_step=147700, grad_norm=3.6511876583099365, loss=1.427301287651062
I0202 17:28:37.594853 139908719027968 logging_writer.py:48] [147800] global_step=147800, grad_norm=3.2996826171875, loss=1.3795499801635742
I0202 17:29:11.107223 139907754342144 logging_writer.py:48] [147900] global_step=147900, grad_norm=3.1784098148345947, loss=1.4403048753738403
I0202 17:29:44.690664 139908719027968 logging_writer.py:48] [148000] global_step=148000, grad_norm=3.5115301609039307, loss=1.564321517944336
I0202 17:30:18.242144 139907754342144 logging_writer.py:48] [148100] global_step=148100, grad_norm=3.612212657928467, loss=1.5759377479553223
I0202 17:30:51.793790 139908719027968 logging_writer.py:48] [148200] global_step=148200, grad_norm=3.412893295288086, loss=1.4046350717544556
I0202 17:31:25.365200 139907754342144 logging_writer.py:48] [148300] global_step=148300, grad_norm=3.5707125663757324, loss=1.4507167339324951
I0202 17:31:58.906138 139908719027968 logging_writer.py:48] [148400] global_step=148400, grad_norm=3.3808176517486572, loss=1.4628843069076538
I0202 17:32:32.504674 139907754342144 logging_writer.py:48] [148500] global_step=148500, grad_norm=3.149585008621216, loss=1.3570809364318848
I0202 17:33:06.030880 139908719027968 logging_writer.py:48] [148600] global_step=148600, grad_norm=3.6459877490997314, loss=1.480165719985962
I0202 17:33:39.551995 139907754342144 logging_writer.py:48] [148700] global_step=148700, grad_norm=3.251951217651367, loss=1.355371117591858
I0202 17:34:13.195764 139908719027968 logging_writer.py:48] [148800] global_step=148800, grad_norm=3.3794288635253906, loss=1.4940857887268066
I0202 17:34:46.745477 139907754342144 logging_writer.py:48] [148900] global_step=148900, grad_norm=3.403324842453003, loss=1.4809174537658691
I0202 17:34:52.263736 140070692116288 spec.py:321] Evaluating on the training split.
I0202 17:34:58.579165 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 17:35:06.944396 140070692116288 spec.py:349] Evaluating on the test split.
I0202 17:35:09.582553 140070692116288 submission_runner.py:408] Time since start: 51767.18s, 	Step: 148918, 	{'train/accuracy': 0.7453164458274841, 'train/loss': 0.9728658199310303, 'validation/accuracy': 0.6709199547767639, 'validation/loss': 1.3506791591644287, 'validation/num_examples': 50000, 'test/accuracy': 0.5452000498771667, 'test/loss': 2.0889601707458496, 'test/num_examples': 10000, 'score': 50019.09476184845, 'total_duration': 51767.180225372314, 'accumulated_submission_time': 50019.09476184845, 'accumulated_eval_time': 1738.840086698532, 'accumulated_logging_time': 4.0529398918151855}
I0202 17:35:09.626388 139907729164032 logging_writer.py:48] [148918] accumulated_eval_time=1738.840087, accumulated_logging_time=4.052940, accumulated_submission_time=50019.094762, global_step=148918, preemption_count=0, score=50019.094762, test/accuracy=0.545200, test/loss=2.088960, test/num_examples=10000, total_duration=51767.180225, train/accuracy=0.745316, train/loss=0.972866, validation/accuracy=0.670920, validation/loss=1.350679, validation/num_examples=50000
I0202 17:35:37.457168 139907737556736 logging_writer.py:48] [149000] global_step=149000, grad_norm=3.445988416671753, loss=1.4939148426055908
I0202 17:36:10.952172 139907729164032 logging_writer.py:48] [149100] global_step=149100, grad_norm=3.4377825260162354, loss=1.4848463535308838
I0202 17:36:44.495978 139907737556736 logging_writer.py:48] [149200] global_step=149200, grad_norm=3.517807722091675, loss=1.4728060960769653
I0202 17:37:18.002533 139907729164032 logging_writer.py:48] [149300] global_step=149300, grad_norm=3.461215019226074, loss=1.4519776105880737
I0202 17:37:51.563694 139907737556736 logging_writer.py:48] [149400] global_step=149400, grad_norm=3.2979507446289062, loss=1.3398771286010742
I0202 17:38:25.074073 139907729164032 logging_writer.py:48] [149500] global_step=149500, grad_norm=4.095339775085449, loss=1.4574687480926514
I0202 17:38:58.643484 139907737556736 logging_writer.py:48] [149600] global_step=149600, grad_norm=3.5433285236358643, loss=1.381261944770813
I0202 17:39:32.172313 139907729164032 logging_writer.py:48] [149700] global_step=149700, grad_norm=3.4852662086486816, loss=1.4267964363098145
I0202 17:40:05.719725 139907737556736 logging_writer.py:48] [149800] global_step=149800, grad_norm=3.435455322265625, loss=1.4923163652420044
I0202 17:40:39.321738 139907729164032 logging_writer.py:48] [149900] global_step=149900, grad_norm=3.303148031234741, loss=1.4230855703353882
I0202 17:41:12.853384 139907737556736 logging_writer.py:48] [150000] global_step=150000, grad_norm=3.5448591709136963, loss=1.3828883171081543
I0202 17:41:46.393845 139907729164032 logging_writer.py:48] [150100] global_step=150100, grad_norm=3.417203664779663, loss=1.3052934408187866
I0202 17:42:19.906411 139907737556736 logging_writer.py:48] [150200] global_step=150200, grad_norm=3.4512438774108887, loss=1.4589738845825195
I0202 17:42:53.437355 139907729164032 logging_writer.py:48] [150300] global_step=150300, grad_norm=3.5705368518829346, loss=1.4706213474273682
I0202 17:43:27.006767 139907737556736 logging_writer.py:48] [150400] global_step=150400, grad_norm=3.357246160507202, loss=1.375868320465088
I0202 17:43:39.912168 140070692116288 spec.py:321] Evaluating on the training split.
I0202 17:43:46.160220 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 17:43:54.853759 140070692116288 spec.py:349] Evaluating on the test split.
I0202 17:43:57.503446 140070692116288 submission_runner.py:408] Time since start: 52295.10s, 	Step: 150440, 	{'train/accuracy': 0.748465359210968, 'train/loss': 0.9664836525917053, 'validation/accuracy': 0.6740999817848206, 'validation/loss': 1.3404970169067383, 'validation/num_examples': 50000, 'test/accuracy': 0.5436000227928162, 'test/loss': 2.0569565296173096, 'test/num_examples': 10000, 'score': 50529.31740260124, 'total_duration': 52295.101024866104, 'accumulated_submission_time': 50529.31740260124, 'accumulated_eval_time': 1756.4312443733215, 'accumulated_logging_time': 4.107654333114624}
I0202 17:43:57.544143 139908719027968 logging_writer.py:48] [150440] accumulated_eval_time=1756.431244, accumulated_logging_time=4.107654, accumulated_submission_time=50529.317403, global_step=150440, preemption_count=0, score=50529.317403, test/accuracy=0.543600, test/loss=2.056957, test/num_examples=10000, total_duration=52295.101025, train/accuracy=0.748465, train/loss=0.966484, validation/accuracy=0.674100, validation/loss=1.340497, validation/num_examples=50000
I0202 17:44:17.976764 139908727420672 logging_writer.py:48] [150500] global_step=150500, grad_norm=3.543823480606079, loss=1.4774326086044312
I0202 17:44:51.479003 139908719027968 logging_writer.py:48] [150600] global_step=150600, grad_norm=3.6294350624084473, loss=1.4303351640701294
I0202 17:45:25.032327 139908727420672 logging_writer.py:48] [150700] global_step=150700, grad_norm=3.6176228523254395, loss=1.5518388748168945
I0202 17:45:58.574755 139908719027968 logging_writer.py:48] [150800] global_step=150800, grad_norm=3.3516173362731934, loss=1.352748155593872
I0202 17:46:32.127889 139908727420672 logging_writer.py:48] [150900] global_step=150900, grad_norm=3.6865720748901367, loss=1.4504042863845825
I0202 17:47:05.704543 139908719027968 logging_writer.py:48] [151000] global_step=151000, grad_norm=3.5341529846191406, loss=1.3571598529815674
I0202 17:47:39.350989 139908727420672 logging_writer.py:48] [151100] global_step=151100, grad_norm=3.5427286624908447, loss=1.3553574085235596
I0202 17:48:12.924118 139908719027968 logging_writer.py:48] [151200] global_step=151200, grad_norm=3.6279330253601074, loss=1.456691026687622
I0202 17:48:46.487359 139908727420672 logging_writer.py:48] [151300] global_step=151300, grad_norm=3.50854229927063, loss=1.4290744066238403
I0202 17:49:20.023144 139908719027968 logging_writer.py:48] [151400] global_step=151400, grad_norm=3.4036006927490234, loss=1.3425344228744507
I0202 17:49:53.573126 139908727420672 logging_writer.py:48] [151500] global_step=151500, grad_norm=3.628364324569702, loss=1.4126739501953125
I0202 17:50:27.121309 139908719027968 logging_writer.py:48] [151600] global_step=151600, grad_norm=3.664787769317627, loss=1.4412685632705688
I0202 17:51:00.694833 139908727420672 logging_writer.py:48] [151700] global_step=151700, grad_norm=3.4453647136688232, loss=1.413148283958435
I0202 17:51:34.286649 139908719027968 logging_writer.py:48] [151800] global_step=151800, grad_norm=3.6594531536102295, loss=1.4421579837799072
I0202 17:52:07.820887 139908727420672 logging_writer.py:48] [151900] global_step=151900, grad_norm=3.461941957473755, loss=1.3107541799545288
I0202 17:52:27.743678 140070692116288 spec.py:321] Evaluating on the training split.
I0202 17:52:34.118002 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 17:52:42.611892 140070692116288 spec.py:349] Evaluating on the test split.
I0202 17:52:45.281614 140070692116288 submission_runner.py:408] Time since start: 52822.88s, 	Step: 151961, 	{'train/accuracy': 0.7463727593421936, 'train/loss': 0.9735182523727417, 'validation/accuracy': 0.6724399924278259, 'validation/loss': 1.3431792259216309, 'validation/num_examples': 50000, 'test/accuracy': 0.5437000393867493, 'test/loss': 2.0814619064331055, 'test/num_examples': 10000, 'score': 51039.453364133835, 'total_duration': 52822.879269361496, 'accumulated_submission_time': 51039.453364133835, 'accumulated_eval_time': 1773.9691338539124, 'accumulated_logging_time': 4.158005237579346}
I0202 17:52:45.333416 139908425447168 logging_writer.py:48] [151961] accumulated_eval_time=1773.969134, accumulated_logging_time=4.158005, accumulated_submission_time=51039.453364, global_step=151961, preemption_count=0, score=51039.453364, test/accuracy=0.543700, test/loss=2.081462, test/num_examples=10000, total_duration=52822.879269, train/accuracy=0.746373, train/loss=0.973518, validation/accuracy=0.672440, validation/loss=1.343179, validation/num_examples=50000
I0202 17:52:58.751606 139908735813376 logging_writer.py:48] [152000] global_step=152000, grad_norm=3.615895986557007, loss=1.4142351150512695
I0202 17:53:32.324887 139908425447168 logging_writer.py:48] [152100] global_step=152100, grad_norm=3.5930442810058594, loss=1.3729103803634644
I0202 17:54:05.944486 139908735813376 logging_writer.py:48] [152200] global_step=152200, grad_norm=3.8449904918670654, loss=1.3775904178619385
I0202 17:54:39.455837 139908425447168 logging_writer.py:48] [152300] global_step=152300, grad_norm=3.4400696754455566, loss=1.3351953029632568
I0202 17:55:13.027415 139908735813376 logging_writer.py:48] [152400] global_step=152400, grad_norm=3.793327569961548, loss=1.5296337604522705
I0202 17:55:46.575500 139908425447168 logging_writer.py:48] [152500] global_step=152500, grad_norm=4.066468238830566, loss=1.345289707183838
I0202 17:56:20.092377 139908735813376 logging_writer.py:48] [152600] global_step=152600, grad_norm=3.7953972816467285, loss=1.4229284524917603
I0202 17:56:53.695579 139908425447168 logging_writer.py:48] [152700] global_step=152700, grad_norm=3.8123281002044678, loss=1.3864576816558838
I0202 17:57:27.204823 139908735813376 logging_writer.py:48] [152800] global_step=152800, grad_norm=3.543607473373413, loss=1.428977370262146
I0202 17:58:00.765620 139908425447168 logging_writer.py:48] [152900] global_step=152900, grad_norm=3.868288993835449, loss=1.4687553644180298
I0202 17:58:34.277664 139908735813376 logging_writer.py:48] [153000] global_step=153000, grad_norm=3.7115705013275146, loss=1.340338945388794
I0202 17:59:07.820832 139908425447168 logging_writer.py:48] [153100] global_step=153100, grad_norm=3.8107266426086426, loss=1.3481758832931519
I0202 17:59:41.337607 139908735813376 logging_writer.py:48] [153200] global_step=153200, grad_norm=3.6473116874694824, loss=1.444940447807312
I0202 18:00:14.949318 139908425447168 logging_writer.py:48] [153300] global_step=153300, grad_norm=3.734567165374756, loss=1.3716347217559814
I0202 18:00:48.456040 139908735813376 logging_writer.py:48] [153400] global_step=153400, grad_norm=3.9602906703948975, loss=1.4037071466445923
I0202 18:01:15.431561 140070692116288 spec.py:321] Evaluating on the training split.
I0202 18:01:21.802808 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 18:01:30.456192 140070692116288 spec.py:349] Evaluating on the test split.
I0202 18:01:33.087670 140070692116288 submission_runner.py:408] Time since start: 53350.69s, 	Step: 153482, 	{'train/accuracy': 0.7898397445678711, 'train/loss': 0.7781878113746643, 'validation/accuracy': 0.6873999834060669, 'validation/loss': 1.2663946151733398, 'validation/num_examples': 50000, 'test/accuracy': 0.5611000061035156, 'test/loss': 1.9653517007827759, 'test/num_examples': 10000, 'score': 51549.48877668381, 'total_duration': 53350.685337781906, 'accumulated_submission_time': 51549.48877668381, 'accumulated_eval_time': 1791.6252291202545, 'accumulated_logging_time': 4.218658447265625}
I0202 18:01:33.134505 139907745949440 logging_writer.py:48] [153482] accumulated_eval_time=1791.625229, accumulated_logging_time=4.218658, accumulated_submission_time=51549.488777, global_step=153482, preemption_count=0, score=51549.488777, test/accuracy=0.561100, test/loss=1.965352, test/num_examples=10000, total_duration=53350.685338, train/accuracy=0.789840, train/loss=0.778188, validation/accuracy=0.687400, validation/loss=1.266395, validation/num_examples=50000
I0202 18:01:39.510052 139907754342144 logging_writer.py:48] [153500] global_step=153500, grad_norm=3.633234977722168, loss=1.3622699975967407
I0202 18:02:12.990147 139907745949440 logging_writer.py:48] [153600] global_step=153600, grad_norm=3.800413131713867, loss=1.3958563804626465
I0202 18:02:46.540494 139907754342144 logging_writer.py:48] [153700] global_step=153700, grad_norm=3.7569456100463867, loss=1.376639485359192
I0202 18:03:20.069364 139907745949440 logging_writer.py:48] [153800] global_step=153800, grad_norm=4.024423599243164, loss=1.3423597812652588
I0202 18:03:53.587311 139907754342144 logging_writer.py:48] [153900] global_step=153900, grad_norm=3.817051887512207, loss=1.3499462604522705
I0202 18:04:27.180640 139907745949440 logging_writer.py:48] [154000] global_step=154000, grad_norm=3.840820074081421, loss=1.4507052898406982
I0202 18:05:00.770871 139907754342144 logging_writer.py:48] [154100] global_step=154100, grad_norm=3.7723090648651123, loss=1.4105374813079834
I0202 18:05:34.282259 139907745949440 logging_writer.py:48] [154200] global_step=154200, grad_norm=3.746030807495117, loss=1.4286129474639893
I0202 18:06:07.793101 139907754342144 logging_writer.py:48] [154300] global_step=154300, grad_norm=3.6821982860565186, loss=1.3022327423095703
I0202 18:06:41.375355 139907745949440 logging_writer.py:48] [154400] global_step=154400, grad_norm=3.8670010566711426, loss=1.4377657175064087
I0202 18:07:14.897764 139907754342144 logging_writer.py:48] [154500] global_step=154500, grad_norm=3.7302727699279785, loss=1.2404557466506958
I0202 18:07:48.447345 139907745949440 logging_writer.py:48] [154600] global_step=154600, grad_norm=3.555079221725464, loss=1.2667913436889648
I0202 18:08:21.947129 139907754342144 logging_writer.py:48] [154700] global_step=154700, grad_norm=3.745861053466797, loss=1.3386895656585693
I0202 18:08:55.533369 139907745949440 logging_writer.py:48] [154800] global_step=154800, grad_norm=3.9147815704345703, loss=1.3028016090393066
I0202 18:09:29.116792 139907754342144 logging_writer.py:48] [154900] global_step=154900, grad_norm=3.7096750736236572, loss=1.1724058389663696
I0202 18:10:02.625375 139907745949440 logging_writer.py:48] [155000] global_step=155000, grad_norm=3.7860352993011475, loss=1.2938711643218994
I0202 18:10:03.113048 140070692116288 spec.py:321] Evaluating on the training split.
I0202 18:10:09.385308 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 18:10:17.875528 140070692116288 spec.py:349] Evaluating on the test split.
I0202 18:10:20.380363 140070692116288 submission_runner.py:408] Time since start: 53877.98s, 	Step: 155003, 	{'train/accuracy': 0.7786391973495483, 'train/loss': 0.8249820470809937, 'validation/accuracy': 0.6894599795341492, 'validation/loss': 1.2767852544784546, 'validation/num_examples': 50000, 'test/accuracy': 0.5552999973297119, 'test/loss': 2.019956350326538, 'test/num_examples': 10000, 'score': 52059.40568423271, 'total_duration': 53877.978026390076, 'accumulated_submission_time': 52059.40568423271, 'accumulated_eval_time': 1808.8925030231476, 'accumulated_logging_time': 4.2747087478637695}
I0202 18:10:20.428545 139908719027968 logging_writer.py:48] [155003] accumulated_eval_time=1808.892503, accumulated_logging_time=4.274709, accumulated_submission_time=52059.405684, global_step=155003, preemption_count=0, score=52059.405684, test/accuracy=0.555300, test/loss=2.019956, test/num_examples=10000, total_duration=53877.978026, train/accuracy=0.778639, train/loss=0.824982, validation/accuracy=0.689460, validation/loss=1.276785, validation/num_examples=50000
I0202 18:10:53.242941 139908727420672 logging_writer.py:48] [155100] global_step=155100, grad_norm=3.9218363761901855, loss=1.373969554901123
I0202 18:11:26.725673 139908719027968 logging_writer.py:48] [155200] global_step=155200, grad_norm=3.811492443084717, loss=1.1908360719680786
I0202 18:12:00.245652 139908727420672 logging_writer.py:48] [155300] global_step=155300, grad_norm=3.6514220237731934, loss=1.3242758512496948
I0202 18:12:33.812051 139908719027968 logging_writer.py:48] [155400] global_step=155400, grad_norm=4.14558744430542, loss=1.3160429000854492
I0202 18:13:07.385948 139908727420672 logging_writer.py:48] [155500] global_step=155500, grad_norm=3.925901174545288, loss=1.340843677520752
I0202 18:13:40.891759 139908719027968 logging_writer.py:48] [155600] global_step=155600, grad_norm=3.7874159812927246, loss=1.3803977966308594
I0202 18:14:14.413601 139908727420672 logging_writer.py:48] [155700] global_step=155700, grad_norm=4.048334121704102, loss=1.3281543254852295
I0202 18:14:47.994382 139908719027968 logging_writer.py:48] [155800] global_step=155800, grad_norm=3.556227445602417, loss=1.293924331665039
I0202 18:15:21.528067 139908727420672 logging_writer.py:48] [155900] global_step=155900, grad_norm=3.876189947128296, loss=1.254130482673645
I0202 18:15:55.128690 139908719027968 logging_writer.py:48] [156000] global_step=156000, grad_norm=4.107476711273193, loss=1.4051393270492554
I0202 18:16:28.657894 139908727420672 logging_writer.py:48] [156100] global_step=156100, grad_norm=3.8346762657165527, loss=1.2575154304504395
I0202 18:17:02.201628 139908719027968 logging_writer.py:48] [156200] global_step=156200, grad_norm=3.9979281425476074, loss=1.2728301286697388
I0202 18:17:35.724120 139908727420672 logging_writer.py:48] [156300] global_step=156300, grad_norm=4.079932689666748, loss=1.3127477169036865
I0202 18:18:09.275419 139908719027968 logging_writer.py:48] [156400] global_step=156400, grad_norm=4.292606830596924, loss=1.3860201835632324
I0202 18:18:42.782887 139908727420672 logging_writer.py:48] [156500] global_step=156500, grad_norm=3.74333119392395, loss=1.197023868560791
I0202 18:18:50.652058 140070692116288 spec.py:321] Evaluating on the training split.
I0202 18:18:56.914787 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 18:19:05.596343 140070692116288 spec.py:349] Evaluating on the test split.
I0202 18:19:08.349248 140070692116288 submission_runner.py:408] Time since start: 54405.95s, 	Step: 156525, 	{'train/accuracy': 0.7767059803009033, 'train/loss': 0.8280686140060425, 'validation/accuracy': 0.6906200051307678, 'validation/loss': 1.253747582435608, 'validation/num_examples': 50000, 'test/accuracy': 0.5636000037193298, 'test/loss': 1.9697444438934326, 'test/num_examples': 10000, 'score': 52569.56496477127, 'total_duration': 54405.946820259094, 'accumulated_submission_time': 52569.56496477127, 'accumulated_eval_time': 1826.5895624160767, 'accumulated_logging_time': 4.334457635879517}
I0202 18:19:08.391977 139907754342144 logging_writer.py:48] [156525] accumulated_eval_time=1826.589562, accumulated_logging_time=4.334458, accumulated_submission_time=52569.564965, global_step=156525, preemption_count=0, score=52569.564965, test/accuracy=0.563600, test/loss=1.969744, test/num_examples=10000, total_duration=54405.946820, train/accuracy=0.776706, train/loss=0.828069, validation/accuracy=0.690620, validation/loss=1.253748, validation/num_examples=50000
I0202 18:19:33.999820 139907762734848 logging_writer.py:48] [156600] global_step=156600, grad_norm=4.103593826293945, loss=1.458186149597168
I0202 18:20:07.472857 139907754342144 logging_writer.py:48] [156700] global_step=156700, grad_norm=3.7220654487609863, loss=1.3046276569366455
I0202 18:20:41.039456 139907762734848 logging_writer.py:48] [156800] global_step=156800, grad_norm=3.871476173400879, loss=1.2761163711547852
I0202 18:21:14.535223 139907754342144 logging_writer.py:48] [156900] global_step=156900, grad_norm=4.061779499053955, loss=1.369447946548462
I0202 18:21:48.041941 139907762734848 logging_writer.py:48] [157000] global_step=157000, grad_norm=4.202646732330322, loss=1.4104180335998535
I0202 18:22:21.591308 139907754342144 logging_writer.py:48] [157100] global_step=157100, grad_norm=4.04762077331543, loss=1.3411624431610107
I0202 18:22:55.167150 139907762734848 logging_writer.py:48] [157200] global_step=157200, grad_norm=3.8014585971832275, loss=1.3146566152572632
I0202 18:23:28.695202 139907754342144 logging_writer.py:48] [157300] global_step=157300, grad_norm=3.9571940898895264, loss=1.283536672592163
I0202 18:24:02.258948 139907762734848 logging_writer.py:48] [157400] global_step=157400, grad_norm=3.78515887260437, loss=1.3084735870361328
I0202 18:24:35.767814 139907754342144 logging_writer.py:48] [157500] global_step=157500, grad_norm=3.9422571659088135, loss=1.3245384693145752
I0202 18:25:09.310347 139907762734848 logging_writer.py:48] [157600] global_step=157600, grad_norm=3.902642011642456, loss=1.2568120956420898
I0202 18:25:42.918105 139907754342144 logging_writer.py:48] [157700] global_step=157700, grad_norm=3.9304769039154053, loss=1.2016210556030273
I0202 18:26:16.440326 139907762734848 logging_writer.py:48] [157800] global_step=157800, grad_norm=4.318172454833984, loss=1.3902056217193604
I0202 18:26:49.946770 139907754342144 logging_writer.py:48] [157900] global_step=157900, grad_norm=4.37999153137207, loss=1.3369758129119873
I0202 18:27:23.480929 139907762734848 logging_writer.py:48] [158000] global_step=158000, grad_norm=4.238680362701416, loss=1.3383806943893433
I0202 18:27:38.372305 140070692116288 spec.py:321] Evaluating on the training split.
I0202 18:27:44.852864 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 18:27:53.188364 140070692116288 spec.py:349] Evaluating on the test split.
I0202 18:27:55.821067 140070692116288 submission_runner.py:408] Time since start: 54933.42s, 	Step: 158046, 	{'train/accuracy': 0.7870694994926453, 'train/loss': 0.7952739596366882, 'validation/accuracy': 0.6987599730491638, 'validation/loss': 1.248716950416565, 'validation/num_examples': 50000, 'test/accuracy': 0.5710000395774841, 'test/loss': 1.9607090950012207, 'test/num_examples': 10000, 'score': 53079.482422828674, 'total_duration': 54933.418738126755, 'accumulated_submission_time': 53079.482422828674, 'accumulated_eval_time': 1844.0382986068726, 'accumulated_logging_time': 4.386325359344482}
I0202 18:27:55.864980 139907737556736 logging_writer.py:48] [158046] accumulated_eval_time=1844.038299, accumulated_logging_time=4.386325, accumulated_submission_time=53079.482423, global_step=158046, preemption_count=0, score=53079.482423, test/accuracy=0.571000, test/loss=1.960709, test/num_examples=10000, total_duration=54933.418738, train/accuracy=0.787069, train/loss=0.795274, validation/accuracy=0.698760, validation/loss=1.248717, validation/num_examples=50000
I0202 18:28:14.297490 139907745949440 logging_writer.py:48] [158100] global_step=158100, grad_norm=4.058628559112549, loss=1.3910250663757324
I0202 18:28:47.792169 139907737556736 logging_writer.py:48] [158200] global_step=158200, grad_norm=3.9336390495300293, loss=1.354900598526001
I0202 18:29:21.357803 139907745949440 logging_writer.py:48] [158300] global_step=158300, grad_norm=3.628854751586914, loss=1.1864047050476074
I0202 18:29:54.872946 139907737556736 logging_writer.py:48] [158400] global_step=158400, grad_norm=4.130890846252441, loss=1.2869306802749634
I0202 18:30:28.433818 139907745949440 logging_writer.py:48] [158500] global_step=158500, grad_norm=4.249881267547607, loss=1.446252703666687
I0202 18:31:01.970433 139907737556736 logging_writer.py:48] [158600] global_step=158600, grad_norm=4.069432258605957, loss=1.2599010467529297
I0202 18:31:35.524621 139907745949440 logging_writer.py:48] [158700] global_step=158700, grad_norm=4.058408260345459, loss=1.2504322528839111
I0202 18:32:09.263055 139907737556736 logging_writer.py:48] [158800] global_step=158800, grad_norm=4.194538116455078, loss=1.2674802541732788
I0202 18:32:42.785487 139907745949440 logging_writer.py:48] [158900] global_step=158900, grad_norm=4.206618309020996, loss=1.3761804103851318
I0202 18:33:16.309159 139907737556736 logging_writer.py:48] [159000] global_step=159000, grad_norm=4.407153606414795, loss=1.3205926418304443
I0202 18:33:49.909453 139907745949440 logging_writer.py:48] [159100] global_step=159100, grad_norm=3.941289186477661, loss=1.2615513801574707
I0202 18:34:23.424950 139907737556736 logging_writer.py:48] [159200] global_step=159200, grad_norm=4.318914890289307, loss=1.3554655313491821
I0202 18:34:56.992100 139907745949440 logging_writer.py:48] [159300] global_step=159300, grad_norm=3.9702517986297607, loss=1.2119204998016357
I0202 18:35:30.537613 139907737556736 logging_writer.py:48] [159400] global_step=159400, grad_norm=4.0609660148620605, loss=1.2515323162078857
I0202 18:36:04.053603 139907745949440 logging_writer.py:48] [159500] global_step=159500, grad_norm=4.446376323699951, loss=1.3907469511032104
I0202 18:36:26.013320 140070692116288 spec.py:321] Evaluating on the training split.
I0202 18:36:32.429290 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 18:36:41.204815 140070692116288 spec.py:349] Evaluating on the test split.
I0202 18:36:43.816408 140070692116288 submission_runner.py:408] Time since start: 55461.41s, 	Step: 159567, 	{'train/accuracy': 0.7997050285339355, 'train/loss': 0.7518975138664246, 'validation/accuracy': 0.7044199705123901, 'validation/loss': 1.2122431993484497, 'validation/num_examples': 50000, 'test/accuracy': 0.5830000042915344, 'test/loss': 1.9153326749801636, 'test/num_examples': 10000, 'score': 53589.56930446625, 'total_duration': 55461.414071798325, 'accumulated_submission_time': 53589.56930446625, 'accumulated_eval_time': 1861.841367483139, 'accumulated_logging_time': 4.439189434051514}
I0202 18:36:43.861937 139907745949440 logging_writer.py:48] [159567] accumulated_eval_time=1861.841367, accumulated_logging_time=4.439189, accumulated_submission_time=53589.569304, global_step=159567, preemption_count=0, score=53589.569304, test/accuracy=0.583000, test/loss=1.915333, test/num_examples=10000, total_duration=55461.414072, train/accuracy=0.799705, train/loss=0.751898, validation/accuracy=0.704420, validation/loss=1.212243, validation/num_examples=50000
I0202 18:36:55.247854 139907754342144 logging_writer.py:48] [159600] global_step=159600, grad_norm=4.129096508026123, loss=1.2601025104522705
I0202 18:37:28.715068 139907745949440 logging_writer.py:48] [159700] global_step=159700, grad_norm=4.314009189605713, loss=1.2304980754852295
I0202 18:38:02.230901 139907754342144 logging_writer.py:48] [159800] global_step=159800, grad_norm=4.122140407562256, loss=1.2174510955810547
I0202 18:38:35.865433 139907745949440 logging_writer.py:48] [159900] global_step=159900, grad_norm=3.9975967407226562, loss=1.2584997415542603
I0202 18:39:09.392128 139907754342144 logging_writer.py:48] [160000] global_step=160000, grad_norm=4.245530128479004, loss=1.2232913970947266
I0202 18:39:42.898160 139907745949440 logging_writer.py:48] [160100] global_step=160100, grad_norm=4.451995849609375, loss=1.2663837671279907
I0202 18:40:16.476221 139907754342144 logging_writer.py:48] [160200] global_step=160200, grad_norm=4.21980619430542, loss=1.252171516418457
I0202 18:40:50.020111 139907745949440 logging_writer.py:48] [160300] global_step=160300, grad_norm=4.071812629699707, loss=1.228263258934021
I0202 18:41:23.583420 139907754342144 logging_writer.py:48] [160400] global_step=160400, grad_norm=4.344470500946045, loss=1.2478978633880615
I0202 18:41:57.153794 139907745949440 logging_writer.py:48] [160500] global_step=160500, grad_norm=4.179111480712891, loss=1.2653331756591797
I0202 18:42:30.681109 139907754342144 logging_writer.py:48] [160600] global_step=160600, grad_norm=4.445243835449219, loss=1.2518415451049805
I0202 18:43:04.248921 139907745949440 logging_writer.py:48] [160700] global_step=160700, grad_norm=4.226293087005615, loss=1.3044615983963013
I0202 18:43:37.772483 139907754342144 logging_writer.py:48] [160800] global_step=160800, grad_norm=4.204617500305176, loss=1.2856407165527344
I0202 18:44:11.357399 139907745949440 logging_writer.py:48] [160900] global_step=160900, grad_norm=4.03713321685791, loss=1.256391167640686
I0202 18:44:44.858677 139907754342144 logging_writer.py:48] [161000] global_step=161000, grad_norm=4.183534622192383, loss=1.2600692510604858
I0202 18:45:13.930412 140070692116288 spec.py:321] Evaluating on the training split.
I0202 18:45:20.208880 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 18:45:28.871747 140070692116288 spec.py:349] Evaluating on the test split.
I0202 18:45:31.505382 140070692116288 submission_runner.py:408] Time since start: 55989.10s, 	Step: 161088, 	{'train/accuracy': 0.7997449040412903, 'train/loss': 0.7390770316123962, 'validation/accuracy': 0.7134400010108948, 'validation/loss': 1.1649560928344727, 'validation/num_examples': 50000, 'test/accuracy': 0.5833000540733337, 'test/loss': 1.886929988861084, 'test/num_examples': 10000, 'score': 54099.57530117035, 'total_duration': 55989.10298204422, 'accumulated_submission_time': 54099.57530117035, 'accumulated_eval_time': 1879.416244983673, 'accumulated_logging_time': 4.49467396736145}
I0202 18:45:31.551178 139907745949440 logging_writer.py:48] [161088] accumulated_eval_time=1879.416245, accumulated_logging_time=4.494674, accumulated_submission_time=54099.575301, global_step=161088, preemption_count=0, score=54099.575301, test/accuracy=0.583300, test/loss=1.886930, test/num_examples=10000, total_duration=55989.102982, train/accuracy=0.799745, train/loss=0.739077, validation/accuracy=0.713440, validation/loss=1.164956, validation/num_examples=50000
I0202 18:45:35.917291 139907754342144 logging_writer.py:48] [161100] global_step=161100, grad_norm=4.030168533325195, loss=1.2915459871292114
I0202 18:46:09.393569 139907745949440 logging_writer.py:48] [161200] global_step=161200, grad_norm=4.192537307739258, loss=1.1225600242614746
I0202 18:46:42.911413 139907754342144 logging_writer.py:48] [161300] global_step=161300, grad_norm=4.055719375610352, loss=1.1541460752487183
I0202 18:47:16.502707 139907745949440 logging_writer.py:48] [161400] global_step=161400, grad_norm=4.140853404998779, loss=1.1763867139816284
I0202 18:47:50.026303 139907754342144 logging_writer.py:48] [161500] global_step=161500, grad_norm=4.420088768005371, loss=1.2452961206436157
I0202 18:48:23.606590 139907745949440 logging_writer.py:48] [161600] global_step=161600, grad_norm=4.245805740356445, loss=1.215157151222229
I0202 18:48:57.116503 139907754342144 logging_writer.py:48] [161700] global_step=161700, grad_norm=4.16920280456543, loss=1.1575887203216553
I0202 18:49:30.623842 139907745949440 logging_writer.py:48] [161800] global_step=161800, grad_norm=4.572299003601074, loss=1.1328816413879395
I0202 18:50:04.157467 139907754342144 logging_writer.py:48] [161900] global_step=161900, grad_norm=4.132766246795654, loss=1.1685853004455566
I0202 18:50:37.730161 139907745949440 logging_writer.py:48] [162000] global_step=162000, grad_norm=3.9444994926452637, loss=1.165048599243164
I0202 18:51:11.274909 139907754342144 logging_writer.py:48] [162100] global_step=162100, grad_norm=4.336705684661865, loss=1.1655995845794678
I0202 18:51:44.877671 139907745949440 logging_writer.py:48] [162200] global_step=162200, grad_norm=4.441226482391357, loss=1.2524285316467285
I0202 18:52:18.386436 139907754342144 logging_writer.py:48] [162300] global_step=162300, grad_norm=4.349488258361816, loss=1.2376545667648315
I0202 18:52:51.933832 139907745949440 logging_writer.py:48] [162400] global_step=162400, grad_norm=4.316311836242676, loss=1.2279232740402222
I0202 18:53:25.454790 139907754342144 logging_writer.py:48] [162500] global_step=162500, grad_norm=4.32798433303833, loss=1.2020777463912964
I0202 18:53:59.057865 139907745949440 logging_writer.py:48] [162600] global_step=162600, grad_norm=4.05531120300293, loss=1.1158748865127563
I0202 18:54:01.567514 140070692116288 spec.py:321] Evaluating on the training split.
I0202 18:54:08.019308 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 18:54:16.752138 140070692116288 spec.py:349] Evaluating on the test split.
I0202 18:54:19.377513 140070692116288 submission_runner.py:408] Time since start: 56516.98s, 	Step: 162609, 	{'train/accuracy': 0.8374122977256775, 'train/loss': 0.5941317677497864, 'validation/accuracy': 0.7181999683380127, 'validation/loss': 1.1413167715072632, 'validation/num_examples': 50000, 'test/accuracy': 0.5940999984741211, 'test/loss': 1.8632287979125977, 'test/num_examples': 10000, 'score': 54609.529450416565, 'total_duration': 56516.9751701355, 'accumulated_submission_time': 54609.529450416565, 'accumulated_eval_time': 1897.226199388504, 'accumulated_logging_time': 4.549734115600586}
I0202 18:54:19.421846 139907745949440 logging_writer.py:48] [162609] accumulated_eval_time=1897.226199, accumulated_logging_time=4.549734, accumulated_submission_time=54609.529450, global_step=162609, preemption_count=0, score=54609.529450, test/accuracy=0.594100, test/loss=1.863229, test/num_examples=10000, total_duration=56516.975170, train/accuracy=0.837412, train/loss=0.594132, validation/accuracy=0.718200, validation/loss=1.141317, validation/num_examples=50000
I0202 18:54:50.295513 139907762734848 logging_writer.py:48] [162700] global_step=162700, grad_norm=4.625612735748291, loss=1.2547303438186646
I0202 18:55:23.789824 139907745949440 logging_writer.py:48] [162800] global_step=162800, grad_norm=4.594665050506592, loss=1.2937860488891602
I0202 18:55:57.410249 139907762734848 logging_writer.py:48] [162900] global_step=162900, grad_norm=4.523369312286377, loss=1.3781311511993408
I0202 18:56:30.961278 139907745949440 logging_writer.py:48] [163000] global_step=163000, grad_norm=4.524725914001465, loss=1.2438514232635498
I0202 18:57:04.528843 139907762734848 logging_writer.py:48] [163100] global_step=163100, grad_norm=4.404381275177002, loss=1.2728755474090576
I0202 18:57:38.057981 139907745949440 logging_writer.py:48] [163200] global_step=163200, grad_norm=4.22280740737915, loss=1.2273293733596802
I0202 18:58:11.785032 139907762734848 logging_writer.py:48] [163300] global_step=163300, grad_norm=4.317692756652832, loss=1.1441490650177002
I0202 18:58:45.307137 139907745949440 logging_writer.py:48] [163400] global_step=163400, grad_norm=4.53352165222168, loss=1.2090709209442139
I0202 18:59:18.876623 139907762734848 logging_writer.py:48] [163500] global_step=163500, grad_norm=4.0995259284973145, loss=1.1076991558074951
I0202 18:59:52.464440 139907745949440 logging_writer.py:48] [163600] global_step=163600, grad_norm=4.2552385330200195, loss=1.1933295726776123
I0202 19:00:25.971203 139907762734848 logging_writer.py:48] [163700] global_step=163700, grad_norm=4.237877368927002, loss=1.21006178855896
I0202 19:00:59.501146 139907745949440 logging_writer.py:48] [163800] global_step=163800, grad_norm=4.48536491394043, loss=1.18330717086792
I0202 19:01:33.008690 139907762734848 logging_writer.py:48] [163900] global_step=163900, grad_norm=4.427381992340088, loss=1.2201601266860962
I0202 19:02:06.524717 139907745949440 logging_writer.py:48] [164000] global_step=164000, grad_norm=4.566150665283203, loss=1.117340087890625
I0202 19:02:40.040572 139907762734848 logging_writer.py:48] [164100] global_step=164100, grad_norm=4.767771244049072, loss=1.2811998128890991
I0202 19:02:49.564898 140070692116288 spec.py:321] Evaluating on the training split.
I0202 19:02:55.912197 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 19:03:04.457951 140070692116288 spec.py:349] Evaluating on the test split.
I0202 19:03:07.101028 140070692116288 submission_runner.py:408] Time since start: 57044.70s, 	Step: 164130, 	{'train/accuracy': 0.8246572017669678, 'train/loss': 0.6434984803199768, 'validation/accuracy': 0.7135599851608276, 'validation/loss': 1.1545459032058716, 'validation/num_examples': 50000, 'test/accuracy': 0.5824000239372253, 'test/loss': 1.869858741760254, 'test/num_examples': 10000, 'score': 55119.610013246536, 'total_duration': 57044.69862341881, 'accumulated_submission_time': 55119.610013246536, 'accumulated_eval_time': 1914.762225151062, 'accumulated_logging_time': 4.603350400924683}
I0202 19:03:07.152642 139907737556736 logging_writer.py:48] [164130] accumulated_eval_time=1914.762225, accumulated_logging_time=4.603350, accumulated_submission_time=55119.610013, global_step=164130, preemption_count=0, score=55119.610013, test/accuracy=0.582400, test/loss=1.869859, test/num_examples=10000, total_duration=57044.698623, train/accuracy=0.824657, train/loss=0.643498, validation/accuracy=0.713560, validation/loss=1.154546, validation/num_examples=50000
I0202 19:03:30.910828 139908710635264 logging_writer.py:48] [164200] global_step=164200, grad_norm=4.676477432250977, loss=1.1973353624343872
I0202 19:04:04.397838 139907737556736 logging_writer.py:48] [164300] global_step=164300, grad_norm=4.329925537109375, loss=1.1646833419799805
I0202 19:04:38.032062 139908710635264 logging_writer.py:48] [164400] global_step=164400, grad_norm=4.7508134841918945, loss=1.2961132526397705
I0202 19:05:11.556633 139907737556736 logging_writer.py:48] [164500] global_step=164500, grad_norm=4.5430450439453125, loss=1.228874683380127
I0202 19:05:45.081595 139908710635264 logging_writer.py:48] [164600] global_step=164600, grad_norm=4.467390537261963, loss=1.1540324687957764
I0202 19:06:18.580044 139907737556736 logging_writer.py:48] [164700] global_step=164700, grad_norm=4.601302146911621, loss=1.2180790901184082
I0202 19:06:52.125574 139908710635264 logging_writer.py:48] [164800] global_step=164800, grad_norm=4.1965508460998535, loss=0.987375020980835
I0202 19:07:25.719733 139907737556736 logging_writer.py:48] [164900] global_step=164900, grad_norm=4.802913665771484, loss=1.1912128925323486
I0202 19:07:59.231415 139908710635264 logging_writer.py:48] [165000] global_step=165000, grad_norm=4.574869632720947, loss=1.1856048107147217
I0202 19:08:32.791296 139907737556736 logging_writer.py:48] [165100] global_step=165100, grad_norm=4.263167858123779, loss=1.1176871061325073
I0202 19:09:06.339074 139908710635264 logging_writer.py:48] [165200] global_step=165200, grad_norm=4.788990020751953, loss=1.269842505455017
I0202 19:09:39.855062 139907737556736 logging_writer.py:48] [165300] global_step=165300, grad_norm=4.571615219116211, loss=1.143127679824829
I0202 19:10:13.371402 139908710635264 logging_writer.py:48] [165400] global_step=165400, grad_norm=4.399917125701904, loss=1.1190775632858276
I0202 19:10:47.025647 139907737556736 logging_writer.py:48] [165500] global_step=165500, grad_norm=4.594738483428955, loss=1.1353929042816162
I0202 19:11:20.553968 139908710635264 logging_writer.py:48] [165600] global_step=165600, grad_norm=4.274082660675049, loss=1.0566353797912598
I0202 19:11:37.110270 140070692116288 spec.py:321] Evaluating on the training split.
I0202 19:11:44.210393 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 19:11:52.843314 140070692116288 spec.py:349] Evaluating on the test split.
I0202 19:11:55.471961 140070692116288 submission_runner.py:408] Time since start: 57573.07s, 	Step: 165651, 	{'train/accuracy': 0.8374919891357422, 'train/loss': 0.596867024898529, 'validation/accuracy': 0.7252799868583679, 'validation/loss': 1.1151868104934692, 'validation/num_examples': 50000, 'test/accuracy': 0.5944000482559204, 'test/loss': 1.8382610082626343, 'test/num_examples': 10000, 'score': 55629.50521326065, 'total_duration': 57573.0696310997, 'accumulated_submission_time': 55629.50521326065, 'accumulated_eval_time': 1933.1238858699799, 'accumulated_logging_time': 4.66450572013855}
I0202 19:11:55.536767 139907745949440 logging_writer.py:48] [165651] accumulated_eval_time=1933.123886, accumulated_logging_time=4.664506, accumulated_submission_time=55629.505213, global_step=165651, preemption_count=0, score=55629.505213, test/accuracy=0.594400, test/loss=1.838261, test/num_examples=10000, total_duration=57573.069631, train/accuracy=0.837492, train/loss=0.596867, validation/accuracy=0.725280, validation/loss=1.115187, validation/num_examples=50000
I0202 19:12:12.273637 139907754342144 logging_writer.py:48] [165700] global_step=165700, grad_norm=4.560421466827393, loss=1.1932684183120728
I0202 19:12:45.751796 139907745949440 logging_writer.py:48] [165800] global_step=165800, grad_norm=4.476616382598877, loss=1.1464356184005737
I0202 19:13:19.272676 139907754342144 logging_writer.py:48] [165900] global_step=165900, grad_norm=4.589740753173828, loss=1.163977861404419
I0202 19:13:52.836128 139907745949440 logging_writer.py:48] [166000] global_step=166000, grad_norm=4.333164215087891, loss=1.1420811414718628
I0202 19:14:26.348139 139907754342144 logging_writer.py:48] [166100] global_step=166100, grad_norm=4.6054511070251465, loss=1.0274829864501953
I0202 19:14:59.914463 139907745949440 logging_writer.py:48] [166200] global_step=166200, grad_norm=5.22156286239624, loss=1.2503528594970703
I0202 19:15:33.450326 139907754342144 logging_writer.py:48] [166300] global_step=166300, grad_norm=4.969810962677002, loss=1.1778205633163452
I0202 19:16:06.996594 139907745949440 logging_writer.py:48] [166400] global_step=166400, grad_norm=4.789336204528809, loss=1.1550451517105103
I0202 19:16:40.500652 139907754342144 logging_writer.py:48] [166500] global_step=166500, grad_norm=4.281639575958252, loss=1.0708385705947876
I0202 19:17:14.134395 139907745949440 logging_writer.py:48] [166600] global_step=166600, grad_norm=4.734193801879883, loss=1.1323421001434326
I0202 19:17:47.658635 139907754342144 logging_writer.py:48] [166700] global_step=166700, grad_norm=4.974337577819824, loss=1.1193952560424805
I0202 19:18:21.187063 139907745949440 logging_writer.py:48] [166800] global_step=166800, grad_norm=4.585442066192627, loss=1.193307638168335
I0202 19:18:54.732160 139907754342144 logging_writer.py:48] [166900] global_step=166900, grad_norm=4.822042465209961, loss=1.1353614330291748
I0202 19:19:28.268705 139907745949440 logging_writer.py:48] [167000] global_step=167000, grad_norm=4.2755560874938965, loss=1.116919994354248
I0202 19:20:01.786423 139907754342144 logging_writer.py:48] [167100] global_step=167100, grad_norm=4.784027099609375, loss=1.1704833507537842
I0202 19:20:25.752164 140070692116288 spec.py:321] Evaluating on the training split.
I0202 19:20:31.992572 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 19:20:40.361346 140070692116288 spec.py:349] Evaluating on the test split.
I0202 19:20:43.029443 140070692116288 submission_runner.py:408] Time since start: 58100.63s, 	Step: 167173, 	{'train/accuracy': 0.8379902839660645, 'train/loss': 0.5885335206985474, 'validation/accuracy': 0.7274599671363831, 'validation/loss': 1.0997930765151978, 'validation/num_examples': 50000, 'test/accuracy': 0.6052000522613525, 'test/loss': 1.8111016750335693, 'test/num_examples': 10000, 'score': 56139.65770363808, 'total_duration': 58100.62710976601, 'accumulated_submission_time': 56139.65770363808, 'accumulated_eval_time': 1950.4011313915253, 'accumulated_logging_time': 4.738975524902344}
I0202 19:20:43.079149 139907737556736 logging_writer.py:48] [167173] accumulated_eval_time=1950.401131, accumulated_logging_time=4.738976, accumulated_submission_time=56139.657704, global_step=167173, preemption_count=0, score=56139.657704, test/accuracy=0.605200, test/loss=1.811102, test/num_examples=10000, total_duration=58100.627110, train/accuracy=0.837990, train/loss=0.588534, validation/accuracy=0.727460, validation/loss=1.099793, validation/num_examples=50000
I0202 19:20:52.476633 139908710635264 logging_writer.py:48] [167200] global_step=167200, grad_norm=4.381064414978027, loss=1.0780130624771118
I0202 19:21:25.969974 139907737556736 logging_writer.py:48] [167300] global_step=167300, grad_norm=4.8602070808410645, loss=1.1489624977111816
I0202 19:21:59.526059 139908710635264 logging_writer.py:48] [167400] global_step=167400, grad_norm=4.313503265380859, loss=1.0396804809570312
I0202 19:22:33.062666 139907737556736 logging_writer.py:48] [167500] global_step=167500, grad_norm=4.726345062255859, loss=1.1016290187835693
I0202 19:23:06.608190 139908710635264 logging_writer.py:48] [167600] global_step=167600, grad_norm=4.556424140930176, loss=1.0512447357177734
I0202 19:23:40.192528 139907737556736 logging_writer.py:48] [167700] global_step=167700, grad_norm=4.788991451263428, loss=1.1093355417251587
I0202 19:24:13.737876 139908710635264 logging_writer.py:48] [167800] global_step=167800, grad_norm=4.6952128410339355, loss=1.1313183307647705
I0202 19:24:47.265804 139907737556736 logging_writer.py:48] [167900] global_step=167900, grad_norm=4.87031364440918, loss=1.2490781545639038
I0202 19:25:20.852735 139908710635264 logging_writer.py:48] [168000] global_step=168000, grad_norm=4.901393890380859, loss=1.1858614683151245
I0202 19:25:54.364114 139907737556736 logging_writer.py:48] [168100] global_step=168100, grad_norm=4.471693515777588, loss=1.1252601146697998
I0202 19:26:27.898307 139908710635264 logging_writer.py:48] [168200] global_step=168200, grad_norm=4.999421119689941, loss=1.1099231243133545
I0202 19:27:01.423570 139907737556736 logging_writer.py:48] [168300] global_step=168300, grad_norm=5.089481353759766, loss=1.1501779556274414
I0202 19:27:34.986237 139908710635264 logging_writer.py:48] [168400] global_step=168400, grad_norm=5.058266639709473, loss=1.131470799446106
I0202 19:28:08.494973 139907737556736 logging_writer.py:48] [168500] global_step=168500, grad_norm=4.953652381896973, loss=1.1089651584625244
I0202 19:28:42.066976 139908710635264 logging_writer.py:48] [168600] global_step=168600, grad_norm=4.79408597946167, loss=1.0803316831588745
I0202 19:29:13.356624 140070692116288 spec.py:321] Evaluating on the training split.
I0202 19:29:19.620984 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 19:29:28.217764 140070692116288 spec.py:349] Evaluating on the test split.
I0202 19:29:30.864895 140070692116288 submission_runner.py:408] Time since start: 58628.46s, 	Step: 168695, 	{'train/accuracy': 0.8406209945678711, 'train/loss': 0.5672109723091125, 'validation/accuracy': 0.730459988117218, 'validation/loss': 1.097076654434204, 'validation/num_examples': 50000, 'test/accuracy': 0.6029000282287598, 'test/loss': 1.8191895484924316, 'test/num_examples': 10000, 'score': 56649.87207078934, 'total_duration': 58628.46256566048, 'accumulated_submission_time': 56649.87207078934, 'accumulated_eval_time': 1967.9093770980835, 'accumulated_logging_time': 4.797713041305542}
I0202 19:29:30.913711 139907754342144 logging_writer.py:48] [168695] accumulated_eval_time=1967.909377, accumulated_logging_time=4.797713, accumulated_submission_time=56649.872071, global_step=168695, preemption_count=0, score=56649.872071, test/accuracy=0.602900, test/loss=1.819190, test/num_examples=10000, total_duration=58628.462566, train/accuracy=0.840621, train/loss=0.567211, validation/accuracy=0.730460, validation/loss=1.097077, validation/num_examples=50000
I0202 19:29:32.918487 139907762734848 logging_writer.py:48] [168700] global_step=168700, grad_norm=4.8756256103515625, loss=1.1865947246551514
I0202 19:30:06.477536 139907754342144 logging_writer.py:48] [168800] global_step=168800, grad_norm=4.96653938293457, loss=1.1441293954849243
I0202 19:30:39.960655 139907762734848 logging_writer.py:48] [168900] global_step=168900, grad_norm=4.847340106964111, loss=1.0848522186279297
I0202 19:31:13.489602 139907754342144 logging_writer.py:48] [169000] global_step=169000, grad_norm=4.597927093505859, loss=1.0649768114089966
I0202 19:31:46.988729 139907762734848 logging_writer.py:48] [169100] global_step=169100, grad_norm=4.959292411804199, loss=1.0968024730682373
I0202 19:32:20.502331 139907754342144 logging_writer.py:48] [169200] global_step=169200, grad_norm=4.816308975219727, loss=1.1590718030929565
I0202 19:32:54.027990 139907762734848 logging_writer.py:48] [169300] global_step=169300, grad_norm=4.675504684448242, loss=1.0373629331588745
I0202 19:33:27.582319 139907754342144 logging_writer.py:48] [169400] global_step=169400, grad_norm=4.772475242614746, loss=1.0541532039642334
I0202 19:34:01.093679 139907762734848 logging_writer.py:48] [169500] global_step=169500, grad_norm=4.922093868255615, loss=1.1867703199386597
I0202 19:34:34.632837 139907754342144 logging_writer.py:48] [169600] global_step=169600, grad_norm=4.965836524963379, loss=1.0772573947906494
I0202 19:35:08.165725 139907762734848 logging_writer.py:48] [169700] global_step=169700, grad_norm=5.131852626800537, loss=1.099039077758789
I0202 19:35:41.768278 139907754342144 logging_writer.py:48] [169800] global_step=169800, grad_norm=4.64680814743042, loss=0.9855667352676392
I0202 19:36:15.253955 139907762734848 logging_writer.py:48] [169900] global_step=169900, grad_norm=5.174971103668213, loss=1.0237256288528442
I0202 19:36:48.975507 139907754342144 logging_writer.py:48] [170000] global_step=170000, grad_norm=4.988613128662109, loss=1.114872932434082
I0202 19:37:22.482766 139907762734848 logging_writer.py:48] [170100] global_step=170100, grad_norm=4.758819103240967, loss=1.075125813484192
I0202 19:37:56.015009 139907754342144 logging_writer.py:48] [170200] global_step=170200, grad_norm=5.307185649871826, loss=1.1521109342575073
I0202 19:38:01.186996 140070692116288 spec.py:321] Evaluating on the training split.
I0202 19:38:07.651445 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 19:38:16.170141 140070692116288 spec.py:349] Evaluating on the test split.
I0202 19:38:18.847498 140070692116288 submission_runner.py:408] Time since start: 59156.45s, 	Step: 170217, 	{'train/accuracy': 0.8475565910339355, 'train/loss': 0.5495447516441345, 'validation/accuracy': 0.7321199774742126, 'validation/loss': 1.0839142799377441, 'validation/num_examples': 50000, 'test/accuracy': 0.6044000387191772, 'test/loss': 1.8097580671310425, 'test/num_examples': 10000, 'score': 57160.08248925209, 'total_duration': 59156.44516658783, 'accumulated_submission_time': 57160.08248925209, 'accumulated_eval_time': 1985.5698521137238, 'accumulated_logging_time': 4.8556458950042725}
I0202 19:38:18.895442 139907729164032 logging_writer.py:48] [170217] accumulated_eval_time=1985.569852, accumulated_logging_time=4.855646, accumulated_submission_time=57160.082489, global_step=170217, preemption_count=0, score=57160.082489, test/accuracy=0.604400, test/loss=1.809758, test/num_examples=10000, total_duration=59156.445167, train/accuracy=0.847557, train/loss=0.549545, validation/accuracy=0.732120, validation/loss=1.083914, validation/num_examples=50000
I0202 19:38:47.037218 139907737556736 logging_writer.py:48] [170300] global_step=170300, grad_norm=4.905506134033203, loss=1.0657789707183838
I0202 19:39:20.535098 139907729164032 logging_writer.py:48] [170400] global_step=170400, grad_norm=4.7059431076049805, loss=0.9685149192810059
I0202 19:39:54.042762 139907737556736 logging_writer.py:48] [170500] global_step=170500, grad_norm=4.742853164672852, loss=1.0227947235107422
I0202 19:40:27.591454 139907729164032 logging_writer.py:48] [170600] global_step=170600, grad_norm=4.809446334838867, loss=1.1220216751098633
I0202 19:41:01.177752 139907737556736 logging_writer.py:48] [170700] global_step=170700, grad_norm=4.727616310119629, loss=1.05899977684021
I0202 19:41:34.690044 139907729164032 logging_writer.py:48] [170800] global_step=170800, grad_norm=5.085618019104004, loss=1.0820019245147705
I0202 19:42:08.231987 139907737556736 logging_writer.py:48] [170900] global_step=170900, grad_norm=4.870208263397217, loss=1.02230966091156
I0202 19:42:41.758605 139907729164032 logging_writer.py:48] [171000] global_step=171000, grad_norm=4.696248531341553, loss=1.0023648738861084
I0202 19:43:15.375490 139907737556736 logging_writer.py:48] [171100] global_step=171100, grad_norm=5.035204887390137, loss=1.064440131187439
I0202 19:43:48.879974 139907729164032 logging_writer.py:48] [171200] global_step=171200, grad_norm=4.932488441467285, loss=1.039618730545044
I0202 19:44:22.380230 139907737556736 logging_writer.py:48] [171300] global_step=171300, grad_norm=4.705965518951416, loss=1.0627408027648926
I0202 19:44:55.880614 139907729164032 logging_writer.py:48] [171400] global_step=171400, grad_norm=4.690896034240723, loss=1.0407716035842896
I0202 19:45:29.415137 139907737556736 logging_writer.py:48] [171500] global_step=171500, grad_norm=4.838096618652344, loss=1.1365315914154053
I0202 19:46:02.944944 139907729164032 logging_writer.py:48] [171600] global_step=171600, grad_norm=5.028842449188232, loss=1.038834810256958
I0202 19:46:36.503444 139907737556736 logging_writer.py:48] [171700] global_step=171700, grad_norm=5.397403717041016, loss=1.185630440711975
I0202 19:46:49.047692 140070692116288 spec.py:321] Evaluating on the training split.
I0202 19:46:55.316188 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 19:47:03.965541 140070692116288 spec.py:349] Evaluating on the test split.
I0202 19:47:06.534906 140070692116288 submission_runner.py:408] Time since start: 59684.13s, 	Step: 171739, 	{'train/accuracy': 0.8664699792861938, 'train/loss': 0.4829581677913666, 'validation/accuracy': 0.7368800044059753, 'validation/loss': 1.0651110410690308, 'validation/num_examples': 50000, 'test/accuracy': 0.6091000437736511, 'test/loss': 1.7815965414047241, 'test/num_examples': 10000, 'score': 57670.17210030556, 'total_duration': 59684.1325712204, 'accumulated_submission_time': 57670.17210030556, 'accumulated_eval_time': 2003.057029247284, 'accumulated_logging_time': 4.913210391998291}
I0202 19:47:06.584118 139908719027968 logging_writer.py:48] [171739] accumulated_eval_time=2003.057029, accumulated_logging_time=4.913210, accumulated_submission_time=57670.172100, global_step=171739, preemption_count=0, score=57670.172100, test/accuracy=0.609100, test/loss=1.781597, test/num_examples=10000, total_duration=59684.132571, train/accuracy=0.866470, train/loss=0.482958, validation/accuracy=0.736880, validation/loss=1.065111, validation/num_examples=50000
I0202 19:47:27.350084 139908727420672 logging_writer.py:48] [171800] global_step=171800, grad_norm=4.884748935699463, loss=1.0545244216918945
I0202 19:48:00.845057 139908719027968 logging_writer.py:48] [171900] global_step=171900, grad_norm=5.667465686798096, loss=1.0222588777542114
I0202 19:48:34.408770 139908727420672 logging_writer.py:48] [172000] global_step=172000, grad_norm=4.873193740844727, loss=1.1265480518341064
I0202 19:49:07.962804 139908719027968 logging_writer.py:48] [172100] global_step=172100, grad_norm=4.945955753326416, loss=1.0242583751678467
I0202 19:49:41.552937 139908727420672 logging_writer.py:48] [172200] global_step=172200, grad_norm=5.073141098022461, loss=1.0600152015686035
I0202 19:50:15.066967 139908719027968 logging_writer.py:48] [172300] global_step=172300, grad_norm=4.862044334411621, loss=1.0699862241744995
I0202 19:50:48.669492 139908727420672 logging_writer.py:48] [172400] global_step=172400, grad_norm=4.785675525665283, loss=0.9446605443954468
I0202 19:51:22.211408 139908719027968 logging_writer.py:48] [172500] global_step=172500, grad_norm=5.113447666168213, loss=1.0184574127197266
I0202 19:51:55.759926 139908727420672 logging_writer.py:48] [172600] global_step=172600, grad_norm=5.010028839111328, loss=1.0181454420089722
I0202 19:52:29.311621 139908719027968 logging_writer.py:48] [172700] global_step=172700, grad_norm=4.760457992553711, loss=0.9921819567680359
I0202 19:53:02.855252 139908727420672 logging_writer.py:48] [172800] global_step=172800, grad_norm=5.1532301902771, loss=0.9891846179962158
I0202 19:53:36.384056 139908719027968 logging_writer.py:48] [172900] global_step=172900, grad_norm=4.9043192863464355, loss=0.998796820640564
I0202 19:54:09.938276 139908727420672 logging_writer.py:48] [173000] global_step=173000, grad_norm=5.385680675506592, loss=1.0301979780197144
I0202 19:54:43.447753 139908719027968 logging_writer.py:48] [173100] global_step=173100, grad_norm=4.668436050415039, loss=1.0211026668548584
I0202 19:55:17.034611 139908727420672 logging_writer.py:48] [173200] global_step=173200, grad_norm=5.331125736236572, loss=1.0654792785644531
I0202 19:55:36.939992 140070692116288 spec.py:321] Evaluating on the training split.
I0202 19:55:43.199285 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 19:55:51.790337 140070692116288 spec.py:349] Evaluating on the test split.
I0202 19:55:54.469922 140070692116288 submission_runner.py:408] Time since start: 60212.07s, 	Step: 173260, 	{'train/accuracy': 0.8635004758834839, 'train/loss': 0.49038711190223694, 'validation/accuracy': 0.7384999990463257, 'validation/loss': 1.065165638923645, 'validation/num_examples': 50000, 'test/accuracy': 0.6144000291824341, 'test/loss': 1.779536247253418, 'test/num_examples': 10000, 'score': 58180.46226763725, 'total_duration': 60212.06752181053, 'accumulated_submission_time': 58180.46226763725, 'accumulated_eval_time': 2020.5868430137634, 'accumulated_logging_time': 4.9735023975372314}
I0202 19:55:54.523933 139907745949440 logging_writer.py:48] [173260] accumulated_eval_time=2020.586843, accumulated_logging_time=4.973502, accumulated_submission_time=58180.462268, global_step=173260, preemption_count=0, score=58180.462268, test/accuracy=0.614400, test/loss=1.779536, test/num_examples=10000, total_duration=60212.067522, train/accuracy=0.863500, train/loss=0.490387, validation/accuracy=0.738500, validation/loss=1.065166, validation/num_examples=50000
I0202 19:56:08.288491 139907754342144 logging_writer.py:48] [173300] global_step=173300, grad_norm=5.206031799316406, loss=0.9910828471183777
I0202 19:56:41.784765 139907745949440 logging_writer.py:48] [173400] global_step=173400, grad_norm=5.381642818450928, loss=0.9800417423248291
I0202 19:57:15.345449 139907754342144 logging_writer.py:48] [173500] global_step=173500, grad_norm=4.856500148773193, loss=0.9575061202049255
I0202 19:57:48.930157 139907745949440 logging_writer.py:48] [173600] global_step=173600, grad_norm=4.920158386230469, loss=0.963316798210144
I0202 19:58:22.459674 139907754342144 logging_writer.py:48] [173700] global_step=173700, grad_norm=5.093960762023926, loss=1.0289088487625122
I0202 19:58:56.049811 139907745949440 logging_writer.py:48] [173800] global_step=173800, grad_norm=5.060532569885254, loss=1.0285930633544922
I0202 19:59:29.562540 139907754342144 logging_writer.py:48] [173900] global_step=173900, grad_norm=5.161281585693359, loss=1.0027921199798584
I0202 20:00:03.136438 139907745949440 logging_writer.py:48] [174000] global_step=174000, grad_norm=4.923023223876953, loss=0.97878497838974
I0202 20:00:36.646297 139907754342144 logging_writer.py:48] [174100] global_step=174100, grad_norm=5.340882301330566, loss=1.0274213552474976
I0202 20:01:10.230908 139907745949440 logging_writer.py:48] [174200] global_step=174200, grad_norm=5.108450889587402, loss=1.0084002017974854
I0202 20:01:43.732617 139907754342144 logging_writer.py:48] [174300] global_step=174300, grad_norm=5.103017807006836, loss=1.0753400325775146
I0202 20:02:17.549755 139907745949440 logging_writer.py:48] [174400] global_step=174400, grad_norm=5.156594276428223, loss=1.008279800415039
I0202 20:02:51.025945 139907754342144 logging_writer.py:48] [174500] global_step=174500, grad_norm=5.0268874168396, loss=0.9398065805435181
I0202 20:03:24.536183 139907745949440 logging_writer.py:48] [174600] global_step=174600, grad_norm=5.31661319732666, loss=0.9961366653442383
I0202 20:03:58.060100 139907754342144 logging_writer.py:48] [174700] global_step=174700, grad_norm=4.776165008544922, loss=0.9191745519638062
I0202 20:04:24.736832 140070692116288 spec.py:321] Evaluating on the training split.
I0202 20:04:31.195703 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 20:04:39.567201 140070692116288 spec.py:349] Evaluating on the test split.
I0202 20:04:42.203632 140070692116288 submission_runner.py:408] Time since start: 60739.80s, 	Step: 174781, 	{'train/accuracy': 0.8696388602256775, 'train/loss': 0.4675772190093994, 'validation/accuracy': 0.7422800064086914, 'validation/loss': 1.044396162033081, 'validation/num_examples': 50000, 'test/accuracy': 0.6187000274658203, 'test/loss': 1.7618048191070557, 'test/num_examples': 10000, 'score': 58690.6109726429, 'total_duration': 60739.801298856735, 'accumulated_submission_time': 58690.6109726429, 'accumulated_eval_time': 2038.053610086441, 'accumulated_logging_time': 5.037261247634888}
I0202 20:04:42.255390 139907737556736 logging_writer.py:48] [174781] accumulated_eval_time=2038.053610, accumulated_logging_time=5.037261, accumulated_submission_time=58690.610973, global_step=174781, preemption_count=0, score=58690.610973, test/accuracy=0.618700, test/loss=1.761805, test/num_examples=10000, total_duration=60739.801299, train/accuracy=0.869639, train/loss=0.467577, validation/accuracy=0.742280, validation/loss=1.044396, validation/num_examples=50000
I0202 20:04:48.963132 139908710635264 logging_writer.py:48] [174800] global_step=174800, grad_norm=5.13115119934082, loss=1.0424532890319824
I0202 20:05:22.422405 139907737556736 logging_writer.py:48] [174900] global_step=174900, grad_norm=4.961528778076172, loss=0.9386386871337891
I0202 20:05:55.970563 139908710635264 logging_writer.py:48] [175000] global_step=175000, grad_norm=5.1319756507873535, loss=0.98188316822052
I0202 20:06:29.501420 139907737556736 logging_writer.py:48] [175100] global_step=175100, grad_norm=4.741094589233398, loss=1.001100778579712
I0202 20:07:03.032763 139908710635264 logging_writer.py:48] [175200] global_step=175200, grad_norm=5.126221179962158, loss=1.0003530979156494
I0202 20:07:36.610437 139907737556736 logging_writer.py:48] [175300] global_step=175300, grad_norm=4.873655319213867, loss=1.023456335067749
I0202 20:08:10.156346 139908710635264 logging_writer.py:48] [175400] global_step=175400, grad_norm=5.06838846206665, loss=1.0299887657165527
I0202 20:08:43.798170 139907737556736 logging_writer.py:48] [175500] global_step=175500, grad_norm=5.0102458000183105, loss=0.9840751886367798
I0202 20:09:17.314435 139908710635264 logging_writer.py:48] [175600] global_step=175600, grad_norm=4.859823226928711, loss=0.9272580742835999
I0202 20:09:50.899209 139907737556736 logging_writer.py:48] [175700] global_step=175700, grad_norm=4.956179618835449, loss=0.9480195641517639
I0202 20:10:24.426797 139908710635264 logging_writer.py:48] [175800] global_step=175800, grad_norm=5.227290630340576, loss=1.0514135360717773
I0202 20:10:58.011429 139907737556736 logging_writer.py:48] [175900] global_step=175900, grad_norm=5.264992713928223, loss=0.9511851072311401
I0202 20:11:31.562808 139908710635264 logging_writer.py:48] [176000] global_step=176000, grad_norm=4.629391670227051, loss=0.9415106773376465
I0202 20:12:05.101046 139907737556736 logging_writer.py:48] [176100] global_step=176100, grad_norm=4.929927825927734, loss=1.0371938943862915
I0202 20:12:38.635227 139908710635264 logging_writer.py:48] [176200] global_step=176200, grad_norm=5.10766077041626, loss=0.9963513612747192
I0202 20:13:12.206206 139907737556736 logging_writer.py:48] [176300] global_step=176300, grad_norm=4.921626091003418, loss=1.0105589628219604
I0202 20:13:12.216247 140070692116288 spec.py:321] Evaluating on the training split.
I0202 20:13:18.597975 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 20:13:27.079691 140070692116288 spec.py:349] Evaluating on the test split.
I0202 20:13:29.712722 140070692116288 submission_runner.py:408] Time since start: 61267.31s, 	Step: 176301, 	{'train/accuracy': 0.8717913031578064, 'train/loss': 0.45689013600349426, 'validation/accuracy': 0.7438399791717529, 'validation/loss': 1.0421042442321777, 'validation/num_examples': 50000, 'test/accuracy': 0.6177000403404236, 'test/loss': 1.7554930448532104, 'test/num_examples': 10000, 'score': 59200.50735998154, 'total_duration': 61267.31038618088, 'accumulated_submission_time': 59200.50735998154, 'accumulated_eval_time': 2055.5500314235687, 'accumulated_logging_time': 5.10002589225769}
I0202 20:13:29.759973 139907754342144 logging_writer.py:48] [176301] accumulated_eval_time=2055.550031, accumulated_logging_time=5.100026, accumulated_submission_time=59200.507360, global_step=176301, preemption_count=0, score=59200.507360, test/accuracy=0.617700, test/loss=1.755493, test/num_examples=10000, total_duration=61267.310386, train/accuracy=0.871791, train/loss=0.456890, validation/accuracy=0.743840, validation/loss=1.042104, validation/num_examples=50000
I0202 20:14:03.267624 139907762734848 logging_writer.py:48] [176400] global_step=176400, grad_norm=4.999971389770508, loss=0.959660530090332
I0202 20:14:36.772993 139907754342144 logging_writer.py:48] [176500] global_step=176500, grad_norm=4.802108287811279, loss=0.9771630764007568
I0202 20:15:10.361386 139907762734848 logging_writer.py:48] [176600] global_step=176600, grad_norm=4.969478130340576, loss=0.9125464558601379
I0202 20:15:43.927829 139907754342144 logging_writer.py:48] [176700] global_step=176700, grad_norm=5.2537078857421875, loss=0.9804435968399048
I0202 20:16:17.451015 139907762734848 logging_writer.py:48] [176800] global_step=176800, grad_norm=5.098354339599609, loss=0.8799948692321777
I0202 20:16:51.029786 139907754342144 logging_writer.py:48] [176900] global_step=176900, grad_norm=5.245432376861572, loss=1.0408341884613037
I0202 20:17:24.551520 139907762734848 logging_writer.py:48] [177000] global_step=177000, grad_norm=5.261659145355225, loss=1.0228619575500488
I0202 20:17:58.121347 139907754342144 logging_writer.py:48] [177100] global_step=177100, grad_norm=5.208425521850586, loss=0.9606547355651855
I0202 20:18:31.633695 139907762734848 logging_writer.py:48] [177200] global_step=177200, grad_norm=5.438802719116211, loss=1.0390645265579224
I0202 20:19:05.240707 139907754342144 logging_writer.py:48] [177300] global_step=177300, grad_norm=5.624382019042969, loss=0.9977849125862122
I0202 20:19:38.757187 139907762734848 logging_writer.py:48] [177400] global_step=177400, grad_norm=5.239889144897461, loss=1.013433814048767
I0202 20:20:12.315214 139907754342144 logging_writer.py:48] [177500] global_step=177500, grad_norm=4.876821994781494, loss=0.8577122092247009
I0202 20:20:45.818260 139907762734848 logging_writer.py:48] [177600] global_step=177600, grad_norm=5.062586784362793, loss=1.0234975814819336
I0202 20:21:19.434934 139907754342144 logging_writer.py:48] [177700] global_step=177700, grad_norm=5.5409040451049805, loss=1.0941929817199707
I0202 20:21:52.985242 139907762734848 logging_writer.py:48] [177800] global_step=177800, grad_norm=5.157196998596191, loss=0.8987942934036255
I0202 20:21:59.837786 140070692116288 spec.py:321] Evaluating on the training split.
I0202 20:22:06.331611 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 20:22:14.682102 140070692116288 spec.py:349] Evaluating on the test split.
I0202 20:22:17.306665 140070692116288 submission_runner.py:408] Time since start: 61794.90s, 	Step: 177822, 	{'train/accuracy': 0.8772520422935486, 'train/loss': 0.43504729866981506, 'validation/accuracy': 0.7451599836349487, 'validation/loss': 1.0354770421981812, 'validation/num_examples': 50000, 'test/accuracy': 0.6203000545501709, 'test/loss': 1.750431776046753, 'test/num_examples': 10000, 'score': 59710.523655653, 'total_duration': 61794.90425157547, 'accumulated_submission_time': 59710.523655653, 'accumulated_eval_time': 2073.0187923908234, 'accumulated_logging_time': 5.156181573867798}
I0202 20:22:17.354670 139907737556736 logging_writer.py:48] [177822] accumulated_eval_time=2073.018792, accumulated_logging_time=5.156182, accumulated_submission_time=59710.523656, global_step=177822, preemption_count=0, score=59710.523656, test/accuracy=0.620300, test/loss=1.750432, test/num_examples=10000, total_duration=61794.904252, train/accuracy=0.877252, train/loss=0.435047, validation/accuracy=0.745160, validation/loss=1.035477, validation/num_examples=50000
I0202 20:22:43.845133 139908710635264 logging_writer.py:48] [177900] global_step=177900, grad_norm=5.102781295776367, loss=0.9592283964157104
I0202 20:23:17.367722 139907737556736 logging_writer.py:48] [178000] global_step=178000, grad_norm=5.276113986968994, loss=0.9078042507171631
I0202 20:23:50.944433 139908710635264 logging_writer.py:48] [178100] global_step=178100, grad_norm=4.991815090179443, loss=0.8990126252174377
I0202 20:24:24.467935 139907737556736 logging_writer.py:48] [178200] global_step=178200, grad_norm=5.214270114898682, loss=0.9104810357093811
I0202 20:24:58.006826 139908710635264 logging_writer.py:48] [178300] global_step=178300, grad_norm=5.238966941833496, loss=0.9635007381439209
I0202 20:25:31.611037 139907737556736 logging_writer.py:48] [178400] global_step=178400, grad_norm=5.119621276855469, loss=0.9776604175567627
I0202 20:26:05.174018 139908710635264 logging_writer.py:48] [178500] global_step=178500, grad_norm=5.390305519104004, loss=0.9940643310546875
I0202 20:26:38.716883 139907737556736 logging_writer.py:48] [178600] global_step=178600, grad_norm=4.9319071769714355, loss=0.9764040112495422
I0202 20:27:12.292604 139908710635264 logging_writer.py:48] [178700] global_step=178700, grad_norm=5.52490234375, loss=0.9336147308349609
I0202 20:27:45.805012 139907737556736 logging_writer.py:48] [178800] global_step=178800, grad_norm=5.875884056091309, loss=1.0545704364776611
I0202 20:28:19.404712 139908710635264 logging_writer.py:48] [178900] global_step=178900, grad_norm=5.486928462982178, loss=1.0530356168746948
I0202 20:28:52.988692 139907737556736 logging_writer.py:48] [179000] global_step=179000, grad_norm=5.518047332763672, loss=0.9740926623344421
I0202 20:29:26.588349 139908710635264 logging_writer.py:48] [179100] global_step=179100, grad_norm=5.047877788543701, loss=0.8405904173851013
I0202 20:30:00.100263 139907737556736 logging_writer.py:48] [179200] global_step=179200, grad_norm=5.305093765258789, loss=0.9493790864944458
I0202 20:30:33.635265 139908710635264 logging_writer.py:48] [179300] global_step=179300, grad_norm=5.214817047119141, loss=0.9312621355056763
I0202 20:30:47.577081 140070692116288 spec.py:321] Evaluating on the training split.
I0202 20:30:53.902584 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 20:31:02.818893 140070692116288 spec.py:349] Evaluating on the test split.
I0202 20:31:05.482024 140070692116288 submission_runner.py:408] Time since start: 62323.08s, 	Step: 179343, 	{'train/accuracy': 0.8833904266357422, 'train/loss': 0.4190746545791626, 'validation/accuracy': 0.7470600008964539, 'validation/loss': 1.032281517982483, 'validation/num_examples': 50000, 'test/accuracy': 0.6243000030517578, 'test/loss': 1.7491586208343506, 'test/num_examples': 10000, 'score': 60220.68492269516, 'total_duration': 62323.07969260216, 'accumulated_submission_time': 60220.68492269516, 'accumulated_eval_time': 2090.9237022399902, 'accumulated_logging_time': 5.213481664657593}
I0202 20:31:05.530485 139907729164032 logging_writer.py:48] [179343] accumulated_eval_time=2090.923702, accumulated_logging_time=5.213482, accumulated_submission_time=60220.684923, global_step=179343, preemption_count=0, score=60220.684923, test/accuracy=0.624300, test/loss=1.749159, test/num_examples=10000, total_duration=62323.079693, train/accuracy=0.883390, train/loss=0.419075, validation/accuracy=0.747060, validation/loss=1.032282, validation/num_examples=50000
I0202 20:31:24.977396 139907737556736 logging_writer.py:48] [179400] global_step=179400, grad_norm=5.395343780517578, loss=0.9910858869552612
I0202 20:31:58.519713 139907729164032 logging_writer.py:48] [179500] global_step=179500, grad_norm=5.386970520019531, loss=1.0218164920806885
I0202 20:32:32.072690 139907737556736 logging_writer.py:48] [179600] global_step=179600, grad_norm=5.211995601654053, loss=1.0148314237594604
I0202 20:33:05.596373 139907729164032 logging_writer.py:48] [179700] global_step=179700, grad_norm=5.32298469543457, loss=0.931672215461731
I0202 20:33:39.186414 139907737556736 logging_writer.py:48] [179800] global_step=179800, grad_norm=6.027259826660156, loss=1.0007045269012451
I0202 20:34:12.739331 139907729164032 logging_writer.py:48] [179900] global_step=179900, grad_norm=5.416535377502441, loss=0.9494336843490601
I0202 20:34:46.553621 139907737556736 logging_writer.py:48] [180000] global_step=180000, grad_norm=5.083005428314209, loss=0.8368077874183655
I0202 20:35:20.068019 139907729164032 logging_writer.py:48] [180100] global_step=180100, grad_norm=5.125054359436035, loss=0.9132382273674011
I0202 20:35:53.641439 139907737556736 logging_writer.py:48] [180200] global_step=180200, grad_norm=5.052483081817627, loss=0.9460755586624146
I0202 20:36:27.159124 139907729164032 logging_writer.py:48] [180300] global_step=180300, grad_norm=5.096307754516602, loss=0.9639509916305542
I0202 20:37:00.730523 139907737556736 logging_writer.py:48] [180400] global_step=180400, grad_norm=5.346519470214844, loss=0.9982523322105408
I0202 20:37:34.262847 139907729164032 logging_writer.py:48] [180500] global_step=180500, grad_norm=5.57110595703125, loss=1.0144270658493042
I0202 20:38:07.810218 139907737556736 logging_writer.py:48] [180600] global_step=180600, grad_norm=5.165855407714844, loss=0.8986086845397949
I0202 20:38:41.362438 139907729164032 logging_writer.py:48] [180700] global_step=180700, grad_norm=5.4404988288879395, loss=0.987317681312561
I0202 20:39:14.888697 139907737556736 logging_writer.py:48] [180800] global_step=180800, grad_norm=5.496650695800781, loss=1.018179178237915
I0202 20:39:35.484173 140070692116288 spec.py:321] Evaluating on the training split.
I0202 20:39:41.916144 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 20:39:50.590368 140070692116288 spec.py:349] Evaluating on the test split.
I0202 20:39:53.232356 140070692116288 submission_runner.py:408] Time since start: 62850.83s, 	Step: 180863, 	{'train/accuracy': 0.8840082883834839, 'train/loss': 0.4111105501651764, 'validation/accuracy': 0.7485599517822266, 'validation/loss': 1.0272870063781738, 'validation/num_examples': 50000, 'test/accuracy': 0.6240000128746033, 'test/loss': 1.746339201927185, 'test/num_examples': 10000, 'score': 60730.57482671738, 'total_duration': 62850.83002829552, 'accumulated_submission_time': 60730.57482671738, 'accumulated_eval_time': 2108.6718752384186, 'accumulated_logging_time': 5.271944284439087}
I0202 20:39:53.284454 139907762734848 logging_writer.py:48] [180863] accumulated_eval_time=2108.671875, accumulated_logging_time=5.271944, accumulated_submission_time=60730.574827, global_step=180863, preemption_count=0, score=60730.574827, test/accuracy=0.624000, test/loss=1.746339, test/num_examples=10000, total_duration=62850.830028, train/accuracy=0.884008, train/loss=0.411111, validation/accuracy=0.748560, validation/loss=1.027287, validation/num_examples=50000
I0202 20:40:06.044014 139908425447168 logging_writer.py:48] [180900] global_step=180900, grad_norm=5.372807502746582, loss=1.0123748779296875
I0202 20:40:39.546613 139907762734848 logging_writer.py:48] [181000] global_step=181000, grad_norm=5.3783135414123535, loss=0.9698081612586975
I0202 20:41:13.174541 139908425447168 logging_writer.py:48] [181100] global_step=181100, grad_norm=5.69490385055542, loss=1.0228042602539062
I0202 20:41:46.709913 139907762734848 logging_writer.py:48] [181200] global_step=181200, grad_norm=5.511067867279053, loss=1.0753097534179688
I0202 20:42:20.254928 139908425447168 logging_writer.py:48] [181300] global_step=181300, grad_norm=5.102334022521973, loss=0.8929576277732849
I0202 20:42:53.827970 139907762734848 logging_writer.py:48] [181400] global_step=181400, grad_norm=5.599961757659912, loss=0.9364625215530396
I0202 20:43:27.356177 139908425447168 logging_writer.py:48] [181500] global_step=181500, grad_norm=5.185731410980225, loss=0.9554239511489868
I0202 20:44:00.896024 139907762734848 logging_writer.py:48] [181600] global_step=181600, grad_norm=5.165568828582764, loss=0.9617674946784973
I0202 20:44:34.516361 139908425447168 logging_writer.py:48] [181700] global_step=181700, grad_norm=5.008971691131592, loss=0.9257984757423401
I0202 20:45:08.090047 139907762734848 logging_writer.py:48] [181800] global_step=181800, grad_norm=5.261452674865723, loss=0.9404515027999878
I0202 20:45:41.620665 139908425447168 logging_writer.py:48] [181900] global_step=181900, grad_norm=5.031304836273193, loss=0.9426462650299072
I0202 20:46:15.154664 139907762734848 logging_writer.py:48] [182000] global_step=182000, grad_norm=5.119620323181152, loss=0.9729698300361633
I0202 20:46:48.747333 139908425447168 logging_writer.py:48] [182100] global_step=182100, grad_norm=4.929774284362793, loss=0.9118279814720154
I0202 20:47:22.413647 139907762734848 logging_writer.py:48] [182200] global_step=182200, grad_norm=4.9219970703125, loss=0.9444383978843689
I0202 20:47:56.014451 139908425447168 logging_writer.py:48] [182300] global_step=182300, grad_norm=5.190988540649414, loss=0.9923642873764038
I0202 20:48:23.328592 140070692116288 spec.py:321] Evaluating on the training split.
I0202 20:48:29.594154 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 20:48:38.205262 140070692116288 spec.py:349] Evaluating on the test split.
I0202 20:48:40.844276 140070692116288 submission_runner.py:408] Time since start: 63378.44s, 	Step: 182383, 	{'train/accuracy': 0.8837690949440002, 'train/loss': 0.4141952693462372, 'validation/accuracy': 0.7486000061035156, 'validation/loss': 1.0234150886535645, 'validation/num_examples': 50000, 'test/accuracy': 0.6265000104904175, 'test/loss': 1.7416620254516602, 'test/num_examples': 10000, 'score': 61240.5575094223, 'total_duration': 63378.4419465065, 'accumulated_submission_time': 61240.5575094223, 'accumulated_eval_time': 2126.1875302791595, 'accumulated_logging_time': 5.333211421966553}
I0202 20:48:40.892483 139907754342144 logging_writer.py:48] [182383] accumulated_eval_time=2126.187530, accumulated_logging_time=5.333211, accumulated_submission_time=61240.557509, global_step=182383, preemption_count=0, score=61240.557509, test/accuracy=0.626500, test/loss=1.741662, test/num_examples=10000, total_duration=63378.441947, train/accuracy=0.883769, train/loss=0.414195, validation/accuracy=0.748600, validation/loss=1.023415, validation/num_examples=50000
I0202 20:48:46.942594 139908727420672 logging_writer.py:48] [182400] global_step=182400, grad_norm=5.435811519622803, loss=0.9659780263900757
I0202 20:49:20.424796 139907754342144 logging_writer.py:48] [182500] global_step=182500, grad_norm=5.25909423828125, loss=0.9475451707839966
I0202 20:49:54.027741 139908727420672 logging_writer.py:48] [182600] global_step=182600, grad_norm=4.719163417816162, loss=0.8752855062484741
I0202 20:50:27.604236 139907754342144 logging_writer.py:48] [182700] global_step=182700, grad_norm=5.678207874298096, loss=1.0393140316009521
I0202 20:51:01.141345 139908727420672 logging_writer.py:48] [182800] global_step=182800, grad_norm=5.096769332885742, loss=0.8375418782234192
I0202 20:51:34.652828 139907754342144 logging_writer.py:48] [182900] global_step=182900, grad_norm=5.378961086273193, loss=0.937678337097168
I0202 20:52:08.155632 139908727420672 logging_writer.py:48] [183000] global_step=183000, grad_norm=5.611936092376709, loss=0.9736834764480591
I0202 20:52:41.698664 139907754342144 logging_writer.py:48] [183100] global_step=183100, grad_norm=5.615900993347168, loss=0.9998291730880737
I0202 20:53:15.211403 139908727420672 logging_writer.py:48] [183200] global_step=183200, grad_norm=5.459934711456299, loss=0.9546598196029663
I0202 20:53:48.856803 139907754342144 logging_writer.py:48] [183300] global_step=183300, grad_norm=5.0283403396606445, loss=0.9560725092887878
I0202 20:54:22.382084 139908727420672 logging_writer.py:48] [183400] global_step=183400, grad_norm=5.38525915145874, loss=0.8762733936309814
I0202 20:54:55.910470 139907754342144 logging_writer.py:48] [183500] global_step=183500, grad_norm=5.070469856262207, loss=0.9141892790794373
I0202 20:55:29.466295 139908727420672 logging_writer.py:48] [183600] global_step=183600, grad_norm=5.10396146774292, loss=0.9040533900260925
I0202 20:56:03.036891 139907754342144 logging_writer.py:48] [183700] global_step=183700, grad_norm=5.001321792602539, loss=0.8707102537155151
I0202 20:56:36.564887 139908727420672 logging_writer.py:48] [183800] global_step=183800, grad_norm=5.084707260131836, loss=0.9902390837669373
I0202 20:57:10.124084 139907754342144 logging_writer.py:48] [183900] global_step=183900, grad_norm=5.124872207641602, loss=0.9027246236801147
I0202 20:57:10.940050 140070692116288 spec.py:321] Evaluating on the training split.
I0202 20:57:17.181613 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 20:57:25.688998 140070692116288 spec.py:349] Evaluating on the test split.
I0202 20:57:28.335905 140070692116288 submission_runner.py:408] Time since start: 63905.93s, 	Step: 183904, 	{'train/accuracy': 0.8855827450752258, 'train/loss': 0.40654659271240234, 'validation/accuracy': 0.7492199540138245, 'validation/loss': 1.0245238542556763, 'validation/num_examples': 50000, 'test/accuracy': 0.6260000467300415, 'test/loss': 1.7438762187957764, 'test/num_examples': 10000, 'score': 61750.542989730835, 'total_duration': 63905.93356466293, 'accumulated_submission_time': 61750.542989730835, 'accumulated_eval_time': 2143.5833439826965, 'accumulated_logging_time': 5.390666723251343}
I0202 20:57:28.384777 139907729164032 logging_writer.py:48] [183904] accumulated_eval_time=2143.583344, accumulated_logging_time=5.390667, accumulated_submission_time=61750.542990, global_step=183904, preemption_count=0, score=61750.542990, test/accuracy=0.626000, test/loss=1.743876, test/num_examples=10000, total_duration=63905.933565, train/accuracy=0.885583, train/loss=0.406547, validation/accuracy=0.749220, validation/loss=1.024524, validation/num_examples=50000
I0202 20:58:00.902994 139907737556736 logging_writer.py:48] [184000] global_step=184000, grad_norm=5.045527935028076, loss=0.9532101154327393
I0202 20:58:34.421709 139907729164032 logging_writer.py:48] [184100] global_step=184100, grad_norm=5.524110317230225, loss=0.959176242351532
I0202 20:59:08.011119 139907737556736 logging_writer.py:48] [184200] global_step=184200, grad_norm=5.5957865715026855, loss=0.9775341153144836
I0202 20:59:41.612284 139907729164032 logging_writer.py:48] [184300] global_step=184300, grad_norm=5.179022312164307, loss=0.9379680156707764
I0202 21:00:15.247890 139907737556736 logging_writer.py:48] [184400] global_step=184400, grad_norm=5.033718109130859, loss=0.8471854329109192
I0202 21:00:48.835262 139907729164032 logging_writer.py:48] [184500] global_step=184500, grad_norm=5.329863548278809, loss=0.9248209595680237
I0202 21:01:22.332047 139907737556736 logging_writer.py:48] [184600] global_step=184600, grad_norm=5.471037864685059, loss=0.9553294777870178
I0202 21:01:55.884415 139907729164032 logging_writer.py:48] [184700] global_step=184700, grad_norm=5.07265567779541, loss=0.894071102142334
I0202 21:02:29.416820 139907737556736 logging_writer.py:48] [184800] global_step=184800, grad_norm=5.3707756996154785, loss=0.9126242995262146
I0202 21:03:02.960689 139907729164032 logging_writer.py:48] [184900] global_step=184900, grad_norm=5.197538375854492, loss=0.9873948097229004
I0202 21:03:36.544649 139907737556736 logging_writer.py:48] [185000] global_step=185000, grad_norm=4.939249515533447, loss=0.8858104348182678
I0202 21:04:10.095743 139907729164032 logging_writer.py:48] [185100] global_step=185100, grad_norm=5.316189289093018, loss=0.9053847193717957
I0202 21:04:43.644363 139907737556736 logging_writer.py:48] [185200] global_step=185200, grad_norm=5.099916458129883, loss=0.9138287305831909
I0202 21:05:17.185108 139907729164032 logging_writer.py:48] [185300] global_step=185300, grad_norm=4.925909519195557, loss=0.8524076342582703
I0202 21:05:50.788873 139907737556736 logging_writer.py:48] [185400] global_step=185400, grad_norm=5.245230197906494, loss=0.9259307384490967
I0202 21:05:58.342864 140070692116288 spec.py:321] Evaluating on the training split.
I0202 21:06:04.750088 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 21:06:13.328673 140070692116288 spec.py:349] Evaluating on the test split.
I0202 21:06:15.935232 140070692116288 submission_runner.py:408] Time since start: 64433.53s, 	Step: 185424, 	{'train/accuracy': 0.8825932741165161, 'train/loss': 0.41047802567481995, 'validation/accuracy': 0.7490999698638916, 'validation/loss': 1.0238432884216309, 'validation/num_examples': 50000, 'test/accuracy': 0.6255000233650208, 'test/loss': 1.741317868232727, 'test/num_examples': 10000, 'score': 62260.43835067749, 'total_duration': 64433.532873392105, 'accumulated_submission_time': 62260.43835067749, 'accumulated_eval_time': 2161.1756496429443, 'accumulated_logging_time': 5.449017286300659}
I0202 21:06:15.986838 139907737556736 logging_writer.py:48] [185424] accumulated_eval_time=2161.175650, accumulated_logging_time=5.449017, accumulated_submission_time=62260.438351, global_step=185424, preemption_count=0, score=62260.438351, test/accuracy=0.625500, test/loss=1.741318, test/num_examples=10000, total_duration=64433.532873, train/accuracy=0.882593, train/loss=0.410478, validation/accuracy=0.749100, validation/loss=1.023843, validation/num_examples=50000
I0202 21:06:41.859041 139908710635264 logging_writer.py:48] [185500] global_step=185500, grad_norm=5.280221462249756, loss=0.9416863322257996
I0202 21:07:15.376918 139907737556736 logging_writer.py:48] [185600] global_step=185600, grad_norm=5.6909589767456055, loss=1.0071659088134766
I0202 21:07:48.923921 139908710635264 logging_writer.py:48] [185700] global_step=185700, grad_norm=5.151540279388428, loss=0.9707155227661133
I0202 21:08:22.485612 139907737556736 logging_writer.py:48] [185800] global_step=185800, grad_norm=6.031607151031494, loss=0.9451040625572205
I0202 21:08:56.027283 139908710635264 logging_writer.py:48] [185900] global_step=185900, grad_norm=5.032657146453857, loss=0.9589743614196777
I0202 21:09:29.569903 139907737556736 logging_writer.py:48] [186000] global_step=186000, grad_norm=5.075109958648682, loss=0.9147425889968872
I0202 21:10:03.088679 139908710635264 logging_writer.py:48] [186100] global_step=186100, grad_norm=5.2480621337890625, loss=0.9288209080696106
I0202 21:10:36.636426 139907737556736 logging_writer.py:48] [186200] global_step=186200, grad_norm=5.467380523681641, loss=1.0000450611114502
I0202 21:11:10.154301 139908710635264 logging_writer.py:48] [186300] global_step=186300, grad_norm=5.385488033294678, loss=0.9206417798995972
I0202 21:11:43.703864 139907737556736 logging_writer.py:48] [186400] global_step=186400, grad_norm=5.353973865509033, loss=0.8908166885375977
I0202 21:12:17.224715 139908710635264 logging_writer.py:48] [186500] global_step=186500, grad_norm=4.962371349334717, loss=0.892258882522583
I0202 21:12:50.822568 139907737556736 logging_writer.py:48] [186600] global_step=186600, grad_norm=5.52734375, loss=0.9897063970565796
I0202 21:13:12.412815 140070692116288 spec.py:321] Evaluating on the training split.
I0202 21:13:18.709091 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 21:13:27.227963 140070692116288 spec.py:349] Evaluating on the test split.
I0202 21:13:29.859583 140070692116288 submission_runner.py:408] Time since start: 64867.46s, 	Step: 186666, 	{'train/accuracy': 0.8877750039100647, 'train/loss': 0.4056401550769806, 'validation/accuracy': 0.7488799691200256, 'validation/loss': 1.0230340957641602, 'validation/num_examples': 50000, 'test/accuracy': 0.6260000467300415, 'test/loss': 1.7412824630737305, 'test/num_examples': 10000, 'score': 62676.811291217804, 'total_duration': 64867.45725274086, 'accumulated_submission_time': 62676.811291217804, 'accumulated_eval_time': 2178.622392654419, 'accumulated_logging_time': 5.510376214981079}
I0202 21:13:29.909450 139907754342144 logging_writer.py:48] [186666] accumulated_eval_time=2178.622393, accumulated_logging_time=5.510376, accumulated_submission_time=62676.811291, global_step=186666, preemption_count=0, score=62676.811291, test/accuracy=0.626000, test/loss=1.741282, test/num_examples=10000, total_duration=64867.457253, train/accuracy=0.887775, train/loss=0.405640, validation/accuracy=0.748880, validation/loss=1.023034, validation/num_examples=50000
I0202 21:13:29.954504 139907762734848 logging_writer.py:48] [186666] global_step=186666, preemption_count=0, score=62676.811291
I0202 21:13:30.270085 140070692116288 checkpoints.py:490] Saving checkpoint at step: 186666
I0202 21:13:31.429820 140070692116288 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_3/imagenet_resnet_jax/trial_4/checkpoint_186666
I0202 21:13:31.454962 140070692116288 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_3/imagenet_resnet_jax/trial_4/checkpoint_186666.
I0202 21:13:32.139453 140070692116288 submission_runner.py:583] Tuning trial 4/5
I0202 21:13:32.139658 140070692116288 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.004958460849689891, one_minus_beta1=0.13625575743, beta2=0.6291854735396584, weight_decay=0.1147386261512052, warmup_factor=0.02)
I0202 21:13:32.148293 140070692116288 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0011160713620483875, 'train/loss': 6.912219524383545, 'validation/accuracy': 0.0011199999134987593, 'validation/loss': 6.912059783935547, 'validation/num_examples': 50000, 'test/accuracy': 0.0009000000427477062, 'test/loss': 6.912177562713623, 'test/num_examples': 10000, 'score': 32.56960868835449, 'total_duration': 50.20822787284851, 'accumulated_submission_time': 32.56960868835449, 'accumulated_eval_time': 17.638524532318115, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1509, {'train/accuracy': 0.21974648535251617, 'train/loss': 3.89326810836792, 'validation/accuracy': 0.20191998779773712, 'validation/loss': 4.026599407196045, 'validation/num_examples': 50000, 'test/accuracy': 0.1509000062942505, 'test/loss': 4.556992053985596, 'test/num_examples': 10000, 'score': 542.8288688659668, 'total_duration': 577.9301149845123, 'accumulated_submission_time': 542.8288688659668, 'accumulated_eval_time': 35.02720069885254, 'accumulated_logging_time': 0.02122807502746582, 'global_step': 1509, 'preemption_count': 0}), (3019, {'train/accuracy': 0.3487125337123871, 'train/loss': 3.0133767127990723, 'validation/accuracy': 0.3220599889755249, 'validation/loss': 3.1563539505004883, 'validation/num_examples': 50000, 'test/accuracy': 0.24180001020431519, 'test/loss': 3.88297438621521, 'test/num_examples': 10000, 'score': 1052.8306775093079, 'total_duration': 1105.6641788482666, 'accumulated_submission_time': 1052.8306775093079, 'accumulated_eval_time': 52.679352045059204, 'accumulated_logging_time': 0.0487828254699707, 'global_step': 3019, 'preemption_count': 0}), (4531, {'train/accuracy': 0.3955078125, 'train/loss': 2.769458055496216, 'validation/accuracy': 0.37136000394821167, 'validation/loss': 2.910965919494629, 'validation/num_examples': 50000, 'test/accuracy': 0.27790001034736633, 'test/loss': 3.6371009349823, 'test/num_examples': 10000, 'score': 1563.0581607818604, 'total_duration': 1633.3022310733795, 'accumulated_submission_time': 1563.0581607818604, 'accumulated_eval_time': 70.0097508430481, 'accumulated_logging_time': 0.0756216049194336, 'global_step': 4531, 'preemption_count': 0}), (6044, {'train/accuracy': 0.337890625, 'train/loss': 3.187882423400879, 'validation/accuracy': 0.30987998843193054, 'validation/loss': 3.3979156017303467, 'validation/num_examples': 50000, 'test/accuracy': 0.24710001051425934, 'test/loss': 4.004795551300049, 'test/num_examples': 10000, 'score': 2073.061323404312, 'total_duration': 2160.9724485874176, 'accumulated_submission_time': 2073.061323404312, 'accumulated_eval_time': 87.59454846382141, 'accumulated_logging_time': 0.1046609878540039, 'global_step': 6044, 'preemption_count': 0}), (7559, {'train/accuracy': 0.24878427386283875, 'train/loss': 3.9911692142486572, 'validation/accuracy': 0.22015999257564545, 'validation/loss': 4.308241844177246, 'validation/num_examples': 50000, 'test/accuracy': 0.1762000024318695, 'test/loss': 4.848935604095459, 'test/num_examples': 10000, 'score': 2583.286673307419, 'total_duration': 2689.5087604522705, 'accumulated_submission_time': 2583.286673307419, 'accumulated_eval_time': 105.82629466056824, 'accumulated_logging_time': 0.13178706169128418, 'global_step': 7559, 'preemption_count': 0}), (9072, {'train/accuracy': 0.40692761540412903, 'train/loss': 2.715965986251831, 'validation/accuracy': 0.3807999789714813, 'validation/loss': 2.8986012935638428, 'validation/num_examples': 50000, 'test/accuracy': 0.28280001878738403, 'test/loss': 3.630770444869995, 'test/num_examples': 10000, 'score': 3093.225257396698, 'total_duration': 3217.094662427902, 'accumulated_submission_time': 3093.225257396698, 'accumulated_eval_time': 123.39727687835693, 'accumulated_logging_time': 0.15583324432373047, 'global_step': 9072, 'preemption_count': 0}), (10587, {'train/accuracy': 0.4227718412876129, 'train/loss': 2.5902175903320312, 'validation/accuracy': 0.3854199945926666, 'validation/loss': 2.8245723247528076, 'validation/num_examples': 50000, 'test/accuracy': 0.2955000102519989, 'test/loss': 3.5228676795959473, 'test/num_examples': 10000, 'score': 3603.417446374893, 'total_duration': 3745.611699104309, 'accumulated_submission_time': 3603.417446374893, 'accumulated_eval_time': 141.6427493095398, 'accumulated_logging_time': 0.183197021484375, 'global_step': 10587, 'preemption_count': 0}), (12101, {'train/accuracy': 0.3722098171710968, 'train/loss': 2.9450957775115967, 'validation/accuracy': 0.3542799949645996, 'validation/loss': 3.0856668949127197, 'validation/num_examples': 50000, 'test/accuracy': 0.26660001277923584, 'test/loss': 3.802535057067871, 'test/num_examples': 10000, 'score': 4113.432627916336, 'total_duration': 4273.278554916382, 'accumulated_submission_time': 4113.432627916336, 'accumulated_eval_time': 159.21364331245422, 'accumulated_logging_time': 0.21118879318237305, 'global_step': 12101, 'preemption_count': 0}), (13615, {'train/accuracy': 0.2633330523967743, 'train/loss': 3.873202085494995, 'validation/accuracy': 0.24587999284267426, 'validation/loss': 3.999222993850708, 'validation/num_examples': 50000, 'test/accuracy': 0.18560001254081726, 'test/loss': 4.675883769989014, 'test/num_examples': 10000, 'score': 4623.356384038925, 'total_duration': 4800.736140012741, 'accumulated_submission_time': 4623.356384038925, 'accumulated_eval_time': 176.6627459526062, 'accumulated_logging_time': 0.24370288848876953, 'global_step': 13615, 'preemption_count': 0}), (15131, {'train/accuracy': 0.43538743257522583, 'train/loss': 2.5323400497436523, 'validation/accuracy': 0.39135998487472534, 'validation/loss': 2.8394031524658203, 'validation/num_examples': 50000, 'test/accuracy': 0.30390000343322754, 'test/loss': 3.497375249862671, 'test/num_examples': 10000, 'score': 5133.596252441406, 'total_duration': 5328.493609189987, 'accumulated_submission_time': 5133.596252441406, 'accumulated_eval_time': 194.09912204742432, 'accumulated_logging_time': 0.27188539505004883, 'global_step': 15131, 'preemption_count': 0}), (16647, {'train/accuracy': 0.30745774507522583, 'train/loss': 3.5628976821899414, 'validation/accuracy': 0.2874000072479248, 'validation/loss': 3.7463135719299316, 'validation/num_examples': 50000, 'test/accuracy': 0.22050000727176666, 'test/loss': 4.4452009201049805, 'test/num_examples': 10000, 'score': 5643.8343007564545, 'total_duration': 5856.744036912918, 'accumulated_submission_time': 5643.8343007564545, 'accumulated_eval_time': 212.02861714363098, 'accumulated_logging_time': 0.3007538318634033, 'global_step': 16647, 'preemption_count': 0}), (18163, {'train/accuracy': 0.24236686527729034, 'train/loss': 3.9114837646484375, 'validation/accuracy': 0.22723999619483948, 'validation/loss': 4.067636013031006, 'validation/num_examples': 50000, 'test/accuracy': 0.16140000522136688, 'test/loss': 4.841095924377441, 'test/num_examples': 10000, 'score': 6154.009658336639, 'total_duration': 6384.4521453380585, 'accumulated_submission_time': 6154.009658336639, 'accumulated_eval_time': 229.48013925552368, 'accumulated_logging_time': 0.3294222354888916, 'global_step': 18163, 'preemption_count': 0}), (19679, {'train/accuracy': 0.34693875908851624, 'train/loss': 3.0987894535064697, 'validation/accuracy': 0.3242200016975403, 'validation/loss': 3.262202739715576, 'validation/num_examples': 50000, 'test/accuracy': 0.2403000146150589, 'test/loss': 3.9924895763397217, 'test/num_examples': 10000, 'score': 6664.045799255371, 'total_duration': 6911.997058391571, 'accumulated_submission_time': 6664.045799255371, 'accumulated_eval_time': 246.9081676006317, 'accumulated_logging_time': 0.35784459114074707, 'global_step': 19679, 'preemption_count': 0}), (21195, {'train/accuracy': 0.28358179330825806, 'train/loss': 3.587366819381714, 'validation/accuracy': 0.262800008058548, 'validation/loss': 3.7317724227905273, 'validation/num_examples': 50000, 'test/accuracy': 0.18440000712871552, 'test/loss': 4.544374465942383, 'test/num_examples': 10000, 'score': 7174.211032867432, 'total_duration': 7439.85765004158, 'accumulated_submission_time': 7174.211032867432, 'accumulated_eval_time': 264.5111756324768, 'accumulated_logging_time': 0.39708518981933594, 'global_step': 21195, 'preemption_count': 0}), (22712, {'train/accuracy': 0.20091278851032257, 'train/loss': 4.426466464996338, 'validation/accuracy': 0.18501999974250793, 'validation/loss': 4.574308395385742, 'validation/num_examples': 50000, 'test/accuracy': 0.14190000295639038, 'test/loss': 5.220768451690674, 'test/num_examples': 10000, 'score': 7684.350539445877, 'total_duration': 7967.571515083313, 'accumulated_submission_time': 7684.350539445877, 'accumulated_eval_time': 281.99954295158386, 'accumulated_logging_time': 0.43106698989868164, 'global_step': 22712, 'preemption_count': 0}), (24229, {'train/accuracy': 0.20440050959587097, 'train/loss': 4.545769691467285, 'validation/accuracy': 0.1773199886083603, 'validation/loss': 4.824890613555908, 'validation/num_examples': 50000, 'test/accuracy': 0.13830000162124634, 'test/loss': 5.4493255615234375, 'test/num_examples': 10000, 'score': 8194.534126281738, 'total_duration': 8495.416982650757, 'accumulated_submission_time': 8194.534126281738, 'accumulated_eval_time': 299.5668559074402, 'accumulated_logging_time': 0.4717752933502197, 'global_step': 24229, 'preemption_count': 0}), (25747, {'train/accuracy': 0.27180325984954834, 'train/loss': 3.84205961227417, 'validation/accuracy': 0.2549799978733063, 'validation/loss': 4.00461483001709, 'validation/num_examples': 50000, 'test/accuracy': 0.19200000166893005, 'test/loss': 4.759922981262207, 'test/num_examples': 10000, 'score': 8704.772486686707, 'total_duration': 9023.064319849014, 'accumulated_submission_time': 8704.772486686707, 'accumulated_eval_time': 316.89233231544495, 'accumulated_logging_time': 0.50246262550354, 'global_step': 25747, 'preemption_count': 0}), (27265, {'train/accuracy': 0.32912150025367737, 'train/loss': 3.240143060684204, 'validation/accuracy': 0.30559998750686646, 'validation/loss': 3.3980844020843506, 'validation/num_examples': 50000, 'test/accuracy': 0.22770000994205475, 'test/loss': 4.118068218231201, 'test/num_examples': 10000, 'score': 9214.88447213173, 'total_duration': 9550.579601049423, 'accumulated_submission_time': 9214.88447213173, 'accumulated_eval_time': 334.2111656665802, 'accumulated_logging_time': 0.5347170829772949, 'global_step': 27265, 'preemption_count': 0}), (28783, {'train/accuracy': 0.2516741156578064, 'train/loss': 3.8752613067626953, 'validation/accuracy': 0.23507998883724213, 'validation/loss': 4.017209053039551, 'validation/num_examples': 50000, 'test/accuracy': 0.16670000553131104, 'test/loss': 4.732311725616455, 'test/num_examples': 10000, 'score': 9724.903431653976, 'total_duration': 10078.332313776016, 'accumulated_submission_time': 9724.903431653976, 'accumulated_eval_time': 351.86159229278564, 'accumulated_logging_time': 0.5654406547546387, 'global_step': 28783, 'preemption_count': 0}), (30301, {'train/accuracy': 0.24918286502361298, 'train/loss': 4.077645778656006, 'validation/accuracy': 0.2342199981212616, 'validation/loss': 4.213882923126221, 'validation/num_examples': 50000, 'test/accuracy': 0.171300008893013, 'test/loss': 4.892067909240723, 'test/num_examples': 10000, 'score': 10235.189134597778, 'total_duration': 10606.446083307266, 'accumulated_submission_time': 10235.189134597778, 'accumulated_eval_time': 369.6040177345276, 'accumulated_logging_time': 0.5961604118347168, 'global_step': 30301, 'preemption_count': 0}), (31820, {'train/accuracy': 0.23429527878761292, 'train/loss': 4.242311954498291, 'validation/accuracy': 0.22317999601364136, 'validation/loss': 4.331212520599365, 'validation/num_examples': 50000, 'test/accuracy': 0.16250000894069672, 'test/loss': 5.108082294464111, 'test/num_examples': 10000, 'score': 10745.33808541298, 'total_duration': 11134.302097797394, 'accumulated_submission_time': 10745.33808541298, 'accumulated_eval_time': 387.2266595363617, 'accumulated_logging_time': 0.6278092861175537, 'global_step': 31820, 'preemption_count': 0}), (33338, {'train/accuracy': 0.3989955186843872, 'train/loss': 2.7934722900390625, 'validation/accuracy': 0.35819998383522034, 'validation/loss': 3.0654101371765137, 'validation/num_examples': 50000, 'test/accuracy': 0.263700008392334, 'test/loss': 3.8857994079589844, 'test/num_examples': 10000, 'score': 11255.258068799973, 'total_duration': 11661.563098192215, 'accumulated_submission_time': 11255.258068799973, 'accumulated_eval_time': 404.47988963127136, 'accumulated_logging_time': 0.6608669757843018, 'global_step': 33338, 'preemption_count': 0}), (34857, {'train/accuracy': 0.29085618257522583, 'train/loss': 3.6698713302612305, 'validation/accuracy': 0.271479994058609, 'validation/loss': 3.857929229736328, 'validation/num_examples': 50000, 'test/accuracy': 0.19920000433921814, 'test/loss': 4.656971454620361, 'test/num_examples': 10000, 'score': 11765.183889389038, 'total_duration': 12188.885138511658, 'accumulated_submission_time': 11765.183889389038, 'accumulated_eval_time': 421.78941106796265, 'accumulated_logging_time': 0.6944155693054199, 'global_step': 34857, 'preemption_count': 0}), (36376, {'train/accuracy': 0.3069993555545807, 'train/loss': 3.6135473251342773, 'validation/accuracy': 0.2903600037097931, 'validation/loss': 3.7695767879486084, 'validation/num_examples': 50000, 'test/accuracy': 0.2078000158071518, 'test/loss': 4.676905632019043, 'test/num_examples': 10000, 'score': 12275.307540178299, 'total_duration': 12716.527085065842, 'accumulated_submission_time': 12275.307540178299, 'accumulated_eval_time': 439.2223870754242, 'accumulated_logging_time': 0.727224588394165, 'global_step': 36376, 'preemption_count': 0}), (37895, {'train/accuracy': 0.31865832209587097, 'train/loss': 3.3709821701049805, 'validation/accuracy': 0.29760000109672546, 'validation/loss': 3.534862756729126, 'validation/num_examples': 50000, 'test/accuracy': 0.22430001199245453, 'test/loss': 4.2304182052612305, 'test/num_examples': 10000, 'score': 12785.335270166397, 'total_duration': 13244.364332437515, 'accumulated_submission_time': 12785.335270166397, 'accumulated_eval_time': 456.94721508026123, 'accumulated_logging_time': 0.7594432830810547, 'global_step': 37895, 'preemption_count': 0}), (39413, {'train/accuracy': 0.25938695669174194, 'train/loss': 3.792681932449341, 'validation/accuracy': 0.24493999779224396, 'validation/loss': 3.898503065109253, 'validation/num_examples': 50000, 'test/accuracy': 0.17670001089572906, 'test/loss': 4.5571184158325195, 'test/num_examples': 10000, 'score': 13295.302300453186, 'total_duration': 13771.746730804443, 'accumulated_submission_time': 13295.302300453186, 'accumulated_eval_time': 474.27638053894043, 'accumulated_logging_time': 0.7922139167785645, 'global_step': 39413, 'preemption_count': 0}), (40933, {'train/accuracy': 0.22833624482154846, 'train/loss': 3.997976064682007, 'validation/accuracy': 0.21911999583244324, 'validation/loss': 4.096891403198242, 'validation/num_examples': 50000, 'test/accuracy': 0.16600000858306885, 'test/loss': 4.765476703643799, 'test/num_examples': 10000, 'score': 13805.345292568207, 'total_duration': 14299.461974859238, 'accumulated_submission_time': 13805.345292568207, 'accumulated_eval_time': 491.86146664619446, 'accumulated_logging_time': 0.8255927562713623, 'global_step': 40933, 'preemption_count': 0}), (42453, {'train/accuracy': 0.3529775142669678, 'train/loss': 3.090325355529785, 'validation/accuracy': 0.3198799788951874, 'validation/loss': 3.338887929916382, 'validation/num_examples': 50000, 'test/accuracy': 0.2339000105857849, 'test/loss': 4.0103983879089355, 'test/num_examples': 10000, 'score': 14315.425469398499, 'total_duration': 14827.108598709106, 'accumulated_submission_time': 14315.425469398499, 'accumulated_eval_time': 509.33509039878845, 'accumulated_logging_time': 0.8656878471374512, 'global_step': 42453, 'preemption_count': 0}), (43973, {'train/accuracy': 0.1209542378783226, 'train/loss': 5.900317192077637, 'validation/accuracy': 0.11394000053405762, 'validation/loss': 6.015324115753174, 'validation/num_examples': 50000, 'test/accuracy': 0.08650000393390656, 'test/loss': 6.511098384857178, 'test/num_examples': 10000, 'score': 14825.367683410645, 'total_duration': 15354.713600158691, 'accumulated_submission_time': 14825.367683410645, 'accumulated_eval_time': 526.9106154441833, 'accumulated_logging_time': 0.9007658958435059, 'global_step': 43973, 'preemption_count': 0}), (45493, {'train/accuracy': 0.17273198068141937, 'train/loss': 4.6645426750183105, 'validation/accuracy': 0.1613999903202057, 'validation/loss': 4.75282621383667, 'validation/num_examples': 50000, 'test/accuracy': 0.11970000714063644, 'test/loss': 5.4320783615112305, 'test/num_examples': 10000, 'score': 15335.417038440704, 'total_duration': 15882.430266857147, 'accumulated_submission_time': 15335.417038440704, 'accumulated_eval_time': 544.4883089065552, 'accumulated_logging_time': 0.9369504451751709, 'global_step': 45493, 'preemption_count': 0}), (47014, {'train/accuracy': 0.1693638414144516, 'train/loss': 4.89009428024292, 'validation/accuracy': 0.16245999932289124, 'validation/loss': 4.977695465087891, 'validation/num_examples': 50000, 'test/accuracy': 0.11620000749826431, 'test/loss': 5.645530700683594, 'test/num_examples': 10000, 'score': 15845.601004362106, 'total_duration': 16410.10750222206, 'accumulated_submission_time': 15845.601004362106, 'accumulated_eval_time': 561.8923692703247, 'accumulated_logging_time': 0.9734163284301758, 'global_step': 47014, 'preemption_count': 0}), (48534, {'train/accuracy': 0.18690210580825806, 'train/loss': 4.8144307136535645, 'validation/accuracy': 0.17753998935222626, 'validation/loss': 4.926436901092529, 'validation/num_examples': 50000, 'test/accuracy': 0.1300000101327896, 'test/loss': 5.572903156280518, 'test/num_examples': 10000, 'score': 16355.56469798088, 'total_duration': 16938.182448148727, 'accumulated_submission_time': 16355.56469798088, 'accumulated_eval_time': 579.9127895832062, 'accumulated_logging_time': 1.0119578838348389, 'global_step': 48534, 'preemption_count': 0}), (50054, {'train/accuracy': 0.2864915430545807, 'train/loss': 3.610297918319702, 'validation/accuracy': 0.2510800063610077, 'validation/loss': 3.9188473224639893, 'validation/num_examples': 50000, 'test/accuracy': 0.1940000057220459, 'test/loss': 4.523690700531006, 'test/num_examples': 10000, 'score': 16865.721137285233, 'total_duration': 17465.871644973755, 'accumulated_submission_time': 16865.721137285233, 'accumulated_eval_time': 597.3582053184509, 'accumulated_logging_time': 1.0468201637268066, 'global_step': 50054, 'preemption_count': 0}), (51574, {'train/accuracy': 0.24388153851032257, 'train/loss': 4.0654730796813965, 'validation/accuracy': 0.2254599928855896, 'validation/loss': 4.252574920654297, 'validation/num_examples': 50000, 'test/accuracy': 0.17340001463890076, 'test/loss': 4.7864227294921875, 'test/num_examples': 10000, 'score': 17375.71496105194, 'total_duration': 17993.777713537216, 'accumulated_submission_time': 17375.71496105194, 'accumulated_eval_time': 615.1842617988586, 'accumulated_logging_time': 1.081218957901001, 'global_step': 51574, 'preemption_count': 0}), (53094, {'train/accuracy': 0.24870455265045166, 'train/loss': 4.032920837402344, 'validation/accuracy': 0.22831998765468597, 'validation/loss': 4.205615043640137, 'validation/num_examples': 50000, 'test/accuracy': 0.17580001056194305, 'test/loss': 4.857440948486328, 'test/num_examples': 10000, 'score': 17885.76908183098, 'total_duration': 18521.48242330551, 'accumulated_submission_time': 17885.76908183098, 'accumulated_eval_time': 632.7463145256042, 'accumulated_logging_time': 1.1168007850646973, 'global_step': 53094, 'preemption_count': 0}), (54614, {'train/accuracy': 0.3034518361091614, 'train/loss': 3.569441318511963, 'validation/accuracy': 0.28283998370170593, 'validation/loss': 3.7012898921966553, 'validation/num_examples': 50000, 'test/accuracy': 0.2151000052690506, 'test/loss': 4.410431385040283, 'test/num_examples': 10000, 'score': 18395.96377325058, 'total_duration': 19049.366036891937, 'accumulated_submission_time': 18395.96377325058, 'accumulated_eval_time': 650.3460447788239, 'accumulated_logging_time': 1.1529819965362549, 'global_step': 54614, 'preemption_count': 0}), (56135, {'train/accuracy': 0.2638711631298065, 'train/loss': 3.8232076168060303, 'validation/accuracy': 0.24963998794555664, 'validation/loss': 3.983182907104492, 'validation/num_examples': 50000, 'test/accuracy': 0.18380001187324524, 'test/loss': 4.662926197052002, 'test/num_examples': 10000, 'score': 18906.143671512604, 'total_duration': 19577.08617591858, 'accumulated_submission_time': 18906.143671512604, 'accumulated_eval_time': 667.7958898544312, 'accumulated_logging_time': 1.19061279296875, 'global_step': 56135, 'preemption_count': 0}), (57656, {'train/accuracy': 0.18769928812980652, 'train/loss': 4.809234619140625, 'validation/accuracy': 0.17725999653339386, 'validation/loss': 4.956613540649414, 'validation/num_examples': 50000, 'test/accuracy': 0.13450001180171967, 'test/loss': 5.564505100250244, 'test/num_examples': 10000, 'score': 19416.319585561752, 'total_duration': 20104.69949555397, 'accumulated_submission_time': 19416.319585561752, 'accumulated_eval_time': 685.1396398544312, 'accumulated_logging_time': 1.230280876159668, 'global_step': 57656, 'preemption_count': 0}), (59177, {'train/accuracy': 0.18211893737316132, 'train/loss': 4.6406402587890625, 'validation/accuracy': 0.1666799932718277, 'validation/loss': 4.858196258544922, 'validation/num_examples': 50000, 'test/accuracy': 0.1234000027179718, 'test/loss': 5.476072311401367, 'test/num_examples': 10000, 'score': 19926.450991630554, 'total_duration': 20632.49976873398, 'accumulated_submission_time': 19926.450991630554, 'accumulated_eval_time': 702.7179343700409, 'accumulated_logging_time': 1.2668704986572266, 'global_step': 59177, 'preemption_count': 0}), (60698, {'train/accuracy': 0.34323182702064514, 'train/loss': 3.1367218494415283, 'validation/accuracy': 0.32367998361587524, 'validation/loss': 3.3062968254089355, 'validation/num_examples': 50000, 'test/accuracy': 0.226500004529953, 'test/loss': 4.168323040008545, 'test/num_examples': 10000, 'score': 20436.43869829178, 'total_duration': 21159.79110598564, 'accumulated_submission_time': 20436.43869829178, 'accumulated_eval_time': 719.9272334575653, 'accumulated_logging_time': 1.3086819648742676, 'global_step': 60698, 'preemption_count': 0}), (62219, {'train/accuracy': 0.2721022069454193, 'train/loss': 3.687821388244629, 'validation/accuracy': 0.25543999671936035, 'validation/loss': 3.826228618621826, 'validation/num_examples': 50000, 'test/accuracy': 0.18440000712871552, 'test/loss': 4.584669589996338, 'test/num_examples': 10000, 'score': 20946.65442109108, 'total_duration': 21687.56720471382, 'accumulated_submission_time': 20946.65442109108, 'accumulated_eval_time': 737.3943648338318, 'accumulated_logging_time': 1.3493919372558594, 'global_step': 62219, 'preemption_count': 0}), (63739, {'train/accuracy': 0.26971060037612915, 'train/loss': 3.7855687141418457, 'validation/accuracy': 0.25672000646591187, 'validation/loss': 3.8716468811035156, 'validation/num_examples': 50000, 'test/accuracy': 0.18940000236034393, 'test/loss': 4.6342620849609375, 'test/num_examples': 10000, 'score': 21456.615305900574, 'total_duration': 22215.171835184097, 'accumulated_submission_time': 21456.615305900574, 'accumulated_eval_time': 754.9472448825836, 'accumulated_logging_time': 1.3876104354858398, 'global_step': 63739, 'preemption_count': 0}), (65260, {'train/accuracy': 0.22847576439380646, 'train/loss': 4.340119361877441, 'validation/accuracy': 0.2192399948835373, 'validation/loss': 4.4335126876831055, 'validation/num_examples': 50000, 'test/accuracy': 0.16040000319480896, 'test/loss': 5.25397253036499, 'test/num_examples': 10000, 'score': 21966.762639045715, 'total_duration': 22742.611171722412, 'accumulated_submission_time': 21966.762639045715, 'accumulated_eval_time': 772.145806312561, 'accumulated_logging_time': 1.428035020828247, 'global_step': 65260, 'preemption_count': 0}), (66781, {'train/accuracy': 0.3475964665412903, 'train/loss': 3.1049602031707764, 'validation/accuracy': 0.33611997961997986, 'validation/loss': 3.2092509269714355, 'validation/num_examples': 50000, 'test/accuracy': 0.25780001282691956, 'test/loss': 3.982248544692993, 'test/num_examples': 10000, 'score': 22476.804877758026, 'total_duration': 23270.36111807823, 'accumulated_submission_time': 22476.804877758026, 'accumulated_eval_time': 789.7588932514191, 'accumulated_logging_time': 1.4698281288146973, 'global_step': 66781, 'preemption_count': 0}), (68302, {'train/accuracy': 0.351283460855484, 'train/loss': 3.137704372406006, 'validation/accuracy': 0.32259997725486755, 'validation/loss': 3.393313407897949, 'validation/num_examples': 50000, 'test/accuracy': 0.23590001463890076, 'test/loss': 4.1841864585876465, 'test/num_examples': 10000, 'score': 22986.811591625214, 'total_duration': 23798.096511363983, 'accumulated_submission_time': 22986.811591625214, 'accumulated_eval_time': 807.3938567638397, 'accumulated_logging_time': 1.5102369785308838, 'global_step': 68302, 'preemption_count': 0}), (69823, {'train/accuracy': 0.19616948068141937, 'train/loss': 4.434426784515381, 'validation/accuracy': 0.18423999845981598, 'validation/loss': 4.5618109703063965, 'validation/num_examples': 50000, 'test/accuracy': 0.13050000369548798, 'test/loss': 5.301344871520996, 'test/num_examples': 10000, 'score': 23496.993092536926, 'total_duration': 24325.841685056686, 'accumulated_submission_time': 23496.993092536926, 'accumulated_eval_time': 824.8659207820892, 'accumulated_logging_time': 1.5485587120056152, 'global_step': 69823, 'preemption_count': 0}), (71344, {'train/accuracy': 0.30813536047935486, 'train/loss': 3.636435031890869, 'validation/accuracy': 0.27733999490737915, 'validation/loss': 3.8866045475006104, 'validation/num_examples': 50000, 'test/accuracy': 0.21320000290870667, 'test/loss': 4.583950042724609, 'test/num_examples': 10000, 'score': 24007.092417240143, 'total_duration': 24853.448628664017, 'accumulated_submission_time': 24007.092417240143, 'accumulated_eval_time': 842.2808480262756, 'accumulated_logging_time': 1.5876126289367676, 'global_step': 71344, 'preemption_count': 0}), (72865, {'train/accuracy': 0.27214205265045166, 'train/loss': 3.783998489379883, 'validation/accuracy': 0.25863999128341675, 'validation/loss': 3.9203364849090576, 'validation/num_examples': 50000, 'test/accuracy': 0.1842000037431717, 'test/loss': 4.706700325012207, 'test/num_examples': 10000, 'score': 24517.101637125015, 'total_duration': 25380.833248138428, 'accumulated_submission_time': 24517.101637125015, 'accumulated_eval_time': 859.5648393630981, 'accumulated_logging_time': 1.626474142074585, 'global_step': 72865, 'preemption_count': 0}), (74387, {'train/accuracy': 0.40790417790412903, 'train/loss': 2.6916909217834473, 'validation/accuracy': 0.38402000069618225, 'validation/loss': 2.868121385574341, 'validation/num_examples': 50000, 'test/accuracy': 0.28690001368522644, 'test/loss': 3.6172990798950195, 'test/num_examples': 10000, 'score': 25027.26301074028, 'total_duration': 25909.73645925522, 'accumulated_submission_time': 25027.26301074028, 'accumulated_eval_time': 878.2137405872345, 'accumulated_logging_time': 1.6661872863769531, 'global_step': 74387, 'preemption_count': 0}), (75908, {'train/accuracy': 0.4138033986091614, 'train/loss': 2.737895965576172, 'validation/accuracy': 0.3630799949169159, 'validation/loss': 3.0857176780700684, 'validation/num_examples': 50000, 'test/accuracy': 0.27140000462532043, 'test/loss': 3.899897336959839, 'test/num_examples': 10000, 'score': 25537.45592713356, 'total_duration': 26437.587026834488, 'accumulated_submission_time': 25537.45592713356, 'accumulated_eval_time': 895.784318447113, 'accumulated_logging_time': 1.700392484664917, 'global_step': 75908, 'preemption_count': 0}), (77429, {'train/accuracy': 0.3894292116165161, 'train/loss': 2.835314989089966, 'validation/accuracy': 0.3652399778366089, 'validation/loss': 3.049356460571289, 'validation/num_examples': 50000, 'test/accuracy': 0.2776000201702118, 'test/loss': 3.8139333724975586, 'test/num_examples': 10000, 'score': 26047.585062265396, 'total_duration': 26965.335773706436, 'accumulated_submission_time': 26047.585062265396, 'accumulated_eval_time': 913.3123028278351, 'accumulated_logging_time': 1.7399368286132812, 'global_step': 77429, 'preemption_count': 0}), (78950, {'train/accuracy': 0.3610690236091614, 'train/loss': 3.041414499282837, 'validation/accuracy': 0.3311599791049957, 'validation/loss': 3.252563238143921, 'validation/num_examples': 50000, 'test/accuracy': 0.24700000882148743, 'test/loss': 3.9893369674682617, 'test/num_examples': 10000, 'score': 26557.61221885681, 'total_duration': 27492.91514992714, 'accumulated_submission_time': 26557.61221885681, 'accumulated_eval_time': 930.7702312469482, 'accumulated_logging_time': 1.7816412448883057, 'global_step': 78950, 'preemption_count': 0}), (80471, {'train/accuracy': 0.3260921537876129, 'train/loss': 3.3770241737365723, 'validation/accuracy': 0.3070800006389618, 'validation/loss': 3.5291993618011475, 'validation/num_examples': 50000, 'test/accuracy': 0.22200001776218414, 'test/loss': 4.42165994644165, 'test/num_examples': 10000, 'score': 27067.556453704834, 'total_duration': 28020.69840979576, 'accumulated_submission_time': 27067.556453704834, 'accumulated_eval_time': 948.5069932937622, 'accumulated_logging_time': 1.830293893814087, 'global_step': 80471, 'preemption_count': 0}), (81991, {'train/accuracy': 0.375019907951355, 'train/loss': 2.999455213546753, 'validation/accuracy': 0.35001999139785767, 'validation/loss': 3.140925168991089, 'validation/num_examples': 50000, 'test/accuracy': 0.25780001282691956, 'test/loss': 3.9556169509887695, 'test/num_examples': 10000, 'score': 27577.539540290833, 'total_duration': 28548.623241901398, 'accumulated_submission_time': 27577.539540290833, 'accumulated_eval_time': 966.3523087501526, 'accumulated_logging_time': 1.8731324672698975, 'global_step': 81991, 'preemption_count': 0}), (83512, {'train/accuracy': 0.4471859037876129, 'train/loss': 2.4820733070373535, 'validation/accuracy': 0.4222399890422821, 'validation/loss': 2.646937131881714, 'validation/num_examples': 50000, 'test/accuracy': 0.32020002603530884, 'test/loss': 3.3737969398498535, 'test/num_examples': 10000, 'score': 28087.726016521454, 'total_duration': 29076.31339788437, 'accumulated_submission_time': 28087.726016521454, 'accumulated_eval_time': 983.7680652141571, 'accumulated_logging_time': 1.908886432647705, 'global_step': 83512, 'preemption_count': 0}), (85033, {'train/accuracy': 0.3713129758834839, 'train/loss': 2.982234001159668, 'validation/accuracy': 0.33959999680519104, 'validation/loss': 3.212437391281128, 'validation/num_examples': 50000, 'test/accuracy': 0.2476000189781189, 'test/loss': 4.028741359710693, 'test/num_examples': 10000, 'score': 28597.715670108795, 'total_duration': 29603.636273384094, 'accumulated_submission_time': 28597.715670108795, 'accumulated_eval_time': 1001.0070824623108, 'accumulated_logging_time': 1.9504098892211914, 'global_step': 85033, 'preemption_count': 0}), (86554, {'train/accuracy': 0.3778499662876129, 'train/loss': 3.040504217147827, 'validation/accuracy': 0.3454599976539612, 'validation/loss': 3.2986605167388916, 'validation/num_examples': 50000, 'test/accuracy': 0.26489999890327454, 'test/loss': 4.093916893005371, 'test/num_examples': 10000, 'score': 29107.751658916473, 'total_duration': 30131.544614315033, 'accumulated_submission_time': 29107.751658916473, 'accumulated_eval_time': 1018.7795708179474, 'accumulated_logging_time': 1.9979043006896973, 'global_step': 86554, 'preemption_count': 0}), (88074, {'train/accuracy': 0.37448182702064514, 'train/loss': 2.9719016551971436, 'validation/accuracy': 0.34935998916625977, 'validation/loss': 3.141242742538452, 'validation/num_examples': 50000, 'test/accuracy': 0.2532000243663788, 'test/loss': 3.9751439094543457, 'test/num_examples': 10000, 'score': 29617.712538719177, 'total_duration': 30659.6422123909, 'accumulated_submission_time': 29617.712538719177, 'accumulated_eval_time': 1036.8213591575623, 'accumulated_logging_time': 2.039724111557007, 'global_step': 88074, 'preemption_count': 0}), (89594, {'train/accuracy': 0.4371412396430969, 'train/loss': 2.587308406829834, 'validation/accuracy': 0.40498000383377075, 'validation/loss': 2.8062405586242676, 'validation/num_examples': 50000, 'test/accuracy': 0.31130000948905945, 'test/loss': 3.605199098587036, 'test/num_examples': 10000, 'score': 30127.646330356598, 'total_duration': 31187.31401515007, 'accumulated_submission_time': 30127.646330356598, 'accumulated_eval_time': 1054.453256368637, 'accumulated_logging_time': 2.091838836669922, 'global_step': 89594, 'preemption_count': 0}), (91114, {'train/accuracy': 0.408223032951355, 'train/loss': 2.8384816646575928, 'validation/accuracy': 0.38561999797821045, 'validation/loss': 3.0218026638031006, 'validation/num_examples': 50000, 'test/accuracy': 0.29110002517700195, 'test/loss': 3.8347907066345215, 'test/num_examples': 10000, 'score': 30637.59294629097, 'total_duration': 31714.630335330963, 'accumulated_submission_time': 30637.59294629097, 'accumulated_eval_time': 1071.72580742836, 'accumulated_logging_time': 2.1351735591888428, 'global_step': 91114, 'preemption_count': 0}), (92635, {'train/accuracy': 0.4818638265132904, 'train/loss': 2.327448844909668, 'validation/accuracy': 0.4464399814605713, 'validation/loss': 2.534179210662842, 'validation/num_examples': 50000, 'test/accuracy': 0.3418000042438507, 'test/loss': 3.3646998405456543, 'test/num_examples': 10000, 'score': 31147.667982816696, 'total_duration': 32242.1359269619, 'accumulated_submission_time': 31147.667982816696, 'accumulated_eval_time': 1089.0549597740173, 'accumulated_logging_time': 2.183558702468872, 'global_step': 92635, 'preemption_count': 0}), (94156, {'train/accuracy': 0.5026904940605164, 'train/loss': 2.148667812347412, 'validation/accuracy': 0.4616999924182892, 'validation/loss': 2.3995540142059326, 'validation/num_examples': 50000, 'test/accuracy': 0.3456000089645386, 'test/loss': 3.1860923767089844, 'test/num_examples': 10000, 'score': 31657.682291984558, 'total_duration': 32769.84343409538, 'accumulated_submission_time': 31657.682291984558, 'accumulated_eval_time': 1106.6452696323395, 'accumulated_logging_time': 2.233637571334839, 'global_step': 94156, 'preemption_count': 0}), (95677, {'train/accuracy': 0.35891661047935486, 'train/loss': 3.0547327995300293, 'validation/accuracy': 0.33701997995376587, 'validation/loss': 3.2362570762634277, 'validation/num_examples': 50000, 'test/accuracy': 0.255700021982193, 'test/loss': 3.914307117462158, 'test/num_examples': 10000, 'score': 32167.90555024147, 'total_duration': 33297.85503602028, 'accumulated_submission_time': 32167.90555024147, 'accumulated_eval_time': 1124.3362169265747, 'accumulated_logging_time': 2.278075695037842, 'global_step': 95677, 'preemption_count': 0}), (97198, {'train/accuracy': 0.4383370578289032, 'train/loss': 2.5672011375427246, 'validation/accuracy': 0.40639999508857727, 'validation/loss': 2.7606093883514404, 'validation/num_examples': 50000, 'test/accuracy': 0.31540000438690186, 'test/loss': 3.509662628173828, 'test/num_examples': 10000, 'score': 32677.89052796364, 'total_duration': 33825.715623378754, 'accumulated_submission_time': 32677.89052796364, 'accumulated_eval_time': 1142.114492893219, 'accumulated_logging_time': 2.3221030235290527, 'global_step': 97198, 'preemption_count': 0}), (98720, {'train/accuracy': 0.3068598508834839, 'train/loss': 3.599402904510498, 'validation/accuracy': 0.28567999601364136, 'validation/loss': 3.769570827484131, 'validation/num_examples': 50000, 'test/accuracy': 0.2054000049829483, 'test/loss': 4.586277484893799, 'test/num_examples': 10000, 'score': 33188.105674266815, 'total_duration': 34353.2997841835, 'accumulated_submission_time': 33188.105674266815, 'accumulated_eval_time': 1159.3864908218384, 'accumulated_logging_time': 2.3658483028411865, 'global_step': 98720, 'preemption_count': 0}), (100241, {'train/accuracy': 0.4196627736091614, 'train/loss': 2.642141103744507, 'validation/accuracy': 0.4020799994468689, 'validation/loss': 2.7794859409332275, 'validation/num_examples': 50000, 'test/accuracy': 0.29490000009536743, 'test/loss': 3.5665860176086426, 'test/num_examples': 10000, 'score': 33698.1875834465, 'total_duration': 34881.077904462814, 'accumulated_submission_time': 33698.1875834465, 'accumulated_eval_time': 1176.9801092147827, 'accumulated_logging_time': 2.4154789447784424, 'global_step': 100241, 'preemption_count': 0}), (101763, {'train/accuracy': 0.4217952787876129, 'train/loss': 2.6498255729675293, 'validation/accuracy': 0.3804999887943268, 'validation/loss': 2.9349420070648193, 'validation/num_examples': 50000, 'test/accuracy': 0.2842999994754791, 'test/loss': 3.691239595413208, 'test/num_examples': 10000, 'score': 34208.35723352432, 'total_duration': 35408.832102775574, 'accumulated_submission_time': 34208.35723352432, 'accumulated_eval_time': 1194.4681041240692, 'accumulated_logging_time': 2.459041118621826, 'global_step': 101763, 'preemption_count': 0}), (103284, {'train/accuracy': 0.46273118257522583, 'train/loss': 2.48760986328125, 'validation/accuracy': 0.427979975938797, 'validation/loss': 2.7530295848846436, 'validation/num_examples': 50000, 'test/accuracy': 0.3148000240325928, 'test/loss': 3.6579771041870117, 'test/num_examples': 10000, 'score': 34718.39355421066, 'total_duration': 35936.52565956116, 'accumulated_submission_time': 34718.39355421066, 'accumulated_eval_time': 1212.0220968723297, 'accumulated_logging_time': 2.508146286010742, 'global_step': 103284, 'preemption_count': 0}), (104805, {'train/accuracy': 0.5387436151504517, 'train/loss': 1.9647153615951538, 'validation/accuracy': 0.4964599907398224, 'validation/loss': 2.206085205078125, 'validation/num_examples': 50000, 'test/accuracy': 0.3752000033855438, 'test/loss': 3.0397000312805176, 'test/num_examples': 10000, 'score': 35228.39146900177, 'total_duration': 36464.10525536537, 'accumulated_submission_time': 35228.39146900177, 'accumulated_eval_time': 1229.5005240440369, 'accumulated_logging_time': 2.5585193634033203, 'global_step': 104805, 'preemption_count': 0}), (106326, {'train/accuracy': 0.5285993218421936, 'train/loss': 2.0710136890411377, 'validation/accuracy': 0.4899199903011322, 'validation/loss': 2.3158931732177734, 'validation/num_examples': 50000, 'test/accuracy': 0.3743000030517578, 'test/loss': 3.179978370666504, 'test/num_examples': 10000, 'score': 35738.42715740204, 'total_duration': 36992.00506877899, 'accumulated_submission_time': 35738.42715740204, 'accumulated_eval_time': 1247.2531068325043, 'accumulated_logging_time': 2.616370439529419, 'global_step': 106326, 'preemption_count': 0}), (107847, {'train/accuracy': 0.4673548936843872, 'train/loss': 2.4306631088256836, 'validation/accuracy': 0.4399999976158142, 'validation/loss': 2.6335792541503906, 'validation/num_examples': 50000, 'test/accuracy': 0.3248000144958496, 'test/loss': 3.541719675064087, 'test/num_examples': 10000, 'score': 36248.5503616333, 'total_duration': 37519.58160948753, 'accumulated_submission_time': 36248.5503616333, 'accumulated_eval_time': 1264.6096456050873, 'accumulated_logging_time': 2.6608407497406006, 'global_step': 107847, 'preemption_count': 0}), (109368, {'train/accuracy': 0.5494060516357422, 'train/loss': 1.9401334524154663, 'validation/accuracy': 0.510699987411499, 'validation/loss': 2.1658456325531006, 'validation/num_examples': 50000, 'test/accuracy': 0.388700008392334, 'test/loss': 3.003093719482422, 'test/num_examples': 10000, 'score': 36758.63085794449, 'total_duration': 38047.09022331238, 'accumulated_submission_time': 36758.63085794449, 'accumulated_eval_time': 1281.939534187317, 'accumulated_logging_time': 2.70654034614563, 'global_step': 109368, 'preemption_count': 0}), (110890, {'train/accuracy': 0.6084582209587097, 'train/loss': 1.6108510494232178, 'validation/accuracy': 0.5411800146102905, 'validation/loss': 1.9688493013381958, 'validation/num_examples': 50000, 'test/accuracy': 0.42670002579689026, 'test/loss': 2.7162270545959473, 'test/num_examples': 10000, 'score': 37268.7436478138, 'total_duration': 38574.781507730484, 'accumulated_submission_time': 37268.7436478138, 'accumulated_eval_time': 1299.4154126644135, 'accumulated_logging_time': 2.756378173828125, 'global_step': 110890, 'preemption_count': 0}), (112411, {'train/accuracy': 0.5785833597183228, 'train/loss': 1.7491344213485718, 'validation/accuracy': 0.5329799652099609, 'validation/loss': 2.0025932788848877, 'validation/num_examples': 50000, 'test/accuracy': 0.41670000553131104, 'test/loss': 2.758341073989868, 'test/num_examples': 10000, 'score': 37778.82757425308, 'total_duration': 39102.51637029648, 'accumulated_submission_time': 37778.82757425308, 'accumulated_eval_time': 1316.9662177562714, 'accumulated_logging_time': 2.803908109664917, 'global_step': 112411, 'preemption_count': 0}), (113933, {'train/accuracy': 0.5904615521430969, 'train/loss': 1.7069220542907715, 'validation/accuracy': 0.5450599789619446, 'validation/loss': 1.9861260652542114, 'validation/num_examples': 50000, 'test/accuracy': 0.41130003333091736, 'test/loss': 2.8443455696105957, 'test/num_examples': 10000, 'score': 38288.882075071335, 'total_duration': 39630.119643211365, 'accumulated_submission_time': 38288.882075071335, 'accumulated_eval_time': 1334.4177539348602, 'accumulated_logging_time': 2.848686695098877, 'global_step': 113933, 'preemption_count': 0}), (115454, {'train/accuracy': 0.5925542116165161, 'train/loss': 1.6781911849975586, 'validation/accuracy': 0.5508800148963928, 'validation/loss': 1.918237566947937, 'validation/num_examples': 50000, 'test/accuracy': 0.42600002884864807, 'test/loss': 2.6891703605651855, 'test/num_examples': 10000, 'score': 38799.00455093384, 'total_duration': 40157.88823246956, 'accumulated_submission_time': 38799.00455093384, 'accumulated_eval_time': 1351.9584307670593, 'accumulated_logging_time': 2.899632215499878, 'global_step': 115454, 'preemption_count': 0}), (116975, {'train/accuracy': 0.43536749482154846, 'train/loss': 2.6714303493499756, 'validation/accuracy': 0.41241997480392456, 'validation/loss': 2.853731870651245, 'validation/num_examples': 50000, 'test/accuracy': 0.3038000166416168, 'test/loss': 3.7805306911468506, 'test/num_examples': 10000, 'score': 39309.09029126167, 'total_duration': 40685.7168943882, 'accumulated_submission_time': 39309.09029126167, 'accumulated_eval_time': 1369.5922305583954, 'accumulated_logging_time': 2.9562196731567383, 'global_step': 116975, 'preemption_count': 0}), (118496, {'train/accuracy': 0.5356146097183228, 'train/loss': 2.0111894607543945, 'validation/accuracy': 0.49865999817848206, 'validation/loss': 2.2324469089508057, 'validation/num_examples': 50000, 'test/accuracy': 0.37870001792907715, 'test/loss': 3.0569660663604736, 'test/num_examples': 10000, 'score': 39819.04309177399, 'total_duration': 41213.57382154465, 'accumulated_submission_time': 39819.04309177399, 'accumulated_eval_time': 1387.3949823379517, 'accumulated_logging_time': 3.004948377609253, 'global_step': 118496, 'preemption_count': 0}), (120017, {'train/accuracy': 0.5190728306770325, 'train/loss': 2.124185562133789, 'validation/accuracy': 0.463919997215271, 'validation/loss': 2.485764741897583, 'validation/num_examples': 50000, 'test/accuracy': 0.3614000082015991, 'test/loss': 3.282891273498535, 'test/num_examples': 10000, 'score': 40329.04471373558, 'total_duration': 41741.30652666092, 'accumulated_submission_time': 40329.04471373558, 'accumulated_eval_time': 1405.0201497077942, 'accumulated_logging_time': 3.0573155879974365, 'global_step': 120017, 'preemption_count': 0}), (121539, {'train/accuracy': 0.5934311151504517, 'train/loss': 1.6687780618667603, 'validation/accuracy': 0.545740008354187, 'validation/loss': 1.940511703491211, 'validation/num_examples': 50000, 'test/accuracy': 0.4230000078678131, 'test/loss': 2.734823703765869, 'test/num_examples': 10000, 'score': 40839.1926074028, 'total_duration': 42268.991443395615, 'accumulated_submission_time': 40839.1926074028, 'accumulated_eval_time': 1422.4556045532227, 'accumulated_logging_time': 3.1066973209381104, 'global_step': 121539, 'preemption_count': 0}), (123061, {'train/accuracy': 0.5442841053009033, 'train/loss': 1.9463707208633423, 'validation/accuracy': 0.5045599937438965, 'validation/loss': 2.186814069747925, 'validation/num_examples': 50000, 'test/accuracy': 0.3936000168323517, 'test/loss': 2.9241855144500732, 'test/num_examples': 10000, 'score': 41349.20190811157, 'total_duration': 42796.67501139641, 'accumulated_submission_time': 41349.20190811157, 'accumulated_eval_time': 1440.025390625, 'accumulated_logging_time': 3.1579654216766357, 'global_step': 123061, 'preemption_count': 0}), (124582, {'train/accuracy': 0.6316764950752258, 'train/loss': 1.5131675004959106, 'validation/accuracy': 0.5785199999809265, 'validation/loss': 1.7937418222427368, 'validation/num_examples': 50000, 'test/accuracy': 0.45840001106262207, 'test/loss': 2.5650177001953125, 'test/num_examples': 10000, 'score': 41859.287153720856, 'total_duration': 43324.37479400635, 'accumulated_submission_time': 41859.287153720856, 'accumulated_eval_time': 1457.5347940921783, 'accumulated_logging_time': 3.2097365856170654, 'global_step': 124582, 'preemption_count': 0}), (126103, {'train/accuracy': 0.630301296710968, 'train/loss': 1.5103756189346313, 'validation/accuracy': 0.5861600041389465, 'validation/loss': 1.7711243629455566, 'validation/num_examples': 50000, 'test/accuracy': 0.4564000070095062, 'test/loss': 2.564093589782715, 'test/num_examples': 10000, 'score': 42369.22559094429, 'total_duration': 43852.23295521736, 'accumulated_submission_time': 42369.22559094429, 'accumulated_eval_time': 1475.3515548706055, 'accumulated_logging_time': 3.2582929134368896, 'global_step': 126103, 'preemption_count': 0}), (127624, {'train/accuracy': 0.6471021771430969, 'train/loss': 1.4206527471542358, 'validation/accuracy': 0.5690199732780457, 'validation/loss': 1.836398720741272, 'validation/num_examples': 50000, 'test/accuracy': 0.44850000739097595, 'test/loss': 2.574965000152588, 'test/num_examples': 10000, 'score': 42879.3197491169, 'total_duration': 44379.98077702522, 'accumulated_submission_time': 42879.3197491169, 'accumulated_eval_time': 1492.9049038887024, 'accumulated_logging_time': 3.306204319000244, 'global_step': 127624, 'preemption_count': 0}), (129145, {'train/accuracy': 0.6431162357330322, 'train/loss': 1.4401280879974365, 'validation/accuracy': 0.5803799629211426, 'validation/loss': 1.769958734512329, 'validation/num_examples': 50000, 'test/accuracy': 0.453900009393692, 'test/loss': 2.5460996627807617, 'test/num_examples': 10000, 'score': 43389.23870754242, 'total_duration': 44907.46559214592, 'accumulated_submission_time': 43389.23870754242, 'accumulated_eval_time': 1510.3695363998413, 'accumulated_logging_time': 3.3543436527252197, 'global_step': 129145, 'preemption_count': 0}), (130666, {'train/accuracy': 0.6172273755073547, 'train/loss': 1.5718461275100708, 'validation/accuracy': 0.5622000098228455, 'validation/loss': 1.8866841793060303, 'validation/num_examples': 50000, 'test/accuracy': 0.4480000138282776, 'test/loss': 2.622263193130493, 'test/num_examples': 10000, 'score': 43899.253200531006, 'total_duration': 45434.875410079956, 'accumulated_submission_time': 43899.253200531006, 'accumulated_eval_time': 1527.6565673351288, 'accumulated_logging_time': 3.409334897994995, 'global_step': 130666, 'preemption_count': 0}), (132187, {'train/accuracy': 0.5950454473495483, 'train/loss': 1.6977694034576416, 'validation/accuracy': 0.5471599698066711, 'validation/loss': 1.9765727519989014, 'validation/num_examples': 50000, 'test/accuracy': 0.43570002913475037, 'test/loss': 2.7501347064971924, 'test/num_examples': 10000, 'score': 44409.178663253784, 'total_duration': 45962.245206832886, 'accumulated_submission_time': 44409.178663253784, 'accumulated_eval_time': 1544.9993915557861, 'accumulated_logging_time': 3.458484411239624, 'global_step': 132187, 'preemption_count': 0}), (133708, {'train/accuracy': 0.6533800959587097, 'train/loss': 1.3872883319854736, 'validation/accuracy': 0.6028199791908264, 'validation/loss': 1.666730284690857, 'validation/num_examples': 50000, 'test/accuracy': 0.48500001430511475, 'test/loss': 2.405172348022461, 'test/num_examples': 10000, 'score': 44919.164951086044, 'total_duration': 46490.850856781006, 'accumulated_submission_time': 44919.164951086044, 'accumulated_eval_time': 1563.513568162918, 'accumulated_logging_time': 3.5099377632141113, 'global_step': 133708, 'preemption_count': 0}), (135229, {'train/accuracy': 0.6850087642669678, 'train/loss': 1.247851014137268, 'validation/accuracy': 0.632099986076355, 'validation/loss': 1.5180271863937378, 'validation/num_examples': 50000, 'test/accuracy': 0.5041000247001648, 'test/loss': 2.260892868041992, 'test/num_examples': 10000, 'score': 45429.14738154411, 'total_duration': 47018.49057650566, 'accumulated_submission_time': 45429.14738154411, 'accumulated_eval_time': 1581.0774466991425, 'accumulated_logging_time': 3.5508670806884766, 'global_step': 135229, 'preemption_count': 0}), (136750, {'train/accuracy': 0.6704998016357422, 'train/loss': 1.3005253076553345, 'validation/accuracy': 0.5945799946784973, 'validation/loss': 1.7191460132598877, 'validation/num_examples': 50000, 'test/accuracy': 0.46480002999305725, 'test/loss': 2.5056369304656982, 'test/num_examples': 10000, 'score': 45939.10034203529, 'total_duration': 47546.12515926361, 'accumulated_submission_time': 45939.10034203529, 'accumulated_eval_time': 1598.6504790782928, 'accumulated_logging_time': 3.605985164642334, 'global_step': 136750, 'preemption_count': 0}), (138271, {'train/accuracy': 0.6985809803009033, 'train/loss': 1.16620934009552, 'validation/accuracy': 0.6342599987983704, 'validation/loss': 1.5143696069717407, 'validation/num_examples': 50000, 'test/accuracy': 0.5098000168800354, 'test/loss': 2.2224693298339844, 'test/num_examples': 10000, 'score': 46449.014721632004, 'total_duration': 48073.62446284294, 'accumulated_submission_time': 46449.014721632004, 'accumulated_eval_time': 1616.1354587078094, 'accumulated_logging_time': 3.6534364223480225, 'global_step': 138271, 'preemption_count': 0}), (139791, {'train/accuracy': 0.6724529266357422, 'train/loss': 1.3127702474594116, 'validation/accuracy': 0.6125199794769287, 'validation/loss': 1.6271332502365112, 'validation/num_examples': 50000, 'test/accuracy': 0.4791000187397003, 'test/loss': 2.424319267272949, 'test/num_examples': 10000, 'score': 46959.01626968384, 'total_duration': 48601.15681409836, 'accumulated_submission_time': 46959.01626968384, 'accumulated_eval_time': 1633.5621328353882, 'accumulated_logging_time': 3.705156087875366, 'global_step': 139791, 'preemption_count': 0}), (141312, {'train/accuracy': 0.6929009556770325, 'train/loss': 1.2138513326644897, 'validation/accuracy': 0.6319199800491333, 'validation/loss': 1.5169399976730347, 'validation/num_examples': 50000, 'test/accuracy': 0.5, 'test/loss': 2.2908971309661865, 'test/num_examples': 10000, 'score': 47468.91835808754, 'total_duration': 49128.500671863556, 'accumulated_submission_time': 47468.91835808754, 'accumulated_eval_time': 1650.8986172676086, 'accumulated_logging_time': 3.7565438747406006, 'global_step': 141312, 'preemption_count': 0}), (142833, {'train/accuracy': 0.7071707248687744, 'train/loss': 1.1520593166351318, 'validation/accuracy': 0.6445599794387817, 'validation/loss': 1.4677226543426514, 'validation/num_examples': 50000, 'test/accuracy': 0.5177000164985657, 'test/loss': 2.194716215133667, 'test/num_examples': 10000, 'score': 47978.88781452179, 'total_duration': 49656.22150874138, 'accumulated_submission_time': 47978.88781452179, 'accumulated_eval_time': 1668.5178027153015, 'accumulated_logging_time': 3.8354225158691406, 'global_step': 142833, 'preemption_count': 0}), (144354, {'train/accuracy': 0.7137077450752258, 'train/loss': 1.1160260438919067, 'validation/accuracy': 0.65065997838974, 'validation/loss': 1.4377646446228027, 'validation/num_examples': 50000, 'test/accuracy': 0.5253000259399414, 'test/loss': 2.1807503700256348, 'test/num_examples': 10000, 'score': 48488.88462114334, 'total_duration': 50183.9176530838, 'accumulated_submission_time': 48488.88462114334, 'accumulated_eval_time': 1686.1094892024994, 'accumulated_logging_time': 3.890009880065918, 'global_step': 144354, 'preemption_count': 0}), (145875, {'train/accuracy': 0.7431241869926453, 'train/loss': 0.9743123650550842, 'validation/accuracy': 0.6586199998855591, 'validation/loss': 1.4034175872802734, 'validation/num_examples': 50000, 'test/accuracy': 0.534000039100647, 'test/loss': 2.145510673522949, 'test/num_examples': 10000, 'score': 48998.85201382637, 'total_duration': 50711.597870111465, 'accumulated_submission_time': 48998.85201382637, 'accumulated_eval_time': 1703.7169313430786, 'accumulated_logging_time': 3.9426703453063965, 'global_step': 145875, 'preemption_count': 0}), (147397, {'train/accuracy': 0.7065529227256775, 'train/loss': 1.1428754329681396, 'validation/accuracy': 0.6351799964904785, 'validation/loss': 1.5308077335357666, 'validation/num_examples': 50000, 'test/accuracy': 0.5109000205993652, 'test/loss': 2.299001932144165, 'test/num_examples': 10000, 'score': 49508.99960565567, 'total_duration': 51239.65789651871, 'accumulated_submission_time': 49508.99960565567, 'accumulated_eval_time': 1721.5213029384613, 'accumulated_logging_time': 3.9980673789978027, 'global_step': 147397, 'preemption_count': 0}), (148918, {'train/accuracy': 0.7453164458274841, 'train/loss': 0.9728658199310303, 'validation/accuracy': 0.6709199547767639, 'validation/loss': 1.3506791591644287, 'validation/num_examples': 50000, 'test/accuracy': 0.5452000498771667, 'test/loss': 2.0889601707458496, 'test/num_examples': 10000, 'score': 50019.09476184845, 'total_duration': 51767.180225372314, 'accumulated_submission_time': 50019.09476184845, 'accumulated_eval_time': 1738.840086698532, 'accumulated_logging_time': 4.0529398918151855, 'global_step': 148918, 'preemption_count': 0}), (150440, {'train/accuracy': 0.748465359210968, 'train/loss': 0.9664836525917053, 'validation/accuracy': 0.6740999817848206, 'validation/loss': 1.3404970169067383, 'validation/num_examples': 50000, 'test/accuracy': 0.5436000227928162, 'test/loss': 2.0569565296173096, 'test/num_examples': 10000, 'score': 50529.31740260124, 'total_duration': 52295.101024866104, 'accumulated_submission_time': 50529.31740260124, 'accumulated_eval_time': 1756.4312443733215, 'accumulated_logging_time': 4.107654333114624, 'global_step': 150440, 'preemption_count': 0}), (151961, {'train/accuracy': 0.7463727593421936, 'train/loss': 0.9735182523727417, 'validation/accuracy': 0.6724399924278259, 'validation/loss': 1.3431792259216309, 'validation/num_examples': 50000, 'test/accuracy': 0.5437000393867493, 'test/loss': 2.0814619064331055, 'test/num_examples': 10000, 'score': 51039.453364133835, 'total_duration': 52822.879269361496, 'accumulated_submission_time': 51039.453364133835, 'accumulated_eval_time': 1773.9691338539124, 'accumulated_logging_time': 4.158005237579346, 'global_step': 151961, 'preemption_count': 0}), (153482, {'train/accuracy': 0.7898397445678711, 'train/loss': 0.7781878113746643, 'validation/accuracy': 0.6873999834060669, 'validation/loss': 1.2663946151733398, 'validation/num_examples': 50000, 'test/accuracy': 0.5611000061035156, 'test/loss': 1.9653517007827759, 'test/num_examples': 10000, 'score': 51549.48877668381, 'total_duration': 53350.685337781906, 'accumulated_submission_time': 51549.48877668381, 'accumulated_eval_time': 1791.6252291202545, 'accumulated_logging_time': 4.218658447265625, 'global_step': 153482, 'preemption_count': 0}), (155003, {'train/accuracy': 0.7786391973495483, 'train/loss': 0.8249820470809937, 'validation/accuracy': 0.6894599795341492, 'validation/loss': 1.2767852544784546, 'validation/num_examples': 50000, 'test/accuracy': 0.5552999973297119, 'test/loss': 2.019956350326538, 'test/num_examples': 10000, 'score': 52059.40568423271, 'total_duration': 53877.978026390076, 'accumulated_submission_time': 52059.40568423271, 'accumulated_eval_time': 1808.8925030231476, 'accumulated_logging_time': 4.2747087478637695, 'global_step': 155003, 'preemption_count': 0}), (156525, {'train/accuracy': 0.7767059803009033, 'train/loss': 0.8280686140060425, 'validation/accuracy': 0.6906200051307678, 'validation/loss': 1.253747582435608, 'validation/num_examples': 50000, 'test/accuracy': 0.5636000037193298, 'test/loss': 1.9697444438934326, 'test/num_examples': 10000, 'score': 52569.56496477127, 'total_duration': 54405.946820259094, 'accumulated_submission_time': 52569.56496477127, 'accumulated_eval_time': 1826.5895624160767, 'accumulated_logging_time': 4.334457635879517, 'global_step': 156525, 'preemption_count': 0}), (158046, {'train/accuracy': 0.7870694994926453, 'train/loss': 0.7952739596366882, 'validation/accuracy': 0.6987599730491638, 'validation/loss': 1.248716950416565, 'validation/num_examples': 50000, 'test/accuracy': 0.5710000395774841, 'test/loss': 1.9607090950012207, 'test/num_examples': 10000, 'score': 53079.482422828674, 'total_duration': 54933.418738126755, 'accumulated_submission_time': 53079.482422828674, 'accumulated_eval_time': 1844.0382986068726, 'accumulated_logging_time': 4.386325359344482, 'global_step': 158046, 'preemption_count': 0}), (159567, {'train/accuracy': 0.7997050285339355, 'train/loss': 0.7518975138664246, 'validation/accuracy': 0.7044199705123901, 'validation/loss': 1.2122431993484497, 'validation/num_examples': 50000, 'test/accuracy': 0.5830000042915344, 'test/loss': 1.9153326749801636, 'test/num_examples': 10000, 'score': 53589.56930446625, 'total_duration': 55461.414071798325, 'accumulated_submission_time': 53589.56930446625, 'accumulated_eval_time': 1861.841367483139, 'accumulated_logging_time': 4.439189434051514, 'global_step': 159567, 'preemption_count': 0}), (161088, {'train/accuracy': 0.7997449040412903, 'train/loss': 0.7390770316123962, 'validation/accuracy': 0.7134400010108948, 'validation/loss': 1.1649560928344727, 'validation/num_examples': 50000, 'test/accuracy': 0.5833000540733337, 'test/loss': 1.886929988861084, 'test/num_examples': 10000, 'score': 54099.57530117035, 'total_duration': 55989.10298204422, 'accumulated_submission_time': 54099.57530117035, 'accumulated_eval_time': 1879.416244983673, 'accumulated_logging_time': 4.49467396736145, 'global_step': 161088, 'preemption_count': 0}), (162609, {'train/accuracy': 0.8374122977256775, 'train/loss': 0.5941317677497864, 'validation/accuracy': 0.7181999683380127, 'validation/loss': 1.1413167715072632, 'validation/num_examples': 50000, 'test/accuracy': 0.5940999984741211, 'test/loss': 1.8632287979125977, 'test/num_examples': 10000, 'score': 54609.529450416565, 'total_duration': 56516.9751701355, 'accumulated_submission_time': 54609.529450416565, 'accumulated_eval_time': 1897.226199388504, 'accumulated_logging_time': 4.549734115600586, 'global_step': 162609, 'preemption_count': 0}), (164130, {'train/accuracy': 0.8246572017669678, 'train/loss': 0.6434984803199768, 'validation/accuracy': 0.7135599851608276, 'validation/loss': 1.1545459032058716, 'validation/num_examples': 50000, 'test/accuracy': 0.5824000239372253, 'test/loss': 1.869858741760254, 'test/num_examples': 10000, 'score': 55119.610013246536, 'total_duration': 57044.69862341881, 'accumulated_submission_time': 55119.610013246536, 'accumulated_eval_time': 1914.762225151062, 'accumulated_logging_time': 4.603350400924683, 'global_step': 164130, 'preemption_count': 0}), (165651, {'train/accuracy': 0.8374919891357422, 'train/loss': 0.596867024898529, 'validation/accuracy': 0.7252799868583679, 'validation/loss': 1.1151868104934692, 'validation/num_examples': 50000, 'test/accuracy': 0.5944000482559204, 'test/loss': 1.8382610082626343, 'test/num_examples': 10000, 'score': 55629.50521326065, 'total_duration': 57573.0696310997, 'accumulated_submission_time': 55629.50521326065, 'accumulated_eval_time': 1933.1238858699799, 'accumulated_logging_time': 4.66450572013855, 'global_step': 165651, 'preemption_count': 0}), (167173, {'train/accuracy': 0.8379902839660645, 'train/loss': 0.5885335206985474, 'validation/accuracy': 0.7274599671363831, 'validation/loss': 1.0997930765151978, 'validation/num_examples': 50000, 'test/accuracy': 0.6052000522613525, 'test/loss': 1.8111016750335693, 'test/num_examples': 10000, 'score': 56139.65770363808, 'total_duration': 58100.62710976601, 'accumulated_submission_time': 56139.65770363808, 'accumulated_eval_time': 1950.4011313915253, 'accumulated_logging_time': 4.738975524902344, 'global_step': 167173, 'preemption_count': 0}), (168695, {'train/accuracy': 0.8406209945678711, 'train/loss': 0.5672109723091125, 'validation/accuracy': 0.730459988117218, 'validation/loss': 1.097076654434204, 'validation/num_examples': 50000, 'test/accuracy': 0.6029000282287598, 'test/loss': 1.8191895484924316, 'test/num_examples': 10000, 'score': 56649.87207078934, 'total_duration': 58628.46256566048, 'accumulated_submission_time': 56649.87207078934, 'accumulated_eval_time': 1967.9093770980835, 'accumulated_logging_time': 4.797713041305542, 'global_step': 168695, 'preemption_count': 0}), (170217, {'train/accuracy': 0.8475565910339355, 'train/loss': 0.5495447516441345, 'validation/accuracy': 0.7321199774742126, 'validation/loss': 1.0839142799377441, 'validation/num_examples': 50000, 'test/accuracy': 0.6044000387191772, 'test/loss': 1.8097580671310425, 'test/num_examples': 10000, 'score': 57160.08248925209, 'total_duration': 59156.44516658783, 'accumulated_submission_time': 57160.08248925209, 'accumulated_eval_time': 1985.5698521137238, 'accumulated_logging_time': 4.8556458950042725, 'global_step': 170217, 'preemption_count': 0}), (171739, {'train/accuracy': 0.8664699792861938, 'train/loss': 0.4829581677913666, 'validation/accuracy': 0.7368800044059753, 'validation/loss': 1.0651110410690308, 'validation/num_examples': 50000, 'test/accuracy': 0.6091000437736511, 'test/loss': 1.7815965414047241, 'test/num_examples': 10000, 'score': 57670.17210030556, 'total_duration': 59684.1325712204, 'accumulated_submission_time': 57670.17210030556, 'accumulated_eval_time': 2003.057029247284, 'accumulated_logging_time': 4.913210391998291, 'global_step': 171739, 'preemption_count': 0}), (173260, {'train/accuracy': 0.8635004758834839, 'train/loss': 0.49038711190223694, 'validation/accuracy': 0.7384999990463257, 'validation/loss': 1.065165638923645, 'validation/num_examples': 50000, 'test/accuracy': 0.6144000291824341, 'test/loss': 1.779536247253418, 'test/num_examples': 10000, 'score': 58180.46226763725, 'total_duration': 60212.06752181053, 'accumulated_submission_time': 58180.46226763725, 'accumulated_eval_time': 2020.5868430137634, 'accumulated_logging_time': 4.9735023975372314, 'global_step': 173260, 'preemption_count': 0}), (174781, {'train/accuracy': 0.8696388602256775, 'train/loss': 0.4675772190093994, 'validation/accuracy': 0.7422800064086914, 'validation/loss': 1.044396162033081, 'validation/num_examples': 50000, 'test/accuracy': 0.6187000274658203, 'test/loss': 1.7618048191070557, 'test/num_examples': 10000, 'score': 58690.6109726429, 'total_duration': 60739.801298856735, 'accumulated_submission_time': 58690.6109726429, 'accumulated_eval_time': 2038.053610086441, 'accumulated_logging_time': 5.037261247634888, 'global_step': 174781, 'preemption_count': 0}), (176301, {'train/accuracy': 0.8717913031578064, 'train/loss': 0.45689013600349426, 'validation/accuracy': 0.7438399791717529, 'validation/loss': 1.0421042442321777, 'validation/num_examples': 50000, 'test/accuracy': 0.6177000403404236, 'test/loss': 1.7554930448532104, 'test/num_examples': 10000, 'score': 59200.50735998154, 'total_duration': 61267.31038618088, 'accumulated_submission_time': 59200.50735998154, 'accumulated_eval_time': 2055.5500314235687, 'accumulated_logging_time': 5.10002589225769, 'global_step': 176301, 'preemption_count': 0}), (177822, {'train/accuracy': 0.8772520422935486, 'train/loss': 0.43504729866981506, 'validation/accuracy': 0.7451599836349487, 'validation/loss': 1.0354770421981812, 'validation/num_examples': 50000, 'test/accuracy': 0.6203000545501709, 'test/loss': 1.750431776046753, 'test/num_examples': 10000, 'score': 59710.523655653, 'total_duration': 61794.90425157547, 'accumulated_submission_time': 59710.523655653, 'accumulated_eval_time': 2073.0187923908234, 'accumulated_logging_time': 5.156181573867798, 'global_step': 177822, 'preemption_count': 0}), (179343, {'train/accuracy': 0.8833904266357422, 'train/loss': 0.4190746545791626, 'validation/accuracy': 0.7470600008964539, 'validation/loss': 1.032281517982483, 'validation/num_examples': 50000, 'test/accuracy': 0.6243000030517578, 'test/loss': 1.7491586208343506, 'test/num_examples': 10000, 'score': 60220.68492269516, 'total_duration': 62323.07969260216, 'accumulated_submission_time': 60220.68492269516, 'accumulated_eval_time': 2090.9237022399902, 'accumulated_logging_time': 5.213481664657593, 'global_step': 179343, 'preemption_count': 0}), (180863, {'train/accuracy': 0.8840082883834839, 'train/loss': 0.4111105501651764, 'validation/accuracy': 0.7485599517822266, 'validation/loss': 1.0272870063781738, 'validation/num_examples': 50000, 'test/accuracy': 0.6240000128746033, 'test/loss': 1.746339201927185, 'test/num_examples': 10000, 'score': 60730.57482671738, 'total_duration': 62850.83002829552, 'accumulated_submission_time': 60730.57482671738, 'accumulated_eval_time': 2108.6718752384186, 'accumulated_logging_time': 5.271944284439087, 'global_step': 180863, 'preemption_count': 0}), (182383, {'train/accuracy': 0.8837690949440002, 'train/loss': 0.4141952693462372, 'validation/accuracy': 0.7486000061035156, 'validation/loss': 1.0234150886535645, 'validation/num_examples': 50000, 'test/accuracy': 0.6265000104904175, 'test/loss': 1.7416620254516602, 'test/num_examples': 10000, 'score': 61240.5575094223, 'total_duration': 63378.4419465065, 'accumulated_submission_time': 61240.5575094223, 'accumulated_eval_time': 2126.1875302791595, 'accumulated_logging_time': 5.333211421966553, 'global_step': 182383, 'preemption_count': 0}), (183904, {'train/accuracy': 0.8855827450752258, 'train/loss': 0.40654659271240234, 'validation/accuracy': 0.7492199540138245, 'validation/loss': 1.0245238542556763, 'validation/num_examples': 50000, 'test/accuracy': 0.6260000467300415, 'test/loss': 1.7438762187957764, 'test/num_examples': 10000, 'score': 61750.542989730835, 'total_duration': 63905.93356466293, 'accumulated_submission_time': 61750.542989730835, 'accumulated_eval_time': 2143.5833439826965, 'accumulated_logging_time': 5.390666723251343, 'global_step': 183904, 'preemption_count': 0}), (185424, {'train/accuracy': 0.8825932741165161, 'train/loss': 0.41047802567481995, 'validation/accuracy': 0.7490999698638916, 'validation/loss': 1.0238432884216309, 'validation/num_examples': 50000, 'test/accuracy': 0.6255000233650208, 'test/loss': 1.741317868232727, 'test/num_examples': 10000, 'score': 62260.43835067749, 'total_duration': 64433.532873392105, 'accumulated_submission_time': 62260.43835067749, 'accumulated_eval_time': 2161.1756496429443, 'accumulated_logging_time': 5.449017286300659, 'global_step': 185424, 'preemption_count': 0}), (186666, {'train/accuracy': 0.8877750039100647, 'train/loss': 0.4056401550769806, 'validation/accuracy': 0.7488799691200256, 'validation/loss': 1.0230340957641602, 'validation/num_examples': 50000, 'test/accuracy': 0.6260000467300415, 'test/loss': 1.7412824630737305, 'test/num_examples': 10000, 'score': 62676.811291217804, 'total_duration': 64867.45725274086, 'accumulated_submission_time': 62676.811291217804, 'accumulated_eval_time': 2178.622392654419, 'accumulated_logging_time': 5.510376214981079, 'global_step': 186666, 'preemption_count': 0})], 'global_step': 186666}
I0202 21:13:32.148654 140070692116288 submission_runner.py:586] Timing: 62676.811291217804
I0202 21:13:32.148729 140070692116288 submission_runner.py:588] Total number of evals: 124
I0202 21:13:32.148771 140070692116288 submission_runner.py:589] ====================
I0202 21:13:32.148814 140070692116288 submission_runner.py:542] Using RNG seed 485521238
I0202 21:13:32.150394 140070692116288 submission_runner.py:551] --- Tuning run 5/5 ---
I0202 21:13:32.150506 140070692116288 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_3/imagenet_resnet_jax/trial_5.
I0202 21:13:32.151803 140070692116288 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_3/imagenet_resnet_jax/trial_5/hparams.json.
I0202 21:13:32.152574 140070692116288 submission_runner.py:206] Initializing dataset.
I0202 21:13:32.161606 140070692116288 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0202 21:13:32.171548 140070692116288 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0202 21:13:32.889734 140070692116288 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0202 21:13:33.140655 140070692116288 submission_runner.py:213] Initializing model.
I0202 21:13:38.885468 140070692116288 submission_runner.py:255] Initializing optimizer.
I0202 21:13:39.265135 140070692116288 submission_runner.py:262] Initializing metrics bundle.
I0202 21:13:39.265279 140070692116288 submission_runner.py:280] Initializing checkpoint and logger.
I0202 21:13:39.282418 140070692116288 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_3/imagenet_resnet_jax/trial_5 with prefix checkpoint_
I0202 21:13:39.282525 140070692116288 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_3/imagenet_resnet_jax/trial_5/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0202 21:13:51.108905 140070692116288 logger_utils.py:220] Unable to record git information. Continuing without it.
I0202 21:14:02.874957 140070692116288 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_3/imagenet_resnet_jax/trial_5/flags_0.json.
I0202 21:14:02.879493 140070692116288 submission_runner.py:314] Starting training loop.
I0202 21:14:35.288556 139907737556736 logging_writer.py:48] [0] global_step=0, grad_norm=0.6918318271636963, loss=6.925329208374023
I0202 21:14:35.301979 140070692116288 spec.py:321] Evaluating on the training split.
I0202 21:14:41.643813 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 21:14:50.492932 140070692116288 spec.py:349] Evaluating on the test split.
I0202 21:14:53.093758 140070692116288 submission_runner.py:408] Time since start: 50.21s, 	Step: 1, 	{'train/accuracy': 0.0010961415246129036, 'train/loss': 6.911187648773193, 'validation/accuracy': 0.0011199999134987593, 'validation/loss': 6.912059783935547, 'validation/num_examples': 50000, 'test/accuracy': 0.0009000000427477062, 'test/loss': 6.912177562713623, 'test/num_examples': 10000, 'score': 32.42240762710571, 'total_duration': 50.21418762207031, 'accumulated_submission_time': 32.42240762710571, 'accumulated_eval_time': 17.791696310043335, 'accumulated_logging_time': 0}
I0202 21:14:53.105121 139907703985920 logging_writer.py:48] [1] accumulated_eval_time=17.791696, accumulated_logging_time=0, accumulated_submission_time=32.422408, global_step=1, preemption_count=0, score=32.422408, test/accuracy=0.000900, test/loss=6.912178, test/num_examples=10000, total_duration=50.214188, train/accuracy=0.001096, train/loss=6.911188, validation/accuracy=0.001120, validation/loss=6.912060, validation/num_examples=50000
I0202 21:15:26.918293 139907712378624 logging_writer.py:48] [100] global_step=100, grad_norm=0.680103063583374, loss=6.806119441986084
I0202 21:16:00.828858 139907703985920 logging_writer.py:48] [200] global_step=200, grad_norm=0.791297197341919, loss=6.549835681915283
I0202 21:16:34.665852 139907712378624 logging_writer.py:48] [300] global_step=300, grad_norm=0.923351526260376, loss=6.28249979019165
I0202 21:17:08.549601 139907703985920 logging_writer.py:48] [400] global_step=400, grad_norm=1.9480758905410767, loss=6.025084972381592
I0202 21:17:42.401753 139907712378624 logging_writer.py:48] [500] global_step=500, grad_norm=2.7521655559539795, loss=5.89957332611084
I0202 21:18:16.252295 139907703985920 logging_writer.py:48] [600] global_step=600, grad_norm=3.2107162475585938, loss=5.768035888671875
I0202 21:18:50.107195 139907712378624 logging_writer.py:48] [700] global_step=700, grad_norm=2.749542713165283, loss=5.444252014160156
I0202 21:19:23.995780 139907703985920 logging_writer.py:48] [800] global_step=800, grad_norm=4.073309898376465, loss=5.367702007293701
I0202 21:19:57.842190 139907712378624 logging_writer.py:48] [900] global_step=900, grad_norm=6.307762145996094, loss=5.1746826171875
I0202 21:20:31.713041 139907703985920 logging_writer.py:48] [1000] global_step=1000, grad_norm=4.872071743011475, loss=5.162113666534424
I0202 21:21:05.578233 139907712378624 logging_writer.py:48] [1100] global_step=1100, grad_norm=6.779968738555908, loss=4.860228538513184
I0202 21:21:39.436936 139907703985920 logging_writer.py:48] [1200] global_step=1200, grad_norm=5.32097053527832, loss=4.8530144691467285
I0202 21:22:13.323274 139907712378624 logging_writer.py:48] [1300] global_step=1300, grad_norm=4.274842262268066, loss=4.677277565002441
I0202 21:22:47.279608 139907703985920 logging_writer.py:48] [1400] global_step=1400, grad_norm=4.773586750030518, loss=4.536643028259277
I0202 21:23:21.166337 139907712378624 logging_writer.py:48] [1500] global_step=1500, grad_norm=4.837024688720703, loss=4.549307823181152
I0202 21:23:23.348759 140070692116288 spec.py:321] Evaluating on the training split.
I0202 21:23:29.650089 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 21:23:38.199142 140070692116288 spec.py:349] Evaluating on the test split.
I0202 21:23:40.898198 140070692116288 submission_runner.py:408] Time since start: 578.02s, 	Step: 1508, 	{'train/accuracy': 0.15250319242477417, 'train/loss': 4.490919589996338, 'validation/accuracy': 0.13887999951839447, 'validation/loss': 4.628087520599365, 'validation/num_examples': 50000, 'test/accuracy': 0.10010000318288803, 'test/loss': 5.178818702697754, 'test/num_examples': 10000, 'score': 542.6018404960632, 'total_duration': 578.0186469554901, 'accumulated_submission_time': 542.6018404960632, 'accumulated_eval_time': 35.341097831726074, 'accumulated_logging_time': 0.022499561309814453}
I0202 21:23:40.919338 139907754342144 logging_writer.py:48] [1508] accumulated_eval_time=35.341098, accumulated_logging_time=0.022500, accumulated_submission_time=542.601840, global_step=1508, preemption_count=0, score=542.601840, test/accuracy=0.100100, test/loss=5.178819, test/num_examples=10000, total_duration=578.018647, train/accuracy=0.152503, train/loss=4.490920, validation/accuracy=0.138880, validation/loss=4.628088, validation/num_examples=50000
I0202 21:24:12.397825 139907762734848 logging_writer.py:48] [1600] global_step=1600, grad_norm=6.397730827331543, loss=4.2896647453308105
I0202 21:24:46.229101 139907754342144 logging_writer.py:48] [1700] global_step=1700, grad_norm=4.797341346740723, loss=4.348367691040039
I0202 21:25:20.072960 139907762734848 logging_writer.py:48] [1800] global_step=1800, grad_norm=5.154905319213867, loss=4.164494514465332
I0202 21:25:53.946180 139907754342144 logging_writer.py:48] [1900] global_step=1900, grad_norm=8.384880065917969, loss=4.106553554534912
I0202 21:26:27.792989 139907762734848 logging_writer.py:48] [2000] global_step=2000, grad_norm=5.0073113441467285, loss=4.001276016235352
I0202 21:27:01.649000 139907754342144 logging_writer.py:48] [2100] global_step=2100, grad_norm=4.307324409484863, loss=3.9707727432250977
I0202 21:27:35.520724 139907762734848 logging_writer.py:48] [2200] global_step=2200, grad_norm=5.099609851837158, loss=3.8643932342529297
I0202 21:28:09.380372 139907754342144 logging_writer.py:48] [2300] global_step=2300, grad_norm=6.498817443847656, loss=3.8363606929779053
I0202 21:28:43.277780 139907762734848 logging_writer.py:48] [2400] global_step=2400, grad_norm=4.491078853607178, loss=3.794630527496338
I0202 21:29:17.230870 139907754342144 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.140631675720215, loss=3.7284772396087646
I0202 21:29:51.131790 139907762734848 logging_writer.py:48] [2600] global_step=2600, grad_norm=4.046797752380371, loss=3.582374095916748
I0202 21:30:24.946838 139907754342144 logging_writer.py:48] [2700] global_step=2700, grad_norm=3.5973494052886963, loss=3.567488193511963
I0202 21:30:58.796665 139907762734848 logging_writer.py:48] [2800] global_step=2800, grad_norm=3.1709609031677246, loss=3.415144205093384
I0202 21:31:32.597019 139907754342144 logging_writer.py:48] [2900] global_step=2900, grad_norm=3.338834524154663, loss=3.3644943237304688
I0202 21:32:06.432929 139907762734848 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.4562995433807373, loss=3.4026618003845215
I0202 21:32:10.964749 140070692116288 spec.py:321] Evaluating on the training split.
I0202 21:32:17.365639 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 21:32:25.936157 140070692116288 spec.py:349] Evaluating on the test split.
I0202 21:32:28.621411 140070692116288 submission_runner.py:408] Time since start: 1105.74s, 	Step: 3015, 	{'train/accuracy': 0.34781569242477417, 'train/loss': 2.9913976192474365, 'validation/accuracy': 0.3228600025177002, 'validation/loss': 3.1558074951171875, 'validation/num_examples': 50000, 'test/accuracy': 0.2410000115633011, 'test/loss': 3.804908037185669, 'test/num_examples': 10000, 'score': 1052.5842320919037, 'total_duration': 1105.7418451309204, 'accumulated_submission_time': 1052.5842320919037, 'accumulated_eval_time': 52.99773406982422, 'accumulated_logging_time': 0.05348396301269531}
I0202 21:32:28.644276 139907720771328 logging_writer.py:48] [3015] accumulated_eval_time=52.997734, accumulated_logging_time=0.053484, accumulated_submission_time=1052.584232, global_step=3015, preemption_count=0, score=1052.584232, test/accuracy=0.241000, test/loss=3.804908, test/num_examples=10000, total_duration=1105.741845, train/accuracy=0.347816, train/loss=2.991398, validation/accuracy=0.322860, validation/loss=3.155807, validation/num_examples=50000
I0202 21:32:57.698185 139907729164032 logging_writer.py:48] [3100] global_step=3100, grad_norm=4.323329925537109, loss=3.289297580718994
I0202 21:33:31.472705 139907720771328 logging_writer.py:48] [3200] global_step=3200, grad_norm=3.9980740547180176, loss=3.1967990398406982
I0202 21:34:05.315365 139907729164032 logging_writer.py:48] [3300] global_step=3300, grad_norm=5.481309413909912, loss=3.2182977199554443
I0202 21:34:39.116690 139907720771328 logging_writer.py:48] [3400] global_step=3400, grad_norm=2.726459503173828, loss=3.2498507499694824
I0202 21:35:12.982747 139907729164032 logging_writer.py:48] [3500] global_step=3500, grad_norm=3.8051018714904785, loss=3.1906239986419678
I0202 21:35:46.876422 139907720771328 logging_writer.py:48] [3600] global_step=3600, grad_norm=2.7730154991149902, loss=3.235238790512085
I0202 21:36:20.680263 139907729164032 logging_writer.py:48] [3700] global_step=3700, grad_norm=3.710996389389038, loss=3.1043622493743896
I0202 21:36:54.468003 139907720771328 logging_writer.py:48] [3800] global_step=3800, grad_norm=3.7934277057647705, loss=3.088284492492676
I0202 21:37:28.283406 139907729164032 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.7750680446624756, loss=2.994161605834961
I0202 21:38:02.077487 139907720771328 logging_writer.py:48] [4000] global_step=4000, grad_norm=3.40514874458313, loss=3.031280279159546
I0202 21:38:35.917077 139907729164032 logging_writer.py:48] [4100] global_step=4100, grad_norm=3.0215108394622803, loss=2.9076812267303467
I0202 21:39:09.746313 139907720771328 logging_writer.py:48] [4200] global_step=4200, grad_norm=3.4173312187194824, loss=2.86916446685791
I0202 21:39:43.566462 139907729164032 logging_writer.py:48] [4300] global_step=4300, grad_norm=3.5399742126464844, loss=2.855429172515869
I0202 21:40:17.385477 139907720771328 logging_writer.py:48] [4400] global_step=4400, grad_norm=2.2822446823120117, loss=2.660729169845581
I0202 21:40:51.245429 139907729164032 logging_writer.py:48] [4500] global_step=4500, grad_norm=4.551393985748291, loss=2.7052364349365234
I0202 21:40:58.840686 140070692116288 spec.py:321] Evaluating on the training split.
I0202 21:41:05.321316 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 21:41:13.754076 140070692116288 spec.py:349] Evaluating on the test split.
I0202 21:41:16.400043 140070692116288 submission_runner.py:408] Time since start: 1633.52s, 	Step: 4524, 	{'train/accuracy': 0.45719069242477417, 'train/loss': 2.4114012718200684, 'validation/accuracy': 0.42965999245643616, 'validation/loss': 2.576097249984741, 'validation/num_examples': 50000, 'test/accuracy': 0.32270002365112305, 'test/loss': 3.2601864337921143, 'test/num_examples': 10000, 'score': 1562.7190339565277, 'total_duration': 1633.5204920768738, 'accumulated_submission_time': 1562.7190339565277, 'accumulated_eval_time': 70.55705571174622, 'accumulated_logging_time': 0.08645176887512207}
I0202 21:41:16.418294 139907712378624 logging_writer.py:48] [4524] accumulated_eval_time=70.557056, accumulated_logging_time=0.086452, accumulated_submission_time=1562.719034, global_step=4524, preemption_count=0, score=1562.719034, test/accuracy=0.322700, test/loss=3.260186, test/num_examples=10000, total_duration=1633.520492, train/accuracy=0.457191, train/loss=2.411401, validation/accuracy=0.429660, validation/loss=2.576097, validation/num_examples=50000
I0202 21:41:42.435843 139907737556736 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.430104970932007, loss=2.7949471473693848
I0202 21:42:16.257004 139907712378624 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.038252830505371, loss=2.792778730392456
I0202 21:42:50.081942 139907737556736 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.5707621574401855, loss=2.6729836463928223
I0202 21:43:23.890473 139907712378624 logging_writer.py:48] [4900] global_step=4900, grad_norm=2.457594156265259, loss=2.616178512573242
I0202 21:43:57.694891 139907737556736 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.006683349609375, loss=2.752039670944214
I0202 21:44:31.512417 139907712378624 logging_writer.py:48] [5100] global_step=5100, grad_norm=2.2189483642578125, loss=2.5912764072418213
I0202 21:45:05.313904 139907737556736 logging_writer.py:48] [5200] global_step=5200, grad_norm=2.6864683628082275, loss=2.6896116733551025
I0202 21:45:39.165802 139907712378624 logging_writer.py:48] [5300] global_step=5300, grad_norm=2.3768746852874756, loss=2.5453388690948486
I0202 21:46:12.967396 139907737556736 logging_writer.py:48] [5400] global_step=5400, grad_norm=2.1050028800964355, loss=2.5079774856567383
I0202 21:46:46.801058 139907712378624 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.2642784118652344, loss=2.5391197204589844
I0202 21:47:20.617901 139907737556736 logging_writer.py:48] [5600] global_step=5600, grad_norm=2.4235434532165527, loss=2.4937992095947266
I0202 21:47:54.456962 139907712378624 logging_writer.py:48] [5700] global_step=5700, grad_norm=2.3132712841033936, loss=2.555145740509033
I0202 21:48:28.250473 139907737556736 logging_writer.py:48] [5800] global_step=5800, grad_norm=2.6968350410461426, loss=2.456533670425415
I0202 21:49:02.109776 139907712378624 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.829302430152893, loss=2.453357696533203
I0202 21:49:35.913562 139907737556736 logging_writer.py:48] [6000] global_step=6000, grad_norm=2.4622888565063477, loss=2.478827714920044
I0202 21:49:46.550243 140070692116288 spec.py:321] Evaluating on the training split.
I0202 21:49:52.942538 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 21:50:01.591234 140070692116288 spec.py:349] Evaluating on the test split.
I0202 21:50:04.376061 140070692116288 submission_runner.py:408] Time since start: 2161.50s, 	Step: 6033, 	{'train/accuracy': 0.5725047588348389, 'train/loss': 1.785803198814392, 'validation/accuracy': 0.5069999694824219, 'validation/loss': 2.1507537364959717, 'validation/num_examples': 50000, 'test/accuracy': 0.38430002331733704, 'test/loss': 2.940765380859375, 'test/num_examples': 10000, 'score': 2072.789868593216, 'total_duration': 2161.4965052604675, 'accumulated_submission_time': 2072.789868593216, 'accumulated_eval_time': 88.38283014297485, 'accumulated_logging_time': 0.11400866508483887}
I0202 21:50:04.393921 139907703985920 logging_writer.py:48] [6033] accumulated_eval_time=88.382830, accumulated_logging_time=0.114009, accumulated_submission_time=2072.789869, global_step=6033, preemption_count=0, score=2072.789869, test/accuracy=0.384300, test/loss=2.940765, test/num_examples=10000, total_duration=2161.496505, train/accuracy=0.572505, train/loss=1.785803, validation/accuracy=0.507000, validation/loss=2.150754, validation/num_examples=50000
I0202 21:50:27.363862 139907712378624 logging_writer.py:48] [6100] global_step=6100, grad_norm=1.969667911529541, loss=2.4148714542388916
I0202 21:51:01.063493 139907703985920 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.549468755722046, loss=2.380796432495117
I0202 21:51:34.755796 139907712378624 logging_writer.py:48] [6300] global_step=6300, grad_norm=2.4138717651367188, loss=2.528665542602539
I0202 21:52:08.552479 139907703985920 logging_writer.py:48] [6400] global_step=6400, grad_norm=1.509386658668518, loss=2.4926085472106934
I0202 21:52:42.300811 139907712378624 logging_writer.py:48] [6500] global_step=6500, grad_norm=3.004711151123047, loss=2.39089298248291
I0202 21:53:16.078989 139907703985920 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.848301649093628, loss=2.5354104042053223
I0202 21:53:49.832646 139907712378624 logging_writer.py:48] [6700] global_step=6700, grad_norm=2.3365962505340576, loss=2.3767452239990234
I0202 21:54:23.569576 139907703985920 logging_writer.py:48] [6800] global_step=6800, grad_norm=2.3226399421691895, loss=2.417163372039795
I0202 21:54:57.355566 139907712378624 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.6640182733535767, loss=2.228588104248047
I0202 21:55:31.253266 139907703985920 logging_writer.py:48] [7000] global_step=7000, grad_norm=2.306020975112915, loss=2.1501283645629883
I0202 21:56:05.020307 139907712378624 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.011000871658325, loss=2.2832469940185547
I0202 21:56:38.815536 139907703985920 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.8829625844955444, loss=2.2637996673583984
I0202 21:57:12.552672 139907712378624 logging_writer.py:48] [7300] global_step=7300, grad_norm=2.0076639652252197, loss=2.2733852863311768
I0202 21:57:46.364832 139907703985920 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.889896035194397, loss=2.373899221420288
I0202 21:58:20.146657 139907712378624 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.698784589767456, loss=2.272611618041992
I0202 21:58:34.459573 140070692116288 spec.py:321] Evaluating on the training split.
I0202 21:58:40.726492 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 21:58:49.156696 140070692116288 spec.py:349] Evaluating on the test split.
I0202 21:58:51.730521 140070692116288 submission_runner.py:408] Time since start: 2688.85s, 	Step: 7544, 	{'train/accuracy': 0.5734614133834839, 'train/loss': 1.7597079277038574, 'validation/accuracy': 0.5224800109863281, 'validation/loss': 2.043307065963745, 'validation/num_examples': 50000, 'test/accuracy': 0.3979000151157379, 'test/loss': 2.8477373123168945, 'test/num_examples': 10000, 'score': 2582.7939958572388, 'total_duration': 2688.8509685993195, 'accumulated_submission_time': 2582.7939958572388, 'accumulated_eval_time': 105.65375065803528, 'accumulated_logging_time': 0.1418612003326416}
I0202 21:58:51.748425 139907737556736 logging_writer.py:48] [7544] accumulated_eval_time=105.653751, accumulated_logging_time=0.141861, accumulated_submission_time=2582.793996, global_step=7544, preemption_count=0, score=2582.793996, test/accuracy=0.397900, test/loss=2.847737, test/num_examples=10000, total_duration=2688.850969, train/accuracy=0.573461, train/loss=1.759708, validation/accuracy=0.522480, validation/loss=2.043307, validation/num_examples=50000
I0202 21:59:10.987453 139907745949440 logging_writer.py:48] [7600] global_step=7600, grad_norm=1.4704680442810059, loss=2.224144458770752
I0202 21:59:44.724448 139907737556736 logging_writer.py:48] [7700] global_step=7700, grad_norm=2.2695059776306152, loss=2.258927345275879
I0202 22:00:18.457276 139907745949440 logging_writer.py:48] [7800] global_step=7800, grad_norm=2.005246639251709, loss=2.224348545074463
I0202 22:00:52.234664 139907737556736 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.9882227182388306, loss=2.2422547340393066
I0202 22:01:25.951979 139907745949440 logging_writer.py:48] [8000] global_step=8000, grad_norm=2.0189127922058105, loss=2.1193361282348633
I0202 22:01:59.792473 139907737556736 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.8131035566329956, loss=2.168936252593994
I0202 22:02:33.592353 139907745949440 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.3964225053787231, loss=2.140108108520508
I0202 22:03:07.381599 139907737556736 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.8088672161102295, loss=2.1076953411102295
I0202 22:03:41.099696 139907745949440 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.8877631425857544, loss=2.1346895694732666
I0202 22:04:14.877880 139907737556736 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.6758371591567993, loss=2.203303575515747
I0202 22:04:48.613609 139907745949440 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.6381117105484009, loss=2.2488746643066406
I0202 22:05:22.399564 139907737556736 logging_writer.py:48] [8700] global_step=8700, grad_norm=2.0961015224456787, loss=2.0874733924865723
I0202 22:05:56.175080 139907745949440 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.9061429500579834, loss=2.118349313735962
I0202 22:06:29.953274 139907737556736 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.6031384468078613, loss=2.083601713180542
I0202 22:07:03.712662 139907745949440 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.6475293636322021, loss=2.189579963684082
I0202 22:07:21.743393 140070692116288 spec.py:321] Evaluating on the training split.
I0202 22:07:28.246444 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 22:07:36.541414 140070692116288 spec.py:349] Evaluating on the test split.
I0202 22:07:39.231183 140070692116288 submission_runner.py:408] Time since start: 3216.35s, 	Step: 9055, 	{'train/accuracy': 0.6027184128761292, 'train/loss': 1.6339234113693237, 'validation/accuracy': 0.5541399717330933, 'validation/loss': 1.8997522592544556, 'validation/num_examples': 50000, 'test/accuracy': 0.4310000240802765, 'test/loss': 2.634711265563965, 'test/num_examples': 10000, 'score': 3092.727585554123, 'total_duration': 3216.351620197296, 'accumulated_submission_time': 3092.727585554123, 'accumulated_eval_time': 123.14149379730225, 'accumulated_logging_time': 0.1691446304321289}
I0202 22:07:39.249544 139907720771328 logging_writer.py:48] [9055] accumulated_eval_time=123.141494, accumulated_logging_time=0.169145, accumulated_submission_time=3092.727586, global_step=9055, preemption_count=0, score=3092.727586, test/accuracy=0.431000, test/loss=2.634711, test/num_examples=10000, total_duration=3216.351620, train/accuracy=0.602718, train/loss=1.633923, validation/accuracy=0.554140, validation/loss=1.899752, validation/num_examples=50000
I0202 22:07:54.808061 139907729164032 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.5044968128204346, loss=2.0451645851135254
I0202 22:08:28.521634 139907720771328 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.4881612062454224, loss=2.1175155639648438
I0202 22:09:02.328566 139907729164032 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.5982019901275635, loss=2.088779926300049
I0202 22:09:36.037140 139907720771328 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.9449212551116943, loss=2.0330569744110107
I0202 22:10:09.817197 139907729164032 logging_writer.py:48] [9500] global_step=9500, grad_norm=2.1494784355163574, loss=2.0692451000213623
I0202 22:10:43.517542 139907720771328 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.3962137699127197, loss=2.1647655963897705
I0202 22:11:17.314563 139907729164032 logging_writer.py:48] [9700] global_step=9700, grad_norm=2.028949737548828, loss=2.245364189147949
I0202 22:11:51.023246 139907720771328 logging_writer.py:48] [9800] global_step=9800, grad_norm=1.8858680725097656, loss=2.1100223064422607
I0202 22:12:24.796232 139907729164032 logging_writer.py:48] [9900] global_step=9900, grad_norm=1.3945574760437012, loss=2.1081655025482178
I0202 22:12:58.526246 139907720771328 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.505417823791504, loss=2.10319185256958
I0202 22:13:32.271039 139907729164032 logging_writer.py:48] [10100] global_step=10100, grad_norm=2.0972483158111572, loss=2.206022262573242
I0202 22:14:05.938828 139907720771328 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.7289540767669678, loss=2.0909552574157715
I0202 22:14:39.741672 139907729164032 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.8152189254760742, loss=2.11794376373291
I0202 22:15:13.554594 139907720771328 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.5941200256347656, loss=2.0064644813537598
I0202 22:15:47.282104 139907729164032 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.6396528482437134, loss=2.0212507247924805
I0202 22:16:09.308835 140070692116288 spec.py:321] Evaluating on the training split.
I0202 22:16:15.742925 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 22:16:24.274834 140070692116288 spec.py:349] Evaluating on the test split.
I0202 22:16:26.912966 140070692116288 submission_runner.py:408] Time since start: 3744.03s, 	Step: 10567, 	{'train/accuracy': 0.6264349222183228, 'train/loss': 1.537442684173584, 'validation/accuracy': 0.5761199593544006, 'validation/loss': 1.796309471130371, 'validation/num_examples': 50000, 'test/accuracy': 0.44350001215934753, 'test/loss': 2.5696029663085938, 'test/num_examples': 10000, 'score': 3602.7257010936737, 'total_duration': 3744.0334181785583, 'accumulated_submission_time': 3602.7257010936737, 'accumulated_eval_time': 140.74559259414673, 'accumulated_logging_time': 0.19663071632385254}
I0202 22:16:26.931869 139907251042048 logging_writer.py:48] [10567] accumulated_eval_time=140.745593, accumulated_logging_time=0.196631, accumulated_submission_time=3602.725701, global_step=10567, preemption_count=0, score=3602.725701, test/accuracy=0.443500, test/loss=2.569603, test/num_examples=10000, total_duration=3744.033418, train/accuracy=0.626435, train/loss=1.537443, validation/accuracy=0.576120, validation/loss=1.796309, validation/num_examples=50000
I0202 22:16:38.386419 139907703985920 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.4706999063491821, loss=1.972538948059082
I0202 22:17:12.062433 139907251042048 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.8643850088119507, loss=1.942505955696106
I0202 22:17:45.838755 139907703985920 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.6934477090835571, loss=2.0432348251342773
I0202 22:18:19.586572 139907251042048 logging_writer.py:48] [10900] global_step=10900, grad_norm=1.5605413913726807, loss=1.8816152811050415
I0202 22:18:53.298356 139907703985920 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.648471474647522, loss=2.098397970199585
I0202 22:19:27.024587 139907251042048 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.852413535118103, loss=2.0349783897399902
I0202 22:20:00.793456 139907703985920 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.36964750289917, loss=2.073740005493164
I0202 22:20:34.528788 139907251042048 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.7060681581497192, loss=2.0428779125213623
I0202 22:21:08.266913 139907703985920 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.7215416431427002, loss=2.1557979583740234
I0202 22:21:42.085538 139907251042048 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.8861331939697266, loss=2.032829761505127
I0202 22:22:15.858072 139907703985920 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.6131768226623535, loss=1.9789412021636963
I0202 22:22:49.635291 139907251042048 logging_writer.py:48] [11700] global_step=11700, grad_norm=2.055166006088257, loss=2.022080898284912
I0202 22:23:23.406348 139907703985920 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.5446231365203857, loss=2.0308127403259277
I0202 22:23:57.143061 139907251042048 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.2436901330947876, loss=1.8267955780029297
I0202 22:24:30.883378 139907703985920 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.4088177680969238, loss=1.9275553226470947
I0202 22:24:56.999822 140070692116288 spec.py:321] Evaluating on the training split.
I0202 22:25:03.541519 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 22:25:12.097953 140070692116288 spec.py:349] Evaluating on the test split.
I0202 22:25:14.906541 140070692116288 submission_runner.py:408] Time since start: 4272.03s, 	Step: 12079, 	{'train/accuracy': 0.6282485723495483, 'train/loss': 1.5030643939971924, 'validation/accuracy': 0.5816199779510498, 'validation/loss': 1.767730951309204, 'validation/num_examples': 50000, 'test/accuracy': 0.4504000246524811, 'test/loss': 2.5434465408325195, 'test/num_examples': 10000, 'score': 4112.730927944183, 'total_duration': 4272.026992321014, 'accumulated_submission_time': 4112.730927944183, 'accumulated_eval_time': 158.65228414535522, 'accumulated_logging_time': 0.22556233406066895}
I0202 22:25:14.925330 139907729164032 logging_writer.py:48] [12079] accumulated_eval_time=158.652284, accumulated_logging_time=0.225562, accumulated_submission_time=4112.730928, global_step=12079, preemption_count=0, score=4112.730928, test/accuracy=0.450400, test/loss=2.543447, test/num_examples=10000, total_duration=4272.026992, train/accuracy=0.628249, train/loss=1.503064, validation/accuracy=0.581620, validation/loss=1.767731, validation/num_examples=50000
I0202 22:25:22.376294 139907745949440 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.699824571609497, loss=1.9902539253234863
I0202 22:25:56.153814 139907729164032 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.6075055599212646, loss=1.9963725805282593
I0202 22:26:29.883426 139907745949440 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.807677984237671, loss=2.1164302825927734
I0202 22:27:03.637246 139907729164032 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.8335840702056885, loss=2.0099380016326904
I0202 22:27:37.411440 139907745949440 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.5906398296356201, loss=1.9419562816619873
I0202 22:28:11.129258 139907729164032 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.5497775077819824, loss=1.880259394645691
I0202 22:28:44.957102 139907745949440 logging_writer.py:48] [12700] global_step=12700, grad_norm=2.489312171936035, loss=2.0678513050079346
I0202 22:29:18.639014 139907729164032 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.7173376083374023, loss=1.8775357007980347
I0202 22:29:52.394927 139907745949440 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.8244414329528809, loss=1.9454574584960938
I0202 22:30:26.200445 139907729164032 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.9324846267700195, loss=1.955258846282959
I0202 22:30:59.918795 139907745949440 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.8401495218276978, loss=1.9758862257003784
I0202 22:31:33.689246 139907729164032 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.564937949180603, loss=2.032331705093384
I0202 22:32:07.449819 139907745949440 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.4387997388839722, loss=1.9751030206680298
I0202 22:32:41.177777 139907729164032 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.435291051864624, loss=1.9313122034072876
I0202 22:33:14.925266 139907745949440 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.809457778930664, loss=1.9211300611495972
I0202 22:33:45.095885 140070692116288 spec.py:321] Evaluating on the training split.
I0202 22:33:51.562542 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 22:33:59.957127 140070692116288 spec.py:349] Evaluating on the test split.
I0202 22:34:02.679469 140070692116288 submission_runner.py:408] Time since start: 4799.80s, 	Step: 13591, 	{'train/accuracy': 0.644949734210968, 'train/loss': 1.4343558549880981, 'validation/accuracy': 0.5983200073242188, 'validation/loss': 1.6884334087371826, 'validation/num_examples': 50000, 'test/accuracy': 0.46790000796318054, 'test/loss': 2.4460861682891846, 'test/num_examples': 10000, 'score': 4622.837368488312, 'total_duration': 4799.799918413162, 'accumulated_submission_time': 4622.837368488312, 'accumulated_eval_time': 176.235848903656, 'accumulated_logging_time': 0.2562377452850342}
I0202 22:34:02.699652 139907720771328 logging_writer.py:48] [13591] accumulated_eval_time=176.235849, accumulated_logging_time=0.256238, accumulated_submission_time=4622.837368, global_step=13591, preemption_count=0, score=4622.837368, test/accuracy=0.467900, test/loss=2.446086, test/num_examples=10000, total_duration=4799.799918, train/accuracy=0.644950, train/loss=1.434356, validation/accuracy=0.598320, validation/loss=1.688433, validation/num_examples=50000
I0202 22:34:06.086126 139907737556736 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.6374739408493042, loss=1.8648759126663208
I0202 22:34:39.809144 139907720771328 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.3221849203109741, loss=1.8669605255126953
I0202 22:35:13.638329 139907737556736 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.5108106136322021, loss=1.9878379106521606
I0202 22:35:47.308754 139907720771328 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.368278980255127, loss=1.8456895351409912
I0202 22:36:21.013927 139907737556736 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.3760089874267578, loss=1.9314295053482056
I0202 22:36:54.795261 139907720771328 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.5885188579559326, loss=1.8659522533416748
I0202 22:37:28.517528 139907737556736 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.7181769609451294, loss=2.021244525909424
I0202 22:38:02.289865 139907720771328 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.6095234155654907, loss=1.9314953088760376
I0202 22:38:36.009500 139907737556736 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.5958651304244995, loss=1.8840422630310059
I0202 22:39:09.778144 139907720771328 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.7913057804107666, loss=1.8523272275924683
I0202 22:39:43.524864 139907737556736 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.6784539222717285, loss=1.911181092262268
I0202 22:40:17.271926 139907720771328 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.8028370141983032, loss=1.9696457386016846
I0202 22:40:50.999333 139907737556736 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.7304961681365967, loss=1.7736915349960327
I0202 22:41:24.849008 139907720771328 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.5803102254867554, loss=1.965557336807251
I0202 22:41:58.613344 139907737556736 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.336073875427246, loss=1.9385099411010742
I0202 22:42:32.383618 139907720771328 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.38960599899292, loss=1.9015368223190308
I0202 22:42:32.873685 140070692116288 spec.py:321] Evaluating on the training split.
I0202 22:42:39.261090 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 22:42:47.680703 140070692116288 spec.py:349] Evaluating on the test split.
I0202 22:42:50.310460 140070692116288 submission_runner.py:408] Time since start: 5327.43s, 	Step: 15103, 	{'train/accuracy': 0.6813815236091614, 'train/loss': 1.2665554285049438, 'validation/accuracy': 0.6004999876022339, 'validation/loss': 1.6677234172821045, 'validation/num_examples': 50000, 'test/accuracy': 0.4739000201225281, 'test/loss': 2.416137933731079, 'test/num_examples': 10000, 'score': 5132.94886469841, 'total_duration': 5327.430913209915, 'accumulated_submission_time': 5132.94886469841, 'accumulated_eval_time': 193.6725881099701, 'accumulated_logging_time': 0.2862663269042969}
I0202 22:42:50.329862 139907251042048 logging_writer.py:48] [15103] accumulated_eval_time=193.672588, accumulated_logging_time=0.286266, accumulated_submission_time=5132.948865, global_step=15103, preemption_count=0, score=5132.948865, test/accuracy=0.473900, test/loss=2.416138, test/num_examples=10000, total_duration=5327.430913, train/accuracy=0.681382, train/loss=1.266555, validation/accuracy=0.600500, validation/loss=1.667723, validation/num_examples=50000
I0202 22:43:23.357148 139907703985920 logging_writer.py:48] [15200] global_step=15200, grad_norm=2.2626404762268066, loss=1.8162167072296143
I0202 22:43:57.062224 139907251042048 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.5298078060150146, loss=1.8435428142547607
I0202 22:44:30.796269 139907703985920 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.877290964126587, loss=2.0656023025512695
I0202 22:45:04.554794 139907251042048 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.7219781875610352, loss=1.7285608053207397
I0202 22:45:38.321004 139907703985920 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.5315425395965576, loss=1.7904560565948486
I0202 22:46:12.095305 139907251042048 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.637774109840393, loss=1.8979326486587524
I0202 22:46:45.787385 139907703985920 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.4426631927490234, loss=1.9200352430343628
I0202 22:47:19.501837 139907251042048 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.8339745998382568, loss=1.8087122440338135
I0202 22:47:53.318267 139907703985920 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.7605549097061157, loss=1.7795164585113525
I0202 22:48:27.111277 139907251042048 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.581966757774353, loss=1.8114694356918335
I0202 22:49:00.875470 139907703985920 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.6702686548233032, loss=1.9306401014328003
I0202 22:49:34.600719 139907251042048 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.829696536064148, loss=1.859742522239685
I0202 22:50:08.289892 139907703985920 logging_writer.py:48] [16400] global_step=16400, grad_norm=2.2513582706451416, loss=1.8323040008544922
I0202 22:50:42.104300 139907251042048 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.520133376121521, loss=1.8907523155212402
I0202 22:51:15.836590 139907703985920 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.82178795337677, loss=1.8413197994232178
I0202 22:51:20.377823 140070692116288 spec.py:321] Evaluating on the training split.
I0202 22:51:27.556271 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 22:51:36.235389 140070692116288 spec.py:349] Evaluating on the test split.
I0202 22:51:38.772559 140070692116288 submission_runner.py:408] Time since start: 5855.89s, 	Step: 16615, 	{'train/accuracy': 0.666015625, 'train/loss': 1.3178972005844116, 'validation/accuracy': 0.6035000085830688, 'validation/loss': 1.6508041620254517, 'validation/num_examples': 50000, 'test/accuracy': 0.4627000093460083, 'test/loss': 2.459235191345215, 'test/num_examples': 10000, 'score': 5642.935264825821, 'total_duration': 5855.89298915863, 'accumulated_submission_time': 5642.935264825821, 'accumulated_eval_time': 212.0672664642334, 'accumulated_logging_time': 0.3150801658630371}
I0202 22:51:38.793604 139907737556736 logging_writer.py:48] [16615] accumulated_eval_time=212.067266, accumulated_logging_time=0.315080, accumulated_submission_time=5642.935265, global_step=16615, preemption_count=0, score=5642.935265, test/accuracy=0.462700, test/loss=2.459235, test/num_examples=10000, total_duration=5855.892989, train/accuracy=0.666016, train/loss=1.317897, validation/accuracy=0.603500, validation/loss=1.650804, validation/num_examples=50000
I0202 22:52:07.810715 139907745949440 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.6748332977294922, loss=1.8099241256713867
I0202 22:52:41.521678 139907737556736 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.4678356647491455, loss=1.8463603258132935
I0202 22:53:15.261430 139907745949440 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.8432105779647827, loss=1.8171226978302002
I0202 22:53:49.008814 139907737556736 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.3785381317138672, loss=1.9679588079452515
I0202 22:54:22.713832 139907745949440 logging_writer.py:48] [17100] global_step=17100, grad_norm=2.0134310722351074, loss=1.8111189603805542
I0202 22:54:56.614076 139907737556736 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.567839503288269, loss=1.8954404592514038
I0202 22:55:30.330944 139907745949440 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.568872332572937, loss=1.8627500534057617
I0202 22:56:04.108031 139907737556736 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.9554998874664307, loss=1.9579445123672485
I0202 22:56:37.874504 139907745949440 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.4061334133148193, loss=1.779429316520691
I0202 22:57:11.605850 139907737556736 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.550635576248169, loss=1.770233392715454
I0202 22:57:45.362608 139907745949440 logging_writer.py:48] [17700] global_step=17700, grad_norm=2.00129771232605, loss=1.7731711864471436
I0202 22:58:19.156442 139907737556736 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.909244179725647, loss=1.784886360168457
I0202 22:58:52.865753 139907745949440 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.4718831777572632, loss=1.8464858531951904
I0202 22:59:26.663821 139907737556736 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.4402693510055542, loss=1.8325709104537964
I0202 23:00:00.397305 139907745949440 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.7645869255065918, loss=1.9220017194747925
I0202 23:00:08.984880 140070692116288 spec.py:321] Evaluating on the training split.
I0202 23:00:15.284060 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 23:00:23.859730 140070692116288 spec.py:349] Evaluating on the test split.
I0202 23:00:26.522881 140070692116288 submission_runner.py:408] Time since start: 6383.64s, 	Step: 18127, 	{'train/accuracy': 0.6575454473495483, 'train/loss': 1.3667640686035156, 'validation/accuracy': 0.6034199595451355, 'validation/loss': 1.652487874031067, 'validation/num_examples': 50000, 'test/accuracy': 0.4727000296115875, 'test/loss': 2.390066385269165, 'test/num_examples': 10000, 'score': 6153.063757181168, 'total_duration': 6383.643333911896, 'accumulated_submission_time': 6153.063757181168, 'accumulated_eval_time': 229.60523438453674, 'accumulated_logging_time': 0.34566211700439453}
I0202 23:00:26.543102 139907712378624 logging_writer.py:48] [18127] accumulated_eval_time=229.605234, accumulated_logging_time=0.345662, accumulated_submission_time=6153.063757, global_step=18127, preemption_count=0, score=6153.063757, test/accuracy=0.472700, test/loss=2.390066, test/num_examples=10000, total_duration=6383.643334, train/accuracy=0.657545, train/loss=1.366764, validation/accuracy=0.603420, validation/loss=1.652488, validation/num_examples=50000
I0202 23:00:51.442025 139907720771328 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.5590018033981323, loss=1.7994259595870972
I0202 23:01:25.218729 139907712378624 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.4696465730667114, loss=1.890216588973999
I0202 23:01:58.942725 139907720771328 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.74307382106781, loss=1.9913853406906128
I0202 23:02:32.672343 139907712378624 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.6649949550628662, loss=1.8282772302627563
I0202 23:03:06.452752 139907720771328 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.7481974363327026, loss=1.8163350820541382
I0202 23:03:40.189665 139907712378624 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.801869511604309, loss=1.712697148323059
I0202 23:04:13.924582 139907720771328 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.6773592233657837, loss=1.8033840656280518
I0202 23:04:47.689499 139907712378624 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.5680525302886963, loss=1.8325433731079102
I0202 23:05:21.426441 139907720771328 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.5418812036514282, loss=1.8336986303329468
I0202 23:05:55.172646 139907712378624 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.5650817155838013, loss=1.8322478532791138
I0202 23:06:28.909991 139907720771328 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.6164456605911255, loss=1.8460406064987183
I0202 23:07:02.650859 139907712378624 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.620794653892517, loss=1.8589539527893066
I0202 23:07:36.496501 139907720771328 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.8583719730377197, loss=1.8923468589782715
I0202 23:08:10.216197 139907712378624 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.5581668615341187, loss=1.7687801122665405
I0202 23:08:43.939286 139907720771328 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.8135948181152344, loss=1.8141833543777466
I0202 23:08:56.576467 140070692116288 spec.py:321] Evaluating on the training split.
I0202 23:09:03.063319 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 23:09:11.533271 140070692116288 spec.py:349] Evaluating on the test split.
I0202 23:09:14.180341 140070692116288 submission_runner.py:408] Time since start: 6911.30s, 	Step: 19639, 	{'train/accuracy': 0.6714365482330322, 'train/loss': 1.3168739080429077, 'validation/accuracy': 0.6110599637031555, 'validation/loss': 1.6108882427215576, 'validation/num_examples': 50000, 'test/accuracy': 0.4854000210762024, 'test/loss': 2.3747594356536865, 'test/num_examples': 10000, 'score': 6663.034357786179, 'total_duration': 6911.30079293251, 'accumulated_submission_time': 6663.034357786179, 'accumulated_eval_time': 247.20907497406006, 'accumulated_logging_time': 0.37534093856811523}
I0202 23:09:14.201100 139907703985920 logging_writer.py:48] [19639] accumulated_eval_time=247.209075, accumulated_logging_time=0.375341, accumulated_submission_time=6663.034358, global_step=19639, preemption_count=0, score=6663.034358, test/accuracy=0.485400, test/loss=2.374759, test/num_examples=10000, total_duration=6911.300793, train/accuracy=0.671437, train/loss=1.316874, validation/accuracy=0.611060, validation/loss=1.610888, validation/num_examples=50000
I0202 23:09:35.113124 139907745949440 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.6879020929336548, loss=1.953325629234314
I0202 23:10:08.847408 139907703985920 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.6708437204360962, loss=1.700392484664917
I0202 23:10:42.554657 139907745949440 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.461582899093628, loss=1.6349258422851562
I0202 23:11:16.289097 139907703985920 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.6934939622879028, loss=1.8036333322525024
I0202 23:11:50.029089 139907745949440 logging_writer.py:48] [20100] global_step=20100, grad_norm=2.103184938430786, loss=1.757796049118042
I0202 23:12:23.797142 139907703985920 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.6169859170913696, loss=1.873451590538025
I0202 23:12:57.486283 139907745949440 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.6367545127868652, loss=1.680601716041565
I0202 23:13:31.256819 139907703985920 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.7020552158355713, loss=1.8736491203308105
I0202 23:14:05.107322 139907745949440 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.8382633924484253, loss=1.7862176895141602
I0202 23:14:38.857572 139907703985920 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.6030805110931396, loss=1.6800663471221924
I0202 23:15:12.603419 139907745949440 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.5330899953842163, loss=1.6670713424682617
I0202 23:15:46.364939 139907703985920 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.7662575244903564, loss=1.833458423614502
I0202 23:16:20.075453 139907745949440 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.8755143880844116, loss=1.9035789966583252
I0202 23:16:53.812873 139907703985920 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.618950366973877, loss=1.8088096380233765
I0202 23:17:27.603370 139907745949440 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.688995599746704, loss=1.7815889120101929
I0202 23:17:44.268888 140070692116288 spec.py:321] Evaluating on the training split.
I0202 23:17:50.541254 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 23:17:59.125128 140070692116288 spec.py:349] Evaluating on the test split.
I0202 23:18:01.805605 140070692116288 submission_runner.py:408] Time since start: 7438.93s, 	Step: 21151, 	{'train/accuracy': 0.6639229655265808, 'train/loss': 1.3415753841400146, 'validation/accuracy': 0.6109600067138672, 'validation/loss': 1.6230710744857788, 'validation/num_examples': 50000, 'test/accuracy': 0.492000013589859, 'test/loss': 2.333085536956787, 'test/num_examples': 10000, 'score': 7173.039954662323, 'total_duration': 7438.92605638504, 'accumulated_submission_time': 7173.039954662323, 'accumulated_eval_time': 264.745756149292, 'accumulated_logging_time': 0.4052441120147705}
I0202 23:18:01.826513 139907737556736 logging_writer.py:48] [21151] accumulated_eval_time=264.745756, accumulated_logging_time=0.405244, accumulated_submission_time=7173.039955, global_step=21151, preemption_count=0, score=7173.039955, test/accuracy=0.492000, test/loss=2.333086, test/num_examples=10000, total_duration=7438.926056, train/accuracy=0.663923, train/loss=1.341575, validation/accuracy=0.610960, validation/loss=1.623071, validation/num_examples=50000
I0202 23:18:18.703586 139907762734848 logging_writer.py:48] [21200] global_step=21200, grad_norm=2.012507677078247, loss=1.7835052013397217
I0202 23:18:52.428254 139907737556736 logging_writer.py:48] [21300] global_step=21300, grad_norm=2.0354561805725098, loss=1.917380928993225
I0202 23:19:26.102425 139907762734848 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.6633719205856323, loss=1.7004468441009521
I0202 23:19:59.781564 139907737556736 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.5030895471572876, loss=1.7758889198303223
I0202 23:20:33.539487 139907762734848 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.6354889869689941, loss=1.717686653137207
I0202 23:21:07.294953 139907737556736 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.7370649576187134, loss=1.7569050788879395
I0202 23:21:41.050545 139907762734848 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.59962797164917, loss=1.8564836978912354
I0202 23:22:14.795845 139907737556736 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.6657161712646484, loss=1.8711678981781006
I0202 23:22:48.552115 139907762734848 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.6814147233963013, loss=1.7615599632263184
I0202 23:23:22.293870 139907737556736 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.476789116859436, loss=1.7749589681625366
I0202 23:23:56.017144 139907762734848 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.9750635623931885, loss=1.8623906373977661
I0202 23:24:29.779073 139907737556736 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.9433091878890991, loss=1.7084581851959229
I0202 23:25:03.536529 139907762734848 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.797602891921997, loss=1.7747043371200562
I0202 23:25:37.271862 139907737556736 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.6392617225646973, loss=1.680877685546875
I0202 23:26:11.030409 139907762734848 logging_writer.py:48] [22600] global_step=22600, grad_norm=2.017277717590332, loss=1.7709134817123413
I0202 23:26:32.105391 140070692116288 spec.py:321] Evaluating on the training split.
I0202 23:26:38.427291 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 23:26:46.747255 140070692116288 spec.py:349] Evaluating on the test split.
I0202 23:26:49.567797 140070692116288 submission_runner.py:408] Time since start: 7966.69s, 	Step: 22664, 	{'train/accuracy': 0.6608737111091614, 'train/loss': 1.3586628437042236, 'validation/accuracy': 0.6041399836540222, 'validation/loss': 1.640969157218933, 'validation/num_examples': 50000, 'test/accuracy': 0.47350001335144043, 'test/loss': 2.4242477416992188, 'test/num_examples': 10000, 'score': 7683.255749940872, 'total_duration': 7966.688236236572, 'accumulated_submission_time': 7683.255749940872, 'accumulated_eval_time': 282.2081139087677, 'accumulated_logging_time': 0.43669915199279785}
I0202 23:26:49.588701 139907720771328 logging_writer.py:48] [22664] accumulated_eval_time=282.208114, accumulated_logging_time=0.436699, accumulated_submission_time=7683.255750, global_step=22664, preemption_count=0, score=7683.255750, test/accuracy=0.473500, test/loss=2.424248, test/num_examples=10000, total_duration=7966.688236, train/accuracy=0.660874, train/loss=1.358663, validation/accuracy=0.604140, validation/loss=1.640969, validation/num_examples=50000
I0202 23:27:02.068724 139907729164032 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.9722884893417358, loss=1.8103889226913452
I0202 23:27:35.853497 139907720771328 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.5365053415298462, loss=1.7966805696487427
I0202 23:28:09.564795 139907729164032 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.7131459712982178, loss=1.7074791193008423
I0202 23:28:43.311206 139907720771328 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.8065917491912842, loss=1.8526779413223267
I0202 23:29:17.048291 139907729164032 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.9225890636444092, loss=1.8607736825942993
I0202 23:29:50.799980 139907720771328 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.9568333625793457, loss=1.7306278944015503
I0202 23:30:24.469508 139907729164032 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.7922954559326172, loss=1.8606066703796387
I0202 23:30:58.272402 139907720771328 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.5755000114440918, loss=1.6763073205947876
I0202 23:31:31.949150 139907729164032 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.708532691001892, loss=1.741045355796814
I0202 23:32:05.719739 139907720771328 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.7838853597640991, loss=1.824005126953125
I0202 23:32:39.417847 139907729164032 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.7920844554901123, loss=1.8643537759780884
I0202 23:33:13.166671 139907720771328 logging_writer.py:48] [23800] global_step=23800, grad_norm=2.0529422760009766, loss=1.7035623788833618
I0202 23:33:46.964188 139907729164032 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.5165538787841797, loss=1.777693748474121
I0202 23:34:20.711841 139907720771328 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.6111385822296143, loss=1.839794397354126
I0202 23:34:54.359360 139907729164032 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.8123763799667358, loss=1.8353594541549683
I0202 23:35:19.817141 140070692116288 spec.py:321] Evaluating on the training split.
I0202 23:35:26.089036 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 23:35:34.673641 140070692116288 spec.py:349] Evaluating on the test split.
I0202 23:35:37.342463 140070692116288 submission_runner.py:408] Time since start: 8494.46s, 	Step: 24177, 	{'train/accuracy': 0.6926219463348389, 'train/loss': 1.2101314067840576, 'validation/accuracy': 0.6140999794006348, 'validation/loss': 1.6118823289871216, 'validation/num_examples': 50000, 'test/accuracy': 0.4853000342845917, 'test/loss': 2.374408483505249, 'test/num_examples': 10000, 'score': 8193.423243284225, 'total_duration': 8494.462911128998, 'accumulated_submission_time': 8193.423243284225, 'accumulated_eval_time': 299.73340010643005, 'accumulated_logging_time': 0.4670734405517578}
I0202 23:35:37.366553 139907712378624 logging_writer.py:48] [24177] accumulated_eval_time=299.733400, accumulated_logging_time=0.467073, accumulated_submission_time=8193.423243, global_step=24177, preemption_count=0, score=8193.423243, test/accuracy=0.485300, test/loss=2.374408, test/num_examples=10000, total_duration=8494.462911, train/accuracy=0.692622, train/loss=1.210131, validation/accuracy=0.614100, validation/loss=1.611882, validation/num_examples=50000
I0202 23:35:45.444477 139907720771328 logging_writer.py:48] [24200] global_step=24200, grad_norm=2.0666186809539795, loss=1.824512243270874
I0202 23:36:19.076805 139907712378624 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.6487038135528564, loss=1.68207848072052
I0202 23:36:52.763541 139907720771328 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.9203795194625854, loss=1.8112740516662598
I0202 23:37:26.516004 139907712378624 logging_writer.py:48] [24500] global_step=24500, grad_norm=2.240077018737793, loss=1.840286374092102
I0202 23:38:00.236532 139907720771328 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.9322623014450073, loss=1.7477022409439087
I0202 23:38:33.985866 139907712378624 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.9916590452194214, loss=1.6242386102676392
I0202 23:39:07.716625 139907720771328 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.7024149894714355, loss=1.8190464973449707
I0202 23:39:41.373425 139907712378624 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.7892954349517822, loss=1.8281197547912598
I0202 23:40:15.158484 139907720771328 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.6810024976730347, loss=1.8319878578186035
I0202 23:40:48.919350 139907712378624 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.81186044216156, loss=1.7361150979995728
I0202 23:41:22.612130 139907720771328 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.8946855068206787, loss=1.7129909992218018
I0202 23:41:56.395193 139907712378624 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.651742696762085, loss=1.7359305620193481
I0202 23:42:30.107050 139907720771328 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.6751682758331299, loss=1.7566174268722534
I0202 23:43:03.865924 139907712378624 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.8159363269805908, loss=1.7761032581329346
I0202 23:43:37.561319 139907720771328 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.893867015838623, loss=1.7302393913269043
I0202 23:44:07.391166 140070692116288 spec.py:321] Evaluating on the training split.
I0202 23:44:13.695803 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 23:44:22.398881 140070692116288 spec.py:349] Evaluating on the test split.
I0202 23:44:25.317812 140070692116288 submission_runner.py:408] Time since start: 9022.44s, 	Step: 25690, 	{'train/accuracy': 0.6783721446990967, 'train/loss': 1.2624869346618652, 'validation/accuracy': 0.6137799620628357, 'validation/loss': 1.6312508583068848, 'validation/num_examples': 50000, 'test/accuracy': 0.4904000163078308, 'test/loss': 2.407479763031006, 'test/num_examples': 10000, 'score': 8703.385436296463, 'total_duration': 9022.43826174736, 'accumulated_submission_time': 8703.385436296463, 'accumulated_eval_time': 317.66002774238586, 'accumulated_logging_time': 0.5013530254364014}
I0202 23:44:25.340301 139907712378624 logging_writer.py:48] [25690] accumulated_eval_time=317.660028, accumulated_logging_time=0.501353, accumulated_submission_time=8703.385436, global_step=25690, preemption_count=0, score=8703.385436, test/accuracy=0.490400, test/loss=2.407480, test/num_examples=10000, total_duration=9022.438262, train/accuracy=0.678372, train/loss=1.262487, validation/accuracy=0.613780, validation/loss=1.631251, validation/num_examples=50000
I0202 23:44:29.056543 139907729164032 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.8065799474716187, loss=1.6895534992218018
I0202 23:45:02.784198 139907712378624 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.6606104373931885, loss=1.7611265182495117
I0202 23:45:36.500560 139907729164032 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.6681194305419922, loss=1.7444813251495361
I0202 23:46:10.218567 139907712378624 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.8155802488327026, loss=1.8514859676361084
I0202 23:46:43.969845 139907729164032 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.7453831434249878, loss=1.7458319664001465
I0202 23:47:17.766228 139907712378624 logging_writer.py:48] [26200] global_step=26200, grad_norm=2.1130833625793457, loss=1.7098170518875122
I0202 23:47:51.496665 139907729164032 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.724852204322815, loss=1.863269567489624
I0202 23:48:25.207121 139907712378624 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.712519884109497, loss=1.7263457775115967
I0202 23:48:58.942432 139907729164032 logging_writer.py:48] [26500] global_step=26500, grad_norm=2.200429916381836, loss=1.7771573066711426
I0202 23:49:32.652761 139907712378624 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.8209285736083984, loss=1.6839027404785156
I0202 23:50:06.395844 139907729164032 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.7270469665527344, loss=1.605894684791565
I0202 23:50:40.087257 139907712378624 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.6653954982757568, loss=1.7490404844284058
I0202 23:51:13.780590 139907729164032 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.8219677209854126, loss=1.72340989112854
I0202 23:51:47.425469 139907712378624 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.5471166372299194, loss=1.8199666738510132
I0202 23:52:21.134228 139907729164032 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.6975687742233276, loss=1.8152093887329102
I0202 23:52:54.872920 139907712378624 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.9084395170211792, loss=1.7026807069778442
I0202 23:52:55.362720 140070692116288 spec.py:321] Evaluating on the training split.
I0202 23:53:01.676341 140070692116288 spec.py:333] Evaluating on the validation split.
I0202 23:53:10.455560 140070692116288 spec.py:349] Evaluating on the test split.
I0202 23:53:13.846354 140070692116288 submission_runner.py:408] Time since start: 9550.97s, 	Step: 27203, 	{'train/accuracy': 0.6784717440605164, 'train/loss': 1.2687817811965942, 'validation/accuracy': 0.6159200072288513, 'validation/loss': 1.5993962287902832, 'validation/num_examples': 50000, 'test/accuracy': 0.4914000332355499, 'test/loss': 2.33853816986084, 'test/num_examples': 10000, 'score': 9213.344497919083, 'total_duration': 9550.966819286346, 'accumulated_submission_time': 9213.344497919083, 'accumulated_eval_time': 336.1436400413513, 'accumulated_logging_time': 0.5356407165527344}
I0202 23:53:13.864116 139907720771328 logging_writer.py:48] [27203] accumulated_eval_time=336.143640, accumulated_logging_time=0.535641, accumulated_submission_time=9213.344498, global_step=27203, preemption_count=0, score=9213.344498, test/accuracy=0.491400, test/loss=2.338538, test/num_examples=10000, total_duration=9550.966819, train/accuracy=0.678472, train/loss=1.268782, validation/accuracy=0.615920, validation/loss=1.599396, validation/num_examples=50000
I0202 23:53:47.012113 139907745949440 logging_writer.py:48] [27300] global_step=27300, grad_norm=2.1285905838012695, loss=1.814147710800171
I0202 23:54:20.711096 139907720771328 logging_writer.py:48] [27400] global_step=27400, grad_norm=2.0072829723358154, loss=1.681228756904602
I0202 23:54:54.403912 139907745949440 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.6142172813415527, loss=1.5921740531921387
I0202 23:55:28.096720 139907720771328 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.9408044815063477, loss=1.6974387168884277
I0202 23:56:01.768265 139907745949440 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.7397947311401367, loss=1.6662007570266724
I0202 23:56:35.578076 139907720771328 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.8013815879821777, loss=1.7749756574630737
I0202 23:57:09.266522 139907745949440 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.9207558631896973, loss=1.7722939252853394
I0202 23:57:42.980910 139907720771328 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.9608008861541748, loss=1.686128854751587
I0202 23:58:16.708252 139907745949440 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.9152339696884155, loss=1.6957483291625977
I0202 23:58:50.442947 139907720771328 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.5895617008209229, loss=1.7194207906723022
I0202 23:59:24.167032 139907745949440 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.8061925172805786, loss=1.8470439910888672
I0202 23:59:58.001652 139907720771328 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.8829376697540283, loss=1.6657695770263672
I0203 00:00:31.736961 139907745949440 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.6838363409042358, loss=1.7262005805969238
I0203 00:01:05.457541 139907720771328 logging_writer.py:48] [28600] global_step=28600, grad_norm=2.091067314147949, loss=1.7835721969604492
I0203 00:01:39.188398 139907745949440 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.6917158365249634, loss=1.7423068284988403
I0203 00:01:44.071405 140070692116288 spec.py:321] Evaluating on the training split.
I0203 00:01:50.486572 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 00:01:58.995663 140070692116288 spec.py:349] Evaluating on the test split.
I0203 00:02:01.673263 140070692116288 submission_runner.py:408] Time since start: 10078.79s, 	Step: 28716, 	{'train/accuracy': 0.6600964665412903, 'train/loss': 1.350205421447754, 'validation/accuracy': 0.6032999753952026, 'validation/loss': 1.6606773138046265, 'validation/num_examples': 50000, 'test/accuracy': 0.47850000858306885, 'test/loss': 2.4138495922088623, 'test/num_examples': 10000, 'score': 9723.491069793701, 'total_duration': 10078.793704748154, 'accumulated_submission_time': 9723.491069793701, 'accumulated_eval_time': 353.745454788208, 'accumulated_logging_time': 0.562096118927002}
I0203 00:02:01.699850 139907703985920 logging_writer.py:48] [28716] accumulated_eval_time=353.745455, accumulated_logging_time=0.562096, accumulated_submission_time=9723.491070, global_step=28716, preemption_count=0, score=9723.491070, test/accuracy=0.478500, test/loss=2.413850, test/num_examples=10000, total_duration=10078.793705, train/accuracy=0.660096, train/loss=1.350205, validation/accuracy=0.603300, validation/loss=1.660677, validation/num_examples=50000
I0203 00:02:30.369410 139907712378624 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.8803437948226929, loss=1.6678683757781982
I0203 00:03:04.031745 139907703985920 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.950853943824768, loss=1.6056773662567139
I0203 00:03:37.748426 139907712378624 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.9227993488311768, loss=1.7623388767242432
I0203 00:04:11.502093 139907703985920 logging_writer.py:48] [29100] global_step=29100, grad_norm=1.8163923025131226, loss=1.7640982866287231
I0203 00:04:45.245307 139907712378624 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.7019973993301392, loss=1.7514855861663818
I0203 00:05:19.012245 139907703985920 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.9471794366836548, loss=1.8314369916915894
I0203 00:05:52.716583 139907712378624 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.7117726802825928, loss=1.6951085329055786
I0203 00:06:26.481656 139907703985920 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.711497187614441, loss=1.7681620121002197
I0203 00:07:00.306353 139907712378624 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.5945355892181396, loss=1.717577338218689
I0203 00:07:34.038008 139907703985920 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.661529779434204, loss=1.606261134147644
I0203 00:08:07.769262 139907712378624 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.827940821647644, loss=1.8102550506591797
I0203 00:08:41.533343 139907703985920 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.7877475023269653, loss=1.6939783096313477
I0203 00:09:15.245115 139907712378624 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.8509589433670044, loss=1.7170979976654053
I0203 00:09:48.982822 139907703985920 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.8182941675186157, loss=1.7472050189971924
I0203 00:10:22.722194 139907712378624 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.558024525642395, loss=1.8035860061645508
I0203 00:10:31.971250 140070692116288 spec.py:321] Evaluating on the training split.
I0203 00:10:38.335614 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 00:10:46.898107 140070692116288 spec.py:349] Evaluating on the test split.
I0203 00:10:49.554264 140070692116288 submission_runner.py:408] Time since start: 10606.67s, 	Step: 30229, 	{'train/accuracy': 0.6486168503761292, 'train/loss': 1.4096747636795044, 'validation/accuracy': 0.5981199741363525, 'validation/loss': 1.6882946491241455, 'validation/num_examples': 50000, 'test/accuracy': 0.465800017118454, 'test/loss': 2.448777437210083, 'test/num_examples': 10000, 'score': 10233.69608449936, 'total_duration': 10606.674701690674, 'accumulated_submission_time': 10233.69608449936, 'accumulated_eval_time': 371.32841968536377, 'accumulated_logging_time': 0.6031649112701416}
I0203 00:10:49.581177 139907745949440 logging_writer.py:48] [30229] accumulated_eval_time=371.328420, accumulated_logging_time=0.603165, accumulated_submission_time=10233.696084, global_step=30229, preemption_count=0, score=10233.696084, test/accuracy=0.465800, test/loss=2.448777, test/num_examples=10000, total_duration=10606.674702, train/accuracy=0.648617, train/loss=1.409675, validation/accuracy=0.598120, validation/loss=1.688295, validation/num_examples=50000
I0203 00:11:13.853531 139907754342144 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.9816089868545532, loss=1.7419770956039429
I0203 00:11:47.590056 139907745949440 logging_writer.py:48] [30400] global_step=30400, grad_norm=2.020477056503296, loss=1.8951069116592407
I0203 00:12:21.298153 139907754342144 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.5382932424545288, loss=1.7359576225280762
I0203 00:12:55.035377 139907745949440 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.786839246749878, loss=1.6587910652160645
I0203 00:13:28.849157 139907754342144 logging_writer.py:48] [30700] global_step=30700, grad_norm=2.2193398475646973, loss=1.6775438785552979
I0203 00:14:02.537675 139907745949440 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.7192418575286865, loss=1.8114323616027832
I0203 00:14:36.268487 139907754342144 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.6656368970870972, loss=1.585927128791809
I0203 00:15:09.963881 139907745949440 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.6452343463897705, loss=1.6809488534927368
I0203 00:15:43.668375 139907754342144 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.7956784963607788, loss=1.7200343608856201
I0203 00:16:17.339542 139907745949440 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.684075951576233, loss=1.629703402519226
I0203 00:16:51.113822 139907754342144 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.7650244235992432, loss=1.7174638509750366
I0203 00:17:24.859550 139907745949440 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.718581199645996, loss=1.7554643154144287
I0203 00:17:58.591595 139907754342144 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.942476511001587, loss=1.7085604667663574
I0203 00:18:32.264711 139907745949440 logging_writer.py:48] [31600] global_step=31600, grad_norm=2.0569469928741455, loss=1.687524437904358
I0203 00:19:06.010362 139907754342144 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.6489899158477783, loss=1.7027506828308105
I0203 00:19:19.628450 140070692116288 spec.py:321] Evaluating on the training split.
I0203 00:19:25.848711 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 00:19:34.297009 140070692116288 spec.py:349] Evaluating on the test split.
I0203 00:19:36.951627 140070692116288 submission_runner.py:408] Time since start: 11134.07s, 	Step: 31742, 	{'train/accuracy': 0.6831752061843872, 'train/loss': 1.2528233528137207, 'validation/accuracy': 0.6192399859428406, 'validation/loss': 1.6038295030593872, 'validation/num_examples': 50000, 'test/accuracy': 0.4930000305175781, 'test/loss': 2.3522725105285645, 'test/num_examples': 10000, 'score': 10743.68086385727, 'total_duration': 11134.072080612183, 'accumulated_submission_time': 10743.68086385727, 'accumulated_eval_time': 388.6515634059906, 'accumulated_logging_time': 0.6397063732147217}
I0203 00:19:36.974005 139907712378624 logging_writer.py:48] [31742] accumulated_eval_time=388.651563, accumulated_logging_time=0.639706, accumulated_submission_time=10743.680864, global_step=31742, preemption_count=0, score=10743.680864, test/accuracy=0.493000, test/loss=2.352273, test/num_examples=10000, total_duration=11134.072081, train/accuracy=0.683175, train/loss=1.252823, validation/accuracy=0.619240, validation/loss=1.603830, validation/num_examples=50000
I0203 00:19:57.138391 139907720771328 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.7756932973861694, loss=1.6388386487960815
I0203 00:20:30.847685 139907712378624 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.9976078271865845, loss=1.7317081689834595
I0203 00:21:04.513167 139907720771328 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.8200017213821411, loss=1.678473949432373
I0203 00:21:38.235217 139907712378624 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.691232681274414, loss=1.5934799909591675
I0203 00:22:11.995168 139907720771328 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.7112979888916016, loss=1.7469358444213867
I0203 00:22:45.692455 139907712378624 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.9974033832550049, loss=1.6578871011734009
I0203 00:23:19.363476 139907720771328 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.7931989431381226, loss=1.6967166662216187
I0203 00:23:53.068272 139907712378624 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.7345455884933472, loss=1.6142858266830444
I0203 00:24:26.810980 139907720771328 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.6582506895065308, loss=1.7215733528137207
I0203 00:25:00.539212 139907712378624 logging_writer.py:48] [32700] global_step=32700, grad_norm=1.7524865865707397, loss=1.7649427652359009
I0203 00:25:34.235936 139907720771328 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.6962298154830933, loss=1.6865079402923584
I0203 00:26:08.097247 139907712378624 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.9154484272003174, loss=1.668839931488037
I0203 00:26:41.810575 139907720771328 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.7229830026626587, loss=1.7931716442108154
I0203 00:27:15.496548 139907712378624 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.7807140350341797, loss=1.5824146270751953
I0203 00:27:49.236878 139907720771328 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.889487385749817, loss=1.72935950756073
I0203 00:28:07.285300 140070692116288 spec.py:321] Evaluating on the training split.
I0203 00:28:13.680373 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 00:28:22.081799 140070692116288 spec.py:349] Evaluating on the test split.
I0203 00:28:24.742526 140070692116288 submission_runner.py:408] Time since start: 11661.86s, 	Step: 33255, 	{'train/accuracy': 0.7012914419174194, 'train/loss': 1.1544904708862305, 'validation/accuracy': 0.6235600113868713, 'validation/loss': 1.5596383810043335, 'validation/num_examples': 50000, 'test/accuracy': 0.5005000233650208, 'test/loss': 2.294847011566162, 'test/num_examples': 10000, 'score': 11253.931037902832, 'total_duration': 11661.862969398499, 'accumulated_submission_time': 11253.931037902832, 'accumulated_eval_time': 406.1087462902069, 'accumulated_logging_time': 0.6711766719818115}
I0203 00:28:24.765620 139907754342144 logging_writer.py:48] [33255] accumulated_eval_time=406.108746, accumulated_logging_time=0.671177, accumulated_submission_time=11253.931038, global_step=33255, preemption_count=0, score=11253.931038, test/accuracy=0.500500, test/loss=2.294847, test/num_examples=10000, total_duration=11661.862969, train/accuracy=0.701291, train/loss=1.154490, validation/accuracy=0.623560, validation/loss=1.559638, validation/num_examples=50000
I0203 00:28:40.251955 139907762734848 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.5829427242279053, loss=1.715938687324524
I0203 00:29:13.946587 139907754342144 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.9813997745513916, loss=1.6940945386886597
I0203 00:29:47.627021 139907762734848 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.972172737121582, loss=1.6423479318618774
I0203 00:30:21.337625 139907754342144 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.9980698823928833, loss=1.714949131011963
I0203 00:30:55.003762 139907762734848 logging_writer.py:48] [33700] global_step=33700, grad_norm=2.3682305812835693, loss=1.7095727920532227
I0203 00:31:28.702721 139907754342144 logging_writer.py:48] [33800] global_step=33800, grad_norm=2.184164047241211, loss=1.7745670080184937
I0203 00:32:02.330497 139907762734848 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.7026015520095825, loss=1.690517783164978
I0203 00:32:36.065790 139907754342144 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.9835125207901, loss=1.795644998550415
I0203 00:33:09.910669 139907762734848 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.9466674327850342, loss=1.6334890127182007
I0203 00:33:43.616340 139907754342144 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.7426947355270386, loss=1.6473472118377686
I0203 00:34:17.286462 139907762734848 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.8379753828048706, loss=1.6678575277328491
I0203 00:34:50.995331 139907754342144 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.7813626527786255, loss=1.5745863914489746
I0203 00:35:24.662116 139907762734848 logging_writer.py:48] [34500] global_step=34500, grad_norm=2.3381099700927734, loss=1.7938600778579712
I0203 00:35:58.387367 139907754342144 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.691174864768982, loss=1.7020920515060425
I0203 00:36:32.107877 139907762734848 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.7331706285476685, loss=1.7099132537841797
I0203 00:36:54.841273 140070692116288 spec.py:321] Evaluating on the training split.
I0203 00:37:01.156864 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 00:37:09.671175 140070692116288 spec.py:349] Evaluating on the test split.
I0203 00:37:12.420094 140070692116288 submission_runner.py:408] Time since start: 12189.54s, 	Step: 34769, 	{'train/accuracy': 0.7081871628761292, 'train/loss': 1.1398144960403442, 'validation/accuracy': 0.6377599835395813, 'validation/loss': 1.4949623346328735, 'validation/num_examples': 50000, 'test/accuracy': 0.5094000101089478, 'test/loss': 2.2475671768188477, 'test/num_examples': 10000, 'score': 11763.943603754044, 'total_duration': 12189.540555000305, 'accumulated_submission_time': 11763.943603754044, 'accumulated_eval_time': 423.68754959106445, 'accumulated_logging_time': 0.7041482925415039}
I0203 00:37:12.439577 139907720771328 logging_writer.py:48] [34769] accumulated_eval_time=423.687550, accumulated_logging_time=0.704148, accumulated_submission_time=11763.943604, global_step=34769, preemption_count=0, score=11763.943604, test/accuracy=0.509400, test/loss=2.247567, test/num_examples=10000, total_duration=12189.540555, train/accuracy=0.708187, train/loss=1.139814, validation/accuracy=0.637760, validation/loss=1.494962, validation/num_examples=50000
I0203 00:37:23.228549 139907729164032 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.6709613800048828, loss=1.6761701107025146
I0203 00:37:56.924349 139907720771328 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.9977588653564453, loss=1.6897318363189697
I0203 00:38:30.615144 139907729164032 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.815692663192749, loss=1.7685635089874268
I0203 00:39:04.398642 139907720771328 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.8176653385162354, loss=1.6460494995117188
I0203 00:39:38.357197 139907729164032 logging_writer.py:48] [35200] global_step=35200, grad_norm=2.064175605773926, loss=1.7424155473709106
I0203 00:40:12.062053 139907720771328 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.714207410812378, loss=1.5077203512191772
I0203 00:40:45.760751 139907729164032 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.6866655349731445, loss=1.5287894010543823
I0203 00:41:19.538195 139907720771328 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.7623240947723389, loss=1.743139386177063
I0203 00:41:53.228071 139907729164032 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.808087944984436, loss=1.7748562097549438
I0203 00:42:26.891476 139907720771328 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.674805760383606, loss=1.759834885597229
I0203 00:43:00.533136 139907729164032 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.7526427507400513, loss=1.6637709140777588
I0203 00:43:34.210164 139907720771328 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.8191088438034058, loss=1.6648128032684326
I0203 00:44:07.960386 139907729164032 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.6292271614074707, loss=1.669835090637207
I0203 00:44:41.750563 139907720771328 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.9347096681594849, loss=1.7244518995285034
I0203 00:45:15.457086 139907729164032 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.9181818962097168, loss=1.670349359512329
I0203 00:45:42.544143 140070692116288 spec.py:321] Evaluating on the training split.
I0203 00:45:48.807190 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 00:45:57.258963 140070692116288 spec.py:349] Evaluating on the test split.
I0203 00:45:59.906307 140070692116288 submission_runner.py:408] Time since start: 12717.03s, 	Step: 36281, 	{'train/accuracy': 0.6898915767669678, 'train/loss': 1.230826497077942, 'validation/accuracy': 0.6264599561691284, 'validation/loss': 1.5518196821212769, 'validation/num_examples': 50000, 'test/accuracy': 0.49080002307891846, 'test/loss': 2.300273895263672, 'test/num_examples': 10000, 'score': 12273.987482309341, 'total_duration': 12717.026733636856, 'accumulated_submission_time': 12273.987482309341, 'accumulated_eval_time': 441.04963994026184, 'accumulated_logging_time': 0.7320287227630615}
I0203 00:45:59.930067 139907712378624 logging_writer.py:48] [36281] accumulated_eval_time=441.049640, accumulated_logging_time=0.732029, accumulated_submission_time=12273.987482, global_step=36281, preemption_count=0, score=12273.987482, test/accuracy=0.490800, test/loss=2.300274, test/num_examples=10000, total_duration=12717.026734, train/accuracy=0.689892, train/loss=1.230826, validation/accuracy=0.626460, validation/loss=1.551820, validation/num_examples=50000
I0203 00:46:06.698883 139907745949440 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.9045040607452393, loss=1.7409987449645996
I0203 00:46:40.380362 139907712378624 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.932259202003479, loss=1.6957550048828125
I0203 00:47:14.082112 139907745949440 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.733728289604187, loss=1.5996893644332886
I0203 00:47:47.736978 139907712378624 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.63228178024292, loss=1.6671168804168701
I0203 00:48:21.436083 139907745949440 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.79740309715271, loss=1.6428923606872559
I0203 00:48:55.098449 139907712378624 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.6143227815628052, loss=1.6237726211547852
I0203 00:49:28.810025 139907745949440 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.8395411968231201, loss=1.8233201503753662
I0203 00:50:02.477563 139907712378624 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.9570459127426147, loss=1.65667724609375
I0203 00:50:36.191263 139907745949440 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.8621457815170288, loss=1.654207706451416
I0203 00:51:09.846218 139907712378624 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.8653125762939453, loss=1.7495359182357788
I0203 00:51:43.567383 139907745949440 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.869259238243103, loss=1.6040749549865723
I0203 00:52:17.370238 139907712378624 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.9348372220993042, loss=1.626570701599121
I0203 00:52:51.146992 139907745949440 logging_writer.py:48] [37500] global_step=37500, grad_norm=2.110049247741699, loss=1.6673624515533447
I0203 00:53:24.816023 139907712378624 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.8799892663955688, loss=1.8321667909622192
I0203 00:53:58.568549 139907745949440 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.681558609008789, loss=1.6448500156402588
I0203 00:54:30.050477 140070692116288 spec.py:321] Evaluating on the training split.
I0203 00:54:36.569542 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 00:54:45.136585 140070692116288 spec.py:349] Evaluating on the test split.
I0203 00:54:47.745019 140070692116288 submission_runner.py:408] Time since start: 13244.87s, 	Step: 37795, 	{'train/accuracy': 0.6783322691917419, 'train/loss': 1.2680907249450684, 'validation/accuracy': 0.6167399883270264, 'validation/loss': 1.5898174047470093, 'validation/num_examples': 50000, 'test/accuracy': 0.4897000193595886, 'test/loss': 2.340038299560547, 'test/num_examples': 10000, 'score': 12784.045414686203, 'total_duration': 13244.865463733673, 'accumulated_submission_time': 12784.045414686203, 'accumulated_eval_time': 458.7441716194153, 'accumulated_logging_time': 0.7648324966430664}
I0203 00:54:47.768155 139907729164032 logging_writer.py:48] [37795] accumulated_eval_time=458.744172, accumulated_logging_time=0.764832, accumulated_submission_time=12784.045415, global_step=37795, preemption_count=0, score=12784.045415, test/accuracy=0.489700, test/loss=2.340038, test/num_examples=10000, total_duration=13244.865464, train/accuracy=0.678332, train/loss=1.268091, validation/accuracy=0.616740, validation/loss=1.589817, validation/num_examples=50000
I0203 00:54:49.804475 139907762734848 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.7269320487976074, loss=1.646624207496643
I0203 00:55:23.527109 139907729164032 logging_writer.py:48] [37900] global_step=37900, grad_norm=2.040313720703125, loss=1.719547986984253
I0203 00:55:57.235547 139907762734848 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.7717540264129639, loss=1.684169054031372
I0203 00:56:30.904847 139907729164032 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.806313395500183, loss=1.648329257965088
I0203 00:57:04.690996 139907762734848 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.8871691226959229, loss=1.6707193851470947
I0203 00:57:38.401287 139907729164032 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.8179086446762085, loss=1.6577341556549072
I0203 00:58:12.106784 139907762734848 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.8214645385742188, loss=1.6979631185531616
I0203 00:58:45.771706 139907729164032 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.849234938621521, loss=1.6570196151733398
I0203 00:59:19.575328 139907762734848 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.8531962633132935, loss=1.719294548034668
I0203 00:59:53.312047 139907729164032 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.9186149835586548, loss=1.8154668807983398
I0203 01:00:27.037443 139907762734848 logging_writer.py:48] [38800] global_step=38800, grad_norm=2.006192207336426, loss=1.6638929843902588
I0203 01:01:00.718419 139907729164032 logging_writer.py:48] [38900] global_step=38900, grad_norm=2.035308599472046, loss=1.6968283653259277
I0203 01:01:34.506674 139907762734848 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.757423758506775, loss=1.6138256788253784
I0203 01:02:08.212284 139907729164032 logging_writer.py:48] [39100] global_step=39100, grad_norm=2.1276111602783203, loss=1.6888718605041504
I0203 01:02:41.972660 139907762734848 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.771915316581726, loss=1.7065718173980713
I0203 01:03:15.631791 139907729164032 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.676081657409668, loss=1.5804705619812012
I0203 01:03:17.787555 140070692116288 spec.py:321] Evaluating on the training split.
I0203 01:03:24.051602 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 01:03:32.717643 140070692116288 spec.py:349] Evaluating on the test split.
I0203 01:03:35.458698 140070692116288 submission_runner.py:408] Time since start: 13772.58s, 	Step: 39308, 	{'train/accuracy': 0.6907684803009033, 'train/loss': 1.2177668809890747, 'validation/accuracy': 0.6317999958992004, 'validation/loss': 1.5191069841384888, 'validation/num_examples': 50000, 'test/accuracy': 0.499500036239624, 'test/loss': 2.2920279502868652, 'test/num_examples': 10000, 'score': 13294.002183437347, 'total_duration': 13772.57915186882, 'accumulated_submission_time': 13294.002183437347, 'accumulated_eval_time': 476.41528153419495, 'accumulated_logging_time': 0.797590970993042}
I0203 01:03:35.482442 139907712378624 logging_writer.py:48] [39308] accumulated_eval_time=476.415282, accumulated_logging_time=0.797591, accumulated_submission_time=13294.002183, global_step=39308, preemption_count=0, score=13294.002183, test/accuracy=0.499500, test/loss=2.292028, test/num_examples=10000, total_duration=13772.579152, train/accuracy=0.690768, train/loss=1.217767, validation/accuracy=0.631800, validation/loss=1.519107, validation/num_examples=50000
I0203 01:04:06.757897 139907720771328 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.7650855779647827, loss=1.519935131072998
I0203 01:04:40.439895 139907712378624 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.9275636672973633, loss=1.728313684463501
I0203 01:05:14.142986 139907720771328 logging_writer.py:48] [39600] global_step=39600, grad_norm=2.0409817695617676, loss=1.631147861480713
I0203 01:05:47.934996 139907712378624 logging_writer.py:48] [39700] global_step=39700, grad_norm=2.0428414344787598, loss=1.7059526443481445
I0203 01:06:21.637961 139907720771328 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.6991980075836182, loss=1.7656035423278809
I0203 01:06:55.296351 139907712378624 logging_writer.py:48] [39900] global_step=39900, grad_norm=2.144153356552124, loss=1.6218608617782593
I0203 01:07:29.005902 139907720771328 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.8436968326568604, loss=1.6815683841705322
I0203 01:08:02.683878 139907712378624 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.8472297191619873, loss=1.7303420305252075
I0203 01:08:36.378486 139907720771328 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.7615032196044922, loss=1.6771464347839355
I0203 01:09:10.053302 139907712378624 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.8897749185562134, loss=1.7629350423812866
I0203 01:09:43.754131 139907720771328 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.912537932395935, loss=1.6100059747695923
I0203 01:10:17.434986 139907712378624 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.792880892753601, loss=1.7174437046051025
I0203 01:10:51.191230 139907720771328 logging_writer.py:48] [40600] global_step=40600, grad_norm=2.0915181636810303, loss=1.6396777629852295
I0203 01:11:24.852990 139907712378624 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.5646239519119263, loss=1.6510112285614014
I0203 01:11:58.541465 139907720771328 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.8656586408615112, loss=1.670059323310852
I0203 01:12:05.524346 140070692116288 spec.py:321] Evaluating on the training split.
I0203 01:12:11.862879 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 01:12:20.231009 140070692116288 spec.py:349] Evaluating on the test split.
I0203 01:12:22.882115 140070692116288 submission_runner.py:408] Time since start: 14300.00s, 	Step: 40822, 	{'train/accuracy': 0.7167769074440002, 'train/loss': 1.1003592014312744, 'validation/accuracy': 0.6209999918937683, 'validation/loss': 1.567124843597412, 'validation/num_examples': 50000, 'test/accuracy': 0.49880000948905945, 'test/loss': 2.3107590675354004, 'test/num_examples': 10000, 'score': 13803.981996297836, 'total_duration': 14300.002563476562, 'accumulated_submission_time': 13803.981996297836, 'accumulated_eval_time': 493.7730107307434, 'accumulated_logging_time': 0.8306655883789062}
I0203 01:12:22.907085 139907745949440 logging_writer.py:48] [40822] accumulated_eval_time=493.773011, accumulated_logging_time=0.830666, accumulated_submission_time=13803.981996, global_step=40822, preemption_count=0, score=13803.981996, test/accuracy=0.498800, test/loss=2.310759, test/num_examples=10000, total_duration=14300.002563, train/accuracy=0.716777, train/loss=1.100359, validation/accuracy=0.621000, validation/loss=1.567125, validation/num_examples=50000
I0203 01:12:49.540859 139907754342144 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.8210550546646118, loss=1.7618833780288696
I0203 01:13:23.224947 139907745949440 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.80802583694458, loss=1.5593318939208984
I0203 01:13:56.940413 139907754342144 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.7837296724319458, loss=1.5389424562454224
I0203 01:14:30.654442 139907745949440 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.7864497900009155, loss=1.7311168909072876
I0203 01:15:04.400902 139907754342144 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.8275117874145508, loss=1.712438941001892
I0203 01:15:38.095949 139907745949440 logging_writer.py:48] [41400] global_step=41400, grad_norm=2.0320558547973633, loss=1.6232069730758667
I0203 01:16:11.812713 139907754342144 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.8611117601394653, loss=1.6030946969985962
I0203 01:16:45.534009 139907745949440 logging_writer.py:48] [41600] global_step=41600, grad_norm=2.1992783546447754, loss=1.7063350677490234
I0203 01:17:19.236838 139907754342144 logging_writer.py:48] [41700] global_step=41700, grad_norm=2.047574281692505, loss=1.7468235492706299
I0203 01:17:52.994431 139907745949440 logging_writer.py:48] [41800] global_step=41800, grad_norm=2.088770866394043, loss=1.5929569005966187
I0203 01:18:26.774430 139907754342144 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.72879159450531, loss=1.5568506717681885
I0203 01:19:00.518634 139907745949440 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.8584827184677124, loss=1.7335171699523926
I0203 01:19:34.231743 139907754342144 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.8064501285552979, loss=1.7175195217132568
I0203 01:20:07.911138 139907745949440 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.9432637691497803, loss=1.703352928161621
I0203 01:20:41.673494 139907754342144 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.822775959968567, loss=1.5013898611068726
I0203 01:20:52.936976 140070692116288 spec.py:321] Evaluating on the training split.
I0203 01:20:59.286281 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 01:21:07.985829 140070692116288 spec.py:349] Evaluating on the test split.
I0203 01:21:10.637214 140070692116288 submission_runner.py:408] Time since start: 14827.76s, 	Step: 42335, 	{'train/accuracy': 0.7200653553009033, 'train/loss': 1.0819741487503052, 'validation/accuracy': 0.6363199949264526, 'validation/loss': 1.509085774421692, 'validation/num_examples': 50000, 'test/accuracy': 0.5099000334739685, 'test/loss': 2.241546869277954, 'test/num_examples': 10000, 'score': 14313.949091911316, 'total_duration': 14827.75766301155, 'accumulated_submission_time': 14313.949091911316, 'accumulated_eval_time': 511.47321367263794, 'accumulated_logging_time': 0.8653779029846191}
I0203 01:21:10.664517 139907712378624 logging_writer.py:48] [42335] accumulated_eval_time=511.473214, accumulated_logging_time=0.865378, accumulated_submission_time=14313.949092, global_step=42335, preemption_count=0, score=14313.949092, test/accuracy=0.509900, test/loss=2.241547, test/num_examples=10000, total_duration=14827.757663, train/accuracy=0.720065, train/loss=1.081974, validation/accuracy=0.636320, validation/loss=1.509086, validation/num_examples=50000
I0203 01:21:32.924982 139907720771328 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.752187728881836, loss=1.6163699626922607
I0203 01:22:06.645523 139907712378624 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.6744213104248047, loss=1.6460397243499756
I0203 01:22:40.776401 139907720771328 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.6870825290679932, loss=1.5934879779815674
I0203 01:23:14.492374 139907712378624 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.8319368362426758, loss=1.5426876544952393
I0203 01:23:48.239449 139907720771328 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.733533501625061, loss=1.6920006275177002
I0203 01:24:21.992533 139907712378624 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.9702696800231934, loss=1.632175087928772
I0203 01:24:55.645188 139907720771328 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.8487074375152588, loss=1.6921087503433228
I0203 01:25:29.453849 139907712378624 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.8197026252746582, loss=1.625688910484314
I0203 01:26:03.132740 139907720771328 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.743069052696228, loss=1.5986040830612183
I0203 01:26:36.828809 139907712378624 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.88955557346344, loss=1.6365671157836914
I0203 01:27:10.505544 139907720771328 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.770122766494751, loss=1.6339812278747559
I0203 01:27:44.190102 139907712378624 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.9159433841705322, loss=1.5933042764663696
I0203 01:28:17.925261 139907720771328 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.8792132139205933, loss=1.5845553874969482
I0203 01:28:51.616115 139907712378624 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.7285422086715698, loss=1.5065768957138062
I0203 01:29:25.297003 139907720771328 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.673173427581787, loss=1.6155592203140259
I0203 01:29:40.928036 140070692116288 spec.py:321] Evaluating on the training split.
I0203 01:29:47.288204 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 01:29:55.759483 140070692116288 spec.py:349] Evaluating on the test split.
I0203 01:29:58.415875 140070692116288 submission_runner.py:408] Time since start: 15355.54s, 	Step: 43848, 	{'train/accuracy': 0.6968072056770325, 'train/loss': 1.1697343587875366, 'validation/accuracy': 0.6294599771499634, 'validation/loss': 1.5310777425765991, 'validation/num_examples': 50000, 'test/accuracy': 0.5049000382423401, 'test/loss': 2.2731704711914062, 'test/num_examples': 10000, 'score': 14824.149604320526, 'total_duration': 15355.536323785782, 'accumulated_submission_time': 14824.149604320526, 'accumulated_eval_time': 528.9610199928284, 'accumulated_logging_time': 0.9021854400634766}
I0203 01:29:58.441241 139907745949440 logging_writer.py:48] [43848] accumulated_eval_time=528.961020, accumulated_logging_time=0.902185, accumulated_submission_time=14824.149604, global_step=43848, preemption_count=0, score=14824.149604, test/accuracy=0.504900, test/loss=2.273170, test/num_examples=10000, total_duration=15355.536324, train/accuracy=0.696807, train/loss=1.169734, validation/accuracy=0.629460, validation/loss=1.531078, validation/num_examples=50000
I0203 01:30:16.291392 139907754342144 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.7858333587646484, loss=1.600409984588623
I0203 01:30:49.984522 139907745949440 logging_writer.py:48] [44000] global_step=44000, grad_norm=2.2092785835266113, loss=1.7156437635421753
I0203 01:31:23.659086 139907754342144 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.9468295574188232, loss=1.614954948425293
I0203 01:31:57.482791 139907745949440 logging_writer.py:48] [44200] global_step=44200, grad_norm=2.2584919929504395, loss=1.6611430644989014
I0203 01:32:31.146294 139907754342144 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.8851749897003174, loss=1.6013803482055664
I0203 01:33:04.852501 139907745949440 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.7779322862625122, loss=1.6293524503707886
I0203 01:33:38.505220 139907754342144 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.896519660949707, loss=1.7115899324417114
I0203 01:34:12.192969 139907745949440 logging_writer.py:48] [44600] global_step=44600, grad_norm=2.017766237258911, loss=1.7122821807861328
I0203 01:34:45.915176 139907754342144 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.8992139101028442, loss=1.7170664072036743
I0203 01:35:19.662201 139907745949440 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.7080128192901611, loss=1.5393155813217163
I0203 01:35:53.340753 139907754342144 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.9169220924377441, loss=1.6025140285491943
I0203 01:36:27.030791 139907745949440 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.917183756828308, loss=1.7556538581848145
I0203 01:37:00.687898 139907754342144 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.9681658744812012, loss=1.5587228536605835
I0203 01:37:34.395435 139907745949440 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.8274002075195312, loss=1.5922690629959106
I0203 01:38:08.148356 139907754342144 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.821337103843689, loss=1.6753008365631104
I0203 01:38:28.530366 140070692116288 spec.py:321] Evaluating on the training split.
I0203 01:38:34.977962 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 01:38:43.317723 140070692116288 spec.py:349] Evaluating on the test split.
I0203 01:38:46.029659 140070692116288 submission_runner.py:408] Time since start: 15883.15s, 	Step: 45362, 	{'train/accuracy': 0.7052175998687744, 'train/loss': 1.1465650796890259, 'validation/accuracy': 0.6387799978256226, 'validation/loss': 1.4946125745773315, 'validation/num_examples': 50000, 'test/accuracy': 0.5038000345230103, 'test/loss': 2.2749900817871094, 'test/num_examples': 10000, 'score': 15334.175417423248, 'total_duration': 15883.150108098984, 'accumulated_submission_time': 15334.175417423248, 'accumulated_eval_time': 546.4602868556976, 'accumulated_logging_time': 0.9373815059661865}
I0203 01:38:46.058868 139907720771328 logging_writer.py:48] [45362] accumulated_eval_time=546.460287, accumulated_logging_time=0.937382, accumulated_submission_time=15334.175417, global_step=45362, preemption_count=0, score=15334.175417, test/accuracy=0.503800, test/loss=2.274990, test/num_examples=10000, total_duration=15883.150108, train/accuracy=0.705218, train/loss=1.146565, validation/accuracy=0.638780, validation/loss=1.494613, validation/num_examples=50000
I0203 01:38:59.169801 139907729164032 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.9377245903015137, loss=1.6632007360458374
I0203 01:39:32.881986 139907720771328 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.8185310363769531, loss=1.6614595651626587
I0203 01:40:06.600694 139907729164032 logging_writer.py:48] [45600] global_step=45600, grad_norm=2.3335354328155518, loss=1.6819339990615845
I0203 01:40:40.272369 139907720771328 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.8816325664520264, loss=1.7115013599395752
I0203 01:41:13.962937 139907729164032 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.9770312309265137, loss=1.6423945426940918
I0203 01:41:47.653833 139907720771328 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.8405834436416626, loss=1.5851688385009766
I0203 01:42:21.339416 139907729164032 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.8904389142990112, loss=1.6542346477508545
I0203 01:42:55.003480 139907720771328 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.9229873418807983, loss=1.7015498876571655
I0203 01:43:28.697323 139907729164032 logging_writer.py:48] [46200] global_step=46200, grad_norm=2.008082628250122, loss=1.6486730575561523
I0203 01:44:02.347382 139907720771328 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.9870725870132446, loss=1.4989124536514282
I0203 01:44:36.130109 139907729164032 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.813156008720398, loss=1.728649616241455
I0203 01:45:09.861384 139907720771328 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.7466479539871216, loss=1.5692888498306274
I0203 01:45:43.558752 139907729164032 logging_writer.py:48] [46600] global_step=46600, grad_norm=2.0227816104888916, loss=1.6657239198684692
I0203 01:46:17.232916 139907720771328 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.8989914655685425, loss=1.5760042667388916
I0203 01:46:50.916167 139907729164032 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.9493197202682495, loss=1.7065303325653076
I0203 01:47:16.299222 140070692116288 spec.py:321] Evaluating on the training split.
I0203 01:47:22.654727 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 01:47:31.328134 140070692116288 spec.py:349] Evaluating on the test split.
I0203 01:47:34.052085 140070692116288 submission_runner.py:408] Time since start: 16411.17s, 	Step: 46877, 	{'train/accuracy': 0.700613796710968, 'train/loss': 1.157369613647461, 'validation/accuracy': 0.6425999999046326, 'validation/loss': 1.466826319694519, 'validation/num_examples': 50000, 'test/accuracy': 0.515500009059906, 'test/loss': 2.1804099082946777, 'test/num_examples': 10000, 'score': 15844.352715015411, 'total_duration': 16411.172538280487, 'accumulated_submission_time': 15844.352715015411, 'accumulated_eval_time': 564.2131533622742, 'accumulated_logging_time': 0.9761536121368408}
I0203 01:47:34.077863 139907712378624 logging_writer.py:48] [46877] accumulated_eval_time=564.213153, accumulated_logging_time=0.976154, accumulated_submission_time=15844.352715, global_step=46877, preemption_count=0, score=15844.352715, test/accuracy=0.515500, test/loss=2.180410, test/num_examples=10000, total_duration=16411.172538, train/accuracy=0.700614, train/loss=1.157370, validation/accuracy=0.642600, validation/loss=1.466826, validation/num_examples=50000
I0203 01:47:42.177052 139907720771328 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.9878696203231812, loss=1.661742925643921
I0203 01:48:15.887244 139907712378624 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.7584691047668457, loss=1.5527682304382324
I0203 01:48:49.585482 139907720771328 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.8709964752197266, loss=1.5514572858810425
I0203 01:49:23.285277 139907712378624 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.6841340065002441, loss=1.5670069456100464
I0203 01:49:57.047575 139907720771328 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.8746248483657837, loss=1.6293702125549316
I0203 01:50:30.722143 139907712378624 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.8208117485046387, loss=1.5312161445617676
I0203 01:51:04.546859 139907720771328 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.9656537771224976, loss=1.5790338516235352
I0203 01:51:38.288018 139907712378624 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.994865894317627, loss=1.7512308359146118
I0203 01:52:12.030587 139907720771328 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.8732658624649048, loss=1.5595016479492188
I0203 01:52:45.680523 139907712378624 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.8329057693481445, loss=1.625848412513733
I0203 01:53:19.442828 139907720771328 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.9697141647338867, loss=1.5594449043273926
I0203 01:53:53.124490 139907712378624 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.7172523736953735, loss=1.5801162719726562
I0203 01:54:26.826993 139907720771328 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.7696821689605713, loss=1.5590646266937256
I0203 01:55:00.557760 139907712378624 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.7425508499145508, loss=1.704907774925232
I0203 01:55:34.283851 139907720771328 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.8215415477752686, loss=1.6761548519134521
I0203 01:56:04.102320 140070692116288 spec.py:321] Evaluating on the training split.
I0203 01:56:10.518211 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 01:56:18.883594 140070692116288 spec.py:349] Evaluating on the test split.
I0203 01:56:21.549406 140070692116288 submission_runner.py:408] Time since start: 16938.67s, 	Step: 48390, 	{'train/accuracy': 0.7057557106018066, 'train/loss': 1.150970220565796, 'validation/accuracy': 0.6429199576377869, 'validation/loss': 1.46943998336792, 'validation/num_examples': 50000, 'test/accuracy': 0.5108000040054321, 'test/loss': 2.2163541316986084, 'test/num_examples': 10000, 'score': 16354.314910888672, 'total_duration': 16938.669855833054, 'accumulated_submission_time': 16354.314910888672, 'accumulated_eval_time': 581.6602036952972, 'accumulated_logging_time': 1.0123159885406494}
I0203 01:56:21.575589 139907737556736 logging_writer.py:48] [48390] accumulated_eval_time=581.660204, accumulated_logging_time=1.012316, accumulated_submission_time=16354.314911, global_step=48390, preemption_count=0, score=16354.314911, test/accuracy=0.510800, test/loss=2.216354, test/num_examples=10000, total_duration=16938.669856, train/accuracy=0.705756, train/loss=1.150970, validation/accuracy=0.642920, validation/loss=1.469440, validation/num_examples=50000
I0203 01:56:25.296497 139907754342144 logging_writer.py:48] [48400] global_step=48400, grad_norm=2.0912294387817383, loss=1.6462056636810303
I0203 01:56:59.002447 139907737556736 logging_writer.py:48] [48500] global_step=48500, grad_norm=2.3014774322509766, loss=1.660266637802124
I0203 01:57:32.673405 139907754342144 logging_writer.py:48] [48600] global_step=48600, grad_norm=2.064088821411133, loss=1.6966643333435059
I0203 01:58:06.492567 139907737556736 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.974799394607544, loss=1.5606129169464111
I0203 01:58:40.170179 139907754342144 logging_writer.py:48] [48800] global_step=48800, grad_norm=2.022324562072754, loss=1.7577698230743408
I0203 01:59:13.874770 139907737556736 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.7423090934753418, loss=1.5210682153701782
I0203 01:59:47.650739 139907754342144 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.7215715646743774, loss=1.5965696573257446
I0203 02:00:21.352552 139907737556736 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.9202958345413208, loss=1.6170984506607056
I0203 02:00:55.013815 139907754342144 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.8846590518951416, loss=1.5968973636627197
I0203 02:01:28.707377 139907737556736 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.8654725551605225, loss=1.5248082876205444
I0203 02:02:02.469563 139907754342144 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.962890625, loss=1.6790716648101807
I0203 02:02:36.178654 139907737556736 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.7092280387878418, loss=1.5354528427124023
I0203 02:03:09.856814 139907754342144 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.7408198118209839, loss=1.558493733406067
I0203 02:03:43.588303 139907737556736 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.9753730297088623, loss=1.5810964107513428
I0203 02:04:17.314785 139907754342144 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.9284089803695679, loss=1.4738574028015137
I0203 02:04:50.950427 139907737556736 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.6144380569458008, loss=1.5384825468063354
I0203 02:04:51.780527 140070692116288 spec.py:321] Evaluating on the training split.
I0203 02:04:58.083760 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 02:05:06.609230 140070692116288 spec.py:349] Evaluating on the test split.
I0203 02:05:09.291471 140070692116288 submission_runner.py:408] Time since start: 17466.41s, 	Step: 49904, 	{'train/accuracy': 0.758230984210968, 'train/loss': 0.9265372157096863, 'validation/accuracy': 0.6476399898529053, 'validation/loss': 1.4463788270950317, 'validation/num_examples': 50000, 'test/accuracy': 0.513700008392334, 'test/loss': 2.200896739959717, 'test/num_examples': 10000, 'score': 16864.458737134933, 'total_duration': 17466.411892175674, 'accumulated_submission_time': 16864.458737134933, 'accumulated_eval_time': 599.1710863113403, 'accumulated_logging_time': 1.0477879047393799}
I0203 02:05:09.323961 139907251042048 logging_writer.py:48] [49904] accumulated_eval_time=599.171086, accumulated_logging_time=1.047788, accumulated_submission_time=16864.458737, global_step=49904, preemption_count=0, score=16864.458737, test/accuracy=0.513700, test/loss=2.200897, test/num_examples=10000, total_duration=17466.411892, train/accuracy=0.758231, train/loss=0.926537, validation/accuracy=0.647640, validation/loss=1.446379, validation/num_examples=50000
I0203 02:05:42.067438 139907703985920 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.9791885614395142, loss=1.6000027656555176
I0203 02:06:15.734570 139907251042048 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.8252876996994019, loss=1.580681562423706
I0203 02:06:49.439503 139907703985920 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.977886438369751, loss=1.6292227506637573
I0203 02:07:23.166969 139907251042048 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.9548805952072144, loss=1.5437541007995605
I0203 02:07:56.876709 139907703985920 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.9951952695846558, loss=1.6914894580841064
I0203 02:08:30.614200 139907251042048 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.689206838607788, loss=1.5235086679458618
I0203 02:09:04.338587 139907703985920 logging_writer.py:48] [50600] global_step=50600, grad_norm=2.4966251850128174, loss=1.672996163368225
I0203 02:09:38.013590 139907251042048 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.9269633293151855, loss=1.5563302040100098
I0203 02:10:11.696371 139907703985920 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.863495945930481, loss=1.6708636283874512
I0203 02:10:45.429699 139907251042048 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.9613614082336426, loss=1.6925384998321533
I0203 02:11:19.277493 139907703985920 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.942669153213501, loss=1.6951032876968384
I0203 02:11:52.968571 139907251042048 logging_writer.py:48] [51100] global_step=51100, grad_norm=2.0216875076293945, loss=1.6375012397766113
I0203 02:12:26.736990 139907703985920 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.866542935371399, loss=1.5484853982925415
I0203 02:13:00.413376 139907251042048 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.903737187385559, loss=1.6562812328338623
I0203 02:13:34.098560 139907703985920 logging_writer.py:48] [51400] global_step=51400, grad_norm=2.2921745777130127, loss=1.5872384309768677
I0203 02:13:39.646044 140070692116288 spec.py:321] Evaluating on the training split.
I0203 02:13:46.001727 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 02:13:54.410906 140070692116288 spec.py:349] Evaluating on the test split.
I0203 02:13:57.102443 140070692116288 submission_runner.py:408] Time since start: 17994.22s, 	Step: 51418, 	{'train/accuracy': 0.7257453799247742, 'train/loss': 1.053958535194397, 'validation/accuracy': 0.6452999711036682, 'validation/loss': 1.461386799812317, 'validation/num_examples': 50000, 'test/accuracy': 0.515500009059906, 'test/loss': 2.1797661781311035, 'test/num_examples': 10000, 'score': 17374.71946334839, 'total_duration': 17994.22289633751, 'accumulated_submission_time': 17374.71946334839, 'accumulated_eval_time': 616.627453327179, 'accumulated_logging_time': 1.0893511772155762}
I0203 02:13:57.132485 139907745949440 logging_writer.py:48] [51418] accumulated_eval_time=616.627453, accumulated_logging_time=1.089351, accumulated_submission_time=17374.719463, global_step=51418, preemption_count=0, score=17374.719463, test/accuracy=0.515500, test/loss=2.179766, test/num_examples=10000, total_duration=17994.222896, train/accuracy=0.725745, train/loss=1.053959, validation/accuracy=0.645300, validation/loss=1.461387, validation/num_examples=50000
I0203 02:14:25.092903 139907754342144 logging_writer.py:48] [51500] global_step=51500, grad_norm=2.019592523574829, loss=1.6755397319793701
I0203 02:14:58.775388 139907745949440 logging_writer.py:48] [51600] global_step=51600, grad_norm=2.0150840282440186, loss=1.541551113128662
I0203 02:15:32.467387 139907754342144 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.8903003931045532, loss=1.6834267377853394
I0203 02:16:06.137763 139907745949440 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.7880806922912598, loss=1.6276133060455322
I0203 02:16:39.779842 139907754342144 logging_writer.py:48] [51900] global_step=51900, grad_norm=2.111941337585449, loss=1.5882643461227417
I0203 02:17:13.436409 139907745949440 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.9334300756454468, loss=1.6051864624023438
I0203 02:17:47.268442 139907754342144 logging_writer.py:48] [52100] global_step=52100, grad_norm=2.011479139328003, loss=1.609206199645996
I0203 02:18:20.948606 139907745949440 logging_writer.py:48] [52200] global_step=52200, grad_norm=2.0026490688323975, loss=1.5501012802124023
I0203 02:18:54.655565 139907754342144 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.910274624824524, loss=1.7035495042800903
I0203 02:19:28.318640 139907745949440 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.735831618309021, loss=1.612525224685669
I0203 02:20:02.032263 139907754342144 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.8965479135513306, loss=1.639472246170044
I0203 02:20:35.734534 139907745949440 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.9538476467132568, loss=1.6963496208190918
I0203 02:21:09.517942 139907754342144 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.9185775518417358, loss=1.6445940732955933
I0203 02:21:43.217327 139907745949440 logging_writer.py:48] [52800] global_step=52800, grad_norm=2.121450901031494, loss=1.6032521724700928
I0203 02:22:16.936385 139907754342144 logging_writer.py:48] [52900] global_step=52900, grad_norm=2.0087780952453613, loss=1.5789262056350708
I0203 02:22:27.192938 140070692116288 spec.py:321] Evaluating on the training split.
I0203 02:22:33.507572 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 02:22:41.955754 140070692116288 spec.py:349] Evaluating on the test split.
I0203 02:22:44.636122 140070692116288 submission_runner.py:408] Time since start: 18521.76s, 	Step: 52932, 	{'train/accuracy': 0.7232341766357422, 'train/loss': 1.0687999725341797, 'validation/accuracy': 0.646399974822998, 'validation/loss': 1.4467506408691406, 'validation/num_examples': 50000, 'test/accuracy': 0.5178000330924988, 'test/loss': 2.1698689460754395, 'test/num_examples': 10000, 'score': 17884.717614650726, 'total_duration': 18521.756559848785, 'accumulated_submission_time': 17884.717614650726, 'accumulated_eval_time': 634.0705862045288, 'accumulated_logging_time': 1.129570722579956}
I0203 02:22:44.668857 139907712378624 logging_writer.py:48] [52932] accumulated_eval_time=634.070586, accumulated_logging_time=1.129571, accumulated_submission_time=17884.717615, global_step=52932, preemption_count=0, score=17884.717615, test/accuracy=0.517800, test/loss=2.169869, test/num_examples=10000, total_duration=18521.756560, train/accuracy=0.723234, train/loss=1.068800, validation/accuracy=0.646400, validation/loss=1.446751, validation/num_examples=50000
I0203 02:23:07.894856 139907720771328 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.9523344039916992, loss=1.631720781326294
I0203 02:23:41.595077 139907712378624 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.9808311462402344, loss=1.6060526371002197
I0203 02:24:15.396116 139907720771328 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.8527536392211914, loss=1.6068007946014404
I0203 02:24:49.117664 139907712378624 logging_writer.py:48] [53300] global_step=53300, grad_norm=2.028374433517456, loss=1.5485856533050537
I0203 02:25:22.763478 139907720771328 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.6235214471817017, loss=1.5411043167114258
I0203 02:25:56.460103 139907712378624 logging_writer.py:48] [53500] global_step=53500, grad_norm=2.05588436126709, loss=1.4824163913726807
I0203 02:26:30.132025 139907720771328 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.8927652835845947, loss=1.403212070465088
I0203 02:27:03.842061 139907712378624 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.7936941385269165, loss=1.5164909362792969
I0203 02:27:37.510839 139907720771328 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.8076820373535156, loss=1.616768717765808
I0203 02:28:11.248170 139907712378624 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.768128514289856, loss=1.562604308128357
I0203 02:28:44.915315 139907720771328 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.9952267408370972, loss=1.6046695709228516
I0203 02:29:18.625344 139907712378624 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.8529425859451294, loss=1.6098566055297852
I0203 02:29:52.326211 139907720771328 logging_writer.py:48] [54200] global_step=54200, grad_norm=2.1082823276519775, loss=1.552316427230835
I0203 02:30:26.159576 139907712378624 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.7800006866455078, loss=1.562707781791687
I0203 02:30:59.879495 139907720771328 logging_writer.py:48] [54400] global_step=54400, grad_norm=2.3002114295959473, loss=1.6168222427368164
I0203 02:31:14.872482 140070692116288 spec.py:321] Evaluating on the training split.
I0203 02:31:21.878051 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 02:31:30.330528 140070692116288 spec.py:349] Evaluating on the test split.
I0203 02:31:32.988786 140070692116288 submission_runner.py:408] Time since start: 19050.11s, 	Step: 54446, 	{'train/accuracy': 0.7245296239852905, 'train/loss': 1.0738086700439453, 'validation/accuracy': 0.6561799645423889, 'validation/loss': 1.4147099256515503, 'validation/num_examples': 50000, 'test/accuracy': 0.5199000239372253, 'test/loss': 2.165266513824463, 'test/num_examples': 10000, 'score': 18394.860572099686, 'total_duration': 19050.109229803085, 'accumulated_submission_time': 18394.860572099686, 'accumulated_eval_time': 652.1868450641632, 'accumulated_logging_time': 1.1715331077575684}
I0203 02:31:33.018758 139907703985920 logging_writer.py:48] [54446] accumulated_eval_time=652.186845, accumulated_logging_time=1.171533, accumulated_submission_time=18394.860572, global_step=54446, preemption_count=0, score=18394.860572, test/accuracy=0.519900, test/loss=2.165267, test/num_examples=10000, total_duration=19050.109230, train/accuracy=0.724530, train/loss=1.073809, validation/accuracy=0.656180, validation/loss=1.414710, validation/num_examples=50000
I0203 02:31:51.527995 139907712378624 logging_writer.py:48] [54500] global_step=54500, grad_norm=2.175736427307129, loss=1.5437207221984863
I0203 02:32:25.210591 139907703985920 logging_writer.py:48] [54600] global_step=54600, grad_norm=2.100106954574585, loss=1.6089482307434082
I0203 02:32:58.889953 139907712378624 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.809313416481018, loss=1.5875176191329956
I0203 02:33:32.598374 139907703985920 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.8265485763549805, loss=1.590834379196167
I0203 02:34:06.262467 139907712378624 logging_writer.py:48] [54900] global_step=54900, grad_norm=2.0715551376342773, loss=1.5094664096832275
I0203 02:34:39.961441 139907703985920 logging_writer.py:48] [55000] global_step=55000, grad_norm=2.105046272277832, loss=1.6402031183242798
I0203 02:35:13.616319 139907712378624 logging_writer.py:48] [55100] global_step=55100, grad_norm=2.07664155960083, loss=1.697310209274292
I0203 02:35:47.285602 139907703985920 logging_writer.py:48] [55200] global_step=55200, grad_norm=2.0798168182373047, loss=1.5934110879898071
I0203 02:36:20.923370 139907712378624 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.8736644983291626, loss=1.5722978115081787
I0203 02:36:54.625377 139907703985920 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.8659818172454834, loss=1.536194086074829
I0203 02:37:28.458372 139907712378624 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.9742780923843384, loss=1.6320745944976807
I0203 02:38:02.157233 139907703985920 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.9415919780731201, loss=1.4999809265136719
I0203 02:38:35.818307 139907712378624 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.9856356382369995, loss=1.6631898880004883
I0203 02:39:09.510518 139907703985920 logging_writer.py:48] [55800] global_step=55800, grad_norm=2.2138140201568604, loss=1.7493906021118164
I0203 02:39:43.182365 139907712378624 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.9340147972106934, loss=1.4800699949264526
I0203 02:40:03.195607 140070692116288 spec.py:321] Evaluating on the training split.
I0203 02:40:09.504063 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 02:40:18.195306 140070692116288 spec.py:349] Evaluating on the test split.
I0203 02:40:20.899423 140070692116288 submission_runner.py:408] Time since start: 19578.02s, 	Step: 55961, 	{'train/accuracy': 0.7122927308082581, 'train/loss': 1.1093248128890991, 'validation/accuracy': 0.6516199707984924, 'validation/loss': 1.4258030652999878, 'validation/num_examples': 50000, 'test/accuracy': 0.5128000378608704, 'test/loss': 2.1755764484405518, 'test/num_examples': 10000, 'score': 18904.974903345108, 'total_duration': 19578.019869804382, 'accumulated_submission_time': 18904.974903345108, 'accumulated_eval_time': 669.8906226158142, 'accumulated_logging_time': 1.2113227844238281}
I0203 02:40:20.926805 139907712378624 logging_writer.py:48] [55961] accumulated_eval_time=669.890623, accumulated_logging_time=1.211323, accumulated_submission_time=18904.974903, global_step=55961, preemption_count=0, score=18904.974903, test/accuracy=0.512800, test/loss=2.175576, test/num_examples=10000, total_duration=19578.019870, train/accuracy=0.712293, train/loss=1.109325, validation/accuracy=0.651620, validation/loss=1.425803, validation/num_examples=50000
I0203 02:40:34.411906 139907720771328 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.835447907447815, loss=1.5191965103149414
I0203 02:41:08.105994 139907712378624 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.793990135192871, loss=1.4373468160629272
I0203 02:41:41.792475 139907720771328 logging_writer.py:48] [56200] global_step=56200, grad_norm=2.109355926513672, loss=1.6620314121246338
I0203 02:42:15.471438 139907712378624 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.7577661275863647, loss=1.624367356300354
I0203 02:42:49.222356 139907720771328 logging_writer.py:48] [56400] global_step=56400, grad_norm=2.021254539489746, loss=1.5303280353546143
I0203 02:43:22.894146 139907712378624 logging_writer.py:48] [56500] global_step=56500, grad_norm=2.0193228721618652, loss=1.5079389810562134
I0203 02:43:56.736496 139907720771328 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.9052307605743408, loss=1.6115813255310059
I0203 02:44:30.422428 139907712378624 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.9526517391204834, loss=1.534018874168396
I0203 02:45:04.123937 139907720771328 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.9870525598526, loss=1.6024762392044067
I0203 02:45:37.858496 139907712378624 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.749661922454834, loss=1.6161874532699585
I0203 02:46:11.558594 139907720771328 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.9708106517791748, loss=1.5363423824310303
I0203 02:46:45.294961 139907712378624 logging_writer.py:48] [57100] global_step=57100, grad_norm=2.05511212348938, loss=1.5239737033843994
I0203 02:47:18.961270 139907720771328 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.8936114311218262, loss=1.661613941192627
I0203 02:47:52.630684 139907712378624 logging_writer.py:48] [57300] global_step=57300, grad_norm=2.002436399459839, loss=1.6315659284591675
I0203 02:48:26.358533 139907720771328 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.9560737609863281, loss=1.5113017559051514
I0203 02:48:51.129344 140070692116288 spec.py:321] Evaluating on the training split.
I0203 02:48:57.415093 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 02:49:06.104283 140070692116288 spec.py:349] Evaluating on the test split.
I0203 02:49:08.729518 140070692116288 submission_runner.py:408] Time since start: 20105.85s, 	Step: 57475, 	{'train/accuracy': 0.70609450340271, 'train/loss': 1.1384251117706299, 'validation/accuracy': 0.6474599838256836, 'validation/loss': 1.4578948020935059, 'validation/num_examples': 50000, 'test/accuracy': 0.5162000060081482, 'test/loss': 2.202220916748047, 'test/num_examples': 10000, 'score': 19415.11595249176, 'total_duration': 20105.849945306778, 'accumulated_submission_time': 19415.11595249176, 'accumulated_eval_time': 687.4907431602478, 'accumulated_logging_time': 1.2484371662139893}
I0203 02:49:08.756933 139907712378624 logging_writer.py:48] [57475] accumulated_eval_time=687.490743, accumulated_logging_time=1.248437, accumulated_submission_time=19415.115952, global_step=57475, preemption_count=0, score=19415.115952, test/accuracy=0.516200, test/loss=2.202221, test/num_examples=10000, total_duration=20105.849945, train/accuracy=0.706095, train/loss=1.138425, validation/accuracy=0.647460, validation/loss=1.457895, validation/num_examples=50000
I0203 02:49:17.498171 139907745949440 logging_writer.py:48] [57500] global_step=57500, grad_norm=2.109158754348755, loss=1.5877041816711426
I0203 02:49:51.144160 139907712378624 logging_writer.py:48] [57600] global_step=57600, grad_norm=2.1780993938446045, loss=1.584391474723816
I0203 02:50:24.925194 139907745949440 logging_writer.py:48] [57700] global_step=57700, grad_norm=2.2011466026306152, loss=1.623595118522644
I0203 02:50:58.583941 139907712378624 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.8597991466522217, loss=1.6675420999526978
I0203 02:51:32.358588 139907745949440 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.8342078924179077, loss=1.465275764465332
I0203 02:52:06.038428 139907712378624 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.948469877243042, loss=1.64815354347229
I0203 02:52:39.726780 139907745949440 logging_writer.py:48] [58100] global_step=58100, grad_norm=2.1180694103240967, loss=1.665188193321228
I0203 02:53:13.405044 139907712378624 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.91024649143219, loss=1.5136768817901611
I0203 02:53:47.105835 139907745949440 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.9494644403457642, loss=1.501359462738037
I0203 02:54:20.752868 139907712378624 logging_writer.py:48] [58400] global_step=58400, grad_norm=2.1302168369293213, loss=1.5634897947311401
I0203 02:54:54.537333 139907745949440 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.8955705165863037, loss=1.5996705293655396
I0203 02:55:28.213578 139907712378624 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.7593927383422852, loss=1.6044807434082031
I0203 02:56:01.905632 139907745949440 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.9557812213897705, loss=1.5312026739120483
I0203 02:56:35.650624 139907712378624 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.8262568712234497, loss=1.6204007863998413
I0203 02:57:09.420113 139907745949440 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.8244469165802002, loss=1.5293902158737183
I0203 02:57:38.842691 140070692116288 spec.py:321] Evaluating on the training split.
I0203 02:57:45.215018 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 02:57:53.609854 140070692116288 spec.py:349] Evaluating on the test split.
I0203 02:57:56.318494 140070692116288 submission_runner.py:408] Time since start: 20633.44s, 	Step: 58989, 	{'train/accuracy': 0.7527901530265808, 'train/loss': 0.939852774143219, 'validation/accuracy': 0.6464999914169312, 'validation/loss': 1.4735795259475708, 'validation/num_examples': 50000, 'test/accuracy': 0.5228000283241272, 'test/loss': 2.1752240657806396, 'test/num_examples': 10000, 'score': 19925.138954639435, 'total_duration': 20633.438943624496, 'accumulated_submission_time': 19925.138954639435, 'accumulated_eval_time': 704.9665122032166, 'accumulated_logging_time': 1.285801649093628}
I0203 02:57:56.345695 139907729164032 logging_writer.py:48] [58989] accumulated_eval_time=704.966512, accumulated_logging_time=1.285802, accumulated_submission_time=19925.138955, global_step=58989, preemption_count=0, score=19925.138955, test/accuracy=0.522800, test/loss=2.175224, test/num_examples=10000, total_duration=20633.438944, train/accuracy=0.752790, train/loss=0.939853, validation/accuracy=0.646500, validation/loss=1.473580, validation/num_examples=50000
I0203 02:58:00.401089 139907737556736 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.9459635019302368, loss=1.534117341041565
I0203 02:58:34.054344 139907729164032 logging_writer.py:48] [59100] global_step=59100, grad_norm=2.0523321628570557, loss=1.6482452154159546
I0203 02:59:07.740435 139907737556736 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.8336520195007324, loss=1.7145442962646484
I0203 02:59:41.428354 139907729164032 logging_writer.py:48] [59300] global_step=59300, grad_norm=2.1631054878234863, loss=1.6243329048156738
I0203 03:00:15.181006 139907737556736 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.859039068222046, loss=1.6728028059005737
I0203 03:00:48.899941 139907729164032 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.8392980098724365, loss=1.5196444988250732
I0203 03:01:22.576726 139907737556736 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.9909335374832153, loss=1.517654299736023
I0203 03:01:56.308319 139907729164032 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.986936330795288, loss=1.5745021104812622
I0203 03:02:29.968868 139907737556736 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.9689600467681885, loss=1.6342641115188599
I0203 03:03:03.656278 139907729164032 logging_writer.py:48] [59900] global_step=59900, grad_norm=2.077674627304077, loss=1.527482509613037
I0203 03:03:37.485884 139907737556736 logging_writer.py:48] [60000] global_step=60000, grad_norm=2.100832939147949, loss=1.5914466381072998
I0203 03:04:11.174624 139907729164032 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.9201103448867798, loss=1.6187670230865479
I0203 03:04:44.865282 139907737556736 logging_writer.py:48] [60200] global_step=60200, grad_norm=2.0664610862731934, loss=1.4886906147003174
I0203 03:05:18.587001 139907729164032 logging_writer.py:48] [60300] global_step=60300, grad_norm=2.0718157291412354, loss=1.6661978960037231
I0203 03:05:52.287203 139907737556736 logging_writer.py:48] [60400] global_step=60400, grad_norm=2.18403959274292, loss=1.5107094049453735
I0203 03:06:25.983697 139907729164032 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.8505806922912598, loss=1.5030405521392822
I0203 03:06:26.473429 140070692116288 spec.py:321] Evaluating on the training split.
I0203 03:06:32.937885 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 03:06:41.525301 140070692116288 spec.py:349] Evaluating on the test split.
I0203 03:06:44.177736 140070692116288 submission_runner.py:408] Time since start: 21161.30s, 	Step: 60503, 	{'train/accuracy': 0.7373046875, 'train/loss': 1.001809000968933, 'validation/accuracy': 0.6541599631309509, 'validation/loss': 1.409217119216919, 'validation/num_examples': 50000, 'test/accuracy': 0.5309000015258789, 'test/loss': 2.1159284114837646, 'test/num_examples': 10000, 'score': 20435.203814983368, 'total_duration': 21161.29817390442, 'accumulated_submission_time': 20435.203814983368, 'accumulated_eval_time': 722.6707804203033, 'accumulated_logging_time': 1.3243467807769775}
I0203 03:06:44.204951 139907712378624 logging_writer.py:48] [60503] accumulated_eval_time=722.670780, accumulated_logging_time=1.324347, accumulated_submission_time=20435.203815, global_step=60503, preemption_count=0, score=20435.203815, test/accuracy=0.530900, test/loss=2.115928, test/num_examples=10000, total_duration=21161.298174, train/accuracy=0.737305, train/loss=1.001809, validation/accuracy=0.654160, validation/loss=1.409217, validation/num_examples=50000
I0203 03:07:17.224791 139907720771328 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.9769104719161987, loss=1.449789047241211
I0203 03:07:50.923764 139907712378624 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.8259884119033813, loss=1.378843069076538
I0203 03:08:24.636840 139907720771328 logging_writer.py:48] [60800] global_step=60800, grad_norm=2.062570333480835, loss=1.608658790588379
I0203 03:08:58.360750 139907712378624 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.9966399669647217, loss=1.471327304840088
I0203 03:09:32.080078 139907720771328 logging_writer.py:48] [61000] global_step=61000, grad_norm=2.2683887481689453, loss=1.627502202987671
I0203 03:10:05.884696 139907712378624 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.9954229593276978, loss=1.5745642185211182
I0203 03:10:39.603721 139907720771328 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.7551329135894775, loss=1.4659379720687866
I0203 03:11:13.276715 139907712378624 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.8716129064559937, loss=1.4667770862579346
I0203 03:11:46.990122 139907720771328 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.8644709587097168, loss=1.5520477294921875
I0203 03:12:20.717198 139907712378624 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.9608656167984009, loss=1.4861189126968384
I0203 03:12:54.461538 139907720771328 logging_writer.py:48] [61600] global_step=61600, grad_norm=2.235139846801758, loss=1.5361199378967285
I0203 03:13:28.121914 139907712378624 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.9020525217056274, loss=1.5143178701400757
I0203 03:14:01.793961 139907720771328 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.880823016166687, loss=1.522914171218872
I0203 03:14:35.512708 139907712378624 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.9159698486328125, loss=1.573693037033081
I0203 03:15:09.237362 139907720771328 logging_writer.py:48] [62000] global_step=62000, grad_norm=2.0296630859375, loss=1.631726861000061
I0203 03:15:14.434155 140070692116288 spec.py:321] Evaluating on the training split.
I0203 03:15:20.804799 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 03:15:29.164844 140070692116288 spec.py:349] Evaluating on the test split.
I0203 03:15:31.748564 140070692116288 submission_runner.py:408] Time since start: 21688.87s, 	Step: 62017, 	{'train/accuracy': 0.7272400856018066, 'train/loss': 1.0499770641326904, 'validation/accuracy': 0.6545799970626831, 'validation/loss': 1.4146177768707275, 'validation/num_examples': 50000, 'test/accuracy': 0.5299000144004822, 'test/loss': 2.1289846897125244, 'test/num_examples': 10000, 'score': 20945.369954109192, 'total_duration': 21688.86900305748, 'accumulated_submission_time': 20945.369954109192, 'accumulated_eval_time': 739.9851453304291, 'accumulated_logging_time': 1.3609263896942139}
I0203 03:15:31.777894 139907737556736 logging_writer.py:48] [62017] accumulated_eval_time=739.985145, accumulated_logging_time=1.360926, accumulated_submission_time=20945.369954, global_step=62017, preemption_count=0, score=20945.369954, test/accuracy=0.529900, test/loss=2.128985, test/num_examples=10000, total_duration=21688.869003, train/accuracy=0.727240, train/loss=1.049977, validation/accuracy=0.654580, validation/loss=1.414618, validation/num_examples=50000
I0203 03:16:00.147046 139907745949440 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.887423038482666, loss=1.4890116453170776
I0203 03:16:33.934787 139907737556736 logging_writer.py:48] [62200] global_step=62200, grad_norm=2.068472146987915, loss=1.5957615375518799
I0203 03:17:07.662599 139907745949440 logging_writer.py:48] [62300] global_step=62300, grad_norm=2.0808491706848145, loss=1.4180707931518555
I0203 03:17:41.324774 139907737556736 logging_writer.py:48] [62400] global_step=62400, grad_norm=2.300361394882202, loss=1.5310235023498535
I0203 03:18:14.988221 139907745949440 logging_writer.py:48] [62500] global_step=62500, grad_norm=2.145540714263916, loss=1.5544016361236572
I0203 03:18:48.682475 139907737556736 logging_writer.py:48] [62600] global_step=62600, grad_norm=2.169260263442993, loss=1.5269114971160889
I0203 03:19:22.446244 139907745949440 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.939359426498413, loss=1.5958356857299805
I0203 03:19:56.134269 139907737556736 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.9568099975585938, loss=1.5015935897827148
I0203 03:20:29.816613 139907745949440 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.8381975889205933, loss=1.6037957668304443
I0203 03:21:03.478421 139907737556736 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.968206763267517, loss=1.455113410949707
I0203 03:21:37.152442 139907745949440 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.9755994081497192, loss=1.6076021194458008
I0203 03:22:10.813199 139907737556736 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.833866834640503, loss=1.4759442806243896
I0203 03:22:44.501059 139907745949440 logging_writer.py:48] [63300] global_step=63300, grad_norm=2.3832406997680664, loss=1.5821019411087036
I0203 03:23:18.417329 139907737556736 logging_writer.py:48] [63400] global_step=63400, grad_norm=2.111846685409546, loss=1.538025975227356
I0203 03:23:52.115779 139907745949440 logging_writer.py:48] [63500] global_step=63500, grad_norm=2.040849208831787, loss=1.430890440940857
I0203 03:24:02.045965 140070692116288 spec.py:321] Evaluating on the training split.
I0203 03:24:08.436663 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 03:24:16.738418 140070692116288 spec.py:349] Evaluating on the test split.
I0203 03:24:19.379631 140070692116288 submission_runner.py:408] Time since start: 22216.50s, 	Step: 63531, 	{'train/accuracy': 0.7257254123687744, 'train/loss': 1.053279995918274, 'validation/accuracy': 0.6569799780845642, 'validation/loss': 1.4097890853881836, 'validation/num_examples': 50000, 'test/accuracy': 0.5286000370979309, 'test/loss': 2.131467580795288, 'test/num_examples': 10000, 'score': 21455.5750977993, 'total_duration': 22216.500081300735, 'accumulated_submission_time': 21455.5750977993, 'accumulated_eval_time': 757.318776845932, 'accumulated_logging_time': 1.4008488655090332}
I0203 03:24:19.411954 139907703985920 logging_writer.py:48] [63531] accumulated_eval_time=757.318777, accumulated_logging_time=1.400849, accumulated_submission_time=21455.575098, global_step=63531, preemption_count=0, score=21455.575098, test/accuracy=0.528600, test/loss=2.131468, test/num_examples=10000, total_duration=22216.500081, train/accuracy=0.725725, train/loss=1.053280, validation/accuracy=0.656980, validation/loss=1.409789, validation/num_examples=50000
I0203 03:24:42.998370 139907712378624 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.9940110445022583, loss=1.5744272470474243
I0203 03:25:16.695672 139907703985920 logging_writer.py:48] [63700] global_step=63700, grad_norm=2.2254018783569336, loss=1.5592126846313477
I0203 03:25:50.372577 139907712378624 logging_writer.py:48] [63800] global_step=63800, grad_norm=2.163743257522583, loss=1.5970604419708252
I0203 03:26:24.071627 139907703985920 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.9899075031280518, loss=1.4694379568099976
I0203 03:26:57.737341 139907712378624 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.927895426750183, loss=1.490743637084961
I0203 03:27:31.423023 139907703985920 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.9424774646759033, loss=1.501968502998352
I0203 03:28:05.093681 139907712378624 logging_writer.py:48] [64200] global_step=64200, grad_norm=2.016709566116333, loss=1.4891343116760254
I0203 03:28:38.789025 139907703985920 logging_writer.py:48] [64300] global_step=64300, grad_norm=2.009699583053589, loss=1.5951489210128784
I0203 03:29:12.429521 139907712378624 logging_writer.py:48] [64400] global_step=64400, grad_norm=2.0255398750305176, loss=1.5360488891601562
I0203 03:29:46.260670 139907703985920 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.9562760591506958, loss=1.641331434249878
I0203 03:30:19.931346 139907712378624 logging_writer.py:48] [64600] global_step=64600, grad_norm=2.1839451789855957, loss=1.5110576152801514
I0203 03:30:53.646356 139907703985920 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.8820875883102417, loss=1.502082347869873
I0203 03:31:27.306216 139907712378624 logging_writer.py:48] [64800] global_step=64800, grad_norm=2.0555667877197266, loss=1.6102516651153564
I0203 03:32:00.992914 139907703985920 logging_writer.py:48] [64900] global_step=64900, grad_norm=2.1501331329345703, loss=1.518456220626831
I0203 03:32:34.668899 139907712378624 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.9893018007278442, loss=1.4480072259902954
I0203 03:32:49.626781 140070692116288 spec.py:321] Evaluating on the training split.
I0203 03:32:55.940527 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 03:33:04.785076 140070692116288 spec.py:349] Evaluating on the test split.
I0203 03:33:07.460916 140070692116288 submission_runner.py:408] Time since start: 22744.58s, 	Step: 65046, 	{'train/accuracy': 0.7092434763908386, 'train/loss': 1.1382043361663818, 'validation/accuracy': 0.6457200050354004, 'validation/loss': 1.4658386707305908, 'validation/num_examples': 50000, 'test/accuracy': 0.5245000123977661, 'test/loss': 2.156514883041382, 'test/num_examples': 10000, 'score': 21965.726397037506, 'total_duration': 22744.581331014633, 'accumulated_submission_time': 21965.726397037506, 'accumulated_eval_time': 775.1528396606445, 'accumulated_logging_time': 1.4429562091827393}
I0203 03:33:07.490460 139907712378624 logging_writer.py:48] [65046] accumulated_eval_time=775.152840, accumulated_logging_time=1.442956, accumulated_submission_time=21965.726397, global_step=65046, preemption_count=0, score=21965.726397, test/accuracy=0.524500, test/loss=2.156515, test/num_examples=10000, total_duration=22744.581331, train/accuracy=0.709243, train/loss=1.138204, validation/accuracy=0.645720, validation/loss=1.465839, validation/num_examples=50000
I0203 03:33:26.033229 139907754342144 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.9946613311767578, loss=1.4845610857009888
I0203 03:33:59.724240 139907712378624 logging_writer.py:48] [65200] global_step=65200, grad_norm=2.118427038192749, loss=1.4743585586547852
I0203 03:34:33.413060 139907754342144 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.9840019941329956, loss=1.4565037488937378
I0203 03:35:07.104361 139907712378624 logging_writer.py:48] [65400] global_step=65400, grad_norm=2.195361852645874, loss=1.468529224395752
I0203 03:35:40.770296 139907754342144 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.9799669981002808, loss=1.580763339996338
I0203 03:36:14.536374 139907712378624 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.885642647743225, loss=1.4164115190505981
I0203 03:36:48.241064 139907754342144 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.9396898746490479, loss=1.5297863483428955
I0203 03:37:21.930220 139907712378624 logging_writer.py:48] [65800] global_step=65800, grad_norm=2.0785951614379883, loss=1.5213810205459595
I0203 03:37:55.597609 139907754342144 logging_writer.py:48] [65900] global_step=65900, grad_norm=2.077004909515381, loss=1.4948251247406006
I0203 03:38:29.273170 139907712378624 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.9163742065429688, loss=1.6242268085479736
I0203 03:39:02.936176 139907754342144 logging_writer.py:48] [66100] global_step=66100, grad_norm=2.0360615253448486, loss=1.5585640668869019
I0203 03:39:36.674630 139907712378624 logging_writer.py:48] [66200] global_step=66200, grad_norm=2.1984059810638428, loss=1.5057969093322754
I0203 03:40:10.378181 139907754342144 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.928980827331543, loss=1.4386358261108398
I0203 03:40:44.042672 139907712378624 logging_writer.py:48] [66400] global_step=66400, grad_norm=2.122415542602539, loss=1.513991117477417
I0203 03:41:17.711075 139907754342144 logging_writer.py:48] [66500] global_step=66500, grad_norm=2.100559949874878, loss=1.5298261642456055
I0203 03:41:37.725759 140070692116288 spec.py:321] Evaluating on the training split.
I0203 03:41:44.003779 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 03:41:52.710558 140070692116288 spec.py:349] Evaluating on the test split.
I0203 03:41:55.367237 140070692116288 submission_runner.py:408] Time since start: 23272.49s, 	Step: 66561, 	{'train/accuracy': 0.7173748016357422, 'train/loss': 1.0853809118270874, 'validation/accuracy': 0.6520000100135803, 'validation/loss': 1.4343310594558716, 'validation/num_examples': 50000, 'test/accuracy': 0.5267000198364258, 'test/loss': 2.145815134048462, 'test/num_examples': 10000, 'score': 22475.8989508152, 'total_duration': 23272.48766350746, 'accumulated_submission_time': 22475.8989508152, 'accumulated_eval_time': 792.794264793396, 'accumulated_logging_time': 1.482445240020752}
I0203 03:41:55.398195 139907729164032 logging_writer.py:48] [66561] accumulated_eval_time=792.794265, accumulated_logging_time=1.482445, accumulated_submission_time=22475.898951, global_step=66561, preemption_count=0, score=22475.898951, test/accuracy=0.526700, test/loss=2.145815, test/num_examples=10000, total_duration=23272.487664, train/accuracy=0.717375, train/loss=1.085381, validation/accuracy=0.652000, validation/loss=1.434331, validation/num_examples=50000
I0203 03:42:08.860580 139907737556736 logging_writer.py:48] [66600] global_step=66600, grad_norm=2.258113384246826, loss=1.541585087776184
I0203 03:42:42.614804 139907729164032 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.9234528541564941, loss=1.5444706678390503
I0203 03:43:16.299074 139907737556736 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.8620586395263672, loss=1.3908677101135254
I0203 03:43:50.007752 139907729164032 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.893068790435791, loss=1.513787031173706
I0203 03:44:23.724347 139907737556736 logging_writer.py:48] [67000] global_step=67000, grad_norm=2.01584792137146, loss=1.5647324323654175
I0203 03:44:57.441826 139907729164032 logging_writer.py:48] [67100] global_step=67100, grad_norm=2.1858835220336914, loss=1.5290234088897705
I0203 03:45:31.199640 139907737556736 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.9258004426956177, loss=1.4716730117797852
I0203 03:46:04.916683 139907729164032 logging_writer.py:48] [67300] global_step=67300, grad_norm=2.010824203491211, loss=1.540134310722351
I0203 03:46:38.653475 139907737556736 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.9504281282424927, loss=1.5201263427734375
I0203 03:47:12.316420 139907729164032 logging_writer.py:48] [67500] global_step=67500, grad_norm=2.1128008365631104, loss=1.4542269706726074
I0203 03:47:46.047759 139907737556736 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.9930076599121094, loss=1.4894951581954956
I0203 03:48:19.731407 139907729164032 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.9329359531402588, loss=1.5763490200042725
I0203 03:48:53.431307 139907737556736 logging_writer.py:48] [67800] global_step=67800, grad_norm=2.1219232082366943, loss=1.506270170211792
I0203 03:49:27.240687 139907729164032 logging_writer.py:48] [67900] global_step=67900, grad_norm=2.019930362701416, loss=1.5600576400756836
I0203 03:50:00.929085 139907737556736 logging_writer.py:48] [68000] global_step=68000, grad_norm=2.325629234313965, loss=1.4065701961517334
I0203 03:50:25.647931 140070692116288 spec.py:321] Evaluating on the training split.
I0203 03:50:32.023519 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 03:50:40.651457 140070692116288 spec.py:349] Evaluating on the test split.
I0203 03:50:43.348367 140070692116288 submission_runner.py:408] Time since start: 23800.47s, 	Step: 68075, 	{'train/accuracy': 0.7562978267669678, 'train/loss': 0.9239857792854309, 'validation/accuracy': 0.6578199863433838, 'validation/loss': 1.4056472778320312, 'validation/num_examples': 50000, 'test/accuracy': 0.5333000421524048, 'test/loss': 2.1278412342071533, 'test/num_examples': 10000, 'score': 22986.08687877655, 'total_duration': 23800.468817472458, 'accumulated_submission_time': 22986.08687877655, 'accumulated_eval_time': 810.4946658611298, 'accumulated_logging_time': 1.5225434303283691}
I0203 03:50:43.377446 139907712378624 logging_writer.py:48] [68075] accumulated_eval_time=810.494666, accumulated_logging_time=1.522543, accumulated_submission_time=22986.086879, global_step=68075, preemption_count=0, score=22986.086879, test/accuracy=0.533300, test/loss=2.127841, test/num_examples=10000, total_duration=23800.468817, train/accuracy=0.756298, train/loss=0.923986, validation/accuracy=0.657820, validation/loss=1.405647, validation/num_examples=50000
I0203 03:50:52.136893 139907720771328 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.9851250648498535, loss=1.6457111835479736
I0203 03:51:25.852061 139907712378624 logging_writer.py:48] [68200] global_step=68200, grad_norm=2.015357255935669, loss=1.4992923736572266
I0203 03:51:59.569891 139907720771328 logging_writer.py:48] [68300] global_step=68300, grad_norm=2.1266214847564697, loss=1.5578242540359497
I0203 03:52:33.245755 139907712378624 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.975007176399231, loss=1.4573696851730347
I0203 03:53:07.027786 139907720771328 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.9720577001571655, loss=1.50311279296875
I0203 03:53:40.707862 139907712378624 logging_writer.py:48] [68600] global_step=68600, grad_norm=2.154587984085083, loss=1.4261510372161865
I0203 03:54:14.489547 139907720771328 logging_writer.py:48] [68700] global_step=68700, grad_norm=2.255643606185913, loss=1.4953550100326538
I0203 03:54:48.163889 139907712378624 logging_writer.py:48] [68800] global_step=68800, grad_norm=2.1046268939971924, loss=1.5041640996932983
I0203 03:55:21.857929 139907720771328 logging_writer.py:48] [68900] global_step=68900, grad_norm=2.038121461868286, loss=1.664952039718628
I0203 03:55:55.668667 139907712378624 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.857481598854065, loss=1.504187822341919
I0203 03:56:29.387150 139907720771328 logging_writer.py:48] [69100] global_step=69100, grad_norm=2.0065057277679443, loss=1.5868861675262451
I0203 03:57:03.057631 139907712378624 logging_writer.py:48] [69200] global_step=69200, grad_norm=2.114901065826416, loss=1.5387134552001953
I0203 03:57:36.767863 139907720771328 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.9950435161590576, loss=1.5629726648330688
I0203 03:58:10.446974 139907712378624 logging_writer.py:48] [69400] global_step=69400, grad_norm=2.1352803707122803, loss=1.4834665060043335
I0203 03:58:44.141891 139907720771328 logging_writer.py:48] [69500] global_step=69500, grad_norm=2.0521600246429443, loss=1.5308728218078613
I0203 03:59:13.561835 140070692116288 spec.py:321] Evaluating on the training split.
I0203 03:59:19.884689 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 03:59:28.257762 140070692116288 spec.py:349] Evaluating on the test split.
I0203 03:59:30.972740 140070692116288 submission_runner.py:408] Time since start: 24328.09s, 	Step: 69589, 	{'train/accuracy': 0.7335578799247742, 'train/loss': 1.0107797384262085, 'validation/accuracy': 0.6535399556159973, 'validation/loss': 1.4225095510482788, 'validation/num_examples': 50000, 'test/accuracy': 0.5343000292778015, 'test/loss': 2.123914957046509, 'test/num_examples': 10000, 'score': 23496.2087392807, 'total_duration': 24328.093178987503, 'accumulated_submission_time': 23496.2087392807, 'accumulated_eval_time': 827.9055438041687, 'accumulated_logging_time': 1.5612945556640625}
I0203 03:59:31.003043 139907745949440 logging_writer.py:48] [69589] accumulated_eval_time=827.905544, accumulated_logging_time=1.561295, accumulated_submission_time=23496.208739, global_step=69589, preemption_count=0, score=23496.208739, test/accuracy=0.534300, test/loss=2.123915, test/num_examples=10000, total_duration=24328.093179, train/accuracy=0.733558, train/loss=1.010780, validation/accuracy=0.653540, validation/loss=1.422510, validation/num_examples=50000
I0203 03:59:35.065963 139907754342144 logging_writer.py:48] [69600] global_step=69600, grad_norm=2.030165910720825, loss=1.406097412109375
I0203 04:00:08.757196 139907745949440 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.8460944890975952, loss=1.4286364316940308
I0203 04:00:42.430082 139907754342144 logging_writer.py:48] [69800] global_step=69800, grad_norm=2.0715582370758057, loss=1.4963668584823608
I0203 04:01:16.120398 139907745949440 logging_writer.py:48] [69900] global_step=69900, grad_norm=2.000842571258545, loss=1.4865591526031494
I0203 04:01:49.805261 139907754342144 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.9178160429000854, loss=1.4563863277435303
I0203 04:02:23.641845 139907745949440 logging_writer.py:48] [70100] global_step=70100, grad_norm=2.2235164642333984, loss=1.4927568435668945
I0203 04:02:57.332474 139907754342144 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.9713046550750732, loss=1.542112112045288
I0203 04:03:31.040748 139907745949440 logging_writer.py:48] [70300] global_step=70300, grad_norm=2.130394697189331, loss=1.6025116443634033
I0203 04:04:04.700546 139907754342144 logging_writer.py:48] [70400] global_step=70400, grad_norm=2.1353325843811035, loss=1.5546419620513916
I0203 04:04:38.391538 139907745949440 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.8722249269485474, loss=1.581212043762207
I0203 04:05:12.062034 139907754342144 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.966256856918335, loss=1.5293887853622437
I0203 04:05:45.767398 139907745949440 logging_writer.py:48] [70700] global_step=70700, grad_norm=2.0085043907165527, loss=1.5379999876022339
I0203 04:06:19.431226 139907754342144 logging_writer.py:48] [70800] global_step=70800, grad_norm=2.3426830768585205, loss=1.519978404045105
I0203 04:06:53.125484 139907745949440 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.9396641254425049, loss=1.4589275121688843
I0203 04:07:26.799576 139907754342144 logging_writer.py:48] [71000] global_step=71000, grad_norm=2.056211233139038, loss=1.5121588706970215
I0203 04:08:00.489971 139907745949440 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.966107726097107, loss=1.4322298765182495
I0203 04:08:00.980599 140070692116288 spec.py:321] Evaluating on the training split.
I0203 04:08:07.514350 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 04:08:15.985573 140070692116288 spec.py:349] Evaluating on the test split.
I0203 04:08:18.643890 140070692116288 submission_runner.py:408] Time since start: 24855.76s, 	Step: 71103, 	{'train/accuracy': 0.7446388602256775, 'train/loss': 0.9683708548545837, 'validation/accuracy': 0.668940007686615, 'validation/loss': 1.3582696914672852, 'validation/num_examples': 50000, 'test/accuracy': 0.5351000428199768, 'test/loss': 2.0900559425354004, 'test/num_examples': 10000, 'score': 24006.12292265892, 'total_duration': 24855.764329195023, 'accumulated_submission_time': 24006.12292265892, 'accumulated_eval_time': 845.568799495697, 'accumulated_logging_time': 1.6021020412445068}
I0203 04:08:18.673316 139907712378624 logging_writer.py:48] [71103] accumulated_eval_time=845.568799, accumulated_logging_time=1.602102, accumulated_submission_time=24006.122923, global_step=71103, preemption_count=0, score=24006.122923, test/accuracy=0.535100, test/loss=2.090056, test/num_examples=10000, total_duration=24855.764329, train/accuracy=0.744639, train/loss=0.968371, validation/accuracy=0.668940, validation/loss=1.358270, validation/num_examples=50000
I0203 04:08:51.731923 139907720771328 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.9340450763702393, loss=1.4994947910308838
I0203 04:09:25.380100 139907712378624 logging_writer.py:48] [71300] global_step=71300, grad_norm=2.26257586479187, loss=1.456045389175415
I0203 04:09:59.096419 139907720771328 logging_writer.py:48] [71400] global_step=71400, grad_norm=2.3121755123138428, loss=1.584946870803833
I0203 04:10:32.750916 139907712378624 logging_writer.py:48] [71500] global_step=71500, grad_norm=2.4337830543518066, loss=1.4394444227218628
I0203 04:11:06.456659 139907720771328 logging_writer.py:48] [71600] global_step=71600, grad_norm=2.2018589973449707, loss=1.6071178913116455
I0203 04:11:40.105735 139907712378624 logging_writer.py:48] [71700] global_step=71700, grad_norm=2.0438425540924072, loss=1.5220409631729126
I0203 04:12:13.785225 139907720771328 logging_writer.py:48] [71800] global_step=71800, grad_norm=2.158431053161621, loss=1.5490750074386597
I0203 04:12:47.435299 139907712378624 logging_writer.py:48] [71900] global_step=71900, grad_norm=2.1012465953826904, loss=1.4256564378738403
I0203 04:13:21.155383 139907720771328 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.9268746376037598, loss=1.4470465183258057
I0203 04:13:54.847929 139907712378624 logging_writer.py:48] [72100] global_step=72100, grad_norm=2.126678228378296, loss=1.43716561794281
I0203 04:14:28.540015 139907720771328 logging_writer.py:48] [72200] global_step=72200, grad_norm=2.183986186981201, loss=1.470458745956421
I0203 04:15:02.265141 139907712378624 logging_writer.py:48] [72300] global_step=72300, grad_norm=2.1256003379821777, loss=1.5923672914505005
I0203 04:15:36.095350 139907720771328 logging_writer.py:48] [72400] global_step=72400, grad_norm=2.239623546600342, loss=1.4966917037963867
I0203 04:16:09.763256 139907712378624 logging_writer.py:48] [72500] global_step=72500, grad_norm=2.1449038982391357, loss=1.534063458442688
I0203 04:16:43.449326 139907720771328 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.9110968112945557, loss=1.5660748481750488
I0203 04:16:48.992562 140070692116288 spec.py:321] Evaluating on the training split.
I0203 04:16:55.275022 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 04:17:03.854060 140070692116288 spec.py:349] Evaluating on the test split.
I0203 04:17:06.376235 140070692116288 submission_runner.py:408] Time since start: 25383.50s, 	Step: 72618, 	{'train/accuracy': 0.7384406924247742, 'train/loss': 0.9917774200439453, 'validation/accuracy': 0.6642999649047852, 'validation/loss': 1.362199068069458, 'validation/num_examples': 50000, 'test/accuracy': 0.5350000262260437, 'test/loss': 2.121037483215332, 'test/num_examples': 10000, 'score': 24516.380070209503, 'total_duration': 25383.496671438217, 'accumulated_submission_time': 24516.380070209503, 'accumulated_eval_time': 862.9524285793304, 'accumulated_logging_time': 1.64097261428833}
I0203 04:17:06.405308 139907745949440 logging_writer.py:48] [72618] accumulated_eval_time=862.952429, accumulated_logging_time=1.640973, accumulated_submission_time=24516.380070, global_step=72618, preemption_count=0, score=24516.380070, test/accuracy=0.535000, test/loss=2.121037, test/num_examples=10000, total_duration=25383.496671, train/accuracy=0.738441, train/loss=0.991777, validation/accuracy=0.664300, validation/loss=1.362199, validation/num_examples=50000
I0203 04:17:34.395787 139907754342144 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.9856809377670288, loss=1.6411607265472412
I0203 04:18:08.094152 139907745949440 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.9779834747314453, loss=1.569559097290039
I0203 04:18:41.758830 139907754342144 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.930168867111206, loss=1.5216494798660278
I0203 04:19:15.441104 139907745949440 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.8810428380966187, loss=1.480462670326233
I0203 04:19:49.125383 139907754342144 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.9351035356521606, loss=1.4666575193405151
I0203 04:20:22.815139 139907745949440 logging_writer.py:48] [73200] global_step=73200, grad_norm=2.0015785694122314, loss=1.4572651386260986
I0203 04:20:56.466162 139907754342144 logging_writer.py:48] [73300] global_step=73300, grad_norm=2.053279161453247, loss=1.523980736732483
I0203 04:21:30.146618 139907745949440 logging_writer.py:48] [73400] global_step=73400, grad_norm=2.1522481441497803, loss=1.485445261001587
I0203 04:22:03.954233 139907754342144 logging_writer.py:48] [73500] global_step=73500, grad_norm=2.137096643447876, loss=1.5721887350082397
I0203 04:22:37.644501 139907745949440 logging_writer.py:48] [73600] global_step=73600, grad_norm=2.323014736175537, loss=1.4607887268066406
I0203 04:23:11.332137 139907754342144 logging_writer.py:48] [73700] global_step=73700, grad_norm=2.0817461013793945, loss=1.5568715333938599
I0203 04:23:45.010092 139907745949440 logging_writer.py:48] [73800] global_step=73800, grad_norm=2.222696304321289, loss=1.5882163047790527
I0203 04:24:18.689667 139907754342144 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.982830286026001, loss=1.5483248233795166
I0203 04:24:52.371135 139907745949440 logging_writer.py:48] [74000] global_step=74000, grad_norm=2.0812840461730957, loss=1.4779857397079468
I0203 04:25:26.032688 139907754342144 logging_writer.py:48] [74100] global_step=74100, grad_norm=2.3459384441375732, loss=1.601707100868225
I0203 04:25:36.619667 140070692116288 spec.py:321] Evaluating on the training split.
I0203 04:25:42.962069 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 04:25:51.371469 140070692116288 spec.py:349] Evaluating on the test split.
I0203 04:25:54.043871 140070692116288 submission_runner.py:408] Time since start: 25911.16s, 	Step: 74133, 	{'train/accuracy': 0.7292729616165161, 'train/loss': 1.0414572954177856, 'validation/accuracy': 0.6623799800872803, 'validation/loss': 1.3939917087554932, 'validation/num_examples': 50000, 'test/accuracy': 0.5330000519752502, 'test/loss': 2.1223597526550293, 'test/num_examples': 10000, 'score': 25026.531310796738, 'total_duration': 25911.16432285309, 'accumulated_submission_time': 25026.531310796738, 'accumulated_eval_time': 880.3766114711761, 'accumulated_logging_time': 1.679915428161621}
I0203 04:25:54.076723 139907712378624 logging_writer.py:48] [74133] accumulated_eval_time=880.376611, accumulated_logging_time=1.679915, accumulated_submission_time=25026.531311, global_step=74133, preemption_count=0, score=25026.531311, test/accuracy=0.533000, test/loss=2.122360, test/num_examples=10000, total_duration=25911.164323, train/accuracy=0.729273, train/loss=1.041457, validation/accuracy=0.662380, validation/loss=1.393992, validation/num_examples=50000
I0203 04:26:17.003294 139907720771328 logging_writer.py:48] [74200] global_step=74200, grad_norm=2.1783149242401123, loss=1.4742194414138794
I0203 04:26:50.673403 139907712378624 logging_writer.py:48] [74300] global_step=74300, grad_norm=2.3241426944732666, loss=1.5002793073654175
I0203 04:27:24.345157 139907720771328 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.9140397310256958, loss=1.3983770608901978
I0203 04:27:58.027903 139907712378624 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.9764150381088257, loss=1.4759787321090698
I0203 04:28:31.884873 139907720771328 logging_writer.py:48] [74600] global_step=74600, grad_norm=2.081904649734497, loss=1.5312933921813965
I0203 04:29:05.610612 139907712378624 logging_writer.py:48] [74700] global_step=74700, grad_norm=2.3438193798065186, loss=1.4952690601348877
I0203 04:29:39.269737 139907720771328 logging_writer.py:48] [74800] global_step=74800, grad_norm=2.314600944519043, loss=1.6006650924682617
I0203 04:30:12.959842 139907712378624 logging_writer.py:48] [74900] global_step=74900, grad_norm=2.4873549938201904, loss=1.4862858057022095
I0203 04:30:46.617289 139907720771328 logging_writer.py:48] [75000] global_step=75000, grad_norm=2.196903705596924, loss=1.6428544521331787
I0203 04:31:20.337924 139907712378624 logging_writer.py:48] [75100] global_step=75100, grad_norm=2.037895441055298, loss=1.5188331604003906
I0203 04:31:54.052170 139907720771328 logging_writer.py:48] [75200] global_step=75200, grad_norm=2.138333320617676, loss=1.4093453884124756
I0203 04:32:27.792973 139907712378624 logging_writer.py:48] [75300] global_step=75300, grad_norm=2.0550730228424072, loss=1.4227590560913086
I0203 04:33:01.470645 139907720771328 logging_writer.py:48] [75400] global_step=75400, grad_norm=2.085193634033203, loss=1.4643068313598633
I0203 04:33:35.156105 139907712378624 logging_writer.py:48] [75500] global_step=75500, grad_norm=2.3001527786254883, loss=1.5689733028411865
I0203 04:34:08.898775 139907720771328 logging_writer.py:48] [75600] global_step=75600, grad_norm=2.2299327850341797, loss=1.5596787929534912
I0203 04:34:24.212776 140070692116288 spec.py:321] Evaluating on the training split.
I0203 04:34:30.635482 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 04:34:39.176202 140070692116288 spec.py:349] Evaluating on the test split.
I0203 04:34:41.837936 140070692116288 submission_runner.py:408] Time since start: 26438.96s, 	Step: 75647, 	{'train/accuracy': 0.7429248690605164, 'train/loss': 0.9894379377365112, 'validation/accuracy': 0.6701799631118774, 'validation/loss': 1.3528382778167725, 'validation/num_examples': 50000, 'test/accuracy': 0.5415000319480896, 'test/loss': 2.0805461406707764, 'test/num_examples': 10000, 'score': 25536.604145526886, 'total_duration': 26438.958388328552, 'accumulated_submission_time': 25536.604145526886, 'accumulated_eval_time': 898.0017364025116, 'accumulated_logging_time': 1.7220737934112549}
I0203 04:34:41.868099 139907703985920 logging_writer.py:48] [75647] accumulated_eval_time=898.001736, accumulated_logging_time=1.722074, accumulated_submission_time=25536.604146, global_step=75647, preemption_count=0, score=25536.604146, test/accuracy=0.541500, test/loss=2.080546, test/num_examples=10000, total_duration=26438.958388, train/accuracy=0.742925, train/loss=0.989438, validation/accuracy=0.670180, validation/loss=1.352838, validation/num_examples=50000
I0203 04:35:00.098349 139907745949440 logging_writer.py:48] [75700] global_step=75700, grad_norm=2.1951870918273926, loss=1.5436586141586304
I0203 04:35:33.818217 139907703985920 logging_writer.py:48] [75800] global_step=75800, grad_norm=2.015146017074585, loss=1.5096019506454468
I0203 04:36:07.505402 139907745949440 logging_writer.py:48] [75900] global_step=75900, grad_norm=2.1776247024536133, loss=1.4121886491775513
I0203 04:36:41.196898 139907703985920 logging_writer.py:48] [76000] global_step=76000, grad_norm=2.1188910007476807, loss=1.5340913534164429
I0203 04:37:14.892746 139907745949440 logging_writer.py:48] [76100] global_step=76100, grad_norm=2.184556484222412, loss=1.435753345489502
I0203 04:37:48.629082 139907703985920 logging_writer.py:48] [76200] global_step=76200, grad_norm=2.5199546813964844, loss=1.6706222295761108
I0203 04:38:22.341078 139907745949440 logging_writer.py:48] [76300] global_step=76300, grad_norm=2.047252893447876, loss=1.4650428295135498
I0203 04:38:56.033954 139907703985920 logging_writer.py:48] [76400] global_step=76400, grad_norm=2.174241304397583, loss=1.4445304870605469
I0203 04:39:29.720415 139907745949440 logging_writer.py:48] [76500] global_step=76500, grad_norm=2.2536938190460205, loss=1.5357780456542969
I0203 04:40:03.427142 139907703985920 logging_writer.py:48] [76600] global_step=76600, grad_norm=2.0999624729156494, loss=1.4774270057678223
I0203 04:40:37.114068 139907745949440 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.918366551399231, loss=1.4088777303695679
I0203 04:41:10.792857 139907703985920 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.9657405614852905, loss=1.4825208187103271
I0203 04:41:44.617393 139907745949440 logging_writer.py:48] [76900] global_step=76900, grad_norm=2.175865650177002, loss=1.449869990348816
I0203 04:42:18.300828 139907703985920 logging_writer.py:48] [77000] global_step=77000, grad_norm=2.0609185695648193, loss=1.473677635192871
I0203 04:42:51.992907 139907745949440 logging_writer.py:48] [77100] global_step=77100, grad_norm=2.223998546600342, loss=1.498150110244751
I0203 04:43:12.013568 140070692116288 spec.py:321] Evaluating on the training split.
I0203 04:43:18.444974 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 04:43:27.110683 140070692116288 spec.py:349] Evaluating on the test split.
I0203 04:43:29.707303 140070692116288 submission_runner.py:408] Time since start: 26966.83s, 	Step: 77161, 	{'train/accuracy': 0.7538663744926453, 'train/loss': 0.933739185333252, 'validation/accuracy': 0.6592999696731567, 'validation/loss': 1.3939270973205566, 'validation/num_examples': 50000, 'test/accuracy': 0.5258000493049622, 'test/loss': 2.1486873626708984, 'test/num_examples': 10000, 'score': 26046.688113451004, 'total_duration': 26966.827723503113, 'accumulated_submission_time': 26046.688113451004, 'accumulated_eval_time': 915.6954228878021, 'accumulated_logging_time': 1.7612836360931396}
I0203 04:43:29.745431 139907703985920 logging_writer.py:48] [77161] accumulated_eval_time=915.695423, accumulated_logging_time=1.761284, accumulated_submission_time=26046.688113, global_step=77161, preemption_count=0, score=26046.688113, test/accuracy=0.525800, test/loss=2.148687, test/num_examples=10000, total_duration=26966.827724, train/accuracy=0.753866, train/loss=0.933739, validation/accuracy=0.659300, validation/loss=1.393927, validation/num_examples=50000
I0203 04:43:43.207227 139907729164032 logging_writer.py:48] [77200] global_step=77200, grad_norm=2.0174431800842285, loss=1.4350496530532837
I0203 04:44:16.900955 139907703985920 logging_writer.py:48] [77300] global_step=77300, grad_norm=2.200484275817871, loss=1.510918140411377
I0203 04:44:50.601758 139907729164032 logging_writer.py:48] [77400] global_step=77400, grad_norm=2.669677257537842, loss=1.4703975915908813
I0203 04:45:24.263701 139907703985920 logging_writer.py:48] [77500] global_step=77500, grad_norm=2.041792869567871, loss=1.446214199066162
I0203 04:45:57.973282 139907729164032 logging_writer.py:48] [77600] global_step=77600, grad_norm=2.159680128097534, loss=1.5124517679214478
I0203 04:46:31.657020 139907703985920 logging_writer.py:48] [77700] global_step=77700, grad_norm=2.2429847717285156, loss=1.4615092277526855
I0203 04:47:05.405358 139907729164032 logging_writer.py:48] [77800] global_step=77800, grad_norm=2.1743009090423584, loss=1.3965260982513428
I0203 04:47:39.077843 139907703985920 logging_writer.py:48] [77900] global_step=77900, grad_norm=2.2601559162139893, loss=1.515153408050537
I0203 04:48:12.845857 139907729164032 logging_writer.py:48] [78000] global_step=78000, grad_norm=2.1022496223449707, loss=1.372241497039795
I0203 04:48:46.505884 139907703985920 logging_writer.py:48] [78100] global_step=78100, grad_norm=2.069269895553589, loss=1.4841490983963013
I0203 04:49:20.192033 139907729164032 logging_writer.py:48] [78200] global_step=78200, grad_norm=2.20469331741333, loss=1.4706522226333618
I0203 04:49:53.864235 139907703985920 logging_writer.py:48] [78300] global_step=78300, grad_norm=2.300233840942383, loss=1.4517176151275635
I0203 04:50:27.550932 139907729164032 logging_writer.py:48] [78400] global_step=78400, grad_norm=2.1859705448150635, loss=1.4879529476165771
I0203 04:51:01.213221 139907703985920 logging_writer.py:48] [78500] global_step=78500, grad_norm=2.167142629623413, loss=1.5041133165359497
I0203 04:51:35.000174 139907729164032 logging_writer.py:48] [78600] global_step=78600, grad_norm=2.1623637676239014, loss=1.4547691345214844
I0203 04:51:59.719066 140070692116288 spec.py:321] Evaluating on the training split.
I0203 04:52:06.139892 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 04:52:14.778399 140070692116288 spec.py:349] Evaluating on the test split.
I0203 04:52:17.422805 140070692116288 submission_runner.py:408] Time since start: 27494.54s, 	Step: 78675, 	{'train/accuracy': 0.7557198405265808, 'train/loss': 0.9259157776832581, 'validation/accuracy': 0.6734399795532227, 'validation/loss': 1.3371450901031494, 'validation/num_examples': 50000, 'test/accuracy': 0.5458000302314758, 'test/loss': 2.0804595947265625, 'test/num_examples': 10000, 'score': 26556.600678920746, 'total_duration': 27494.543236494064, 'accumulated_submission_time': 26556.600678920746, 'accumulated_eval_time': 933.3991062641144, 'accumulated_logging_time': 1.8085589408874512}
I0203 04:52:17.465922 139907745949440 logging_writer.py:48] [78675] accumulated_eval_time=933.399106, accumulated_logging_time=1.808559, accumulated_submission_time=26556.600679, global_step=78675, preemption_count=0, score=26556.600679, test/accuracy=0.545800, test/loss=2.080460, test/num_examples=10000, total_duration=27494.543236, train/accuracy=0.755720, train/loss=0.925916, validation/accuracy=0.673440, validation/loss=1.337145, validation/num_examples=50000
I0203 04:52:26.231951 139907754342144 logging_writer.py:48] [78700] global_step=78700, grad_norm=2.184042453765869, loss=1.4560267925262451
I0203 04:52:59.901997 139907745949440 logging_writer.py:48] [78800] global_step=78800, grad_norm=2.0539298057556152, loss=1.4981446266174316
I0203 04:53:33.583278 139907754342144 logging_writer.py:48] [78900] global_step=78900, grad_norm=2.2355756759643555, loss=1.4690401554107666
I0203 04:54:07.242411 139907745949440 logging_writer.py:48] [79000] global_step=79000, grad_norm=2.135983943939209, loss=1.4663020372390747
I0203 04:54:41.017333 139907754342144 logging_writer.py:48] [79100] global_step=79100, grad_norm=1.829758644104004, loss=1.3695487976074219
I0203 04:55:14.741903 139907745949440 logging_writer.py:48] [79200] global_step=79200, grad_norm=2.2175357341766357, loss=1.3634592294692993
I0203 04:55:48.447760 139907754342144 logging_writer.py:48] [79300] global_step=79300, grad_norm=2.130382537841797, loss=1.6080639362335205
I0203 04:56:22.111932 139907745949440 logging_writer.py:48] [79400] global_step=79400, grad_norm=2.1143908500671387, loss=1.455198049545288
I0203 04:56:55.801467 139907754342144 logging_writer.py:48] [79500] global_step=79500, grad_norm=2.202261209487915, loss=1.3848751783370972
I0203 04:57:29.447825 139907745949440 logging_writer.py:48] [79600] global_step=79600, grad_norm=2.140235185623169, loss=1.3756258487701416
I0203 04:58:03.143753 139907754342144 logging_writer.py:48] [79700] global_step=79700, grad_norm=2.2702391147613525, loss=1.3474658727645874
I0203 04:58:36.817498 139907745949440 logging_writer.py:48] [79800] global_step=79800, grad_norm=2.2880585193634033, loss=1.4794423580169678
I0203 04:59:10.507398 139907754342144 logging_writer.py:48] [79900] global_step=79900, grad_norm=2.179178476333618, loss=1.5040957927703857
I0203 04:59:44.155322 139907745949440 logging_writer.py:48] [80000] global_step=80000, grad_norm=2.1887032985687256, loss=1.4520868062973022
I0203 05:00:17.849632 139907754342144 logging_writer.py:48] [80100] global_step=80100, grad_norm=2.178586721420288, loss=1.52190101146698
I0203 05:00:47.593252 140070692116288 spec.py:321] Evaluating on the training split.
I0203 05:00:53.988426 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 05:01:02.811028 140070692116288 spec.py:349] Evaluating on the test split.
I0203 05:01:05.504381 140070692116288 submission_runner.py:408] Time since start: 28022.62s, 	Step: 80190, 	{'train/accuracy': 0.7496811151504517, 'train/loss': 0.9471098780632019, 'validation/accuracy': 0.6690799593925476, 'validation/loss': 1.3521523475646973, 'validation/num_examples': 50000, 'test/accuracy': 0.5390000343322754, 'test/loss': 2.0906593799591064, 'test/num_examples': 10000, 'score': 27066.66562986374, 'total_duration': 28022.62483239174, 'accumulated_submission_time': 27066.66562986374, 'accumulated_eval_time': 951.3102207183838, 'accumulated_logging_time': 1.8615412712097168}
I0203 05:01:05.536457 139907712378624 logging_writer.py:48] [80190] accumulated_eval_time=951.310221, accumulated_logging_time=1.861541, accumulated_submission_time=27066.665630, global_step=80190, preemption_count=0, score=27066.665630, test/accuracy=0.539000, test/loss=2.090659, test/num_examples=10000, total_duration=28022.624832, train/accuracy=0.749681, train/loss=0.947110, validation/accuracy=0.669080, validation/loss=1.352152, validation/num_examples=50000
I0203 05:01:09.341318 139907720771328 logging_writer.py:48] [80200] global_step=80200, grad_norm=2.1176974773406982, loss=1.432038426399231
I0203 05:01:42.942199 139907712378624 logging_writer.py:48] [80300] global_step=80300, grad_norm=2.194000005722046, loss=1.4564707279205322
I0203 05:02:16.664353 139907720771328 logging_writer.py:48] [80400] global_step=80400, grad_norm=2.395986557006836, loss=1.5593512058258057
I0203 05:02:50.404383 139907712378624 logging_writer.py:48] [80500] global_step=80500, grad_norm=2.1176114082336426, loss=1.491870403289795
I0203 05:03:24.114836 139907720771328 logging_writer.py:48] [80600] global_step=80600, grad_norm=2.3275225162506104, loss=1.4043676853179932
I0203 05:03:57.866637 139907712378624 logging_writer.py:48] [80700] global_step=80700, grad_norm=2.099943161010742, loss=1.429877519607544
I0203 05:04:31.523869 139907720771328 logging_writer.py:48] [80800] global_step=80800, grad_norm=2.0313570499420166, loss=1.5420056581497192
I0203 05:05:05.210627 139907712378624 logging_writer.py:48] [80900] global_step=80900, grad_norm=2.0636885166168213, loss=1.473498821258545
I0203 05:05:38.887524 139907720771328 logging_writer.py:48] [81000] global_step=81000, grad_norm=2.2281301021575928, loss=1.5482509136199951
I0203 05:06:12.567562 139907712378624 logging_writer.py:48] [81100] global_step=81100, grad_norm=2.1733624935150146, loss=1.5094337463378906
I0203 05:06:46.223298 139907720771328 logging_writer.py:48] [81200] global_step=81200, grad_norm=2.210585117340088, loss=1.4281643629074097
I0203 05:07:19.926079 139907712378624 logging_writer.py:48] [81300] global_step=81300, grad_norm=2.06231689453125, loss=1.4691510200500488
I0203 05:07:53.693927 139907720771328 logging_writer.py:48] [81400] global_step=81400, grad_norm=2.219115734100342, loss=1.5418955087661743
I0203 05:08:27.381318 139907712378624 logging_writer.py:48] [81500] global_step=81500, grad_norm=2.4816269874572754, loss=1.4339797496795654
I0203 05:09:01.011412 139907720771328 logging_writer.py:48] [81600] global_step=81600, grad_norm=2.0327887535095215, loss=1.403731346130371
I0203 05:09:34.700098 139907712378624 logging_writer.py:48] [81700] global_step=81700, grad_norm=2.344226598739624, loss=1.605595588684082
I0203 05:09:35.525044 140070692116288 spec.py:321] Evaluating on the training split.
I0203 05:09:41.876767 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 05:09:50.258104 140070692116288 spec.py:349] Evaluating on the test split.
I0203 05:09:52.916904 140070692116288 submission_runner.py:408] Time since start: 28550.04s, 	Step: 81704, 	{'train/accuracy': 0.7504384517669678, 'train/loss': 0.9462156295776367, 'validation/accuracy': 0.6738199591636658, 'validation/loss': 1.321738362312317, 'validation/num_examples': 50000, 'test/accuracy': 0.5506000518798828, 'test/loss': 2.0084962844848633, 'test/num_examples': 10000, 'score': 27576.591657161713, 'total_duration': 28550.037356376648, 'accumulated_submission_time': 27576.591657161713, 'accumulated_eval_time': 968.7020602226257, 'accumulated_logging_time': 1.9029486179351807}
I0203 05:09:52.948577 139907712378624 logging_writer.py:48] [81704] accumulated_eval_time=968.702060, accumulated_logging_time=1.902949, accumulated_submission_time=27576.591657, global_step=81704, preemption_count=0, score=27576.591657, test/accuracy=0.550600, test/loss=2.008496, test/num_examples=10000, total_duration=28550.037356, train/accuracy=0.750438, train/loss=0.946216, validation/accuracy=0.673820, validation/loss=1.321738, validation/num_examples=50000
I0203 05:10:25.611108 139907745949440 logging_writer.py:48] [81800] global_step=81800, grad_norm=2.1753170490264893, loss=1.4113942384719849
I0203 05:10:59.283817 139907712378624 logging_writer.py:48] [81900] global_step=81900, grad_norm=1.9950915575027466, loss=1.422669768333435
I0203 05:11:32.979252 139907745949440 logging_writer.py:48] [82000] global_step=82000, grad_norm=2.0615038871765137, loss=1.350477695465088
I0203 05:12:06.648096 139907712378624 logging_writer.py:48] [82100] global_step=82100, grad_norm=2.1883344650268555, loss=1.522261381149292
I0203 05:12:40.350488 139907745949440 logging_writer.py:48] [82200] global_step=82200, grad_norm=2.089573621749878, loss=1.4240429401397705
I0203 05:13:14.013672 139907712378624 logging_writer.py:48] [82300] global_step=82300, grad_norm=2.1492998600006104, loss=1.4947402477264404
I0203 05:13:47.692638 139907745949440 logging_writer.py:48] [82400] global_step=82400, grad_norm=2.091064929962158, loss=1.3562414646148682
I0203 05:14:21.560967 139907712378624 logging_writer.py:48] [82500] global_step=82500, grad_norm=2.010094404220581, loss=1.418022871017456
I0203 05:14:55.328881 139907745949440 logging_writer.py:48] [82600] global_step=82600, grad_norm=2.1655964851379395, loss=1.4050854444503784
I0203 05:15:28.993258 139907712378624 logging_writer.py:48] [82700] global_step=82700, grad_norm=2.592872381210327, loss=1.4691879749298096
I0203 05:16:02.686275 139907745949440 logging_writer.py:48] [82800] global_step=82800, grad_norm=2.314671277999878, loss=1.593157410621643
I0203 05:16:36.357312 139907712378624 logging_writer.py:48] [82900] global_step=82900, grad_norm=2.303992509841919, loss=1.3743128776550293
I0203 05:17:10.045867 139907745949440 logging_writer.py:48] [83000] global_step=83000, grad_norm=2.196722984313965, loss=1.5394926071166992
I0203 05:17:43.696192 139907712378624 logging_writer.py:48] [83100] global_step=83100, grad_norm=2.1826138496398926, loss=1.4867050647735596
I0203 05:18:17.391349 139907745949440 logging_writer.py:48] [83200] global_step=83200, grad_norm=2.0630059242248535, loss=1.3535503149032593
I0203 05:18:22.919529 140070692116288 spec.py:321] Evaluating on the training split.
I0203 05:18:29.242461 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 05:18:37.583096 140070692116288 spec.py:349] Evaluating on the test split.
I0203 05:18:40.263662 140070692116288 submission_runner.py:408] Time since start: 29077.38s, 	Step: 83218, 	{'train/accuracy': 0.7326610088348389, 'train/loss': 1.0291450023651123, 'validation/accuracy': 0.6612399816513062, 'validation/loss': 1.3827193975448608, 'validation/num_examples': 50000, 'test/accuracy': 0.5326000452041626, 'test/loss': 2.109022378921509, 'test/num_examples': 10000, 'score': 28086.49978995323, 'total_duration': 29077.384110450745, 'accumulated_submission_time': 28086.49978995323, 'accumulated_eval_time': 986.0461583137512, 'accumulated_logging_time': 1.945774793624878}
I0203 05:18:40.296254 139907737556736 logging_writer.py:48] [83218] accumulated_eval_time=986.046158, accumulated_logging_time=1.945775, accumulated_submission_time=28086.499790, global_step=83218, preemption_count=0, score=28086.499790, test/accuracy=0.532600, test/loss=2.109022, test/num_examples=10000, total_duration=29077.384110, train/accuracy=0.732661, train/loss=1.029145, validation/accuracy=0.661240, validation/loss=1.382719, validation/num_examples=50000
I0203 05:19:08.287120 139907762734848 logging_writer.py:48] [83300] global_step=83300, grad_norm=2.2065672874450684, loss=1.4352328777313232
I0203 05:19:42.012725 139907737556736 logging_writer.py:48] [83400] global_step=83400, grad_norm=2.1566479206085205, loss=1.4339855909347534
I0203 05:20:15.668082 139907762734848 logging_writer.py:48] [83500] global_step=83500, grad_norm=2.295090436935425, loss=1.4402469396591187
I0203 05:20:49.463526 139907737556736 logging_writer.py:48] [83600] global_step=83600, grad_norm=2.167933225631714, loss=1.4764037132263184
I0203 05:21:23.174473 139907762734848 logging_writer.py:48] [83700] global_step=83700, grad_norm=2.1807403564453125, loss=1.4509353637695312
I0203 05:21:56.889920 139907737556736 logging_writer.py:48] [83800] global_step=83800, grad_norm=2.171229839324951, loss=1.4698173999786377
I0203 05:22:30.625932 139907762734848 logging_writer.py:48] [83900] global_step=83900, grad_norm=2.135115623474121, loss=1.4307467937469482
I0203 05:23:04.347650 139907737556736 logging_writer.py:48] [84000] global_step=84000, grad_norm=2.3351027965545654, loss=1.472586750984192
I0203 05:23:38.063525 139907762734848 logging_writer.py:48] [84100] global_step=84100, grad_norm=2.3053297996520996, loss=1.5177087783813477
I0203 05:24:11.794866 139907737556736 logging_writer.py:48] [84200] global_step=84200, grad_norm=2.1240792274475098, loss=1.386406421661377
I0203 05:24:45.472056 139907762734848 logging_writer.py:48] [84300] global_step=84300, grad_norm=2.203434944152832, loss=1.3704683780670166
I0203 05:25:19.216099 139907737556736 logging_writer.py:48] [84400] global_step=84400, grad_norm=2.3554468154907227, loss=1.44972825050354
I0203 05:25:52.944091 139907762734848 logging_writer.py:48] [84500] global_step=84500, grad_norm=2.22525954246521, loss=1.449865698814392
I0203 05:26:26.677234 139907737556736 logging_writer.py:48] [84600] global_step=84600, grad_norm=2.1112759113311768, loss=1.409740686416626
I0203 05:27:00.341430 139907762734848 logging_writer.py:48] [84700] global_step=84700, grad_norm=2.0362088680267334, loss=1.4241435527801514
I0203 05:27:10.596352 140070692116288 spec.py:321] Evaluating on the training split.
I0203 05:27:16.901402 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 05:27:25.694168 140070692116288 spec.py:349] Evaluating on the test split.
I0203 05:27:28.454739 140070692116288 submission_runner.py:408] Time since start: 29605.58s, 	Step: 84732, 	{'train/accuracy': 0.7528101205825806, 'train/loss': 0.9364287257194519, 'validation/accuracy': 0.6754199862480164, 'validation/loss': 1.3129218816757202, 'validation/num_examples': 50000, 'test/accuracy': 0.5491000413894653, 'test/loss': 2.0207293033599854, 'test/num_examples': 10000, 'score': 28596.737845897675, 'total_duration': 29605.5751888752, 'accumulated_submission_time': 28596.737845897675, 'accumulated_eval_time': 1003.904506444931, 'accumulated_logging_time': 1.9876015186309814}
I0203 05:27:28.489349 139907720771328 logging_writer.py:48] [84732] accumulated_eval_time=1003.904506, accumulated_logging_time=1.987602, accumulated_submission_time=28596.737846, global_step=84732, preemption_count=0, score=28596.737846, test/accuracy=0.549100, test/loss=2.020729, test/num_examples=10000, total_duration=29605.575189, train/accuracy=0.752810, train/loss=0.936429, validation/accuracy=0.675420, validation/loss=1.312922, validation/num_examples=50000
I0203 05:27:51.761250 139907729164032 logging_writer.py:48] [84800] global_step=84800, grad_norm=2.3310365676879883, loss=1.356137752532959
I0203 05:28:25.458925 139907720771328 logging_writer.py:48] [84900] global_step=84900, grad_norm=2.1942107677459717, loss=1.4340444803237915
I0203 05:28:59.127311 139907729164032 logging_writer.py:48] [85000] global_step=85000, grad_norm=2.166717290878296, loss=1.4913966655731201
I0203 05:29:32.816688 139907720771328 logging_writer.py:48] [85100] global_step=85100, grad_norm=2.1635470390319824, loss=1.4583371877670288
I0203 05:30:06.494289 139907729164032 logging_writer.py:48] [85200] global_step=85200, grad_norm=2.4941298961639404, loss=1.4194309711456299
I0203 05:30:40.191318 139907720771328 logging_writer.py:48] [85300] global_step=85300, grad_norm=2.142544984817505, loss=1.466618537902832
I0203 05:31:13.870473 139907729164032 logging_writer.py:48] [85400] global_step=85400, grad_norm=2.0987672805786133, loss=1.4003630876541138
I0203 05:31:47.566891 139907720771328 logging_writer.py:48] [85500] global_step=85500, grad_norm=2.3516552448272705, loss=1.511244297027588
I0203 05:32:21.221078 139907729164032 logging_writer.py:48] [85600] global_step=85600, grad_norm=2.184248924255371, loss=1.4389607906341553
I0203 05:32:54.918260 139907720771328 logging_writer.py:48] [85700] global_step=85700, grad_norm=2.2811667919158936, loss=1.4985871315002441
I0203 05:33:28.574452 139907729164032 logging_writer.py:48] [85800] global_step=85800, grad_norm=2.580162763595581, loss=1.4550808668136597
I0203 05:34:02.352206 139907720771328 logging_writer.py:48] [85900] global_step=85900, grad_norm=2.1539595127105713, loss=1.4138636589050293
I0203 05:34:36.070817 139907729164032 logging_writer.py:48] [86000] global_step=86000, grad_norm=2.13912296295166, loss=1.4363733530044556
I0203 05:35:09.798524 139907720771328 logging_writer.py:48] [86100] global_step=86100, grad_norm=2.475023031234741, loss=1.5231348276138306
I0203 05:35:43.458480 139907729164032 logging_writer.py:48] [86200] global_step=86200, grad_norm=2.346198320388794, loss=1.4189306497573853
I0203 05:35:58.777215 140070692116288 spec.py:321] Evaluating on the training split.
I0203 05:36:05.222972 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 05:36:13.577923 140070692116288 spec.py:349] Evaluating on the test split.
I0203 05:36:16.242434 140070692116288 submission_runner.py:408] Time since start: 30133.36s, 	Step: 86247, 	{'train/accuracy': 0.7684949040412903, 'train/loss': 0.8733639717102051, 'validation/accuracy': 0.6672799587249756, 'validation/loss': 1.357527494430542, 'validation/num_examples': 50000, 'test/accuracy': 0.5403000116348267, 'test/loss': 2.0849599838256836, 'test/num_examples': 10000, 'score': 29106.95946264267, 'total_duration': 30133.36285853386, 'accumulated_submission_time': 29106.95946264267, 'accumulated_eval_time': 1021.369663476944, 'accumulated_logging_time': 2.0333995819091797}
I0203 05:36:16.283446 139907703985920 logging_writer.py:48] [86247] accumulated_eval_time=1021.369663, accumulated_logging_time=2.033400, accumulated_submission_time=29106.959463, global_step=86247, preemption_count=0, score=29106.959463, test/accuracy=0.540300, test/loss=2.084960, test/num_examples=10000, total_duration=30133.362859, train/accuracy=0.768495, train/loss=0.873364, validation/accuracy=0.667280, validation/loss=1.357527, validation/num_examples=50000
I0203 05:36:34.510980 139907712378624 logging_writer.py:48] [86300] global_step=86300, grad_norm=2.276010036468506, loss=1.509481430053711
I0203 05:37:08.170700 139907703985920 logging_writer.py:48] [86400] global_step=86400, grad_norm=2.3215277194976807, loss=1.5217983722686768
I0203 05:37:41.849496 139907712378624 logging_writer.py:48] [86500] global_step=86500, grad_norm=2.4479777812957764, loss=1.4757232666015625
I0203 05:38:15.517603 139907703985920 logging_writer.py:48] [86600] global_step=86600, grad_norm=2.2891128063201904, loss=1.4241423606872559
I0203 05:38:49.197563 139907712378624 logging_writer.py:48] [86700] global_step=86700, grad_norm=2.313760280609131, loss=1.430922031402588
I0203 05:39:22.879375 139907703985920 logging_writer.py:48] [86800] global_step=86800, grad_norm=2.463107109069824, loss=1.5135654211044312
I0203 05:39:56.548348 139907712378624 logging_writer.py:48] [86900] global_step=86900, grad_norm=2.060068130493164, loss=1.4305812120437622
I0203 05:40:30.324920 139907703985920 logging_writer.py:48] [87000] global_step=87000, grad_norm=2.269573450088501, loss=1.3622636795043945
I0203 05:41:04.065941 139907712378624 logging_writer.py:48] [87100] global_step=87100, grad_norm=2.580359935760498, loss=1.526469349861145
I0203 05:41:37.757681 139907703985920 logging_writer.py:48] [87200] global_step=87200, grad_norm=2.507957935333252, loss=1.3998628854751587
I0203 05:42:11.438331 139907712378624 logging_writer.py:48] [87300] global_step=87300, grad_norm=2.1178455352783203, loss=1.3552348613739014
I0203 05:42:45.123955 139907703985920 logging_writer.py:48] [87400] global_step=87400, grad_norm=2.078176736831665, loss=1.367453694343567
I0203 05:43:18.859385 139907712378624 logging_writer.py:48] [87500] global_step=87500, grad_norm=2.3213305473327637, loss=1.4257994890213013
I0203 05:43:52.521960 139907703985920 logging_writer.py:48] [87600] global_step=87600, grad_norm=2.329301595687866, loss=1.4572083950042725
I0203 05:44:26.250344 139907712378624 logging_writer.py:48] [87700] global_step=87700, grad_norm=2.2649240493774414, loss=1.437298059463501
I0203 05:44:46.249338 140070692116288 spec.py:321] Evaluating on the training split.
I0203 05:44:52.584461 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 05:45:01.344640 140070692116288 spec.py:349] Evaluating on the test split.
I0203 05:45:04.153395 140070692116288 submission_runner.py:408] Time since start: 30661.27s, 	Step: 87761, 	{'train/accuracy': 0.7531489133834839, 'train/loss': 0.9372791051864624, 'validation/accuracy': 0.6677599549293518, 'validation/loss': 1.3522411584854126, 'validation/num_examples': 50000, 'test/accuracy': 0.534600019454956, 'test/loss': 2.101754903793335, 'test/num_examples': 10000, 'score': 29616.86198425293, 'total_duration': 30661.273845672607, 'accumulated_submission_time': 29616.86198425293, 'accumulated_eval_time': 1039.2736871242523, 'accumulated_logging_time': 2.084794282913208}
I0203 05:45:04.188391 139907754342144 logging_writer.py:48] [87761] accumulated_eval_time=1039.273687, accumulated_logging_time=2.084794, accumulated_submission_time=29616.861984, global_step=87761, preemption_count=0, score=29616.861984, test/accuracy=0.534600, test/loss=2.101755, test/num_examples=10000, total_duration=30661.273846, train/accuracy=0.753149, train/loss=0.937279, validation/accuracy=0.667760, validation/loss=1.352241, validation/num_examples=50000
I0203 05:45:17.684347 139907762734848 logging_writer.py:48] [87800] global_step=87800, grad_norm=2.244335651397705, loss=1.5296216011047363
I0203 05:45:51.344461 139907754342144 logging_writer.py:48] [87900] global_step=87900, grad_norm=2.2277672290802, loss=1.3871185779571533
I0203 05:46:25.038940 139907762734848 logging_writer.py:48] [88000] global_step=88000, grad_norm=2.4491963386535645, loss=1.4666838645935059
I0203 05:46:58.692317 139907754342144 logging_writer.py:48] [88100] global_step=88100, grad_norm=2.172191858291626, loss=1.3939586877822876
I0203 05:47:32.528062 139907762734848 logging_writer.py:48] [88200] global_step=88200, grad_norm=2.0208261013031006, loss=1.486788034439087
I0203 05:48:06.184502 139907754342144 logging_writer.py:48] [88300] global_step=88300, grad_norm=2.3665850162506104, loss=1.449116826057434
I0203 05:48:39.896592 139907762734848 logging_writer.py:48] [88400] global_step=88400, grad_norm=2.2143635749816895, loss=1.3906575441360474
I0203 05:49:13.567753 139907754342144 logging_writer.py:48] [88500] global_step=88500, grad_norm=2.195967197418213, loss=1.4195592403411865
I0203 05:49:47.278145 139907762734848 logging_writer.py:48] [88600] global_step=88600, grad_norm=2.2617084980010986, loss=1.4016964435577393
I0203 05:50:20.938590 139907754342144 logging_writer.py:48] [88700] global_step=88700, grad_norm=2.166510581970215, loss=1.4471209049224854
I0203 05:50:54.613229 139907762734848 logging_writer.py:48] [88800] global_step=88800, grad_norm=2.210577964782715, loss=1.4231871366500854
I0203 05:51:28.346083 139907754342144 logging_writer.py:48] [88900] global_step=88900, grad_norm=2.034707546234131, loss=1.4170002937316895
I0203 05:52:02.060331 139907762734848 logging_writer.py:48] [89000] global_step=89000, grad_norm=2.2347452640533447, loss=1.4068201780319214
I0203 05:52:35.718562 139907754342144 logging_writer.py:48] [89100] global_step=89100, grad_norm=2.3353753089904785, loss=1.3186649084091187
I0203 05:53:09.407048 139907762734848 logging_writer.py:48] [89200] global_step=89200, grad_norm=2.1549627780914307, loss=1.3503825664520264
I0203 05:53:34.233106 140070692116288 spec.py:321] Evaluating on the training split.
I0203 05:53:40.614834 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 05:53:48.996459 140070692116288 spec.py:349] Evaluating on the test split.
I0203 05:53:51.627809 140070692116288 submission_runner.py:408] Time since start: 31188.75s, 	Step: 89275, 	{'train/accuracy': 0.748465359210968, 'train/loss': 0.9442353248596191, 'validation/accuracy': 0.6695799827575684, 'validation/loss': 1.3373024463653564, 'validation/num_examples': 50000, 'test/accuracy': 0.5367000102996826, 'test/loss': 2.0899970531463623, 'test/num_examples': 10000, 'score': 30126.845595359802, 'total_duration': 31188.748248815536, 'accumulated_submission_time': 30126.845595359802, 'accumulated_eval_time': 1056.6683535575867, 'accumulated_logging_time': 2.1289286613464355}
I0203 05:53:51.658469 139907703985920 logging_writer.py:48] [89275] accumulated_eval_time=1056.668354, accumulated_logging_time=2.128929, accumulated_submission_time=30126.845595, global_step=89275, preemption_count=0, score=30126.845595, test/accuracy=0.536700, test/loss=2.089997, test/num_examples=10000, total_duration=31188.748249, train/accuracy=0.748465, train/loss=0.944235, validation/accuracy=0.669580, validation/loss=1.337302, validation/num_examples=50000
I0203 05:54:00.414448 139907712378624 logging_writer.py:48] [89300] global_step=89300, grad_norm=2.231309413909912, loss=1.3489667177200317
I0203 05:54:34.009257 139907703985920 logging_writer.py:48] [89400] global_step=89400, grad_norm=2.328808307647705, loss=1.3644096851348877
I0203 05:55:07.740269 139907712378624 logging_writer.py:48] [89500] global_step=89500, grad_norm=2.2869513034820557, loss=1.3716731071472168
I0203 05:55:41.460816 139907703985920 logging_writer.py:48] [89600] global_step=89600, grad_norm=2.4245238304138184, loss=1.4340397119522095
I0203 05:56:15.158801 139907712378624 logging_writer.py:48] [89700] global_step=89700, grad_norm=2.375056743621826, loss=1.4852373600006104
I0203 05:56:48.890325 139907703985920 logging_writer.py:48] [89800] global_step=89800, grad_norm=2.315479040145874, loss=1.432691216468811
I0203 05:57:22.558863 139907712378624 logging_writer.py:48] [89900] global_step=89900, grad_norm=2.1786088943481445, loss=1.3707175254821777
I0203 05:57:56.281905 139907703985920 logging_writer.py:48] [90000] global_step=90000, grad_norm=2.253256320953369, loss=1.396977424621582
I0203 05:58:29.982590 139907712378624 logging_writer.py:48] [90100] global_step=90100, grad_norm=2.486588478088379, loss=1.4618725776672363
I0203 05:59:03.657430 139907703985920 logging_writer.py:48] [90200] global_step=90200, grad_norm=2.304426908493042, loss=1.4116171598434448
I0203 05:59:37.340332 139907712378624 logging_writer.py:48] [90300] global_step=90300, grad_norm=2.315836191177368, loss=1.5114109516143799
I0203 06:00:11.207694 139907703985920 logging_writer.py:48] [90400] global_step=90400, grad_norm=2.1296298503875732, loss=1.4717671871185303
I0203 06:00:44.957838 139907712378624 logging_writer.py:48] [90500] global_step=90500, grad_norm=2.2313084602355957, loss=1.4916117191314697
I0203 06:01:18.626732 139907703985920 logging_writer.py:48] [90600] global_step=90600, grad_norm=2.3524441719055176, loss=1.3552722930908203
I0203 06:01:52.344310 139907712378624 logging_writer.py:48] [90700] global_step=90700, grad_norm=2.175548553466797, loss=1.3619812726974487
I0203 06:02:21.803530 140070692116288 spec.py:321] Evaluating on the training split.
I0203 06:02:28.280750 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 06:02:36.487536 140070692116288 spec.py:349] Evaluating on the test split.
I0203 06:02:39.158347 140070692116288 submission_runner.py:408] Time since start: 31716.28s, 	Step: 90789, 	{'train/accuracy': 0.7472097873687744, 'train/loss': 0.958838939666748, 'validation/accuracy': 0.6690399646759033, 'validation/loss': 1.3475358486175537, 'validation/num_examples': 50000, 'test/accuracy': 0.5403000116348267, 'test/loss': 2.083833694458008, 'test/num_examples': 10000, 'score': 30636.927599668503, 'total_duration': 31716.278796434402, 'accumulated_submission_time': 30636.927599668503, 'accumulated_eval_time': 1074.0231430530548, 'accumulated_logging_time': 2.1696858406066895}
I0203 06:02:39.192635 139907703985920 logging_writer.py:48] [90789] accumulated_eval_time=1074.023143, accumulated_logging_time=2.169686, accumulated_submission_time=30636.927600, global_step=90789, preemption_count=0, score=30636.927600, test/accuracy=0.540300, test/loss=2.083834, test/num_examples=10000, total_duration=31716.278796, train/accuracy=0.747210, train/loss=0.958839, validation/accuracy=0.669040, validation/loss=1.347536, validation/num_examples=50000
I0203 06:02:43.238378 139907712378624 logging_writer.py:48] [90800] global_step=90800, grad_norm=2.2613673210144043, loss=1.459930419921875
I0203 06:03:16.905032 139907703985920 logging_writer.py:48] [90900] global_step=90900, grad_norm=2.215301752090454, loss=1.3705013990402222
I0203 06:03:50.614460 139907712378624 logging_writer.py:48] [91000] global_step=91000, grad_norm=2.423882007598877, loss=1.3635765314102173
I0203 06:04:24.335808 139907703985920 logging_writer.py:48] [91100] global_step=91100, grad_norm=2.382805347442627, loss=1.2966645956039429
I0203 06:04:58.048398 139907712378624 logging_writer.py:48] [91200] global_step=91200, grad_norm=2.2599170207977295, loss=1.3168494701385498
I0203 06:05:31.811725 139907703985920 logging_writer.py:48] [91300] global_step=91300, grad_norm=2.4320430755615234, loss=1.4062641859054565
I0203 06:06:05.472317 139907712378624 logging_writer.py:48] [91400] global_step=91400, grad_norm=2.114884853363037, loss=1.201490044593811
I0203 06:06:39.305118 139907703985920 logging_writer.py:48] [91500] global_step=91500, grad_norm=2.2428948879241943, loss=1.4979074001312256
I0203 06:07:12.988701 139907712378624 logging_writer.py:48] [91600] global_step=91600, grad_norm=2.524163246154785, loss=1.4088592529296875
I0203 06:07:46.695282 139907703985920 logging_writer.py:48] [91700] global_step=91700, grad_norm=2.4818503856658936, loss=1.5987184047698975
I0203 06:08:20.367609 139907712378624 logging_writer.py:48] [91800] global_step=91800, grad_norm=2.0657505989074707, loss=1.3760948181152344
I0203 06:08:54.072472 139907703985920 logging_writer.py:48] [91900] global_step=91900, grad_norm=2.3309006690979004, loss=1.2986786365509033
I0203 06:09:27.737383 139907712378624 logging_writer.py:48] [92000] global_step=92000, grad_norm=2.51225209236145, loss=1.3853672742843628
I0203 06:10:01.453894 139907703985920 logging_writer.py:48] [92100] global_step=92100, grad_norm=2.262363910675049, loss=1.338884949684143
I0203 06:10:35.091385 139907712378624 logging_writer.py:48] [92200] global_step=92200, grad_norm=2.3726444244384766, loss=1.3386286497116089
I0203 06:11:08.785774 139907703985920 logging_writer.py:48] [92300] global_step=92300, grad_norm=2.2670633792877197, loss=1.2626957893371582
I0203 06:11:09.275178 140070692116288 spec.py:321] Evaluating on the training split.
I0203 06:11:15.556458 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 06:11:23.880007 140070692116288 spec.py:349] Evaluating on the test split.
I0203 06:11:26.568256 140070692116288 submission_runner.py:408] Time since start: 32243.69s, 	Step: 92303, 	{'train/accuracy': 0.7487842440605164, 'train/loss': 0.9438230991363525, 'validation/accuracy': 0.6720799803733826, 'validation/loss': 1.337753415107727, 'validation/num_examples': 50000, 'test/accuracy': 0.5448000431060791, 'test/loss': 2.058915138244629, 'test/num_examples': 10000, 'score': 31146.947617292404, 'total_duration': 32243.688675642014, 'accumulated_submission_time': 31146.947617292404, 'accumulated_eval_time': 1091.3161630630493, 'accumulated_logging_time': 2.213765859603882}
I0203 06:11:26.601915 139907737556736 logging_writer.py:48] [92303] accumulated_eval_time=1091.316163, accumulated_logging_time=2.213766, accumulated_submission_time=31146.947617, global_step=92303, preemption_count=0, score=31146.947617, test/accuracy=0.544800, test/loss=2.058915, test/num_examples=10000, total_duration=32243.688676, train/accuracy=0.748784, train/loss=0.943823, validation/accuracy=0.672080, validation/loss=1.337753, validation/num_examples=50000
I0203 06:11:59.657136 139907745949440 logging_writer.py:48] [92400] global_step=92400, grad_norm=2.3024098873138428, loss=1.3312773704528809
I0203 06:12:33.357848 139907737556736 logging_writer.py:48] [92500] global_step=92500, grad_norm=2.3112709522247314, loss=1.366266131401062
I0203 06:13:07.132537 139907745949440 logging_writer.py:48] [92600] global_step=92600, grad_norm=2.3404839038848877, loss=1.3702176809310913
I0203 06:13:40.852480 139907737556736 logging_writer.py:48] [92700] global_step=92700, grad_norm=2.4737210273742676, loss=1.3877278566360474
I0203 06:14:14.939390 139907745949440 logging_writer.py:48] [92800] global_step=92800, grad_norm=2.382124662399292, loss=1.4726406335830688
I0203 06:14:48.649787 139907737556736 logging_writer.py:48] [92900] global_step=92900, grad_norm=2.335951566696167, loss=1.4599707126617432
I0203 06:15:22.342824 139907745949440 logging_writer.py:48] [93000] global_step=93000, grad_norm=2.183945655822754, loss=1.3395365476608276
I0203 06:15:56.064244 139907737556736 logging_writer.py:48] [93100] global_step=93100, grad_norm=2.3868465423583984, loss=1.452520728111267
I0203 06:16:29.759612 139907745949440 logging_writer.py:48] [93200] global_step=93200, grad_norm=2.2033801078796387, loss=1.3302509784698486
I0203 06:17:03.503457 139907737556736 logging_writer.py:48] [93300] global_step=93300, grad_norm=2.6880149841308594, loss=1.422101378440857
I0203 06:17:37.270590 139907745949440 logging_writer.py:48] [93400] global_step=93400, grad_norm=2.295865774154663, loss=1.3902170658111572
I0203 06:18:10.952245 139907737556736 logging_writer.py:48] [93500] global_step=93500, grad_norm=2.261892557144165, loss=1.3772188425064087
I0203 06:18:44.727634 139907745949440 logging_writer.py:48] [93600] global_step=93600, grad_norm=2.526289939880371, loss=1.384244680404663
I0203 06:19:18.406260 139907737556736 logging_writer.py:48] [93700] global_step=93700, grad_norm=2.4630439281463623, loss=1.4837344884872437
I0203 06:19:52.173037 139907745949440 logging_writer.py:48] [93800] global_step=93800, grad_norm=2.53902006149292, loss=1.3196635246276855
I0203 06:19:56.717957 140070692116288 spec.py:321] Evaluating on the training split.
I0203 06:20:03.954036 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 06:20:12.299565 140070692116288 spec.py:349] Evaluating on the test split.
I0203 06:20:14.979197 140070692116288 submission_runner.py:408] Time since start: 32772.10s, 	Step: 93815, 	{'train/accuracy': 0.7768255472183228, 'train/loss': 0.8307573795318604, 'validation/accuracy': 0.6793599724769592, 'validation/loss': 1.3027634620666504, 'validation/num_examples': 50000, 'test/accuracy': 0.5523000359535217, 'test/loss': 2.045220375061035, 'test/num_examples': 10000, 'score': 31657.000898361206, 'total_duration': 32772.09962654114, 'accumulated_submission_time': 31657.000898361206, 'accumulated_eval_time': 1109.5773482322693, 'accumulated_logging_time': 2.25687313079834}
I0203 06:20:15.014776 139907251042048 logging_writer.py:48] [93815] accumulated_eval_time=1109.577348, accumulated_logging_time=2.256873, accumulated_submission_time=31657.000898, global_step=93815, preemption_count=0, score=31657.000898, test/accuracy=0.552300, test/loss=2.045220, test/num_examples=10000, total_duration=32772.099627, train/accuracy=0.776826, train/loss=0.830757, validation/accuracy=0.679360, validation/loss=1.302763, validation/num_examples=50000
I0203 06:20:43.983047 139907703985920 logging_writer.py:48] [93900] global_step=93900, grad_norm=2.39418363571167, loss=1.5061274766921997
I0203 06:21:17.670040 139907251042048 logging_writer.py:48] [94000] global_step=94000, grad_norm=2.3247711658477783, loss=1.3548613786697388
I0203 06:21:51.329900 139907703985920 logging_writer.py:48] [94100] global_step=94100, grad_norm=2.6744749546051025, loss=1.4352493286132812
I0203 06:22:25.033185 139907251042048 logging_writer.py:48] [94200] global_step=94200, grad_norm=2.3566555976867676, loss=1.3236004114151
I0203 06:22:58.730384 139907703985920 logging_writer.py:48] [94300] global_step=94300, grad_norm=2.683465003967285, loss=1.437790870666504
I0203 06:23:32.467068 139907251042048 logging_writer.py:48] [94400] global_step=94400, grad_norm=2.3419127464294434, loss=1.3936622142791748
I0203 06:24:06.123506 139907703985920 logging_writer.py:48] [94500] global_step=94500, grad_norm=2.1860313415527344, loss=1.352546215057373
I0203 06:24:39.799282 139907251042048 logging_writer.py:48] [94600] global_step=94600, grad_norm=2.3177337646484375, loss=1.311184287071228
I0203 06:25:13.466147 139907703985920 logging_writer.py:48] [94700] global_step=94700, grad_norm=2.3777642250061035, loss=1.3937405347824097
I0203 06:25:47.166166 139907251042048 logging_writer.py:48] [94800] global_step=94800, grad_norm=2.335216999053955, loss=1.304849624633789
I0203 06:26:20.954741 139907703985920 logging_writer.py:48] [94900] global_step=94900, grad_norm=2.4325764179229736, loss=1.3567256927490234
I0203 06:26:54.672009 139907251042048 logging_writer.py:48] [95000] global_step=95000, grad_norm=2.4112918376922607, loss=1.4727463722229004
I0203 06:27:28.326270 139907703985920 logging_writer.py:48] [95100] global_step=95100, grad_norm=2.2759525775909424, loss=1.353965401649475
I0203 06:28:02.023798 139907251042048 logging_writer.py:48] [95200] global_step=95200, grad_norm=2.3653159141540527, loss=1.3826453685760498
I0203 06:28:35.689475 139907703985920 logging_writer.py:48] [95300] global_step=95300, grad_norm=2.4855117797851562, loss=1.4789795875549316
I0203 06:28:45.282445 140070692116288 spec.py:321] Evaluating on the training split.
I0203 06:28:51.697538 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 06:28:59.977567 140070692116288 spec.py:349] Evaluating on the test split.
I0203 06:29:02.674065 140070692116288 submission_runner.py:408] Time since start: 33299.79s, 	Step: 95330, 	{'train/accuracy': 0.7841796875, 'train/loss': 0.8101245760917664, 'validation/accuracy': 0.6791799664497375, 'validation/loss': 1.3027790784835815, 'validation/num_examples': 50000, 'test/accuracy': 0.5487000346183777, 'test/loss': 2.0057966709136963, 'test/num_examples': 10000, 'score': 32167.20648097992, 'total_duration': 33299.79450273514, 'accumulated_submission_time': 32167.20648097992, 'accumulated_eval_time': 1126.9689333438873, 'accumulated_logging_time': 2.3015289306640625}
I0203 06:29:02.713094 139907703985920 logging_writer.py:48] [95330] accumulated_eval_time=1126.968933, accumulated_logging_time=2.301529, accumulated_submission_time=32167.206481, global_step=95330, preemption_count=0, score=32167.206481, test/accuracy=0.548700, test/loss=2.005797, test/num_examples=10000, total_duration=33299.794503, train/accuracy=0.784180, train/loss=0.810125, validation/accuracy=0.679180, validation/loss=1.302779, validation/num_examples=50000
I0203 06:29:26.633653 139907745949440 logging_writer.py:48] [95400] global_step=95400, grad_norm=2.364305257797241, loss=1.2635502815246582
I0203 06:30:00.320628 139907703985920 logging_writer.py:48] [95500] global_step=95500, grad_norm=2.4525399208068848, loss=1.404241919517517
I0203 06:30:33.982068 139907745949440 logging_writer.py:48] [95600] global_step=95600, grad_norm=2.575024366378784, loss=1.317745327949524
I0203 06:31:07.680657 139907703985920 logging_writer.py:48] [95700] global_step=95700, grad_norm=2.4801409244537354, loss=1.3825647830963135
I0203 06:31:41.418282 139907745949440 logging_writer.py:48] [95800] global_step=95800, grad_norm=2.434669256210327, loss=1.4625837802886963
I0203 06:32:15.122065 139907703985920 logging_writer.py:48] [95900] global_step=95900, grad_norm=2.2379794120788574, loss=1.3858401775360107
I0203 06:32:48.874045 139907745949440 logging_writer.py:48] [96000] global_step=96000, grad_norm=2.3759922981262207, loss=1.2974340915679932
I0203 06:33:22.604880 139907703985920 logging_writer.py:48] [96100] global_step=96100, grad_norm=2.372447967529297, loss=1.3744958639144897
I0203 06:33:56.254244 139907745949440 logging_writer.py:48] [96200] global_step=96200, grad_norm=2.432612895965576, loss=1.361414909362793
I0203 06:34:29.967254 139907703985920 logging_writer.py:48] [96300] global_step=96300, grad_norm=2.426342725753784, loss=1.4221469163894653
I0203 06:35:03.640457 139907745949440 logging_writer.py:48] [96400] global_step=96400, grad_norm=2.725351095199585, loss=1.4834568500518799
I0203 06:35:37.318868 139907703985920 logging_writer.py:48] [96500] global_step=96500, grad_norm=2.262830972671509, loss=1.347381830215454
I0203 06:36:10.971248 139907745949440 logging_writer.py:48] [96600] global_step=96600, grad_norm=2.584071397781372, loss=1.4358996152877808
I0203 06:36:44.665507 139907703985920 logging_writer.py:48] [96700] global_step=96700, grad_norm=2.3076469898223877, loss=1.311208963394165
I0203 06:37:18.340134 139907745949440 logging_writer.py:48] [96800] global_step=96800, grad_norm=2.3817622661590576, loss=1.4483064413070679
I0203 06:37:32.686989 140070692116288 spec.py:321] Evaluating on the training split.
I0203 06:37:38.924395 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 06:37:47.393116 140070692116288 spec.py:349] Evaluating on the test split.
I0203 06:37:50.063952 140070692116288 submission_runner.py:408] Time since start: 33827.18s, 	Step: 96844, 	{'train/accuracy': 0.7759685516357422, 'train/loss': 0.8316084146499634, 'validation/accuracy': 0.6843999624252319, 'validation/loss': 1.2773557901382446, 'validation/num_examples': 50000, 'test/accuracy': 0.5568000078201294, 'test/loss': 2.0034146308898926, 'test/num_examples': 10000, 'score': 32677.118038654327, 'total_duration': 33827.184403419495, 'accumulated_submission_time': 32677.118038654327, 'accumulated_eval_time': 1144.3458700180054, 'accumulated_logging_time': 2.350280284881592}
I0203 06:37:50.101365 139907703985920 logging_writer.py:48] [96844] accumulated_eval_time=1144.345870, accumulated_logging_time=2.350280, accumulated_submission_time=32677.118039, global_step=96844, preemption_count=0, score=32677.118039, test/accuracy=0.556800, test/loss=2.003415, test/num_examples=10000, total_duration=33827.184403, train/accuracy=0.775969, train/loss=0.831608, validation/accuracy=0.684400, validation/loss=1.277356, validation/num_examples=50000
I0203 06:38:09.278287 139907720771328 logging_writer.py:48] [96900] global_step=96900, grad_norm=2.6015384197235107, loss=1.4436663389205933
I0203 06:38:43.011227 139907703985920 logging_writer.py:48] [97000] global_step=97000, grad_norm=2.6430275440216064, loss=1.362901210784912
I0203 06:39:16.790895 139907720771328 logging_writer.py:48] [97100] global_step=97100, grad_norm=2.3452279567718506, loss=1.3314718008041382
I0203 06:39:50.532110 139907703985920 logging_writer.py:48] [97200] global_step=97200, grad_norm=2.433008909225464, loss=1.3141354322433472
I0203 06:40:24.233777 139907720771328 logging_writer.py:48] [97300] global_step=97300, grad_norm=2.577983856201172, loss=1.3711373805999756
I0203 06:40:57.905266 139907703985920 logging_writer.py:48] [97400] global_step=97400, grad_norm=2.2646305561065674, loss=1.3417267799377441
I0203 06:41:31.612766 139907720771328 logging_writer.py:48] [97500] global_step=97500, grad_norm=2.32501220703125, loss=1.2578811645507812
I0203 06:42:05.289570 139907703985920 logging_writer.py:48] [97600] global_step=97600, grad_norm=2.661726236343384, loss=1.3386286497116089
I0203 06:42:39.023971 139907720771328 logging_writer.py:48] [97700] global_step=97700, grad_norm=2.3536791801452637, loss=1.3298132419586182
I0203 06:43:12.694965 139907703985920 logging_writer.py:48] [97800] global_step=97800, grad_norm=2.6337966918945312, loss=1.5259290933609009
I0203 06:43:46.392205 139907720771328 logging_writer.py:48] [97900] global_step=97900, grad_norm=2.4730849266052246, loss=1.3600735664367676
I0203 06:44:20.066810 139907703985920 logging_writer.py:48] [98000] global_step=98000, grad_norm=2.371760129928589, loss=1.3649905920028687
I0203 06:44:53.749702 139907720771328 logging_writer.py:48] [98100] global_step=98100, grad_norm=2.1210272312164307, loss=1.291292428970337
I0203 06:45:27.411761 139907703985920 logging_writer.py:48] [98200] global_step=98200, grad_norm=2.295988082885742, loss=1.2740979194641113
I0203 06:46:01.260918 139907720771328 logging_writer.py:48] [98300] global_step=98300, grad_norm=2.3578577041625977, loss=1.3444204330444336
I0203 06:46:20.263610 140070692116288 spec.py:321] Evaluating on the training split.
I0203 06:46:26.601880 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 06:46:35.204545 140070692116288 spec.py:349] Evaluating on the test split.
I0203 06:46:37.892728 140070692116288 submission_runner.py:408] Time since start: 34355.01s, 	Step: 98358, 	{'train/accuracy': 0.7757692933082581, 'train/loss': 0.8344160318374634, 'validation/accuracy': 0.6873799562454224, 'validation/loss': 1.2644789218902588, 'validation/num_examples': 50000, 'test/accuracy': 0.558899998664856, 'test/loss': 1.9838387966156006, 'test/num_examples': 10000, 'score': 33187.21611762047, 'total_duration': 34355.01317358017, 'accumulated_submission_time': 33187.21611762047, 'accumulated_eval_time': 1161.9749476909637, 'accumulated_logging_time': 2.399492025375366}
I0203 06:46:37.925900 139907754342144 logging_writer.py:48] [98358] accumulated_eval_time=1161.974948, accumulated_logging_time=2.399492, accumulated_submission_time=33187.216118, global_step=98358, preemption_count=0, score=33187.216118, test/accuracy=0.558900, test/loss=1.983839, test/num_examples=10000, total_duration=34355.013174, train/accuracy=0.775769, train/loss=0.834416, validation/accuracy=0.687380, validation/loss=1.264479, validation/num_examples=50000
I0203 06:46:52.409853 139907762734848 logging_writer.py:48] [98400] global_step=98400, grad_norm=2.4903979301452637, loss=1.3013381958007812
I0203 06:47:26.044814 139907754342144 logging_writer.py:48] [98500] global_step=98500, grad_norm=2.560121536254883, loss=1.4271211624145508
I0203 06:47:59.748692 139907762734848 logging_writer.py:48] [98600] global_step=98600, grad_norm=2.399744749069214, loss=1.3189342021942139
I0203 06:48:33.439043 139907754342144 logging_writer.py:48] [98700] global_step=98700, grad_norm=2.522021770477295, loss=1.3108657598495483
I0203 06:49:07.193251 139907762734848 logging_writer.py:48] [98800] global_step=98800, grad_norm=2.373037815093994, loss=1.2406151294708252
I0203 06:49:40.879989 139907754342144 logging_writer.py:48] [98900] global_step=98900, grad_norm=2.757035493850708, loss=1.3137871026992798
I0203 06:50:14.580265 139907762734848 logging_writer.py:48] [99000] global_step=99000, grad_norm=2.8260719776153564, loss=1.4230855703353882
I0203 06:50:48.235765 139907754342144 logging_writer.py:48] [99100] global_step=99100, grad_norm=2.3261289596557617, loss=1.310251235961914
I0203 06:51:21.894803 139907762734848 logging_writer.py:48] [99200] global_step=99200, grad_norm=2.4871327877044678, loss=1.372847318649292
I0203 06:51:55.566944 139907754342144 logging_writer.py:48] [99300] global_step=99300, grad_norm=2.5171141624450684, loss=1.3305307626724243
I0203 06:52:29.346463 139907762734848 logging_writer.py:48] [99400] global_step=99400, grad_norm=2.5354080200195312, loss=1.4102684259414673
I0203 06:53:03.049888 139907754342144 logging_writer.py:48] [99500] global_step=99500, grad_norm=2.3046185970306396, loss=1.3525097370147705
I0203 06:53:36.782862 139907762734848 logging_writer.py:48] [99600] global_step=99600, grad_norm=2.5578362941741943, loss=1.3341689109802246
I0203 06:54:10.466676 139907754342144 logging_writer.py:48] [99700] global_step=99700, grad_norm=2.112161874771118, loss=1.2457960844039917
I0203 06:54:44.197359 139907762734848 logging_writer.py:48] [99800] global_step=99800, grad_norm=2.4189486503601074, loss=1.3106926679611206
I0203 06:55:07.934924 140070692116288 spec.py:321] Evaluating on the training split.
I0203 06:55:14.277486 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 06:55:22.632875 140070692116288 spec.py:349] Evaluating on the test split.
I0203 06:55:25.180296 140070692116288 submission_runner.py:408] Time since start: 34882.30s, 	Step: 99872, 	{'train/accuracy': 0.7700095772743225, 'train/loss': 0.8556905388832092, 'validation/accuracy': 0.6885600090026855, 'validation/loss': 1.2676442861557007, 'validation/num_examples': 50000, 'test/accuracy': 0.5624000430107117, 'test/loss': 1.9713383913040161, 'test/num_examples': 10000, 'score': 33697.160153627396, 'total_duration': 34882.30074048042, 'accumulated_submission_time': 33697.160153627396, 'accumulated_eval_time': 1179.2202832698822, 'accumulated_logging_time': 2.4442760944366455}
I0203 06:55:25.216259 139907712378624 logging_writer.py:48] [99872] accumulated_eval_time=1179.220283, accumulated_logging_time=2.444276, accumulated_submission_time=33697.160154, global_step=99872, preemption_count=0, score=33697.160154, test/accuracy=0.562400, test/loss=1.971338, test/num_examples=10000, total_duration=34882.300740, train/accuracy=0.770010, train/loss=0.855691, validation/accuracy=0.688560, validation/loss=1.267644, validation/num_examples=50000
I0203 06:55:34.959617 139907720771328 logging_writer.py:48] [99900] global_step=99900, grad_norm=2.285468101501465, loss=1.4086928367614746
I0203 06:56:08.615268 139907712378624 logging_writer.py:48] [100000] global_step=100000, grad_norm=2.760596513748169, loss=1.4031174182891846
I0203 06:56:42.328714 139907720771328 logging_writer.py:48] [100100] global_step=100100, grad_norm=2.5852062702178955, loss=1.3690557479858398
I0203 06:57:15.999014 139907712378624 logging_writer.py:48] [100200] global_step=100200, grad_norm=2.5396182537078857, loss=1.2287018299102783
I0203 06:57:49.693661 139907720771328 logging_writer.py:48] [100300] global_step=100300, grad_norm=2.5517501831054688, loss=1.4160863161087036
I0203 06:58:23.369685 139907712378624 logging_writer.py:48] [100400] global_step=100400, grad_norm=2.392960786819458, loss=1.3521193265914917
I0203 06:58:57.119940 139907720771328 logging_writer.py:48] [100500] global_step=100500, grad_norm=2.4017462730407715, loss=1.4422956705093384
I0203 06:59:30.871633 139907712378624 logging_writer.py:48] [100600] global_step=100600, grad_norm=2.5207698345184326, loss=1.3297882080078125
I0203 07:00:04.529992 139907720771328 logging_writer.py:48] [100700] global_step=100700, grad_norm=3.07393741607666, loss=1.3840044736862183
I0203 07:00:38.251321 139907712378624 logging_writer.py:48] [100800] global_step=100800, grad_norm=2.4649806022644043, loss=1.2830448150634766
I0203 07:01:11.945283 139907720771328 logging_writer.py:48] [100900] global_step=100900, grad_norm=2.4897751808166504, loss=1.2881298065185547
I0203 07:01:45.632217 139907712378624 logging_writer.py:48] [101000] global_step=101000, grad_norm=2.4849085807800293, loss=1.309862732887268
I0203 07:02:19.311175 139907720771328 logging_writer.py:48] [101100] global_step=101100, grad_norm=2.255023241043091, loss=1.3726520538330078
I0203 07:02:52.993426 139907712378624 logging_writer.py:48] [101200] global_step=101200, grad_norm=2.4683921337127686, loss=1.3963600397109985
I0203 07:03:26.671260 139907720771328 logging_writer.py:48] [101300] global_step=101300, grad_norm=2.966670274734497, loss=1.380779504776001
I0203 07:03:55.436397 140070692116288 spec.py:321] Evaluating on the training split.
I0203 07:04:01.776598 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 07:04:10.230825 140070692116288 spec.py:349] Evaluating on the test split.
I0203 07:04:13.034384 140070692116288 submission_runner.py:408] Time since start: 35410.15s, 	Step: 101387, 	{'train/accuracy': 0.7763273119926453, 'train/loss': 0.8312370777130127, 'validation/accuracy': 0.6921399831771851, 'validation/loss': 1.2392897605895996, 'validation/num_examples': 50000, 'test/accuracy': 0.5706000328063965, 'test/loss': 1.9568129777908325, 'test/num_examples': 10000, 'score': 34207.31728053093, 'total_duration': 35410.15484523773, 'accumulated_submission_time': 34207.31728053093, 'accumulated_eval_time': 1196.8182473182678, 'accumulated_logging_time': 2.4896559715270996}
I0203 07:04:13.063021 139907754342144 logging_writer.py:48] [101387] accumulated_eval_time=1196.818247, accumulated_logging_time=2.489656, accumulated_submission_time=34207.317281, global_step=101387, preemption_count=0, score=34207.317281, test/accuracy=0.570600, test/loss=1.956813, test/num_examples=10000, total_duration=35410.154845, train/accuracy=0.776327, train/loss=0.831237, validation/accuracy=0.692140, validation/loss=1.239290, validation/num_examples=50000
I0203 07:04:17.776288 139907762734848 logging_writer.py:48] [101400] global_step=101400, grad_norm=2.440782308578491, loss=1.3949798345565796
I0203 07:04:51.484228 139907754342144 logging_writer.py:48] [101500] global_step=101500, grad_norm=2.5616180896759033, loss=1.2737988233566284
I0203 07:05:25.253033 139907762734848 logging_writer.py:48] [101600] global_step=101600, grad_norm=2.3537323474884033, loss=1.3532088994979858
I0203 07:05:59.019154 139907754342144 logging_writer.py:48] [101700] global_step=101700, grad_norm=2.4806325435638428, loss=1.2925605773925781
I0203 07:06:32.681489 139907762734848 logging_writer.py:48] [101800] global_step=101800, grad_norm=2.612262010574341, loss=1.3531266450881958
I0203 07:07:06.434419 139907754342144 logging_writer.py:48] [101900] global_step=101900, grad_norm=2.1969010829925537, loss=1.2358254194259644
I0203 07:07:40.119641 139907762734848 logging_writer.py:48] [102000] global_step=102000, grad_norm=2.658073663711548, loss=1.3458318710327148
I0203 07:08:13.788156 139907754342144 logging_writer.py:48] [102100] global_step=102100, grad_norm=2.538353681564331, loss=1.3726065158843994
I0203 07:08:47.444205 139907762734848 logging_writer.py:48] [102200] global_step=102200, grad_norm=2.568115711212158, loss=1.3007819652557373
I0203 07:09:21.131068 139907754342144 logging_writer.py:48] [102300] global_step=102300, grad_norm=2.6526577472686768, loss=1.3792754411697388
I0203 07:09:54.784592 139907762734848 logging_writer.py:48] [102400] global_step=102400, grad_norm=2.55665922164917, loss=1.3892961740493774
I0203 07:10:28.486658 139907754342144 logging_writer.py:48] [102500] global_step=102500, grad_norm=2.3184621334075928, loss=1.329108715057373
I0203 07:11:02.226826 139907762734848 logging_writer.py:48] [102600] global_step=102600, grad_norm=2.5489556789398193, loss=1.264302372932434
I0203 07:11:35.938640 139907754342144 logging_writer.py:48] [102700] global_step=102700, grad_norm=2.4480202198028564, loss=1.3857481479644775
I0203 07:12:09.703782 139907762734848 logging_writer.py:48] [102800] global_step=102800, grad_norm=2.3640329837799072, loss=1.3038767576217651
I0203 07:12:43.424725 139907754342144 logging_writer.py:48] [102900] global_step=102900, grad_norm=2.520965337753296, loss=1.2058260440826416
I0203 07:12:43.432615 140070692116288 spec.py:321] Evaluating on the training split.
I0203 07:12:49.887954 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 07:12:58.462985 140070692116288 spec.py:349] Evaluating on the test split.
I0203 07:13:01.142393 140070692116288 submission_runner.py:408] Time since start: 35938.26s, 	Step: 102901, 	{'train/accuracy': 0.8036909699440002, 'train/loss': 0.7377466559410095, 'validation/accuracy': 0.6786800026893616, 'validation/loss': 1.3162022829055786, 'validation/num_examples': 50000, 'test/accuracy': 0.5493000149726868, 'test/loss': 2.0565006732940674, 'test/num_examples': 10000, 'score': 34717.62727546692, 'total_duration': 35938.262838840485, 'accumulated_submission_time': 34717.62727546692, 'accumulated_eval_time': 1214.52796459198, 'accumulated_logging_time': 2.526557683944702}
I0203 07:13:01.181442 139907712378624 logging_writer.py:48] [102901] accumulated_eval_time=1214.527965, accumulated_logging_time=2.526558, accumulated_submission_time=34717.627275, global_step=102901, preemption_count=0, score=34717.627275, test/accuracy=0.549300, test/loss=2.056501, test/num_examples=10000, total_duration=35938.262839, train/accuracy=0.803691, train/loss=0.737747, validation/accuracy=0.678680, validation/loss=1.316202, validation/num_examples=50000
I0203 07:13:34.863260 139907720771328 logging_writer.py:48] [103000] global_step=103000, grad_norm=2.9162285327911377, loss=1.3817472457885742
I0203 07:14:08.540179 139907712378624 logging_writer.py:48] [103100] global_step=103100, grad_norm=2.600346088409424, loss=1.284424066543579
I0203 07:14:42.195904 139907720771328 logging_writer.py:48] [103200] global_step=103200, grad_norm=2.605555534362793, loss=1.341098427772522
I0203 07:15:15.852622 139907712378624 logging_writer.py:48] [103300] global_step=103300, grad_norm=2.6586461067199707, loss=1.3013970851898193
I0203 07:15:49.545397 139907720771328 logging_writer.py:48] [103400] global_step=103400, grad_norm=2.583219051361084, loss=1.3054949045181274
I0203 07:16:23.241174 139907712378624 logging_writer.py:48] [103500] global_step=103500, grad_norm=2.5706000328063965, loss=1.3127268552780151
I0203 07:16:56.924795 139907720771328 logging_writer.py:48] [103600] global_step=103600, grad_norm=2.5967578887939453, loss=1.3146100044250488
I0203 07:17:30.595773 139907712378624 logging_writer.py:48] [103700] global_step=103700, grad_norm=2.51185941696167, loss=1.3249273300170898
I0203 07:18:04.276448 139907720771328 logging_writer.py:48] [103800] global_step=103800, grad_norm=2.6250436305999756, loss=1.366295337677002
I0203 07:18:38.087245 139907712378624 logging_writer.py:48] [103900] global_step=103900, grad_norm=2.4613332748413086, loss=1.3053408861160278
I0203 07:19:11.791060 139907720771328 logging_writer.py:48] [104000] global_step=104000, grad_norm=2.4171576499938965, loss=1.2843904495239258
I0203 07:19:45.467322 139907712378624 logging_writer.py:48] [104100] global_step=104100, grad_norm=2.408651351928711, loss=1.2802366018295288
I0203 07:20:19.157956 139907720771328 logging_writer.py:48] [104200] global_step=104200, grad_norm=2.851055383682251, loss=1.327347755432129
I0203 07:20:52.822838 139907712378624 logging_writer.py:48] [104300] global_step=104300, grad_norm=2.68584942817688, loss=1.2730400562286377
I0203 07:21:26.493116 139907720771328 logging_writer.py:48] [104400] global_step=104400, grad_norm=2.416332721710205, loss=1.2267663478851318
I0203 07:21:31.352149 140070692116288 spec.py:321] Evaluating on the training split.
I0203 07:21:37.718893 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 07:21:46.076741 140070692116288 spec.py:349] Evaluating on the test split.
I0203 07:21:48.757195 140070692116288 submission_runner.py:408] Time since start: 36465.88s, 	Step: 104416, 	{'train/accuracy': 0.8050262928009033, 'train/loss': 0.7113240957260132, 'validation/accuracy': 0.6964199542999268, 'validation/loss': 1.2255223989486694, 'validation/num_examples': 50000, 'test/accuracy': 0.5697000026702881, 'test/loss': 1.9121817350387573, 'test/num_examples': 10000, 'score': 35227.735830545425, 'total_duration': 36465.87762641907, 'accumulated_submission_time': 35227.735830545425, 'accumulated_eval_time': 1231.9329543113708, 'accumulated_logging_time': 2.574941396713257}
I0203 07:21:48.796694 139907712378624 logging_writer.py:48] [104416] accumulated_eval_time=1231.932954, accumulated_logging_time=2.574941, accumulated_submission_time=35227.735831, global_step=104416, preemption_count=0, score=35227.735831, test/accuracy=0.569700, test/loss=1.912182, test/num_examples=10000, total_duration=36465.877626, train/accuracy=0.805026, train/loss=0.711324, validation/accuracy=0.696420, validation/loss=1.225522, validation/num_examples=50000
I0203 07:22:17.438605 139907745949440 logging_writer.py:48] [104500] global_step=104500, grad_norm=2.8208000659942627, loss=1.3843656778335571
I0203 07:22:51.141957 139907712378624 logging_writer.py:48] [104600] global_step=104600, grad_norm=2.889726400375366, loss=1.321202278137207
I0203 07:23:24.797271 139907745949440 logging_writer.py:48] [104700] global_step=104700, grad_norm=2.6014442443847656, loss=1.2765029668807983
I0203 07:23:58.454628 139907712378624 logging_writer.py:48] [104800] global_step=104800, grad_norm=2.6258511543273926, loss=1.304714560508728
I0203 07:24:32.118835 139907745949440 logging_writer.py:48] [104900] global_step=104900, grad_norm=3.0497584342956543, loss=1.3229717016220093
I0203 07:25:05.946802 139907712378624 logging_writer.py:48] [105000] global_step=105000, grad_norm=2.6276533603668213, loss=1.3541760444641113
I0203 07:25:39.678672 139907745949440 logging_writer.py:48] [105100] global_step=105100, grad_norm=2.514530897140503, loss=1.329113483428955
I0203 07:26:13.387119 139907712378624 logging_writer.py:48] [105200] global_step=105200, grad_norm=2.435889720916748, loss=1.31022047996521
I0203 07:26:47.039672 139907745949440 logging_writer.py:48] [105300] global_step=105300, grad_norm=2.8250982761383057, loss=1.4627618789672852
I0203 07:27:20.738010 139907712378624 logging_writer.py:48] [105400] global_step=105400, grad_norm=2.4531970024108887, loss=1.377834439277649
I0203 07:27:54.373596 139907745949440 logging_writer.py:48] [105500] global_step=105500, grad_norm=2.596454620361328, loss=1.335059404373169
I0203 07:28:28.061770 139907712378624 logging_writer.py:48] [105600] global_step=105600, grad_norm=2.59871506690979, loss=1.2983365058898926
I0203 07:29:01.718930 139907745949440 logging_writer.py:48] [105700] global_step=105700, grad_norm=2.82753586769104, loss=1.3305481672286987
I0203 07:29:35.427712 139907712378624 logging_writer.py:48] [105800] global_step=105800, grad_norm=2.732203960418701, loss=1.2989685535430908
I0203 07:30:09.070738 139907745949440 logging_writer.py:48] [105900] global_step=105900, grad_norm=2.913059711456299, loss=1.2214925289154053
I0203 07:30:19.008762 140070692116288 spec.py:321] Evaluating on the training split.
I0203 07:30:25.315311 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 07:30:33.628809 140070692116288 spec.py:349] Evaluating on the test split.
I0203 07:30:36.285470 140070692116288 submission_runner.py:408] Time since start: 36993.41s, 	Step: 105931, 	{'train/accuracy': 0.7947624325752258, 'train/loss': 0.7517896294593811, 'validation/accuracy': 0.69896000623703, 'validation/loss': 1.214800238609314, 'validation/num_examples': 50000, 'test/accuracy': 0.5741000175476074, 'test/loss': 1.9478965997695923, 'test/num_examples': 10000, 'score': 35737.8859539032, 'total_duration': 36993.40591478348, 'accumulated_submission_time': 35737.8859539032, 'accumulated_eval_time': 1249.209624528885, 'accumulated_logging_time': 2.623447895050049}
I0203 07:30:36.325502 139907729164032 logging_writer.py:48] [105931] accumulated_eval_time=1249.209625, accumulated_logging_time=2.623448, accumulated_submission_time=35737.885954, global_step=105931, preemption_count=0, score=35737.885954, test/accuracy=0.574100, test/loss=1.947897, test/num_examples=10000, total_duration=36993.405915, train/accuracy=0.794762, train/loss=0.751790, validation/accuracy=0.698960, validation/loss=1.214800, validation/num_examples=50000
I0203 07:30:59.867947 139907737556736 logging_writer.py:48] [106000] global_step=106000, grad_norm=2.6881046295166016, loss=1.3794636726379395
I0203 07:31:33.758367 139907729164032 logging_writer.py:48] [106100] global_step=106100, grad_norm=2.916287422180176, loss=1.2660009860992432
I0203 07:32:07.417940 139907737556736 logging_writer.py:48] [106200] global_step=106200, grad_norm=2.581688165664673, loss=1.238451361656189
I0203 07:32:41.112456 139907729164032 logging_writer.py:48] [106300] global_step=106300, grad_norm=2.9266035556793213, loss=1.25723135471344
I0203 07:33:14.785645 139907737556736 logging_writer.py:48] [106400] global_step=106400, grad_norm=2.790505886077881, loss=1.2531322240829468
I0203 07:33:48.456391 139907729164032 logging_writer.py:48] [106500] global_step=106500, grad_norm=2.4825894832611084, loss=1.3060033321380615
I0203 07:34:22.088840 139907737556736 logging_writer.py:48] [106600] global_step=106600, grad_norm=2.8172144889831543, loss=1.2727909088134766
I0203 07:34:55.870052 139907729164032 logging_writer.py:48] [106700] global_step=106700, grad_norm=2.5094170570373535, loss=1.2900781631469727
I0203 07:35:29.557002 139907737556736 logging_writer.py:48] [106800] global_step=106800, grad_norm=2.716583013534546, loss=1.3019399642944336
I0203 07:36:03.242929 139907729164032 logging_writer.py:48] [106900] global_step=106900, grad_norm=2.7107574939727783, loss=1.2649184465408325
I0203 07:36:36.972236 139907737556736 logging_writer.py:48] [107000] global_step=107000, grad_norm=2.4887728691101074, loss=1.202644944190979
I0203 07:37:10.667639 139907729164032 logging_writer.py:48] [107100] global_step=107100, grad_norm=2.637314558029175, loss=1.319951057434082
I0203 07:37:44.331225 139907737556736 logging_writer.py:48] [107200] global_step=107200, grad_norm=2.5568580627441406, loss=1.3924208879470825
I0203 07:38:18.154081 139907729164032 logging_writer.py:48] [107300] global_step=107300, grad_norm=2.7385036945343018, loss=1.3682758808135986
I0203 07:38:51.831776 139907737556736 logging_writer.py:48] [107400] global_step=107400, grad_norm=2.612478494644165, loss=1.3103820085525513
I0203 07:39:06.453227 140070692116288 spec.py:321] Evaluating on the training split.
I0203 07:39:12.767966 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 07:39:21.288666 140070692116288 spec.py:349] Evaluating on the test split.
I0203 07:39:24.050494 140070692116288 submission_runner.py:408] Time since start: 37521.17s, 	Step: 107445, 	{'train/accuracy': 0.7896404266357422, 'train/loss': 0.7744253277778625, 'validation/accuracy': 0.6942200064659119, 'validation/loss': 1.2482731342315674, 'validation/num_examples': 50000, 'test/accuracy': 0.5676000118255615, 'test/loss': 1.9585601091384888, 'test/num_examples': 10000, 'score': 36247.9498064518, 'total_duration': 37521.17093753815, 'accumulated_submission_time': 36247.9498064518, 'accumulated_eval_time': 1266.8068754673004, 'accumulated_logging_time': 2.6738781929016113}
I0203 07:39:24.088658 139907712378624 logging_writer.py:48] [107445] accumulated_eval_time=1266.806875, accumulated_logging_time=2.673878, accumulated_submission_time=36247.949806, global_step=107445, preemption_count=0, score=36247.949806, test/accuracy=0.567600, test/loss=1.958560, test/num_examples=10000, total_duration=37521.170938, train/accuracy=0.789640, train/loss=0.774425, validation/accuracy=0.694220, validation/loss=1.248273, validation/num_examples=50000
I0203 07:39:43.161688 139907720771328 logging_writer.py:48] [107500] global_step=107500, grad_norm=2.8806591033935547, loss=1.317856788635254
I0203 07:40:16.847606 139907712378624 logging_writer.py:48] [107600] global_step=107600, grad_norm=2.538844585418701, loss=1.2348089218139648
I0203 07:40:50.573733 139907720771328 logging_writer.py:48] [107700] global_step=107700, grad_norm=2.802278757095337, loss=1.348137378692627
I0203 07:41:24.259718 139907712378624 logging_writer.py:48] [107800] global_step=107800, grad_norm=2.5706627368927, loss=1.254939317703247
I0203 07:41:58.020576 139907720771328 logging_writer.py:48] [107900] global_step=107900, grad_norm=2.6774559020996094, loss=1.3201817274093628
I0203 07:42:31.738483 139907712378624 logging_writer.py:48] [108000] global_step=108000, grad_norm=2.898341178894043, loss=1.3600554466247559
I0203 07:43:05.480714 139907720771328 logging_writer.py:48] [108100] global_step=108100, grad_norm=2.609907627105713, loss=1.31863272190094
I0203 07:43:39.150653 139907712378624 logging_writer.py:48] [108200] global_step=108200, grad_norm=2.6987338066101074, loss=1.311875820159912
I0203 07:44:12.894235 139907720771328 logging_writer.py:48] [108300] global_step=108300, grad_norm=2.5817275047302246, loss=1.3125404119491577
I0203 07:44:46.685625 139907712378624 logging_writer.py:48] [108400] global_step=108400, grad_norm=2.460700273513794, loss=1.324197769165039
I0203 07:45:20.415957 139907720771328 logging_writer.py:48] [108500] global_step=108500, grad_norm=2.59930682182312, loss=1.1663371324539185
I0203 07:45:54.071649 139907712378624 logging_writer.py:48] [108600] global_step=108600, grad_norm=2.9885849952697754, loss=1.436843991279602
I0203 07:46:27.810184 139907720771328 logging_writer.py:48] [108700] global_step=108700, grad_norm=2.507697582244873, loss=1.293683409690857
I0203 07:47:01.463662 139907712378624 logging_writer.py:48] [108800] global_step=108800, grad_norm=2.9439094066619873, loss=1.3249170780181885
I0203 07:47:35.167821 139907720771328 logging_writer.py:48] [108900] global_step=108900, grad_norm=2.3718411922454834, loss=1.2512441873550415
I0203 07:47:54.143726 140070692116288 spec.py:321] Evaluating on the training split.
I0203 07:48:00.479003 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 07:48:09.380807 140070692116288 spec.py:349] Evaluating on the test split.
I0203 07:48:12.117468 140070692116288 submission_runner.py:408] Time since start: 38049.24s, 	Step: 108958, 	{'train/accuracy': 0.7810905575752258, 'train/loss': 0.8077903389930725, 'validation/accuracy': 0.6916999816894531, 'validation/loss': 1.2639470100402832, 'validation/num_examples': 50000, 'test/accuracy': 0.5630000233650208, 'test/loss': 1.98625648021698, 'test/num_examples': 10000, 'score': 36757.94217133522, 'total_duration': 38049.23787140846, 'accumulated_submission_time': 36757.94217133522, 'accumulated_eval_time': 1284.7805352210999, 'accumulated_logging_time': 2.722330331802368}
I0203 07:48:12.169485 139907712378624 logging_writer.py:48] [108958] accumulated_eval_time=1284.780535, accumulated_logging_time=2.722330, accumulated_submission_time=36757.942171, global_step=108958, preemption_count=0, score=36757.942171, test/accuracy=0.563000, test/loss=1.986256, test/num_examples=10000, total_duration=38049.237871, train/accuracy=0.781091, train/loss=0.807790, validation/accuracy=0.691700, validation/loss=1.263947, validation/num_examples=50000
I0203 07:48:26.675555 139907720771328 logging_writer.py:48] [109000] global_step=109000, grad_norm=2.642756223678589, loss=1.3126428127288818
I0203 07:49:00.371712 139907712378624 logging_writer.py:48] [109100] global_step=109100, grad_norm=2.8889923095703125, loss=1.2655044794082642
I0203 07:49:34.069648 139907720771328 logging_writer.py:48] [109200] global_step=109200, grad_norm=2.8069264888763428, loss=1.1999188661575317
I0203 07:50:07.745633 139907712378624 logging_writer.py:48] [109300] global_step=109300, grad_norm=2.634906530380249, loss=1.3431270122528076
I0203 07:50:41.440942 139907720771328 logging_writer.py:48] [109400] global_step=109400, grad_norm=2.965643882751465, loss=1.3185116052627563
I0203 07:51:15.181847 139907712378624 logging_writer.py:48] [109500] global_step=109500, grad_norm=2.7969748973846436, loss=1.2664942741394043
I0203 07:51:48.968084 139907720771328 logging_writer.py:48] [109600] global_step=109600, grad_norm=2.609797716140747, loss=1.2802528142929077
I0203 07:52:22.633620 139907712378624 logging_writer.py:48] [109700] global_step=109700, grad_norm=2.6884944438934326, loss=1.228222370147705
I0203 07:52:56.329477 139907720771328 logging_writer.py:48] [109800] global_step=109800, grad_norm=2.821941614151001, loss=1.2269734144210815
I0203 07:53:29.979603 139907712378624 logging_writer.py:48] [109900] global_step=109900, grad_norm=2.7143306732177734, loss=1.3012712001800537
I0203 07:54:03.658345 139907720771328 logging_writer.py:48] [110000] global_step=110000, grad_norm=2.633733034133911, loss=1.2495396137237549
I0203 07:54:37.341586 139907712378624 logging_writer.py:48] [110100] global_step=110100, grad_norm=2.7279818058013916, loss=1.2443007230758667
I0203 07:55:11.111248 139907720771328 logging_writer.py:48] [110200] global_step=110200, grad_norm=2.6880645751953125, loss=1.2908633947372437
I0203 07:55:44.800998 139907712378624 logging_writer.py:48] [110300] global_step=110300, grad_norm=2.853612184524536, loss=1.222265362739563
I0203 07:56:18.561553 139907720771328 logging_writer.py:48] [110400] global_step=110400, grad_norm=2.800489902496338, loss=1.400905728340149
I0203 07:56:42.280478 140070692116288 spec.py:321] Evaluating on the training split.
I0203 07:56:48.577121 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 07:56:57.060596 140070692116288 spec.py:349] Evaluating on the test split.
I0203 07:56:59.761680 140070692116288 submission_runner.py:408] Time since start: 38576.88s, 	Step: 110472, 	{'train/accuracy': 0.7847775816917419, 'train/loss': 0.7921836972236633, 'validation/accuracy': 0.6990000009536743, 'validation/loss': 1.2325977087020874, 'validation/num_examples': 50000, 'test/accuracy': 0.5737000107765198, 'test/loss': 1.9548555612564087, 'test/num_examples': 10000, 'score': 37267.98639202118, 'total_duration': 38576.88211965561, 'accumulated_submission_time': 37267.98639202118, 'accumulated_eval_time': 1302.261702299118, 'accumulated_logging_time': 2.787564992904663}
I0203 07:56:59.800864 139907745949440 logging_writer.py:48] [110472] accumulated_eval_time=1302.261702, accumulated_logging_time=2.787565, accumulated_submission_time=37267.986392, global_step=110472, preemption_count=0, score=37267.986392, test/accuracy=0.573700, test/loss=1.954856, test/num_examples=10000, total_duration=38576.882120, train/accuracy=0.784778, train/loss=0.792184, validation/accuracy=0.699000, validation/loss=1.232598, validation/num_examples=50000
I0203 07:57:09.549844 139907754342144 logging_writer.py:48] [110500] global_step=110500, grad_norm=3.2270238399505615, loss=1.31345534324646
I0203 07:57:43.188762 139907745949440 logging_writer.py:48] [110600] global_step=110600, grad_norm=2.63627290725708, loss=1.2517327070236206
I0203 07:58:17.021446 139907754342144 logging_writer.py:48] [110700] global_step=110700, grad_norm=2.7283806800842285, loss=1.3465088605880737
I0203 07:58:50.720517 139907745949440 logging_writer.py:48] [110800] global_step=110800, grad_norm=2.7804110050201416, loss=1.2966344356536865
I0203 07:59:24.396668 139907754342144 logging_writer.py:48] [110900] global_step=110900, grad_norm=2.776984453201294, loss=1.3326468467712402
I0203 07:59:58.111978 139907745949440 logging_writer.py:48] [111000] global_step=111000, grad_norm=2.8191184997558594, loss=1.2606768608093262
I0203 08:00:31.793380 139907754342144 logging_writer.py:48] [111100] global_step=111100, grad_norm=2.737617254257202, loss=1.2950063943862915
I0203 08:01:05.465478 139907745949440 logging_writer.py:48] [111200] global_step=111200, grad_norm=2.619201898574829, loss=1.1749441623687744
I0203 08:01:39.170149 139907754342144 logging_writer.py:48] [111300] global_step=111300, grad_norm=2.6769187450408936, loss=1.2222262620925903
I0203 08:02:12.854130 139907745949440 logging_writer.py:48] [111400] global_step=111400, grad_norm=2.8583850860595703, loss=1.2289239168167114
I0203 08:02:46.551860 139907754342144 logging_writer.py:48] [111500] global_step=111500, grad_norm=2.867540121078491, loss=1.30659019947052
I0203 08:03:20.242003 139907745949440 logging_writer.py:48] [111600] global_step=111600, grad_norm=3.082427978515625, loss=1.268479824066162
I0203 08:03:53.921372 139907754342144 logging_writer.py:48] [111700] global_step=111700, grad_norm=2.8746209144592285, loss=1.28855299949646
I0203 08:04:27.832578 139907745949440 logging_writer.py:48] [111800] global_step=111800, grad_norm=2.933917760848999, loss=1.2836964130401611
I0203 08:05:01.544558 139907754342144 logging_writer.py:48] [111900] global_step=111900, grad_norm=2.55078387260437, loss=1.2209151983261108
I0203 08:05:30.011693 140070692116288 spec.py:321] Evaluating on the training split.
I0203 08:05:36.328378 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 08:05:44.908743 140070692116288 spec.py:349] Evaluating on the test split.
I0203 08:05:47.571392 140070692116288 submission_runner.py:408] Time since start: 39104.69s, 	Step: 111986, 	{'train/accuracy': 0.8351203799247742, 'train/loss': 0.5935096740722656, 'validation/accuracy': 0.6996999979019165, 'validation/loss': 1.23037850856781, 'validation/num_examples': 50000, 'test/accuracy': 0.5711000561714172, 'test/loss': 1.9375004768371582, 'test/num_examples': 10000, 'score': 37778.13521909714, 'total_duration': 39104.69184041023, 'accumulated_submission_time': 37778.13521909714, 'accumulated_eval_time': 1319.8213753700256, 'accumulated_logging_time': 2.836085319519043}
I0203 08:05:47.612078 139907712378624 logging_writer.py:48] [111986] accumulated_eval_time=1319.821375, accumulated_logging_time=2.836085, accumulated_submission_time=37778.135219, global_step=111986, preemption_count=0, score=37778.135219, test/accuracy=0.571100, test/loss=1.937500, test/num_examples=10000, total_duration=39104.691840, train/accuracy=0.835120, train/loss=0.593510, validation/accuracy=0.699700, validation/loss=1.230379, validation/num_examples=50000
I0203 08:05:52.650702 139907720771328 logging_writer.py:48] [112000] global_step=112000, grad_norm=3.2244985103607178, loss=1.2123521566390991
I0203 08:06:26.305107 139907712378624 logging_writer.py:48] [112100] global_step=112100, grad_norm=2.946021318435669, loss=1.2088806629180908
I0203 08:06:59.952268 139907720771328 logging_writer.py:48] [112200] global_step=112200, grad_norm=2.764444351196289, loss=1.2315336465835571
I0203 08:07:33.642119 139907712378624 logging_writer.py:48] [112300] global_step=112300, grad_norm=2.765207529067993, loss=1.3240444660186768
I0203 08:08:07.321235 139907720771328 logging_writer.py:48] [112400] global_step=112400, grad_norm=2.7041900157928467, loss=1.368688702583313
I0203 08:08:41.016063 139907712378624 logging_writer.py:48] [112500] global_step=112500, grad_norm=3.012937068939209, loss=1.236142635345459
I0203 08:09:14.670814 139907720771328 logging_writer.py:48] [112600] global_step=112600, grad_norm=2.407620668411255, loss=1.240098237991333
I0203 08:09:48.378414 139907712378624 logging_writer.py:48] [112700] global_step=112700, grad_norm=2.743323802947998, loss=1.1706314086914062
I0203 08:10:22.038500 139907720771328 logging_writer.py:48] [112800] global_step=112800, grad_norm=2.631593942642212, loss=1.1649963855743408
I0203 08:10:55.911915 139907712378624 logging_writer.py:48] [112900] global_step=112900, grad_norm=2.9023115634918213, loss=1.3231065273284912
I0203 08:11:29.617703 139907720771328 logging_writer.py:48] [113000] global_step=113000, grad_norm=2.7144949436187744, loss=1.2112544775009155
I0203 08:12:03.328147 139907712378624 logging_writer.py:48] [113100] global_step=113100, grad_norm=2.7874085903167725, loss=1.1911391019821167
I0203 08:12:36.999091 139907720771328 logging_writer.py:48] [113200] global_step=113200, grad_norm=2.7419204711914062, loss=1.2607226371765137
I0203 08:13:10.692371 139907712378624 logging_writer.py:48] [113300] global_step=113300, grad_norm=2.5766754150390625, loss=1.183807373046875
I0203 08:13:44.361037 139907720771328 logging_writer.py:48] [113400] global_step=113400, grad_norm=2.9756181240081787, loss=1.1905481815338135
I0203 08:14:18.051618 139907712378624 logging_writer.py:48] [113500] global_step=113500, grad_norm=2.66268253326416, loss=1.267547845840454
I0203 08:14:18.060016 140070692116288 spec.py:321] Evaluating on the training split.
I0203 08:14:24.414762 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 08:14:32.937279 140070692116288 spec.py:349] Evaluating on the test split.
I0203 08:14:35.611531 140070692116288 submission_runner.py:408] Time since start: 39632.73s, 	Step: 113501, 	{'train/accuracy': 0.8151108026504517, 'train/loss': 0.672951340675354, 'validation/accuracy': 0.6999399662017822, 'validation/loss': 1.2182530164718628, 'validation/num_examples': 50000, 'test/accuracy': 0.5791000127792358, 'test/loss': 1.9064098596572876, 'test/num_examples': 10000, 'score': 38288.519334316254, 'total_duration': 39632.731977939606, 'accumulated_submission_time': 38288.519334316254, 'accumulated_eval_time': 1337.3728301525116, 'accumulated_logging_time': 2.8874781131744385}
I0203 08:14:35.652204 139907745949440 logging_writer.py:48] [113501] accumulated_eval_time=1337.372830, accumulated_logging_time=2.887478, accumulated_submission_time=38288.519334, global_step=113501, preemption_count=0, score=38288.519334, test/accuracy=0.579100, test/loss=1.906410, test/num_examples=10000, total_duration=39632.731978, train/accuracy=0.815111, train/loss=0.672951, validation/accuracy=0.699940, validation/loss=1.218253, validation/num_examples=50000
I0203 08:15:09.344737 139907754342144 logging_writer.py:48] [113600] global_step=113600, grad_norm=2.812697172164917, loss=1.2673981189727783
I0203 08:15:43.032277 139907745949440 logging_writer.py:48] [113700] global_step=113700, grad_norm=3.0202322006225586, loss=1.2323001623153687
I0203 08:16:16.819471 139907754342144 logging_writer.py:48] [113800] global_step=113800, grad_norm=2.9615070819854736, loss=1.2900378704071045
I0203 08:16:50.518295 139907745949440 logging_writer.py:48] [113900] global_step=113900, grad_norm=2.666771173477173, loss=1.2707291841506958
I0203 08:17:24.345836 139907754342144 logging_writer.py:48] [114000] global_step=114000, grad_norm=2.7527384757995605, loss=1.2659478187561035
I0203 08:17:58.036942 139907745949440 logging_writer.py:48] [114100] global_step=114100, grad_norm=2.706653356552124, loss=1.1958742141723633
I0203 08:18:31.764271 139907754342144 logging_writer.py:48] [114200] global_step=114200, grad_norm=2.8779547214508057, loss=1.19978928565979
I0203 08:19:05.463309 139907745949440 logging_writer.py:48] [114300] global_step=114300, grad_norm=2.658729314804077, loss=1.1203291416168213
I0203 08:19:39.160869 139907754342144 logging_writer.py:48] [114400] global_step=114400, grad_norm=2.770716667175293, loss=1.1808536052703857
I0203 08:20:12.836623 139907745949440 logging_writer.py:48] [114500] global_step=114500, grad_norm=2.6763901710510254, loss=1.299927830696106
I0203 08:20:46.490870 139907754342144 logging_writer.py:48] [114600] global_step=114600, grad_norm=2.90657114982605, loss=1.2626159191131592
I0203 08:21:20.158223 139907745949440 logging_writer.py:48] [114700] global_step=114700, grad_norm=2.8192636966705322, loss=1.193786382675171
I0203 08:21:53.862641 139907754342144 logging_writer.py:48] [114800] global_step=114800, grad_norm=2.8153228759765625, loss=1.2295821905136108
I0203 08:22:27.582616 139907745949440 logging_writer.py:48] [114900] global_step=114900, grad_norm=2.5623433589935303, loss=1.2678251266479492
I0203 08:23:01.274477 139907754342144 logging_writer.py:48] [115000] global_step=115000, grad_norm=3.0430350303649902, loss=1.191519856452942
I0203 08:23:05.813338 140070692116288 spec.py:321] Evaluating on the training split.
I0203 08:23:12.142934 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 08:23:20.516278 140070692116288 spec.py:349] Evaluating on the test split.
I0203 08:23:23.235116 140070692116288 submission_runner.py:408] Time since start: 40160.36s, 	Step: 115015, 	{'train/accuracy': 0.80961012840271, 'train/loss': 0.6891085505485535, 'validation/accuracy': 0.705299973487854, 'validation/loss': 1.2037498950958252, 'validation/num_examples': 50000, 'test/accuracy': 0.5841000080108643, 'test/loss': 1.930444598197937, 'test/num_examples': 10000, 'score': 38798.61583328247, 'total_duration': 40160.355558395386, 'accumulated_submission_time': 38798.61583328247, 'accumulated_eval_time': 1354.7945802211761, 'accumulated_logging_time': 2.939371109008789}
I0203 08:23:23.273583 139907703985920 logging_writer.py:48] [115015] accumulated_eval_time=1354.794580, accumulated_logging_time=2.939371, accumulated_submission_time=38798.615833, global_step=115015, preemption_count=0, score=38798.615833, test/accuracy=0.584100, test/loss=1.930445, test/num_examples=10000, total_duration=40160.355558, train/accuracy=0.809610, train/loss=0.689109, validation/accuracy=0.705300, validation/loss=1.203750, validation/num_examples=50000
I0203 08:23:52.244181 139907712378624 logging_writer.py:48] [115100] global_step=115100, grad_norm=2.7373178005218506, loss=1.1948487758636475
I0203 08:24:25.952942 139907703985920 logging_writer.py:48] [115200] global_step=115200, grad_norm=3.100959539413452, loss=1.2548028230667114
I0203 08:24:59.618161 139907712378624 logging_writer.py:48] [115300] global_step=115300, grad_norm=2.692448854446411, loss=1.2311620712280273
I0203 08:25:33.302872 139907703985920 logging_writer.py:48] [115400] global_step=115400, grad_norm=2.845828056335449, loss=1.238487958908081
I0203 08:26:06.955691 139907712378624 logging_writer.py:48] [115500] global_step=115500, grad_norm=2.838927984237671, loss=1.228951096534729
I0203 08:26:40.659474 139907703985920 logging_writer.py:48] [115600] global_step=115600, grad_norm=2.77341628074646, loss=1.1620277166366577
I0203 08:27:14.298441 139907712378624 logging_writer.py:48] [115700] global_step=115700, grad_norm=3.118117094039917, loss=1.3480764627456665
I0203 08:27:47.970523 139907703985920 logging_writer.py:48] [115800] global_step=115800, grad_norm=3.0583994388580322, loss=1.1846998929977417
I0203 08:28:21.637208 139907712378624 logging_writer.py:48] [115900] global_step=115900, grad_norm=2.7543537616729736, loss=1.0899598598480225
I0203 08:28:55.333485 139907703985920 logging_writer.py:48] [116000] global_step=116000, grad_norm=2.8429455757141113, loss=1.2305294275283813
I0203 08:29:28.986141 139907712378624 logging_writer.py:48] [116100] global_step=116100, grad_norm=2.865410327911377, loss=1.256737470626831
I0203 08:30:02.688177 139907703985920 logging_writer.py:48] [116200] global_step=116200, grad_norm=2.8533475399017334, loss=1.2933216094970703
I0203 08:30:36.487748 139907712378624 logging_writer.py:48] [116300] global_step=116300, grad_norm=2.7712411880493164, loss=1.2165765762329102
I0203 08:31:10.212950 139907703985920 logging_writer.py:48] [116400] global_step=116400, grad_norm=2.7524755001068115, loss=1.2042349576950073
I0203 08:31:43.872531 139907712378624 logging_writer.py:48] [116500] global_step=116500, grad_norm=2.991185188293457, loss=1.107055902481079
I0203 08:31:53.458168 140070692116288 spec.py:321] Evaluating on the training split.
I0203 08:31:59.783499 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 08:32:08.603802 140070692116288 spec.py:349] Evaluating on the test split.
I0203 08:32:11.288249 140070692116288 submission_runner.py:408] Time since start: 40688.41s, 	Step: 116530, 	{'train/accuracy': 0.8014189600944519, 'train/loss': 0.7300856113433838, 'validation/accuracy': 0.7023199796676636, 'validation/loss': 1.212501883506775, 'validation/num_examples': 50000, 'test/accuracy': 0.5679000020027161, 'test/loss': 1.9563889503479004, 'test/num_examples': 10000, 'score': 39308.739602565765, 'total_duration': 40688.40866851807, 'accumulated_submission_time': 39308.739602565765, 'accumulated_eval_time': 1372.6245946884155, 'accumulated_logging_time': 2.987029552459717}
I0203 08:32:11.323987 139907737556736 logging_writer.py:48] [116530] accumulated_eval_time=1372.624595, accumulated_logging_time=2.987030, accumulated_submission_time=39308.739603, global_step=116530, preemption_count=0, score=39308.739603, test/accuracy=0.567900, test/loss=1.956389, test/num_examples=10000, total_duration=40688.408669, train/accuracy=0.801419, train/loss=0.730086, validation/accuracy=0.702320, validation/loss=1.212502, validation/num_examples=50000
I0203 08:32:35.326915 139907745949440 logging_writer.py:48] [116600] global_step=116600, grad_norm=2.9881348609924316, loss=1.213153600692749
I0203 08:33:08.999010 139907737556736 logging_writer.py:48] [116700] global_step=116700, grad_norm=3.0382769107818604, loss=1.2430955171585083
I0203 08:33:42.679136 139907745949440 logging_writer.py:48] [116800] global_step=116800, grad_norm=2.824425220489502, loss=1.1619924306869507
I0203 08:34:16.340434 139907737556736 logging_writer.py:48] [116900] global_step=116900, grad_norm=3.005070447921753, loss=1.1825969219207764
I0203 08:34:50.014476 139907745949440 logging_writer.py:48] [117000] global_step=117000, grad_norm=2.816610097885132, loss=1.238860011100769
I0203 08:35:23.688289 139907737556736 logging_writer.py:48] [117100] global_step=117100, grad_norm=2.7509641647338867, loss=1.1464178562164307
I0203 08:35:57.356688 139907745949440 logging_writer.py:48] [117200] global_step=117200, grad_norm=2.8670601844787598, loss=1.2592127323150635
I0203 08:36:31.110309 139907737556736 logging_writer.py:48] [117300] global_step=117300, grad_norm=3.140618324279785, loss=1.1896312236785889
I0203 08:37:04.851033 139907745949440 logging_writer.py:48] [117400] global_step=117400, grad_norm=2.646266460418701, loss=1.115944743156433
I0203 08:37:38.621880 139907737556736 logging_writer.py:48] [117500] global_step=117500, grad_norm=2.7795956134796143, loss=1.1846407651901245
I0203 08:38:12.288157 139907745949440 logging_writer.py:48] [117600] global_step=117600, grad_norm=2.8657500743865967, loss=1.2576665878295898
I0203 08:38:45.986726 139907737556736 logging_writer.py:48] [117700] global_step=117700, grad_norm=2.7819344997406006, loss=1.216050386428833
I0203 08:39:19.624250 139907745949440 logging_writer.py:48] [117800] global_step=117800, grad_norm=2.8901283740997314, loss=1.2126119136810303
I0203 08:39:53.328634 139907737556736 logging_writer.py:48] [117900] global_step=117900, grad_norm=2.7799570560455322, loss=1.1458594799041748
I0203 08:40:27.002610 139907745949440 logging_writer.py:48] [118000] global_step=118000, grad_norm=3.043649435043335, loss=1.2531598806381226
I0203 08:40:41.622153 140070692116288 spec.py:321] Evaluating on the training split.
I0203 08:40:48.026881 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 08:40:56.752601 140070692116288 spec.py:349] Evaluating on the test split.
I0203 08:40:59.391582 140070692116288 submission_runner.py:408] Time since start: 41216.51s, 	Step: 118045, 	{'train/accuracy': 0.8037906289100647, 'train/loss': 0.7134524583816528, 'validation/accuracy': 0.7069399952888489, 'validation/loss': 1.1913037300109863, 'validation/num_examples': 50000, 'test/accuracy': 0.5809000134468079, 'test/loss': 1.901960849761963, 'test/num_examples': 10000, 'score': 39818.97498655319, 'total_duration': 41216.51202297211, 'accumulated_submission_time': 39818.97498655319, 'accumulated_eval_time': 1390.393991947174, 'accumulated_logging_time': 3.032386541366577}
I0203 08:40:59.430611 139907712378624 logging_writer.py:48] [118045] accumulated_eval_time=1390.393992, accumulated_logging_time=3.032387, accumulated_submission_time=39818.974987, global_step=118045, preemption_count=0, score=39818.974987, test/accuracy=0.580900, test/loss=1.901961, test/num_examples=10000, total_duration=41216.512023, train/accuracy=0.803791, train/loss=0.713452, validation/accuracy=0.706940, validation/loss=1.191304, validation/num_examples=50000
I0203 08:41:18.299628 139907720771328 logging_writer.py:48] [118100] global_step=118100, grad_norm=2.9567532539367676, loss=1.220383882522583
I0203 08:41:51.959070 139907712378624 logging_writer.py:48] [118200] global_step=118200, grad_norm=2.9493253231048584, loss=1.2313659191131592
I0203 08:42:25.659657 139907720771328 logging_writer.py:48] [118300] global_step=118300, grad_norm=3.020503044128418, loss=1.3028284311294556
I0203 08:42:59.320227 139907712378624 logging_writer.py:48] [118400] global_step=118400, grad_norm=2.9904489517211914, loss=1.112535834312439
I0203 08:43:33.030223 139907720771328 logging_writer.py:48] [118500] global_step=118500, grad_norm=2.9810492992401123, loss=1.1966235637664795
I0203 08:44:06.981968 139907712378624 logging_writer.py:48] [118600] global_step=118600, grad_norm=2.9209787845611572, loss=1.1611645221710205
I0203 08:44:40.686215 139907720771328 logging_writer.py:48] [118700] global_step=118700, grad_norm=2.9912734031677246, loss=1.2266545295715332
I0203 08:45:14.368727 139907712378624 logging_writer.py:48] [118800] global_step=118800, grad_norm=3.0620296001434326, loss=1.1945862770080566
I0203 08:45:48.099908 139907720771328 logging_writer.py:48] [118900] global_step=118900, grad_norm=2.923785924911499, loss=1.2074822187423706
I0203 08:46:21.771521 139907712378624 logging_writer.py:48] [119000] global_step=119000, grad_norm=3.1331493854522705, loss=1.137251853942871
I0203 08:46:55.450567 139907720771328 logging_writer.py:48] [119100] global_step=119100, grad_norm=2.8487327098846436, loss=1.2435123920440674
I0203 08:47:29.118866 139907712378624 logging_writer.py:48] [119200] global_step=119200, grad_norm=3.001728057861328, loss=1.1969605684280396
I0203 08:48:02.803505 139907720771328 logging_writer.py:48] [119300] global_step=119300, grad_norm=2.8141300678253174, loss=1.116955280303955
I0203 08:48:36.463278 139907712378624 logging_writer.py:48] [119400] global_step=119400, grad_norm=2.724672555923462, loss=1.1255302429199219
I0203 08:49:10.161361 139907720771328 logging_writer.py:48] [119500] global_step=119500, grad_norm=3.29526424407959, loss=1.2538659572601318
I0203 08:49:29.486145 140070692116288 spec.py:321] Evaluating on the training split.
I0203 08:49:35.821713 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 08:49:44.267048 140070692116288 spec.py:349] Evaluating on the test split.
I0203 08:49:46.936787 140070692116288 submission_runner.py:408] Time since start: 41744.06s, 	Step: 119559, 	{'train/accuracy': 0.7993462681770325, 'train/loss': 0.7303955554962158, 'validation/accuracy': 0.7000199556350708, 'validation/loss': 1.2318073511123657, 'validation/num_examples': 50000, 'test/accuracy': 0.567300021648407, 'test/loss': 1.9584038257598877, 'test/num_examples': 10000, 'score': 40328.96877479553, 'total_duration': 41744.0572385788, 'accumulated_submission_time': 40328.96877479553, 'accumulated_eval_time': 1407.8446052074432, 'accumulated_logging_time': 3.0808701515197754}
I0203 08:49:46.978394 139907712378624 logging_writer.py:48] [119559] accumulated_eval_time=1407.844605, accumulated_logging_time=3.080870, accumulated_submission_time=40328.968775, global_step=119559, preemption_count=0, score=40328.968775, test/accuracy=0.567300, test/loss=1.958404, test/num_examples=10000, total_duration=41744.057239, train/accuracy=0.799346, train/loss=0.730396, validation/accuracy=0.700020, validation/loss=1.231807, validation/num_examples=50000
I0203 08:50:01.119779 139907737556736 logging_writer.py:48] [119600] global_step=119600, grad_norm=3.2500534057617188, loss=1.3445942401885986
I0203 08:50:34.930325 139907712378624 logging_writer.py:48] [119700] global_step=119700, grad_norm=3.143388271331787, loss=1.2142140865325928
I0203 08:51:08.668954 139907737556736 logging_writer.py:48] [119800] global_step=119800, grad_norm=2.963677406311035, loss=1.1567097902297974
I0203 08:51:42.340994 139907712378624 logging_writer.py:48] [119900] global_step=119900, grad_norm=2.784137487411499, loss=1.1887636184692383
I0203 08:52:16.047321 139907737556736 logging_writer.py:48] [120000] global_step=120000, grad_norm=3.1053876876831055, loss=1.2195523977279663
I0203 08:52:49.757034 139907712378624 logging_writer.py:48] [120100] global_step=120100, grad_norm=2.9571921825408936, loss=1.2004828453063965
I0203 08:53:23.442496 139907737556736 logging_writer.py:48] [120200] global_step=120200, grad_norm=3.2962305545806885, loss=1.2326290607452393
I0203 08:53:57.109487 139907712378624 logging_writer.py:48] [120300] global_step=120300, grad_norm=3.3433189392089844, loss=1.2634247541427612
I0203 08:54:30.817272 139907737556736 logging_writer.py:48] [120400] global_step=120400, grad_norm=2.856700897216797, loss=1.1486430168151855
I0203 08:55:04.524724 139907712378624 logging_writer.py:48] [120500] global_step=120500, grad_norm=2.8095343112945557, loss=1.0887415409088135
I0203 08:55:38.230157 139907737556736 logging_writer.py:48] [120600] global_step=120600, grad_norm=2.970130443572998, loss=1.2897762060165405
I0203 08:56:11.951505 139907712378624 logging_writer.py:48] [120700] global_step=120700, grad_norm=3.2941014766693115, loss=1.3012546300888062
I0203 08:56:45.762917 139907737556736 logging_writer.py:48] [120800] global_step=120800, grad_norm=3.0056629180908203, loss=1.233017921447754
I0203 08:57:19.477246 139907712378624 logging_writer.py:48] [120900] global_step=120900, grad_norm=2.8596112728118896, loss=1.2038146257400513
I0203 08:57:53.172410 139907737556736 logging_writer.py:48] [121000] global_step=121000, grad_norm=3.040600538253784, loss=1.2303860187530518
I0203 08:58:17.214696 140070692116288 spec.py:321] Evaluating on the training split.
I0203 08:58:23.531893 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 08:58:32.042839 140070692116288 spec.py:349] Evaluating on the test split.
I0203 08:58:34.703413 140070692116288 submission_runner.py:408] Time since start: 42271.82s, 	Step: 121073, 	{'train/accuracy': 0.8441884517669678, 'train/loss': 0.5559610724449158, 'validation/accuracy': 0.7035399675369263, 'validation/loss': 1.1964123249053955, 'validation/num_examples': 50000, 'test/accuracy': 0.5856000185012817, 'test/loss': 1.882620930671692, 'test/num_examples': 10000, 'score': 40839.1434905529, 'total_duration': 42271.82386422157, 'accumulated_submission_time': 40839.1434905529, 'accumulated_eval_time': 1425.3332903385162, 'accumulated_logging_time': 3.131650686264038}
I0203 08:58:34.744612 139907729164032 logging_writer.py:48] [121073] accumulated_eval_time=1425.333290, accumulated_logging_time=3.131651, accumulated_submission_time=40839.143491, global_step=121073, preemption_count=0, score=40839.143491, test/accuracy=0.585600, test/loss=1.882621, test/num_examples=10000, total_duration=42271.823864, train/accuracy=0.844188, train/loss=0.555961, validation/accuracy=0.703540, validation/loss=1.196412, validation/num_examples=50000
I0203 08:58:44.169782 139907745949440 logging_writer.py:48] [121100] global_step=121100, grad_norm=3.139265537261963, loss=1.0937470197677612
I0203 08:59:17.825040 139907729164032 logging_writer.py:48] [121200] global_step=121200, grad_norm=2.8217785358428955, loss=1.1695879697799683
I0203 08:59:51.516448 139907745949440 logging_writer.py:48] [121300] global_step=121300, grad_norm=3.07409930229187, loss=1.1148157119750977
I0203 09:00:25.211581 139907729164032 logging_writer.py:48] [121400] global_step=121400, grad_norm=2.9785103797912598, loss=1.1424813270568848
I0203 09:00:58.873378 139907745949440 logging_writer.py:48] [121500] global_step=121500, grad_norm=2.9323084354400635, loss=1.1990867853164673
I0203 09:01:32.571535 139907729164032 logging_writer.py:48] [121600] global_step=121600, grad_norm=3.320932388305664, loss=1.240630865097046
I0203 09:02:06.293574 139907745949440 logging_writer.py:48] [121700] global_step=121700, grad_norm=3.0374999046325684, loss=1.2305262088775635
I0203 09:02:40.020142 139907729164032 logging_writer.py:48] [121800] global_step=121800, grad_norm=2.8993828296661377, loss=1.0825778245925903
I0203 09:03:13.683248 139907745949440 logging_writer.py:48] [121900] global_step=121900, grad_norm=3.1324100494384766, loss=1.3034671545028687
I0203 09:03:47.514241 139907729164032 logging_writer.py:48] [122000] global_step=122000, grad_norm=2.923025369644165, loss=1.1252343654632568
I0203 09:04:21.187578 139907745949440 logging_writer.py:48] [122100] global_step=122100, grad_norm=2.9963104724884033, loss=1.2504545450210571
I0203 09:04:54.886643 139907729164032 logging_writer.py:48] [122200] global_step=122200, grad_norm=3.0054070949554443, loss=1.1266050338745117
I0203 09:05:28.555969 139907745949440 logging_writer.py:48] [122300] global_step=122300, grad_norm=2.9584176540374756, loss=1.174269437789917
I0203 09:06:02.272837 139907729164032 logging_writer.py:48] [122400] global_step=122400, grad_norm=3.032625436782837, loss=1.2008943557739258
I0203 09:06:35.924049 139907745949440 logging_writer.py:48] [122500] global_step=122500, grad_norm=3.4134202003479004, loss=1.204892873764038
I0203 09:07:05.032453 140070692116288 spec.py:321] Evaluating on the training split.
I0203 09:07:11.411571 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 09:07:19.841140 140070692116288 spec.py:349] Evaluating on the test split.
I0203 09:07:22.479309 140070692116288 submission_runner.py:408] Time since start: 42799.60s, 	Step: 122588, 	{'train/accuracy': 0.83203125, 'train/loss': 0.6034864783287048, 'validation/accuracy': 0.7110399603843689, 'validation/loss': 1.172864317893982, 'validation/num_examples': 50000, 'test/accuracy': 0.5904000401496887, 'test/loss': 1.8778742551803589, 'test/num_examples': 10000, 'score': 41349.369044303894, 'total_duration': 42799.59975862503, 'accumulated_submission_time': 41349.369044303894, 'accumulated_eval_time': 1442.7801163196564, 'accumulated_logging_time': 3.1828622817993164}
I0203 09:07:22.518851 139907712378624 logging_writer.py:48] [122588] accumulated_eval_time=1442.780116, accumulated_logging_time=3.182862, accumulated_submission_time=41349.369044, global_step=122588, preemption_count=0, score=41349.369044, test/accuracy=0.590400, test/loss=1.877874, test/num_examples=10000, total_duration=42799.599759, train/accuracy=0.832031, train/loss=0.603486, validation/accuracy=0.711040, validation/loss=1.172864, validation/num_examples=50000
I0203 09:07:26.886098 139907720771328 logging_writer.py:48] [122600] global_step=122600, grad_norm=3.23626971244812, loss=1.1891732215881348
I0203 09:08:00.493174 139907712378624 logging_writer.py:48] [122700] global_step=122700, grad_norm=2.987381935119629, loss=1.0603408813476562
I0203 09:08:34.151045 139907720771328 logging_writer.py:48] [122800] global_step=122800, grad_norm=2.95668625831604, loss=1.1337547302246094
I0203 09:09:07.822673 139907712378624 logging_writer.py:48] [122900] global_step=122900, grad_norm=2.785013437271118, loss=1.028949499130249
I0203 09:09:41.482290 139907720771328 logging_writer.py:48] [123000] global_step=123000, grad_norm=3.042356014251709, loss=1.2002317905426025
I0203 09:10:15.297611 139907712378624 logging_writer.py:48] [123100] global_step=123100, grad_norm=2.8860251903533936, loss=1.1617109775543213
I0203 09:10:48.982583 139907720771328 logging_writer.py:48] [123200] global_step=123200, grad_norm=3.1562936305999756, loss=1.2214016914367676
I0203 09:11:22.652124 139907712378624 logging_writer.py:48] [123300] global_step=123300, grad_norm=3.1292057037353516, loss=1.1119604110717773
I0203 09:11:56.310109 139907720771328 logging_writer.py:48] [123400] global_step=123400, grad_norm=3.490894079208374, loss=1.1798696517944336
I0203 09:12:30.010507 139907712378624 logging_writer.py:48] [123500] global_step=123500, grad_norm=3.212926149368286, loss=1.2229013442993164
I0203 09:13:03.662576 139907720771328 logging_writer.py:48] [123600] global_step=123600, grad_norm=2.971104621887207, loss=1.135508418083191
I0203 09:13:37.345258 139907712378624 logging_writer.py:48] [123700] global_step=123700, grad_norm=2.9706995487213135, loss=1.0885285139083862
I0203 09:14:11.001390 139907720771328 logging_writer.py:48] [123800] global_step=123800, grad_norm=3.291205406188965, loss=1.1680859327316284
I0203 09:14:44.694421 139907712378624 logging_writer.py:48] [123900] global_step=123900, grad_norm=2.8508224487304688, loss=1.146344542503357
I0203 09:15:18.378407 139907720771328 logging_writer.py:48] [124000] global_step=124000, grad_norm=2.8967931270599365, loss=1.0791116952896118
I0203 09:15:52.146459 139907712378624 logging_writer.py:48] [124100] global_step=124100, grad_norm=2.8715503215789795, loss=1.1185710430145264
I0203 09:15:52.642661 140070692116288 spec.py:321] Evaluating on the training split.
I0203 09:15:59.216732 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 09:16:07.907968 140070692116288 spec.py:349] Evaluating on the test split.
I0203 09:16:10.645590 140070692116288 submission_runner.py:408] Time since start: 43327.77s, 	Step: 124103, 	{'train/accuracy': 0.8218072056770325, 'train/loss': 0.6350313425064087, 'validation/accuracy': 0.7073799967765808, 'validation/loss': 1.1969265937805176, 'validation/num_examples': 50000, 'test/accuracy': 0.5773000121116638, 'test/loss': 1.9539101123809814, 'test/num_examples': 10000, 'score': 41859.428148031235, 'total_duration': 43327.76603150368, 'accumulated_submission_time': 41859.428148031235, 'accumulated_eval_time': 1460.7830305099487, 'accumulated_logging_time': 3.2322473526000977}
I0203 09:16:10.688958 139907712378624 logging_writer.py:48] [124103] accumulated_eval_time=1460.783031, accumulated_logging_time=3.232247, accumulated_submission_time=41859.428148, global_step=124103, preemption_count=0, score=41859.428148, test/accuracy=0.577300, test/loss=1.953910, test/num_examples=10000, total_duration=43327.766032, train/accuracy=0.821807, train/loss=0.635031, validation/accuracy=0.707380, validation/loss=1.196927, validation/num_examples=50000
I0203 09:16:43.798468 139907720771328 logging_writer.py:48] [124200] global_step=124200, grad_norm=2.8894107341766357, loss=1.17411470413208
I0203 09:17:17.513275 139907712378624 logging_writer.py:48] [124300] global_step=124300, grad_norm=3.300950050354004, loss=1.2329586744308472
I0203 09:17:51.191879 139907720771328 logging_writer.py:48] [124400] global_step=124400, grad_norm=2.904628038406372, loss=1.1259602308273315
I0203 09:18:24.898530 139907712378624 logging_writer.py:48] [124500] global_step=124500, grad_norm=2.9086709022521973, loss=1.1689307689666748
I0203 09:18:58.572191 139907720771328 logging_writer.py:48] [124600] global_step=124600, grad_norm=2.811263084411621, loss=1.083148717880249
I0203 09:19:32.257422 139907712378624 logging_writer.py:48] [124700] global_step=124700, grad_norm=3.217155933380127, loss=1.148753046989441
I0203 09:20:05.936342 139907720771328 logging_writer.py:48] [124800] global_step=124800, grad_norm=3.022650957107544, loss=1.1526633501052856
I0203 09:20:39.625805 139907712378624 logging_writer.py:48] [124900] global_step=124900, grad_norm=3.302877902984619, loss=1.1543731689453125
I0203 09:21:13.289203 139907720771328 logging_writer.py:48] [125000] global_step=125000, grad_norm=3.1474132537841797, loss=1.1289806365966797
I0203 09:21:46.978548 139907712378624 logging_writer.py:48] [125100] global_step=125100, grad_norm=3.321601152420044, loss=1.2320727109909058
I0203 09:22:20.665563 139907720771328 logging_writer.py:48] [125200] global_step=125200, grad_norm=3.226712703704834, loss=1.2316784858703613
I0203 09:22:54.353056 139907712378624 logging_writer.py:48] [125300] global_step=125300, grad_norm=3.0832531452178955, loss=1.1379882097244263
I0203 09:23:28.166006 139907720771328 logging_writer.py:48] [125400] global_step=125400, grad_norm=3.237140417098999, loss=1.1278352737426758
I0203 09:24:01.853163 139907712378624 logging_writer.py:48] [125500] global_step=125500, grad_norm=3.342247247695923, loss=1.2212283611297607
I0203 09:24:35.545446 139907720771328 logging_writer.py:48] [125600] global_step=125600, grad_norm=3.0092544555664062, loss=1.1442234516143799
I0203 09:24:40.736953 140070692116288 spec.py:321] Evaluating on the training split.
I0203 09:24:47.011308 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 09:24:55.649181 140070692116288 spec.py:349] Evaluating on the test split.
I0203 09:24:58.306950 140070692116288 submission_runner.py:408] Time since start: 43855.43s, 	Step: 125617, 	{'train/accuracy': 0.8284637928009033, 'train/loss': 0.6209262609481812, 'validation/accuracy': 0.7165399789810181, 'validation/loss': 1.1599438190460205, 'validation/num_examples': 50000, 'test/accuracy': 0.5925000309944153, 'test/loss': 1.8581621646881104, 'test/num_examples': 10000, 'score': 42369.41453456879, 'total_duration': 43855.42738699913, 'accumulated_submission_time': 42369.41453456879, 'accumulated_eval_time': 1478.3529794216156, 'accumulated_logging_time': 3.285027503967285}
I0203 09:24:58.346422 139907703985920 logging_writer.py:48] [125617] accumulated_eval_time=1478.352979, accumulated_logging_time=3.285028, accumulated_submission_time=42369.414535, global_step=125617, preemption_count=0, score=42369.414535, test/accuracy=0.592500, test/loss=1.858162, test/num_examples=10000, total_duration=43855.427387, train/accuracy=0.828464, train/loss=0.620926, validation/accuracy=0.716540, validation/loss=1.159944, validation/num_examples=50000
I0203 09:25:26.645110 139907737556736 logging_writer.py:48] [125700] global_step=125700, grad_norm=3.411450147628784, loss=1.1558706760406494
I0203 09:26:00.326360 139907703985920 logging_writer.py:48] [125800] global_step=125800, grad_norm=2.8823111057281494, loss=1.064369797706604
I0203 09:26:33.990008 139907737556736 logging_writer.py:48] [125900] global_step=125900, grad_norm=3.2744693756103516, loss=1.1163280010223389
I0203 09:27:07.687614 139907703985920 logging_writer.py:48] [126000] global_step=126000, grad_norm=3.3039839267730713, loss=1.2379038333892822
I0203 09:27:41.400147 139907737556736 logging_writer.py:48] [126100] global_step=126100, grad_norm=3.149068593978882, loss=1.0503331422805786
I0203 09:28:15.143349 139907703985920 logging_writer.py:48] [126200] global_step=126200, grad_norm=3.2093043327331543, loss=1.1190611124038696
I0203 09:28:48.797776 139907737556736 logging_writer.py:48] [126300] global_step=126300, grad_norm=3.1506080627441406, loss=1.1248542070388794
I0203 09:29:22.492731 139907703985920 logging_writer.py:48] [126400] global_step=126400, grad_norm=3.158874273300171, loss=1.0806989669799805
I0203 09:29:56.279824 139907737556736 logging_writer.py:48] [126500] global_step=126500, grad_norm=3.1863248348236084, loss=1.1385122537612915
I0203 09:30:29.996965 139907703985920 logging_writer.py:48] [126600] global_step=126600, grad_norm=3.18355655670166, loss=0.9987215995788574
I0203 09:31:03.642484 139907737556736 logging_writer.py:48] [126700] global_step=126700, grad_norm=3.092087507247925, loss=1.0648565292358398
I0203 09:31:37.331927 139907703985920 logging_writer.py:48] [126800] global_step=126800, grad_norm=3.0715019702911377, loss=1.0704809427261353
I0203 09:32:10.983570 139907737556736 logging_writer.py:48] [126900] global_step=126900, grad_norm=3.223602771759033, loss=1.0464946031570435
I0203 09:32:44.693678 139907703985920 logging_writer.py:48] [127000] global_step=127000, grad_norm=3.1606483459472656, loss=1.2035976648330688
I0203 09:33:18.347032 139907737556736 logging_writer.py:48] [127100] global_step=127100, grad_norm=3.183950662612915, loss=1.0901539325714111
I0203 09:33:28.600019 140070692116288 spec.py:321] Evaluating on the training split.
I0203 09:33:35.048927 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 09:33:43.377285 140070692116288 spec.py:349] Evaluating on the test split.
I0203 09:33:46.060819 140070692116288 submission_runner.py:408] Time since start: 44383.18s, 	Step: 127132, 	{'train/accuracy': 0.8194355964660645, 'train/loss': 0.6457960605621338, 'validation/accuracy': 0.7070199847221375, 'validation/loss': 1.2006008625030518, 'validation/num_examples': 50000, 'test/accuracy': 0.5856000185012817, 'test/loss': 1.9250835180282593, 'test/num_examples': 10000, 'score': 42879.60580301285, 'total_duration': 44383.18127179146, 'accumulated_submission_time': 42879.60580301285, 'accumulated_eval_time': 1495.813749074936, 'accumulated_logging_time': 3.3334877490997314}
I0203 09:33:46.099325 139907729164032 logging_writer.py:48] [127132] accumulated_eval_time=1495.813749, accumulated_logging_time=3.333488, accumulated_submission_time=42879.605803, global_step=127132, preemption_count=0, score=42879.605803, test/accuracy=0.585600, test/loss=1.925084, test/num_examples=10000, total_duration=44383.181272, train/accuracy=0.819436, train/loss=0.645796, validation/accuracy=0.707020, validation/loss=1.200601, validation/num_examples=50000
I0203 09:34:09.286957 139907745949440 logging_writer.py:48] [127200] global_step=127200, grad_norm=3.3146989345550537, loss=1.1363998651504517
I0203 09:34:42.947674 139907729164032 logging_writer.py:48] [127300] global_step=127300, grad_norm=3.300515651702881, loss=1.2857623100280762
I0203 09:35:16.643252 139907745949440 logging_writer.py:48] [127400] global_step=127400, grad_norm=3.396139144897461, loss=1.1427364349365234
I0203 09:35:50.313283 139907729164032 logging_writer.py:48] [127500] global_step=127500, grad_norm=3.088780164718628, loss=1.1312339305877686
I0203 09:36:24.109100 139907745949440 logging_writer.py:48] [127600] global_step=127600, grad_norm=2.9565296173095703, loss=1.0749428272247314
I0203 09:36:57.798694 139907729164032 logging_writer.py:48] [127700] global_step=127700, grad_norm=3.2410638332366943, loss=1.130077838897705
I0203 09:37:31.491830 139907745949440 logging_writer.py:48] [127800] global_step=127800, grad_norm=3.15822434425354, loss=1.1553387641906738
I0203 09:38:05.169108 139907729164032 logging_writer.py:48] [127900] global_step=127900, grad_norm=3.558518648147583, loss=1.2051334381103516
I0203 09:38:38.859725 139907745949440 logging_writer.py:48] [128000] global_step=128000, grad_norm=3.075138807296753, loss=1.1490370035171509
I0203 09:39:12.522070 139907729164032 logging_writer.py:48] [128100] global_step=128100, grad_norm=3.2334301471710205, loss=1.1598786115646362
I0203 09:39:46.176897 139907745949440 logging_writer.py:48] [128200] global_step=128200, grad_norm=3.4239535331726074, loss=1.1476447582244873
I0203 09:40:19.881579 139907729164032 logging_writer.py:48] [128300] global_step=128300, grad_norm=3.2452824115753174, loss=1.1653203964233398
I0203 09:40:53.579720 139907745949440 logging_writer.py:48] [128400] global_step=128400, grad_norm=3.2230279445648193, loss=1.14029061794281
I0203 09:41:27.222276 139907729164032 logging_writer.py:48] [128500] global_step=128500, grad_norm=3.4730868339538574, loss=1.1183149814605713
I0203 09:42:00.891092 139907745949440 logging_writer.py:48] [128600] global_step=128600, grad_norm=2.996173620223999, loss=1.0907148122787476
I0203 09:42:16.200291 140070692116288 spec.py:321] Evaluating on the training split.
I0203 09:42:22.447239 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 09:42:31.092941 140070692116288 spec.py:349] Evaluating on the test split.
I0203 09:42:33.767079 140070692116288 submission_runner.py:408] Time since start: 44910.89s, 	Step: 128647, 	{'train/accuracy': 0.8309949040412903, 'train/loss': 0.6023525595664978, 'validation/accuracy': 0.7137399911880493, 'validation/loss': 1.1708685159683228, 'validation/num_examples': 50000, 'test/accuracy': 0.5854000449180603, 'test/loss': 1.8945751190185547, 'test/num_examples': 10000, 'score': 43389.64476656914, 'total_duration': 44910.88752961159, 'accumulated_submission_time': 43389.64476656914, 'accumulated_eval_time': 1513.3805103302002, 'accumulated_logging_time': 3.381448268890381}
I0203 09:42:33.806677 139907720771328 logging_writer.py:48] [128647] accumulated_eval_time=1513.380510, accumulated_logging_time=3.381448, accumulated_submission_time=43389.644767, global_step=128647, preemption_count=0, score=43389.644767, test/accuracy=0.585400, test/loss=1.894575, test/num_examples=10000, total_duration=44910.887530, train/accuracy=0.830995, train/loss=0.602353, validation/accuracy=0.713740, validation/loss=1.170869, validation/num_examples=50000
I0203 09:42:52.120058 139907737556736 logging_writer.py:48] [128700] global_step=128700, grad_norm=3.193218469619751, loss=0.9541168212890625
I0203 09:43:25.781289 139907720771328 logging_writer.py:48] [128800] global_step=128800, grad_norm=3.1387228965759277, loss=1.1524982452392578
I0203 09:43:59.481784 139907737556736 logging_writer.py:48] [128900] global_step=128900, grad_norm=3.2446553707122803, loss=1.1030842065811157
I0203 09:44:33.144841 139907720771328 logging_writer.py:48] [129000] global_step=129000, grad_norm=3.0711770057678223, loss=1.0133743286132812
I0203 09:45:06.848681 139907737556736 logging_writer.py:48] [129100] global_step=129100, grad_norm=3.290386199951172, loss=1.1479334831237793
I0203 09:45:40.575845 139907720771328 logging_writer.py:48] [129200] global_step=129200, grad_norm=3.4220292568206787, loss=1.1278839111328125
I0203 09:46:14.302881 139907737556736 logging_writer.py:48] [129300] global_step=129300, grad_norm=3.513810634613037, loss=1.1446592807769775
I0203 09:46:47.957439 139907720771328 logging_writer.py:48] [129400] global_step=129400, grad_norm=2.94341778755188, loss=0.9905726909637451
I0203 09:47:21.649836 139907737556736 logging_writer.py:48] [129500] global_step=129500, grad_norm=3.408576488494873, loss=1.1049718856811523
I0203 09:47:55.323522 139907720771328 logging_writer.py:48] [129600] global_step=129600, grad_norm=3.1909525394439697, loss=0.9863977432250977
I0203 09:48:29.104152 139907737556736 logging_writer.py:48] [129700] global_step=129700, grad_norm=3.4325315952301025, loss=1.0549830198287964
I0203 09:49:02.754546 139907720771328 logging_writer.py:48] [129800] global_step=129800, grad_norm=3.0398921966552734, loss=1.0323210954666138
I0203 09:49:36.712980 139907737556736 logging_writer.py:48] [129900] global_step=129900, grad_norm=3.1004812717437744, loss=1.1399753093719482
I0203 09:50:10.396685 139907720771328 logging_writer.py:48] [130000] global_step=130000, grad_norm=3.074103355407715, loss=1.1737103462219238
I0203 09:50:44.093078 139907737556736 logging_writer.py:48] [130100] global_step=130100, grad_norm=3.157252788543701, loss=1.0496504306793213
I0203 09:51:03.805362 140070692116288 spec.py:321] Evaluating on the training split.
I0203 09:51:10.147311 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 09:51:18.843612 140070692116288 spec.py:349] Evaluating on the test split.
I0203 09:51:21.398053 140070692116288 submission_runner.py:408] Time since start: 45438.52s, 	Step: 130160, 	{'train/accuracy': 0.8641980290412903, 'train/loss': 0.48220840096473694, 'validation/accuracy': 0.7169199585914612, 'validation/loss': 1.1536674499511719, 'validation/num_examples': 50000, 'test/accuracy': 0.5945000052452087, 'test/loss': 1.87068510055542, 'test/num_examples': 10000, 'score': 43899.57999563217, 'total_duration': 45438.51849889755, 'accumulated_submission_time': 43899.57999563217, 'accumulated_eval_time': 1530.9731702804565, 'accumulated_logging_time': 3.430178165435791}
I0203 09:51:21.442128 139907712378624 logging_writer.py:48] [130160] accumulated_eval_time=1530.973170, accumulated_logging_time=3.430178, accumulated_submission_time=43899.579996, global_step=130160, preemption_count=0, score=43899.579996, test/accuracy=0.594500, test/loss=1.870685, test/num_examples=10000, total_duration=45438.518499, train/accuracy=0.864198, train/loss=0.482208, validation/accuracy=0.716920, validation/loss=1.153667, validation/num_examples=50000
I0203 09:51:35.264445 139907729164032 logging_writer.py:48] [130200] global_step=130200, grad_norm=3.458324432373047, loss=1.037940502166748
I0203 09:52:08.969370 139907712378624 logging_writer.py:48] [130300] global_step=130300, grad_norm=3.2716264724731445, loss=1.0449728965759277
I0203 09:52:42.653756 139907729164032 logging_writer.py:48] [130400] global_step=130400, grad_norm=3.1078977584838867, loss=0.9728106260299683
I0203 09:53:16.323477 139907712378624 logging_writer.py:48] [130500] global_step=130500, grad_norm=3.4099061489105225, loss=0.9662259817123413
I0203 09:53:49.996075 139907729164032 logging_writer.py:48] [130600] global_step=130600, grad_norm=3.2789063453674316, loss=1.0118207931518555
I0203 09:54:23.663351 139907712378624 logging_writer.py:48] [130700] global_step=130700, grad_norm=3.665010452270508, loss=1.0874943733215332
I0203 09:54:57.356074 139907729164032 logging_writer.py:48] [130800] global_step=130800, grad_norm=3.2500717639923096, loss=1.0201587677001953
I0203 09:55:31.057923 139907712378624 logging_writer.py:48] [130900] global_step=130900, grad_norm=3.170445203781128, loss=1.0460283756256104
I0203 09:56:04.897989 139907729164032 logging_writer.py:48] [131000] global_step=131000, grad_norm=3.0871784687042236, loss=1.058597207069397
I0203 09:56:38.577337 139907712378624 logging_writer.py:48] [131100] global_step=131100, grad_norm=3.283674955368042, loss=0.9953464269638062
I0203 09:57:12.231594 139907729164032 logging_writer.py:48] [131200] global_step=131200, grad_norm=3.215247392654419, loss=1.0316073894500732
I0203 09:57:45.883314 139907712378624 logging_writer.py:48] [131300] global_step=131300, grad_norm=3.1714069843292236, loss=1.0240663290023804
I0203 09:58:19.567597 139907729164032 logging_writer.py:48] [131400] global_step=131400, grad_norm=3.09394907951355, loss=1.0514017343521118
I0203 09:58:53.241595 139907712378624 logging_writer.py:48] [131500] global_step=131500, grad_norm=3.363663911819458, loss=1.1845624446868896
I0203 09:59:26.918681 139907729164032 logging_writer.py:48] [131600] global_step=131600, grad_norm=3.1440579891204834, loss=1.0770151615142822
I0203 09:59:51.642392 140070692116288 spec.py:321] Evaluating on the training split.
I0203 09:59:58.676285 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 10:00:07.419895 140070692116288 spec.py:349] Evaluating on the test split.
I0203 10:00:10.115875 140070692116288 submission_runner.py:408] Time since start: 45967.24s, 	Step: 131675, 	{'train/accuracy': 0.853535532951355, 'train/loss': 0.5158511400222778, 'validation/accuracy': 0.7181800007820129, 'validation/loss': 1.1625126600265503, 'validation/num_examples': 50000, 'test/accuracy': 0.5956000089645386, 'test/loss': 1.894251823425293, 'test/num_examples': 10000, 'score': 44409.718037605286, 'total_duration': 45967.23632621765, 'accumulated_submission_time': 44409.718037605286, 'accumulated_eval_time': 1549.4466173648834, 'accumulated_logging_time': 3.483417510986328}
I0203 10:00:10.156080 139907703985920 logging_writer.py:48] [131675] accumulated_eval_time=1549.446617, accumulated_logging_time=3.483418, accumulated_submission_time=44409.718038, global_step=131675, preemption_count=0, score=44409.718038, test/accuracy=0.595600, test/loss=1.894252, test/num_examples=10000, total_duration=45967.236326, train/accuracy=0.853536, train/loss=0.515851, validation/accuracy=0.718180, validation/loss=1.162513, validation/num_examples=50000
I0203 10:00:18.918229 139907720771328 logging_writer.py:48] [131700] global_step=131700, grad_norm=3.2718794345855713, loss=1.1127632856369019
I0203 10:00:52.583833 139907703985920 logging_writer.py:48] [131800] global_step=131800, grad_norm=3.3957183361053467, loss=1.0679783821105957
I0203 10:01:26.262920 139907720771328 logging_writer.py:48] [131900] global_step=131900, grad_norm=3.3600564002990723, loss=1.07014000415802
I0203 10:01:59.961108 139907703985920 logging_writer.py:48] [132000] global_step=132000, grad_norm=3.0594265460968018, loss=1.028213381767273
I0203 10:02:33.773004 139907720771328 logging_writer.py:48] [132100] global_step=132100, grad_norm=3.486433744430542, loss=1.1150554418563843
I0203 10:03:07.453240 139907703985920 logging_writer.py:48] [132200] global_step=132200, grad_norm=3.1600046157836914, loss=1.0857903957366943
I0203 10:03:41.119521 139907720771328 logging_writer.py:48] [132300] global_step=132300, grad_norm=3.4307169914245605, loss=1.0559587478637695
I0203 10:04:14.842071 139907703985920 logging_writer.py:48] [132400] global_step=132400, grad_norm=3.471121311187744, loss=1.0864490270614624
I0203 10:04:48.523579 139907720771328 logging_writer.py:48] [132500] global_step=132500, grad_norm=3.373563051223755, loss=0.9592522382736206
I0203 10:05:22.228786 139907703985920 logging_writer.py:48] [132600] global_step=132600, grad_norm=3.478478193283081, loss=1.0800657272338867
I0203 10:05:55.871983 139907720771328 logging_writer.py:48] [132700] global_step=132700, grad_norm=3.672250509262085, loss=1.0894365310668945
I0203 10:06:29.611456 139907703985920 logging_writer.py:48] [132800] global_step=132800, grad_norm=3.2188363075256348, loss=0.9938112497329712
I0203 10:07:03.284362 139907720771328 logging_writer.py:48] [132900] global_step=132900, grad_norm=3.3731689453125, loss=1.1097465753555298
I0203 10:07:37.017338 139907703985920 logging_writer.py:48] [133000] global_step=133000, grad_norm=3.5752363204956055, loss=1.1934328079223633
I0203 10:08:10.670731 139907720771328 logging_writer.py:48] [133100] global_step=133100, grad_norm=3.323798418045044, loss=1.0070840120315552
I0203 10:08:40.117839 140070692116288 spec.py:321] Evaluating on the training split.
I0203 10:08:46.494447 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 10:08:55.198468 140070692116288 spec.py:349] Evaluating on the test split.
I0203 10:08:57.840811 140070692116288 submission_runner.py:408] Time since start: 46494.96s, 	Step: 133189, 	{'train/accuracy': 0.8512635231018066, 'train/loss': 0.518912672996521, 'validation/accuracy': 0.7234999537467957, 'validation/loss': 1.1488239765167236, 'validation/num_examples': 50000, 'test/accuracy': 0.5996000170707703, 'test/loss': 1.8523231744766235, 'test/num_examples': 10000, 'score': 44919.616294384, 'total_duration': 46494.96126246452, 'accumulated_submission_time': 44919.616294384, 'accumulated_eval_time': 1567.1695573329926, 'accumulated_logging_time': 3.5336878299713135}
I0203 10:08:57.885095 139907703985920 logging_writer.py:48] [133189] accumulated_eval_time=1567.169557, accumulated_logging_time=3.533688, accumulated_submission_time=44919.616294, global_step=133189, preemption_count=0, score=44919.616294, test/accuracy=0.599600, test/loss=1.852323, test/num_examples=10000, total_duration=46494.961262, train/accuracy=0.851264, train/loss=0.518913, validation/accuracy=0.723500, validation/loss=1.148824, validation/num_examples=50000
I0203 10:09:01.937641 139907712378624 logging_writer.py:48] [133200] global_step=133200, grad_norm=3.5917158126831055, loss=1.1671268939971924
I0203 10:09:35.663970 139907703985920 logging_writer.py:48] [133300] global_step=133300, grad_norm=3.0810720920562744, loss=1.0235929489135742
I0203 10:10:09.328213 139907712378624 logging_writer.py:48] [133400] global_step=133400, grad_norm=3.3471670150756836, loss=1.1312506198883057
I0203 10:10:43.025691 139907703985920 logging_writer.py:48] [133500] global_step=133500, grad_norm=3.2605721950531006, loss=1.075843095779419
I0203 10:11:16.708621 139907712378624 logging_writer.py:48] [133600] global_step=133600, grad_norm=3.6055002212524414, loss=0.9946461915969849
I0203 10:11:50.389852 139907703985920 logging_writer.py:48] [133700] global_step=133700, grad_norm=3.709312915802002, loss=1.0224016904830933
I0203 10:12:24.060021 139907712378624 logging_writer.py:48] [133800] global_step=133800, grad_norm=3.6293962001800537, loss=1.164627194404602
I0203 10:12:57.738432 139907703985920 logging_writer.py:48] [133900] global_step=133900, grad_norm=3.1150176525115967, loss=1.0082381963729858
I0203 10:13:31.405957 139907712378624 logging_writer.py:48] [134000] global_step=134000, grad_norm=3.303372621536255, loss=0.9980055093765259
I0203 10:14:05.130401 139907703985920 logging_writer.py:48] [134100] global_step=134100, grad_norm=3.5452721118927, loss=1.0539807081222534
I0203 10:14:38.843082 139907712378624 logging_writer.py:48] [134200] global_step=134200, grad_norm=3.351135015487671, loss=1.0180952548980713
I0203 10:15:12.520412 139907703985920 logging_writer.py:48] [134300] global_step=134300, grad_norm=3.3526251316070557, loss=1.1098037958145142
I0203 10:15:46.326547 139907712378624 logging_writer.py:48] [134400] global_step=134400, grad_norm=3.06441593170166, loss=0.9912901520729065
I0203 10:16:20.033292 139907703985920 logging_writer.py:48] [134500] global_step=134500, grad_norm=3.4678421020507812, loss=1.1254361867904663
I0203 10:16:53.691069 139907712378624 logging_writer.py:48] [134600] global_step=134600, grad_norm=3.3871350288391113, loss=1.0090798139572144
I0203 10:17:27.401633 139907703985920 logging_writer.py:48] [134700] global_step=134700, grad_norm=3.5486721992492676, loss=1.0853054523468018
I0203 10:17:27.895663 140070692116288 spec.py:321] Evaluating on the training split.
I0203 10:17:34.224466 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 10:17:42.905720 140070692116288 spec.py:349] Evaluating on the test split.
I0203 10:17:45.488995 140070692116288 submission_runner.py:408] Time since start: 47022.61s, 	Step: 134703, 	{'train/accuracy': 0.8521404266357422, 'train/loss': 0.5233726501464844, 'validation/accuracy': 0.7243199944496155, 'validation/loss': 1.1318014860153198, 'validation/num_examples': 50000, 'test/accuracy': 0.6012000441551208, 'test/loss': 1.8476316928863525, 'test/num_examples': 10000, 'score': 45429.56394815445, 'total_duration': 47022.609437942505, 'accumulated_submission_time': 45429.56394815445, 'accumulated_eval_time': 1584.762847185135, 'accumulated_logging_time': 3.5877740383148193}
I0203 10:17:45.529986 139907712378624 logging_writer.py:48] [134703] accumulated_eval_time=1584.762847, accumulated_logging_time=3.587774, accumulated_submission_time=45429.563948, global_step=134703, preemption_count=0, score=45429.563948, test/accuracy=0.601200, test/loss=1.847632, test/num_examples=10000, total_duration=47022.609438, train/accuracy=0.852140, train/loss=0.523373, validation/accuracy=0.724320, validation/loss=1.131801, validation/num_examples=50000
I0203 10:18:18.484503 139907720771328 logging_writer.py:48] [134800] global_step=134800, grad_norm=3.201979875564575, loss=1.016871452331543
I0203 10:18:52.182346 139907712378624 logging_writer.py:48] [134900] global_step=134900, grad_norm=3.2634775638580322, loss=0.9733068943023682
I0203 10:19:25.828340 139907720771328 logging_writer.py:48] [135000] global_step=135000, grad_norm=3.516920328140259, loss=1.0374891757965088
I0203 10:19:59.558999 139907712378624 logging_writer.py:48] [135100] global_step=135100, grad_norm=3.3097970485687256, loss=0.9872131943702698
I0203 10:20:33.226043 139907720771328 logging_writer.py:48] [135200] global_step=135200, grad_norm=3.337862253189087, loss=1.1014466285705566
I0203 10:21:06.928712 139907712378624 logging_writer.py:48] [135300] global_step=135300, grad_norm=3.4987752437591553, loss=1.072792649269104
I0203 10:21:40.611311 139907720771328 logging_writer.py:48] [135400] global_step=135400, grad_norm=3.7430882453918457, loss=1.1396851539611816
I0203 10:22:14.402021 139907712378624 logging_writer.py:48] [135500] global_step=135500, grad_norm=3.415064811706543, loss=1.1068366765975952
I0203 10:22:48.098189 139907720771328 logging_writer.py:48] [135600] global_step=135600, grad_norm=3.478956460952759, loss=1.0167336463928223
I0203 10:23:21.799633 139907712378624 logging_writer.py:48] [135700] global_step=135700, grad_norm=3.3847901821136475, loss=0.9910199642181396
I0203 10:23:55.476517 139907720771328 logging_writer.py:48] [135800] global_step=135800, grad_norm=3.32228684425354, loss=1.0239195823669434
I0203 10:24:29.156239 139907712378624 logging_writer.py:48] [135900] global_step=135900, grad_norm=3.254464626312256, loss=0.9536017775535583
I0203 10:25:02.910485 139907720771328 logging_writer.py:48] [136000] global_step=136000, grad_norm=3.399644374847412, loss=1.0628989934921265
I0203 10:25:36.636728 139907712378624 logging_writer.py:48] [136100] global_step=136100, grad_norm=3.5345640182495117, loss=1.0598194599151611
I0203 10:26:10.360717 139907720771328 logging_writer.py:48] [136200] global_step=136200, grad_norm=3.137151002883911, loss=1.0186985731124878
I0203 10:26:15.582591 140070692116288 spec.py:321] Evaluating on the training split.
I0203 10:26:21.962389 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 10:26:30.575143 140070692116288 spec.py:349] Evaluating on the test split.
I0203 10:26:33.248792 140070692116288 submission_runner.py:408] Time since start: 47550.37s, 	Step: 136217, 	{'train/accuracy': 0.8546316623687744, 'train/loss': 0.5151609182357788, 'validation/accuracy': 0.7261799573898315, 'validation/loss': 1.1283327341079712, 'validation/num_examples': 50000, 'test/accuracy': 0.6022000312805176, 'test/loss': 1.8703584671020508, 'test/num_examples': 10000, 'score': 45939.55491280556, 'total_duration': 47550.36924123764, 'accumulated_submission_time': 45939.55491280556, 'accumulated_eval_time': 1602.4290103912354, 'accumulated_logging_time': 3.637843132019043}
I0203 10:26:33.292800 139907754342144 logging_writer.py:48] [136217] accumulated_eval_time=1602.429010, accumulated_logging_time=3.637843, accumulated_submission_time=45939.554913, global_step=136217, preemption_count=0, score=45939.554913, test/accuracy=0.602200, test/loss=1.870358, test/num_examples=10000, total_duration=47550.369241, train/accuracy=0.854632, train/loss=0.515161, validation/accuracy=0.726180, validation/loss=1.128333, validation/num_examples=50000
I0203 10:27:01.582214 139907762734848 logging_writer.py:48] [136300] global_step=136300, grad_norm=3.3327090740203857, loss=0.971321702003479
I0203 10:27:35.309984 139907754342144 logging_writer.py:48] [136400] global_step=136400, grad_norm=3.653057336807251, loss=1.0071622133255005
I0203 10:28:08.993352 139907762734848 logging_writer.py:48] [136500] global_step=136500, grad_norm=3.6460986137390137, loss=1.0188974142074585
I0203 10:28:42.762406 139907754342144 logging_writer.py:48] [136600] global_step=136600, grad_norm=3.489405632019043, loss=1.0170470476150513
I0203 10:29:16.493846 139907762734848 logging_writer.py:48] [136700] global_step=136700, grad_norm=3.347776412963867, loss=1.010648250579834
I0203 10:29:50.197746 139907754342144 logging_writer.py:48] [136800] global_step=136800, grad_norm=3.401578903198242, loss=1.062685489654541
I0203 10:30:23.840512 139907762734848 logging_writer.py:48] [136900] global_step=136900, grad_norm=3.1123640537261963, loss=0.9921608567237854
I0203 10:30:57.554260 139907754342144 logging_writer.py:48] [137000] global_step=137000, grad_norm=3.4404280185699463, loss=0.9337812662124634
I0203 10:31:31.201711 139907762734848 logging_writer.py:48] [137100] global_step=137100, grad_norm=3.5916428565979004, loss=1.0711594820022583
I0203 10:32:04.955115 139907754342144 logging_writer.py:48] [137200] global_step=137200, grad_norm=3.4444077014923096, loss=1.0114002227783203
I0203 10:32:38.631520 139907762734848 logging_writer.py:48] [137300] global_step=137300, grad_norm=3.7937350273132324, loss=0.9805641174316406
I0203 10:33:12.311038 139907754342144 logging_writer.py:48] [137400] global_step=137400, grad_norm=3.774108648300171, loss=0.9818467497825623
I0203 10:33:45.959601 139907762734848 logging_writer.py:48] [137500] global_step=137500, grad_norm=3.7574033737182617, loss=1.0732016563415527
I0203 10:34:19.653681 139907754342144 logging_writer.py:48] [137600] global_step=137600, grad_norm=3.604116678237915, loss=0.991451621055603
I0203 10:34:53.322325 139907762734848 logging_writer.py:48] [137700] global_step=137700, grad_norm=3.703874349594116, loss=1.0961982011795044
I0203 10:35:03.572959 140070692116288 spec.py:321] Evaluating on the training split.
I0203 10:35:10.334567 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 10:35:18.741777 140070692116288 spec.py:349] Evaluating on the test split.
I0203 10:35:21.418898 140070692116288 submission_runner.py:408] Time since start: 48078.54s, 	Step: 137732, 	{'train/accuracy': 0.8350605964660645, 'train/loss': 0.5876715183258057, 'validation/accuracy': 0.7061399817466736, 'validation/loss': 1.225954532623291, 'validation/num_examples': 50000, 'test/accuracy': 0.5824000239372253, 'test/loss': 1.9551299810409546, 'test/num_examples': 10000, 'score': 46449.772149086, 'total_duration': 48078.53934550285, 'accumulated_submission_time': 46449.772149086, 'accumulated_eval_time': 1620.2749452590942, 'accumulated_logging_time': 3.6919307708740234}
I0203 10:35:21.461080 139907712378624 logging_writer.py:48] [137732] accumulated_eval_time=1620.274945, accumulated_logging_time=3.691931, accumulated_submission_time=46449.772149, global_step=137732, preemption_count=0, score=46449.772149, test/accuracy=0.582400, test/loss=1.955130, test/num_examples=10000, total_duration=48078.539346, train/accuracy=0.835061, train/loss=0.587672, validation/accuracy=0.706140, validation/loss=1.225955, validation/num_examples=50000
I0203 10:35:44.658434 139907720771328 logging_writer.py:48] [137800] global_step=137800, grad_norm=3.5975565910339355, loss=0.9808385968208313
I0203 10:36:18.324815 139907712378624 logging_writer.py:48] [137900] global_step=137900, grad_norm=3.4554951190948486, loss=1.0086369514465332
I0203 10:36:52.018190 139907720771328 logging_writer.py:48] [138000] global_step=138000, grad_norm=3.739561080932617, loss=1.0204527378082275
I0203 10:37:25.780219 139907712378624 logging_writer.py:48] [138100] global_step=138100, grad_norm=3.3096587657928467, loss=0.9895420670509338
I0203 10:37:59.486961 139907720771328 logging_writer.py:48] [138200] global_step=138200, grad_norm=3.7700986862182617, loss=1.0204522609710693
I0203 10:38:33.156367 139907712378624 logging_writer.py:48] [138300] global_step=138300, grad_norm=3.376826286315918, loss=0.9827486276626587
I0203 10:39:06.838862 139907720771328 logging_writer.py:48] [138400] global_step=138400, grad_norm=3.861283302307129, loss=1.0790433883666992
I0203 10:39:40.592789 139907712378624 logging_writer.py:48] [138500] global_step=138500, grad_norm=3.4177329540252686, loss=0.9245419502258301
I0203 10:40:14.271993 139907720771328 logging_writer.py:48] [138600] global_step=138600, grad_norm=3.5717036724090576, loss=1.0326827764511108
I0203 10:40:47.943847 139907712378624 logging_writer.py:48] [138700] global_step=138700, grad_norm=3.8243303298950195, loss=1.0323988199234009
I0203 10:41:21.677521 139907720771328 logging_writer.py:48] [138800] global_step=138800, grad_norm=3.484489679336548, loss=0.9618321657180786
I0203 10:41:55.496110 139907712378624 logging_writer.py:48] [138900] global_step=138900, grad_norm=3.674160957336426, loss=0.9624562859535217
I0203 10:42:29.174930 139907720771328 logging_writer.py:48] [139000] global_step=139000, grad_norm=3.5671463012695312, loss=1.0360822677612305
I0203 10:43:02.939399 139907712378624 logging_writer.py:48] [139100] global_step=139100, grad_norm=3.441990613937378, loss=0.968355119228363
I0203 10:43:36.637987 139907720771328 logging_writer.py:48] [139200] global_step=139200, grad_norm=3.4400131702423096, loss=0.9888097047805786
I0203 10:43:51.583960 140070692116288 spec.py:321] Evaluating on the training split.
I0203 10:43:57.893481 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 10:44:06.454745 140070692116288 spec.py:349] Evaluating on the test split.
I0203 10:44:09.155911 140070692116288 submission_runner.py:408] Time since start: 48606.28s, 	Step: 139246, 	{'train/accuracy': 0.8834900856018066, 'train/loss': 0.4090985059738159, 'validation/accuracy': 0.7286199927330017, 'validation/loss': 1.1191855669021606, 'validation/num_examples': 50000, 'test/accuracy': 0.6080000400543213, 'test/loss': 1.8433808088302612, 'test/num_examples': 10000, 'score': 46959.832931280136, 'total_duration': 48606.27636384964, 'accumulated_submission_time': 46959.832931280136, 'accumulated_eval_time': 1637.8468651771545, 'accumulated_logging_time': 3.7433359622955322}
I0203 10:44:09.197884 139907754342144 logging_writer.py:48] [139246] accumulated_eval_time=1637.846865, accumulated_logging_time=3.743336, accumulated_submission_time=46959.832931, global_step=139246, preemption_count=0, score=46959.832931, test/accuracy=0.608000, test/loss=1.843381, test/num_examples=10000, total_duration=48606.276364, train/accuracy=0.883490, train/loss=0.409099, validation/accuracy=0.728620, validation/loss=1.119186, validation/num_examples=50000
I0203 10:44:27.737970 139907762734848 logging_writer.py:48] [139300] global_step=139300, grad_norm=3.7498533725738525, loss=1.0757379531860352
I0203 10:45:01.388103 139907754342144 logging_writer.py:48] [139400] global_step=139400, grad_norm=3.3860199451446533, loss=0.9131304621696472
I0203 10:45:35.086095 139907762734848 logging_writer.py:48] [139500] global_step=139500, grad_norm=3.4595484733581543, loss=1.0322856903076172
I0203 10:46:08.753443 139907754342144 logging_writer.py:48] [139600] global_step=139600, grad_norm=3.9324300289154053, loss=0.9475606679916382
I0203 10:46:42.459779 139907762734848 logging_writer.py:48] [139700] global_step=139700, grad_norm=3.462991952896118, loss=0.8913593888282776
I0203 10:47:16.105594 139907754342144 logging_writer.py:48] [139800] global_step=139800, grad_norm=3.68168568611145, loss=0.9361329078674316
I0203 10:47:49.766970 139907762734848 logging_writer.py:48] [139900] global_step=139900, grad_norm=3.807450532913208, loss=0.9933726787567139
I0203 10:48:23.525228 139907754342144 logging_writer.py:48] [140000] global_step=140000, grad_norm=3.6887450218200684, loss=0.946685791015625
I0203 10:48:57.258209 139907762734848 logging_writer.py:48] [140100] global_step=140100, grad_norm=3.719782590866089, loss=1.092961311340332
I0203 10:49:30.938117 139907754342144 logging_writer.py:48] [140200] global_step=140200, grad_norm=3.3304531574249268, loss=0.9142419695854187
I0203 10:50:04.632673 139907762734848 logging_writer.py:48] [140300] global_step=140300, grad_norm=3.6471190452575684, loss=0.981741189956665
I0203 10:50:38.297877 139907754342144 logging_writer.py:48] [140400] global_step=140400, grad_norm=3.506289482116699, loss=0.9433066844940186
I0203 10:51:12.009459 139907762734848 logging_writer.py:48] [140500] global_step=140500, grad_norm=3.813702344894409, loss=1.035918951034546
I0203 10:51:45.726134 139907754342144 logging_writer.py:48] [140600] global_step=140600, grad_norm=3.426478862762451, loss=0.9061292409896851
I0203 10:52:19.475422 139907762734848 logging_writer.py:48] [140700] global_step=140700, grad_norm=3.871002435684204, loss=0.9565801620483398
I0203 10:52:39.451940 140070692116288 spec.py:321] Evaluating on the training split.
I0203 10:52:45.817241 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 10:52:54.530383 140070692116288 spec.py:349] Evaluating on the test split.
I0203 10:52:57.215959 140070692116288 submission_runner.py:408] Time since start: 49134.34s, 	Step: 140761, 	{'train/accuracy': 0.8775510191917419, 'train/loss': 0.42347419261932373, 'validation/accuracy': 0.7308799624443054, 'validation/loss': 1.1201832294464111, 'validation/num_examples': 50000, 'test/accuracy': 0.6022000312805176, 'test/loss': 1.865876317024231, 'test/num_examples': 10000, 'score': 47470.02541399002, 'total_duration': 49134.336408376694, 'accumulated_submission_time': 47470.02541399002, 'accumulated_eval_time': 1655.6108510494232, 'accumulated_logging_time': 3.794053316116333}
I0203 10:52:57.257101 139907729164032 logging_writer.py:48] [140761] accumulated_eval_time=1655.610851, accumulated_logging_time=3.794053, accumulated_submission_time=47470.025414, global_step=140761, preemption_count=0, score=47470.025414, test/accuracy=0.602200, test/loss=1.865876, test/num_examples=10000, total_duration=49134.336408, train/accuracy=0.877551, train/loss=0.423474, validation/accuracy=0.730880, validation/loss=1.120183, validation/num_examples=50000
I0203 10:53:10.744101 139907737556736 logging_writer.py:48] [140800] global_step=140800, grad_norm=3.677751064300537, loss=0.8893013596534729
I0203 10:53:44.399597 139907729164032 logging_writer.py:48] [140900] global_step=140900, grad_norm=3.5229713916778564, loss=0.9417216181755066
I0203 10:54:18.086369 139907737556736 logging_writer.py:48] [141000] global_step=141000, grad_norm=3.749521493911743, loss=0.9801192283630371
I0203 10:54:51.851896 139907729164032 logging_writer.py:48] [141100] global_step=141100, grad_norm=3.529752492904663, loss=0.9588081240653992
I0203 10:55:25.601356 139907737556736 logging_writer.py:48] [141200] global_step=141200, grad_norm=3.5184433460235596, loss=0.9550386071205139
I0203 10:55:59.274616 139907729164032 logging_writer.py:48] [141300] global_step=141300, grad_norm=4.103952884674072, loss=1.0214004516601562
I0203 10:56:32.980653 139907737556736 logging_writer.py:48] [141400] global_step=141400, grad_norm=3.468510150909424, loss=0.9212930202484131
I0203 10:57:06.642517 139907729164032 logging_writer.py:48] [141500] global_step=141500, grad_norm=3.622274398803711, loss=0.9937413930892944
I0203 10:57:40.336321 139907737556736 logging_writer.py:48] [141600] global_step=141600, grad_norm=4.096658706665039, loss=1.0664668083190918
I0203 10:58:14.013814 139907729164032 logging_writer.py:48] [141700] global_step=141700, grad_norm=4.120826721191406, loss=0.9537107944488525
I0203 10:58:47.679438 139907737556736 logging_writer.py:48] [141800] global_step=141800, grad_norm=3.3685853481292725, loss=0.9537585377693176
I0203 10:59:21.355857 139907729164032 logging_writer.py:48] [141900] global_step=141900, grad_norm=3.5608913898468018, loss=0.8693883419036865
I0203 10:59:55.063136 139907737556736 logging_writer.py:48] [142000] global_step=142000, grad_norm=3.644447088241577, loss=0.9630287885665894
I0203 11:00:28.772045 139907729164032 logging_writer.py:48] [142100] global_step=142100, grad_norm=3.683171033859253, loss=1.0362253189086914
I0203 11:01:02.461493 139907737556736 logging_writer.py:48] [142200] global_step=142200, grad_norm=3.7120938301086426, loss=0.9345930218696594
I0203 11:01:27.308463 140070692116288 spec.py:321] Evaluating on the training split.
I0203 11:01:33.656862 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 11:01:42.121567 140070692116288 spec.py:349] Evaluating on the test split.
I0203 11:01:44.815203 140070692116288 submission_runner.py:408] Time since start: 49661.94s, 	Step: 142275, 	{'train/accuracy': 0.869559109210968, 'train/loss': 0.45493537187576294, 'validation/accuracy': 0.729699969291687, 'validation/loss': 1.1267502307891846, 'validation/num_examples': 50000, 'test/accuracy': 0.6037000417709351, 'test/loss': 1.8605526685714722, 'test/num_examples': 10000, 'score': 47980.01170706749, 'total_duration': 49661.93563914299, 'accumulated_submission_time': 47980.01170706749, 'accumulated_eval_time': 1673.117571592331, 'accumulated_logging_time': 3.8474504947662354}
I0203 11:01:44.860480 139907712378624 logging_writer.py:48] [142275] accumulated_eval_time=1673.117572, accumulated_logging_time=3.847450, accumulated_submission_time=47980.011707, global_step=142275, preemption_count=0, score=47980.011707, test/accuracy=0.603700, test/loss=1.860553, test/num_examples=10000, total_duration=49661.935639, train/accuracy=0.869559, train/loss=0.454935, validation/accuracy=0.729700, validation/loss=1.126750, validation/num_examples=50000
I0203 11:01:53.633358 139907720771328 logging_writer.py:48] [142300] global_step=142300, grad_norm=3.5628278255462646, loss=0.9549466371536255
I0203 11:02:27.323359 139907712378624 logging_writer.py:48] [142400] global_step=142400, grad_norm=4.002548694610596, loss=0.9690418243408203
I0203 11:03:01.019962 139907720771328 logging_writer.py:48] [142500] global_step=142500, grad_norm=3.881781578063965, loss=0.9825347065925598
I0203 11:03:34.736663 139907712378624 logging_writer.py:48] [142600] global_step=142600, grad_norm=3.6902177333831787, loss=0.9640563726425171
I0203 11:04:08.384029 139907720771328 logging_writer.py:48] [142700] global_step=142700, grad_norm=3.271087169647217, loss=0.9074257612228394
I0203 11:04:42.064710 139907712378624 logging_writer.py:48] [142800] global_step=142800, grad_norm=3.9040980339050293, loss=1.039284348487854
I0203 11:05:15.727141 139907720771328 logging_writer.py:48] [142900] global_step=142900, grad_norm=4.014997482299805, loss=0.9581910371780396
I0203 11:05:49.761785 139907712378624 logging_writer.py:48] [143000] global_step=143000, grad_norm=3.5317258834838867, loss=0.9578893780708313
I0203 11:06:23.418185 139907720771328 logging_writer.py:48] [143100] global_step=143100, grad_norm=4.275051116943359, loss=1.0056142807006836
I0203 11:06:57.148766 139907712378624 logging_writer.py:48] [143200] global_step=143200, grad_norm=4.3504791259765625, loss=0.9233797788619995
I0203 11:07:30.834706 139907720771328 logging_writer.py:48] [143300] global_step=143300, grad_norm=3.7354986667633057, loss=1.0397744178771973
I0203 11:08:04.712921 139907712378624 logging_writer.py:48] [143400] global_step=143400, grad_norm=3.828122615814209, loss=0.9778751730918884
I0203 11:08:38.449432 139907720771328 logging_writer.py:48] [143500] global_step=143500, grad_norm=3.5873093605041504, loss=0.9035404920578003
I0203 11:09:12.148819 139907712378624 logging_writer.py:48] [143600] global_step=143600, grad_norm=4.280659198760986, loss=0.8986491560935974
I0203 11:09:45.804243 139907720771328 logging_writer.py:48] [143700] global_step=143700, grad_norm=3.673248767852783, loss=0.8263642191886902
I0203 11:10:14.892784 140070692116288 spec.py:321] Evaluating on the training split.
I0203 11:10:21.192157 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 11:10:29.677163 140070692116288 spec.py:349] Evaluating on the test split.
I0203 11:10:32.366498 140070692116288 submission_runner.py:408] Time since start: 50189.49s, 	Step: 143788, 	{'train/accuracy': 0.8769331574440002, 'train/loss': 0.42777085304260254, 'validation/accuracy': 0.7282199859619141, 'validation/loss': 1.118950605392456, 'validation/num_examples': 50000, 'test/accuracy': 0.6041000485420227, 'test/loss': 1.8592493534088135, 'test/num_examples': 10000, 'score': 48489.98080587387, 'total_duration': 50189.48695039749, 'accumulated_submission_time': 48489.98080587387, 'accumulated_eval_time': 1690.5912556648254, 'accumulated_logging_time': 3.9025094509124756}
I0203 11:10:32.408899 139907703985920 logging_writer.py:48] [143788] accumulated_eval_time=1690.591256, accumulated_logging_time=3.902509, accumulated_submission_time=48489.980806, global_step=143788, preemption_count=0, score=48489.980806, test/accuracy=0.604100, test/loss=1.859249, test/num_examples=10000, total_duration=50189.486950, train/accuracy=0.876933, train/loss=0.427771, validation/accuracy=0.728220, validation/loss=1.118951, validation/num_examples=50000
I0203 11:10:36.811634 139907745949440 logging_writer.py:48] [143800] global_step=143800, grad_norm=4.578250408172607, loss=1.064629316329956
I0203 11:11:10.466471 139907703985920 logging_writer.py:48] [143900] global_step=143900, grad_norm=3.5196173191070557, loss=0.9310656189918518
I0203 11:11:44.199828 139907745949440 logging_writer.py:48] [144000] global_step=144000, grad_norm=3.6600189208984375, loss=0.925360918045044
I0203 11:12:17.904670 139907703985920 logging_writer.py:48] [144100] global_step=144100, grad_norm=3.862196922302246, loss=0.9503002166748047
I0203 11:12:51.570911 139907745949440 logging_writer.py:48] [144200] global_step=144200, grad_norm=3.597468852996826, loss=0.9141729474067688
I0203 11:13:25.285856 139907703985920 logging_writer.py:48] [144300] global_step=144300, grad_norm=3.8431897163391113, loss=0.905748188495636
I0203 11:13:59.012075 139907745949440 logging_writer.py:48] [144400] global_step=144400, grad_norm=3.7463786602020264, loss=0.985783576965332
I0203 11:14:32.825979 139907703985920 logging_writer.py:48] [144500] global_step=144500, grad_norm=3.8163435459136963, loss=0.8749681115150452
I0203 11:15:06.506772 139907745949440 logging_writer.py:48] [144600] global_step=144600, grad_norm=3.8496758937835693, loss=0.9585552215576172
I0203 11:15:40.191994 139907703985920 logging_writer.py:48] [144700] global_step=144700, grad_norm=3.5000102519989014, loss=0.9043115973472595
I0203 11:16:13.877273 139907745949440 logging_writer.py:48] [144800] global_step=144800, grad_norm=3.894853115081787, loss=0.8631749153137207
I0203 11:16:47.666360 139907703985920 logging_writer.py:48] [144900] global_step=144900, grad_norm=3.5306525230407715, loss=0.8597249984741211
I0203 11:17:21.333766 139907745949440 logging_writer.py:48] [145000] global_step=145000, grad_norm=3.709568500518799, loss=0.8988885283470154
I0203 11:17:55.015984 139907703985920 logging_writer.py:48] [145100] global_step=145100, grad_norm=3.80265474319458, loss=0.9002363681793213
I0203 11:18:28.684470 139907745949440 logging_writer.py:48] [145200] global_step=145200, grad_norm=3.8254384994506836, loss=0.9706839323043823
I0203 11:19:02.448119 139907703985920 logging_writer.py:48] [145300] global_step=145300, grad_norm=3.757983446121216, loss=0.9461943507194519
I0203 11:19:02.458382 140070692116288 spec.py:321] Evaluating on the training split.
I0203 11:19:08.828356 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 11:19:17.230741 140070692116288 spec.py:349] Evaluating on the test split.
I0203 11:19:19.926009 140070692116288 submission_runner.py:408] Time since start: 50717.05s, 	Step: 145301, 	{'train/accuracy': 0.8763153553009033, 'train/loss': 0.42836129665374756, 'validation/accuracy': 0.7317599654197693, 'validation/loss': 1.1129244565963745, 'validation/num_examples': 50000, 'test/accuracy': 0.6092000007629395, 'test/loss': 1.8546922206878662, 'test/num_examples': 10000, 'score': 48999.96798682213, 'total_duration': 50717.04645681381, 'accumulated_submission_time': 48999.96798682213, 'accumulated_eval_time': 1708.0588409900665, 'accumulated_logging_time': 3.9540607929229736}
I0203 11:19:19.970149 139907729164032 logging_writer.py:48] [145301] accumulated_eval_time=1708.058841, accumulated_logging_time=3.954061, accumulated_submission_time=48999.967987, global_step=145301, preemption_count=0, score=48999.967987, test/accuracy=0.609200, test/loss=1.854692, test/num_examples=10000, total_duration=50717.046457, train/accuracy=0.876315, train/loss=0.428361, validation/accuracy=0.731760, validation/loss=1.112924, validation/num_examples=50000
I0203 11:19:53.685384 139907737556736 logging_writer.py:48] [145400] global_step=145400, grad_norm=3.9220755100250244, loss=0.8303003311157227
I0203 11:20:27.401528 139907729164032 logging_writer.py:48] [145500] global_step=145500, grad_norm=3.678704261779785, loss=0.9447482228279114
I0203 11:21:01.159538 139907737556736 logging_writer.py:48] [145600] global_step=145600, grad_norm=3.9543237686157227, loss=0.9472067952156067
I0203 11:21:34.904219 139907729164032 logging_writer.py:48] [145700] global_step=145700, grad_norm=3.890251874923706, loss=0.949634850025177
I0203 11:22:08.579938 139907737556736 logging_writer.py:48] [145800] global_step=145800, grad_norm=3.6042134761810303, loss=0.9249010682106018
I0203 11:22:42.264640 139907729164032 logging_writer.py:48] [145900] global_step=145900, grad_norm=3.8145594596862793, loss=0.8714326024055481
I0203 11:23:15.944706 139907737556736 logging_writer.py:48] [146000] global_step=146000, grad_norm=4.287434101104736, loss=1.0639262199401855
I0203 11:23:49.638701 139907729164032 logging_writer.py:48] [146100] global_step=146100, grad_norm=4.305354595184326, loss=0.9951715469360352
I0203 11:24:23.303714 139907737556736 logging_writer.py:48] [146200] global_step=146200, grad_norm=3.8979148864746094, loss=0.9094046950340271
I0203 11:24:57.006307 139907729164032 logging_writer.py:48] [146300] global_step=146300, grad_norm=4.1377787590026855, loss=0.9496101140975952
I0203 11:25:30.676175 139907737556736 logging_writer.py:48] [146400] global_step=146400, grad_norm=4.155674934387207, loss=0.9095925092697144
I0203 11:26:04.367672 139907729164032 logging_writer.py:48] [146500] global_step=146500, grad_norm=4.109469413757324, loss=0.998680830001831
I0203 11:26:38.088225 139907737556736 logging_writer.py:48] [146600] global_step=146600, grad_norm=3.72885799407959, loss=0.8228423595428467
I0203 11:27:11.817278 139907729164032 logging_writer.py:48] [146700] global_step=146700, grad_norm=3.9295406341552734, loss=0.8863263130187988
I0203 11:27:45.614434 139907737556736 logging_writer.py:48] [146800] global_step=146800, grad_norm=4.148760795593262, loss=1.0017235279083252
I0203 11:27:50.148206 140070692116288 spec.py:321] Evaluating on the training split.
I0203 11:27:56.476319 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 11:28:04.950305 140070692116288 spec.py:349] Evaluating on the test split.
I0203 11:28:07.637732 140070692116288 submission_runner.py:408] Time since start: 51244.76s, 	Step: 146815, 	{'train/accuracy': 0.8947902917861938, 'train/loss': 0.3664727807044983, 'validation/accuracy': 0.7371000051498413, 'validation/loss': 1.0927094221115112, 'validation/num_examples': 50000, 'test/accuracy': 0.6107000112533569, 'test/loss': 1.8342806100845337, 'test/num_examples': 10000, 'score': 49510.08322453499, 'total_duration': 51244.758184194565, 'accumulated_submission_time': 49510.08322453499, 'accumulated_eval_time': 1725.5483317375183, 'accumulated_logging_time': 4.008327007293701}
I0203 11:28:07.679193 139907712378624 logging_writer.py:48] [146815] accumulated_eval_time=1725.548332, accumulated_logging_time=4.008327, accumulated_submission_time=49510.083225, global_step=146815, preemption_count=0, score=49510.083225, test/accuracy=0.610700, test/loss=1.834281, test/num_examples=10000, total_duration=51244.758184, train/accuracy=0.894790, train/loss=0.366473, validation/accuracy=0.737100, validation/loss=1.092709, validation/num_examples=50000
I0203 11:28:36.612843 139907720771328 logging_writer.py:48] [146900] global_step=146900, grad_norm=3.9360580444335938, loss=0.967224657535553
I0203 11:29:10.299515 139907712378624 logging_writer.py:48] [147000] global_step=147000, grad_norm=3.600454807281494, loss=0.8119663000106812
I0203 11:29:43.963941 139907720771328 logging_writer.py:48] [147100] global_step=147100, grad_norm=3.7332818508148193, loss=0.8813072443008423
I0203 11:30:17.747379 139907712378624 logging_writer.py:48] [147200] global_step=147200, grad_norm=3.7157647609710693, loss=0.9038644433021545
I0203 11:30:51.439397 139907720771328 logging_writer.py:48] [147300] global_step=147300, grad_norm=4.027201175689697, loss=0.9183648824691772
I0203 11:31:25.142278 139907712378624 logging_writer.py:48] [147400] global_step=147400, grad_norm=3.9605414867401123, loss=0.9145517349243164
I0203 11:31:58.846342 139907720771328 logging_writer.py:48] [147500] global_step=147500, grad_norm=4.029736042022705, loss=0.9511754512786865
I0203 11:32:32.557384 139907712378624 logging_writer.py:48] [147600] global_step=147600, grad_norm=3.9202611446380615, loss=0.9363356828689575
I0203 11:33:06.249923 139907720771328 logging_writer.py:48] [147700] global_step=147700, grad_norm=4.142121315002441, loss=0.9111254811286926
I0203 11:33:39.955765 139907712378624 logging_writer.py:48] [147800] global_step=147800, grad_norm=3.905526638031006, loss=0.8545354604721069
I0203 11:34:13.753694 139907720771328 logging_writer.py:48] [147900] global_step=147900, grad_norm=3.787864923477173, loss=0.8754556179046631
I0203 11:34:47.451295 139907712378624 logging_writer.py:48] [148000] global_step=148000, grad_norm=3.9813308715820312, loss=0.9420929551124573
I0203 11:35:21.125274 139907720771328 logging_writer.py:48] [148100] global_step=148100, grad_norm=4.189950466156006, loss=0.947425127029419
I0203 11:35:54.797246 139907712378624 logging_writer.py:48] [148200] global_step=148200, grad_norm=3.95247220993042, loss=0.8545438051223755
I0203 11:36:28.452058 139907720771328 logging_writer.py:48] [148300] global_step=148300, grad_norm=4.323169231414795, loss=0.8873566389083862
I0203 11:36:37.725401 140070692116288 spec.py:321] Evaluating on the training split.
I0203 11:36:44.221517 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 11:36:52.862853 140070692116288 spec.py:349] Evaluating on the test split.
I0203 11:36:55.487900 140070692116288 submission_runner.py:408] Time since start: 51772.61s, 	Step: 148329, 	{'train/accuracy': 0.9049345850944519, 'train/loss': 0.327584832906723, 'validation/accuracy': 0.7351999878883362, 'validation/loss': 1.1068998575210571, 'validation/num_examples': 50000, 'test/accuracy': 0.6067000031471252, 'test/loss': 1.8497717380523682, 'test/num_examples': 10000, 'score': 50020.06638741493, 'total_duration': 51772.60830807686, 'accumulated_submission_time': 50020.06638741493, 'accumulated_eval_time': 1743.310753107071, 'accumulated_logging_time': 4.059577941894531}
I0203 11:36:55.533487 139907737556736 logging_writer.py:48] [148329] accumulated_eval_time=1743.310753, accumulated_logging_time=4.059578, accumulated_submission_time=50020.066387, global_step=148329, preemption_count=0, score=50020.066387, test/accuracy=0.606700, test/loss=1.849772, test/num_examples=10000, total_duration=51772.608308, train/accuracy=0.904935, train/loss=0.327585, validation/accuracy=0.735200, validation/loss=1.106900, validation/num_examples=50000
I0203 11:37:19.797625 139907745949440 logging_writer.py:48] [148400] global_step=148400, grad_norm=4.015533924102783, loss=0.9468031525611877
I0203 11:37:53.465368 139907737556736 logging_writer.py:48] [148500] global_step=148500, grad_norm=3.7272019386291504, loss=0.8299131989479065
I0203 11:38:27.171686 139907745949440 logging_writer.py:48] [148600] global_step=148600, grad_norm=4.078368186950684, loss=0.883346438407898
I0203 11:39:00.841932 139907737556736 logging_writer.py:48] [148700] global_step=148700, grad_norm=3.84470796585083, loss=0.8400440812110901
I0203 11:39:34.501479 139907745949440 logging_writer.py:48] [148800] global_step=148800, grad_norm=4.1934380531311035, loss=0.8803427219390869
I0203 11:40:08.253850 139907737556736 logging_writer.py:48] [148900] global_step=148900, grad_norm=3.93681001663208, loss=0.9409143328666687
I0203 11:40:42.090336 139907745949440 logging_writer.py:48] [149000] global_step=149000, grad_norm=4.166874408721924, loss=0.8907723426818848
I0203 11:41:15.790594 139907737556736 logging_writer.py:48] [149100] global_step=149100, grad_norm=4.113682270050049, loss=0.893176794052124
I0203 11:41:49.479549 139907745949440 logging_writer.py:48] [149200] global_step=149200, grad_norm=3.9339706897735596, loss=0.9086999297142029
I0203 11:42:23.165243 139907737556736 logging_writer.py:48] [149300] global_step=149300, grad_norm=3.7639288902282715, loss=0.937637209892273
I0203 11:42:56.847286 139907745949440 logging_writer.py:48] [149400] global_step=149400, grad_norm=3.9096803665161133, loss=0.8768290281295776
I0203 11:43:30.551726 139907737556736 logging_writer.py:48] [149500] global_step=149500, grad_norm=4.144906044006348, loss=0.9266810417175293
I0203 11:44:04.276972 139907745949440 logging_writer.py:48] [149600] global_step=149600, grad_norm=3.7317452430725098, loss=0.8179062604904175
I0203 11:44:37.962503 139907737556736 logging_writer.py:48] [149700] global_step=149700, grad_norm=4.24489688873291, loss=0.8837777376174927
I0203 11:45:11.698371 139907745949440 logging_writer.py:48] [149800] global_step=149800, grad_norm=4.26938533782959, loss=0.9148557186126709
I0203 11:45:25.649346 140070692116288 spec.py:321] Evaluating on the training split.
I0203 11:45:31.965830 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 11:45:40.324641 140070692116288 spec.py:349] Evaluating on the test split.
I0203 11:45:42.999659 140070692116288 submission_runner.py:408] Time since start: 52300.12s, 	Step: 149843, 	{'train/accuracy': 0.9015664458274841, 'train/loss': 0.34114494919776917, 'validation/accuracy': 0.7376599907875061, 'validation/loss': 1.0968347787857056, 'validation/num_examples': 50000, 'test/accuracy': 0.6149000525474548, 'test/loss': 1.8277121782302856, 'test/num_examples': 10000, 'score': 50530.12015199661, 'total_duration': 52300.12011003494, 'accumulated_submission_time': 50530.12015199661, 'accumulated_eval_time': 1760.661033153534, 'accumulated_logging_time': 4.114696264266968}
I0203 11:45:43.043534 139907712378624 logging_writer.py:48] [149843] accumulated_eval_time=1760.661033, accumulated_logging_time=4.114696, accumulated_submission_time=50530.120152, global_step=149843, preemption_count=0, score=50530.120152, test/accuracy=0.614900, test/loss=1.827712, test/num_examples=10000, total_duration=52300.120110, train/accuracy=0.901566, train/loss=0.341145, validation/accuracy=0.737660, validation/loss=1.096835, validation/num_examples=50000
I0203 11:46:02.614662 139907720771328 logging_writer.py:48] [149900] global_step=149900, grad_norm=3.926185131072998, loss=0.9040629267692566
I0203 11:46:36.299698 139907712378624 logging_writer.py:48] [150000] global_step=150000, grad_norm=3.7189152240753174, loss=0.8657791614532471
I0203 11:47:10.086461 139907720771328 logging_writer.py:48] [150100] global_step=150100, grad_norm=4.23726224899292, loss=0.7999842166900635
I0203 11:47:43.802468 139907712378624 logging_writer.py:48] [150200] global_step=150200, grad_norm=4.046444892883301, loss=0.9556044340133667
I0203 11:48:17.479808 139907720771328 logging_writer.py:48] [150300] global_step=150300, grad_norm=3.905587673187256, loss=0.8638589382171631
I0203 11:48:51.130943 139907712378624 logging_writer.py:48] [150400] global_step=150400, grad_norm=4.298814296722412, loss=0.8816323280334473
I0203 11:49:24.826549 139907720771328 logging_writer.py:48] [150500] global_step=150500, grad_norm=4.328543186187744, loss=0.9470316171646118
I0203 11:49:58.501634 139907712378624 logging_writer.py:48] [150600] global_step=150600, grad_norm=4.580611228942871, loss=0.8502340912818909
I0203 11:50:32.184862 139907720771328 logging_writer.py:48] [150700] global_step=150700, grad_norm=4.474338531494141, loss=0.9604027271270752
I0203 11:51:05.832637 139907712378624 logging_writer.py:48] [150800] global_step=150800, grad_norm=3.9356915950775146, loss=0.8202892541885376
I0203 11:51:39.552797 139907720771328 logging_writer.py:48] [150900] global_step=150900, grad_norm=4.0386199951171875, loss=0.8951176404953003
I0203 11:52:13.221616 139907712378624 logging_writer.py:48] [151000] global_step=151000, grad_norm=4.17975378036499, loss=0.7965307235717773
I0203 11:52:46.886220 139907720771328 logging_writer.py:48] [151100] global_step=151100, grad_norm=3.838300943374634, loss=0.850621223449707
I0203 11:53:20.603032 139907712378624 logging_writer.py:48] [151200] global_step=151200, grad_norm=4.352757930755615, loss=0.929835319519043
I0203 11:53:54.415295 139907720771328 logging_writer.py:48] [151300] global_step=151300, grad_norm=4.2779459953308105, loss=0.9237401485443115
I0203 11:54:13.111049 140070692116288 spec.py:321] Evaluating on the training split.
I0203 11:54:19.462025 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 11:54:28.155314 140070692116288 spec.py:349] Evaluating on the test split.
I0203 11:54:30.880203 140070692116288 submission_runner.py:408] Time since start: 52828.00s, 	Step: 151357, 	{'train/accuracy': 0.9032804369926453, 'train/loss': 0.33565738797187805, 'validation/accuracy': 0.7389000058174133, 'validation/loss': 1.0878074169158936, 'validation/num_examples': 50000, 'test/accuracy': 0.6163000464439392, 'test/loss': 1.8217467069625854, 'test/num_examples': 10000, 'score': 51040.12117242813, 'total_duration': 52828.000644207, 'accumulated_submission_time': 51040.12117242813, 'accumulated_eval_time': 1778.4301431179047, 'accumulated_logging_time': 4.172338247299194}
I0203 11:54:30.927352 139907745949440 logging_writer.py:48] [151357] accumulated_eval_time=1778.430143, accumulated_logging_time=4.172338, accumulated_submission_time=51040.121172, global_step=151357, preemption_count=0, score=51040.121172, test/accuracy=0.616300, test/loss=1.821747, test/num_examples=10000, total_duration=52828.000644, train/accuracy=0.903280, train/loss=0.335657, validation/accuracy=0.738900, validation/loss=1.087807, validation/num_examples=50000
I0203 11:54:45.728801 139907754342144 logging_writer.py:48] [151400] global_step=151400, grad_norm=4.093933582305908, loss=0.8625839352607727
I0203 11:55:19.372709 139907745949440 logging_writer.py:48] [151500] global_step=151500, grad_norm=3.861008405685425, loss=0.9078768491744995
I0203 11:55:53.047711 139907754342144 logging_writer.py:48] [151600] global_step=151600, grad_norm=4.231354236602783, loss=0.8510453701019287
I0203 11:56:26.738497 139907745949440 logging_writer.py:48] [151700] global_step=151700, grad_norm=4.161385536193848, loss=0.8717163801193237
I0203 11:57:00.422382 139907754342144 logging_writer.py:48] [151800] global_step=151800, grad_norm=4.1558518409729, loss=0.8703711628913879
I0203 11:57:34.114141 139907745949440 logging_writer.py:48] [151900] global_step=151900, grad_norm=3.8249080181121826, loss=0.7788958549499512
I0203 11:58:07.776610 139907754342144 logging_writer.py:48] [152000] global_step=152000, grad_norm=3.7677290439605713, loss=0.8434960842132568
I0203 11:58:41.437925 139907745949440 logging_writer.py:48] [152100] global_step=152100, grad_norm=4.046047687530518, loss=0.8239368796348572
I0203 11:59:15.111202 139907754342144 logging_writer.py:48] [152200] global_step=152200, grad_norm=3.883986711502075, loss=0.8218497037887573
I0203 11:59:48.794869 139907745949440 logging_writer.py:48] [152300] global_step=152300, grad_norm=3.782464027404785, loss=0.7540289759635925
I0203 12:00:22.574031 139907754342144 logging_writer.py:48] [152400] global_step=152400, grad_norm=4.273960590362549, loss=0.9772563576698303
I0203 12:00:56.241212 139907745949440 logging_writer.py:48] [152500] global_step=152500, grad_norm=4.002087593078613, loss=0.7955270409584045
I0203 12:01:29.912551 139907754342144 logging_writer.py:48] [152600] global_step=152600, grad_norm=4.123477935791016, loss=0.8566287159919739
I0203 12:02:03.640998 139907745949440 logging_writer.py:48] [152700] global_step=152700, grad_norm=3.8224375247955322, loss=0.7864649891853333
I0203 12:02:37.336465 139907754342144 logging_writer.py:48] [152800] global_step=152800, grad_norm=3.917351007461548, loss=0.8588026165962219
I0203 12:03:01.055009 140070692116288 spec.py:321] Evaluating on the training split.
I0203 12:03:07.599426 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 12:03:15.989355 140070692116288 spec.py:349] Evaluating on the test split.
I0203 12:03:18.629464 140070692116288 submission_runner.py:408] Time since start: 53355.75s, 	Step: 152872, 	{'train/accuracy': 0.9049944281578064, 'train/loss': 0.3283750116825104, 'validation/accuracy': 0.7387199997901917, 'validation/loss': 1.1011860370635986, 'validation/num_examples': 50000, 'test/accuracy': 0.6160000562667847, 'test/loss': 1.8262497186660767, 'test/num_examples': 10000, 'score': 51550.185572862625, 'total_duration': 53355.749900341034, 'accumulated_submission_time': 51550.185572862625, 'accumulated_eval_time': 1796.0045523643494, 'accumulated_logging_time': 4.2290143966674805}
I0203 12:03:18.677070 139907720771328 logging_writer.py:48] [152872] accumulated_eval_time=1796.004552, accumulated_logging_time=4.229014, accumulated_submission_time=51550.185573, global_step=152872, preemption_count=0, score=51550.185573, test/accuracy=0.616000, test/loss=1.826250, test/num_examples=10000, total_duration=53355.749900, train/accuracy=0.904994, train/loss=0.328375, validation/accuracy=0.738720, validation/loss=1.101186, validation/num_examples=50000
I0203 12:03:28.458034 139907729164032 logging_writer.py:48] [152900] global_step=152900, grad_norm=4.2068257331848145, loss=0.8784219622612
I0203 12:04:02.133332 139907720771328 logging_writer.py:48] [153000] global_step=153000, grad_norm=4.034521579742432, loss=0.8113125562667847
I0203 12:04:35.798543 139907729164032 logging_writer.py:48] [153100] global_step=153100, grad_norm=4.553977966308594, loss=0.8085228800773621
I0203 12:05:09.503706 139907720771328 logging_writer.py:48] [153200] global_step=153200, grad_norm=4.132776260375977, loss=0.9227164387702942
I0203 12:05:43.143275 139907729164032 logging_writer.py:48] [153300] global_step=153300, grad_norm=4.120934963226318, loss=0.8288305401802063
I0203 12:06:16.874725 139907720771328 logging_writer.py:48] [153400] global_step=153400, grad_norm=4.128005504608154, loss=0.841475248336792
I0203 12:06:50.647299 139907729164032 logging_writer.py:48] [153500] global_step=153500, grad_norm=4.362801551818848, loss=0.8306211829185486
I0203 12:07:24.394515 139907720771328 logging_writer.py:48] [153600] global_step=153600, grad_norm=4.391571521759033, loss=0.8594323396682739
I0203 12:07:58.058921 139907729164032 logging_writer.py:48] [153700] global_step=153700, grad_norm=4.132299423217773, loss=0.859136700630188
I0203 12:08:31.768260 139907720771328 logging_writer.py:48] [153800] global_step=153800, grad_norm=4.3152265548706055, loss=0.8154377937316895
I0203 12:09:05.413131 139907729164032 logging_writer.py:48] [153900] global_step=153900, grad_norm=4.294800281524658, loss=0.8373036980628967
I0203 12:09:39.117959 139907720771328 logging_writer.py:48] [154000] global_step=154000, grad_norm=4.349469184875488, loss=0.9068807363510132
I0203 12:10:12.836426 139907729164032 logging_writer.py:48] [154100] global_step=154100, grad_norm=4.114431381225586, loss=0.8663985729217529
I0203 12:10:46.556380 139907720771328 logging_writer.py:48] [154200] global_step=154200, grad_norm=4.385070323944092, loss=0.8950396776199341
I0203 12:11:20.243552 139907729164032 logging_writer.py:48] [154300] global_step=154300, grad_norm=3.9112675189971924, loss=0.7638825178146362
I0203 12:11:48.769487 140070692116288 spec.py:321] Evaluating on the training split.
I0203 12:11:55.099183 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 12:12:03.925503 140070692116288 spec.py:349] Evaluating on the test split.
I0203 12:12:06.597756 140070692116288 submission_runner.py:408] Time since start: 53883.72s, 	Step: 154386, 	{'train/accuracy': 0.9026825428009033, 'train/loss': 0.33135420083999634, 'validation/accuracy': 0.7376799583435059, 'validation/loss': 1.1047000885009766, 'validation/num_examples': 50000, 'test/accuracy': 0.616100013256073, 'test/loss': 1.8450510501861572, 'test/num_examples': 10000, 'score': 52060.21515059471, 'total_duration': 53883.71820926666, 'accumulated_submission_time': 52060.21515059471, 'accumulated_eval_time': 1813.8327918052673, 'accumulated_logging_time': 4.287072420120239}
I0203 12:12:06.641946 139907712378624 logging_writer.py:48] [154386] accumulated_eval_time=1813.832792, accumulated_logging_time=4.287072, accumulated_submission_time=52060.215151, global_step=154386, preemption_count=0, score=52060.215151, test/accuracy=0.616100, test/loss=1.845051, test/num_examples=10000, total_duration=53883.718209, train/accuracy=0.902683, train/loss=0.331354, validation/accuracy=0.737680, validation/loss=1.104700, validation/num_examples=50000
I0203 12:12:11.697107 139907745949440 logging_writer.py:48] [154400] global_step=154400, grad_norm=4.286769866943359, loss=0.8964617848396301
I0203 12:12:45.419822 139907712378624 logging_writer.py:48] [154500] global_step=154500, grad_norm=4.003208160400391, loss=0.8296359777450562
I0203 12:13:19.124432 139907745949440 logging_writer.py:48] [154600] global_step=154600, grad_norm=3.9098572731018066, loss=0.7644913196563721
I0203 12:13:52.985171 139907712378624 logging_writer.py:48] [154700] global_step=154700, grad_norm=3.8056039810180664, loss=0.8044285774230957
I0203 12:14:26.665728 139907745949440 logging_writer.py:48] [154800] global_step=154800, grad_norm=4.452834606170654, loss=0.7689949870109558
I0203 12:15:00.349629 139907712378624 logging_writer.py:48] [154900] global_step=154900, grad_norm=3.7235748767852783, loss=0.726767361164093
I0203 12:15:34.025612 139907745949440 logging_writer.py:48] [155000] global_step=155000, grad_norm=4.247977256774902, loss=0.8069887161254883
I0203 12:16:07.785957 139907712378624 logging_writer.py:48] [155100] global_step=155100, grad_norm=4.270483493804932, loss=0.843093991279602
I0203 12:16:41.482669 139907745949440 logging_writer.py:48] [155200] global_step=155200, grad_norm=4.135775089263916, loss=0.7245764136314392
I0203 12:17:15.165471 139907712378624 logging_writer.py:48] [155300] global_step=155300, grad_norm=4.476739883422852, loss=0.8227972388267517
I0203 12:17:48.839219 139907745949440 logging_writer.py:48] [155400] global_step=155400, grad_norm=3.9355955123901367, loss=0.7937515377998352
I0203 12:18:22.526225 139907712378624 logging_writer.py:48] [155500] global_step=155500, grad_norm=4.342262268066406, loss=0.7839640974998474
I0203 12:18:56.205364 139907745949440 logging_writer.py:48] [155600] global_step=155600, grad_norm=4.14445161819458, loss=0.8274041414260864
I0203 12:19:29.895541 139907712378624 logging_writer.py:48] [155700] global_step=155700, grad_norm=4.284239768981934, loss=0.8088934421539307
I0203 12:20:03.655495 139907745949440 logging_writer.py:48] [155800] global_step=155800, grad_norm=3.8808934688568115, loss=0.7717025279998779
I0203 12:20:36.846335 140070692116288 spec.py:321] Evaluating on the training split.
I0203 12:20:43.106112 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 12:20:51.614951 140070692116288 spec.py:349] Evaluating on the test split.
I0203 12:20:54.292503 140070692116288 submission_runner.py:408] Time since start: 54411.41s, 	Step: 155900, 	{'train/accuracy': 0.9301259517669678, 'train/loss': 0.25026360154151917, 'validation/accuracy': 0.7440800070762634, 'validation/loss': 1.0782766342163086, 'validation/num_examples': 50000, 'test/accuracy': 0.6147000193595886, 'test/loss': 1.8321329355239868, 'test/num_examples': 10000, 'score': 52570.35527801514, 'total_duration': 54411.412940740585, 'accumulated_submission_time': 52570.35527801514, 'accumulated_eval_time': 1831.2789142131805, 'accumulated_logging_time': 4.342725038528442}
I0203 12:20:54.335468 139907729164032 logging_writer.py:48] [155900] accumulated_eval_time=1831.278914, accumulated_logging_time=4.342725, accumulated_submission_time=52570.355278, global_step=155900, preemption_count=0, score=52570.355278, test/accuracy=0.614700, test/loss=1.832133, test/num_examples=10000, total_duration=54411.412941, train/accuracy=0.930126, train/loss=0.250264, validation/accuracy=0.744080, validation/loss=1.078277, validation/num_examples=50000
I0203 12:20:54.681077 139907737556736 logging_writer.py:48] [155900] global_step=155900, grad_norm=4.053395748138428, loss=0.7797964811325073
I0203 12:21:28.304054 139907729164032 logging_writer.py:48] [156000] global_step=156000, grad_norm=4.526456356048584, loss=0.8415709733963013
I0203 12:22:02.006148 139907737556736 logging_writer.py:48] [156100] global_step=156100, grad_norm=4.06370210647583, loss=0.7513300776481628
I0203 12:22:35.769214 139907729164032 logging_writer.py:48] [156200] global_step=156200, grad_norm=3.7837328910827637, loss=0.7210553884506226
I0203 12:23:09.484127 139907737556736 logging_writer.py:48] [156300] global_step=156300, grad_norm=3.9890379905700684, loss=0.7742171883583069
I0203 12:23:43.151107 139907729164032 logging_writer.py:48] [156400] global_step=156400, grad_norm=4.098067283630371, loss=0.878181517124176
I0203 12:24:16.847404 139907737556736 logging_writer.py:48] [156500] global_step=156500, grad_norm=4.33341646194458, loss=0.6814491748809814
I0203 12:24:50.525508 139907729164032 logging_writer.py:48] [156600] global_step=156600, grad_norm=4.449696063995361, loss=0.9027397632598877
I0203 12:25:24.248376 139907737556736 logging_writer.py:48] [156700] global_step=156700, grad_norm=4.173336505889893, loss=0.8223706483840942
I0203 12:25:57.925390 139907729164032 logging_writer.py:48] [156800] global_step=156800, grad_norm=4.149615287780762, loss=0.7804185152053833
I0203 12:26:31.727092 139907737556736 logging_writer.py:48] [156900] global_step=156900, grad_norm=4.243556022644043, loss=0.846845805644989
I0203 12:27:05.417562 139907729164032 logging_writer.py:48] [157000] global_step=157000, grad_norm=4.544960975646973, loss=0.845257043838501
I0203 12:27:39.117239 139907737556736 logging_writer.py:48] [157100] global_step=157100, grad_norm=4.378320217132568, loss=0.8042757511138916
I0203 12:28:12.808926 139907729164032 logging_writer.py:48] [157200] global_step=157200, grad_norm=4.288869857788086, loss=0.7919262647628784
I0203 12:28:46.561960 139907737556736 logging_writer.py:48] [157300] global_step=157300, grad_norm=4.208771705627441, loss=0.767394483089447
I0203 12:29:20.237008 139907729164032 logging_writer.py:48] [157400] global_step=157400, grad_norm=4.049524307250977, loss=0.8226814866065979
I0203 12:29:24.433292 140070692116288 spec.py:321] Evaluating on the training split.
I0203 12:29:30.768023 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 12:29:39.371214 140070692116288 spec.py:349] Evaluating on the test split.
I0203 12:29:42.051970 140070692116288 submission_runner.py:408] Time since start: 54939.17s, 	Step: 157414, 	{'train/accuracy': 0.9299465417861938, 'train/loss': 0.24397999048233032, 'validation/accuracy': 0.742579996585846, 'validation/loss': 1.0852277278900146, 'validation/num_examples': 50000, 'test/accuracy': 0.6175000071525574, 'test/loss': 1.8458549976348877, 'test/num_examples': 10000, 'score': 53080.38762998581, 'total_duration': 54939.17242026329, 'accumulated_submission_time': 53080.38762998581, 'accumulated_eval_time': 1848.8975548744202, 'accumulated_logging_time': 4.397834777832031}
I0203 12:29:42.097425 139907720771328 logging_writer.py:48] [157414] accumulated_eval_time=1848.897555, accumulated_logging_time=4.397835, accumulated_submission_time=53080.387630, global_step=157414, preemption_count=0, score=53080.387630, test/accuracy=0.617500, test/loss=1.845855, test/num_examples=10000, total_duration=54939.172420, train/accuracy=0.929947, train/loss=0.243980, validation/accuracy=0.742580, validation/loss=1.085228, validation/num_examples=50000
I0203 12:30:11.377349 139907745949440 logging_writer.py:48] [157500] global_step=157500, grad_norm=4.602638244628906, loss=0.807027280330658
I0203 12:30:45.077074 139907720771328 logging_writer.py:48] [157600] global_step=157600, grad_norm=3.8242249488830566, loss=0.7590423822402954
I0203 12:31:18.728507 139907745949440 logging_writer.py:48] [157700] global_step=157700, grad_norm=4.0887980461120605, loss=0.7675049901008606
I0203 12:31:52.414366 139907720771328 logging_writer.py:48] [157800] global_step=157800, grad_norm=4.157087802886963, loss=0.8133439421653748
I0203 12:32:26.087798 139907745949440 logging_writer.py:48] [157900] global_step=157900, grad_norm=4.18579626083374, loss=0.8715479373931885
I0203 12:32:59.774355 139907720771328 logging_writer.py:48] [158000] global_step=158000, grad_norm=4.770621299743652, loss=0.8146808743476868
I0203 12:33:33.561845 139907745949440 logging_writer.py:48] [158100] global_step=158100, grad_norm=4.603877544403076, loss=0.8974297046661377
I0203 12:34:07.267852 139907720771328 logging_writer.py:48] [158200] global_step=158200, grad_norm=4.223793029785156, loss=0.8477394580841064
I0203 12:34:40.945890 139907745949440 logging_writer.py:48] [158300] global_step=158300, grad_norm=3.976997137069702, loss=0.7585554122924805
I0203 12:35:14.664314 139907720771328 logging_writer.py:48] [158400] global_step=158400, grad_norm=4.063480854034424, loss=0.7788023948669434
I0203 12:35:48.339025 139907745949440 logging_writer.py:48] [158500] global_step=158500, grad_norm=4.388408184051514, loss=0.8793892860412598
I0203 12:36:22.024296 139907720771328 logging_writer.py:48] [158600] global_step=158600, grad_norm=4.075223922729492, loss=0.7706595063209534
I0203 12:36:55.693951 139907745949440 logging_writer.py:48] [158700] global_step=158700, grad_norm=4.271266937255859, loss=0.7630426287651062
I0203 12:37:29.371296 139907720771328 logging_writer.py:48] [158800] global_step=158800, grad_norm=4.467008590698242, loss=0.807452917098999
I0203 12:38:03.050224 139907745949440 logging_writer.py:48] [158900] global_step=158900, grad_norm=4.498652458190918, loss=0.7661789655685425
I0203 12:38:12.273745 140070692116288 spec.py:321] Evaluating on the training split.
I0203 12:38:18.636973 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 12:38:27.162231 140070692116288 spec.py:349] Evaluating on the test split.
I0203 12:38:29.803398 140070692116288 submission_runner.py:408] Time since start: 55466.92s, 	Step: 158929, 	{'train/accuracy': 0.9267578125, 'train/loss': 0.2488669753074646, 'validation/accuracy': 0.7440999746322632, 'validation/loss': 1.0787391662597656, 'validation/num_examples': 50000, 'test/accuracy': 0.6193000078201294, 'test/loss': 1.8259567022323608, 'test/num_examples': 10000, 'score': 53590.50220036507, 'total_duration': 55466.92384791374, 'accumulated_submission_time': 53590.50220036507, 'accumulated_eval_time': 1866.427173614502, 'accumulated_logging_time': 4.452563047409058}
I0203 12:38:29.848722 139907729164032 logging_writer.py:48] [158929] accumulated_eval_time=1866.427174, accumulated_logging_time=4.452563, accumulated_submission_time=53590.502200, global_step=158929, preemption_count=0, score=53590.502200, test/accuracy=0.619300, test/loss=1.825957, test/num_examples=10000, total_duration=55466.923848, train/accuracy=0.926758, train/loss=0.248867, validation/accuracy=0.744100, validation/loss=1.078739, validation/num_examples=50000
I0203 12:38:54.133244 139907737556736 logging_writer.py:48] [159000] global_step=159000, grad_norm=4.5050153732299805, loss=0.7840422987937927
I0203 12:39:27.801149 139907729164032 logging_writer.py:48] [159100] global_step=159100, grad_norm=4.194389820098877, loss=0.7309401035308838
I0203 12:40:01.606016 139907737556736 logging_writer.py:48] [159200] global_step=159200, grad_norm=4.352677822113037, loss=0.7933984398841858
I0203 12:40:35.287101 139907729164032 logging_writer.py:48] [159300] global_step=159300, grad_norm=4.093919277191162, loss=0.7291873097419739
I0203 12:41:08.972642 139907737556736 logging_writer.py:48] [159400] global_step=159400, grad_norm=4.477221965789795, loss=0.7197378873825073
I0203 12:41:42.635070 139907729164032 logging_writer.py:48] [159500] global_step=159500, grad_norm=4.479023456573486, loss=0.8655620217323303
I0203 12:42:16.341287 139907737556736 logging_writer.py:48] [159600] global_step=159600, grad_norm=4.097016334533691, loss=0.7662841081619263
I0203 12:42:50.068494 139907729164032 logging_writer.py:48] [159700] global_step=159700, grad_norm=4.378640174865723, loss=0.6903260350227356
I0203 12:43:23.790923 139907737556736 logging_writer.py:48] [159800] global_step=159800, grad_norm=3.9562318325042725, loss=0.7133702635765076
I0203 12:43:57.472896 139907729164032 logging_writer.py:48] [159900] global_step=159900, grad_norm=4.422882080078125, loss=0.7547577619552612
I0203 12:44:31.153593 139907737556736 logging_writer.py:48] [160000] global_step=160000, grad_norm=4.349213600158691, loss=0.7811299562454224
I0203 12:45:04.833079 139907729164032 logging_writer.py:48] [160100] global_step=160100, grad_norm=4.653422832489014, loss=0.7996930480003357
I0203 12:45:38.579946 139907737556736 logging_writer.py:48] [160200] global_step=160200, grad_norm=4.014809608459473, loss=0.7368742227554321
I0203 12:46:12.355954 139907729164032 logging_writer.py:48] [160300] global_step=160300, grad_norm=4.374563694000244, loss=0.7625078558921814
I0203 12:46:46.088796 139907737556736 logging_writer.py:48] [160400] global_step=160400, grad_norm=4.341324806213379, loss=0.7600200176239014
I0203 12:47:00.032897 140070692116288 spec.py:321] Evaluating on the training split.
I0203 12:47:06.518569 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 12:47:15.069068 140070692116288 spec.py:349] Evaluating on the test split.
I0203 12:47:17.789807 140070692116288 submission_runner.py:408] Time since start: 55994.91s, 	Step: 160443, 	{'train/accuracy': 0.9273557066917419, 'train/loss': 0.25223514437675476, 'validation/accuracy': 0.7450199723243713, 'validation/loss': 1.0835787057876587, 'validation/num_examples': 50000, 'test/accuracy': 0.6230000257492065, 'test/loss': 1.8304731845855713, 'test/num_examples': 10000, 'score': 54100.62163352966, 'total_duration': 55994.9102306366, 'accumulated_submission_time': 54100.62163352966, 'accumulated_eval_time': 1884.1840229034424, 'accumulated_logging_time': 4.509575366973877}
I0203 12:47:17.838093 139907720771328 logging_writer.py:48] [160443] accumulated_eval_time=1884.184023, accumulated_logging_time=4.509575, accumulated_submission_time=54100.621634, global_step=160443, preemption_count=0, score=54100.621634, test/accuracy=0.623000, test/loss=1.830473, test/num_examples=10000, total_duration=55994.910231, train/accuracy=0.927356, train/loss=0.252235, validation/accuracy=0.745020, validation/loss=1.083579, validation/num_examples=50000
I0203 12:47:37.384599 139907745949440 logging_writer.py:48] [160500] global_step=160500, grad_norm=4.7067036628723145, loss=0.7724918127059937
I0203 12:48:11.029557 139907720771328 logging_writer.py:48] [160600] global_step=160600, grad_norm=3.9192910194396973, loss=0.7032590508460999
I0203 12:48:44.744472 139907745949440 logging_writer.py:48] [160700] global_step=160700, grad_norm=4.398268699645996, loss=0.7681151628494263
I0203 12:49:18.420375 139907720771328 logging_writer.py:48] [160800] global_step=160800, grad_norm=4.196502208709717, loss=0.797675371170044
I0203 12:49:52.120082 139907745949440 logging_writer.py:48] [160900] global_step=160900, grad_norm=4.254647731781006, loss=0.7505435347557068
I0203 12:50:25.779512 139907720771328 logging_writer.py:48] [161000] global_step=161000, grad_norm=4.243900299072266, loss=0.7502734661102295
I0203 12:50:59.455527 139907745949440 logging_writer.py:48] [161100] global_step=161100, grad_norm=4.236536979675293, loss=0.8064650893211365
I0203 12:51:33.107531 139907720771328 logging_writer.py:48] [161200] global_step=161200, grad_norm=4.0840959548950195, loss=0.667334258556366
I0203 12:52:06.781747 139907745949440 logging_writer.py:48] [161300] global_step=161300, grad_norm=4.271731853485107, loss=0.6630069613456726
I0203 12:52:40.443433 139907720771328 logging_writer.py:48] [161400] global_step=161400, grad_norm=4.419787883758545, loss=0.7268276214599609
I0203 12:53:14.335244 139907745949440 logging_writer.py:48] [161500] global_step=161500, grad_norm=4.148002624511719, loss=0.711415708065033
I0203 12:53:48.043197 139907720771328 logging_writer.py:48] [161600] global_step=161600, grad_norm=4.411296844482422, loss=0.7877691984176636
I0203 12:54:21.737750 139907745949440 logging_writer.py:48] [161700] global_step=161700, grad_norm=4.165645599365234, loss=0.6827632188796997
I0203 12:54:55.415783 139907720771328 logging_writer.py:48] [161800] global_step=161800, grad_norm=4.261248588562012, loss=0.6369711756706238
I0203 12:55:29.178068 139907745949440 logging_writer.py:48] [161900] global_step=161900, grad_norm=4.2178544998168945, loss=0.6972984075546265
I0203 12:55:47.829346 140070692116288 spec.py:321] Evaluating on the training split.
I0203 12:55:54.134770 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 12:56:02.883523 140070692116288 spec.py:349] Evaluating on the test split.
I0203 12:56:05.560370 140070692116288 submission_runner.py:408] Time since start: 56522.68s, 	Step: 161957, 	{'train/accuracy': 0.928730845451355, 'train/loss': 0.24525007605552673, 'validation/accuracy': 0.7450199723243713, 'validation/loss': 1.079473853111267, 'validation/num_examples': 50000, 'test/accuracy': 0.6254000067710876, 'test/loss': 1.832878589630127, 'test/num_examples': 10000, 'score': 54610.548907756805, 'total_duration': 56522.68081855774, 'accumulated_submission_time': 54610.548907756805, 'accumulated_eval_time': 1901.915011882782, 'accumulated_logging_time': 4.5679240226745605}
I0203 12:56:05.610083 139907251042048 logging_writer.py:48] [161957] accumulated_eval_time=1901.915012, accumulated_logging_time=4.567924, accumulated_submission_time=54610.548908, global_step=161957, preemption_count=0, score=54610.548908, test/accuracy=0.625400, test/loss=1.832879, test/num_examples=10000, total_duration=56522.680819, train/accuracy=0.928731, train/loss=0.245250, validation/accuracy=0.745020, validation/loss=1.079474, validation/num_examples=50000
I0203 12:56:20.455109 139907703985920 logging_writer.py:48] [162000] global_step=162000, grad_norm=4.1229705810546875, loss=0.7379573583602905
I0203 12:56:54.159995 139907251042048 logging_writer.py:48] [162100] global_step=162100, grad_norm=4.3751630783081055, loss=0.718258798122406
I0203 12:57:27.858616 139907703985920 logging_writer.py:48] [162200] global_step=162200, grad_norm=4.673810005187988, loss=0.7812149524688721
I0203 12:58:01.547817 139907251042048 logging_writer.py:48] [162300] global_step=162300, grad_norm=4.838123321533203, loss=0.7478220462799072
I0203 12:58:35.297348 139907703985920 logging_writer.py:48] [162400] global_step=162400, grad_norm=4.437987327575684, loss=0.7123892307281494
I0203 12:59:09.022456 139907251042048 logging_writer.py:48] [162500] global_step=162500, grad_norm=4.381656169891357, loss=0.7710456252098083
I0203 12:59:42.807659 139907703985920 logging_writer.py:48] [162600] global_step=162600, grad_norm=4.429055213928223, loss=0.7156694531440735
I0203 13:00:16.482285 139907251042048 logging_writer.py:48] [162700] global_step=162700, grad_norm=4.437323570251465, loss=0.7391840815544128
I0203 13:00:50.183420 139907703985920 logging_writer.py:48] [162800] global_step=162800, grad_norm=4.189901828765869, loss=0.7554046511650085
I0203 13:01:23.932205 139907251042048 logging_writer.py:48] [162900] global_step=162900, grad_norm=4.503177165985107, loss=0.8349966406822205
I0203 13:01:57.612432 139907703985920 logging_writer.py:48] [163000] global_step=163000, grad_norm=4.288903713226318, loss=0.7420008182525635
I0203 13:02:31.364984 139907251042048 logging_writer.py:48] [163100] global_step=163100, grad_norm=4.3497514724731445, loss=0.8278435468673706
I0203 13:03:05.046976 139907703985920 logging_writer.py:48] [163200] global_step=163200, grad_norm=4.290807723999023, loss=0.7511884570121765
I0203 13:03:38.737421 139907251042048 logging_writer.py:48] [163300] global_step=163300, grad_norm=4.327552318572998, loss=0.6832508444786072
I0203 13:04:12.396055 139907703985920 logging_writer.py:48] [163400] global_step=163400, grad_norm=4.426630020141602, loss=0.7300899028778076
I0203 13:04:35.772257 140070692116288 spec.py:321] Evaluating on the training split.
I0203 13:04:42.145988 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 13:04:50.479317 140070692116288 spec.py:349] Evaluating on the test split.
I0203 13:04:53.237483 140070692116288 submission_runner.py:408] Time since start: 57050.36s, 	Step: 163471, 	{'train/accuracy': 0.9323580861091614, 'train/loss': 0.23463422060012817, 'validation/accuracy': 0.7455399632453918, 'validation/loss': 1.0828678607940674, 'validation/num_examples': 50000, 'test/accuracy': 0.6281000375747681, 'test/loss': 1.822871208190918, 'test/num_examples': 10000, 'score': 55120.64704847336, 'total_duration': 57050.35792398453, 'accumulated_submission_time': 55120.64704847336, 'accumulated_eval_time': 1919.3801970481873, 'accumulated_logging_time': 4.62821364402771}
I0203 13:04:53.279727 139907251042048 logging_writer.py:48] [163471] accumulated_eval_time=1919.380197, accumulated_logging_time=4.628214, accumulated_submission_time=55120.647048, global_step=163471, preemption_count=0, score=55120.647048, test/accuracy=0.628100, test/loss=1.822871, test/num_examples=10000, total_duration=57050.357924, train/accuracy=0.932358, train/loss=0.234634, validation/accuracy=0.745540, validation/loss=1.082868, validation/num_examples=50000
I0203 13:05:03.386227 139907703985920 logging_writer.py:48] [163500] global_step=163500, grad_norm=4.121339321136475, loss=0.664925217628479
I0203 13:05:37.135650 139907251042048 logging_writer.py:48] [163600] global_step=163600, grad_norm=4.782631874084473, loss=0.7630489468574524
I0203 13:06:11.037806 139907703985920 logging_writer.py:48] [163700] global_step=163700, grad_norm=4.265998363494873, loss=0.7362741231918335
I0203 13:06:44.782718 139907251042048 logging_writer.py:48] [163800] global_step=163800, grad_norm=4.201473712921143, loss=0.7093089818954468
I0203 13:07:18.438634 139907703985920 logging_writer.py:48] [163900] global_step=163900, grad_norm=4.605630874633789, loss=0.7705247402191162
I0203 13:07:52.156303 139907251042048 logging_writer.py:48] [164000] global_step=164000, grad_norm=4.1283745765686035, loss=0.6657723784446716
I0203 13:08:25.828829 139907703985920 logging_writer.py:48] [164100] global_step=164100, grad_norm=4.78610897064209, loss=0.8096562623977661
I0203 13:08:59.527922 139907251042048 logging_writer.py:48] [164200] global_step=164200, grad_norm=4.540214538574219, loss=0.7623608708381653
I0203 13:09:33.226910 139907703985920 logging_writer.py:48] [164300] global_step=164300, grad_norm=4.547785758972168, loss=0.7219759225845337
I0203 13:10:06.932675 139907251042048 logging_writer.py:48] [164400] global_step=164400, grad_norm=4.797230243682861, loss=0.7701118588447571
I0203 13:10:40.574309 139907703985920 logging_writer.py:48] [164500] global_step=164500, grad_norm=4.616398811340332, loss=0.7903224229812622
I0203 13:11:14.227826 139907251042048 logging_writer.py:48] [164600] global_step=164600, grad_norm=4.5383148193359375, loss=0.7236310839653015
I0203 13:11:47.913663 139907703985920 logging_writer.py:48] [164700] global_step=164700, grad_norm=4.657891273498535, loss=0.748169481754303
I0203 13:12:21.794924 139907251042048 logging_writer.py:48] [164800] global_step=164800, grad_norm=4.011836528778076, loss=0.5535047650337219
I0203 13:12:55.527900 139907703985920 logging_writer.py:48] [164900] global_step=164900, grad_norm=4.529260635375977, loss=0.7484837174415588
I0203 13:13:23.292623 140070692116288 spec.py:321] Evaluating on the training split.
I0203 13:13:29.654064 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 13:13:38.058906 140070692116288 spec.py:349] Evaluating on the test split.
I0203 13:13:40.718902 140070692116288 submission_runner.py:408] Time since start: 57577.84s, 	Step: 164984, 	{'train/accuracy': 0.9518494606018066, 'train/loss': 0.1781339794397354, 'validation/accuracy': 0.7479000091552734, 'validation/loss': 1.0789545774459839, 'validation/num_examples': 50000, 'test/accuracy': 0.6242000460624695, 'test/loss': 1.8314135074615479, 'test/num_examples': 10000, 'score': 55630.59670042992, 'total_duration': 57577.83935189247, 'accumulated_submission_time': 55630.59670042992, 'accumulated_eval_time': 1936.8064422607422, 'accumulated_logging_time': 4.680109977722168}
I0203 13:13:40.767045 139907712378624 logging_writer.py:48] [164984] accumulated_eval_time=1936.806442, accumulated_logging_time=4.680110, accumulated_submission_time=55630.596700, global_step=164984, preemption_count=0, score=55630.596700, test/accuracy=0.624200, test/loss=1.831414, test/num_examples=10000, total_duration=57577.839352, train/accuracy=0.951849, train/loss=0.178134, validation/accuracy=0.747900, validation/loss=1.078955, validation/num_examples=50000
I0203 13:13:46.497475 139907729164032 logging_writer.py:48] [165000] global_step=165000, grad_norm=4.311508655548096, loss=0.6895703077316284
I0203 13:14:20.125039 139907712378624 logging_writer.py:48] [165100] global_step=165100, grad_norm=4.239437103271484, loss=0.704461932182312
I0203 13:14:53.870093 139907729164032 logging_writer.py:48] [165200] global_step=165200, grad_norm=4.872734546661377, loss=0.781976044178009
I0203 13:15:27.574891 139907712378624 logging_writer.py:48] [165300] global_step=165300, grad_norm=4.4036784172058105, loss=0.7269148826599121
I0203 13:16:01.262958 139907729164032 logging_writer.py:48] [165400] global_step=165400, grad_norm=4.158109664916992, loss=0.6785760521888733
I0203 13:16:34.905381 139907712378624 logging_writer.py:48] [165500] global_step=165500, grad_norm=4.6072096824646, loss=0.6961981058120728
I0203 13:17:08.582567 139907729164032 logging_writer.py:48] [165600] global_step=165600, grad_norm=4.4321818351745605, loss=0.6361019015312195
I0203 13:17:42.273265 139907712378624 logging_writer.py:48] [165700] global_step=165700, grad_norm=4.4602837562561035, loss=0.7059631943702698
I0203 13:18:15.960670 139907729164032 logging_writer.py:48] [165800] global_step=165800, grad_norm=4.188309669494629, loss=0.6939421892166138
I0203 13:18:49.631748 139907712378624 logging_writer.py:48] [165900] global_step=165900, grad_norm=4.0302653312683105, loss=0.685204029083252
I0203 13:19:23.434886 139907729164032 logging_writer.py:48] [166000] global_step=166000, grad_norm=4.774885177612305, loss=0.7652204632759094
I0203 13:19:57.100599 139907712378624 logging_writer.py:48] [166100] global_step=166100, grad_norm=3.772886037826538, loss=0.568442702293396
I0203 13:20:30.829854 139907729164032 logging_writer.py:48] [166200] global_step=166200, grad_norm=4.447455883026123, loss=0.7796866297721863
I0203 13:21:04.514133 139907712378624 logging_writer.py:48] [166300] global_step=166300, grad_norm=4.466606140136719, loss=0.7349989414215088
I0203 13:21:38.267328 139907729164032 logging_writer.py:48] [166400] global_step=166400, grad_norm=4.251739978790283, loss=0.7115704417228699
I0203 13:22:10.752640 140070692116288 spec.py:321] Evaluating on the training split.
I0203 13:22:17.028403 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 13:22:25.397154 140070692116288 spec.py:349] Evaluating on the test split.
I0203 13:22:28.074380 140070692116288 submission_runner.py:408] Time since start: 58105.19s, 	Step: 166498, 	{'train/accuracy': 0.9480029940605164, 'train/loss': 0.1860027015209198, 'validation/accuracy': 0.746999979019165, 'validation/loss': 1.078598976135254, 'validation/num_examples': 50000, 'test/accuracy': 0.6274000406265259, 'test/loss': 1.8304781913757324, 'test/num_examples': 10000, 'score': 56140.51879167557, 'total_duration': 58105.194816827774, 'accumulated_submission_time': 56140.51879167557, 'accumulated_eval_time': 1954.128136396408, 'accumulated_logging_time': 4.73781156539917}
I0203 13:22:28.119025 139907745949440 logging_writer.py:48] [166498] accumulated_eval_time=1954.128136, accumulated_logging_time=4.737812, accumulated_submission_time=56140.518792, global_step=166498, preemption_count=0, score=56140.518792, test/accuracy=0.627400, test/loss=1.830478, test/num_examples=10000, total_duration=58105.194817, train/accuracy=0.948003, train/loss=0.186003, validation/accuracy=0.747000, validation/loss=1.078599, validation/num_examples=50000
I0203 13:22:29.138803 139907754342144 logging_writer.py:48] [166500] global_step=166500, grad_norm=3.9856865406036377, loss=0.6355285048484802
I0203 13:23:02.777978 139907745949440 logging_writer.py:48] [166600] global_step=166600, grad_norm=4.64264440536499, loss=0.7251235246658325
I0203 13:23:36.477711 139907754342144 logging_writer.py:48] [166700] global_step=166700, grad_norm=4.773402690887451, loss=0.6849551796913147
I0203 13:24:10.143198 139907745949440 logging_writer.py:48] [166800] global_step=166800, grad_norm=4.961698532104492, loss=0.7335829734802246
I0203 13:24:43.845912 139907754342144 logging_writer.py:48] [166900] global_step=166900, grad_norm=4.456892967224121, loss=0.6999752521514893
I0203 13:25:17.509173 139907745949440 logging_writer.py:48] [167000] global_step=167000, grad_norm=4.552465438842773, loss=0.7242101430892944
I0203 13:25:51.334116 139907754342144 logging_writer.py:48] [167100] global_step=167100, grad_norm=4.656376361846924, loss=0.7533301115036011
I0203 13:26:25.020686 139907745949440 logging_writer.py:48] [167200] global_step=167200, grad_norm=4.471044063568115, loss=0.6800661683082581
I0203 13:26:58.739002 139907754342144 logging_writer.py:48] [167300] global_step=167300, grad_norm=4.253878116607666, loss=0.7093221545219421
I0203 13:27:32.387652 139907745949440 logging_writer.py:48] [167400] global_step=167400, grad_norm=4.252144813537598, loss=0.6561717391014099
I0203 13:28:06.086147 139907754342144 logging_writer.py:48] [167500] global_step=167500, grad_norm=4.637588024139404, loss=0.7055872678756714
I0203 13:28:39.828434 139907745949440 logging_writer.py:48] [167600] global_step=167600, grad_norm=4.109276294708252, loss=0.6199725866317749
I0203 13:29:13.576423 139907754342144 logging_writer.py:48] [167700] global_step=167700, grad_norm=4.0481858253479, loss=0.6675904989242554
I0203 13:29:47.228142 139907745949440 logging_writer.py:48] [167800] global_step=167800, grad_norm=4.2079877853393555, loss=0.7070611119270325
I0203 13:30:20.935645 139907754342144 logging_writer.py:48] [167900] global_step=167900, grad_norm=4.894294738769531, loss=0.7661685943603516
I0203 13:30:54.582255 139907745949440 logging_writer.py:48] [168000] global_step=168000, grad_norm=4.739091396331787, loss=0.7197516560554504
I0203 13:30:58.111693 140070692116288 spec.py:321] Evaluating on the training split.
I0203 13:31:04.699634 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 13:31:13.142382 140070692116288 spec.py:349] Evaluating on the test split.
I0203 13:31:15.811021 140070692116288 submission_runner.py:408] Time since start: 58632.93s, 	Step: 168012, 	{'train/accuracy': 0.9465281963348389, 'train/loss': 0.18971571326255798, 'validation/accuracy': 0.7482799887657166, 'validation/loss': 1.0680350065231323, 'validation/num_examples': 50000, 'test/accuracy': 0.626800000667572, 'test/loss': 1.8228516578674316, 'test/num_examples': 10000, 'score': 56650.4464943409, 'total_duration': 58632.931473731995, 'accumulated_submission_time': 56650.4464943409, 'accumulated_eval_time': 1971.8274443149567, 'accumulated_logging_time': 4.794127941131592}
I0203 13:31:15.864880 139907251042048 logging_writer.py:48] [168012] accumulated_eval_time=1971.827444, accumulated_logging_time=4.794128, accumulated_submission_time=56650.446494, global_step=168012, preemption_count=0, score=56650.446494, test/accuracy=0.626800, test/loss=1.822852, test/num_examples=10000, total_duration=58632.931474, train/accuracy=0.946528, train/loss=0.189716, validation/accuracy=0.748280, validation/loss=1.068035, validation/num_examples=50000
I0203 13:31:45.888314 139907703985920 logging_writer.py:48] [168100] global_step=168100, grad_norm=4.168403625488281, loss=0.6837940216064453
I0203 13:32:19.596647 139907251042048 logging_writer.py:48] [168200] global_step=168200, grad_norm=4.649034023284912, loss=0.6700235605239868
I0203 13:32:53.267355 139907703985920 logging_writer.py:48] [168300] global_step=168300, grad_norm=4.448766708374023, loss=0.7104895710945129
I0203 13:33:26.945739 139907251042048 logging_writer.py:48] [168400] global_step=168400, grad_norm=4.028717041015625, loss=0.658295750617981
I0203 13:34:00.668217 139907703985920 logging_writer.py:48] [168500] global_step=168500, grad_norm=4.597327709197998, loss=0.697023868560791
I0203 13:34:34.340238 139907251042048 logging_writer.py:48] [168600] global_step=168600, grad_norm=4.736440181732178, loss=0.7186611294746399
I0203 13:35:07.997902 139907703985920 logging_writer.py:48] [168700] global_step=168700, grad_norm=4.509204864501953, loss=0.6926328539848328
I0203 13:35:41.777284 139907251042048 logging_writer.py:48] [168800] global_step=168800, grad_norm=4.5511932373046875, loss=0.7438586950302124
I0203 13:36:15.467152 139907703985920 logging_writer.py:48] [168900] global_step=168900, grad_norm=4.35274076461792, loss=0.6784590482711792
I0203 13:36:49.155414 139907251042048 logging_writer.py:48] [169000] global_step=169000, grad_norm=4.160203456878662, loss=0.6935206651687622
I0203 13:37:22.826691 139907703985920 logging_writer.py:48] [169100] global_step=169100, grad_norm=4.448203086853027, loss=0.6737546324729919
I0203 13:37:56.528155 139907251042048 logging_writer.py:48] [169200] global_step=169200, grad_norm=4.373926639556885, loss=0.7182475328445435
I0203 13:38:30.189256 139907703985920 logging_writer.py:48] [169300] global_step=169300, grad_norm=4.483713150024414, loss=0.6409932971000671
I0203 13:39:04.001899 139907251042048 logging_writer.py:48] [169400] global_step=169400, grad_norm=4.3631086349487305, loss=0.6155369281768799
I0203 13:39:37.689797 139907703985920 logging_writer.py:48] [169500] global_step=169500, grad_norm=4.623486518859863, loss=0.730470597743988
I0203 13:39:45.967493 140070692116288 spec.py:321] Evaluating on the training split.
I0203 13:39:52.353916 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 13:40:00.927848 140070692116288 spec.py:349] Evaluating on the test split.
I0203 13:40:03.768049 140070692116288 submission_runner.py:408] Time since start: 59160.89s, 	Step: 169526, 	{'train/accuracy': 0.9483816623687744, 'train/loss': 0.1834833025932312, 'validation/accuracy': 0.7514199614524841, 'validation/loss': 1.0675344467163086, 'validation/num_examples': 50000, 'test/accuracy': 0.627500057220459, 'test/loss': 1.824575662612915, 'test/num_examples': 10000, 'score': 57160.486573934555, 'total_duration': 59160.888498306274, 'accumulated_submission_time': 57160.486573934555, 'accumulated_eval_time': 1989.6279754638672, 'accumulated_logging_time': 4.857915878295898}
I0203 13:40:03.815028 139907737556736 logging_writer.py:48] [169526] accumulated_eval_time=1989.627975, accumulated_logging_time=4.857916, accumulated_submission_time=57160.486574, global_step=169526, preemption_count=0, score=57160.486574, test/accuracy=0.627500, test/loss=1.824576, test/num_examples=10000, total_duration=59160.888498, train/accuracy=0.948382, train/loss=0.183483, validation/accuracy=0.751420, validation/loss=1.067534, validation/num_examples=50000
I0203 13:40:29.085914 139907745949440 logging_writer.py:48] [169600] global_step=169600, grad_norm=4.424396514892578, loss=0.6887211203575134
I0203 13:41:02.735097 139907737556736 logging_writer.py:48] [169700] global_step=169700, grad_norm=4.705908298492432, loss=0.6908811330795288
I0203 13:41:36.439650 139907745949440 logging_writer.py:48] [169800] global_step=169800, grad_norm=4.132127285003662, loss=0.6120744943618774
I0203 13:42:10.098960 139907737556736 logging_writer.py:48] [169900] global_step=169900, grad_norm=4.475977420806885, loss=0.67237788438797
I0203 13:42:43.789879 139907745949440 logging_writer.py:48] [170000] global_step=170000, grad_norm=4.712423324584961, loss=0.6994494795799255
I0203 13:43:17.456378 139907737556736 logging_writer.py:48] [170100] global_step=170100, grad_norm=4.505185127258301, loss=0.6666437387466431
I0203 13:43:51.152283 139907745949440 logging_writer.py:48] [170200] global_step=170200, grad_norm=4.495352745056152, loss=0.6969395279884338
I0203 13:44:24.814622 139907737556736 logging_writer.py:48] [170300] global_step=170300, grad_norm=4.439558506011963, loss=0.6483219861984253
I0203 13:44:58.550657 139907745949440 logging_writer.py:48] [170400] global_step=170400, grad_norm=4.220696449279785, loss=0.5707833766937256
I0203 13:45:32.461003 139907737556736 logging_writer.py:48] [170500] global_step=170500, grad_norm=4.5163493156433105, loss=0.6687395572662354
I0203 13:46:06.203460 139907745949440 logging_writer.py:48] [170600] global_step=170600, grad_norm=4.277310848236084, loss=0.705950915813446
I0203 13:46:39.879417 139907737556736 logging_writer.py:48] [170700] global_step=170700, grad_norm=4.153266429901123, loss=0.6516354084014893
I0203 13:47:13.581190 139907745949440 logging_writer.py:48] [170800] global_step=170800, grad_norm=4.517814636230469, loss=0.6916036605834961
I0203 13:47:47.228241 139907737556736 logging_writer.py:48] [170900] global_step=170900, grad_norm=4.31118631362915, loss=0.6543589234352112
I0203 13:48:20.929711 139907745949440 logging_writer.py:48] [171000] global_step=171000, grad_norm=4.461960792541504, loss=0.6477134823799133
I0203 13:48:33.869012 140070692116288 spec.py:321] Evaluating on the training split.
I0203 13:48:40.812549 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 13:48:49.414872 140070692116288 spec.py:349] Evaluating on the test split.
I0203 13:48:52.211938 140070692116288 submission_runner.py:408] Time since start: 59689.33s, 	Step: 171040, 	{'train/accuracy': 0.950215220451355, 'train/loss': 0.17819266021251678, 'validation/accuracy': 0.7507599592208862, 'validation/loss': 1.0668600797653198, 'validation/num_examples': 50000, 'test/accuracy': 0.6289000511169434, 'test/loss': 1.8254389762878418, 'test/num_examples': 10000, 'score': 57670.477942705154, 'total_duration': 59689.33236479759, 'accumulated_submission_time': 57670.477942705154, 'accumulated_eval_time': 2007.9708423614502, 'accumulated_logging_time': 4.914608955383301}
I0203 13:48:52.259343 139907251042048 logging_writer.py:48] [171040] accumulated_eval_time=2007.970842, accumulated_logging_time=4.914609, accumulated_submission_time=57670.477943, global_step=171040, preemption_count=0, score=57670.477943, test/accuracy=0.628900, test/loss=1.825439, test/num_examples=10000, total_duration=59689.332365, train/accuracy=0.950215, train/loss=0.178193, validation/accuracy=0.750760, validation/loss=1.066860, validation/num_examples=50000
I0203 13:49:12.798133 139907703985920 logging_writer.py:48] [171100] global_step=171100, grad_norm=4.422293663024902, loss=0.6564311385154724
I0203 13:49:46.505222 139907251042048 logging_writer.py:48] [171200] global_step=171200, grad_norm=4.210217475891113, loss=0.6344004273414612
I0203 13:50:20.210065 139907703985920 logging_writer.py:48] [171300] global_step=171300, grad_norm=4.2867231369018555, loss=0.6695389151573181
I0203 13:50:53.881309 139907251042048 logging_writer.py:48] [171400] global_step=171400, grad_norm=4.328596591949463, loss=0.662476658821106
I0203 13:51:27.578609 139907703985920 logging_writer.py:48] [171500] global_step=171500, grad_norm=5.280110836029053, loss=0.7387516498565674
I0203 13:52:01.382013 139907251042048 logging_writer.py:48] [171600] global_step=171600, grad_norm=4.184694290161133, loss=0.5892317295074463
I0203 13:52:35.061621 139907703985920 logging_writer.py:48] [171700] global_step=171700, grad_norm=4.5592122077941895, loss=0.7453219890594482
I0203 13:53:08.725912 139907251042048 logging_writer.py:48] [171800] global_step=171800, grad_norm=4.530733108520508, loss=0.6511940360069275
I0203 13:53:42.403695 139907703985920 logging_writer.py:48] [171900] global_step=171900, grad_norm=4.789236545562744, loss=0.6759344339370728
I0203 13:54:16.075639 139907251042048 logging_writer.py:48] [172000] global_step=172000, grad_norm=4.770888328552246, loss=0.7512603402137756
I0203 13:54:49.768928 139907703985920 logging_writer.py:48] [172100] global_step=172100, grad_norm=4.283106327056885, loss=0.6482375860214233
I0203 13:55:23.443852 139907251042048 logging_writer.py:48] [172200] global_step=172200, grad_norm=4.573827266693115, loss=0.7156616449356079
I0203 13:55:57.222994 139907703985920 logging_writer.py:48] [172300] global_step=172300, grad_norm=4.455850124359131, loss=0.6960130929946899
I0203 13:56:30.919508 139907251042048 logging_writer.py:48] [172400] global_step=172400, grad_norm=4.159506797790527, loss=0.5838932394981384
I0203 13:57:04.707597 139907703985920 logging_writer.py:48] [172500] global_step=172500, grad_norm=4.893854141235352, loss=0.6403997540473938
I0203 13:57:22.370287 140070692116288 spec.py:321] Evaluating on the training split.
I0203 13:57:28.646329 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 13:57:37.268636 140070692116288 spec.py:349] Evaluating on the test split.
I0203 13:57:39.968969 140070692116288 submission_runner.py:408] Time since start: 60217.09s, 	Step: 172554, 	{'train/accuracy': 0.9512715339660645, 'train/loss': 0.17513197660446167, 'validation/accuracy': 0.7514399886131287, 'validation/loss': 1.0617859363555908, 'validation/num_examples': 50000, 'test/accuracy': 0.6328000426292419, 'test/loss': 1.818946361541748, 'test/num_examples': 10000, 'score': 58180.52669739723, 'total_duration': 60217.089391469955, 'accumulated_submission_time': 58180.52669739723, 'accumulated_eval_time': 2025.5694625377655, 'accumulated_logging_time': 4.971103191375732}
I0203 13:57:40.021044 139907745949440 logging_writer.py:48] [172554] accumulated_eval_time=2025.569463, accumulated_logging_time=4.971103, accumulated_submission_time=58180.526697, global_step=172554, preemption_count=0, score=58180.526697, test/accuracy=0.632800, test/loss=1.818946, test/num_examples=10000, total_duration=60217.089391, train/accuracy=0.951272, train/loss=0.175132, validation/accuracy=0.751440, validation/loss=1.061786, validation/num_examples=50000
I0203 13:57:55.832645 139907754342144 logging_writer.py:48] [172600] global_step=172600, grad_norm=4.600361347198486, loss=0.6667442321777344
I0203 13:58:29.600278 139907745949440 logging_writer.py:48] [172700] global_step=172700, grad_norm=4.486858367919922, loss=0.6495829820632935
I0203 13:59:03.337425 139907754342144 logging_writer.py:48] [172800] global_step=172800, grad_norm=4.4879679679870605, loss=0.6286116242408752
I0203 13:59:37.027792 139907745949440 logging_writer.py:48] [172900] global_step=172900, grad_norm=4.6227617263793945, loss=0.6361396312713623
I0203 14:00:10.701810 139907754342144 logging_writer.py:48] [173000] global_step=173000, grad_norm=4.7393999099731445, loss=0.6339763402938843
I0203 14:00:44.396947 139907745949440 logging_writer.py:48] [173100] global_step=173100, grad_norm=4.295481204986572, loss=0.6744725704193115
I0203 14:01:18.061242 139907754342144 logging_writer.py:48] [173200] global_step=173200, grad_norm=4.743474960327148, loss=0.7043747305870056
I0203 14:01:51.769257 139907745949440 logging_writer.py:48] [173300] global_step=173300, grad_norm=4.756542205810547, loss=0.680925190448761
I0203 14:02:25.507881 139907754342144 logging_writer.py:48] [173400] global_step=173400, grad_norm=4.4883527755737305, loss=0.6257187128067017
I0203 14:02:59.235870 139907745949440 logging_writer.py:48] [173500] global_step=173500, grad_norm=4.458154201507568, loss=0.604680061340332
I0203 14:03:32.902126 139907754342144 logging_writer.py:48] [173600] global_step=173600, grad_norm=4.554060459136963, loss=0.5923758745193481
I0203 14:04:06.588510 139907745949440 logging_writer.py:48] [173700] global_step=173700, grad_norm=4.8396382331848145, loss=0.6474334001541138
I0203 14:04:40.257989 139907754342144 logging_writer.py:48] [173800] global_step=173800, grad_norm=4.412545204162598, loss=0.6705384254455566
I0203 14:05:14.076485 139907745949440 logging_writer.py:48] [173900] global_step=173900, grad_norm=4.489748477935791, loss=0.6656238436698914
I0203 14:05:47.745794 139907754342144 logging_writer.py:48] [174000] global_step=174000, grad_norm=4.43383264541626, loss=0.6102483868598938
I0203 14:06:10.136754 140070692116288 spec.py:321] Evaluating on the training split.
I0203 14:06:16.402519 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 14:06:24.847942 140070692116288 spec.py:349] Evaluating on the test split.
I0203 14:06:27.539207 140070692116288 submission_runner.py:408] Time since start: 60744.66s, 	Step: 174068, 	{'train/accuracy': 0.9608178734779358, 'train/loss': 0.14954684674739838, 'validation/accuracy': 0.7513999938964844, 'validation/loss': 1.0650503635406494, 'validation/num_examples': 50000, 'test/accuracy': 0.6322000026702881, 'test/loss': 1.8160579204559326, 'test/num_examples': 10000, 'score': 58690.58052444458, 'total_duration': 60744.659641981125, 'accumulated_submission_time': 58690.58052444458, 'accumulated_eval_time': 2042.9718658924103, 'accumulated_logging_time': 5.033244848251343}
I0203 14:06:27.587401 139907712378624 logging_writer.py:48] [174068] accumulated_eval_time=2042.971866, accumulated_logging_time=5.033245, accumulated_submission_time=58690.580524, global_step=174068, preemption_count=0, score=58690.580524, test/accuracy=0.632200, test/loss=1.816058, test/num_examples=10000, total_duration=60744.659642, train/accuracy=0.960818, train/loss=0.149547, validation/accuracy=0.751400, validation/loss=1.065050, validation/num_examples=50000
I0203 14:06:38.713248 139907720771328 logging_writer.py:48] [174100] global_step=174100, grad_norm=4.848405838012695, loss=0.6371241211891174
I0203 14:07:12.399852 139907712378624 logging_writer.py:48] [174200] global_step=174200, grad_norm=4.714486598968506, loss=0.6568331718444824
I0203 14:07:46.073807 139907720771328 logging_writer.py:48] [174300] global_step=174300, grad_norm=4.4253716468811035, loss=0.704578697681427
I0203 14:08:19.763343 139907712378624 logging_writer.py:48] [174400] global_step=174400, grad_norm=4.62060546875, loss=0.6548123359680176
I0203 14:08:53.431421 139907720771328 logging_writer.py:48] [174500] global_step=174500, grad_norm=4.468303203582764, loss=0.5815454125404358
I0203 14:09:27.074973 139907712378624 logging_writer.py:48] [174600] global_step=174600, grad_norm=4.478845119476318, loss=0.6238951683044434
I0203 14:10:00.820293 139907720771328 logging_writer.py:48] [174700] global_step=174700, grad_norm=4.381369590759277, loss=0.6162062883377075
I0203 14:10:34.512587 139907712378624 logging_writer.py:48] [174800] global_step=174800, grad_norm=4.581235885620117, loss=0.6738718152046204
I0203 14:11:08.232030 139907720771328 logging_writer.py:48] [174900] global_step=174900, grad_norm=4.329929351806641, loss=0.6471838355064392
I0203 14:11:42.042294 139907712378624 logging_writer.py:48] [175000] global_step=175000, grad_norm=4.438710689544678, loss=0.6628642082214355
I0203 14:12:15.755085 139907720771328 logging_writer.py:48] [175100] global_step=175100, grad_norm=4.245852470397949, loss=0.6271281242370605
I0203 14:12:49.457523 139907712378624 logging_writer.py:48] [175200] global_step=175200, grad_norm=4.46626091003418, loss=0.6623552441596985
I0203 14:13:23.117705 139907720771328 logging_writer.py:48] [175300] global_step=175300, grad_norm=4.484277725219727, loss=0.6736400127410889
I0203 14:13:56.813318 139907712378624 logging_writer.py:48] [175400] global_step=175400, grad_norm=4.379890441894531, loss=0.6833780407905579
I0203 14:14:30.480319 139907720771328 logging_writer.py:48] [175500] global_step=175500, grad_norm=4.616176128387451, loss=0.647995114326477
I0203 14:14:57.663661 140070692116288 spec.py:321] Evaluating on the training split.
I0203 14:15:04.140242 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 14:15:12.670140 140070692116288 spec.py:349] Evaluating on the test split.
I0203 14:15:15.262325 140070692116288 submission_runner.py:408] Time since start: 61272.38s, 	Step: 175582, 	{'train/accuracy': 0.9582270383834839, 'train/loss': 0.15545976161956787, 'validation/accuracy': 0.7520999908447266, 'validation/loss': 1.0633561611175537, 'validation/num_examples': 50000, 'test/accuracy': 0.6325000524520874, 'test/loss': 1.820577621459961, 'test/num_examples': 10000, 'score': 59200.59376168251, 'total_duration': 61272.38275671005, 'accumulated_submission_time': 59200.59376168251, 'accumulated_eval_time': 2060.5704913139343, 'accumulated_logging_time': 5.091560125350952}
I0203 14:15:15.309076 139907745949440 logging_writer.py:48] [175582] accumulated_eval_time=2060.570491, accumulated_logging_time=5.091560, accumulated_submission_time=59200.593762, global_step=175582, preemption_count=0, score=59200.593762, test/accuracy=0.632500, test/loss=1.820578, test/num_examples=10000, total_duration=61272.382757, train/accuracy=0.958227, train/loss=0.155460, validation/accuracy=0.752100, validation/loss=1.063356, validation/num_examples=50000
I0203 14:15:21.716749 139907754342144 logging_writer.py:48] [175600] global_step=175600, grad_norm=4.142716884613037, loss=0.5822704434394836
I0203 14:15:55.364830 139907745949440 logging_writer.py:48] [175700] global_step=175700, grad_norm=4.434039115905762, loss=0.5724313855171204
I0203 14:16:29.067203 139907754342144 logging_writer.py:48] [175800] global_step=175800, grad_norm=4.6249213218688965, loss=0.6932404041290283
I0203 14:17:02.765366 139907745949440 logging_writer.py:48] [175900] global_step=175900, grad_norm=4.640076637268066, loss=0.5892944931983948
I0203 14:17:36.456810 139907754342144 logging_writer.py:48] [176000] global_step=176000, grad_norm=4.537891387939453, loss=0.667063295841217
I0203 14:18:10.167329 139907745949440 logging_writer.py:48] [176100] global_step=176100, grad_norm=4.53422212600708, loss=0.680034875869751
I0203 14:18:43.899715 139907754342144 logging_writer.py:48] [176200] global_step=176200, grad_norm=4.5573506355285645, loss=0.6206094026565552
I0203 14:19:17.575130 139907745949440 logging_writer.py:48] [176300] global_step=176300, grad_norm=4.664261817932129, loss=0.6904529333114624
I0203 14:19:51.248462 139907754342144 logging_writer.py:48] [176400] global_step=176400, grad_norm=4.504374027252197, loss=0.616759181022644
I0203 14:20:24.930380 139907745949440 logging_writer.py:48] [176500] global_step=176500, grad_norm=4.49981689453125, loss=0.6174116134643555
I0203 14:20:58.615468 139907754342144 logging_writer.py:48] [176600] global_step=176600, grad_norm=4.229496002197266, loss=0.5685678720474243
I0203 14:21:32.304110 139907745949440 logging_writer.py:48] [176700] global_step=176700, grad_norm=4.281357288360596, loss=0.6242325305938721
I0203 14:22:05.996494 139907754342144 logging_writer.py:48] [176800] global_step=176800, grad_norm=4.374527931213379, loss=0.5606035590171814
I0203 14:22:39.772417 139907745949440 logging_writer.py:48] [176900] global_step=176900, grad_norm=4.647959232330322, loss=0.6519566178321838
I0203 14:23:13.470825 139907754342144 logging_writer.py:48] [177000] global_step=177000, grad_norm=4.490976333618164, loss=0.6485602855682373
I0203 14:23:45.311862 140070692116288 spec.py:321] Evaluating on the training split.
I0203 14:23:51.763231 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 14:24:00.449977 140070692116288 spec.py:349] Evaluating on the test split.
I0203 14:24:03.139923 140070692116288 submission_runner.py:408] Time since start: 61800.26s, 	Step: 177096, 	{'train/accuracy': 0.9596021771430969, 'train/loss': 0.1510808914899826, 'validation/accuracy': 0.7536599636077881, 'validation/loss': 1.0556384325027466, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.812854528427124, 'test/num_examples': 10000, 'score': 59710.533801317215, 'total_duration': 61800.26037359238, 'accumulated_submission_time': 59710.533801317215, 'accumulated_eval_time': 2078.398533344269, 'accumulated_logging_time': 5.148442506790161}
I0203 14:24:03.192471 139907729164032 logging_writer.py:48] [177096] accumulated_eval_time=2078.398533, accumulated_logging_time=5.148443, accumulated_submission_time=59710.533801, global_step=177096, preemption_count=0, score=59710.533801, test/accuracy=0.631000, test/loss=1.812855, test/num_examples=10000, total_duration=61800.260374, train/accuracy=0.959602, train/loss=0.151081, validation/accuracy=0.753660, validation/loss=1.055638, validation/num_examples=50000
I0203 14:24:04.900583 139907737556736 logging_writer.py:48] [177100] global_step=177100, grad_norm=4.019499778747559, loss=0.5925611853599548
I0203 14:24:38.556337 139907729164032 logging_writer.py:48] [177200] global_step=177200, grad_norm=4.96654748916626, loss=0.699103593826294
I0203 14:25:12.324703 139907737556736 logging_writer.py:48] [177300] global_step=177300, grad_norm=4.556619167327881, loss=0.6033247709274292
I0203 14:25:45.993099 139907729164032 logging_writer.py:48] [177400] global_step=177400, grad_norm=4.6309075355529785, loss=0.6604628562927246
I0203 14:26:19.740365 139907737556736 logging_writer.py:48] [177500] global_step=177500, grad_norm=4.23210334777832, loss=0.5685352683067322
I0203 14:26:53.414731 139907729164032 logging_writer.py:48] [177600] global_step=177600, grad_norm=4.586182117462158, loss=0.6671848297119141
I0203 14:27:27.161210 139907737556736 logging_writer.py:48] [177700] global_step=177700, grad_norm=4.962828636169434, loss=0.7064694166183472
I0203 14:28:00.830391 139907729164032 logging_writer.py:48] [177800] global_step=177800, grad_norm=4.530780792236328, loss=0.6339924335479736
I0203 14:28:34.529568 139907737556736 logging_writer.py:48] [177900] global_step=177900, grad_norm=4.835794925689697, loss=0.6144822835922241
I0203 14:29:08.181842 139907729164032 logging_writer.py:48] [178000] global_step=178000, grad_norm=4.268507957458496, loss=0.6170774698257446
I0203 14:29:41.894268 139907737556736 logging_writer.py:48] [178100] global_step=178100, grad_norm=4.482866287231445, loss=0.5619825720787048
I0203 14:30:15.555920 139907729164032 logging_writer.py:48] [178200] global_step=178200, grad_norm=4.232826232910156, loss=0.5812721848487854
I0203 14:30:49.268420 139907737556736 logging_writer.py:48] [178300] global_step=178300, grad_norm=4.392710208892822, loss=0.6299229860305786
I0203 14:31:23.035285 139907729164032 logging_writer.py:48] [178400] global_step=178400, grad_norm=4.431349754333496, loss=0.6441064476966858
I0203 14:31:56.724023 139907737556736 logging_writer.py:48] [178500] global_step=178500, grad_norm=4.6223368644714355, loss=0.6577300429344177
I0203 14:32:30.377264 139907729164032 logging_writer.py:48] [178600] global_step=178600, grad_norm=4.388505935668945, loss=0.6385697722434998
I0203 14:32:33.221811 140070692116288 spec.py:321] Evaluating on the training split.
I0203 14:32:39.553994 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 14:32:48.248533 140070692116288 spec.py:349] Evaluating on the test split.
I0203 14:32:50.920866 140070692116288 submission_runner.py:408] Time since start: 62328.04s, 	Step: 178610, 	{'train/accuracy': 0.95902419090271, 'train/loss': 0.1536361575126648, 'validation/accuracy': 0.7538599967956543, 'validation/loss': 1.0557180643081665, 'validation/num_examples': 50000, 'test/accuracy': 0.6331000328063965, 'test/loss': 1.810882329940796, 'test/num_examples': 10000, 'score': 60220.49709105492, 'total_duration': 62328.04130482674, 'accumulated_submission_time': 60220.49709105492, 'accumulated_eval_time': 2096.097542285919, 'accumulated_logging_time': 5.214344024658203}
I0203 14:32:50.966311 139907712378624 logging_writer.py:48] [178610] accumulated_eval_time=2096.097542, accumulated_logging_time=5.214344, accumulated_submission_time=60220.497091, global_step=178610, preemption_count=0, score=60220.497091, test/accuracy=0.633100, test/loss=1.810882, test/num_examples=10000, total_duration=62328.041305, train/accuracy=0.959024, train/loss=0.153636, validation/accuracy=0.753860, validation/loss=1.055718, validation/num_examples=50000
I0203 14:33:21.639465 139907720771328 logging_writer.py:48] [178700] global_step=178700, grad_norm=4.2220635414123535, loss=0.5890305638313293
I0203 14:33:55.324653 139907712378624 logging_writer.py:48] [178800] global_step=178800, grad_norm=4.621462345123291, loss=0.667152464389801
I0203 14:34:29.026022 139907720771328 logging_writer.py:48] [178900] global_step=178900, grad_norm=4.654928207397461, loss=0.632158100605011
I0203 14:35:02.693036 139907712378624 logging_writer.py:48] [179000] global_step=179000, grad_norm=4.730133056640625, loss=0.6483508348464966
I0203 14:35:36.425511 139907720771328 logging_writer.py:48] [179100] global_step=179100, grad_norm=4.372905254364014, loss=0.5217625498771667
I0203 14:36:10.126504 139907712378624 logging_writer.py:48] [179200] global_step=179200, grad_norm=4.545777320861816, loss=0.6130741238594055
I0203 14:36:43.812985 139907720771328 logging_writer.py:48] [179300] global_step=179300, grad_norm=4.383504867553711, loss=0.5596386790275574
I0203 14:37:17.491877 139907712378624 logging_writer.py:48] [179400] global_step=179400, grad_norm=4.830682754516602, loss=0.6560161709785461
I0203 14:37:51.379066 139907720771328 logging_writer.py:48] [179500] global_step=179500, grad_norm=4.877913475036621, loss=0.655259370803833
I0203 14:38:25.152082 139907712378624 logging_writer.py:48] [179600] global_step=179600, grad_norm=4.521589279174805, loss=0.6772494912147522
I0203 14:38:58.843290 139907720771328 logging_writer.py:48] [179700] global_step=179700, grad_norm=4.4801554679870605, loss=0.6058127284049988
I0203 14:39:32.573693 139907712378624 logging_writer.py:48] [179800] global_step=179800, grad_norm=4.255950927734375, loss=0.6319612264633179
I0203 14:40:06.297745 139907720771328 logging_writer.py:48] [179900] global_step=179900, grad_norm=4.365452289581299, loss=0.6729581356048584
I0203 14:40:39.989403 139907712378624 logging_writer.py:48] [180000] global_step=180000, grad_norm=4.1138129234313965, loss=0.5290623903274536
I0203 14:41:13.720382 139907720771328 logging_writer.py:48] [180100] global_step=180100, grad_norm=4.405206203460693, loss=0.5876495242118835
I0203 14:41:20.949349 140070692116288 spec.py:321] Evaluating on the training split.
I0203 14:41:27.269930 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 14:41:36.033247 140070692116288 spec.py:349] Evaluating on the test split.
I0203 14:41:38.620039 140070692116288 submission_runner.py:408] Time since start: 62855.74s, 	Step: 180123, 	{'train/accuracy': 0.9585060477256775, 'train/loss': 0.15022099018096924, 'validation/accuracy': 0.7537199854850769, 'validation/loss': 1.0573610067367554, 'validation/num_examples': 50000, 'test/accuracy': 0.6345000267028809, 'test/loss': 1.8141127824783325, 'test/num_examples': 10000, 'score': 60730.41804790497, 'total_duration': 62855.74048471451, 'accumulated_submission_time': 60730.41804790497, 'accumulated_eval_time': 2113.768192052841, 'accumulated_logging_time': 5.269103527069092}
I0203 14:41:38.666722 139907737556736 logging_writer.py:48] [180123] accumulated_eval_time=2113.768192, accumulated_logging_time=5.269104, accumulated_submission_time=60730.418048, global_step=180123, preemption_count=0, score=60730.418048, test/accuracy=0.634500, test/loss=1.814113, test/num_examples=10000, total_duration=62855.740485, train/accuracy=0.958506, train/loss=0.150221, validation/accuracy=0.753720, validation/loss=1.057361, validation/num_examples=50000
I0203 14:42:04.964649 139907762734848 logging_writer.py:48] [180200] global_step=180200, grad_norm=4.6120285987854, loss=0.613453209400177
I0203 14:42:38.628635 139907737556736 logging_writer.py:48] [180300] global_step=180300, grad_norm=4.67091178894043, loss=0.6700389385223389
I0203 14:43:12.342993 139907762734848 logging_writer.py:48] [180400] global_step=180400, grad_norm=4.845345973968506, loss=0.6856933832168579
I0203 14:43:46.087257 139907737556736 logging_writer.py:48] [180500] global_step=180500, grad_norm=4.4231672286987305, loss=0.642034113407135
I0203 14:44:19.934410 139907762734848 logging_writer.py:48] [180600] global_step=180600, grad_norm=4.124637126922607, loss=0.5788629055023193
I0203 14:44:53.632416 139907737556736 logging_writer.py:48] [180700] global_step=180700, grad_norm=4.745646953582764, loss=0.6299853920936584
I0203 14:45:27.354322 139907762734848 logging_writer.py:48] [180800] global_step=180800, grad_norm=4.933041572570801, loss=0.6542516350746155
I0203 14:46:01.023467 139907737556736 logging_writer.py:48] [180900] global_step=180900, grad_norm=4.506235599517822, loss=0.6740860342979431
I0203 14:46:34.728990 139907762734848 logging_writer.py:48] [181000] global_step=181000, grad_norm=4.454658031463623, loss=0.6378944516181946
I0203 14:47:08.358141 139907737556736 logging_writer.py:48] [181100] global_step=181100, grad_norm=4.577864646911621, loss=0.6863461136817932
I0203 14:47:42.058734 139907762734848 logging_writer.py:48] [181200] global_step=181200, grad_norm=5.074002265930176, loss=0.6949777603149414
I0203 14:48:15.797061 139907737556736 logging_writer.py:48] [181300] global_step=181300, grad_norm=4.693070411682129, loss=0.56260085105896
I0203 14:48:49.492155 139907762734848 logging_writer.py:48] [181400] global_step=181400, grad_norm=4.649233818054199, loss=0.6623567938804626
I0203 14:49:23.208703 139907737556736 logging_writer.py:48] [181500] global_step=181500, grad_norm=4.679030418395996, loss=0.6411753296852112
I0203 14:49:56.944570 139907762734848 logging_writer.py:48] [181600] global_step=181600, grad_norm=4.391022682189941, loss=0.6475560069084167
I0203 14:50:08.888106 140070692116288 spec.py:321] Evaluating on the training split.
I0203 14:50:15.168686 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 14:50:23.574048 140070692116288 spec.py:349] Evaluating on the test split.
I0203 14:50:26.305991 140070692116288 submission_runner.py:408] Time since start: 63383.43s, 	Step: 181637, 	{'train/accuracy': 0.9604192972183228, 'train/loss': 0.14689314365386963, 'validation/accuracy': 0.7546799778938293, 'validation/loss': 1.0567476749420166, 'validation/num_examples': 50000, 'test/accuracy': 0.633400022983551, 'test/loss': 1.8171926736831665, 'test/num_examples': 10000, 'score': 61240.57641124725, 'total_duration': 63383.426446676254, 'accumulated_submission_time': 61240.57641124725, 'accumulated_eval_time': 2131.186047077179, 'accumulated_logging_time': 5.325862407684326}
I0203 14:50:26.360813 139907720771328 logging_writer.py:48] [181637] accumulated_eval_time=2131.186047, accumulated_logging_time=5.325862, accumulated_submission_time=61240.576411, global_step=181637, preemption_count=0, score=61240.576411, test/accuracy=0.633400, test/loss=1.817193, test/num_examples=10000, total_duration=63383.426447, train/accuracy=0.960419, train/loss=0.146893, validation/accuracy=0.754680, validation/loss=1.056748, validation/num_examples=50000
I0203 14:50:47.931106 139907729164032 logging_writer.py:48] [181700] global_step=181700, grad_norm=4.556295394897461, loss=0.6669690608978271
I0203 14:51:21.744054 139907720771328 logging_writer.py:48] [181800] global_step=181800, grad_norm=4.594827651977539, loss=0.6531884074211121
I0203 14:51:55.438674 139907729164032 logging_writer.py:48] [181900] global_step=181900, grad_norm=4.439070701599121, loss=0.6176523566246033
I0203 14:52:29.125912 139907720771328 logging_writer.py:48] [182000] global_step=182000, grad_norm=4.368403911590576, loss=0.6430066227912903
I0203 14:53:02.783814 139907729164032 logging_writer.py:48] [182100] global_step=182100, grad_norm=4.6138715744018555, loss=0.5993149280548096
I0203 14:53:36.465687 139907720771328 logging_writer.py:48] [182200] global_step=182200, grad_norm=4.744283676147461, loss=0.6027458906173706
I0203 14:54:10.229582 139907729164032 logging_writer.py:48] [182300] global_step=182300, grad_norm=4.582229137420654, loss=0.6327320337295532
I0203 14:54:43.896395 139907720771328 logging_writer.py:48] [182400] global_step=182400, grad_norm=4.581273555755615, loss=0.6260217428207397
I0203 14:55:17.589975 139907729164032 logging_writer.py:48] [182500] global_step=182500, grad_norm=4.374210357666016, loss=0.6101171970367432
I0203 14:55:51.324007 139907720771328 logging_writer.py:48] [182600] global_step=182600, grad_norm=3.9878368377685547, loss=0.5549427270889282
I0203 14:56:25.005042 139907729164032 logging_writer.py:48] [182700] global_step=182700, grad_norm=4.721949577331543, loss=0.6883252263069153
I0203 14:56:58.655356 139907720771328 logging_writer.py:48] [182800] global_step=182800, grad_norm=4.4599785804748535, loss=0.5836948156356812
I0203 14:57:32.438603 139907729164032 logging_writer.py:48] [182900] global_step=182900, grad_norm=4.454676628112793, loss=0.6224523186683655
I0203 14:58:06.156093 139907720771328 logging_writer.py:48] [183000] global_step=183000, grad_norm=4.561045169830322, loss=0.6422064900398254
I0203 14:58:39.853348 139907729164032 logging_writer.py:48] [183100] global_step=183100, grad_norm=4.904818534851074, loss=0.6488972902297974
I0203 14:58:56.490611 140070692116288 spec.py:321] Evaluating on the training split.
I0203 14:59:03.033972 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 14:59:11.731040 140070692116288 spec.py:349] Evaluating on the test split.
I0203 14:59:14.416201 140070692116288 submission_runner.py:408] Time since start: 63911.54s, 	Step: 183151, 	{'train/accuracy': 0.9615154266357422, 'train/loss': 0.14372612535953522, 'validation/accuracy': 0.7544800043106079, 'validation/loss': 1.0552575588226318, 'validation/num_examples': 50000, 'test/accuracy': 0.6343000531196594, 'test/loss': 1.8140968084335327, 'test/num_examples': 10000, 'score': 61750.644112825394, 'total_duration': 63911.53665685654, 'accumulated_submission_time': 61750.644112825394, 'accumulated_eval_time': 2149.111617088318, 'accumulated_logging_time': 5.389604806900024}
I0203 14:59:14.458310 139907712378624 logging_writer.py:48] [183151] accumulated_eval_time=2149.111617, accumulated_logging_time=5.389605, accumulated_submission_time=61750.644113, global_step=183151, preemption_count=0, score=61750.644113, test/accuracy=0.634300, test/loss=1.814097, test/num_examples=10000, total_duration=63911.536657, train/accuracy=0.961515, train/loss=0.143726, validation/accuracy=0.754480, validation/loss=1.055258, validation/num_examples=50000
I0203 14:59:31.302323 139907737556736 logging_writer.py:48] [183200] global_step=183200, grad_norm=4.696651935577393, loss=0.6157653331756592
I0203 15:00:04.984185 139907712378624 logging_writer.py:48] [183300] global_step=183300, grad_norm=4.385146617889404, loss=0.5970209240913391
I0203 15:00:38.653774 139907737556736 logging_writer.py:48] [183400] global_step=183400, grad_norm=4.325351238250732, loss=0.5734288096427917
I0203 15:01:12.335713 139907712378624 logging_writer.py:48] [183500] global_step=183500, grad_norm=4.1601643562316895, loss=0.5908575654029846
I0203 15:01:46.016445 139907737556736 logging_writer.py:48] [183600] global_step=183600, grad_norm=4.7292022705078125, loss=0.6282645463943481
I0203 15:02:19.711974 139907712378624 logging_writer.py:48] [183700] global_step=183700, grad_norm=4.114976406097412, loss=0.5951050519943237
I0203 15:02:53.458891 139907737556736 logging_writer.py:48] [183800] global_step=183800, grad_norm=4.753674030303955, loss=0.672245442867279
I0203 15:03:27.144050 139907712378624 logging_writer.py:48] [183900] global_step=183900, grad_norm=4.343578815460205, loss=0.5866787433624268
I0203 15:04:00.947124 139907737556736 logging_writer.py:48] [184000] global_step=184000, grad_norm=4.603428840637207, loss=0.6492385864257812
I0203 15:04:34.629996 139907712378624 logging_writer.py:48] [184100] global_step=184100, grad_norm=4.547234535217285, loss=0.6365734934806824
I0203 15:05:08.382560 139907737556736 logging_writer.py:48] [184200] global_step=184200, grad_norm=4.847785472869873, loss=0.6271952986717224
I0203 15:05:42.082029 139907712378624 logging_writer.py:48] [184300] global_step=184300, grad_norm=4.37076473236084, loss=0.6238563656806946
I0203 15:06:15.742354 139907737556736 logging_writer.py:48] [184400] global_step=184400, grad_norm=4.6113786697387695, loss=0.5829914212226868
I0203 15:06:49.430906 139907712378624 logging_writer.py:48] [184500] global_step=184500, grad_norm=4.415351867675781, loss=0.6034784913063049
I0203 15:07:23.193868 139907737556736 logging_writer.py:48] [184600] global_step=184600, grad_norm=4.40985631942749, loss=0.5970813632011414
I0203 15:07:44.575940 140070692116288 spec.py:321] Evaluating on the training split.
I0203 15:07:50.890340 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 15:07:59.347113 140070692116288 spec.py:349] Evaluating on the test split.
I0203 15:08:02.020902 140070692116288 submission_runner.py:408] Time since start: 64439.14s, 	Step: 184665, 	{'train/accuracy': 0.9613161683082581, 'train/loss': 0.14664237201213837, 'validation/accuracy': 0.7540599703788757, 'validation/loss': 1.0541568994522095, 'validation/num_examples': 50000, 'test/accuracy': 0.6353000402450562, 'test/loss': 1.8131952285766602, 'test/num_examples': 10000, 'score': 62260.7015068531, 'total_duration': 64439.141348838806, 'accumulated_submission_time': 62260.7015068531, 'accumulated_eval_time': 2166.556547164917, 'accumulated_logging_time': 5.439829349517822}
I0203 15:08:02.087382 139907703985920 logging_writer.py:48] [184665] accumulated_eval_time=2166.556547, accumulated_logging_time=5.439829, accumulated_submission_time=62260.701507, global_step=184665, preemption_count=0, score=62260.701507, test/accuracy=0.635300, test/loss=1.813195, test/num_examples=10000, total_duration=64439.141349, train/accuracy=0.961316, train/loss=0.146642, validation/accuracy=0.754060, validation/loss=1.054157, validation/num_examples=50000
I0203 15:08:14.216160 139907729164032 logging_writer.py:48] [184700] global_step=184700, grad_norm=4.4713921546936035, loss=0.5961873531341553
I0203 15:08:47.908943 139907703985920 logging_writer.py:48] [184800] global_step=184800, grad_norm=4.8511247634887695, loss=0.6158636808395386
I0203 15:09:21.580739 139907729164032 logging_writer.py:48] [184900] global_step=184900, grad_norm=4.54312801361084, loss=0.6902498006820679
I0203 15:09:55.295659 139907703985920 logging_writer.py:48] [185000] global_step=185000, grad_norm=4.804392337799072, loss=0.6111026406288147
I0203 15:10:29.003131 139907729164032 logging_writer.py:48] [185100] global_step=185100, grad_norm=4.362864971160889, loss=0.571815013885498
I0203 15:11:02.845894 139907703985920 logging_writer.py:48] [185200] global_step=185200, grad_norm=4.15739107131958, loss=0.5981359481811523
I0203 15:11:36.525890 139907729164032 logging_writer.py:48] [185300] global_step=185300, grad_norm=4.3870463371276855, loss=0.573733389377594
I0203 15:12:10.244514 139907703985920 logging_writer.py:48] [185400] global_step=185400, grad_norm=4.547386646270752, loss=0.6026442646980286
I0203 15:12:43.943675 139907729164032 logging_writer.py:48] [185500] global_step=185500, grad_norm=4.303447246551514, loss=0.5829023122787476
I0203 15:13:17.662444 139907703985920 logging_writer.py:48] [185600] global_step=185600, grad_norm=4.3157782554626465, loss=0.7009779214859009
I0203 15:13:51.388462 139907729164032 logging_writer.py:48] [185700] global_step=185700, grad_norm=4.581582069396973, loss=0.6362630724906921
I0203 15:14:25.101833 139907703985920 logging_writer.py:48] [185800] global_step=185800, grad_norm=4.133626937866211, loss=0.6122630834579468
I0203 15:14:58.772336 139907729164032 logging_writer.py:48] [185900] global_step=185900, grad_norm=4.331816673278809, loss=0.6064491868019104
I0203 15:15:32.463341 139907703985920 logging_writer.py:48] [186000] global_step=186000, grad_norm=4.083336353302002, loss=0.6231319904327393
I0203 15:16:06.138269 139907729164032 logging_writer.py:48] [186100] global_step=186100, grad_norm=4.29386568069458, loss=0.5943943858146667
I0203 15:16:32.198149 140070692116288 spec.py:321] Evaluating on the training split.
I0203 15:16:38.517852 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 15:16:46.889458 140070692116288 spec.py:349] Evaluating on the test split.
I0203 15:16:49.524037 140070692116288 submission_runner.py:408] Time since start: 64966.64s, 	Step: 186179, 	{'train/accuracy': 0.9597018361091614, 'train/loss': 0.14918778836727142, 'validation/accuracy': 0.7537399530410767, 'validation/loss': 1.0555750131607056, 'validation/num_examples': 50000, 'test/accuracy': 0.634600043296814, 'test/loss': 1.8140379190444946, 'test/num_examples': 10000, 'score': 62770.74982833862, 'total_duration': 64966.6444914341, 'accumulated_submission_time': 62770.74982833862, 'accumulated_eval_time': 2183.8824162483215, 'accumulated_logging_time': 5.5159690380096436}
I0203 15:16:49.574093 139907703985920 logging_writer.py:48] [186179] accumulated_eval_time=2183.882416, accumulated_logging_time=5.515969, accumulated_submission_time=62770.749828, global_step=186179, preemption_count=0, score=62770.749828, test/accuracy=0.634600, test/loss=1.814038, test/num_examples=10000, total_duration=64966.644491, train/accuracy=0.959702, train/loss=0.149188, validation/accuracy=0.753740, validation/loss=1.055575, validation/num_examples=50000
I0203 15:16:56.985593 139907712378624 logging_writer.py:48] [186200] global_step=186200, grad_norm=4.404409885406494, loss=0.6628541946411133
I0203 15:17:30.757869 139907703985920 logging_writer.py:48] [186300] global_step=186300, grad_norm=4.454769134521484, loss=0.6091709136962891
I0203 15:18:04.463941 139907712378624 logging_writer.py:48] [186400] global_step=186400, grad_norm=4.615877151489258, loss=0.6047577261924744
I0203 15:18:38.151311 139907703985920 logging_writer.py:48] [186500] global_step=186500, grad_norm=4.378769874572754, loss=0.599116325378418
I0203 15:19:11.897319 139907712378624 logging_writer.py:48] [186600] global_step=186600, grad_norm=4.40711784362793, loss=0.6475784182548523
I0203 15:19:33.635841 140070692116288 spec.py:321] Evaluating on the training split.
I0203 15:19:39.929845 140070692116288 spec.py:333] Evaluating on the validation split.
I0203 15:19:48.371561 140070692116288 spec.py:349] Evaluating on the test split.
I0203 15:19:50.883152 140070692116288 submission_runner.py:408] Time since start: 65148.00s, 	Step: 186666, 	{'train/accuracy': 0.9602798223495483, 'train/loss': 0.14576058089733124, 'validation/accuracy': 0.75409996509552, 'validation/loss': 1.054492473602295, 'validation/num_examples': 50000, 'test/accuracy': 0.6342000365257263, 'test/loss': 1.8139417171478271, 'test/num_examples': 10000, 'score': 62934.7845287323, 'total_duration': 65148.003600120544, 'accumulated_submission_time': 62934.7845287323, 'accumulated_eval_time': 2201.129693508148, 'accumulated_logging_time': 5.575985908508301}
I0203 15:19:50.936764 139907754342144 logging_writer.py:48] [186666] accumulated_eval_time=2201.129694, accumulated_logging_time=5.575986, accumulated_submission_time=62934.784529, global_step=186666, preemption_count=0, score=62934.784529, test/accuracy=0.634200, test/loss=1.813942, test/num_examples=10000, total_duration=65148.003600, train/accuracy=0.960280, train/loss=0.145761, validation/accuracy=0.754100, validation/loss=1.054492, validation/num_examples=50000
I0203 15:19:50.980590 139907762734848 logging_writer.py:48] [186666] global_step=186666, preemption_count=0, score=62934.784529
I0203 15:19:51.273949 140070692116288 checkpoints.py:490] Saving checkpoint at step: 186666
I0203 15:19:52.450385 140070692116288 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_3/imagenet_resnet_jax/trial_5/checkpoint_186666
I0203 15:19:52.471183 140070692116288 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_3/imagenet_resnet_jax/trial_5/checkpoint_186666.
I0203 15:19:53.145097 140070692116288 submission_runner.py:583] Tuning trial 5/5
I0203 15:19:53.145329 140070692116288 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0203 15:19:53.154994 140070692116288 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0010961415246129036, 'train/loss': 6.911187648773193, 'validation/accuracy': 0.0011199999134987593, 'validation/loss': 6.912059783935547, 'validation/num_examples': 50000, 'test/accuracy': 0.0009000000427477062, 'test/loss': 6.912177562713623, 'test/num_examples': 10000, 'score': 32.42240762710571, 'total_duration': 50.21418762207031, 'accumulated_submission_time': 32.42240762710571, 'accumulated_eval_time': 17.791696310043335, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1508, {'train/accuracy': 0.15250319242477417, 'train/loss': 4.490919589996338, 'validation/accuracy': 0.13887999951839447, 'validation/loss': 4.628087520599365, 'validation/num_examples': 50000, 'test/accuracy': 0.10010000318288803, 'test/loss': 5.178818702697754, 'test/num_examples': 10000, 'score': 542.6018404960632, 'total_duration': 578.0186469554901, 'accumulated_submission_time': 542.6018404960632, 'accumulated_eval_time': 35.341097831726074, 'accumulated_logging_time': 0.022499561309814453, 'global_step': 1508, 'preemption_count': 0}), (3015, {'train/accuracy': 0.34781569242477417, 'train/loss': 2.9913976192474365, 'validation/accuracy': 0.3228600025177002, 'validation/loss': 3.1558074951171875, 'validation/num_examples': 50000, 'test/accuracy': 0.2410000115633011, 'test/loss': 3.804908037185669, 'test/num_examples': 10000, 'score': 1052.5842320919037, 'total_duration': 1105.7418451309204, 'accumulated_submission_time': 1052.5842320919037, 'accumulated_eval_time': 52.99773406982422, 'accumulated_logging_time': 0.05348396301269531, 'global_step': 3015, 'preemption_count': 0}), (4524, {'train/accuracy': 0.45719069242477417, 'train/loss': 2.4114012718200684, 'validation/accuracy': 0.42965999245643616, 'validation/loss': 2.576097249984741, 'validation/num_examples': 50000, 'test/accuracy': 0.32270002365112305, 'test/loss': 3.2601864337921143, 'test/num_examples': 10000, 'score': 1562.7190339565277, 'total_duration': 1633.5204920768738, 'accumulated_submission_time': 1562.7190339565277, 'accumulated_eval_time': 70.55705571174622, 'accumulated_logging_time': 0.08645176887512207, 'global_step': 4524, 'preemption_count': 0}), (6033, {'train/accuracy': 0.5725047588348389, 'train/loss': 1.785803198814392, 'validation/accuracy': 0.5069999694824219, 'validation/loss': 2.1507537364959717, 'validation/num_examples': 50000, 'test/accuracy': 0.38430002331733704, 'test/loss': 2.940765380859375, 'test/num_examples': 10000, 'score': 2072.789868593216, 'total_duration': 2161.4965052604675, 'accumulated_submission_time': 2072.789868593216, 'accumulated_eval_time': 88.38283014297485, 'accumulated_logging_time': 0.11400866508483887, 'global_step': 6033, 'preemption_count': 0}), (7544, {'train/accuracy': 0.5734614133834839, 'train/loss': 1.7597079277038574, 'validation/accuracy': 0.5224800109863281, 'validation/loss': 2.043307065963745, 'validation/num_examples': 50000, 'test/accuracy': 0.3979000151157379, 'test/loss': 2.8477373123168945, 'test/num_examples': 10000, 'score': 2582.7939958572388, 'total_duration': 2688.8509685993195, 'accumulated_submission_time': 2582.7939958572388, 'accumulated_eval_time': 105.65375065803528, 'accumulated_logging_time': 0.1418612003326416, 'global_step': 7544, 'preemption_count': 0}), (9055, {'train/accuracy': 0.6027184128761292, 'train/loss': 1.6339234113693237, 'validation/accuracy': 0.5541399717330933, 'validation/loss': 1.8997522592544556, 'validation/num_examples': 50000, 'test/accuracy': 0.4310000240802765, 'test/loss': 2.634711265563965, 'test/num_examples': 10000, 'score': 3092.727585554123, 'total_duration': 3216.351620197296, 'accumulated_submission_time': 3092.727585554123, 'accumulated_eval_time': 123.14149379730225, 'accumulated_logging_time': 0.1691446304321289, 'global_step': 9055, 'preemption_count': 0}), (10567, {'train/accuracy': 0.6264349222183228, 'train/loss': 1.537442684173584, 'validation/accuracy': 0.5761199593544006, 'validation/loss': 1.796309471130371, 'validation/num_examples': 50000, 'test/accuracy': 0.44350001215934753, 'test/loss': 2.5696029663085938, 'test/num_examples': 10000, 'score': 3602.7257010936737, 'total_duration': 3744.0334181785583, 'accumulated_submission_time': 3602.7257010936737, 'accumulated_eval_time': 140.74559259414673, 'accumulated_logging_time': 0.19663071632385254, 'global_step': 10567, 'preemption_count': 0}), (12079, {'train/accuracy': 0.6282485723495483, 'train/loss': 1.5030643939971924, 'validation/accuracy': 0.5816199779510498, 'validation/loss': 1.767730951309204, 'validation/num_examples': 50000, 'test/accuracy': 0.4504000246524811, 'test/loss': 2.5434465408325195, 'test/num_examples': 10000, 'score': 4112.730927944183, 'total_duration': 4272.026992321014, 'accumulated_submission_time': 4112.730927944183, 'accumulated_eval_time': 158.65228414535522, 'accumulated_logging_time': 0.22556233406066895, 'global_step': 12079, 'preemption_count': 0}), (13591, {'train/accuracy': 0.644949734210968, 'train/loss': 1.4343558549880981, 'validation/accuracy': 0.5983200073242188, 'validation/loss': 1.6884334087371826, 'validation/num_examples': 50000, 'test/accuracy': 0.46790000796318054, 'test/loss': 2.4460861682891846, 'test/num_examples': 10000, 'score': 4622.837368488312, 'total_duration': 4799.799918413162, 'accumulated_submission_time': 4622.837368488312, 'accumulated_eval_time': 176.235848903656, 'accumulated_logging_time': 0.2562377452850342, 'global_step': 13591, 'preemption_count': 0}), (15103, {'train/accuracy': 0.6813815236091614, 'train/loss': 1.2665554285049438, 'validation/accuracy': 0.6004999876022339, 'validation/loss': 1.6677234172821045, 'validation/num_examples': 50000, 'test/accuracy': 0.4739000201225281, 'test/loss': 2.416137933731079, 'test/num_examples': 10000, 'score': 5132.94886469841, 'total_duration': 5327.430913209915, 'accumulated_submission_time': 5132.94886469841, 'accumulated_eval_time': 193.6725881099701, 'accumulated_logging_time': 0.2862663269042969, 'global_step': 15103, 'preemption_count': 0}), (16615, {'train/accuracy': 0.666015625, 'train/loss': 1.3178972005844116, 'validation/accuracy': 0.6035000085830688, 'validation/loss': 1.6508041620254517, 'validation/num_examples': 50000, 'test/accuracy': 0.4627000093460083, 'test/loss': 2.459235191345215, 'test/num_examples': 10000, 'score': 5642.935264825821, 'total_duration': 5855.89298915863, 'accumulated_submission_time': 5642.935264825821, 'accumulated_eval_time': 212.0672664642334, 'accumulated_logging_time': 0.3150801658630371, 'global_step': 16615, 'preemption_count': 0}), (18127, {'train/accuracy': 0.6575454473495483, 'train/loss': 1.3667640686035156, 'validation/accuracy': 0.6034199595451355, 'validation/loss': 1.652487874031067, 'validation/num_examples': 50000, 'test/accuracy': 0.4727000296115875, 'test/loss': 2.390066385269165, 'test/num_examples': 10000, 'score': 6153.063757181168, 'total_duration': 6383.643333911896, 'accumulated_submission_time': 6153.063757181168, 'accumulated_eval_time': 229.60523438453674, 'accumulated_logging_time': 0.34566211700439453, 'global_step': 18127, 'preemption_count': 0}), (19639, {'train/accuracy': 0.6714365482330322, 'train/loss': 1.3168739080429077, 'validation/accuracy': 0.6110599637031555, 'validation/loss': 1.6108882427215576, 'validation/num_examples': 50000, 'test/accuracy': 0.4854000210762024, 'test/loss': 2.3747594356536865, 'test/num_examples': 10000, 'score': 6663.034357786179, 'total_duration': 6911.30079293251, 'accumulated_submission_time': 6663.034357786179, 'accumulated_eval_time': 247.20907497406006, 'accumulated_logging_time': 0.37534093856811523, 'global_step': 19639, 'preemption_count': 0}), (21151, {'train/accuracy': 0.6639229655265808, 'train/loss': 1.3415753841400146, 'validation/accuracy': 0.6109600067138672, 'validation/loss': 1.6230710744857788, 'validation/num_examples': 50000, 'test/accuracy': 0.492000013589859, 'test/loss': 2.333085536956787, 'test/num_examples': 10000, 'score': 7173.039954662323, 'total_duration': 7438.92605638504, 'accumulated_submission_time': 7173.039954662323, 'accumulated_eval_time': 264.745756149292, 'accumulated_logging_time': 0.4052441120147705, 'global_step': 21151, 'preemption_count': 0}), (22664, {'train/accuracy': 0.6608737111091614, 'train/loss': 1.3586628437042236, 'validation/accuracy': 0.6041399836540222, 'validation/loss': 1.640969157218933, 'validation/num_examples': 50000, 'test/accuracy': 0.47350001335144043, 'test/loss': 2.4242477416992188, 'test/num_examples': 10000, 'score': 7683.255749940872, 'total_duration': 7966.688236236572, 'accumulated_submission_time': 7683.255749940872, 'accumulated_eval_time': 282.2081139087677, 'accumulated_logging_time': 0.43669915199279785, 'global_step': 22664, 'preemption_count': 0}), (24177, {'train/accuracy': 0.6926219463348389, 'train/loss': 1.2101314067840576, 'validation/accuracy': 0.6140999794006348, 'validation/loss': 1.6118823289871216, 'validation/num_examples': 50000, 'test/accuracy': 0.4853000342845917, 'test/loss': 2.374408483505249, 'test/num_examples': 10000, 'score': 8193.423243284225, 'total_duration': 8494.462911128998, 'accumulated_submission_time': 8193.423243284225, 'accumulated_eval_time': 299.73340010643005, 'accumulated_logging_time': 0.4670734405517578, 'global_step': 24177, 'preemption_count': 0}), (25690, {'train/accuracy': 0.6783721446990967, 'train/loss': 1.2624869346618652, 'validation/accuracy': 0.6137799620628357, 'validation/loss': 1.6312508583068848, 'validation/num_examples': 50000, 'test/accuracy': 0.4904000163078308, 'test/loss': 2.407479763031006, 'test/num_examples': 10000, 'score': 8703.385436296463, 'total_duration': 9022.43826174736, 'accumulated_submission_time': 8703.385436296463, 'accumulated_eval_time': 317.66002774238586, 'accumulated_logging_time': 0.5013530254364014, 'global_step': 25690, 'preemption_count': 0}), (27203, {'train/accuracy': 0.6784717440605164, 'train/loss': 1.2687817811965942, 'validation/accuracy': 0.6159200072288513, 'validation/loss': 1.5993962287902832, 'validation/num_examples': 50000, 'test/accuracy': 0.4914000332355499, 'test/loss': 2.33853816986084, 'test/num_examples': 10000, 'score': 9213.344497919083, 'total_duration': 9550.966819286346, 'accumulated_submission_time': 9213.344497919083, 'accumulated_eval_time': 336.1436400413513, 'accumulated_logging_time': 0.5356407165527344, 'global_step': 27203, 'preemption_count': 0}), (28716, {'train/accuracy': 0.6600964665412903, 'train/loss': 1.350205421447754, 'validation/accuracy': 0.6032999753952026, 'validation/loss': 1.6606773138046265, 'validation/num_examples': 50000, 'test/accuracy': 0.47850000858306885, 'test/loss': 2.4138495922088623, 'test/num_examples': 10000, 'score': 9723.491069793701, 'total_duration': 10078.793704748154, 'accumulated_submission_time': 9723.491069793701, 'accumulated_eval_time': 353.745454788208, 'accumulated_logging_time': 0.562096118927002, 'global_step': 28716, 'preemption_count': 0}), (30229, {'train/accuracy': 0.6486168503761292, 'train/loss': 1.4096747636795044, 'validation/accuracy': 0.5981199741363525, 'validation/loss': 1.6882946491241455, 'validation/num_examples': 50000, 'test/accuracy': 0.465800017118454, 'test/loss': 2.448777437210083, 'test/num_examples': 10000, 'score': 10233.69608449936, 'total_duration': 10606.674701690674, 'accumulated_submission_time': 10233.69608449936, 'accumulated_eval_time': 371.32841968536377, 'accumulated_logging_time': 0.6031649112701416, 'global_step': 30229, 'preemption_count': 0}), (31742, {'train/accuracy': 0.6831752061843872, 'train/loss': 1.2528233528137207, 'validation/accuracy': 0.6192399859428406, 'validation/loss': 1.6038295030593872, 'validation/num_examples': 50000, 'test/accuracy': 0.4930000305175781, 'test/loss': 2.3522725105285645, 'test/num_examples': 10000, 'score': 10743.68086385727, 'total_duration': 11134.072080612183, 'accumulated_submission_time': 10743.68086385727, 'accumulated_eval_time': 388.6515634059906, 'accumulated_logging_time': 0.6397063732147217, 'global_step': 31742, 'preemption_count': 0}), (33255, {'train/accuracy': 0.7012914419174194, 'train/loss': 1.1544904708862305, 'validation/accuracy': 0.6235600113868713, 'validation/loss': 1.5596383810043335, 'validation/num_examples': 50000, 'test/accuracy': 0.5005000233650208, 'test/loss': 2.294847011566162, 'test/num_examples': 10000, 'score': 11253.931037902832, 'total_duration': 11661.862969398499, 'accumulated_submission_time': 11253.931037902832, 'accumulated_eval_time': 406.1087462902069, 'accumulated_logging_time': 0.6711766719818115, 'global_step': 33255, 'preemption_count': 0}), (34769, {'train/accuracy': 0.7081871628761292, 'train/loss': 1.1398144960403442, 'validation/accuracy': 0.6377599835395813, 'validation/loss': 1.4949623346328735, 'validation/num_examples': 50000, 'test/accuracy': 0.5094000101089478, 'test/loss': 2.2475671768188477, 'test/num_examples': 10000, 'score': 11763.943603754044, 'total_duration': 12189.540555000305, 'accumulated_submission_time': 11763.943603754044, 'accumulated_eval_time': 423.68754959106445, 'accumulated_logging_time': 0.7041482925415039, 'global_step': 34769, 'preemption_count': 0}), (36281, {'train/accuracy': 0.6898915767669678, 'train/loss': 1.230826497077942, 'validation/accuracy': 0.6264599561691284, 'validation/loss': 1.5518196821212769, 'validation/num_examples': 50000, 'test/accuracy': 0.49080002307891846, 'test/loss': 2.300273895263672, 'test/num_examples': 10000, 'score': 12273.987482309341, 'total_duration': 12717.026733636856, 'accumulated_submission_time': 12273.987482309341, 'accumulated_eval_time': 441.04963994026184, 'accumulated_logging_time': 0.7320287227630615, 'global_step': 36281, 'preemption_count': 0}), (37795, {'train/accuracy': 0.6783322691917419, 'train/loss': 1.2680907249450684, 'validation/accuracy': 0.6167399883270264, 'validation/loss': 1.5898174047470093, 'validation/num_examples': 50000, 'test/accuracy': 0.4897000193595886, 'test/loss': 2.340038299560547, 'test/num_examples': 10000, 'score': 12784.045414686203, 'total_duration': 13244.865463733673, 'accumulated_submission_time': 12784.045414686203, 'accumulated_eval_time': 458.7441716194153, 'accumulated_logging_time': 0.7648324966430664, 'global_step': 37795, 'preemption_count': 0}), (39308, {'train/accuracy': 0.6907684803009033, 'train/loss': 1.2177668809890747, 'validation/accuracy': 0.6317999958992004, 'validation/loss': 1.5191069841384888, 'validation/num_examples': 50000, 'test/accuracy': 0.499500036239624, 'test/loss': 2.2920279502868652, 'test/num_examples': 10000, 'score': 13294.002183437347, 'total_duration': 13772.57915186882, 'accumulated_submission_time': 13294.002183437347, 'accumulated_eval_time': 476.41528153419495, 'accumulated_logging_time': 0.797590970993042, 'global_step': 39308, 'preemption_count': 0}), (40822, {'train/accuracy': 0.7167769074440002, 'train/loss': 1.1003592014312744, 'validation/accuracy': 0.6209999918937683, 'validation/loss': 1.567124843597412, 'validation/num_examples': 50000, 'test/accuracy': 0.49880000948905945, 'test/loss': 2.3107590675354004, 'test/num_examples': 10000, 'score': 13803.981996297836, 'total_duration': 14300.002563476562, 'accumulated_submission_time': 13803.981996297836, 'accumulated_eval_time': 493.7730107307434, 'accumulated_logging_time': 0.8306655883789062, 'global_step': 40822, 'preemption_count': 0}), (42335, {'train/accuracy': 0.7200653553009033, 'train/loss': 1.0819741487503052, 'validation/accuracy': 0.6363199949264526, 'validation/loss': 1.509085774421692, 'validation/num_examples': 50000, 'test/accuracy': 0.5099000334739685, 'test/loss': 2.241546869277954, 'test/num_examples': 10000, 'score': 14313.949091911316, 'total_duration': 14827.75766301155, 'accumulated_submission_time': 14313.949091911316, 'accumulated_eval_time': 511.47321367263794, 'accumulated_logging_time': 0.8653779029846191, 'global_step': 42335, 'preemption_count': 0}), (43848, {'train/accuracy': 0.6968072056770325, 'train/loss': 1.1697343587875366, 'validation/accuracy': 0.6294599771499634, 'validation/loss': 1.5310777425765991, 'validation/num_examples': 50000, 'test/accuracy': 0.5049000382423401, 'test/loss': 2.2731704711914062, 'test/num_examples': 10000, 'score': 14824.149604320526, 'total_duration': 15355.536323785782, 'accumulated_submission_time': 14824.149604320526, 'accumulated_eval_time': 528.9610199928284, 'accumulated_logging_time': 0.9021854400634766, 'global_step': 43848, 'preemption_count': 0}), (45362, {'train/accuracy': 0.7052175998687744, 'train/loss': 1.1465650796890259, 'validation/accuracy': 0.6387799978256226, 'validation/loss': 1.4946125745773315, 'validation/num_examples': 50000, 'test/accuracy': 0.5038000345230103, 'test/loss': 2.2749900817871094, 'test/num_examples': 10000, 'score': 15334.175417423248, 'total_duration': 15883.150108098984, 'accumulated_submission_time': 15334.175417423248, 'accumulated_eval_time': 546.4602868556976, 'accumulated_logging_time': 0.9373815059661865, 'global_step': 45362, 'preemption_count': 0}), (46877, {'train/accuracy': 0.700613796710968, 'train/loss': 1.157369613647461, 'validation/accuracy': 0.6425999999046326, 'validation/loss': 1.466826319694519, 'validation/num_examples': 50000, 'test/accuracy': 0.515500009059906, 'test/loss': 2.1804099082946777, 'test/num_examples': 10000, 'score': 15844.352715015411, 'total_duration': 16411.172538280487, 'accumulated_submission_time': 15844.352715015411, 'accumulated_eval_time': 564.2131533622742, 'accumulated_logging_time': 0.9761536121368408, 'global_step': 46877, 'preemption_count': 0}), (48390, {'train/accuracy': 0.7057557106018066, 'train/loss': 1.150970220565796, 'validation/accuracy': 0.6429199576377869, 'validation/loss': 1.46943998336792, 'validation/num_examples': 50000, 'test/accuracy': 0.5108000040054321, 'test/loss': 2.2163541316986084, 'test/num_examples': 10000, 'score': 16354.314910888672, 'total_duration': 16938.669855833054, 'accumulated_submission_time': 16354.314910888672, 'accumulated_eval_time': 581.6602036952972, 'accumulated_logging_time': 1.0123159885406494, 'global_step': 48390, 'preemption_count': 0}), (49904, {'train/accuracy': 0.758230984210968, 'train/loss': 0.9265372157096863, 'validation/accuracy': 0.6476399898529053, 'validation/loss': 1.4463788270950317, 'validation/num_examples': 50000, 'test/accuracy': 0.513700008392334, 'test/loss': 2.200896739959717, 'test/num_examples': 10000, 'score': 16864.458737134933, 'total_duration': 17466.411892175674, 'accumulated_submission_time': 16864.458737134933, 'accumulated_eval_time': 599.1710863113403, 'accumulated_logging_time': 1.0477879047393799, 'global_step': 49904, 'preemption_count': 0}), (51418, {'train/accuracy': 0.7257453799247742, 'train/loss': 1.053958535194397, 'validation/accuracy': 0.6452999711036682, 'validation/loss': 1.461386799812317, 'validation/num_examples': 50000, 'test/accuracy': 0.515500009059906, 'test/loss': 2.1797661781311035, 'test/num_examples': 10000, 'score': 17374.71946334839, 'total_duration': 17994.22289633751, 'accumulated_submission_time': 17374.71946334839, 'accumulated_eval_time': 616.627453327179, 'accumulated_logging_time': 1.0893511772155762, 'global_step': 51418, 'preemption_count': 0}), (52932, {'train/accuracy': 0.7232341766357422, 'train/loss': 1.0687999725341797, 'validation/accuracy': 0.646399974822998, 'validation/loss': 1.4467506408691406, 'validation/num_examples': 50000, 'test/accuracy': 0.5178000330924988, 'test/loss': 2.1698689460754395, 'test/num_examples': 10000, 'score': 17884.717614650726, 'total_duration': 18521.756559848785, 'accumulated_submission_time': 17884.717614650726, 'accumulated_eval_time': 634.0705862045288, 'accumulated_logging_time': 1.129570722579956, 'global_step': 52932, 'preemption_count': 0}), (54446, {'train/accuracy': 0.7245296239852905, 'train/loss': 1.0738086700439453, 'validation/accuracy': 0.6561799645423889, 'validation/loss': 1.4147099256515503, 'validation/num_examples': 50000, 'test/accuracy': 0.5199000239372253, 'test/loss': 2.165266513824463, 'test/num_examples': 10000, 'score': 18394.860572099686, 'total_duration': 19050.109229803085, 'accumulated_submission_time': 18394.860572099686, 'accumulated_eval_time': 652.1868450641632, 'accumulated_logging_time': 1.1715331077575684, 'global_step': 54446, 'preemption_count': 0}), (55961, {'train/accuracy': 0.7122927308082581, 'train/loss': 1.1093248128890991, 'validation/accuracy': 0.6516199707984924, 'validation/loss': 1.4258030652999878, 'validation/num_examples': 50000, 'test/accuracy': 0.5128000378608704, 'test/loss': 2.1755764484405518, 'test/num_examples': 10000, 'score': 18904.974903345108, 'total_duration': 19578.019869804382, 'accumulated_submission_time': 18904.974903345108, 'accumulated_eval_time': 669.8906226158142, 'accumulated_logging_time': 1.2113227844238281, 'global_step': 55961, 'preemption_count': 0}), (57475, {'train/accuracy': 0.70609450340271, 'train/loss': 1.1384251117706299, 'validation/accuracy': 0.6474599838256836, 'validation/loss': 1.4578948020935059, 'validation/num_examples': 50000, 'test/accuracy': 0.5162000060081482, 'test/loss': 2.202220916748047, 'test/num_examples': 10000, 'score': 19415.11595249176, 'total_duration': 20105.849945306778, 'accumulated_submission_time': 19415.11595249176, 'accumulated_eval_time': 687.4907431602478, 'accumulated_logging_time': 1.2484371662139893, 'global_step': 57475, 'preemption_count': 0}), (58989, {'train/accuracy': 0.7527901530265808, 'train/loss': 0.939852774143219, 'validation/accuracy': 0.6464999914169312, 'validation/loss': 1.4735795259475708, 'validation/num_examples': 50000, 'test/accuracy': 0.5228000283241272, 'test/loss': 2.1752240657806396, 'test/num_examples': 10000, 'score': 19925.138954639435, 'total_duration': 20633.438943624496, 'accumulated_submission_time': 19925.138954639435, 'accumulated_eval_time': 704.9665122032166, 'accumulated_logging_time': 1.285801649093628, 'global_step': 58989, 'preemption_count': 0}), (60503, {'train/accuracy': 0.7373046875, 'train/loss': 1.001809000968933, 'validation/accuracy': 0.6541599631309509, 'validation/loss': 1.409217119216919, 'validation/num_examples': 50000, 'test/accuracy': 0.5309000015258789, 'test/loss': 2.1159284114837646, 'test/num_examples': 10000, 'score': 20435.203814983368, 'total_duration': 21161.29817390442, 'accumulated_submission_time': 20435.203814983368, 'accumulated_eval_time': 722.6707804203033, 'accumulated_logging_time': 1.3243467807769775, 'global_step': 60503, 'preemption_count': 0}), (62017, {'train/accuracy': 0.7272400856018066, 'train/loss': 1.0499770641326904, 'validation/accuracy': 0.6545799970626831, 'validation/loss': 1.4146177768707275, 'validation/num_examples': 50000, 'test/accuracy': 0.5299000144004822, 'test/loss': 2.1289846897125244, 'test/num_examples': 10000, 'score': 20945.369954109192, 'total_duration': 21688.86900305748, 'accumulated_submission_time': 20945.369954109192, 'accumulated_eval_time': 739.9851453304291, 'accumulated_logging_time': 1.3609263896942139, 'global_step': 62017, 'preemption_count': 0}), (63531, {'train/accuracy': 0.7257254123687744, 'train/loss': 1.053279995918274, 'validation/accuracy': 0.6569799780845642, 'validation/loss': 1.4097890853881836, 'validation/num_examples': 50000, 'test/accuracy': 0.5286000370979309, 'test/loss': 2.131467580795288, 'test/num_examples': 10000, 'score': 21455.5750977993, 'total_duration': 22216.500081300735, 'accumulated_submission_time': 21455.5750977993, 'accumulated_eval_time': 757.318776845932, 'accumulated_logging_time': 1.4008488655090332, 'global_step': 63531, 'preemption_count': 0}), (65046, {'train/accuracy': 0.7092434763908386, 'train/loss': 1.1382043361663818, 'validation/accuracy': 0.6457200050354004, 'validation/loss': 1.4658386707305908, 'validation/num_examples': 50000, 'test/accuracy': 0.5245000123977661, 'test/loss': 2.156514883041382, 'test/num_examples': 10000, 'score': 21965.726397037506, 'total_duration': 22744.581331014633, 'accumulated_submission_time': 21965.726397037506, 'accumulated_eval_time': 775.1528396606445, 'accumulated_logging_time': 1.4429562091827393, 'global_step': 65046, 'preemption_count': 0}), (66561, {'train/accuracy': 0.7173748016357422, 'train/loss': 1.0853809118270874, 'validation/accuracy': 0.6520000100135803, 'validation/loss': 1.4343310594558716, 'validation/num_examples': 50000, 'test/accuracy': 0.5267000198364258, 'test/loss': 2.145815134048462, 'test/num_examples': 10000, 'score': 22475.8989508152, 'total_duration': 23272.48766350746, 'accumulated_submission_time': 22475.8989508152, 'accumulated_eval_time': 792.794264793396, 'accumulated_logging_time': 1.482445240020752, 'global_step': 66561, 'preemption_count': 0}), (68075, {'train/accuracy': 0.7562978267669678, 'train/loss': 0.9239857792854309, 'validation/accuracy': 0.6578199863433838, 'validation/loss': 1.4056472778320312, 'validation/num_examples': 50000, 'test/accuracy': 0.5333000421524048, 'test/loss': 2.1278412342071533, 'test/num_examples': 10000, 'score': 22986.08687877655, 'total_duration': 23800.468817472458, 'accumulated_submission_time': 22986.08687877655, 'accumulated_eval_time': 810.4946658611298, 'accumulated_logging_time': 1.5225434303283691, 'global_step': 68075, 'preemption_count': 0}), (69589, {'train/accuracy': 0.7335578799247742, 'train/loss': 1.0107797384262085, 'validation/accuracy': 0.6535399556159973, 'validation/loss': 1.4225095510482788, 'validation/num_examples': 50000, 'test/accuracy': 0.5343000292778015, 'test/loss': 2.123914957046509, 'test/num_examples': 10000, 'score': 23496.2087392807, 'total_duration': 24328.093178987503, 'accumulated_submission_time': 23496.2087392807, 'accumulated_eval_time': 827.9055438041687, 'accumulated_logging_time': 1.5612945556640625, 'global_step': 69589, 'preemption_count': 0}), (71103, {'train/accuracy': 0.7446388602256775, 'train/loss': 0.9683708548545837, 'validation/accuracy': 0.668940007686615, 'validation/loss': 1.3582696914672852, 'validation/num_examples': 50000, 'test/accuracy': 0.5351000428199768, 'test/loss': 2.0900559425354004, 'test/num_examples': 10000, 'score': 24006.12292265892, 'total_duration': 24855.764329195023, 'accumulated_submission_time': 24006.12292265892, 'accumulated_eval_time': 845.568799495697, 'accumulated_logging_time': 1.6021020412445068, 'global_step': 71103, 'preemption_count': 0}), (72618, {'train/accuracy': 0.7384406924247742, 'train/loss': 0.9917774200439453, 'validation/accuracy': 0.6642999649047852, 'validation/loss': 1.362199068069458, 'validation/num_examples': 50000, 'test/accuracy': 0.5350000262260437, 'test/loss': 2.121037483215332, 'test/num_examples': 10000, 'score': 24516.380070209503, 'total_duration': 25383.496671438217, 'accumulated_submission_time': 24516.380070209503, 'accumulated_eval_time': 862.9524285793304, 'accumulated_logging_time': 1.64097261428833, 'global_step': 72618, 'preemption_count': 0}), (74133, {'train/accuracy': 0.7292729616165161, 'train/loss': 1.0414572954177856, 'validation/accuracy': 0.6623799800872803, 'validation/loss': 1.3939917087554932, 'validation/num_examples': 50000, 'test/accuracy': 0.5330000519752502, 'test/loss': 2.1223597526550293, 'test/num_examples': 10000, 'score': 25026.531310796738, 'total_duration': 25911.16432285309, 'accumulated_submission_time': 25026.531310796738, 'accumulated_eval_time': 880.3766114711761, 'accumulated_logging_time': 1.679915428161621, 'global_step': 74133, 'preemption_count': 0}), (75647, {'train/accuracy': 0.7429248690605164, 'train/loss': 0.9894379377365112, 'validation/accuracy': 0.6701799631118774, 'validation/loss': 1.3528382778167725, 'validation/num_examples': 50000, 'test/accuracy': 0.5415000319480896, 'test/loss': 2.0805461406707764, 'test/num_examples': 10000, 'score': 25536.604145526886, 'total_duration': 26438.958388328552, 'accumulated_submission_time': 25536.604145526886, 'accumulated_eval_time': 898.0017364025116, 'accumulated_logging_time': 1.7220737934112549, 'global_step': 75647, 'preemption_count': 0}), (77161, {'train/accuracy': 0.7538663744926453, 'train/loss': 0.933739185333252, 'validation/accuracy': 0.6592999696731567, 'validation/loss': 1.3939270973205566, 'validation/num_examples': 50000, 'test/accuracy': 0.5258000493049622, 'test/loss': 2.1486873626708984, 'test/num_examples': 10000, 'score': 26046.688113451004, 'total_duration': 26966.827723503113, 'accumulated_submission_time': 26046.688113451004, 'accumulated_eval_time': 915.6954228878021, 'accumulated_logging_time': 1.7612836360931396, 'global_step': 77161, 'preemption_count': 0}), (78675, {'train/accuracy': 0.7557198405265808, 'train/loss': 0.9259157776832581, 'validation/accuracy': 0.6734399795532227, 'validation/loss': 1.3371450901031494, 'validation/num_examples': 50000, 'test/accuracy': 0.5458000302314758, 'test/loss': 2.0804595947265625, 'test/num_examples': 10000, 'score': 26556.600678920746, 'total_duration': 27494.543236494064, 'accumulated_submission_time': 26556.600678920746, 'accumulated_eval_time': 933.3991062641144, 'accumulated_logging_time': 1.8085589408874512, 'global_step': 78675, 'preemption_count': 0}), (80190, {'train/accuracy': 0.7496811151504517, 'train/loss': 0.9471098780632019, 'validation/accuracy': 0.6690799593925476, 'validation/loss': 1.3521523475646973, 'validation/num_examples': 50000, 'test/accuracy': 0.5390000343322754, 'test/loss': 2.0906593799591064, 'test/num_examples': 10000, 'score': 27066.66562986374, 'total_duration': 28022.62483239174, 'accumulated_submission_time': 27066.66562986374, 'accumulated_eval_time': 951.3102207183838, 'accumulated_logging_time': 1.8615412712097168, 'global_step': 80190, 'preemption_count': 0}), (81704, {'train/accuracy': 0.7504384517669678, 'train/loss': 0.9462156295776367, 'validation/accuracy': 0.6738199591636658, 'validation/loss': 1.321738362312317, 'validation/num_examples': 50000, 'test/accuracy': 0.5506000518798828, 'test/loss': 2.0084962844848633, 'test/num_examples': 10000, 'score': 27576.591657161713, 'total_duration': 28550.037356376648, 'accumulated_submission_time': 27576.591657161713, 'accumulated_eval_time': 968.7020602226257, 'accumulated_logging_time': 1.9029486179351807, 'global_step': 81704, 'preemption_count': 0}), (83218, {'train/accuracy': 0.7326610088348389, 'train/loss': 1.0291450023651123, 'validation/accuracy': 0.6612399816513062, 'validation/loss': 1.3827193975448608, 'validation/num_examples': 50000, 'test/accuracy': 0.5326000452041626, 'test/loss': 2.109022378921509, 'test/num_examples': 10000, 'score': 28086.49978995323, 'total_duration': 29077.384110450745, 'accumulated_submission_time': 28086.49978995323, 'accumulated_eval_time': 986.0461583137512, 'accumulated_logging_time': 1.945774793624878, 'global_step': 83218, 'preemption_count': 0}), (84732, {'train/accuracy': 0.7528101205825806, 'train/loss': 0.9364287257194519, 'validation/accuracy': 0.6754199862480164, 'validation/loss': 1.3129218816757202, 'validation/num_examples': 50000, 'test/accuracy': 0.5491000413894653, 'test/loss': 2.0207293033599854, 'test/num_examples': 10000, 'score': 28596.737845897675, 'total_duration': 29605.5751888752, 'accumulated_submission_time': 28596.737845897675, 'accumulated_eval_time': 1003.904506444931, 'accumulated_logging_time': 1.9876015186309814, 'global_step': 84732, 'preemption_count': 0}), (86247, {'train/accuracy': 0.7684949040412903, 'train/loss': 0.8733639717102051, 'validation/accuracy': 0.6672799587249756, 'validation/loss': 1.357527494430542, 'validation/num_examples': 50000, 'test/accuracy': 0.5403000116348267, 'test/loss': 2.0849599838256836, 'test/num_examples': 10000, 'score': 29106.95946264267, 'total_duration': 30133.36285853386, 'accumulated_submission_time': 29106.95946264267, 'accumulated_eval_time': 1021.369663476944, 'accumulated_logging_time': 2.0333995819091797, 'global_step': 86247, 'preemption_count': 0}), (87761, {'train/accuracy': 0.7531489133834839, 'train/loss': 0.9372791051864624, 'validation/accuracy': 0.6677599549293518, 'validation/loss': 1.3522411584854126, 'validation/num_examples': 50000, 'test/accuracy': 0.534600019454956, 'test/loss': 2.101754903793335, 'test/num_examples': 10000, 'score': 29616.86198425293, 'total_duration': 30661.273845672607, 'accumulated_submission_time': 29616.86198425293, 'accumulated_eval_time': 1039.2736871242523, 'accumulated_logging_time': 2.084794282913208, 'global_step': 87761, 'preemption_count': 0}), (89275, {'train/accuracy': 0.748465359210968, 'train/loss': 0.9442353248596191, 'validation/accuracy': 0.6695799827575684, 'validation/loss': 1.3373024463653564, 'validation/num_examples': 50000, 'test/accuracy': 0.5367000102996826, 'test/loss': 2.0899970531463623, 'test/num_examples': 10000, 'score': 30126.845595359802, 'total_duration': 31188.748248815536, 'accumulated_submission_time': 30126.845595359802, 'accumulated_eval_time': 1056.6683535575867, 'accumulated_logging_time': 2.1289286613464355, 'global_step': 89275, 'preemption_count': 0}), (90789, {'train/accuracy': 0.7472097873687744, 'train/loss': 0.958838939666748, 'validation/accuracy': 0.6690399646759033, 'validation/loss': 1.3475358486175537, 'validation/num_examples': 50000, 'test/accuracy': 0.5403000116348267, 'test/loss': 2.083833694458008, 'test/num_examples': 10000, 'score': 30636.927599668503, 'total_duration': 31716.278796434402, 'accumulated_submission_time': 30636.927599668503, 'accumulated_eval_time': 1074.0231430530548, 'accumulated_logging_time': 2.1696858406066895, 'global_step': 90789, 'preemption_count': 0}), (92303, {'train/accuracy': 0.7487842440605164, 'train/loss': 0.9438230991363525, 'validation/accuracy': 0.6720799803733826, 'validation/loss': 1.337753415107727, 'validation/num_examples': 50000, 'test/accuracy': 0.5448000431060791, 'test/loss': 2.058915138244629, 'test/num_examples': 10000, 'score': 31146.947617292404, 'total_duration': 32243.688675642014, 'accumulated_submission_time': 31146.947617292404, 'accumulated_eval_time': 1091.3161630630493, 'accumulated_logging_time': 2.213765859603882, 'global_step': 92303, 'preemption_count': 0}), (93815, {'train/accuracy': 0.7768255472183228, 'train/loss': 0.8307573795318604, 'validation/accuracy': 0.6793599724769592, 'validation/loss': 1.3027634620666504, 'validation/num_examples': 50000, 'test/accuracy': 0.5523000359535217, 'test/loss': 2.045220375061035, 'test/num_examples': 10000, 'score': 31657.000898361206, 'total_duration': 32772.09962654114, 'accumulated_submission_time': 31657.000898361206, 'accumulated_eval_time': 1109.5773482322693, 'accumulated_logging_time': 2.25687313079834, 'global_step': 93815, 'preemption_count': 0}), (95330, {'train/accuracy': 0.7841796875, 'train/loss': 0.8101245760917664, 'validation/accuracy': 0.6791799664497375, 'validation/loss': 1.3027790784835815, 'validation/num_examples': 50000, 'test/accuracy': 0.5487000346183777, 'test/loss': 2.0057966709136963, 'test/num_examples': 10000, 'score': 32167.20648097992, 'total_duration': 33299.79450273514, 'accumulated_submission_time': 32167.20648097992, 'accumulated_eval_time': 1126.9689333438873, 'accumulated_logging_time': 2.3015289306640625, 'global_step': 95330, 'preemption_count': 0}), (96844, {'train/accuracy': 0.7759685516357422, 'train/loss': 0.8316084146499634, 'validation/accuracy': 0.6843999624252319, 'validation/loss': 1.2773557901382446, 'validation/num_examples': 50000, 'test/accuracy': 0.5568000078201294, 'test/loss': 2.0034146308898926, 'test/num_examples': 10000, 'score': 32677.118038654327, 'total_duration': 33827.184403419495, 'accumulated_submission_time': 32677.118038654327, 'accumulated_eval_time': 1144.3458700180054, 'accumulated_logging_time': 2.350280284881592, 'global_step': 96844, 'preemption_count': 0}), (98358, {'train/accuracy': 0.7757692933082581, 'train/loss': 0.8344160318374634, 'validation/accuracy': 0.6873799562454224, 'validation/loss': 1.2644789218902588, 'validation/num_examples': 50000, 'test/accuracy': 0.558899998664856, 'test/loss': 1.9838387966156006, 'test/num_examples': 10000, 'score': 33187.21611762047, 'total_duration': 34355.01317358017, 'accumulated_submission_time': 33187.21611762047, 'accumulated_eval_time': 1161.9749476909637, 'accumulated_logging_time': 2.399492025375366, 'global_step': 98358, 'preemption_count': 0}), (99872, {'train/accuracy': 0.7700095772743225, 'train/loss': 0.8556905388832092, 'validation/accuracy': 0.6885600090026855, 'validation/loss': 1.2676442861557007, 'validation/num_examples': 50000, 'test/accuracy': 0.5624000430107117, 'test/loss': 1.9713383913040161, 'test/num_examples': 10000, 'score': 33697.160153627396, 'total_duration': 34882.30074048042, 'accumulated_submission_time': 33697.160153627396, 'accumulated_eval_time': 1179.2202832698822, 'accumulated_logging_time': 2.4442760944366455, 'global_step': 99872, 'preemption_count': 0}), (101387, {'train/accuracy': 0.7763273119926453, 'train/loss': 0.8312370777130127, 'validation/accuracy': 0.6921399831771851, 'validation/loss': 1.2392897605895996, 'validation/num_examples': 50000, 'test/accuracy': 0.5706000328063965, 'test/loss': 1.9568129777908325, 'test/num_examples': 10000, 'score': 34207.31728053093, 'total_duration': 35410.15484523773, 'accumulated_submission_time': 34207.31728053093, 'accumulated_eval_time': 1196.8182473182678, 'accumulated_logging_time': 2.4896559715270996, 'global_step': 101387, 'preemption_count': 0}), (102901, {'train/accuracy': 0.8036909699440002, 'train/loss': 0.7377466559410095, 'validation/accuracy': 0.6786800026893616, 'validation/loss': 1.3162022829055786, 'validation/num_examples': 50000, 'test/accuracy': 0.5493000149726868, 'test/loss': 2.0565006732940674, 'test/num_examples': 10000, 'score': 34717.62727546692, 'total_duration': 35938.262838840485, 'accumulated_submission_time': 34717.62727546692, 'accumulated_eval_time': 1214.52796459198, 'accumulated_logging_time': 2.526557683944702, 'global_step': 102901, 'preemption_count': 0}), (104416, {'train/accuracy': 0.8050262928009033, 'train/loss': 0.7113240957260132, 'validation/accuracy': 0.6964199542999268, 'validation/loss': 1.2255223989486694, 'validation/num_examples': 50000, 'test/accuracy': 0.5697000026702881, 'test/loss': 1.9121817350387573, 'test/num_examples': 10000, 'score': 35227.735830545425, 'total_duration': 36465.87762641907, 'accumulated_submission_time': 35227.735830545425, 'accumulated_eval_time': 1231.9329543113708, 'accumulated_logging_time': 2.574941396713257, 'global_step': 104416, 'preemption_count': 0}), (105931, {'train/accuracy': 0.7947624325752258, 'train/loss': 0.7517896294593811, 'validation/accuracy': 0.69896000623703, 'validation/loss': 1.214800238609314, 'validation/num_examples': 50000, 'test/accuracy': 0.5741000175476074, 'test/loss': 1.9478965997695923, 'test/num_examples': 10000, 'score': 35737.8859539032, 'total_duration': 36993.40591478348, 'accumulated_submission_time': 35737.8859539032, 'accumulated_eval_time': 1249.209624528885, 'accumulated_logging_time': 2.623447895050049, 'global_step': 105931, 'preemption_count': 0}), (107445, {'train/accuracy': 0.7896404266357422, 'train/loss': 0.7744253277778625, 'validation/accuracy': 0.6942200064659119, 'validation/loss': 1.2482731342315674, 'validation/num_examples': 50000, 'test/accuracy': 0.5676000118255615, 'test/loss': 1.9585601091384888, 'test/num_examples': 10000, 'score': 36247.9498064518, 'total_duration': 37521.17093753815, 'accumulated_submission_time': 36247.9498064518, 'accumulated_eval_time': 1266.8068754673004, 'accumulated_logging_time': 2.6738781929016113, 'global_step': 107445, 'preemption_count': 0}), (108958, {'train/accuracy': 0.7810905575752258, 'train/loss': 0.8077903389930725, 'validation/accuracy': 0.6916999816894531, 'validation/loss': 1.2639470100402832, 'validation/num_examples': 50000, 'test/accuracy': 0.5630000233650208, 'test/loss': 1.98625648021698, 'test/num_examples': 10000, 'score': 36757.94217133522, 'total_duration': 38049.23787140846, 'accumulated_submission_time': 36757.94217133522, 'accumulated_eval_time': 1284.7805352210999, 'accumulated_logging_time': 2.722330331802368, 'global_step': 108958, 'preemption_count': 0}), (110472, {'train/accuracy': 0.7847775816917419, 'train/loss': 0.7921836972236633, 'validation/accuracy': 0.6990000009536743, 'validation/loss': 1.2325977087020874, 'validation/num_examples': 50000, 'test/accuracy': 0.5737000107765198, 'test/loss': 1.9548555612564087, 'test/num_examples': 10000, 'score': 37267.98639202118, 'total_duration': 38576.88211965561, 'accumulated_submission_time': 37267.98639202118, 'accumulated_eval_time': 1302.261702299118, 'accumulated_logging_time': 2.787564992904663, 'global_step': 110472, 'preemption_count': 0}), (111986, {'train/accuracy': 0.8351203799247742, 'train/loss': 0.5935096740722656, 'validation/accuracy': 0.6996999979019165, 'validation/loss': 1.23037850856781, 'validation/num_examples': 50000, 'test/accuracy': 0.5711000561714172, 'test/loss': 1.9375004768371582, 'test/num_examples': 10000, 'score': 37778.13521909714, 'total_duration': 39104.69184041023, 'accumulated_submission_time': 37778.13521909714, 'accumulated_eval_time': 1319.8213753700256, 'accumulated_logging_time': 2.836085319519043, 'global_step': 111986, 'preemption_count': 0}), (113501, {'train/accuracy': 0.8151108026504517, 'train/loss': 0.672951340675354, 'validation/accuracy': 0.6999399662017822, 'validation/loss': 1.2182530164718628, 'validation/num_examples': 50000, 'test/accuracy': 0.5791000127792358, 'test/loss': 1.9064098596572876, 'test/num_examples': 10000, 'score': 38288.519334316254, 'total_duration': 39632.731977939606, 'accumulated_submission_time': 38288.519334316254, 'accumulated_eval_time': 1337.3728301525116, 'accumulated_logging_time': 2.8874781131744385, 'global_step': 113501, 'preemption_count': 0}), (115015, {'train/accuracy': 0.80961012840271, 'train/loss': 0.6891085505485535, 'validation/accuracy': 0.705299973487854, 'validation/loss': 1.2037498950958252, 'validation/num_examples': 50000, 'test/accuracy': 0.5841000080108643, 'test/loss': 1.930444598197937, 'test/num_examples': 10000, 'score': 38798.61583328247, 'total_duration': 40160.355558395386, 'accumulated_submission_time': 38798.61583328247, 'accumulated_eval_time': 1354.7945802211761, 'accumulated_logging_time': 2.939371109008789, 'global_step': 115015, 'preemption_count': 0}), (116530, {'train/accuracy': 0.8014189600944519, 'train/loss': 0.7300856113433838, 'validation/accuracy': 0.7023199796676636, 'validation/loss': 1.212501883506775, 'validation/num_examples': 50000, 'test/accuracy': 0.5679000020027161, 'test/loss': 1.9563889503479004, 'test/num_examples': 10000, 'score': 39308.739602565765, 'total_duration': 40688.40866851807, 'accumulated_submission_time': 39308.739602565765, 'accumulated_eval_time': 1372.6245946884155, 'accumulated_logging_time': 2.987029552459717, 'global_step': 116530, 'preemption_count': 0}), (118045, {'train/accuracy': 0.8037906289100647, 'train/loss': 0.7134524583816528, 'validation/accuracy': 0.7069399952888489, 'validation/loss': 1.1913037300109863, 'validation/num_examples': 50000, 'test/accuracy': 0.5809000134468079, 'test/loss': 1.901960849761963, 'test/num_examples': 10000, 'score': 39818.97498655319, 'total_duration': 41216.51202297211, 'accumulated_submission_time': 39818.97498655319, 'accumulated_eval_time': 1390.393991947174, 'accumulated_logging_time': 3.032386541366577, 'global_step': 118045, 'preemption_count': 0}), (119559, {'train/accuracy': 0.7993462681770325, 'train/loss': 0.7303955554962158, 'validation/accuracy': 0.7000199556350708, 'validation/loss': 1.2318073511123657, 'validation/num_examples': 50000, 'test/accuracy': 0.567300021648407, 'test/loss': 1.9584038257598877, 'test/num_examples': 10000, 'score': 40328.96877479553, 'total_duration': 41744.0572385788, 'accumulated_submission_time': 40328.96877479553, 'accumulated_eval_time': 1407.8446052074432, 'accumulated_logging_time': 3.0808701515197754, 'global_step': 119559, 'preemption_count': 0}), (121073, {'train/accuracy': 0.8441884517669678, 'train/loss': 0.5559610724449158, 'validation/accuracy': 0.7035399675369263, 'validation/loss': 1.1964123249053955, 'validation/num_examples': 50000, 'test/accuracy': 0.5856000185012817, 'test/loss': 1.882620930671692, 'test/num_examples': 10000, 'score': 40839.1434905529, 'total_duration': 42271.82386422157, 'accumulated_submission_time': 40839.1434905529, 'accumulated_eval_time': 1425.3332903385162, 'accumulated_logging_time': 3.131650686264038, 'global_step': 121073, 'preemption_count': 0}), (122588, {'train/accuracy': 0.83203125, 'train/loss': 0.6034864783287048, 'validation/accuracy': 0.7110399603843689, 'validation/loss': 1.172864317893982, 'validation/num_examples': 50000, 'test/accuracy': 0.5904000401496887, 'test/loss': 1.8778742551803589, 'test/num_examples': 10000, 'score': 41349.369044303894, 'total_duration': 42799.59975862503, 'accumulated_submission_time': 41349.369044303894, 'accumulated_eval_time': 1442.7801163196564, 'accumulated_logging_time': 3.1828622817993164, 'global_step': 122588, 'preemption_count': 0}), (124103, {'train/accuracy': 0.8218072056770325, 'train/loss': 0.6350313425064087, 'validation/accuracy': 0.7073799967765808, 'validation/loss': 1.1969265937805176, 'validation/num_examples': 50000, 'test/accuracy': 0.5773000121116638, 'test/loss': 1.9539101123809814, 'test/num_examples': 10000, 'score': 41859.428148031235, 'total_duration': 43327.76603150368, 'accumulated_submission_time': 41859.428148031235, 'accumulated_eval_time': 1460.7830305099487, 'accumulated_logging_time': 3.2322473526000977, 'global_step': 124103, 'preemption_count': 0}), (125617, {'train/accuracy': 0.8284637928009033, 'train/loss': 0.6209262609481812, 'validation/accuracy': 0.7165399789810181, 'validation/loss': 1.1599438190460205, 'validation/num_examples': 50000, 'test/accuracy': 0.5925000309944153, 'test/loss': 1.8581621646881104, 'test/num_examples': 10000, 'score': 42369.41453456879, 'total_duration': 43855.42738699913, 'accumulated_submission_time': 42369.41453456879, 'accumulated_eval_time': 1478.3529794216156, 'accumulated_logging_time': 3.285027503967285, 'global_step': 125617, 'preemption_count': 0}), (127132, {'train/accuracy': 0.8194355964660645, 'train/loss': 0.6457960605621338, 'validation/accuracy': 0.7070199847221375, 'validation/loss': 1.2006008625030518, 'validation/num_examples': 50000, 'test/accuracy': 0.5856000185012817, 'test/loss': 1.9250835180282593, 'test/num_examples': 10000, 'score': 42879.60580301285, 'total_duration': 44383.18127179146, 'accumulated_submission_time': 42879.60580301285, 'accumulated_eval_time': 1495.813749074936, 'accumulated_logging_time': 3.3334877490997314, 'global_step': 127132, 'preemption_count': 0}), (128647, {'train/accuracy': 0.8309949040412903, 'train/loss': 0.6023525595664978, 'validation/accuracy': 0.7137399911880493, 'validation/loss': 1.1708685159683228, 'validation/num_examples': 50000, 'test/accuracy': 0.5854000449180603, 'test/loss': 1.8945751190185547, 'test/num_examples': 10000, 'score': 43389.64476656914, 'total_duration': 44910.88752961159, 'accumulated_submission_time': 43389.64476656914, 'accumulated_eval_time': 1513.3805103302002, 'accumulated_logging_time': 3.381448268890381, 'global_step': 128647, 'preemption_count': 0}), (130160, {'train/accuracy': 0.8641980290412903, 'train/loss': 0.48220840096473694, 'validation/accuracy': 0.7169199585914612, 'validation/loss': 1.1536674499511719, 'validation/num_examples': 50000, 'test/accuracy': 0.5945000052452087, 'test/loss': 1.87068510055542, 'test/num_examples': 10000, 'score': 43899.57999563217, 'total_duration': 45438.51849889755, 'accumulated_submission_time': 43899.57999563217, 'accumulated_eval_time': 1530.9731702804565, 'accumulated_logging_time': 3.430178165435791, 'global_step': 130160, 'preemption_count': 0}), (131675, {'train/accuracy': 0.853535532951355, 'train/loss': 0.5158511400222778, 'validation/accuracy': 0.7181800007820129, 'validation/loss': 1.1625126600265503, 'validation/num_examples': 50000, 'test/accuracy': 0.5956000089645386, 'test/loss': 1.894251823425293, 'test/num_examples': 10000, 'score': 44409.718037605286, 'total_duration': 45967.23632621765, 'accumulated_submission_time': 44409.718037605286, 'accumulated_eval_time': 1549.4466173648834, 'accumulated_logging_time': 3.483417510986328, 'global_step': 131675, 'preemption_count': 0}), (133189, {'train/accuracy': 0.8512635231018066, 'train/loss': 0.518912672996521, 'validation/accuracy': 0.7234999537467957, 'validation/loss': 1.1488239765167236, 'validation/num_examples': 50000, 'test/accuracy': 0.5996000170707703, 'test/loss': 1.8523231744766235, 'test/num_examples': 10000, 'score': 44919.616294384, 'total_duration': 46494.96126246452, 'accumulated_submission_time': 44919.616294384, 'accumulated_eval_time': 1567.1695573329926, 'accumulated_logging_time': 3.5336878299713135, 'global_step': 133189, 'preemption_count': 0}), (134703, {'train/accuracy': 0.8521404266357422, 'train/loss': 0.5233726501464844, 'validation/accuracy': 0.7243199944496155, 'validation/loss': 1.1318014860153198, 'validation/num_examples': 50000, 'test/accuracy': 0.6012000441551208, 'test/loss': 1.8476316928863525, 'test/num_examples': 10000, 'score': 45429.56394815445, 'total_duration': 47022.609437942505, 'accumulated_submission_time': 45429.56394815445, 'accumulated_eval_time': 1584.762847185135, 'accumulated_logging_time': 3.5877740383148193, 'global_step': 134703, 'preemption_count': 0}), (136217, {'train/accuracy': 0.8546316623687744, 'train/loss': 0.5151609182357788, 'validation/accuracy': 0.7261799573898315, 'validation/loss': 1.1283327341079712, 'validation/num_examples': 50000, 'test/accuracy': 0.6022000312805176, 'test/loss': 1.8703584671020508, 'test/num_examples': 10000, 'score': 45939.55491280556, 'total_duration': 47550.36924123764, 'accumulated_submission_time': 45939.55491280556, 'accumulated_eval_time': 1602.4290103912354, 'accumulated_logging_time': 3.637843132019043, 'global_step': 136217, 'preemption_count': 0}), (137732, {'train/accuracy': 0.8350605964660645, 'train/loss': 0.5876715183258057, 'validation/accuracy': 0.7061399817466736, 'validation/loss': 1.225954532623291, 'validation/num_examples': 50000, 'test/accuracy': 0.5824000239372253, 'test/loss': 1.9551299810409546, 'test/num_examples': 10000, 'score': 46449.772149086, 'total_duration': 48078.53934550285, 'accumulated_submission_time': 46449.772149086, 'accumulated_eval_time': 1620.2749452590942, 'accumulated_logging_time': 3.6919307708740234, 'global_step': 137732, 'preemption_count': 0}), (139246, {'train/accuracy': 0.8834900856018066, 'train/loss': 0.4090985059738159, 'validation/accuracy': 0.7286199927330017, 'validation/loss': 1.1191855669021606, 'validation/num_examples': 50000, 'test/accuracy': 0.6080000400543213, 'test/loss': 1.8433808088302612, 'test/num_examples': 10000, 'score': 46959.832931280136, 'total_duration': 48606.27636384964, 'accumulated_submission_time': 46959.832931280136, 'accumulated_eval_time': 1637.8468651771545, 'accumulated_logging_time': 3.7433359622955322, 'global_step': 139246, 'preemption_count': 0}), (140761, {'train/accuracy': 0.8775510191917419, 'train/loss': 0.42347419261932373, 'validation/accuracy': 0.7308799624443054, 'validation/loss': 1.1201832294464111, 'validation/num_examples': 50000, 'test/accuracy': 0.6022000312805176, 'test/loss': 1.865876317024231, 'test/num_examples': 10000, 'score': 47470.02541399002, 'total_duration': 49134.336408376694, 'accumulated_submission_time': 47470.02541399002, 'accumulated_eval_time': 1655.6108510494232, 'accumulated_logging_time': 3.794053316116333, 'global_step': 140761, 'preemption_count': 0}), (142275, {'train/accuracy': 0.869559109210968, 'train/loss': 0.45493537187576294, 'validation/accuracy': 0.729699969291687, 'validation/loss': 1.1267502307891846, 'validation/num_examples': 50000, 'test/accuracy': 0.6037000417709351, 'test/loss': 1.8605526685714722, 'test/num_examples': 10000, 'score': 47980.01170706749, 'total_duration': 49661.93563914299, 'accumulated_submission_time': 47980.01170706749, 'accumulated_eval_time': 1673.117571592331, 'accumulated_logging_time': 3.8474504947662354, 'global_step': 142275, 'preemption_count': 0}), (143788, {'train/accuracy': 0.8769331574440002, 'train/loss': 0.42777085304260254, 'validation/accuracy': 0.7282199859619141, 'validation/loss': 1.118950605392456, 'validation/num_examples': 50000, 'test/accuracy': 0.6041000485420227, 'test/loss': 1.8592493534088135, 'test/num_examples': 10000, 'score': 48489.98080587387, 'total_duration': 50189.48695039749, 'accumulated_submission_time': 48489.98080587387, 'accumulated_eval_time': 1690.5912556648254, 'accumulated_logging_time': 3.9025094509124756, 'global_step': 143788, 'preemption_count': 0}), (145301, {'train/accuracy': 0.8763153553009033, 'train/loss': 0.42836129665374756, 'validation/accuracy': 0.7317599654197693, 'validation/loss': 1.1129244565963745, 'validation/num_examples': 50000, 'test/accuracy': 0.6092000007629395, 'test/loss': 1.8546922206878662, 'test/num_examples': 10000, 'score': 48999.96798682213, 'total_duration': 50717.04645681381, 'accumulated_submission_time': 48999.96798682213, 'accumulated_eval_time': 1708.0588409900665, 'accumulated_logging_time': 3.9540607929229736, 'global_step': 145301, 'preemption_count': 0}), (146815, {'train/accuracy': 0.8947902917861938, 'train/loss': 0.3664727807044983, 'validation/accuracy': 0.7371000051498413, 'validation/loss': 1.0927094221115112, 'validation/num_examples': 50000, 'test/accuracy': 0.6107000112533569, 'test/loss': 1.8342806100845337, 'test/num_examples': 10000, 'score': 49510.08322453499, 'total_duration': 51244.758184194565, 'accumulated_submission_time': 49510.08322453499, 'accumulated_eval_time': 1725.5483317375183, 'accumulated_logging_time': 4.008327007293701, 'global_step': 146815, 'preemption_count': 0}), (148329, {'train/accuracy': 0.9049345850944519, 'train/loss': 0.327584832906723, 'validation/accuracy': 0.7351999878883362, 'validation/loss': 1.1068998575210571, 'validation/num_examples': 50000, 'test/accuracy': 0.6067000031471252, 'test/loss': 1.8497717380523682, 'test/num_examples': 10000, 'score': 50020.06638741493, 'total_duration': 51772.60830807686, 'accumulated_submission_time': 50020.06638741493, 'accumulated_eval_time': 1743.310753107071, 'accumulated_logging_time': 4.059577941894531, 'global_step': 148329, 'preemption_count': 0}), (149843, {'train/accuracy': 0.9015664458274841, 'train/loss': 0.34114494919776917, 'validation/accuracy': 0.7376599907875061, 'validation/loss': 1.0968347787857056, 'validation/num_examples': 50000, 'test/accuracy': 0.6149000525474548, 'test/loss': 1.8277121782302856, 'test/num_examples': 10000, 'score': 50530.12015199661, 'total_duration': 52300.12011003494, 'accumulated_submission_time': 50530.12015199661, 'accumulated_eval_time': 1760.661033153534, 'accumulated_logging_time': 4.114696264266968, 'global_step': 149843, 'preemption_count': 0}), (151357, {'train/accuracy': 0.9032804369926453, 'train/loss': 0.33565738797187805, 'validation/accuracy': 0.7389000058174133, 'validation/loss': 1.0878074169158936, 'validation/num_examples': 50000, 'test/accuracy': 0.6163000464439392, 'test/loss': 1.8217467069625854, 'test/num_examples': 10000, 'score': 51040.12117242813, 'total_duration': 52828.000644207, 'accumulated_submission_time': 51040.12117242813, 'accumulated_eval_time': 1778.4301431179047, 'accumulated_logging_time': 4.172338247299194, 'global_step': 151357, 'preemption_count': 0}), (152872, {'train/accuracy': 0.9049944281578064, 'train/loss': 0.3283750116825104, 'validation/accuracy': 0.7387199997901917, 'validation/loss': 1.1011860370635986, 'validation/num_examples': 50000, 'test/accuracy': 0.6160000562667847, 'test/loss': 1.8262497186660767, 'test/num_examples': 10000, 'score': 51550.185572862625, 'total_duration': 53355.749900341034, 'accumulated_submission_time': 51550.185572862625, 'accumulated_eval_time': 1796.0045523643494, 'accumulated_logging_time': 4.2290143966674805, 'global_step': 152872, 'preemption_count': 0}), (154386, {'train/accuracy': 0.9026825428009033, 'train/loss': 0.33135420083999634, 'validation/accuracy': 0.7376799583435059, 'validation/loss': 1.1047000885009766, 'validation/num_examples': 50000, 'test/accuracy': 0.616100013256073, 'test/loss': 1.8450510501861572, 'test/num_examples': 10000, 'score': 52060.21515059471, 'total_duration': 53883.71820926666, 'accumulated_submission_time': 52060.21515059471, 'accumulated_eval_time': 1813.8327918052673, 'accumulated_logging_time': 4.287072420120239, 'global_step': 154386, 'preemption_count': 0}), (155900, {'train/accuracy': 0.9301259517669678, 'train/loss': 0.25026360154151917, 'validation/accuracy': 0.7440800070762634, 'validation/loss': 1.0782766342163086, 'validation/num_examples': 50000, 'test/accuracy': 0.6147000193595886, 'test/loss': 1.8321329355239868, 'test/num_examples': 10000, 'score': 52570.35527801514, 'total_duration': 54411.412940740585, 'accumulated_submission_time': 52570.35527801514, 'accumulated_eval_time': 1831.2789142131805, 'accumulated_logging_time': 4.342725038528442, 'global_step': 155900, 'preemption_count': 0}), (157414, {'train/accuracy': 0.9299465417861938, 'train/loss': 0.24397999048233032, 'validation/accuracy': 0.742579996585846, 'validation/loss': 1.0852277278900146, 'validation/num_examples': 50000, 'test/accuracy': 0.6175000071525574, 'test/loss': 1.8458549976348877, 'test/num_examples': 10000, 'score': 53080.38762998581, 'total_duration': 54939.17242026329, 'accumulated_submission_time': 53080.38762998581, 'accumulated_eval_time': 1848.8975548744202, 'accumulated_logging_time': 4.397834777832031, 'global_step': 157414, 'preemption_count': 0}), (158929, {'train/accuracy': 0.9267578125, 'train/loss': 0.2488669753074646, 'validation/accuracy': 0.7440999746322632, 'validation/loss': 1.0787391662597656, 'validation/num_examples': 50000, 'test/accuracy': 0.6193000078201294, 'test/loss': 1.8259567022323608, 'test/num_examples': 10000, 'score': 53590.50220036507, 'total_duration': 55466.92384791374, 'accumulated_submission_time': 53590.50220036507, 'accumulated_eval_time': 1866.427173614502, 'accumulated_logging_time': 4.452563047409058, 'global_step': 158929, 'preemption_count': 0}), (160443, {'train/accuracy': 0.9273557066917419, 'train/loss': 0.25223514437675476, 'validation/accuracy': 0.7450199723243713, 'validation/loss': 1.0835787057876587, 'validation/num_examples': 50000, 'test/accuracy': 0.6230000257492065, 'test/loss': 1.8304731845855713, 'test/num_examples': 10000, 'score': 54100.62163352966, 'total_duration': 55994.9102306366, 'accumulated_submission_time': 54100.62163352966, 'accumulated_eval_time': 1884.1840229034424, 'accumulated_logging_time': 4.509575366973877, 'global_step': 160443, 'preemption_count': 0}), (161957, {'train/accuracy': 0.928730845451355, 'train/loss': 0.24525007605552673, 'validation/accuracy': 0.7450199723243713, 'validation/loss': 1.079473853111267, 'validation/num_examples': 50000, 'test/accuracy': 0.6254000067710876, 'test/loss': 1.832878589630127, 'test/num_examples': 10000, 'score': 54610.548907756805, 'total_duration': 56522.68081855774, 'accumulated_submission_time': 54610.548907756805, 'accumulated_eval_time': 1901.915011882782, 'accumulated_logging_time': 4.5679240226745605, 'global_step': 161957, 'preemption_count': 0}), (163471, {'train/accuracy': 0.9323580861091614, 'train/loss': 0.23463422060012817, 'validation/accuracy': 0.7455399632453918, 'validation/loss': 1.0828678607940674, 'validation/num_examples': 50000, 'test/accuracy': 0.6281000375747681, 'test/loss': 1.822871208190918, 'test/num_examples': 10000, 'score': 55120.64704847336, 'total_duration': 57050.35792398453, 'accumulated_submission_time': 55120.64704847336, 'accumulated_eval_time': 1919.3801970481873, 'accumulated_logging_time': 4.62821364402771, 'global_step': 163471, 'preemption_count': 0}), (164984, {'train/accuracy': 0.9518494606018066, 'train/loss': 0.1781339794397354, 'validation/accuracy': 0.7479000091552734, 'validation/loss': 1.0789545774459839, 'validation/num_examples': 50000, 'test/accuracy': 0.6242000460624695, 'test/loss': 1.8314135074615479, 'test/num_examples': 10000, 'score': 55630.59670042992, 'total_duration': 57577.83935189247, 'accumulated_submission_time': 55630.59670042992, 'accumulated_eval_time': 1936.8064422607422, 'accumulated_logging_time': 4.680109977722168, 'global_step': 164984, 'preemption_count': 0}), (166498, {'train/accuracy': 0.9480029940605164, 'train/loss': 0.1860027015209198, 'validation/accuracy': 0.746999979019165, 'validation/loss': 1.078598976135254, 'validation/num_examples': 50000, 'test/accuracy': 0.6274000406265259, 'test/loss': 1.8304781913757324, 'test/num_examples': 10000, 'score': 56140.51879167557, 'total_duration': 58105.194816827774, 'accumulated_submission_time': 56140.51879167557, 'accumulated_eval_time': 1954.128136396408, 'accumulated_logging_time': 4.73781156539917, 'global_step': 166498, 'preemption_count': 0}), (168012, {'train/accuracy': 0.9465281963348389, 'train/loss': 0.18971571326255798, 'validation/accuracy': 0.7482799887657166, 'validation/loss': 1.0680350065231323, 'validation/num_examples': 50000, 'test/accuracy': 0.626800000667572, 'test/loss': 1.8228516578674316, 'test/num_examples': 10000, 'score': 56650.4464943409, 'total_duration': 58632.931473731995, 'accumulated_submission_time': 56650.4464943409, 'accumulated_eval_time': 1971.8274443149567, 'accumulated_logging_time': 4.794127941131592, 'global_step': 168012, 'preemption_count': 0}), (169526, {'train/accuracy': 0.9483816623687744, 'train/loss': 0.1834833025932312, 'validation/accuracy': 0.7514199614524841, 'validation/loss': 1.0675344467163086, 'validation/num_examples': 50000, 'test/accuracy': 0.627500057220459, 'test/loss': 1.824575662612915, 'test/num_examples': 10000, 'score': 57160.486573934555, 'total_duration': 59160.888498306274, 'accumulated_submission_time': 57160.486573934555, 'accumulated_eval_time': 1989.6279754638672, 'accumulated_logging_time': 4.857915878295898, 'global_step': 169526, 'preemption_count': 0}), (171040, {'train/accuracy': 0.950215220451355, 'train/loss': 0.17819266021251678, 'validation/accuracy': 0.7507599592208862, 'validation/loss': 1.0668600797653198, 'validation/num_examples': 50000, 'test/accuracy': 0.6289000511169434, 'test/loss': 1.8254389762878418, 'test/num_examples': 10000, 'score': 57670.477942705154, 'total_duration': 59689.33236479759, 'accumulated_submission_time': 57670.477942705154, 'accumulated_eval_time': 2007.9708423614502, 'accumulated_logging_time': 4.914608955383301, 'global_step': 171040, 'preemption_count': 0}), (172554, {'train/accuracy': 0.9512715339660645, 'train/loss': 0.17513197660446167, 'validation/accuracy': 0.7514399886131287, 'validation/loss': 1.0617859363555908, 'validation/num_examples': 50000, 'test/accuracy': 0.6328000426292419, 'test/loss': 1.818946361541748, 'test/num_examples': 10000, 'score': 58180.52669739723, 'total_duration': 60217.089391469955, 'accumulated_submission_time': 58180.52669739723, 'accumulated_eval_time': 2025.5694625377655, 'accumulated_logging_time': 4.971103191375732, 'global_step': 172554, 'preemption_count': 0}), (174068, {'train/accuracy': 0.9608178734779358, 'train/loss': 0.14954684674739838, 'validation/accuracy': 0.7513999938964844, 'validation/loss': 1.0650503635406494, 'validation/num_examples': 50000, 'test/accuracy': 0.6322000026702881, 'test/loss': 1.8160579204559326, 'test/num_examples': 10000, 'score': 58690.58052444458, 'total_duration': 60744.659641981125, 'accumulated_submission_time': 58690.58052444458, 'accumulated_eval_time': 2042.9718658924103, 'accumulated_logging_time': 5.033244848251343, 'global_step': 174068, 'preemption_count': 0}), (175582, {'train/accuracy': 0.9582270383834839, 'train/loss': 0.15545976161956787, 'validation/accuracy': 0.7520999908447266, 'validation/loss': 1.0633561611175537, 'validation/num_examples': 50000, 'test/accuracy': 0.6325000524520874, 'test/loss': 1.820577621459961, 'test/num_examples': 10000, 'score': 59200.59376168251, 'total_duration': 61272.38275671005, 'accumulated_submission_time': 59200.59376168251, 'accumulated_eval_time': 2060.5704913139343, 'accumulated_logging_time': 5.091560125350952, 'global_step': 175582, 'preemption_count': 0}), (177096, {'train/accuracy': 0.9596021771430969, 'train/loss': 0.1510808914899826, 'validation/accuracy': 0.7536599636077881, 'validation/loss': 1.0556384325027466, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.812854528427124, 'test/num_examples': 10000, 'score': 59710.533801317215, 'total_duration': 61800.26037359238, 'accumulated_submission_time': 59710.533801317215, 'accumulated_eval_time': 2078.398533344269, 'accumulated_logging_time': 5.148442506790161, 'global_step': 177096, 'preemption_count': 0}), (178610, {'train/accuracy': 0.95902419090271, 'train/loss': 0.1536361575126648, 'validation/accuracy': 0.7538599967956543, 'validation/loss': 1.0557180643081665, 'validation/num_examples': 50000, 'test/accuracy': 0.6331000328063965, 'test/loss': 1.810882329940796, 'test/num_examples': 10000, 'score': 60220.49709105492, 'total_duration': 62328.04130482674, 'accumulated_submission_time': 60220.49709105492, 'accumulated_eval_time': 2096.097542285919, 'accumulated_logging_time': 5.214344024658203, 'global_step': 178610, 'preemption_count': 0}), (180123, {'train/accuracy': 0.9585060477256775, 'train/loss': 0.15022099018096924, 'validation/accuracy': 0.7537199854850769, 'validation/loss': 1.0573610067367554, 'validation/num_examples': 50000, 'test/accuracy': 0.6345000267028809, 'test/loss': 1.8141127824783325, 'test/num_examples': 10000, 'score': 60730.41804790497, 'total_duration': 62855.74048471451, 'accumulated_submission_time': 60730.41804790497, 'accumulated_eval_time': 2113.768192052841, 'accumulated_logging_time': 5.269103527069092, 'global_step': 180123, 'preemption_count': 0}), (181637, {'train/accuracy': 0.9604192972183228, 'train/loss': 0.14689314365386963, 'validation/accuracy': 0.7546799778938293, 'validation/loss': 1.0567476749420166, 'validation/num_examples': 50000, 'test/accuracy': 0.633400022983551, 'test/loss': 1.8171926736831665, 'test/num_examples': 10000, 'score': 61240.57641124725, 'total_duration': 63383.426446676254, 'accumulated_submission_time': 61240.57641124725, 'accumulated_eval_time': 2131.186047077179, 'accumulated_logging_time': 5.325862407684326, 'global_step': 181637, 'preemption_count': 0}), (183151, {'train/accuracy': 0.9615154266357422, 'train/loss': 0.14372612535953522, 'validation/accuracy': 0.7544800043106079, 'validation/loss': 1.0552575588226318, 'validation/num_examples': 50000, 'test/accuracy': 0.6343000531196594, 'test/loss': 1.8140968084335327, 'test/num_examples': 10000, 'score': 61750.644112825394, 'total_duration': 63911.53665685654, 'accumulated_submission_time': 61750.644112825394, 'accumulated_eval_time': 2149.111617088318, 'accumulated_logging_time': 5.389604806900024, 'global_step': 183151, 'preemption_count': 0}), (184665, {'train/accuracy': 0.9613161683082581, 'train/loss': 0.14664237201213837, 'validation/accuracy': 0.7540599703788757, 'validation/loss': 1.0541568994522095, 'validation/num_examples': 50000, 'test/accuracy': 0.6353000402450562, 'test/loss': 1.8131952285766602, 'test/num_examples': 10000, 'score': 62260.7015068531, 'total_duration': 64439.141348838806, 'accumulated_submission_time': 62260.7015068531, 'accumulated_eval_time': 2166.556547164917, 'accumulated_logging_time': 5.439829349517822, 'global_step': 184665, 'preemption_count': 0}), (186179, {'train/accuracy': 0.9597018361091614, 'train/loss': 0.14918778836727142, 'validation/accuracy': 0.7537399530410767, 'validation/loss': 1.0555750131607056, 'validation/num_examples': 50000, 'test/accuracy': 0.634600043296814, 'test/loss': 1.8140379190444946, 'test/num_examples': 10000, 'score': 62770.74982833862, 'total_duration': 64966.6444914341, 'accumulated_submission_time': 62770.74982833862, 'accumulated_eval_time': 2183.8824162483215, 'accumulated_logging_time': 5.5159690380096436, 'global_step': 186179, 'preemption_count': 0}), (186666, {'train/accuracy': 0.9602798223495483, 'train/loss': 0.14576058089733124, 'validation/accuracy': 0.75409996509552, 'validation/loss': 1.054492473602295, 'validation/num_examples': 50000, 'test/accuracy': 0.6342000365257263, 'test/loss': 1.8139417171478271, 'test/num_examples': 10000, 'score': 62934.7845287323, 'total_duration': 65148.003600120544, 'accumulated_submission_time': 62934.7845287323, 'accumulated_eval_time': 2201.129693508148, 'accumulated_logging_time': 5.575985908508301, 'global_step': 186666, 'preemption_count': 0})], 'global_step': 186666}
I0203 15:19:53.155383 140070692116288 submission_runner.py:586] Timing: 62934.7845287323
I0203 15:19:53.155493 140070692116288 submission_runner.py:588] Total number of evals: 125
I0203 15:19:53.155554 140070692116288 submission_runner.py:589] ====================
I0203 15:19:53.157050 140070692116288 submission_runner.py:673] Final imagenet_resnet score: 62676.811291217804
