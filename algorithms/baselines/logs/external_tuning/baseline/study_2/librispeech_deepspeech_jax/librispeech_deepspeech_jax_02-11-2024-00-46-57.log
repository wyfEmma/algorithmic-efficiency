python3 submission_runner.py --framework=jax --workload=librispeech_deepspeech --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification/study_2 --overwrite=true --save_checkpoints=false --num_tuning_trials=5 --rng_seed=1231499801 --max_global_steps=48000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_deepspeech_jax_02-11-2024-00-46-57.log
I0211 00:47:18.260290 140441227016000 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification/study_2/librispeech_deepspeech_jax.
I0211 00:47:19.295806 140441227016000 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0211 00:47:19.297110 140441227016000 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0211 00:47:19.297260 140441227016000 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0211 00:47:19.298441 140441227016000 submission_runner.py:542] Using RNG seed 1231499801
I0211 00:47:20.397716 140441227016000 submission_runner.py:551] --- Tuning run 1/5 ---
I0211 00:47:20.397912 140441227016000 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_2/librispeech_deepspeech_jax/trial_1.
I0211 00:47:20.398215 140441227016000 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_2/librispeech_deepspeech_jax/trial_1/hparams.json.
I0211 00:47:20.582483 140441227016000 submission_runner.py:206] Initializing dataset.
I0211 00:47:20.582731 140441227016000 submission_runner.py:213] Initializing model.
I0211 00:47:23.112197 140441227016000 submission_runner.py:255] Initializing optimizer.
I0211 00:47:23.812749 140441227016000 submission_runner.py:262] Initializing metrics bundle.
I0211 00:47:23.812968 140441227016000 submission_runner.py:280] Initializing checkpoint and logger.
I0211 00:47:23.814005 140441227016000 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_2/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I0211 00:47:23.814155 140441227016000 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_2/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I0211 00:47:23.814398 140441227016000 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0211 00:47:23.814479 140441227016000 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0211 00:47:24.123243 140441227016000 logger_utils.py:220] Unable to record git information. Continuing without it.
I0211 00:47:24.402046 140441227016000 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_2/librispeech_deepspeech_jax/trial_1/flags_0.json.
I0211 00:47:24.415397 140441227016000 submission_runner.py:314] Starting training loop.
I0211 00:47:24.721449 140441227016000 input_pipeline.py:20] Loading split = train-clean-100
I0211 00:47:24.760751 140441227016000 input_pipeline.py:20] Loading split = train-clean-360
I0211 00:47:24.890662 140441227016000 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0211 00:48:08.538990 140278010713856 logging_writer.py:48] [0] global_step=0, grad_norm=19.473045349121094, loss=32.96833801269531
I0211 00:48:08.572644 140441227016000 spec.py:321] Evaluating on the training split.
I0211 00:48:08.830770 140441227016000 input_pipeline.py:20] Loading split = train-clean-100
I0211 00:48:08.866219 140441227016000 input_pipeline.py:20] Loading split = train-clean-360
I0211 00:48:09.230818 140441227016000 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0211 00:49:59.008410 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 00:49:59.205068 140441227016000 input_pipeline.py:20] Loading split = dev-clean
I0211 00:49:59.211006 140441227016000 input_pipeline.py:20] Loading split = dev-other
I0211 00:51:12.099604 140441227016000 spec.py:349] Evaluating on the test split.
I0211 00:51:12.298596 140441227016000 input_pipeline.py:20] Loading split = test-clean
I0211 00:51:54.839775 140441227016000 submission_runner.py:408] Time since start: 270.42s, 	Step: 1, 	{'train/ctc_loss': Array(31.563702, dtype=float32), 'train/wer': 3.280985152947546, 'validation/ctc_loss': Array(30.570293, dtype=float32), 'validation/wer': 2.911601996582253, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.657606, dtype=float32), 'test/wer': 3.232161355188593, 'test/num_examples': 2472, 'score': 44.15716743469238, 'total_duration': 270.42189955711365, 'accumulated_submission_time': 44.15716743469238, 'accumulated_eval_time': 226.2646632194519, 'accumulated_logging_time': 0}
I0211 00:51:54.866533 140271585064704 logging_writer.py:48] [1] accumulated_eval_time=226.264663, accumulated_logging_time=0, accumulated_submission_time=44.157167, global_step=1, preemption_count=0, score=44.157167, test/ctc_loss=30.65760612487793, test/num_examples=2472, test/wer=3.232161, total_duration=270.421900, train/ctc_loss=31.563701629638672, train/wer=3.280985, validation/ctc_loss=30.570293426513672, validation/num_examples=5348, validation/wer=2.911602
I0211 00:53:19.986047 140283373565696 logging_writer.py:48] [100] global_step=100, grad_norm=6.953511714935303, loss=9.512772560119629
I0211 00:54:36.900788 140283381958400 logging_writer.py:48] [200] global_step=200, grad_norm=2.7786052227020264, loss=6.6726274490356445
I0211 00:55:53.320667 140283373565696 logging_writer.py:48] [300] global_step=300, grad_norm=0.967351496219635, loss=5.928050518035889
I0211 00:57:10.377078 140283381958400 logging_writer.py:48] [400] global_step=400, grad_norm=0.5167700052261353, loss=5.85891056060791
I0211 00:58:27.204994 140283373565696 logging_writer.py:48] [500] global_step=500, grad_norm=0.6006690263748169, loss=5.83176851272583
I0211 00:59:50.483788 140283381958400 logging_writer.py:48] [600] global_step=600, grad_norm=0.482693612575531, loss=5.797192096710205
I0211 01:01:14.760720 140283373565696 logging_writer.py:48] [700] global_step=700, grad_norm=0.378465473651886, loss=5.772836685180664
I0211 01:02:33.515882 140283381958400 logging_writer.py:48] [800] global_step=800, grad_norm=0.42708656191825867, loss=5.680384635925293
I0211 01:03:58.277784 140283373565696 logging_writer.py:48] [900] global_step=900, grad_norm=0.4224222004413605, loss=5.5707807540893555
I0211 01:05:23.256712 140283381958400 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.9037690758705139, loss=5.4944868087768555
I0211 01:06:46.760544 140285390001920 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.6559107899665833, loss=5.296062469482422
I0211 01:08:04.328341 140285381609216 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.2580963373184204, loss=4.991153240203857
I0211 01:09:22.034937 140285390001920 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.3792940378189087, loss=4.544064521789551
I0211 01:10:40.684909 140285381609216 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.5327479839324951, loss=4.102383613586426
I0211 01:12:07.432579 140285390001920 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.9314594268798828, loss=3.7746987342834473
I0211 01:13:35.415870 140285381609216 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.2932472229003906, loss=3.5489351749420166
I0211 01:15:00.897368 140285390001920 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.895531415939331, loss=3.342949151992798
I0211 01:15:55.804111 140441227016000 spec.py:321] Evaluating on the training split.
I0211 01:16:33.129932 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 01:17:19.350255 140441227016000 spec.py:349] Evaluating on the test split.
I0211 01:17:42.665728 140441227016000 submission_runner.py:408] Time since start: 1818.25s, 	Step: 1767, 	{'train/ctc_loss': Array(6.473987, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': Array(6.4356875, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.3765306, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1485.011241197586, 'total_duration': 1818.2476708889008, 'accumulated_submission_time': 1485.011241197586, 'accumulated_eval_time': 333.1237199306488, 'accumulated_logging_time': 0.04142189025878906}
I0211 01:17:42.694844 140285390001920 logging_writer.py:48] [1767] accumulated_eval_time=333.123720, accumulated_logging_time=0.041422, accumulated_submission_time=1485.011241, global_step=1767, preemption_count=0, score=1485.011241, test/ctc_loss=6.376530647277832, test/num_examples=2472, test/wer=0.899580, total_duration=1818.247671, train/ctc_loss=6.473987102508545, train/wer=0.944636, validation/ctc_loss=6.43568754196167, validation/num_examples=5348, validation/wer=0.896618
I0211 01:18:08.706324 140285381609216 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.6431448459625244, loss=3.1910057067871094
I0211 01:19:24.712616 140285390001920 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.7440943717956543, loss=3.0495731830596924
I0211 01:20:40.198778 140285381609216 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.8505327701568604, loss=2.9230926036834717
I0211 01:22:00.153653 140285390001920 logging_writer.py:48] [2100] global_step=2100, grad_norm=3.4145076274871826, loss=2.8555023670196533
I0211 01:23:16.408068 140285381609216 logging_writer.py:48] [2200] global_step=2200, grad_norm=2.5926737785339355, loss=2.7946977615356445
I0211 01:24:35.026173 140285390001920 logging_writer.py:48] [2300] global_step=2300, grad_norm=3.131178617477417, loss=2.741563558578491
I0211 01:26:00.512292 140285381609216 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.0639474391937256, loss=2.6803267002105713
I0211 01:27:24.761119 140285390001920 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.936511993408203, loss=2.6281726360321045
I0211 01:28:47.434496 140285381609216 logging_writer.py:48] [2600] global_step=2600, grad_norm=2.510591745376587, loss=2.485750913619995
I0211 01:30:10.235255 140285390001920 logging_writer.py:48] [2700] global_step=2700, grad_norm=2.8480913639068604, loss=2.435706615447998
I0211 01:31:30.739917 140285381609216 logging_writer.py:48] [2800] global_step=2800, grad_norm=3.7132866382598877, loss=2.510906934738159
I0211 01:32:59.590749 140285390001920 logging_writer.py:48] [2900] global_step=2900, grad_norm=4.012956142425537, loss=2.4026150703430176
I0211 01:34:24.874163 140285381609216 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.259812831878662, loss=2.332202672958374
I0211 01:35:53.239230 140285390001920 logging_writer.py:48] [3100] global_step=3100, grad_norm=2.675532579421997, loss=2.3211259841918945
I0211 01:37:10.594156 140285381609216 logging_writer.py:48] [3200] global_step=3200, grad_norm=4.650546073913574, loss=2.254101276397705
I0211 01:38:30.536355 140285390001920 logging_writer.py:48] [3300] global_step=3300, grad_norm=4.5164384841918945, loss=2.2264716625213623
I0211 01:39:51.381133 140285381609216 logging_writer.py:48] [3400] global_step=3400, grad_norm=3.780531883239746, loss=2.248483419418335
I0211 01:41:14.340997 140285390001920 logging_writer.py:48] [3500] global_step=3500, grad_norm=4.616823673248291, loss=2.1617088317871094
I0211 01:41:43.221774 140441227016000 spec.py:321] Evaluating on the training split.
I0211 01:42:28.021767 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 01:43:17.112793 140441227016000 spec.py:349] Evaluating on the test split.
I0211 01:43:43.108351 140441227016000 submission_runner.py:408] Time since start: 3378.69s, 	Step: 3537, 	{'train/ctc_loss': Array(3.7205336, dtype=float32), 'train/wer': 0.7672056042404709, 'validation/ctc_loss': Array(4.0462813, dtype=float32), 'validation/wer': 0.7830116724755496, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.662109, dtype=float32), 'test/wer': 0.7429772713423923, 'test/num_examples': 2472, 'score': 2925.4576818943024, 'total_duration': 3378.6899876594543, 'accumulated_submission_time': 2925.4576818943024, 'accumulated_eval_time': 453.00739574432373, 'accumulated_logging_time': 0.08230781555175781}
I0211 01:43:43.137150 140285390001920 logging_writer.py:48] [3537] accumulated_eval_time=453.007396, accumulated_logging_time=0.082308, accumulated_submission_time=2925.457682, global_step=3537, preemption_count=0, score=2925.457682, test/ctc_loss=3.662108898162842, test/num_examples=2472, test/wer=0.742977, total_duration=3378.689988, train/ctc_loss=3.720533609390259, train/wer=0.767206, validation/ctc_loss=4.046281337738037, validation/num_examples=5348, validation/wer=0.783012
I0211 01:44:31.171511 140285381609216 logging_writer.py:48] [3600] global_step=3600, grad_norm=2.8412792682647705, loss=2.1211776733398438
I0211 01:45:46.077134 140285390001920 logging_writer.py:48] [3700] global_step=3700, grad_norm=3.1103243827819824, loss=2.1528162956237793
I0211 01:47:01.827950 140285381609216 logging_writer.py:48] [3800] global_step=3800, grad_norm=2.970688581466675, loss=2.1259989738464355
I0211 01:48:26.599808 140285390001920 logging_writer.py:48] [3900] global_step=3900, grad_norm=3.770599126815796, loss=2.079712390899658
I0211 01:49:52.624572 140285381609216 logging_writer.py:48] [4000] global_step=4000, grad_norm=5.749728202819824, loss=2.0935614109039307
I0211 01:51:18.149932 140285390001920 logging_writer.py:48] [4100] global_step=4100, grad_norm=4.822004318237305, loss=2.0914688110351562
I0211 01:52:38.501346 140283423921920 logging_writer.py:48] [4200] global_step=4200, grad_norm=4.210311412811279, loss=1.9915119409561157
I0211 01:53:54.293262 140283415529216 logging_writer.py:48] [4300] global_step=4300, grad_norm=2.6960184574127197, loss=2.0569493770599365
I0211 01:55:14.186207 140283423921920 logging_writer.py:48] [4400] global_step=4400, grad_norm=4.279494285583496, loss=1.9657121896743774
I0211 01:56:34.069631 140283415529216 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.675780773162842, loss=2.042055368423462
I0211 01:58:02.693747 140283423921920 logging_writer.py:48] [4600] global_step=4600, grad_norm=4.427332878112793, loss=1.9828221797943115
I0211 01:59:28.781714 140283415529216 logging_writer.py:48] [4700] global_step=4700, grad_norm=3.0590970516204834, loss=2.0020010471343994
I0211 02:00:57.125253 140283423921920 logging_writer.py:48] [4800] global_step=4800, grad_norm=3.530097007751465, loss=1.9443715810775757
I0211 02:02:21.513910 140283415529216 logging_writer.py:48] [4900] global_step=4900, grad_norm=4.63032865524292, loss=2.004509210586548
I0211 02:03:48.294688 140283423921920 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.795675039291382, loss=1.8764657974243164
I0211 02:05:16.034085 140283415529216 logging_writer.py:48] [5100] global_step=5100, grad_norm=3.5135719776153564, loss=1.887010097503662
I0211 02:06:41.760372 140284079281920 logging_writer.py:48] [5200] global_step=5200, grad_norm=3.5944392681121826, loss=1.9028223752975464
I0211 02:07:43.360617 140441227016000 spec.py:321] Evaluating on the training split.
I0211 02:08:37.549644 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 02:09:29.398474 140441227016000 spec.py:349] Evaluating on the test split.
I0211 02:09:55.468974 140441227016000 submission_runner.py:408] Time since start: 4951.05s, 	Step: 5280, 	{'train/ctc_loss': Array(0.60861146, dtype=float32), 'train/wer': 0.2074207610523375, 'validation/ctc_loss': Array(1.0145419, dtype=float32), 'validation/wer': 0.285150178128349, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.671695, dtype=float32), 'test/wer': 0.21412467247577843, 'test/num_examples': 2472, 'score': 4365.597179412842, 'total_duration': 4951.050794363022, 'accumulated_submission_time': 4365.597179412842, 'accumulated_eval_time': 585.1130557060242, 'accumulated_logging_time': 0.12415671348571777}
I0211 02:09:55.494251 140284150961920 logging_writer.py:48] [5280] accumulated_eval_time=585.113056, accumulated_logging_time=0.124157, accumulated_submission_time=4365.597179, global_step=5280, preemption_count=0, score=4365.597179, test/ctc_loss=0.6716949939727783, test/num_examples=2472, test/wer=0.214125, total_duration=4951.050794, train/ctc_loss=0.6086114645004272, train/wer=0.207421, validation/ctc_loss=1.0145418643951416, validation/num_examples=5348, validation/wer=0.285150
I0211 02:10:11.485929 140284142569216 logging_writer.py:48] [5300] global_step=5300, grad_norm=5.131211280822754, loss=1.8532381057739258
I0211 02:11:26.754388 140284150961920 logging_writer.py:48] [5400] global_step=5400, grad_norm=2.7616825103759766, loss=1.905533790588379
I0211 02:12:42.625363 140284142569216 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.9781415462493896, loss=1.9307738542556763
I0211 02:13:57.876679 140284150961920 logging_writer.py:48] [5600] global_step=5600, grad_norm=3.0305800437927246, loss=1.8520009517669678
I0211 02:15:17.325521 140284142569216 logging_writer.py:48] [5700] global_step=5700, grad_norm=2.8229572772979736, loss=1.8651221990585327
I0211 02:16:43.822015 140284150961920 logging_writer.py:48] [5800] global_step=5800, grad_norm=3.2533509731292725, loss=1.7972897291183472
I0211 02:18:10.819633 140284142569216 logging_writer.py:48] [5900] global_step=5900, grad_norm=3.659773111343384, loss=1.8383681774139404
I0211 02:19:39.041006 140284150961920 logging_writer.py:48] [6000] global_step=6000, grad_norm=3.427371025085449, loss=1.8490099906921387
I0211 02:21:01.857012 140284142569216 logging_writer.py:48] [6100] global_step=6100, grad_norm=2.521181106567383, loss=1.8323463201522827
I0211 02:22:29.465165 140284150961920 logging_writer.py:48] [6200] global_step=6200, grad_norm=3.958700656890869, loss=1.7662078142166138
I0211 02:23:46.044687 140284142569216 logging_writer.py:48] [6300] global_step=6300, grad_norm=4.709691047668457, loss=1.7868919372558594
I0211 02:25:02.364102 140284150961920 logging_writer.py:48] [6400] global_step=6400, grad_norm=4.248286247253418, loss=1.7740728855133057
I0211 02:26:21.030807 140284142569216 logging_writer.py:48] [6500] global_step=6500, grad_norm=3.5967767238616943, loss=1.6899430751800537
I0211 02:27:43.532973 140284150961920 logging_writer.py:48] [6600] global_step=6600, grad_norm=3.001425266265869, loss=1.7879111766815186
I0211 02:29:12.950246 140284142569216 logging_writer.py:48] [6700] global_step=6700, grad_norm=2.531282424926758, loss=1.817785382270813
I0211 02:30:38.706977 140284150961920 logging_writer.py:48] [6800] global_step=6800, grad_norm=2.155482292175293, loss=1.7090725898742676
I0211 02:32:03.498424 140284142569216 logging_writer.py:48] [6900] global_step=6900, grad_norm=3.017730236053467, loss=1.7131696939468384
I0211 02:33:30.262918 140284150961920 logging_writer.py:48] [7000] global_step=7000, grad_norm=3.3434927463531494, loss=1.8547495603561401
I0211 02:33:55.949475 140441227016000 spec.py:321] Evaluating on the training split.
I0211 02:35:01.607599 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 02:35:54.022096 140441227016000 spec.py:349] Evaluating on the test split.
I0211 02:36:20.389497 140441227016000 submission_runner.py:408] Time since start: 6535.97s, 	Step: 7030, 	{'train/ctc_loss': Array(0.49595514, dtype=float32), 'train/wer': 0.16683597063536115, 'validation/ctc_loss': Array(0.8424785, dtype=float32), 'validation/wer': 0.2426503953580428, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5282411, dtype=float32), 'test/wer': 0.17077976154205513, 'test/num_examples': 2472, 'score': 5805.970959663391, 'total_duration': 6535.9713497161865, 'accumulated_submission_time': 5805.970959663391, 'accumulated_eval_time': 729.5503969192505, 'accumulated_logging_time': 0.16060924530029297}
I0211 02:36:20.415846 140284150961920 logging_writer.py:48] [7030] accumulated_eval_time=729.550397, accumulated_logging_time=0.160609, accumulated_submission_time=5805.970960, global_step=7030, preemption_count=0, score=5805.970960, test/ctc_loss=0.5282410979270935, test/num_examples=2472, test/wer=0.170780, total_duration=6535.971350, train/ctc_loss=0.49595513939857483, train/wer=0.166836, validation/ctc_loss=0.8424785137176514, validation/num_examples=5348, validation/wer=0.242650
I0211 02:37:13.281327 140284142569216 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.6870360374450684, loss=1.767981767654419
I0211 02:38:28.123741 140284150961920 logging_writer.py:48] [7200] global_step=7200, grad_norm=2.489063262939453, loss=1.8103463649749756
I0211 02:39:48.183103 140282840241920 logging_writer.py:48] [7300] global_step=7300, grad_norm=2.1057403087615967, loss=1.7849454879760742
I0211 02:41:08.664036 140282831849216 logging_writer.py:48] [7400] global_step=7400, grad_norm=2.4340641498565674, loss=1.7200865745544434
I0211 02:42:28.336972 140282840241920 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.9974207878112793, loss=1.7407523393630981
I0211 02:43:46.438247 140282831849216 logging_writer.py:48] [7600] global_step=7600, grad_norm=3.9325456619262695, loss=1.7232036590576172
I0211 02:45:11.977428 140282840241920 logging_writer.py:48] [7700] global_step=7700, grad_norm=2.2726335525512695, loss=1.716442346572876
I0211 02:46:37.238024 140282831849216 logging_writer.py:48] [7800] global_step=7800, grad_norm=2.531315326690674, loss=1.7124900817871094
I0211 02:48:02.383636 140282840241920 logging_writer.py:48] [7900] global_step=7900, grad_norm=2.1620841026306152, loss=1.66208815574646
I0211 02:49:30.444698 140282831849216 logging_writer.py:48] [8000] global_step=8000, grad_norm=3.2129275798797607, loss=1.6681793928146362
I0211 02:50:56.991651 140282840241920 logging_writer.py:48] [8100] global_step=8100, grad_norm=6.582836151123047, loss=1.6535838842391968
I0211 02:52:22.565222 140282831849216 logging_writer.py:48] [8200] global_step=8200, grad_norm=3.2176077365875244, loss=1.7677863836288452
I0211 02:53:45.741539 140285390001920 logging_writer.py:48] [8300] global_step=8300, grad_norm=3.6711361408233643, loss=1.7244064807891846
I0211 02:55:04.890714 140285381609216 logging_writer.py:48] [8400] global_step=8400, grad_norm=2.582754611968994, loss=1.6849699020385742
I0211 02:56:21.543511 140285390001920 logging_writer.py:48] [8500] global_step=8500, grad_norm=4.21978235244751, loss=1.6447336673736572
I0211 02:57:44.092796 140285381609216 logging_writer.py:48] [8600] global_step=8600, grad_norm=2.2726263999938965, loss=1.6542739868164062
I0211 02:59:11.763159 140285390001920 logging_writer.py:48] [8700] global_step=8700, grad_norm=2.550262928009033, loss=1.6770093441009521
I0211 03:00:20.607898 140441227016000 spec.py:321] Evaluating on the training split.
I0211 03:01:19.014112 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 03:02:10.098813 140441227016000 spec.py:349] Evaluating on the test split.
I0211 03:02:36.812513 140441227016000 submission_runner.py:408] Time since start: 8112.39s, 	Step: 8781, 	{'train/ctc_loss': Array(0.4343019, dtype=float32), 'train/wer': 0.1476276935788993, 'validation/ctc_loss': Array(0.7638279, dtype=float32), 'validation/wer': 0.2213232667484094, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.47641897, dtype=float32), 'test/wer': 0.1522149777588203, 'test/num_examples': 2472, 'score': 7246.0776579380035, 'total_duration': 8112.394110202789, 'accumulated_submission_time': 7246.0776579380035, 'accumulated_eval_time': 865.7520883083344, 'accumulated_logging_time': 0.2023007869720459}
I0211 03:02:36.841995 140285390001920 logging_writer.py:48] [8781] accumulated_eval_time=865.752088, accumulated_logging_time=0.202301, accumulated_submission_time=7246.077658, global_step=8781, preemption_count=0, score=7246.077658, test/ctc_loss=0.47641897201538086, test/num_examples=2472, test/wer=0.152215, total_duration=8112.394110, train/ctc_loss=0.4343019127845764, train/wer=0.147628, validation/ctc_loss=0.763827919960022, validation/num_examples=5348, validation/wer=0.221323
I0211 03:02:51.844180 140285381609216 logging_writer.py:48] [8800] global_step=8800, grad_norm=2.605682373046875, loss=1.6416376829147339
I0211 03:04:06.726204 140285390001920 logging_writer.py:48] [8900] global_step=8900, grad_norm=3.851344108581543, loss=1.7024948596954346
I0211 03:05:21.671915 140285381609216 logging_writer.py:48] [9000] global_step=9000, grad_norm=2.4293174743652344, loss=1.6816530227661133
I0211 03:06:42.909339 140285390001920 logging_writer.py:48] [9100] global_step=9100, grad_norm=2.5723581314086914, loss=1.7504040002822876
I0211 03:08:11.409899 140285381609216 logging_writer.py:48] [9200] global_step=9200, grad_norm=2.413698434829712, loss=1.6923096179962158
I0211 03:09:39.125847 140285062321920 logging_writer.py:48] [9300] global_step=9300, grad_norm=2.304607391357422, loss=1.6613562107086182
I0211 03:10:55.574061 140285053929216 logging_writer.py:48] [9400] global_step=9400, grad_norm=2.810044288635254, loss=1.6958619356155396
I0211 03:12:12.311055 140285062321920 logging_writer.py:48] [9500] global_step=9500, grad_norm=2.8213181495666504, loss=1.6784850358963013
I0211 03:13:30.396317 140285053929216 logging_writer.py:48] [9600] global_step=9600, grad_norm=2.5635669231414795, loss=1.662754774093628
I0211 03:14:53.072308 140285062321920 logging_writer.py:48] [9700] global_step=9700, grad_norm=2.1383559703826904, loss=1.7137620449066162
I0211 03:16:21.195333 140285053929216 logging_writer.py:48] [9800] global_step=9800, grad_norm=2.4098970890045166, loss=1.626679539680481
I0211 03:17:48.958934 140285062321920 logging_writer.py:48] [9900] global_step=9900, grad_norm=3.905683994293213, loss=1.6366431713104248
I0211 03:19:16.963498 140285053929216 logging_writer.py:48] [10000] global_step=10000, grad_norm=3.1380977630615234, loss=1.6553562879562378
I0211 03:20:42.664863 140285062321920 logging_writer.py:48] [10100] global_step=10100, grad_norm=3.2124149799346924, loss=1.6067957878112793
I0211 03:22:11.169323 140285053929216 logging_writer.py:48] [10200] global_step=10200, grad_norm=3.1651787757873535, loss=1.6380362510681152
I0211 03:23:39.871641 140285390001920 logging_writer.py:48] [10300] global_step=10300, grad_norm=2.2867190837860107, loss=1.6530004739761353
I0211 03:24:58.369611 140285381609216 logging_writer.py:48] [10400] global_step=10400, grad_norm=3.0094828605651855, loss=1.5975970029830933
I0211 03:26:18.293846 140285390001920 logging_writer.py:48] [10500] global_step=10500, grad_norm=2.7892820835113525, loss=1.628643274307251
I0211 03:26:37.494523 140441227016000 spec.py:321] Evaluating on the training split.
I0211 03:27:35.788874 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 03:28:27.473810 140441227016000 spec.py:349] Evaluating on the test split.
I0211 03:28:53.782186 140441227016000 submission_runner.py:408] Time since start: 9689.36s, 	Step: 10527, 	{'train/ctc_loss': Array(0.42308322, dtype=float32), 'train/wer': 0.14123690991245713, 'validation/ctc_loss': Array(0.724271, dtype=float32), 'validation/wer': 0.20801915483167113, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.44031197, dtype=float32), 'test/wer': 0.14069831210773262, 'test/num_examples': 2472, 'score': 8686.643969297409, 'total_duration': 9689.364002227783, 'accumulated_submission_time': 8686.643969297409, 'accumulated_eval_time': 1002.0370271205902, 'accumulated_logging_time': 0.24602818489074707}
I0211 03:28:53.809749 140285390001920 logging_writer.py:48] [10527] accumulated_eval_time=1002.037027, accumulated_logging_time=0.246028, accumulated_submission_time=8686.643969, global_step=10527, preemption_count=0, score=8686.643969, test/ctc_loss=0.4403119683265686, test/num_examples=2472, test/wer=0.140698, total_duration=9689.364002, train/ctc_loss=0.42308321595191956, train/wer=0.141237, validation/ctc_loss=0.7242709994316101, validation/num_examples=5348, validation/wer=0.208019
I0211 03:29:49.076044 140285381609216 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.899609923362732, loss=1.5836528539657593
I0211 03:31:04.268593 140285390001920 logging_writer.py:48] [10700] global_step=10700, grad_norm=3.271726608276367, loss=1.6270081996917725
I0211 03:32:22.191807 140285381609216 logging_writer.py:48] [10800] global_step=10800, grad_norm=3.957353353500366, loss=1.6484944820404053
I0211 03:33:48.403980 140285390001920 logging_writer.py:48] [10900] global_step=10900, grad_norm=2.1632766723632812, loss=1.6651431322097778
I0211 03:35:17.009231 140285381609216 logging_writer.py:48] [11000] global_step=11000, grad_norm=2.2546145915985107, loss=1.6274387836456299
I0211 03:36:43.159993 140285390001920 logging_writer.py:48] [11100] global_step=11100, grad_norm=2.721501111984253, loss=1.6152400970458984
I0211 03:38:08.650165 140285381609216 logging_writer.py:48] [11200] global_step=11200, grad_norm=2.9065632820129395, loss=1.600452184677124
I0211 03:39:36.170590 140285390001920 logging_writer.py:48] [11300] global_step=11300, grad_norm=2.13608980178833, loss=1.6012918949127197
I0211 03:40:58.589218 140283208881920 logging_writer.py:48] [11400] global_step=11400, grad_norm=4.756042957305908, loss=1.575960397720337
I0211 03:42:15.273382 140283200489216 logging_writer.py:48] [11500] global_step=11500, grad_norm=3.59287691116333, loss=1.6992747783660889
I0211 03:43:34.816984 140283208881920 logging_writer.py:48] [11600] global_step=11600, grad_norm=3.307730197906494, loss=1.6591131687164307
I0211 03:44:54.316084 140283200489216 logging_writer.py:48] [11700] global_step=11700, grad_norm=2.979794979095459, loss=1.5587595701217651
I0211 03:46:22.049336 140283208881920 logging_writer.py:48] [11800] global_step=11800, grad_norm=2.1502184867858887, loss=1.6416499614715576
I0211 03:47:47.863359 140283200489216 logging_writer.py:48] [11900] global_step=11900, grad_norm=2.7813684940338135, loss=1.6334080696105957
I0211 03:49:15.246124 140283208881920 logging_writer.py:48] [12000] global_step=12000, grad_norm=3.467860460281372, loss=1.6129703521728516
I0211 03:50:42.420293 140283200489216 logging_writer.py:48] [12100] global_step=12100, grad_norm=3.831001043319702, loss=1.5963411331176758
I0211 03:52:08.734228 140283208881920 logging_writer.py:48] [12200] global_step=12200, grad_norm=2.7099711894989014, loss=1.6415131092071533
I0211 03:52:53.823666 140441227016000 spec.py:321] Evaluating on the training split.
I0211 03:53:47.358644 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 03:54:40.501955 140441227016000 spec.py:349] Evaluating on the test split.
I0211 03:55:06.624915 140441227016000 submission_runner.py:408] Time since start: 11262.21s, 	Step: 12254, 	{'train/ctc_loss': Array(0.3849505, dtype=float32), 'train/wer': 0.12991619339331675, 'validation/ctc_loss': Array(0.70631933, dtype=float32), 'validation/wer': 0.20398350985257344, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42263985, dtype=float32), 'test/wer': 0.13685942355737005, 'test/num_examples': 2472, 'score': 10126.575286149979, 'total_duration': 11262.206323862076, 'accumulated_submission_time': 10126.575286149979, 'accumulated_eval_time': 1134.8351573944092, 'accumulated_logging_time': 0.28536081314086914}
I0211 03:55:06.650235 140283208881920 logging_writer.py:48] [12254] accumulated_eval_time=1134.835157, accumulated_logging_time=0.285361, accumulated_submission_time=10126.575286, global_step=12254, preemption_count=0, score=10126.575286, test/ctc_loss=0.4226398468017578, test/num_examples=2472, test/wer=0.136859, total_duration=11262.206324, train/ctc_loss=0.3849504888057709, train/wer=0.129916, validation/ctc_loss=0.7063193321228027, validation/num_examples=5348, validation/wer=0.203984
I0211 03:55:42.020730 140283200489216 logging_writer.py:48] [12300] global_step=12300, grad_norm=2.846315383911133, loss=1.6512763500213623
I0211 03:57:02.482152 140284519601920 logging_writer.py:48] [12400] global_step=12400, grad_norm=2.39135479927063, loss=1.5936015844345093
I0211 03:58:19.240964 140284511209216 logging_writer.py:48] [12500] global_step=12500, grad_norm=2.639101982116699, loss=1.6071330308914185
I0211 03:59:38.759591 140284519601920 logging_writer.py:48] [12600] global_step=12600, grad_norm=2.546252965927124, loss=1.5726393461227417
I0211 04:01:02.143563 140284511209216 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.7641429901123047, loss=1.5909305810928345
I0211 04:02:25.164614 140284519601920 logging_writer.py:48] [12800] global_step=12800, grad_norm=3.299203395843506, loss=1.6136415004730225
I0211 04:03:52.656153 140284511209216 logging_writer.py:48] [12900] global_step=12900, grad_norm=4.1761674880981445, loss=1.6032427549362183
I0211 04:05:19.243069 140284519601920 logging_writer.py:48] [13000] global_step=13000, grad_norm=3.6153993606567383, loss=1.6315699815750122
I0211 04:06:48.450199 140284511209216 logging_writer.py:48] [13100] global_step=13100, grad_norm=2.917144298553467, loss=1.6196595430374146
I0211 04:08:18.015517 140284519601920 logging_writer.py:48] [13200] global_step=13200, grad_norm=2.849013328552246, loss=1.5917798280715942
I0211 04:09:48.906317 140284511209216 logging_writer.py:48] [13300] global_step=13300, grad_norm=5.177732944488525, loss=1.5922290086746216
I0211 04:11:17.339091 140284519601920 logging_writer.py:48] [13400] global_step=13400, grad_norm=2.9205572605133057, loss=1.5718799829483032
I0211 04:12:33.513740 140284511209216 logging_writer.py:48] [13500] global_step=13500, grad_norm=2.4703567028045654, loss=1.5843232870101929
I0211 04:13:51.674071 140284519601920 logging_writer.py:48] [13600] global_step=13600, grad_norm=2.7398934364318848, loss=1.5971043109893799
I0211 04:15:09.363894 140284511209216 logging_writer.py:48] [13700] global_step=13700, grad_norm=4.7921271324157715, loss=1.5623455047607422
I0211 04:16:34.140995 140284519601920 logging_writer.py:48] [13800] global_step=13800, grad_norm=2.7897756099700928, loss=1.612433671951294
I0211 04:17:59.519044 140284511209216 logging_writer.py:48] [13900] global_step=13900, grad_norm=4.002048492431641, loss=1.584152340888977
I0211 04:19:06.776343 140441227016000 spec.py:321] Evaluating on the training split.
I0211 04:20:08.790538 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 04:21:02.666905 140441227016000 spec.py:349] Evaluating on the test split.
I0211 04:21:29.170928 140441227016000 submission_runner.py:408] Time since start: 12844.75s, 	Step: 13977, 	{'train/ctc_loss': Array(0.32947904, dtype=float32), 'train/wer': 0.1149223848774019, 'validation/ctc_loss': Array(0.66567683, dtype=float32), 'validation/wer': 0.19206966797648126, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3976388, dtype=float32), 'test/wer': 0.1286941685454878, 'test/num_examples': 2472, 'score': 11566.618283510208, 'total_duration': 12844.752668619156, 'accumulated_submission_time': 11566.618283510208, 'accumulated_eval_time': 1277.2269763946533, 'accumulated_logging_time': 0.32526302337646484}
I0211 04:21:29.198031 140284519601920 logging_writer.py:48] [13977] accumulated_eval_time=1277.226976, accumulated_logging_time=0.325263, accumulated_submission_time=11566.618284, global_step=13977, preemption_count=0, score=11566.618284, test/ctc_loss=0.39763879776000977, test/num_examples=2472, test/wer=0.128694, total_duration=12844.752669, train/ctc_loss=0.32947903871536255, train/wer=0.114922, validation/ctc_loss=0.6656768321990967, validation/num_examples=5348, validation/wer=0.192070
I0211 04:21:47.179277 140284511209216 logging_writer.py:48] [14000] global_step=14000, grad_norm=2.6998133659362793, loss=1.6169923543930054
I0211 04:23:02.088960 140284519601920 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.95350182056427, loss=1.6417258977890015
I0211 04:24:16.990297 140284511209216 logging_writer.py:48] [14200] global_step=14200, grad_norm=2.9383437633514404, loss=1.5735182762145996
I0211 04:25:41.574314 140284519601920 logging_writer.py:48] [14300] global_step=14300, grad_norm=3.093289613723755, loss=1.6838749647140503
I0211 04:27:07.607695 140284511209216 logging_writer.py:48] [14400] global_step=14400, grad_norm=2.568990468978882, loss=1.582398772239685
I0211 04:28:30.331210 140284519601920 logging_writer.py:48] [14500] global_step=14500, grad_norm=2.366523504257202, loss=1.5462977886199951
I0211 04:29:45.687088 140284511209216 logging_writer.py:48] [14600] global_step=14600, grad_norm=6.843161106109619, loss=1.542982816696167
I0211 04:31:05.361009 140284519601920 logging_writer.py:48] [14700] global_step=14700, grad_norm=2.1957998275756836, loss=1.5337684154510498
I0211 04:32:28.604685 140284511209216 logging_writer.py:48] [14800] global_step=14800, grad_norm=4.103470802307129, loss=1.5628573894500732
I0211 04:33:52.810616 140284519601920 logging_writer.py:48] [14900] global_step=14900, grad_norm=2.8817713260650635, loss=1.5666857957839966
I0211 04:35:15.193540 140284511209216 logging_writer.py:48] [15000] global_step=15000, grad_norm=2.9262855052948, loss=1.586648941040039
I0211 04:36:42.928992 140284519601920 logging_writer.py:48] [15100] global_step=15100, grad_norm=2.6850128173828125, loss=1.479028582572937
I0211 04:38:10.173620 140284511209216 logging_writer.py:48] [15200] global_step=15200, grad_norm=2.7175211906433105, loss=1.4792711734771729
I0211 04:39:38.024863 140284519601920 logging_writer.py:48] [15300] global_step=15300, grad_norm=3.678953170776367, loss=1.5937377214431763
I0211 04:41:05.278702 140284511209216 logging_writer.py:48] [15400] global_step=15400, grad_norm=2.899260997772217, loss=1.532183289527893
I0211 04:42:29.885004 140283864241920 logging_writer.py:48] [15500] global_step=15500, grad_norm=2.254011869430542, loss=1.5277901887893677
I0211 04:43:44.939296 140283855849216 logging_writer.py:48] [15600] global_step=15600, grad_norm=3.907792806625366, loss=1.5712226629257202
I0211 04:45:04.501869 140283864241920 logging_writer.py:48] [15700] global_step=15700, grad_norm=3.2662525177001953, loss=1.555128574371338
I0211 04:45:30.399274 140441227016000 spec.py:321] Evaluating on the training split.
I0211 04:46:24.133973 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 04:47:15.777544 140441227016000 spec.py:349] Evaluating on the test split.
I0211 04:47:42.401776 140441227016000 submission_runner.py:408] Time since start: 14417.98s, 	Step: 15733, 	{'train/ctc_loss': Array(0.3160166, dtype=float32), 'train/wer': 0.10870409482414653, 'validation/ctc_loss': Array(0.65010995, dtype=float32), 'validation/wer': 0.18813056952798402, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3880893, dtype=float32), 'test/wer': 0.12428655576544188, 'test/num_examples': 2472, 'score': 13007.737249851227, 'total_duration': 14417.983313083649, 'accumulated_submission_time': 13007.737249851227, 'accumulated_eval_time': 1409.226472377777, 'accumulated_logging_time': 0.3636348247528076}
I0211 04:47:42.427784 140284519601920 logging_writer.py:48] [15733] accumulated_eval_time=1409.226472, accumulated_logging_time=0.363635, accumulated_submission_time=13007.737250, global_step=15733, preemption_count=0, score=13007.737250, test/ctc_loss=0.38808929920196533, test/num_examples=2472, test/wer=0.124287, total_duration=14417.983313, train/ctc_loss=0.31601661443710327, train/wer=0.108704, validation/ctc_loss=0.6501099467277527, validation/num_examples=5348, validation/wer=0.188131
I0211 04:48:33.745032 140284511209216 logging_writer.py:48] [15800] global_step=15800, grad_norm=3.056903600692749, loss=1.5357640981674194
I0211 04:49:48.608566 140284519601920 logging_writer.py:48] [15900] global_step=15900, grad_norm=2.9395956993103027, loss=1.5309282541275024
I0211 04:51:08.635989 140284511209216 logging_writer.py:48] [16000] global_step=16000, grad_norm=2.1867165565490723, loss=1.5227288007736206
I0211 04:52:32.875597 140284519601920 logging_writer.py:48] [16100] global_step=16100, grad_norm=3.440673828125, loss=1.558250069618225
I0211 04:53:57.139413 140284511209216 logging_writer.py:48] [16200] global_step=16200, grad_norm=2.244419813156128, loss=1.5404880046844482
I0211 04:55:24.509703 140284519601920 logging_writer.py:48] [16300] global_step=16300, grad_norm=2.5336170196533203, loss=1.550932765007019
I0211 04:56:51.229159 140284511209216 logging_writer.py:48] [16400] global_step=16400, grad_norm=2.500535488128662, loss=1.5151394605636597
I0211 04:58:20.544373 140284519601920 logging_writer.py:48] [16500] global_step=16500, grad_norm=2.5500104427337646, loss=1.4779068231582642
I0211 04:59:38.045195 140284511209216 logging_writer.py:48] [16600] global_step=16600, grad_norm=2.932149648666382, loss=1.5191625356674194
I0211 05:00:53.837640 140284519601920 logging_writer.py:48] [16700] global_step=16700, grad_norm=2.6951427459716797, loss=1.5236608982086182
I0211 05:02:11.820527 140284511209216 logging_writer.py:48] [16800] global_step=16800, grad_norm=3.4807026386260986, loss=1.566275715827942
I0211 05:03:37.892164 140284519601920 logging_writer.py:48] [16900] global_step=16900, grad_norm=2.971635103225708, loss=1.4896116256713867
I0211 05:05:03.123682 140284511209216 logging_writer.py:48] [17000] global_step=17000, grad_norm=2.3196849822998047, loss=1.4989469051361084
I0211 05:06:27.093796 140284519601920 logging_writer.py:48] [17100] global_step=17100, grad_norm=2.533634662628174, loss=1.497301459312439
I0211 05:07:51.673715 140284511209216 logging_writer.py:48] [17200] global_step=17200, grad_norm=2.921142339706421, loss=1.5183130502700806
I0211 05:09:17.124647 140284519601920 logging_writer.py:48] [17300] global_step=17300, grad_norm=2.658663272857666, loss=1.561996340751648
I0211 05:10:44.389054 140284511209216 logging_writer.py:48] [17400] global_step=17400, grad_norm=3.831144332885742, loss=1.5046569108963013
I0211 05:11:43.103538 140441227016000 spec.py:321] Evaluating on the training split.
I0211 05:12:42.097528 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 05:13:33.366966 140441227016000 spec.py:349] Evaluating on the test split.
I0211 05:14:00.300749 140441227016000 submission_runner.py:408] Time since start: 15995.88s, 	Step: 17470, 	{'train/ctc_loss': Array(0.3091924, dtype=float32), 'train/wer': 0.10841596636402151, 'validation/ctc_loss': Array(0.62598205, dtype=float32), 'validation/wer': 0.18160402405939544, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.370601, dtype=float32), 'test/wer': 0.1191274145390287, 'test/num_examples': 2472, 'score': 14448.331030368805, 'total_duration': 15995.882573604584, 'accumulated_submission_time': 14448.331030368805, 'accumulated_eval_time': 1546.4209856987, 'accumulated_logging_time': 0.4011971950531006}
I0211 05:14:00.327313 140284883121920 logging_writer.py:48] [17470] accumulated_eval_time=1546.420986, accumulated_logging_time=0.401197, accumulated_submission_time=14448.331030, global_step=17470, preemption_count=0, score=14448.331030, test/ctc_loss=0.37060099840164185, test/num_examples=2472, test/wer=0.119127, total_duration=15995.882574, train/ctc_loss=0.30919238924980164, train/wer=0.108416, validation/ctc_loss=0.6259820461273193, validation/num_examples=5348, validation/wer=0.181604
I0211 05:14:23.888629 140284874729216 logging_writer.py:48] [17500] global_step=17500, grad_norm=2.8354172706604004, loss=1.5557531118392944
I0211 05:15:44.467432 140284883121920 logging_writer.py:48] [17600] global_step=17600, grad_norm=2.6474416255950928, loss=1.5482560396194458
I0211 05:17:03.890043 140284874729216 logging_writer.py:48] [17700] global_step=17700, grad_norm=2.6268491744995117, loss=1.562575340270996
I0211 05:18:25.968442 140284883121920 logging_writer.py:48] [17800] global_step=17800, grad_norm=3.4635136127471924, loss=1.5330811738967896
I0211 05:19:47.410704 140284874729216 logging_writer.py:48] [17900] global_step=17900, grad_norm=2.2734549045562744, loss=1.588568925857544
I0211 05:21:12.542618 140284883121920 logging_writer.py:48] [18000] global_step=18000, grad_norm=2.8140363693237305, loss=1.5327904224395752
I0211 05:22:36.958196 140284874729216 logging_writer.py:48] [18100] global_step=18100, grad_norm=2.5356993675231934, loss=1.5271165370941162
I0211 05:24:07.903723 140284883121920 logging_writer.py:48] [18200] global_step=18200, grad_norm=3.182469606399536, loss=1.5409493446350098
I0211 05:25:31.863446 140284874729216 logging_writer.py:48] [18300] global_step=18300, grad_norm=2.737196207046509, loss=1.5044854879379272
I0211 05:26:56.573437 140284883121920 logging_writer.py:48] [18400] global_step=18400, grad_norm=4.130111217498779, loss=1.5360461473464966
I0211 05:28:22.495494 140284874729216 logging_writer.py:48] [18500] global_step=18500, grad_norm=4.408894062042236, loss=1.5502840280532837
I0211 05:29:48.436182 140284883121920 logging_writer.py:48] [18600] global_step=18600, grad_norm=2.9804458618164062, loss=1.5257152318954468
I0211 05:31:06.609332 140284874729216 logging_writer.py:48] [18700] global_step=18700, grad_norm=2.120607614517212, loss=1.527637004852295
I0211 05:32:29.751500 140284883121920 logging_writer.py:48] [18800] global_step=18800, grad_norm=3.2011983394622803, loss=1.484130620956421
I0211 05:33:49.999102 140284874729216 logging_writer.py:48] [18900] global_step=18900, grad_norm=3.0307488441467285, loss=1.4988757371902466
I0211 05:35:15.414549 140284883121920 logging_writer.py:48] [19000] global_step=19000, grad_norm=2.0298385620117188, loss=1.512266755104065
I0211 05:36:41.688850 140284874729216 logging_writer.py:48] [19100] global_step=19100, grad_norm=2.3782334327697754, loss=1.4495407342910767
I0211 05:38:00.517189 140441227016000 spec.py:321] Evaluating on the training split.
I0211 05:38:54.238231 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 05:39:47.193127 140441227016000 spec.py:349] Evaluating on the test split.
I0211 05:40:13.475946 140441227016000 submission_runner.py:408] Time since start: 17569.06s, 	Step: 19191, 	{'train/ctc_loss': Array(0.31268734, dtype=float32), 'train/wer': 0.10624818213067505, 'validation/ctc_loss': Array(0.60505414, dtype=float32), 'validation/wer': 0.1773077034476766, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35781944, dtype=float32), 'test/wer': 0.11474011333861435, 'test/num_examples': 2472, 'score': 15888.437488555908, 'total_duration': 17569.057680606842, 'accumulated_submission_time': 15888.437488555908, 'accumulated_eval_time': 1679.376972436905, 'accumulated_logging_time': 0.4410090446472168}
I0211 05:40:13.506718 140284883121920 logging_writer.py:48] [19191] accumulated_eval_time=1679.376972, accumulated_logging_time=0.441009, accumulated_submission_time=15888.437489, global_step=19191, preemption_count=0, score=15888.437489, test/ctc_loss=0.35781943798065186, test/num_examples=2472, test/wer=0.114740, total_duration=17569.057681, train/ctc_loss=0.31268733739852905, train/wer=0.106248, validation/ctc_loss=0.6050541400909424, validation/num_examples=5348, validation/wer=0.177308
I0211 05:40:21.174176 140284874729216 logging_writer.py:48] [19200] global_step=19200, grad_norm=3.279906749725342, loss=1.5702067613601685
I0211 05:41:36.430909 140284883121920 logging_writer.py:48] [19300] global_step=19300, grad_norm=2.7953603267669678, loss=1.483119010925293
I0211 05:42:51.752843 140284874729216 logging_writer.py:48] [19400] global_step=19400, grad_norm=3.3404505252838135, loss=1.517870306968689
I0211 05:44:12.440180 140284883121920 logging_writer.py:48] [19500] global_step=19500, grad_norm=3.330918788909912, loss=1.4917267560958862
I0211 05:45:41.353248 140284883121920 logging_writer.py:48] [19600] global_step=19600, grad_norm=2.6029982566833496, loss=1.5516581535339355
I0211 05:46:59.641406 140284874729216 logging_writer.py:48] [19700] global_step=19700, grad_norm=2.6812403202056885, loss=1.5067133903503418
I0211 05:48:15.366615 140284883121920 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.8869236707687378, loss=1.4482048749923706
I0211 05:49:35.628760 140284874729216 logging_writer.py:48] [19900] global_step=19900, grad_norm=2.982912063598633, loss=1.5839473009109497
I0211 05:51:00.462412 140284883121920 logging_writer.py:48] [20000] global_step=20000, grad_norm=2.1451101303100586, loss=1.4988422393798828
I0211 05:52:27.193693 140284874729216 logging_writer.py:48] [20100] global_step=20100, grad_norm=2.2114977836608887, loss=1.4409219026565552
I0211 05:53:53.885812 140284883121920 logging_writer.py:48] [20200] global_step=20200, grad_norm=3.402944326400757, loss=1.4482486248016357
I0211 05:55:20.729420 140284874729216 logging_writer.py:48] [20300] global_step=20300, grad_norm=2.6302545070648193, loss=1.495882272720337
I0211 05:56:47.125470 140284883121920 logging_writer.py:48] [20400] global_step=20400, grad_norm=3.3166663646698, loss=1.4664616584777832
I0211 05:58:16.574348 140284874729216 logging_writer.py:48] [20500] global_step=20500, grad_norm=4.975632667541504, loss=1.5994712114334106
I0211 05:59:45.725371 140284883121920 logging_writer.py:48] [20600] global_step=20600, grad_norm=2.864354372024536, loss=1.452646017074585
I0211 06:01:03.810914 140284874729216 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.952825665473938, loss=1.4647068977355957
I0211 06:02:20.922869 140284883121920 logging_writer.py:48] [20800] global_step=20800, grad_norm=2.27248215675354, loss=1.4515252113342285
I0211 06:03:39.591664 140284874729216 logging_writer.py:48] [20900] global_step=20900, grad_norm=2.8849198818206787, loss=1.5134443044662476
I0211 06:04:13.972926 140441227016000 spec.py:321] Evaluating on the training split.
I0211 06:05:11.035075 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 06:06:02.031722 140441227016000 spec.py:349] Evaluating on the test split.
I0211 06:06:30.497248 140441227016000 submission_runner.py:408] Time since start: 19146.08s, 	Step: 20942, 	{'train/ctc_loss': Array(0.31071246, dtype=float32), 'train/wer': 0.10659244994328225, 'validation/ctc_loss': Array(0.5931073, dtype=float32), 'validation/wer': 0.17373548181546097, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34689486, dtype=float32), 'test/wer': 0.11303394064956432, 'test/num_examples': 2472, 'score': 17328.820681095123, 'total_duration': 19146.078830242157, 'accumulated_submission_time': 17328.820681095123, 'accumulated_eval_time': 1815.8983781337738, 'accumulated_logging_time': 0.48394131660461426}
I0211 06:06:30.529016 140284883121920 logging_writer.py:48] [20942] accumulated_eval_time=1815.898378, accumulated_logging_time=0.483941, accumulated_submission_time=17328.820681, global_step=20942, preemption_count=0, score=17328.820681, test/ctc_loss=0.34689486026763916, test/num_examples=2472, test/wer=0.113034, total_duration=19146.078830, train/ctc_loss=0.31071245670318604, train/wer=0.106592, validation/ctc_loss=0.593107283115387, validation/num_examples=5348, validation/wer=0.173735
I0211 06:07:14.837012 140284874729216 logging_writer.py:48] [21000] global_step=21000, grad_norm=2.964245319366455, loss=1.455954670906067
I0211 06:08:29.752547 140284883121920 logging_writer.py:48] [21100] global_step=21100, grad_norm=2.80754017829895, loss=1.5042763948440552
I0211 06:09:49.980516 140284874729216 logging_writer.py:48] [21200] global_step=21200, grad_norm=3.2357072830200195, loss=1.421863079071045
I0211 06:11:14.699507 140284883121920 logging_writer.py:48] [21300] global_step=21300, grad_norm=2.7890546321868896, loss=1.4907361268997192
I0211 06:12:42.009377 140284874729216 logging_writer.py:48] [21400] global_step=21400, grad_norm=2.859053611755371, loss=1.4916225671768188
I0211 06:14:09.258339 140284883121920 logging_writer.py:48] [21500] global_step=21500, grad_norm=2.8563132286071777, loss=1.4502345323562622
I0211 06:15:34.983466 140284874729216 logging_writer.py:48] [21600] global_step=21600, grad_norm=4.1570725440979, loss=1.4812923669815063
I0211 06:17:00.897078 140284883121920 logging_writer.py:48] [21700] global_step=21700, grad_norm=3.5288503170013428, loss=1.3834679126739502
I0211 06:18:16.443566 140284874729216 logging_writer.py:48] [21800] global_step=21800, grad_norm=2.7974941730499268, loss=1.485290765762329
I0211 06:19:31.758982 140284883121920 logging_writer.py:48] [21900] global_step=21900, grad_norm=2.9204294681549072, loss=1.4300122261047363
I0211 06:20:55.121127 140284874729216 logging_writer.py:48] [22000] global_step=22000, grad_norm=2.102263927459717, loss=1.460457444190979
I0211 06:22:18.903985 140284883121920 logging_writer.py:48] [22100] global_step=22100, grad_norm=2.6620278358459473, loss=1.4710665941238403
I0211 06:23:48.703424 140284874729216 logging_writer.py:48] [22200] global_step=22200, grad_norm=3.7181456089019775, loss=1.4225215911865234
I0211 06:25:18.801066 140284883121920 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.9746707677841187, loss=1.4047585725784302
I0211 06:26:44.937832 140284874729216 logging_writer.py:48] [22400] global_step=22400, grad_norm=2.4075560569763184, loss=1.436089038848877
I0211 06:28:10.994070 140284883121920 logging_writer.py:48] [22500] global_step=22500, grad_norm=3.3794069290161133, loss=1.4414325952529907
I0211 06:29:36.545825 140284874729216 logging_writer.py:48] [22600] global_step=22600, grad_norm=4.074309825897217, loss=1.466446876525879
I0211 06:30:30.644572 140441227016000 spec.py:321] Evaluating on the training split.
I0211 06:31:26.472638 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 06:32:18.985017 140441227016000 spec.py:349] Evaluating on the test split.
I0211 06:32:45.361066 140441227016000 submission_runner.py:408] Time since start: 20720.94s, 	Step: 22661, 	{'train/ctc_loss': Array(0.29349634, dtype=float32), 'train/wer': 0.10054532359296224, 'validation/ctc_loss': Array(0.57543117, dtype=float32), 'validation/wer': 0.168493005203858, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33769685, dtype=float32), 'test/wer': 0.10811853837872971, 'test/num_examples': 2472, 'score': 18768.853582143784, 'total_duration': 20720.94268655777, 'accumulated_submission_time': 18768.853582143784, 'accumulated_eval_time': 1950.6121413707733, 'accumulated_logging_time': 0.5289254188537598}
I0211 06:32:45.387911 140284883121920 logging_writer.py:48] [22661] accumulated_eval_time=1950.612141, accumulated_logging_time=0.528925, accumulated_submission_time=18768.853582, global_step=22661, preemption_count=0, score=18768.853582, test/ctc_loss=0.3376968502998352, test/num_examples=2472, test/wer=0.108119, total_duration=20720.942687, train/ctc_loss=0.29349634051322937, train/wer=0.100545, validation/ctc_loss=0.5754311680793762, validation/num_examples=5348, validation/wer=0.168493
I0211 06:33:16.095113 140284874729216 logging_writer.py:48] [22700] global_step=22700, grad_norm=2.2584197521209717, loss=1.4088549613952637
I0211 06:34:32.327577 140284883121920 logging_writer.py:48] [22800] global_step=22800, grad_norm=3.352245807647705, loss=1.4579746723175049
I0211 06:35:47.378067 140284874729216 logging_writer.py:48] [22900] global_step=22900, grad_norm=3.8585383892059326, loss=1.4498822689056396
I0211 06:37:02.621739 140284883121920 logging_writer.py:48] [23000] global_step=23000, grad_norm=2.943021774291992, loss=1.486854910850525
I0211 06:38:21.521693 140284874729216 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.739266037940979, loss=1.4839625358581543
I0211 06:39:47.222527 140284883121920 logging_writer.py:48] [23200] global_step=23200, grad_norm=3.1414713859558105, loss=1.4021610021591187
I0211 06:41:17.193636 140284874729216 logging_writer.py:48] [23300] global_step=23300, grad_norm=4.091973304748535, loss=1.413217544555664
I0211 06:42:45.089271 140284883121920 logging_writer.py:48] [23400] global_step=23400, grad_norm=5.106176376342773, loss=1.4728697538375854
I0211 06:44:15.474417 140284874729216 logging_writer.py:48] [23500] global_step=23500, grad_norm=3.3936054706573486, loss=1.427957534790039
I0211 06:45:40.617947 140284883121920 logging_writer.py:48] [23600] global_step=23600, grad_norm=2.2062764167785645, loss=1.4870922565460205
I0211 06:47:07.993222 140284883121920 logging_writer.py:48] [23700] global_step=23700, grad_norm=3.972998857498169, loss=1.4160000085830688
I0211 06:48:24.459977 140284874729216 logging_writer.py:48] [23800] global_step=23800, grad_norm=2.673142194747925, loss=1.4230279922485352
I0211 06:49:43.773204 140284883121920 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.8364003896713257, loss=1.4229073524475098
I0211 06:51:01.551995 140284874729216 logging_writer.py:48] [24000] global_step=24000, grad_norm=2.976142168045044, loss=1.438940167427063
I0211 06:52:26.292900 140284883121920 logging_writer.py:48] [24100] global_step=24100, grad_norm=3.1608686447143555, loss=1.4460809230804443
I0211 06:53:55.952149 140284874729216 logging_writer.py:48] [24200] global_step=24200, grad_norm=5.57922887802124, loss=1.407320261001587
I0211 06:55:23.507547 140284883121920 logging_writer.py:48] [24300] global_step=24300, grad_norm=2.263136386871338, loss=1.385759711265564
I0211 06:56:45.709823 140441227016000 spec.py:321] Evaluating on the training split.
I0211 06:57:48.096652 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 06:58:41.302532 140441227016000 spec.py:349] Evaluating on the test split.
I0211 06:59:07.638804 140441227016000 submission_runner.py:408] Time since start: 22303.22s, 	Step: 24399, 	{'train/ctc_loss': Array(0.26594213, dtype=float32), 'train/wer': 0.0915554664076686, 'validation/ctc_loss': Array(0.5630687, dtype=float32), 'validation/wer': 0.16348224026569605, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.325007, dtype=float32), 'test/wer': 0.10482806247841894, 'test/num_examples': 2472, 'score': 20209.092022895813, 'total_duration': 22303.220635175705, 'accumulated_submission_time': 20209.092022895813, 'accumulated_eval_time': 2092.538419485092, 'accumulated_logging_time': 0.5695466995239258}
I0211 06:59:07.665874 140284079277824 logging_writer.py:48] [24399] accumulated_eval_time=2092.538419, accumulated_logging_time=0.569547, accumulated_submission_time=20209.092023, global_step=24399, preemption_count=0, score=20209.092023, test/ctc_loss=0.32500699162483215, test/num_examples=2472, test/wer=0.104828, total_duration=22303.220635, train/ctc_loss=0.26594212651252747, train/wer=0.091555, validation/ctc_loss=0.563068687915802, validation/num_examples=5348, validation/wer=0.163482
I0211 06:59:09.269320 140284070885120 logging_writer.py:48] [24400] global_step=24400, grad_norm=2.668177843093872, loss=1.4061542749404907
I0211 07:00:24.590944 140284079277824 logging_writer.py:48] [24500] global_step=24500, grad_norm=2.414921283721924, loss=1.4501094818115234
I0211 07:01:39.547103 140284070885120 logging_writer.py:48] [24600] global_step=24600, grad_norm=2.814073324203491, loss=1.4061440229415894
I0211 07:03:03.235559 140284079277824 logging_writer.py:48] [24700] global_step=24700, grad_norm=2.949734687805176, loss=1.464585781097412
I0211 07:04:25.348677 140283423917824 logging_writer.py:48] [24800] global_step=24800, grad_norm=3.9533376693725586, loss=1.459369421005249
I0211 07:05:41.227318 140283415525120 logging_writer.py:48] [24900] global_step=24900, grad_norm=3.1255838871002197, loss=1.4371120929718018
I0211 07:06:58.830242 140283423917824 logging_writer.py:48] [25000] global_step=25000, grad_norm=3.004467248916626, loss=1.4106976985931396
I0211 07:08:24.641147 140283415525120 logging_writer.py:48] [25100] global_step=25100, grad_norm=2.454852819442749, loss=1.4792429208755493
I0211 07:09:46.649852 140283423917824 logging_writer.py:48] [25200] global_step=25200, grad_norm=3.1767146587371826, loss=1.459160566329956
I0211 07:11:12.607194 140283415525120 logging_writer.py:48] [25300] global_step=25300, grad_norm=2.6295840740203857, loss=1.4117562770843506
I0211 07:12:37.545714 140283423917824 logging_writer.py:48] [25400] global_step=25400, grad_norm=3.320631742477417, loss=1.4741294384002686
I0211 07:14:05.835537 140283415525120 logging_writer.py:48] [25500] global_step=25500, grad_norm=2.4725632667541504, loss=1.4221729040145874
I0211 07:15:33.768446 140283423917824 logging_writer.py:48] [25600] global_step=25600, grad_norm=2.9125444889068604, loss=1.415581464767456
I0211 07:17:03.451849 140283415525120 logging_writer.py:48] [25700] global_step=25700, grad_norm=3.9820942878723145, loss=1.4414392709732056
I0211 07:18:28.793952 140283423917824 logging_writer.py:48] [25800] global_step=25800, grad_norm=2.7382490634918213, loss=1.4236160516738892
I0211 07:19:47.123471 140283415525120 logging_writer.py:48] [25900] global_step=25900, grad_norm=3.000896453857422, loss=1.2970918416976929
I0211 07:21:08.250836 140283423917824 logging_writer.py:48] [26000] global_step=26000, grad_norm=2.860860824584961, loss=1.393004298210144
I0211 07:22:28.951435 140283415525120 logging_writer.py:48] [26100] global_step=26100, grad_norm=3.6863906383514404, loss=1.391579508781433
I0211 07:23:07.801867 140441227016000 spec.py:321] Evaluating on the training split.
I0211 07:24:12.044086 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 07:25:05.450346 140441227016000 spec.py:349] Evaluating on the test split.
I0211 07:25:31.524768 140441227016000 submission_runner.py:408] Time since start: 23887.11s, 	Step: 26147, 	{'train/ctc_loss': Array(0.25006896, dtype=float32), 'train/wer': 0.08782180025561438, 'validation/ctc_loss': Array(0.5523809, dtype=float32), 'validation/wer': 0.1610009944292652, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3190817, dtype=float32), 'test/wer': 0.10212662238742307, 'test/num_examples': 2472, 'score': 21649.144829034805, 'total_duration': 23887.106551885605, 'accumulated_submission_time': 21649.144829034805, 'accumulated_eval_time': 2236.258577108383, 'accumulated_logging_time': 0.6100704669952393}
I0211 07:25:31.554552 140283126953728 logging_writer.py:48] [26147] accumulated_eval_time=2236.258577, accumulated_logging_time=0.610070, accumulated_submission_time=21649.144829, global_step=26147, preemption_count=0, score=21649.144829, test/ctc_loss=0.31908169388771057, test/num_examples=2472, test/wer=0.102127, total_duration=23887.106552, train/ctc_loss=0.2500689625740051, train/wer=0.087822, validation/ctc_loss=0.5523809194564819, validation/num_examples=5348, validation/wer=0.161001
I0211 07:26:12.121402 140283118561024 logging_writer.py:48] [26200] global_step=26200, grad_norm=2.969210386276245, loss=1.399700403213501
I0211 07:27:27.140877 140283126953728 logging_writer.py:48] [26300] global_step=26300, grad_norm=3.445746898651123, loss=1.4187521934509277
I0211 07:28:41.928890 140283118561024 logging_writer.py:48] [26400] global_step=26400, grad_norm=2.6487221717834473, loss=1.368243932723999
I0211 07:30:10.355250 140283126953728 logging_writer.py:48] [26500] global_step=26500, grad_norm=3.6746842861175537, loss=1.3894619941711426
I0211 07:31:39.449305 140283118561024 logging_writer.py:48] [26600] global_step=26600, grad_norm=3.026982069015503, loss=1.416218876838684
I0211 07:33:07.203689 140283126953728 logging_writer.py:48] [26700] global_step=26700, grad_norm=2.4810428619384766, loss=1.4145523309707642
I0211 07:34:37.060697 140283126953728 logging_writer.py:48] [26800] global_step=26800, grad_norm=2.3346688747406006, loss=1.418514370918274
I0211 07:35:55.645985 140283118561024 logging_writer.py:48] [26900] global_step=26900, grad_norm=3.0573618412017822, loss=1.4158947467803955
I0211 07:37:16.689229 140283126953728 logging_writer.py:48] [27000] global_step=27000, grad_norm=2.8378517627716064, loss=1.4176472425460815
I0211 07:38:35.491289 140283118561024 logging_writer.py:48] [27100] global_step=27100, grad_norm=2.6296684741973877, loss=1.4424710273742676
I0211 07:40:03.967740 140283126953728 logging_writer.py:48] [27200] global_step=27200, grad_norm=2.8219714164733887, loss=1.3380756378173828
I0211 07:41:29.420189 140283118561024 logging_writer.py:48] [27300] global_step=27300, grad_norm=3.005007028579712, loss=1.4315019845962524
I0211 07:42:54.574040 140283126953728 logging_writer.py:48] [27400] global_step=27400, grad_norm=2.5603127479553223, loss=1.3766729831695557
I0211 07:44:24.303623 140283118561024 logging_writer.py:48] [27500] global_step=27500, grad_norm=3.627039909362793, loss=1.3733199834823608
I0211 07:45:51.950700 140283126953728 logging_writer.py:48] [27600] global_step=27600, grad_norm=2.7153618335723877, loss=1.460186243057251
I0211 07:47:19.256365 140283118561024 logging_writer.py:48] [27700] global_step=27700, grad_norm=3.242168664932251, loss=1.4400553703308105
I0211 07:48:45.276026 140283126953728 logging_writer.py:48] [27800] global_step=27800, grad_norm=2.312074899673462, loss=1.3952891826629639
I0211 07:49:31.818910 140441227016000 spec.py:321] Evaluating on the training split.
I0211 07:50:32.623411 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 07:51:25.550671 140441227016000 spec.py:349] Evaluating on the test split.
I0211 07:51:53.368098 140441227016000 submission_runner.py:408] Time since start: 25468.95s, 	Step: 27855, 	{'train/ctc_loss': Array(0.23860557, dtype=float32), 'train/wer': 0.0828977917787483, 'validation/ctc_loss': Array(0.5355645, dtype=float32), 'validation/wer': 0.15535302238914045, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30643246, dtype=float32), 'test/wer': 0.09810492962037658, 'test/num_examples': 2472, 'score': 23089.323447704315, 'total_duration': 25468.949790000916, 'accumulated_submission_time': 23089.323447704315, 'accumulated_eval_time': 2377.8051433563232, 'accumulated_logging_time': 0.6545779705047607}
I0211 07:51:53.396007 140283126953728 logging_writer.py:48] [27855] accumulated_eval_time=2377.805143, accumulated_logging_time=0.654578, accumulated_submission_time=23089.323448, global_step=27855, preemption_count=0, score=23089.323448, test/ctc_loss=0.30643245577812195, test/num_examples=2472, test/wer=0.098105, total_duration=25468.949790, train/ctc_loss=0.2386055737733841, train/wer=0.082898, validation/ctc_loss=0.5355644822120667, validation/num_examples=5348, validation/wer=0.155353
I0211 07:52:28.025626 140283118561024 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.9617210626602173, loss=1.3450061082839966
I0211 07:53:43.156209 140283126953728 logging_writer.py:48] [28000] global_step=28000, grad_norm=2.6290371417999268, loss=1.3348149061203003
I0211 07:54:58.238219 140283118561024 logging_writer.py:48] [28100] global_step=28100, grad_norm=2.748124837875366, loss=1.3686147928237915
I0211 07:56:13.162905 140283126953728 logging_writer.py:48] [28200] global_step=28200, grad_norm=2.3638367652893066, loss=1.4051263332366943
I0211 07:57:37.114183 140283118561024 logging_writer.py:48] [28300] global_step=28300, grad_norm=3.200117826461792, loss=1.4432659149169922
I0211 07:59:03.661776 140283126953728 logging_writer.py:48] [28400] global_step=28400, grad_norm=3.005758762359619, loss=1.3362284898757935
I0211 08:00:31.711751 140283118561024 logging_writer.py:48] [28500] global_step=28500, grad_norm=3.2463619709014893, loss=1.3635121583938599
I0211 08:01:59.535817 140283126953728 logging_writer.py:48] [28600] global_step=28600, grad_norm=2.92934250831604, loss=1.4103870391845703
I0211 08:03:27.828521 140283118561024 logging_writer.py:48] [28700] global_step=28700, grad_norm=3.5660362243652344, loss=1.3515195846557617
I0211 08:04:58.117151 140283126953728 logging_writer.py:48] [28800] global_step=28800, grad_norm=2.3356664180755615, loss=1.3664013147354126
I0211 08:06:22.403830 140282799273728 logging_writer.py:48] [28900] global_step=28900, grad_norm=2.9066810607910156, loss=1.3082219362258911
I0211 08:07:41.870234 140282790881024 logging_writer.py:48] [29000] global_step=29000, grad_norm=2.67561674118042, loss=1.3269487619400024
I0211 08:08:57.045540 140282799273728 logging_writer.py:48] [29100] global_step=29100, grad_norm=2.774824619293213, loss=1.361687421798706
I0211 08:10:17.868711 140282790881024 logging_writer.py:48] [29200] global_step=29200, grad_norm=5.10183048248291, loss=1.2869069576263428
I0211 08:11:42.037852 140282799273728 logging_writer.py:48] [29300] global_step=29300, grad_norm=3.0127439498901367, loss=1.4119489192962646
I0211 08:13:10.307412 140282790881024 logging_writer.py:48] [29400] global_step=29400, grad_norm=3.0713982582092285, loss=1.3359038829803467
I0211 08:14:38.873592 140282799273728 logging_writer.py:48] [29500] global_step=29500, grad_norm=2.7077574729919434, loss=1.3629387617111206
I0211 08:15:53.928612 140441227016000 spec.py:321] Evaluating on the training split.
I0211 08:16:47.956677 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 08:17:40.359451 140441227016000 spec.py:349] Evaluating on the test split.
I0211 08:18:07.759483 140441227016000 submission_runner.py:408] Time since start: 27043.34s, 	Step: 29591, 	{'train/ctc_loss': Array(0.25175303, dtype=float32), 'train/wer': 0.08719557115617958, 'validation/ctc_loss': Array(0.5208304, dtype=float32), 'validation/wer': 0.15108566573660176, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.293739, dtype=float32), 'test/wer': 0.09325046208843661, 'test/num_examples': 2472, 'score': 24529.774584531784, 'total_duration': 27043.341345071793, 'accumulated_submission_time': 24529.774584531784, 'accumulated_eval_time': 2511.6333718299866, 'accumulated_logging_time': 0.694037675857544}
I0211 08:18:07.789509 140285098161920 logging_writer.py:48] [29591] accumulated_eval_time=2511.633372, accumulated_logging_time=0.694038, accumulated_submission_time=24529.774585, global_step=29591, preemption_count=0, score=24529.774585, test/ctc_loss=0.29373899102211, test/num_examples=2472, test/wer=0.093250, total_duration=27043.341345, train/ctc_loss=0.251753032207489, train/wer=0.087196, validation/ctc_loss=0.5208303928375244, validation/num_examples=5348, validation/wer=0.151086
I0211 08:18:15.407956 140285089769216 logging_writer.py:48] [29600] global_step=29600, grad_norm=2.6223130226135254, loss=1.32638418674469
I0211 08:19:30.445474 140285098161920 logging_writer.py:48] [29700] global_step=29700, grad_norm=3.050971269607544, loss=1.370684027671814
I0211 08:20:45.222776 140285089769216 logging_writer.py:48] [29800] global_step=29800, grad_norm=3.394625663757324, loss=1.3721450567245483
I0211 08:22:08.760938 140284012721920 logging_writer.py:48] [29900] global_step=29900, grad_norm=2.0841751098632812, loss=1.3832855224609375
I0211 08:23:27.345098 140284004329216 logging_writer.py:48] [30000] global_step=30000, grad_norm=2.2968461513519287, loss=1.370513677597046
I0211 08:24:45.456789 140284012721920 logging_writer.py:48] [30100] global_step=30100, grad_norm=2.4455726146698, loss=1.3262741565704346
I0211 08:26:07.984993 140284004329216 logging_writer.py:48] [30200] global_step=30200, grad_norm=2.3168203830718994, loss=1.362019419670105
I0211 08:27:30.373522 140284012721920 logging_writer.py:48] [30300] global_step=30300, grad_norm=2.564453363418579, loss=1.2785313129425049
I0211 08:28:55.988478 140284004329216 logging_writer.py:48] [30400] global_step=30400, grad_norm=3.141709566116333, loss=1.3316967487335205
I0211 08:30:25.101362 140284012721920 logging_writer.py:48] [30500] global_step=30500, grad_norm=2.672405481338501, loss=1.3294399976730347
I0211 08:31:55.625674 140284004329216 logging_writer.py:48] [30600] global_step=30600, grad_norm=2.2739005088806152, loss=1.3244507312774658
I0211 08:33:23.616895 140284012721920 logging_writer.py:48] [30700] global_step=30700, grad_norm=2.8282954692840576, loss=1.346854567527771
I0211 08:34:51.782089 140284004329216 logging_writer.py:48] [30800] global_step=30800, grad_norm=2.862025499343872, loss=1.300858974456787
I0211 08:36:23.741799 140285098161920 logging_writer.py:48] [30900] global_step=30900, grad_norm=2.4128353595733643, loss=1.312684416770935
I0211 08:37:39.983254 140285089769216 logging_writer.py:48] [31000] global_step=31000, grad_norm=5.583098888397217, loss=1.369330883026123
I0211 08:38:57.395967 140285098161920 logging_writer.py:48] [31100] global_step=31100, grad_norm=5.180118083953857, loss=1.3238306045532227
I0211 08:40:14.344980 140285089769216 logging_writer.py:48] [31200] global_step=31200, grad_norm=2.656749963760376, loss=1.28095543384552
I0211 08:41:37.082757 140285098161920 logging_writer.py:48] [31300] global_step=31300, grad_norm=3.479466438293457, loss=1.3163223266601562
I0211 08:42:08.570191 140441227016000 spec.py:321] Evaluating on the training split.
I0211 08:43:02.888929 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 08:43:54.703946 140441227016000 spec.py:349] Evaluating on the test split.
I0211 08:44:21.337978 140441227016000 submission_runner.py:408] Time since start: 28616.92s, 	Step: 31338, 	{'train/ctc_loss': Array(0.22907293, dtype=float32), 'train/wer': 0.07686965203283794, 'validation/ctc_loss': Array(0.50683457, dtype=float32), 'validation/wer': 0.14897129671645248, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2868147, dtype=float32), 'test/wer': 0.09144273150122885, 'test/num_examples': 2472, 'score': 25970.473658800125, 'total_duration': 28616.918856859207, 'accumulated_submission_time': 25970.473658800125, 'accumulated_eval_time': 2644.3974990844727, 'accumulated_logging_time': 0.7355659008026123}
I0211 08:44:21.375446 140285098161920 logging_writer.py:48] [31338] accumulated_eval_time=2644.397499, accumulated_logging_time=0.735566, accumulated_submission_time=25970.473659, global_step=31338, preemption_count=0, score=25970.473659, test/ctc_loss=0.28681468963623047, test/num_examples=2472, test/wer=0.091443, total_duration=28616.918857, train/ctc_loss=0.2290729284286499, train/wer=0.076870, validation/ctc_loss=0.5068345665931702, validation/num_examples=5348, validation/wer=0.148971
I0211 08:45:08.587365 140285089769216 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.9990544319152832, loss=1.2685164213180542
I0211 08:46:23.601200 140285098161920 logging_writer.py:48] [31500] global_step=31500, grad_norm=2.2907214164733887, loss=1.2984111309051514
I0211 08:47:44.090681 140285089769216 logging_writer.py:48] [31600] global_step=31600, grad_norm=2.1513724327087402, loss=1.2889530658721924
I0211 08:49:10.978338 140285098161920 logging_writer.py:48] [31700] global_step=31700, grad_norm=3.4807050228118896, loss=1.3161876201629639
I0211 08:50:37.251466 140285089769216 logging_writer.py:48] [31800] global_step=31800, grad_norm=2.8401124477386475, loss=1.30323326587677
I0211 08:52:08.420176 140285098161920 logging_writer.py:48] [31900] global_step=31900, grad_norm=2.141514778137207, loss=1.3421920537948608
I0211 08:53:30.706497 140284770481920 logging_writer.py:48] [32000] global_step=32000, grad_norm=2.148669481277466, loss=1.2661254405975342
I0211 08:54:52.564241 140284762089216 logging_writer.py:48] [32100] global_step=32100, grad_norm=2.3177011013031006, loss=1.3419299125671387
I0211 08:56:10.442770 140284770481920 logging_writer.py:48] [32200] global_step=32200, grad_norm=3.128577947616577, loss=1.3298059701919556
I0211 08:57:34.311594 140284762089216 logging_writer.py:48] [32300] global_step=32300, grad_norm=2.5155065059661865, loss=1.329347848892212
I0211 08:59:02.258396 140284770481920 logging_writer.py:48] [32400] global_step=32400, grad_norm=2.6224381923675537, loss=1.3034801483154297
I0211 09:00:28.862713 140284762089216 logging_writer.py:48] [32500] global_step=32500, grad_norm=2.289473056793213, loss=1.3899704217910767
I0211 09:01:50.561087 140284770481920 logging_writer.py:48] [32600] global_step=32600, grad_norm=4.844293117523193, loss=1.2397037744522095
I0211 09:03:13.101310 140284762089216 logging_writer.py:48] [32700] global_step=32700, grad_norm=2.1424105167388916, loss=1.2916311025619507
I0211 09:04:42.740227 140284770481920 logging_writer.py:48] [32800] global_step=32800, grad_norm=4.106076717376709, loss=1.2225207090377808
I0211 09:06:08.817131 140284762089216 logging_writer.py:48] [32900] global_step=32900, grad_norm=6.5082106590271, loss=1.348598599433899
I0211 09:07:35.433420 140284770481920 logging_writer.py:48] [33000] global_step=33000, grad_norm=2.7914650440216064, loss=1.280484676361084
I0211 09:08:21.965703 140441227016000 spec.py:321] Evaluating on the training split.
I0211 09:09:24.900407 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 09:10:17.688253 140441227016000 spec.py:349] Evaluating on the test split.
I0211 09:10:43.561776 140441227016000 submission_runner.py:408] Time since start: 30199.14s, 	Step: 33063, 	{'train/ctc_loss': Array(0.23234938, dtype=float32), 'train/wer': 0.07819426821938778, 'validation/ctc_loss': Array(0.49369305, dtype=float32), 'validation/wer': 0.14422120741091168, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27371708, dtype=float32), 'test/wer': 0.08809132086202344, 'test/num_examples': 2472, 'score': 27410.98273205757, 'total_duration': 30199.143298387527, 'accumulated_submission_time': 27410.98273205757, 'accumulated_eval_time': 2785.9905619621277, 'accumulated_logging_time': 0.7848289012908936}
I0211 09:10:43.594099 140284478641920 logging_writer.py:48] [33063] accumulated_eval_time=2785.990562, accumulated_logging_time=0.784829, accumulated_submission_time=27410.982732, global_step=33063, preemption_count=0, score=27410.982732, test/ctc_loss=0.27371707558631897, test/num_examples=2472, test/wer=0.088091, total_duration=30199.143298, train/ctc_loss=0.23234938085079193, train/wer=0.078194, validation/ctc_loss=0.4936930537223816, validation/num_examples=5348, validation/wer=0.144221
I0211 09:11:12.083849 140284470249216 logging_writer.py:48] [33100] global_step=33100, grad_norm=2.03947114944458, loss=1.2791482210159302
I0211 09:12:27.336815 140284478641920 logging_writer.py:48] [33200] global_step=33200, grad_norm=2.264716863632202, loss=1.285117268562317
I0211 09:13:42.652221 140284470249216 logging_writer.py:48] [33300] global_step=33300, grad_norm=2.8866984844207764, loss=1.253800392150879
I0211 09:14:58.864264 140284478641920 logging_writer.py:48] [33400] global_step=33400, grad_norm=2.9484994411468506, loss=1.2927093505859375
I0211 09:16:27.271646 140284470249216 logging_writer.py:48] [33500] global_step=33500, grad_norm=3.2452948093414307, loss=1.2535690069198608
I0211 09:17:56.319803 140284478641920 logging_writer.py:48] [33600] global_step=33600, grad_norm=2.8443424701690674, loss=1.2901058197021484
I0211 09:19:23.310750 140284470249216 logging_writer.py:48] [33700] global_step=33700, grad_norm=2.0036118030548096, loss=1.2936203479766846
I0211 09:20:51.477079 140284478641920 logging_writer.py:48] [33800] global_step=33800, grad_norm=2.694808006286621, loss=1.2439935207366943
I0211 09:22:18.444405 140284470249216 logging_writer.py:48] [33900] global_step=33900, grad_norm=3.344451904296875, loss=1.280773401260376
I0211 09:23:48.999737 140285098161920 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.9728004932403564, loss=1.2563077211380005
I0211 09:25:05.448879 140285089769216 logging_writer.py:48] [34100] global_step=34100, grad_norm=2.9467108249664307, loss=1.3435207605361938
I0211 09:26:23.065157 140285098161920 logging_writer.py:48] [34200] global_step=34200, grad_norm=2.7472281455993652, loss=1.247206211090088
I0211 09:27:41.023066 140285089769216 logging_writer.py:48] [34300] global_step=34300, grad_norm=3.2390873432159424, loss=1.31044602394104
I0211 09:29:00.163834 140285098161920 logging_writer.py:48] [34400] global_step=34400, grad_norm=2.1792211532592773, loss=1.2808758020401
I0211 09:30:26.176863 140285089769216 logging_writer.py:48] [34500] global_step=34500, grad_norm=2.8845269680023193, loss=1.2829824686050415
I0211 09:31:53.377760 140285098161920 logging_writer.py:48] [34600] global_step=34600, grad_norm=2.6972973346710205, loss=1.256632685661316
I0211 09:33:21.041960 140285089769216 logging_writer.py:48] [34700] global_step=34700, grad_norm=3.5105960369110107, loss=1.302958607673645
I0211 09:34:43.597008 140441227016000 spec.py:321] Evaluating on the training split.
I0211 09:35:49.699580 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 09:36:42.920795 140441227016000 spec.py:349] Evaluating on the test split.
I0211 09:37:09.307384 140441227016000 submission_runner.py:408] Time since start: 31784.89s, 	Step: 34798, 	{'train/ctc_loss': Array(0.21853557, dtype=float32), 'train/wer': 0.07230090476038237, 'validation/ctc_loss': Array(0.47827327, dtype=float32), 'validation/wer': 0.13898838545236877, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26734018, dtype=float32), 'test/wer': 0.08638514817297341, 'test/num_examples': 2472, 'score': 28850.900230646133, 'total_duration': 31784.88902115822, 'accumulated_submission_time': 28850.900230646133, 'accumulated_eval_time': 2931.6980545520782, 'accumulated_logging_time': 0.8314599990844727}
I0211 09:37:09.337144 140284509357824 logging_writer.py:48] [34798] accumulated_eval_time=2931.698055, accumulated_logging_time=0.831460, accumulated_submission_time=28850.900231, global_step=34798, preemption_count=0, score=28850.900231, test/ctc_loss=0.26734018325805664, test/num_examples=2472, test/wer=0.086385, total_duration=31784.889021, train/ctc_loss=0.21853557229042053, train/wer=0.072301, validation/ctc_loss=0.47827327251434326, validation/num_examples=5348, validation/wer=0.138988
I0211 09:37:11.703392 140284500965120 logging_writer.py:48] [34800] global_step=34800, grad_norm=2.3027446269989014, loss=1.175746202468872
I0211 09:38:27.240718 140284509357824 logging_writer.py:48] [34900] global_step=34900, grad_norm=3.290191173553467, loss=1.2197984457015991
I0211 09:39:42.545165 140284500965120 logging_writer.py:48] [35000] global_step=35000, grad_norm=5.290782928466797, loss=1.3146929740905762
I0211 09:41:02.727924 140284509357824 logging_writer.py:48] [35100] global_step=35100, grad_norm=5.557498455047607, loss=1.2414261102676392
I0211 09:42:20.980146 140284500965120 logging_writer.py:48] [35200] global_step=35200, grad_norm=2.450395345687866, loss=1.2825686931610107
I0211 09:43:40.618951 140284509357824 logging_writer.py:48] [35300] global_step=35300, grad_norm=2.3409881591796875, loss=1.2650020122528076
I0211 09:45:05.026022 140284500965120 logging_writer.py:48] [35400] global_step=35400, grad_norm=3.3674380779266357, loss=1.307112455368042
I0211 09:46:32.630455 140284509357824 logging_writer.py:48] [35500] global_step=35500, grad_norm=3.604646921157837, loss=1.2670888900756836
I0211 09:47:58.886223 140284500965120 logging_writer.py:48] [35600] global_step=35600, grad_norm=3.8930695056915283, loss=1.2365440130233765
I0211 09:49:26.215922 140284509357824 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.9747037887573242, loss=1.2688318490982056
I0211 09:50:53.481858 140284500965120 logging_writer.py:48] [35800] global_step=35800, grad_norm=2.2901241779327393, loss=1.2295489311218262
I0211 09:52:17.880545 140284509357824 logging_writer.py:48] [35900] global_step=35900, grad_norm=2.931140422821045, loss=1.3035815954208374
I0211 09:53:45.549799 140284500965120 logging_writer.py:48] [36000] global_step=36000, grad_norm=3.2252166271209717, loss=1.240100383758545
I0211 09:55:11.296774 140284509357824 logging_writer.py:48] [36100] global_step=36100, grad_norm=2.4850902557373047, loss=1.2631385326385498
I0211 09:56:29.427248 140284500965120 logging_writer.py:48] [36200] global_step=36200, grad_norm=2.2951879501342773, loss=1.2040196657180786
I0211 09:57:50.402003 140284509357824 logging_writer.py:48] [36300] global_step=36300, grad_norm=2.2902963161468506, loss=1.2546361684799194
I0211 09:59:08.406736 140284500965120 logging_writer.py:48] [36400] global_step=36400, grad_norm=3.4569058418273926, loss=1.2467306852340698
I0211 10:00:33.659419 140284509357824 logging_writer.py:48] [36500] global_step=36500, grad_norm=2.5841434001922607, loss=1.2489540576934814
I0211 10:01:09.493792 140441227016000 spec.py:321] Evaluating on the training split.
I0211 10:02:05.095904 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 10:02:56.902802 140441227016000 spec.py:349] Evaluating on the test split.
I0211 10:03:23.540253 140441227016000 submission_runner.py:408] Time since start: 33359.12s, 	Step: 36544, 	{'train/ctc_loss': Array(0.16265668, dtype=float32), 'train/wer': 0.057286061314227726, 'validation/ctc_loss': Array(0.46007198, dtype=float32), 'validation/wer': 0.13384245537136624, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25568554, dtype=float32), 'test/wer': 0.08142912274287571, 'test/num_examples': 2472, 'score': 30290.972578525543, 'total_duration': 33359.12208724022, 'accumulated_submission_time': 30290.972578525543, 'accumulated_eval_time': 3065.7418246269226, 'accumulated_logging_time': 0.8742971420288086}
I0211 10:03:23.571703 140284376241920 logging_writer.py:48] [36544] accumulated_eval_time=3065.741825, accumulated_logging_time=0.874297, accumulated_submission_time=30290.972579, global_step=36544, preemption_count=0, score=30290.972579, test/ctc_loss=0.2556855380535126, test/num_examples=2472, test/wer=0.081429, total_duration=33359.122087, train/ctc_loss=0.16265667974948883, train/wer=0.057286, validation/ctc_loss=0.46007198095321655, validation/num_examples=5348, validation/wer=0.133842
I0211 10:04:06.774888 140284367849216 logging_writer.py:48] [36600] global_step=36600, grad_norm=6.082652568817139, loss=1.2331492900848389
I0211 10:05:22.275593 140284376241920 logging_writer.py:48] [36700] global_step=36700, grad_norm=3.0716118812561035, loss=1.2292152643203735
I0211 10:06:38.997478 140284367849216 logging_writer.py:48] [36800] global_step=36800, grad_norm=2.9856231212615967, loss=1.1965155601501465
I0211 10:08:05.370588 140284376241920 logging_writer.py:48] [36900] global_step=36900, grad_norm=2.919037342071533, loss=1.2685637474060059
I0211 10:09:32.711437 140284367849216 logging_writer.py:48] [37000] global_step=37000, grad_norm=2.519683599472046, loss=1.2318618297576904
I0211 10:11:02.843574 140283720881920 logging_writer.py:48] [37100] global_step=37100, grad_norm=3.547135353088379, loss=1.2627049684524536
I0211 10:12:18.360125 140283712489216 logging_writer.py:48] [37200] global_step=37200, grad_norm=2.199913263320923, loss=1.234395146369934
I0211 10:13:37.011219 140283720881920 logging_writer.py:48] [37300] global_step=37300, grad_norm=2.6187362670898438, loss=1.2023433446884155
I0211 10:15:00.374118 140283712489216 logging_writer.py:48] [37400] global_step=37400, grad_norm=3.2287418842315674, loss=1.2413475513458252
I0211 10:16:25.540478 140283720881920 logging_writer.py:48] [37500] global_step=37500, grad_norm=2.33958101272583, loss=1.2328258752822876
I0211 10:17:53.882245 140283712489216 logging_writer.py:48] [37600] global_step=37600, grad_norm=2.232814311981201, loss=1.2318470478057861
I0211 10:19:20.791538 140283720881920 logging_writer.py:48] [37700] global_step=37700, grad_norm=2.4978206157684326, loss=1.2271617650985718
I0211 10:20:46.636482 140283712489216 logging_writer.py:48] [37800] global_step=37800, grad_norm=2.3370254039764404, loss=1.2318594455718994
I0211 10:22:13.151547 140283720881920 logging_writer.py:48] [37900] global_step=37900, grad_norm=3.567681074142456, loss=1.2310839891433716
I0211 10:23:36.513429 140283712489216 logging_writer.py:48] [38000] global_step=38000, grad_norm=2.318110942840576, loss=1.203457236289978
I0211 10:25:02.861870 140283720881920 logging_writer.py:48] [38100] global_step=38100, grad_norm=4.230978488922119, loss=1.1936863660812378
I0211 10:26:22.623025 140283720881920 logging_writer.py:48] [38200] global_step=38200, grad_norm=5.002096176147461, loss=1.2340354919433594
I0211 10:27:23.680404 140441227016000 spec.py:321] Evaluating on the training split.
I0211 10:28:18.644231 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 10:29:12.806509 140441227016000 spec.py:349] Evaluating on the test split.
I0211 10:29:39.995636 140441227016000 submission_runner.py:408] Time since start: 34935.58s, 	Step: 38279, 	{'train/ctc_loss': Array(0.17948455, dtype=float32), 'train/wer': 0.0625910751620874, 'validation/ctc_loss': Array(0.44596314, dtype=float32), 'validation/wer': 0.12960406267800767, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24500446, dtype=float32), 'test/wer': 0.07783397314809172, 'test/num_examples': 2472, 'score': 31731.000568151474, 'total_duration': 34935.57740569115, 'accumulated_submission_time': 31731.000568151474, 'accumulated_eval_time': 3202.054293870926, 'accumulated_logging_time': 0.9165394306182861}
I0211 10:29:40.026642 140284806321920 logging_writer.py:48] [38279] accumulated_eval_time=3202.054294, accumulated_logging_time=0.916539, accumulated_submission_time=31731.000568, global_step=38279, preemption_count=0, score=31731.000568, test/ctc_loss=0.24500446021556854, test/num_examples=2472, test/wer=0.077834, total_duration=34935.577406, train/ctc_loss=0.1794845461845398, train/wer=0.062591, validation/ctc_loss=0.44596314430236816, validation/num_examples=5348, validation/wer=0.129604
I0211 10:29:56.559580 140284797929216 logging_writer.py:48] [38300] global_step=38300, grad_norm=2.454036235809326, loss=1.2110278606414795
I0211 10:31:11.499969 140284806321920 logging_writer.py:48] [38400] global_step=38400, grad_norm=3.06101393699646, loss=1.2776378393173218
I0211 10:32:26.655647 140284797929216 logging_writer.py:48] [38500] global_step=38500, grad_norm=2.0447442531585693, loss=1.1933550834655762
I0211 10:33:43.907021 140284806321920 logging_writer.py:48] [38600] global_step=38600, grad_norm=2.6994407176971436, loss=1.2117012739181519
I0211 10:35:13.073987 140284797929216 logging_writer.py:48] [38700] global_step=38700, grad_norm=4.640988826751709, loss=1.2359620332717896
I0211 10:36:39.094595 140284806321920 logging_writer.py:48] [38800] global_step=38800, grad_norm=2.3469786643981934, loss=1.1771515607833862
I0211 10:38:06.142132 140284797929216 logging_writer.py:48] [38900] global_step=38900, grad_norm=2.0023088455200195, loss=1.1833415031433105
I0211 10:39:32.488453 140284806321920 logging_writer.py:48] [39000] global_step=39000, grad_norm=3.2004387378692627, loss=1.211934208869934
I0211 10:40:59.918889 140284797929216 logging_writer.py:48] [39100] global_step=39100, grad_norm=3.739574432373047, loss=1.1925657987594604
I0211 10:42:24.727701 140284150961920 logging_writer.py:48] [39200] global_step=39200, grad_norm=2.5036845207214355, loss=1.1782456636428833
I0211 10:43:42.009087 140284142569216 logging_writer.py:48] [39300] global_step=39300, grad_norm=4.369342803955078, loss=1.2051770687103271
I0211 10:44:57.341376 140284150961920 logging_writer.py:48] [39400] global_step=39400, grad_norm=4.192931175231934, loss=1.1393619775772095
I0211 10:46:18.299791 140284142569216 logging_writer.py:48] [39500] global_step=39500, grad_norm=2.628518581390381, loss=1.1460288763046265
I0211 10:47:40.198331 140284150961920 logging_writer.py:48] [39600] global_step=39600, grad_norm=4.350334644317627, loss=1.1402626037597656
I0211 10:49:06.076537 140284142569216 logging_writer.py:48] [39700] global_step=39700, grad_norm=5.498129367828369, loss=1.1923166513442993
I0211 10:50:32.254440 140284150961920 logging_writer.py:48] [39800] global_step=39800, grad_norm=2.124133348464966, loss=1.2288990020751953
I0211 10:52:02.499364 140284142569216 logging_writer.py:48] [39900] global_step=39900, grad_norm=2.93013334274292, loss=1.1423795223236084
I0211 10:53:31.443851 140284150961920 logging_writer.py:48] [40000] global_step=40000, grad_norm=2.509451389312744, loss=1.2591897249221802
I0211 10:53:40.983866 140441227016000 spec.py:321] Evaluating on the training split.
I0211 10:54:34.948638 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 10:55:26.560750 140441227016000 spec.py:349] Evaluating on the test split.
I0211 10:55:52.771944 140441227016000 submission_runner.py:408] Time since start: 36508.35s, 	Step: 40011, 	{'train/ctc_loss': Array(0.22586584, dtype=float32), 'train/wer': 0.07717926048068213, 'validation/ctc_loss': Array(0.43340683, dtype=float32), 'validation/wer': 0.12585805729071126, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23872232, dtype=float32), 'test/wer': 0.07545751833120062, 'test/num_examples': 2472, 'score': 33171.87407159805, 'total_duration': 36508.35352563858, 'accumulated_submission_time': 33171.87407159805, 'accumulated_eval_time': 3333.839416027069, 'accumulated_logging_time': 0.9601418972015381}
I0211 10:55:52.805418 140284806321920 logging_writer.py:48] [40011] accumulated_eval_time=3333.839416, accumulated_logging_time=0.960142, accumulated_submission_time=33171.874072, global_step=40011, preemption_count=0, score=33171.874072, test/ctc_loss=0.2387223243713379, test/num_examples=2472, test/wer=0.075458, total_duration=36508.353526, train/ctc_loss=0.22586584091186523, train/wer=0.077179, validation/ctc_loss=0.4334068298339844, validation/num_examples=5348, validation/wer=0.125858
I0211 10:57:00.575034 140284797929216 logging_writer.py:48] [40100] global_step=40100, grad_norm=2.9308791160583496, loss=1.1769028902053833
I0211 10:58:19.634680 140284806321920 logging_writer.py:48] [40200] global_step=40200, grad_norm=3.4648706912994385, loss=1.137424111366272
I0211 10:59:36.865384 140284797929216 logging_writer.py:48] [40300] global_step=40300, grad_norm=2.1627488136291504, loss=1.1541659832000732
I0211 11:00:57.209988 140284806321920 logging_writer.py:48] [40400] global_step=40400, grad_norm=2.0584909915924072, loss=1.151709794998169
I0211 11:02:19.674903 140284797929216 logging_writer.py:48] [40500] global_step=40500, grad_norm=3.025176763534546, loss=1.1974852085113525
I0211 11:03:40.914435 140284806321920 logging_writer.py:48] [40600] global_step=40600, grad_norm=3.511401653289795, loss=1.1533762216567993
I0211 11:05:06.092836 140284797929216 logging_writer.py:48] [40700] global_step=40700, grad_norm=3.4058492183685303, loss=1.220767855644226
I0211 11:06:34.673849 140284806321920 logging_writer.py:48] [40800] global_step=40800, grad_norm=2.6878552436828613, loss=1.190139651298523
I0211 11:08:01.985498 140284797929216 logging_writer.py:48] [40900] global_step=40900, grad_norm=3.7881147861480713, loss=1.1645082235336304
I0211 11:09:27.571572 140284806321920 logging_writer.py:48] [41000] global_step=41000, grad_norm=2.4954588413238525, loss=1.1824653148651123
I0211 11:10:56.912120 140284797929216 logging_writer.py:48] [41100] global_step=41100, grad_norm=2.6014888286590576, loss=1.1595215797424316
I0211 11:12:29.611555 140284806321920 logging_writer.py:48] [41200] global_step=41200, grad_norm=4.42464017868042, loss=1.2073580026626587
I0211 11:13:47.089244 140284797929216 logging_writer.py:48] [41300] global_step=41300, grad_norm=2.4993696212768555, loss=1.1319694519042969
I0211 11:15:06.892974 140284806321920 logging_writer.py:48] [41400] global_step=41400, grad_norm=3.325594663619995, loss=1.1563072204589844
I0211 11:16:32.171982 140284797929216 logging_writer.py:48] [41500] global_step=41500, grad_norm=3.074138641357422, loss=1.1263538599014282
I0211 11:17:51.626321 140284806321920 logging_writer.py:48] [41600] global_step=41600, grad_norm=2.870723009109497, loss=1.1418790817260742
I0211 11:19:18.163698 140284797929216 logging_writer.py:48] [41700] global_step=41700, grad_norm=4.231473445892334, loss=1.165145993232727
I0211 11:19:52.992112 140441227016000 spec.py:321] Evaluating on the training split.
I0211 11:20:46.177262 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 11:21:38.428992 140441227016000 spec.py:349] Evaluating on the test split.
I0211 11:22:06.479420 140441227016000 submission_runner.py:408] Time since start: 38082.06s, 	Step: 41741, 	{'train/ctc_loss': Array(0.22595026, dtype=float32), 'train/wer': 0.07742995608897672, 'validation/ctc_loss': Array(0.4262832, dtype=float32), 'validation/wer': 0.12293269741351845, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22935602, dtype=float32), 'test/wer': 0.07255296244388926, 'test/num_examples': 2472, 'score': 34611.97883796692, 'total_duration': 38082.061220407486, 'accumulated_submission_time': 34611.97883796692, 'accumulated_eval_time': 3467.3239908218384, 'accumulated_logging_time': 1.0061976909637451}
I0211 11:22:06.510515 140285390001920 logging_writer.py:48] [41741] accumulated_eval_time=3467.323991, accumulated_logging_time=1.006198, accumulated_submission_time=34611.978838, global_step=41741, preemption_count=0, score=34611.978838, test/ctc_loss=0.22935602068901062, test/num_examples=2472, test/wer=0.072553, total_duration=38082.061220, train/ctc_loss=0.22595025599002838, train/wer=0.077430, validation/ctc_loss=0.42628321051597595, validation/num_examples=5348, validation/wer=0.122933
I0211 11:22:51.674951 140285381609216 logging_writer.py:48] [41800] global_step=41800, grad_norm=2.6711578369140625, loss=1.1992155313491821
I0211 11:24:06.564407 140285390001920 logging_writer.py:48] [41900] global_step=41900, grad_norm=2.1235134601593018, loss=1.166096568107605
I0211 11:25:25.000697 140285381609216 logging_writer.py:48] [42000] global_step=42000, grad_norm=2.4595444202423096, loss=1.1385146379470825
I0211 11:26:53.036742 140285390001920 logging_writer.py:48] [42100] global_step=42100, grad_norm=3.0819873809814453, loss=1.1776450872421265
I0211 11:28:18.468460 140285381609216 logging_writer.py:48] [42200] global_step=42200, grad_norm=3.88752818107605, loss=1.1376491785049438
I0211 11:29:43.654044 140283935921920 logging_writer.py:48] [42300] global_step=42300, grad_norm=3.3006937503814697, loss=1.1425755023956299
I0211 11:31:00.184395 140283927529216 logging_writer.py:48] [42400] global_step=42400, grad_norm=2.8872132301330566, loss=1.1752216815948486
I0211 11:32:21.090281 140283935921920 logging_writer.py:48] [42500] global_step=42500, grad_norm=3.882451057434082, loss=1.1265416145324707
I0211 11:33:46.892158 140283927529216 logging_writer.py:48] [42600] global_step=42600, grad_norm=4.409626007080078, loss=1.168966293334961
I0211 11:35:14.255218 140283935921920 logging_writer.py:48] [42700] global_step=42700, grad_norm=4.026594638824463, loss=1.1823526620864868
I0211 11:36:43.111832 140283927529216 logging_writer.py:48] [42800] global_step=42800, grad_norm=3.5801374912261963, loss=1.1365832090377808
I0211 11:38:11.719647 140283935921920 logging_writer.py:48] [42900] global_step=42900, grad_norm=5.309421062469482, loss=1.2115905284881592
I0211 11:39:39.148456 140283927529216 logging_writer.py:48] [43000] global_step=43000, grad_norm=3.2634689807891846, loss=1.1103227138519287
I0211 11:41:08.847743 140283935921920 logging_writer.py:48] [43100] global_step=43100, grad_norm=3.3265087604522705, loss=1.121578574180603
I0211 11:42:32.812157 140283927529216 logging_writer.py:48] [43200] global_step=43200, grad_norm=2.335014820098877, loss=1.1496418714523315
I0211 11:43:59.589492 140282952881920 logging_writer.py:48] [43300] global_step=43300, grad_norm=3.516214609146118, loss=1.1553031206130981
I0211 11:45:17.399086 140282944489216 logging_writer.py:48] [43400] global_step=43400, grad_norm=2.6319992542266846, loss=1.2109099626541138
I0211 11:46:06.656202 140441227016000 spec.py:321] Evaluating on the training split.
I0211 11:47:02.450915 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 11:47:53.527102 140441227016000 spec.py:349] Evaluating on the test split.
I0211 11:48:20.992809 140441227016000 submission_runner.py:408] Time since start: 39656.57s, 	Step: 43467, 	{'train/ctc_loss': Array(0.25700453, dtype=float32), 'train/wer': 0.08875057765090581, 'validation/ctc_loss': Array(0.41875762, dtype=float32), 'validation/wer': 0.12103073076069011, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22580484, dtype=float32), 'test/wer': 0.07086710133447079, 'test/num_examples': 2472, 'score': 36052.042571783066, 'total_duration': 39656.574353694916, 'accumulated_submission_time': 36052.042571783066, 'accumulated_eval_time': 3601.6576216220856, 'accumulated_logging_time': 1.0495140552520752}
I0211 11:48:21.023693 140284294317824 logging_writer.py:48] [43467] accumulated_eval_time=3601.657622, accumulated_logging_time=1.049514, accumulated_submission_time=36052.042572, global_step=43467, preemption_count=0, score=36052.042572, test/ctc_loss=0.22580483555793762, test/num_examples=2472, test/wer=0.070867, total_duration=39656.574354, train/ctc_loss=0.2570045292377472, train/wer=0.088751, validation/ctc_loss=0.4187576174736023, validation/num_examples=5348, validation/wer=0.121031
I0211 11:48:46.731110 140284285925120 logging_writer.py:48] [43500] global_step=43500, grad_norm=2.6424691677093506, loss=1.1589300632476807
I0211 11:50:01.511561 140284294317824 logging_writer.py:48] [43600] global_step=43600, grad_norm=6.588850021362305, loss=1.1681451797485352
I0211 11:51:17.217383 140284285925120 logging_writer.py:48] [43700] global_step=43700, grad_norm=2.7628211975097656, loss=1.1225097179412842
I0211 11:52:40.234271 140284294317824 logging_writer.py:48] [43800] global_step=43800, grad_norm=4.693492412567139, loss=1.132330060005188
I0211 11:54:07.971642 140284285925120 logging_writer.py:48] [43900] global_step=43900, grad_norm=2.7859702110290527, loss=1.1124240159988403
I0211 11:55:37.480681 140284294317824 logging_writer.py:48] [44000] global_step=44000, grad_norm=2.5670628547668457, loss=1.158374309539795
I0211 11:57:05.110072 140284285925120 logging_writer.py:48] [44100] global_step=44100, grad_norm=4.984543323516846, loss=1.1522327661514282
I0211 11:58:31.905037 140284294317824 logging_writer.py:48] [44200] global_step=44200, grad_norm=3.460066795349121, loss=1.1265407800674438
I0211 12:00:00.679486 140284294317824 logging_writer.py:48] [44300] global_step=44300, grad_norm=2.851228713989258, loss=1.1514573097229004
I0211 12:01:15.686429 140284285925120 logging_writer.py:48] [44400] global_step=44400, grad_norm=3.303542137145996, loss=1.1015515327453613
I0211 12:02:35.780833 140284294317824 logging_writer.py:48] [44500] global_step=44500, grad_norm=4.088740825653076, loss=1.1298565864562988
I0211 12:03:56.866614 140284285925120 logging_writer.py:48] [44600] global_step=44600, grad_norm=3.5406582355499268, loss=1.166109561920166
I0211 12:05:21.201623 140284294317824 logging_writer.py:48] [44700] global_step=44700, grad_norm=3.484834671020508, loss=1.1737457513809204
I0211 12:06:47.236214 140284285925120 logging_writer.py:48] [44800] global_step=44800, grad_norm=3.9523119926452637, loss=1.1138249635696411
I0211 12:08:12.838928 140284294317824 logging_writer.py:48] [44900] global_step=44900, grad_norm=2.726776599884033, loss=1.1668668985366821
I0211 12:09:38.909809 140284285925120 logging_writer.py:48] [45000] global_step=45000, grad_norm=3.477576971054077, loss=1.1397711038589478
I0211 12:11:04.668507 140284294317824 logging_writer.py:48] [45100] global_step=45100, grad_norm=3.0088553428649902, loss=1.1297434568405151
I0211 12:12:21.590869 140441227016000 spec.py:321] Evaluating on the training split.
I0211 12:13:13.288567 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 12:14:06.998789 140441227016000 spec.py:349] Evaluating on the test split.
I0211 12:14:33.763983 140441227016000 submission_runner.py:408] Time since start: 41229.35s, 	Step: 45193, 	{'train/ctc_loss': Array(0.22935137, dtype=float32), 'train/wer': 0.07723330723171759, 'validation/ctc_loss': Array(0.41290858, dtype=float32), 'validation/wer': 0.119177037373162, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22298104, dtype=float32), 'test/wer': 0.06981089919363029, 'test/num_examples': 2472, 'score': 37492.52834439278, 'total_duration': 41229.34570097923, 'accumulated_submission_time': 37492.52834439278, 'accumulated_eval_time': 3733.8279616832733, 'accumulated_logging_time': 1.090902328491211}
I0211 12:14:33.797869 140285390001920 logging_writer.py:48] [45193] accumulated_eval_time=3733.827962, accumulated_logging_time=1.090902, accumulated_submission_time=37492.528344, global_step=45193, preemption_count=0, score=37492.528344, test/ctc_loss=0.2229810357093811, test/num_examples=2472, test/wer=0.069811, total_duration=41229.345701, train/ctc_loss=0.22935137152671814, train/wer=0.077233, validation/ctc_loss=0.4129085838794708, validation/num_examples=5348, validation/wer=0.119177
I0211 12:14:39.910503 140285381609216 logging_writer.py:48] [45200] global_step=45200, grad_norm=3.139089345932007, loss=1.168001651763916
I0211 12:15:55.313967 140285390001920 logging_writer.py:48] [45300] global_step=45300, grad_norm=3.4304215908050537, loss=1.1207302808761597
I0211 12:17:16.123582 140284304561920 logging_writer.py:48] [45400] global_step=45400, grad_norm=5.916536808013916, loss=1.1648191213607788
I0211 12:18:31.847451 140283019437824 logging_writer.py:48] [45500] global_step=45500, grad_norm=3.4845664501190186, loss=1.1247001886367798
I0211 12:19:51.251743 140284304561920 logging_writer.py:48] [45600] global_step=45600, grad_norm=2.9387693405151367, loss=1.1083219051361084
I0211 12:21:15.378569 140283019437824 logging_writer.py:48] [45700] global_step=45700, grad_norm=2.639354944229126, loss=1.1275500059127808
I0211 12:22:40.458908 140284304561920 logging_writer.py:48] [45800] global_step=45800, grad_norm=3.868037223815918, loss=1.1408652067184448
I0211 12:24:06.881836 140283019437824 logging_writer.py:48] [45900] global_step=45900, grad_norm=2.9754233360290527, loss=1.1581833362579346
I0211 12:25:35.540249 140284304561920 logging_writer.py:48] [46000] global_step=46000, grad_norm=5.624582290649414, loss=1.1318901777267456
I0211 12:27:06.273272 140283019437824 logging_writer.py:48] [46100] global_step=46100, grad_norm=2.6100497245788574, loss=1.1253254413604736
I0211 12:28:31.425021 140284304561920 logging_writer.py:48] [46200] global_step=46200, grad_norm=4.946253299713135, loss=1.1345720291137695
I0211 12:30:01.269919 140283019437824 logging_writer.py:48] [46300] global_step=46300, grad_norm=4.855155944824219, loss=1.113635778427124
I0211 12:31:24.094822 140284304561920 logging_writer.py:48] [46400] global_step=46400, grad_norm=12.245805740356445, loss=1.1499991416931152
I0211 12:32:42.185094 140284002477824 logging_writer.py:48] [46500] global_step=46500, grad_norm=3.288548707962036, loss=1.1065173149108887
I0211 12:33:59.660521 140284304561920 logging_writer.py:48] [46600] global_step=46600, grad_norm=3.4936859607696533, loss=1.1556406021118164
I0211 12:35:19.708977 140284002477824 logging_writer.py:48] [46700] global_step=46700, grad_norm=2.760007619857788, loss=1.1425848007202148
I0211 12:36:45.104016 140284304561920 logging_writer.py:48] [46800] global_step=46800, grad_norm=2.318995714187622, loss=1.1270525455474854
I0211 12:38:14.892402 140284002477824 logging_writer.py:48] [46900] global_step=46900, grad_norm=2.9203555583953857, loss=1.1590858697891235
I0211 12:38:34.278928 140441227016000 spec.py:321] Evaluating on the training split.
I0211 12:39:28.048353 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 12:40:21.154621 140441227016000 spec.py:349] Evaluating on the test split.
I0211 12:40:47.244863 140441227016000 submission_runner.py:408] Time since start: 42802.83s, 	Step: 46925, 	{'train/ctc_loss': Array(0.21121906, dtype=float32), 'train/wer': 0.07314085934107535, 'validation/ctc_loss': Array(0.4120718, dtype=float32), 'validation/wer': 0.11874257798546009, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22249565, dtype=float32), 'test/wer': 0.06995308025105113, 'test/num_examples': 2472, 'score': 38932.92411899567, 'total_duration': 42802.826154470444, 'accumulated_submission_time': 38932.92411899567, 'accumulated_eval_time': 3866.7906522750854, 'accumulated_logging_time': 1.1397252082824707}
I0211 12:40:47.279210 140284012721920 logging_writer.py:48] [46925] accumulated_eval_time=3866.790652, accumulated_logging_time=1.139725, accumulated_submission_time=38932.924119, global_step=46925, preemption_count=0, score=38932.924119, test/ctc_loss=0.2224956452846527, test/num_examples=2472, test/wer=0.069953, total_duration=42802.826154, train/ctc_loss=0.21121905744075775, train/wer=0.073141, validation/ctc_loss=0.4120717942714691, validation/num_examples=5348, validation/wer=0.118743
I0211 12:41:44.028312 140284004329216 logging_writer.py:48] [47000] global_step=47000, grad_norm=2.970139503479004, loss=1.0734374523162842
I0211 12:42:59.052578 140284012721920 logging_writer.py:48] [47100] global_step=47100, grad_norm=3.1440536975860596, loss=1.1739132404327393
I0211 12:44:20.004084 140284004329216 logging_writer.py:48] [47200] global_step=47200, grad_norm=3.5492138862609863, loss=1.1396310329437256
I0211 12:45:45.841832 140284012721920 logging_writer.py:48] [47300] global_step=47300, grad_norm=3.6933751106262207, loss=1.1334723234176636
I0211 12:47:12.270283 140285390001920 logging_writer.py:48] [47400] global_step=47400, grad_norm=4.995904922485352, loss=1.1784284114837646
I0211 12:48:27.528522 140285381609216 logging_writer.py:48] [47500] global_step=47500, grad_norm=3.477405309677124, loss=1.1848394870758057
I0211 12:49:45.030067 140285390001920 logging_writer.py:48] [47600] global_step=47600, grad_norm=3.2738640308380127, loss=1.1100976467132568
I0211 12:51:02.222315 140285381609216 logging_writer.py:48] [47700] global_step=47700, grad_norm=3.2373576164245605, loss=1.1603566408157349
I0211 12:52:23.740392 140285390001920 logging_writer.py:48] [47800] global_step=47800, grad_norm=3.873781681060791, loss=1.044832706451416
I0211 12:53:48.311252 140285381609216 logging_writer.py:48] [47900] global_step=47900, grad_norm=3.986093044281006, loss=1.1565085649490356
I0211 12:55:16.777475 140441227016000 spec.py:321] Evaluating on the training split.
I0211 12:56:10.769716 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 12:57:03.508324 140441227016000 spec.py:349] Evaluating on the test split.
I0211 12:57:30.888664 140441227016000 submission_runner.py:408] Time since start: 43806.47s, 	Step: 48000, 	{'train/ctc_loss': Array(0.19107874, dtype=float32), 'train/wer': 0.06727098717767457, 'validation/ctc_loss': Array(0.4123398, dtype=float32), 'validation/wer': 0.11909980014868166, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22241092, dtype=float32), 'test/wer': 0.0701561960473666, 'test/num_examples': 2472, 'score': 39802.3655128479, 'total_duration': 43806.46564793587, 'accumulated_submission_time': 39802.3655128479, 'accumulated_eval_time': 4000.894311904907, 'accumulated_logging_time': 1.186645746231079}
I0211 12:57:30.936390 140285390001920 logging_writer.py:48] [48000] accumulated_eval_time=4000.894312, accumulated_logging_time=1.186646, accumulated_submission_time=39802.365513, global_step=48000, preemption_count=0, score=39802.365513, test/ctc_loss=0.2224109172821045, test/num_examples=2472, test/wer=0.070156, total_duration=43806.465648, train/ctc_loss=0.19107873737812042, train/wer=0.067271, validation/ctc_loss=0.41233980655670166, validation/num_examples=5348, validation/wer=0.119100
I0211 12:57:30.967091 140285381609216 logging_writer.py:48] [48000] global_step=48000, preemption_count=0, score=39802.365513
I0211 12:57:31.191276 140441227016000 checkpoints.py:490] Saving checkpoint at step: 48000
I0211 12:57:32.207612 140441227016000 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_2/librispeech_deepspeech_jax/trial_1/checkpoint_48000
I0211 12:57:32.227396 140441227016000 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_2/librispeech_deepspeech_jax/trial_1/checkpoint_48000.
I0211 12:57:33.449420 140441227016000 submission_runner.py:583] Tuning trial 1/5
I0211 12:57:33.449673 140441227016000 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0211 12:57:33.461850 140441227016000 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.563702, dtype=float32), 'train/wer': 3.280985152947546, 'validation/ctc_loss': Array(30.570293, dtype=float32), 'validation/wer': 2.911601996582253, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.657606, dtype=float32), 'test/wer': 3.232161355188593, 'test/num_examples': 2472, 'score': 44.15716743469238, 'total_duration': 270.42189955711365, 'accumulated_submission_time': 44.15716743469238, 'accumulated_eval_time': 226.2646632194519, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1767, {'train/ctc_loss': Array(6.473987, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': Array(6.4356875, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.3765306, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1485.011241197586, 'total_duration': 1818.2476708889008, 'accumulated_submission_time': 1485.011241197586, 'accumulated_eval_time': 333.1237199306488, 'accumulated_logging_time': 0.04142189025878906, 'global_step': 1767, 'preemption_count': 0}), (3537, {'train/ctc_loss': Array(3.7205336, dtype=float32), 'train/wer': 0.7672056042404709, 'validation/ctc_loss': Array(4.0462813, dtype=float32), 'validation/wer': 0.7830116724755496, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.662109, dtype=float32), 'test/wer': 0.7429772713423923, 'test/num_examples': 2472, 'score': 2925.4576818943024, 'total_duration': 3378.6899876594543, 'accumulated_submission_time': 2925.4576818943024, 'accumulated_eval_time': 453.00739574432373, 'accumulated_logging_time': 0.08230781555175781, 'global_step': 3537, 'preemption_count': 0}), (5280, {'train/ctc_loss': Array(0.60861146, dtype=float32), 'train/wer': 0.2074207610523375, 'validation/ctc_loss': Array(1.0145419, dtype=float32), 'validation/wer': 0.285150178128349, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.671695, dtype=float32), 'test/wer': 0.21412467247577843, 'test/num_examples': 2472, 'score': 4365.597179412842, 'total_duration': 4951.050794363022, 'accumulated_submission_time': 4365.597179412842, 'accumulated_eval_time': 585.1130557060242, 'accumulated_logging_time': 0.12415671348571777, 'global_step': 5280, 'preemption_count': 0}), (7030, {'train/ctc_loss': Array(0.49595514, dtype=float32), 'train/wer': 0.16683597063536115, 'validation/ctc_loss': Array(0.8424785, dtype=float32), 'validation/wer': 0.2426503953580428, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5282411, dtype=float32), 'test/wer': 0.17077976154205513, 'test/num_examples': 2472, 'score': 5805.970959663391, 'total_duration': 6535.9713497161865, 'accumulated_submission_time': 5805.970959663391, 'accumulated_eval_time': 729.5503969192505, 'accumulated_logging_time': 0.16060924530029297, 'global_step': 7030, 'preemption_count': 0}), (8781, {'train/ctc_loss': Array(0.4343019, dtype=float32), 'train/wer': 0.1476276935788993, 'validation/ctc_loss': Array(0.7638279, dtype=float32), 'validation/wer': 0.2213232667484094, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.47641897, dtype=float32), 'test/wer': 0.1522149777588203, 'test/num_examples': 2472, 'score': 7246.0776579380035, 'total_duration': 8112.394110202789, 'accumulated_submission_time': 7246.0776579380035, 'accumulated_eval_time': 865.7520883083344, 'accumulated_logging_time': 0.2023007869720459, 'global_step': 8781, 'preemption_count': 0}), (10527, {'train/ctc_loss': Array(0.42308322, dtype=float32), 'train/wer': 0.14123690991245713, 'validation/ctc_loss': Array(0.724271, dtype=float32), 'validation/wer': 0.20801915483167113, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.44031197, dtype=float32), 'test/wer': 0.14069831210773262, 'test/num_examples': 2472, 'score': 8686.643969297409, 'total_duration': 9689.364002227783, 'accumulated_submission_time': 8686.643969297409, 'accumulated_eval_time': 1002.0370271205902, 'accumulated_logging_time': 0.24602818489074707, 'global_step': 10527, 'preemption_count': 0}), (12254, {'train/ctc_loss': Array(0.3849505, dtype=float32), 'train/wer': 0.12991619339331675, 'validation/ctc_loss': Array(0.70631933, dtype=float32), 'validation/wer': 0.20398350985257344, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42263985, dtype=float32), 'test/wer': 0.13685942355737005, 'test/num_examples': 2472, 'score': 10126.575286149979, 'total_duration': 11262.206323862076, 'accumulated_submission_time': 10126.575286149979, 'accumulated_eval_time': 1134.8351573944092, 'accumulated_logging_time': 0.28536081314086914, 'global_step': 12254, 'preemption_count': 0}), (13977, {'train/ctc_loss': Array(0.32947904, dtype=float32), 'train/wer': 0.1149223848774019, 'validation/ctc_loss': Array(0.66567683, dtype=float32), 'validation/wer': 0.19206966797648126, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3976388, dtype=float32), 'test/wer': 0.1286941685454878, 'test/num_examples': 2472, 'score': 11566.618283510208, 'total_duration': 12844.752668619156, 'accumulated_submission_time': 11566.618283510208, 'accumulated_eval_time': 1277.2269763946533, 'accumulated_logging_time': 0.32526302337646484, 'global_step': 13977, 'preemption_count': 0}), (15733, {'train/ctc_loss': Array(0.3160166, dtype=float32), 'train/wer': 0.10870409482414653, 'validation/ctc_loss': Array(0.65010995, dtype=float32), 'validation/wer': 0.18813056952798402, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3880893, dtype=float32), 'test/wer': 0.12428655576544188, 'test/num_examples': 2472, 'score': 13007.737249851227, 'total_duration': 14417.983313083649, 'accumulated_submission_time': 13007.737249851227, 'accumulated_eval_time': 1409.226472377777, 'accumulated_logging_time': 0.3636348247528076, 'global_step': 15733, 'preemption_count': 0}), (17470, {'train/ctc_loss': Array(0.3091924, dtype=float32), 'train/wer': 0.10841596636402151, 'validation/ctc_loss': Array(0.62598205, dtype=float32), 'validation/wer': 0.18160402405939544, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.370601, dtype=float32), 'test/wer': 0.1191274145390287, 'test/num_examples': 2472, 'score': 14448.331030368805, 'total_duration': 15995.882573604584, 'accumulated_submission_time': 14448.331030368805, 'accumulated_eval_time': 1546.4209856987, 'accumulated_logging_time': 0.4011971950531006, 'global_step': 17470, 'preemption_count': 0}), (19191, {'train/ctc_loss': Array(0.31268734, dtype=float32), 'train/wer': 0.10624818213067505, 'validation/ctc_loss': Array(0.60505414, dtype=float32), 'validation/wer': 0.1773077034476766, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35781944, dtype=float32), 'test/wer': 0.11474011333861435, 'test/num_examples': 2472, 'score': 15888.437488555908, 'total_duration': 17569.057680606842, 'accumulated_submission_time': 15888.437488555908, 'accumulated_eval_time': 1679.376972436905, 'accumulated_logging_time': 0.4410090446472168, 'global_step': 19191, 'preemption_count': 0}), (20942, {'train/ctc_loss': Array(0.31071246, dtype=float32), 'train/wer': 0.10659244994328225, 'validation/ctc_loss': Array(0.5931073, dtype=float32), 'validation/wer': 0.17373548181546097, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34689486, dtype=float32), 'test/wer': 0.11303394064956432, 'test/num_examples': 2472, 'score': 17328.820681095123, 'total_duration': 19146.078830242157, 'accumulated_submission_time': 17328.820681095123, 'accumulated_eval_time': 1815.8983781337738, 'accumulated_logging_time': 0.48394131660461426, 'global_step': 20942, 'preemption_count': 0}), (22661, {'train/ctc_loss': Array(0.29349634, dtype=float32), 'train/wer': 0.10054532359296224, 'validation/ctc_loss': Array(0.57543117, dtype=float32), 'validation/wer': 0.168493005203858, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33769685, dtype=float32), 'test/wer': 0.10811853837872971, 'test/num_examples': 2472, 'score': 18768.853582143784, 'total_duration': 20720.94268655777, 'accumulated_submission_time': 18768.853582143784, 'accumulated_eval_time': 1950.6121413707733, 'accumulated_logging_time': 0.5289254188537598, 'global_step': 22661, 'preemption_count': 0}), (24399, {'train/ctc_loss': Array(0.26594213, dtype=float32), 'train/wer': 0.0915554664076686, 'validation/ctc_loss': Array(0.5630687, dtype=float32), 'validation/wer': 0.16348224026569605, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.325007, dtype=float32), 'test/wer': 0.10482806247841894, 'test/num_examples': 2472, 'score': 20209.092022895813, 'total_duration': 22303.220635175705, 'accumulated_submission_time': 20209.092022895813, 'accumulated_eval_time': 2092.538419485092, 'accumulated_logging_time': 0.5695466995239258, 'global_step': 24399, 'preemption_count': 0}), (26147, {'train/ctc_loss': Array(0.25006896, dtype=float32), 'train/wer': 0.08782180025561438, 'validation/ctc_loss': Array(0.5523809, dtype=float32), 'validation/wer': 0.1610009944292652, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3190817, dtype=float32), 'test/wer': 0.10212662238742307, 'test/num_examples': 2472, 'score': 21649.144829034805, 'total_duration': 23887.106551885605, 'accumulated_submission_time': 21649.144829034805, 'accumulated_eval_time': 2236.258577108383, 'accumulated_logging_time': 0.6100704669952393, 'global_step': 26147, 'preemption_count': 0}), (27855, {'train/ctc_loss': Array(0.23860557, dtype=float32), 'train/wer': 0.0828977917787483, 'validation/ctc_loss': Array(0.5355645, dtype=float32), 'validation/wer': 0.15535302238914045, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30643246, dtype=float32), 'test/wer': 0.09810492962037658, 'test/num_examples': 2472, 'score': 23089.323447704315, 'total_duration': 25468.949790000916, 'accumulated_submission_time': 23089.323447704315, 'accumulated_eval_time': 2377.8051433563232, 'accumulated_logging_time': 0.6545779705047607, 'global_step': 27855, 'preemption_count': 0}), (29591, {'train/ctc_loss': Array(0.25175303, dtype=float32), 'train/wer': 0.08719557115617958, 'validation/ctc_loss': Array(0.5208304, dtype=float32), 'validation/wer': 0.15108566573660176, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.293739, dtype=float32), 'test/wer': 0.09325046208843661, 'test/num_examples': 2472, 'score': 24529.774584531784, 'total_duration': 27043.341345071793, 'accumulated_submission_time': 24529.774584531784, 'accumulated_eval_time': 2511.6333718299866, 'accumulated_logging_time': 0.694037675857544, 'global_step': 29591, 'preemption_count': 0}), (31338, {'train/ctc_loss': Array(0.22907293, dtype=float32), 'train/wer': 0.07686965203283794, 'validation/ctc_loss': Array(0.50683457, dtype=float32), 'validation/wer': 0.14897129671645248, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2868147, dtype=float32), 'test/wer': 0.09144273150122885, 'test/num_examples': 2472, 'score': 25970.473658800125, 'total_duration': 28616.918856859207, 'accumulated_submission_time': 25970.473658800125, 'accumulated_eval_time': 2644.3974990844727, 'accumulated_logging_time': 0.7355659008026123, 'global_step': 31338, 'preemption_count': 0}), (33063, {'train/ctc_loss': Array(0.23234938, dtype=float32), 'train/wer': 0.07819426821938778, 'validation/ctc_loss': Array(0.49369305, dtype=float32), 'validation/wer': 0.14422120741091168, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27371708, dtype=float32), 'test/wer': 0.08809132086202344, 'test/num_examples': 2472, 'score': 27410.98273205757, 'total_duration': 30199.143298387527, 'accumulated_submission_time': 27410.98273205757, 'accumulated_eval_time': 2785.9905619621277, 'accumulated_logging_time': 0.7848289012908936, 'global_step': 33063, 'preemption_count': 0}), (34798, {'train/ctc_loss': Array(0.21853557, dtype=float32), 'train/wer': 0.07230090476038237, 'validation/ctc_loss': Array(0.47827327, dtype=float32), 'validation/wer': 0.13898838545236877, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26734018, dtype=float32), 'test/wer': 0.08638514817297341, 'test/num_examples': 2472, 'score': 28850.900230646133, 'total_duration': 31784.88902115822, 'accumulated_submission_time': 28850.900230646133, 'accumulated_eval_time': 2931.6980545520782, 'accumulated_logging_time': 0.8314599990844727, 'global_step': 34798, 'preemption_count': 0}), (36544, {'train/ctc_loss': Array(0.16265668, dtype=float32), 'train/wer': 0.057286061314227726, 'validation/ctc_loss': Array(0.46007198, dtype=float32), 'validation/wer': 0.13384245537136624, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25568554, dtype=float32), 'test/wer': 0.08142912274287571, 'test/num_examples': 2472, 'score': 30290.972578525543, 'total_duration': 33359.12208724022, 'accumulated_submission_time': 30290.972578525543, 'accumulated_eval_time': 3065.7418246269226, 'accumulated_logging_time': 0.8742971420288086, 'global_step': 36544, 'preemption_count': 0}), (38279, {'train/ctc_loss': Array(0.17948455, dtype=float32), 'train/wer': 0.0625910751620874, 'validation/ctc_loss': Array(0.44596314, dtype=float32), 'validation/wer': 0.12960406267800767, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24500446, dtype=float32), 'test/wer': 0.07783397314809172, 'test/num_examples': 2472, 'score': 31731.000568151474, 'total_duration': 34935.57740569115, 'accumulated_submission_time': 31731.000568151474, 'accumulated_eval_time': 3202.054293870926, 'accumulated_logging_time': 0.9165394306182861, 'global_step': 38279, 'preemption_count': 0}), (40011, {'train/ctc_loss': Array(0.22586584, dtype=float32), 'train/wer': 0.07717926048068213, 'validation/ctc_loss': Array(0.43340683, dtype=float32), 'validation/wer': 0.12585805729071126, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23872232, dtype=float32), 'test/wer': 0.07545751833120062, 'test/num_examples': 2472, 'score': 33171.87407159805, 'total_duration': 36508.35352563858, 'accumulated_submission_time': 33171.87407159805, 'accumulated_eval_time': 3333.839416027069, 'accumulated_logging_time': 0.9601418972015381, 'global_step': 40011, 'preemption_count': 0}), (41741, {'train/ctc_loss': Array(0.22595026, dtype=float32), 'train/wer': 0.07742995608897672, 'validation/ctc_loss': Array(0.4262832, dtype=float32), 'validation/wer': 0.12293269741351845, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22935602, dtype=float32), 'test/wer': 0.07255296244388926, 'test/num_examples': 2472, 'score': 34611.97883796692, 'total_duration': 38082.061220407486, 'accumulated_submission_time': 34611.97883796692, 'accumulated_eval_time': 3467.3239908218384, 'accumulated_logging_time': 1.0061976909637451, 'global_step': 41741, 'preemption_count': 0}), (43467, {'train/ctc_loss': Array(0.25700453, dtype=float32), 'train/wer': 0.08875057765090581, 'validation/ctc_loss': Array(0.41875762, dtype=float32), 'validation/wer': 0.12103073076069011, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22580484, dtype=float32), 'test/wer': 0.07086710133447079, 'test/num_examples': 2472, 'score': 36052.042571783066, 'total_duration': 39656.574353694916, 'accumulated_submission_time': 36052.042571783066, 'accumulated_eval_time': 3601.6576216220856, 'accumulated_logging_time': 1.0495140552520752, 'global_step': 43467, 'preemption_count': 0}), (45193, {'train/ctc_loss': Array(0.22935137, dtype=float32), 'train/wer': 0.07723330723171759, 'validation/ctc_loss': Array(0.41290858, dtype=float32), 'validation/wer': 0.119177037373162, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22298104, dtype=float32), 'test/wer': 0.06981089919363029, 'test/num_examples': 2472, 'score': 37492.52834439278, 'total_duration': 41229.34570097923, 'accumulated_submission_time': 37492.52834439278, 'accumulated_eval_time': 3733.8279616832733, 'accumulated_logging_time': 1.090902328491211, 'global_step': 45193, 'preemption_count': 0}), (46925, {'train/ctc_loss': Array(0.21121906, dtype=float32), 'train/wer': 0.07314085934107535, 'validation/ctc_loss': Array(0.4120718, dtype=float32), 'validation/wer': 0.11874257798546009, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22249565, dtype=float32), 'test/wer': 0.06995308025105113, 'test/num_examples': 2472, 'score': 38932.92411899567, 'total_duration': 42802.826154470444, 'accumulated_submission_time': 38932.92411899567, 'accumulated_eval_time': 3866.7906522750854, 'accumulated_logging_time': 1.1397252082824707, 'global_step': 46925, 'preemption_count': 0}), (48000, {'train/ctc_loss': Array(0.19107874, dtype=float32), 'train/wer': 0.06727098717767457, 'validation/ctc_loss': Array(0.4123398, dtype=float32), 'validation/wer': 0.11909980014868166, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22241092, dtype=float32), 'test/wer': 0.0701561960473666, 'test/num_examples': 2472, 'score': 39802.3655128479, 'total_duration': 43806.46564793587, 'accumulated_submission_time': 39802.3655128479, 'accumulated_eval_time': 4000.894311904907, 'accumulated_logging_time': 1.186645746231079, 'global_step': 48000, 'preemption_count': 0})], 'global_step': 48000}
I0211 12:57:33.462090 140441227016000 submission_runner.py:586] Timing: 39802.3655128479
I0211 12:57:33.462171 140441227016000 submission_runner.py:588] Total number of evals: 29
I0211 12:57:33.462228 140441227016000 submission_runner.py:589] ====================
I0211 12:57:33.462300 140441227016000 submission_runner.py:542] Using RNG seed 1231499801
I0211 12:57:33.465748 140441227016000 submission_runner.py:551] --- Tuning run 2/5 ---
I0211 12:57:33.465882 140441227016000 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_2/librispeech_deepspeech_jax/trial_2.
I0211 12:57:33.467859 140441227016000 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_2/librispeech_deepspeech_jax/trial_2/hparams.json.
I0211 12:57:33.470218 140441227016000 submission_runner.py:206] Initializing dataset.
I0211 12:57:33.470343 140441227016000 submission_runner.py:213] Initializing model.
I0211 12:57:34.704163 140441227016000 submission_runner.py:255] Initializing optimizer.
I0211 12:57:34.856079 140441227016000 submission_runner.py:262] Initializing metrics bundle.
I0211 12:57:34.856269 140441227016000 submission_runner.py:280] Initializing checkpoint and logger.
I0211 12:57:34.952388 140441227016000 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_2/librispeech_deepspeech_jax/trial_2 with prefix checkpoint_
I0211 12:57:34.952520 140441227016000 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_2/librispeech_deepspeech_jax/trial_2/meta_data_0.json.
I0211 12:57:34.952874 140441227016000 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0211 12:57:34.952986 140441227016000 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0211 12:57:35.481323 140441227016000 logger_utils.py:220] Unable to record git information. Continuing without it.
I0211 12:57:35.967718 140441227016000 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_2/librispeech_deepspeech_jax/trial_2/flags_0.json.
I0211 12:57:36.087344 140441227016000 submission_runner.py:314] Starting training loop.
I0211 12:57:36.090871 140441227016000 input_pipeline.py:20] Loading split = train-clean-100
I0211 12:57:36.135778 140441227016000 input_pipeline.py:20] Loading split = train-clean-360
I0211 12:57:36.751485 140441227016000 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0211 12:57:51.785218 140284316911360 logging_writer.py:48] [0] global_step=0, grad_norm=19.677785873413086, loss=33.1132926940918
I0211 12:57:51.799454 140441227016000 spec.py:321] Evaluating on the training split.
I0211 12:59:21.456390 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 13:00:20.302824 140441227016000 spec.py:349] Evaluating on the test split.
I0211 13:00:51.176189 140441227016000 submission_runner.py:408] Time since start: 195.09s, 	Step: 1, 	{'train/ctc_loss': Array(31.744686, dtype=float32), 'train/wer': 3.0868096476888236, 'validation/ctc_loss': Array(30.570248, dtype=float32), 'validation/wer': 2.9115054500516524, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.657555, dtype=float32), 'test/wer': 3.2318363699144883, 'test/num_examples': 2472, 'score': 15.712034225463867, 'total_duration': 195.08607053756714, 'accumulated_submission_time': 15.712034225463867, 'accumulated_eval_time': 179.37396621704102, 'accumulated_logging_time': 0}
I0211 13:00:51.194425 140284839892736 logging_writer.py:48] [1] accumulated_eval_time=179.373966, accumulated_logging_time=0, accumulated_submission_time=15.712034, global_step=1, preemption_count=0, score=15.712034, test/ctc_loss=30.657554626464844, test/num_examples=2472, test/wer=3.231836, total_duration=195.086071, train/ctc_loss=31.744686126708984, train/wer=3.086810, validation/ctc_loss=30.570247650146484, validation/num_examples=5348, validation/wer=2.911505
I0211 13:02:16.801109 140284764358400 logging_writer.py:48] [100] global_step=100, grad_norm=2.8287088871002197, loss=7.553215503692627
I0211 13:03:33.531541 140284772751104 logging_writer.py:48] [200] global_step=200, grad_norm=1.2264386415481567, loss=5.994001865386963
I0211 13:04:50.490297 140284764358400 logging_writer.py:48] [300] global_step=300, grad_norm=0.7438927292823792, loss=5.84991455078125
I0211 13:06:07.347018 140284772751104 logging_writer.py:48] [400] global_step=400, grad_norm=0.5525296330451965, loss=5.825764179229736
I0211 13:07:24.021392 140284764358400 logging_writer.py:48] [500] global_step=500, grad_norm=1.8384137153625488, loss=5.796289920806885
I0211 13:08:41.358550 140284772751104 logging_writer.py:48] [600] global_step=600, grad_norm=0.6384586691856384, loss=5.648862361907959
I0211 13:10:07.244110 140284764358400 logging_writer.py:48] [700] global_step=700, grad_norm=0.7080190181732178, loss=5.501969814300537
I0211 13:11:33.854430 140284772751104 logging_writer.py:48] [800] global_step=800, grad_norm=1.6105295419692993, loss=5.229035377502441
I0211 13:13:01.891951 140284764358400 logging_writer.py:48] [900] global_step=900, grad_norm=1.7680718898773193, loss=4.784316062927246
I0211 13:14:26.523718 140284772751104 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.9115158319473267, loss=4.332874298095703
I0211 13:15:49.690311 140284839892736 logging_writer.py:48] [1100] global_step=1100, grad_norm=2.094503402709961, loss=3.9810988903045654
I0211 13:17:05.968738 140284831500032 logging_writer.py:48] [1200] global_step=1200, grad_norm=3.119706392288208, loss=3.654475212097168
I0211 13:18:22.361488 140284839892736 logging_writer.py:48] [1300] global_step=1300, grad_norm=2.6983160972595215, loss=3.4632182121276855
I0211 13:19:43.243053 140284831500032 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.9865665435791016, loss=3.3184807300567627
I0211 13:21:09.781305 140284839892736 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.400275945663452, loss=3.175116539001465
I0211 13:22:39.239788 140284831500032 logging_writer.py:48] [1600] global_step=1600, grad_norm=3.775763511657715, loss=3.0617611408233643
I0211 13:24:04.610080 140284839892736 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.6201658248901367, loss=2.9204938411712646
I0211 13:24:51.851436 140441227016000 spec.py:321] Evaluating on the training split.
I0211 13:25:31.112046 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 13:26:18.192331 140441227016000 spec.py:349] Evaluating on the test split.
I0211 13:26:41.656758 140441227016000 submission_runner.py:408] Time since start: 1745.56s, 	Step: 1755, 	{'train/ctc_loss': Array(6.613655, dtype=float32), 'train/wer': 0.9419290070434633, 'validation/ctc_loss': Array(6.4508014, dtype=float32), 'validation/wer': 0.8961545516861852, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.4480104, dtype=float32), 'test/wer': 0.8991123839701014, 'test/num_examples': 2472, 'score': 1456.2849533557892, 'total_duration': 1745.5630271434784, 'accumulated_submission_time': 1456.2849533557892, 'accumulated_eval_time': 289.1729745864868, 'accumulated_logging_time': 0.031392574310302734}
I0211 13:26:41.689346 140284839892736 logging_writer.py:48] [1755] accumulated_eval_time=289.172975, accumulated_logging_time=0.031393, accumulated_submission_time=1456.284953, global_step=1755, preemption_count=0, score=1456.284953, test/ctc_loss=6.448010444641113, test/num_examples=2472, test/wer=0.899112, total_duration=1745.563027, train/ctc_loss=6.613655090332031, train/wer=0.941929, validation/ctc_loss=6.450801372528076, validation/num_examples=5348, validation/wer=0.896155
I0211 13:27:16.637023 140284831500032 logging_writer.py:48] [1800] global_step=1800, grad_norm=3.44100284576416, loss=2.8721375465393066
I0211 13:28:31.668461 140284839892736 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.0985782146453857, loss=2.754826068878174
I0211 13:29:51.304293 140284831500032 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.7927963733673096, loss=2.679885149002075
I0211 13:31:19.532520 140284839892736 logging_writer.py:48] [2100] global_step=2100, grad_norm=3.228196382522583, loss=2.578996181488037
I0211 13:32:35.136947 140284831500032 logging_writer.py:48] [2200] global_step=2200, grad_norm=4.18775749206543, loss=2.5788021087646484
I0211 13:33:51.606715 140284839892736 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.5398740768432617, loss=2.5235886573791504
I0211 13:35:11.487951 140284831500032 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.1588780879974365, loss=2.4076874256134033
I0211 13:36:34.712575 140284839892736 logging_writer.py:48] [2500] global_step=2500, grad_norm=2.993849039077759, loss=2.4594430923461914
I0211 13:38:01.666069 140284831500032 logging_writer.py:48] [2600] global_step=2600, grad_norm=2.998180627822876, loss=2.366419553756714
I0211 13:39:28.089443 140284839892736 logging_writer.py:48] [2700] global_step=2700, grad_norm=3.987961530685425, loss=2.345993757247925
I0211 13:41:00.564162 140284831500032 logging_writer.py:48] [2800] global_step=2800, grad_norm=2.8452532291412354, loss=2.2829437255859375
I0211 13:42:30.937944 140284839892736 logging_writer.py:48] [2900] global_step=2900, grad_norm=2.687316417694092, loss=2.244659662246704
I0211 13:44:00.882989 140284831500032 logging_writer.py:48] [3000] global_step=3000, grad_norm=2.9936444759368896, loss=2.181462049484253
I0211 13:45:29.869041 140284839892736 logging_writer.py:48] [3100] global_step=3100, grad_norm=3.275944232940674, loss=2.2156901359558105
I0211 13:46:46.019431 140284831500032 logging_writer.py:48] [3200] global_step=3200, grad_norm=3.2302985191345215, loss=2.173431158065796
I0211 13:48:00.839279 140284839892736 logging_writer.py:48] [3300] global_step=3300, grad_norm=3.494910478591919, loss=2.1224887371063232
I0211 13:49:20.631098 140284831500032 logging_writer.py:48] [3400] global_step=3400, grad_norm=9.996990203857422, loss=2.2165021896362305
I0211 13:50:42.302502 140441227016000 spec.py:321] Evaluating on the training split.
I0211 13:51:29.376214 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 13:52:20.201260 140441227016000 spec.py:349] Evaluating on the test split.
I0211 13:52:45.573176 140441227016000 submission_runner.py:408] Time since start: 3309.48s, 	Step: 3499, 	{'train/ctc_loss': Array(3.93307, dtype=float32), 'train/wer': 0.757276603130865, 'validation/ctc_loss': Array(3.7532241, dtype=float32), 'validation/wer': 0.7071840273419775, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.2973654, dtype=float32), 'test/wer': 0.6442629943330693, 'test/num_examples': 2472, 'score': 2896.8062427043915, 'total_duration': 3309.4783482551575, 'accumulated_submission_time': 2896.8062427043915, 'accumulated_eval_time': 412.4362471103668, 'accumulated_logging_time': 0.08355402946472168}
I0211 13:52:45.613162 140284839892736 logging_writer.py:48] [3499] accumulated_eval_time=412.436247, accumulated_logging_time=0.083554, accumulated_submission_time=2896.806243, global_step=3499, preemption_count=0, score=2896.806243, test/ctc_loss=3.297365427017212, test/num_examples=2472, test/wer=0.644263, total_duration=3309.478348, train/ctc_loss=3.933069944381714, train/wer=0.757277, validation/ctc_loss=3.7532241344451904, validation/num_examples=5348, validation/wer=0.707184
I0211 13:52:47.261923 140284831500032 logging_writer.py:48] [3500] global_step=3500, grad_norm=3.6754167079925537, loss=2.0115163326263428
I0211 13:54:01.905829 140284839892736 logging_writer.py:48] [3600] global_step=3600, grad_norm=3.6798744201660156, loss=2.088355779647827
I0211 13:55:17.431519 140284831500032 logging_writer.py:48] [3700] global_step=3700, grad_norm=3.563343048095703, loss=2.030068874359131
I0211 13:56:43.486304 140284839892736 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.9958775043487549, loss=2.035046100616455
I0211 13:58:12.909640 140284831500032 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.3604280948638916, loss=2.006664752960205
I0211 13:59:41.390778 140284839892736 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.8578433990478516, loss=1.9933658838272095
I0211 14:01:10.113302 140284831500032 logging_writer.py:48] [4100] global_step=4100, grad_norm=3.6477949619293213, loss=2.0408713817596436
I0211 14:02:31.656081 140284839892736 logging_writer.py:48] [4200] global_step=4200, grad_norm=2.5748074054718018, loss=1.96779203414917
I0211 14:03:47.992543 140284831500032 logging_writer.py:48] [4300] global_step=4300, grad_norm=2.415325880050659, loss=1.9267977476119995
I0211 14:05:04.237168 140284839892736 logging_writer.py:48] [4400] global_step=4400, grad_norm=4.504411697387695, loss=1.8968658447265625
I0211 14:06:26.232501 140284831500032 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.5975096225738525, loss=1.9656747579574585
I0211 14:07:52.429156 140284839892736 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.62052059173584, loss=1.9313673973083496
I0211 14:09:21.611468 140284831500032 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.388120174407959, loss=1.9323720932006836
I0211 14:10:49.749455 140284839892736 logging_writer.py:48] [4800] global_step=4800, grad_norm=3.3698835372924805, loss=1.923466682434082
I0211 14:12:18.164710 140284831500032 logging_writer.py:48] [4900] global_step=4900, grad_norm=4.368038177490234, loss=1.869645357131958
I0211 14:13:44.031768 140284839892736 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.781785249710083, loss=1.8448032140731812
I0211 14:15:11.610706 140284831500032 logging_writer.py:48] [5100] global_step=5100, grad_norm=3.0944039821624756, loss=1.8611305952072144
I0211 14:16:34.851466 140284839892736 logging_writer.py:48] [5200] global_step=5200, grad_norm=2.0683815479278564, loss=1.8018689155578613
I0211 14:16:46.138500 140441227016000 spec.py:321] Evaluating on the training split.
I0211 14:17:40.202084 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 14:18:33.116049 140441227016000 spec.py:349] Evaluating on the test split.
I0211 14:19:00.105272 140441227016000 submission_runner.py:408] Time since start: 4884.01s, 	Step: 5216, 	{'train/ctc_loss': Array(0.9103338, dtype=float32), 'train/wer': 0.2712767425810904, 'validation/ctc_loss': Array(0.9936676, dtype=float32), 'validation/wer': 0.2741631829460208, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6439003, dtype=float32), 'test/wer': 0.2018564783783235, 'test/num_examples': 2472, 'score': 4337.244078874588, 'total_duration': 4884.011987924576, 'accumulated_submission_time': 4337.244078874588, 'accumulated_eval_time': 546.3971445560455, 'accumulated_logging_time': 0.14002203941345215}
I0211 14:19:00.138039 140284839892736 logging_writer.py:48] [5216] accumulated_eval_time=546.397145, accumulated_logging_time=0.140022, accumulated_submission_time=4337.244079, global_step=5216, preemption_count=0, score=4337.244079, test/ctc_loss=0.6439002752304077, test/num_examples=2472, test/wer=0.201856, total_duration=4884.011988, train/ctc_loss=0.9103338122367859, train/wer=0.271277, validation/ctc_loss=0.9936676025390625, validation/num_examples=5348, validation/wer=0.274163
I0211 14:20:04.337446 140284831500032 logging_writer.py:48] [5300] global_step=5300, grad_norm=2.9107658863067627, loss=1.7334153652191162
I0211 14:21:19.587616 140284839892736 logging_writer.py:48] [5400] global_step=5400, grad_norm=4.0617547035217285, loss=1.8149470090866089
I0211 14:22:35.072256 140284831500032 logging_writer.py:48] [5500] global_step=5500, grad_norm=3.6238369941711426, loss=1.8338654041290283
I0211 14:23:53.337475 140284839892736 logging_writer.py:48] [5600] global_step=5600, grad_norm=2.6921536922454834, loss=1.815259575843811
I0211 14:25:22.414042 140284831500032 logging_writer.py:48] [5700] global_step=5700, grad_norm=4.382084369659424, loss=1.7683050632476807
I0211 14:26:52.313640 140284839892736 logging_writer.py:48] [5800] global_step=5800, grad_norm=3.8817641735076904, loss=1.8469367027282715
I0211 14:28:18.503411 140284831500032 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.904242753982544, loss=1.8146662712097168
I0211 14:29:43.508204 140284839892736 logging_writer.py:48] [6000] global_step=6000, grad_norm=4.072750091552734, loss=1.8052889108657837
I0211 14:31:11.114482 140284831500032 logging_writer.py:48] [6100] global_step=6100, grad_norm=3.6071789264678955, loss=1.8160723447799683
I0211 14:32:40.898276 140284839892736 logging_writer.py:48] [6200] global_step=6200, grad_norm=3.0581557750701904, loss=1.711485505104065
I0211 14:33:56.073299 140284831500032 logging_writer.py:48] [6300] global_step=6300, grad_norm=2.5822041034698486, loss=1.7424654960632324
I0211 14:35:11.262192 140284839892736 logging_writer.py:48] [6400] global_step=6400, grad_norm=3.427088975906372, loss=1.721511960029602
I0211 14:36:32.975993 140284831500032 logging_writer.py:48] [6500] global_step=6500, grad_norm=3.1314351558685303, loss=1.6928271055221558
I0211 14:37:59.158684 140284839892736 logging_writer.py:48] [6600] global_step=6600, grad_norm=4.555562496185303, loss=1.7102487087249756
I0211 14:39:24.349294 140284831500032 logging_writer.py:48] [6700] global_step=6700, grad_norm=2.878640651702881, loss=1.7158235311508179
I0211 14:40:51.110417 140284839892736 logging_writer.py:48] [6800] global_step=6800, grad_norm=2.888451099395752, loss=1.7098263502120972
I0211 14:42:16.544414 140284831500032 logging_writer.py:48] [6900] global_step=6900, grad_norm=2.351867198944092, loss=1.7457937002182007
I0211 14:43:00.521261 140441227016000 spec.py:321] Evaluating on the training split.
I0211 14:43:53.807867 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 14:44:45.584042 140441227016000 spec.py:349] Evaluating on the test split.
I0211 14:45:11.949517 140441227016000 submission_runner.py:408] Time since start: 6455.86s, 	Step: 6949, 	{'train/ctc_loss': Array(0.6986057, dtype=float32), 'train/wer': 0.22088870574499503, 'validation/ctc_loss': Array(0.8042088, dtype=float32), 'validation/wer': 0.23036967666566902, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.50204223, dtype=float32), 'test/wer': 0.16135518859301687, 'test/num_examples': 2472, 'score': 5777.536859035492, 'total_duration': 6455.857183456421, 'accumulated_submission_time': 5777.536859035492, 'accumulated_eval_time': 677.8204755783081, 'accumulated_logging_time': 0.19183111190795898}
I0211 14:45:11.984163 140284993492736 logging_writer.py:48] [6949] accumulated_eval_time=677.820476, accumulated_logging_time=0.191831, accumulated_submission_time=5777.536859, global_step=6949, preemption_count=0, score=5777.536859, test/ctc_loss=0.5020422339439392, test/num_examples=2472, test/wer=0.161355, total_duration=6455.857183, train/ctc_loss=0.6986057162284851, train/wer=0.220889, validation/ctc_loss=0.8042088150978088, validation/num_examples=5348, validation/wer=0.230370
I0211 14:45:50.788101 140284985100032 logging_writer.py:48] [7000] global_step=7000, grad_norm=2.8376624584198, loss=1.7253086566925049
I0211 14:47:05.720072 140284993492736 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.1902308464050293, loss=1.6207807064056396
I0211 14:48:26.566580 140284985100032 logging_writer.py:48] [7200] global_step=7200, grad_norm=2.4919493198394775, loss=1.7747682332992554
I0211 14:49:47.546097 140284993492736 logging_writer.py:48] [7300] global_step=7300, grad_norm=2.6630163192749023, loss=1.6948832273483276
I0211 14:51:04.376598 140284985100032 logging_writer.py:48] [7400] global_step=7400, grad_norm=4.096009731292725, loss=1.6344654560089111
I0211 14:52:24.316054 140284993492736 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.895199775695801, loss=1.6535102128982544
I0211 14:53:44.348503 140284985100032 logging_writer.py:48] [7600] global_step=7600, grad_norm=3.5569345951080322, loss=1.6427115201950073
I0211 14:55:11.456346 140284993492736 logging_writer.py:48] [7700] global_step=7700, grad_norm=2.5945961475372314, loss=1.6629018783569336
I0211 14:56:40.598729 140284985100032 logging_writer.py:48] [7800] global_step=7800, grad_norm=2.348165512084961, loss=1.632140874862671
I0211 14:58:09.178022 140284993492736 logging_writer.py:48] [7900] global_step=7900, grad_norm=3.597803831100464, loss=1.6604527235031128
I0211 14:59:40.531414 140284985100032 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.8803147077560425, loss=1.6393742561340332
I0211 15:01:08.489738 140284993492736 logging_writer.py:48] [8100] global_step=8100, grad_norm=5.1544952392578125, loss=1.6948421001434326
I0211 15:02:36.373245 140284985100032 logging_writer.py:48] [8200] global_step=8200, grad_norm=2.1010677814483643, loss=1.5913532972335815
I0211 15:04:01.849863 140284993492736 logging_writer.py:48] [8300] global_step=8300, grad_norm=2.47603702545166, loss=1.710278868675232
I0211 15:05:20.116012 140284985100032 logging_writer.py:48] [8400] global_step=8400, grad_norm=2.115115165710449, loss=1.661084771156311
I0211 15:06:38.497240 140284993492736 logging_writer.py:48] [8500] global_step=8500, grad_norm=3.2749521732330322, loss=1.5891598463058472
I0211 15:08:04.189481 140284985100032 logging_writer.py:48] [8600] global_step=8600, grad_norm=2.416306972503662, loss=1.605862021446228
I0211 15:09:12.380312 140441227016000 spec.py:321] Evaluating on the training split.
I0211 15:10:07.118296 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 15:11:01.827904 140441227016000 spec.py:349] Evaluating on the test split.
I0211 15:11:29.779913 140441227016000 submission_runner.py:408] Time since start: 8033.69s, 	Step: 8679, 	{'train/ctc_loss': Array(0.59292424, dtype=float32), 'train/wer': 0.19187749812130386, 'validation/ctc_loss': Array(0.7251276, dtype=float32), 'validation/wer': 0.21049074601504195, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4397295, dtype=float32), 'test/wer': 0.14384660695062254, 'test/num_examples': 2472, 'score': 7217.844273805618, 'total_duration': 8033.686626672745, 'accumulated_submission_time': 7217.844273805618, 'accumulated_eval_time': 815.2142312526703, 'accumulated_logging_time': 0.24258112907409668}
I0211 15:11:29.816420 140285423572736 logging_writer.py:48] [8679] accumulated_eval_time=815.214231, accumulated_logging_time=0.242581, accumulated_submission_time=7217.844274, global_step=8679, preemption_count=0, score=7217.844274, test/ctc_loss=0.4397295117378235, test/num_examples=2472, test/wer=0.143847, total_duration=8033.686627, train/ctc_loss=0.5929242372512817, train/wer=0.191877, validation/ctc_loss=0.7251275777816772, validation/num_examples=5348, validation/wer=0.210491
I0211 15:11:46.336199 140285415180032 logging_writer.py:48] [8700] global_step=8700, grad_norm=2.7999889850616455, loss=1.6319022178649902
I0211 15:13:01.213526 140285423572736 logging_writer.py:48] [8800] global_step=8800, grad_norm=2.6533925533294678, loss=1.6543450355529785
I0211 15:14:16.922056 140285415180032 logging_writer.py:48] [8900] global_step=8900, grad_norm=3.1612532138824463, loss=1.662805199623108
I0211 15:15:43.624046 140285423572736 logging_writer.py:48] [9000] global_step=9000, grad_norm=2.9863836765289307, loss=1.6138436794281006
I0211 15:17:11.186735 140285415180032 logging_writer.py:48] [9100] global_step=9100, grad_norm=2.7629363536834717, loss=1.5802381038665771
I0211 15:18:41.224530 140285423572736 logging_writer.py:48] [9200] global_step=9200, grad_norm=2.5824172496795654, loss=1.6109743118286133
I0211 15:20:11.180555 140284768212736 logging_writer.py:48] [9300] global_step=9300, grad_norm=2.769909620285034, loss=1.6156554222106934
I0211 15:21:29.495309 140284759820032 logging_writer.py:48] [9400] global_step=9400, grad_norm=2.5944511890411377, loss=1.6505807638168335
I0211 15:22:47.198879 140284768212736 logging_writer.py:48] [9500] global_step=9500, grad_norm=2.882657051086426, loss=1.6321240663528442
I0211 15:24:08.925128 140284759820032 logging_writer.py:48] [9600] global_step=9600, grad_norm=2.5693228244781494, loss=1.4754847288131714
I0211 15:25:31.899698 140284768212736 logging_writer.py:48] [9700] global_step=9700, grad_norm=2.8865909576416016, loss=1.6278926134109497
I0211 15:27:00.316116 140284759820032 logging_writer.py:48] [9800] global_step=9800, grad_norm=3.215512990951538, loss=1.5719507932662964
I0211 15:28:27.895255 140284768212736 logging_writer.py:48] [9900] global_step=9900, grad_norm=3.064110517501831, loss=1.5121939182281494
I0211 15:29:55.501444 140284759820032 logging_writer.py:48] [10000] global_step=10000, grad_norm=3.150864839553833, loss=1.5863935947418213
I0211 15:31:22.557562 140284768212736 logging_writer.py:48] [10100] global_step=10100, grad_norm=2.721036195755005, loss=1.5689584016799927
I0211 15:32:52.014589 140284759820032 logging_writer.py:48] [10200] global_step=10200, grad_norm=2.879591941833496, loss=1.5665671825408936
I0211 15:34:24.422532 140284768212736 logging_writer.py:48] [10300] global_step=10300, grad_norm=4.297262668609619, loss=1.564897060394287
I0211 15:35:30.319055 140441227016000 spec.py:321] Evaluating on the training split.
I0211 15:36:24.432994 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 15:37:17.083268 140441227016000 spec.py:349] Evaluating on the test split.
I0211 15:37:44.366292 140441227016000 submission_runner.py:408] Time since start: 9608.27s, 	Step: 10387, 	{'train/ctc_loss': Array(0.77348566, dtype=float32), 'train/wer': 0.24253480395453628, 'validation/ctc_loss': Array(0.9241919, dtype=float32), 'validation/wer': 0.25763441690722844, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.55918086, dtype=float32), 'test/wer': 0.17896532813356894, 'test/num_examples': 2472, 'score': 8658.25753068924, 'total_duration': 9608.27274942398, 'accumulated_submission_time': 8658.25753068924, 'accumulated_eval_time': 949.2553472518921, 'accumulated_logging_time': 0.29802942276000977}
I0211 15:37:44.405125 140284839892736 logging_writer.py:48] [10387] accumulated_eval_time=949.255347, accumulated_logging_time=0.298029, accumulated_submission_time=8658.257531, global_step=10387, preemption_count=0, score=8658.257531, test/ctc_loss=0.5591808557510376, test/num_examples=2472, test/wer=0.178965, total_duration=9608.272749, train/ctc_loss=0.7734856605529785, train/wer=0.242535, validation/ctc_loss=0.9241918921470642, validation/num_examples=5348, validation/wer=0.257634
I0211 15:37:54.897188 140284831500032 logging_writer.py:48] [10400] global_step=10400, grad_norm=2.0187957286834717, loss=1.8136826753616333
I0211 15:39:09.906361 140284839892736 logging_writer.py:48] [10500] global_step=10500, grad_norm=2.6396689414978027, loss=1.6324365139007568
I0211 15:40:25.850557 140284831500032 logging_writer.py:48] [10600] global_step=10600, grad_norm=2.257938861846924, loss=1.6101044416427612
I0211 15:41:40.960609 140284839892736 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.8786635398864746, loss=1.6901581287384033
I0211 15:42:59.177179 140284831500032 logging_writer.py:48] [10800] global_step=10800, grad_norm=2.4498488903045654, loss=1.6380995512008667
I0211 15:44:27.949450 140284839892736 logging_writer.py:48] [10900] global_step=10900, grad_norm=2.3720977306365967, loss=1.5503935813903809
I0211 15:45:58.211948 140284831500032 logging_writer.py:48] [11000] global_step=11000, grad_norm=3.0653162002563477, loss=1.6100414991378784
I0211 15:47:25.684444 140284839892736 logging_writer.py:48] [11100] global_step=11100, grad_norm=3.039949655532837, loss=1.5536162853240967
I0211 15:48:55.021453 140284831500032 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.6165869235992432, loss=1.5974100828170776
I0211 15:50:25.335283 140284839892736 logging_writer.py:48] [11300] global_step=11300, grad_norm=2.363457441329956, loss=1.5911164283752441
I0211 15:51:48.500030 140284839892736 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.7929463386535645, loss=1.5633729696273804
I0211 15:53:04.654402 140284831500032 logging_writer.py:48] [11500] global_step=11500, grad_norm=2.884141683578491, loss=1.5778170824050903
I0211 15:54:25.278882 140284839892736 logging_writer.py:48] [11600] global_step=11600, grad_norm=2.3764686584472656, loss=1.581206202507019
I0211 15:55:43.698004 140284831500032 logging_writer.py:48] [11700] global_step=11700, grad_norm=2.1056127548217773, loss=1.4751650094985962
I0211 15:57:12.083033 140284839892736 logging_writer.py:48] [11800] global_step=11800, grad_norm=2.1221115589141846, loss=1.6393331289291382
I0211 15:58:40.178582 140284831500032 logging_writer.py:48] [11900] global_step=11900, grad_norm=2.531247854232788, loss=1.5308321714401245
I0211 16:00:09.539841 140284839892736 logging_writer.py:48] [12000] global_step=12000, grad_norm=2.164278745651245, loss=1.53948175907135
I0211 16:01:40.453350 140284831500032 logging_writer.py:48] [12100] global_step=12100, grad_norm=2.7224538326263428, loss=1.5634808540344238
I0211 16:01:44.574063 140441227016000 spec.py:321] Evaluating on the training split.
I0211 16:02:38.343907 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 16:03:31.058895 140441227016000 spec.py:349] Evaluating on the test split.
I0211 16:03:58.194271 140441227016000 submission_runner.py:408] Time since start: 11182.10s, 	Step: 12106, 	{'train/ctc_loss': Array(0.49431863, dtype=float32), 'train/wer': 0.16404431564054583, 'validation/ctc_loss': Array(0.6491058, dtype=float32), 'validation/wer': 0.18740647054848084, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37781128, dtype=float32), 'test/wer': 0.12325066520423293, 'test/num_examples': 2472, 'score': 10098.338337421417, 'total_duration': 11182.101341962814, 'accumulated_submission_time': 10098.338337421417, 'accumulated_eval_time': 1082.87002658844, 'accumulated_logging_time': 0.3533177375793457}
I0211 16:03:58.231367 140284839892736 logging_writer.py:48] [12106] accumulated_eval_time=1082.870027, accumulated_logging_time=0.353318, accumulated_submission_time=10098.338337, global_step=12106, preemption_count=0, score=10098.338337, test/ctc_loss=0.3778112828731537, test/num_examples=2472, test/wer=0.123251, total_duration=11182.101342, train/ctc_loss=0.4943186342716217, train/wer=0.164044, validation/ctc_loss=0.6491057872772217, validation/num_examples=5348, validation/wer=0.187406
I0211 16:05:09.556453 140284831500032 logging_writer.py:48] [12200] global_step=12200, grad_norm=3.124969959259033, loss=1.5233772993087769
I0211 16:06:25.716001 140284839892736 logging_writer.py:48] [12300] global_step=12300, grad_norm=2.1563119888305664, loss=1.5193655490875244
I0211 16:07:49.634754 140284839892736 logging_writer.py:48] [12400] global_step=12400, grad_norm=3.186699390411377, loss=1.5147281885147095
I0211 16:09:07.660523 140284831500032 logging_writer.py:48] [12500] global_step=12500, grad_norm=2.9084055423736572, loss=1.4963932037353516
I0211 16:10:26.879295 140284839892736 logging_writer.py:48] [12600] global_step=12600, grad_norm=2.5644936561584473, loss=1.5068563222885132
I0211 16:11:50.081042 140284831500032 logging_writer.py:48] [12700] global_step=12700, grad_norm=2.9253602027893066, loss=1.5854989290237427
I0211 16:13:11.642348 140284839892736 logging_writer.py:48] [12800] global_step=12800, grad_norm=2.441732883453369, loss=1.5523320436477661
I0211 16:14:42.532817 140284831500032 logging_writer.py:48] [12900] global_step=12900, grad_norm=3.949733257293701, loss=1.531928539276123
I0211 16:16:11.754313 140284839892736 logging_writer.py:48] [13000] global_step=13000, grad_norm=3.260347604751587, loss=1.442652702331543
I0211 16:17:39.262969 140284831500032 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.542351245880127, loss=1.4583951234817505
I0211 16:19:10.439732 140284839892736 logging_writer.py:48] [13200] global_step=13200, grad_norm=2.902662515640259, loss=1.5704020261764526
I0211 16:20:40.934629 140284831500032 logging_writer.py:48] [13300] global_step=13300, grad_norm=2.4794185161590576, loss=1.4809426069259644
I0211 16:22:11.115810 140284839892736 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.837765097618103, loss=1.500957727432251
I0211 16:23:27.887782 140284831500032 logging_writer.py:48] [13500] global_step=13500, grad_norm=2.9324636459350586, loss=1.444430947303772
I0211 16:24:49.077481 140284839892736 logging_writer.py:48] [13600] global_step=13600, grad_norm=3.023348093032837, loss=1.4405404329299927
I0211 16:26:07.701883 140284831500032 logging_writer.py:48] [13700] global_step=13700, grad_norm=3.685124635696411, loss=1.4746114015579224
I0211 16:27:33.308450 140284839892736 logging_writer.py:48] [13800] global_step=13800, grad_norm=3.7507731914520264, loss=1.4691540002822876
I0211 16:27:58.224022 140441227016000 spec.py:321] Evaluating on the training split.
I0211 16:28:52.488602 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 16:29:45.591503 140441227016000 spec.py:349] Evaluating on the test split.
I0211 16:30:13.259031 140441227016000 submission_runner.py:408] Time since start: 12757.17s, 	Step: 13829, 	{'train/ctc_loss': Array(0.45966643, dtype=float32), 'train/wer': 0.1526637057536937, 'validation/ctc_loss': Array(0.6343328, dtype=float32), 'validation/wer': 0.18310049528370198, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3647178, dtype=float32), 'test/wer': 0.11951333455202812, 'test/num_examples': 2472, 'score': 11538.243397474289, 'total_duration': 12757.166206598282, 'accumulated_submission_time': 11538.243397474289, 'accumulated_eval_time': 1217.89963889122, 'accumulated_logging_time': 0.40578389167785645}
I0211 16:30:13.293725 140284993492736 logging_writer.py:48] [13829] accumulated_eval_time=1217.899639, accumulated_logging_time=0.405784, accumulated_submission_time=11538.243397, global_step=13829, preemption_count=0, score=11538.243397, test/ctc_loss=0.3647178113460541, test/num_examples=2472, test/wer=0.119513, total_duration=12757.166207, train/ctc_loss=0.4596664309501648, train/wer=0.152664, validation/ctc_loss=0.6343327760696411, validation/num_examples=5348, validation/wer=0.183100
I0211 16:31:07.143809 140284985100032 logging_writer.py:48] [13900] global_step=13900, grad_norm=2.887753486633301, loss=1.5776770114898682
I0211 16:32:22.490436 140284993492736 logging_writer.py:48] [14000] global_step=14000, grad_norm=2.380080461502075, loss=1.515140414237976
I0211 16:33:40.843806 140284985100032 logging_writer.py:48] [14100] global_step=14100, grad_norm=3.2288243770599365, loss=1.4430663585662842
I0211 16:35:09.179792 140284993492736 logging_writer.py:48] [14200] global_step=14200, grad_norm=2.7063100337982178, loss=1.4305295944213867
I0211 16:36:36.413687 140284985100032 logging_writer.py:48] [14300] global_step=14300, grad_norm=3.7291436195373535, loss=1.5058826208114624
I0211 16:38:05.771208 140284993492736 logging_writer.py:48] [14400] global_step=14400, grad_norm=2.152534246444702, loss=1.5190815925598145
I0211 16:39:26.440034 140284993492736 logging_writer.py:48] [14500] global_step=14500, grad_norm=2.926758289337158, loss=1.4531477689743042
I0211 16:40:44.356470 140284985100032 logging_writer.py:48] [14600] global_step=14600, grad_norm=2.6933836936950684, loss=1.4877455234527588
I0211 16:42:02.779330 140284993492736 logging_writer.py:48] [14700] global_step=14700, grad_norm=2.314072608947754, loss=1.491838812828064
I0211 16:43:27.482443 140284985100032 logging_writer.py:48] [14800] global_step=14800, grad_norm=2.9076602458953857, loss=1.5082001686096191
I0211 16:44:54.060299 140284993492736 logging_writer.py:48] [14900] global_step=14900, grad_norm=2.3223013877868652, loss=1.4874111413955688
I0211 16:46:23.938987 140284985100032 logging_writer.py:48] [15000] global_step=15000, grad_norm=2.6315624713897705, loss=1.5388401746749878
I0211 16:47:53.865861 140284993492736 logging_writer.py:48] [15100] global_step=15100, grad_norm=4.498766899108887, loss=1.4613116979599
I0211 16:49:21.345223 140284985100032 logging_writer.py:48] [15200] global_step=15200, grad_norm=2.229527235031128, loss=1.4739304780960083
I0211 16:50:48.009263 140284993492736 logging_writer.py:48] [15300] global_step=15300, grad_norm=2.6934828758239746, loss=1.4974175691604614
I0211 16:52:15.888651 140284985100032 logging_writer.py:48] [15400] global_step=15400, grad_norm=2.0164036750793457, loss=1.479274034500122
I0211 16:53:40.350929 140284993492736 logging_writer.py:48] [15500] global_step=15500, grad_norm=2.3907992839813232, loss=1.451157808303833
I0211 16:54:13.591155 140441227016000 spec.py:321] Evaluating on the training split.
I0211 16:55:08.880601 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 16:56:01.412984 140441227016000 spec.py:349] Evaluating on the test split.
I0211 16:56:29.215302 140441227016000 submission_runner.py:408] Time since start: 14333.12s, 	Step: 15544, 	{'train/ctc_loss': Array(0.4444199, dtype=float32), 'train/wer': 0.14530537415237108, 'validation/ctc_loss': Array(0.5959213, dtype=float32), 'validation/wer': 0.1713025092443303, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34296244, dtype=float32), 'test/wer': 0.11114496374383036, 'test/num_examples': 2472, 'score': 12978.453676700592, 'total_duration': 14333.121833562851, 'accumulated_submission_time': 12978.453676700592, 'accumulated_eval_time': 1353.5177383422852, 'accumulated_logging_time': 0.4561934471130371}
I0211 16:56:29.248672 140284993492736 logging_writer.py:48] [15544] accumulated_eval_time=1353.517738, accumulated_logging_time=0.456193, accumulated_submission_time=12978.453677, global_step=15544, preemption_count=0, score=12978.453677, test/ctc_loss=0.34296244382858276, test/num_examples=2472, test/wer=0.111145, total_duration=14333.121834, train/ctc_loss=0.44441989064216614, train/wer=0.145305, validation/ctc_loss=0.5959212779998779, validation/num_examples=5348, validation/wer=0.171303
I0211 16:57:12.231489 140284985100032 logging_writer.py:48] [15600] global_step=15600, grad_norm=2.538724422454834, loss=1.476578712463379
I0211 16:58:27.280448 140284993492736 logging_writer.py:48] [15700] global_step=15700, grad_norm=2.539510726928711, loss=1.525325894355774
I0211 16:59:42.402522 140284985100032 logging_writer.py:48] [15800] global_step=15800, grad_norm=2.3069162368774414, loss=1.4544992446899414
I0211 17:00:58.180473 140284993492736 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.8484840393066406, loss=1.4919291734695435
I0211 17:02:24.639996 140284985100032 logging_writer.py:48] [16000] global_step=16000, grad_norm=2.0700507164001465, loss=1.4799706935882568
I0211 17:03:54.783842 140284993492736 logging_writer.py:48] [16100] global_step=16100, grad_norm=2.6026570796966553, loss=1.3990819454193115
I0211 17:05:26.154686 140284985100032 logging_writer.py:48] [16200] global_step=16200, grad_norm=2.2710001468658447, loss=1.415913701057434
I0211 17:06:53.202577 140284993492736 logging_writer.py:48] [16300] global_step=16300, grad_norm=2.3057827949523926, loss=1.4808236360549927
I0211 17:08:21.220371 140284985100032 logging_writer.py:48] [16400] global_step=16400, grad_norm=2.957977056503296, loss=1.3925704956054688
I0211 17:09:51.386208 140284993492736 logging_writer.py:48] [16500] global_step=16500, grad_norm=2.370047092437744, loss=1.4321515560150146
I0211 17:11:07.699233 140284985100032 logging_writer.py:48] [16600] global_step=16600, grad_norm=3.508092164993286, loss=1.4472739696502686
I0211 17:12:24.465301 140284993492736 logging_writer.py:48] [16700] global_step=16700, grad_norm=2.2558155059814453, loss=1.4535958766937256
I0211 17:13:47.930385 140284985100032 logging_writer.py:48] [16800] global_step=16800, grad_norm=2.620764970779419, loss=1.4221115112304688
I0211 17:15:07.357603 140284993492736 logging_writer.py:48] [16900] global_step=16900, grad_norm=5.118731498718262, loss=1.400611400604248
I0211 17:16:35.198215 140284985100032 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.9260872602462769, loss=1.4445912837982178
I0211 17:18:04.399006 140284993492736 logging_writer.py:48] [17100] global_step=17100, grad_norm=2.8313517570495605, loss=1.3896030187606812
I0211 17:19:36.447524 140284985100032 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.936343789100647, loss=1.4426815509796143
I0211 17:20:29.323816 140441227016000 spec.py:321] Evaluating on the training split.
I0211 17:21:23.565477 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 17:22:17.649703 140441227016000 spec.py:349] Evaluating on the test split.
I0211 17:22:44.264613 140441227016000 submission_runner.py:408] Time since start: 15908.17s, 	Step: 17258, 	{'train/ctc_loss': Array(0.41884962, dtype=float32), 'train/wer': 0.13926902350126338, 'validation/ctc_loss': Array(0.58758664, dtype=float32), 'validation/wer': 0.16857989708139837, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3266319, dtype=float32), 'test/wer': 0.10622956147299575, 'test/num_examples': 2472, 'score': 14418.44036602974, 'total_duration': 15908.17206454277, 'accumulated_submission_time': 14418.44036602974, 'accumulated_eval_time': 1488.4534299373627, 'accumulated_logging_time': 0.5051791667938232}
I0211 17:22:44.301226 140284993492736 logging_writer.py:48] [17258] accumulated_eval_time=1488.453430, accumulated_logging_time=0.505179, accumulated_submission_time=14418.440366, global_step=17258, preemption_count=0, score=14418.440366, test/ctc_loss=0.32663190364837646, test/num_examples=2472, test/wer=0.106230, total_duration=15908.172065, train/ctc_loss=0.4188496172428131, train/wer=0.139269, validation/ctc_loss=0.5875866413116455, validation/num_examples=5348, validation/wer=0.168580
I0211 17:23:16.557953 140284985100032 logging_writer.py:48] [17300] global_step=17300, grad_norm=2.8557517528533936, loss=1.4203938245773315
I0211 17:24:31.476825 140284993492736 logging_writer.py:48] [17400] global_step=17400, grad_norm=2.3860716819763184, loss=1.4251714944839478
I0211 17:25:51.073182 140284985100032 logging_writer.py:48] [17500] global_step=17500, grad_norm=2.6525661945343018, loss=1.445386290550232
I0211 17:27:13.889126 140284993492736 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.9907801151275635, loss=1.4145257472991943
I0211 17:28:34.058860 140284985100032 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.4688740968704224, loss=1.4214738607406616
I0211 17:29:55.150805 140284993492736 logging_writer.py:48] [17800] global_step=17800, grad_norm=2.062885046005249, loss=1.4250816106796265
I0211 17:31:17.286880 140284985100032 logging_writer.py:48] [17900] global_step=17900, grad_norm=2.5874719619750977, loss=1.3722831010818481
I0211 17:32:43.193115 140284993492736 logging_writer.py:48] [18000] global_step=18000, grad_norm=2.465038537979126, loss=1.4547697305679321
I0211 17:34:13.313620 140284985100032 logging_writer.py:48] [18100] global_step=18100, grad_norm=3.019106864929199, loss=1.3866060972213745
I0211 17:35:42.870771 140284993492736 logging_writer.py:48] [18200] global_step=18200, grad_norm=2.701054096221924, loss=1.4614996910095215
I0211 17:37:09.693582 140284985100032 logging_writer.py:48] [18300] global_step=18300, grad_norm=2.7116751670837402, loss=1.4102966785430908
I0211 17:38:34.911190 140284993492736 logging_writer.py:48] [18400] global_step=18400, grad_norm=2.9427506923675537, loss=1.3920007944107056
I0211 17:40:04.468193 140284985100032 logging_writer.py:48] [18500] global_step=18500, grad_norm=2.8539535999298096, loss=1.4660437107086182
I0211 17:41:28.357673 140284993492736 logging_writer.py:48] [18600] global_step=18600, grad_norm=3.4413602352142334, loss=1.411778450012207
I0211 17:42:46.619693 140284985100032 logging_writer.py:48] [18700] global_step=18700, grad_norm=3.0512895584106445, loss=1.3946037292480469
I0211 17:44:03.123479 140284993492736 logging_writer.py:48] [18800] global_step=18800, grad_norm=2.601870059967041, loss=1.3900161981582642
I0211 17:45:24.270964 140284985100032 logging_writer.py:48] [18900] global_step=18900, grad_norm=3.2388737201690674, loss=1.4375251531600952
I0211 17:46:44.840659 140441227016000 spec.py:321] Evaluating on the training split.
I0211 17:47:39.316113 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 17:48:33.141205 140441227016000 spec.py:349] Evaluating on the test split.
I0211 17:49:01.316102 140441227016000 submission_runner.py:408] Time since start: 17485.22s, 	Step: 18996, 	{'train/ctc_loss': Array(0.41031313, dtype=float32), 'train/wer': 0.13893716170594292, 'validation/ctc_loss': Array(0.5695679, dtype=float32), 'validation/wer': 0.16348224026569605, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31985068, dtype=float32), 'test/wer': 0.10594519935815408, 'test/num_examples': 2472, 'score': 15858.892181396484, 'total_duration': 17485.223326921463, 'accumulated_submission_time': 15858.892181396484, 'accumulated_eval_time': 1624.9235422611237, 'accumulated_logging_time': 0.5569183826446533}
I0211 17:49:01.353130 140284993492736 logging_writer.py:48] [18996] accumulated_eval_time=1624.923542, accumulated_logging_time=0.556918, accumulated_submission_time=15858.892181, global_step=18996, preemption_count=0, score=15858.892181, test/ctc_loss=0.3198506832122803, test/num_examples=2472, test/wer=0.105945, total_duration=17485.223327, train/ctc_loss=0.41031312942504883, train/wer=0.138937, validation/ctc_loss=0.5695679187774658, validation/num_examples=5348, validation/wer=0.163482
I0211 17:49:05.336797 140284985100032 logging_writer.py:48] [19000] global_step=19000, grad_norm=2.0895254611968994, loss=1.3624858856201172
I0211 17:50:20.877107 140284993492736 logging_writer.py:48] [19100] global_step=19100, grad_norm=2.670027017593384, loss=1.4152090549468994
I0211 17:51:36.128463 140284985100032 logging_writer.py:48] [19200] global_step=19200, grad_norm=2.616502285003662, loss=1.4696637392044067
I0211 17:53:02.996008 140284993492736 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.6435561180114746, loss=1.3850340843200684
I0211 17:54:32.660466 140284985100032 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.7738789319992065, loss=1.4012064933776855
I0211 17:56:03.876146 140284993492736 logging_writer.py:48] [19500] global_step=19500, grad_norm=2.9815211296081543, loss=1.4197356700897217
I0211 17:57:28.750299 140284993492736 logging_writer.py:48] [19600] global_step=19600, grad_norm=3.163011074066162, loss=1.4304323196411133
I0211 17:58:46.020687 140284985100032 logging_writer.py:48] [19700] global_step=19700, grad_norm=2.5143465995788574, loss=1.3791935443878174
I0211 18:00:03.947230 140284993492736 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.80058753490448, loss=1.3663873672485352
I0211 18:01:21.383556 140284985100032 logging_writer.py:48] [19900] global_step=19900, grad_norm=2.9029176235198975, loss=1.4217498302459717
I0211 18:02:44.639525 140284993492736 logging_writer.py:48] [20000] global_step=20000, grad_norm=2.2003133296966553, loss=1.4005036354064941
I0211 18:04:11.015642 140284985100032 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.9090356826782227, loss=1.3345086574554443
I0211 18:05:39.487488 140284993492736 logging_writer.py:48] [20200] global_step=20200, grad_norm=4.075026512145996, loss=1.3834927082061768
I0211 18:07:03.962614 140284985100032 logging_writer.py:48] [20300] global_step=20300, grad_norm=3.022648811340332, loss=1.42079758644104
I0211 18:08:34.465585 140284993492736 logging_writer.py:48] [20400] global_step=20400, grad_norm=2.757401943206787, loss=1.4399938583374023
I0211 18:10:06.242728 140284985100032 logging_writer.py:48] [20500] global_step=20500, grad_norm=2.3410496711730957, loss=1.436983585357666
I0211 18:11:35.203871 140284993492736 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.6501147747039795, loss=1.3057600259780884
I0211 18:12:51.073343 140284985100032 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.9857873916625977, loss=1.3901963233947754
I0211 18:13:01.879752 140441227016000 spec.py:321] Evaluating on the training split.
I0211 18:13:55.773638 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 18:14:48.946859 140441227016000 spec.py:349] Evaluating on the test split.
I0211 18:15:15.522963 140441227016000 submission_runner.py:408] Time since start: 19059.43s, 	Step: 20715, 	{'train/ctc_loss': Array(0.36195385, dtype=float32), 'train/wer': 0.12168618161053411, 'validation/ctc_loss': Array(0.5525102, dtype=float32), 'validation/wer': 0.16128097936800642, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30855164, dtype=float32), 'test/wer': 0.10050169601689923, 'test/num_examples': 2472, 'score': 17299.33094882965, 'total_duration': 19059.429202079773, 'accumulated_submission_time': 17299.33094882965, 'accumulated_eval_time': 1758.560397386551, 'accumulated_logging_time': 0.6087489128112793}
I0211 18:15:15.558809 140285423572736 logging_writer.py:48] [20715] accumulated_eval_time=1758.560397, accumulated_logging_time=0.608749, accumulated_submission_time=17299.330949, global_step=20715, preemption_count=0, score=17299.330949, test/ctc_loss=0.3085516393184662, test/num_examples=2472, test/wer=0.100502, total_duration=19059.429202, train/ctc_loss=0.36195385456085205, train/wer=0.121686, validation/ctc_loss=0.5525102019309998, validation/num_examples=5348, validation/wer=0.161281
I0211 18:16:20.280683 140285415180032 logging_writer.py:48] [20800] global_step=20800, grad_norm=3.056838274002075, loss=1.3721510171890259
I0211 18:17:35.219410 140285423572736 logging_writer.py:48] [20900] global_step=20900, grad_norm=2.6085498332977295, loss=1.3397196531295776
I0211 18:18:50.492179 140285415180032 logging_writer.py:48] [21000] global_step=21000, grad_norm=2.7438690662384033, loss=1.3480088710784912
I0211 18:20:13.191861 140285423572736 logging_writer.py:48] [21100] global_step=21100, grad_norm=2.305088520050049, loss=1.3568681478500366
I0211 18:21:41.900706 140285415180032 logging_writer.py:48] [21200] global_step=21200, grad_norm=2.923895835876465, loss=1.3683782815933228
I0211 18:23:07.213894 140285423572736 logging_writer.py:48] [21300] global_step=21300, grad_norm=2.05914568901062, loss=1.4016834497451782
I0211 18:24:36.527086 140285415180032 logging_writer.py:48] [21400] global_step=21400, grad_norm=2.971907615661621, loss=1.4138742685317993
I0211 18:26:08.568692 140285423572736 logging_writer.py:48] [21500] global_step=21500, grad_norm=3.3536298274993896, loss=1.397963523864746
I0211 18:27:36.893562 140285415180032 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.6359390020370483, loss=1.3627173900604248
I0211 18:28:58.568041 140284768212736 logging_writer.py:48] [21700] global_step=21700, grad_norm=3.700650930404663, loss=1.3075382709503174
I0211 18:30:14.362709 140284759820032 logging_writer.py:48] [21800] global_step=21800, grad_norm=2.389946937561035, loss=1.3747577667236328
I0211 18:31:33.414294 140284768212736 logging_writer.py:48] [21900] global_step=21900, grad_norm=3.293696880340576, loss=1.3644530773162842
I0211 18:32:53.707144 140284759820032 logging_writer.py:48] [22000] global_step=22000, grad_norm=2.6713156700134277, loss=1.457504153251648
I0211 18:34:20.638104 140284768212736 logging_writer.py:48] [22100] global_step=22100, grad_norm=3.0761003494262695, loss=1.3391739130020142
I0211 18:35:49.349380 140284759820032 logging_writer.py:48] [22200] global_step=22200, grad_norm=2.2485368251800537, loss=1.366050124168396
I0211 18:37:21.183567 140284768212736 logging_writer.py:48] [22300] global_step=22300, grad_norm=3.2506747245788574, loss=1.4724302291870117
I0211 18:38:51.599610 140284759820032 logging_writer.py:48] [22400] global_step=22400, grad_norm=2.3124608993530273, loss=1.3802921772003174
I0211 18:39:15.732718 140441227016000 spec.py:321] Evaluating on the training split.
I0211 18:40:09.720591 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 18:41:02.568198 140441227016000 spec.py:349] Evaluating on the test split.
I0211 18:41:29.469972 140441227016000 submission_runner.py:408] Time since start: 20633.38s, 	Step: 22428, 	{'train/ctc_loss': Array(0.3208047, dtype=float32), 'train/wer': 0.10747480926407368, 'validation/ctc_loss': Array(0.53712636, dtype=float32), 'validation/wer': 0.15489925369531846, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30128074, dtype=float32), 'test/wer': 0.09650031482948429, 'test/num_examples': 2472, 'score': 18739.416509628296, 'total_duration': 20633.377140283585, 'accumulated_submission_time': 18739.416509628296, 'accumulated_eval_time': 1892.2922401428223, 'accumulated_logging_time': 0.6600890159606934}
I0211 18:41:29.506357 140284768212736 logging_writer.py:48] [22428] accumulated_eval_time=1892.292240, accumulated_logging_time=0.660089, accumulated_submission_time=18739.416510, global_step=22428, preemption_count=0, score=18739.416510, test/ctc_loss=0.3012807369232178, test/num_examples=2472, test/wer=0.096500, total_duration=20633.377140, train/ctc_loss=0.3208046853542328, train/wer=0.107475, validation/ctc_loss=0.537126362323761, validation/num_examples=5348, validation/wer=0.154899
I0211 18:42:24.720775 140284759820032 logging_writer.py:48] [22500] global_step=22500, grad_norm=2.1530025005340576, loss=1.3607805967330933
I0211 18:43:39.814760 140284768212736 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.5858432054519653, loss=1.4101988077163696
I0211 18:44:59.530691 140285423572736 logging_writer.py:48] [22700] global_step=22700, grad_norm=2.5993833541870117, loss=1.306832194328308
I0211 18:46:18.820097 140285415180032 logging_writer.py:48] [22800] global_step=22800, grad_norm=2.216116428375244, loss=1.4173662662506104
I0211 18:47:37.332976 140285423572736 logging_writer.py:48] [22900] global_step=22900, grad_norm=2.305075168609619, loss=1.368701696395874
I0211 18:48:56.524484 140285415180032 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.7631385326385498, loss=1.2900055646896362
I0211 18:50:22.354099 140285423572736 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.9942255020141602, loss=1.350043773651123
I0211 18:51:51.384123 140285415180032 logging_writer.py:48] [23200] global_step=23200, grad_norm=3.395730972290039, loss=1.3624701499938965
I0211 18:53:20.245519 140285423572736 logging_writer.py:48] [23300] global_step=23300, grad_norm=2.957277297973633, loss=1.3439021110534668
I0211 18:54:49.180938 140285415180032 logging_writer.py:48] [23400] global_step=23400, grad_norm=2.680253028869629, loss=1.3271712064743042
I0211 18:56:16.918378 140285423572736 logging_writer.py:48] [23500] global_step=23500, grad_norm=2.932863712310791, loss=1.3496689796447754
I0211 18:57:45.912839 140285415180032 logging_writer.py:48] [23600] global_step=23600, grad_norm=3.00294828414917, loss=1.3811554908752441
I0211 18:59:17.650256 140284768212736 logging_writer.py:48] [23700] global_step=23700, grad_norm=4.947315216064453, loss=1.3603315353393555
I0211 19:00:34.058837 140284759820032 logging_writer.py:48] [23800] global_step=23800, grad_norm=2.2942211627960205, loss=1.3519870042800903
I0211 19:01:54.380593 140284768212736 logging_writer.py:48] [23900] global_step=23900, grad_norm=2.017988920211792, loss=1.3034977912902832
I0211 19:03:12.052101 140284759820032 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.7391828298568726, loss=1.2932525873184204
I0211 19:04:33.052992 140284768212736 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.8457447290420532, loss=1.314824104309082
I0211 19:05:30.002957 140441227016000 spec.py:321] Evaluating on the training split.
I0211 19:06:23.699191 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 19:07:16.395888 140441227016000 spec.py:349] Evaluating on the test split.
I0211 19:07:43.232299 140441227016000 submission_runner.py:408] Time since start: 22207.14s, 	Step: 24166, 	{'train/ctc_loss': Array(0.36535513, dtype=float32), 'train/wer': 0.12454700430134233, 'validation/ctc_loss': Array(0.52740115, dtype=float32), 'validation/wer': 0.1513463413692229, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.289528, dtype=float32), 'test/wer': 0.093412954725489, 'test/num_examples': 2472, 'score': 20179.82127547264, 'total_duration': 22207.138612270355, 'accumulated_submission_time': 20179.82127547264, 'accumulated_eval_time': 2025.5153098106384, 'accumulated_logging_time': 0.7153546810150146}
I0211 19:07:43.268213 140284768212736 logging_writer.py:48] [24166] accumulated_eval_time=2025.515310, accumulated_logging_time=0.715355, accumulated_submission_time=20179.821275, global_step=24166, preemption_count=0, score=20179.821275, test/ctc_loss=0.2895280122756958, test/num_examples=2472, test/wer=0.093413, total_duration=22207.138612, train/ctc_loss=0.36535513401031494, train/wer=0.124547, validation/ctc_loss=0.5274011492729187, validation/num_examples=5348, validation/wer=0.151346
I0211 19:08:09.674365 140284759820032 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.7392578125, loss=1.3340339660644531
I0211 19:09:24.418823 140284768212736 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.8500317335128784, loss=1.3216530084609985
I0211 19:10:43.455293 140284759820032 logging_writer.py:48] [24400] global_step=24400, grad_norm=2.2664906978607178, loss=1.3104941844940186
I0211 19:12:13.245789 140284768212736 logging_writer.py:48] [24500] global_step=24500, grad_norm=4.011956691741943, loss=1.3781864643096924
I0211 19:13:42.362041 140284759820032 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.9544178247451782, loss=1.332480788230896
I0211 19:15:09.684880 140284768212736 logging_writer.py:48] [24700] global_step=24700, grad_norm=4.173197269439697, loss=1.3886557817459106
I0211 19:16:31.072972 140284768212736 logging_writer.py:48] [24800] global_step=24800, grad_norm=2.5806081295013428, loss=1.3472280502319336
I0211 19:17:46.605743 140284759820032 logging_writer.py:48] [24900] global_step=24900, grad_norm=2.4006547927856445, loss=1.3226828575134277
I0211 19:19:04.367750 140284768212736 logging_writer.py:48] [25000] global_step=25000, grad_norm=2.307311534881592, loss=1.326998233795166
I0211 19:20:25.328882 140284759820032 logging_writer.py:48] [25100] global_step=25100, grad_norm=3.9439926147460938, loss=1.3520395755767822
I0211 19:21:49.577904 140284768212736 logging_writer.py:48] [25200] global_step=25200, grad_norm=2.1771621704101562, loss=1.3505594730377197
I0211 19:23:19.053406 140284759820032 logging_writer.py:48] [25300] global_step=25300, grad_norm=2.8502118587493896, loss=1.3596205711364746
I0211 19:24:49.867407 140284768212736 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.7914597988128662, loss=1.320351481437683
I0211 19:26:21.654702 140284759820032 logging_writer.py:48] [25500] global_step=25500, grad_norm=2.3670175075531006, loss=1.339076280593872
I0211 19:27:52.817809 140284768212736 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.9442538022994995, loss=1.2836717367172241
I0211 19:29:19.152116 140284759820032 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.9639853239059448, loss=1.3527872562408447
I0211 19:30:45.016893 140284768212736 logging_writer.py:48] [25800] global_step=25800, grad_norm=2.6662235260009766, loss=1.2913001775741577
I0211 19:31:43.555455 140441227016000 spec.py:321] Evaluating on the training split.
I0211 19:32:37.526149 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 19:33:30.208319 140441227016000 spec.py:349] Evaluating on the test split.
I0211 19:33:57.935899 140441227016000 submission_runner.py:408] Time since start: 23781.84s, 	Step: 25875, 	{'train/ctc_loss': Array(0.32537457, dtype=float32), 'train/wer': 0.10948236632536974, 'validation/ctc_loss': Array(0.51202977, dtype=float32), 'validation/wer': 0.14770653716558696, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2792349, dtype=float32), 'test/wer': 0.09168647045680743, 'test/num_examples': 2472, 'score': 21620.019072532654, 'total_duration': 23781.841695070267, 'accumulated_submission_time': 21620.019072532654, 'accumulated_eval_time': 2159.8889672756195, 'accumulated_logging_time': 0.7683267593383789}
I0211 19:33:57.973426 140284768212736 logging_writer.py:48] [25875] accumulated_eval_time=2159.888967, accumulated_logging_time=0.768327, accumulated_submission_time=21620.019073, global_step=25875, preemption_count=0, score=21620.019073, test/ctc_loss=0.2792348861694336, test/num_examples=2472, test/wer=0.091686, total_duration=23781.841695, train/ctc_loss=0.325374573469162, train/wer=0.109482, validation/ctc_loss=0.512029767036438, validation/num_examples=5348, validation/wer=0.147707
I0211 19:34:17.740045 140284759820032 logging_writer.py:48] [25900] global_step=25900, grad_norm=2.006783962249756, loss=1.2182526588439941
I0211 19:35:32.534907 140284768212736 logging_writer.py:48] [26000] global_step=26000, grad_norm=4.273081302642822, loss=1.2838828563690186
I0211 19:36:47.793329 140284759820032 logging_writer.py:48] [26100] global_step=26100, grad_norm=2.24432373046875, loss=1.3233147859573364
I0211 19:38:06.787219 140284768212736 logging_writer.py:48] [26200] global_step=26200, grad_norm=2.7729642391204834, loss=1.349626064300537
I0211 19:39:35.069335 140284759820032 logging_writer.py:48] [26300] global_step=26300, grad_norm=2.696467399597168, loss=1.3272082805633545
I0211 19:41:05.185971 140284768212736 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.6569674015045166, loss=1.271461844444275
I0211 19:42:36.799069 140284759820032 logging_writer.py:48] [26500] global_step=26500, grad_norm=3.548793315887451, loss=1.3578413724899292
I0211 19:44:07.775972 140284768212736 logging_writer.py:48] [26600] global_step=26600, grad_norm=2.209597110748291, loss=1.2912453413009644
I0211 19:45:37.311004 140284759820032 logging_writer.py:48] [26700] global_step=26700, grad_norm=2.6586246490478516, loss=1.3369979858398438
I0211 19:47:06.450715 140284768212736 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.9079563617706299, loss=1.2977656126022339
I0211 19:48:23.945259 140284759820032 logging_writer.py:48] [26900] global_step=26900, grad_norm=5.004307746887207, loss=1.2827858924865723
I0211 19:49:44.616693 140284768212736 logging_writer.py:48] [27000] global_step=27000, grad_norm=2.1826682090759277, loss=1.310583472251892
I0211 19:51:08.746453 140284759820032 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.7912505865097046, loss=1.3710911273956299
I0211 19:52:32.300945 140284768212736 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.8299320936203003, loss=1.302163004875183
I0211 19:54:00.705672 140284759820032 logging_writer.py:48] [27300] global_step=27300, grad_norm=3.329040050506592, loss=1.3154553174972534
I0211 19:55:32.940394 140284768212736 logging_writer.py:48] [27400] global_step=27400, grad_norm=4.036105632781982, loss=1.299343228340149
I0211 19:56:57.458826 140284759820032 logging_writer.py:48] [27500] global_step=27500, grad_norm=2.398144245147705, loss=1.2795919179916382
I0211 19:57:58.192278 140441227016000 spec.py:321] Evaluating on the training split.
I0211 19:58:53.706169 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 19:59:47.755974 140441227016000 spec.py:349] Evaluating on the test split.
I0211 20:00:15.647701 140441227016000 submission_runner.py:408] Time since start: 25359.56s, 	Step: 27568, 	{'train/ctc_loss': Array(0.31351584, dtype=float32), 'train/wer': 0.1057912797410919, 'validation/ctc_loss': Array(0.5010615, dtype=float32), 'validation/wer': 0.1443370632476322, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2763063, dtype=float32), 'test/wer': 0.08935063879917941, 'test/num_examples': 2472, 'score': 23060.151054382324, 'total_duration': 25359.555035591125, 'accumulated_submission_time': 23060.151054382324, 'accumulated_eval_time': 2297.339144229889, 'accumulated_logging_time': 0.822613000869751}
I0211 20:00:15.684636 140285423572736 logging_writer.py:48] [27568] accumulated_eval_time=2297.339144, accumulated_logging_time=0.822613, accumulated_submission_time=23060.151054, global_step=27568, preemption_count=0, score=23060.151054, test/ctc_loss=0.27630630135536194, test/num_examples=2472, test/wer=0.089351, total_duration=25359.555036, train/ctc_loss=0.313515841960907, train/wer=0.105791, validation/ctc_loss=0.5010614991188049, validation/num_examples=5348, validation/wer=0.144337
I0211 20:00:40.390230 140285415180032 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.404436707496643, loss=1.279419183731079
I0211 20:01:55.186470 140285423572736 logging_writer.py:48] [27700] global_step=27700, grad_norm=2.074345588684082, loss=1.3101290464401245
I0211 20:03:10.248937 140285415180032 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.8473401069641113, loss=1.2523510456085205
I0211 20:04:31.116863 140285423572736 logging_writer.py:48] [27900] global_step=27900, grad_norm=2.3203985691070557, loss=1.3101698160171509
I0211 20:05:47.252667 140285415180032 logging_writer.py:48] [28000] global_step=28000, grad_norm=2.6193134784698486, loss=1.2575132846832275
I0211 20:07:08.257865 140285423572736 logging_writer.py:48] [28100] global_step=28100, grad_norm=2.0412323474884033, loss=1.2849247455596924
I0211 20:08:31.187858 140285415180032 logging_writer.py:48] [28200] global_step=28200, grad_norm=2.748802423477173, loss=1.2935764789581299
I0211 20:10:00.693481 140285423572736 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.815710186958313, loss=1.2981541156768799
I0211 20:11:29.306900 140285415180032 logging_writer.py:48] [28400] global_step=28400, grad_norm=3.541242837905884, loss=1.2540247440338135
I0211 20:12:57.659260 140285423572736 logging_writer.py:48] [28500] global_step=28500, grad_norm=2.353217124938965, loss=1.2952158451080322
I0211 20:14:27.209009 140285415180032 logging_writer.py:48] [28600] global_step=28600, grad_norm=2.9830996990203857, loss=1.2147780656814575
I0211 20:15:57.233493 140285423572736 logging_writer.py:48] [28700] global_step=28700, grad_norm=2.9903433322906494, loss=1.3441592454910278
I0211 20:17:20.513494 140285415180032 logging_writer.py:48] [28800] global_step=28800, grad_norm=3.064296007156372, loss=1.2344930171966553
I0211 20:18:46.048386 140285423572736 logging_writer.py:48] [28900] global_step=28900, grad_norm=2.7000701427459717, loss=1.2753016948699951
I0211 20:20:04.150451 140285415180032 logging_writer.py:48] [29000] global_step=29000, grad_norm=2.9506444931030273, loss=1.2491214275360107
I0211 20:21:20.332116 140285423572736 logging_writer.py:48] [29100] global_step=29100, grad_norm=3.606724262237549, loss=1.2529385089874268
I0211 20:22:38.463767 140285415180032 logging_writer.py:48] [29200] global_step=29200, grad_norm=2.285816192626953, loss=1.3012503385543823
I0211 20:24:04.848276 140285423572736 logging_writer.py:48] [29300] global_step=29300, grad_norm=2.4448938369750977, loss=1.2717534303665161
I0211 20:24:17.055239 140441227016000 spec.py:321] Evaluating on the training split.
I0211 20:25:11.841677 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 20:26:03.750858 140441227016000 spec.py:349] Evaluating on the test split.
I0211 20:26:30.691200 140441227016000 submission_runner.py:408] Time since start: 26934.60s, 	Step: 29314, 	{'train/ctc_loss': Array(0.28522658, dtype=float32), 'train/wer': 0.09909472289688673, 'validation/ctc_loss': Array(0.48595044, dtype=float32), 'validation/wer': 0.14087104279907703, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26795527, dtype=float32), 'test/wer': 0.08597891658034246, 'test/num_examples': 2472, 'score': 24501.43314099312, 'total_duration': 26934.59812092781, 'accumulated_submission_time': 24501.43314099312, 'accumulated_eval_time': 2430.969462156296, 'accumulated_logging_time': 0.8745872974395752}
I0211 20:26:30.730377 140285423572736 logging_writer.py:48] [29314] accumulated_eval_time=2430.969462, accumulated_logging_time=0.874587, accumulated_submission_time=24501.433141, global_step=29314, preemption_count=0, score=24501.433141, test/ctc_loss=0.2679552733898163, test/num_examples=2472, test/wer=0.085979, total_duration=26934.598121, train/ctc_loss=0.28522658348083496, train/wer=0.099095, validation/ctc_loss=0.48595044016838074, validation/num_examples=5348, validation/wer=0.140871
I0211 20:27:37.027906 140285415180032 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.9649708271026611, loss=1.2649112939834595
I0211 20:28:52.032510 140285423572736 logging_writer.py:48] [29500] global_step=29500, grad_norm=2.7188949584960938, loss=1.270595669746399
I0211 20:30:09.745232 140285415180032 logging_writer.py:48] [29600] global_step=29600, grad_norm=2.1823840141296387, loss=1.219114899635315
I0211 20:31:38.963840 140285423572736 logging_writer.py:48] [29700] global_step=29700, grad_norm=2.5936243534088135, loss=1.1747241020202637
I0211 20:33:02.829373 140285415180032 logging_writer.py:48] [29800] global_step=29800, grad_norm=2.984135627746582, loss=1.310742735862732
I0211 20:34:31.448915 140285423572736 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.6445640325546265, loss=1.2498054504394531
I0211 20:35:51.115278 140285415180032 logging_writer.py:48] [30000] global_step=30000, grad_norm=2.1201934814453125, loss=1.264466404914856
I0211 20:37:11.594293 140285423572736 logging_writer.py:48] [30100] global_step=30100, grad_norm=2.0871453285217285, loss=1.257758378982544
I0211 20:38:30.835149 140285415180032 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.9328454732894897, loss=1.23393714427948
I0211 20:39:52.004214 140285423572736 logging_writer.py:48] [30300] global_step=30300, grad_norm=2.2941322326660156, loss=1.2289329767227173
I0211 20:41:21.215487 140285415180032 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.9754772186279297, loss=1.250694751739502
I0211 20:42:50.832890 140285423572736 logging_writer.py:48] [30500] global_step=30500, grad_norm=2.458116292953491, loss=1.2259432077407837
I0211 20:44:19.414941 140285415180032 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.8551340103149414, loss=1.2094411849975586
I0211 20:45:47.321977 140285423572736 logging_writer.py:48] [30700] global_step=30700, grad_norm=5.823939323425293, loss=1.1955277919769287
I0211 20:47:15.682687 140285415180032 logging_writer.py:48] [30800] global_step=30800, grad_norm=3.760084867477417, loss=1.215126633644104
I0211 20:48:48.598154 140285423572736 logging_writer.py:48] [30900] global_step=30900, grad_norm=2.51802659034729, loss=1.2568641901016235
I0211 20:50:04.169250 140285415180032 logging_writer.py:48] [31000] global_step=31000, grad_norm=2.4908769130706787, loss=1.2471210956573486
I0211 20:50:31.355523 140441227016000 spec.py:321] Evaluating on the training split.
I0211 20:51:24.725939 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 20:52:18.683719 140441227016000 spec.py:349] Evaluating on the test split.
I0211 20:52:45.192827 140441227016000 submission_runner.py:408] Time since start: 28509.10s, 	Step: 31036, 	{'train/ctc_loss': Array(0.26872805, dtype=float32), 'train/wer': 0.09431319773841977, 'validation/ctc_loss': Array(0.47574943, dtype=float32), 'validation/wer': 0.1365940314934783, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26138502, dtype=float32), 'test/wer': 0.08388682387829302, 'test/num_examples': 2472, 'score': 25941.97016787529, 'total_duration': 28509.099543333054, 'accumulated_submission_time': 25941.97016787529, 'accumulated_eval_time': 2564.800893306732, 'accumulated_logging_time': 0.9301862716674805}
I0211 20:52:45.228814 140285423572736 logging_writer.py:48] [31036] accumulated_eval_time=2564.800893, accumulated_logging_time=0.930186, accumulated_submission_time=25941.970168, global_step=31036, preemption_count=0, score=25941.970168, test/ctc_loss=0.2613850235939026, test/num_examples=2472, test/wer=0.083887, total_duration=28509.099543, train/ctc_loss=0.2687280476093292, train/wer=0.094313, validation/ctc_loss=0.4757494330406189, validation/num_examples=5348, validation/wer=0.136594
I0211 20:53:33.961261 140285415180032 logging_writer.py:48] [31100] global_step=31100, grad_norm=2.2070722579956055, loss=1.2351796627044678
I0211 20:54:48.906619 140285423572736 logging_writer.py:48] [31200] global_step=31200, grad_norm=2.685154438018799, loss=1.2408744096755981
I0211 20:56:04.125537 140285415180032 logging_writer.py:48] [31300] global_step=31300, grad_norm=2.1820108890533447, loss=1.2058624029159546
I0211 20:57:23.036557 140285423572736 logging_writer.py:48] [31400] global_step=31400, grad_norm=2.2666125297546387, loss=1.1765793561935425
I0211 20:58:51.449416 140285415180032 logging_writer.py:48] [31500] global_step=31500, grad_norm=2.366893768310547, loss=1.3055893182754517
I0211 21:00:19.717647 140285423572736 logging_writer.py:48] [31600] global_step=31600, grad_norm=2.7634694576263428, loss=1.187451720237732
I0211 21:01:50.073523 140285415180032 logging_writer.py:48] [31700] global_step=31700, grad_norm=2.7751142978668213, loss=1.2874364852905273
I0211 21:03:20.714793 140285423572736 logging_writer.py:48] [31800] global_step=31800, grad_norm=3.027181386947632, loss=1.2832391262054443
I0211 21:04:52.743944 140285415180032 logging_writer.py:48] [31900] global_step=31900, grad_norm=2.6100380420684814, loss=1.2964228391647339
I0211 21:06:14.800659 140285423572736 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.954692006111145, loss=1.2305502891540527
I0211 21:07:30.374889 140285415180032 logging_writer.py:48] [32100] global_step=32100, grad_norm=2.5255274772644043, loss=1.2683665752410889
I0211 21:08:45.545726 140285423572736 logging_writer.py:48] [32200] global_step=32200, grad_norm=2.127364158630371, loss=1.2437877655029297
I0211 21:10:06.308655 140285415180032 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.7562940120697021, loss=1.2337520122528076
I0211 21:11:30.716712 140285423572736 logging_writer.py:48] [32400] global_step=32400, grad_norm=2.778714895248413, loss=1.3152357339859009
I0211 21:12:58.854337 140285415180032 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.853796124458313, loss=1.2625441551208496
I0211 21:14:26.152044 140285423572736 logging_writer.py:48] [32600] global_step=32600, grad_norm=3.3941383361816406, loss=1.2362934350967407
I0211 21:15:54.100558 140285415180032 logging_writer.py:48] [32700] global_step=32700, grad_norm=3.9201958179473877, loss=1.2223373651504517
I0211 21:16:45.438028 140441227016000 spec.py:321] Evaluating on the training split.
I0211 21:17:39.951519 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 21:18:32.628920 140441227016000 spec.py:349] Evaluating on the test split.
I0211 21:18:59.514878 140441227016000 submission_runner.py:408] Time since start: 30083.42s, 	Step: 32757, 	{'train/ctc_loss': Array(0.2560898, dtype=float32), 'train/wer': 0.08913130823889437, 'validation/ctc_loss': Array(0.46562022, dtype=float32), 'validation/wer': 0.13359143439180513, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24835972, dtype=float32), 'test/wer': 0.08144943432250726, 'test/num_examples': 2472, 'score': 27382.090871334076, 'total_duration': 30083.422049045563, 'accumulated_submission_time': 27382.090871334076, 'accumulated_eval_time': 2698.8723435401917, 'accumulated_logging_time': 0.9823896884918213}
I0211 21:18:59.551223 140285423572736 logging_writer.py:48] [32757] accumulated_eval_time=2698.872344, accumulated_logging_time=0.982390, accumulated_submission_time=27382.090871, global_step=32757, preemption_count=0, score=27382.090871, test/ctc_loss=0.24835972487926483, test/num_examples=2472, test/wer=0.081449, total_duration=30083.422049, train/ctc_loss=0.25608980655670166, train/wer=0.089131, validation/ctc_loss=0.465620219707489, validation/num_examples=5348, validation/wer=0.133591
I0211 21:19:32.497475 140285415180032 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.7622873783111572, loss=1.1960759162902832
I0211 21:20:47.413859 140285423572736 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.9302188158035278, loss=1.219928503036499
I0211 21:22:06.376558 140285423572736 logging_writer.py:48] [33000] global_step=33000, grad_norm=2.3176562786102295, loss=1.1769675016403198
I0211 21:23:21.685916 140285415180032 logging_writer.py:48] [33100] global_step=33100, grad_norm=2.129620313644409, loss=1.2206368446350098
I0211 21:24:39.746220 140285423572736 logging_writer.py:48] [33200] global_step=33200, grad_norm=6.4333391189575195, loss=1.1950263977050781
I0211 21:25:58.322353 140285415180032 logging_writer.py:48] [33300] global_step=33300, grad_norm=3.204118490219116, loss=1.1957013607025146
I0211 21:27:18.772367 140285423572736 logging_writer.py:48] [33400] global_step=33400, grad_norm=3.034337282180786, loss=1.2489513158798218
I0211 21:28:46.397411 140285415180032 logging_writer.py:48] [33500] global_step=33500, grad_norm=2.9898226261138916, loss=1.2279528379440308
I0211 21:30:14.847298 140285423572736 logging_writer.py:48] [33600] global_step=33600, grad_norm=2.726759672164917, loss=1.1796786785125732
I0211 21:31:42.716833 140285415180032 logging_writer.py:48] [33700] global_step=33700, grad_norm=3.904665946960449, loss=1.2175315618515015
I0211 21:33:10.563671 140285423572736 logging_writer.py:48] [33800] global_step=33800, grad_norm=2.7354373931884766, loss=1.202988624572754
I0211 21:34:39.678124 140285415180032 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.6800819635391235, loss=1.2069846391677856
I0211 21:36:06.539172 140285423572736 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.7035795450210571, loss=1.1999657154083252
I0211 21:37:22.567929 140285415180032 logging_writer.py:48] [34100] global_step=34100, grad_norm=2.252087354660034, loss=1.2222174406051636
I0211 21:38:38.600391 140285423572736 logging_writer.py:48] [34200] global_step=34200, grad_norm=2.300321578979492, loss=1.189082384109497
I0211 21:39:56.560001 140285415180032 logging_writer.py:48] [34300] global_step=34300, grad_norm=4.274571418762207, loss=1.2323695421218872
I0211 21:41:16.700078 140285423572736 logging_writer.py:48] [34400] global_step=34400, grad_norm=2.743609666824341, loss=1.220143437385559
I0211 21:42:44.958849 140285415180032 logging_writer.py:48] [34500] global_step=34500, grad_norm=2.0959665775299072, loss=1.228595495223999
I0211 21:43:00.252032 140441227016000 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0211 21:44:19.436689 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 21:45:11.615624 140441227016000 spec.py:349] Evaluating on the test split.
I0211 21:45:38.533920 140441227016000 submission_runner.py:408] Time since start: 31682.44s, 	Step: 34518, 	{'train/ctc_loss': Array(0.15375972, dtype=float32), 'train/wer': 0.05475259309641217, 'validation/ctc_loss': Array(0.45650312, dtype=float32), 'validation/wer': 0.13060814659625206, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2446342, dtype=float32), 'test/wer': 0.0796010805760364, 'test/num_examples': 2472, 'score': 28822.697194814682, 'total_duration': 31682.440099477768, 'accumulated_submission_time': 28822.697194814682, 'accumulated_eval_time': 2857.1478266716003, 'accumulated_logging_time': 1.0384981632232666}
I0211 21:45:38.575434 140284876867328 logging_writer.py:48] [34518] accumulated_eval_time=2857.147827, accumulated_logging_time=1.038498, accumulated_submission_time=28822.697195, global_step=34518, preemption_count=0, score=28822.697195, test/ctc_loss=0.24463419616222382, test/num_examples=2472, test/wer=0.079601, total_duration=31682.440099, train/ctc_loss=0.15375971794128418, train/wer=0.054753, validation/ctc_loss=0.45650312304496765, validation/num_examples=5348, validation/wer=0.130608
I0211 21:46:40.579492 140284868474624 logging_writer.py:48] [34600] global_step=34600, grad_norm=2.0523877143859863, loss=1.2312291860580444
I0211 21:47:55.420698 140284876867328 logging_writer.py:48] [34700] global_step=34700, grad_norm=2.2496676445007324, loss=1.262498378753662
I0211 21:49:19.205919 140284868474624 logging_writer.py:48] [34800] global_step=34800, grad_norm=3.0492186546325684, loss=1.196000576019287
I0211 21:50:47.320668 140284876867328 logging_writer.py:48] [34900] global_step=34900, grad_norm=2.733008861541748, loss=1.2616429328918457
I0211 21:52:18.037575 140284868474624 logging_writer.py:48] [35000] global_step=35000, grad_norm=2.2866532802581787, loss=1.1966909170150757
I0211 21:53:39.747895 140284880852736 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.9520347118377686, loss=1.2117509841918945
I0211 21:54:56.168081 140284872460032 logging_writer.py:48] [35200] global_step=35200, grad_norm=2.139971971511841, loss=1.2054648399353027
I0211 21:56:15.367381 140284880852736 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.7852027416229248, loss=1.2067750692367554
I0211 21:57:36.113801 140284872460032 logging_writer.py:48] [35400] global_step=35400, grad_norm=3.7459423542022705, loss=1.26033353805542
I0211 21:59:02.461322 140284880852736 logging_writer.py:48] [35500] global_step=35500, grad_norm=3.4377553462982178, loss=1.208176612854004
I0211 22:00:26.245258 140284872460032 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.9000264406204224, loss=1.1918892860412598
I0211 22:01:53.916022 140284880852736 logging_writer.py:48] [35700] global_step=35700, grad_norm=3.862821102142334, loss=1.171886920928955
I0211 22:03:22.355376 140284872460032 logging_writer.py:48] [35800] global_step=35800, grad_norm=2.387676477432251, loss=1.1615813970565796
I0211 22:04:53.565770 140284880852736 logging_writer.py:48] [35900] global_step=35900, grad_norm=3.288426637649536, loss=1.1831575632095337
I0211 22:06:20.706784 140284872460032 logging_writer.py:48] [36000] global_step=36000, grad_norm=2.766355037689209, loss=1.154016137123108
I0211 22:07:47.667683 140285208532736 logging_writer.py:48] [36100] global_step=36100, grad_norm=3.6370105743408203, loss=1.2058110237121582
I0211 22:09:03.270997 140285200140032 logging_writer.py:48] [36200] global_step=36200, grad_norm=2.2817726135253906, loss=1.1475383043289185
I0211 22:09:39.469659 140441227016000 spec.py:321] Evaluating on the training split.
I0211 22:10:34.740902 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 22:11:27.551575 140441227016000 spec.py:349] Evaluating on the test split.
I0211 22:11:54.338925 140441227016000 submission_runner.py:408] Time since start: 33258.25s, 	Step: 36248, 	{'train/ctc_loss': Array(0.14782521, dtype=float32), 'train/wer': 0.05197591600733533, 'validation/ctc_loss': Array(0.44794834, dtype=float32), 'validation/wer': 0.12872548924954383, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23961341, dtype=float32), 'test/wer': 0.07759023419251315, 'test/num_examples': 2472, 'score': 30263.501261234283, 'total_duration': 33258.248254299164, 'accumulated_submission_time': 30263.501261234283, 'accumulated_eval_time': 2992.0138483047485, 'accumulated_logging_time': 1.0968949794769287}
I0211 22:11:54.370871 140285208532736 logging_writer.py:48] [36248] accumulated_eval_time=2992.013848, accumulated_logging_time=1.096895, accumulated_submission_time=30263.501261, global_step=36248, preemption_count=0, score=30263.501261, test/ctc_loss=0.23961341381072998, test/num_examples=2472, test/wer=0.077590, total_duration=33258.248254, train/ctc_loss=0.1478252112865448, train/wer=0.051976, validation/ctc_loss=0.4479483366012573, validation/num_examples=5348, validation/wer=0.128725
I0211 22:12:34.512277 140285200140032 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.693608045578003, loss=1.1627016067504883
I0211 22:13:49.461220 140285208532736 logging_writer.py:48] [36400] global_step=36400, grad_norm=3.0187437534332275, loss=1.1603667736053467
I0211 22:15:06.927942 140285200140032 logging_writer.py:48] [36500] global_step=36500, grad_norm=2.8245067596435547, loss=1.1670382022857666
I0211 22:16:34.093428 140285208532736 logging_writer.py:48] [36600] global_step=36600, grad_norm=3.6122941970825195, loss=1.2025479078292847
I0211 22:18:05.274539 140285200140032 logging_writer.py:48] [36700] global_step=36700, grad_norm=3.5649425983428955, loss=1.1617590188980103
I0211 22:19:33.682687 140285208532736 logging_writer.py:48] [36800] global_step=36800, grad_norm=2.3807950019836426, loss=1.1773251295089722
I0211 22:21:02.984125 140285200140032 logging_writer.py:48] [36900] global_step=36900, grad_norm=3.071664810180664, loss=1.2376652956008911
I0211 22:22:32.435966 140285208532736 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.7465282678604126, loss=1.1949424743652344
I0211 22:24:00.218691 140284880852736 logging_writer.py:48] [37100] global_step=37100, grad_norm=2.3171989917755127, loss=1.1529287099838257
I0211 22:25:18.007891 140284872460032 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.996957540512085, loss=1.2173360586166382
I0211 22:26:38.537701 140284880852736 logging_writer.py:48] [37300] global_step=37300, grad_norm=2.1190528869628906, loss=1.155397891998291
I0211 22:27:57.682437 140284872460032 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.9857524633407593, loss=1.171467900276184
I0211 22:29:20.618389 140284880852736 logging_writer.py:48] [37500] global_step=37500, grad_norm=2.07491135597229, loss=1.158069133758545
I0211 22:30:47.609434 140284872460032 logging_writer.py:48] [37600] global_step=37600, grad_norm=2.6056485176086426, loss=1.1980515718460083
I0211 22:32:19.610216 140284880852736 logging_writer.py:48] [37700] global_step=37700, grad_norm=3.005547523498535, loss=1.2254889011383057
I0211 22:33:50.368127 140284872460032 logging_writer.py:48] [37800] global_step=37800, grad_norm=4.074529647827148, loss=1.1963385343551636
I0211 22:35:21.156591 140284880852736 logging_writer.py:48] [37900] global_step=37900, grad_norm=2.8634748458862305, loss=1.174741268157959
I0211 22:35:54.436831 140441227016000 spec.py:321] Evaluating on the training split.
I0211 22:36:50.128104 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 22:37:44.194182 140441227016000 spec.py:349] Evaluating on the test split.
I0211 22:38:10.758109 140441227016000 submission_runner.py:408] Time since start: 34834.66s, 	Step: 37941, 	{'train/ctc_loss': Array(0.15493704, dtype=float32), 'train/wer': 0.05480588810486535, 'validation/ctc_loss': Array(0.44182682, dtype=float32), 'validation/wer': 0.12585805729071126, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23499914, dtype=float32), 'test/wer': 0.0761684236183048, 'test/num_examples': 2472, 'score': 31703.481546401978, 'total_duration': 34834.663165569305, 'accumulated_submission_time': 31703.481546401978, 'accumulated_eval_time': 3128.327612876892, 'accumulated_logging_time': 1.1431083679199219}
I0211 22:38:10.795458 140284880852736 logging_writer.py:48] [37941] accumulated_eval_time=3128.327613, accumulated_logging_time=1.143108, accumulated_submission_time=31703.481546, global_step=37941, preemption_count=0, score=31703.481546, test/ctc_loss=0.2349991351366043, test/num_examples=2472, test/wer=0.076168, total_duration=34834.663166, train/ctc_loss=0.1549370437860489, train/wer=0.054806, validation/ctc_loss=0.44182682037353516, validation/num_examples=5348, validation/wer=0.125858
I0211 22:38:56.221704 140284872460032 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.8767668008804321, loss=1.184170126914978
I0211 22:40:11.737686 140284880852736 logging_writer.py:48] [38100] global_step=38100, grad_norm=2.3001396656036377, loss=1.210616946220398
I0211 22:41:32.678919 140284880852736 logging_writer.py:48] [38200] global_step=38200, grad_norm=3.3523969650268555, loss=1.1741210222244263
I0211 22:42:50.942042 140284872460032 logging_writer.py:48] [38300] global_step=38300, grad_norm=2.9396820068359375, loss=1.1898988485336304
I0211 22:44:10.080516 140284880852736 logging_writer.py:48] [38400] global_step=38400, grad_norm=2.6899526119232178, loss=1.1131218671798706
I0211 22:45:37.310089 140284872460032 logging_writer.py:48] [38500] global_step=38500, grad_norm=3.57030987739563, loss=1.1890149116516113
I0211 22:47:03.599422 140284880852736 logging_writer.py:48] [38600] global_step=38600, grad_norm=2.042814016342163, loss=1.1340309381484985
I0211 22:48:29.041521 140284872460032 logging_writer.py:48] [38700] global_step=38700, grad_norm=2.072277307510376, loss=1.2038546800613403
I0211 22:50:01.451890 140284880852736 logging_writer.py:48] [38800] global_step=38800, grad_norm=2.667954206466675, loss=1.1251188516616821
I0211 22:51:33.073531 140284872460032 logging_writer.py:48] [38900] global_step=38900, grad_norm=4.496917724609375, loss=1.1588305234909058
I0211 22:53:04.453937 140284880852736 logging_writer.py:48] [39000] global_step=39000, grad_norm=3.7190775871276855, loss=1.1635466814041138
I0211 22:54:29.522187 140284872460032 logging_writer.py:48] [39100] global_step=39100, grad_norm=2.0629425048828125, loss=1.143917202949524
I0211 22:55:55.646437 140284880852736 logging_writer.py:48] [39200] global_step=39200, grad_norm=3.380281686782837, loss=1.1562596559524536
I0211 22:57:12.128214 140284872460032 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.7898023128509521, loss=1.1996313333511353
I0211 22:58:31.253573 140284880852736 logging_writer.py:48] [39400] global_step=39400, grad_norm=3.1570823192596436, loss=1.1667488813400269
I0211 22:59:53.073032 140284872460032 logging_writer.py:48] [39500] global_step=39500, grad_norm=3.279285430908203, loss=1.158939242362976
I0211 23:01:18.143282 140284880852736 logging_writer.py:48] [39600] global_step=39600, grad_norm=3.25382661819458, loss=1.1630696058273315
I0211 23:02:11.455297 140441227016000 spec.py:321] Evaluating on the training split.
I0211 23:03:07.696702 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 23:03:59.525938 140441227016000 spec.py:349] Evaluating on the test split.
I0211 23:04:26.953403 140441227016000 submission_runner.py:408] Time since start: 36410.86s, 	Step: 39661, 	{'train/ctc_loss': Array(0.13454847, dtype=float32), 'train/wer': 0.04841284748309542, 'validation/ctc_loss': Array(0.4356194, dtype=float32), 'validation/wer': 0.12465122565820597, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23229034, dtype=float32), 'test/wer': 0.07492941726078037, 'test/num_examples': 2472, 'score': 33144.05407190323, 'total_duration': 36410.860171079636, 'accumulated_submission_time': 33144.05407190323, 'accumulated_eval_time': 3263.8199138641357, 'accumulated_logging_time': 1.1951828002929688}
I0211 23:04:26.991882 140284880852736 logging_writer.py:48] [39661] accumulated_eval_time=3263.819914, accumulated_logging_time=1.195183, accumulated_submission_time=33144.054072, global_step=39661, preemption_count=0, score=33144.054072, test/ctc_loss=0.2322903424501419, test/num_examples=2472, test/wer=0.074929, total_duration=36410.860171, train/ctc_loss=0.13454847037792206, train/wer=0.048413, validation/ctc_loss=0.43561941385269165, validation/num_examples=5348, validation/wer=0.124651
I0211 23:04:57.476149 140284872460032 logging_writer.py:48] [39700] global_step=39700, grad_norm=4.527709007263184, loss=1.1520054340362549
I0211 23:06:12.757603 140284880852736 logging_writer.py:48] [39800] global_step=39800, grad_norm=2.0774755477905273, loss=1.1917791366577148
I0211 23:07:29.434763 140284872460032 logging_writer.py:48] [39900] global_step=39900, grad_norm=2.3269801139831543, loss=1.1841927766799927
I0211 23:08:57.990190 140284880852736 logging_writer.py:48] [40000] global_step=40000, grad_norm=2.8259670734405518, loss=1.164848804473877
I0211 23:10:24.922844 140284872460032 logging_writer.py:48] [40100] global_step=40100, grad_norm=2.3899896144866943, loss=1.19438636302948
I0211 23:11:53.582264 140285208532736 logging_writer.py:48] [40200] global_step=40200, grad_norm=4.505771160125732, loss=1.1757988929748535
I0211 23:13:09.291607 140285200140032 logging_writer.py:48] [40300] global_step=40300, grad_norm=3.123366117477417, loss=1.1780760288238525
I0211 23:14:26.073526 140285208532736 logging_writer.py:48] [40400] global_step=40400, grad_norm=2.0851354598999023, loss=1.1044549942016602
I0211 23:15:48.641847 140285200140032 logging_writer.py:48] [40500] global_step=40500, grad_norm=2.4554502964019775, loss=1.1581758260726929
I0211 23:17:11.521891 140285208532736 logging_writer.py:48] [40600] global_step=40600, grad_norm=2.9737346172332764, loss=1.1410431861877441
I0211 23:18:36.410797 140285200140032 logging_writer.py:48] [40700] global_step=40700, grad_norm=2.1023240089416504, loss=1.1650168895721436
I0211 23:20:07.594617 140285208532736 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.9767920970916748, loss=1.126035213470459
I0211 23:21:36.402236 140285200140032 logging_writer.py:48] [40900] global_step=40900, grad_norm=2.798820734024048, loss=1.173368215560913
I0211 23:23:05.586584 140285208532736 logging_writer.py:48] [41000] global_step=41000, grad_norm=2.6893224716186523, loss=1.1545209884643555
I0211 23:24:34.427853 140285200140032 logging_writer.py:48] [41100] global_step=41100, grad_norm=3.450439929962158, loss=1.1792402267456055
I0211 23:26:08.028970 140284880852736 logging_writer.py:48] [41200] global_step=41200, grad_norm=2.9880051612854004, loss=1.1432894468307495
I0211 23:27:27.075334 140284872460032 logging_writer.py:48] [41300] global_step=41300, grad_norm=2.8017704486846924, loss=1.1457691192626953
I0211 23:28:27.879638 140441227016000 spec.py:321] Evaluating on the training split.
I0211 23:29:23.278522 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 23:30:15.670387 140441227016000 spec.py:349] Evaluating on the test split.
I0211 23:30:42.286131 140441227016000 submission_runner.py:408] Time since start: 37986.19s, 	Step: 41377, 	{'train/ctc_loss': Array(0.14412792, dtype=float32), 'train/wer': 0.0515004984779097, 'validation/ctc_loss': Array(0.43128958, dtype=float32), 'validation/wer': 0.12311613582165924, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2291814, dtype=float32), 'test/wer': 0.07340604878841428, 'test/num_examples': 2472, 'score': 34584.85236620903, 'total_duration': 37986.192954301834, 'accumulated_submission_time': 34584.85236620903, 'accumulated_eval_time': 3398.2206456661224, 'accumulated_logging_time': 1.250486135482788}
I0211 23:30:42.330527 140284916692736 logging_writer.py:48] [41377] accumulated_eval_time=3398.220646, accumulated_logging_time=1.250486, accumulated_submission_time=34584.852366, global_step=41377, preemption_count=0, score=34584.852366, test/ctc_loss=0.22918139398097992, test/num_examples=2472, test/wer=0.073406, total_duration=37986.192954, train/ctc_loss=0.14412792026996613, train/wer=0.051500, validation/ctc_loss=0.43128958344459534, validation/num_examples=5348, validation/wer=0.123116
I0211 23:31:00.487971 140284908300032 logging_writer.py:48] [41400] global_step=41400, grad_norm=2.963160991668701, loss=1.1753623485565186
I0211 23:32:15.846743 140284916692736 logging_writer.py:48] [41500] global_step=41500, grad_norm=2.3701043128967285, loss=1.1420034170150757
I0211 23:33:30.984450 140284908300032 logging_writer.py:48] [41600] global_step=41600, grad_norm=2.1592164039611816, loss=1.1675076484680176
I0211 23:34:54.101093 140284916692736 logging_writer.py:48] [41700] global_step=41700, grad_norm=2.2625675201416016, loss=1.200116515159607
I0211 23:36:22.928775 140284908300032 logging_writer.py:48] [41800] global_step=41800, grad_norm=3.8886406421661377, loss=1.1161726713180542
I0211 23:37:55.219058 140284916692736 logging_writer.py:48] [41900] global_step=41900, grad_norm=3.215047597885132, loss=1.1425516605377197
I0211 23:39:23.817991 140284908300032 logging_writer.py:48] [42000] global_step=42000, grad_norm=2.0829155445098877, loss=1.1704243421554565
I0211 23:40:53.287483 140284916692736 logging_writer.py:48] [42100] global_step=42100, grad_norm=3.9280903339385986, loss=1.102457880973816
I0211 23:42:22.608763 140284908300032 logging_writer.py:48] [42200] global_step=42200, grad_norm=2.087333917617798, loss=1.1702353954315186
I0211 23:43:44.475526 140284916692736 logging_writer.py:48] [42300] global_step=42300, grad_norm=4.377134323120117, loss=1.1738040447235107
I0211 23:44:59.841825 140284908300032 logging_writer.py:48] [42400] global_step=42400, grad_norm=5.638169288635254, loss=1.0948641300201416
I0211 23:46:21.314205 140284916692736 logging_writer.py:48] [42500] global_step=42500, grad_norm=2.484127998352051, loss=1.0945407152175903
I0211 23:47:41.804166 140284908300032 logging_writer.py:48] [42600] global_step=42600, grad_norm=3.341632127761841, loss=1.106765627861023
I0211 23:49:08.359586 140284916692736 logging_writer.py:48] [42700] global_step=42700, grad_norm=4.06350564956665, loss=1.124004602432251
I0211 23:50:37.726903 140284908300032 logging_writer.py:48] [42800] global_step=42800, grad_norm=2.437440872192383, loss=1.1432427167892456
I0211 23:52:09.176161 140284916692736 logging_writer.py:48] [42900] global_step=42900, grad_norm=2.4050278663635254, loss=1.1567479372024536
I0211 23:53:39.274410 140284908300032 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.771677851676941, loss=1.1545099020004272
I0211 23:54:43.351800 140441227016000 spec.py:321] Evaluating on the training split.
I0211 23:55:38.897417 140441227016000 spec.py:333] Evaluating on the validation split.
I0211 23:56:32.413573 140441227016000 spec.py:349] Evaluating on the test split.
I0211 23:56:58.709873 140441227016000 submission_runner.py:408] Time since start: 39562.62s, 	Step: 43071, 	{'train/ctc_loss': Array(0.1331379, dtype=float32), 'train/wer': 0.04820648166946264, 'validation/ctc_loss': Array(0.42728096, dtype=float32), 'validation/wer': 0.12219894378095524, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22690861, dtype=float32), 'test/wer': 0.07334511404951963, 'test/num_examples': 2472, 'score': 36025.78731369972, 'total_duration': 39562.61579680443, 'accumulated_submission_time': 36025.78731369972, 'accumulated_eval_time': 3533.5720529556274, 'accumulated_logging_time': 1.3105106353759766}
I0211 23:56:58.746930 140284916692736 logging_writer.py:48] [43071] accumulated_eval_time=3533.572053, accumulated_logging_time=1.310511, accumulated_submission_time=36025.787314, global_step=43071, preemption_count=0, score=36025.787314, test/ctc_loss=0.2269086092710495, test/num_examples=2472, test/wer=0.073345, total_duration=39562.615797, train/ctc_loss=0.13313789665699005, train/wer=0.048206, validation/ctc_loss=0.4272809624671936, validation/num_examples=5348, validation/wer=0.122199
I0211 23:57:21.665792 140284908300032 logging_writer.py:48] [43100] global_step=43100, grad_norm=2.9308860301971436, loss=1.1247602701187134
I0211 23:58:37.062778 140284916692736 logging_writer.py:48] [43200] global_step=43200, grad_norm=2.5510141849517822, loss=1.1603621244430542
I0211 23:59:56.176291 140284916692736 logging_writer.py:48] [43300] global_step=43300, grad_norm=2.534363031387329, loss=1.0901938676834106
I0212 00:01:11.861137 140284908300032 logging_writer.py:48] [43400] global_step=43400, grad_norm=3.502023935317993, loss=1.1496014595031738
I0212 00:02:32.447743 140284916692736 logging_writer.py:48] [43500] global_step=43500, grad_norm=2.1119582653045654, loss=1.154969334602356
I0212 00:03:55.969273 140284908300032 logging_writer.py:48] [43600] global_step=43600, grad_norm=4.856941223144531, loss=1.1436502933502197
I0212 00:05:19.724851 140284916692736 logging_writer.py:48] [43700] global_step=43700, grad_norm=4.317368507385254, loss=1.1227853298187256
I0212 00:06:47.261669 140284908300032 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.9166491031646729, loss=1.0756474733352661
I0212 00:08:12.462577 140284916692736 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.9922235012054443, loss=1.1089049577713013
I0212 00:09:42.683993 140284908300032 logging_writer.py:48] [44000] global_step=44000, grad_norm=2.206157684326172, loss=1.1492360830307007
I0212 00:11:09.621220 140284916692736 logging_writer.py:48] [44100] global_step=44100, grad_norm=3.0110442638397217, loss=1.0979715585708618
I0212 00:12:37.023704 140284908300032 logging_writer.py:48] [44200] global_step=44200, grad_norm=2.420417547225952, loss=1.1217385530471802
I0212 00:14:08.216459 140284916692736 logging_writer.py:48] [44300] global_step=44300, grad_norm=2.3947482109069824, loss=1.1065088510513306
I0212 00:15:27.916172 140284908300032 logging_writer.py:48] [44400] global_step=44400, grad_norm=2.854025363922119, loss=1.1173837184906006
I0212 00:16:47.469173 140284916692736 logging_writer.py:48] [44500] global_step=44500, grad_norm=3.4610064029693604, loss=1.1566064357757568
I0212 00:18:05.880095 140284908300032 logging_writer.py:48] [44600] global_step=44600, grad_norm=2.3699333667755127, loss=1.08342707157135
I0212 00:19:28.909115 140284916692736 logging_writer.py:48] [44700] global_step=44700, grad_norm=2.732924461364746, loss=1.1396023035049438
I0212 00:20:56.868018 140284908300032 logging_writer.py:48] [44800] global_step=44800, grad_norm=2.493804931640625, loss=1.1847399473190308
I0212 00:20:59.500991 140441227016000 spec.py:321] Evaluating on the training split.
I0212 00:21:55.550520 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 00:22:47.782135 140441227016000 spec.py:349] Evaluating on the test split.
I0212 00:23:15.314186 140441227016000 submission_runner.py:408] Time since start: 41139.22s, 	Step: 44804, 	{'train/ctc_loss': Array(0.15674452, dtype=float32), 'train/wer': 0.052520602626834924, 'validation/ctc_loss': Array(0.42669344, dtype=float32), 'validation/wer': 0.12226652635237553, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22530259, dtype=float32), 'test/wer': 0.0731216866735726, 'test/num_examples': 2472, 'score': 37466.45243930817, 'total_duration': 41139.22075676918, 'accumulated_submission_time': 37466.45243930817, 'accumulated_eval_time': 3669.3792510032654, 'accumulated_logging_time': 1.3633804321289062}
I0212 00:23:15.352698 140284993492736 logging_writer.py:48] [44804] accumulated_eval_time=3669.379251, accumulated_logging_time=1.363380, accumulated_submission_time=37466.452439, global_step=44804, preemption_count=0, score=37466.452439, test/ctc_loss=0.225302591919899, test/num_examples=2472, test/wer=0.073122, total_duration=41139.220757, train/ctc_loss=0.15674452483654022, train/wer=0.052521, validation/ctc_loss=0.4266934394836426, validation/num_examples=5348, validation/wer=0.122267
I0212 00:24:28.279235 140284985100032 logging_writer.py:48] [44900] global_step=44900, grad_norm=3.372392177581787, loss=1.138980507850647
I0212 00:25:43.645308 140284993492736 logging_writer.py:48] [45000] global_step=45000, grad_norm=4.5910797119140625, loss=1.1245973110198975
I0212 00:27:09.936236 140284985100032 logging_writer.py:48] [45100] global_step=45100, grad_norm=3.3293628692626953, loss=1.115853190422058
I0212 00:28:39.782586 140284993492736 logging_writer.py:48] [45200] global_step=45200, grad_norm=3.078691005706787, loss=1.1355037689208984
I0212 00:30:10.885847 140284985100032 logging_writer.py:48] [45300] global_step=45300, grad_norm=2.8113584518432617, loss=1.0636709928512573
I0212 00:31:34.477108 140284993492736 logging_writer.py:48] [45400] global_step=45400, grad_norm=4.455389499664307, loss=1.0769902467727661
I0212 00:32:52.885903 140284985100032 logging_writer.py:48] [45500] global_step=45500, grad_norm=3.424177408218384, loss=1.1282323598861694
I0212 00:34:12.311169 140284993492736 logging_writer.py:48] [45600] global_step=45600, grad_norm=2.9931836128234863, loss=1.1175148487091064
I0212 00:35:32.932258 140284985100032 logging_writer.py:48] [45700] global_step=45700, grad_norm=2.998323440551758, loss=1.1322070360183716
I0212 00:36:59.641031 140284993492736 logging_writer.py:48] [45800] global_step=45800, grad_norm=2.26248836517334, loss=1.1298080682754517
I0212 00:38:27.357984 140284985100032 logging_writer.py:48] [45900] global_step=45900, grad_norm=2.8804047107696533, loss=1.0454237461090088
I0212 00:39:58.955084 140284993492736 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.773603916168213, loss=1.1034064292907715
I0212 00:41:29.377803 140284985100032 logging_writer.py:48] [46100] global_step=46100, grad_norm=2.7680091857910156, loss=1.147916316986084
I0212 00:42:59.483389 140284993492736 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.840769648551941, loss=1.1723324060440063
I0212 00:44:29.320772 140284985100032 logging_writer.py:48] [46300] global_step=46300, grad_norm=3.703181028366089, loss=1.1314384937286377
I0212 00:45:56.498584 140284993492736 logging_writer.py:48] [46400] global_step=46400, grad_norm=2.5387675762176514, loss=1.0808684825897217
I0212 00:47:14.284151 140284985100032 logging_writer.py:48] [46500] global_step=46500, grad_norm=3.1131510734558105, loss=1.1016002893447876
I0212 00:47:15.658928 140441227016000 spec.py:321] Evaluating on the training split.
I0212 00:48:11.927564 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 00:49:05.945229 140441227016000 spec.py:349] Evaluating on the test split.
I0212 00:49:33.724326 140441227016000 submission_runner.py:408] Time since start: 42717.63s, 	Step: 46503, 	{'train/ctc_loss': Array(0.14168745, dtype=float32), 'train/wer': 0.050901187446988974, 'validation/ctc_loss': Array(0.42561495, dtype=float32), 'validation/wer': 0.1211658959035307, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22481138, dtype=float32), 'test/wer': 0.07253265086425771, 'test/num_examples': 2472, 'score': 38906.67197370529, 'total_duration': 42717.630824804306, 'accumulated_submission_time': 38906.67197370529, 'accumulated_eval_time': 3807.4385566711426, 'accumulated_logging_time': 1.4167790412902832}
I0212 00:49:33.761757 140284993492736 logging_writer.py:48] [46503] accumulated_eval_time=3807.438557, accumulated_logging_time=1.416779, accumulated_submission_time=38906.671974, global_step=46503, preemption_count=0, score=38906.671974, test/ctc_loss=0.2248113751411438, test/num_examples=2472, test/wer=0.072533, total_duration=42717.630825, train/ctc_loss=0.14168745279312134, train/wer=0.050901, validation/ctc_loss=0.42561495304107666, validation/num_examples=5348, validation/wer=0.121166
I0212 00:50:47.303269 140284985100032 logging_writer.py:48] [46600] global_step=46600, grad_norm=2.611722469329834, loss=1.1691921949386597
I0212 00:52:02.461996 140284993492736 logging_writer.py:48] [46700] global_step=46700, grad_norm=3.6088144779205322, loss=1.0989245176315308
I0212 00:53:20.249082 140284985100032 logging_writer.py:48] [46800] global_step=46800, grad_norm=2.832230567932129, loss=1.1371219158172607
I0212 00:54:50.234946 140284993492736 logging_writer.py:48] [46900] global_step=46900, grad_norm=2.5285964012145996, loss=1.1321724653244019
I0212 00:56:20.828265 140284985100032 logging_writer.py:48] [47000] global_step=47000, grad_norm=3.2924997806549072, loss=1.1064352989196777
I0212 00:57:48.826955 140284993492736 logging_writer.py:48] [47100] global_step=47100, grad_norm=3.291583299636841, loss=1.1182278394699097
I0212 00:59:19.425198 140284985100032 logging_writer.py:48] [47200] global_step=47200, grad_norm=2.0756170749664307, loss=1.1661794185638428
I0212 01:00:47.013474 140284993492736 logging_writer.py:48] [47300] global_step=47300, grad_norm=3.970261335372925, loss=1.1717370748519897
I0212 01:02:16.060437 140284993492736 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.9652099609375, loss=1.129686713218689
I0212 01:03:32.548806 140284985100032 logging_writer.py:48] [47500] global_step=47500, grad_norm=2.8531975746154785, loss=1.1400110721588135
I0212 01:04:53.077454 140284993492736 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.8557409048080444, loss=1.1160165071487427
I0212 01:06:12.141681 140284985100032 logging_writer.py:48] [47700] global_step=47700, grad_norm=2.98285174369812, loss=1.1388499736785889
I0212 01:07:38.175418 140284993492736 logging_writer.py:48] [47800] global_step=47800, grad_norm=3.9596126079559326, loss=1.135943055152893
I0212 01:09:10.660839 140284985100032 logging_writer.py:48] [47900] global_step=47900, grad_norm=2.5150294303894043, loss=1.1483118534088135
I0212 01:10:41.389287 140441227016000 spec.py:321] Evaluating on the training split.
I0212 01:11:37.842000 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 01:12:30.173740 140441227016000 spec.py:349] Evaluating on the test split.
I0212 01:12:56.420556 140441227016000 submission_runner.py:408] Time since start: 44120.33s, 	Step: 48000, 	{'train/ctc_loss': Array(0.12119035, dtype=float32), 'train/wer': 0.04460789943515802, 'validation/ctc_loss': Array(0.42536923, dtype=float32), 'validation/wer': 0.12134933431167151, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22480996, dtype=float32), 'test/wer': 0.07304044035504641, 'test/num_examples': 2472, 'score': 40174.220403671265, 'total_duration': 44120.32517743111, 'accumulated_submission_time': 40174.220403671265, 'accumulated_eval_time': 3942.461863040924, 'accumulated_logging_time': 1.4705607891082764}
I0212 01:12:56.464444 140285423572736 logging_writer.py:48] [48000] accumulated_eval_time=3942.461863, accumulated_logging_time=1.470561, accumulated_submission_time=40174.220404, global_step=48000, preemption_count=0, score=40174.220404, test/ctc_loss=0.22480995953083038, test/num_examples=2472, test/wer=0.073040, total_duration=44120.325177, train/ctc_loss=0.12119035422801971, train/wer=0.044608, validation/ctc_loss=0.4253692328929901, validation/num_examples=5348, validation/wer=0.121349
I0212 01:12:56.491422 140285415180032 logging_writer.py:48] [48000] global_step=48000, preemption_count=0, score=40174.220404
I0212 01:12:56.691579 140441227016000 checkpoints.py:490] Saving checkpoint at step: 48000
I0212 01:12:57.697630 140441227016000 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_2/librispeech_deepspeech_jax/trial_2/checkpoint_48000
I0212 01:12:57.716798 140441227016000 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_2/librispeech_deepspeech_jax/trial_2/checkpoint_48000.
I0212 01:12:58.773475 140441227016000 submission_runner.py:583] Tuning trial 2/5
I0212 01:12:58.773736 140441227016000 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.2, learning_rate=0.0008445074561975979, one_minus_beta1=0.11042418465, beta2=0.9978504782314613, weight_decay=0.08135402759553023, warmup_factor=0.05)
I0212 01:12:58.786231 140441227016000 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.744686, dtype=float32), 'train/wer': 3.0868096476888236, 'validation/ctc_loss': Array(30.570248, dtype=float32), 'validation/wer': 2.9115054500516524, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.657555, dtype=float32), 'test/wer': 3.2318363699144883, 'test/num_examples': 2472, 'score': 15.712034225463867, 'total_duration': 195.08607053756714, 'accumulated_submission_time': 15.712034225463867, 'accumulated_eval_time': 179.37396621704102, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1755, {'train/ctc_loss': Array(6.613655, dtype=float32), 'train/wer': 0.9419290070434633, 'validation/ctc_loss': Array(6.4508014, dtype=float32), 'validation/wer': 0.8961545516861852, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.4480104, dtype=float32), 'test/wer': 0.8991123839701014, 'test/num_examples': 2472, 'score': 1456.2849533557892, 'total_duration': 1745.5630271434784, 'accumulated_submission_time': 1456.2849533557892, 'accumulated_eval_time': 289.1729745864868, 'accumulated_logging_time': 0.031392574310302734, 'global_step': 1755, 'preemption_count': 0}), (3499, {'train/ctc_loss': Array(3.93307, dtype=float32), 'train/wer': 0.757276603130865, 'validation/ctc_loss': Array(3.7532241, dtype=float32), 'validation/wer': 0.7071840273419775, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.2973654, dtype=float32), 'test/wer': 0.6442629943330693, 'test/num_examples': 2472, 'score': 2896.8062427043915, 'total_duration': 3309.4783482551575, 'accumulated_submission_time': 2896.8062427043915, 'accumulated_eval_time': 412.4362471103668, 'accumulated_logging_time': 0.08355402946472168, 'global_step': 3499, 'preemption_count': 0}), (5216, {'train/ctc_loss': Array(0.9103338, dtype=float32), 'train/wer': 0.2712767425810904, 'validation/ctc_loss': Array(0.9936676, dtype=float32), 'validation/wer': 0.2741631829460208, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6439003, dtype=float32), 'test/wer': 0.2018564783783235, 'test/num_examples': 2472, 'score': 4337.244078874588, 'total_duration': 4884.011987924576, 'accumulated_submission_time': 4337.244078874588, 'accumulated_eval_time': 546.3971445560455, 'accumulated_logging_time': 0.14002203941345215, 'global_step': 5216, 'preemption_count': 0}), (6949, {'train/ctc_loss': Array(0.6986057, dtype=float32), 'train/wer': 0.22088870574499503, 'validation/ctc_loss': Array(0.8042088, dtype=float32), 'validation/wer': 0.23036967666566902, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.50204223, dtype=float32), 'test/wer': 0.16135518859301687, 'test/num_examples': 2472, 'score': 5777.536859035492, 'total_duration': 6455.857183456421, 'accumulated_submission_time': 5777.536859035492, 'accumulated_eval_time': 677.8204755783081, 'accumulated_logging_time': 0.19183111190795898, 'global_step': 6949, 'preemption_count': 0}), (8679, {'train/ctc_loss': Array(0.59292424, dtype=float32), 'train/wer': 0.19187749812130386, 'validation/ctc_loss': Array(0.7251276, dtype=float32), 'validation/wer': 0.21049074601504195, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4397295, dtype=float32), 'test/wer': 0.14384660695062254, 'test/num_examples': 2472, 'score': 7217.844273805618, 'total_duration': 8033.686626672745, 'accumulated_submission_time': 7217.844273805618, 'accumulated_eval_time': 815.2142312526703, 'accumulated_logging_time': 0.24258112907409668, 'global_step': 8679, 'preemption_count': 0}), (10387, {'train/ctc_loss': Array(0.77348566, dtype=float32), 'train/wer': 0.24253480395453628, 'validation/ctc_loss': Array(0.9241919, dtype=float32), 'validation/wer': 0.25763441690722844, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.55918086, dtype=float32), 'test/wer': 0.17896532813356894, 'test/num_examples': 2472, 'score': 8658.25753068924, 'total_duration': 9608.27274942398, 'accumulated_submission_time': 8658.25753068924, 'accumulated_eval_time': 949.2553472518921, 'accumulated_logging_time': 0.29802942276000977, 'global_step': 10387, 'preemption_count': 0}), (12106, {'train/ctc_loss': Array(0.49431863, dtype=float32), 'train/wer': 0.16404431564054583, 'validation/ctc_loss': Array(0.6491058, dtype=float32), 'validation/wer': 0.18740647054848084, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37781128, dtype=float32), 'test/wer': 0.12325066520423293, 'test/num_examples': 2472, 'score': 10098.338337421417, 'total_duration': 11182.101341962814, 'accumulated_submission_time': 10098.338337421417, 'accumulated_eval_time': 1082.87002658844, 'accumulated_logging_time': 0.3533177375793457, 'global_step': 12106, 'preemption_count': 0}), (13829, {'train/ctc_loss': Array(0.45966643, dtype=float32), 'train/wer': 0.1526637057536937, 'validation/ctc_loss': Array(0.6343328, dtype=float32), 'validation/wer': 0.18310049528370198, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3647178, dtype=float32), 'test/wer': 0.11951333455202812, 'test/num_examples': 2472, 'score': 11538.243397474289, 'total_duration': 12757.166206598282, 'accumulated_submission_time': 11538.243397474289, 'accumulated_eval_time': 1217.89963889122, 'accumulated_logging_time': 0.40578389167785645, 'global_step': 13829, 'preemption_count': 0}), (15544, {'train/ctc_loss': Array(0.4444199, dtype=float32), 'train/wer': 0.14530537415237108, 'validation/ctc_loss': Array(0.5959213, dtype=float32), 'validation/wer': 0.1713025092443303, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34296244, dtype=float32), 'test/wer': 0.11114496374383036, 'test/num_examples': 2472, 'score': 12978.453676700592, 'total_duration': 14333.121833562851, 'accumulated_submission_time': 12978.453676700592, 'accumulated_eval_time': 1353.5177383422852, 'accumulated_logging_time': 0.4561934471130371, 'global_step': 15544, 'preemption_count': 0}), (17258, {'train/ctc_loss': Array(0.41884962, dtype=float32), 'train/wer': 0.13926902350126338, 'validation/ctc_loss': Array(0.58758664, dtype=float32), 'validation/wer': 0.16857989708139837, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3266319, dtype=float32), 'test/wer': 0.10622956147299575, 'test/num_examples': 2472, 'score': 14418.44036602974, 'total_duration': 15908.17206454277, 'accumulated_submission_time': 14418.44036602974, 'accumulated_eval_time': 1488.4534299373627, 'accumulated_logging_time': 0.5051791667938232, 'global_step': 17258, 'preemption_count': 0}), (18996, {'train/ctc_loss': Array(0.41031313, dtype=float32), 'train/wer': 0.13893716170594292, 'validation/ctc_loss': Array(0.5695679, dtype=float32), 'validation/wer': 0.16348224026569605, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31985068, dtype=float32), 'test/wer': 0.10594519935815408, 'test/num_examples': 2472, 'score': 15858.892181396484, 'total_duration': 17485.223326921463, 'accumulated_submission_time': 15858.892181396484, 'accumulated_eval_time': 1624.9235422611237, 'accumulated_logging_time': 0.5569183826446533, 'global_step': 18996, 'preemption_count': 0}), (20715, {'train/ctc_loss': Array(0.36195385, dtype=float32), 'train/wer': 0.12168618161053411, 'validation/ctc_loss': Array(0.5525102, dtype=float32), 'validation/wer': 0.16128097936800642, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30855164, dtype=float32), 'test/wer': 0.10050169601689923, 'test/num_examples': 2472, 'score': 17299.33094882965, 'total_duration': 19059.429202079773, 'accumulated_submission_time': 17299.33094882965, 'accumulated_eval_time': 1758.560397386551, 'accumulated_logging_time': 0.6087489128112793, 'global_step': 20715, 'preemption_count': 0}), (22428, {'train/ctc_loss': Array(0.3208047, dtype=float32), 'train/wer': 0.10747480926407368, 'validation/ctc_loss': Array(0.53712636, dtype=float32), 'validation/wer': 0.15489925369531846, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30128074, dtype=float32), 'test/wer': 0.09650031482948429, 'test/num_examples': 2472, 'score': 18739.416509628296, 'total_duration': 20633.377140283585, 'accumulated_submission_time': 18739.416509628296, 'accumulated_eval_time': 1892.2922401428223, 'accumulated_logging_time': 0.6600890159606934, 'global_step': 22428, 'preemption_count': 0}), (24166, {'train/ctc_loss': Array(0.36535513, dtype=float32), 'train/wer': 0.12454700430134233, 'validation/ctc_loss': Array(0.52740115, dtype=float32), 'validation/wer': 0.1513463413692229, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.289528, dtype=float32), 'test/wer': 0.093412954725489, 'test/num_examples': 2472, 'score': 20179.82127547264, 'total_duration': 22207.138612270355, 'accumulated_submission_time': 20179.82127547264, 'accumulated_eval_time': 2025.5153098106384, 'accumulated_logging_time': 0.7153546810150146, 'global_step': 24166, 'preemption_count': 0}), (25875, {'train/ctc_loss': Array(0.32537457, dtype=float32), 'train/wer': 0.10948236632536974, 'validation/ctc_loss': Array(0.51202977, dtype=float32), 'validation/wer': 0.14770653716558696, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2792349, dtype=float32), 'test/wer': 0.09168647045680743, 'test/num_examples': 2472, 'score': 21620.019072532654, 'total_duration': 23781.841695070267, 'accumulated_submission_time': 21620.019072532654, 'accumulated_eval_time': 2159.8889672756195, 'accumulated_logging_time': 0.7683267593383789, 'global_step': 25875, 'preemption_count': 0}), (27568, {'train/ctc_loss': Array(0.31351584, dtype=float32), 'train/wer': 0.1057912797410919, 'validation/ctc_loss': Array(0.5010615, dtype=float32), 'validation/wer': 0.1443370632476322, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2763063, dtype=float32), 'test/wer': 0.08935063879917941, 'test/num_examples': 2472, 'score': 23060.151054382324, 'total_duration': 25359.555035591125, 'accumulated_submission_time': 23060.151054382324, 'accumulated_eval_time': 2297.339144229889, 'accumulated_logging_time': 0.822613000869751, 'global_step': 27568, 'preemption_count': 0}), (29314, {'train/ctc_loss': Array(0.28522658, dtype=float32), 'train/wer': 0.09909472289688673, 'validation/ctc_loss': Array(0.48595044, dtype=float32), 'validation/wer': 0.14087104279907703, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26795527, dtype=float32), 'test/wer': 0.08597891658034246, 'test/num_examples': 2472, 'score': 24501.43314099312, 'total_duration': 26934.59812092781, 'accumulated_submission_time': 24501.43314099312, 'accumulated_eval_time': 2430.969462156296, 'accumulated_logging_time': 0.8745872974395752, 'global_step': 29314, 'preemption_count': 0}), (31036, {'train/ctc_loss': Array(0.26872805, dtype=float32), 'train/wer': 0.09431319773841977, 'validation/ctc_loss': Array(0.47574943, dtype=float32), 'validation/wer': 0.1365940314934783, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26138502, dtype=float32), 'test/wer': 0.08388682387829302, 'test/num_examples': 2472, 'score': 25941.97016787529, 'total_duration': 28509.099543333054, 'accumulated_submission_time': 25941.97016787529, 'accumulated_eval_time': 2564.800893306732, 'accumulated_logging_time': 0.9301862716674805, 'global_step': 31036, 'preemption_count': 0}), (32757, {'train/ctc_loss': Array(0.2560898, dtype=float32), 'train/wer': 0.08913130823889437, 'validation/ctc_loss': Array(0.46562022, dtype=float32), 'validation/wer': 0.13359143439180513, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24835972, dtype=float32), 'test/wer': 0.08144943432250726, 'test/num_examples': 2472, 'score': 27382.090871334076, 'total_duration': 30083.422049045563, 'accumulated_submission_time': 27382.090871334076, 'accumulated_eval_time': 2698.8723435401917, 'accumulated_logging_time': 0.9823896884918213, 'global_step': 32757, 'preemption_count': 0}), (34518, {'train/ctc_loss': Array(0.15375972, dtype=float32), 'train/wer': 0.05475259309641217, 'validation/ctc_loss': Array(0.45650312, dtype=float32), 'validation/wer': 0.13060814659625206, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2446342, dtype=float32), 'test/wer': 0.0796010805760364, 'test/num_examples': 2472, 'score': 28822.697194814682, 'total_duration': 31682.440099477768, 'accumulated_submission_time': 28822.697194814682, 'accumulated_eval_time': 2857.1478266716003, 'accumulated_logging_time': 1.0384981632232666, 'global_step': 34518, 'preemption_count': 0}), (36248, {'train/ctc_loss': Array(0.14782521, dtype=float32), 'train/wer': 0.05197591600733533, 'validation/ctc_loss': Array(0.44794834, dtype=float32), 'validation/wer': 0.12872548924954383, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23961341, dtype=float32), 'test/wer': 0.07759023419251315, 'test/num_examples': 2472, 'score': 30263.501261234283, 'total_duration': 33258.248254299164, 'accumulated_submission_time': 30263.501261234283, 'accumulated_eval_time': 2992.0138483047485, 'accumulated_logging_time': 1.0968949794769287, 'global_step': 36248, 'preemption_count': 0}), (37941, {'train/ctc_loss': Array(0.15493704, dtype=float32), 'train/wer': 0.05480588810486535, 'validation/ctc_loss': Array(0.44182682, dtype=float32), 'validation/wer': 0.12585805729071126, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23499914, dtype=float32), 'test/wer': 0.0761684236183048, 'test/num_examples': 2472, 'score': 31703.481546401978, 'total_duration': 34834.663165569305, 'accumulated_submission_time': 31703.481546401978, 'accumulated_eval_time': 3128.327612876892, 'accumulated_logging_time': 1.1431083679199219, 'global_step': 37941, 'preemption_count': 0}), (39661, {'train/ctc_loss': Array(0.13454847, dtype=float32), 'train/wer': 0.04841284748309542, 'validation/ctc_loss': Array(0.4356194, dtype=float32), 'validation/wer': 0.12465122565820597, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23229034, dtype=float32), 'test/wer': 0.07492941726078037, 'test/num_examples': 2472, 'score': 33144.05407190323, 'total_duration': 36410.860171079636, 'accumulated_submission_time': 33144.05407190323, 'accumulated_eval_time': 3263.8199138641357, 'accumulated_logging_time': 1.1951828002929688, 'global_step': 39661, 'preemption_count': 0}), (41377, {'train/ctc_loss': Array(0.14412792, dtype=float32), 'train/wer': 0.0515004984779097, 'validation/ctc_loss': Array(0.43128958, dtype=float32), 'validation/wer': 0.12311613582165924, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2291814, dtype=float32), 'test/wer': 0.07340604878841428, 'test/num_examples': 2472, 'score': 34584.85236620903, 'total_duration': 37986.192954301834, 'accumulated_submission_time': 34584.85236620903, 'accumulated_eval_time': 3398.2206456661224, 'accumulated_logging_time': 1.250486135482788, 'global_step': 41377, 'preemption_count': 0}), (43071, {'train/ctc_loss': Array(0.1331379, dtype=float32), 'train/wer': 0.04820648166946264, 'validation/ctc_loss': Array(0.42728096, dtype=float32), 'validation/wer': 0.12219894378095524, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22690861, dtype=float32), 'test/wer': 0.07334511404951963, 'test/num_examples': 2472, 'score': 36025.78731369972, 'total_duration': 39562.61579680443, 'accumulated_submission_time': 36025.78731369972, 'accumulated_eval_time': 3533.5720529556274, 'accumulated_logging_time': 1.3105106353759766, 'global_step': 43071, 'preemption_count': 0}), (44804, {'train/ctc_loss': Array(0.15674452, dtype=float32), 'train/wer': 0.052520602626834924, 'validation/ctc_loss': Array(0.42669344, dtype=float32), 'validation/wer': 0.12226652635237553, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22530259, dtype=float32), 'test/wer': 0.0731216866735726, 'test/num_examples': 2472, 'score': 37466.45243930817, 'total_duration': 41139.22075676918, 'accumulated_submission_time': 37466.45243930817, 'accumulated_eval_time': 3669.3792510032654, 'accumulated_logging_time': 1.3633804321289062, 'global_step': 44804, 'preemption_count': 0}), (46503, {'train/ctc_loss': Array(0.14168745, dtype=float32), 'train/wer': 0.050901187446988974, 'validation/ctc_loss': Array(0.42561495, dtype=float32), 'validation/wer': 0.1211658959035307, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22481138, dtype=float32), 'test/wer': 0.07253265086425771, 'test/num_examples': 2472, 'score': 38906.67197370529, 'total_duration': 42717.630824804306, 'accumulated_submission_time': 38906.67197370529, 'accumulated_eval_time': 3807.4385566711426, 'accumulated_logging_time': 1.4167790412902832, 'global_step': 46503, 'preemption_count': 0}), (48000, {'train/ctc_loss': Array(0.12119035, dtype=float32), 'train/wer': 0.04460789943515802, 'validation/ctc_loss': Array(0.42536923, dtype=float32), 'validation/wer': 0.12134933431167151, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22480996, dtype=float32), 'test/wer': 0.07304044035504641, 'test/num_examples': 2472, 'score': 40174.220403671265, 'total_duration': 44120.32517743111, 'accumulated_submission_time': 40174.220403671265, 'accumulated_eval_time': 3942.461863040924, 'accumulated_logging_time': 1.4705607891082764, 'global_step': 48000, 'preemption_count': 0})], 'global_step': 48000}
I0212 01:12:58.786448 140441227016000 submission_runner.py:586] Timing: 40174.220403671265
I0212 01:12:58.786507 140441227016000 submission_runner.py:588] Total number of evals: 29
I0212 01:12:58.786560 140441227016000 submission_runner.py:589] ====================
I0212 01:12:58.786622 140441227016000 submission_runner.py:542] Using RNG seed 1231499801
I0212 01:12:58.789962 140441227016000 submission_runner.py:551] --- Tuning run 3/5 ---
I0212 01:12:58.790122 140441227016000 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_2/librispeech_deepspeech_jax/trial_3.
I0212 01:12:58.793750 140441227016000 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_2/librispeech_deepspeech_jax/trial_3/hparams.json.
I0212 01:12:58.795998 140441227016000 submission_runner.py:206] Initializing dataset.
I0212 01:12:58.796121 140441227016000 submission_runner.py:213] Initializing model.
I0212 01:12:59.906197 140441227016000 submission_runner.py:255] Initializing optimizer.
I0212 01:13:00.054196 140441227016000 submission_runner.py:262] Initializing metrics bundle.
I0212 01:13:00.054375 140441227016000 submission_runner.py:280] Initializing checkpoint and logger.
I0212 01:13:00.058476 140441227016000 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_2/librispeech_deepspeech_jax/trial_3 with prefix checkpoint_
I0212 01:13:00.058622 140441227016000 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_2/librispeech_deepspeech_jax/trial_3/meta_data_0.json.
I0212 01:13:00.059000 140441227016000 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0212 01:13:00.059097 140441227016000 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0212 01:13:00.623952 140441227016000 logger_utils.py:220] Unable to record git information. Continuing without it.
I0212 01:13:01.129634 140441227016000 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_2/librispeech_deepspeech_jax/trial_3/flags_0.json.
I0212 01:13:01.146143 140441227016000 submission_runner.py:314] Starting training loop.
I0212 01:13:01.149957 140441227016000 input_pipeline.py:20] Loading split = train-clean-100
I0212 01:13:01.192821 140441227016000 input_pipeline.py:20] Loading split = train-clean-360
I0212 01:13:01.325626 140441227016000 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0212 01:13:16.607617 140284995180288 logging_writer.py:48] [0] global_step=0, grad_norm=18.90143585205078, loss=32.93537902832031
I0212 01:13:16.626728 140441227016000 spec.py:321] Evaluating on the training split.
I0212 01:14:47.058586 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 01:15:46.385501 140441227016000 spec.py:349] Evaluating on the test split.
I0212 01:16:19.093885 140441227016000 submission_runner.py:408] Time since start: 197.94s, 	Step: 1, 	{'train/ctc_loss': Array(31.647242, dtype=float32), 'train/wer': 3.052888001878375, 'validation/ctc_loss': Array(30.570385, dtype=float32), 'validation/wer': 2.911727507072033, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.6577, dtype=float32), 'test/wer': 3.2319176162330145, 'test/num_examples': 2472, 'score': 15.480498790740967, 'total_duration': 197.94407558441162, 'accumulated_submission_time': 15.480498790740967, 'accumulated_eval_time': 182.46350383758545, 'accumulated_logging_time': 0}
I0212 01:16:19.115224 140285423572736 logging_writer.py:48] [1] accumulated_eval_time=182.463504, accumulated_logging_time=0, accumulated_submission_time=15.480499, global_step=1, preemption_count=0, score=15.480499, test/ctc_loss=30.657699584960938, test/num_examples=2472, test/wer=3.231918, total_duration=197.944076, train/ctc_loss=31.647241592407227, train/wer=3.052888, validation/ctc_loss=30.570384979248047, validation/num_examples=5348, validation/wer=2.911728
I0212 01:17:45.225439 140285314467584 logging_writer.py:48] [100] global_step=100, grad_norm=6.479203224182129, loss=9.542410850524902
I0212 01:19:01.751976 140285322860288 logging_writer.py:48] [200] global_step=200, grad_norm=1.5030441284179688, loss=6.613250732421875
I0212 01:20:18.412690 140285314467584 logging_writer.py:48] [300] global_step=300, grad_norm=1.058241605758667, loss=5.956338405609131
I0212 01:21:34.760856 140285322860288 logging_writer.py:48] [400] global_step=400, grad_norm=0.8683003187179565, loss=5.8752946853637695
I0212 01:22:59.637557 140285314467584 logging_writer.py:48] [500] global_step=500, grad_norm=0.5737671852111816, loss=5.813207626342773
I0212 01:24:29.315186 140285322860288 logging_writer.py:48] [600] global_step=600, grad_norm=0.28706061840057373, loss=5.825191497802734
I0212 01:25:59.180795 140285314467584 logging_writer.py:48] [700] global_step=700, grad_norm=0.3880297839641571, loss=5.769650459289551
I0212 01:27:28.892888 140285322860288 logging_writer.py:48] [800] global_step=800, grad_norm=0.3452724814414978, loss=5.678322792053223
I0212 01:28:59.155496 140285314467584 logging_writer.py:48] [900] global_step=900, grad_norm=0.41014841198921204, loss=5.541311740875244
I0212 01:30:25.780680 140285322860288 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.7775082588195801, loss=5.471586227416992
I0212 01:31:51.175622 140285423572736 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.7421144247055054, loss=5.2798027992248535
I0212 01:33:08.199567 140285415180032 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.9911707043647766, loss=4.857293128967285
I0212 01:34:25.794539 140285423572736 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.1893672943115234, loss=4.413285255432129
I0212 01:35:46.727015 140285415180032 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.353041410446167, loss=4.041402816772461
I0212 01:37:15.297354 140285423572736 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.285044550895691, loss=3.7141332626342773
I0212 01:38:45.561006 140285415180032 logging_writer.py:48] [1600] global_step=1600, grad_norm=4.744394302368164, loss=3.55075740814209
I0212 01:40:12.333163 140285423572736 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.2735936641693115, loss=3.34653902053833
I0212 01:40:19.294011 140441227016000 spec.py:321] Evaluating on the training split.
I0212 01:40:59.284217 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 01:41:48.253791 140441227016000 spec.py:349] Evaluating on the test split.
I0212 01:42:12.698531 140441227016000 submission_runner.py:408] Time since start: 1751.54s, 	Step: 1709, 	{'train/ctc_loss': Array(6.3669677, dtype=float32), 'train/wer': 0.9411137440758294, 'validation/ctc_loss': Array(6.3865676, dtype=float32), 'validation/wer': 0.8960773144617048, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.3034024, dtype=float32), 'test/wer': 0.8991123839701014, 'test/num_examples': 2472, 'score': 1455.57701587677, 'total_duration': 1751.5442821979523, 'accumulated_submission_time': 1455.57701587677, 'accumulated_eval_time': 295.8599810600281, 'accumulated_logging_time': 0.033608436584472656}
I0212 01:42:12.734375 140285423572736 logging_writer.py:48] [1709] accumulated_eval_time=295.859981, accumulated_logging_time=0.033608, accumulated_submission_time=1455.577016, global_step=1709, preemption_count=0, score=1455.577016, test/ctc_loss=6.303402423858643, test/num_examples=2472, test/wer=0.899112, total_duration=1751.544282, train/ctc_loss=6.366967678070068, train/wer=0.941114, validation/ctc_loss=6.38656759262085, validation/num_examples=5348, validation/wer=0.896077
I0212 01:43:23.122198 140285415180032 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.7594614028930664, loss=3.1909291744232178
I0212 01:44:39.343299 140285423572736 logging_writer.py:48] [1900] global_step=1900, grad_norm=3.6892316341400146, loss=3.0836265087127686
I0212 01:45:59.505592 140285415180032 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.6727688312530518, loss=2.9494738578796387
I0212 01:47:27.502913 140285423572736 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.486997365951538, loss=2.859062910079956
I0212 01:48:43.467638 140285415180032 logging_writer.py:48] [2200] global_step=2200, grad_norm=3.6516308784484863, loss=2.8180601596832275
I0212 01:50:05.037455 140285423572736 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.974529504776001, loss=2.6783905029296875
I0212 01:51:29.726440 140285415180032 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.970262289047241, loss=2.646635055541992
I0212 01:52:55.699973 140285423572736 logging_writer.py:48] [2500] global_step=2500, grad_norm=2.722663164138794, loss=2.572324752807617
I0212 01:54:25.929513 140285415180032 logging_writer.py:48] [2600] global_step=2600, grad_norm=4.16172981262207, loss=2.5187530517578125
I0212 01:55:55.345071 140285423572736 logging_writer.py:48] [2700] global_step=2700, grad_norm=3.72871994972229, loss=2.4158973693847656
I0212 01:57:24.710931 140285415180032 logging_writer.py:48] [2800] global_step=2800, grad_norm=4.3629045486450195, loss=2.4020307064056396
I0212 01:58:49.126107 140285423572736 logging_writer.py:48] [2900] global_step=2900, grad_norm=4.529335021972656, loss=2.411466121673584
I0212 02:00:14.670024 140285415180032 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.5939905643463135, loss=2.329287528991699
I0212 02:01:49.964356 140285423572736 logging_writer.py:48] [3100] global_step=3100, grad_norm=5.955594539642334, loss=2.3581724166870117
I0212 02:03:07.942164 140285415180032 logging_writer.py:48] [3200] global_step=3200, grad_norm=3.1391608715057373, loss=2.279484987258911
I0212 02:04:26.855775 140285423572736 logging_writer.py:48] [3300] global_step=3300, grad_norm=3.2757649421691895, loss=2.2643895149230957
I0212 02:05:44.896044 140285415180032 logging_writer.py:48] [3400] global_step=3400, grad_norm=4.322299957275391, loss=2.2273659706115723
I0212 02:06:13.004538 140441227016000 spec.py:321] Evaluating on the training split.
I0212 02:06:52.899935 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 02:07:41.776989 140441227016000 spec.py:349] Evaluating on the test split.
I0212 02:08:06.926482 140441227016000 submission_runner.py:408] Time since start: 3305.77s, 	Step: 3436, 	{'train/ctc_loss': Array(4.5963387, dtype=float32), 'train/wer': 0.9283475590008253, 'validation/ctc_loss': Array(4.687805, dtype=float32), 'validation/wer': 0.8887301234830126, 'validation/num_examples': 5348, 'test/ctc_loss': Array(4.3991666, dtype=float32), 'test/wer': 0.8860723498466476, 'test/num_examples': 2472, 'score': 2895.7558012008667, 'total_duration': 3305.7746090888977, 'accumulated_submission_time': 2895.7558012008667, 'accumulated_eval_time': 409.7762682437897, 'accumulated_logging_time': 0.0885474681854248}
I0212 02:08:06.963098 140285423572736 logging_writer.py:48] [3436] accumulated_eval_time=409.776268, accumulated_logging_time=0.088547, accumulated_submission_time=2895.755801, global_step=3436, preemption_count=0, score=2895.755801, test/ctc_loss=4.399166584014893, test/num_examples=2472, test/wer=0.886072, total_duration=3305.774609, train/ctc_loss=4.596338748931885, train/wer=0.928348, validation/ctc_loss=4.68780517578125, validation/num_examples=5348, validation/wer=0.888730
I0212 02:08:56.127929 140285415180032 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.5389244556427, loss=2.0964858531951904
I0212 02:10:11.085162 140285423572736 logging_writer.py:48] [3600] global_step=3600, grad_norm=4.7113237380981445, loss=2.244817018508911
I0212 02:11:31.953974 140285415180032 logging_writer.py:48] [3700] global_step=3700, grad_norm=3.716278314590454, loss=2.119765281677246
I0212 02:13:03.563445 140285423572736 logging_writer.py:48] [3800] global_step=3800, grad_norm=3.108823776245117, loss=2.1070358753204346
I0212 02:14:28.313273 140285415180032 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.79571795463562, loss=2.086374282836914
I0212 02:15:58.085019 140285423572736 logging_writer.py:48] [4000] global_step=4000, grad_norm=3.908376932144165, loss=2.1119794845581055
I0212 02:17:27.380038 140285415180032 logging_writer.py:48] [4100] global_step=4100, grad_norm=3.4273030757904053, loss=2.07857608795166
I0212 02:18:50.096893 140285423572736 logging_writer.py:48] [4200] global_step=4200, grad_norm=4.4168829917907715, loss=2.056337594985962
I0212 02:20:06.529432 140285415180032 logging_writer.py:48] [4300] global_step=4300, grad_norm=4.362059116363525, loss=2.0913376808166504
I0212 02:21:22.316763 140285423572736 logging_writer.py:48] [4400] global_step=4400, grad_norm=3.995112895965576, loss=1.9992750883102417
I0212 02:22:43.397134 140285415180032 logging_writer.py:48] [4500] global_step=4500, grad_norm=3.2635672092437744, loss=2.0812344551086426
I0212 02:24:10.529268 140285423572736 logging_writer.py:48] [4600] global_step=4600, grad_norm=3.2043473720550537, loss=2.0646913051605225
I0212 02:25:41.387801 140285415180032 logging_writer.py:48] [4700] global_step=4700, grad_norm=3.2853047847747803, loss=1.9817523956298828
I0212 02:27:10.934284 140285423572736 logging_writer.py:48] [4800] global_step=4800, grad_norm=3.2638943195343018, loss=2.005596876144409
I0212 02:28:40.843998 140285415180032 logging_writer.py:48] [4900] global_step=4900, grad_norm=4.7679033279418945, loss=2.650071620941162
I0212 02:30:08.344010 140285423572736 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.2083656787872314, loss=2.15208101272583
I0212 02:31:39.537372 140285415180032 logging_writer.py:48] [5100] global_step=5100, grad_norm=2.4305152893066406, loss=2.064615249633789
I0212 02:32:07.416935 140441227016000 spec.py:321] Evaluating on the training split.
I0212 02:33:02.479453 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 02:33:55.580437 140441227016000 spec.py:349] Evaluating on the test split.
I0212 02:34:22.435317 140441227016000 submission_runner.py:408] Time since start: 4881.28s, 	Step: 5133, 	{'train/ctc_loss': Array(0.764981, dtype=float32), 'train/wer': 0.2510452520218764, 'validation/ctc_loss': Array(1.1377617, dtype=float32), 'validation/wer': 0.3193373046139587, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.77849424, dtype=float32), 'test/wer': 0.24920277049946174, 'test/num_examples': 2472, 'score': 4336.122656345367, 'total_duration': 4881.283031463623, 'accumulated_submission_time': 4336.122656345367, 'accumulated_eval_time': 544.7885777950287, 'accumulated_logging_time': 0.14203882217407227}
I0212 02:34:22.470812 140285423572736 logging_writer.py:48] [5133] accumulated_eval_time=544.788578, accumulated_logging_time=0.142039, accumulated_submission_time=4336.122656, global_step=5133, preemption_count=0, score=4336.122656, test/ctc_loss=0.7784942388534546, test/num_examples=2472, test/wer=0.249203, total_duration=4881.283031, train/ctc_loss=0.7649809718132019, train/wer=0.251045, validation/ctc_loss=1.1377617120742798, validation/num_examples=5348, validation/wer=0.319337
I0212 02:35:19.272634 140285423572736 logging_writer.py:48] [5200] global_step=5200, grad_norm=2.978055715560913, loss=1.9330633878707886
I0212 02:36:38.796195 140285415180032 logging_writer.py:48] [5300] global_step=5300, grad_norm=1.9688345193862915, loss=1.9623879194259644
I0212 02:37:57.347460 140285423572736 logging_writer.py:48] [5400] global_step=5400, grad_norm=3.3325893878936768, loss=1.9283891916275024
I0212 02:39:22.533765 140285415180032 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.143968343734741, loss=2.0422675609588623
I0212 02:40:48.751465 140285423572736 logging_writer.py:48] [5600] global_step=5600, grad_norm=2.449143648147583, loss=1.9638261795043945
I0212 02:42:16.651044 140285415180032 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.9628522396087646, loss=1.9139838218688965
I0212 02:43:44.644382 140285423572736 logging_writer.py:48] [5800] global_step=5800, grad_norm=2.6342787742614746, loss=1.8966937065124512
I0212 02:45:13.727799 140285415180032 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.0960097312927246, loss=1.8412398099899292
I0212 02:46:40.292450 140285423572736 logging_writer.py:48] [6000] global_step=6000, grad_norm=2.0416290760040283, loss=1.8942300081253052
I0212 02:48:07.904501 140285415180032 logging_writer.py:48] [6100] global_step=6100, grad_norm=1.9791746139526367, loss=1.8807405233383179
I0212 02:49:36.793607 140285423572736 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.82094144821167, loss=1.8297661542892456
I0212 02:50:52.142349 140285415180032 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.7884985208511353, loss=1.869605302810669
I0212 02:52:10.804100 140285423572736 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.2643864154815674, loss=1.852515697479248
I0212 02:53:33.439490 140285415180032 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.0769553184509277, loss=1.7629958391189575
I0212 02:54:58.829969 140285423572736 logging_writer.py:48] [6600] global_step=6600, grad_norm=2.473590850830078, loss=1.8083161115646362
I0212 02:56:28.168695 140285415180032 logging_writer.py:48] [6700] global_step=6700, grad_norm=2.395932197570801, loss=1.769381046295166
I0212 02:57:55.589648 140285423572736 logging_writer.py:48] [6800] global_step=6800, grad_norm=2.5112571716308594, loss=1.7823764085769653
I0212 02:58:23.167297 140441227016000 spec.py:321] Evaluating on the training split.
I0212 02:59:21.591888 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 03:00:14.308703 140441227016000 spec.py:349] Evaluating on the test split.
I0212 03:00:41.024267 140441227016000 submission_runner.py:408] Time since start: 6459.87s, 	Step: 6831, 	{'train/ctc_loss': Array(0.5411437, dtype=float32), 'train/wer': 0.18125766188892395, 'validation/ctc_loss': Array(0.8500217, dtype=float32), 'validation/wer': 0.2485880069899688, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5582751, dtype=float32), 'test/wer': 0.17900595129283203, 'test/num_examples': 2472, 'score': 5776.731394290924, 'total_duration': 6459.871329784393, 'accumulated_submission_time': 5776.731394290924, 'accumulated_eval_time': 682.6388504505157, 'accumulated_logging_time': 0.19313955307006836}
I0212 03:00:41.136591 140285423572736 logging_writer.py:48] [6831] accumulated_eval_time=682.638850, accumulated_logging_time=0.193140, accumulated_submission_time=5776.731394, global_step=6831, preemption_count=0, score=5776.731394, test/ctc_loss=0.5582751035690308, test/num_examples=2472, test/wer=0.179006, total_duration=6459.871330, train/ctc_loss=0.5411437153816223, train/wer=0.181258, validation/ctc_loss=0.8500217199325562, validation/num_examples=5348, validation/wer=0.248588
I0212 03:01:33.747745 140285415180032 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.9542776346206665, loss=1.863494634628296
I0212 03:02:48.485358 140285423572736 logging_writer.py:48] [7000] global_step=7000, grad_norm=2.6705305576324463, loss=1.8657656908035278
I0212 03:04:09.584865 140285415180032 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.516946315765381, loss=1.7547649145126343
I0212 03:05:38.385171 140285423572736 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.9985746145248413, loss=1.7856367826461792
I0212 03:06:59.781949 140285423572736 logging_writer.py:48] [7300] global_step=7300, grad_norm=2.4186532497406006, loss=1.7534013986587524
I0212 03:08:18.373399 140285415180032 logging_writer.py:48] [7400] global_step=7400, grad_norm=2.3147759437561035, loss=1.7816588878631592
I0212 03:09:41.247482 140285423572736 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.6404306888580322, loss=1.75290048122406
I0212 03:11:04.234638 140285415180032 logging_writer.py:48] [7600] global_step=7600, grad_norm=2.9103572368621826, loss=1.7628246545791626
I0212 03:12:33.305805 140285423572736 logging_writer.py:48] [7700] global_step=7700, grad_norm=3.0677804946899414, loss=1.723283052444458
I0212 03:13:58.362187 140285415180032 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.7872002124786377, loss=1.7744334936141968
I0212 03:15:27.832148 140285423572736 logging_writer.py:48] [7900] global_step=7900, grad_norm=3.015160083770752, loss=1.7498137950897217
I0212 03:16:56.935773 140285415180032 logging_writer.py:48] [8000] global_step=8000, grad_norm=2.1571972370147705, loss=1.7174674272537231
I0212 03:18:22.513835 140285423572736 logging_writer.py:48] [8100] global_step=8100, grad_norm=2.676553964614868, loss=1.7162046432495117
I0212 03:19:53.929682 140285415180032 logging_writer.py:48] [8200] global_step=8200, grad_norm=2.441028118133545, loss=1.7637441158294678
I0212 03:21:17.933347 140285423572736 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.408209204673767, loss=1.6856729984283447
I0212 03:22:35.661063 140285415180032 logging_writer.py:48] [8400] global_step=8400, grad_norm=2.191530704498291, loss=1.8180776834487915
I0212 03:23:54.086180 140285423572736 logging_writer.py:48] [8500] global_step=8500, grad_norm=4.922859191894531, loss=1.6744970083236694
I0212 03:24:41.418291 140441227016000 spec.py:321] Evaluating on the training split.
I0212 03:25:37.717174 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 03:26:30.069140 140441227016000 spec.py:349] Evaluating on the test split.
I0212 03:26:56.710052 140441227016000 submission_runner.py:408] Time since start: 8035.56s, 	Step: 8559, 	{'train/ctc_loss': Array(0.45767567, dtype=float32), 'train/wer': 0.15207675097327233, 'validation/ctc_loss': Array(0.78322417, dtype=float32), 'validation/wer': 0.22473135927860433, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4910314, dtype=float32), 'test/wer': 0.15711006845002337, 'test/num_examples': 2472, 'score': 7216.925114154816, 'total_duration': 8035.558212518692, 'accumulated_submission_time': 7216.925114154816, 'accumulated_eval_time': 817.924996137619, 'accumulated_logging_time': 0.3204481601715088}
I0212 03:26:56.819739 140285423572736 logging_writer.py:48] [8559] accumulated_eval_time=817.924996, accumulated_logging_time=0.320448, accumulated_submission_time=7216.925114, global_step=8559, preemption_count=0, score=7216.925114, test/ctc_loss=0.4910314083099365, test/num_examples=2472, test/wer=0.157110, total_duration=8035.558213, train/ctc_loss=0.45767566561698914, train/wer=0.152077, validation/ctc_loss=0.7832241654396057, validation/num_examples=5348, validation/wer=0.224731
I0212 03:27:28.259880 140285415180032 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.8906397819519043, loss=1.670704960823059
I0212 03:28:43.090443 140285423572736 logging_writer.py:48] [8700] global_step=8700, grad_norm=2.667448043823242, loss=1.7024627923965454
I0212 03:30:01.785661 140285415180032 logging_writer.py:48] [8800] global_step=8800, grad_norm=3.244614362716675, loss=1.6762319803237915
I0212 03:31:28.562287 140285423572736 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.993233561515808, loss=1.7440710067749023
I0212 03:32:52.936313 140285415180032 logging_writer.py:48] [9000] global_step=9000, grad_norm=4.124908924102783, loss=1.7170850038528442
I0212 03:34:18.151548 140285423572736 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.7636561393737793, loss=1.7074695825576782
I0212 03:35:46.562568 140285415180032 logging_writer.py:48] [9200] global_step=9200, grad_norm=2.105947494506836, loss=1.7161595821380615
I0212 03:37:17.359912 140285423572736 logging_writer.py:48] [9300] global_step=9300, grad_norm=2.6215038299560547, loss=1.698962688446045
I0212 03:38:35.068138 140285415180032 logging_writer.py:48] [9400] global_step=9400, grad_norm=2.77738356590271, loss=1.726694107055664
I0212 03:39:55.844752 140285423572736 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.9516502618789673, loss=1.6960843801498413
I0212 03:41:18.483199 140285415180032 logging_writer.py:48] [9600] global_step=9600, grad_norm=2.355001449584961, loss=1.6473580598831177
I0212 03:42:46.030249 140285423572736 logging_writer.py:48] [9700] global_step=9700, grad_norm=2.112053394317627, loss=1.6904600858688354
I0212 03:44:16.561771 140285415180032 logging_writer.py:48] [9800] global_step=9800, grad_norm=2.499229907989502, loss=1.6496284008026123
I0212 03:45:44.094304 140285423572736 logging_writer.py:48] [9900] global_step=9900, grad_norm=2.1254849433898926, loss=1.6270105838775635
I0212 03:47:11.040280 140285415180032 logging_writer.py:48] [10000] global_step=10000, grad_norm=2.096919298171997, loss=1.60551118850708
I0212 03:48:40.718387 140285423572736 logging_writer.py:48] [10100] global_step=10100, grad_norm=2.4381368160247803, loss=1.6323621273040771
I0212 03:50:08.466846 140285415180032 logging_writer.py:48] [10200] global_step=10200, grad_norm=2.001164197921753, loss=1.72868013381958
I0212 03:50:56.954641 140441227016000 spec.py:321] Evaluating on the training split.
I0212 03:51:53.076510 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 03:52:46.482383 140441227016000 spec.py:349] Evaluating on the test split.
I0212 03:53:14.097007 140441227016000 submission_runner.py:408] Time since start: 9612.94s, 	Step: 10255, 	{'train/ctc_loss': Array(0.3818851, dtype=float32), 'train/wer': 0.13084762562542507, 'validation/ctc_loss': Array(0.7319703, dtype=float32), 'validation/wer': 0.21012386919876033, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4505234, dtype=float32), 'test/wer': 0.14606056913046128, 'test/num_examples': 2472, 'score': 8656.972178220749, 'total_duration': 9612.94243979454, 'accumulated_submission_time': 8656.972178220749, 'accumulated_eval_time': 955.0590310096741, 'accumulated_logging_time': 0.44647741317749023}
I0212 03:53:14.137701 140285423572736 logging_writer.py:48] [10255] accumulated_eval_time=955.059031, accumulated_logging_time=0.446477, accumulated_submission_time=8656.972178, global_step=10255, preemption_count=0, score=8656.972178, test/ctc_loss=0.45052340626716614, test/num_examples=2472, test/wer=0.146061, total_duration=9612.942440, train/ctc_loss=0.3818851113319397, train/wer=0.130848, validation/ctc_loss=0.7319703102111816, validation/num_examples=5348, validation/wer=0.210124
I0212 03:53:52.160459 140285423572736 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.6925327777862549, loss=1.6277360916137695
I0212 03:55:09.680680 140285415180032 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.9307161569595337, loss=1.6926982402801514
I0212 03:56:27.529230 140285423572736 logging_writer.py:48] [10500] global_step=10500, grad_norm=2.612616539001465, loss=1.6385419368743896
I0212 03:57:50.174277 140285415180032 logging_writer.py:48] [10600] global_step=10600, grad_norm=2.84360408782959, loss=1.6398981809616089
I0212 03:59:13.189457 140285423572736 logging_writer.py:48] [10700] global_step=10700, grad_norm=2.325096368789673, loss=1.6161084175109863
I0212 04:00:41.442551 140285415180032 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.9390689134597778, loss=1.6143592596054077
I0212 04:02:09.261874 140285423572736 logging_writer.py:48] [10900] global_step=10900, grad_norm=2.5218422412872314, loss=1.6287418603897095
I0212 04:03:37.789415 140285415180032 logging_writer.py:48] [11000] global_step=11000, grad_norm=2.3188235759735107, loss=1.6065186262130737
I0212 04:05:04.428792 140285423572736 logging_writer.py:48] [11100] global_step=11100, grad_norm=2.212559700012207, loss=1.6438747644424438
I0212 04:06:34.237233 140285415180032 logging_writer.py:48] [11200] global_step=11200, grad_norm=2.9845917224884033, loss=1.576088547706604
I0212 04:08:02.757359 140285423572736 logging_writer.py:48] [11300] global_step=11300, grad_norm=2.1392555236816406, loss=1.5686166286468506
I0212 04:09:28.154410 140285423572736 logging_writer.py:48] [11400] global_step=11400, grad_norm=3.541588306427002, loss=1.606851577758789
I0212 04:10:45.108439 140285415180032 logging_writer.py:48] [11500] global_step=11500, grad_norm=2.2177858352661133, loss=1.6888564825057983
I0212 04:12:05.826427 140285423572736 logging_writer.py:48] [11600] global_step=11600, grad_norm=2.3603227138519287, loss=1.5640192031860352
I0212 04:13:29.865045 140285415180032 logging_writer.py:48] [11700] global_step=11700, grad_norm=2.0467731952667236, loss=1.5919355154037476
I0212 04:14:54.927455 140285423572736 logging_writer.py:48] [11800] global_step=11800, grad_norm=2.3305375576019287, loss=1.6838339567184448
I0212 04:16:24.202162 140285415180032 logging_writer.py:48] [11900] global_step=11900, grad_norm=3.1417481899261475, loss=1.6149969100952148
I0212 04:17:14.347886 140441227016000 spec.py:321] Evaluating on the training split.
I0212 04:18:11.733796 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 04:19:05.171196 140441227016000 spec.py:349] Evaluating on the test split.
I0212 04:19:32.918494 140441227016000 submission_runner.py:408] Time since start: 11191.77s, 	Step: 11957, 	{'train/ctc_loss': Array(0.3600609, dtype=float32), 'train/wer': 0.1224991440237527, 'validation/ctc_loss': Array(0.6953865, dtype=float32), 'validation/wer': 0.19995751952653582, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42062852, dtype=float32), 'test/wer': 0.1345439034793736, 'test/num_examples': 2472, 'score': 10097.09136915207, 'total_duration': 11191.76549577713, 'accumulated_submission_time': 10097.09136915207, 'accumulated_eval_time': 1093.6228725910187, 'accumulated_logging_time': 0.5056447982788086}
I0212 04:19:32.952858 140285423572736 logging_writer.py:48] [11957] accumulated_eval_time=1093.622873, accumulated_logging_time=0.505645, accumulated_submission_time=10097.091369, global_step=11957, preemption_count=0, score=10097.091369, test/ctc_loss=0.42062851786613464, test/num_examples=2472, test/wer=0.134544, total_duration=11191.765496, train/ctc_loss=0.3600609004497528, train/wer=0.122499, validation/ctc_loss=0.695386528968811, validation/num_examples=5348, validation/wer=0.199958
I0212 04:20:05.954258 140285415180032 logging_writer.py:48] [12000] global_step=12000, grad_norm=3.1743733882904053, loss=1.672080397605896
I0212 04:21:21.395745 140285423572736 logging_writer.py:48] [12100] global_step=12100, grad_norm=2.167931079864502, loss=1.582111120223999
I0212 04:22:38.936594 140285415180032 logging_writer.py:48] [12200] global_step=12200, grad_norm=2.7895030975341797, loss=1.628493070602417
I0212 04:24:08.757993 140285423572736 logging_writer.py:48] [12300] global_step=12300, grad_norm=2.660066604614258, loss=1.5614954233169556
I0212 04:25:36.278270 140285423572736 logging_writer.py:48] [12400] global_step=12400, grad_norm=2.72987699508667, loss=1.6083931922912598
I0212 04:26:54.494843 140285415180032 logging_writer.py:48] [12500] global_step=12500, grad_norm=3.672003984451294, loss=1.5870121717453003
I0212 04:28:13.682638 140285423572736 logging_writer.py:48] [12600] global_step=12600, grad_norm=2.1336758136749268, loss=1.6225924491882324
I0212 04:29:35.583984 140285415180032 logging_writer.py:48] [12700] global_step=12700, grad_norm=2.4810471534729004, loss=1.52421236038208
I0212 04:30:59.972901 140285423572736 logging_writer.py:48] [12800] global_step=12800, grad_norm=2.6303324699401855, loss=1.5938080549240112
I0212 04:32:30.277973 140285415180032 logging_writer.py:48] [12900] global_step=12900, grad_norm=2.8656086921691895, loss=1.6049119234085083
I0212 04:33:57.548320 140285423572736 logging_writer.py:48] [13000] global_step=13000, grad_norm=2.1789984703063965, loss=1.5417646169662476
I0212 04:35:22.840880 140285415180032 logging_writer.py:48] [13100] global_step=13100, grad_norm=2.2787718772888184, loss=1.535067081451416
I0212 04:36:54.303859 140285423572736 logging_writer.py:48] [13200] global_step=13200, grad_norm=3.0751168727874756, loss=1.5842005014419556
I0212 04:38:24.702509 140285415180032 logging_writer.py:48] [13300] global_step=13300, grad_norm=2.9450833797454834, loss=1.594891905784607
I0212 04:39:53.035068 140285423572736 logging_writer.py:48] [13400] global_step=13400, grad_norm=2.565500259399414, loss=1.5597703456878662
I0212 04:41:08.994975 140285415180032 logging_writer.py:48] [13500] global_step=13500, grad_norm=2.449301242828369, loss=1.5818763971328735
I0212 04:42:28.565620 140285423572736 logging_writer.py:48] [13600] global_step=13600, grad_norm=2.549826145172119, loss=1.6425023078918457
I0212 04:43:33.601970 140441227016000 spec.py:321] Evaluating on the training split.
I0212 04:44:28.780961 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 04:45:21.455606 140441227016000 spec.py:349] Evaluating on the test split.
I0212 04:45:48.056158 140441227016000 submission_runner.py:408] Time since start: 12766.90s, 	Step: 13684, 	{'train/ctc_loss': Array(0.37431747, dtype=float32), 'train/wer': 0.1254869075957585, 'validation/ctc_loss': Array(0.67228556, dtype=float32), 'validation/wer': 0.19349855662936752, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40535486, dtype=float32), 'test/wer': 0.1304003412345378, 'test/num_examples': 2472, 'score': 11537.651688098907, 'total_duration': 12766.904225826263, 'accumulated_submission_time': 11537.651688098907, 'accumulated_eval_time': 1228.071349620819, 'accumulated_logging_time': 0.5557355880737305}
I0212 04:45:48.092830 140285423572736 logging_writer.py:48] [13684] accumulated_eval_time=1228.071350, accumulated_logging_time=0.555736, accumulated_submission_time=11537.651688, global_step=13684, preemption_count=0, score=11537.651688, test/ctc_loss=0.4053548574447632, test/num_examples=2472, test/wer=0.130400, total_duration=12766.904226, train/ctc_loss=0.374317467212677, train/wer=0.125487, validation/ctc_loss=0.6722855567932129, validation/num_examples=5348, validation/wer=0.193499
I0212 04:46:00.954607 140285415180032 logging_writer.py:48] [13700] global_step=13700, grad_norm=3.5668349266052246, loss=1.5573867559432983
I0212 04:47:16.265627 140285423572736 logging_writer.py:48] [13800] global_step=13800, grad_norm=2.428727626800537, loss=1.556726336479187
I0212 04:48:31.400946 140285415180032 logging_writer.py:48] [13900] global_step=13900, grad_norm=2.7557902336120605, loss=1.5770025253295898
I0212 04:49:58.753275 140285423572736 logging_writer.py:48] [14000] global_step=14000, grad_norm=2.691424608230591, loss=1.5640454292297363
I0212 04:51:29.414691 140285415180032 logging_writer.py:48] [14100] global_step=14100, grad_norm=3.159487247467041, loss=1.551193356513977
I0212 04:52:55.425152 140285423572736 logging_writer.py:48] [14200] global_step=14200, grad_norm=2.1759560108184814, loss=1.5538018941879272
I0212 04:54:21.409585 140285415180032 logging_writer.py:48] [14300] global_step=14300, grad_norm=3.1537461280822754, loss=1.5741419792175293
I0212 04:55:49.802159 140285423572736 logging_writer.py:48] [14400] global_step=14400, grad_norm=3.9069831371307373, loss=1.5433169603347778
I0212 04:57:12.062524 140285423572736 logging_writer.py:48] [14500] global_step=14500, grad_norm=2.7209389209747314, loss=1.574069857597351
I0212 04:58:27.315801 140285415180032 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.9947696924209595, loss=1.5302659273147583
I0212 04:59:47.195878 140285423572736 logging_writer.py:48] [14700] global_step=14700, grad_norm=2.011470317840576, loss=1.498645305633545
I0212 05:01:11.172324 140285415180032 logging_writer.py:48] [14800] global_step=14800, grad_norm=2.105778932571411, loss=1.5157092809677124
I0212 05:02:37.893501 140285423572736 logging_writer.py:48] [14900] global_step=14900, grad_norm=2.773667573928833, loss=1.5716572999954224
I0212 05:04:05.447397 140285415180032 logging_writer.py:48] [15000] global_step=15000, grad_norm=2.1683645248413086, loss=1.5914753675460815
I0212 05:05:37.165243 140285423572736 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.9751098155975342, loss=1.5201164484024048
I0212 05:07:06.138227 140285415180032 logging_writer.py:48] [15200] global_step=15200, grad_norm=2.3740742206573486, loss=1.6119431257247925
I0212 05:08:30.028999 140285423572736 logging_writer.py:48] [15300] global_step=15300, grad_norm=2.9167838096618652, loss=1.5809913873672485
I0212 05:09:48.619450 140441227016000 spec.py:321] Evaluating on the training split.
I0212 05:10:43.846716 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 05:11:38.255862 140441227016000 spec.py:349] Evaluating on the test split.
I0212 05:12:05.825526 140441227016000 submission_runner.py:408] Time since start: 14344.67s, 	Step: 15387, 	{'train/ctc_loss': Array(0.33916324, dtype=float32), 'train/wer': 0.11298625599904125, 'validation/ctc_loss': Array(0.6459791, dtype=float32), 'validation/wer': 0.18616102030373538, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3900358, dtype=float32), 'test/wer': 0.1256474316007556, 'test/num_examples': 2472, 'score': 12978.08913564682, 'total_duration': 14344.672752857208, 'accumulated_submission_time': 12978.08913564682, 'accumulated_eval_time': 1365.2708704471588, 'accumulated_logging_time': 0.6095635890960693}
I0212 05:12:05.861299 140285423572736 logging_writer.py:48] [15387] accumulated_eval_time=1365.270870, accumulated_logging_time=0.609564, accumulated_submission_time=12978.089136, global_step=15387, preemption_count=0, score=12978.089136, test/ctc_loss=0.39003580808639526, test/num_examples=2472, test/wer=0.125647, total_duration=14344.672753, train/ctc_loss=0.33916324377059937, train/wer=0.112986, validation/ctc_loss=0.645979106426239, validation/num_examples=5348, validation/wer=0.186161
I0212 05:12:16.489870 140285415180032 logging_writer.py:48] [15400] global_step=15400, grad_norm=2.636809825897217, loss=1.5582733154296875
I0212 05:13:37.907281 140285423572736 logging_writer.py:48] [15500] global_step=15500, grad_norm=3.301175832748413, loss=1.5698063373565674
I0212 05:14:56.769599 140285415180032 logging_writer.py:48] [15600] global_step=15600, grad_norm=3.160712242126465, loss=1.5605324506759644
I0212 05:16:13.853157 140285423572736 logging_writer.py:48] [15700] global_step=15700, grad_norm=2.3785440921783447, loss=1.5050346851348877
I0212 05:17:38.387304 140285415180032 logging_writer.py:48] [15800] global_step=15800, grad_norm=4.810314178466797, loss=1.527505874633789
I0212 05:19:07.123562 140285423572736 logging_writer.py:48] [15900] global_step=15900, grad_norm=2.3767008781433105, loss=1.561105489730835
I0212 05:20:38.137225 140285415180032 logging_writer.py:48] [16000] global_step=16000, grad_norm=3.186936378479004, loss=1.5246843099594116
I0212 05:22:04.660216 140285423572736 logging_writer.py:48] [16100] global_step=16100, grad_norm=2.1552724838256836, loss=1.5315929651260376
I0212 05:23:31.931618 140285415180032 logging_writer.py:48] [16200] global_step=16200, grad_norm=2.6097092628479004, loss=1.4989169836044312
I0212 05:25:00.250612 140285423572736 logging_writer.py:48] [16300] global_step=16300, grad_norm=4.444085597991943, loss=1.5461056232452393
I0212 05:26:29.333636 140285415180032 logging_writer.py:48] [16400] global_step=16400, grad_norm=3.50286602973938, loss=1.528990387916565
I0212 05:27:59.421689 140285423572736 logging_writer.py:48] [16500] global_step=16500, grad_norm=2.109666109085083, loss=1.5133508443832397
I0212 05:29:16.433753 140285415180032 logging_writer.py:48] [16600] global_step=16600, grad_norm=2.023676633834839, loss=1.5613033771514893
I0212 05:30:37.802888 140285423572736 logging_writer.py:48] [16700] global_step=16700, grad_norm=3.4461562633514404, loss=1.5028769969940186
I0212 05:31:57.387606 140285415180032 logging_writer.py:48] [16800] global_step=16800, grad_norm=2.4073967933654785, loss=1.5835113525390625
I0212 05:33:22.135069 140285423572736 logging_writer.py:48] [16900] global_step=16900, grad_norm=2.409738063812256, loss=1.4701042175292969
I0212 05:34:53.621540 140285415180032 logging_writer.py:48] [17000] global_step=17000, grad_norm=2.7950003147125244, loss=1.5173696279525757
I0212 05:36:05.966282 140441227016000 spec.py:321] Evaluating on the training split.
I0212 05:37:02.658699 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 05:37:54.333821 140441227016000 spec.py:349] Evaluating on the test split.
I0212 05:38:22.552676 140441227016000 submission_runner.py:408] Time since start: 15921.40s, 	Step: 17085, 	{'train/ctc_loss': Array(0.32999718, dtype=float32), 'train/wer': 0.11262952703395916, 'validation/ctc_loss': Array(0.6329018, dtype=float32), 'validation/wer': 0.18273361846742037, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37440822, dtype=float32), 'test/wer': 0.12140231145776206, 'test/num_examples': 2472, 'score': 14418.107741355896, 'total_duration': 15921.399963855743, 'accumulated_submission_time': 14418.107741355896, 'accumulated_eval_time': 1501.8507792949677, 'accumulated_logging_time': 0.659881591796875}
I0212 05:38:22.586156 140285423572736 logging_writer.py:48] [17085] accumulated_eval_time=1501.850779, accumulated_logging_time=0.659882, accumulated_submission_time=14418.107741, global_step=17085, preemption_count=0, score=14418.107741, test/ctc_loss=0.37440821528434753, test/num_examples=2472, test/wer=0.121402, total_duration=15921.399964, train/ctc_loss=0.329997181892395, train/wer=0.112630, validation/ctc_loss=0.6329017877578735, validation/num_examples=5348, validation/wer=0.182734
I0212 05:38:34.870645 140285415180032 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.8878122568130493, loss=1.5021576881408691
I0212 05:39:49.744949 140285423572736 logging_writer.py:48] [17200] global_step=17200, grad_norm=2.5711379051208496, loss=1.5517830848693848
I0212 05:41:04.944518 140285415180032 logging_writer.py:48] [17300] global_step=17300, grad_norm=2.226430892944336, loss=1.5508784055709839
I0212 05:42:34.395711 140285423572736 logging_writer.py:48] [17400] global_step=17400, grad_norm=2.1999433040618896, loss=1.5481493473052979
I0212 05:44:03.429736 140285415180032 logging_writer.py:48] [17500] global_step=17500, grad_norm=2.301389217376709, loss=1.601637601852417
I0212 05:45:26.159706 140285423572736 logging_writer.py:48] [17600] global_step=17600, grad_norm=2.15000581741333, loss=1.4759222269058228
I0212 05:46:42.524289 140285415180032 logging_writer.py:48] [17700] global_step=17700, grad_norm=2.3278915882110596, loss=1.5255564451217651
I0212 05:48:03.561195 140285423572736 logging_writer.py:48] [17800] global_step=17800, grad_norm=2.2337450981140137, loss=1.4957945346832275
I0212 05:49:27.219421 140285415180032 logging_writer.py:48] [17900] global_step=17900, grad_norm=2.758183002471924, loss=1.528083086013794
I0212 05:50:55.750371 140285423572736 logging_writer.py:48] [18000] global_step=18000, grad_norm=2.4131133556365967, loss=1.5772874355316162
I0212 05:52:21.472122 140285415180032 logging_writer.py:48] [18100] global_step=18100, grad_norm=2.701582431793213, loss=1.4553335905075073
I0212 05:53:51.815421 140285423572736 logging_writer.py:48] [18200] global_step=18200, grad_norm=2.9589779376983643, loss=1.4800612926483154
I0212 05:55:17.353600 140285415180032 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.9821205139160156, loss=1.5222052335739136
I0212 05:56:46.618022 140285423572736 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.984117031097412, loss=1.4940481185913086
I0212 05:58:15.304034 140285415180032 logging_writer.py:48] [18500] global_step=18500, grad_norm=2.116933584213257, loss=1.533195972442627
I0212 05:59:40.361530 140285423572736 logging_writer.py:48] [18600] global_step=18600, grad_norm=3.251556158065796, loss=1.5230152606964111
I0212 06:00:56.655873 140285415180032 logging_writer.py:48] [18700] global_step=18700, grad_norm=3.108241081237793, loss=1.4807426929473877
I0212 06:02:14.389479 140285423572736 logging_writer.py:48] [18800] global_step=18800, grad_norm=3.123185634613037, loss=1.4489585161209106
I0212 06:02:22.658799 140441227016000 spec.py:321] Evaluating on the training split.
I0212 06:03:18.126398 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 06:04:13.132461 140441227016000 spec.py:349] Evaluating on the test split.
I0212 06:04:40.668354 140441227016000 submission_runner.py:408] Time since start: 17499.52s, 	Step: 18812, 	{'train/ctc_loss': Array(0.35147545, dtype=float32), 'train/wer': 0.11254347617983981, 'validation/ctc_loss': Array(0.6095573, dtype=float32), 'validation/wer': 0.175338154223428, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36095637, dtype=float32), 'test/wer': 0.11585725021834949, 'test/num_examples': 2472, 'score': 15858.09096622467, 'total_duration': 17499.5164706707, 'accumulated_submission_time': 15858.09096622467, 'accumulated_eval_time': 1639.8546781539917, 'accumulated_logging_time': 0.7098729610443115}
I0212 06:04:40.703284 140285423572736 logging_writer.py:48] [18812] accumulated_eval_time=1639.854678, accumulated_logging_time=0.709873, accumulated_submission_time=15858.090966, global_step=18812, preemption_count=0, score=15858.090966, test/ctc_loss=0.3609563708305359, test/num_examples=2472, test/wer=0.115857, total_duration=17499.516471, train/ctc_loss=0.35147544741630554, train/wer=0.112543, validation/ctc_loss=0.6095572710037231, validation/num_examples=5348, validation/wer=0.175338
I0212 06:05:47.712970 140285415180032 logging_writer.py:48] [18900] global_step=18900, grad_norm=2.128345251083374, loss=1.5437034368515015
I0212 06:07:03.924649 140285423572736 logging_writer.py:48] [19000] global_step=19000, grad_norm=2.1143386363983154, loss=1.5145678520202637
I0212 06:08:21.865532 140285415180032 logging_writer.py:48] [19100] global_step=19100, grad_norm=2.3566033840179443, loss=1.4758572578430176
I0212 06:09:53.996230 140285423572736 logging_writer.py:48] [19200] global_step=19200, grad_norm=2.762237310409546, loss=1.5446405410766602
I0212 06:11:18.816314 140285415180032 logging_writer.py:48] [19300] global_step=19300, grad_norm=2.135488986968994, loss=1.518506646156311
I0212 06:12:49.360254 140285423572736 logging_writer.py:48] [19400] global_step=19400, grad_norm=3.0929362773895264, loss=1.4962249994277954
I0212 06:14:21.190061 140285415180032 logging_writer.py:48] [19500] global_step=19500, grad_norm=2.516946792602539, loss=1.487001657485962
I0212 06:15:46.726516 140285423572736 logging_writer.py:48] [19600] global_step=19600, grad_norm=2.432631254196167, loss=1.5197418928146362
I0212 06:17:03.796140 140285415180032 logging_writer.py:48] [19700] global_step=19700, grad_norm=2.9755167961120605, loss=1.5775861740112305
I0212 06:18:19.245189 140285423572736 logging_writer.py:48] [19800] global_step=19800, grad_norm=3.1737122535705566, loss=1.460111379623413
I0212 06:19:40.062809 140285415180032 logging_writer.py:48] [19900] global_step=19900, grad_norm=3.014047145843506, loss=1.5120939016342163
I0212 06:21:06.442887 140285423572736 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.8459017276763916, loss=1.4238944053649902
I0212 06:22:33.427904 140285415180032 logging_writer.py:48] [20100] global_step=20100, grad_norm=3.5204267501831055, loss=1.4674468040466309
I0212 06:24:02.157316 140285423572736 logging_writer.py:48] [20200] global_step=20200, grad_norm=2.0152523517608643, loss=1.5170249938964844
I0212 06:25:33.735778 140285415180032 logging_writer.py:48] [20300] global_step=20300, grad_norm=2.55893874168396, loss=1.5269044637680054
I0212 06:27:04.168363 140285423572736 logging_writer.py:48] [20400] global_step=20400, grad_norm=2.9342691898345947, loss=1.5148693323135376
I0212 06:28:30.903583 140285415180032 logging_writer.py:48] [20500] global_step=20500, grad_norm=3.3250226974487305, loss=1.48854660987854
I0212 06:28:40.766909 140441227016000 spec.py:321] Evaluating on the training split.
I0212 06:29:37.786606 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 06:30:32.253466 140441227016000 spec.py:349] Evaluating on the test split.
I0212 06:30:58.812654 140441227016000 submission_runner.py:408] Time since start: 19077.66s, 	Step: 20513, 	{'train/ctc_loss': Array(0.2695812, dtype=float32), 'train/wer': 0.09233033205488443, 'validation/ctc_loss': Array(0.59979016, dtype=float32), 'validation/wer': 0.1738803016113616, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35536996, dtype=float32), 'test/wer': 0.11553226494424472, 'test/num_examples': 2472, 'score': 17298.066687345505, 'total_duration': 19077.658970832825, 'accumulated_submission_time': 17298.066687345505, 'accumulated_eval_time': 1777.8929901123047, 'accumulated_logging_time': 0.7604987621307373}
I0212 06:30:58.847468 140285423572736 logging_writer.py:48] [20513] accumulated_eval_time=1777.892990, accumulated_logging_time=0.760499, accumulated_submission_time=17298.066687, global_step=20513, preemption_count=0, score=17298.066687, test/ctc_loss=0.3553699553012848, test/num_examples=2472, test/wer=0.115532, total_duration=19077.658971, train/ctc_loss=0.2695811986923218, train/wer=0.092330, validation/ctc_loss=0.5997901558876038, validation/num_examples=5348, validation/wer=0.173880
I0212 06:32:08.705722 140285423572736 logging_writer.py:48] [20600] global_step=20600, grad_norm=2.0158474445343018, loss=1.4861377477645874
I0212 06:33:25.133287 140285415180032 logging_writer.py:48] [20700] global_step=20700, grad_norm=3.5171263217926025, loss=1.4200154542922974
I0212 06:34:41.922777 140285423572736 logging_writer.py:48] [20800] global_step=20800, grad_norm=2.995311737060547, loss=1.48619544506073
I0212 06:36:01.250837 140285415180032 logging_writer.py:48] [20900] global_step=20900, grad_norm=2.2705039978027344, loss=1.4834415912628174
I0212 06:37:27.661861 140285423572736 logging_writer.py:48] [21000] global_step=21000, grad_norm=2.3767223358154297, loss=1.4822818040847778
I0212 06:38:59.339319 140285415180032 logging_writer.py:48] [21100] global_step=21100, grad_norm=2.0193605422973633, loss=1.452038288116455
I0212 06:40:28.239013 140285423572736 logging_writer.py:48] [21200] global_step=21200, grad_norm=5.445840835571289, loss=1.4479260444641113
I0212 06:41:54.486784 140285415180032 logging_writer.py:48] [21300] global_step=21300, grad_norm=2.2414376735687256, loss=1.4713183641433716
I0212 06:43:21.221958 140285423572736 logging_writer.py:48] [21400] global_step=21400, grad_norm=2.960031747817993, loss=1.463405966758728
I0212 06:44:47.514927 140285415180032 logging_writer.py:48] [21500] global_step=21500, grad_norm=2.3546323776245117, loss=1.4852874279022217
I0212 06:46:19.981127 140285423572736 logging_writer.py:48] [21600] global_step=21600, grad_norm=4.068005084991455, loss=1.4604878425598145
I0212 06:47:44.610736 140285423572736 logging_writer.py:48] [21700] global_step=21700, grad_norm=2.262096881866455, loss=1.4334933757781982
I0212 06:49:02.803931 140285415180032 logging_writer.py:48] [21800] global_step=21800, grad_norm=2.8390190601348877, loss=1.4396878480911255
I0212 06:50:21.268672 140285423572736 logging_writer.py:48] [21900] global_step=21900, grad_norm=2.4226958751678467, loss=1.4393999576568604
I0212 06:51:46.761906 140285415180032 logging_writer.py:48] [22000] global_step=22000, grad_norm=2.152336359024048, loss=1.4934332370758057
I0212 06:53:15.191402 140285423572736 logging_writer.py:48] [22100] global_step=22100, grad_norm=2.219139575958252, loss=1.405348777770996
I0212 06:54:46.752622 140285415180032 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.8848623037338257, loss=1.455509066581726
I0212 06:54:59.740587 140441227016000 spec.py:321] Evaluating on the training split.
I0212 06:55:56.000562 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 06:56:50.165682 140441227016000 spec.py:349] Evaluating on the test split.
I0212 06:57:17.502578 140441227016000 submission_runner.py:408] Time since start: 20656.35s, 	Step: 22217, 	{'train/ctc_loss': Array(0.2810426, dtype=float32), 'train/wer': 0.09618531316817139, 'validation/ctc_loss': Array(0.58093655, dtype=float32), 'validation/wer': 0.16857024242833835, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3375406, dtype=float32), 'test/wer': 0.11025125424004224, 'test/num_examples': 2472, 'score': 18738.8721241951, 'total_duration': 20656.351114034653, 'accumulated_submission_time': 18738.8721241951, 'accumulated_eval_time': 1915.6497299671173, 'accumulated_logging_time': 0.8108630180358887}
I0212 06:57:17.536654 140285423572736 logging_writer.py:48] [22217] accumulated_eval_time=1915.649730, accumulated_logging_time=0.810863, accumulated_submission_time=18738.872124, global_step=22217, preemption_count=0, score=18738.872124, test/ctc_loss=0.3375405967235565, test/num_examples=2472, test/wer=0.110251, total_duration=20656.351114, train/ctc_loss=0.28104260563850403, train/wer=0.096185, validation/ctc_loss=0.5809365510940552, validation/num_examples=5348, validation/wer=0.168570
I0212 06:58:21.064219 140285415180032 logging_writer.py:48] [22300] global_step=22300, grad_norm=2.877570629119873, loss=1.5121445655822754
I0212 06:59:36.173089 140285423572736 logging_writer.py:48] [22400] global_step=22400, grad_norm=2.751802921295166, loss=1.5186073780059814
I0212 07:01:03.238985 140285415180032 logging_writer.py:48] [22500] global_step=22500, grad_norm=2.7241082191467285, loss=1.4307039976119995
I0212 07:02:34.224153 140285423572736 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.9946266412734985, loss=1.4341835975646973
I0212 07:04:01.245877 140285423572736 logging_writer.py:48] [22700] global_step=22700, grad_norm=2.26365327835083, loss=1.39096999168396
I0212 07:05:16.626461 140285415180032 logging_writer.py:48] [22800] global_step=22800, grad_norm=2.6040263175964355, loss=1.4274283647537231
I0212 07:06:36.984478 140285423572736 logging_writer.py:48] [22900] global_step=22900, grad_norm=3.2434141635894775, loss=1.4129791259765625
I0212 07:08:00.349988 140285415180032 logging_writer.py:48] [23000] global_step=23000, grad_norm=2.7196364402770996, loss=1.4544886350631714
I0212 07:09:25.646203 140285423572736 logging_writer.py:48] [23100] global_step=23100, grad_norm=3.0806891918182373, loss=1.4489305019378662
I0212 07:10:54.454834 140285415180032 logging_writer.py:48] [23200] global_step=23200, grad_norm=2.544039249420166, loss=1.3888752460479736
I0212 07:12:25.152912 140285423572736 logging_writer.py:48] [23300] global_step=23300, grad_norm=2.0124435424804688, loss=1.4334816932678223
I0212 07:13:53.851686 140285415180032 logging_writer.py:48] [23400] global_step=23400, grad_norm=2.162585496902466, loss=1.4502573013305664
I0212 07:15:20.066959 140285423572736 logging_writer.py:48] [23500] global_step=23500, grad_norm=2.519927501678467, loss=1.4529796838760376
I0212 07:16:47.429236 140285415180032 logging_writer.py:48] [23600] global_step=23600, grad_norm=2.8733999729156494, loss=1.4425798654556274
I0212 07:18:19.954998 140285423572736 logging_writer.py:48] [23700] global_step=23700, grad_norm=2.6627919673919678, loss=1.4248359203338623
I0212 07:19:37.497488 140285415180032 logging_writer.py:48] [23800] global_step=23800, grad_norm=3.4242970943450928, loss=1.425825834274292
I0212 07:20:57.658670 140285423572736 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.760833501815796, loss=1.4024475812911987
I0212 07:21:18.090723 140441227016000 spec.py:321] Evaluating on the training split.
I0212 07:22:12.988843 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 07:23:06.770166 140441227016000 spec.py:349] Evaluating on the test split.
I0212 07:23:33.690657 140441227016000 submission_runner.py:408] Time since start: 22232.54s, 	Step: 23927, 	{'train/ctc_loss': Array(0.3692097, dtype=float32), 'train/wer': 0.12319453909908326, 'validation/ctc_loss': Array(0.56566906, dtype=float32), 'validation/wer': 0.16485320100022205, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32571787, dtype=float32), 'test/wer': 0.1042999614079987, 'test/num_examples': 2472, 'score': 20179.337572574615, 'total_duration': 22232.538994073868, 'accumulated_submission_time': 20179.337572574615, 'accumulated_eval_time': 2051.2442207336426, 'accumulated_logging_time': 0.8616993427276611}
I0212 07:23:33.724421 140285423572736 logging_writer.py:48] [23927] accumulated_eval_time=2051.244221, accumulated_logging_time=0.861699, accumulated_submission_time=20179.337573, global_step=23927, preemption_count=0, score=20179.337573, test/ctc_loss=0.32571786642074585, test/num_examples=2472, test/wer=0.104300, total_duration=22232.538994, train/ctc_loss=0.3692097067832947, train/wer=0.123195, validation/ctc_loss=0.565669059753418, validation/num_examples=5348, validation/wer=0.164853
I0212 07:24:29.580866 140285415180032 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.8602313995361328, loss=1.38988196849823
I0212 07:25:44.766707 140285423572736 logging_writer.py:48] [24100] global_step=24100, grad_norm=3.0885348320007324, loss=1.4495227336883545
I0212 07:27:06.598772 140285415180032 logging_writer.py:48] [24200] global_step=24200, grad_norm=2.6371335983276367, loss=1.3999803066253662
I0212 07:28:36.594197 140285423572736 logging_writer.py:48] [24300] global_step=24300, grad_norm=3.1134467124938965, loss=1.4234061241149902
I0212 07:30:05.560732 140285415180032 logging_writer.py:48] [24400] global_step=24400, grad_norm=2.4083194732666016, loss=1.3880044221878052
I0212 07:31:33.363890 140285423572736 logging_writer.py:48] [24500] global_step=24500, grad_norm=3.8731813430786133, loss=1.4692083597183228
I0212 07:33:03.261044 140285415180032 logging_writer.py:48] [24600] global_step=24600, grad_norm=2.4349114894866943, loss=1.4465255737304688
I0212 07:34:31.369936 140285423572736 logging_writer.py:48] [24700] global_step=24700, grad_norm=2.9477360248565674, loss=1.4560317993164062
I0212 07:35:52.849604 140285423572736 logging_writer.py:48] [24800] global_step=24800, grad_norm=2.5395281314849854, loss=1.398297667503357
I0212 07:37:11.343932 140285415180032 logging_writer.py:48] [24900] global_step=24900, grad_norm=3.889326810836792, loss=1.4160476922988892
I0212 07:38:32.022304 140285423572736 logging_writer.py:48] [25000] global_step=25000, grad_norm=2.415707588195801, loss=1.3873159885406494
I0212 07:39:56.110095 140285415180032 logging_writer.py:48] [25100] global_step=25100, grad_norm=3.260671854019165, loss=1.4729994535446167
I0212 07:41:23.984155 140285423572736 logging_writer.py:48] [25200] global_step=25200, grad_norm=3.371793746948242, loss=1.393618106842041
I0212 07:42:51.161586 140285415180032 logging_writer.py:48] [25300] global_step=25300, grad_norm=2.5298430919647217, loss=1.4359780550003052
I0212 07:44:24.088294 140285423572736 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.9404500722885132, loss=1.3896245956420898
I0212 07:45:51.684788 140285415180032 logging_writer.py:48] [25500] global_step=25500, grad_norm=3.1665215492248535, loss=1.4155434370040894
I0212 07:47:23.678773 140285423572736 logging_writer.py:48] [25600] global_step=25600, grad_norm=2.816978931427002, loss=1.4168912172317505
I0212 07:47:34.461829 140441227016000 spec.py:321] Evaluating on the training split.
I0212 07:48:29.169731 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 07:49:21.527347 140441227016000 spec.py:349] Evaluating on the test split.
I0212 07:49:48.507898 140441227016000 submission_runner.py:408] Time since start: 23807.36s, 	Step: 25613, 	{'train/ctc_loss': Array(0.3844082, dtype=float32), 'train/wer': 0.12607921747375775, 'validation/ctc_loss': Array(0.54311615, dtype=float32), 'validation/wer': 0.157988742674532, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31971592, dtype=float32), 'test/wer': 0.10326407084678975, 'test/num_examples': 2472, 'score': 21619.988522529602, 'total_duration': 23807.355211019516, 'accumulated_submission_time': 21619.988522529602, 'accumulated_eval_time': 2185.283809185028, 'accumulated_logging_time': 0.9100189208984375}
I0212 07:49:48.549271 140285423572736 logging_writer.py:48] [25613] accumulated_eval_time=2185.283809, accumulated_logging_time=0.910019, accumulated_submission_time=21619.988523, global_step=25613, preemption_count=0, score=21619.988523, test/ctc_loss=0.3197159171104431, test/num_examples=2472, test/wer=0.103264, total_duration=23807.355211, train/ctc_loss=0.38440820574760437, train/wer=0.126079, validation/ctc_loss=0.5431161522865295, validation/num_examples=5348, validation/wer=0.157989
I0212 07:50:54.645438 140285415180032 logging_writer.py:48] [25700] global_step=25700, grad_norm=2.474414110183716, loss=1.431115984916687
I0212 07:52:15.730823 140285423572736 logging_writer.py:48] [25800] global_step=25800, grad_norm=3.2000973224639893, loss=1.4224880933761597
I0212 07:53:35.732183 140285415180032 logging_writer.py:48] [25900] global_step=25900, grad_norm=2.352130651473999, loss=1.3323737382888794
I0212 07:54:55.891175 140285423572736 logging_writer.py:48] [26000] global_step=26000, grad_norm=2.0772719383239746, loss=1.3336255550384521
I0212 07:56:17.771661 140285415180032 logging_writer.py:48] [26100] global_step=26100, grad_norm=3.020470142364502, loss=1.4397757053375244
I0212 07:57:41.126597 140285423572736 logging_writer.py:48] [26200] global_step=26200, grad_norm=2.8553924560546875, loss=1.3982030153274536
I0212 07:59:11.094422 140285415180032 logging_writer.py:48] [26300] global_step=26300, grad_norm=2.801791191101074, loss=1.3825525045394897
I0212 08:00:41.910162 140285423572736 logging_writer.py:48] [26400] global_step=26400, grad_norm=2.1199376583099365, loss=1.408491849899292
I0212 08:02:09.905105 140285415180032 logging_writer.py:48] [26500] global_step=26500, grad_norm=3.402352809906006, loss=1.3496041297912598
I0212 08:03:41.420272 140285423572736 logging_writer.py:48] [26600] global_step=26600, grad_norm=2.184166431427002, loss=1.413719892501831
I0212 08:05:11.799339 140285415180032 logging_writer.py:48] [26700] global_step=26700, grad_norm=2.82957124710083, loss=1.33054518699646
I0212 08:06:38.747767 140285423572736 logging_writer.py:48] [26800] global_step=26800, grad_norm=2.125032424926758, loss=1.3590871095657349
I0212 08:07:55.284835 140285415180032 logging_writer.py:48] [26900] global_step=26900, grad_norm=2.2030255794525146, loss=1.3600705862045288
I0212 08:09:16.675778 140285423572736 logging_writer.py:48] [27000] global_step=27000, grad_norm=2.5394997596740723, loss=1.3682717084884644
I0212 08:10:42.611596 140285415180032 logging_writer.py:48] [27100] global_step=27100, grad_norm=2.0152618885040283, loss=1.3533532619476318
I0212 08:12:04.978847 140285423572736 logging_writer.py:48] [27200] global_step=27200, grad_norm=2.308583974838257, loss=1.395531415939331
I0212 08:13:33.452136 140285415180032 logging_writer.py:48] [27300] global_step=27300, grad_norm=3.2288849353790283, loss=1.43695867061615
I0212 08:13:48.526312 140441227016000 spec.py:321] Evaluating on the training split.
I0212 08:14:44.377042 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 08:15:37.104159 140441227016000 spec.py:349] Evaluating on the test split.
I0212 08:16:04.187985 140441227016000 submission_runner.py:408] Time since start: 25383.04s, 	Step: 27319, 	{'train/ctc_loss': Array(0.42835096, dtype=float32), 'train/wer': 0.14189251902728792, 'validation/ctc_loss': Array(0.540189, dtype=float32), 'validation/wer': 0.15711982389912818, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3093404, dtype=float32), 'test/wer': 0.0984096033148498, 'test/num_examples': 2472, 'score': 23059.87901854515, 'total_duration': 25383.035180568695, 'accumulated_submission_time': 23059.87901854515, 'accumulated_eval_time': 2320.938892841339, 'accumulated_logging_time': 0.9658107757568359}
I0212 08:16:04.226369 140285423572736 logging_writer.py:48] [27319] accumulated_eval_time=2320.938893, accumulated_logging_time=0.965811, accumulated_submission_time=23059.879019, global_step=27319, preemption_count=0, score=23059.879019, test/ctc_loss=0.30934038758277893, test/num_examples=2472, test/wer=0.098410, total_duration=25383.035181, train/ctc_loss=0.42835095524787903, train/wer=0.141893, validation/ctc_loss=0.5401890277862549, validation/num_examples=5348, validation/wer=0.157120
I0212 08:17:05.623928 140285415180032 logging_writer.py:48] [27400] global_step=27400, grad_norm=2.0987491607666016, loss=1.4013246297836304
I0212 08:18:20.474462 140285423572736 logging_writer.py:48] [27500] global_step=27500, grad_norm=2.8840019702911377, loss=1.3601478338241577
I0212 08:19:46.097341 140285415180032 logging_writer.py:48] [27600] global_step=27600, grad_norm=2.196211099624634, loss=1.3998132944107056
I0212 08:21:14.187827 140285423572736 logging_writer.py:48] [27700] global_step=27700, grad_norm=2.655042886734009, loss=1.4160962104797363
I0212 08:22:45.795849 140285415180032 logging_writer.py:48] [27800] global_step=27800, grad_norm=2.2320144176483154, loss=1.3754044771194458
I0212 08:24:08.172741 140285423572736 logging_writer.py:48] [27900] global_step=27900, grad_norm=2.687901020050049, loss=1.3713186979293823
I0212 08:25:26.162792 140285415180032 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.8180116415023804, loss=1.3473212718963623
I0212 08:26:46.674286 140285423572736 logging_writer.py:48] [28100] global_step=28100, grad_norm=2.263378381729126, loss=1.3162866830825806
I0212 08:28:10.943427 140285415180032 logging_writer.py:48] [28200] global_step=28200, grad_norm=2.0051915645599365, loss=1.3629748821258545
I0212 08:29:39.757510 140285423572736 logging_writer.py:48] [28300] global_step=28300, grad_norm=2.8222877979278564, loss=1.3483446836471558
I0212 08:31:07.528747 140285415180032 logging_writer.py:48] [28400] global_step=28400, grad_norm=2.494786500930786, loss=1.3654125928878784
I0212 08:32:34.514941 140285423572736 logging_writer.py:48] [28500] global_step=28500, grad_norm=2.8263115882873535, loss=1.3383467197418213
I0212 08:34:05.684663 140285415180032 logging_writer.py:48] [28600] global_step=28600, grad_norm=2.82696270942688, loss=1.2806428670883179
I0212 08:35:33.276846 140285423572736 logging_writer.py:48] [28700] global_step=28700, grad_norm=2.4755945205688477, loss=1.3265562057495117
I0212 08:37:02.363473 140285415180032 logging_writer.py:48] [28800] global_step=28800, grad_norm=2.0690877437591553, loss=1.307905912399292
I0212 08:38:26.529294 140285423572736 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.864668369293213, loss=1.3495370149612427
I0212 08:39:42.457297 140285415180032 logging_writer.py:48] [29000] global_step=29000, grad_norm=2.603630542755127, loss=1.335649013519287
I0212 08:40:04.820414 140441227016000 spec.py:321] Evaluating on the training split.
I0212 08:40:57.391093 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 08:41:50.020496 140441227016000 spec.py:349] Evaluating on the test split.
I0212 08:42:18.052115 140441227016000 submission_runner.py:408] Time since start: 26956.90s, 	Step: 29031, 	{'train/ctc_loss': Array(0.3626228, dtype=float32), 'train/wer': 0.11829440075126188, 'validation/ctc_loss': Array(0.52205914, dtype=float32), 'validation/wer': 0.1520800950017861, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29937524, dtype=float32), 'test/wer': 0.09499725793674973, 'test/num_examples': 2472, 'score': 24500.383773565292, 'total_duration': 26956.89998602867, 'accumulated_submission_time': 24500.383773565292, 'accumulated_eval_time': 2454.1646745204926, 'accumulated_logging_time': 1.0208978652954102}
I0212 08:42:18.094396 140285423572736 logging_writer.py:48] [29031] accumulated_eval_time=2454.164675, accumulated_logging_time=1.020898, accumulated_submission_time=24500.383774, global_step=29031, preemption_count=0, score=24500.383774, test/ctc_loss=0.2993752360343933, test/num_examples=2472, test/wer=0.094997, total_duration=26956.899986, train/ctc_loss=0.36262279748916626, train/wer=0.118294, validation/ctc_loss=0.5220591425895691, validation/num_examples=5348, validation/wer=0.152080
I0212 08:43:10.425468 140285415180032 logging_writer.py:48] [29100] global_step=29100, grad_norm=2.5671584606170654, loss=1.3006638288497925
I0212 08:44:25.553227 140285423572736 logging_writer.py:48] [29200] global_step=29200, grad_norm=2.5379061698913574, loss=1.332748532295227
I0212 08:45:41.629617 140285415180032 logging_writer.py:48] [29300] global_step=29300, grad_norm=2.0504348278045654, loss=1.3788111209869385
I0212 08:47:08.648594 140285423572736 logging_writer.py:48] [29400] global_step=29400, grad_norm=2.4222278594970703, loss=1.3208147287368774
I0212 08:48:37.271196 140285415180032 logging_writer.py:48] [29500] global_step=29500, grad_norm=2.801496982574463, loss=1.3606092929840088
I0212 08:50:05.021887 140285423572736 logging_writer.py:48] [29600] global_step=29600, grad_norm=2.7708802223205566, loss=1.3374102115631104
I0212 08:51:35.912887 140285415180032 logging_writer.py:48] [29700] global_step=29700, grad_norm=2.5482120513916016, loss=1.3307442665100098
I0212 08:53:05.844532 140285423572736 logging_writer.py:48] [29800] global_step=29800, grad_norm=2.036999225616455, loss=1.3353209495544434
I0212 08:54:36.052053 140285423572736 logging_writer.py:48] [29900] global_step=29900, grad_norm=2.1546473503112793, loss=1.3560314178466797
I0212 08:55:51.863113 140285415180032 logging_writer.py:48] [30000] global_step=30000, grad_norm=3.477787733078003, loss=1.3351223468780518
I0212 08:57:10.558143 140285423572736 logging_writer.py:48] [30100] global_step=30100, grad_norm=2.228153705596924, loss=1.2612043619155884
I0212 08:58:32.670191 140285415180032 logging_writer.py:48] [30200] global_step=30200, grad_norm=2.6762287616729736, loss=1.3209481239318848
I0212 09:00:00.330456 140285423572736 logging_writer.py:48] [30300] global_step=30300, grad_norm=3.470054864883423, loss=1.332438588142395
I0212 09:01:29.507968 140285415180032 logging_writer.py:48] [30400] global_step=30400, grad_norm=2.1648123264312744, loss=1.3333475589752197
I0212 09:02:54.058861 140285423572736 logging_writer.py:48] [30500] global_step=30500, grad_norm=2.1422364711761475, loss=1.3001556396484375
I0212 09:04:24.291615 140285415180032 logging_writer.py:48] [30600] global_step=30600, grad_norm=2.343317747116089, loss=1.3344637155532837
I0212 09:05:54.458376 140285423572736 logging_writer.py:48] [30700] global_step=30700, grad_norm=2.341385841369629, loss=1.2512569427490234
I0212 09:06:19.049530 140441227016000 spec.py:321] Evaluating on the training split.
I0212 09:07:14.120635 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 09:08:07.450912 140441227016000 spec.py:349] Evaluating on the test split.
I0212 09:08:34.053603 140441227016000 submission_runner.py:408] Time since start: 28532.90s, 	Step: 30729, 	{'train/ctc_loss': Array(0.32588735, dtype=float32), 'train/wer': 0.11000286582864592, 'validation/ctc_loss': Array(0.50159395, dtype=float32), 'validation/wer': 0.14702105679832395, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28334433, dtype=float32), 'test/wer': 0.0904880872585461, 'test/num_examples': 2472, 'score': 25941.25056886673, 'total_duration': 28532.90132164955, 'accumulated_submission_time': 25941.25056886673, 'accumulated_eval_time': 2589.162698030472, 'accumulated_logging_time': 1.0792968273162842}
I0212 09:08:34.089205 140285423572736 logging_writer.py:48] [30729] accumulated_eval_time=2589.162698, accumulated_logging_time=1.079297, accumulated_submission_time=25941.250569, global_step=30729, preemption_count=0, score=25941.250569, test/ctc_loss=0.2833443284034729, test/num_examples=2472, test/wer=0.090488, total_duration=28532.901322, train/ctc_loss=0.3258873522281647, train/wer=0.110003, validation/ctc_loss=0.5015939474105835, validation/num_examples=5348, validation/wer=0.147021
I0212 09:09:28.137404 140285415180032 logging_writer.py:48] [30800] global_step=30800, grad_norm=2.946688652038574, loss=1.272958755493164
I0212 09:10:46.474144 140285423572736 logging_writer.py:48] [30900] global_step=30900, grad_norm=2.125352144241333, loss=1.3459324836730957
I0212 09:12:05.157484 140285415180032 logging_writer.py:48] [31000] global_step=31000, grad_norm=3.571402072906494, loss=1.3576689958572388
I0212 09:13:25.975787 140285423572736 logging_writer.py:48] [31100] global_step=31100, grad_norm=2.7354156970977783, loss=1.220516324043274
I0212 09:14:44.813829 140285415180032 logging_writer.py:48] [31200] global_step=31200, grad_norm=2.6232800483703613, loss=1.3327816724777222
I0212 09:16:07.630415 140285423572736 logging_writer.py:48] [31300] global_step=31300, grad_norm=2.1224474906921387, loss=1.301736831665039
I0212 09:17:36.313498 140285415180032 logging_writer.py:48] [31400] global_step=31400, grad_norm=2.055082321166992, loss=1.2792872190475464
I0212 09:19:02.079536 140285423572736 logging_writer.py:48] [31500] global_step=31500, grad_norm=2.590282440185547, loss=1.3229317665100098
I0212 09:20:30.918313 140285415180032 logging_writer.py:48] [31600] global_step=31600, grad_norm=2.1821720600128174, loss=1.263895034790039
I0212 09:21:59.027101 140285423572736 logging_writer.py:48] [31700] global_step=31700, grad_norm=2.135241985321045, loss=1.3458292484283447
I0212 09:23:26.542682 140285415180032 logging_writer.py:48] [31800] global_step=31800, grad_norm=2.142158031463623, loss=1.2591040134429932
I0212 09:24:57.991401 140285423572736 logging_writer.py:48] [31900] global_step=31900, grad_norm=2.4013640880584717, loss=1.2997610569000244
I0212 09:26:22.679058 140285423572736 logging_writer.py:48] [32000] global_step=32000, grad_norm=3.341167688369751, loss=1.252579927444458
I0212 09:27:39.880591 140285415180032 logging_writer.py:48] [32100] global_step=32100, grad_norm=3.132138967514038, loss=1.317529320716858
I0212 09:28:58.694684 140285423572736 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.928404450416565, loss=1.2711156606674194
I0212 09:30:21.753880 140285415180032 logging_writer.py:48] [32300] global_step=32300, grad_norm=2.7926905155181885, loss=1.3558510541915894
I0212 09:31:48.645926 140285423572736 logging_writer.py:48] [32400] global_step=32400, grad_norm=9.101859092712402, loss=1.2800108194351196
I0212 09:32:34.261550 140441227016000 spec.py:321] Evaluating on the training split.
I0212 09:33:29.501066 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 09:34:24.389930 140441227016000 spec.py:349] Evaluating on the test split.
I0212 09:34:51.376435 140441227016000 submission_runner.py:408] Time since start: 30110.22s, 	Step: 32452, 	{'train/ctc_loss': Array(0.2819919, dtype=float32), 'train/wer': 0.0966568077732058, 'validation/ctc_loss': Array(0.48701313, dtype=float32), 'validation/wer': 0.14218407561524277, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27825376, dtype=float32), 'test/wer': 0.0895131314362318, 'test/num_examples': 2472, 'score': 27381.333554029465, 'total_duration': 30110.221838235855, 'accumulated_submission_time': 27381.333554029465, 'accumulated_eval_time': 2726.2692017555237, 'accumulated_logging_time': 1.1319363117218018}
I0212 09:34:51.415977 140285423572736 logging_writer.py:48] [32452] accumulated_eval_time=2726.269202, accumulated_logging_time=1.131936, accumulated_submission_time=27381.333554, global_step=32452, preemption_count=0, score=27381.333554, test/ctc_loss=0.2782537639141083, test/num_examples=2472, test/wer=0.089513, total_duration=30110.221838, train/ctc_loss=0.2819918990135193, train/wer=0.096657, validation/ctc_loss=0.4870131313800812, validation/num_examples=5348, validation/wer=0.142184
I0212 09:35:28.114230 140285415180032 logging_writer.py:48] [32500] global_step=32500, grad_norm=2.2394466400146484, loss=1.2871243953704834
I0212 09:36:43.416911 140285423572736 logging_writer.py:48] [32600] global_step=32600, grad_norm=2.5371320247650146, loss=1.3573429584503174
I0212 09:38:02.729947 140285415180032 logging_writer.py:48] [32700] global_step=32700, grad_norm=2.479426145553589, loss=1.235788345336914
I0212 09:39:30.187411 140285423572736 logging_writer.py:48] [32800] global_step=32800, grad_norm=2.2331795692443848, loss=1.290962815284729
I0212 09:40:58.256799 140285415180032 logging_writer.py:48] [32900] global_step=32900, grad_norm=2.7541775703430176, loss=1.3392747640609741
I0212 09:42:26.458019 140285423572736 logging_writer.py:48] [33000] global_step=33000, grad_norm=2.0320563316345215, loss=1.2950007915496826
I0212 09:43:43.459965 140285415180032 logging_writer.py:48] [33100] global_step=33100, grad_norm=2.168468952178955, loss=1.2806472778320312
I0212 09:45:01.706638 140285423572736 logging_writer.py:48] [33200] global_step=33200, grad_norm=3.6306166648864746, loss=1.3035023212432861
I0212 09:46:27.007839 140285415180032 logging_writer.py:48] [33300] global_step=33300, grad_norm=2.379498243331909, loss=1.2447043657302856
I0212 09:47:54.400529 140285423572736 logging_writer.py:48] [33400] global_step=33400, grad_norm=5.509018898010254, loss=1.2908214330673218
I0212 09:49:21.608730 140285415180032 logging_writer.py:48] [33500] global_step=33500, grad_norm=2.0964269638061523, loss=1.3497931957244873
I0212 09:50:52.047670 140285423572736 logging_writer.py:48] [33600] global_step=33600, grad_norm=2.085123300552368, loss=1.2400727272033691
I0212 09:52:21.204416 140285415180032 logging_writer.py:48] [33700] global_step=33700, grad_norm=2.1239593029022217, loss=1.279632568359375
I0212 09:53:46.273682 140285423572736 logging_writer.py:48] [33800] global_step=33800, grad_norm=3.2931430339813232, loss=1.28944993019104
I0212 09:55:18.049347 140285415180032 logging_writer.py:48] [33900] global_step=33900, grad_norm=3.372675657272339, loss=1.2488797903060913
I0212 09:56:50.858697 140285423572736 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.966384768486023, loss=1.2508139610290527
I0212 09:58:09.404052 140285415180032 logging_writer.py:48] [34100] global_step=34100, grad_norm=2.3492705821990967, loss=1.2974404096603394
I0212 09:58:51.984835 140441227016000 spec.py:321] Evaluating on the training split.
I0212 09:59:46.733901 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 10:00:40.621920 140441227016000 spec.py:349] Evaluating on the test split.
I0212 10:01:07.591747 140441227016000 submission_runner.py:408] Time since start: 31686.44s, 	Step: 34156, 	{'train/ctc_loss': Array(0.30927896, dtype=float32), 'train/wer': 0.10505333653279815, 'validation/ctc_loss': Array(0.46904442, dtype=float32), 'validation/wer': 0.13654575822817808, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26531848, dtype=float32), 'test/wer': 0.08522738813397518, 'test/num_examples': 2472, 'score': 28821.81571483612, 'total_duration': 31686.440123796463, 'accumulated_submission_time': 28821.81571483612, 'accumulated_eval_time': 2861.870703935623, 'accumulated_logging_time': 1.1866295337677002}
I0212 10:01:07.629848 140285423572736 logging_writer.py:48] [34156] accumulated_eval_time=2861.870704, accumulated_logging_time=1.186630, accumulated_submission_time=28821.815715, global_step=34156, preemption_count=0, score=28821.815715, test/ctc_loss=0.26531848311424255, test/num_examples=2472, test/wer=0.085227, total_duration=31686.440124, train/ctc_loss=0.3092789649963379, train/wer=0.105053, validation/ctc_loss=0.46904441714286804, validation/num_examples=5348, validation/wer=0.136546
I0212 10:01:41.359227 140285415180032 logging_writer.py:48] [34200] global_step=34200, grad_norm=2.4755494594573975, loss=1.2209031581878662
I0212 10:02:56.052515 140285423572736 logging_writer.py:48] [34300] global_step=34300, grad_norm=2.8029286861419678, loss=1.2678018808364868
I0212 10:04:11.436632 140285415180032 logging_writer.py:48] [34400] global_step=34400, grad_norm=4.068766117095947, loss=1.2623658180236816
I0212 10:05:38.140597 140285423572736 logging_writer.py:48] [34500] global_step=34500, grad_norm=2.2873637676239014, loss=1.2264225482940674
I0212 10:07:05.868518 140285415180032 logging_writer.py:48] [34600] global_step=34600, grad_norm=2.9698829650878906, loss=1.2919718027114868
I0212 10:08:36.090163 140285423572736 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.6533362865447998, loss=1.293969750404358
I0212 10:10:08.867338 140285415180032 logging_writer.py:48] [34800] global_step=34800, grad_norm=2.07061505317688, loss=1.205317735671997
I0212 10:11:36.227944 140285423572736 logging_writer.py:48] [34900] global_step=34900, grad_norm=3.246622323989868, loss=1.2557934522628784
I0212 10:13:04.225942 140285415180032 logging_writer.py:48] [35000] global_step=35000, grad_norm=2.275735855102539, loss=1.2067450284957886
I0212 10:14:26.414793 140285423572736 logging_writer.py:48] [35100] global_step=35100, grad_norm=2.378070831298828, loss=1.2225885391235352
I0212 10:15:45.126695 140285415180032 logging_writer.py:48] [35200] global_step=35200, grad_norm=2.2230687141418457, loss=1.2665609121322632
I0212 10:17:06.739321 140285423572736 logging_writer.py:48] [35300] global_step=35300, grad_norm=3.051226854324341, loss=1.2673306465148926
I0212 10:18:28.743373 140285415180032 logging_writer.py:48] [35400] global_step=35400, grad_norm=2.824549913406372, loss=1.261975646018982
I0212 10:19:54.007255 140285423572736 logging_writer.py:48] [35500] global_step=35500, grad_norm=3.506507396697998, loss=1.2659014463424683
I0212 10:21:23.306895 140285415180032 logging_writer.py:48] [35600] global_step=35600, grad_norm=2.495741605758667, loss=1.252963900566101
I0212 10:22:53.346043 140285423572736 logging_writer.py:48] [35700] global_step=35700, grad_norm=3.841695785522461, loss=1.2709434032440186
I0212 10:24:24.048135 140285415180032 logging_writer.py:48] [35800] global_step=35800, grad_norm=2.1403956413269043, loss=1.2479907274246216
I0212 10:25:08.223771 140441227016000 spec.py:321] Evaluating on the training split.
I0212 10:26:05.231013 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 10:26:58.043724 140441227016000 spec.py:349] Evaluating on the test split.
I0212 10:27:24.697611 140441227016000 submission_runner.py:408] Time since start: 33263.54s, 	Step: 35850, 	{'train/ctc_loss': Array(0.26929548, dtype=float32), 'train/wer': 0.09210244893594267, 'validation/ctc_loss': Array(0.45808133, dtype=float32), 'validation/wer': 0.13422864149376793, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25412038, dtype=float32), 'test/wer': 0.08144943432250726, 'test/num_examples': 2472, 'score': 30262.3197350502, 'total_duration': 33263.54372572899, 'accumulated_submission_time': 30262.3197350502, 'accumulated_eval_time': 2998.3368847370148, 'accumulated_logging_time': 1.2433674335479736}
I0212 10:27:24.741908 140285423572736 logging_writer.py:48] [35850] accumulated_eval_time=2998.336885, accumulated_logging_time=1.243367, accumulated_submission_time=30262.319735, global_step=35850, preemption_count=0, score=30262.319735, test/ctc_loss=0.2541203796863556, test/num_examples=2472, test/wer=0.081449, total_duration=33263.543726, train/ctc_loss=0.26929548382759094, train/wer=0.092102, validation/ctc_loss=0.45808133482933044, validation/num_examples=5348, validation/wer=0.134229
I0212 10:28:02.827180 140285415180032 logging_writer.py:48] [35900] global_step=35900, grad_norm=2.3938488960266113, loss=1.2082772254943848
I0212 10:29:18.635121 140285423572736 logging_writer.py:48] [36000] global_step=36000, grad_norm=2.4258015155792236, loss=1.2456541061401367
I0212 10:30:38.757174 140285423572736 logging_writer.py:48] [36100] global_step=36100, grad_norm=3.498396158218384, loss=1.2173993587493896
I0212 10:31:56.283049 140285415180032 logging_writer.py:48] [36200] global_step=36200, grad_norm=2.428617238998413, loss=1.212937831878662
I0212 10:33:15.626631 140285423572736 logging_writer.py:48] [36300] global_step=36300, grad_norm=3.118425130844116, loss=1.2185379266738892
I0212 10:34:39.807019 140285415180032 logging_writer.py:48] [36400] global_step=36400, grad_norm=3.599191665649414, loss=1.2746871709823608
I0212 10:36:04.564200 140285423572736 logging_writer.py:48] [36500] global_step=36500, grad_norm=2.939925193786621, loss=1.2192145586013794
I0212 10:37:31.602286 140285415180032 logging_writer.py:48] [36600] global_step=36600, grad_norm=3.3523902893066406, loss=1.2475162744522095
I0212 10:38:59.634865 140285423572736 logging_writer.py:48] [36700] global_step=36700, grad_norm=2.813068389892578, loss=1.2276661396026611
I0212 10:40:29.354475 140285415180032 logging_writer.py:48] [36800] global_step=36800, grad_norm=2.8824214935302734, loss=1.2578009366989136
I0212 10:41:59.335473 140285423572736 logging_writer.py:48] [36900] global_step=36900, grad_norm=2.207986831665039, loss=1.246771216392517
I0212 10:43:29.715692 140285415180032 logging_writer.py:48] [37000] global_step=37000, grad_norm=2.7277297973632812, loss=1.1357498168945312
I0212 10:45:01.993493 140285423572736 logging_writer.py:48] [37100] global_step=37100, grad_norm=2.55743145942688, loss=1.2106066942214966
I0212 10:46:20.628925 140285415180032 logging_writer.py:48] [37200] global_step=37200, grad_norm=6.005654811859131, loss=1.2369030714035034
I0212 10:47:38.631529 140285423572736 logging_writer.py:48] [37300] global_step=37300, grad_norm=2.404818058013916, loss=1.261654019355774
I0212 10:49:01.314439 140285415180032 logging_writer.py:48] [37400] global_step=37400, grad_norm=2.7008650302886963, loss=1.1680797338485718
I0212 10:50:24.448998 140285423572736 logging_writer.py:48] [37500] global_step=37500, grad_norm=2.8916211128234863, loss=1.2228296995162964
I0212 10:51:24.793124 140441227016000 spec.py:321] Evaluating on the training split.
I0212 10:52:19.626899 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 10:53:12.006765 140441227016000 spec.py:349] Evaluating on the test split.
I0212 10:53:39.851201 140441227016000 submission_runner.py:408] Time since start: 34838.70s, 	Step: 37571, 	{'train/ctc_loss': Array(0.2542912, dtype=float32), 'train/wer': 0.08848222020038028, 'validation/ctc_loss': Array(0.44569063, dtype=float32), 'validation/wer': 0.13101364202477384, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24617566, dtype=float32), 'test/wer': 0.07828082789998578, 'test/num_examples': 2472, 'score': 31702.28162097931, 'total_duration': 34838.69946312904, 'accumulated_submission_time': 31702.28162097931, 'accumulated_eval_time': 3133.3894832134247, 'accumulated_logging_time': 1.3041977882385254}
I0212 10:53:39.887014 140285423572736 logging_writer.py:48] [37571] accumulated_eval_time=3133.389483, accumulated_logging_time=1.304198, accumulated_submission_time=31702.281621, global_step=37571, preemption_count=0, score=31702.281621, test/ctc_loss=0.24617566168308258, test/num_examples=2472, test/wer=0.078281, total_duration=34838.699463, train/ctc_loss=0.25429120659828186, train/wer=0.088482, validation/ctc_loss=0.4456906318664551, validation/num_examples=5348, validation/wer=0.131014
I0212 10:54:02.686595 140285415180032 logging_writer.py:48] [37600] global_step=37600, grad_norm=2.6769449710845947, loss=1.1863632202148438
I0212 10:55:18.306981 140285423572736 logging_writer.py:48] [37700] global_step=37700, grad_norm=2.60184645652771, loss=1.2253696918487549
I0212 10:56:33.952247 140285415180032 logging_writer.py:48] [37800] global_step=37800, grad_norm=2.7679576873779297, loss=1.1900485754013062
I0212 10:58:03.852583 140285423572736 logging_writer.py:48] [37900] global_step=37900, grad_norm=3.734267473220825, loss=1.2676925659179688
I0212 10:59:32.049056 140285415180032 logging_writer.py:48] [38000] global_step=38000, grad_norm=3.269280433654785, loss=1.2387518882751465
I0212 11:01:01.665165 140285423572736 logging_writer.py:48] [38100] global_step=38100, grad_norm=3.033831834793091, loss=1.1927729845046997
I0212 11:02:24.306941 140285423572736 logging_writer.py:48] [38200] global_step=38200, grad_norm=2.4304845333099365, loss=1.2246798276901245
I0212 11:03:40.346719 140285415180032 logging_writer.py:48] [38300] global_step=38300, grad_norm=2.728959083557129, loss=1.2183845043182373
I0212 11:05:01.635706 140285423572736 logging_writer.py:48] [38400] global_step=38400, grad_norm=2.1525912284851074, loss=1.1580957174301147
I0212 11:06:25.091164 140285415180032 logging_writer.py:48] [38500] global_step=38500, grad_norm=2.343548536300659, loss=1.2113004922866821
I0212 11:07:54.588264 140285423572736 logging_writer.py:48] [38600] global_step=38600, grad_norm=4.152379512786865, loss=1.2208415269851685
I0212 11:09:25.426901 140285415180032 logging_writer.py:48] [38700] global_step=38700, grad_norm=4.089395999908447, loss=1.2028930187225342
I0212 11:10:54.998913 140285423572736 logging_writer.py:48] [38800] global_step=38800, grad_norm=3.5341362953186035, loss=1.1746325492858887
I0212 11:12:25.880113 140285415180032 logging_writer.py:48] [38900] global_step=38900, grad_norm=4.60200309753418, loss=1.2076817750930786
I0212 11:13:56.887965 140285423572736 logging_writer.py:48] [39000] global_step=39000, grad_norm=2.6622347831726074, loss=1.1892259120941162
I0212 11:15:27.945021 140285415180032 logging_writer.py:48] [39100] global_step=39100, grad_norm=6.2077836990356445, loss=1.1821234226226807
I0212 11:16:53.220443 140285423572736 logging_writer.py:48] [39200] global_step=39200, grad_norm=2.5431294441223145, loss=1.1517534255981445
I0212 11:17:40.533190 140441227016000 spec.py:321] Evaluating on the training split.
I0212 11:18:35.529873 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 11:19:27.828362 140441227016000 spec.py:349] Evaluating on the test split.
I0212 11:19:55.448812 140441227016000 submission_runner.py:408] Time since start: 36414.30s, 	Step: 39261, 	{'train/ctc_loss': Array(0.2523249, dtype=float32), 'train/wer': 0.08504993586721774, 'validation/ctc_loss': Array(0.4367461, dtype=float32), 'validation/wer': 0.12747038435173833, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2394587, dtype=float32), 'test/wer': 0.07596530782198932, 'test/num_examples': 2472, 'score': 33142.84040021896, 'total_duration': 36414.29718565941, 'accumulated_submission_time': 33142.84040021896, 'accumulated_eval_time': 3268.299718618393, 'accumulated_logging_time': 1.3554682731628418}
I0212 11:19:55.488474 140285423572736 logging_writer.py:48] [39261] accumulated_eval_time=3268.299719, accumulated_logging_time=1.355468, accumulated_submission_time=33142.840400, global_step=39261, preemption_count=0, score=33142.840400, test/ctc_loss=0.23945869505405426, test/num_examples=2472, test/wer=0.075965, total_duration=36414.297186, train/ctc_loss=0.2523249089717865, train/wer=0.085050, validation/ctc_loss=0.43674609065055847, validation/num_examples=5348, validation/wer=0.127470
I0212 11:20:25.881850 140285415180032 logging_writer.py:48] [39300] global_step=39300, grad_norm=3.363769769668579, loss=1.1624674797058105
I0212 11:21:40.810866 140285423572736 logging_writer.py:48] [39400] global_step=39400, grad_norm=2.7029590606689453, loss=1.1779531240463257
I0212 11:22:56.055154 140285415180032 logging_writer.py:48] [39500] global_step=39500, grad_norm=2.6333467960357666, loss=1.1691664457321167
I0212 11:24:14.709359 140285423572736 logging_writer.py:48] [39600] global_step=39600, grad_norm=5.344745635986328, loss=1.1342647075653076
I0212 11:25:43.128759 140285415180032 logging_writer.py:48] [39700] global_step=39700, grad_norm=2.48860764503479, loss=1.1916815042495728
I0212 11:27:11.283202 140285423572736 logging_writer.py:48] [39800] global_step=39800, grad_norm=2.1342580318450928, loss=1.2049751281738281
I0212 11:28:40.032615 140285415180032 logging_writer.py:48] [39900] global_step=39900, grad_norm=2.787911891937256, loss=1.1629165410995483
I0212 11:30:09.845302 140285423572736 logging_writer.py:48] [40000] global_step=40000, grad_norm=2.773000955581665, loss=1.1757800579071045
I0212 11:31:39.171087 140285415180032 logging_writer.py:48] [40100] global_step=40100, grad_norm=3.6937215328216553, loss=1.222791075706482
I0212 11:33:07.864477 140285423572736 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.9554990530014038, loss=1.1341300010681152
I0212 11:34:26.089375 140285415180032 logging_writer.py:48] [40300] global_step=40300, grad_norm=2.481384515762329, loss=1.1384202241897583
I0212 11:35:43.470510 140285423572736 logging_writer.py:48] [40400] global_step=40400, grad_norm=2.2996397018432617, loss=1.1373157501220703
I0212 11:37:05.209574 140285415180032 logging_writer.py:48] [40500] global_step=40500, grad_norm=4.453646183013916, loss=1.1673593521118164
I0212 11:38:28.849048 140285423572736 logging_writer.py:48] [40600] global_step=40600, grad_norm=2.498249053955078, loss=1.1188833713531494
I0212 11:39:59.311820 140285415180032 logging_writer.py:48] [40700] global_step=40700, grad_norm=3.9420716762542725, loss=1.1813700199127197
I0212 11:41:28.724402 140285423572736 logging_writer.py:48] [40800] global_step=40800, grad_norm=2.3730862140655518, loss=1.1516119241714478
I0212 11:42:54.544549 140285415180032 logging_writer.py:48] [40900] global_step=40900, grad_norm=3.1328420639038086, loss=1.1614995002746582
I0212 11:43:56.439833 140441227016000 spec.py:321] Evaluating on the training split.
I0212 11:44:53.179027 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 11:45:45.884375 140441227016000 spec.py:349] Evaluating on the test split.
I0212 11:46:14.095926 140441227016000 submission_runner.py:408] Time since start: 37992.94s, 	Step: 40968, 	{'train/ctc_loss': Array(0.24387786, dtype=float32), 'train/wer': 0.08422291768353958, 'validation/ctc_loss': Array(0.42268944, dtype=float32), 'validation/wer': 0.12450640586230534, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23126832, dtype=float32), 'test/wer': 0.07344667194767737, 'test/num_examples': 2472, 'score': 34583.702951192856, 'total_duration': 37992.941672325134, 'accumulated_submission_time': 34583.702951192856, 'accumulated_eval_time': 3405.9477915763855, 'accumulated_logging_time': 1.4116151332855225}
I0212 11:46:14.136821 140285423572736 logging_writer.py:48] [40968] accumulated_eval_time=3405.947792, accumulated_logging_time=1.411615, accumulated_submission_time=34583.702951, global_step=40968, preemption_count=0, score=34583.702951, test/ctc_loss=0.23126831650733948, test/num_examples=2472, test/wer=0.073447, total_duration=37992.941672, train/ctc_loss=0.2438778579235077, train/wer=0.084223, validation/ctc_loss=0.42268943786621094, validation/num_examples=5348, validation/wer=0.124506
I0212 11:46:38.831391 140285415180032 logging_writer.py:48] [41000] global_step=41000, grad_norm=2.5212833881378174, loss=1.1580216884613037
I0212 11:47:53.687822 140285423572736 logging_writer.py:48] [41100] global_step=41100, grad_norm=3.903674364089966, loss=1.1450860500335693
I0212 11:49:15.290391 140285423572736 logging_writer.py:48] [41200] global_step=41200, grad_norm=2.7934510707855225, loss=1.0988558530807495
I0212 11:50:32.018089 140285415180032 logging_writer.py:48] [41300] global_step=41300, grad_norm=3.676852226257324, loss=1.1800252199172974
I0212 11:51:48.119860 140285423572736 logging_writer.py:48] [41400] global_step=41400, grad_norm=4.055246829986572, loss=1.1632297039031982
I0212 11:53:08.011857 140285415180032 logging_writer.py:48] [41500] global_step=41500, grad_norm=2.575648307800293, loss=1.1443626880645752
I0212 11:54:31.490094 140285423572736 logging_writer.py:48] [41600] global_step=41600, grad_norm=2.424677848815918, loss=1.1228930950164795
I0212 11:55:59.111975 140285415180032 logging_writer.py:48] [41700] global_step=41700, grad_norm=2.4690117835998535, loss=1.1575441360473633
I0212 11:57:28.710283 140285423572736 logging_writer.py:48] [41800] global_step=41800, grad_norm=2.7376394271850586, loss=1.1487244367599487
I0212 11:59:01.315855 140285415180032 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.9773505926132202, loss=1.1544313430786133
I0212 12:00:31.803298 140285423572736 logging_writer.py:48] [42000] global_step=42000, grad_norm=3.580573081970215, loss=1.147277593612671
I0212 12:02:04.310526 140285415180032 logging_writer.py:48] [42100] global_step=42100, grad_norm=2.369506359100342, loss=1.1209027767181396
I0212 12:03:35.106294 140285423572736 logging_writer.py:48] [42200] global_step=42200, grad_norm=2.169464111328125, loss=1.1565227508544922
I0212 12:04:57.014197 140285423572736 logging_writer.py:48] [42300] global_step=42300, grad_norm=4.054210662841797, loss=1.1938446760177612
I0212 12:06:15.392725 140285415180032 logging_writer.py:48] [42400] global_step=42400, grad_norm=3.2134957313537598, loss=1.165906310081482
I0212 12:07:35.178932 140285423572736 logging_writer.py:48] [42500] global_step=42500, grad_norm=3.0182507038116455, loss=1.0917549133300781
I0212 12:08:56.110925 140285415180032 logging_writer.py:48] [42600] global_step=42600, grad_norm=2.3448312282562256, loss=1.185997724533081
I0212 12:10:14.235224 140441227016000 spec.py:321] Evaluating on the training split.
I0212 12:11:08.487543 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 12:12:00.781431 140441227016000 spec.py:349] Evaluating on the test split.
I0212 12:12:27.573957 140441227016000 submission_runner.py:408] Time since start: 39566.42s, 	Step: 42693, 	{'train/ctc_loss': Array(0.24088885, dtype=float32), 'train/wer': 0.08084833819656964, 'validation/ctc_loss': Array(0.41713747, dtype=float32), 'validation/wer': 0.12214101586259497, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22636868, dtype=float32), 'test/wer': 0.07241078138646842, 'test/num_examples': 2472, 'score': 36023.71224784851, 'total_duration': 39566.42193007469, 'accumulated_submission_time': 36023.71224784851, 'accumulated_eval_time': 3539.2807710170746, 'accumulated_logging_time': 1.4675979614257812}
I0212 12:12:27.612710 140285423572736 logging_writer.py:48] [42693] accumulated_eval_time=3539.280771, accumulated_logging_time=1.467598, accumulated_submission_time=36023.712248, global_step=42693, preemption_count=0, score=36023.712248, test/ctc_loss=0.22636868059635162, test/num_examples=2472, test/wer=0.072411, total_duration=39566.421930, train/ctc_loss=0.24088884890079498, train/wer=0.080848, validation/ctc_loss=0.41713747382164, validation/num_examples=5348, validation/wer=0.122141
I0212 12:12:33.869299 140285415180032 logging_writer.py:48] [42700] global_step=42700, grad_norm=2.4004693031311035, loss=1.1309664249420166
I0212 12:13:49.770012 140285423572736 logging_writer.py:48] [42800] global_step=42800, grad_norm=2.2218375205993652, loss=1.1582013368606567
I0212 12:15:04.921244 140285415180032 logging_writer.py:48] [42900] global_step=42900, grad_norm=3.3266677856445312, loss=1.1080992221832275
I0212 12:16:29.691737 140285423572736 logging_writer.py:48] [43000] global_step=43000, grad_norm=2.7350075244903564, loss=1.1253079175949097
I0212 12:17:57.669834 140285415180032 logging_writer.py:48] [43100] global_step=43100, grad_norm=2.2343695163726807, loss=1.146173119544983
I0212 12:19:26.496584 140285423572736 logging_writer.py:48] [43200] global_step=43200, grad_norm=2.794116497039795, loss=1.117661476135254
I0212 12:20:54.191221 140285423572736 logging_writer.py:48] [43300] global_step=43300, grad_norm=2.375873327255249, loss=1.0347769260406494
I0212 12:22:12.680660 140285415180032 logging_writer.py:48] [43400] global_step=43400, grad_norm=3.121337413787842, loss=1.1674988269805908
I0212 12:23:30.904315 140285423572736 logging_writer.py:48] [43500] global_step=43500, grad_norm=2.3518645763397217, loss=1.1216402053833008
I0212 12:24:52.281104 140285415180032 logging_writer.py:48] [43600] global_step=43600, grad_norm=3.0689685344696045, loss=1.1757020950317383
I0212 12:26:20.201273 140285423572736 logging_writer.py:48] [43700] global_step=43700, grad_norm=6.467021465301514, loss=1.142026662826538
I0212 12:27:48.778571 140285415180032 logging_writer.py:48] [43800] global_step=43800, grad_norm=2.0571963787078857, loss=1.126906394958496
I0212 12:29:18.001245 140285423572736 logging_writer.py:48] [43900] global_step=43900, grad_norm=2.32497501373291, loss=1.0611400604248047
I0212 12:30:47.140338 140285415180032 logging_writer.py:48] [44000] global_step=44000, grad_norm=3.303392171859741, loss=1.0653502941131592
I0212 12:32:17.691825 140285423572736 logging_writer.py:48] [44100] global_step=44100, grad_norm=3.565037727355957, loss=1.1626825332641602
I0212 12:33:47.566671 140285415180032 logging_writer.py:48] [44200] global_step=44200, grad_norm=2.6505541801452637, loss=1.1112169027328491
I0212 12:35:20.754160 140285423572736 logging_writer.py:48] [44300] global_step=44300, grad_norm=3.1096441745758057, loss=1.060209035873413
I0212 12:36:27.872469 140441227016000 spec.py:321] Evaluating on the training split.
I0212 12:37:22.278720 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 12:38:16.439413 140441227016000 spec.py:349] Evaluating on the test split.
I0212 12:38:42.855922 140441227016000 submission_runner.py:408] Time since start: 41141.70s, 	Step: 44390, 	{'train/ctc_loss': Array(0.21339326, dtype=float32), 'train/wer': 0.0741663739706858, 'validation/ctc_loss': Array(0.4088802, dtype=float32), 'validation/wer': 0.1195439141894436, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22205788, dtype=float32), 'test/wer': 0.07076554343631304, 'test/num_examples': 2472, 'score': 37463.885217666626, 'total_duration': 41141.70326757431, 'accumulated_submission_time': 37463.885217666626, 'accumulated_eval_time': 3674.2578065395355, 'accumulated_logging_time': 1.5213351249694824}
I0212 12:38:42.893198 140285423572736 logging_writer.py:48] [44390] accumulated_eval_time=3674.257807, accumulated_logging_time=1.521335, accumulated_submission_time=37463.885218, global_step=44390, preemption_count=0, score=37463.885218, test/ctc_loss=0.22205787897109985, test/num_examples=2472, test/wer=0.070766, total_duration=41141.703268, train/ctc_loss=0.21339325606822968, train/wer=0.074166, validation/ctc_loss=0.40888020396232605, validation/num_examples=5348, validation/wer=0.119544
I0212 12:38:51.220776 140285415180032 logging_writer.py:48] [44400] global_step=44400, grad_norm=2.0331571102142334, loss=1.0495491027832031
I0212 12:40:06.210828 140285423572736 logging_writer.py:48] [44500] global_step=44500, grad_norm=4.454913139343262, loss=1.123850703239441
I0212 12:41:21.943478 140285415180032 logging_writer.py:48] [44600] global_step=44600, grad_norm=2.6043829917907715, loss=1.1056510210037231
I0212 12:42:37.946058 140285423572736 logging_writer.py:48] [44700] global_step=44700, grad_norm=3.106567859649658, loss=1.113202452659607
I0212 12:44:06.870062 140285415180032 logging_writer.py:48] [44800] global_step=44800, grad_norm=2.292158842086792, loss=1.1374611854553223
I0212 12:45:36.637444 140285423572736 logging_writer.py:48] [44900] global_step=44900, grad_norm=2.901927947998047, loss=1.1421178579330444
I0212 12:47:06.013573 140285415180032 logging_writer.py:48] [45000] global_step=45000, grad_norm=2.5873570442199707, loss=1.0960801839828491
I0212 12:48:33.280598 140285423572736 logging_writer.py:48] [45100] global_step=45100, grad_norm=2.8146982192993164, loss=1.0926727056503296
I0212 12:50:01.454352 140285415180032 logging_writer.py:48] [45200] global_step=45200, grad_norm=2.7129769325256348, loss=1.087323784828186
I0212 12:51:28.844043 140285423572736 logging_writer.py:48] [45300] global_step=45300, grad_norm=2.875249147415161, loss=1.1018503904342651
I0212 12:52:52.855090 140285423572736 logging_writer.py:48] [45400] global_step=45400, grad_norm=3.0947015285491943, loss=1.074136734008789
I0212 12:54:10.614907 140285415180032 logging_writer.py:48] [45500] global_step=45500, grad_norm=3.9815523624420166, loss=1.0953874588012695
I0212 12:55:30.417363 140285423572736 logging_writer.py:48] [45600] global_step=45600, grad_norm=4.030051231384277, loss=1.1222174167633057
I0212 12:56:52.948869 140285415180032 logging_writer.py:48] [45700] global_step=45700, grad_norm=3.009915590286255, loss=1.186078429222107
I0212 12:58:23.612936 140285423572736 logging_writer.py:48] [45800] global_step=45800, grad_norm=2.62021541595459, loss=1.137445092201233
I0212 12:59:53.260240 140285415180032 logging_writer.py:48] [45900] global_step=45900, grad_norm=2.6312685012817383, loss=1.15819251537323
I0212 13:01:21.166360 140285423572736 logging_writer.py:48] [46000] global_step=46000, grad_norm=2.6492955684661865, loss=1.1899725198745728
I0212 13:02:43.179519 140441227016000 spec.py:321] Evaluating on the training split.
I0212 13:03:36.323626 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 13:04:28.923053 140441227016000 spec.py:349] Evaluating on the test split.
I0212 13:04:55.161991 140441227016000 submission_runner.py:408] Time since start: 42714.01s, 	Step: 46095, 	{'train/ctc_loss': Array(0.21221867, dtype=float32), 'train/wer': 0.07441587169915664, 'validation/ctc_loss': Array(0.4071756, dtype=float32), 'validation/wer': 0.11890670708748081, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22028401, dtype=float32), 'test/wer': 0.07033900026405053, 'test/num_examples': 2472, 'score': 38904.08066558838, 'total_duration': 42714.010046720505, 'accumulated_submission_time': 38904.08066558838, 'accumulated_eval_time': 3806.2345554828644, 'accumulated_logging_time': 1.577507495880127}
I0212 13:04:55.199818 140285423572736 logging_writer.py:48] [46095] accumulated_eval_time=3806.234555, accumulated_logging_time=1.577507, accumulated_submission_time=38904.080666, global_step=46095, preemption_count=0, score=38904.080666, test/ctc_loss=0.22028401494026184, test/num_examples=2472, test/wer=0.070339, total_duration=42714.010047, train/ctc_loss=0.21221867203712463, train/wer=0.074416, validation/ctc_loss=0.40717560052871704, validation/num_examples=5348, validation/wer=0.118907
I0212 13:04:59.846082 140285415180032 logging_writer.py:48] [46100] global_step=46100, grad_norm=3.2896132469177246, loss=1.0936987400054932
I0212 13:06:15.699556 140285423572736 logging_writer.py:48] [46200] global_step=46200, grad_norm=2.5250964164733887, loss=1.1669448614120483
I0212 13:07:30.837094 140285415180032 logging_writer.py:48] [46300] global_step=46300, grad_norm=3.452585220336914, loss=1.1270601749420166
I0212 13:08:54.847366 140285423572736 logging_writer.py:48] [46400] global_step=46400, grad_norm=2.4617645740509033, loss=1.1094006299972534
I0212 13:10:12.070521 140285415180032 logging_writer.py:48] [46500] global_step=46500, grad_norm=2.858006477355957, loss=1.0940260887145996
I0212 13:11:32.592937 140285423572736 logging_writer.py:48] [46600] global_step=46600, grad_norm=5.030040264129639, loss=1.0896337032318115
I0212 13:12:55.410406 140285415180032 logging_writer.py:48] [46700] global_step=46700, grad_norm=2.5210399627685547, loss=1.1481965780258179
I0212 13:14:22.722851 140285423572736 logging_writer.py:48] [46800] global_step=46800, grad_norm=5.493581771850586, loss=1.144517183303833
I0212 13:15:51.459495 140285415180032 logging_writer.py:48] [46900] global_step=46900, grad_norm=2.792546272277832, loss=1.137182593345642
I0212 13:17:21.466084 140285423572736 logging_writer.py:48] [47000] global_step=47000, grad_norm=2.0854101181030273, loss=1.108472466468811
I0212 13:18:47.844616 140285415180032 logging_writer.py:48] [47100] global_step=47100, grad_norm=2.41048526763916, loss=1.126089096069336
I0212 13:20:15.634260 140285423572736 logging_writer.py:48] [47200] global_step=47200, grad_norm=2.667623996734619, loss=1.08585524559021
I0212 13:21:47.316347 140285415180032 logging_writer.py:48] [47300] global_step=47300, grad_norm=2.8599634170532227, loss=1.141295313835144
I0212 13:23:17.233933 140285423572736 logging_writer.py:48] [47400] global_step=47400, grad_norm=2.815380573272705, loss=1.1326326131820679
I0212 13:24:37.781359 140285415180032 logging_writer.py:48] [47500] global_step=47500, grad_norm=2.2847349643707275, loss=1.1456434726715088
I0212 13:25:55.613388 140285423572736 logging_writer.py:48] [47600] global_step=47600, grad_norm=5.589725971221924, loss=1.1463664770126343
I0212 13:27:15.272726 140285415180032 logging_writer.py:48] [47700] global_step=47700, grad_norm=2.4260637760162354, loss=1.1083760261535645
I0212 13:28:35.258536 140285423572736 logging_writer.py:48] [47800] global_step=47800, grad_norm=2.7531497478485107, loss=1.12389075756073
I0212 13:28:55.204922 140441227016000 spec.py:321] Evaluating on the training split.
I0212 13:29:49.880976 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 13:30:42.681996 140441227016000 spec.py:349] Evaluating on the test split.
I0212 13:31:09.901910 140441227016000 submission_runner.py:408] Time since start: 44288.75s, 	Step: 47825, 	{'train/ctc_loss': Array(0.20782495, dtype=float32), 'train/wer': 0.07204704842895798, 'validation/ctc_loss': Array(0.40657282, dtype=float32), 'validation/wer': 0.11909014549562162, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2197189, dtype=float32), 'test/wer': 0.07011557288810351, 'test/num_examples': 2472, 'score': 40343.996287584305, 'total_duration': 44288.75029063225, 'accumulated_submission_time': 40343.996287584305, 'accumulated_eval_time': 3940.92617559433, 'accumulated_logging_time': 1.6310737133026123}
I0212 13:31:09.946236 140285423572736 logging_writer.py:48] [47825] accumulated_eval_time=3940.926176, accumulated_logging_time=1.631074, accumulated_submission_time=40343.996288, global_step=47825, preemption_count=0, score=40343.996288, test/ctc_loss=0.21971890330314636, test/num_examples=2472, test/wer=0.070116, total_duration=44288.750291, train/ctc_loss=0.2078249454498291, train/wer=0.072047, validation/ctc_loss=0.4065728187561035, validation/num_examples=5348, validation/wer=0.119090
I0212 13:32:06.737627 140285415180032 logging_writer.py:48] [47900] global_step=47900, grad_norm=2.6352360248565674, loss=1.102050542831421
I0212 13:33:21.942800 140441227016000 spec.py:321] Evaluating on the training split.
I0212 13:34:14.514326 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 13:35:02.514969 140441227016000 spec.py:349] Evaluating on the test split.
I0212 13:35:25.793509 140441227016000 submission_runner.py:408] Time since start: 44544.64s, 	Step: 48000, 	{'train/ctc_loss': Array(0.21455996, dtype=float32), 'train/wer': 0.07427663280772798, 'validation/ctc_loss': Array(0.40663552, dtype=float32), 'validation/wer': 0.11909980014868166, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2197332, dtype=float32), 'test/wer': 0.07007494972884042, 'test/num_examples': 2472, 'score': 40475.96862196922, 'total_duration': 44544.644236803055, 'accumulated_submission_time': 40475.96862196922, 'accumulated_eval_time': 4064.7738120555878, 'accumulated_logging_time': 1.6917657852172852}
I0212 13:35:25.822738 140285423572736 logging_writer.py:48] [48000] accumulated_eval_time=4064.773812, accumulated_logging_time=1.691766, accumulated_submission_time=40475.968622, global_step=48000, preemption_count=0, score=40475.968622, test/ctc_loss=0.21973319351673126, test/num_examples=2472, test/wer=0.070075, total_duration=44544.644237, train/ctc_loss=0.21455995738506317, train/wer=0.074277, validation/ctc_loss=0.4066355228424072, validation/num_examples=5348, validation/wer=0.119100
I0212 13:35:25.846952 140285415180032 logging_writer.py:48] [48000] global_step=48000, preemption_count=0, score=40475.968622
I0212 13:35:26.070585 140441227016000 checkpoints.py:490] Saving checkpoint at step: 48000
I0212 13:35:27.074541 140441227016000 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_2/librispeech_deepspeech_jax/trial_3/checkpoint_48000
I0212 13:35:27.094061 140441227016000 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_2/librispeech_deepspeech_jax/trial_3/checkpoint_48000.
I0212 13:35:28.411731 140441227016000 submission_runner.py:583] Tuning trial 3/5
I0212 13:35:28.411994 140441227016000 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0212 13:35:28.425614 140441227016000 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.647242, dtype=float32), 'train/wer': 3.052888001878375, 'validation/ctc_loss': Array(30.570385, dtype=float32), 'validation/wer': 2.911727507072033, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.6577, dtype=float32), 'test/wer': 3.2319176162330145, 'test/num_examples': 2472, 'score': 15.480498790740967, 'total_duration': 197.94407558441162, 'accumulated_submission_time': 15.480498790740967, 'accumulated_eval_time': 182.46350383758545, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1709, {'train/ctc_loss': Array(6.3669677, dtype=float32), 'train/wer': 0.9411137440758294, 'validation/ctc_loss': Array(6.3865676, dtype=float32), 'validation/wer': 0.8960773144617048, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.3034024, dtype=float32), 'test/wer': 0.8991123839701014, 'test/num_examples': 2472, 'score': 1455.57701587677, 'total_duration': 1751.5442821979523, 'accumulated_submission_time': 1455.57701587677, 'accumulated_eval_time': 295.8599810600281, 'accumulated_logging_time': 0.033608436584472656, 'global_step': 1709, 'preemption_count': 0}), (3436, {'train/ctc_loss': Array(4.5963387, dtype=float32), 'train/wer': 0.9283475590008253, 'validation/ctc_loss': Array(4.687805, dtype=float32), 'validation/wer': 0.8887301234830126, 'validation/num_examples': 5348, 'test/ctc_loss': Array(4.3991666, dtype=float32), 'test/wer': 0.8860723498466476, 'test/num_examples': 2472, 'score': 2895.7558012008667, 'total_duration': 3305.7746090888977, 'accumulated_submission_time': 2895.7558012008667, 'accumulated_eval_time': 409.7762682437897, 'accumulated_logging_time': 0.0885474681854248, 'global_step': 3436, 'preemption_count': 0}), (5133, {'train/ctc_loss': Array(0.764981, dtype=float32), 'train/wer': 0.2510452520218764, 'validation/ctc_loss': Array(1.1377617, dtype=float32), 'validation/wer': 0.3193373046139587, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.77849424, dtype=float32), 'test/wer': 0.24920277049946174, 'test/num_examples': 2472, 'score': 4336.122656345367, 'total_duration': 4881.283031463623, 'accumulated_submission_time': 4336.122656345367, 'accumulated_eval_time': 544.7885777950287, 'accumulated_logging_time': 0.14203882217407227, 'global_step': 5133, 'preemption_count': 0}), (6831, {'train/ctc_loss': Array(0.5411437, dtype=float32), 'train/wer': 0.18125766188892395, 'validation/ctc_loss': Array(0.8500217, dtype=float32), 'validation/wer': 0.2485880069899688, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5582751, dtype=float32), 'test/wer': 0.17900595129283203, 'test/num_examples': 2472, 'score': 5776.731394290924, 'total_duration': 6459.871329784393, 'accumulated_submission_time': 5776.731394290924, 'accumulated_eval_time': 682.6388504505157, 'accumulated_logging_time': 0.19313955307006836, 'global_step': 6831, 'preemption_count': 0}), (8559, {'train/ctc_loss': Array(0.45767567, dtype=float32), 'train/wer': 0.15207675097327233, 'validation/ctc_loss': Array(0.78322417, dtype=float32), 'validation/wer': 0.22473135927860433, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4910314, dtype=float32), 'test/wer': 0.15711006845002337, 'test/num_examples': 2472, 'score': 7216.925114154816, 'total_duration': 8035.558212518692, 'accumulated_submission_time': 7216.925114154816, 'accumulated_eval_time': 817.924996137619, 'accumulated_logging_time': 0.3204481601715088, 'global_step': 8559, 'preemption_count': 0}), (10255, {'train/ctc_loss': Array(0.3818851, dtype=float32), 'train/wer': 0.13084762562542507, 'validation/ctc_loss': Array(0.7319703, dtype=float32), 'validation/wer': 0.21012386919876033, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4505234, dtype=float32), 'test/wer': 0.14606056913046128, 'test/num_examples': 2472, 'score': 8656.972178220749, 'total_duration': 9612.94243979454, 'accumulated_submission_time': 8656.972178220749, 'accumulated_eval_time': 955.0590310096741, 'accumulated_logging_time': 0.44647741317749023, 'global_step': 10255, 'preemption_count': 0}), (11957, {'train/ctc_loss': Array(0.3600609, dtype=float32), 'train/wer': 0.1224991440237527, 'validation/ctc_loss': Array(0.6953865, dtype=float32), 'validation/wer': 0.19995751952653582, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42062852, dtype=float32), 'test/wer': 0.1345439034793736, 'test/num_examples': 2472, 'score': 10097.09136915207, 'total_duration': 11191.76549577713, 'accumulated_submission_time': 10097.09136915207, 'accumulated_eval_time': 1093.6228725910187, 'accumulated_logging_time': 0.5056447982788086, 'global_step': 11957, 'preemption_count': 0}), (13684, {'train/ctc_loss': Array(0.37431747, dtype=float32), 'train/wer': 0.1254869075957585, 'validation/ctc_loss': Array(0.67228556, dtype=float32), 'validation/wer': 0.19349855662936752, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40535486, dtype=float32), 'test/wer': 0.1304003412345378, 'test/num_examples': 2472, 'score': 11537.651688098907, 'total_duration': 12766.904225826263, 'accumulated_submission_time': 11537.651688098907, 'accumulated_eval_time': 1228.071349620819, 'accumulated_logging_time': 0.5557355880737305, 'global_step': 13684, 'preemption_count': 0}), (15387, {'train/ctc_loss': Array(0.33916324, dtype=float32), 'train/wer': 0.11298625599904125, 'validation/ctc_loss': Array(0.6459791, dtype=float32), 'validation/wer': 0.18616102030373538, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3900358, dtype=float32), 'test/wer': 0.1256474316007556, 'test/num_examples': 2472, 'score': 12978.08913564682, 'total_duration': 14344.672752857208, 'accumulated_submission_time': 12978.08913564682, 'accumulated_eval_time': 1365.2708704471588, 'accumulated_logging_time': 0.6095635890960693, 'global_step': 15387, 'preemption_count': 0}), (17085, {'train/ctc_loss': Array(0.32999718, dtype=float32), 'train/wer': 0.11262952703395916, 'validation/ctc_loss': Array(0.6329018, dtype=float32), 'validation/wer': 0.18273361846742037, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37440822, dtype=float32), 'test/wer': 0.12140231145776206, 'test/num_examples': 2472, 'score': 14418.107741355896, 'total_duration': 15921.399963855743, 'accumulated_submission_time': 14418.107741355896, 'accumulated_eval_time': 1501.8507792949677, 'accumulated_logging_time': 0.659881591796875, 'global_step': 17085, 'preemption_count': 0}), (18812, {'train/ctc_loss': Array(0.35147545, dtype=float32), 'train/wer': 0.11254347617983981, 'validation/ctc_loss': Array(0.6095573, dtype=float32), 'validation/wer': 0.175338154223428, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36095637, dtype=float32), 'test/wer': 0.11585725021834949, 'test/num_examples': 2472, 'score': 15858.09096622467, 'total_duration': 17499.5164706707, 'accumulated_submission_time': 15858.09096622467, 'accumulated_eval_time': 1639.8546781539917, 'accumulated_logging_time': 0.7098729610443115, 'global_step': 18812, 'preemption_count': 0}), (20513, {'train/ctc_loss': Array(0.2695812, dtype=float32), 'train/wer': 0.09233033205488443, 'validation/ctc_loss': Array(0.59979016, dtype=float32), 'validation/wer': 0.1738803016113616, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35536996, dtype=float32), 'test/wer': 0.11553226494424472, 'test/num_examples': 2472, 'score': 17298.066687345505, 'total_duration': 19077.658970832825, 'accumulated_submission_time': 17298.066687345505, 'accumulated_eval_time': 1777.8929901123047, 'accumulated_logging_time': 0.7604987621307373, 'global_step': 20513, 'preemption_count': 0}), (22217, {'train/ctc_loss': Array(0.2810426, dtype=float32), 'train/wer': 0.09618531316817139, 'validation/ctc_loss': Array(0.58093655, dtype=float32), 'validation/wer': 0.16857024242833835, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3375406, dtype=float32), 'test/wer': 0.11025125424004224, 'test/num_examples': 2472, 'score': 18738.8721241951, 'total_duration': 20656.351114034653, 'accumulated_submission_time': 18738.8721241951, 'accumulated_eval_time': 1915.6497299671173, 'accumulated_logging_time': 0.8108630180358887, 'global_step': 22217, 'preemption_count': 0}), (23927, {'train/ctc_loss': Array(0.3692097, dtype=float32), 'train/wer': 0.12319453909908326, 'validation/ctc_loss': Array(0.56566906, dtype=float32), 'validation/wer': 0.16485320100022205, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32571787, dtype=float32), 'test/wer': 0.1042999614079987, 'test/num_examples': 2472, 'score': 20179.337572574615, 'total_duration': 22232.538994073868, 'accumulated_submission_time': 20179.337572574615, 'accumulated_eval_time': 2051.2442207336426, 'accumulated_logging_time': 0.8616993427276611, 'global_step': 23927, 'preemption_count': 0}), (25613, {'train/ctc_loss': Array(0.3844082, dtype=float32), 'train/wer': 0.12607921747375775, 'validation/ctc_loss': Array(0.54311615, dtype=float32), 'validation/wer': 0.157988742674532, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31971592, dtype=float32), 'test/wer': 0.10326407084678975, 'test/num_examples': 2472, 'score': 21619.988522529602, 'total_duration': 23807.355211019516, 'accumulated_submission_time': 21619.988522529602, 'accumulated_eval_time': 2185.283809185028, 'accumulated_logging_time': 0.9100189208984375, 'global_step': 25613, 'preemption_count': 0}), (27319, {'train/ctc_loss': Array(0.42835096, dtype=float32), 'train/wer': 0.14189251902728792, 'validation/ctc_loss': Array(0.540189, dtype=float32), 'validation/wer': 0.15711982389912818, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3093404, dtype=float32), 'test/wer': 0.0984096033148498, 'test/num_examples': 2472, 'score': 23059.87901854515, 'total_duration': 25383.035180568695, 'accumulated_submission_time': 23059.87901854515, 'accumulated_eval_time': 2320.938892841339, 'accumulated_logging_time': 0.9658107757568359, 'global_step': 27319, 'preemption_count': 0}), (29031, {'train/ctc_loss': Array(0.3626228, dtype=float32), 'train/wer': 0.11829440075126188, 'validation/ctc_loss': Array(0.52205914, dtype=float32), 'validation/wer': 0.1520800950017861, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29937524, dtype=float32), 'test/wer': 0.09499725793674973, 'test/num_examples': 2472, 'score': 24500.383773565292, 'total_duration': 26956.89998602867, 'accumulated_submission_time': 24500.383773565292, 'accumulated_eval_time': 2454.1646745204926, 'accumulated_logging_time': 1.0208978652954102, 'global_step': 29031, 'preemption_count': 0}), (30729, {'train/ctc_loss': Array(0.32588735, dtype=float32), 'train/wer': 0.11000286582864592, 'validation/ctc_loss': Array(0.50159395, dtype=float32), 'validation/wer': 0.14702105679832395, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28334433, dtype=float32), 'test/wer': 0.0904880872585461, 'test/num_examples': 2472, 'score': 25941.25056886673, 'total_duration': 28532.90132164955, 'accumulated_submission_time': 25941.25056886673, 'accumulated_eval_time': 2589.162698030472, 'accumulated_logging_time': 1.0792968273162842, 'global_step': 30729, 'preemption_count': 0}), (32452, {'train/ctc_loss': Array(0.2819919, dtype=float32), 'train/wer': 0.0966568077732058, 'validation/ctc_loss': Array(0.48701313, dtype=float32), 'validation/wer': 0.14218407561524277, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27825376, dtype=float32), 'test/wer': 0.0895131314362318, 'test/num_examples': 2472, 'score': 27381.333554029465, 'total_duration': 30110.221838235855, 'accumulated_submission_time': 27381.333554029465, 'accumulated_eval_time': 2726.2692017555237, 'accumulated_logging_time': 1.1319363117218018, 'global_step': 32452, 'preemption_count': 0}), (34156, {'train/ctc_loss': Array(0.30927896, dtype=float32), 'train/wer': 0.10505333653279815, 'validation/ctc_loss': Array(0.46904442, dtype=float32), 'validation/wer': 0.13654575822817808, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26531848, dtype=float32), 'test/wer': 0.08522738813397518, 'test/num_examples': 2472, 'score': 28821.81571483612, 'total_duration': 31686.440123796463, 'accumulated_submission_time': 28821.81571483612, 'accumulated_eval_time': 2861.870703935623, 'accumulated_logging_time': 1.1866295337677002, 'global_step': 34156, 'preemption_count': 0}), (35850, {'train/ctc_loss': Array(0.26929548, dtype=float32), 'train/wer': 0.09210244893594267, 'validation/ctc_loss': Array(0.45808133, dtype=float32), 'validation/wer': 0.13422864149376793, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25412038, dtype=float32), 'test/wer': 0.08144943432250726, 'test/num_examples': 2472, 'score': 30262.3197350502, 'total_duration': 33263.54372572899, 'accumulated_submission_time': 30262.3197350502, 'accumulated_eval_time': 2998.3368847370148, 'accumulated_logging_time': 1.2433674335479736, 'global_step': 35850, 'preemption_count': 0}), (37571, {'train/ctc_loss': Array(0.2542912, dtype=float32), 'train/wer': 0.08848222020038028, 'validation/ctc_loss': Array(0.44569063, dtype=float32), 'validation/wer': 0.13101364202477384, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24617566, dtype=float32), 'test/wer': 0.07828082789998578, 'test/num_examples': 2472, 'score': 31702.28162097931, 'total_duration': 34838.69946312904, 'accumulated_submission_time': 31702.28162097931, 'accumulated_eval_time': 3133.3894832134247, 'accumulated_logging_time': 1.3041977882385254, 'global_step': 37571, 'preemption_count': 0}), (39261, {'train/ctc_loss': Array(0.2523249, dtype=float32), 'train/wer': 0.08504993586721774, 'validation/ctc_loss': Array(0.4367461, dtype=float32), 'validation/wer': 0.12747038435173833, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2394587, dtype=float32), 'test/wer': 0.07596530782198932, 'test/num_examples': 2472, 'score': 33142.84040021896, 'total_duration': 36414.29718565941, 'accumulated_submission_time': 33142.84040021896, 'accumulated_eval_time': 3268.299718618393, 'accumulated_logging_time': 1.3554682731628418, 'global_step': 39261, 'preemption_count': 0}), (40968, {'train/ctc_loss': Array(0.24387786, dtype=float32), 'train/wer': 0.08422291768353958, 'validation/ctc_loss': Array(0.42268944, dtype=float32), 'validation/wer': 0.12450640586230534, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23126832, dtype=float32), 'test/wer': 0.07344667194767737, 'test/num_examples': 2472, 'score': 34583.702951192856, 'total_duration': 37992.941672325134, 'accumulated_submission_time': 34583.702951192856, 'accumulated_eval_time': 3405.9477915763855, 'accumulated_logging_time': 1.4116151332855225, 'global_step': 40968, 'preemption_count': 0}), (42693, {'train/ctc_loss': Array(0.24088885, dtype=float32), 'train/wer': 0.08084833819656964, 'validation/ctc_loss': Array(0.41713747, dtype=float32), 'validation/wer': 0.12214101586259497, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22636868, dtype=float32), 'test/wer': 0.07241078138646842, 'test/num_examples': 2472, 'score': 36023.71224784851, 'total_duration': 39566.42193007469, 'accumulated_submission_time': 36023.71224784851, 'accumulated_eval_time': 3539.2807710170746, 'accumulated_logging_time': 1.4675979614257812, 'global_step': 42693, 'preemption_count': 0}), (44390, {'train/ctc_loss': Array(0.21339326, dtype=float32), 'train/wer': 0.0741663739706858, 'validation/ctc_loss': Array(0.4088802, dtype=float32), 'validation/wer': 0.1195439141894436, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22205788, dtype=float32), 'test/wer': 0.07076554343631304, 'test/num_examples': 2472, 'score': 37463.885217666626, 'total_duration': 41141.70326757431, 'accumulated_submission_time': 37463.885217666626, 'accumulated_eval_time': 3674.2578065395355, 'accumulated_logging_time': 1.5213351249694824, 'global_step': 44390, 'preemption_count': 0}), (46095, {'train/ctc_loss': Array(0.21221867, dtype=float32), 'train/wer': 0.07441587169915664, 'validation/ctc_loss': Array(0.4071756, dtype=float32), 'validation/wer': 0.11890670708748081, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22028401, dtype=float32), 'test/wer': 0.07033900026405053, 'test/num_examples': 2472, 'score': 38904.08066558838, 'total_duration': 42714.010046720505, 'accumulated_submission_time': 38904.08066558838, 'accumulated_eval_time': 3806.2345554828644, 'accumulated_logging_time': 1.577507495880127, 'global_step': 46095, 'preemption_count': 0}), (47825, {'train/ctc_loss': Array(0.20782495, dtype=float32), 'train/wer': 0.07204704842895798, 'validation/ctc_loss': Array(0.40657282, dtype=float32), 'validation/wer': 0.11909014549562162, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2197189, dtype=float32), 'test/wer': 0.07011557288810351, 'test/num_examples': 2472, 'score': 40343.996287584305, 'total_duration': 44288.75029063225, 'accumulated_submission_time': 40343.996287584305, 'accumulated_eval_time': 3940.92617559433, 'accumulated_logging_time': 1.6310737133026123, 'global_step': 47825, 'preemption_count': 0}), (48000, {'train/ctc_loss': Array(0.21455996, dtype=float32), 'train/wer': 0.07427663280772798, 'validation/ctc_loss': Array(0.40663552, dtype=float32), 'validation/wer': 0.11909980014868166, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2197332, dtype=float32), 'test/wer': 0.07007494972884042, 'test/num_examples': 2472, 'score': 40475.96862196922, 'total_duration': 44544.644236803055, 'accumulated_submission_time': 40475.96862196922, 'accumulated_eval_time': 4064.7738120555878, 'accumulated_logging_time': 1.6917657852172852, 'global_step': 48000, 'preemption_count': 0})], 'global_step': 48000}
I0212 13:35:28.425857 140441227016000 submission_runner.py:586] Timing: 40475.96862196922
I0212 13:35:28.425920 140441227016000 submission_runner.py:588] Total number of evals: 30
I0212 13:35:28.425973 140441227016000 submission_runner.py:589] ====================
I0212 13:35:28.426052 140441227016000 submission_runner.py:542] Using RNG seed 1231499801
I0212 13:35:28.429538 140441227016000 submission_runner.py:551] --- Tuning run 4/5 ---
I0212 13:35:28.429679 140441227016000 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_2/librispeech_deepspeech_jax/trial_4.
I0212 13:35:28.431435 140441227016000 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_2/librispeech_deepspeech_jax/trial_4/hparams.json.
I0212 13:35:28.433713 140441227016000 submission_runner.py:206] Initializing dataset.
I0212 13:35:28.433840 140441227016000 submission_runner.py:213] Initializing model.
I0212 13:35:29.744075 140441227016000 submission_runner.py:255] Initializing optimizer.
I0212 13:35:29.897280 140441227016000 submission_runner.py:262] Initializing metrics bundle.
I0212 13:35:29.897470 140441227016000 submission_runner.py:280] Initializing checkpoint and logger.
I0212 13:35:29.901776 140441227016000 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_2/librispeech_deepspeech_jax/trial_4 with prefix checkpoint_
I0212 13:35:29.901911 140441227016000 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_2/librispeech_deepspeech_jax/trial_4/meta_data_0.json.
I0212 13:35:29.902277 140441227016000 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0212 13:35:29.902368 140441227016000 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0212 13:35:30.462521 140441227016000 logger_utils.py:220] Unable to record git information. Continuing without it.
I0212 13:35:30.962058 140441227016000 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_2/librispeech_deepspeech_jax/trial_4/flags_0.json.
I0212 13:35:30.977734 140441227016000 submission_runner.py:314] Starting training loop.
I0212 13:35:30.981327 140441227016000 input_pipeline.py:20] Loading split = train-clean-100
I0212 13:35:31.490684 140441227016000 input_pipeline.py:20] Loading split = train-clean-360
I0212 13:35:31.625456 140441227016000 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0212 13:35:46.785855 140285264111360 logging_writer.py:48] [0] global_step=0, grad_norm=19.243675231933594, loss=33.47202682495117
I0212 13:35:46.800020 140441227016000 spec.py:321] Evaluating on the training split.
I0212 13:37:11.442965 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 13:38:10.075276 140441227016000 spec.py:349] Evaluating on the test split.
I0212 13:38:40.588833 140441227016000 submission_runner.py:408] Time since start: 189.61s, 	Step: 1, 	{'train/ctc_loss': Array(32.005363, dtype=float32), 'train/wer': 2.9794811320754717, 'validation/ctc_loss': Array(30.57026, dtype=float32), 'validation/wer': 2.911621305888373, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.657564, dtype=float32), 'test/wer': 3.231897304653383, 'test/num_examples': 2472, 'score': 15.822179079055786, 'total_duration': 189.60808277130127, 'accumulated_submission_time': 15.822179079055786, 'accumulated_eval_time': 173.78581547737122, 'accumulated_logging_time': 0}
I0212 13:38:40.607873 140285423572736 logging_writer.py:48] [1] accumulated_eval_time=173.785815, accumulated_logging_time=0, accumulated_submission_time=15.822179, global_step=1, preemption_count=0, score=15.822179, test/ctc_loss=30.657564163208008, test/num_examples=2472, test/wer=3.231897, total_duration=189.608083, train/ctc_loss=32.00536346435547, train/wer=2.979481, validation/ctc_loss=30.57025909423828, validation/num_examples=5348, validation/wer=2.911621
I0212 13:40:06.468307 140285348038400 logging_writer.py:48] [100] global_step=100, grad_norm=1.107951283454895, loss=5.760642051696777
I0212 13:41:22.639633 140285356431104 logging_writer.py:48] [200] global_step=200, grad_norm=4.900049209594727, loss=5.123514652252197
I0212 13:42:38.218576 140285348038400 logging_writer.py:48] [300] global_step=300, grad_norm=2.607330322265625, loss=3.708300828933716
I0212 13:43:58.340714 140285356431104 logging_writer.py:48] [400] global_step=400, grad_norm=3.9285709857940674, loss=3.237816095352173
I0212 13:45:28.190487 140285348038400 logging_writer.py:48] [500] global_step=500, grad_norm=1.9085493087768555, loss=2.9476702213287354
I0212 13:46:52.998434 140285356431104 logging_writer.py:48] [600] global_step=600, grad_norm=3.147136926651001, loss=2.8430275917053223
I0212 13:48:23.646253 140285348038400 logging_writer.py:48] [700] global_step=700, grad_norm=4.634905815124512, loss=2.822092294692993
I0212 13:49:53.330926 140285356431104 logging_writer.py:48] [800] global_step=800, grad_norm=2.8927974700927734, loss=2.738206624984741
I0212 13:51:21.394002 140285348038400 logging_writer.py:48] [900] global_step=900, grad_norm=2.747918128967285, loss=2.6702375411987305
I0212 13:52:49.666580 140285356431104 logging_writer.py:48] [1000] global_step=1000, grad_norm=3.313479423522949, loss=2.6732451915740967
I0212 13:54:12.304604 140285423572736 logging_writer.py:48] [1100] global_step=1100, grad_norm=2.352348566055298, loss=2.531986951828003
I0212 13:55:29.599649 140285415180032 logging_writer.py:48] [1200] global_step=1200, grad_norm=2.0655124187469482, loss=2.4331581592559814
I0212 13:56:46.147778 140285423572736 logging_writer.py:48] [1300] global_step=1300, grad_norm=3.406442165374756, loss=2.4035723209381104
I0212 13:58:10.475851 140285415180032 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.819621205329895, loss=2.2921741008758545
I0212 13:59:37.396774 140285423572736 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.6934531927108765, loss=2.2981863021850586
I0212 14:01:07.973442 140285415180032 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.9950222969055176, loss=2.235811471939087
I0212 14:02:37.312317 140285423572736 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.871128797531128, loss=2.299039840698242
I0212 14:02:40.625010 140441227016000 spec.py:321] Evaluating on the training split.
I0212 14:03:33.786615 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 14:04:27.206617 140441227016000 spec.py:349] Evaluating on the test split.
I0212 14:04:54.604153 140441227016000 submission_runner.py:408] Time since start: 1763.62s, 	Step: 1705, 	{'train/ctc_loss': Array(1.7329582, dtype=float32), 'train/wer': 0.4562996089686897, 'validation/ctc_loss': Array(1.7820381, dtype=float32), 'validation/wer': 0.44021356092568814, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.3494298, dtype=float32), 'test/wer': 0.3802530822822091, 'test/num_examples': 2472, 'score': 1455.7558295726776, 'total_duration': 1763.6183669567108, 'accumulated_submission_time': 1455.7558295726776, 'accumulated_eval_time': 307.7569811344147, 'accumulated_logging_time': 0.02973175048828125}
I0212 14:04:54.642532 140285423572736 logging_writer.py:48] [1705] accumulated_eval_time=307.756981, accumulated_logging_time=0.029732, accumulated_submission_time=1455.755830, global_step=1705, preemption_count=0, score=1455.755830, test/ctc_loss=1.3494298458099365, test/num_examples=2472, test/wer=0.380253, total_duration=1763.618367, train/ctc_loss=1.732958197593689, train/wer=0.456300, validation/ctc_loss=1.7820380926132202, validation/num_examples=5348, validation/wer=0.440214
I0212 14:06:06.496494 140285415180032 logging_writer.py:48] [1800] global_step=1800, grad_norm=3.231898784637451, loss=2.2521793842315674
I0212 14:07:21.491404 140285423572736 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.3761935234069824, loss=2.2239584922790527
I0212 14:08:47.304559 140285415180032 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.7848012447357178, loss=2.2072997093200684
I0212 14:10:14.921339 140285423572736 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.624798536300659, loss=2.2383906841278076
I0212 14:11:33.709315 140285415180032 logging_writer.py:48] [2200] global_step=2200, grad_norm=2.564129114151001, loss=2.2393994331359863
I0212 14:12:52.738651 140285423572736 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.1024277210235596, loss=2.192314863204956
I0212 14:14:13.328069 140285415180032 logging_writer.py:48] [2400] global_step=2400, grad_norm=5.003159523010254, loss=2.148191452026367
I0212 14:15:40.730747 140285423572736 logging_writer.py:48] [2500] global_step=2500, grad_norm=2.80836820602417, loss=2.152179002761841
I0212 14:17:10.822337 140285415180032 logging_writer.py:48] [2600] global_step=2600, grad_norm=2.4666717052459717, loss=2.0805981159210205
I0212 14:18:39.576215 140285423572736 logging_writer.py:48] [2700] global_step=2700, grad_norm=2.7932496070861816, loss=2.102820873260498
I0212 14:20:08.917387 140285415180032 logging_writer.py:48] [2800] global_step=2800, grad_norm=5.116051197052002, loss=2.083054304122925
I0212 14:21:36.557530 140285423572736 logging_writer.py:48] [2900] global_step=2900, grad_norm=2.7549259662628174, loss=2.08298397064209
I0212 14:23:05.974757 140285415180032 logging_writer.py:48] [3000] global_step=3000, grad_norm=4.919744968414307, loss=2.0313549041748047
I0212 14:24:38.722050 140285423572736 logging_writer.py:48] [3100] global_step=3100, grad_norm=3.726884126663208, loss=2.042799472808838
I0212 14:25:55.456744 140285415180032 logging_writer.py:48] [3200] global_step=3200, grad_norm=3.2733843326568604, loss=2.040604591369629
I0212 14:27:14.858185 140285423572736 logging_writer.py:48] [3300] global_step=3300, grad_norm=14.08650016784668, loss=2.0546212196350098
I0212 14:28:38.535743 140285415180032 logging_writer.py:48] [3400] global_step=3400, grad_norm=4.90895414352417, loss=2.003570318222046
I0212 14:28:55.116455 140441227016000 spec.py:321] Evaluating on the training split.
I0212 14:29:48.672118 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 14:30:40.536319 140441227016000 spec.py:349] Evaluating on the test split.
I0212 14:31:07.127489 140441227016000 submission_runner.py:408] Time since start: 3336.15s, 	Step: 3421, 	{'train/ctc_loss': Array(0.9513836, dtype=float32), 'train/wer': 0.2833366333833055, 'validation/ctc_loss': Array(1.08201, dtype=float32), 'validation/wer': 0.2937428193517866, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.7284901, dtype=float32), 'test/wer': 0.22505230231755124, 'test/num_examples': 2472, 'score': 2896.13534617424, 'total_duration': 3336.1467773914337, 'accumulated_submission_time': 2896.13534617424, 'accumulated_eval_time': 439.76512384414673, 'accumulated_logging_time': 0.08956670761108398}
I0212 14:31:07.156875 140285423572736 logging_writer.py:48] [3421] accumulated_eval_time=439.765124, accumulated_logging_time=0.089567, accumulated_submission_time=2896.135346, global_step=3421, preemption_count=0, score=2896.135346, test/ctc_loss=0.7284901142120361, test/num_examples=2472, test/wer=0.225052, total_duration=3336.146777, train/ctc_loss=0.9513835906982422, train/wer=0.283337, validation/ctc_loss=1.08201003074646, validation/num_examples=5348, validation/wer=0.293743
I0212 14:32:07.106120 140285415180032 logging_writer.py:48] [3500] global_step=3500, grad_norm=3.7394092082977295, loss=2.019057035446167
I0212 14:33:22.302649 140285423572736 logging_writer.py:48] [3600] global_step=3600, grad_norm=3.3099164962768555, loss=2.0895676612854004
I0212 14:34:45.298461 140285415180032 logging_writer.py:48] [3700] global_step=3700, grad_norm=4.872695446014404, loss=2.0615792274475098
I0212 14:36:14.193669 140285423572736 logging_writer.py:48] [3800] global_step=3800, grad_norm=3.1523993015289307, loss=2.1219704151153564
I0212 14:37:41.404628 140285415180032 logging_writer.py:48] [3900] global_step=3900, grad_norm=4.666297435760498, loss=2.0628719329833984
I0212 14:39:11.315291 140285423572736 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.91011905670166, loss=2.0120391845703125
I0212 14:40:41.357244 140285415180032 logging_writer.py:48] [4100] global_step=4100, grad_norm=3.1303012371063232, loss=2.0403201580047607
I0212 14:42:03.255971 140285423572736 logging_writer.py:48] [4200] global_step=4200, grad_norm=2.893441915512085, loss=2.0187253952026367
I0212 14:43:20.850583 140285415180032 logging_writer.py:48] [4300] global_step=4300, grad_norm=3.2788350582122803, loss=2.0222604274749756
I0212 14:44:43.158839 140285423572736 logging_writer.py:48] [4400] global_step=4400, grad_norm=2.123718023300171, loss=1.975339412689209
I0212 14:46:09.217039 140285415180032 logging_writer.py:48] [4500] global_step=4500, grad_norm=3.6106173992156982, loss=2.063058376312256
I0212 14:47:38.723637 140285423572736 logging_writer.py:48] [4600] global_step=4600, grad_norm=12.722772598266602, loss=2.0297369956970215
I0212 14:49:05.145250 140285415180032 logging_writer.py:48] [4700] global_step=4700, grad_norm=8.384468078613281, loss=2.447899341583252
I0212 14:50:33.521720 140285423572736 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.286440372467041, loss=2.018942356109619
I0212 14:52:01.856365 140285415180032 logging_writer.py:48] [4900] global_step=4900, grad_norm=2.8476762771606445, loss=1.9673330783843994
I0212 14:53:33.473548 140285423572736 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.2325143814086914, loss=2.0804519653320312
I0212 14:55:00.553016 140285415180032 logging_writer.py:48] [5100] global_step=5100, grad_norm=3.260760545730591, loss=2.056786298751831
I0212 14:55:08.131496 140441227016000 spec.py:321] Evaluating on the training split.
I0212 14:56:01.722110 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 14:56:54.770319 140441227016000 spec.py:349] Evaluating on the test split.
I0212 14:57:21.860199 140441227016000 submission_runner.py:408] Time since start: 4910.88s, 	Step: 5110, 	{'train/ctc_loss': Array(0.8250965, dtype=float32), 'train/wer': 0.24666468510283174, 'validation/ctc_loss': Array(0.96908915, dtype=float32), 'validation/wer': 0.2676849107427325, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.64543873, dtype=float32), 'test/wer': 0.20126744256900858, 'test/num_examples': 2472, 'score': 4337.026045560837, 'total_duration': 4910.876049280167, 'accumulated_submission_time': 4337.026045560837, 'accumulated_eval_time': 573.4874782562256, 'accumulated_logging_time': 0.1315174102783203}
I0212 14:57:21.895287 140285423572736 logging_writer.py:48] [5110] accumulated_eval_time=573.487478, accumulated_logging_time=0.131517, accumulated_submission_time=4337.026046, global_step=5110, preemption_count=0, score=4337.026046, test/ctc_loss=0.6454387307167053, test/num_examples=2472, test/wer=0.201267, total_duration=4910.876049, train/ctc_loss=0.8250964879989624, train/wer=0.246665, validation/ctc_loss=0.969089150428772, validation/num_examples=5348, validation/wer=0.267685
I0212 14:58:37.221194 140285423572736 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.5799602270126343, loss=1.9307587146759033
I0212 14:59:54.446076 140285415180032 logging_writer.py:48] [5300] global_step=5300, grad_norm=2.0802478790283203, loss=1.9317505359649658
I0212 15:01:14.519443 140285423572736 logging_writer.py:48] [5400] global_step=5400, grad_norm=2.691025495529175, loss=2.008915424346924
I0212 15:02:38.354328 140285415180032 logging_writer.py:48] [5500] global_step=5500, grad_norm=3.1400692462921143, loss=2.0021982192993164
I0212 15:04:04.744073 140285423572736 logging_writer.py:48] [5600] global_step=5600, grad_norm=3.3728041648864746, loss=2.0042502880096436
I0212 15:05:28.595933 140285415180032 logging_writer.py:48] [5700] global_step=5700, grad_norm=3.247539520263672, loss=1.9750022888183594
I0212 15:06:59.200863 140285423572736 logging_writer.py:48] [5800] global_step=5800, grad_norm=3.2797067165374756, loss=1.9764078855514526
I0212 15:08:31.909753 140285415180032 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.4889135360717773, loss=1.9400825500488281
I0212 15:10:02.055156 140285423572736 logging_writer.py:48] [6000] global_step=6000, grad_norm=7.348606109619141, loss=1.9968960285186768
I0212 15:11:28.470975 140285415180032 logging_writer.py:48] [6100] global_step=6100, grad_norm=2.869753122329712, loss=1.9266456365585327
I0212 15:12:59.122946 140285423572736 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.6349918842315674, loss=1.8839086294174194
I0212 15:14:17.586596 140285415180032 logging_writer.py:48] [6300] global_step=6300, grad_norm=2.8511695861816406, loss=1.8889843225479126
I0212 15:15:36.647937 140285423572736 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.6579036712646484, loss=1.9595850706100464
I0212 15:16:54.170099 140285415180032 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.7395596504211426, loss=2.0359435081481934
I0212 15:18:18.654322 140285423572736 logging_writer.py:48] [6600] global_step=6600, grad_norm=2.578984498977661, loss=1.9498347043991089
I0212 15:19:47.904201 140285415180032 logging_writer.py:48] [6700] global_step=6700, grad_norm=2.6732726097106934, loss=2.0011143684387207
I0212 15:21:20.419086 140285423572736 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.944111704826355, loss=1.9194064140319824
I0212 15:21:22.108604 140441227016000 spec.py:321] Evaluating on the training split.
I0212 15:22:16.829680 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 15:23:11.329449 140441227016000 spec.py:349] Evaluating on the test split.
I0212 15:23:39.055577 140441227016000 submission_runner.py:408] Time since start: 6488.07s, 	Step: 6803, 	{'train/ctc_loss': Array(0.8884563, dtype=float32), 'train/wer': 0.267509416380854, 'validation/ctc_loss': Array(0.9415473, dtype=float32), 'validation/wer': 0.26318584241675275, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6189214, dtype=float32), 'test/wer': 0.19078666747912984, 'test/num_examples': 2472, 'score': 5777.149932384491, 'total_duration': 6488.072125196457, 'accumulated_submission_time': 5777.149932384491, 'accumulated_eval_time': 710.4288213253021, 'accumulated_logging_time': 0.1832258701324463}
I0212 15:23:39.090432 140285423572736 logging_writer.py:48] [6803] accumulated_eval_time=710.428821, accumulated_logging_time=0.183226, accumulated_submission_time=5777.149932, global_step=6803, preemption_count=0, score=5777.149932, test/ctc_loss=0.6189213991165161, test/num_examples=2472, test/wer=0.190787, total_duration=6488.072125, train/ctc_loss=0.8884562849998474, train/wer=0.267509, validation/ctc_loss=0.9415472745895386, validation/num_examples=5348, validation/wer=0.263186
I0212 15:24:52.725661 140285415180032 logging_writer.py:48] [6900] global_step=6900, grad_norm=4.425760746002197, loss=1.977020502090454
I0212 15:26:07.605324 140285423572736 logging_writer.py:48] [7000] global_step=7000, grad_norm=2.762070655822754, loss=1.9461743831634521
I0212 15:27:33.131742 140285415180032 logging_writer.py:48] [7100] global_step=7100, grad_norm=3.843212366104126, loss=1.9008195400238037
I0212 15:29:02.952106 140285423572736 logging_writer.py:48] [7200] global_step=7200, grad_norm=3.6582765579223633, loss=1.9379456043243408
I0212 15:30:24.915656 140285423572736 logging_writer.py:48] [7300] global_step=7300, grad_norm=2.2383463382720947, loss=1.971744179725647
I0212 15:31:42.172898 140285415180032 logging_writer.py:48] [7400] global_step=7400, grad_norm=3.141464948654175, loss=1.9173740148544312
I0212 15:33:06.774292 140285423572736 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.570040702819824, loss=1.9017432928085327
I0212 15:34:32.143501 140285415180032 logging_writer.py:48] [7600] global_step=7600, grad_norm=4.523428440093994, loss=1.8666530847549438
I0212 15:35:58.528114 140285423572736 logging_writer.py:48] [7700] global_step=7700, grad_norm=3.8023598194122314, loss=1.8812625408172607
I0212 15:37:24.274188 140285415180032 logging_writer.py:48] [7800] global_step=7800, grad_norm=2.232038974761963, loss=1.9355518817901611
I0212 15:38:52.070069 140285423572736 logging_writer.py:48] [7900] global_step=7900, grad_norm=3.4233224391937256, loss=1.8847792148590088
I0212 15:40:18.844539 140285415180032 logging_writer.py:48] [8000] global_step=8000, grad_norm=5.1613545417785645, loss=1.9177168607711792
I0212 15:41:46.729772 140285423572736 logging_writer.py:48] [8100] global_step=8100, grad_norm=2.3574392795562744, loss=1.8838177919387817
I0212 15:43:14.660115 140285415180032 logging_writer.py:48] [8200] global_step=8200, grad_norm=3.343763589859009, loss=1.8827922344207764
I0212 15:44:39.409207 140285423572736 logging_writer.py:48] [8300] global_step=8300, grad_norm=4.219058513641357, loss=1.9847661256790161
I0212 15:45:56.460211 140285415180032 logging_writer.py:48] [8400] global_step=8400, grad_norm=3.522221088409424, loss=1.8875802755355835
I0212 15:47:15.092669 140285423572736 logging_writer.py:48] [8500] global_step=8500, grad_norm=3.425440788269043, loss=1.8351207971572876
I0212 15:47:39.527590 140441227016000 spec.py:321] Evaluating on the training split.
I0212 15:48:33.784719 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 15:49:28.436604 140441227016000 spec.py:349] Evaluating on the test split.
I0212 15:49:55.640252 140441227016000 submission_runner.py:408] Time since start: 8064.66s, 	Step: 8533, 	{'train/ctc_loss': Array(0.7801244, dtype=float32), 'train/wer': 0.23471242189340483, 'validation/ctc_loss': Array(0.90787, dtype=float32), 'validation/wer': 0.25158094943858195, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.57711726, dtype=float32), 'test/wer': 0.1780106838908862, 'test/num_examples': 2472, 'score': 7217.49794960022, 'total_duration': 8064.6559591293335, 'accumulated_submission_time': 7217.49794960022, 'accumulated_eval_time': 846.5350136756897, 'accumulated_logging_time': 0.23366570472717285}
I0212 15:49:55.677324 140285423572736 logging_writer.py:48] [8533] accumulated_eval_time=846.535014, accumulated_logging_time=0.233666, accumulated_submission_time=7217.497950, global_step=8533, preemption_count=0, score=7217.497950, test/ctc_loss=0.5771172642707825, test/num_examples=2472, test/wer=0.178011, total_duration=8064.655959, train/ctc_loss=0.7801244258880615, train/wer=0.234712, validation/ctc_loss=0.9078699946403503, validation/num_examples=5348, validation/wer=0.251581
I0212 15:50:46.593673 140285415180032 logging_writer.py:48] [8600] global_step=8600, grad_norm=2.037360668182373, loss=1.8327140808105469
I0212 15:52:01.602164 140285423572736 logging_writer.py:48] [8700] global_step=8700, grad_norm=2.1770551204681396, loss=1.8229849338531494
I0212 15:53:22.897246 140285415180032 logging_writer.py:48] [8800] global_step=8800, grad_norm=2.6877388954162598, loss=1.8451507091522217
I0212 15:54:52.595713 140285423572736 logging_writer.py:48] [8900] global_step=8900, grad_norm=3.215409755706787, loss=1.8724536895751953
I0212 15:56:22.895858 140285415180032 logging_writer.py:48] [9000] global_step=9000, grad_norm=2.5522449016571045, loss=1.87472665309906
I0212 15:57:49.988158 140285423572736 logging_writer.py:48] [9100] global_step=9100, grad_norm=9.052183151245117, loss=1.8864915370941162
I0212 15:59:21.427503 140285415180032 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.7337558269500732, loss=1.8982312679290771
I0212 16:00:51.144617 140285423572736 logging_writer.py:48] [9300] global_step=9300, grad_norm=2.0396671295166016, loss=1.9132065773010254
I0212 16:02:08.874803 140285415180032 logging_writer.py:48] [9400] global_step=9400, grad_norm=2.218527317047119, loss=1.9307548999786377
I0212 16:03:28.248622 140285423572736 logging_writer.py:48] [9500] global_step=9500, grad_norm=3.4950954914093018, loss=1.8325860500335693
I0212 16:04:50.684448 140285415180032 logging_writer.py:48] [9600] global_step=9600, grad_norm=2.982257127761841, loss=1.8337987661361694
I0212 16:06:17.207574 140285423572736 logging_writer.py:48] [9700] global_step=9700, grad_norm=3.6931746006011963, loss=1.9153364896774292
I0212 16:07:44.499620 140285415180032 logging_writer.py:48] [9800] global_step=9800, grad_norm=2.265766143798828, loss=1.8768601417541504
I0212 16:09:09.823846 140285423572736 logging_writer.py:48] [9900] global_step=9900, grad_norm=3.143162250518799, loss=1.7783740758895874
I0212 16:10:40.588237 140285415180032 logging_writer.py:48] [10000] global_step=10000, grad_norm=3.499849796295166, loss=1.7867203950881958
I0212 16:12:12.363441 140285423572736 logging_writer.py:48] [10100] global_step=10100, grad_norm=2.746277093887329, loss=1.8156894445419312
I0212 16:13:39.934521 140285415180032 logging_writer.py:48] [10200] global_step=10200, grad_norm=2.7286314964294434, loss=1.8040046691894531
I0212 16:13:55.771941 140441227016000 spec.py:321] Evaluating on the training split.
I0212 16:14:51.157662 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 16:15:45.649334 140441227016000 spec.py:349] Evaluating on the test split.
I0212 16:16:12.647011 140441227016000 submission_runner.py:408] Time since start: 9641.66s, 	Step: 10220, 	{'train/ctc_loss': Array(0.78038126, dtype=float32), 'train/wer': 0.2305007804084504, 'validation/ctc_loss': Array(0.8497801, dtype=float32), 'validation/wer': 0.23874991552178573, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5482055, dtype=float32), 'test/wer': 0.16828143724737474, 'test/num_examples': 2472, 'score': 8657.429280042648, 'total_duration': 9641.66242980957, 'accumulated_submission_time': 8657.429280042648, 'accumulated_eval_time': 983.4033079147339, 'accumulated_logging_time': 0.36255407333374023}
I0212 16:16:12.688163 140285423572736 logging_writer.py:48] [10220] accumulated_eval_time=983.403308, accumulated_logging_time=0.362554, accumulated_submission_time=8657.429280, global_step=10220, preemption_count=0, score=8657.429280, test/ctc_loss=0.5482054948806763, test/num_examples=2472, test/wer=0.168281, total_duration=9641.662430, train/ctc_loss=0.7803812623023987, train/wer=0.230501, validation/ctc_loss=0.8497800827026367, validation/num_examples=5348, validation/wer=0.238750
I0212 16:17:16.997682 140285423572736 logging_writer.py:48] [10300] global_step=10300, grad_norm=2.514993906021118, loss=1.883413553237915
I0212 16:18:33.395557 140285415180032 logging_writer.py:48] [10400] global_step=10400, grad_norm=3.8975296020507812, loss=1.871023416519165
I0212 16:19:52.673060 140285423572736 logging_writer.py:48] [10500] global_step=10500, grad_norm=2.1352577209472656, loss=1.7622214555740356
I0212 16:21:12.787034 140285415180032 logging_writer.py:48] [10600] global_step=10600, grad_norm=2.019864797592163, loss=1.7619149684906006
I0212 16:22:37.436492 140285423572736 logging_writer.py:48] [10700] global_step=10700, grad_norm=4.383542060852051, loss=1.819725513458252
I0212 16:24:08.600990 140285415180032 logging_writer.py:48] [10800] global_step=10800, grad_norm=3.696371555328369, loss=1.852247953414917
I0212 16:25:40.110357 140285423572736 logging_writer.py:48] [10900] global_step=10900, grad_norm=4.818033218383789, loss=1.8891618251800537
I0212 16:27:10.833877 140285415180032 logging_writer.py:48] [11000] global_step=11000, grad_norm=2.7469990253448486, loss=1.755135178565979
I0212 16:28:41.031991 140285423572736 logging_writer.py:48] [11100] global_step=11100, grad_norm=2.7497706413269043, loss=1.8390929698944092
I0212 16:30:08.766782 140285415180032 logging_writer.py:48] [11200] global_step=11200, grad_norm=5.181008815765381, loss=1.7690019607543945
I0212 16:31:37.510801 140285423572736 logging_writer.py:48] [11300] global_step=11300, grad_norm=2.225881338119507, loss=1.8098442554473877
I0212 16:33:00.828407 140285423572736 logging_writer.py:48] [11400] global_step=11400, grad_norm=3.3542122840881348, loss=1.82445228099823
I0212 16:34:17.108566 140285415180032 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.8577197790145874, loss=1.8406484127044678
I0212 16:35:35.427552 140285423572736 logging_writer.py:48] [11600] global_step=11600, grad_norm=2.194943904876709, loss=1.8393677473068237
I0212 16:36:56.599181 140285415180032 logging_writer.py:48] [11700] global_step=11700, grad_norm=4.558579444885254, loss=1.805660605430603
I0212 16:38:22.348638 140285423572736 logging_writer.py:48] [11800] global_step=11800, grad_norm=2.2787904739379883, loss=1.863144040107727
I0212 16:39:52.731194 140285415180032 logging_writer.py:48] [11900] global_step=11900, grad_norm=3.086819648742676, loss=1.793904423713684
I0212 16:40:12.843856 140441227016000 spec.py:321] Evaluating on the training split.
I0212 16:41:08.234045 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 16:42:02.161880 140441227016000 spec.py:349] Evaluating on the test split.
I0212 16:42:29.943088 140441227016000 submission_runner.py:408] Time since start: 11218.96s, 	Step: 11925, 	{'train/ctc_loss': Array(0.6961746, dtype=float32), 'train/wer': 0.213959671084794, 'validation/ctc_loss': Array(0.8327891, dtype=float32), 'validation/wer': 0.23424119254274597, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5307556, dtype=float32), 'test/wer': 0.16681900351390327, 'test/num_examples': 2472, 'score': 10097.492525815964, 'total_duration': 11218.957754611969, 'accumulated_submission_time': 10097.492525815964, 'accumulated_eval_time': 1120.4950244426727, 'accumulated_logging_time': 0.4233393669128418}
I0212 16:42:29.975996 140285423572736 logging_writer.py:48] [11925] accumulated_eval_time=1120.495024, accumulated_logging_time=0.423339, accumulated_submission_time=10097.492526, global_step=11925, preemption_count=0, score=10097.492526, test/ctc_loss=0.5307555794715881, test/num_examples=2472, test/wer=0.166819, total_duration=11218.957755, train/ctc_loss=0.6961746215820312, train/wer=0.213960, validation/ctc_loss=0.8327891230583191, validation/num_examples=5348, validation/wer=0.234241
I0212 16:43:27.183658 140285415180032 logging_writer.py:48] [12000] global_step=12000, grad_norm=2.4238388538360596, loss=1.7805460691452026
I0212 16:44:42.391342 140285423572736 logging_writer.py:48] [12100] global_step=12100, grad_norm=3.6896469593048096, loss=1.769007921218872
I0212 16:46:03.841395 140285415180032 logging_writer.py:48] [12200] global_step=12200, grad_norm=3.0902931690216064, loss=1.8790984153747559
I0212 16:47:31.862252 140285423572736 logging_writer.py:48] [12300] global_step=12300, grad_norm=3.302312135696411, loss=1.8426223993301392
I0212 16:48:59.403434 140285423572736 logging_writer.py:48] [12400] global_step=12400, grad_norm=3.050287961959839, loss=1.7830137014389038
I0212 16:50:15.663566 140285415180032 logging_writer.py:48] [12500] global_step=12500, grad_norm=3.550783634185791, loss=1.7596417665481567
I0212 16:51:35.947038 140285423572736 logging_writer.py:48] [12600] global_step=12600, grad_norm=3.5854713916778564, loss=1.8103662729263306
I0212 16:52:56.922426 140285415180032 logging_writer.py:48] [12700] global_step=12700, grad_norm=2.309316635131836, loss=1.7817591428756714
I0212 16:54:21.026568 140285423572736 logging_writer.py:48] [12800] global_step=12800, grad_norm=3.3575637340545654, loss=1.8567856550216675
I0212 16:55:48.565437 140285415180032 logging_writer.py:48] [12900] global_step=12900, grad_norm=2.903066396713257, loss=1.7713837623596191
I0212 16:57:20.208659 140285423572736 logging_writer.py:48] [13000] global_step=13000, grad_norm=3.580519914627075, loss=1.764041543006897
I0212 16:58:51.146061 140285415180032 logging_writer.py:48] [13100] global_step=13100, grad_norm=2.749929189682007, loss=1.7695448398590088
I0212 17:00:15.234190 140285423572736 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.971830129623413, loss=1.877437710762024
I0212 17:01:44.880671 140285415180032 logging_writer.py:48] [13300] global_step=13300, grad_norm=2.293269395828247, loss=1.7926952838897705
I0212 17:03:17.462825 140285423572736 logging_writer.py:48] [13400] global_step=13400, grad_norm=2.294933557510376, loss=1.80156672000885
I0212 17:04:33.697701 140285415180032 logging_writer.py:48] [13500] global_step=13500, grad_norm=3.290262222290039, loss=1.8337576389312744
I0212 17:05:51.129350 140285423572736 logging_writer.py:48] [13600] global_step=13600, grad_norm=3.4868416786193848, loss=1.8097666501998901
I0212 17:06:30.555249 140441227016000 spec.py:321] Evaluating on the training split.
I0212 17:07:24.451285 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 17:08:19.506670 140441227016000 spec.py:349] Evaluating on the test split.
I0212 17:08:46.030026 140441227016000 submission_runner.py:408] Time since start: 12795.05s, 	Step: 13649, 	{'train/ctc_loss': Array(0.68453616, dtype=float32), 'train/wer': 0.21149859912074895, 'validation/ctc_loss': Array(0.8041042, dtype=float32), 'validation/wer': 0.22599611882946985, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.49814272, dtype=float32), 'test/wer': 0.15863343692238946, 'test/num_examples': 2472, 'score': 11537.983419895172, 'total_duration': 12795.047011137009, 'accumulated_submission_time': 11537.983419895172, 'accumulated_eval_time': 1255.964623451233, 'accumulated_logging_time': 0.4713277816772461}
I0212 17:08:46.067357 140285423572736 logging_writer.py:48] [13649] accumulated_eval_time=1255.964623, accumulated_logging_time=0.471328, accumulated_submission_time=11537.983420, global_step=13649, preemption_count=0, score=11537.983420, test/ctc_loss=0.49814271926879883, test/num_examples=2472, test/wer=0.158633, total_duration=12795.047011, train/ctc_loss=0.6845361590385437, train/wer=0.211499, validation/ctc_loss=0.804104208946228, validation/num_examples=5348, validation/wer=0.225996
I0212 17:09:25.100297 140285415180032 logging_writer.py:48] [13700] global_step=13700, grad_norm=2.197672128677368, loss=1.7841863632202148
I0212 17:10:40.218555 140285423572736 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.8376574516296387, loss=1.7670693397521973
I0212 17:11:57.332038 140285415180032 logging_writer.py:48] [13900] global_step=13900, grad_norm=4.076992988586426, loss=1.869482398033142
I0212 17:13:30.016345 140285423572736 logging_writer.py:48] [14000] global_step=14000, grad_norm=2.1675539016723633, loss=1.81699538230896
I0212 17:15:00.844783 140285415180032 logging_writer.py:48] [14100] global_step=14100, grad_norm=2.204233407974243, loss=1.7620928287506104
I0212 17:16:28.994397 140285423572736 logging_writer.py:48] [14200] global_step=14200, grad_norm=3.5798537731170654, loss=1.799009084701538
I0212 17:17:56.159422 140285415180032 logging_writer.py:48] [14300] global_step=14300, grad_norm=5.364120006561279, loss=1.8143514394760132
I0212 17:19:23.870510 140285423572736 logging_writer.py:48] [14400] global_step=14400, grad_norm=2.8972325325012207, loss=1.7672510147094727
I0212 17:20:44.558166 140285423572736 logging_writer.py:48] [14500] global_step=14500, grad_norm=3.1821823120117188, loss=1.7823920249938965
I0212 17:22:03.819121 140285415180032 logging_writer.py:48] [14600] global_step=14600, grad_norm=2.8984427452087402, loss=1.7308048009872437
I0212 17:23:20.338415 140285423572736 logging_writer.py:48] [14700] global_step=14700, grad_norm=3.6189286708831787, loss=1.7696667909622192
I0212 17:24:42.686763 140285415180032 logging_writer.py:48] [14800] global_step=14800, grad_norm=2.2648494243621826, loss=1.7300667762756348
I0212 17:26:10.236952 140285423572736 logging_writer.py:48] [14900] global_step=14900, grad_norm=5.899809837341309, loss=1.8489611148834229
I0212 17:27:39.182493 140285415180032 logging_writer.py:48] [15000] global_step=15000, grad_norm=3.2960152626037598, loss=1.826181173324585
I0212 17:29:07.598347 140285423572736 logging_writer.py:48] [15100] global_step=15100, grad_norm=3.6878502368927, loss=1.727089285850525
I0212 17:30:37.247377 140285415180032 logging_writer.py:48] [15200] global_step=15200, grad_norm=2.3152756690979004, loss=1.751425862312317
I0212 17:32:08.255008 140285423572736 logging_writer.py:48] [15300] global_step=15300, grad_norm=4.6776227951049805, loss=1.7866586446762085
I0212 17:32:46.792264 140441227016000 spec.py:321] Evaluating on the training split.
I0212 17:33:41.593733 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 17:34:36.015302 140441227016000 spec.py:349] Evaluating on the test split.
I0212 17:35:04.277659 140441227016000 submission_runner.py:408] Time since start: 14373.29s, 	Step: 15342, 	{'train/ctc_loss': Array(0.68597907, dtype=float32), 'train/wer': 0.21039335576396417, 'validation/ctc_loss': Array(0.7896287, dtype=float32), 'validation/wer': 0.22074398756480687, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.49917656, dtype=float32), 'test/wer': 0.1561554242073406, 'test/num_examples': 2472, 'score': 12978.61863040924, 'total_duration': 14373.29497885704, 'accumulated_submission_time': 12978.61863040924, 'accumulated_eval_time': 1393.445172548294, 'accumulated_logging_time': 0.5251693725585938}
I0212 17:35:04.309192 140285423572736 logging_writer.py:48] [15342] accumulated_eval_time=1393.445173, accumulated_logging_time=0.525169, accumulated_submission_time=12978.618630, global_step=15342, preemption_count=0, score=12978.618630, test/ctc_loss=0.499176561832428, test/num_examples=2472, test/wer=0.156155, total_duration=14373.294979, train/ctc_loss=0.6859790682792664, train/wer=0.210393, validation/ctc_loss=0.7896286845207214, validation/num_examples=5348, validation/wer=0.220744
I0212 17:35:48.476110 140285415180032 logging_writer.py:48] [15400] global_step=15400, grad_norm=2.5413691997528076, loss=1.7444775104522705
I0212 17:37:06.909084 140285423572736 logging_writer.py:48] [15500] global_step=15500, grad_norm=4.273727893829346, loss=1.819272756576538
I0212 17:38:22.216859 140285415180032 logging_writer.py:48] [15600] global_step=15600, grad_norm=2.4807615280151367, loss=1.7831454277038574
I0212 17:39:41.428889 140285423572736 logging_writer.py:48] [15700] global_step=15700, grad_norm=2.35121488571167, loss=1.7809315919876099
I0212 17:40:58.392319 140285415180032 logging_writer.py:48] [15800] global_step=15800, grad_norm=2.7910706996917725, loss=1.8270890712738037
I0212 17:42:21.695663 140285423572736 logging_writer.py:48] [15900] global_step=15900, grad_norm=2.4480209350585938, loss=1.7716064453125
I0212 17:43:50.477038 140285415180032 logging_writer.py:48] [16000] global_step=16000, grad_norm=3.3948025703430176, loss=1.8089805841445923
I0212 17:45:21.109496 140285423572736 logging_writer.py:48] [16100] global_step=16100, grad_norm=3.6492767333984375, loss=1.831486701965332
I0212 17:46:50.893610 140285415180032 logging_writer.py:48] [16200] global_step=16200, grad_norm=4.836198806762695, loss=1.7613004446029663
I0212 17:48:18.889035 140285423572736 logging_writer.py:48] [16300] global_step=16300, grad_norm=4.025827884674072, loss=1.8392231464385986
I0212 17:49:46.612848 140285415180032 logging_writer.py:48] [16400] global_step=16400, grad_norm=2.267530679702759, loss=1.758087396621704
I0212 17:51:17.423978 140285423572736 logging_writer.py:48] [16500] global_step=16500, grad_norm=2.4825222492218018, loss=1.6945443153381348
I0212 17:52:32.230588 140285415180032 logging_writer.py:48] [16600] global_step=16600, grad_norm=3.205955743789673, loss=1.7654074430465698
I0212 17:53:50.129829 140285423572736 logging_writer.py:48] [16700] global_step=16700, grad_norm=2.25285005569458, loss=1.7736300230026245
I0212 17:55:08.322725 140285415180032 logging_writer.py:48] [16800] global_step=16800, grad_norm=2.662095069885254, loss=1.7425522804260254
I0212 17:56:30.872519 140285423572736 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.7266736030578613, loss=1.6931723356246948
I0212 17:57:56.852661 140285415180032 logging_writer.py:48] [17000] global_step=17000, grad_norm=2.801297187805176, loss=1.6573768854141235
I0212 17:59:04.718675 140441227016000 spec.py:321] Evaluating on the training split.
I0212 18:00:08.717479 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 18:01:03.226581 140441227016000 spec.py:349] Evaluating on the test split.
I0212 18:01:30.509932 140441227016000 submission_runner.py:408] Time since start: 15959.53s, 	Step: 17076, 	{'train/ctc_loss': Array(0.43890128, dtype=float32), 'train/wer': 0.1444019164369262, 'validation/ctc_loss': Array(0.7677971, dtype=float32), 'validation/wer': 0.21517325274916246, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4785223, dtype=float32), 'test/wer': 0.15109784087908518, 'test/num_examples': 2472, 'score': 14418.936494112015, 'total_duration': 15959.526646614075, 'accumulated_submission_time': 14418.936494112015, 'accumulated_eval_time': 1539.2309548854828, 'accumulated_logging_time': 0.5735268592834473}
I0212 18:01:30.543547 140285423572736 logging_writer.py:48] [17076] accumulated_eval_time=1539.230955, accumulated_logging_time=0.573527, accumulated_submission_time=14418.936494, global_step=17076, preemption_count=0, score=14418.936494, test/ctc_loss=0.47852230072021484, test/num_examples=2472, test/wer=0.151098, total_duration=15959.526647, train/ctc_loss=0.43890127539634705, train/wer=0.144402, validation/ctc_loss=0.7677971124649048, validation/num_examples=5348, validation/wer=0.215173
I0212 18:01:49.568935 140285415180032 logging_writer.py:48] [17100] global_step=17100, grad_norm=3.2533562183380127, loss=1.756230354309082
I0212 18:03:04.714293 140285423572736 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.840869426727295, loss=1.7151007652282715
I0212 18:04:19.892737 140285415180032 logging_writer.py:48] [17300] global_step=17300, grad_norm=4.95068883895874, loss=1.7943717241287231
I0212 18:05:49.983818 140285423572736 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.8637385368347168, loss=1.6982554197311401
I0212 18:07:18.099483 140285415180032 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.838202714920044, loss=1.7303191423416138
I0212 18:08:39.747032 140285423572736 logging_writer.py:48] [17600] global_step=17600, grad_norm=2.597599744796753, loss=1.6996214389801025
I0212 18:09:57.439279 140285415180032 logging_writer.py:48] [17700] global_step=17700, grad_norm=2.2133913040161133, loss=1.8133643865585327
I0212 18:11:18.130204 140285423572736 logging_writer.py:48] [17800] global_step=17800, grad_norm=2.430288314819336, loss=1.7381930351257324
I0212 18:12:40.322391 140285415180032 logging_writer.py:48] [17900] global_step=17900, grad_norm=2.4797768592834473, loss=1.7246397733688354
I0212 18:14:06.646867 140285423572736 logging_writer.py:48] [18000] global_step=18000, grad_norm=4.9507527351379395, loss=1.7259446382522583
I0212 18:15:34.649678 140285415180032 logging_writer.py:48] [18100] global_step=18100, grad_norm=3.6104416847229004, loss=1.7259596586227417
I0212 18:17:05.248608 140285423572736 logging_writer.py:48] [18200] global_step=18200, grad_norm=2.101658344268799, loss=1.7482880353927612
I0212 18:18:35.218556 140285415180032 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.6431314945220947, loss=1.7382760047912598
I0212 18:20:07.361098 140285423572736 logging_writer.py:48] [18400] global_step=18400, grad_norm=2.5483388900756836, loss=1.6438772678375244
I0212 18:21:37.957574 140285415180032 logging_writer.py:48] [18500] global_step=18500, grad_norm=3.0521433353424072, loss=1.7618401050567627
I0212 18:23:02.162966 140285423572736 logging_writer.py:48] [18600] global_step=18600, grad_norm=2.5252296924591064, loss=1.6546374559402466
I0212 18:24:18.825566 140285415180032 logging_writer.py:48] [18700] global_step=18700, grad_norm=3.0612645149230957, loss=1.6894757747650146
I0212 18:25:31.125798 140441227016000 spec.py:321] Evaluating on the training split.
I0212 18:26:27.213597 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 18:27:20.395671 140441227016000 spec.py:349] Evaluating on the test split.
I0212 18:27:46.757504 140441227016000 submission_runner.py:408] Time since start: 17535.77s, 	Step: 18793, 	{'train/ctc_loss': Array(0.4114516, dtype=float32), 'train/wer': 0.13331886798301354, 'validation/ctc_loss': Array(0.739945, dtype=float32), 'validation/wer': 0.20781640711741023, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.45743635, dtype=float32), 'test/wer': 0.1456137143785672, 'test/num_examples': 2472, 'score': 15859.429361820221, 'total_duration': 17535.774045705795, 'accumulated_submission_time': 15859.429361820221, 'accumulated_eval_time': 1674.8570148944855, 'accumulated_logging_time': 0.6237502098083496}
I0212 18:27:46.794172 140285423572736 logging_writer.py:48] [18793] accumulated_eval_time=1674.857015, accumulated_logging_time=0.623750, accumulated_submission_time=15859.429362, global_step=18793, preemption_count=0, score=15859.429362, test/ctc_loss=0.45743635296821594, test/num_examples=2472, test/wer=0.145614, total_duration=17535.774046, train/ctc_loss=0.4114516079425812, train/wer=0.133319, validation/ctc_loss=0.7399449944496155, validation/num_examples=5348, validation/wer=0.207816
I0212 18:27:52.888960 140285415180032 logging_writer.py:48] [18800] global_step=18800, grad_norm=2.9631388187408447, loss=1.7168457508087158
I0212 18:29:07.766663 140285423572736 logging_writer.py:48] [18900] global_step=18900, grad_norm=2.7329165935516357, loss=1.7132830619812012
I0212 18:30:23.515354 140285415180032 logging_writer.py:48] [19000] global_step=19000, grad_norm=2.491238832473755, loss=1.6824417114257812
I0212 18:31:44.430668 140285423572736 logging_writer.py:48] [19100] global_step=19100, grad_norm=2.764960289001465, loss=1.738446831703186
I0212 18:33:15.619640 140285415180032 logging_writer.py:48] [19200] global_step=19200, grad_norm=3.2442619800567627, loss=1.7174214124679565
I0212 18:34:43.717321 140285423572736 logging_writer.py:48] [19300] global_step=19300, grad_norm=3.2906408309936523, loss=1.605741024017334
I0212 18:36:11.875218 140285415180032 logging_writer.py:48] [19400] global_step=19400, grad_norm=2.497278928756714, loss=1.6766002178192139
I0212 18:37:41.460584 140285423572736 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.7976152896881104, loss=1.6038837432861328
I0212 18:39:07.954747 140285423572736 logging_writer.py:48] [19600] global_step=19600, grad_norm=3.032705545425415, loss=1.7038418054580688
I0212 18:40:24.748132 140285415180032 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.6636019945144653, loss=1.7446885108947754
I0212 18:41:41.748311 140285423572736 logging_writer.py:48] [19800] global_step=19800, grad_norm=2.7672719955444336, loss=1.6141091585159302
I0212 18:43:03.283322 140285415180032 logging_writer.py:48] [19900] global_step=19900, grad_norm=5.277750015258789, loss=1.7304580211639404
I0212 18:44:23.729124 140285423572736 logging_writer.py:48] [20000] global_step=20000, grad_norm=3.575937271118164, loss=1.6585710048675537
I0212 18:45:52.631741 140285415180032 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.9925081729888916, loss=1.651107668876648
I0212 18:47:23.041297 140285423572736 logging_writer.py:48] [20200] global_step=20200, grad_norm=2.152651309967041, loss=1.6362407207489014
I0212 18:48:49.063280 140285415180032 logging_writer.py:48] [20300] global_step=20300, grad_norm=2.6714494228363037, loss=1.6546697616577148
I0212 18:50:18.045968 140285423572736 logging_writer.py:48] [20400] global_step=20400, grad_norm=3.4363560676574707, loss=1.6515874862670898
I0212 18:51:44.577762 140285415180032 logging_writer.py:48] [20500] global_step=20500, grad_norm=2.0614991188049316, loss=1.7551378011703491
I0212 18:51:47.665565 140441227016000 spec.py:321] Evaluating on the training split.
I0212 18:52:44.124672 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 18:53:38.076590 140441227016000 spec.py:349] Evaluating on the test split.
I0212 18:54:05.385591 140441227016000 submission_runner.py:408] Time since start: 19114.40s, 	Step: 20505, 	{'train/ctc_loss': Array(0.38734055, dtype=float32), 'train/wer': 0.129115201993911, 'validation/ctc_loss': Array(0.71387607, dtype=float32), 'validation/wer': 0.20162777450592312, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43603286, dtype=float32), 'test/wer': 0.13740783620742186, 'test/num_examples': 2472, 'score': 17300.21093249321, 'total_duration': 19114.40056157112, 'accumulated_submission_time': 17300.21093249321, 'accumulated_eval_time': 1812.569813489914, 'accumulated_logging_time': 0.6772727966308594}
I0212 18:54:05.420955 140285423572736 logging_writer.py:48] [20505] accumulated_eval_time=1812.569813, accumulated_logging_time=0.677273, accumulated_submission_time=17300.210932, global_step=20505, preemption_count=0, score=17300.210932, test/ctc_loss=0.43603286147117615, test/num_examples=2472, test/wer=0.137408, total_duration=19114.400562, train/ctc_loss=0.3873405456542969, train/wer=0.129115, validation/ctc_loss=0.7138760685920715, validation/num_examples=5348, validation/wer=0.201628
I0212 18:55:21.307108 140285423572736 logging_writer.py:48] [20600] global_step=20600, grad_norm=2.6146774291992188, loss=1.5984041690826416
I0212 18:56:38.458862 140285415180032 logging_writer.py:48] [20700] global_step=20700, grad_norm=2.2338390350341797, loss=1.6479060649871826
I0212 18:57:56.486574 140285423572736 logging_writer.py:48] [20800] global_step=20800, grad_norm=2.5718801021575928, loss=1.639461874961853
I0212 18:59:15.767116 140285415180032 logging_writer.py:48] [20900] global_step=20900, grad_norm=2.2228474617004395, loss=1.642665147781372
I0212 19:00:42.316864 140285423572736 logging_writer.py:48] [21000] global_step=21000, grad_norm=2.004444122314453, loss=1.6231892108917236
I0212 19:02:06.935444 140285415180032 logging_writer.py:48] [21100] global_step=21100, grad_norm=4.1916823387146, loss=1.712273120880127
I0212 19:03:34.994400 140285423572736 logging_writer.py:48] [21200] global_step=21200, grad_norm=2.5572266578674316, loss=1.6566057205200195
I0212 19:05:01.277746 140285415180032 logging_writer.py:48] [21300] global_step=21300, grad_norm=3.3909807205200195, loss=1.6660592555999756
I0212 19:06:32.507097 140285423572736 logging_writer.py:48] [21400] global_step=21400, grad_norm=2.7563154697418213, loss=1.6792007684707642
I0212 19:08:02.227671 140285415180032 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.6719412803649902, loss=1.6134294271469116
I0212 19:09:33.175487 140285423572736 logging_writer.py:48] [21600] global_step=21600, grad_norm=2.3817808628082275, loss=1.6616259813308716
I0212 19:10:58.764536 140285423572736 logging_writer.py:48] [21700] global_step=21700, grad_norm=2.67800235748291, loss=1.5997833013534546
I0212 19:12:17.263841 140285415180032 logging_writer.py:48] [21800] global_step=21800, grad_norm=2.566413640975952, loss=1.6633071899414062
I0212 19:13:33.267482 140285423572736 logging_writer.py:48] [21900] global_step=21900, grad_norm=2.0462582111358643, loss=1.6333565711975098
I0212 19:14:52.865540 140285415180032 logging_writer.py:48] [22000] global_step=22000, grad_norm=2.8689584732055664, loss=1.6762933731079102
I0212 19:16:20.985248 140285423572736 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.7744570970535278, loss=1.653440237045288
I0212 19:17:52.698087 140285415180032 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.3200101852416992, loss=1.637992262840271
I0212 19:18:05.640427 140441227016000 spec.py:321] Evaluating on the training split.
I0212 19:19:01.760505 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 19:19:56.041703 140441227016000 spec.py:349] Evaluating on the test split.
I0212 19:20:22.487502 140441227016000 submission_runner.py:408] Time since start: 20691.50s, 	Step: 22215, 	{'train/ctc_loss': Array(0.3631994, dtype=float32), 'train/wer': 0.12213056330055974, 'validation/ctc_loss': Array(0.67903066, dtype=float32), 'validation/wer': 0.1932861542620466, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.41533288, dtype=float32), 'test/wer': 0.13015660227895923, 'test/num_examples': 2472, 'score': 18740.341069221497, 'total_duration': 20691.503726243973, 'accumulated_submission_time': 18740.341069221497, 'accumulated_eval_time': 1949.4109256267548, 'accumulated_logging_time': 0.7298746109008789}
I0212 19:20:22.523769 140285423572736 logging_writer.py:48] [22215] accumulated_eval_time=1949.410926, accumulated_logging_time=0.729875, accumulated_submission_time=18740.341069, global_step=22215, preemption_count=0, score=18740.341069, test/ctc_loss=0.4153328835964203, test/num_examples=2472, test/wer=0.130157, total_duration=20691.503726, train/ctc_loss=0.3631994128227234, train/wer=0.122131, validation/ctc_loss=0.6790306568145752, validation/num_examples=5348, validation/wer=0.193286
I0212 19:21:27.266992 140285415180032 logging_writer.py:48] [22300] global_step=22300, grad_norm=5.263276100158691, loss=1.6037410497665405
I0212 19:22:42.421411 140285423572736 logging_writer.py:48] [22400] global_step=22400, grad_norm=2.0275719165802, loss=1.634716510772705
I0212 19:24:07.520359 140285415180032 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.9091635942459106, loss=1.6486539840698242
I0212 19:25:35.010842 140285423572736 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.6334372758865356, loss=1.6323955059051514
I0212 19:27:02.626982 140285423572736 logging_writer.py:48] [22700] global_step=22700, grad_norm=3.2509799003601074, loss=1.574723482131958
I0212 19:28:21.418440 140285415180032 logging_writer.py:48] [22800] global_step=22800, grad_norm=2.086791515350342, loss=1.5910825729370117
I0212 19:29:39.870899 140285423572736 logging_writer.py:48] [22900] global_step=22900, grad_norm=2.918484926223755, loss=1.5826871395111084
I0212 19:31:01.742830 140285415180032 logging_writer.py:48] [23000] global_step=23000, grad_norm=2.3231148719787598, loss=1.5882962942123413
I0212 19:32:23.870394 140285423572736 logging_writer.py:48] [23100] global_step=23100, grad_norm=3.3896284103393555, loss=1.6313635110855103
I0212 19:33:53.010601 140285415180032 logging_writer.py:48] [23200] global_step=23200, grad_norm=2.405686378479004, loss=1.5663459300994873
I0212 19:35:20.990180 140285423572736 logging_writer.py:48] [23300] global_step=23300, grad_norm=2.195091962814331, loss=1.570772647857666
I0212 19:36:53.527992 140285415180032 logging_writer.py:48] [23400] global_step=23400, grad_norm=2.6937761306762695, loss=1.5790890455245972
I0212 19:38:19.641587 140285423572736 logging_writer.py:48] [23500] global_step=23500, grad_norm=6.331902027130127, loss=1.6400034427642822
I0212 19:39:46.388564 140285415180032 logging_writer.py:48] [23600] global_step=23600, grad_norm=3.3331992626190186, loss=1.5340675115585327
I0212 19:41:18.765400 140285423572736 logging_writer.py:48] [23700] global_step=23700, grad_norm=3.0516369342803955, loss=1.626771330833435
I0212 19:42:36.913113 140285415180032 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.68662428855896, loss=1.5748976469039917
I0212 19:43:52.887897 140285423572736 logging_writer.py:48] [23900] global_step=23900, grad_norm=2.161022663116455, loss=1.611512303352356
I0212 19:44:22.825818 140441227016000 spec.py:321] Evaluating on the training split.
I0212 19:45:18.841022 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 19:46:11.558458 140441227016000 spec.py:349] Evaluating on the test split.
I0212 19:46:39.292770 140441227016000 submission_runner.py:408] Time since start: 22268.31s, 	Step: 23940, 	{'train/ctc_loss': Array(0.357722, dtype=float32), 'train/wer': 0.11924585829726445, 'validation/ctc_loss': Array(0.66510963, dtype=float32), 'validation/wer': 0.19041872230321402, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39538053, dtype=float32), 'test/wer': 0.12595210529522882, 'test/num_examples': 2472, 'score': 20180.55277776718, 'total_duration': 22268.30971980095, 'accumulated_submission_time': 20180.55277776718, 'accumulated_eval_time': 2085.8726336956024, 'accumulated_logging_time': 0.7837283611297607}
I0212 19:46:39.329672 140285423572736 logging_writer.py:48] [23940] accumulated_eval_time=2085.872634, accumulated_logging_time=0.783728, accumulated_submission_time=20180.552778, global_step=23940, preemption_count=0, score=20180.552778, test/ctc_loss=0.39538052678108215, test/num_examples=2472, test/wer=0.125952, total_duration=22268.309720, train/ctc_loss=0.3577220141887665, train/wer=0.119246, validation/ctc_loss=0.6651096343994141, validation/num_examples=5348, validation/wer=0.190419
I0212 19:47:25.115986 140285415180032 logging_writer.py:48] [24000] global_step=24000, grad_norm=3.3651294708251953, loss=1.5881619453430176
I0212 19:48:39.827256 140285423572736 logging_writer.py:48] [24100] global_step=24100, grad_norm=2.1322906017303467, loss=1.593894124031067
I0212 19:49:58.074427 140285415180032 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.8343744277954102, loss=1.5974156856536865
I0212 19:51:28.887190 140285423572736 logging_writer.py:48] [24300] global_step=24300, grad_norm=2.591088056564331, loss=1.5186175107955933
I0212 19:52:56.270445 140285415180032 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.5723505020141602, loss=1.5448824167251587
I0212 19:54:25.161794 140285423572736 logging_writer.py:48] [24500] global_step=24500, grad_norm=3.9902868270874023, loss=1.6724300384521484
I0212 19:55:57.053337 140285415180032 logging_writer.py:48] [24600] global_step=24600, grad_norm=2.0071144104003906, loss=1.609651803970337
I0212 19:57:29.373957 140285423572736 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.9499764442443848, loss=1.6399935483932495
I0212 19:58:51.714699 140285423572736 logging_writer.py:48] [24800] global_step=24800, grad_norm=2.2638678550720215, loss=1.592753529548645
I0212 20:00:08.194475 140285415180032 logging_writer.py:48] [24900] global_step=24900, grad_norm=2.5455472469329834, loss=1.5975415706634521
I0212 20:01:30.175201 140285423572736 logging_writer.py:48] [25000] global_step=25000, grad_norm=3.0906198024749756, loss=1.596288800239563
I0212 20:02:53.857597 140285415180032 logging_writer.py:48] [25100] global_step=25100, grad_norm=4.318281173706055, loss=1.62149977684021
I0212 20:04:21.488773 140285423572736 logging_writer.py:48] [25200] global_step=25200, grad_norm=2.3033127784729004, loss=1.6203399896621704
I0212 20:05:50.212278 140285415180032 logging_writer.py:48] [25300] global_step=25300, grad_norm=3.603447675704956, loss=1.6149402856826782
I0212 20:07:19.402313 140285423572736 logging_writer.py:48] [25400] global_step=25400, grad_norm=3.662014961242676, loss=1.6293376684188843
I0212 20:08:46.405295 140285415180032 logging_writer.py:48] [25500] global_step=25500, grad_norm=4.162583827972412, loss=1.6084198951721191
I0212 20:10:15.891623 140285423572736 logging_writer.py:48] [25600] global_step=25600, grad_norm=3.586491107940674, loss=1.5946530103683472
I0212 20:10:39.407953 140441227016000 spec.py:321] Evaluating on the training split.
I0212 20:11:35.426472 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 20:12:28.481945 140441227016000 spec.py:349] Evaluating on the test split.
I0212 20:12:54.778986 140441227016000 submission_runner.py:408] Time since start: 23843.80s, 	Step: 25627, 	{'train/ctc_loss': Array(0.327648, dtype=float32), 'train/wer': 0.1109453339589517, 'validation/ctc_loss': Array(0.6450211, dtype=float32), 'validation/wer': 0.18391148614074554, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3858443, dtype=float32), 'test/wer': 0.12211321674486625, 'test/num_examples': 2472, 'score': 21620.54094862938, 'total_duration': 23843.795273303986, 'accumulated_submission_time': 21620.54094862938, 'accumulated_eval_time': 2221.237764120102, 'accumulated_logging_time': 0.8373799324035645}
I0212 20:12:54.818917 140285423572736 logging_writer.py:48] [25627] accumulated_eval_time=2221.237764, accumulated_logging_time=0.837380, accumulated_submission_time=21620.540949, global_step=25627, preemption_count=0, score=21620.540949, test/ctc_loss=0.38584429025650024, test/num_examples=2472, test/wer=0.122113, total_duration=23843.795273, train/ctc_loss=0.32764801383018494, train/wer=0.110945, validation/ctc_loss=0.6450210809707642, validation/num_examples=5348, validation/wer=0.183911
I0212 20:13:50.307553 140285415180032 logging_writer.py:48] [25700] global_step=25700, grad_norm=3.0671591758728027, loss=1.5932068824768066
I0212 20:15:10.275007 140285423572736 logging_writer.py:48] [25800] global_step=25800, grad_norm=5.382083415985107, loss=1.65298330783844
I0212 20:16:29.248555 140285415180032 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.802560806274414, loss=1.6328251361846924
I0212 20:17:50.752415 140285423572736 logging_writer.py:48] [26000] global_step=26000, grad_norm=2.2826433181762695, loss=1.6096678972244263
I0212 20:19:15.035934 140285415180032 logging_writer.py:48] [26100] global_step=26100, grad_norm=3.136805772781372, loss=1.525097131729126
I0212 20:20:41.763993 140285423572736 logging_writer.py:48] [26200] global_step=26200, grad_norm=2.332703113555908, loss=1.5827531814575195
I0212 20:22:14.116308 140285415180032 logging_writer.py:48] [26300] global_step=26300, grad_norm=2.3716025352478027, loss=1.5005102157592773
I0212 20:23:44.834429 140285423572736 logging_writer.py:48] [26400] global_step=26400, grad_norm=3.3107385635375977, loss=1.503169298171997
I0212 20:25:13.094530 140285415180032 logging_writer.py:48] [26500] global_step=26500, grad_norm=2.762777328491211, loss=1.5635052919387817
I0212 20:26:41.813059 140285423572736 logging_writer.py:48] [26600] global_step=26600, grad_norm=3.6364309787750244, loss=1.6188100576400757
I0212 20:28:14.692504 140285415180032 logging_writer.py:48] [26700] global_step=26700, grad_norm=2.5181355476379395, loss=1.5479291677474976
I0212 20:29:44.486804 140285423572736 logging_writer.py:48] [26800] global_step=26800, grad_norm=2.6380763053894043, loss=1.4705809354782104
I0212 20:31:01.617817 140285415180032 logging_writer.py:48] [26900] global_step=26900, grad_norm=2.0085995197296143, loss=1.5217522382736206
I0212 20:32:22.744729 140285423572736 logging_writer.py:48] [27000] global_step=27000, grad_norm=2.705514669418335, loss=1.5505738258361816
I0212 20:33:44.614279 140285415180032 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.8697210550308228, loss=1.5612715482711792
I0212 20:35:08.212153 140285423572736 logging_writer.py:48] [27200] global_step=27200, grad_norm=2.214794158935547, loss=1.4642804861068726
I0212 20:36:36.792280 140285415180032 logging_writer.py:48] [27300] global_step=27300, grad_norm=2.4141273498535156, loss=1.5351842641830444
I0212 20:36:55.149951 140441227016000 spec.py:321] Evaluating on the training split.
I0212 20:37:50.134258 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 20:38:42.702434 140441227016000 spec.py:349] Evaluating on the test split.
I0212 20:39:10.080067 140441227016000 submission_runner.py:408] Time since start: 25419.09s, 	Step: 27322, 	{'train/ctc_loss': Array(0.35698888, dtype=float32), 'train/wer': 0.115199530197328, 'validation/ctc_loss': Array(0.6116994, dtype=float32), 'validation/wer': 0.17459474593780472, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36358392, dtype=float32), 'test/wer': 0.11715719131476855, 'test/num_examples': 2472, 'score': 23060.78440976143, 'total_duration': 25419.094624519348, 'accumulated_submission_time': 23060.78440976143, 'accumulated_eval_time': 2356.1602787971497, 'accumulated_logging_time': 0.893054723739624}
I0212 20:39:10.118315 140285423572736 logging_writer.py:48] [27322] accumulated_eval_time=2356.160279, accumulated_logging_time=0.893055, accumulated_submission_time=23060.784410, global_step=27322, preemption_count=0, score=23060.784410, test/ctc_loss=0.36358392238616943, test/num_examples=2472, test/wer=0.117157, total_duration=25419.094625, train/ctc_loss=0.3569888770580292, train/wer=0.115200, validation/ctc_loss=0.6116994023323059, validation/num_examples=5348, validation/wer=0.174595
I0212 20:40:08.962364 140285415180032 logging_writer.py:48] [27400] global_step=27400, grad_norm=2.45050311088562, loss=1.4956841468811035
I0212 20:41:23.476038 140285423572736 logging_writer.py:48] [27500] global_step=27500, grad_norm=3.149545192718506, loss=1.523903489112854
I0212 20:42:49.080438 140285415180032 logging_writer.py:48] [27600] global_step=27600, grad_norm=2.4963438510894775, loss=1.5823798179626465
I0212 20:44:15.782588 140285423572736 logging_writer.py:48] [27700] global_step=27700, grad_norm=2.561983823776245, loss=1.5924755334854126
I0212 20:45:46.788664 140285415180032 logging_writer.py:48] [27800] global_step=27800, grad_norm=3.5423765182495117, loss=1.5278626680374146
I0212 20:47:08.464969 140285423572736 logging_writer.py:48] [27900] global_step=27900, grad_norm=2.1062142848968506, loss=1.494417667388916
I0212 20:48:24.841551 140285415180032 logging_writer.py:48] [28000] global_step=28000, grad_norm=2.2234280109405518, loss=1.4735301733016968
I0212 20:49:44.539648 140285423572736 logging_writer.py:48] [28100] global_step=28100, grad_norm=2.493410348892212, loss=1.4958528280258179
I0212 20:51:09.642083 140285415180032 logging_writer.py:48] [28200] global_step=28200, grad_norm=2.1722540855407715, loss=1.4919627904891968
I0212 20:52:37.386505 140285423572736 logging_writer.py:48] [28300] global_step=28300, grad_norm=2.2085165977478027, loss=1.537949562072754
I0212 20:54:08.115168 140285415180032 logging_writer.py:48] [28400] global_step=28400, grad_norm=2.251260280609131, loss=1.5355286598205566
I0212 20:55:40.854503 140285423572736 logging_writer.py:48] [28500] global_step=28500, grad_norm=2.3877978324890137, loss=1.5077686309814453
I0212 20:57:06.496461 140285415180032 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.6962227821350098, loss=1.472792625427246
I0212 20:58:37.023730 140285423572736 logging_writer.py:48] [28700] global_step=28700, grad_norm=2.373363971710205, loss=1.4810794591903687
I0212 21:00:07.862947 140285415180032 logging_writer.py:48] [28800] global_step=28800, grad_norm=3.059950351715088, loss=1.4498498439788818
I0212 21:01:29.760403 140285423572736 logging_writer.py:48] [28900] global_step=28900, grad_norm=2.3524975776672363, loss=1.490792989730835
I0212 21:02:46.102107 140285415180032 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.6807419061660767, loss=1.480154037475586
I0212 21:03:10.096001 140441227016000 spec.py:321] Evaluating on the training split.
I0212 21:04:08.424593 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 21:05:01.372197 140441227016000 spec.py:349] Evaluating on the test split.
I0212 21:05:28.688604 140441227016000 submission_runner.py:408] Time since start: 26997.71s, 	Step: 29032, 	{'train/ctc_loss': Array(0.30421615, dtype=float32), 'train/wer': 0.10361551330156368, 'validation/ctc_loss': Array(0.59509784, dtype=float32), 'validation/wer': 0.17090666846886857, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3473988, dtype=float32), 'test/wer': 0.11404951963114171, 'test/num_examples': 2472, 'score': 24500.674536943436, 'total_duration': 26997.705008029938, 'accumulated_submission_time': 24500.674536943436, 'accumulated_eval_time': 2494.747106552124, 'accumulated_logging_time': 0.9459781646728516}
I0212 21:05:28.722521 140285423572736 logging_writer.py:48] [29032] accumulated_eval_time=2494.747107, accumulated_logging_time=0.945978, accumulated_submission_time=24500.674537, global_step=29032, preemption_count=0, score=24500.674537, test/ctc_loss=0.3473987877368927, test/num_examples=2472, test/wer=0.114050, total_duration=26997.705008, train/ctc_loss=0.3042161464691162, train/wer=0.103616, validation/ctc_loss=0.5950978398323059, validation/num_examples=5348, validation/wer=0.170907
I0212 21:06:20.197472 140285415180032 logging_writer.py:48] [29100] global_step=29100, grad_norm=2.3017308712005615, loss=1.4851927757263184
I0212 21:07:34.837986 140285423572736 logging_writer.py:48] [29200] global_step=29200, grad_norm=2.295668125152588, loss=1.481292724609375
I0212 21:08:52.866192 140285415180032 logging_writer.py:48] [29300] global_step=29300, grad_norm=2.8507983684539795, loss=1.554632306098938
I0212 21:10:20.653170 140285423572736 logging_writer.py:48] [29400] global_step=29400, grad_norm=3.683166027069092, loss=1.4620152711868286
I0212 21:11:49.396281 140285415180032 logging_writer.py:48] [29500] global_step=29500, grad_norm=2.0445635318756104, loss=1.4786605834960938
I0212 21:13:14.343233 140285423572736 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.845586895942688, loss=1.4701555967330933
I0212 21:14:42.829975 140285415180032 logging_writer.py:48] [29700] global_step=29700, grad_norm=2.561434268951416, loss=1.4492298364639282
I0212 21:16:13.929210 140285423572736 logging_writer.py:48] [29800] global_step=29800, grad_norm=3.7330355644226074, loss=1.4879910945892334
I0212 21:17:44.358983 140285423572736 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.9590671062469482, loss=1.4569803476333618
I0212 21:19:00.046290 140285415180032 logging_writer.py:48] [30000] global_step=30000, grad_norm=2.57597279548645, loss=1.5137555599212646
I0212 21:20:20.385411 140285423572736 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.7184008359909058, loss=1.4428234100341797
I0212 21:21:41.028237 140285415180032 logging_writer.py:48] [30200] global_step=30200, grad_norm=2.3021957874298096, loss=1.4170808792114258
I0212 21:23:05.659357 140285423572736 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.727530837059021, loss=1.471935510635376
I0212 21:24:35.479555 140285415180032 logging_writer.py:48] [30400] global_step=30400, grad_norm=2.0928561687469482, loss=1.482962965965271
I0212 21:26:05.596533 140285423572736 logging_writer.py:48] [30500] global_step=30500, grad_norm=2.9945263862609863, loss=1.4492361545562744
I0212 21:27:34.067698 140285415180032 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.6306509971618652, loss=1.4116202592849731
I0212 21:29:05.107496 140285423572736 logging_writer.py:48] [30700] global_step=30700, grad_norm=2.2856640815734863, loss=1.4479998350143433
I0212 21:29:29.532091 140441227016000 spec.py:321] Evaluating on the training split.
I0212 21:30:26.052199 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 21:31:20.527866 140441227016000 spec.py:349] Evaluating on the test split.
I0212 21:31:47.084085 140441227016000 submission_runner.py:408] Time since start: 28576.10s, 	Step: 30728, 	{'train/ctc_loss': Array(0.26352453, dtype=float32), 'train/wer': 0.09056334555602767, 'validation/ctc_loss': Array(0.5693506, dtype=float32), 'validation/wer': 0.1632891472044952, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3357012, dtype=float32), 'test/wer': 0.10669672780452136, 'test/num_examples': 2472, 'score': 25941.396134853363, 'total_duration': 28576.09837579727, 'accumulated_submission_time': 25941.396134853363, 'accumulated_eval_time': 2632.291247367859, 'accumulated_logging_time': 0.9961137771606445}
I0212 21:31:47.124616 140285423572736 logging_writer.py:48] [30728] accumulated_eval_time=2632.291247, accumulated_logging_time=0.996114, accumulated_submission_time=25941.396135, global_step=30728, preemption_count=0, score=25941.396135, test/ctc_loss=0.33570119738578796, test/num_examples=2472, test/wer=0.106697, total_duration=28576.098376, train/ctc_loss=0.26352453231811523, train/wer=0.090563, validation/ctc_loss=0.5693506002426147, validation/num_examples=5348, validation/wer=0.163289
I0212 21:32:42.077886 140285415180032 logging_writer.py:48] [30800] global_step=30800, grad_norm=2.7069132328033447, loss=1.4910564422607422
I0212 21:34:00.826095 140285423572736 logging_writer.py:48] [30900] global_step=30900, grad_norm=2.632244110107422, loss=1.4668123722076416
I0212 21:35:17.430106 140285415180032 logging_writer.py:48] [31000] global_step=31000, grad_norm=2.080625057220459, loss=1.4849348068237305
I0212 21:36:39.086549 140285423572736 logging_writer.py:48] [31100] global_step=31100, grad_norm=2.68061900138855, loss=1.5013937950134277
I0212 21:38:03.495095 140285415180032 logging_writer.py:48] [31200] global_step=31200, grad_norm=2.281196355819702, loss=1.504310965538025
I0212 21:39:27.841996 140285423572736 logging_writer.py:48] [31300] global_step=31300, grad_norm=2.4850916862487793, loss=1.4346204996109009
I0212 21:40:54.351445 140285415180032 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.6954524517059326, loss=1.4014215469360352
I0212 21:42:24.400210 140285423572736 logging_writer.py:48] [31500] global_step=31500, grad_norm=2.203752279281616, loss=1.4387688636779785
I0212 21:43:50.176869 140285415180032 logging_writer.py:48] [31600] global_step=31600, grad_norm=2.6541430950164795, loss=1.401784896850586
I0212 21:45:19.728332 140285423572736 logging_writer.py:48] [31700] global_step=31700, grad_norm=2.257181406021118, loss=1.4708114862442017
I0212 21:46:47.007180 140285415180032 logging_writer.py:48] [31800] global_step=31800, grad_norm=2.472398281097412, loss=1.4410864114761353
I0212 21:48:18.097254 140285423572736 logging_writer.py:48] [31900] global_step=31900, grad_norm=2.2070260047912598, loss=1.4066331386566162
I0212 21:49:41.135380 140285423572736 logging_writer.py:48] [32000] global_step=32000, grad_norm=2.5009958744049072, loss=1.4116185903549194
I0212 21:50:56.368551 140285415180032 logging_writer.py:48] [32100] global_step=32100, grad_norm=2.0915167331695557, loss=1.4230496883392334
I0212 21:52:18.169756 140285423572736 logging_writer.py:48] [32200] global_step=32200, grad_norm=2.452207565307617, loss=1.4703195095062256
I0212 21:53:41.285686 140285415180032 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.4224917888641357, loss=1.4103999137878418
I0212 21:55:07.456352 140285423572736 logging_writer.py:48] [32400] global_step=32400, grad_norm=3.9213130474090576, loss=1.4873460531234741
I0212 21:55:48.136649 140441227016000 spec.py:321] Evaluating on the training split.
I0212 21:56:43.740921 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 21:57:36.243608 140441227016000 spec.py:349] Evaluating on the test split.
I0212 21:58:03.250145 140441227016000 submission_runner.py:408] Time since start: 30152.27s, 	Step: 32447, 	{'train/ctc_loss': Array(0.25451177, dtype=float32), 'train/wer': 0.086225316926675, 'validation/ctc_loss': Array(0.54807425, dtype=float32), 'validation/wer': 0.15767979377661065, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31943408, dtype=float32), 'test/wer': 0.10291877399305344, 'test/num_examples': 2472, 'score': 27382.316435575485, 'total_duration': 30152.26672244072, 'accumulated_submission_time': 27382.316435575485, 'accumulated_eval_time': 2767.3991503715515, 'accumulated_logging_time': 1.0550148487091064}
I0212 21:58:03.285595 140285423572736 logging_writer.py:48] [32447] accumulated_eval_time=2767.399150, accumulated_logging_time=1.055015, accumulated_submission_time=27382.316436, global_step=32447, preemption_count=0, score=27382.316436, test/ctc_loss=0.3194340765476227, test/num_examples=2472, test/wer=0.102919, total_duration=30152.266722, train/ctc_loss=0.2545117735862732, train/wer=0.086225, validation/ctc_loss=0.5480742454528809, validation/num_examples=5348, validation/wer=0.157680
I0212 21:58:43.787077 140285415180032 logging_writer.py:48] [32500] global_step=32500, grad_norm=2.731426954269409, loss=1.4304183721542358
I0212 21:59:58.644860 140285423572736 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.8302291631698608, loss=1.4308867454528809
I0212 22:01:17.869157 140285415180032 logging_writer.py:48] [32700] global_step=32700, grad_norm=3.635333299636841, loss=1.3966469764709473
I0212 22:02:46.888619 140285423572736 logging_writer.py:48] [32800] global_step=32800, grad_norm=2.390176296234131, loss=1.3870183229446411
I0212 22:04:16.188287 140285415180032 logging_writer.py:48] [32900] global_step=32900, grad_norm=3.1793394088745117, loss=1.3992329835891724
I0212 22:05:44.961585 140285423572736 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.3808236122131348, loss=1.3586052656173706
I0212 22:07:02.470979 140285415180032 logging_writer.py:48] [33100] global_step=33100, grad_norm=2.207301378250122, loss=1.410670280456543
I0212 22:08:24.071257 140285423572736 logging_writer.py:48] [33200] global_step=33200, grad_norm=2.707165241241455, loss=1.4580949544906616
I0212 22:09:41.452336 140285415180032 logging_writer.py:48] [33300] global_step=33300, grad_norm=2.0143115520477295, loss=1.4174226522445679
I0212 22:11:04.457502 140285423572736 logging_writer.py:48] [33400] global_step=33400, grad_norm=2.536674737930298, loss=1.4489637613296509
I0212 22:12:32.689607 140285415180032 logging_writer.py:48] [33500] global_step=33500, grad_norm=2.9808433055877686, loss=1.362853765487671
I0212 22:14:04.479947 140285423572736 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.9046885967254639, loss=1.386110782623291
I0212 22:15:32.568070 140285415180032 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.5829432010650635, loss=1.4761921167373657
I0212 22:17:01.274459 140285423572736 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.5325723886489868, loss=1.4107190370559692
I0212 22:18:31.792241 140285415180032 logging_writer.py:48] [33900] global_step=33900, grad_norm=2.87294864654541, loss=1.3653244972229004
I0212 22:20:03.615624 140285423572736 logging_writer.py:48] [34000] global_step=34000, grad_norm=3.816908359527588, loss=1.3453876972198486
I0212 22:21:19.187637 140285415180032 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.5275380611419678, loss=1.461989402770996
I0212 22:22:04.000826 140441227016000 spec.py:321] Evaluating on the training split.
I0212 22:22:59.124626 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 22:23:52.937484 140441227016000 spec.py:349] Evaluating on the test split.
I0212 22:24:20.205967 140441227016000 submission_runner.py:408] Time since start: 31729.22s, 	Step: 34160, 	{'train/ctc_loss': Array(0.2348298, dtype=float32), 'train/wer': 0.08166989664082687, 'validation/ctc_loss': Array(0.53372824, dtype=float32), 'validation/wer': 0.15307452426697046, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30711773, dtype=float32), 'test/wer': 0.09796274856295574, 'test/num_examples': 2472, 'score': 28822.94229197502, 'total_duration': 31729.222623348236, 'accumulated_submission_time': 28822.94229197502, 'accumulated_eval_time': 2903.5987594127655, 'accumulated_logging_time': 1.1068816184997559}
I0212 22:24:20.241589 140285423572736 logging_writer.py:48] [34160] accumulated_eval_time=2903.598759, accumulated_logging_time=1.106882, accumulated_submission_time=28822.942292, global_step=34160, preemption_count=0, score=28822.942292, test/ctc_loss=0.3071177303791046, test/num_examples=2472, test/wer=0.097963, total_duration=31729.222623, train/ctc_loss=0.23482979834079742, train/wer=0.081670, validation/ctc_loss=0.5337282419204712, validation/num_examples=5348, validation/wer=0.153075
I0212 22:24:51.123675 140285415180032 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.5607597827911377, loss=1.3774923086166382
I0212 22:26:06.335011 140285423572736 logging_writer.py:48] [34300] global_step=34300, grad_norm=2.713557004928589, loss=1.437822937965393
I0212 22:27:21.859378 140285415180032 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.9199717044830322, loss=1.382736325263977
I0212 22:28:41.730039 140285423572736 logging_writer.py:48] [34500] global_step=34500, grad_norm=2.4880104064941406, loss=1.3805735111236572
I0212 22:30:08.972918 140285415180032 logging_writer.py:48] [34600] global_step=34600, grad_norm=3.446122169494629, loss=1.413429856300354
I0212 22:31:39.803732 140285423572736 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.436008334159851, loss=1.3893603086471558
I0212 22:33:09.636873 140285415180032 logging_writer.py:48] [34800] global_step=34800, grad_norm=2.3082544803619385, loss=1.293702483177185
I0212 22:34:34.642934 140285423572736 logging_writer.py:48] [34900] global_step=34900, grad_norm=2.405724287033081, loss=1.371809720993042
I0212 22:36:05.081738 140285415180032 logging_writer.py:48] [35000] global_step=35000, grad_norm=2.1800389289855957, loss=1.3746113777160645
I0212 22:37:29.841011 140285423572736 logging_writer.py:48] [35100] global_step=35100, grad_norm=2.4158811569213867, loss=1.2677823305130005
I0212 22:38:48.646434 140285415180032 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.7534040212631226, loss=1.3584336042404175
I0212 22:40:07.698960 140285423572736 logging_writer.py:48] [35300] global_step=35300, grad_norm=2.1490395069122314, loss=1.368796944618225
I0212 22:41:33.729722 140285415180032 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.8237242698669434, loss=1.4370311498641968
I0212 22:43:00.197395 140285423572736 logging_writer.py:48] [35500] global_step=35500, grad_norm=2.9401776790618896, loss=1.3468363285064697
I0212 22:44:29.953894 140285415180032 logging_writer.py:48] [35600] global_step=35600, grad_norm=2.3630104064941406, loss=1.317568063735962
I0212 22:45:58.112356 140285423572736 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.5027060508728027, loss=1.3376933336257935
I0212 22:47:24.443202 140285415180032 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.8071839809417725, loss=1.3220573663711548
I0212 22:48:20.897769 140441227016000 spec.py:321] Evaluating on the training split.
I0212 22:49:16.808656 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 22:50:09.313848 140441227016000 spec.py:349] Evaluating on the test split.
I0212 22:50:35.606363 140441227016000 submission_runner.py:408] Time since start: 33304.62s, 	Step: 35863, 	{'train/ctc_loss': Array(0.23932621, dtype=float32), 'train/wer': 0.0797680012210462, 'validation/ctc_loss': Array(0.5034849, dtype=float32), 'validation/wer': 0.14430809928845206, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28727466, dtype=float32), 'test/wer': 0.09174740519570207, 'test/num_examples': 2472, 'score': 30263.509187936783, 'total_duration': 33304.622517347336, 'accumulated_submission_time': 30263.509187936783, 'accumulated_eval_time': 3038.3013293743134, 'accumulated_logging_time': 1.1584124565124512}
I0212 22:50:35.647625 140285423572736 logging_writer.py:48] [35863] accumulated_eval_time=3038.301329, accumulated_logging_time=1.158412, accumulated_submission_time=30263.509188, global_step=35863, preemption_count=0, score=30263.509188, test/ctc_loss=0.28727465867996216, test/num_examples=2472, test/wer=0.091747, total_duration=33304.622517, train/ctc_loss=0.23932620882987976, train/wer=0.079768, validation/ctc_loss=0.5034849047660828, validation/num_examples=5348, validation/wer=0.144308
I0212 22:51:04.234032 140285415180032 logging_writer.py:48] [35900] global_step=35900, grad_norm=2.226519823074341, loss=1.3518197536468506
I0212 22:52:19.712812 140285423572736 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.6307052373886108, loss=1.3485819101333618
I0212 22:53:40.176001 140285423572736 logging_writer.py:48] [36100] global_step=36100, grad_norm=2.2646613121032715, loss=1.3200703859329224
I0212 22:54:58.049814 140285415180032 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.7050058841705322, loss=1.3118168115615845
I0212 22:56:15.115032 140285423572736 logging_writer.py:48] [36300] global_step=36300, grad_norm=2.6630043983459473, loss=1.3087568283081055
I0212 22:57:39.108725 140285415180032 logging_writer.py:48] [36400] global_step=36400, grad_norm=2.4169886112213135, loss=1.2827942371368408
I0212 22:59:01.901880 140285423572736 logging_writer.py:48] [36500] global_step=36500, grad_norm=2.3521933555603027, loss=1.3559197187423706
I0212 23:00:29.803362 140285415180032 logging_writer.py:48] [36600] global_step=36600, grad_norm=2.15427565574646, loss=1.3486106395721436
I0212 23:01:58.020688 140285423572736 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.5704567432403564, loss=1.3146584033966064
I0212 23:03:28.318764 140285415180032 logging_writer.py:48] [36800] global_step=36800, grad_norm=3.9297056198120117, loss=1.3340543508529663
I0212 23:04:57.376911 140285423572736 logging_writer.py:48] [36900] global_step=36900, grad_norm=2.219200611114502, loss=1.3374162912368774
I0212 23:06:24.463364 140285415180032 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.9911603927612305, loss=1.2984520196914673
I0212 23:07:54.307818 140285423572736 logging_writer.py:48] [37100] global_step=37100, grad_norm=2.1769700050354004, loss=1.3243911266326904
I0212 23:09:11.573994 140285415180032 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.4068523645401, loss=1.3062644004821777
I0212 23:10:28.877081 140285423572736 logging_writer.py:48] [37300] global_step=37300, grad_norm=3.1916940212249756, loss=1.2978020906448364
I0212 23:11:51.198283 140285415180032 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.7601453065872192, loss=1.2841811180114746
I0212 23:13:14.978912 140285423572736 logging_writer.py:48] [37500] global_step=37500, grad_norm=2.83512544631958, loss=1.2900726795196533
I0212 23:14:36.031085 140441227016000 spec.py:321] Evaluating on the training split.
I0212 23:15:34.203252 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 23:16:26.774956 140441227016000 spec.py:349] Evaluating on the test split.
I0212 23:16:54.477833 140441227016000 submission_runner.py:408] Time since start: 34883.49s, 	Step: 37591, 	{'train/ctc_loss': Array(0.21954021, dtype=float32), 'train/wer': 0.07452886321631484, 'validation/ctc_loss': Array(0.4808648, dtype=float32), 'validation/wer': 0.1389304575340085, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27169424, dtype=float32), 'test/wer': 0.0879491398046026, 'test/num_examples': 2472, 'score': 31703.803174734116, 'total_duration': 34883.49290180206, 'accumulated_submission_time': 31703.803174734116, 'accumulated_eval_time': 3176.7409851551056, 'accumulated_logging_time': 1.2146975994110107}
I0212 23:16:54.518322 140285423572736 logging_writer.py:48] [37591] accumulated_eval_time=3176.740985, accumulated_logging_time=1.214698, accumulated_submission_time=31703.803175, global_step=37591, preemption_count=0, score=31703.803175, test/ctc_loss=0.27169424295425415, test/num_examples=2472, test/wer=0.087949, total_duration=34883.492902, train/ctc_loss=0.21954020857810974, train/wer=0.074529, validation/ctc_loss=0.4808647930622101, validation/num_examples=5348, validation/wer=0.138930
I0212 23:17:02.137261 140285415180032 logging_writer.py:48] [37600] global_step=37600, grad_norm=2.769735097885132, loss=1.3002644777297974
I0212 23:18:16.843693 140285423572736 logging_writer.py:48] [37700] global_step=37700, grad_norm=2.2245781421661377, loss=1.26836097240448
I0212 23:19:32.149437 140285415180032 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.3911417722702026, loss=1.2757654190063477
I0212 23:20:58.416693 140285423572736 logging_writer.py:48] [37900] global_step=37900, grad_norm=2.917147397994995, loss=1.3008760213851929
I0212 23:22:27.819997 140285415180032 logging_writer.py:48] [38000] global_step=38000, grad_norm=2.8048901557922363, loss=1.3113594055175781
I0212 23:23:58.115524 140285423572736 logging_writer.py:48] [38100] global_step=38100, grad_norm=2.489596128463745, loss=1.319362759590149
I0212 23:25:20.718627 140285423572736 logging_writer.py:48] [38200] global_step=38200, grad_norm=2.876448631286621, loss=1.3593677282333374
I0212 23:26:40.489216 140285415180032 logging_writer.py:48] [38300] global_step=38300, grad_norm=4.351474285125732, loss=1.2890509366989136
I0212 23:28:01.643774 140285423572736 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.469780445098877, loss=1.2812187671661377
I0212 23:29:22.985591 140285415180032 logging_writer.py:48] [38500] global_step=38500, grad_norm=2.445145606994629, loss=1.3049824237823486
I0212 23:30:52.294483 140285423572736 logging_writer.py:48] [38600] global_step=38600, grad_norm=6.220401763916016, loss=1.25503671169281
I0212 23:32:18.998422 140285415180032 logging_writer.py:48] [38700] global_step=38700, grad_norm=2.3564655780792236, loss=1.2673875093460083
I0212 23:33:46.969633 140285423572736 logging_writer.py:48] [38800] global_step=38800, grad_norm=3.2433125972747803, loss=1.2296653985977173
I0212 23:35:14.008058 140285415180032 logging_writer.py:48] [38900] global_step=38900, grad_norm=3.4128942489624023, loss=1.232964038848877
I0212 23:36:45.294120 140285423572736 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.3809915781021118, loss=1.227993369102478
I0212 23:38:11.376334 140285415180032 logging_writer.py:48] [39100] global_step=39100, grad_norm=2.819321632385254, loss=1.258891224861145
I0212 23:39:37.288344 140285423572736 logging_writer.py:48] [39200] global_step=39200, grad_norm=4.031445026397705, loss=1.2018048763275146
I0212 23:40:52.500391 140285415180032 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.7888882160186768, loss=1.2468770742416382
I0212 23:40:54.613823 140441227016000 spec.py:321] Evaluating on the training split.
I0212 23:41:50.947631 140441227016000 spec.py:333] Evaluating on the validation split.
I0212 23:42:43.447884 140441227016000 spec.py:349] Evaluating on the test split.
I0212 23:43:11.403181 140441227016000 submission_runner.py:408] Time since start: 36460.42s, 	Step: 39304, 	{'train/ctc_loss': Array(0.19475468, dtype=float32), 'train/wer': 0.06745336333564918, 'validation/ctc_loss': Array(0.45533058, dtype=float32), 'validation/wer': 0.13129362696351507, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2551368, dtype=float32), 'test/wer': 0.08207909329108524, 'test/num_examples': 2472, 'score': 33143.80793213844, 'total_duration': 36460.420042037964, 'accumulated_submission_time': 33143.80793213844, 'accumulated_eval_time': 3313.524995803833, 'accumulated_logging_time': 1.2724831104278564}
I0212 23:43:11.441617 140285423572736 logging_writer.py:48] [39304] accumulated_eval_time=3313.524996, accumulated_logging_time=1.272483, accumulated_submission_time=33143.807932, global_step=39304, preemption_count=0, score=33143.807932, test/ctc_loss=0.25513678789138794, test/num_examples=2472, test/wer=0.082079, total_duration=36460.420042, train/ctc_loss=0.1947546750307083, train/wer=0.067453, validation/ctc_loss=0.45533058047294617, validation/num_examples=5348, validation/wer=0.131294
I0212 23:44:23.837972 140285415180032 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.6080906391143799, loss=1.278663992881775
I0212 23:45:38.661936 140285423572736 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.8715001344680786, loss=1.273777723312378
I0212 23:46:56.623649 140285415180032 logging_writer.py:48] [39600] global_step=39600, grad_norm=2.916971445083618, loss=1.2350791692733765
I0212 23:48:25.480660 140285423572736 logging_writer.py:48] [39700] global_step=39700, grad_norm=3.3389313220977783, loss=1.2482566833496094
I0212 23:49:51.508911 140285415180032 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.5169506072998047, loss=1.297163963317871
I0212 23:51:20.325214 140285423572736 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.7298071384429932, loss=1.2248859405517578
I0212 23:52:51.014513 140285415180032 logging_writer.py:48] [40000] global_step=40000, grad_norm=4.143226146697998, loss=1.243831992149353
I0212 23:54:21.702319 140285423572736 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.9241328239440918, loss=1.276418924331665
I0212 23:55:49.145497 140285423572736 logging_writer.py:48] [40200] global_step=40200, grad_norm=2.0293221473693848, loss=1.236346960067749
I0212 23:57:04.743904 140285415180032 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.9744819402694702, loss=1.2109426259994507
I0212 23:58:21.121257 140285423572736 logging_writer.py:48] [40400] global_step=40400, grad_norm=2.734639883041382, loss=1.170339822769165
I0212 23:59:42.713580 140285415180032 logging_writer.py:48] [40500] global_step=40500, grad_norm=2.1573243141174316, loss=1.287168264389038
I0213 00:01:06.867717 140285423572736 logging_writer.py:48] [40600] global_step=40600, grad_norm=2.62214732170105, loss=1.2224984169006348
I0213 00:02:33.897397 140285415180032 logging_writer.py:48] [40700] global_step=40700, grad_norm=3.0795581340789795, loss=1.237932801246643
I0213 00:04:04.006910 140285423572736 logging_writer.py:48] [40800] global_step=40800, grad_norm=2.188524007797241, loss=1.2251002788543701
I0213 00:05:32.577905 140285415180032 logging_writer.py:48] [40900] global_step=40900, grad_norm=3.5523674488067627, loss=1.2021528482437134
I0213 00:06:58.779110 140285423572736 logging_writer.py:48] [41000] global_step=41000, grad_norm=3.0752272605895996, loss=1.237744688987732
I0213 00:07:11.524286 140441227016000 spec.py:321] Evaluating on the training split.
I0213 00:08:08.049873 140441227016000 spec.py:333] Evaluating on the validation split.
I0213 00:09:00.579542 140441227016000 spec.py:349] Evaluating on the test split.
I0213 00:09:26.913908 140441227016000 submission_runner.py:408] Time since start: 38035.93s, 	Step: 41016, 	{'train/ctc_loss': Array(0.18020418, dtype=float32), 'train/wer': 0.06146448816967662, 'validation/ctc_loss': Array(0.43848684, dtype=float32), 'validation/wer': 0.12627320737229308, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24059556, dtype=float32), 'test/wer': 0.07720431417951375, 'test/num_examples': 2472, 'score': 34583.80059170723, 'total_duration': 38035.929822444916, 'accumulated_submission_time': 34583.80059170723, 'accumulated_eval_time': 3448.9083466529846, 'accumulated_logging_time': 1.3277525901794434}
I0213 00:09:26.951291 140285423572736 logging_writer.py:48] [41016] accumulated_eval_time=3448.908347, accumulated_logging_time=1.327753, accumulated_submission_time=34583.800592, global_step=41016, preemption_count=0, score=34583.800592, test/ctc_loss=0.24059556424617767, test/num_examples=2472, test/wer=0.077204, total_duration=38035.929822, train/ctc_loss=0.18020418286323547, train/wer=0.061464, validation/ctc_loss=0.43848684430122375, validation/num_examples=5348, validation/wer=0.126273
I0213 00:10:30.641936 140285415180032 logging_writer.py:48] [41100] global_step=41100, grad_norm=2.9034318923950195, loss=1.2342591285705566
I0213 00:11:49.389632 140285423572736 logging_writer.py:48] [41200] global_step=41200, grad_norm=3.757361888885498, loss=1.2458152770996094
I0213 00:13:05.493122 140285415180032 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.9801900386810303, loss=1.2437002658843994
I0213 00:14:22.632561 140285423572736 logging_writer.py:48] [41400] global_step=41400, grad_norm=2.9501566886901855, loss=1.2026503086090088
I0213 00:15:45.454924 140285415180032 logging_writer.py:48] [41500] global_step=41500, grad_norm=2.603311538696289, loss=1.2159534692764282
I0213 00:17:10.076717 140285423572736 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.7610160112380981, loss=1.20255708694458
I0213 00:18:35.995378 140285415180032 logging_writer.py:48] [41700] global_step=41700, grad_norm=4.803617000579834, loss=1.1786658763885498
I0213 00:20:00.813152 140285423572736 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.5169625282287598, loss=1.213870644569397
I0213 00:21:31.569800 140285415180032 logging_writer.py:48] [41900] global_step=41900, grad_norm=2.6210811138153076, loss=1.2285634279251099
I0213 00:23:01.616482 140285423572736 logging_writer.py:48] [42000] global_step=42000, grad_norm=2.274397134780884, loss=1.2196563482284546
I0213 00:24:29.341706 140285415180032 logging_writer.py:48] [42100] global_step=42100, grad_norm=3.2579197883605957, loss=1.2015782594680786
I0213 00:25:58.625914 140285423572736 logging_writer.py:48] [42200] global_step=42200, grad_norm=2.5566954612731934, loss=1.1796950101852417
I0213 00:27:21.462473 140285423572736 logging_writer.py:48] [42300] global_step=42300, grad_norm=2.171053171157837, loss=1.238491415977478
I0213 00:28:36.660583 140285415180032 logging_writer.py:48] [42400] global_step=42400, grad_norm=3.115171194076538, loss=1.2024112939834595
I0213 00:29:58.049687 140285423572736 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.706122636795044, loss=1.1313456296920776
I0213 00:31:17.041420 140285415180032 logging_writer.py:48] [42600] global_step=42600, grad_norm=2.4670815467834473, loss=1.2094271183013916
I0213 00:32:44.779476 140285423572736 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.645431399345398, loss=1.1975067853927612
I0213 00:33:27.958115 140441227016000 spec.py:321] Evaluating on the training split.
I0213 00:34:23.626324 140441227016000 spec.py:333] Evaluating on the validation split.
I0213 00:35:16.208665 140441227016000 spec.py:349] Evaluating on the test split.
I0213 00:35:42.717546 140441227016000 submission_runner.py:408] Time since start: 39611.73s, 	Step: 42751, 	{'train/ctc_loss': Array(0.15269303, dtype=float32), 'train/wer': 0.05257432539472989, 'validation/ctc_loss': Array(0.4185573, dtype=float32), 'validation/wer': 0.11923496529152225, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23369418, dtype=float32), 'test/wer': 0.07462474356630715, 'test/num_examples': 2472, 'score': 36024.716413497925, 'total_duration': 39611.73369860649, 'accumulated_submission_time': 36024.716413497925, 'accumulated_eval_time': 3583.6617407798767, 'accumulated_logging_time': 1.3814823627471924}
I0213 00:35:42.757044 140285423572736 logging_writer.py:48] [42751] accumulated_eval_time=3583.661741, accumulated_logging_time=1.381482, accumulated_submission_time=36024.716413, global_step=42751, preemption_count=0, score=36024.716413, test/ctc_loss=0.2336941808462143, test/num_examples=2472, test/wer=0.074625, total_duration=39611.733699, train/ctc_loss=0.1526930332183838, train/wer=0.052574, validation/ctc_loss=0.4185572862625122, validation/num_examples=5348, validation/wer=0.119235
I0213 00:36:20.921854 140285415180032 logging_writer.py:48] [42800] global_step=42800, grad_norm=3.2978460788726807, loss=1.1732637882232666
I0213 00:37:36.427433 140285423572736 logging_writer.py:48] [42900] global_step=42900, grad_norm=13.603172302246094, loss=1.2280640602111816
I0213 00:38:57.809008 140285415180032 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.9336363077163696, loss=1.140233039855957
I0213 00:40:26.026911 140285423572736 logging_writer.py:48] [43100] global_step=43100, grad_norm=3.4579946994781494, loss=1.1934901475906372
I0213 00:41:55.376583 140285415180032 logging_writer.py:48] [43200] global_step=43200, grad_norm=2.5880048274993896, loss=1.1818805932998657
I0213 00:43:20.734112 140285423572736 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.7000994682312012, loss=1.13156259059906
I0213 00:44:38.457258 140285415180032 logging_writer.py:48] [43400] global_step=43400, grad_norm=3.0529582500457764, loss=1.2721918821334839
I0213 00:45:55.255993 140285423572736 logging_writer.py:48] [43500] global_step=43500, grad_norm=2.1343696117401123, loss=1.1846716403961182
I0213 00:47:14.973608 140285415180032 logging_writer.py:48] [43600] global_step=43600, grad_norm=2.307203531265259, loss=1.175471305847168
I0213 00:48:41.403695 140285423572736 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.8746634721755981, loss=1.1503303050994873
I0213 00:50:12.783223 140285415180032 logging_writer.py:48] [43800] global_step=43800, grad_norm=2.745939016342163, loss=1.134785532951355
I0213 00:51:42.614717 140285423572736 logging_writer.py:48] [43900] global_step=43900, grad_norm=2.7749245166778564, loss=1.1739686727523804
I0213 00:53:11.989210 140285415180032 logging_writer.py:48] [44000] global_step=44000, grad_norm=2.5337791442871094, loss=1.1139332056045532
I0213 00:54:44.334833 140285423572736 logging_writer.py:48] [44100] global_step=44100, grad_norm=3.3038644790649414, loss=1.1486454010009766
I0213 00:56:15.246739 140285415180032 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.493157148361206, loss=1.2424650192260742
I0213 00:57:45.350198 140285423572736 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.2217971086502075, loss=1.1924530267715454
I0213 00:59:00.403662 140285415180032 logging_writer.py:48] [44400] global_step=44400, grad_norm=2.9213931560516357, loss=1.1294478178024292
I0213 00:59:42.968304 140441227016000 spec.py:321] Evaluating on the training split.
I0213 01:00:39.523290 140441227016000 spec.py:333] Evaluating on the validation split.
I0213 01:01:31.944392 140441227016000 spec.py:349] Evaluating on the test split.
I0213 01:01:58.903953 140441227016000 submission_runner.py:408] Time since start: 41187.92s, 	Step: 44456, 	{'train/ctc_loss': Array(0.15371695, dtype=float32), 'train/wer': 0.05171779705170247, 'validation/ctc_loss': Array(0.4076263, dtype=float32), 'validation/wer': 0.11592341929192775, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22438748, dtype=float32), 'test/wer': 0.07190299189567972, 'test/num_examples': 2472, 'score': 37464.84016394615, 'total_duration': 41187.92023229599, 'accumulated_submission_time': 37464.84016394615, 'accumulated_eval_time': 3719.5914764404297, 'accumulated_logging_time': 1.4356229305267334}
I0213 01:01:58.943112 140285423572736 logging_writer.py:48] [44456] accumulated_eval_time=3719.591476, accumulated_logging_time=1.435623, accumulated_submission_time=37464.840164, global_step=44456, preemption_count=0, score=37464.840164, test/ctc_loss=0.22438748180866241, test/num_examples=2472, test/wer=0.071903, total_duration=41187.920232, train/ctc_loss=0.15371695160865784, train/wer=0.051718, validation/ctc_loss=0.40762630105018616, validation/num_examples=5348, validation/wer=0.115923
I0213 01:02:32.720780 140285415180032 logging_writer.py:48] [44500] global_step=44500, grad_norm=2.732278347015381, loss=1.1770386695861816
I0213 01:03:47.355703 140285423572736 logging_writer.py:48] [44600] global_step=44600, grad_norm=2.5099260807037354, loss=1.1485034227371216
I0213 01:05:02.441042 140285415180032 logging_writer.py:48] [44700] global_step=44700, grad_norm=2.1821858882904053, loss=1.1387227773666382
I0213 01:06:21.947358 140285423572736 logging_writer.py:48] [44800] global_step=44800, grad_norm=2.822122097015381, loss=1.161729335784912
I0213 01:07:47.170317 140285415180032 logging_writer.py:48] [44900] global_step=44900, grad_norm=2.9822726249694824, loss=1.1680406332015991
I0213 01:09:16.519819 140285423572736 logging_writer.py:48] [45000] global_step=45000, grad_norm=3.2052769660949707, loss=1.1429617404937744
I0213 01:10:47.272300 140285415180032 logging_writer.py:48] [45100] global_step=45100, grad_norm=3.182870626449585, loss=1.1577093601226807
I0213 01:12:15.779128 140285423572736 logging_writer.py:48] [45200] global_step=45200, grad_norm=5.380103588104248, loss=1.113422155380249
I0213 01:13:43.429302 140285415180032 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.500580906867981, loss=1.1321629285812378
I0213 01:15:04.889024 140285423572736 logging_writer.py:48] [45400] global_step=45400, grad_norm=2.397608757019043, loss=1.0801405906677246
I0213 01:16:21.323345 140285415180032 logging_writer.py:48] [45500] global_step=45500, grad_norm=2.235961675643921, loss=1.1761877536773682
I0213 01:17:42.384655 140285423572736 logging_writer.py:48] [45600] global_step=45600, grad_norm=2.9775655269622803, loss=1.1193668842315674
I0213 01:19:04.406147 140285415180032 logging_writer.py:48] [45700] global_step=45700, grad_norm=2.2708890438079834, loss=1.1458959579467773
I0213 01:20:33.903580 140285423572736 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.6929240226745605, loss=1.1873106956481934
I0213 01:22:04.401825 140285415180032 logging_writer.py:48] [45900] global_step=45900, grad_norm=3.6164638996124268, loss=1.1406644582748413
I0213 01:23:33.168149 140285423572736 logging_writer.py:48] [46000] global_step=46000, grad_norm=2.4044134616851807, loss=1.1535799503326416
I0213 01:25:02.727395 140285415180032 logging_writer.py:48] [46100] global_step=46100, grad_norm=2.780726909637451, loss=1.1367729902267456
I0213 01:25:59.234537 140441227016000 spec.py:321] Evaluating on the training split.
I0213 01:26:54.184010 140441227016000 spec.py:333] Evaluating on the validation split.
I0213 01:27:47.447041 140441227016000 spec.py:349] Evaluating on the test split.
I0213 01:28:14.276296 140441227016000 submission_runner.py:408] Time since start: 42763.29s, 	Step: 46165, 	{'train/ctc_loss': Array(0.16266528, dtype=float32), 'train/wer': 0.05495259447340699, 'validation/ctc_loss': Array(0.40298778, dtype=float32), 'validation/wer': 0.11462004112882204, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22044119, dtype=float32), 'test/wer': 0.0706233623788922, 'test/num_examples': 2472, 'score': 38905.040621995926, 'total_duration': 42763.2930085659, 'accumulated_submission_time': 38905.040621995926, 'accumulated_eval_time': 3854.6277837753296, 'accumulated_logging_time': 1.4938104152679443}
I0213 01:28:14.316107 140285423572736 logging_writer.py:48] [46165] accumulated_eval_time=3854.627784, accumulated_logging_time=1.493810, accumulated_submission_time=38905.040622, global_step=46165, preemption_count=0, score=38905.040622, test/ctc_loss=0.22044119238853455, test/num_examples=2472, test/wer=0.070623, total_duration=42763.293009, train/ctc_loss=0.16266527771949768, train/wer=0.054953, validation/ctc_loss=0.4029877781867981, validation/num_examples=5348, validation/wer=0.114620
I0213 01:28:41.821047 140285415180032 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.8117995262145996, loss=1.1820247173309326
I0213 01:29:56.556237 140285423572736 logging_writer.py:48] [46300] global_step=46300, grad_norm=2.0133230686187744, loss=1.146108865737915
I0213 01:31:15.027519 140285423572736 logging_writer.py:48] [46400] global_step=46400, grad_norm=3.2652428150177, loss=1.1002119779586792
I0213 01:32:32.801723 140285415180032 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.907066822052002, loss=1.157818078994751
I0213 01:33:54.377751 140285423572736 logging_writer.py:48] [46600] global_step=46600, grad_norm=2.1295597553253174, loss=1.149977445602417
I0213 01:35:13.169600 140285415180032 logging_writer.py:48] [46700] global_step=46700, grad_norm=2.150453567504883, loss=1.0892009735107422
I0213 01:36:37.424994 140285423572736 logging_writer.py:48] [46800] global_step=46800, grad_norm=2.2642147541046143, loss=1.1632425785064697
I0213 01:38:07.541967 140285415180032 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.7842135429382324, loss=1.15453040599823
I0213 01:39:35.404370 140285423572736 logging_writer.py:48] [47000] global_step=47000, grad_norm=3.2147343158721924, loss=1.1471400260925293
I0213 01:41:04.215668 140285415180032 logging_writer.py:48] [47100] global_step=47100, grad_norm=2.96785569190979, loss=1.1939446926116943
I0213 01:42:31.464355 140285423572736 logging_writer.py:48] [47200] global_step=47200, grad_norm=2.511807680130005, loss=1.136093258857727
I0213 01:43:55.914464 140285415180032 logging_writer.py:48] [47300] global_step=47300, grad_norm=3.5811121463775635, loss=1.1187399625778198
I0213 01:45:26.174722 140285423572736 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.7995752096176147, loss=1.1291643381118774
I0213 01:46:42.264410 140285415180032 logging_writer.py:48] [47500] global_step=47500, grad_norm=4.848803997039795, loss=1.1480520963668823
I0213 01:47:59.038335 140285423572736 logging_writer.py:48] [47600] global_step=47600, grad_norm=2.6375091075897217, loss=1.1781895160675049
I0213 01:49:21.831020 140285415180032 logging_writer.py:48] [47700] global_step=47700, grad_norm=2.3646814823150635, loss=1.1563934087753296
I0213 01:50:46.952542 140285423572736 logging_writer.py:48] [47800] global_step=47800, grad_norm=2.35365629196167, loss=1.1033427715301514
I0213 01:52:12.300949 140285415180032 logging_writer.py:48] [47900] global_step=47900, grad_norm=4.496975421905518, loss=1.1376560926437378
I0213 01:52:14.906950 140441227016000 spec.py:321] Evaluating on the training split.
I0213 01:53:10.854753 140441227016000 spec.py:333] Evaluating on the validation split.
I0213 01:54:03.856992 140441227016000 spec.py:349] Evaluating on the test split.
I0213 01:54:31.163393 140441227016000 submission_runner.py:408] Time since start: 44340.18s, 	Step: 47904, 	{'train/ctc_loss': Array(0.14715919, dtype=float32), 'train/wer': 0.048795612762676126, 'validation/ctc_loss': Array(0.40185603, dtype=float32), 'validation/wer': 0.11478417023084275, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21938264, dtype=float32), 'test/wer': 0.0707046086974184, 'test/num_examples': 2472, 'score': 40345.54191684723, 'total_duration': 44340.17746424675, 'accumulated_submission_time': 40345.54191684723, 'accumulated_eval_time': 3990.876124382019, 'accumulated_logging_time': 1.5495550632476807}
I0213 01:54:31.209700 140285423572736 logging_writer.py:48] [47904] accumulated_eval_time=3990.876124, accumulated_logging_time=1.549555, accumulated_submission_time=40345.541917, global_step=47904, preemption_count=0, score=40345.541917, test/ctc_loss=0.219382643699646, test/num_examples=2472, test/wer=0.070705, total_duration=44340.177464, train/ctc_loss=0.14715918898582458, train/wer=0.048796, validation/ctc_loss=0.40185603499412537, validation/num_examples=5348, validation/wer=0.114784
I0213 01:55:43.255227 140441227016000 spec.py:321] Evaluating on the training split.
I0213 01:56:40.946526 140441227016000 spec.py:333] Evaluating on the validation split.
I0213 01:57:29.681896 140441227016000 spec.py:349] Evaluating on the test split.
I0213 01:57:54.118762 140441227016000 submission_runner.py:408] Time since start: 44543.14s, 	Step: 48000, 	{'train/ctc_loss': Array(0.1594734, dtype=float32), 'train/wer': 0.054616546975526925, 'validation/ctc_loss': Array(0.401908, dtype=float32), 'validation/wer': 0.11480347953696284, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21940319, dtype=float32), 'test/wer': 0.0706639855381553, 'test/num_examples': 2472, 'score': 40417.566965818405, 'total_duration': 44543.13734269142, 'accumulated_submission_time': 40417.566965818405, 'accumulated_eval_time': 4121.736053466797, 'accumulated_logging_time': 1.6115646362304688}
I0213 01:57:54.157491 140285423572736 logging_writer.py:48] [48000] accumulated_eval_time=4121.736053, accumulated_logging_time=1.611565, accumulated_submission_time=40417.566966, global_step=48000, preemption_count=0, score=40417.566966, test/ctc_loss=0.2194031924009323, test/num_examples=2472, test/wer=0.070664, total_duration=44543.137343, train/ctc_loss=0.15947340428829193, train/wer=0.054617, validation/ctc_loss=0.4019080102443695, validation/num_examples=5348, validation/wer=0.114803
I0213 01:57:54.184118 140285415180032 logging_writer.py:48] [48000] global_step=48000, preemption_count=0, score=40417.566966
I0213 01:57:54.455310 140441227016000 checkpoints.py:490] Saving checkpoint at step: 48000
I0213 01:57:55.651057 140441227016000 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_2/librispeech_deepspeech_jax/trial_4/checkpoint_48000
I0213 01:57:55.671678 140441227016000 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_2/librispeech_deepspeech_jax/trial_4/checkpoint_48000.
I0213 01:57:57.014909 140441227016000 submission_runner.py:583] Tuning trial 4/5
I0213 01:57:57.015248 140441227016000 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.004958460849689891, one_minus_beta1=0.13625575743, beta2=0.6291854735396584, weight_decay=0.1147386261512052, warmup_factor=0.02)
I0213 01:57:57.030662 140441227016000 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(32.005363, dtype=float32), 'train/wer': 2.9794811320754717, 'validation/ctc_loss': Array(30.57026, dtype=float32), 'validation/wer': 2.911621305888373, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.657564, dtype=float32), 'test/wer': 3.231897304653383, 'test/num_examples': 2472, 'score': 15.822179079055786, 'total_duration': 189.60808277130127, 'accumulated_submission_time': 15.822179079055786, 'accumulated_eval_time': 173.78581547737122, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1705, {'train/ctc_loss': Array(1.7329582, dtype=float32), 'train/wer': 0.4562996089686897, 'validation/ctc_loss': Array(1.7820381, dtype=float32), 'validation/wer': 0.44021356092568814, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.3494298, dtype=float32), 'test/wer': 0.3802530822822091, 'test/num_examples': 2472, 'score': 1455.7558295726776, 'total_duration': 1763.6183669567108, 'accumulated_submission_time': 1455.7558295726776, 'accumulated_eval_time': 307.7569811344147, 'accumulated_logging_time': 0.02973175048828125, 'global_step': 1705, 'preemption_count': 0}), (3421, {'train/ctc_loss': Array(0.9513836, dtype=float32), 'train/wer': 0.2833366333833055, 'validation/ctc_loss': Array(1.08201, dtype=float32), 'validation/wer': 0.2937428193517866, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.7284901, dtype=float32), 'test/wer': 0.22505230231755124, 'test/num_examples': 2472, 'score': 2896.13534617424, 'total_duration': 3336.1467773914337, 'accumulated_submission_time': 2896.13534617424, 'accumulated_eval_time': 439.76512384414673, 'accumulated_logging_time': 0.08956670761108398, 'global_step': 3421, 'preemption_count': 0}), (5110, {'train/ctc_loss': Array(0.8250965, dtype=float32), 'train/wer': 0.24666468510283174, 'validation/ctc_loss': Array(0.96908915, dtype=float32), 'validation/wer': 0.2676849107427325, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.64543873, dtype=float32), 'test/wer': 0.20126744256900858, 'test/num_examples': 2472, 'score': 4337.026045560837, 'total_duration': 4910.876049280167, 'accumulated_submission_time': 4337.026045560837, 'accumulated_eval_time': 573.4874782562256, 'accumulated_logging_time': 0.1315174102783203, 'global_step': 5110, 'preemption_count': 0}), (6803, {'train/ctc_loss': Array(0.8884563, dtype=float32), 'train/wer': 0.267509416380854, 'validation/ctc_loss': Array(0.9415473, dtype=float32), 'validation/wer': 0.26318584241675275, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6189214, dtype=float32), 'test/wer': 0.19078666747912984, 'test/num_examples': 2472, 'score': 5777.149932384491, 'total_duration': 6488.072125196457, 'accumulated_submission_time': 5777.149932384491, 'accumulated_eval_time': 710.4288213253021, 'accumulated_logging_time': 0.1832258701324463, 'global_step': 6803, 'preemption_count': 0}), (8533, {'train/ctc_loss': Array(0.7801244, dtype=float32), 'train/wer': 0.23471242189340483, 'validation/ctc_loss': Array(0.90787, dtype=float32), 'validation/wer': 0.25158094943858195, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.57711726, dtype=float32), 'test/wer': 0.1780106838908862, 'test/num_examples': 2472, 'score': 7217.49794960022, 'total_duration': 8064.6559591293335, 'accumulated_submission_time': 7217.49794960022, 'accumulated_eval_time': 846.5350136756897, 'accumulated_logging_time': 0.23366570472717285, 'global_step': 8533, 'preemption_count': 0}), (10220, {'train/ctc_loss': Array(0.78038126, dtype=float32), 'train/wer': 0.2305007804084504, 'validation/ctc_loss': Array(0.8497801, dtype=float32), 'validation/wer': 0.23874991552178573, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5482055, dtype=float32), 'test/wer': 0.16828143724737474, 'test/num_examples': 2472, 'score': 8657.429280042648, 'total_duration': 9641.66242980957, 'accumulated_submission_time': 8657.429280042648, 'accumulated_eval_time': 983.4033079147339, 'accumulated_logging_time': 0.36255407333374023, 'global_step': 10220, 'preemption_count': 0}), (11925, {'train/ctc_loss': Array(0.6961746, dtype=float32), 'train/wer': 0.213959671084794, 'validation/ctc_loss': Array(0.8327891, dtype=float32), 'validation/wer': 0.23424119254274597, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5307556, dtype=float32), 'test/wer': 0.16681900351390327, 'test/num_examples': 2472, 'score': 10097.492525815964, 'total_duration': 11218.957754611969, 'accumulated_submission_time': 10097.492525815964, 'accumulated_eval_time': 1120.4950244426727, 'accumulated_logging_time': 0.4233393669128418, 'global_step': 11925, 'preemption_count': 0}), (13649, {'train/ctc_loss': Array(0.68453616, dtype=float32), 'train/wer': 0.21149859912074895, 'validation/ctc_loss': Array(0.8041042, dtype=float32), 'validation/wer': 0.22599611882946985, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.49814272, dtype=float32), 'test/wer': 0.15863343692238946, 'test/num_examples': 2472, 'score': 11537.983419895172, 'total_duration': 12795.047011137009, 'accumulated_submission_time': 11537.983419895172, 'accumulated_eval_time': 1255.964623451233, 'accumulated_logging_time': 0.4713277816772461, 'global_step': 13649, 'preemption_count': 0}), (15342, {'train/ctc_loss': Array(0.68597907, dtype=float32), 'train/wer': 0.21039335576396417, 'validation/ctc_loss': Array(0.7896287, dtype=float32), 'validation/wer': 0.22074398756480687, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.49917656, dtype=float32), 'test/wer': 0.1561554242073406, 'test/num_examples': 2472, 'score': 12978.61863040924, 'total_duration': 14373.29497885704, 'accumulated_submission_time': 12978.61863040924, 'accumulated_eval_time': 1393.445172548294, 'accumulated_logging_time': 0.5251693725585938, 'global_step': 15342, 'preemption_count': 0}), (17076, {'train/ctc_loss': Array(0.43890128, dtype=float32), 'train/wer': 0.1444019164369262, 'validation/ctc_loss': Array(0.7677971, dtype=float32), 'validation/wer': 0.21517325274916246, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4785223, dtype=float32), 'test/wer': 0.15109784087908518, 'test/num_examples': 2472, 'score': 14418.936494112015, 'total_duration': 15959.526646614075, 'accumulated_submission_time': 14418.936494112015, 'accumulated_eval_time': 1539.2309548854828, 'accumulated_logging_time': 0.5735268592834473, 'global_step': 17076, 'preemption_count': 0}), (18793, {'train/ctc_loss': Array(0.4114516, dtype=float32), 'train/wer': 0.13331886798301354, 'validation/ctc_loss': Array(0.739945, dtype=float32), 'validation/wer': 0.20781640711741023, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.45743635, dtype=float32), 'test/wer': 0.1456137143785672, 'test/num_examples': 2472, 'score': 15859.429361820221, 'total_duration': 17535.774045705795, 'accumulated_submission_time': 15859.429361820221, 'accumulated_eval_time': 1674.8570148944855, 'accumulated_logging_time': 0.6237502098083496, 'global_step': 18793, 'preemption_count': 0}), (20505, {'train/ctc_loss': Array(0.38734055, dtype=float32), 'train/wer': 0.129115201993911, 'validation/ctc_loss': Array(0.71387607, dtype=float32), 'validation/wer': 0.20162777450592312, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43603286, dtype=float32), 'test/wer': 0.13740783620742186, 'test/num_examples': 2472, 'score': 17300.21093249321, 'total_duration': 19114.40056157112, 'accumulated_submission_time': 17300.21093249321, 'accumulated_eval_time': 1812.569813489914, 'accumulated_logging_time': 0.6772727966308594, 'global_step': 20505, 'preemption_count': 0}), (22215, {'train/ctc_loss': Array(0.3631994, dtype=float32), 'train/wer': 0.12213056330055974, 'validation/ctc_loss': Array(0.67903066, dtype=float32), 'validation/wer': 0.1932861542620466, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.41533288, dtype=float32), 'test/wer': 0.13015660227895923, 'test/num_examples': 2472, 'score': 18740.341069221497, 'total_duration': 20691.503726243973, 'accumulated_submission_time': 18740.341069221497, 'accumulated_eval_time': 1949.4109256267548, 'accumulated_logging_time': 0.7298746109008789, 'global_step': 22215, 'preemption_count': 0}), (23940, {'train/ctc_loss': Array(0.357722, dtype=float32), 'train/wer': 0.11924585829726445, 'validation/ctc_loss': Array(0.66510963, dtype=float32), 'validation/wer': 0.19041872230321402, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39538053, dtype=float32), 'test/wer': 0.12595210529522882, 'test/num_examples': 2472, 'score': 20180.55277776718, 'total_duration': 22268.30971980095, 'accumulated_submission_time': 20180.55277776718, 'accumulated_eval_time': 2085.8726336956024, 'accumulated_logging_time': 0.7837283611297607, 'global_step': 23940, 'preemption_count': 0}), (25627, {'train/ctc_loss': Array(0.327648, dtype=float32), 'train/wer': 0.1109453339589517, 'validation/ctc_loss': Array(0.6450211, dtype=float32), 'validation/wer': 0.18391148614074554, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3858443, dtype=float32), 'test/wer': 0.12211321674486625, 'test/num_examples': 2472, 'score': 21620.54094862938, 'total_duration': 23843.795273303986, 'accumulated_submission_time': 21620.54094862938, 'accumulated_eval_time': 2221.237764120102, 'accumulated_logging_time': 0.8373799324035645, 'global_step': 25627, 'preemption_count': 0}), (27322, {'train/ctc_loss': Array(0.35698888, dtype=float32), 'train/wer': 0.115199530197328, 'validation/ctc_loss': Array(0.6116994, dtype=float32), 'validation/wer': 0.17459474593780472, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36358392, dtype=float32), 'test/wer': 0.11715719131476855, 'test/num_examples': 2472, 'score': 23060.78440976143, 'total_duration': 25419.094624519348, 'accumulated_submission_time': 23060.78440976143, 'accumulated_eval_time': 2356.1602787971497, 'accumulated_logging_time': 0.893054723739624, 'global_step': 27322, 'preemption_count': 0}), (29032, {'train/ctc_loss': Array(0.30421615, dtype=float32), 'train/wer': 0.10361551330156368, 'validation/ctc_loss': Array(0.59509784, dtype=float32), 'validation/wer': 0.17090666846886857, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3473988, dtype=float32), 'test/wer': 0.11404951963114171, 'test/num_examples': 2472, 'score': 24500.674536943436, 'total_duration': 26997.705008029938, 'accumulated_submission_time': 24500.674536943436, 'accumulated_eval_time': 2494.747106552124, 'accumulated_logging_time': 0.9459781646728516, 'global_step': 29032, 'preemption_count': 0}), (30728, {'train/ctc_loss': Array(0.26352453, dtype=float32), 'train/wer': 0.09056334555602767, 'validation/ctc_loss': Array(0.5693506, dtype=float32), 'validation/wer': 0.1632891472044952, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3357012, dtype=float32), 'test/wer': 0.10669672780452136, 'test/num_examples': 2472, 'score': 25941.396134853363, 'total_duration': 28576.09837579727, 'accumulated_submission_time': 25941.396134853363, 'accumulated_eval_time': 2632.291247367859, 'accumulated_logging_time': 0.9961137771606445, 'global_step': 30728, 'preemption_count': 0}), (32447, {'train/ctc_loss': Array(0.25451177, dtype=float32), 'train/wer': 0.086225316926675, 'validation/ctc_loss': Array(0.54807425, dtype=float32), 'validation/wer': 0.15767979377661065, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31943408, dtype=float32), 'test/wer': 0.10291877399305344, 'test/num_examples': 2472, 'score': 27382.316435575485, 'total_duration': 30152.26672244072, 'accumulated_submission_time': 27382.316435575485, 'accumulated_eval_time': 2767.3991503715515, 'accumulated_logging_time': 1.0550148487091064, 'global_step': 32447, 'preemption_count': 0}), (34160, {'train/ctc_loss': Array(0.2348298, dtype=float32), 'train/wer': 0.08166989664082687, 'validation/ctc_loss': Array(0.53372824, dtype=float32), 'validation/wer': 0.15307452426697046, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30711773, dtype=float32), 'test/wer': 0.09796274856295574, 'test/num_examples': 2472, 'score': 28822.94229197502, 'total_duration': 31729.222623348236, 'accumulated_submission_time': 28822.94229197502, 'accumulated_eval_time': 2903.5987594127655, 'accumulated_logging_time': 1.1068816184997559, 'global_step': 34160, 'preemption_count': 0}), (35863, {'train/ctc_loss': Array(0.23932621, dtype=float32), 'train/wer': 0.0797680012210462, 'validation/ctc_loss': Array(0.5034849, dtype=float32), 'validation/wer': 0.14430809928845206, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28727466, dtype=float32), 'test/wer': 0.09174740519570207, 'test/num_examples': 2472, 'score': 30263.509187936783, 'total_duration': 33304.622517347336, 'accumulated_submission_time': 30263.509187936783, 'accumulated_eval_time': 3038.3013293743134, 'accumulated_logging_time': 1.1584124565124512, 'global_step': 35863, 'preemption_count': 0}), (37591, {'train/ctc_loss': Array(0.21954021, dtype=float32), 'train/wer': 0.07452886321631484, 'validation/ctc_loss': Array(0.4808648, dtype=float32), 'validation/wer': 0.1389304575340085, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27169424, dtype=float32), 'test/wer': 0.0879491398046026, 'test/num_examples': 2472, 'score': 31703.803174734116, 'total_duration': 34883.49290180206, 'accumulated_submission_time': 31703.803174734116, 'accumulated_eval_time': 3176.7409851551056, 'accumulated_logging_time': 1.2146975994110107, 'global_step': 37591, 'preemption_count': 0}), (39304, {'train/ctc_loss': Array(0.19475468, dtype=float32), 'train/wer': 0.06745336333564918, 'validation/ctc_loss': Array(0.45533058, dtype=float32), 'validation/wer': 0.13129362696351507, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2551368, dtype=float32), 'test/wer': 0.08207909329108524, 'test/num_examples': 2472, 'score': 33143.80793213844, 'total_duration': 36460.420042037964, 'accumulated_submission_time': 33143.80793213844, 'accumulated_eval_time': 3313.524995803833, 'accumulated_logging_time': 1.2724831104278564, 'global_step': 39304, 'preemption_count': 0}), (41016, {'train/ctc_loss': Array(0.18020418, dtype=float32), 'train/wer': 0.06146448816967662, 'validation/ctc_loss': Array(0.43848684, dtype=float32), 'validation/wer': 0.12627320737229308, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24059556, dtype=float32), 'test/wer': 0.07720431417951375, 'test/num_examples': 2472, 'score': 34583.80059170723, 'total_duration': 38035.929822444916, 'accumulated_submission_time': 34583.80059170723, 'accumulated_eval_time': 3448.9083466529846, 'accumulated_logging_time': 1.3277525901794434, 'global_step': 41016, 'preemption_count': 0}), (42751, {'train/ctc_loss': Array(0.15269303, dtype=float32), 'train/wer': 0.05257432539472989, 'validation/ctc_loss': Array(0.4185573, dtype=float32), 'validation/wer': 0.11923496529152225, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23369418, dtype=float32), 'test/wer': 0.07462474356630715, 'test/num_examples': 2472, 'score': 36024.716413497925, 'total_duration': 39611.73369860649, 'accumulated_submission_time': 36024.716413497925, 'accumulated_eval_time': 3583.6617407798767, 'accumulated_logging_time': 1.3814823627471924, 'global_step': 42751, 'preemption_count': 0}), (44456, {'train/ctc_loss': Array(0.15371695, dtype=float32), 'train/wer': 0.05171779705170247, 'validation/ctc_loss': Array(0.4076263, dtype=float32), 'validation/wer': 0.11592341929192775, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22438748, dtype=float32), 'test/wer': 0.07190299189567972, 'test/num_examples': 2472, 'score': 37464.84016394615, 'total_duration': 41187.92023229599, 'accumulated_submission_time': 37464.84016394615, 'accumulated_eval_time': 3719.5914764404297, 'accumulated_logging_time': 1.4356229305267334, 'global_step': 44456, 'preemption_count': 0}), (46165, {'train/ctc_loss': Array(0.16266528, dtype=float32), 'train/wer': 0.05495259447340699, 'validation/ctc_loss': Array(0.40298778, dtype=float32), 'validation/wer': 0.11462004112882204, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22044119, dtype=float32), 'test/wer': 0.0706233623788922, 'test/num_examples': 2472, 'score': 38905.040621995926, 'total_duration': 42763.2930085659, 'accumulated_submission_time': 38905.040621995926, 'accumulated_eval_time': 3854.6277837753296, 'accumulated_logging_time': 1.4938104152679443, 'global_step': 46165, 'preemption_count': 0}), (47904, {'train/ctc_loss': Array(0.14715919, dtype=float32), 'train/wer': 0.048795612762676126, 'validation/ctc_loss': Array(0.40185603, dtype=float32), 'validation/wer': 0.11478417023084275, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21938264, dtype=float32), 'test/wer': 0.0707046086974184, 'test/num_examples': 2472, 'score': 40345.54191684723, 'total_duration': 44340.17746424675, 'accumulated_submission_time': 40345.54191684723, 'accumulated_eval_time': 3990.876124382019, 'accumulated_logging_time': 1.5495550632476807, 'global_step': 47904, 'preemption_count': 0}), (48000, {'train/ctc_loss': Array(0.1594734, dtype=float32), 'train/wer': 0.054616546975526925, 'validation/ctc_loss': Array(0.401908, dtype=float32), 'validation/wer': 0.11480347953696284, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21940319, dtype=float32), 'test/wer': 0.0706639855381553, 'test/num_examples': 2472, 'score': 40417.566965818405, 'total_duration': 44543.13734269142, 'accumulated_submission_time': 40417.566965818405, 'accumulated_eval_time': 4121.736053466797, 'accumulated_logging_time': 1.6115646362304688, 'global_step': 48000, 'preemption_count': 0})], 'global_step': 48000}
I0213 01:57:57.030950 140441227016000 submission_runner.py:586] Timing: 40417.566965818405
I0213 01:57:57.031026 140441227016000 submission_runner.py:588] Total number of evals: 30
I0213 01:57:57.031089 140441227016000 submission_runner.py:589] ====================
I0213 01:57:57.031178 140441227016000 submission_runner.py:542] Using RNG seed 1231499801
I0213 01:57:57.035499 140441227016000 submission_runner.py:551] --- Tuning run 5/5 ---
I0213 01:57:57.035655 140441227016000 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_2/librispeech_deepspeech_jax/trial_5.
I0213 01:57:57.038902 140441227016000 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_2/librispeech_deepspeech_jax/trial_5/hparams.json.
I0213 01:57:57.041490 140441227016000 submission_runner.py:206] Initializing dataset.
I0213 01:57:57.041633 140441227016000 submission_runner.py:213] Initializing model.
I0213 01:57:58.331773 140441227016000 submission_runner.py:255] Initializing optimizer.
I0213 01:57:58.479945 140441227016000 submission_runner.py:262] Initializing metrics bundle.
I0213 01:57:58.480202 140441227016000 submission_runner.py:280] Initializing checkpoint and logger.
I0213 01:57:58.484443 140441227016000 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_2/librispeech_deepspeech_jax/trial_5 with prefix checkpoint_
I0213 01:57:58.484612 140441227016000 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_2/librispeech_deepspeech_jax/trial_5/meta_data_0.json.
I0213 01:57:58.485021 140441227016000 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0213 01:57:58.485116 140441227016000 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0213 01:57:59.044952 140441227016000 logger_utils.py:220] Unable to record git information. Continuing without it.
I0213 01:57:59.558285 140441227016000 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_2/librispeech_deepspeech_jax/trial_5/flags_0.json.
I0213 01:57:59.574753 140441227016000 submission_runner.py:314] Starting training loop.
I0213 01:57:59.578939 140441227016000 input_pipeline.py:20] Loading split = train-clean-100
I0213 01:57:59.627377 140441227016000 input_pipeline.py:20] Loading split = train-clean-360
I0213 01:58:00.382459 140441227016000 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0213 01:58:18.206534 140285356431104 logging_writer.py:48] [0] global_step=0, grad_norm=17.643762588500977, loss=32.94794845581055
I0213 01:58:18.224477 140441227016000 spec.py:321] Evaluating on the training split.
I0213 01:59:48.738748 140441227016000 spec.py:333] Evaluating on the validation split.
I0213 02:00:47.614497 140441227016000 spec.py:349] Evaluating on the test split.
I0213 02:01:18.189701 140441227016000 submission_runner.py:408] Time since start: 198.61s, 	Step: 1, 	{'train/ctc_loss': Array(31.628937, dtype=float32), 'train/wer': 3.1009376811671285, 'validation/ctc_loss': Array(30.570345, dtype=float32), 'validation/wer': 2.911437867480232, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.657665, dtype=float32), 'test/wer': 3.2318769930737514, 'test/num_examples': 2472, 'score': 18.649582386016846, 'total_duration': 198.61209559440613, 'accumulated_submission_time': 18.649582386016846, 'accumulated_eval_time': 179.9624104499817, 'accumulated_logging_time': 0}
I0213 02:01:18.207914 140285423572736 logging_writer.py:48] [1] accumulated_eval_time=179.962410, accumulated_logging_time=0, accumulated_submission_time=18.649582, global_step=1, preemption_count=0, score=18.649582, test/ctc_loss=30.657665252685547, test/num_examples=2472, test/wer=3.231877, total_duration=198.612096, train/ctc_loss=31.628936767578125, train/wer=3.100938, validation/ctc_loss=30.570344924926758, validation/num_examples=5348, validation/wer=2.911438
I0213 02:02:45.021159 140285314467584 logging_writer.py:48] [100] global_step=100, grad_norm=1.2905042171478271, loss=6.02442741394043
I0213 02:04:01.471549 140285356431104 logging_writer.py:48] [200] global_step=200, grad_norm=0.7758315801620483, loss=5.8357086181640625
I0213 02:05:18.400113 140285314467584 logging_writer.py:48] [300] global_step=300, grad_norm=0.30706945061683655, loss=5.728255271911621
I0213 02:06:35.144514 140285356431104 logging_writer.py:48] [400] global_step=400, grad_norm=1.7293211221694946, loss=5.485109329223633
I0213 02:07:58.264589 140285314467584 logging_writer.py:48] [500] global_step=500, grad_norm=1.411003589630127, loss=4.827337265014648
I0213 02:09:27.998720 140285356431104 logging_writer.py:48] [600] global_step=600, grad_norm=1.5390568971633911, loss=3.9598588943481445
I0213 02:10:55.878692 140285314467584 logging_writer.py:48] [700] global_step=700, grad_norm=3.361262321472168, loss=3.496615171432495
I0213 02:12:21.610686 140285356431104 logging_writer.py:48] [800] global_step=800, grad_norm=2.3696413040161133, loss=3.2559921741485596
I0213 02:13:47.215334 140285314467584 logging_writer.py:48] [900] global_step=900, grad_norm=2.221332550048828, loss=3.0608444213867188
I0213 02:15:18.238607 140285356431104 logging_writer.py:48] [1000] global_step=1000, grad_norm=3.1279451847076416, loss=2.9521114826202393
I0213 02:16:41.171333 140285423572736 logging_writer.py:48] [1100] global_step=1100, grad_norm=2.5945444107055664, loss=2.732706069946289
I0213 02:17:57.885789 140285415180032 logging_writer.py:48] [1200] global_step=1200, grad_norm=3.0844545364379883, loss=2.632288932800293
I0213 02:19:16.234453 140285423572736 logging_writer.py:48] [1300] global_step=1300, grad_norm=2.7289888858795166, loss=2.6051101684570312
I0213 02:20:41.405539 140285415180032 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.2248597145080566, loss=2.464379072189331
I0213 02:22:08.310888 140285423572736 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.528254985809326, loss=2.4408907890319824
I0213 02:23:38.647999 140285415180032 logging_writer.py:48] [1600] global_step=1600, grad_norm=3.036837339401245, loss=2.5266149044036865
I0213 02:25:09.450066 140285423572736 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.036123514175415, loss=2.4041543006896973
I0213 02:25:18.626630 140441227016000 spec.py:321] Evaluating on the training split.
I0213 02:26:10.598800 140441227016000 spec.py:333] Evaluating on the validation split.
I0213 02:27:01.292799 140441227016000 spec.py:349] Evaluating on the test split.
I0213 02:27:27.551314 140441227016000 submission_runner.py:408] Time since start: 1767.97s, 	Step: 1712, 	{'train/ctc_loss': Array(2.5453644, dtype=float32), 'train/wer': 0.5917621452917745, 'validation/ctc_loss': Array(3.0406559, dtype=float32), 'validation/wer': 0.6442163800843816, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.5435536, dtype=float32), 'test/wer': 0.5706741413279711, 'test/num_examples': 2472, 'score': 1458.985486984253, 'total_duration': 1767.9684970378876, 'accumulated_submission_time': 1458.985486984253, 'accumulated_eval_time': 308.87907791137695, 'accumulated_logging_time': 0.029255151748657227}
I0213 02:27:27.585568 140285423572736 logging_writer.py:48] [1712] accumulated_eval_time=308.879078, accumulated_logging_time=0.029255, accumulated_submission_time=1458.985487, global_step=1712, preemption_count=0, score=1458.985487, test/ctc_loss=2.543553590774536, test/num_examples=2472, test/wer=0.570674, total_duration=1767.968497, train/ctc_loss=2.5453643798828125, train/wer=0.591762, validation/ctc_loss=3.0406558513641357, validation/num_examples=5348, validation/wer=0.644216
I0213 02:28:34.268951 140285415180032 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.064016580581665, loss=2.2666306495666504
I0213 02:29:49.369382 140285423572736 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.147709846496582, loss=2.2792601585388184
I0213 02:31:12.495717 140285415180032 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.7720024585723877, loss=2.2677650451660156
I0213 02:32:40.114915 140285423572736 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.1742141246795654, loss=2.171503782272339
I0213 02:33:58.324922 140285415180032 logging_writer.py:48] [2200] global_step=2200, grad_norm=2.5565288066864014, loss=2.2861361503601074
I0213 02:35:18.481202 140285423572736 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.7373626232147217, loss=2.27164888381958
I0213 02:36:36.781950 140285415180032 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.6853975057601929, loss=2.107135772705078
I0213 02:38:01.513770 140285423572736 logging_writer.py:48] [2500] global_step=2500, grad_norm=2.6247127056121826, loss=2.140214443206787
I0213 02:39:30.544728 140285415180032 logging_writer.py:48] [2600] global_step=2600, grad_norm=2.510669231414795, loss=2.0723705291748047
I0213 02:40:56.589659 140285423572736 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.8255650997161865, loss=2.0695407390594482
I0213 02:42:27.208379 140285415180032 logging_writer.py:48] [2800] global_step=2800, grad_norm=2.6548943519592285, loss=2.075101852416992
I0213 02:43:55.075350 140285423572736 logging_writer.py:48] [2900] global_step=2900, grad_norm=3.2086992263793945, loss=2.055704116821289
I0213 02:45:20.588269 140285415180032 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.5861479043960571, loss=2.0392563343048096
I0213 02:46:50.167708 140285423572736 logging_writer.py:48] [3100] global_step=3100, grad_norm=2.4723706245422363, loss=2.0658326148986816
I0213 02:48:06.138579 140285415180032 logging_writer.py:48] [3200] global_step=3200, grad_norm=4.59979248046875, loss=2.0648605823516846
I0213 02:49:25.578688 140285423572736 logging_writer.py:48] [3300] global_step=3300, grad_norm=3.4149372577667236, loss=1.92019522190094
I0213 02:50:44.309440 140285415180032 logging_writer.py:48] [3400] global_step=3400, grad_norm=3.689981460571289, loss=1.9943337440490723
I0213 02:51:28.013713 140441227016000 spec.py:321] Evaluating on the training split.
I0213 02:52:24.507720 140441227016000 spec.py:333] Evaluating on the validation split.
I0213 02:53:17.316132 140441227016000 spec.py:349] Evaluating on the test split.
I0213 02:53:45.011776 140441227016000 submission_runner.py:408] Time since start: 3345.43s, 	Step: 3454, 	{'train/ctc_loss': Array(0.6002691, dtype=float32), 'train/wer': 0.19352086887769965, 'validation/ctc_loss': Array(1.0050833, dtype=float32), 'validation/wer': 0.2805642179248289, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.66275734, dtype=float32), 'test/wer': 0.20537038165458127, 'test/num_examples': 2472, 'score': 2899.323889017105, 'total_duration': 3345.4317281246185, 'accumulated_submission_time': 2899.323889017105, 'accumulated_eval_time': 445.87190437316895, 'accumulated_logging_time': 0.07917284965515137}
I0213 02:53:45.044720 140285423572736 logging_writer.py:48] [3454] accumulated_eval_time=445.871904, accumulated_logging_time=0.079173, accumulated_submission_time=2899.323889, global_step=3454, preemption_count=0, score=2899.323889, test/ctc_loss=0.6627573370933533, test/num_examples=2472, test/wer=0.205370, total_duration=3345.431728, train/ctc_loss=0.600269079208374, train/wer=0.193521, validation/ctc_loss=1.0050833225250244, validation/num_examples=5348, validation/wer=0.280564
I0213 02:54:20.497486 140285415180032 logging_writer.py:48] [3500] global_step=3500, grad_norm=3.0240116119384766, loss=1.964743971824646
I0213 02:55:35.802070 140285423572736 logging_writer.py:48] [3600] global_step=3600, grad_norm=9.929198265075684, loss=1.972283959388733
I0213 02:56:52.492705 140285415180032 logging_writer.py:48] [3700] global_step=3700, grad_norm=2.3972280025482178, loss=1.9081552028656006
I0213 02:58:20.553502 140285423572736 logging_writer.py:48] [3800] global_step=3800, grad_norm=3.341670513153076, loss=1.993655800819397
I0213 02:59:48.231122 140285415180032 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.919975996017456, loss=1.9710708856582642
I0213 03:01:16.619735 140285423572736 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.8729177713394165, loss=1.8805426359176636
I0213 03:02:46.907747 140285415180032 logging_writer.py:48] [4100] global_step=4100, grad_norm=2.634913682937622, loss=1.9243377447128296
I0213 03:04:10.465985 140285423572736 logging_writer.py:48] [4200] global_step=4200, grad_norm=2.655439615249634, loss=1.955567479133606
I0213 03:05:26.046586 140285415180032 logging_writer.py:48] [4300] global_step=4300, grad_norm=2.602353572845459, loss=1.8940435647964478
I0213 03:06:44.580841 140285423572736 logging_writer.py:48] [4400] global_step=4400, grad_norm=3.386491298675537, loss=1.832069754600525
I0213 03:08:06.973325 140285415180032 logging_writer.py:48] [4500] global_step=4500, grad_norm=3.545834541320801, loss=1.8722434043884277
I0213 03:09:32.759986 140285423572736 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.0751190185546875, loss=1.9391969442367554
I0213 03:11:00.777186 140285415180032 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.4863481521606445, loss=2.0150532722473145
I0213 03:12:29.385385 140285423572736 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.1067702770233154, loss=1.8172338008880615
I0213 03:13:59.744132 140285415180032 logging_writer.py:48] [4900] global_step=4900, grad_norm=2.1686134338378906, loss=1.8655309677124023
I0213 03:15:26.295263 140285423572736 logging_writer.py:48] [5000] global_step=5000, grad_norm=1.957300066947937, loss=1.8023221492767334
I0213 03:16:57.785371 140285415180032 logging_writer.py:48] [5100] global_step=5100, grad_norm=3.6511096954345703, loss=1.8167004585266113
I0213 03:17:46.647836 140441227016000 spec.py:321] Evaluating on the training split.
I0213 03:18:41.207067 140441227016000 spec.py:333] Evaluating on the validation split.
I0213 03:19:34.019184 140441227016000 spec.py:349] Evaluating on the test split.
I0213 03:20:01.691712 140441227016000 submission_runner.py:408] Time since start: 4922.11s, 	Step: 5151, 	{'train/ctc_loss': Array(0.6795138, dtype=float32), 'train/wer': 0.21180016942253319, 'validation/ctc_loss': Array(0.8538837, dtype=float32), 'validation/wer': 0.2422835185417612, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.542488, dtype=float32), 'test/wer': 0.16789551723437532, 'test/num_examples': 2472, 'score': 4340.835663795471, 'total_duration': 4922.1106123924255, 'accumulated_submission_time': 4340.835663795471, 'accumulated_eval_time': 580.9096684455872, 'accumulated_logging_time': 0.1311507225036621}
I0213 03:20:01.729043 140285423572736 logging_writer.py:48] [5151] accumulated_eval_time=580.909668, accumulated_logging_time=0.131151, accumulated_submission_time=4340.835664, global_step=5151, preemption_count=0, score=4340.835664, test/ctc_loss=0.5424879789352417, test/num_examples=2472, test/wer=0.167896, total_duration=4922.110612, train/ctc_loss=0.6795138120651245, train/wer=0.211800, validation/ctc_loss=0.853883683681488, validation/num_examples=5348, validation/wer=0.242284
I0213 03:20:39.209021 140285415180032 logging_writer.py:48] [5200] global_step=5200, grad_norm=2.1704835891723633, loss=1.7861905097961426
I0213 03:21:54.573895 140285423572736 logging_writer.py:48] [5300] global_step=5300, grad_norm=2.5088794231414795, loss=1.8141342401504517
I0213 03:23:10.050917 140285415180032 logging_writer.py:48] [5400] global_step=5400, grad_norm=1.8885033130645752, loss=1.8506523370742798
I0213 03:24:25.051066 140285423572736 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.814103364944458, loss=1.8175872564315796
I0213 03:25:47.242548 140285415180032 logging_writer.py:48] [5600] global_step=5600, grad_norm=2.943188190460205, loss=1.8891754150390625
I0213 03:27:13.965889 140285423572736 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.7188060283660889, loss=1.8234546184539795
I0213 03:28:44.363641 140285415180032 logging_writer.py:48] [5800] global_step=5800, grad_norm=3.4389443397521973, loss=1.8185421228408813
I0213 03:30:14.614087 140285423572736 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.270771026611328, loss=1.7909175157546997
I0213 03:31:44.088479 140285415180032 logging_writer.py:48] [6000] global_step=6000, grad_norm=3.0105998516082764, loss=1.804856538772583
I0213 03:33:10.966255 140285423572736 logging_writer.py:48] [6100] global_step=6100, grad_norm=2.479449510574341, loss=1.7708088159561157
I0213 03:34:37.988802 140285423572736 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.4878315925598145, loss=1.7615818977355957
I0213 03:35:53.354844 140285415180032 logging_writer.py:48] [6300] global_step=6300, grad_norm=2.1549594402313232, loss=1.813118815422058
I0213 03:37:09.991438 140285423572736 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.6095685958862305, loss=1.7471524477005005
I0213 03:38:31.145669 140285415180032 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.8431193828582764, loss=1.7469048500061035
I0213 03:39:53.018466 140285423572736 logging_writer.py:48] [6600] global_step=6600, grad_norm=3.4927773475646973, loss=1.724352478981018
I0213 03:41:20.388538 140285415180032 logging_writer.py:48] [6700] global_step=6700, grad_norm=4.165955066680908, loss=1.757232904434204
I0213 03:42:51.920294 140285423572736 logging_writer.py:48] [6800] global_step=6800, grad_norm=4.323060989379883, loss=1.7466012239456177
I0213 03:44:01.922213 140441227016000 spec.py:321] Evaluating on the training split.
I0213 03:44:58.612583 140441227016000 spec.py:333] Evaluating on the validation split.
I0213 03:45:52.561404 140441227016000 spec.py:349] Evaluating on the test split.
I0213 03:46:19.737569 140441227016000 submission_runner.py:408] Time since start: 6500.16s, 	Step: 6877, 	{'train/ctc_loss': Array(0.62634224, dtype=float32), 'train/wer': 0.19708784345232527, 'validation/ctc_loss': Array(0.78297794, dtype=float32), 'validation/wer': 0.22196047385037218, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.48380572, dtype=float32), 'test/wer': 0.15446956309792212, 'test/num_examples': 2472, 'score': 5780.938099622726, 'total_duration': 6500.155675172806, 'accumulated_submission_time': 5780.938099622726, 'accumulated_eval_time': 718.7179839611053, 'accumulated_logging_time': 0.18588995933532715}
I0213 03:46:19.775698 140285423572736 logging_writer.py:48] [6877] accumulated_eval_time=718.717984, accumulated_logging_time=0.185890, accumulated_submission_time=5780.938100, global_step=6877, preemption_count=0, score=5780.938100, test/ctc_loss=0.48380571603775024, test/num_examples=2472, test/wer=0.154470, total_duration=6500.155675, train/ctc_loss=0.626342236995697, train/wer=0.197088, validation/ctc_loss=0.7829779386520386, validation/num_examples=5348, validation/wer=0.221960
I0213 03:46:37.873614 140285415180032 logging_writer.py:48] [6900] global_step=6900, grad_norm=2.039365291595459, loss=1.788114070892334
I0213 03:47:53.078202 140285423572736 logging_writer.py:48] [7000] global_step=7000, grad_norm=3.9979751110076904, loss=2.1735446453094482
I0213 03:49:08.917547 140285415180032 logging_writer.py:48] [7100] global_step=7100, grad_norm=3.3297221660614014, loss=1.73557448387146
I0213 03:50:38.654615 140285423572736 logging_writer.py:48] [7200] global_step=7200, grad_norm=2.3697893619537354, loss=1.7917330265045166
I0213 03:51:59.076220 140285423572736 logging_writer.py:48] [7300] global_step=7300, grad_norm=5.210710048675537, loss=1.6825810670852661
I0213 03:53:15.326235 140285415180032 logging_writer.py:48] [7400] global_step=7400, grad_norm=2.3788323402404785, loss=1.7258399724960327
I0213 03:54:34.930802 140285423572736 logging_writer.py:48] [7500] global_step=7500, grad_norm=3.2316160202026367, loss=1.6828526258468628
I0213 03:55:55.909581 140285415180032 logging_writer.py:48] [7600] global_step=7600, grad_norm=3.2314393520355225, loss=1.7246993780136108
I0213 03:57:20.813930 140285423572736 logging_writer.py:48] [7700] global_step=7700, grad_norm=3.812493324279785, loss=1.7581225633621216
I0213 03:58:51.592892 140285415180032 logging_writer.py:48] [7800] global_step=7800, grad_norm=2.7702839374542236, loss=1.7213557958602905
I0213 04:00:18.227411 140285423572736 logging_writer.py:48] [7900] global_step=7900, grad_norm=3.2509827613830566, loss=1.7817896604537964
I0213 04:01:47.493018 140285415180032 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.9803466796875, loss=1.6338846683502197
I0213 04:03:18.419833 140285423572736 logging_writer.py:48] [8100] global_step=8100, grad_norm=3.18721604347229, loss=1.7388930320739746
I0213 04:04:44.806948 140285415180032 logging_writer.py:48] [8200] global_step=8200, grad_norm=2.706193447113037, loss=1.6865023374557495
I0213 04:06:09.026269 140285423572736 logging_writer.py:48] [8300] global_step=8300, grad_norm=2.825566053390503, loss=1.7086998224258423
I0213 04:07:24.571471 140285415180032 logging_writer.py:48] [8400] global_step=8400, grad_norm=2.451955556869507, loss=1.717358112335205
I0213 04:08:40.750569 140285423572736 logging_writer.py:48] [8500] global_step=8500, grad_norm=2.2942423820495605, loss=1.7320750951766968
I0213 04:10:02.532531 140285415180032 logging_writer.py:48] [8600] global_step=8600, grad_norm=2.634507417678833, loss=1.6538697481155396
I0213 04:10:20.448030 140441227016000 spec.py:321] Evaluating on the training split.
I0213 04:11:13.501206 140441227016000 spec.py:333] Evaluating on the validation split.
I0213 04:12:05.851044 140441227016000 spec.py:349] Evaluating on the test split.
I0213 04:12:33.421107 140441227016000 submission_runner.py:408] Time since start: 8073.84s, 	Step: 8624, 	{'train/ctc_loss': Array(0.69155705, dtype=float32), 'train/wer': 0.21579534105952575, 'validation/ctc_loss': Array(0.738769, dtype=float32), 'validation/wer': 0.21139828340268593, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.44854873, dtype=float32), 'test/wer': 0.14309507850425526, 'test/num_examples': 2472, 'score': 7221.522592782974, 'total_duration': 8073.839767456055, 'accumulated_submission_time': 7221.522592782974, 'accumulated_eval_time': 851.6845552921295, 'accumulated_logging_time': 0.23926997184753418}
I0213 04:12:33.459830 140285423572736 logging_writer.py:48] [8624] accumulated_eval_time=851.684555, accumulated_logging_time=0.239270, accumulated_submission_time=7221.522593, global_step=8624, preemption_count=0, score=7221.522593, test/ctc_loss=0.44854873418807983, test/num_examples=2472, test/wer=0.143095, total_duration=8073.839767, train/ctc_loss=0.6915570497512817, train/wer=0.215795, validation/ctc_loss=0.738768994808197, validation/num_examples=5348, validation/wer=0.211398
I0213 04:13:31.078437 140285415180032 logging_writer.py:48] [8700] global_step=8700, grad_norm=3.007444143295288, loss=1.6359715461730957
I0213 04:14:46.113725 140285423572736 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.7338351011276245, loss=1.6908397674560547
I0213 04:16:09.332886 140285415180032 logging_writer.py:48] [8900] global_step=8900, grad_norm=2.3105697631835938, loss=1.6279456615447998
I0213 04:17:36.032654 140285423572736 logging_writer.py:48] [9000] global_step=9000, grad_norm=3.385587215423584, loss=1.6894084215164185
I0213 04:19:04.129096 140285415180032 logging_writer.py:48] [9100] global_step=9100, grad_norm=2.712226629257202, loss=1.663894534111023
I0213 04:20:34.525032 140285423572736 logging_writer.py:48] [9200] global_step=9200, grad_norm=3.426191806793213, loss=1.6804486513137817
I0213 04:22:03.787689 140285423572736 logging_writer.py:48] [9300] global_step=9300, grad_norm=3.304474115371704, loss=1.6739293336868286
I0213 04:23:19.665480 140285415180032 logging_writer.py:48] [9400] global_step=9400, grad_norm=2.525020122528076, loss=1.6597812175750732
I0213 04:24:36.122723 140285423572736 logging_writer.py:48] [9500] global_step=9500, grad_norm=2.755073308944702, loss=1.7109626531600952
I0213 04:25:58.962727 140285415180032 logging_writer.py:48] [9600] global_step=9600, grad_norm=2.554043769836426, loss=1.604388952255249
I0213 04:27:23.231577 140285423572736 logging_writer.py:48] [9700] global_step=9700, grad_norm=2.472538471221924, loss=1.7121177911758423
I0213 04:28:53.016081 140285415180032 logging_writer.py:48] [9800] global_step=9800, grad_norm=2.6291122436523438, loss=1.7070354223251343
I0213 04:30:22.612912 140285423572736 logging_writer.py:48] [9900] global_step=9900, grad_norm=2.00187611579895, loss=1.6181087493896484
I0213 04:31:50.039626 140285415180032 logging_writer.py:48] [10000] global_step=10000, grad_norm=2.213825225830078, loss=1.6862645149230957
I0213 04:33:14.696895 140285423572736 logging_writer.py:48] [10100] global_step=10100, grad_norm=3.2263150215148926, loss=1.6726515293121338
I0213 04:34:42.557264 140285415180032 logging_writer.py:48] [10200] global_step=10200, grad_norm=2.7624828815460205, loss=1.65375554561615
I0213 04:36:12.680104 140285423572736 logging_writer.py:48] [10300] global_step=10300, grad_norm=3.7620275020599365, loss=1.6536613702774048
I0213 04:36:33.661576 140441227016000 spec.py:321] Evaluating on the training split.
I0213 04:37:28.792851 140441227016000 spec.py:333] Evaluating on the validation split.
I0213 04:38:23.901088 140441227016000 spec.py:349] Evaluating on the test split.
I0213 04:38:51.533867 140441227016000 submission_runner.py:408] Time since start: 9651.95s, 	Step: 10329, 	{'train/ctc_loss': Array(0.58510447, dtype=float32), 'train/wer': 0.1821902111638566, 'validation/ctc_loss': Array(0.71844894, dtype=float32), 'validation/wer': 0.20581789393398148, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42991805, dtype=float32), 'test/wer': 0.137895314118579, 'test/num_examples': 2472, 'score': 8661.632459640503, 'total_duration': 9651.952268838882, 'accumulated_submission_time': 8661.632459640503, 'accumulated_eval_time': 989.5500612258911, 'accumulated_logging_time': 0.2970449924468994}
I0213 04:38:51.570483 140285423572736 logging_writer.py:48] [10329] accumulated_eval_time=989.550061, accumulated_logging_time=0.297045, accumulated_submission_time=8661.632460, global_step=10329, preemption_count=0, score=8661.632460, test/ctc_loss=0.4299180507659912, test/num_examples=2472, test/wer=0.137895, total_duration=9651.952269, train/ctc_loss=0.5851044654846191, train/wer=0.182190, validation/ctc_loss=0.7184489369392395, validation/num_examples=5348, validation/wer=0.205818
I0213 04:39:45.713025 140285415180032 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.9683406352996826, loss=1.6153285503387451
I0213 04:41:01.628395 140285423572736 logging_writer.py:48] [10500] global_step=10500, grad_norm=4.453317165374756, loss=1.612927794456482
I0213 04:42:16.790441 140285415180032 logging_writer.py:48] [10600] global_step=10600, grad_norm=3.0724353790283203, loss=1.5862923860549927
I0213 04:43:31.590351 140285423572736 logging_writer.py:48] [10700] global_step=10700, grad_norm=3.9175407886505127, loss=1.7002543210983276
I0213 04:44:53.839820 140285415180032 logging_writer.py:48] [10800] global_step=10800, grad_norm=2.608078718185425, loss=1.6903576850891113
I0213 04:46:23.776246 140285423572736 logging_writer.py:48] [10900] global_step=10900, grad_norm=2.7050089836120605, loss=1.6027194261550903
I0213 04:47:49.572770 140285415180032 logging_writer.py:48] [11000] global_step=11000, grad_norm=2.1912662982940674, loss=1.654423713684082
I0213 04:49:16.481176 140285423572736 logging_writer.py:48] [11100] global_step=11100, grad_norm=2.544522523880005, loss=1.6500158309936523
I0213 04:50:45.119456 140285415180032 logging_writer.py:48] [11200] global_step=11200, grad_norm=2.2750158309936523, loss=1.6588054895401
I0213 04:52:11.756033 140285423572736 logging_writer.py:48] [11300] global_step=11300, grad_norm=4.934863090515137, loss=1.635209560394287
I0213 04:53:33.748390 140285423572736 logging_writer.py:48] [11400] global_step=11400, grad_norm=3.18923282623291, loss=1.6266642808914185
I0213 04:54:50.850990 140285415180032 logging_writer.py:48] [11500] global_step=11500, grad_norm=2.6752541065216064, loss=1.6778780221939087
I0213 04:56:11.000560 140285423572736 logging_writer.py:48] [11600] global_step=11600, grad_norm=2.718061685562134, loss=1.701604962348938
I0213 04:57:28.727276 140285415180032 logging_writer.py:48] [11700] global_step=11700, grad_norm=4.471757888793945, loss=1.5975173711776733
I0213 04:58:54.310734 140285423572736 logging_writer.py:48] [11800] global_step=11800, grad_norm=5.730242729187012, loss=1.6781866550445557
I0213 05:00:25.464314 140285415180032 logging_writer.py:48] [11900] global_step=11900, grad_norm=3.8237221240997314, loss=1.6083810329437256
I0213 05:01:54.727555 140285423572736 logging_writer.py:48] [12000] global_step=12000, grad_norm=2.225580930709839, loss=1.5553603172302246
I0213 05:02:51.691236 140441227016000 spec.py:321] Evaluating on the training split.
I0213 05:03:45.849353 140441227016000 spec.py:333] Evaluating on the validation split.
I0213 05:04:39.542723 140441227016000 spec.py:349] Evaluating on the test split.
I0213 05:05:07.602445 140441227016000 submission_runner.py:408] Time since start: 11228.02s, 	Step: 12066, 	{'train/ctc_loss': Array(0.58945584, dtype=float32), 'train/wer': 0.18889116365828415, 'validation/ctc_loss': Array(0.697573, dtype=float32), 'validation/wer': 0.1997258078530948, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4067004, dtype=float32), 'test/wer': 0.1325533686754819, 'test/num_examples': 2472, 'score': 10101.663102388382, 'total_duration': 11228.021137475967, 'accumulated_submission_time': 10101.663102388382, 'accumulated_eval_time': 1125.454788684845, 'accumulated_logging_time': 0.35008716583251953}
I0213 05:05:07.639719 140285423572736 logging_writer.py:48] [12066] accumulated_eval_time=1125.454789, accumulated_logging_time=0.350087, accumulated_submission_time=10101.663102, global_step=12066, preemption_count=0, score=10101.663102, test/ctc_loss=0.40670040249824524, test/num_examples=2472, test/wer=0.132553, total_duration=11228.021137, train/ctc_loss=0.5894558429718018, train/wer=0.188891, validation/ctc_loss=0.6975730061531067, validation/num_examples=5348, validation/wer=0.199726
I0213 05:05:34.056751 140285415180032 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.7265150547027588, loss=1.5558533668518066
I0213 05:06:49.237972 140285423572736 logging_writer.py:48] [12200] global_step=12200, grad_norm=4.083216667175293, loss=1.7038334608078003
I0213 05:08:06.052197 140285415180032 logging_writer.py:48] [12300] global_step=12300, grad_norm=2.5220866203308105, loss=1.6573283672332764
I0213 05:09:35.630070 140285423572736 logging_writer.py:48] [12400] global_step=12400, grad_norm=3.419830083847046, loss=1.6047919988632202
I0213 05:10:53.097047 140285415180032 logging_writer.py:48] [12500] global_step=12500, grad_norm=3.827627658843994, loss=1.5184659957885742
I0213 05:12:11.193393 140285423572736 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.860285758972168, loss=1.6020939350128174
I0213 05:13:32.494601 140285415180032 logging_writer.py:48] [12700] global_step=12700, grad_norm=2.0674831867218018, loss=1.563818097114563
I0213 05:15:00.037485 140285423572736 logging_writer.py:48] [12800] global_step=12800, grad_norm=3.169952154159546, loss=1.5730876922607422
I0213 05:16:28.365784 140285415180032 logging_writer.py:48] [12900] global_step=12900, grad_norm=3.7798867225646973, loss=1.5635427236557007
I0213 05:17:56.751486 140285423572736 logging_writer.py:48] [13000] global_step=13000, grad_norm=3.7175099849700928, loss=1.5243903398513794
I0213 05:19:24.527521 140285415180032 logging_writer.py:48] [13100] global_step=13100, grad_norm=2.3784682750701904, loss=1.6370244026184082
I0213 05:20:55.137433 140285423572736 logging_writer.py:48] [13200] global_step=13200, grad_norm=2.9511194229125977, loss=1.6007288694381714
I0213 05:22:25.187207 140285415180032 logging_writer.py:48] [13300] global_step=13300, grad_norm=3.650254249572754, loss=1.5316985845565796
I0213 05:23:56.534063 140285423572736 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.6008232831954956, loss=1.5806456804275513
I0213 05:25:12.419638 140285415180032 logging_writer.py:48] [13500] global_step=13500, grad_norm=2.1910204887390137, loss=1.550357460975647
I0213 05:26:29.846248 140285423572736 logging_writer.py:48] [13600] global_step=13600, grad_norm=3.642869234085083, loss=1.630287528038025
I0213 05:27:53.367888 140285415180032 logging_writer.py:48] [13700] global_step=13700, grad_norm=2.574834108352661, loss=1.5182335376739502
I0213 05:29:08.026435 140441227016000 spec.py:321] Evaluating on the training split.
I0213 05:30:03.259046 140441227016000 spec.py:333] Evaluating on the validation split.
I0213 05:30:57.138553 140441227016000 spec.py:349] Evaluating on the test split.
I0213 05:31:25.395357 140441227016000 submission_runner.py:408] Time since start: 12805.81s, 	Step: 13788, 	{'train/ctc_loss': Array(0.44308084, dtype=float32), 'train/wer': 0.1463969658659924, 'validation/ctc_loss': Array(0.6531635, dtype=float32), 'validation/wer': 0.18738716124236077, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39410502, dtype=float32), 'test/wer': 0.12727235797127942, 'test/num_examples': 2472, 'score': 11541.959456205368, 'total_duration': 12805.813822984695, 'accumulated_submission_time': 11541.959456205368, 'accumulated_eval_time': 1262.816997051239, 'accumulated_logging_time': 0.4042665958404541}
I0213 05:31:25.427429 140285423572736 logging_writer.py:48] [13788] accumulated_eval_time=1262.816997, accumulated_logging_time=0.404267, accumulated_submission_time=11541.959456, global_step=13788, preemption_count=0, score=11541.959456, test/ctc_loss=0.3941050171852112, test/num_examples=2472, test/wer=0.127272, total_duration=12805.813823, train/ctc_loss=0.4430808424949646, train/wer=0.146397, validation/ctc_loss=0.653163492679596, validation/num_examples=5348, validation/wer=0.187387
I0213 05:31:35.516942 140285415180032 logging_writer.py:48] [13800] global_step=13800, grad_norm=4.304577350616455, loss=1.5465930700302124
I0213 05:32:50.559616 140285423572736 logging_writer.py:48] [13900] global_step=13900, grad_norm=2.105351686477661, loss=1.6415894031524658
I0213 05:34:05.488481 140285415180032 logging_writer.py:48] [14000] global_step=14000, grad_norm=3.3391823768615723, loss=1.5578887462615967
I0213 05:35:35.041921 140285423572736 logging_writer.py:48] [14100] global_step=14100, grad_norm=2.0726099014282227, loss=1.5620434284210205
I0213 05:37:03.104461 140285415180032 logging_writer.py:48] [14200] global_step=14200, grad_norm=3.885263204574585, loss=1.586018443107605
I0213 05:38:28.051086 140285423572736 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.9547772407531738, loss=1.6586167812347412
I0213 05:39:57.253123 140285415180032 logging_writer.py:48] [14400] global_step=14400, grad_norm=2.2473645210266113, loss=1.5866386890411377
I0213 05:41:18.655890 140285423572736 logging_writer.py:48] [14500] global_step=14500, grad_norm=2.150304079055786, loss=1.5601404905319214
I0213 05:42:37.161558 140285415180032 logging_writer.py:48] [14600] global_step=14600, grad_norm=2.190218210220337, loss=1.5126526355743408
I0213 05:43:57.058212 140285423572736 logging_writer.py:48] [14700] global_step=14700, grad_norm=2.72922420501709, loss=1.5106290578842163
I0213 05:45:15.724536 140285415180032 logging_writer.py:48] [14800] global_step=14800, grad_norm=4.215917587280273, loss=1.5865890979766846
I0213 05:46:44.584511 140285423572736 logging_writer.py:48] [14900] global_step=14900, grad_norm=3.7896888256073, loss=1.5822257995605469
I0213 05:48:15.636304 140285415180032 logging_writer.py:48] [15000] global_step=15000, grad_norm=4.458339691162109, loss=1.6059315204620361
I0213 05:49:41.700987 140285423572736 logging_writer.py:48] [15100] global_step=15100, grad_norm=3.019249200820923, loss=1.5590481758117676
I0213 05:51:11.627371 140285415180032 logging_writer.py:48] [15200] global_step=15200, grad_norm=2.256502151489258, loss=1.4709672927856445
I0213 05:52:39.960188 140285423572736 logging_writer.py:48] [15300] global_step=15300, grad_norm=4.361713409423828, loss=1.5781890153884888
I0213 05:54:07.262822 140285415180032 logging_writer.py:48] [15400] global_step=15400, grad_norm=3.2675061225891113, loss=1.5227595567703247
I0213 05:55:26.127676 140441227016000 spec.py:321] Evaluating on the training split.
I0213 05:56:21.324012 140441227016000 spec.py:333] Evaluating on the validation split.
I0213 05:57:13.661056 140441227016000 spec.py:349] Evaluating on the test split.
I0213 05:57:41.002595 140441227016000 submission_runner.py:408] Time since start: 14381.42s, 	Step: 15492, 	{'train/ctc_loss': Array(0.5043031, dtype=float32), 'train/wer': 0.163937207851208, 'validation/ctc_loss': Array(0.64487046, dtype=float32), 'validation/wer': 0.1834673720999836, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37502876, dtype=float32), 'test/wer': 0.12061015985213171, 'test/num_examples': 2472, 'score': 12982.570106267929, 'total_duration': 14381.421175718307, 'accumulated_submission_time': 12982.570106267929, 'accumulated_eval_time': 1397.6854872703552, 'accumulated_logging_time': 0.454848051071167}
I0213 05:57:41.035993 140285423572736 logging_writer.py:48] [15492] accumulated_eval_time=1397.685487, accumulated_logging_time=0.454848, accumulated_submission_time=12982.570106, global_step=15492, preemption_count=0, score=12982.570106, test/ctc_loss=0.3750287592411041, test/num_examples=2472, test/wer=0.120610, total_duration=14381.421176, train/ctc_loss=0.5043030977249146, train/wer=0.163937, validation/ctc_loss=0.6448704600334167, validation/num_examples=5348, validation/wer=0.183467
I0213 05:57:47.881508 140285415180032 logging_writer.py:48] [15500] global_step=15500, grad_norm=3.0355162620544434, loss=1.5806564092636108
I0213 05:59:02.748799 140285423572736 logging_writer.py:48] [15600] global_step=15600, grad_norm=2.0091354846954346, loss=1.5505802631378174
I0213 06:00:17.775049 140285415180032 logging_writer.py:48] [15700] global_step=15700, grad_norm=4.129887580871582, loss=1.5983115434646606
I0213 06:01:33.049937 140285423572736 logging_writer.py:48] [15800] global_step=15800, grad_norm=3.353163719177246, loss=1.5984574556350708
I0213 06:02:54.260591 140285415180032 logging_writer.py:48] [15900] global_step=15900, grad_norm=2.4538004398345947, loss=1.5739144086837769
I0213 06:04:23.513564 140285423572736 logging_writer.py:48] [16000] global_step=16000, grad_norm=3.4837331771850586, loss=1.562443733215332
I0213 06:05:51.924498 140285415180032 logging_writer.py:48] [16100] global_step=16100, grad_norm=3.4874508380889893, loss=1.5866308212280273
I0213 06:07:20.793374 140285423572736 logging_writer.py:48] [16200] global_step=16200, grad_norm=3.0106379985809326, loss=1.5437554121017456
I0213 06:08:47.306321 140285415180032 logging_writer.py:48] [16300] global_step=16300, grad_norm=2.18442702293396, loss=1.548730731010437
I0213 06:10:19.165386 140285423572736 logging_writer.py:48] [16400] global_step=16400, grad_norm=5.563080310821533, loss=1.5184485912322998
I0213 06:11:49.031830 140285423572736 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.7899527549743652, loss=1.508217692375183
I0213 06:13:04.790788 140285415180032 logging_writer.py:48] [16600] global_step=16600, grad_norm=2.242072105407715, loss=1.5313968658447266
I0213 06:14:21.205258 140285423572736 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.9989625215530396, loss=1.501301646232605
I0213 06:15:42.862800 140285415180032 logging_writer.py:48] [16800] global_step=16800, grad_norm=2.3057215213775635, loss=1.585730791091919
I0213 06:17:09.858023 140285423572736 logging_writer.py:48] [16900] global_step=16900, grad_norm=2.9466278553009033, loss=1.526643991470337
I0213 06:18:39.116771 140285415180032 logging_writer.py:48] [17000] global_step=17000, grad_norm=2.9205174446105957, loss=1.5605286359786987
I0213 06:20:06.726205 140285423572736 logging_writer.py:48] [17100] global_step=17100, grad_norm=2.797196626663208, loss=1.5040788650512695
I0213 06:21:38.432957 140285415180032 logging_writer.py:48] [17200] global_step=17200, grad_norm=2.7509849071502686, loss=1.545405626296997
I0213 06:21:41.536248 140441227016000 spec.py:321] Evaluating on the training split.
I0213 06:22:37.052168 140441227016000 spec.py:333] Evaluating on the validation split.
I0213 06:23:31.988961 140441227016000 spec.py:349] Evaluating on the test split.
I0213 06:23:59.243607 140441227016000 submission_runner.py:408] Time since start: 15959.66s, 	Step: 17205, 	{'train/ctc_loss': Array(0.44294888, dtype=float32), 'train/wer': 0.1443313293253173, 'validation/ctc_loss': Array(0.6099825, dtype=float32), 'validation/wer': 0.17565675777440937, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3541107, dtype=float32), 'test/wer': 0.1146385554404566, 'test/num_examples': 2472, 'score': 14422.980370283127, 'total_duration': 15959.662130594254, 'accumulated_submission_time': 14422.980370283127, 'accumulated_eval_time': 1535.3861787319183, 'accumulated_logging_time': 0.5043253898620605}
I0213 06:23:59.280731 140285423572736 logging_writer.py:48] [17205] accumulated_eval_time=1535.386179, accumulated_logging_time=0.504325, accumulated_submission_time=14422.980370, global_step=17205, preemption_count=0, score=14422.980370, test/ctc_loss=0.3541106879711151, test/num_examples=2472, test/wer=0.114639, total_duration=15959.662131, train/ctc_loss=0.4429488778114319, train/wer=0.144331, validation/ctc_loss=0.6099824905395508, validation/num_examples=5348, validation/wer=0.175657
I0213 06:25:11.513122 140285415180032 logging_writer.py:48] [17300] global_step=17300, grad_norm=4.401077747344971, loss=1.4793362617492676
I0213 06:26:27.833788 140285423572736 logging_writer.py:48] [17400] global_step=17400, grad_norm=3.656222105026245, loss=1.5675442218780518
I0213 06:27:49.952594 140285415180032 logging_writer.py:48] [17500] global_step=17500, grad_norm=2.1854822635650635, loss=1.5378055572509766
I0213 06:29:11.242861 140285423572736 logging_writer.py:48] [17600] global_step=17600, grad_norm=2.0967929363250732, loss=1.5302423238754272
I0213 06:30:29.644849 140285415180032 logging_writer.py:48] [17700] global_step=17700, grad_norm=2.618875741958618, loss=1.517973780632019
I0213 06:31:49.239541 140285423572736 logging_writer.py:48] [17800] global_step=17800, grad_norm=2.300718069076538, loss=1.5194851160049438
I0213 06:33:09.915987 140285415180032 logging_writer.py:48] [17900] global_step=17900, grad_norm=2.807316780090332, loss=1.4970018863677979
I0213 06:34:38.691957 140285423572736 logging_writer.py:48] [18000] global_step=18000, grad_norm=3.1593639850616455, loss=1.4919921159744263
I0213 06:36:08.829050 140285415180032 logging_writer.py:48] [18100] global_step=18100, grad_norm=2.3313868045806885, loss=1.4880300760269165
I0213 06:37:33.778740 140285423572736 logging_writer.py:48] [18200] global_step=18200, grad_norm=2.6939172744750977, loss=1.5267020463943481
I0213 06:39:01.504367 140285415180032 logging_writer.py:48] [18300] global_step=18300, grad_norm=3.4757025241851807, loss=1.5911931991577148
I0213 06:40:32.248126 140285423572736 logging_writer.py:48] [18400] global_step=18400, grad_norm=3.725543737411499, loss=1.4700652360916138
I0213 06:42:01.426838 140285415180032 logging_writer.py:48] [18500] global_step=18500, grad_norm=3.566817283630371, loss=1.513008713722229
I0213 06:43:27.119018 140285423572736 logging_writer.py:48] [18600] global_step=18600, grad_norm=3.3393428325653076, loss=1.5178085565567017
I0213 06:44:44.839884 140285415180032 logging_writer.py:48] [18700] global_step=18700, grad_norm=2.2923905849456787, loss=1.4083315134048462
I0213 06:46:05.297112 140285423572736 logging_writer.py:48] [18800] global_step=18800, grad_norm=2.457504987716675, loss=1.502193808555603
I0213 06:47:29.926404 140285415180032 logging_writer.py:48] [18900] global_step=18900, grad_norm=2.5759220123291016, loss=1.4834024906158447
I0213 06:47:59.399271 140441227016000 spec.py:321] Evaluating on the training split.
I0213 06:48:53.406981 140441227016000 spec.py:333] Evaluating on the validation split.
I0213 06:49:46.430309 140441227016000 spec.py:349] Evaluating on the test split.
I0213 06:50:14.915076 140441227016000 submission_runner.py:408] Time since start: 17535.33s, 	Step: 18938, 	{'train/ctc_loss': Array(0.43057463, dtype=float32), 'train/wer': 0.1437458643843492, 'validation/ctc_loss': Array(0.59888196, dtype=float32), 'validation/wer': 0.1724417583054153, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3469349, dtype=float32), 'test/wer': 0.112688643795828, 'test/num_examples': 2472, 'score': 15863.009141683578, 'total_duration': 17535.333825826645, 'accumulated_submission_time': 15863.009141683578, 'accumulated_eval_time': 1670.8955328464508, 'accumulated_logging_time': 0.5572524070739746}
I0213 06:50:14.958344 140285423572736 logging_writer.py:48] [18938] accumulated_eval_time=1670.895533, accumulated_logging_time=0.557252, accumulated_submission_time=15863.009142, global_step=18938, preemption_count=0, score=15863.009142, test/ctc_loss=0.3469349145889282, test/num_examples=2472, test/wer=0.112689, total_duration=17535.333826, train/ctc_loss=0.4305746257305145, train/wer=0.143746, validation/ctc_loss=0.5988819599151611, validation/num_examples=5348, validation/wer=0.172442
I0213 06:51:02.143768 140285415180032 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.9328477382659912, loss=1.5136350393295288
I0213 06:52:17.624618 140285423572736 logging_writer.py:48] [19100] global_step=19100, grad_norm=2.1838324069976807, loss=1.499067783355713
I0213 06:53:41.962033 140285415180032 logging_writer.py:48] [19200] global_step=19200, grad_norm=2.4123497009277344, loss=1.5659096240997314
I0213 06:55:09.350367 140285423572736 logging_writer.py:48] [19300] global_step=19300, grad_norm=3.2509052753448486, loss=1.4792428016662598
I0213 06:56:37.153655 140285415180032 logging_writer.py:48] [19400] global_step=19400, grad_norm=3.268284320831299, loss=1.4933626651763916
I0213 06:58:02.225218 140285423572736 logging_writer.py:48] [19500] global_step=19500, grad_norm=2.3763182163238525, loss=1.4780175685882568
I0213 06:59:33.381731 140285423572736 logging_writer.py:48] [19600] global_step=19600, grad_norm=2.418947219848633, loss=1.479927659034729
I0213 07:00:51.133848 140285415180032 logging_writer.py:48] [19700] global_step=19700, grad_norm=3.2423975467681885, loss=1.507563829421997
I0213 07:02:07.889729 140285423572736 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.7674270868301392, loss=1.4235285520553589
I0213 07:03:26.385883 140285415180032 logging_writer.py:48] [19900] global_step=19900, grad_norm=2.5578742027282715, loss=1.5003544092178345
I0213 07:04:51.108199 140285423572736 logging_writer.py:48] [20000] global_step=20000, grad_norm=2.5779640674591064, loss=1.4546746015548706
I0213 07:06:18.695315 140285415180032 logging_writer.py:48] [20100] global_step=20100, grad_norm=2.8163280487060547, loss=1.484742522239685
I0213 07:07:47.107084 140285423572736 logging_writer.py:48] [20200] global_step=20200, grad_norm=4.060719966888428, loss=1.5143532752990723
I0213 07:09:14.032854 140285415180032 logging_writer.py:48] [20300] global_step=20300, grad_norm=3.0246431827545166, loss=1.4550353288650513
I0213 07:10:44.479825 140285423572736 logging_writer.py:48] [20400] global_step=20400, grad_norm=2.790771245956421, loss=1.5099620819091797
I0213 07:12:14.897928 140285415180032 logging_writer.py:48] [20500] global_step=20500, grad_norm=4.1253461837768555, loss=1.5169010162353516
I0213 07:13:44.805534 140285423572736 logging_writer.py:48] [20600] global_step=20600, grad_norm=2.914921998977661, loss=1.4177684783935547
I0213 07:14:15.256404 140441227016000 spec.py:321] Evaluating on the training split.
I0213 07:15:10.078904 140441227016000 spec.py:333] Evaluating on the validation split.
I0213 07:16:02.314204 140441227016000 spec.py:349] Evaluating on the test split.
I0213 07:16:30.249650 140441227016000 submission_runner.py:408] Time since start: 19110.67s, 	Step: 20641, 	{'train/ctc_loss': Array(0.4117053, dtype=float32), 'train/wer': 0.13532842736440448, 'validation/ctc_loss': Array(0.5848249, dtype=float32), 'validation/wer': 0.1684061133263176, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33290595, dtype=float32), 'test/wer': 0.1072248288749416, 'test/num_examples': 2472, 'score': 17303.21779513359, 'total_duration': 19110.668027877808, 'accumulated_submission_time': 17303.21779513359, 'accumulated_eval_time': 1805.8819556236267, 'accumulated_logging_time': 0.6184689998626709}
I0213 07:16:30.280842 140285423572736 logging_writer.py:48] [20641] accumulated_eval_time=1805.881956, accumulated_logging_time=0.618469, accumulated_submission_time=17303.217795, global_step=20641, preemption_count=0, score=17303.217795, test/ctc_loss=0.33290594816207886, test/num_examples=2472, test/wer=0.107225, total_duration=19110.668028, train/ctc_loss=0.41170528531074524, train/wer=0.135328, validation/ctc_loss=0.5848249197006226, validation/num_examples=5348, validation/wer=0.168406
I0213 07:17:15.916824 140285415180032 logging_writer.py:48] [20700] global_step=20700, grad_norm=2.4680542945861816, loss=1.4282708168029785
I0213 07:18:30.948930 140285423572736 logging_writer.py:48] [20800] global_step=20800, grad_norm=3.9765548706054688, loss=1.4624607563018799
I0213 07:19:46.060894 140285415180032 logging_writer.py:48] [20900] global_step=20900, grad_norm=3.2458012104034424, loss=1.4605891704559326
I0213 07:21:02.039432 140285423572736 logging_writer.py:48] [21000] global_step=21000, grad_norm=2.576965808868408, loss=1.430938482284546
I0213 07:22:24.693346 140285415180032 logging_writer.py:48] [21100] global_step=21100, grad_norm=2.081479549407959, loss=1.4759451150894165
I0213 07:23:54.260092 140285423572736 logging_writer.py:48] [21200] global_step=21200, grad_norm=2.569612979888916, loss=1.4447211027145386
I0213 07:25:25.988064 140285415180032 logging_writer.py:48] [21300] global_step=21300, grad_norm=3.6785919666290283, loss=1.4811952114105225
I0213 07:26:53.196250 140285423572736 logging_writer.py:48] [21400] global_step=21400, grad_norm=2.6118736267089844, loss=1.5135412216186523
I0213 07:28:21.193568 140285415180032 logging_writer.py:48] [21500] global_step=21500, grad_norm=2.531878709793091, loss=1.5152380466461182
I0213 07:29:52.691819 140285423572736 logging_writer.py:48] [21600] global_step=21600, grad_norm=2.0976555347442627, loss=1.4089345932006836
I0213 07:31:15.628794 140285423572736 logging_writer.py:48] [21700] global_step=21700, grad_norm=2.54154372215271, loss=1.37470543384552
I0213 07:32:33.711260 140285415180032 logging_writer.py:48] [21800] global_step=21800, grad_norm=2.954862594604492, loss=1.4544312953948975
I0213 07:33:54.542833 140285423572736 logging_writer.py:48] [21900] global_step=21900, grad_norm=2.3605053424835205, loss=1.371781349182129
I0213 07:35:17.056989 140285415180032 logging_writer.py:48] [22000] global_step=22000, grad_norm=4.104665756225586, loss=1.4838159084320068
I0213 07:36:40.959020 140285423572736 logging_writer.py:48] [22100] global_step=22100, grad_norm=2.246964454650879, loss=1.4906996488571167
I0213 07:38:12.098844 140285415180032 logging_writer.py:48] [22200] global_step=22200, grad_norm=3.245347261428833, loss=1.4488587379455566
I0213 07:39:38.400533 140285423572736 logging_writer.py:48] [22300] global_step=22300, grad_norm=2.9284555912017822, loss=1.490142583847046
I0213 07:40:30.633373 140441227016000 spec.py:321] Evaluating on the training split.
I0213 07:41:24.441710 140441227016000 spec.py:333] Evaluating on the validation split.
I0213 07:42:18.233482 140441227016000 spec.py:349] Evaluating on the test split.
I0213 07:42:45.402603 140441227016000 submission_runner.py:408] Time since start: 20685.82s, 	Step: 22360, 	{'train/ctc_loss': Array(0.40284434, dtype=float32), 'train/wer': 0.13384243915217367, 'validation/ctc_loss': Array(0.5693361, dtype=float32), 'validation/wer': 0.16293192504127363, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3231798, dtype=float32), 'test/wer': 0.10419840350984096, 'test/num_examples': 2472, 'score': 18743.48190689087, 'total_duration': 20685.821735858917, 'accumulated_submission_time': 18743.48190689087, 'accumulated_eval_time': 1940.645127773285, 'accumulated_logging_time': 0.6647696495056152}
I0213 07:42:45.435345 140285423572736 logging_writer.py:48] [22360] accumulated_eval_time=1940.645128, accumulated_logging_time=0.664770, accumulated_submission_time=18743.481907, global_step=22360, preemption_count=0, score=18743.481907, test/ctc_loss=0.32317981123924255, test/num_examples=2472, test/wer=0.104198, total_duration=20685.821736, train/ctc_loss=0.4028443396091461, train/wer=0.133842, validation/ctc_loss=0.5693361163139343, validation/num_examples=5348, validation/wer=0.162932
I0213 07:43:16.493750 140285415180032 logging_writer.py:48] [22400] global_step=22400, grad_norm=3.017789363861084, loss=1.4486650228500366
I0213 07:44:31.833819 140285423572736 logging_writer.py:48] [22500] global_step=22500, grad_norm=2.3592987060546875, loss=1.4344561100006104
I0213 07:45:51.739168 140285415180032 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.4515202045440674, loss=1.4454288482666016
I0213 07:47:20.147157 140285423572736 logging_writer.py:48] [22700] global_step=22700, grad_norm=3.7851667404174805, loss=1.3930786848068237
I0213 07:48:37.507866 140285415180032 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.8587746620178223, loss=1.5924829244613647
I0213 07:49:57.453917 140285423572736 logging_writer.py:48] [22900] global_step=22900, grad_norm=2.5708703994750977, loss=1.4316273927688599
I0213 07:51:20.074804 140285415180032 logging_writer.py:48] [23000] global_step=23000, grad_norm=2.490018844604492, loss=1.4703013896942139
I0213 07:52:44.068581 140285423572736 logging_writer.py:48] [23100] global_step=23100, grad_norm=4.414085865020752, loss=1.3901914358139038
I0213 07:54:13.252307 140285415180032 logging_writer.py:48] [23200] global_step=23200, grad_norm=2.0911030769348145, loss=1.3854151964187622
I0213 07:55:40.968599 140285423572736 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.5320128202438354, loss=1.4528738260269165
I0213 07:57:12.394395 140285415180032 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.7970517873764038, loss=1.4141225814819336
I0213 07:58:42.478669 140285423572736 logging_writer.py:48] [23500] global_step=23500, grad_norm=4.427203178405762, loss=1.451760172843933
I0213 08:00:11.529313 140285415180032 logging_writer.py:48] [23600] global_step=23600, grad_norm=2.4104092121124268, loss=1.4184248447418213
I0213 08:01:45.654840 140285423572736 logging_writer.py:48] [23700] global_step=23700, grad_norm=2.403324842453003, loss=1.4429131746292114
I0213 08:03:03.013412 140285415180032 logging_writer.py:48] [23800] global_step=23800, grad_norm=3.641033411026001, loss=1.4436650276184082
I0213 08:04:23.345466 140285423572736 logging_writer.py:48] [23900] global_step=23900, grad_norm=2.2178561687469482, loss=1.4070738554000854
I0213 08:05:44.200873 140285415180032 logging_writer.py:48] [24000] global_step=24000, grad_norm=2.0302977561950684, loss=1.4577301740646362
I0213 08:06:45.987753 140441227016000 spec.py:321] Evaluating on the training split.
I0213 08:07:39.736919 140441227016000 spec.py:333] Evaluating on the validation split.
I0213 08:08:33.400589 140441227016000 spec.py:349] Evaluating on the test split.
I0213 08:08:59.597786 140441227016000 submission_runner.py:408] Time since start: 22260.02s, 	Step: 24075, 	{'train/ctc_loss': Array(0.43493432, dtype=float32), 'train/wer': 0.13886030069804906, 'validation/ctc_loss': Array(0.5609899, dtype=float32), 'validation/wer': 0.16197611438832946, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31203714, dtype=float32), 'test/wer': 0.10269534661710641, 'test/num_examples': 2472, 'score': 20183.945833444595, 'total_duration': 22260.01620745659, 'accumulated_submission_time': 20183.945833444595, 'accumulated_eval_time': 2074.2483954429626, 'accumulated_logging_time': 0.7133445739746094}
I0213 08:08:59.633709 140285423572736 logging_writer.py:48] [24075] accumulated_eval_time=2074.248395, accumulated_logging_time=0.713345, accumulated_submission_time=20183.945833, global_step=24075, preemption_count=0, score=20183.945833, test/ctc_loss=0.3120371401309967, test/num_examples=2472, test/wer=0.102695, total_duration=22260.016207, train/ctc_loss=0.4349343180656433, train/wer=0.138860, validation/ctc_loss=0.5609899163246155, validation/num_examples=5348, validation/wer=0.161976
I0213 08:09:19.269715 140285415180032 logging_writer.py:48] [24100] global_step=24100, grad_norm=2.4223084449768066, loss=1.4195491075515747
I0213 08:10:34.011484 140285423572736 logging_writer.py:48] [24200] global_step=24200, grad_norm=3.028285503387451, loss=1.3551440238952637
I0213 08:11:50.934762 140285415180032 logging_writer.py:48] [24300] global_step=24300, grad_norm=2.7288923263549805, loss=1.3964356184005737
I0213 08:13:20.485979 140285423572736 logging_writer.py:48] [24400] global_step=24400, grad_norm=2.180084466934204, loss=1.4028658866882324
I0213 08:14:51.795109 140285415180032 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.7078964710235596, loss=1.4274508953094482
I0213 08:16:21.709419 140285423572736 logging_writer.py:48] [24600] global_step=24600, grad_norm=3.445101499557495, loss=1.4424400329589844
I0213 08:17:50.317956 140285415180032 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.8575694561004639, loss=1.4089349508285522
I0213 08:19:12.514111 140285423572736 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.779168963432312, loss=1.3598530292510986
I0213 08:20:31.723355 140285415180032 logging_writer.py:48] [24900] global_step=24900, grad_norm=3.942451000213623, loss=1.4579166173934937
I0213 08:21:52.230902 140285423572736 logging_writer.py:48] [25000] global_step=25000, grad_norm=2.7704198360443115, loss=1.4695488214492798
I0213 08:23:13.952036 140285415180032 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.8832663297653198, loss=1.447320818901062
I0213 08:24:44.224380 140285423572736 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.9465150833129883, loss=1.3621742725372314
I0213 08:26:14.574848 140285415180032 logging_writer.py:48] [25300] global_step=25300, grad_norm=3.0985145568847656, loss=1.4876818656921387
I0213 08:27:44.333977 140285423572736 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.728858232498169, loss=1.3959959745407104
I0213 08:29:14.213685 140285415180032 logging_writer.py:48] [25500] global_step=25500, grad_norm=2.1641621589660645, loss=1.4323359727859497
I0213 08:30:42.891621 140285423572736 logging_writer.py:48] [25600] global_step=25600, grad_norm=3.1234872341156006, loss=1.374205470085144
I0213 08:32:09.483078 140285415180032 logging_writer.py:48] [25700] global_step=25700, grad_norm=2.2592315673828125, loss=1.4981575012207031
I0213 08:33:00.299380 140441227016000 spec.py:321] Evaluating on the training split.
I0213 08:33:54.982844 140441227016000 spec.py:333] Evaluating on the validation split.
I0213 08:34:49.291736 140441227016000 spec.py:349] Evaluating on the test split.
I0213 08:35:16.556253 140441227016000 submission_runner.py:408] Time since start: 23836.97s, 	Step: 25757, 	{'train/ctc_loss': Array(0.361608, dtype=float32), 'train/wer': 0.12085050508394159, 'validation/ctc_loss': Array(0.5388162, dtype=float32), 'validation/wer': 0.15627021442984446, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30561492, dtype=float32), 'test/wer': 0.09869396542969147, 'test/num_examples': 2472, 'score': 21624.523320913315, 'total_duration': 23836.974450588226, 'accumulated_submission_time': 21624.523320913315, 'accumulated_eval_time': 2210.4984588623047, 'accumulated_logging_time': 0.7659845352172852}
I0213 08:35:16.595162 140285423572736 logging_writer.py:48] [25757] accumulated_eval_time=2210.498459, accumulated_logging_time=0.765985, accumulated_submission_time=21624.523321, global_step=25757, preemption_count=0, score=21624.523321, test/ctc_loss=0.3056149184703827, test/num_examples=2472, test/wer=0.098694, total_duration=23836.974451, train/ctc_loss=0.36160799860954285, train/wer=0.120851, validation/ctc_loss=0.5388162136077881, validation/num_examples=5348, validation/wer=0.156270
I0213 08:35:49.575793 140285415180032 logging_writer.py:48] [25800] global_step=25800, grad_norm=2.297071933746338, loss=1.412523865699768
I0213 08:37:04.951668 140285423572736 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.6191149950027466, loss=1.39163339138031
I0213 08:38:19.978124 140285415180032 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.577589750289917, loss=1.3466591835021973
I0213 08:39:35.298427 140285423572736 logging_writer.py:48] [26100] global_step=26100, grad_norm=3.8032872676849365, loss=1.4443275928497314
I0213 08:40:51.244782 140285415180032 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.9556262493133545, loss=1.3921679258346558
I0213 08:42:19.373378 140285423572736 logging_writer.py:48] [26300] global_step=26300, grad_norm=2.335742950439453, loss=1.4482431411743164
I0213 08:43:49.616076 140285415180032 logging_writer.py:48] [26400] global_step=26400, grad_norm=2.429194688796997, loss=1.3914705514907837
I0213 08:45:17.927661 140285423572736 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.705273151397705, loss=1.4112783670425415
I0213 08:46:46.325348 140285415180032 logging_writer.py:48] [26600] global_step=26600, grad_norm=2.2763357162475586, loss=1.4051481485366821
I0213 08:48:16.264694 140285423572736 logging_writer.py:48] [26700] global_step=26700, grad_norm=9.183079719543457, loss=1.4222795963287354
I0213 08:49:45.495108 140285423572736 logging_writer.py:48] [26800] global_step=26800, grad_norm=2.3011834621429443, loss=1.359818935394287
I0213 08:51:02.768585 140285415180032 logging_writer.py:48] [26900] global_step=26900, grad_norm=3.017627477645874, loss=1.3841732740402222
I0213 08:52:21.633822 140285423572736 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.6900948286056519, loss=1.3764111995697021
I0213 08:53:38.786610 140285415180032 logging_writer.py:48] [27100] global_step=27100, grad_norm=3.472581386566162, loss=1.3601739406585693
I0213 08:55:04.580273 140285423572736 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.8841787576675415, loss=1.379892349243164
I0213 08:56:33.632242 140285415180032 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.6883776187896729, loss=1.3971740007400513
I0213 08:58:00.911558 140285423572736 logging_writer.py:48] [27400] global_step=27400, grad_norm=3.4845001697540283, loss=1.3678157329559326
I0213 08:59:17.095420 140441227016000 spec.py:321] Evaluating on the training split.
I0213 09:00:10.858496 140441227016000 spec.py:333] Evaluating on the validation split.
I0213 09:01:04.798772 140441227016000 spec.py:349] Evaluating on the test split.
I0213 09:01:31.405187 140441227016000 submission_runner.py:408] Time since start: 25411.83s, 	Step: 27487, 	{'train/ctc_loss': Array(0.37259036, dtype=float32), 'train/wer': 0.12723949872360177, 'validation/ctc_loss': Array(0.5171766, dtype=float32), 'validation/wer': 0.15005261785917723, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29226407, dtype=float32), 'test/wer': 0.09477383056080271, 'test/num_examples': 2472, 'score': 23064.93439102173, 'total_duration': 25411.825412988663, 'accumulated_submission_time': 23064.93439102173, 'accumulated_eval_time': 2344.8034658432007, 'accumulated_logging_time': 0.8201003074645996}
I0213 09:01:31.440510 140285423572736 logging_writer.py:48] [27487] accumulated_eval_time=2344.803466, accumulated_logging_time=0.820100, accumulated_submission_time=23064.934391, global_step=27487, preemption_count=0, score=23064.934391, test/ctc_loss=0.29226407408714294, test/num_examples=2472, test/wer=0.094774, total_duration=25411.825413, train/ctc_loss=0.3725903630256653, train/wer=0.127239, validation/ctc_loss=0.517176628112793, validation/num_examples=5348, validation/wer=0.150053
I0213 09:01:42.424358 140285415180032 logging_writer.py:48] [27500] global_step=27500, grad_norm=3.4228830337524414, loss=1.2904173135757446
I0213 09:02:58.511553 140285423572736 logging_writer.py:48] [27600] global_step=27600, grad_norm=2.387676477432251, loss=1.3769956827163696
I0213 09:04:14.683644 140285415180032 logging_writer.py:48] [27700] global_step=27700, grad_norm=2.071547746658325, loss=1.3923739194869995
I0213 09:05:39.934780 140285423572736 logging_writer.py:48] [27800] global_step=27800, grad_norm=3.0221500396728516, loss=1.4246385097503662
I0213 09:07:02.213927 140285423572736 logging_writer.py:48] [27900] global_step=27900, grad_norm=3.121164083480835, loss=1.3924020528793335
I0213 09:08:20.341473 140285415180032 logging_writer.py:48] [28000] global_step=28000, grad_norm=2.494826078414917, loss=1.3156728744506836
I0213 09:09:43.464115 140285423572736 logging_writer.py:48] [28100] global_step=28100, grad_norm=2.8908615112304688, loss=1.3490453958511353
I0213 09:11:06.358831 140285415180032 logging_writer.py:48] [28200] global_step=28200, grad_norm=2.2783398628234863, loss=1.3874543905258179
I0213 09:12:33.063672 140285423572736 logging_writer.py:48] [28300] global_step=28300, grad_norm=2.6253085136413574, loss=1.353847861289978
I0213 09:14:03.886326 140285415180032 logging_writer.py:48] [28400] global_step=28400, grad_norm=2.3654067516326904, loss=1.354967474937439
I0213 09:15:29.119596 140285423572736 logging_writer.py:48] [28500] global_step=28500, grad_norm=2.1110517978668213, loss=1.3620814085006714
I0213 09:16:56.483435 140285415180032 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.8692859411239624, loss=1.3660974502563477
I0213 09:18:22.835509 140285423572736 logging_writer.py:48] [28700] global_step=28700, grad_norm=2.0614917278289795, loss=1.3802696466445923
I0213 09:19:53.745373 140285415180032 logging_writer.py:48] [28800] global_step=28800, grad_norm=3.2029237747192383, loss=1.2928129434585571
I0213 09:21:18.977807 140285423572736 logging_writer.py:48] [28900] global_step=28900, grad_norm=4.999816417694092, loss=1.38038170337677
I0213 09:22:37.617859 140285415180032 logging_writer.py:48] [29000] global_step=29000, grad_norm=3.173962116241455, loss=1.378449559211731
I0213 09:23:57.926506 140285423572736 logging_writer.py:48] [29100] global_step=29100, grad_norm=2.616945266723633, loss=1.3521144390106201
I0213 09:25:21.924604 140285415180032 logging_writer.py:48] [29200] global_step=29200, grad_norm=2.481032609939575, loss=1.2983044385910034
I0213 09:25:31.421411 140441227016000 spec.py:321] Evaluating on the training split.
I0213 09:26:25.455479 140441227016000 spec.py:333] Evaluating on the validation split.
I0213 09:27:17.984726 140441227016000 spec.py:349] Evaluating on the test split.
I0213 09:27:45.534631 140441227016000 submission_runner.py:408] Time since start: 26985.95s, 	Step: 29212, 	{'train/ctc_loss': Array(0.30891111, dtype=float32), 'train/wer': 0.10437902857990669, 'validation/ctc_loss': Array(0.5069906, dtype=float32), 'validation/wer': 0.1449549610434749, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2780522, dtype=float32), 'test/wer': 0.09018341356407288, 'test/num_examples': 2472, 'score': 24504.827920675278, 'total_duration': 26985.95388007164, 'accumulated_submission_time': 24504.827920675278, 'accumulated_eval_time': 2478.910748243332, 'accumulated_logging_time': 0.8709836006164551}
I0213 09:27:45.576792 140285423572736 logging_writer.py:48] [29212] accumulated_eval_time=2478.910748, accumulated_logging_time=0.870984, accumulated_submission_time=24504.827921, global_step=29212, preemption_count=0, score=24504.827921, test/ctc_loss=0.2780522108078003, test/num_examples=2472, test/wer=0.090183, total_duration=26985.953880, train/ctc_loss=0.30891111493110657, train/wer=0.104379, validation/ctc_loss=0.5069906115531921, validation/num_examples=5348, validation/wer=0.144955
I0213 09:28:52.135038 140285415180032 logging_writer.py:48] [29300] global_step=29300, grad_norm=2.7375576496124268, loss=1.3332750797271729
I0213 09:30:08.243397 140285423572736 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.727330207824707, loss=1.324405550956726
I0213 09:31:35.466704 140285415180032 logging_writer.py:48] [29500] global_step=29500, grad_norm=2.3634092807769775, loss=1.33366060256958
I0213 09:33:02.546942 140285423572736 logging_writer.py:48] [29600] global_step=29600, grad_norm=2.8183207511901855, loss=1.3674176931381226
I0213 09:34:29.358760 140285415180032 logging_writer.py:48] [29700] global_step=29700, grad_norm=2.9668939113616943, loss=1.2632389068603516
I0213 09:36:00.622516 140285423572736 logging_writer.py:48] [29800] global_step=29800, grad_norm=2.2571258544921875, loss=1.3440001010894775
I0213 09:37:28.981752 140285397882624 logging_writer.py:48] [29900] global_step=29900, grad_norm=3.5039379596710205, loss=1.334489107131958
I0213 09:38:45.583582 140285389489920 logging_writer.py:48] [30000] global_step=30000, grad_norm=2.5546085834503174, loss=1.3781646490097046
I0213 09:40:02.834634 140285397882624 logging_writer.py:48] [30100] global_step=30100, grad_norm=3.285717010498047, loss=1.3353651762008667
I0213 09:41:21.848658 140285389489920 logging_writer.py:48] [30200] global_step=30200, grad_norm=2.362342357635498, loss=1.2982165813446045
I0213 09:42:50.165249 140285397882624 logging_writer.py:48] [30300] global_step=30300, grad_norm=2.52485728263855, loss=1.3324381113052368
I0213 09:44:16.799573 140285389489920 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.5959208011627197, loss=1.3355238437652588
I0213 09:45:45.725955 140285397882624 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.8390730619430542, loss=1.2564706802368164
I0213 09:47:12.115955 140285389489920 logging_writer.py:48] [30600] global_step=30600, grad_norm=2.697983741760254, loss=1.2892429828643799
I0213 09:48:43.131586 140285397882624 logging_writer.py:48] [30700] global_step=30700, grad_norm=2.068537473678589, loss=1.2491264343261719
I0213 09:50:12.254972 140285389489920 logging_writer.py:48] [30800] global_step=30800, grad_norm=4.415125370025635, loss=1.2938358783721924
I0213 09:51:44.039114 140285397882624 logging_writer.py:48] [30900] global_step=30900, grad_norm=2.9004404544830322, loss=1.3398469686508179
I0213 09:51:46.197561 140441227016000 spec.py:321] Evaluating on the training split.
I0213 09:52:39.767060 140441227016000 spec.py:333] Evaluating on the validation split.
I0213 09:53:31.726317 140441227016000 spec.py:349] Evaluating on the test split.
I0213 09:53:58.671218 140441227016000 submission_runner.py:408] Time since start: 28559.09s, 	Step: 30904, 	{'train/ctc_loss': Array(0.3336694, dtype=float32), 'train/wer': 0.11066489436918465, 'validation/ctc_loss': Array(0.5003634, dtype=float32), 'validation/wer': 0.14309161300288675, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27133593, dtype=float32), 'test/wer': 0.08754290821197165, 'test/num_examples': 2472, 'score': 25945.360632419586, 'total_duration': 28559.09006714821, 'accumulated_submission_time': 25945.360632419586, 'accumulated_eval_time': 2611.3780493736267, 'accumulated_logging_time': 0.9297785758972168}
I0213 09:53:58.710924 140285397882624 logging_writer.py:48] [30904] accumulated_eval_time=2611.378049, accumulated_logging_time=0.929779, accumulated_submission_time=25945.360632, global_step=30904, preemption_count=0, score=25945.360632, test/ctc_loss=0.2713359296321869, test/num_examples=2472, test/wer=0.087543, total_duration=28559.090067, train/ctc_loss=0.33366939425468445, train/wer=0.110665, validation/ctc_loss=0.5003634095191956, validation/num_examples=5348, validation/wer=0.143092
I0213 09:55:11.259632 140285389489920 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.7499744892120361, loss=1.30562162399292
I0213 09:56:26.159105 140285397882624 logging_writer.py:48] [31100] global_step=31100, grad_norm=2.6845316886901855, loss=1.2912020683288574
I0213 09:57:40.998298 140285389489920 logging_writer.py:48] [31200] global_step=31200, grad_norm=2.4741272926330566, loss=1.2788325548171997
I0213 09:58:59.583331 140285397882624 logging_writer.py:48] [31300] global_step=31300, grad_norm=2.092433214187622, loss=1.2615495920181274
I0213 10:00:30.676940 140285389489920 logging_writer.py:48] [31400] global_step=31400, grad_norm=2.90109920501709, loss=1.271146297454834
I0213 10:01:56.926548 140285397882624 logging_writer.py:48] [31500] global_step=31500, grad_norm=2.0311572551727295, loss=1.2884666919708252
I0213 10:03:29.367971 140285389489920 logging_writer.py:48] [31600] global_step=31600, grad_norm=3.1765525341033936, loss=1.3112013339996338
I0213 10:05:00.072794 140285397882624 logging_writer.py:48] [31700] global_step=31700, grad_norm=4.212985515594482, loss=1.314366340637207
I0213 10:06:28.111563 140285389489920 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.9183480739593506, loss=1.2838016748428345
I0213 10:07:58.467197 140285397882624 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.5388060808181763, loss=1.3337851762771606
I0213 10:09:24.322230 140285397882624 logging_writer.py:48] [32000] global_step=32000, grad_norm=2.123756170272827, loss=1.2542366981506348
I0213 10:10:43.115868 140285389489920 logging_writer.py:48] [32100] global_step=32100, grad_norm=2.707143545150757, loss=1.321000099182129
I0213 10:12:02.181812 140285397882624 logging_writer.py:48] [32200] global_step=32200, grad_norm=2.150104284286499, loss=1.2836558818817139
I0213 10:13:21.356784 140285389489920 logging_writer.py:48] [32300] global_step=32300, grad_norm=3.4534859657287598, loss=1.2587146759033203
I0213 10:14:48.747733 140285397882624 logging_writer.py:48] [32400] global_step=32400, grad_norm=3.407081127166748, loss=1.3418872356414795
I0213 10:16:19.021905 140285389489920 logging_writer.py:48] [32500] global_step=32500, grad_norm=2.168196439743042, loss=1.292373538017273
I0213 10:17:47.659096 140285397882624 logging_writer.py:48] [32600] global_step=32600, grad_norm=5.313483715057373, loss=1.2690823078155518
I0213 10:17:59.156689 140441227016000 spec.py:321] Evaluating on the training split.
I0213 10:18:53.123199 140441227016000 spec.py:333] Evaluating on the validation split.
I0213 10:19:47.220836 140441227016000 spec.py:349] Evaluating on the test split.
I0213 10:20:13.641726 140441227016000 submission_runner.py:408] Time since start: 30134.06s, 	Step: 32614, 	{'train/ctc_loss': Array(0.3215798, dtype=float32), 'train/wer': 0.10961561435145943, 'validation/ctc_loss': Array(0.4891848, dtype=float32), 'validation/wer': 0.1402531450032343, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26732162, dtype=float32), 'test/wer': 0.08496333759876505, 'test/num_examples': 2472, 'score': 27385.716521024704, 'total_duration': 30134.061345100403, 'accumulated_submission_time': 27385.716521024704, 'accumulated_eval_time': 2745.85751581192, 'accumulated_logging_time': 0.98575758934021}
I0213 10:20:13.680249 140285397882624 logging_writer.py:48] [32614] accumulated_eval_time=2745.857516, accumulated_logging_time=0.985758, accumulated_submission_time=27385.716521, global_step=32614, preemption_count=0, score=27385.716521, test/ctc_loss=0.2673216164112091, test/num_examples=2472, test/wer=0.084963, total_duration=30134.061345, train/ctc_loss=0.32157981395721436, train/wer=0.109616, validation/ctc_loss=0.48918479681015015, validation/num_examples=5348, validation/wer=0.140253
I0213 10:21:19.098060 140285389489920 logging_writer.py:48] [32700] global_step=32700, grad_norm=4.520387649536133, loss=1.3103151321411133
I0213 10:22:34.110105 140285397882624 logging_writer.py:48] [32800] global_step=32800, grad_norm=2.7854955196380615, loss=1.2561583518981934
I0213 10:24:00.039398 140285389489920 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.7988370656967163, loss=1.2846125364303589
I0213 10:25:28.167940 140285397882624 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.874866247177124, loss=1.2339853048324585
I0213 10:26:48.461560 140285389489920 logging_writer.py:48] [33100] global_step=33100, grad_norm=2.9607231616973877, loss=1.309509038925171
I0213 10:28:05.783777 140285397882624 logging_writer.py:48] [33200] global_step=33200, grad_norm=2.363250970840454, loss=1.3115752935409546
I0213 10:29:23.848203 140285389489920 logging_writer.py:48] [33300] global_step=33300, grad_norm=3.1086435317993164, loss=1.2658063173294067
I0213 10:30:49.430508 140285397882624 logging_writer.py:48] [33400] global_step=33400, grad_norm=2.26216459274292, loss=1.2429835796356201
I0213 10:32:20.652074 140285389489920 logging_writer.py:48] [33500] global_step=33500, grad_norm=3.072845220565796, loss=1.2437520027160645
I0213 10:33:52.466507 140285397882624 logging_writer.py:48] [33600] global_step=33600, grad_norm=3.656087875366211, loss=1.3085914850234985
I0213 10:35:23.472669 140285389489920 logging_writer.py:48] [33700] global_step=33700, grad_norm=2.84197735786438, loss=1.2921351194381714
I0213 10:36:54.419554 140285397882624 logging_writer.py:48] [33800] global_step=33800, grad_norm=3.2763872146606445, loss=1.298232078552246
I0213 10:38:21.568945 140285389489920 logging_writer.py:48] [33900] global_step=33900, grad_norm=2.0743906497955322, loss=1.2379103899002075
I0213 10:39:52.651615 140285397882624 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.8926464319229126, loss=1.2189645767211914
I0213 10:41:11.791730 140285389489920 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.9445170164108276, loss=1.2962472438812256
I0213 10:42:29.792276 140285397882624 logging_writer.py:48] [34200] global_step=34200, grad_norm=2.495762348175049, loss=1.2567963600158691
I0213 10:43:51.158274 140285389489920 logging_writer.py:48] [34300] global_step=34300, grad_norm=2.739300489425659, loss=1.249206304550171
I0213 10:44:13.875204 140441227016000 spec.py:321] Evaluating on the training split.
I0213 10:45:09.045476 140441227016000 spec.py:333] Evaluating on the validation split.
I0213 10:46:01.104737 140441227016000 spec.py:349] Evaluating on the test split.
I0213 10:46:28.931502 140441227016000 submission_runner.py:408] Time since start: 31709.35s, 	Step: 34328, 	{'train/ctc_loss': Array(0.30576283, dtype=float32), 'train/wer': 0.10333758409920409, 'validation/ctc_loss': Array(0.4679304, dtype=float32), 'validation/wer': 0.13436380663660852, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2541731, dtype=float32), 'test/wer': 0.0815916153799281, 'test/num_examples': 2472, 'score': 28825.82073378563, 'total_duration': 31709.3510825634, 'accumulated_submission_time': 28825.82073378563, 'accumulated_eval_time': 2880.9082431793213, 'accumulated_logging_time': 1.0413026809692383}
I0213 10:46:28.970338 140285397882624 logging_writer.py:48] [34328] accumulated_eval_time=2880.908243, accumulated_logging_time=1.041303, accumulated_submission_time=28825.820734, global_step=34328, preemption_count=0, score=28825.820734, test/ctc_loss=0.2541730999946594, test/num_examples=2472, test/wer=0.081592, total_duration=31709.351083, train/ctc_loss=0.3057628273963928, train/wer=0.103338, validation/ctc_loss=0.467930406332016, validation/num_examples=5348, validation/wer=0.134364
I0213 10:47:23.656651 140285389489920 logging_writer.py:48] [34400] global_step=34400, grad_norm=2.307312250137329, loss=1.2176848649978638
I0213 10:48:38.541100 140285397882624 logging_writer.py:48] [34500] global_step=34500, grad_norm=3.2668676376342773, loss=1.2318708896636963
I0213 10:50:03.094781 140285389489920 logging_writer.py:48] [34600] global_step=34600, grad_norm=2.938288450241089, loss=1.3587934970855713
I0213 10:51:30.047310 140285397882624 logging_writer.py:48] [34700] global_step=34700, grad_norm=2.826805353164673, loss=1.2416940927505493
I0213 10:52:56.784608 140285389489920 logging_writer.py:48] [34800] global_step=34800, grad_norm=2.0569634437561035, loss=1.221211552619934
I0213 10:54:26.249340 140285397882624 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.724037766456604, loss=1.2663137912750244
I0213 10:55:54.071313 140285389489920 logging_writer.py:48] [35000] global_step=35000, grad_norm=2.346651792526245, loss=1.3149445056915283
I0213 10:57:16.616156 140285397882624 logging_writer.py:48] [35100] global_step=35100, grad_norm=2.2750985622406006, loss=1.2347608804702759
I0213 10:58:36.350161 140285389489920 logging_writer.py:48] [35200] global_step=35200, grad_norm=2.093378782272339, loss=1.2486509084701538
I0213 10:59:55.032036 140285397882624 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.777921438217163, loss=1.2222448587417603
I0213 11:01:13.880897 140285389489920 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.7731366157531738, loss=1.321599006652832
I0213 11:02:39.323026 140285397882624 logging_writer.py:48] [35500] global_step=35500, grad_norm=2.8425328731536865, loss=1.2728466987609863
I0213 11:04:10.686342 140285389489920 logging_writer.py:48] [35600] global_step=35600, grad_norm=2.2569739818573, loss=1.2327468395233154
I0213 11:05:39.392357 140285397882624 logging_writer.py:48] [35700] global_step=35700, grad_norm=4.0052490234375, loss=1.237305760383606
I0213 11:07:10.582711 140285389489920 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.664462685585022, loss=1.2233506441116333
I0213 11:08:39.720227 140285397882624 logging_writer.py:48] [35900] global_step=35900, grad_norm=2.504608154296875, loss=1.3034896850585938
I0213 11:10:10.616621 140285389489920 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.3684922456741333, loss=1.175600290298462
I0213 11:10:29.500758 140441227016000 spec.py:321] Evaluating on the training split.
I0213 11:11:23.095664 140441227016000 spec.py:333] Evaluating on the validation split.
I0213 11:12:16.215675 140441227016000 spec.py:349] Evaluating on the test split.
I0213 11:12:43.380851 140441227016000 submission_runner.py:408] Time since start: 33283.80s, 	Step: 36023, 	{'train/ctc_loss': Array(0.24863774, dtype=float32), 'train/wer': 0.08587385761028997, 'validation/ctc_loss': Array(0.45736533, dtype=float32), 'validation/wer': 0.13183428753487744, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24383034, dtype=float32), 'test/wer': 0.07836207421851198, 'test/num_examples': 2472, 'score': 30266.262042284012, 'total_duration': 33283.799632787704, 'accumulated_submission_time': 30266.262042284012, 'accumulated_eval_time': 3014.7819306850433, 'accumulated_logging_time': 1.0968976020812988}
I0213 11:12:43.418396 140285397882624 logging_writer.py:48] [36023] accumulated_eval_time=3014.781931, accumulated_logging_time=1.096898, accumulated_submission_time=30266.262042, global_step=36023, preemption_count=0, score=30266.262042, test/ctc_loss=0.2438303381204605, test/num_examples=2472, test/wer=0.078362, total_duration=33283.799633, train/ctc_loss=0.24863773584365845, train/wer=0.085874, validation/ctc_loss=0.45736533403396606, validation/num_examples=5348, validation/wer=0.131834
I0213 11:13:47.725291 140285397882624 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.6684099435806274, loss=1.2614558935165405
I0213 11:15:03.403572 140285389489920 logging_writer.py:48] [36200] global_step=36200, grad_norm=3.678063154220581, loss=1.195578694343567
I0213 11:16:21.470154 140285397882624 logging_writer.py:48] [36300] global_step=36300, grad_norm=2.5860047340393066, loss=1.2232913970947266
I0213 11:17:45.358894 140285389489920 logging_writer.py:48] [36400] global_step=36400, grad_norm=2.5500407218933105, loss=1.2530943155288696
I0213 11:19:09.626897 140285397882624 logging_writer.py:48] [36500] global_step=36500, grad_norm=2.12099027633667, loss=1.2689473628997803
I0213 11:20:39.250872 140285389489920 logging_writer.py:48] [36600] global_step=36600, grad_norm=2.2418394088745117, loss=1.2004239559173584
I0213 11:22:11.293076 140285397882624 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.9009544849395752, loss=1.1863576173782349
I0213 11:23:38.179208 140285389489920 logging_writer.py:48] [36800] global_step=36800, grad_norm=8.007782936096191, loss=1.1884316205978394
I0213 11:25:06.725437 140285397882624 logging_writer.py:48] [36900] global_step=36900, grad_norm=3.268866777420044, loss=1.2376660108566284
I0213 11:26:34.818778 140285389489920 logging_writer.py:48] [37000] global_step=37000, grad_norm=4.387222766876221, loss=1.235863208770752
I0213 11:28:06.672608 140285397882624 logging_writer.py:48] [37100] global_step=37100, grad_norm=2.072920322418213, loss=1.2704333066940308
I0213 11:29:23.073751 140285389489920 logging_writer.py:48] [37200] global_step=37200, grad_norm=2.5048651695251465, loss=1.233941674232483
I0213 11:30:41.682392 140285397882624 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.6210993528366089, loss=1.1568026542663574
I0213 11:32:05.336506 140285389489920 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.8509860038757324, loss=1.2331284284591675
I0213 11:33:31.857246 140285397882624 logging_writer.py:48] [37500] global_step=37500, grad_norm=4.552544116973877, loss=1.2007249593734741
I0213 11:34:57.919870 140285389489920 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.5236790180206299, loss=1.2175021171569824
I0213 11:36:28.455975 140285397882624 logging_writer.py:48] [37700] global_step=37700, grad_norm=2.333251953125, loss=1.2004685401916504
I0213 11:36:43.952414 140441227016000 spec.py:321] Evaluating on the training split.
I0213 11:37:37.347026 140441227016000 spec.py:333] Evaluating on the validation split.
I0213 11:38:31.154689 140441227016000 spec.py:349] Evaluating on the test split.
I0213 11:38:58.434603 140441227016000 submission_runner.py:408] Time since start: 34858.85s, 	Step: 37719, 	{'train/ctc_loss': Array(0.22751224, dtype=float32), 'train/wer': 0.07755462420951846, 'validation/ctc_loss': Array(0.44469142, dtype=float32), 'validation/wer': 0.12733521920889773, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24005848, dtype=float32), 'test/wer': 0.07653403205167265, 'test/num_examples': 2472, 'score': 31706.707235336304, 'total_duration': 34858.85399699211, 'accumulated_submission_time': 31706.707235336304, 'accumulated_eval_time': 3149.2583301067352, 'accumulated_logging_time': 1.1505632400512695}
I0213 11:38:58.473494 140285397882624 logging_writer.py:48] [37719] accumulated_eval_time=3149.258330, accumulated_logging_time=1.150563, accumulated_submission_time=31706.707235, global_step=37719, preemption_count=0, score=31706.707235, test/ctc_loss=0.24005848169326782, test/num_examples=2472, test/wer=0.076534, total_duration=34858.853997, train/ctc_loss=0.22751224040985107, train/wer=0.077555, validation/ctc_loss=0.44469141960144043, validation/num_examples=5348, validation/wer=0.127335
I0213 11:39:59.857187 140285389489920 logging_writer.py:48] [37800] global_step=37800, grad_norm=2.144249439239502, loss=1.2531867027282715
I0213 11:41:15.134673 140285397882624 logging_writer.py:48] [37900] global_step=37900, grad_norm=2.7311787605285645, loss=1.2537071704864502
I0213 11:42:36.720603 140285389489920 logging_writer.py:48] [38000] global_step=38000, grad_norm=2.1913771629333496, loss=1.2285325527191162
I0213 11:44:05.275217 140285397882624 logging_writer.py:48] [38100] global_step=38100, grad_norm=2.90464186668396, loss=1.217013955116272
I0213 11:45:25.773830 140285397882624 logging_writer.py:48] [38200] global_step=38200, grad_norm=2.026153326034546, loss=1.1995391845703125
I0213 11:46:44.030476 140285389489920 logging_writer.py:48] [38300] global_step=38300, grad_norm=4.595622539520264, loss=1.2367857694625854
I0213 11:48:05.356501 140285397882624 logging_writer.py:48] [38400] global_step=38400, grad_norm=2.40055513381958, loss=1.2218022346496582
I0213 11:49:31.656131 140285389489920 logging_writer.py:48] [38500] global_step=38500, grad_norm=2.352837324142456, loss=1.258001685142517
I0213 11:51:02.086977 140285397882624 logging_writer.py:48] [38600] global_step=38600, grad_norm=2.655449390411377, loss=1.1650080680847168
I0213 11:52:27.672309 140285389489920 logging_writer.py:48] [38700] global_step=38700, grad_norm=2.8036069869995117, loss=1.206274390220642
I0213 11:53:59.290173 140285397882624 logging_writer.py:48] [38800] global_step=38800, grad_norm=2.0250723361968994, loss=1.2137088775634766
I0213 11:55:30.567667 140285389489920 logging_writer.py:48] [38900] global_step=38900, grad_norm=2.0062038898468018, loss=1.1926127672195435
I0213 11:56:59.716951 140285397882624 logging_writer.py:48] [39000] global_step=39000, grad_norm=2.369269371032715, loss=1.215233325958252
I0213 11:58:27.537918 140285389489920 logging_writer.py:48] [39100] global_step=39100, grad_norm=2.389249086380005, loss=1.2219963073730469
I0213 11:59:52.044507 140285397882624 logging_writer.py:48] [39200] global_step=39200, grad_norm=2.846703052520752, loss=1.2478969097137451
I0213 12:01:11.508499 140285389489920 logging_writer.py:48] [39300] global_step=39300, grad_norm=3.333303451538086, loss=1.1528847217559814
I0213 12:02:30.850875 140285397882624 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.9309202432632446, loss=1.1589951515197754
I0213 12:02:58.885689 140441227016000 spec.py:321] Evaluating on the training split.
I0213 12:03:52.829482 140441227016000 spec.py:333] Evaluating on the validation split.
I0213 12:04:46.062328 140441227016000 spec.py:349] Evaluating on the test split.
I0213 12:05:13.847529 140441227016000 submission_runner.py:408] Time since start: 36434.27s, 	Step: 39437, 	{'train/ctc_loss': Array(0.2554167, dtype=float32), 'train/wer': 0.0892404637499159, 'validation/ctc_loss': Array(0.43711883, dtype=float32), 'validation/wer': 0.12433262210722458, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23413983, dtype=float32), 'test/wer': 0.07456380882741251, 'test/num_examples': 2472, 'score': 33147.02962565422, 'total_duration': 36434.26735687256, 'accumulated_submission_time': 33147.02962565422, 'accumulated_eval_time': 3284.214804172516, 'accumulated_logging_time': 1.2067761421203613}
I0213 12:05:13.894942 140285397882624 logging_writer.py:48] [39437] accumulated_eval_time=3284.214804, accumulated_logging_time=1.206776, accumulated_submission_time=33147.029626, global_step=39437, preemption_count=0, score=33147.029626, test/ctc_loss=0.2341398298740387, test/num_examples=2472, test/wer=0.074564, total_duration=36434.267357, train/ctc_loss=0.2554166913032532, train/wer=0.089240, validation/ctc_loss=0.4371188282966614, validation/num_examples=5348, validation/wer=0.124333
I0213 12:06:02.843062 140285389489920 logging_writer.py:48] [39500] global_step=39500, grad_norm=2.531284809112549, loss=1.2118425369262695
I0213 12:07:17.788362 140285397882624 logging_writer.py:48] [39600] global_step=39600, grad_norm=3.795217275619507, loss=1.1936289072036743
I0213 12:08:37.465106 140285389489920 logging_writer.py:48] [39700] global_step=39700, grad_norm=3.5871713161468506, loss=1.2124303579330444
I0213 12:10:07.187823 140285397882624 logging_writer.py:48] [39800] global_step=39800, grad_norm=3.2722184658050537, loss=1.1543583869934082
I0213 12:11:35.993919 140285389489920 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.957100749015808, loss=1.2041438817977905
I0213 12:13:02.202000 140285397882624 logging_writer.py:48] [40000] global_step=40000, grad_norm=2.2901411056518555, loss=1.1914939880371094
I0213 12:14:30.769584 140285389489920 logging_writer.py:48] [40100] global_step=40100, grad_norm=2.134536027908325, loss=1.2021280527114868
I0213 12:15:59.687723 140285397882624 logging_writer.py:48] [40200] global_step=40200, grad_norm=4.6667046546936035, loss=1.1506741046905518
I0213 12:17:15.202482 140285389489920 logging_writer.py:48] [40300] global_step=40300, grad_norm=2.161567449569702, loss=1.1555728912353516
I0213 12:18:32.840325 140285397882624 logging_writer.py:48] [40400] global_step=40400, grad_norm=2.0174875259399414, loss=1.119246006011963
I0213 12:19:54.371071 140285389489920 logging_writer.py:48] [40500] global_step=40500, grad_norm=3.072213649749756, loss=1.2114428281784058
I0213 12:21:18.209753 140285397882624 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.399848222732544, loss=1.1450883150100708
I0213 12:22:49.862150 140285389489920 logging_writer.py:48] [40700] global_step=40700, grad_norm=3.0106098651885986, loss=1.1977664232254028
I0213 12:24:19.555491 140285397882624 logging_writer.py:48] [40800] global_step=40800, grad_norm=2.095176935195923, loss=1.1714375019073486
I0213 12:25:48.734006 140285389489920 logging_writer.py:48] [40900] global_step=40900, grad_norm=3.6061019897460938, loss=1.2064334154129028
I0213 12:27:16.619133 140285397882624 logging_writer.py:48] [41000] global_step=41000, grad_norm=2.7977938652038574, loss=1.1743714809417725
I0213 12:28:45.087782 140285389489920 logging_writer.py:48] [41100] global_step=41100, grad_norm=2.3555920124053955, loss=1.1943951845169067
I0213 12:29:13.934154 140441227016000 spec.py:321] Evaluating on the training split.
I0213 12:30:07.946362 140441227016000 spec.py:333] Evaluating on the validation split.
I0213 12:31:00.210911 140441227016000 spec.py:349] Evaluating on the test split.
I0213 12:31:27.387075 140441227016000 submission_runner.py:408] Time since start: 38007.81s, 	Step: 41134, 	{'train/ctc_loss': Array(0.2320539, dtype=float32), 'train/wer': 0.08016946666516231, 'validation/ctc_loss': Array(0.42751628, dtype=float32), 'validation/wer': 0.12143622618921189, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23012903, dtype=float32), 'test/wer': 0.07413726565515, 'test/num_examples': 2472, 'score': 34586.98317909241, 'total_duration': 38007.80613279343, 'accumulated_submission_time': 34586.98317909241, 'accumulated_eval_time': 3417.66161775589, 'accumulated_logging_time': 1.2691385746002197}
I0213 12:31:27.424700 140285397882624 logging_writer.py:48] [41134] accumulated_eval_time=3417.661618, accumulated_logging_time=1.269139, accumulated_submission_time=34586.983179, global_step=41134, preemption_count=0, score=34586.983179, test/ctc_loss=0.23012903332710266, test/num_examples=2472, test/wer=0.074137, total_duration=38007.806133, train/ctc_loss=0.23205390572547913, train/wer=0.080169, validation/ctc_loss=0.42751628160476685, validation/num_examples=5348, validation/wer=0.121436
I0213 12:32:22.166427 140285397882624 logging_writer.py:48] [41200] global_step=41200, grad_norm=3.772258996963501, loss=1.1472870111465454
I0213 12:33:41.004138 140285389489920 logging_writer.py:48] [41300] global_step=41300, grad_norm=5.728643417358398, loss=1.1354871988296509
I0213 12:34:56.732500 140285397882624 logging_writer.py:48] [41400] global_step=41400, grad_norm=3.282773971557617, loss=1.1808135509490967
I0213 12:36:20.510104 140285389489920 logging_writer.py:48] [41500] global_step=41500, grad_norm=2.154365301132202, loss=1.1355417966842651
I0213 12:37:48.290241 140285397882624 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.9087523221969604, loss=1.1646902561187744
I0213 12:39:17.517423 140285389489920 logging_writer.py:48] [41700] global_step=41700, grad_norm=2.298043966293335, loss=1.1751798391342163
I0213 12:40:44.372485 140285397882624 logging_writer.py:48] [41800] global_step=41800, grad_norm=2.0589723587036133, loss=1.187583088874817
I0213 12:42:12.649180 140285389489920 logging_writer.py:48] [41900] global_step=41900, grad_norm=2.7918782234191895, loss=1.1866941452026367
I0213 12:43:40.270077 140285397882624 logging_writer.py:48] [42000] global_step=42000, grad_norm=2.0121185779571533, loss=1.1386626958847046
I0213 12:45:11.444289 140285389489920 logging_writer.py:48] [42100] global_step=42100, grad_norm=2.9019312858581543, loss=1.1702170372009277
I0213 12:46:42.330166 140285397882624 logging_writer.py:48] [42200] global_step=42200, grad_norm=3.6805875301361084, loss=1.1977035999298096
I0213 12:48:05.199197 140285397882624 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.7718693017959595, loss=1.2049847841262817
I0213 12:49:21.811625 140285389489920 logging_writer.py:48] [42400] global_step=42400, grad_norm=2.7236499786376953, loss=1.1754213571548462
I0213 12:50:39.990759 140285397882624 logging_writer.py:48] [42500] global_step=42500, grad_norm=2.034205675125122, loss=1.150968074798584
I0213 12:52:03.837155 140285389489920 logging_writer.py:48] [42600] global_step=42600, grad_norm=2.6830484867095947, loss=1.1519161462783813
I0213 12:53:29.003266 140285397882624 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.5348515510559082, loss=1.1731557846069336
I0213 12:54:59.494624 140285389489920 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.6375452280044556, loss=1.1859809160232544
I0213 12:55:27.543313 140441227016000 spec.py:321] Evaluating on the training split.
I0213 12:56:22.419865 140441227016000 spec.py:333] Evaluating on the validation split.
I0213 12:57:14.568957 140441227016000 spec.py:349] Evaluating on the test split.
I0213 12:57:41.747581 140441227016000 submission_runner.py:408] Time since start: 39582.17s, 	Step: 42832, 	{'train/ctc_loss': Array(0.24211791, dtype=float32), 'train/wer': 0.08087093464149121, 'validation/ctc_loss': Array(0.42340276, dtype=float32), 'validation/wer': 0.12034525039342711, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.225074, dtype=float32), 'test/wer': 0.07196392663457438, 'test/num_examples': 2472, 'score': 36027.01313138008, 'total_duration': 39582.16630578041, 'accumulated_submission_time': 36027.01313138008, 'accumulated_eval_time': 3551.85942363739, 'accumulated_logging_time': 1.322899580001831}
I0213 12:57:41.785688 140285397882624 logging_writer.py:48] [42832] accumulated_eval_time=3551.859424, accumulated_logging_time=1.322900, accumulated_submission_time=36027.013131, global_step=42832, preemption_count=0, score=36027.013131, test/ctc_loss=0.22507399320602417, test/num_examples=2472, test/wer=0.071964, total_duration=39582.166306, train/ctc_loss=0.24211791157722473, train/wer=0.080871, validation/ctc_loss=0.4234027564525604, validation/num_examples=5348, validation/wer=0.120345
I0213 12:58:33.551195 140285389489920 logging_writer.py:48] [42900] global_step=42900, grad_norm=2.9673237800598145, loss=1.19199800491333
I0213 12:59:49.009702 140285397882624 logging_writer.py:48] [43000] global_step=43000, grad_norm=2.616060495376587, loss=1.1188197135925293
I0213 13:01:09.243980 140285389489920 logging_writer.py:48] [43100] global_step=43100, grad_norm=2.5713746547698975, loss=1.1267528533935547
I0213 13:02:36.585896 140285397882624 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.9885579347610474, loss=1.1646702289581299
I0213 13:04:02.948506 140285397882624 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.7891162633895874, loss=1.1461515426635742
I0213 13:05:18.882456 140285389489920 logging_writer.py:48] [43400] global_step=43400, grad_norm=2.6870620250701904, loss=1.205649495124817
I0213 13:06:36.736320 140285397882624 logging_writer.py:48] [43500] global_step=43500, grad_norm=2.3717663288116455, loss=1.1602383852005005
I0213 13:07:56.729025 140285389489920 logging_writer.py:48] [43600] global_step=43600, grad_norm=2.122847557067871, loss=1.1897399425506592
I0213 13:09:21.585174 140285397882624 logging_writer.py:48] [43700] global_step=43700, grad_norm=2.111567735671997, loss=1.1741390228271484
I0213 13:10:51.186411 140285389489920 logging_writer.py:48] [43800] global_step=43800, grad_norm=2.465228319168091, loss=1.1425014734268188
I0213 13:12:20.883104 140285397882624 logging_writer.py:48] [43900] global_step=43900, grad_norm=4.538317680358887, loss=1.1215016841888428
I0213 13:13:49.145098 140285389489920 logging_writer.py:48] [44000] global_step=44000, grad_norm=5.0948286056518555, loss=1.1990302801132202
I0213 13:15:20.195996 140285397882624 logging_writer.py:48] [44100] global_step=44100, grad_norm=2.2755441665649414, loss=1.1789886951446533
I0213 13:16:51.446537 140285389489920 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.327174425125122, loss=1.0896486043930054
I0213 13:18:24.696708 140285397882624 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.9794769287109375, loss=1.181400179862976
I0213 13:19:43.334406 140285389489920 logging_writer.py:48] [44400] global_step=44400, grad_norm=2.5901870727539062, loss=1.1249415874481201
I0213 13:21:02.810282 140285397882624 logging_writer.py:48] [44500] global_step=44500, grad_norm=2.4807684421539307, loss=1.1269919872283936
I0213 13:21:42.252504 140441227016000 spec.py:321] Evaluating on the training split.
I0213 13:22:37.189745 140441227016000 spec.py:333] Evaluating on the validation split.
I0213 13:23:32.068070 140441227016000 spec.py:349] Evaluating on the test split.
I0213 13:23:59.404526 140441227016000 submission_runner.py:408] Time since start: 41159.82s, 	Step: 44550, 	{'train/ctc_loss': Array(0.2000132, dtype=float32), 'train/wer': 0.06981170255237928, 'validation/ctc_loss': Array(0.4208656, dtype=float32), 'validation/wer': 0.11988182704654508, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22411768, dtype=float32), 'test/wer': 0.07169987609936425, 'test/num_examples': 2472, 'score': 37467.38977432251, 'total_duration': 41159.824618816376, 'accumulated_submission_time': 37467.38977432251, 'accumulated_eval_time': 3689.00634765625, 'accumulated_logging_time': 1.3793542385101318}
I0213 13:23:59.442094 140285397882624 logging_writer.py:48] [44550] accumulated_eval_time=3689.006348, accumulated_logging_time=1.379354, accumulated_submission_time=37467.389774, global_step=44550, preemption_count=0, score=37467.389774, test/ctc_loss=0.2241176813840866, test/num_examples=2472, test/wer=0.071700, total_duration=41159.824619, train/ctc_loss=0.20001320540905, train/wer=0.069812, validation/ctc_loss=0.42086559534072876, validation/num_examples=5348, validation/wer=0.119882
I0213 13:24:37.640740 140285389489920 logging_writer.py:48] [44600] global_step=44600, grad_norm=2.7864913940429688, loss=1.1298081874847412
I0213 13:25:53.067241 140285397882624 logging_writer.py:48] [44700] global_step=44700, grad_norm=2.4347400665283203, loss=1.1607352495193481
I0213 13:27:14.493391 140285389489920 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.622599720954895, loss=1.1220148801803589
I0213 13:28:45.791009 140285397882624 logging_writer.py:48] [44900] global_step=44900, grad_norm=2.506932258605957, loss=1.1737405061721802
I0213 13:30:17.185966 140285389489920 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.9186090230941772, loss=1.1254708766937256
I0213 13:31:46.943462 140285397882624 logging_writer.py:48] [45100] global_step=45100, grad_norm=2.8321566581726074, loss=1.1626765727996826
I0213 13:33:14.985738 140285389489920 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.7065238952636719, loss=1.1394788026809692
I0213 13:34:42.662839 140285397882624 logging_writer.py:48] [45300] global_step=45300, grad_norm=2.8153183460235596, loss=1.0868064165115356
I0213 13:36:05.136327 140285397882624 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.8422242403030396, loss=1.0666110515594482
I0213 13:37:23.553144 140285389489920 logging_writer.py:48] [45500] global_step=45500, grad_norm=4.206991195678711, loss=1.1697434186935425
I0213 13:38:44.457368 140285397882624 logging_writer.py:48] [45600] global_step=45600, grad_norm=4.403617858886719, loss=1.1417896747589111
I0213 13:40:07.038878 140285389489920 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.9714475870132446, loss=1.1890829801559448
I0213 13:41:34.263021 140285397882624 logging_writer.py:48] [45800] global_step=45800, grad_norm=2.0244951248168945, loss=1.2286574840545654
I0213 13:43:04.292299 140285389489920 logging_writer.py:48] [45900] global_step=45900, grad_norm=6.041098594665527, loss=1.1141959428787231
I0213 13:44:33.151255 140285397882624 logging_writer.py:48] [46000] global_step=46000, grad_norm=3.4429585933685303, loss=1.187117576599121
I0213 13:46:02.617818 140285389489920 logging_writer.py:48] [46100] global_step=46100, grad_norm=3.335780143737793, loss=1.1619505882263184
I0213 13:47:33.182577 140285397882624 logging_writer.py:48] [46200] global_step=46200, grad_norm=3.728532075881958, loss=1.1866401433944702
I0213 13:47:59.928696 140441227016000 spec.py:321] Evaluating on the training split.
I0213 13:48:54.043335 140441227016000 spec.py:333] Evaluating on the validation split.
I0213 13:49:47.702179 140441227016000 spec.py:349] Evaluating on the test split.
I0213 13:50:15.506515 140441227016000 submission_runner.py:408] Time since start: 42735.92s, 	Step: 46233, 	{'train/ctc_loss': Array(0.20435266, dtype=float32), 'train/wer': 0.07182548768841555, 'validation/ctc_loss': Array(0.41903222, dtype=float32), 'validation/wer': 0.11900325361808123, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22205608, dtype=float32), 'test/wer': 0.07042024658257673, 'test/num_examples': 2472, 'score': 38907.78763270378, 'total_duration': 42735.924476861954, 'accumulated_submission_time': 38907.78763270378, 'accumulated_eval_time': 3824.5769357681274, 'accumulated_logging_time': 1.4339289665222168}
I0213 13:50:15.550385 140285397882624 logging_writer.py:48] [46233] accumulated_eval_time=3824.576936, accumulated_logging_time=1.433929, accumulated_submission_time=38907.787633, global_step=46233, preemption_count=0, score=38907.787633, test/ctc_loss=0.2220560759305954, test/num_examples=2472, test/wer=0.070420, total_duration=42735.924477, train/ctc_loss=0.20435266196727753, train/wer=0.071825, validation/ctc_loss=0.4190322160720825, validation/num_examples=5348, validation/wer=0.119003
I0213 13:51:06.842280 140285389489920 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.7601195573806763, loss=1.1695339679718018
I0213 13:52:26.557257 140285397882624 logging_writer.py:48] [46400] global_step=46400, grad_norm=2.051607131958008, loss=1.1653374433517456
I0213 13:53:42.303890 140285389489920 logging_writer.py:48] [46500] global_step=46500, grad_norm=4.234860897064209, loss=1.121147871017456
I0213 13:54:57.645277 140285397882624 logging_writer.py:48] [46600] global_step=46600, grad_norm=3.1469199657440186, loss=1.1985924243927002
I0213 13:56:18.796569 140285389489920 logging_writer.py:48] [46700] global_step=46700, grad_norm=2.6189115047454834, loss=1.1481397151947021
I0213 13:57:44.074153 140285397882624 logging_writer.py:48] [46800] global_step=46800, grad_norm=2.0113141536712646, loss=1.165182113647461
I0213 13:59:14.272780 140285389489920 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.5827571153640747, loss=1.1261779069900513
I0213 14:00:42.329358 140285397882624 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.6572095155715942, loss=1.1731960773468018
I0213 14:02:12.958035 140285389489920 logging_writer.py:48] [47100] global_step=47100, grad_norm=2.1569926738739014, loss=1.1591283082962036
I0213 14:03:42.364101 140285397882624 logging_writer.py:48] [47200] global_step=47200, grad_norm=4.077079772949219, loss=1.1362167596817017
I0213 14:05:12.484234 140285389489920 logging_writer.py:48] [47300] global_step=47300, grad_norm=2.1000213623046875, loss=1.1115694046020508
I0213 14:06:40.993004 140285397882624 logging_writer.py:48] [47400] global_step=47400, grad_norm=3.6087467670440674, loss=1.156556248664856
I0213 14:07:57.174961 140285389489920 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.9806668758392334, loss=1.1776344776153564
I0213 14:09:13.177805 140285397882624 logging_writer.py:48] [47600] global_step=47600, grad_norm=4.979935169219971, loss=1.092624306678772
I0213 14:10:33.393992 140285389489920 logging_writer.py:48] [47700] global_step=47700, grad_norm=3.7453155517578125, loss=1.1808792352676392
I0213 14:11:56.484491 140285397882624 logging_writer.py:48] [47800] global_step=47800, grad_norm=2.319361448287964, loss=1.1351649761199951
I0213 14:13:21.965600 140285389489920 logging_writer.py:48] [47900] global_step=47900, grad_norm=3.758315324783325, loss=1.103145718574524
I0213 14:14:15.887960 140441227016000 spec.py:321] Evaluating on the training split.
I0213 14:15:09.178085 140441227016000 spec.py:333] Evaluating on the validation split.
I0213 14:16:03.987004 140441227016000 spec.py:349] Evaluating on the test split.
I0213 14:16:31.718301 140441227016000 submission_runner.py:408] Time since start: 44312.14s, 	Step: 47961, 	{'train/ctc_loss': Array(0.22109444, dtype=float32), 'train/wer': 0.0784157378244303, 'validation/ctc_loss': Array(0.41872, dtype=float32), 'validation/wer': 0.11862672214873958, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22187033, dtype=float32), 'test/wer': 0.0707452318566815, 'test/num_examples': 2472, 'score': 40348.03506684303, 'total_duration': 44312.138027668, 'accumulated_submission_time': 40348.03506684303, 'accumulated_eval_time': 3960.401820898056, 'accumulated_logging_time': 1.495056390762329}
I0213 14:16:31.761845 140285397882624 logging_writer.py:48] [47961] accumulated_eval_time=3960.401821, accumulated_logging_time=1.495056, accumulated_submission_time=40348.035067, global_step=47961, preemption_count=0, score=40348.035067, test/ctc_loss=0.2218703329563141, test/num_examples=2472, test/wer=0.070745, total_duration=44312.138028, train/ctc_loss=0.22109444439411163, train/wer=0.078416, validation/ctc_loss=0.418720006942749, validation/num_examples=5348, validation/wer=0.118627
I0213 14:17:01.077082 140441227016000 spec.py:321] Evaluating on the training split.
I0213 14:18:03.454843 140441227016000 spec.py:333] Evaluating on the validation split.
I0213 14:18:50.238268 140441227016000 spec.py:349] Evaluating on the test split.
I0213 14:19:14.053693 140441227016000 submission_runner.py:408] Time since start: 44474.48s, 	Step: 48000, 	{'train/ctc_loss': Array(0.14898208, dtype=float32), 'train/wer': 0.052723391806529206, 'validation/ctc_loss': Array(0.4187202, dtype=float32), 'validation/wer': 0.11863637680179963, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2218685, dtype=float32), 'test/wer': 0.0707452318566815, 'test/num_examples': 2472, 'score': 40377.33175325394, 'total_duration': 44474.47600841522, 'accumulated_submission_time': 40377.33175325394, 'accumulated_eval_time': 4093.375575065613, 'accumulated_logging_time': 1.5548663139343262}
I0213 14:19:14.083194 140285397882624 logging_writer.py:48] [48000] accumulated_eval_time=4093.375575, accumulated_logging_time=1.554866, accumulated_submission_time=40377.331753, global_step=48000, preemption_count=0, score=40377.331753, test/ctc_loss=0.22186850011348724, test/num_examples=2472, test/wer=0.070745, total_duration=44474.476008, train/ctc_loss=0.14898207783699036, train/wer=0.052723, validation/ctc_loss=0.41872018575668335, validation/num_examples=5348, validation/wer=0.118636
I0213 14:19:14.105656 140285389489920 logging_writer.py:48] [48000] global_step=48000, preemption_count=0, score=40377.331753
I0213 14:19:14.305630 140441227016000 checkpoints.py:490] Saving checkpoint at step: 48000
I0213 14:19:15.322173 140441227016000 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_2/librispeech_deepspeech_jax/trial_5/checkpoint_48000
I0213 14:19:15.341595 140441227016000 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_2/librispeech_deepspeech_jax/trial_5/checkpoint_48000.
I0213 14:19:16.803442 140441227016000 submission_runner.py:583] Tuning trial 5/5
I0213 14:19:16.803713 140441227016000 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0213 14:19:16.816820 140441227016000 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.628937, dtype=float32), 'train/wer': 3.1009376811671285, 'validation/ctc_loss': Array(30.570345, dtype=float32), 'validation/wer': 2.911437867480232, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.657665, dtype=float32), 'test/wer': 3.2318769930737514, 'test/num_examples': 2472, 'score': 18.649582386016846, 'total_duration': 198.61209559440613, 'accumulated_submission_time': 18.649582386016846, 'accumulated_eval_time': 179.9624104499817, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1712, {'train/ctc_loss': Array(2.5453644, dtype=float32), 'train/wer': 0.5917621452917745, 'validation/ctc_loss': Array(3.0406559, dtype=float32), 'validation/wer': 0.6442163800843816, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.5435536, dtype=float32), 'test/wer': 0.5706741413279711, 'test/num_examples': 2472, 'score': 1458.985486984253, 'total_duration': 1767.9684970378876, 'accumulated_submission_time': 1458.985486984253, 'accumulated_eval_time': 308.87907791137695, 'accumulated_logging_time': 0.029255151748657227, 'global_step': 1712, 'preemption_count': 0}), (3454, {'train/ctc_loss': Array(0.6002691, dtype=float32), 'train/wer': 0.19352086887769965, 'validation/ctc_loss': Array(1.0050833, dtype=float32), 'validation/wer': 0.2805642179248289, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.66275734, dtype=float32), 'test/wer': 0.20537038165458127, 'test/num_examples': 2472, 'score': 2899.323889017105, 'total_duration': 3345.4317281246185, 'accumulated_submission_time': 2899.323889017105, 'accumulated_eval_time': 445.87190437316895, 'accumulated_logging_time': 0.07917284965515137, 'global_step': 3454, 'preemption_count': 0}), (5151, {'train/ctc_loss': Array(0.6795138, dtype=float32), 'train/wer': 0.21180016942253319, 'validation/ctc_loss': Array(0.8538837, dtype=float32), 'validation/wer': 0.2422835185417612, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.542488, dtype=float32), 'test/wer': 0.16789551723437532, 'test/num_examples': 2472, 'score': 4340.835663795471, 'total_duration': 4922.1106123924255, 'accumulated_submission_time': 4340.835663795471, 'accumulated_eval_time': 580.9096684455872, 'accumulated_logging_time': 0.1311507225036621, 'global_step': 5151, 'preemption_count': 0}), (6877, {'train/ctc_loss': Array(0.62634224, dtype=float32), 'train/wer': 0.19708784345232527, 'validation/ctc_loss': Array(0.78297794, dtype=float32), 'validation/wer': 0.22196047385037218, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.48380572, dtype=float32), 'test/wer': 0.15446956309792212, 'test/num_examples': 2472, 'score': 5780.938099622726, 'total_duration': 6500.155675172806, 'accumulated_submission_time': 5780.938099622726, 'accumulated_eval_time': 718.7179839611053, 'accumulated_logging_time': 0.18588995933532715, 'global_step': 6877, 'preemption_count': 0}), (8624, {'train/ctc_loss': Array(0.69155705, dtype=float32), 'train/wer': 0.21579534105952575, 'validation/ctc_loss': Array(0.738769, dtype=float32), 'validation/wer': 0.21139828340268593, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.44854873, dtype=float32), 'test/wer': 0.14309507850425526, 'test/num_examples': 2472, 'score': 7221.522592782974, 'total_duration': 8073.839767456055, 'accumulated_submission_time': 7221.522592782974, 'accumulated_eval_time': 851.6845552921295, 'accumulated_logging_time': 0.23926997184753418, 'global_step': 8624, 'preemption_count': 0}), (10329, {'train/ctc_loss': Array(0.58510447, dtype=float32), 'train/wer': 0.1821902111638566, 'validation/ctc_loss': Array(0.71844894, dtype=float32), 'validation/wer': 0.20581789393398148, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42991805, dtype=float32), 'test/wer': 0.137895314118579, 'test/num_examples': 2472, 'score': 8661.632459640503, 'total_duration': 9651.952268838882, 'accumulated_submission_time': 8661.632459640503, 'accumulated_eval_time': 989.5500612258911, 'accumulated_logging_time': 0.2970449924468994, 'global_step': 10329, 'preemption_count': 0}), (12066, {'train/ctc_loss': Array(0.58945584, dtype=float32), 'train/wer': 0.18889116365828415, 'validation/ctc_loss': Array(0.697573, dtype=float32), 'validation/wer': 0.1997258078530948, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4067004, dtype=float32), 'test/wer': 0.1325533686754819, 'test/num_examples': 2472, 'score': 10101.663102388382, 'total_duration': 11228.021137475967, 'accumulated_submission_time': 10101.663102388382, 'accumulated_eval_time': 1125.454788684845, 'accumulated_logging_time': 0.35008716583251953, 'global_step': 12066, 'preemption_count': 0}), (13788, {'train/ctc_loss': Array(0.44308084, dtype=float32), 'train/wer': 0.1463969658659924, 'validation/ctc_loss': Array(0.6531635, dtype=float32), 'validation/wer': 0.18738716124236077, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39410502, dtype=float32), 'test/wer': 0.12727235797127942, 'test/num_examples': 2472, 'score': 11541.959456205368, 'total_duration': 12805.813822984695, 'accumulated_submission_time': 11541.959456205368, 'accumulated_eval_time': 1262.816997051239, 'accumulated_logging_time': 0.4042665958404541, 'global_step': 13788, 'preemption_count': 0}), (15492, {'train/ctc_loss': Array(0.5043031, dtype=float32), 'train/wer': 0.163937207851208, 'validation/ctc_loss': Array(0.64487046, dtype=float32), 'validation/wer': 0.1834673720999836, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37502876, dtype=float32), 'test/wer': 0.12061015985213171, 'test/num_examples': 2472, 'score': 12982.570106267929, 'total_duration': 14381.421175718307, 'accumulated_submission_time': 12982.570106267929, 'accumulated_eval_time': 1397.6854872703552, 'accumulated_logging_time': 0.454848051071167, 'global_step': 15492, 'preemption_count': 0}), (17205, {'train/ctc_loss': Array(0.44294888, dtype=float32), 'train/wer': 0.1443313293253173, 'validation/ctc_loss': Array(0.6099825, dtype=float32), 'validation/wer': 0.17565675777440937, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3541107, dtype=float32), 'test/wer': 0.1146385554404566, 'test/num_examples': 2472, 'score': 14422.980370283127, 'total_duration': 15959.662130594254, 'accumulated_submission_time': 14422.980370283127, 'accumulated_eval_time': 1535.3861787319183, 'accumulated_logging_time': 0.5043253898620605, 'global_step': 17205, 'preemption_count': 0}), (18938, {'train/ctc_loss': Array(0.43057463, dtype=float32), 'train/wer': 0.1437458643843492, 'validation/ctc_loss': Array(0.59888196, dtype=float32), 'validation/wer': 0.1724417583054153, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3469349, dtype=float32), 'test/wer': 0.112688643795828, 'test/num_examples': 2472, 'score': 15863.009141683578, 'total_duration': 17535.333825826645, 'accumulated_submission_time': 15863.009141683578, 'accumulated_eval_time': 1670.8955328464508, 'accumulated_logging_time': 0.5572524070739746, 'global_step': 18938, 'preemption_count': 0}), (20641, {'train/ctc_loss': Array(0.4117053, dtype=float32), 'train/wer': 0.13532842736440448, 'validation/ctc_loss': Array(0.5848249, dtype=float32), 'validation/wer': 0.1684061133263176, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33290595, dtype=float32), 'test/wer': 0.1072248288749416, 'test/num_examples': 2472, 'score': 17303.21779513359, 'total_duration': 19110.668027877808, 'accumulated_submission_time': 17303.21779513359, 'accumulated_eval_time': 1805.8819556236267, 'accumulated_logging_time': 0.6184689998626709, 'global_step': 20641, 'preemption_count': 0}), (22360, {'train/ctc_loss': Array(0.40284434, dtype=float32), 'train/wer': 0.13384243915217367, 'validation/ctc_loss': Array(0.5693361, dtype=float32), 'validation/wer': 0.16293192504127363, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3231798, dtype=float32), 'test/wer': 0.10419840350984096, 'test/num_examples': 2472, 'score': 18743.48190689087, 'total_duration': 20685.821735858917, 'accumulated_submission_time': 18743.48190689087, 'accumulated_eval_time': 1940.645127773285, 'accumulated_logging_time': 0.6647696495056152, 'global_step': 22360, 'preemption_count': 0}), (24075, {'train/ctc_loss': Array(0.43493432, dtype=float32), 'train/wer': 0.13886030069804906, 'validation/ctc_loss': Array(0.5609899, dtype=float32), 'validation/wer': 0.16197611438832946, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31203714, dtype=float32), 'test/wer': 0.10269534661710641, 'test/num_examples': 2472, 'score': 20183.945833444595, 'total_duration': 22260.01620745659, 'accumulated_submission_time': 20183.945833444595, 'accumulated_eval_time': 2074.2483954429626, 'accumulated_logging_time': 0.7133445739746094, 'global_step': 24075, 'preemption_count': 0}), (25757, {'train/ctc_loss': Array(0.361608, dtype=float32), 'train/wer': 0.12085050508394159, 'validation/ctc_loss': Array(0.5388162, dtype=float32), 'validation/wer': 0.15627021442984446, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30561492, dtype=float32), 'test/wer': 0.09869396542969147, 'test/num_examples': 2472, 'score': 21624.523320913315, 'total_duration': 23836.974450588226, 'accumulated_submission_time': 21624.523320913315, 'accumulated_eval_time': 2210.4984588623047, 'accumulated_logging_time': 0.7659845352172852, 'global_step': 25757, 'preemption_count': 0}), (27487, {'train/ctc_loss': Array(0.37259036, dtype=float32), 'train/wer': 0.12723949872360177, 'validation/ctc_loss': Array(0.5171766, dtype=float32), 'validation/wer': 0.15005261785917723, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29226407, dtype=float32), 'test/wer': 0.09477383056080271, 'test/num_examples': 2472, 'score': 23064.93439102173, 'total_duration': 25411.825412988663, 'accumulated_submission_time': 23064.93439102173, 'accumulated_eval_time': 2344.8034658432007, 'accumulated_logging_time': 0.8201003074645996, 'global_step': 27487, 'preemption_count': 0}), (29212, {'train/ctc_loss': Array(0.30891111, dtype=float32), 'train/wer': 0.10437902857990669, 'validation/ctc_loss': Array(0.5069906, dtype=float32), 'validation/wer': 0.1449549610434749, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2780522, dtype=float32), 'test/wer': 0.09018341356407288, 'test/num_examples': 2472, 'score': 24504.827920675278, 'total_duration': 26985.95388007164, 'accumulated_submission_time': 24504.827920675278, 'accumulated_eval_time': 2478.910748243332, 'accumulated_logging_time': 0.8709836006164551, 'global_step': 29212, 'preemption_count': 0}), (30904, {'train/ctc_loss': Array(0.3336694, dtype=float32), 'train/wer': 0.11066489436918465, 'validation/ctc_loss': Array(0.5003634, dtype=float32), 'validation/wer': 0.14309161300288675, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27133593, dtype=float32), 'test/wer': 0.08754290821197165, 'test/num_examples': 2472, 'score': 25945.360632419586, 'total_duration': 28559.09006714821, 'accumulated_submission_time': 25945.360632419586, 'accumulated_eval_time': 2611.3780493736267, 'accumulated_logging_time': 0.9297785758972168, 'global_step': 30904, 'preemption_count': 0}), (32614, {'train/ctc_loss': Array(0.3215798, dtype=float32), 'train/wer': 0.10961561435145943, 'validation/ctc_loss': Array(0.4891848, dtype=float32), 'validation/wer': 0.1402531450032343, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26732162, dtype=float32), 'test/wer': 0.08496333759876505, 'test/num_examples': 2472, 'score': 27385.716521024704, 'total_duration': 30134.061345100403, 'accumulated_submission_time': 27385.716521024704, 'accumulated_eval_time': 2745.85751581192, 'accumulated_logging_time': 0.98575758934021, 'global_step': 32614, 'preemption_count': 0}), (34328, {'train/ctc_loss': Array(0.30576283, dtype=float32), 'train/wer': 0.10333758409920409, 'validation/ctc_loss': Array(0.4679304, dtype=float32), 'validation/wer': 0.13436380663660852, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2541731, dtype=float32), 'test/wer': 0.0815916153799281, 'test/num_examples': 2472, 'score': 28825.82073378563, 'total_duration': 31709.3510825634, 'accumulated_submission_time': 28825.82073378563, 'accumulated_eval_time': 2880.9082431793213, 'accumulated_logging_time': 1.0413026809692383, 'global_step': 34328, 'preemption_count': 0}), (36023, {'train/ctc_loss': Array(0.24863774, dtype=float32), 'train/wer': 0.08587385761028997, 'validation/ctc_loss': Array(0.45736533, dtype=float32), 'validation/wer': 0.13183428753487744, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24383034, dtype=float32), 'test/wer': 0.07836207421851198, 'test/num_examples': 2472, 'score': 30266.262042284012, 'total_duration': 33283.799632787704, 'accumulated_submission_time': 30266.262042284012, 'accumulated_eval_time': 3014.7819306850433, 'accumulated_logging_time': 1.0968976020812988, 'global_step': 36023, 'preemption_count': 0}), (37719, {'train/ctc_loss': Array(0.22751224, dtype=float32), 'train/wer': 0.07755462420951846, 'validation/ctc_loss': Array(0.44469142, dtype=float32), 'validation/wer': 0.12733521920889773, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24005848, dtype=float32), 'test/wer': 0.07653403205167265, 'test/num_examples': 2472, 'score': 31706.707235336304, 'total_duration': 34858.85399699211, 'accumulated_submission_time': 31706.707235336304, 'accumulated_eval_time': 3149.2583301067352, 'accumulated_logging_time': 1.1505632400512695, 'global_step': 37719, 'preemption_count': 0}), (39437, {'train/ctc_loss': Array(0.2554167, dtype=float32), 'train/wer': 0.0892404637499159, 'validation/ctc_loss': Array(0.43711883, dtype=float32), 'validation/wer': 0.12433262210722458, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23413983, dtype=float32), 'test/wer': 0.07456380882741251, 'test/num_examples': 2472, 'score': 33147.02962565422, 'total_duration': 36434.26735687256, 'accumulated_submission_time': 33147.02962565422, 'accumulated_eval_time': 3284.214804172516, 'accumulated_logging_time': 1.2067761421203613, 'global_step': 39437, 'preemption_count': 0}), (41134, {'train/ctc_loss': Array(0.2320539, dtype=float32), 'train/wer': 0.08016946666516231, 'validation/ctc_loss': Array(0.42751628, dtype=float32), 'validation/wer': 0.12143622618921189, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23012903, dtype=float32), 'test/wer': 0.07413726565515, 'test/num_examples': 2472, 'score': 34586.98317909241, 'total_duration': 38007.80613279343, 'accumulated_submission_time': 34586.98317909241, 'accumulated_eval_time': 3417.66161775589, 'accumulated_logging_time': 1.2691385746002197, 'global_step': 41134, 'preemption_count': 0}), (42832, {'train/ctc_loss': Array(0.24211791, dtype=float32), 'train/wer': 0.08087093464149121, 'validation/ctc_loss': Array(0.42340276, dtype=float32), 'validation/wer': 0.12034525039342711, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.225074, dtype=float32), 'test/wer': 0.07196392663457438, 'test/num_examples': 2472, 'score': 36027.01313138008, 'total_duration': 39582.16630578041, 'accumulated_submission_time': 36027.01313138008, 'accumulated_eval_time': 3551.85942363739, 'accumulated_logging_time': 1.322899580001831, 'global_step': 42832, 'preemption_count': 0}), (44550, {'train/ctc_loss': Array(0.2000132, dtype=float32), 'train/wer': 0.06981170255237928, 'validation/ctc_loss': Array(0.4208656, dtype=float32), 'validation/wer': 0.11988182704654508, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22411768, dtype=float32), 'test/wer': 0.07169987609936425, 'test/num_examples': 2472, 'score': 37467.38977432251, 'total_duration': 41159.824618816376, 'accumulated_submission_time': 37467.38977432251, 'accumulated_eval_time': 3689.00634765625, 'accumulated_logging_time': 1.3793542385101318, 'global_step': 44550, 'preemption_count': 0}), (46233, {'train/ctc_loss': Array(0.20435266, dtype=float32), 'train/wer': 0.07182548768841555, 'validation/ctc_loss': Array(0.41903222, dtype=float32), 'validation/wer': 0.11900325361808123, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22205608, dtype=float32), 'test/wer': 0.07042024658257673, 'test/num_examples': 2472, 'score': 38907.78763270378, 'total_duration': 42735.924476861954, 'accumulated_submission_time': 38907.78763270378, 'accumulated_eval_time': 3824.5769357681274, 'accumulated_logging_time': 1.4339289665222168, 'global_step': 46233, 'preemption_count': 0}), (47961, {'train/ctc_loss': Array(0.22109444, dtype=float32), 'train/wer': 0.0784157378244303, 'validation/ctc_loss': Array(0.41872, dtype=float32), 'validation/wer': 0.11862672214873958, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22187033, dtype=float32), 'test/wer': 0.0707452318566815, 'test/num_examples': 2472, 'score': 40348.03506684303, 'total_duration': 44312.138027668, 'accumulated_submission_time': 40348.03506684303, 'accumulated_eval_time': 3960.401820898056, 'accumulated_logging_time': 1.495056390762329, 'global_step': 47961, 'preemption_count': 0}), (48000, {'train/ctc_loss': Array(0.14898208, dtype=float32), 'train/wer': 0.052723391806529206, 'validation/ctc_loss': Array(0.4187202, dtype=float32), 'validation/wer': 0.11863637680179963, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2218685, dtype=float32), 'test/wer': 0.0707452318566815, 'test/num_examples': 2472, 'score': 40377.33175325394, 'total_duration': 44474.47600841522, 'accumulated_submission_time': 40377.33175325394, 'accumulated_eval_time': 4093.375575065613, 'accumulated_logging_time': 1.5548663139343262, 'global_step': 48000, 'preemption_count': 0})], 'global_step': 48000}
I0213 14:19:16.817151 140441227016000 submission_runner.py:586] Timing: 40377.33175325394
I0213 14:19:16.817233 140441227016000 submission_runner.py:588] Total number of evals: 30
I0213 14:19:16.817301 140441227016000 submission_runner.py:589] ====================
I0213 14:19:16.863069 140441227016000 submission_runner.py:673] Final librispeech_deepspeech score: 39802.3655128479
