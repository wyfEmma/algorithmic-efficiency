python3 submission_runner.py --framework=jax --workload=librispeech_conformer --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification/study_2 --overwrite=true --save_checkpoints=false --num_tuning_trials=5 --rng_seed=1365630931 --max_global_steps=80000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_conformer_jax_02-14-2024-02-44-53.log
I0214 02:45:13.858191 139646656866112 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification/study_2/librispeech_conformer_jax.
I0214 02:45:14.913858 139646656866112 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0214 02:45:14.915317 139646656866112 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0214 02:45:14.915459 139646656866112 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0214 02:45:14.916433 139646656866112 submission_runner.py:542] Using RNG seed 1365630931
I0214 02:45:16.002030 139646656866112 submission_runner.py:551] --- Tuning run 1/5 ---
I0214 02:45:16.002246 139646656866112 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_2/librispeech_conformer_jax/trial_1.
I0214 02:45:16.002551 139646656866112 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_2/librispeech_conformer_jax/trial_1/hparams.json.
I0214 02:45:16.192624 139646656866112 submission_runner.py:206] Initializing dataset.
I0214 02:45:16.192842 139646656866112 submission_runner.py:213] Initializing model.
I0214 02:45:21.122130 139646656866112 submission_runner.py:255] Initializing optimizer.
I0214 02:45:22.377383 139646656866112 submission_runner.py:262] Initializing metrics bundle.
I0214 02:45:22.377577 139646656866112 submission_runner.py:280] Initializing checkpoint and logger.
I0214 02:45:22.378877 139646656866112 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_2/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I0214 02:45:22.379021 139646656866112 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_2/librispeech_conformer_jax/trial_1/meta_data_0.json.
I0214 02:45:22.379236 139646656866112 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0214 02:45:22.379300 139646656866112 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0214 02:45:22.691303 139646656866112 logger_utils.py:220] Unable to record git information. Continuing without it.
I0214 02:45:22.970416 139646656866112 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_2/librispeech_conformer_jax/trial_1/flags_0.json.
I0214 02:45:22.984772 139646656866112 submission_runner.py:314] Starting training loop.
I0214 02:45:23.281919 139646656866112 input_pipeline.py:20] Loading split = train-clean-100
I0214 02:45:23.320468 139646656866112 input_pipeline.py:20] Loading split = train-clean-360
I0214 02:45:23.736851 139646656866112 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0214 02:46:25.282732 139471865501440 logging_writer.py:48] [0] global_step=0, grad_norm=42.52507781982422, loss=32.34053421020508
I0214 02:46:25.317634 139646656866112 spec.py:321] Evaluating on the training split.
I0214 02:46:25.490488 139646656866112 input_pipeline.py:20] Loading split = train-clean-100
I0214 02:46:25.526330 139646656866112 input_pipeline.py:20] Loading split = train-clean-360
I0214 02:46:25.925453 139646656866112 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0214 02:47:39.419799 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 02:47:39.536421 139646656866112 input_pipeline.py:20] Loading split = dev-clean
I0214 02:47:39.541865 139646656866112 input_pipeline.py:20] Loading split = dev-other
I0214 02:48:44.689900 139646656866112 spec.py:349] Evaluating on the test split.
I0214 02:48:44.809841 139646656866112 input_pipeline.py:20] Loading split = test-clean
I0214 02:49:22.908736 139646656866112 submission_runner.py:408] Time since start: 239.92s, 	Step: 1, 	{'train/ctc_loss': Array(31.690332, dtype=float32), 'train/wer': 1.115215625845949, 'validation/ctc_loss': Array(30.757864, dtype=float32), 'validation/wer': 1.1779352558965794, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.871214, dtype=float32), 'test/wer': 1.190014827453131, 'test/num_examples': 2472, 'score': 62.33279633522034, 'total_duration': 239.9211916923523, 'accumulated_submission_time': 62.33279633522034, 'accumulated_eval_time': 177.5883333683014, 'accumulated_logging_time': 0}
I0214 02:49:22.937780 139460991751936 logging_writer.py:48] [1] accumulated_eval_time=177.588333, accumulated_logging_time=0, accumulated_submission_time=62.332796, global_step=1, preemption_count=0, score=62.332796, test/ctc_loss=30.871213912963867, test/num_examples=2472, test/wer=1.190015, total_duration=239.921192, train/ctc_loss=31.690332412719727, train/wer=1.115216, validation/ctc_loss=30.757863998413086, validation/num_examples=5348, validation/wer=1.177935
I0214 02:51:02.027916 139474356872960 logging_writer.py:48] [100] global_step=100, grad_norm=27.410890579223633, loss=7.350632667541504
I0214 02:52:18.584710 139474365265664 logging_writer.py:48] [200] global_step=200, grad_norm=2.985887050628662, loss=6.135007858276367
I0214 02:53:35.147247 139474356872960 logging_writer.py:48] [300] global_step=300, grad_norm=0.36946871876716614, loss=5.870321750640869
I0214 02:54:53.717522 139474365265664 logging_writer.py:48] [400] global_step=400, grad_norm=0.2716549038887024, loss=5.815187454223633
I0214 02:56:16.000545 139474356872960 logging_writer.py:48] [500] global_step=500, grad_norm=0.4551999866962433, loss=5.8088579177856445
I0214 02:57:41.516237 139474365265664 logging_writer.py:48] [600] global_step=600, grad_norm=0.37593650817871094, loss=5.806507110595703
I0214 02:59:06.514042 139474356872960 logging_writer.py:48] [700] global_step=700, grad_norm=0.4537558853626251, loss=5.7737531661987305
I0214 03:00:32.555652 139474365265664 logging_writer.py:48] [800] global_step=800, grad_norm=0.7711817026138306, loss=5.803458213806152
I0214 03:01:55.319740 139474356872960 logging_writer.py:48] [900] global_step=900, grad_norm=0.4113439619541168, loss=5.770241737365723
I0214 03:03:22.806589 139474365265664 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.3028858006000519, loss=5.77243709564209
I0214 03:04:45.684551 139474432407296 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.42878666520118713, loss=5.797902584075928
I0214 03:06:02.789195 139474424014592 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.252154678106308, loss=5.7946343421936035
I0214 03:07:20.082170 139474432407296 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.882431149482727, loss=5.780158519744873
I0214 03:08:36.709862 139474424014592 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.8957254886627197, loss=5.723970890045166
I0214 03:09:53.336962 139474432407296 logging_writer.py:48] [1500] global_step=1500, grad_norm=3.025589942932129, loss=5.588040828704834
I0214 03:11:16.700525 139474424014592 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.947935163974762, loss=5.451842784881592
I0214 03:12:43.470223 139474432407296 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.7213605642318726, loss=5.325309753417969
I0214 03:13:23.306297 139646656866112 spec.py:321] Evaluating on the training split.
I0214 03:14:00.365256 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 03:14:48.439102 139646656866112 spec.py:349] Evaluating on the test split.
I0214 03:15:12.311631 139646656866112 submission_runner.py:408] Time since start: 1789.32s, 	Step: 1748, 	{'train/ctc_loss': Array(5.9307604, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': Array(6.0122747, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.9551396, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1502.6152069568634, 'total_duration': 1789.3208968639374, 'accumulated_submission_time': 1502.6152069568634, 'accumulated_eval_time': 286.587749004364, 'accumulated_logging_time': 0.046248435974121094}
I0214 03:15:12.344197 139474432407296 logging_writer.py:48] [1748] accumulated_eval_time=286.587749, accumulated_logging_time=0.046248, accumulated_submission_time=1502.615207, global_step=1748, preemption_count=0, score=1502.615207, test/ctc_loss=5.955139636993408, test/num_examples=2472, test/wer=0.899580, total_duration=1789.320897, train/ctc_loss=5.930760383605957, train/wer=0.944636, validation/ctc_loss=6.012274742126465, validation/num_examples=5348, validation/wer=0.896618
I0214 03:15:53.001038 139474424014592 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.1194727420806885, loss=4.868768215179443
I0214 03:17:09.683524 139474432407296 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.7928574681282043, loss=4.361123561859131
I0214 03:18:26.376184 139474424014592 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.0489983558654785, loss=3.9402730464935303
I0214 03:19:46.168317 139475743127296 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.8391044735908508, loss=3.735527515411377
I0214 03:21:02.804589 139475734734592 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.0049693584442139, loss=3.5099236965179443
I0214 03:22:20.013966 139475743127296 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.049812912940979, loss=3.3364791870117188
I0214 03:23:37.007577 139475734734592 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.0317238569259644, loss=3.199394464492798
I0214 03:24:55.388579 139475743127296 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.1312521696090698, loss=3.105492115020752
I0214 03:26:22.095804 139475734734592 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.641055703163147, loss=2.9914872646331787
I0214 03:27:48.313621 139475743127296 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.2395813465118408, loss=2.9838056564331055
I0214 03:29:15.037966 139475734734592 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.1348025798797607, loss=2.861238956451416
I0214 03:30:41.846259 139475743127296 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.2104957103729248, loss=2.7846498489379883
I0214 03:32:04.669243 139475734734592 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.2204346656799316, loss=2.7335729598999023
I0214 03:33:34.356986 139476398487296 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.3100305795669556, loss=2.7234063148498535
I0214 03:34:50.913569 139476390094592 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.134994626045227, loss=2.661672592163086
I0214 03:36:07.177093 139476398487296 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.0069340467453003, loss=2.5885772705078125
I0214 03:37:23.845767 139476390094592 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.9703659415245056, loss=2.5879855155944824
I0214 03:38:41.136356 139476398487296 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.1153751611709595, loss=2.492144823074341
I0214 03:39:12.583732 139646656866112 spec.py:321] Evaluating on the training split.
I0214 03:39:58.270982 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 03:40:47.002347 139646656866112 spec.py:349] Evaluating on the test split.
I0214 03:41:12.621560 139646656866112 submission_runner.py:408] Time since start: 3349.63s, 	Step: 3538, 	{'train/ctc_loss': Array(3.3182616, dtype=float32), 'train/wer': 0.719244599525877, 'validation/ctc_loss': Array(3.7128878, dtype=float32), 'validation/wer': 0.7624858800698997, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.388314, dtype=float32), 'test/wer': 0.7071273332927102, 'test/num_examples': 2472, 'score': 2942.7661283016205, 'total_duration': 3349.6305611133575, 'accumulated_submission_time': 2942.7661283016205, 'accumulated_eval_time': 406.61941361427307, 'accumulated_logging_time': 0.0941307544708252}
I0214 03:41:12.658756 139475814807296 logging_writer.py:48] [3538] accumulated_eval_time=406.619414, accumulated_logging_time=0.094131, accumulated_submission_time=2942.766128, global_step=3538, preemption_count=0, score=2942.766128, test/ctc_loss=3.3883140087127686, test/num_examples=2472, test/wer=0.707127, total_duration=3349.630561, train/ctc_loss=3.3182616233825684, train/wer=0.719245, validation/ctc_loss=3.712887763977051, validation/num_examples=5348, validation/wer=0.762486
I0214 03:42:00.710211 139475806414592 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.0210418701171875, loss=2.450683832168579
I0214 03:43:17.590424 139475814807296 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.0718224048614502, loss=2.4505364894866943
I0214 03:44:33.864473 139475806414592 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.9851308465003967, loss=2.364933490753174
I0214 03:45:55.922763 139475814807296 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.9294441938400269, loss=2.4215219020843506
I0214 03:47:21.972383 139475806414592 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.4426501989364624, loss=2.4170384407043457
I0214 03:48:46.335444 139475814807296 logging_writer.py:48] [4100] global_step=4100, grad_norm=1.0274006128311157, loss=2.3105297088623047
I0214 03:50:08.277256 139474504087296 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.876999020576477, loss=2.2972869873046875
I0214 03:51:25.525731 139474495694592 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.9250796437263489, loss=2.202779531478882
I0214 03:52:42.367795 139474504087296 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.8819118142127991, loss=2.170717477798462
I0214 03:54:02.380271 139474495694592 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.9703252911567688, loss=2.131669521331787
I0214 03:55:23.150931 139474504087296 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.7982214689254761, loss=2.072183609008789
I0214 03:56:48.184175 139474495694592 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.7839095592498779, loss=2.0939409732818604
I0214 03:58:16.030463 139474504087296 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.8955137133598328, loss=2.123969316482544
I0214 03:59:42.873735 139474495694592 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.8903906941413879, loss=2.064836263656616
I0214 04:01:11.844470 139474504087296 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.8776193261146545, loss=2.051276683807373
I0214 04:02:41.584167 139474495694592 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.8850569128990173, loss=2.0460472106933594
I0214 04:04:06.732516 139474504087296 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.828650712966919, loss=2.0138258934020996
I0214 04:05:13.088936 139646656866112 spec.py:321] Evaluating on the training split.
I0214 04:06:08.173646 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 04:07:02.013545 139646656866112 spec.py:349] Evaluating on the test split.
I0214 04:07:28.642197 139646656866112 submission_runner.py:408] Time since start: 4925.65s, 	Step: 5287, 	{'train/ctc_loss': Array(0.7781294, dtype=float32), 'train/wer': 0.2672749603344034, 'validation/ctc_loss': Array(1.1462481, dtype=float32), 'validation/wer': 0.3319366268573139, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.8411037, dtype=float32), 'test/wer': 0.2719517396867954, 'test/num_examples': 2472, 'score': 4383.109746217728, 'total_duration': 4925.651102542877, 'accumulated_submission_time': 4383.109746217728, 'accumulated_eval_time': 542.1664445400238, 'accumulated_logging_time': 0.1464216709136963}
I0214 04:07:28.674325 139475968407296 logging_writer.py:48] [5287] accumulated_eval_time=542.166445, accumulated_logging_time=0.146422, accumulated_submission_time=4383.109746, global_step=5287, preemption_count=0, score=4383.109746, test/ctc_loss=0.8411036729812622, test/num_examples=2472, test/wer=0.271952, total_duration=4925.651103, train/ctc_loss=0.7781293988227844, train/wer=0.267275, validation/ctc_loss=1.1462481021881104, validation/num_examples=5348, validation/wer=0.331937
I0214 04:07:39.341855 139475960014592 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.8769460320472717, loss=1.9783509969711304
I0214 04:08:55.266285 139475968407296 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.8903148770332336, loss=1.9457813501358032
I0214 04:10:11.837599 139475960014592 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.8159237504005432, loss=1.9348000288009644
I0214 04:11:28.035263 139475968407296 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.8397042155265808, loss=1.931707501411438
I0214 04:12:50.833405 139475960014592 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.8185830116271973, loss=1.9414173364639282
I0214 04:14:17.454643 139475968407296 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.8892423510551453, loss=1.9251811504364014
I0214 04:15:41.609521 139475960014592 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.7669879198074341, loss=1.9372882843017578
I0214 04:17:05.594884 139475968407296 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.0236657857894897, loss=1.9092379808425903
I0214 04:18:35.479488 139475960014592 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.8799917101860046, loss=1.793819785118103
I0214 04:20:04.913482 139475968407296 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.9180933833122253, loss=1.8936880826950073
I0214 04:21:20.856946 139475960014592 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.8244866132736206, loss=1.8285553455352783
I0214 04:22:36.937026 139475968407296 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.797928512096405, loss=1.8204994201660156
I0214 04:23:57.193067 139475960014592 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.7961063981056213, loss=1.7995457649230957
I0214 04:25:21.115180 139475968407296 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.82502681016922, loss=1.8626326322555542
I0214 04:26:47.252199 139475960014592 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.7453776001930237, loss=1.8114330768585205
I0214 04:28:14.993199 139475968407296 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.7514188289642334, loss=1.7782844305038452
I0214 04:29:42.007294 139475960014592 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.7006416916847229, loss=1.7619518041610718
I0214 04:31:09.020088 139475968407296 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.8083574175834656, loss=1.76775062084198
I0214 04:31:28.893434 139646656866112 spec.py:321] Evaluating on the training split.
I0214 04:32:23.761738 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 04:33:17.868908 139646656866112 spec.py:349] Evaluating on the test split.
I0214 04:33:44.529280 139646656866112 submission_runner.py:408] Time since start: 6501.54s, 	Step: 7026, 	{'train/ctc_loss': Array(0.5286893, dtype=float32), 'train/wer': 0.1846181235534983, 'validation/ctc_loss': Array(0.84069234, dtype=float32), 'validation/wer': 0.25316431254042887, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.58280253, dtype=float32), 'test/wer': 0.1943208823350192, 'test/num_examples': 2472, 'score': 5823.235984802246, 'total_duration': 6501.536516189575, 'accumulated_submission_time': 5823.235984802246, 'accumulated_eval_time': 677.7943549156189, 'accumulated_logging_time': 0.19841790199279785}
I0214 04:33:44.566743 139476398487296 logging_writer.py:48] [7026] accumulated_eval_time=677.794355, accumulated_logging_time=0.198418, accumulated_submission_time=5823.235985, global_step=7026, preemption_count=0, score=5823.235985, test/ctc_loss=0.5828025341033936, test/num_examples=2472, test/wer=0.194321, total_duration=6501.536516, train/ctc_loss=0.5286893248558044, train/wer=0.184618, validation/ctc_loss=0.8406923413276672, validation/num_examples=5348, validation/wer=0.253164
I0214 04:34:41.391164 139476390094592 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.7613477110862732, loss=1.7735557556152344
I0214 04:35:57.971166 139476398487296 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.7831845879554749, loss=1.7408745288848877
I0214 04:37:18.462688 139476398487296 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.8075580596923828, loss=1.7489482164382935
I0214 04:38:34.629147 139476390094592 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.7587738037109375, loss=1.8029264211654663
I0214 04:39:53.458500 139476398487296 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.7792134881019592, loss=1.7482250928878784
I0214 04:41:14.389675 139476390094592 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.6700427532196045, loss=1.7170056104660034
I0214 04:42:39.904048 139476398487296 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.6292825937271118, loss=1.7060576677322388
I0214 04:44:07.692054 139476390094592 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.6869361400604248, loss=1.7157989740371704
I0214 04:45:31.005915 139476398487296 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.8948052525520325, loss=1.7164603471755981
I0214 04:46:57.954466 139476390094592 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.7830219268798828, loss=1.7431312799453735
I0214 04:48:24.697963 139476398487296 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.7265517711639404, loss=1.7020477056503296
I0214 04:49:50.947762 139476390094592 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.7071579098701477, loss=1.691145420074463
I0214 04:51:14.714638 139475313047296 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.6874141097068787, loss=1.7138707637786865
I0214 04:52:30.853038 139475304654592 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.7284954786300659, loss=1.7214243412017822
I0214 04:53:47.369333 139475313047296 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.8788906335830688, loss=1.7103618383407593
I0214 04:55:06.547239 139475304654592 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.8163686990737915, loss=1.6332359313964844
I0214 04:56:28.717444 139475313047296 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.7312890887260437, loss=1.6908304691314697
I0214 04:57:45.152040 139646656866112 spec.py:321] Evaluating on the training split.
I0214 04:58:39.010557 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 04:59:31.534400 139646656866112 spec.py:349] Evaluating on the test split.
I0214 04:59:59.351261 139646656866112 submission_runner.py:408] Time since start: 8076.36s, 	Step: 8791, 	{'train/ctc_loss': Array(0.44677088, dtype=float32), 'train/wer': 0.15943486485898803, 'validation/ctc_loss': Array(0.7451185, dtype=float32), 'validation/wer': 0.2265271247477722, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4925594, dtype=float32), 'test/wer': 0.16637214876200923, 'test/num_examples': 2472, 'score': 7263.7350125312805, 'total_duration': 8076.360379934311, 'accumulated_submission_time': 7263.7350125312805, 'accumulated_eval_time': 811.9875221252441, 'accumulated_logging_time': 0.2500596046447754}
I0214 04:59:59.391335 139475676567296 logging_writer.py:48] [8791] accumulated_eval_time=811.987522, accumulated_logging_time=0.250060, accumulated_submission_time=7263.735013, global_step=8791, preemption_count=0, score=7263.735013, test/ctc_loss=0.49255940318107605, test/num_examples=2472, test/wer=0.166372, total_duration=8076.360380, train/ctc_loss=0.44677087664604187, train/wer=0.159435, validation/ctc_loss=0.7451184988021851, validation/num_examples=5348, validation/wer=0.226527
I0214 05:00:07.144678 139475668174592 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.9035040140151978, loss=1.6738128662109375
I0214 05:01:23.482996 139475676567296 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.6360498070716858, loss=1.6761938333511353
I0214 05:02:39.610874 139475668174592 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.6821654438972473, loss=1.6501206159591675
I0214 05:03:56.737551 139475676567296 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.7316529750823975, loss=1.656102180480957
I0214 05:05:23.243994 139475668174592 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.7692998051643372, loss=1.5883322954177856
I0214 05:06:50.022808 139475676567296 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.7526479363441467, loss=1.6180340051651
I0214 05:08:07.911376 139475668174592 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.7104291319847107, loss=1.6069592237472534
I0214 05:09:24.433209 139475676567296 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.70536208152771, loss=1.6691471338272095
I0214 05:10:43.499477 139475668174592 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.7743356227874756, loss=1.614111065864563
I0214 05:12:07.162255 139475676567296 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.6296932101249695, loss=1.6364532709121704
I0214 05:13:33.699223 139475668174592 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.6554393172264099, loss=1.6246435642242432
I0214 05:15:02.193149 139475676567296 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.621271550655365, loss=1.6805745363235474
I0214 05:16:26.261245 139475668174592 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.6635451912879944, loss=1.6477391719818115
I0214 05:17:53.090538 139475676567296 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.774827778339386, loss=1.58523690700531
I0214 05:19:24.712211 139475668174592 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.6445325613021851, loss=1.638080358505249
I0214 05:20:58.097412 139475676567296 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.8017760515213013, loss=1.6046689748764038
I0214 05:22:14.090108 139475668174592 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.8088546991348267, loss=1.5960184335708618
I0214 05:23:30.268814 139475676567296 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.6331517100334167, loss=1.550418496131897
I0214 05:23:59.530776 139646656866112 spec.py:321] Evaluating on the training split.
I0214 05:24:54.548727 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 05:25:48.582544 139646656866112 spec.py:349] Evaluating on the test split.
I0214 05:26:16.366064 139646656866112 submission_runner.py:408] Time since start: 9653.37s, 	Step: 10539, 	{'train/ctc_loss': Array(0.4121761, dtype=float32), 'train/wer': 0.14543534043563386, 'validation/ctc_loss': Array(0.67398584, dtype=float32), 'validation/wer': 0.20539308919933963, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4373133, dtype=float32), 'test/wer': 0.1492088639733512, 'test/num_examples': 2472, 'score': 8703.784867048264, 'total_duration': 9653.374977111816, 'accumulated_submission_time': 8703.784867048264, 'accumulated_eval_time': 948.816588640213, 'accumulated_logging_time': 0.30726146697998047}
I0214 05:26:16.400074 139476106647296 logging_writer.py:48] [10539] accumulated_eval_time=948.816589, accumulated_logging_time=0.307261, accumulated_submission_time=8703.784867, global_step=10539, preemption_count=0, score=8703.784867, test/ctc_loss=0.4373132884502411, test/num_examples=2472, test/wer=0.149209, total_duration=9653.374977, train/ctc_loss=0.41217610239982605, train/wer=0.145435, validation/ctc_loss=0.6739858388900757, validation/num_examples=5348, validation/wer=0.205393
I0214 05:27:03.885401 139476098254592 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.6765058636665344, loss=1.5616490840911865
I0214 05:28:20.138082 139476106647296 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.6302884221076965, loss=1.5940876007080078
I0214 05:29:36.230136 139476098254592 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.6971849203109741, loss=1.5965849161148071
I0214 05:31:03.029973 139476106647296 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.6344509720802307, loss=1.6042073965072632
I0214 05:32:29.083195 139476098254592 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.593163788318634, loss=1.495010256767273
I0214 05:33:55.394501 139476106647296 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.7133639454841614, loss=1.580689787864685
I0214 05:35:25.839834 139476098254592 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.7104952931404114, loss=1.6334292888641357
I0214 05:36:55.828449 139476106647296 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.683303713798523, loss=1.550850749015808
I0214 05:38:19.384055 139475778967296 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.6613432168960571, loss=1.5856783390045166
I0214 05:39:35.848683 139475770574592 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.6368840932846069, loss=1.448594093322754
I0214 05:40:52.133781 139475778967296 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.7195576429367065, loss=1.4778069257736206
I0214 05:42:12.718335 139475770574592 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.807439923286438, loss=1.5497865676879883
I0214 05:43:40.160763 139475778967296 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.6294280290603638, loss=1.5351492166519165
I0214 05:45:12.261083 139475770574592 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.6041617393493652, loss=1.5255500078201294
I0214 05:46:39.442208 139475778967296 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.8672219514846802, loss=1.5847309827804565
I0214 05:48:09.825827 139475770574592 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.6954829692840576, loss=1.5560593605041504
I0214 05:49:35.356282 139475778967296 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.6866737008094788, loss=1.5162169933319092
I0214 05:50:16.511588 139646656866112 spec.py:321] Evaluating on the training split.
I0214 05:51:15.510588 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 05:52:08.117804 139646656866112 spec.py:349] Evaluating on the test split.
I0214 05:52:35.871920 139646656866112 submission_runner.py:408] Time since start: 11232.88s, 	Step: 12248, 	{'train/ctc_loss': Array(0.36185822, dtype=float32), 'train/wer': 0.12938974326250186, 'validation/ctc_loss': Array(0.6489348, dtype=float32), 'validation/wer': 0.19732179924114426, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.411901, dtype=float32), 'test/wer': 0.1403936384132594, 'test/num_examples': 2472, 'score': 10143.811375379562, 'total_duration': 11232.879916191101, 'accumulated_submission_time': 10143.811375379562, 'accumulated_eval_time': 1088.169734954834, 'accumulated_logging_time': 0.355985164642334}
I0214 05:52:35.908653 139475487127296 logging_writer.py:48] [12248] accumulated_eval_time=1088.169735, accumulated_logging_time=0.355985, accumulated_submission_time=10143.811375, global_step=12248, preemption_count=0, score=10143.811375, test/ctc_loss=0.41190099716186523, test/num_examples=2472, test/wer=0.140394, total_duration=11232.879916, train/ctc_loss=0.36185821890830994, train/wer=0.129390, validation/ctc_loss=0.6489347815513611, validation/num_examples=5348, validation/wer=0.197322
I0214 05:53:16.017045 139475478734592 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.858024537563324, loss=1.5753084421157837
I0214 05:54:36.044096 139474831767296 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.6877370476722717, loss=1.5171494483947754
I0214 05:55:52.166297 139474823374592 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.6112300157546997, loss=1.510408878326416
I0214 05:57:08.659547 139474831767296 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.7517942786216736, loss=1.551904559135437
I0214 05:58:29.083068 139474823374592 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.5583406090736389, loss=1.5154720544815063
I0214 05:59:54.579846 139474831767296 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.8406045436859131, loss=1.5074304342269897
I0214 06:01:23.941270 139474823374592 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.663218080997467, loss=1.515925407409668
I0214 06:02:50.632212 139474831767296 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.6520057916641235, loss=1.4335373640060425
I0214 06:04:20.516070 139474823374592 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.6098529696464539, loss=1.5174592733383179
I0214 06:05:47.120518 139474831767296 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.5642421245574951, loss=1.5092761516571045
I0214 06:07:15.809545 139474823374592 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.6145579218864441, loss=1.51314115524292
I0214 06:08:47.054160 139476106647296 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.6397632956504822, loss=1.4369585514068604
I0214 06:10:04.906830 139476098254592 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.6761977076530457, loss=1.4354331493377686
I0214 06:11:22.647149 139476106647296 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.6229075789451599, loss=1.5418339967727661
I0214 06:12:39.000700 139476098254592 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.8143114447593689, loss=1.494615912437439
I0214 06:14:00.234113 139476106647296 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.5826574563980103, loss=1.483322024345398
I0214 06:15:29.176881 139476098254592 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.6446507573127747, loss=1.4683469533920288
I0214 06:16:36.513988 139646656866112 spec.py:321] Evaluating on the training split.
I0214 06:17:32.371003 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 06:18:24.828116 139646656866112 spec.py:349] Evaluating on the test split.
I0214 06:18:52.226074 139646656866112 submission_runner.py:408] Time since start: 12809.23s, 	Step: 13977, 	{'train/ctc_loss': Array(0.30664057, dtype=float32), 'train/wer': 0.1145063574195897, 'validation/ctc_loss': Array(0.6024696, dtype=float32), 'validation/wer': 0.18286878361026096, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3806148, dtype=float32), 'test/wer': 0.1301362906993277, 'test/num_examples': 2472, 'score': 11584.330972671509, 'total_duration': 12809.234954595566, 'accumulated_submission_time': 11584.330972671509, 'accumulated_eval_time': 1223.8755309581757, 'accumulated_logging_time': 0.4083857536315918}
I0214 06:18:52.265996 139475968407296 logging_writer.py:48] [13977] accumulated_eval_time=1223.875531, accumulated_logging_time=0.408386, accumulated_submission_time=11584.330973, global_step=13977, preemption_count=0, score=11584.330973, test/ctc_loss=0.3806147873401642, test/num_examples=2472, test/wer=0.130136, total_duration=12809.234955, train/ctc_loss=0.3066405653953552, train/wer=0.114506, validation/ctc_loss=0.6024696230888367, validation/num_examples=5348, validation/wer=0.182869
I0214 06:19:10.513782 139475960014592 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.6349198222160339, loss=1.4764149188995361
I0214 06:20:26.468806 139475968407296 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.6298909783363342, loss=1.448917031288147
I0214 06:21:42.860471 139475960014592 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.6885577440261841, loss=1.4964189529418945
I0214 06:23:06.521018 139475968407296 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.8867102861404419, loss=1.5418146848678589
I0214 06:24:32.487079 139475960014592 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.684304416179657, loss=1.509578824043274
I0214 06:25:53.604365 139475968407296 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.7960740923881531, loss=1.449204683303833
I0214 06:27:09.729600 139475960014592 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.6772844195365906, loss=1.4595308303833008
I0214 06:28:27.802507 139475968407296 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.7016332149505615, loss=1.4689819812774658
I0214 06:29:50.784702 139475960014592 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.9664270877838135, loss=1.4983893632888794
I0214 06:31:16.710747 139475968407296 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.6894603371620178, loss=1.4859788417816162
I0214 06:32:45.547219 139475960014592 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.6671634912490845, loss=1.4614880084991455
I0214 06:34:15.780664 139475968407296 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.6402295827865601, loss=1.4332796335220337
I0214 06:35:45.884243 139475960014592 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.6010946035385132, loss=1.4359588623046875
I0214 06:37:12.125190 139475968407296 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.721487283706665, loss=1.5446442365646362
I0214 06:38:41.056882 139475960014592 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.689579427242279, loss=1.5073604583740234
I0214 06:40:08.330796 139474985367296 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.7290964722633362, loss=1.442856788635254
I0214 06:41:26.579078 139474976974592 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.6138200163841248, loss=1.4608594179153442
I0214 06:42:44.194700 139474985367296 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.5898455381393433, loss=1.3879923820495605
I0214 06:42:52.325896 139646656866112 spec.py:321] Evaluating on the training split.
I0214 06:43:47.620698 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 06:44:40.424710 139646656866112 spec.py:349] Evaluating on the test split.
I0214 06:45:06.707900 139646656866112 submission_runner.py:408] Time since start: 14383.72s, 	Step: 15712, 	{'train/ctc_loss': Array(0.28140113, dtype=float32), 'train/wer': 0.10300102676505168, 'validation/ctc_loss': Array(0.5813166, dtype=float32), 'validation/wer': 0.17756837908029774, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36267507, dtype=float32), 'test/wer': 0.12211321674486625, 'test/num_examples': 2472, 'score': 13024.305043458939, 'total_duration': 14383.716886281967, 'accumulated_submission_time': 13024.305043458939, 'accumulated_eval_time': 1358.2513551712036, 'accumulated_logging_time': 0.46280694007873535}
I0214 06:45:06.742311 139476398487296 logging_writer.py:48] [15712] accumulated_eval_time=1358.251355, accumulated_logging_time=0.462807, accumulated_submission_time=13024.305043, global_step=15712, preemption_count=0, score=13024.305043, test/ctc_loss=0.3626750707626343, test/num_examples=2472, test/wer=0.122113, total_duration=14383.716886, train/ctc_loss=0.281401127576828, train/wer=0.103001, validation/ctc_loss=0.5813165903091431, validation/num_examples=5348, validation/wer=0.177568
I0214 06:46:14.037900 139476390094592 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.6625845432281494, loss=1.4701204299926758
I0214 06:47:30.080173 139476398487296 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.7103469371795654, loss=1.430739402770996
I0214 06:48:47.294791 139476390094592 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.6638377904891968, loss=1.4987391233444214
I0214 06:50:13.811337 139476398487296 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.5996323227882385, loss=1.4370570182800293
I0214 06:51:40.844225 139476390094592 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.7175058126449585, loss=1.4325761795043945
I0214 06:53:10.658498 139476398487296 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.9194443821907043, loss=1.4848477840423584
I0214 06:54:38.890620 139476390094592 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.6839324235916138, loss=1.4923442602157593
I0214 06:56:09.511854 139475743127296 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.6845295429229736, loss=1.4224419593811035
I0214 06:57:25.618674 139475734734592 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.724718451499939, loss=1.4265042543411255
I0214 06:58:42.595810 139475743127296 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.7374365925788879, loss=1.4745969772338867
I0214 07:00:01.095039 139475734734592 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.6685376167297363, loss=1.4595924615859985
I0214 07:01:21.734902 139475743127296 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.7345796823501587, loss=1.4087547063827515
I0214 07:02:49.268301 139475734734592 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.5728051066398621, loss=1.4491444826126099
I0214 07:04:18.104117 139475743127296 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.6417295336723328, loss=1.4260414838790894
I0214 07:05:50.044622 139475734734592 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.6753533482551575, loss=1.4132596254348755
I0214 07:07:17.427711 139475743127296 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.9331541657447815, loss=1.4475430250167847
I0214 07:08:43.938049 139475734734592 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.6552360653877258, loss=1.4539319276809692
I0214 07:09:07.383997 139646656866112 spec.py:321] Evaluating on the training split.
I0214 07:10:02.804679 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 07:10:55.820825 139646656866112 spec.py:349] Evaluating on the test split.
I0214 07:11:22.106497 139646656866112 submission_runner.py:408] Time since start: 15959.12s, 	Step: 17428, 	{'train/ctc_loss': Array(0.27844656, dtype=float32), 'train/wer': 0.10552537446304172, 'validation/ctc_loss': Array(0.5747308, dtype=float32), 'validation/wer': 0.17573399499888973, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35867026, dtype=float32), 'test/wer': 0.12081327564844718, 'test/num_examples': 2472, 'score': 14464.859641313553, 'total_duration': 15959.115698099136, 'accumulated_submission_time': 14464.859641313553, 'accumulated_eval_time': 1492.9678757190704, 'accumulated_logging_time': 0.5146021842956543}
I0214 07:11:22.143295 139475743127296 logging_writer.py:48] [17428] accumulated_eval_time=1492.967876, accumulated_logging_time=0.514602, accumulated_submission_time=14464.859641, global_step=17428, preemption_count=0, score=14464.859641, test/ctc_loss=0.35867026448249817, test/num_examples=2472, test/wer=0.120813, total_duration=15959.115698, train/ctc_loss=0.2784465551376343, train/wer=0.105525, validation/ctc_loss=0.5747308135032654, validation/num_examples=5348, validation/wer=0.175734
I0214 07:12:17.694120 139475734734592 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.7708473801612854, loss=1.4489725828170776
I0214 07:13:39.790574 139476398487296 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.7301398515701294, loss=1.4632909297943115
I0214 07:14:58.223840 139476390094592 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.7473289966583252, loss=1.4691441059112549
I0214 07:16:18.347117 139476398487296 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.6623367667198181, loss=1.3742419481277466
I0214 07:17:38.973891 139476390094592 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.789878249168396, loss=1.404357671737671
I0214 07:19:05.457130 139476398487296 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.7599321007728577, loss=1.435028076171875
I0214 07:20:32.155468 139476390094592 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.7943564057350159, loss=1.3739783763885498
I0214 07:22:00.876377 139476398487296 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.6176495552062988, loss=1.4353514909744263
I0214 07:23:29.852819 139476390094592 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.653039813041687, loss=1.453718900680542
I0214 07:24:57.342983 139476398487296 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.6178988218307495, loss=1.4366282224655151
I0214 07:26:25.983383 139476390094592 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.6522558927536011, loss=1.4446887969970703
I0214 07:27:52.000133 139476398487296 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.7947606444358826, loss=1.3444197177886963
I0214 07:29:09.089170 139476390094592 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.6257711052894592, loss=1.3519465923309326
I0214 07:30:30.743695 139476398487296 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.6790314316749573, loss=1.4397644996643066
I0214 07:31:55.982149 139476390094592 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.6488215923309326, loss=1.3298118114471436
I0214 07:33:24.866847 139476398487296 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.6768501400947571, loss=1.4059785604476929
I0214 07:34:54.661405 139476390094592 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.6058924198150635, loss=1.4097373485565186
I0214 07:35:22.228324 139646656866112 spec.py:321] Evaluating on the training split.
I0214 07:36:17.881521 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 07:37:12.394856 139646656866112 spec.py:349] Evaluating on the test split.
I0214 07:37:39.525351 139646656866112 submission_runner.py:408] Time since start: 17536.53s, 	Step: 19133, 	{'train/ctc_loss': Array(0.2847909, dtype=float32), 'train/wer': 0.10319151749120813, 'validation/ctc_loss': Array(0.5575726, dtype=float32), 'validation/wer': 0.16925572279560133, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33965674, dtype=float32), 'test/wer': 0.11506509861271912, 'test/num_examples': 2472, 'score': 15904.858520269394, 'total_duration': 17536.534660100937, 'accumulated_submission_time': 15904.858520269394, 'accumulated_eval_time': 1630.2590498924255, 'accumulated_logging_time': 0.5669634342193604}
I0214 07:37:39.564035 139476398487296 logging_writer.py:48] [19133] accumulated_eval_time=1630.259050, accumulated_logging_time=0.566963, accumulated_submission_time=15904.858520, global_step=19133, preemption_count=0, score=15904.858520, test/ctc_loss=0.3396567404270172, test/num_examples=2472, test/wer=0.115065, total_duration=17536.534660, train/ctc_loss=0.28479090332984924, train/wer=0.103192, validation/ctc_loss=0.557572603225708, validation/num_examples=5348, validation/wer=0.169256
I0214 07:38:31.125637 139476390094592 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.6309427618980408, loss=1.4054681062698364
I0214 07:39:46.945581 139476398487296 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.710365355014801, loss=1.424428939819336
I0214 07:41:07.640874 139476390094592 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.7468901872634888, loss=1.4874426126480103
I0214 07:42:41.029198 139476398487296 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.6675589680671692, loss=1.4212843179702759
I0214 07:44:11.387318 139475415447296 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.7611547708511353, loss=1.3989381790161133
I0214 07:45:27.495957 139475407054592 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.8245159983634949, loss=1.3801865577697754
I0214 07:46:46.094180 139475415447296 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.7425360679626465, loss=1.390256643295288
I0214 07:48:07.709967 139475407054592 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.790239691734314, loss=1.3915379047393799
I0214 07:49:33.707926 139475415447296 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.63846755027771, loss=1.379409670829773
I0214 07:51:02.765532 139475407054592 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.671284556388855, loss=1.3404583930969238
I0214 07:52:30.491593 139475415447296 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.6927350163459778, loss=1.3565226793289185
I0214 07:54:00.788883 139475407054592 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.6475328207015991, loss=1.397149682044983
I0214 07:55:26.331737 139475415447296 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.6769364476203918, loss=1.4524903297424316
I0214 07:56:54.189790 139475407054592 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.7567596435546875, loss=1.4208649396896362
I0214 07:58:22.009736 139476398487296 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.7194002270698547, loss=1.3299789428710938
I0214 07:59:40.128562 139476390094592 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.6603098511695862, loss=1.3287019729614258
I0214 08:00:57.723312 139476398487296 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.6029259562492371, loss=1.3712681531906128
I0214 08:01:40.078866 139646656866112 spec.py:321] Evaluating on the training split.
I0214 08:02:36.202807 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 08:03:29.661016 139646656866112 spec.py:349] Evaluating on the test split.
I0214 08:03:58.419534 139646656866112 submission_runner.py:408] Time since start: 19115.43s, 	Step: 20853, 	{'train/ctc_loss': Array(0.27910382, dtype=float32), 'train/wer': 0.10145355739045558, 'validation/ctc_loss': Array(0.5391951, dtype=float32), 'validation/wer': 0.16466976259208124, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32976592, dtype=float32), 'test/wer': 0.11252615115877562, 'test/num_examples': 2472, 'score': 17345.28639960289, 'total_duration': 19115.42906999588, 'accumulated_submission_time': 17345.28639960289, 'accumulated_eval_time': 1768.594096660614, 'accumulated_logging_time': 0.622377872467041}
I0214 08:03:58.453629 139476398487296 logging_writer.py:48] [20853] accumulated_eval_time=1768.594097, accumulated_logging_time=0.622378, accumulated_submission_time=17345.286400, global_step=20853, preemption_count=0, score=17345.286400, test/ctc_loss=0.3297659158706665, test/num_examples=2472, test/wer=0.112526, total_duration=19115.429070, train/ctc_loss=0.2791038155555725, train/wer=0.101454, validation/ctc_loss=0.5391951203346252, validation/num_examples=5348, validation/wer=0.164670
I0214 08:04:34.767790 139476390094592 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.6777336001396179, loss=1.3850054740905762
I0214 08:05:50.956483 139476398487296 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.7574422359466553, loss=1.3837642669677734
I0214 08:07:09.097668 139476390094592 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.6838828325271606, loss=1.322137713432312
I0214 08:08:39.269338 139476398487296 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.8165780901908875, loss=1.3900560140609741
I0214 08:10:09.376642 139476390094592 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.7606934905052185, loss=1.3753807544708252
I0214 08:11:36.370961 139476398487296 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.9666261672973633, loss=1.3790795803070068
I0214 08:13:07.316103 139476390094592 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.5857594609260559, loss=1.3767938613891602
I0214 08:14:37.726502 139476398487296 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.7104824185371399, loss=1.3938486576080322
I0214 08:16:04.144106 139476398487296 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.695482611656189, loss=1.377426028251648
I0214 08:17:20.153763 139476390094592 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.8411045074462891, loss=1.347688913345337
I0214 08:18:38.647471 139476398487296 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.9111315011978149, loss=1.3707499504089355
I0214 08:19:57.496114 139476390094592 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.7605330348014832, loss=1.3892991542816162
I0214 08:21:23.621721 139476398487296 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.7293011546134949, loss=1.3854351043701172
I0214 08:22:54.007365 139476390094592 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.7849926352500916, loss=1.3158366680145264
I0214 08:24:23.223597 139476398487296 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.6003727316856384, loss=1.3607633113861084
I0214 08:25:49.970005 139476390094592 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.6441040635108948, loss=1.3886260986328125
I0214 08:27:21.354007 139476398487296 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.6158939599990845, loss=1.3794463872909546
I0214 08:27:58.424725 139646656866112 spec.py:321] Evaluating on the training split.
I0214 08:28:55.324304 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 08:29:47.473564 139646656866112 spec.py:349] Evaluating on the test split.
I0214 08:30:14.274393 139646656866112 submission_runner.py:408] Time since start: 20691.28s, 	Step: 22545, 	{'train/ctc_loss': Array(0.2605467, dtype=float32), 'train/wer': 0.09435127070686285, 'validation/ctc_loss': Array(0.52861744, dtype=float32), 'validation/wer': 0.16012242100080135, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.313901, dtype=float32), 'test/wer': 0.1081591615379928, 'test/num_examples': 2472, 'score': 18785.17260837555, 'total_duration': 20691.28274512291, 'accumulated_submission_time': 18785.17260837555, 'accumulated_eval_time': 1904.4369497299194, 'accumulated_logging_time': 0.6713097095489502}
I0214 08:30:14.309937 139476398487296 logging_writer.py:48] [22545] accumulated_eval_time=1904.436950, accumulated_logging_time=0.671310, accumulated_submission_time=18785.172608, global_step=22545, preemption_count=0, score=18785.172608, test/ctc_loss=0.31390100717544556, test/num_examples=2472, test/wer=0.108159, total_duration=20691.282745, train/ctc_loss=0.2605467140674591, train/wer=0.094351, validation/ctc_loss=0.5286174416542053, validation/num_examples=5348, validation/wer=0.160122
I0214 08:30:56.735397 139476390094592 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.6707510352134705, loss=1.4425733089447021
I0214 08:32:16.686591 139476398487296 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.6207846403121948, loss=1.3652331829071045
I0214 08:33:34.446187 139476390094592 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.6940380930900574, loss=1.3524342775344849
I0214 08:34:54.866801 139476398487296 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.6591643691062927, loss=1.3467134237289429
I0214 08:36:13.858260 139476390094592 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.7500900626182556, loss=1.3848950862884521
I0214 08:37:37.779537 139476398487296 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.6652138829231262, loss=1.310542345046997
I0214 08:39:05.430684 139476390094592 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.8461139798164368, loss=1.3804751634597778
I0214 08:40:33.284922 139476398487296 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.6331600546836853, loss=1.3353233337402344
I0214 08:42:04.159968 139476390094592 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.6697234511375427, loss=1.3630911111831665
I0214 08:43:34.681245 139476398487296 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.6767328381538391, loss=1.2989760637283325
I0214 08:45:05.456013 139476390094592 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.6709873676300049, loss=1.337644100189209
I0214 08:46:37.443072 139476398487296 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.6274287700653076, loss=1.2924437522888184
I0214 08:47:54.106081 139476390094592 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.677628219127655, loss=1.2891658544540405
I0214 08:49:10.339841 139476398487296 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.8966541290283203, loss=1.312726616859436
I0214 08:50:27.934301 139476390094592 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.6635670065879822, loss=1.3209835290908813
I0214 08:51:53.466912 139476398487296 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.7091202735900879, loss=1.3852852582931519
I0214 08:53:21.148180 139476390094592 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.735482931137085, loss=1.3173495531082153
I0214 08:54:14.597239 139646656866112 spec.py:321] Evaluating on the training split.
I0214 08:55:09.663507 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 08:56:03.644079 139646656866112 spec.py:349] Evaluating on the test split.
I0214 08:56:30.050045 139646656866112 submission_runner.py:408] Time since start: 22267.06s, 	Step: 24263, 	{'train/ctc_loss': Array(0.2430587, dtype=float32), 'train/wer': 0.09053671719944259, 'validation/ctc_loss': Array(0.5190067, dtype=float32), 'validation/wer': 0.15627021442984446, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31107822, dtype=float32), 'test/wer': 0.10448276562468263, 'test/num_examples': 2472, 'score': 20225.373789548874, 'total_duration': 22267.058785676956, 'accumulated_submission_time': 20225.373789548874, 'accumulated_eval_time': 2039.88334441185, 'accumulated_logging_time': 0.7225484848022461}
I0214 08:56:30.086697 139475599767296 logging_writer.py:48] [24263] accumulated_eval_time=2039.883344, accumulated_logging_time=0.722548, accumulated_submission_time=20225.373790, global_step=24263, preemption_count=0, score=20225.373790, test/ctc_loss=0.3110782206058502, test/num_examples=2472, test/wer=0.104483, total_duration=22267.058786, train/ctc_loss=0.2430586963891983, train/wer=0.090537, validation/ctc_loss=0.5190067291259766, validation/num_examples=5348, validation/wer=0.156270
I0214 08:56:58.843112 139475591374592 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.5827775001525879, loss=1.2774245738983154
I0214 08:58:14.888303 139475599767296 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.6489621996879578, loss=1.3267409801483154
I0214 08:59:31.040444 139475591374592 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.7052378058433533, loss=1.3101180791854858
I0214 09:00:59.827839 139475599767296 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.7225907444953918, loss=1.3753557205200195
I0214 09:02:27.792283 139475591374592 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.6784052848815918, loss=1.3097670078277588
I0214 09:03:50.062395 139475599767296 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.7277707457542419, loss=1.3460038900375366
I0214 09:05:06.085777 139475591374592 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.6425148248672485, loss=1.2946113348007202
I0214 09:06:26.363684 139475599767296 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.6962601542472839, loss=1.3234633207321167
I0214 09:07:48.691822 139475591374592 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.6073204278945923, loss=1.2862145900726318
I0214 09:09:14.459659 139475599767296 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.842093288898468, loss=1.3714956045150757
I0214 09:10:44.665145 139475591374592 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.759507954120636, loss=1.3705635070800781
I0214 09:12:15.362025 139475599767296 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.6128303408622742, loss=1.317126750946045
I0214 09:13:45.226947 139475591374592 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.6561484932899475, loss=1.3671585321426392
I0214 09:15:14.032393 139475599767296 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.7352250814437866, loss=1.369634747505188
I0214 09:16:41.079491 139475591374592 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.8187825083732605, loss=1.3271280527114868
I0214 09:18:08.024374 139474944407296 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.8329213857650757, loss=1.2904202938079834
I0214 09:19:27.393989 139474936014592 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.7528057098388672, loss=1.2518649101257324
I0214 09:20:30.681814 139646656866112 spec.py:321] Evaluating on the training split.
I0214 09:21:25.821688 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 09:22:18.360980 139646656866112 spec.py:349] Evaluating on the test split.
I0214 09:22:45.277200 139646656866112 submission_runner.py:408] Time since start: 23842.29s, 	Step: 25979, 	{'train/ctc_loss': Array(0.22930108, dtype=float32), 'train/wer': 0.08657502803933331, 'validation/ctc_loss': Array(0.51106757, dtype=float32), 'validation/wer': 0.15343174643019203, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3014969, dtype=float32), 'test/wer': 0.10139540552068735, 'test/num_examples': 2472, 'score': 21665.884313106537, 'total_duration': 23842.28617978096, 'accumulated_submission_time': 21665.884313106537, 'accumulated_eval_time': 2174.472556114197, 'accumulated_logging_time': 0.7736678123474121}
I0214 09:22:45.316328 139476398487296 logging_writer.py:48] [25979] accumulated_eval_time=2174.472556, accumulated_logging_time=0.773668, accumulated_submission_time=21665.884313, global_step=25979, preemption_count=0, score=21665.884313, test/ctc_loss=0.3014968931674957, test/num_examples=2472, test/wer=0.101395, total_duration=23842.286180, train/ctc_loss=0.2293010801076889, train/wer=0.086575, validation/ctc_loss=0.5110675692558289, validation/num_examples=5348, validation/wer=0.153432
I0214 09:23:02.029075 139476390094592 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.7925744652748108, loss=1.289414882659912
I0214 09:24:17.856564 139476398487296 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.7336763143539429, loss=1.3104405403137207
I0214 09:25:33.780602 139476390094592 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.6764068603515625, loss=1.3585988283157349
I0214 09:26:55.730555 139476398487296 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.8184269666671753, loss=1.315706491470337
I0214 09:28:25.256594 139476390094592 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.6203249096870422, loss=1.295748233795166
I0214 09:29:53.077056 139476398487296 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.6720820665359497, loss=1.2719006538391113
I0214 09:31:21.182838 139476390094592 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.7511588931083679, loss=1.285585880279541
I0214 09:32:50.750381 139476398487296 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.8236296772956848, loss=1.3296746015548706
I0214 09:34:19.236838 139475640727296 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.649039626121521, loss=1.2679752111434937
I0214 09:35:35.596826 139475632334592 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.7052075266838074, loss=1.308188796043396
I0214 09:36:53.153995 139475640727296 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.8219254016876221, loss=1.3113549947738647
I0214 09:38:10.505258 139475632334592 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.6581592559814453, loss=1.2874647378921509
I0214 09:39:36.169116 139475640727296 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.7007243037223816, loss=1.3411052227020264
I0214 09:41:00.535673 139475632334592 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.7961873412132263, loss=1.3577542304992676
I0214 09:42:27.003203 139475640727296 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.7233062982559204, loss=1.3237336874008179
I0214 09:43:54.266185 139475632334592 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.6783396601676941, loss=1.354761004447937
I0214 09:45:23.997678 139475640727296 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.7888911366462708, loss=1.3350545167922974
I0214 09:46:45.906718 139646656866112 spec.py:321] Evaluating on the training split.
I0214 09:47:44.363283 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 09:48:37.149496 139646656866112 spec.py:349] Evaluating on the test split.
I0214 09:49:05.103284 139646656866112 submission_runner.py:408] Time since start: 25422.11s, 	Step: 27691, 	{'train/ctc_loss': Array(0.22043964, dtype=float32), 'train/wer': 0.07960438166736558, 'validation/ctc_loss': Array(0.49129182, dtype=float32), 'validation/wer': 0.14787066626760767, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2954965, dtype=float32), 'test/wer': 0.09869396542969147, 'test/num_examples': 2472, 'score': 23106.38948559761, 'total_duration': 25422.112336874008, 'accumulated_submission_time': 23106.38948559761, 'accumulated_eval_time': 2313.663006067276, 'accumulated_logging_time': 0.8280179500579834}
I0214 09:49:05.140772 139475640727296 logging_writer.py:48] [27691] accumulated_eval_time=2313.663006, accumulated_logging_time=0.828018, accumulated_submission_time=23106.389486, global_step=27691, preemption_count=0, score=23106.389486, test/ctc_loss=0.29549649357795715, test/num_examples=2472, test/wer=0.098694, total_duration=25422.112337, train/ctc_loss=0.22043964266777039, train/wer=0.079604, validation/ctc_loss=0.4912918210029602, validation/num_examples=5348, validation/wer=0.147871
I0214 09:49:12.784356 139475632334592 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.7658151388168335, loss=1.332485556602478
I0214 09:50:28.442728 139475640727296 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.0099496841430664, loss=1.33938467502594
I0214 09:51:49.864080 139475313047296 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.7043160200119019, loss=1.2290416955947876
I0214 09:53:11.058454 139475304654592 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.5649327039718628, loss=1.260420799255371
I0214 09:54:33.925215 139475313047296 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.6548857688903809, loss=1.2675436735153198
I0214 09:55:59.070208 139475304654592 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.7180008292198181, loss=1.3030658960342407
I0214 09:57:27.472194 139475313047296 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.6955161690711975, loss=1.3320690393447876
I0214 09:58:53.321285 139475304654592 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.622118353843689, loss=1.3021076917648315
I0214 10:00:22.517334 139475313047296 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.9263137578964233, loss=1.326216220855713
I0214 10:01:52.510263 139475304654592 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.7398732304573059, loss=1.3948310613632202
I0214 10:03:18.947395 139475313047296 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.7525298595428467, loss=1.3187624216079712
I0214 10:04:50.313549 139475304654592 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.7338684797286987, loss=1.285709023475647
I0214 10:06:15.883507 139474657687296 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.8411005735397339, loss=1.262251377105713
I0214 10:07:34.674139 139474649294592 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.6791910529136658, loss=1.280182957649231
I0214 10:08:56.415066 139474657687296 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.6735059022903442, loss=1.27584969997406
I0214 10:10:15.252151 139474649294592 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.6940626502037048, loss=1.256798267364502
I0214 10:11:39.314850 139474657687296 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.6900745630264282, loss=1.3018128871917725
I0214 10:13:05.981325 139646656866112 spec.py:321] Evaluating on the training split.
I0214 10:14:00.242464 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 10:14:54.051790 139646656866112 spec.py:349] Evaluating on the test split.
I0214 10:15:21.498501 139646656866112 submission_runner.py:408] Time since start: 26998.51s, 	Step: 29397, 	{'train/ctc_loss': Array(0.23616512, dtype=float32), 'train/wer': 0.08828978159126365, 'validation/ctc_loss': Array(0.48435923, dtype=float32), 'validation/wer': 0.14642246830860134, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28681365, dtype=float32), 'test/wer': 0.09666280746653667, 'test/num_examples': 2472, 'score': 24547.143973588943, 'total_duration': 26998.507881879807, 'accumulated_submission_time': 24547.143973588943, 'accumulated_eval_time': 2449.1744163036346, 'accumulated_logging_time': 0.8814880847930908}
I0214 10:15:21.535530 139474365847296 logging_writer.py:48] [29397] accumulated_eval_time=2449.174416, accumulated_logging_time=0.881488, accumulated_submission_time=24547.143974, global_step=29397, preemption_count=0, score=24547.143974, test/ctc_loss=0.2868136465549469, test/num_examples=2472, test/wer=0.096663, total_duration=26998.507882, train/ctc_loss=0.2361651211977005, train/wer=0.088290, validation/ctc_loss=0.4843592345714569, validation/num_examples=5348, validation/wer=0.146422
I0214 10:15:24.675518 139474357454592 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.679656982421875, loss=1.264579176902771
I0214 10:16:40.289590 139474365847296 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.6688042283058167, loss=1.3333709239959717
I0214 10:17:56.783543 139474357454592 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.6449133157730103, loss=1.2903215885162354
I0214 10:19:22.363106 139474365847296 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.7060781717300415, loss=1.2446192502975464
I0214 10:20:49.251907 139474357454592 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.7292889356613159, loss=1.3437042236328125
I0214 10:22:16.492457 139476398487296 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.6698968410491943, loss=1.2843555212020874
I0214 10:23:34.106103 139476390094592 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.7022910714149475, loss=1.2767997980117798
I0214 10:24:51.792031 139476398487296 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.7813894748687744, loss=1.2646890878677368
I0214 10:26:11.941325 139476390094592 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.6762809753417969, loss=1.3263224363327026
I0214 10:27:36.119110 139476398487296 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.5880113840103149, loss=1.2964733839035034
I0214 10:29:02.476032 139476390094592 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.6909936666488647, loss=1.2500993013381958
I0214 10:30:33.890876 139476398487296 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.850421130657196, loss=1.262406587600708
I0214 10:32:03.299991 139476390094592 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.7244070172309875, loss=1.2675353288650513
I0214 10:33:30.726225 139476398487296 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.8992986083030701, loss=1.3330087661743164
I0214 10:35:02.801638 139476390094592 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.764682948589325, loss=1.2545617818832397
I0214 10:36:33.973108 139475640727296 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.8692728877067566, loss=1.3017042875289917
I0214 10:37:51.659748 139475632334592 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.7776438593864441, loss=1.3069028854370117
I0214 10:39:08.722444 139475640727296 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.7330698370933533, loss=1.2662228345870972
I0214 10:39:22.136238 139646656866112 spec.py:321] Evaluating on the training split.
I0214 10:40:17.078111 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 10:41:10.699900 139646656866112 spec.py:349] Evaluating on the test split.
I0214 10:41:37.895844 139646656866112 submission_runner.py:408] Time since start: 28574.90s, 	Step: 31119, 	{'train/ctc_loss': Array(0.21892151, dtype=float32), 'train/wer': 0.07905631609504121, 'validation/ctc_loss': Array(0.47866896, dtype=float32), 'validation/wer': 0.14374812941096962, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28513002, dtype=float32), 'test/wer': 0.09580972112201167, 'test/num_examples': 2472, 'score': 25987.656771183014, 'total_duration': 28574.90484571457, 'accumulated_submission_time': 25987.656771183014, 'accumulated_eval_time': 2584.9278602600098, 'accumulated_logging_time': 0.9351787567138672}
I0214 10:41:37.932980 139475640727296 logging_writer.py:48] [31119] accumulated_eval_time=2584.927860, accumulated_logging_time=0.935179, accumulated_submission_time=25987.656771, global_step=31119, preemption_count=0, score=25987.656771, test/ctc_loss=0.28513002395629883, test/num_examples=2472, test/wer=0.095810, total_duration=28574.904846, train/ctc_loss=0.2189215123653412, train/wer=0.079056, validation/ctc_loss=0.4786689579486847, validation/num_examples=5348, validation/wer=0.143748
I0214 10:42:40.209445 139475632334592 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.6728823184967041, loss=1.255659818649292
I0214 10:43:56.108255 139475640727296 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.8132601380348206, loss=1.28331458568573
I0214 10:45:13.559829 139475632334592 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.7105425596237183, loss=1.2424497604370117
I0214 10:46:44.003597 139475640727296 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.7076190710067749, loss=1.2614563703536987
I0214 10:48:12.084048 139475632334592 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.6885778903961182, loss=1.256766676902771
I0214 10:49:39.571982 139475640727296 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.6929140090942383, loss=1.2640397548675537
I0214 10:51:08.647879 139475632334592 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.7790597677230835, loss=1.285412311553955
I0214 10:52:34.299868 139475640727296 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.6791263818740845, loss=1.2947678565979004
I0214 10:53:57.220494 139476398487296 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.6996263861656189, loss=1.2410575151443481
I0214 10:55:14.020868 139476390094592 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.6858779788017273, loss=1.2047700881958008
I0214 10:56:33.853240 139476398487296 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.7415090203285217, loss=1.3062931299209595
I0214 10:57:56.541888 139476390094592 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.7475968599319458, loss=1.1853801012039185
I0214 10:59:20.701551 139476398487296 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.767127513885498, loss=1.2794668674468994
I0214 11:00:49.678447 139476390094592 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.748280942440033, loss=1.2754383087158203
I0214 11:02:18.012599 139476398487296 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.7879483699798584, loss=1.3428456783294678
I0214 11:03:47.845931 139476390094592 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.7196763753890991, loss=1.258217453956604
I0214 11:05:16.971265 139476398487296 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.8899346590042114, loss=1.2691253423690796
I0214 11:05:38.245091 139646656866112 spec.py:321] Evaluating on the training split.
I0214 11:06:36.110446 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 11:07:30.009070 139646656866112 spec.py:349] Evaluating on the test split.
I0214 11:07:56.684502 139646656866112 submission_runner.py:408] Time since start: 30153.69s, 	Step: 32826, 	{'train/ctc_loss': Array(0.22863874, dtype=float32), 'train/wer': 0.08293051281919565, 'validation/ctc_loss': Array(0.47382867, dtype=float32), 'validation/wer': 0.14113171843169817, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27610952, dtype=float32), 'test/wer': 0.09363638210143603, 'test/num_examples': 2472, 'score': 27427.883678913116, 'total_duration': 30153.691357135773, 'accumulated_submission_time': 27427.883678913116, 'accumulated_eval_time': 2723.3589627742767, 'accumulated_logging_time': 0.9874227046966553}
I0214 11:07:56.731016 139475676567296 logging_writer.py:48] [32826] accumulated_eval_time=2723.358963, accumulated_logging_time=0.987423, accumulated_submission_time=27427.883679, global_step=32826, preemption_count=0, score=27427.883679, test/ctc_loss=0.276109516620636, test/num_examples=2472, test/wer=0.093636, total_duration=30153.691357, train/ctc_loss=0.22863873839378357, train/wer=0.082931, validation/ctc_loss=0.47382867336273193, validation/num_examples=5348, validation/wer=0.141132
I0214 11:08:53.422339 139475668174592 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.772175669670105, loss=1.269224762916565
I0214 11:10:13.680032 139475021207296 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.6235414743423462, loss=1.2316068410873413
I0214 11:11:30.349370 139475012814592 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.6394335627555847, loss=1.2539933919906616
I0214 11:12:52.072478 139475021207296 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.7470538020133972, loss=1.23648202419281
I0214 11:14:14.628132 139475012814592 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.7969663143157959, loss=1.2976938486099243
I0214 11:15:43.578714 139475021207296 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.6904364228248596, loss=1.2742352485656738
I0214 11:17:11.579030 139475012814592 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.8335602879524231, loss=1.3082314729690552
I0214 11:18:36.865685 139475021207296 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.6559368968009949, loss=1.2322884798049927
I0214 11:20:08.945285 139475012814592 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.795448362827301, loss=1.239357590675354
I0214 11:21:37.065504 139475021207296 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.6737332344055176, loss=1.2976152896881104
I0214 11:23:08.247828 139475012814592 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.6603475213050842, loss=1.2443556785583496
I0214 11:24:40.531934 139475021207296 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.7105501294136047, loss=1.2134673595428467
I0214 11:25:57.003869 139475012814592 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.780342161655426, loss=1.2360796928405762
I0214 11:27:12.949838 139475021207296 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.7639971971511841, loss=1.230150818824768
I0214 11:28:33.591004 139475012814592 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.7472996711730957, loss=1.2966227531433105
I0214 11:29:56.194638 139475021207296 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.720500648021698, loss=1.2433173656463623
I0214 11:31:23.266982 139475012814592 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.80687415599823, loss=1.2444791793823242
I0214 11:31:56.685239 139646656866112 spec.py:321] Evaluating on the training split.
I0214 11:32:52.132299 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 11:33:44.461549 139646656866112 spec.py:349] Evaluating on the test split.
I0214 11:34:11.623239 139646656866112 submission_runner.py:408] Time since start: 31728.63s, 	Step: 34538, 	{'train/ctc_loss': Array(0.21834032, dtype=float32), 'train/wer': 0.07759732301757302, 'validation/ctc_loss': Array(0.46095514, dtype=float32), 'validation/wer': 0.1372602025546212, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26683512, dtype=float32), 'test/wer': 0.09014279040480978, 'test/num_examples': 2472, 'score': 28867.750519037247, 'total_duration': 31728.63215994835, 'accumulated_submission_time': 28867.750519037247, 'accumulated_eval_time': 2858.2907309532166, 'accumulated_logging_time': 1.050553798675537}
I0214 11:34:11.666746 139475021207296 logging_writer.py:48] [34538] accumulated_eval_time=2858.290731, accumulated_logging_time=1.050554, accumulated_submission_time=28867.750519, global_step=34538, preemption_count=0, score=28867.750519, test/ctc_loss=0.26683512330055237, test/num_examples=2472, test/wer=0.090143, total_duration=31728.632160, train/ctc_loss=0.21834032237529755, train/wer=0.077597, validation/ctc_loss=0.4609551429748535, validation/num_examples=5348, validation/wer=0.137260
I0214 11:34:59.396138 139475012814592 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.749584972858429, loss=1.2740966081619263
I0214 11:36:15.238182 139475021207296 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.7657206654548645, loss=1.2205616235733032
I0214 11:37:39.794468 139475012814592 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.8320533633232117, loss=1.2518174648284912
I0214 11:39:04.828863 139475021207296 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.8479267358779907, loss=1.237962245941162
I0214 11:40:35.068513 139475012814592 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.7188041806221008, loss=1.3044408559799194
I0214 11:41:57.157448 139475021207296 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.7825959920883179, loss=1.2605655193328857
I0214 11:43:13.277542 139475012814592 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.6648475527763367, loss=1.2247724533081055
I0214 11:44:31.812259 139475021207296 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.8299707770347595, loss=1.2448049783706665
I0214 11:45:56.191118 139475012814592 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.6851922273635864, loss=1.207794427871704
I0214 11:47:24.394992 139475021207296 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.8921618461608887, loss=1.2065589427947998
I0214 11:48:53.523074 139475012814592 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.7528281211853027, loss=1.256159782409668
I0214 11:50:23.580594 139475021207296 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.6674421429634094, loss=1.2110694646835327
I0214 11:51:53.868542 139475012814592 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.7710812091827393, loss=1.1996709108352661
I0214 11:53:22.998481 139475021207296 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.7229404449462891, loss=1.242954134941101
I0214 11:54:54.299965 139475012814592 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.7465160489082336, loss=1.280320405960083
I0214 11:56:20.096649 139475676567296 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.6750094294548035, loss=1.189629077911377
I0214 11:57:35.996188 139475668174592 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.7235646843910217, loss=1.2567498683929443
I0214 11:58:12.223016 139646656866112 spec.py:321] Evaluating on the training split.
I0214 11:59:10.580755 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 12:00:05.675787 139646656866112 spec.py:349] Evaluating on the test split.
I0214 12:00:32.313368 139646656866112 submission_runner.py:408] Time since start: 33309.32s, 	Step: 36249, 	{'train/ctc_loss': Array(0.17301418, dtype=float32), 'train/wer': 0.06496827690825416, 'validation/ctc_loss': Array(0.4529431, dtype=float32), 'validation/wer': 0.13483688463655058, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26671857, dtype=float32), 'test/wer': 0.08983811671033656, 'test/num_examples': 2472, 'score': 30308.221432209015, 'total_duration': 33309.32222747803, 'accumulated_submission_time': 30308.221432209015, 'accumulated_eval_time': 2998.3748049736023, 'accumulated_logging_time': 1.1091015338897705}
I0214 12:00:32.353067 139476106647296 logging_writer.py:48] [36249] accumulated_eval_time=2998.374805, accumulated_logging_time=1.109102, accumulated_submission_time=30308.221432, global_step=36249, preemption_count=0, score=30308.221432, test/ctc_loss=0.2667185664176941, test/num_examples=2472, test/wer=0.089838, total_duration=33309.322227, train/ctc_loss=0.17301417887210846, train/wer=0.064968, validation/ctc_loss=0.4529430866241455, validation/num_examples=5348, validation/wer=0.134837
I0214 12:01:11.719105 139476098254592 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.7476481795310974, loss=1.244766354560852
I0214 12:02:27.640121 139476106647296 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.8565866351127625, loss=1.1818116903305054
I0214 12:03:43.630761 139476098254592 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.7026943564414978, loss=1.239956021308899
I0214 12:05:09.731247 139476106647296 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.7223771214485168, loss=1.222138524055481
I0214 12:06:39.605834 139476098254592 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.9052442908287048, loss=1.214461088180542
I0214 12:08:06.244535 139476106647296 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.7476339936256409, loss=1.2192438840866089
I0214 12:09:33.574894 139476098254592 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.7670276761054993, loss=1.1891303062438965
I0214 12:11:03.876510 139476106647296 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.6983261704444885, loss=1.3295918703079224
I0214 12:12:32.671880 139476106647296 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.905154824256897, loss=1.2199203968048096
I0214 12:13:51.063075 139476098254592 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.6700625419616699, loss=1.2357548475265503
I0214 12:15:10.206451 139476106647296 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.6099799275398254, loss=1.204820990562439
I0214 12:16:27.109137 139476098254592 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.8290033340454102, loss=1.1827175617218018
I0214 12:17:50.976493 139476106647296 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.7688888907432556, loss=1.2291477918624878
I0214 12:19:18.501970 139476098254592 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.6859110593795776, loss=1.1297279596328735
I0214 12:20:46.999061 139476106647296 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.6699265241622925, loss=1.240716576576233
I0214 12:22:16.869680 139476098254592 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.626104474067688, loss=1.2038356065750122
I0214 12:23:45.248333 139476106647296 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.8340644240379333, loss=1.204351782798767
I0214 12:24:32.783248 139646656866112 spec.py:321] Evaluating on the training split.
I0214 12:25:28.043148 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 12:26:22.573468 139646656866112 spec.py:349] Evaluating on the test split.
I0214 12:26:49.189173 139646656866112 submission_runner.py:408] Time since start: 34886.20s, 	Step: 37952, 	{'train/ctc_loss': Array(0.19108309, dtype=float32), 'train/wer': 0.06909040991742518, 'validation/ctc_loss': Array(0.43196315, dtype=float32), 'validation/wer': 0.13027988839221064, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25399256, dtype=float32), 'test/wer': 0.08425243231166088, 'test/num_examples': 2472, 'score': 31748.563665390015, 'total_duration': 34886.196565151215, 'accumulated_submission_time': 31748.563665390015, 'accumulated_eval_time': 3134.772925376892, 'accumulated_logging_time': 1.1662836074829102}
I0214 12:26:49.235669 139476106647296 logging_writer.py:48] [37952] accumulated_eval_time=3134.772925, accumulated_logging_time=1.166284, accumulated_submission_time=31748.563665, global_step=37952, preemption_count=0, score=31748.563665, test/ctc_loss=0.25399255752563477, test/num_examples=2472, test/wer=0.084252, total_duration=34886.196565, train/ctc_loss=0.19108308851718903, train/wer=0.069090, validation/ctc_loss=0.43196314573287964, validation/num_examples=5348, validation/wer=0.130280
I0214 12:27:26.264315 139476098254592 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.6965922117233276, loss=1.2234735488891602
I0214 12:28:42.412897 139476106647296 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.8095274567604065, loss=1.234104037284851
I0214 12:30:02.707598 139476106647296 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.6266599297523499, loss=1.1956623792648315
I0214 12:31:19.381156 139476098254592 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.8614739179611206, loss=1.2261215448379517
I0214 12:32:39.746700 139476106647296 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.7481023073196411, loss=1.281857967376709
I0214 12:34:05.092260 139476098254592 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.7519235610961914, loss=1.2404005527496338
I0214 12:35:33.165060 139476106647296 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.6267984509468079, loss=1.1531031131744385
I0214 12:37:00.736011 139476098254592 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.7997973561286926, loss=1.2131491899490356
I0214 12:38:28.175197 139476106647296 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.7766658663749695, loss=1.2140800952911377
I0214 12:39:58.309867 139476098254592 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.7857564687728882, loss=1.2218670845031738
I0214 12:41:26.231746 139476106647296 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.7880033254623413, loss=1.2556325197219849
I0214 12:42:57.359713 139476098254592 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.7870161533355713, loss=1.1866282224655151
I0214 12:44:22.568449 139476106647296 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.7489702701568604, loss=1.226070523262024
I0214 12:45:39.056260 139476098254592 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.6354718804359436, loss=1.2321373224258423
I0214 12:46:56.910397 139476106647296 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.8701677918434143, loss=1.2173597812652588
I0214 12:48:18.376178 139476098254592 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.8585838079452515, loss=1.2092043161392212
I0214 12:49:46.607910 139476106647296 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.7041589617729187, loss=1.1629846096038818
I0214 12:50:49.350392 139646656866112 spec.py:321] Evaluating on the training split.
I0214 12:51:44.200985 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 12:52:37.026364 139646656866112 spec.py:349] Evaluating on the test split.
I0214 12:53:05.038589 139646656866112 submission_runner.py:408] Time since start: 36462.05s, 	Step: 39671, 	{'train/ctc_loss': Array(0.23817594, dtype=float32), 'train/wer': 0.08681242982594121, 'validation/ctc_loss': Array(0.43086866, dtype=float32), 'validation/wer': 0.12896685557604487, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24960111, dtype=float32), 'test/wer': 0.08329778806897813, 'test/num_examples': 2472, 'score': 33188.59181380272, 'total_duration': 36462.046426296234, 'accumulated_submission_time': 33188.59181380272, 'accumulated_eval_time': 3270.4538078308105, 'accumulated_logging_time': 1.2276201248168945}
I0214 12:53:05.082766 139476106647296 logging_writer.py:48] [39671] accumulated_eval_time=3270.453808, accumulated_logging_time=1.227620, accumulated_submission_time=33188.591814, global_step=39671, preemption_count=0, score=33188.591814, test/ctc_loss=0.2496011108160019, test/num_examples=2472, test/wer=0.083298, total_duration=36462.046426, train/ctc_loss=0.23817594349384308, train/wer=0.086812, validation/ctc_loss=0.43086865544319153, validation/num_examples=5348, validation/wer=0.128967
I0214 12:53:27.776266 139476098254592 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.6822540163993835, loss=1.192430853843689
I0214 12:54:43.582519 139476106647296 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.8854154944419861, loss=1.1487196683883667
I0214 12:56:00.011380 139476098254592 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.8687686920166016, loss=1.2482436895370483
I0214 12:57:29.618889 139476106647296 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.9390360713005066, loss=1.2340128421783447
I0214 12:58:57.894445 139476098254592 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.7908786535263062, loss=1.1690174341201782
I0214 13:00:28.305224 139476106647296 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.7000589370727539, loss=1.2322055101394653
I0214 13:01:46.156684 139476098254592 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.7286467552185059, loss=1.18954336643219
I0214 13:03:04.056221 139476106647296 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.7542413473129272, loss=1.185739278793335
I0214 13:04:28.036680 139476098254592 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.7927319407463074, loss=1.1514148712158203
I0214 13:05:54.802932 139476106647296 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.7294973134994507, loss=1.172688364982605
I0214 13:07:20.065466 139476098254592 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.9425666332244873, loss=1.233237624168396
I0214 13:08:49.266874 139476106647296 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.776933491230011, loss=1.2191593647003174
I0214 13:10:15.912114 139476098254592 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.7453343868255615, loss=1.1961183547973633
I0214 13:11:43.119054 139476106647296 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.9384170174598694, loss=1.2465465068817139
I0214 13:13:12.733761 139476098254592 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.7673554420471191, loss=1.1839255094528198
I0214 13:14:45.894460 139476106647296 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.8269171118736267, loss=1.2269459962844849
I0214 13:16:03.452599 139476098254592 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.7754207849502563, loss=1.166245698928833
I0214 13:17:05.261539 139646656866112 spec.py:321] Evaluating on the training split.
I0214 13:17:58.925118 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 13:18:51.184316 139646656866112 spec.py:349] Evaluating on the test split.
I0214 13:19:17.929891 139646656866112 submission_runner.py:408] Time since start: 38034.94s, 	Step: 41382, 	{'train/ctc_loss': Array(0.2418203, dtype=float32), 'train/wer': 0.08833832027845473, 'validation/ctc_loss': Array(0.4284742, dtype=float32), 'validation/wer': 0.1280110449231007, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24681735, dtype=float32), 'test/wer': 0.0825665712022424, 'test/num_examples': 2472, 'score': 34628.68599200249, 'total_duration': 38034.938487529755, 'accumulated_submission_time': 34628.68599200249, 'accumulated_eval_time': 3403.1156027317047, 'accumulated_logging_time': 1.2870965003967285}
I0214 13:19:17.974390 139475517843200 logging_writer.py:48] [41382] accumulated_eval_time=3403.115603, accumulated_logging_time=1.287097, accumulated_submission_time=34628.685992, global_step=41382, preemption_count=0, score=34628.685992, test/ctc_loss=0.24681735038757324, test/num_examples=2472, test/wer=0.082567, total_duration=38034.938488, train/ctc_loss=0.2418203055858612, train/wer=0.088338, validation/ctc_loss=0.42847418785095215, validation/num_examples=5348, validation/wer=0.128011
I0214 13:19:32.615488 139475509450496 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.8443343639373779, loss=1.159287691116333
I0214 13:20:48.759292 139475517843200 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.7479455471038818, loss=1.1949329376220703
I0214 13:22:05.131930 139475509450496 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.7854859828948975, loss=1.190374493598938
I0214 13:23:26.031114 139475517843200 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.7144778370857239, loss=1.2019715309143066
I0214 13:24:51.025003 139475509450496 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.8975026607513428, loss=1.1750311851501465
I0214 13:26:16.464777 139475517843200 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.7717154026031494, loss=1.1863497495651245
I0214 13:27:44.094721 139475509450496 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.8229836225509644, loss=1.1818954944610596
I0214 13:29:11.998683 139475517843200 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.7925432920455933, loss=1.2041783332824707
I0214 13:30:39.686053 139475509450496 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.7484960556030273, loss=1.2471956014633179
I0214 13:32:03.061035 139475517843200 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.7308969497680664, loss=1.2204114198684692
I0214 13:33:19.850545 139475509450496 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.7274019718170166, loss=1.1458375453948975
I0214 13:34:36.479493 139475517843200 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.7575330138206482, loss=1.1983940601348877
I0214 13:35:58.312234 139475509450496 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.7548731565475464, loss=1.2181862592697144
I0214 13:37:25.050621 139475517843200 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.8591305613517761, loss=1.126668095588684
I0214 13:38:51.619832 139475509450496 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.7128049731254578, loss=1.183078646659851
I0214 13:40:17.645570 139475517843200 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.7679240107536316, loss=1.1966586112976074
I0214 13:41:48.377269 139475509450496 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.7336333394050598, loss=1.173143744468689
I0214 13:43:16.063704 139475517843200 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.8176169991493225, loss=1.1302309036254883
I0214 13:43:18.162737 139646656866112 spec.py:321] Evaluating on the training split.
I0214 13:44:10.587938 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 13:45:03.049385 139646656866112 spec.py:349] Evaluating on the test split.
I0214 13:45:30.343914 139646656866112 submission_runner.py:408] Time since start: 39607.35s, 	Step: 43104, 	{'train/ctc_loss': Array(0.2755499, dtype=float32), 'train/wer': 0.10171335645886834, 'validation/ctc_loss': Array(0.4144993, dtype=float32), 'validation/wer': 0.12386919876034255, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23967984, dtype=float32), 'test/wer': 0.08124631852619178, 'test/num_examples': 2472, 'score': 36068.79007220268, 'total_duration': 39607.35124826431, 'accumulated_submission_time': 36068.79007220268, 'accumulated_eval_time': 3535.2889487743378, 'accumulated_logging_time': 1.3462481498718262}
I0214 13:45:30.386369 139475517843200 logging_writer.py:48] [43104] accumulated_eval_time=3535.288949, accumulated_logging_time=1.346248, accumulated_submission_time=36068.790072, global_step=43104, preemption_count=0, score=36068.790072, test/ctc_loss=0.23967984318733215, test/num_examples=2472, test/wer=0.081246, total_duration=39607.351248, train/ctc_loss=0.27554988861083984, train/wer=0.101713, validation/ctc_loss=0.41449931263923645, validation/num_examples=5348, validation/wer=0.123869
I0214 13:46:44.225059 139475509450496 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.9254233241081238, loss=1.112470030784607
I0214 13:48:05.694365 139475517843200 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.8341740369796753, loss=1.1341618299484253
I0214 13:49:22.027245 139475509450496 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.8010461330413818, loss=1.1562422513961792
I0214 13:50:40.568486 139475517843200 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.9034145474433899, loss=1.1902943849563599
I0214 13:52:02.058301 139475509450496 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.829331636428833, loss=1.1983966827392578
I0214 13:53:29.732424 139475517843200 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.7640085220336914, loss=1.1549123525619507
I0214 13:54:58.657904 139475509450496 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.9064648151397705, loss=1.1413843631744385
I0214 13:56:28.401999 139475517843200 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.6569993495941162, loss=1.1415581703186035
I0214 13:57:59.458161 139475509450496 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.7597638964653015, loss=1.1584758758544922
I0214 13:59:27.537905 139475517843200 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.7711419463157654, loss=1.1807200908660889
I0214 14:00:58.367798 139475509450496 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.8020894527435303, loss=1.120402455329895
I0214 14:02:29.842412 139475517843200 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.8555353283882141, loss=1.2162648439407349
I0214 14:03:45.672263 139475509450496 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.8489751815795898, loss=1.1851422786712646
I0214 14:05:03.888697 139475517843200 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.9610152244567871, loss=1.1261767148971558
I0214 14:06:22.513466 139475509450496 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.7260411381721497, loss=1.2310454845428467
I0214 14:07:47.129480 139475517843200 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.7543319463729858, loss=1.1259170770645142
I0214 14:09:15.303801 139475509450496 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.8793706297874451, loss=1.140184760093689
I0214 14:09:30.402781 139646656866112 spec.py:321] Evaluating on the training split.
I0214 14:10:23.516378 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 14:11:17.751245 139646656866112 spec.py:349] Evaluating on the test split.
I0214 14:11:44.647260 139646656866112 submission_runner.py:408] Time since start: 41181.66s, 	Step: 44819, 	{'train/ctc_loss': Array(0.23959982, dtype=float32), 'train/wer': 0.08535817108137228, 'validation/ctc_loss': Array(0.40491655, dtype=float32), 'validation/wer': 0.12032594108730703, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23447508, dtype=float32), 'test/wer': 0.07935734162045782, 'test/num_examples': 2472, 'score': 37508.72058033943, 'total_duration': 41181.65663433075, 'accumulated_submission_time': 37508.72058033943, 'accumulated_eval_time': 3669.527673482895, 'accumulated_logging_time': 1.4051265716552734}
I0214 14:11:44.690485 139475517843200 logging_writer.py:48] [44819] accumulated_eval_time=3669.527673, accumulated_logging_time=1.405127, accumulated_submission_time=37508.720580, global_step=44819, preemption_count=0, score=37508.720580, test/ctc_loss=0.23447507619857788, test/num_examples=2472, test/wer=0.079357, total_duration=41181.656634, train/ctc_loss=0.2395998239517212, train/wer=0.085358, validation/ctc_loss=0.40491655468940735, validation/num_examples=5348, validation/wer=0.120326
I0214 14:12:46.940086 139475509450496 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.8406529426574707, loss=1.181272268295288
I0214 14:14:02.808035 139475517843200 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.9122814536094666, loss=1.1745408773422241
I0214 14:15:22.679217 139475509450496 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.7545145750045776, loss=1.183057427406311
I0214 14:16:51.358021 139475517843200 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.9182516932487488, loss=1.1888154745101929
I0214 14:18:18.339810 139475509450496 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.7804557681083679, loss=1.1624294519424438
I0214 14:19:42.120078 139475517843200 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.6617814302444458, loss=1.1238465309143066
I0214 14:21:01.360220 139475509450496 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.7095122337341309, loss=1.1465617418289185
I0214 14:22:24.017984 139475517843200 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.7284716963768005, loss=1.1307861804962158
I0214 14:23:44.323318 139475509450496 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.7678206562995911, loss=1.156905174255371
I0214 14:25:11.330698 139475517843200 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.9189465641975403, loss=1.1656571626663208
I0214 14:26:41.614588 139475509450496 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.982416033744812, loss=1.14494788646698
I0214 14:28:12.554523 139475517843200 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.7439405918121338, loss=1.115186095237732
I0214 14:29:40.275708 139475509450496 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.8190246820449829, loss=1.1103198528289795
I0214 14:31:11.485900 139475517843200 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.7780443429946899, loss=1.1517307758331299
I0214 14:32:40.327687 139475509450496 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.8228455185890198, loss=1.161789059638977
I0214 14:34:04.158190 139475517843200 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.861661434173584, loss=1.1638681888580322
I0214 14:35:20.376800 139475509450496 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.7497150897979736, loss=1.170716643333435
I0214 14:35:45.070878 139646656866112 spec.py:321] Evaluating on the training split.
I0214 14:36:38.591192 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 14:37:31.537822 139646656866112 spec.py:349] Evaluating on the test split.
I0214 14:37:59.065613 139646656866112 submission_runner.py:408] Time since start: 42756.07s, 	Step: 46534, 	{'train/ctc_loss': Array(0.22094382, dtype=float32), 'train/wer': 0.0819313462416354, 'validation/ctc_loss': Array(0.40488222, dtype=float32), 'validation/wer': 0.1207990190872491, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23017377, dtype=float32), 'test/wer': 0.07783397314809172, 'test/num_examples': 2472, 'score': 38949.01678466797, 'total_duration': 42756.07475566864, 'accumulated_submission_time': 38949.01678466797, 'accumulated_eval_time': 3803.5163989067078, 'accumulated_logging_time': 1.4624698162078857}
I0214 14:37:59.105051 139475814807296 logging_writer.py:48] [46534] accumulated_eval_time=3803.516399, accumulated_logging_time=1.462470, accumulated_submission_time=38949.016785, global_step=46534, preemption_count=0, score=38949.016785, test/ctc_loss=0.2301737666130066, test/num_examples=2472, test/wer=0.077834, total_duration=42756.074756, train/ctc_loss=0.22094382345676422, train/wer=0.081931, validation/ctc_loss=0.4048822224140167, validation/num_examples=5348, validation/wer=0.120799
I0214 14:38:49.699455 139475806414592 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.7732558250427246, loss=1.164811134338379
I0214 14:40:05.503201 139475814807296 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.7973712086677551, loss=1.1108702421188354
I0214 14:41:21.948431 139475806414592 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.7171081900596619, loss=1.1324957609176636
I0214 14:42:45.795253 139475814807296 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.9534844160079956, loss=1.1365160942077637
I0214 14:44:11.768474 139475806414592 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.7879024147987366, loss=1.1319923400878906
I0214 14:45:44.174747 139475814807296 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.8070171475410461, loss=1.1435691118240356
I0214 14:47:13.892494 139475806414592 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.8012681007385254, loss=1.123542070388794
I0214 14:48:42.963233 139475814807296 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.7132015824317932, loss=1.167900800704956
I0214 14:50:13.908316 139474944407296 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.8369312286376953, loss=1.1513253450393677
I0214 14:51:30.655391 139474936014592 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.8106727004051208, loss=1.087722659111023
I0214 14:52:46.806515 139474944407296 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.7774314284324646, loss=1.0648071765899658
I0214 14:54:06.978714 139474936014592 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.7624543309211731, loss=1.1378676891326904
I0214 14:55:26.777546 139474944407296 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.8477876782417297, loss=1.1896817684173584
I0214 14:56:54.175989 139474936014592 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.7799781560897827, loss=1.109211802482605
I0214 14:58:22.921532 139474944407296 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.6843854784965515, loss=1.1005403995513916
I0214 14:59:51.824862 139474936014592 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.9343468546867371, loss=1.1977418661117554
I0214 15:01:21.509429 139474944407296 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.9631233811378479, loss=1.1316053867340088
I0214 15:01:59.368170 139646656866112 spec.py:321] Evaluating on the training split.
I0214 15:02:53.598487 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 15:03:47.458595 139646656866112 spec.py:349] Evaluating on the test split.
I0214 15:04:14.880236 139646656866112 submission_runner.py:408] Time since start: 44331.89s, 	Step: 48243, 	{'train/ctc_loss': Array(0.18676206, dtype=float32), 'train/wer': 0.07017477079685477, 'validation/ctc_loss': Array(0.39040998, dtype=float32), 'validation/wer': 0.11653166243471041, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22285704, dtype=float32), 'test/wer': 0.07492941726078037, 'test/num_examples': 2472, 'score': 40389.19332933426, 'total_duration': 44331.88853049278, 'accumulated_submission_time': 40389.19332933426, 'accumulated_eval_time': 3939.021583557129, 'accumulated_logging_time': 1.5191435813903809}
I0214 15:04:14.924437 139474944407296 logging_writer.py:48] [48243] accumulated_eval_time=3939.021584, accumulated_logging_time=1.519144, accumulated_submission_time=40389.193329, global_step=48243, preemption_count=0, score=40389.193329, test/ctc_loss=0.2228570431470871, test/num_examples=2472, test/wer=0.074929, total_duration=44331.888530, train/ctc_loss=0.18676206469535828, train/wer=0.070175, validation/ctc_loss=0.3904099762439728, validation/num_examples=5348, validation/wer=0.116532
I0214 15:04:59.152393 139474936014592 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.8446082472801208, loss=1.1285938024520874
I0214 15:06:15.104886 139474944407296 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.8310586214065552, loss=1.1381182670593262
I0214 15:07:35.266941 139474944407296 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.814412534236908, loss=1.134526252746582
I0214 15:08:55.158324 139474936014592 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.8268601298332214, loss=1.1386557817459106
I0214 15:10:14.116171 139474944407296 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.803436815738678, loss=1.1305686235427856
I0214 15:11:39.444773 139474936014592 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.8852742910385132, loss=1.139196753501892
I0214 15:13:09.605145 139474944407296 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.9260363578796387, loss=1.148929476737976
I0214 15:14:35.731117 139474936014592 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.8918150067329407, loss=1.166813611984253
I0214 15:16:06.032652 139474944407296 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.3950828313827515, loss=1.0897138118743896
I0214 15:17:35.797551 139474936014592 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.7926254272460938, loss=1.1169397830963135
I0214 15:19:04.785012 139474944407296 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.0045084953308105, loss=1.0912758111953735
I0214 15:20:37.229233 139474936014592 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.9302204251289368, loss=1.0728493928909302
I0214 15:22:03.492186 139474944407296 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.871871829032898, loss=1.0869594812393188
I0214 15:23:22.007641 139474936014592 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.8644204139709473, loss=1.082478404045105
I0214 15:24:41.854415 139474944407296 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.926224946975708, loss=1.1235935688018799
I0214 15:26:04.574486 139474936014592 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.713588535785675, loss=1.1091657876968384
I0214 15:27:33.635957 139474944407296 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.901634931564331, loss=1.0980719327926636
I0214 15:28:15.036194 139646656866112 spec.py:321] Evaluating on the training split.
I0214 15:29:09.894480 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 15:30:02.665274 139646656866112 spec.py:349] Evaluating on the test split.
I0214 15:30:29.154982 139646656866112 submission_runner.py:408] Time since start: 45906.16s, 	Step: 49950, 	{'train/ctc_loss': Array(0.20000587, dtype=float32), 'train/wer': 0.07463326942816643, 'validation/ctc_loss': Array(0.3872141, dtype=float32), 'validation/wer': 0.11352906533303726, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21055159, dtype=float32), 'test/wer': 0.07072492027704995, 'test/num_examples': 2472, 'score': 41829.21695446968, 'total_duration': 45906.164083480835, 'accumulated_submission_time': 41829.21695446968, 'accumulated_eval_time': 4073.134298324585, 'accumulated_logging_time': 1.579833984375}
I0214 15:30:29.194153 139476398487296 logging_writer.py:48] [49950] accumulated_eval_time=4073.134298, accumulated_logging_time=1.579834, accumulated_submission_time=41829.216954, global_step=49950, preemption_count=0, score=41829.216954, test/ctc_loss=0.21055158972740173, test/num_examples=2472, test/wer=0.070725, total_duration=45906.164083, train/ctc_loss=0.20000587403774261, train/wer=0.074633, validation/ctc_loss=0.3872140944004059, validation/num_examples=5348, validation/wer=0.113529
I0214 15:31:07.769992 139476390094592 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.8593642115592957, loss=1.1272403001785278
I0214 15:32:23.730079 139476398487296 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.8392831683158875, loss=1.0678871870040894
I0214 15:33:42.893558 139476390094592 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.7609082460403442, loss=1.135301113128662
I0214 15:35:11.081061 139476398487296 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.2502522468566895, loss=1.0854642391204834
I0214 15:36:39.908127 139476390094592 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.875603199005127, loss=1.1058400869369507
I0214 15:38:06.488506 139476398487296 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.0927467346191406, loss=1.0865936279296875
I0214 15:39:25.108618 139476390094592 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.7614248991012573, loss=1.0873522758483887
I0214 15:40:43.715456 139476398487296 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.8053244948387146, loss=1.088487148284912
I0214 15:42:02.601487 139476390094592 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.8327333927154541, loss=1.1448774337768555
I0214 15:43:27.912075 139476398487296 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.8168853521347046, loss=1.0710233449935913
I0214 15:44:54.300105 139476390094592 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.8575184941291809, loss=1.1174120903015137
I0214 15:46:22.883247 139476398487296 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.8771786689758301, loss=1.0937168598175049
I0214 15:47:54.914929 139476390094592 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.92168128490448, loss=1.1022634506225586
I0214 15:49:25.142078 139476398487296 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.8856543898582458, loss=1.0756250619888306
I0214 15:50:52.296809 139476390094592 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.0462416410446167, loss=1.12898588180542
I0214 15:52:23.210063 139475200407296 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.9604302644729614, loss=1.1428923606872559
I0214 15:53:39.754112 139475192014592 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.8931301236152649, loss=1.108554720878601
I0214 15:54:29.833945 139646656866112 spec.py:321] Evaluating on the training split.
I0214 15:55:24.658077 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 15:56:17.063865 139646656866112 spec.py:349] Evaluating on the test split.
I0214 15:56:44.196316 139646656866112 submission_runner.py:408] Time since start: 47481.21s, 	Step: 51667, 	{'train/ctc_loss': Array(0.17628975, dtype=float32), 'train/wer': 0.06667668785432056, 'validation/ctc_loss': Array(0.3812173, dtype=float32), 'validation/wer': 0.11206155806791084, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20984745, dtype=float32), 'test/wer': 0.07082647817520768, 'test/num_examples': 2472, 'score': 43269.77072787285, 'total_duration': 47481.20555949211, 'accumulated_submission_time': 43269.77072787285, 'accumulated_eval_time': 4207.490744113922, 'accumulated_logging_time': 1.6347463130950928}
I0214 15:56:44.236985 139475313047296 logging_writer.py:48] [51667] accumulated_eval_time=4207.490744, accumulated_logging_time=1.634746, accumulated_submission_time=43269.770728, global_step=51667, preemption_count=0, score=43269.770728, test/ctc_loss=0.20984745025634766, test/num_examples=2472, test/wer=0.070826, total_duration=47481.205559, train/ctc_loss=0.17628975212574005, train/wer=0.066677, validation/ctc_loss=0.3812173008918762, validation/num_examples=5348, validation/wer=0.112062
I0214 15:57:10.035347 139475304654592 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.7871798872947693, loss=1.076520562171936
I0214 15:58:25.849713 139475313047296 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.80069500207901, loss=1.1079965829849243
I0214 15:59:41.724498 139475304654592 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.9629983901977539, loss=1.106214165687561
I0214 16:01:03.013645 139475313047296 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.0410027503967285, loss=1.1245737075805664
I0214 16:02:30.479173 139475304654592 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.8192638754844666, loss=1.0866830348968506
I0214 16:03:59.215529 139475313047296 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.0860164165496826, loss=1.119032382965088
I0214 16:05:27.310436 139475304654592 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.7450001239776611, loss=1.0159038305282593
I0214 16:06:57.091255 139475313047296 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.8241702914237976, loss=1.0724124908447266
I0214 16:08:24.626519 139475304654592 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.0035313367843628, loss=1.113666296005249
I0214 16:09:48.804088 139474985367296 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.8988152146339417, loss=1.0673426389694214
I0214 16:11:06.314350 139474976974592 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.9173836708068848, loss=1.092892050743103
I0214 16:12:23.508180 139474985367296 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.9346498250961304, loss=1.0147802829742432
I0214 16:13:40.664335 139474976974592 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.7880411744117737, loss=0.9905574917793274
I0214 16:15:03.856069 139474985367296 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.150468349456787, loss=1.065373182296753
I0214 16:16:29.357450 139474976974592 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.8934091329574585, loss=1.0857573747634888
I0214 16:17:54.962025 139474985367296 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.8686448335647583, loss=1.017885446548462
I0214 16:19:23.099882 139474976974592 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.8131616115570068, loss=1.0618606805801392
I0214 16:20:44.520579 139646656866112 spec.py:321] Evaluating on the training split.
I0214 16:21:39.439216 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 16:22:33.001351 139646656866112 spec.py:349] Evaluating on the test split.
I0214 16:23:00.629237 139646656866112 submission_runner.py:408] Time since start: 49057.64s, 	Step: 53392, 	{'train/ctc_loss': Array(0.17431833, dtype=float32), 'train/wer': 0.06628950194729819, 'validation/ctc_loss': Array(0.36423317, dtype=float32), 'validation/wer': 0.10812245961941358, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20121741, dtype=float32), 'test/wer': 0.06851095809721122, 'test/num_examples': 2472, 'score': 44709.96631407738, 'total_duration': 49057.63871669769, 'accumulated_submission_time': 44709.96631407738, 'accumulated_eval_time': 4343.593742609024, 'accumulated_logging_time': 1.6926517486572266}
I0214 16:23:00.670877 139474401687296 logging_writer.py:48] [53392] accumulated_eval_time=4343.593743, accumulated_logging_time=1.692652, accumulated_submission_time=44709.966314, global_step=53392, preemption_count=0, score=44709.966314, test/ctc_loss=0.2012174129486084, test/num_examples=2472, test/wer=0.068511, total_duration=49057.638717, train/ctc_loss=0.174318328499794, train/wer=0.066290, validation/ctc_loss=0.3642331659793854, validation/num_examples=5348, validation/wer=0.108122
I0214 16:23:07.581233 139474393294592 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.8377430438995361, loss=1.0930206775665283
I0214 16:24:23.290493 139474401687296 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.8740679621696472, loss=1.135311245918274
I0214 16:25:42.921704 139476398487296 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.8260946273803711, loss=1.0379022359848022
I0214 16:26:59.934070 139476390094592 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.8072880506515503, loss=1.0307976007461548
I0214 16:28:16.279920 139476398487296 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.8974035382270813, loss=1.009122610092163
I0214 16:29:39.407924 139476390094592 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.9098568558692932, loss=1.0693154335021973
I0214 16:31:03.986975 139476398487296 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.9223445653915405, loss=1.083734393119812
I0214 16:32:32.074655 139476390094592 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.9546362161636353, loss=1.0489141941070557
I0214 16:34:01.523307 139476398487296 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.9512312412261963, loss=1.0623444318771362
I0214 16:35:28.222062 139476390094592 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.9200412631034851, loss=1.0263065099716187
I0214 16:36:56.837256 139476398487296 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.7830246686935425, loss=1.0270682573318481
I0214 16:38:22.406857 139476390094592 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.8859882354736328, loss=1.1343998908996582
I0214 16:39:50.514950 139475313047296 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.963927149772644, loss=1.07735276222229
I0214 16:41:07.680377 139475304654592 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.0256460905075073, loss=1.072548270225525
I0214 16:42:26.437016 139475313047296 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.0681523084640503, loss=1.0818943977355957
I0214 16:43:48.873934 139475304654592 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.8956846594810486, loss=1.1114790439605713
I0214 16:45:13.956089 139475313047296 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.9065399169921875, loss=1.071542501449585
I0214 16:46:43.588426 139475304654592 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.9293012619018555, loss=1.0085560083389282
I0214 16:47:01.355346 139646656866112 spec.py:321] Evaluating on the training split.
I0214 16:47:57.067801 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 16:48:51.167800 139646656866112 spec.py:349] Evaluating on the test split.
I0214 16:49:18.039834 139646656866112 submission_runner.py:408] Time since start: 50635.05s, 	Step: 55121, 	{'train/ctc_loss': Array(0.16579323, dtype=float32), 'train/wer': 0.062470669427191164, 'validation/ctc_loss': Array(0.36580062, dtype=float32), 'validation/wer': 0.10823831545613409, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20039488, dtype=float32), 'test/wer': 0.06753600227489692, 'test/num_examples': 2472, 'score': 46150.562536239624, 'total_duration': 50635.04708957672, 'accumulated_submission_time': 46150.562536239624, 'accumulated_eval_time': 4480.270308256149, 'accumulated_logging_time': 1.7505762577056885}
I0214 16:49:18.082584 139475313047296 logging_writer.py:48] [55121] accumulated_eval_time=4480.270308, accumulated_logging_time=1.750576, accumulated_submission_time=46150.562536, global_step=55121, preemption_count=0, score=46150.562536, test/ctc_loss=0.2003948837518692, test/num_examples=2472, test/wer=0.067536, total_duration=50635.047090, train/ctc_loss=0.16579322516918182, train/wer=0.062471, validation/ctc_loss=0.3658006191253662, validation/num_examples=5348, validation/wer=0.108238
I0214 16:50:18.535037 139475304654592 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.9023101329803467, loss=1.0871896743774414
I0214 16:51:34.320096 139475313047296 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.2968883514404297, loss=1.026036262512207
I0214 16:52:54.504942 139475304654592 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.8171209096908569, loss=1.0516997575759888
I0214 16:54:22.247110 139475313047296 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.0612300634384155, loss=1.0480221509933472
I0214 16:55:47.162696 139475304654592 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.0704697370529175, loss=1.061678409576416
I0214 16:57:09.219789 139476398487296 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.8074067234992981, loss=0.9897111058235168
I0214 16:58:25.313444 139476390094592 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.7741132378578186, loss=1.0227972269058228
I0214 16:59:45.888486 139476398487296 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.9367906451225281, loss=1.0236715078353882
I0214 17:01:09.863725 139476390094592 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.8096705079078674, loss=1.0742310285568237
I0214 17:02:37.193541 139476398487296 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.8621739149093628, loss=1.0222995281219482
I0214 17:04:04.081606 139476390094592 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.9335030913352966, loss=1.0478432178497314
I0214 17:05:32.574598 139476398487296 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.1646047830581665, loss=1.0473053455352783
I0214 17:07:02.414992 139476390094592 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.9205963015556335, loss=1.0310593843460083
I0214 17:08:32.517928 139476398487296 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.887959897518158, loss=0.9929637908935547
I0214 17:10:00.161403 139476390094592 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.9830262660980225, loss=1.0846531391143799
I0214 17:11:24.365791 139476398487296 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.0979470014572144, loss=1.023711919784546
I0214 17:12:40.578706 139476390094592 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.111341953277588, loss=0.9900774359703064
I0214 17:13:18.441747 139646656866112 spec.py:321] Evaluating on the training split.
I0214 17:14:11.523845 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 17:15:03.703415 139646656866112 spec.py:349] Evaluating on the test split.
I0214 17:15:31.092658 139646656866112 submission_runner.py:408] Time since start: 52208.10s, 	Step: 56851, 	{'train/ctc_loss': Array(0.16412912, dtype=float32), 'train/wer': 0.061519178599636, 'validation/ctc_loss': Array(0.35546932, dtype=float32), 'validation/wer': 0.10464678451779835, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19666456, dtype=float32), 'test/wer': 0.06631730749700404, 'test/num_examples': 2472, 'score': 47590.833641052246, 'total_duration': 52208.10142946243, 'accumulated_submission_time': 47590.833641052246, 'accumulated_eval_time': 4612.914803504944, 'accumulated_logging_time': 1.8098342418670654}
I0214 17:15:31.138450 139475891607296 logging_writer.py:48] [56851] accumulated_eval_time=4612.914804, accumulated_logging_time=1.809834, accumulated_submission_time=47590.833641, global_step=56851, preemption_count=0, score=47590.833641, test/ctc_loss=0.19666455686092377, test/num_examples=2472, test/wer=0.066317, total_duration=52208.101429, train/ctc_loss=0.1641291230916977, train/wer=0.061519, validation/ctc_loss=0.35546931624412537, validation/num_examples=5348, validation/wer=0.104647
I0214 17:16:09.079045 139475883214592 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.1456507444381714, loss=1.0285147428512573
I0214 17:17:24.893893 139475891607296 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.9241427779197693, loss=1.0180569887161255
I0214 17:18:40.849172 139475883214592 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.9460254907608032, loss=1.0356780290603638
I0214 17:20:05.691803 139475891607296 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.8902424573898315, loss=1.0136054754257202
I0214 17:21:32.645096 139475883214592 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.0046799182891846, loss=1.0130714178085327
I0214 17:23:03.362956 139475891607296 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.055747389793396, loss=1.0016077756881714
I0214 17:24:30.128185 139475883214592 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.9647611379623413, loss=1.0536978244781494
I0214 17:25:57.729655 139475891607296 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.9238401055335999, loss=1.067906141281128
I0214 17:27:29.026406 139475563927296 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.0775220394134521, loss=1.0463640689849854
I0214 17:28:44.845010 139475555534592 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.9501062631607056, loss=1.0104444026947021
I0214 17:30:02.199389 139475563927296 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.0818675756454468, loss=0.9898182153701782
I0214 17:31:20.398098 139475555534592 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.9575656652450562, loss=0.988403856754303
I0214 17:32:45.756512 139475563927296 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.9790015816688538, loss=1.009412169456482
I0214 17:34:13.545775 139475555534592 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.9521132111549377, loss=1.0012584924697876
I0214 17:35:43.041828 139475563927296 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.9376379251480103, loss=1.037840485572815
I0214 17:37:08.544685 139475555534592 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.0155959129333496, loss=0.9907399415969849
I0214 17:38:39.500061 139475563927296 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.9993256330490112, loss=1.02100670337677
I0214 17:39:31.126020 139646656866112 spec.py:321] Evaluating on the training split.
I0214 17:40:26.602866 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 17:41:20.684703 139646656866112 spec.py:349] Evaluating on the test split.
I0214 17:41:47.431280 139646656866112 submission_runner.py:408] Time since start: 53784.44s, 	Step: 58559, 	{'train/ctc_loss': Array(0.16294, dtype=float32), 'train/wer': 0.06126727583615592, 'validation/ctc_loss': Array(0.34518048, dtype=float32), 'validation/wer': 0.10165384206918525, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19008121, dtype=float32), 'test/wer': 0.06274246948185161, 'test/num_examples': 2472, 'score': 49030.73461127281, 'total_duration': 53784.438963890076, 'accumulated_submission_time': 49030.73461127281, 'accumulated_eval_time': 4749.212601184845, 'accumulated_logging_time': 1.8706691265106201}
I0214 17:41:47.475020 139475348887296 logging_writer.py:48] [58559] accumulated_eval_time=4749.212601, accumulated_logging_time=1.870669, accumulated_submission_time=49030.734611, global_step=58559, preemption_count=0, score=49030.734611, test/ctc_loss=0.19008120894432068, test/num_examples=2472, test/wer=0.062742, total_duration=53784.438964, train/ctc_loss=0.16293999552726746, train/wer=0.061267, validation/ctc_loss=0.345180481672287, validation/num_examples=5348, validation/wer=0.101654
I0214 17:42:19.264053 139475340494592 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.9174492955207825, loss=0.9822923541069031
I0214 17:43:35.098087 139475348887296 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.9346822500228882, loss=1.0268983840942383
I0214 17:44:56.759166 139475021207296 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.2656726837158203, loss=0.9959776401519775
I0214 17:46:12.969281 139475012814592 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.8829030990600586, loss=1.0147614479064941
I0214 17:47:32.568141 139475021207296 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.8993740081787109, loss=0.9745622277259827
I0214 17:48:55.494953 139475012814592 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.1968402862548828, loss=1.034826636314392
I0214 17:50:23.702938 139475021207296 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.03061842918396, loss=1.0672630071640015
I0214 17:51:53.649298 139475012814592 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.0497888326644897, loss=0.9470141530036926
I0214 17:53:20.093075 139475021207296 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.8568532466888428, loss=1.0306516885757446
I0214 17:54:47.863931 139475012814592 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.0284522771835327, loss=1.034268856048584
I0214 17:56:15.933078 139475021207296 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.9940313696861267, loss=1.0306034088134766
I0214 17:57:47.046984 139475012814592 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.0192745923995972, loss=1.021427869796753
I0214 17:59:11.116485 139475021207296 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.9674067497253418, loss=1.0310231447219849
I0214 18:00:27.816151 139475012814592 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.9440586566925049, loss=0.9638243317604065
I0214 18:01:48.043988 139475021207296 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.9847859740257263, loss=1.0152063369750977
I0214 18:03:09.806806 139475012814592 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.3870201110839844, loss=0.9703571200370789
I0214 18:04:35.550473 139475021207296 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.0473490953445435, loss=0.9922206401824951
I0214 18:05:47.813517 139646656866112 spec.py:321] Evaluating on the training split.
I0214 18:06:41.740932 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 18:07:36.232904 139646656866112 spec.py:349] Evaluating on the test split.
I0214 18:08:04.180613 139646656866112 submission_runner.py:408] Time since start: 55361.19s, 	Step: 60285, 	{'train/ctc_loss': Array(0.13095543, dtype=float32), 'train/wer': 0.05084403793126639, 'validation/ctc_loss': Array(0.33887717, dtype=float32), 'validation/wer': 0.09931741602865501, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18431841, dtype=float32), 'test/wer': 0.060386326244592045, 'test/num_examples': 2472, 'score': 50470.985605716705, 'total_duration': 55361.18985915184, 'accumulated_submission_time': 50470.985605716705, 'accumulated_eval_time': 4885.57377076149, 'accumulated_logging_time': 1.9312732219696045}
I0214 18:08:04.229073 139475021207296 logging_writer.py:48] [60285] accumulated_eval_time=4885.573771, accumulated_logging_time=1.931273, accumulated_submission_time=50470.985606, global_step=60285, preemption_count=0, score=50470.985606, test/ctc_loss=0.184318408370018, test/num_examples=2472, test/wer=0.060386, total_duration=55361.189859, train/ctc_loss=0.13095542788505554, train/wer=0.050844, validation/ctc_loss=0.3388771712779999, validation/num_examples=5348, validation/wer=0.099317
I0214 18:08:16.401198 139475012814592 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.121216058731079, loss=0.9875898361206055
I0214 18:09:32.173191 139475021207296 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.9181820154190063, loss=1.006883978843689
I0214 18:10:48.111796 139475012814592 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.1031570434570312, loss=1.0298352241516113
I0214 18:12:13.874451 139475021207296 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.1187005043029785, loss=0.9900068640708923
I0214 18:13:41.781438 139475012814592 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.093245267868042, loss=0.9706512689590454
I0214 18:15:08.869453 139475891607296 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.9148617386817932, loss=0.8998367786407471
I0214 18:16:27.063430 139475883214592 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.0829869508743286, loss=0.9985317587852478
I0214 18:17:43.978754 139475891607296 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.0116074085235596, loss=0.9779354929924011
I0214 18:19:05.508606 139475883214592 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.0657259225845337, loss=1.0092405080795288
I0214 18:20:27.383547 139475891607296 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.098257064819336, loss=0.9870815277099609
I0214 18:21:55.533493 139475883214592 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.2792919874191284, loss=0.9382850527763367
I0214 18:23:26.209196 139475891607296 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.9261780381202698, loss=0.9603760838508606
I0214 18:24:53.854175 139475883214592 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.0405508279800415, loss=1.0381088256835938
I0214 18:26:23.632335 139475891607296 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.0268683433532715, loss=0.9370213747024536
I0214 18:27:48.299989 139475883214592 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.310792088508606, loss=0.9568772912025452
I0214 18:29:19.758994 139475021207296 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.0775059461593628, loss=1.0277608633041382
I0214 18:30:35.853656 139475012814592 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.9375209808349609, loss=1.0185438394546509
I0214 18:31:53.291965 139475021207296 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.096653699874878, loss=0.9480387568473816
I0214 18:32:04.443969 139646656866112 spec.py:321] Evaluating on the training split.
I0214 18:32:59.665490 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 18:33:53.249333 139646656866112 spec.py:349] Evaluating on the test split.
I0214 18:34:20.331017 139646656866112 submission_runner.py:408] Time since start: 56937.34s, 	Step: 62016, 	{'train/ctc_loss': Array(0.12969963, dtype=float32), 'train/wer': 0.049176212861119985, 'validation/ctc_loss': Array(0.3313753, dtype=float32), 'validation/wer': 0.09697133533506473, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17523469, dtype=float32), 'test/wer': 0.05945199358154084, 'test/num_examples': 2472, 'score': 51911.11315703392, 'total_duration': 56937.3402159214, 'accumulated_submission_time': 51911.11315703392, 'accumulated_eval_time': 5021.4548535346985, 'accumulated_logging_time': 1.996178150177002}
I0214 18:34:20.376047 139475384727296 logging_writer.py:48] [62016] accumulated_eval_time=5021.454854, accumulated_logging_time=1.996178, accumulated_submission_time=51911.113157, global_step=62016, preemption_count=0, score=51911.113157, test/ctc_loss=0.17523469030857086, test/num_examples=2472, test/wer=0.059452, total_duration=56937.340216, train/ctc_loss=0.12969963252544403, train/wer=0.049176, validation/ctc_loss=0.3313753008842468, validation/num_examples=5348, validation/wer=0.096971
I0214 18:35:25.202403 139475376334592 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.4696779251098633, loss=0.9427023530006409
I0214 18:36:41.236336 139475384727296 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.8931677341461182, loss=0.9776309728622437
I0214 18:37:58.059251 139475376334592 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.3157086372375488, loss=0.917364776134491
I0214 18:39:24.438296 139475384727296 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.9459750652313232, loss=0.9592342376708984
I0214 18:40:49.517610 139475376334592 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.1324965953826904, loss=1.0171163082122803
I0214 18:42:17.597120 139475384727296 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.9965234398841858, loss=0.9019579291343689
I0214 18:43:47.901449 139475376334592 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.0236258506774902, loss=0.960528552532196
I0214 18:45:16.489338 139475384727296 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.0485550165176392, loss=0.976672351360321
I0214 18:46:38.413945 139475384727296 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.9739274382591248, loss=0.9186355471611023
I0214 18:47:55.380741 139475376334592 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.1013050079345703, loss=0.981735110282898
I0214 18:49:15.975788 139475384727296 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.046810269355774, loss=0.9974667429924011
I0214 18:50:37.439760 139475376334592 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.0607768297195435, loss=0.9416057467460632
I0214 18:52:05.099343 139475384727296 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.0841362476348877, loss=0.9643805027008057
I0214 18:53:33.787068 139475376334592 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.298924207687378, loss=0.961812436580658
I0214 18:55:05.434443 139475384727296 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.174638032913208, loss=0.9383342862129211
I0214 18:56:32.216667 139475376334592 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.3677271604537964, loss=0.960722804069519
I0214 18:58:01.848131 139475384727296 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.9770346283912659, loss=0.9418115615844727
I0214 18:58:21.210796 139646656866112 spec.py:321] Evaluating on the training split.
I0214 18:59:15.180338 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 19:00:07.856749 139646656866112 spec.py:349] Evaluating on the test split.
I0214 19:00:35.561599 139646656866112 submission_runner.py:408] Time since start: 58512.57s, 	Step: 63722, 	{'train/ctc_loss': Array(0.11556399, dtype=float32), 'train/wer': 0.044251885908137306, 'validation/ctc_loss': Array(0.32159543, dtype=float32), 'validation/wer': 0.09288741709066685, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17089857, dtype=float32), 'test/wer': 0.05750208193691223, 'test/num_examples': 2472, 'score': 53351.861990213394, 'total_duration': 58512.570706129074, 'accumulated_submission_time': 53351.861990213394, 'accumulated_eval_time': 5155.799608707428, 'accumulated_logging_time': 2.057328224182129}
I0214 19:00:35.607721 139474790799104 logging_writer.py:48] [63722] accumulated_eval_time=5155.799609, accumulated_logging_time=2.057328, accumulated_submission_time=53351.861990, global_step=63722, preemption_count=0, score=53351.861990, test/ctc_loss=0.17089857161045074, test/num_examples=2472, test/wer=0.057502, total_duration=58512.570706, train/ctc_loss=0.11556398868560791, train/wer=0.044252, validation/ctc_loss=0.3215954303741455, validation/num_examples=5348, validation/wer=0.092887
I0214 19:01:35.427214 139474782406400 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.2215962409973145, loss=0.9534817337989807
I0214 19:02:55.746809 139474790799104 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.1428629159927368, loss=0.9515669345855713
I0214 19:04:14.323241 139474782406400 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.103684425354004, loss=0.9788632988929749
I0214 19:05:31.709363 139474790799104 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.072670340538025, loss=0.9583402872085571
I0214 19:06:54.435208 139474782406400 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.2284682989120483, loss=0.9602606892585754
I0214 19:08:18.459377 139474790799104 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.1469591856002808, loss=0.9591858983039856
I0214 19:09:46.562668 139474782406400 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.104630947113037, loss=0.9203295707702637
I0214 19:11:15.940260 139474790799104 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.0315053462982178, loss=0.9337644577026367
I0214 19:12:43.377360 139474782406400 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.158480167388916, loss=0.9629896283149719
I0214 19:14:11.768590 139474790799104 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.3971161842346191, loss=0.9199938178062439
I0214 19:15:42.538306 139474782406400 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.478159785270691, loss=0.933860182762146
I0214 19:17:12.144998 139474790799104 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.9388573169708252, loss=0.9566745758056641
I0214 19:18:29.247185 139474782406400 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.1600227355957031, loss=0.8994771242141724
I0214 19:19:46.920500 139474790799104 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.1459228992462158, loss=0.9191287159919739
I0214 19:21:04.677122 139474782406400 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.333155632019043, loss=0.9398017525672913
I0214 19:22:29.905162 139474790799104 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.072930097579956, loss=0.9437554478645325
I0214 19:23:58.638314 139474782406400 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.1776574850082397, loss=0.908436119556427
I0214 19:24:35.853900 139646656866112 spec.py:321] Evaluating on the training split.
I0214 19:25:29.039872 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 19:26:23.411789 139646656866112 spec.py:349] Evaluating on the test split.
I0214 19:26:51.042967 139646656866112 submission_runner.py:408] Time since start: 60088.05s, 	Step: 65443, 	{'train/ctc_loss': Array(0.11451922, dtype=float32), 'train/wer': 0.04346464583202856, 'validation/ctc_loss': Array(0.3201817, dtype=float32), 'validation/wer': 0.09174816802958186, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1703582, dtype=float32), 'test/wer': 0.056384945057177095, 'test/num_examples': 2472, 'score': 54792.02154159546, 'total_duration': 60088.04986286163, 'accumulated_submission_time': 54792.02154159546, 'accumulated_eval_time': 5290.980396270752, 'accumulated_logging_time': 2.120563507080078}
I0214 19:26:51.087747 139474790799104 logging_writer.py:48] [65443] accumulated_eval_time=5290.980396, accumulated_logging_time=2.120564, accumulated_submission_time=54792.021542, global_step=65443, preemption_count=0, score=54792.021542, test/ctc_loss=0.17035819590091705, test/num_examples=2472, test/wer=0.056385, total_duration=60088.049863, train/ctc_loss=0.11451922357082367, train/wer=0.043465, validation/ctc_loss=0.3201816976070404, validation/num_examples=5348, validation/wer=0.091748
I0214 19:27:34.952588 139474782406400 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.0669209957122803, loss=0.9452577829360962
I0214 19:28:51.058961 139474790799104 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.2968220710754395, loss=0.926708459854126
I0214 19:30:10.119088 139474782406400 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.0614936351776123, loss=0.9199156761169434
I0214 19:31:38.976921 139474790799104 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.1543490886688232, loss=0.891272783279419
I0214 19:33:07.516587 139474782406400 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.064677357673645, loss=0.9346426129341125
I0214 19:34:30.769245 139474463119104 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.216676950454712, loss=0.9113671779632568
I0214 19:35:47.356067 139474454726400 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.2883025407791138, loss=0.952403724193573
I0214 19:37:04.705565 139474463119104 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.0590258836746216, loss=0.9386497735977173
I0214 19:38:23.919105 139474454726400 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.187085747718811, loss=0.8777068853378296
I0214 19:39:47.154928 139474463119104 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.0861763954162598, loss=0.9199934005737305
I0214 19:41:17.553503 139474454726400 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.1283609867095947, loss=0.9400293827056885
I0214 19:42:50.104867 139474463119104 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.3816354274749756, loss=0.9143534302711487
I0214 19:44:17.429526 139474454726400 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.3231271505355835, loss=0.903069257736206
I0214 19:45:44.462018 139474463119104 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.0923008918762207, loss=0.9644094705581665
I0214 19:47:14.591809 139474454726400 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.2915846109390259, loss=0.9035472869873047
I0214 19:48:41.163589 139474463119104 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.1941179037094116, loss=0.9028170108795166
I0214 19:49:59.139366 139474454726400 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.2431279420852661, loss=0.9422487616539001
I0214 19:50:51.431542 139646656866112 spec.py:321] Evaluating on the training split.
I0214 19:51:45.025426 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 19:52:37.918672 139646656866112 spec.py:349] Evaluating on the test split.
I0214 19:53:05.146579 139646656866112 submission_runner.py:408] Time since start: 61662.16s, 	Step: 67169, 	{'train/ctc_loss': Array(0.1024924, dtype=float32), 'train/wer': 0.03876814024731111, 'validation/ctc_loss': Array(0.31268138, dtype=float32), 'validation/wer': 0.08930554080539116, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16590431, dtype=float32), 'test/wer': 0.05441472183291694, 'test/num_examples': 2472, 'score': 56232.27780032158, 'total_duration': 61662.15571832657, 'accumulated_submission_time': 56232.27780032158, 'accumulated_eval_time': 5424.689416408539, 'accumulated_logging_time': 2.1815385818481445}
I0214 19:53:05.197606 139474463119104 logging_writer.py:48] [67169] accumulated_eval_time=5424.689416, accumulated_logging_time=2.181539, accumulated_submission_time=56232.277800, global_step=67169, preemption_count=0, score=56232.277800, test/ctc_loss=0.16590431332588196, test/num_examples=2472, test/wer=0.054415, total_duration=61662.155718, train/ctc_loss=0.10249239951372147, train/wer=0.038768, validation/ctc_loss=0.3126813769340515, validation/num_examples=5348, validation/wer=0.089306
I0214 19:53:29.545849 139474454726400 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.1246387958526611, loss=0.9347575902938843
I0214 19:54:45.378746 139474463119104 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.1434208154678345, loss=0.8989746570587158
I0214 19:56:01.634351 139474454726400 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.137060523033142, loss=0.9256036877632141
I0214 19:57:25.311077 139474463119104 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.0746207237243652, loss=0.9083130359649658
I0214 19:58:51.830718 139474454726400 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.1862053871154785, loss=0.9038916230201721
I0214 20:00:18.508386 139474463119104 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.498101830482483, loss=0.9031772613525391
I0214 20:01:48.529383 139474454726400 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.6267412900924683, loss=0.8849690556526184
I0214 20:03:16.770013 139474463119104 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.205409049987793, loss=0.943991482257843
I0214 20:04:48.583181 139474463119104 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.1223926544189453, loss=0.8621644973754883
I0214 20:06:05.309018 139474454726400 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.0654816627502441, loss=0.9235321879386902
I0214 20:07:22.009328 139474463119104 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.0486078262329102, loss=0.8716506361961365
I0214 20:08:39.099886 139474454726400 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.1231998205184937, loss=0.8985571265220642
I0214 20:10:04.586369 139474463119104 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.3386485576629639, loss=0.9152719378471375
I0214 20:11:33.423752 139474454726400 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.0339545011520386, loss=0.8958048224449158
I0214 20:13:00.210561 139474463119104 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.2845709323883057, loss=0.9352036118507385
I0214 20:14:28.414712 139474454726400 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.373778223991394, loss=0.897111713886261
I0214 20:15:57.275441 139474463119104 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.1444010734558105, loss=0.8716067671775818
I0214 20:17:05.297319 139646656866112 spec.py:321] Evaluating on the training split.
I0214 20:17:59.079650 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 20:18:51.360237 139646656866112 spec.py:349] Evaluating on the test split.
I0214 20:19:18.081490 139646656866112 submission_runner.py:408] Time since start: 63235.09s, 	Step: 68876, 	{'train/ctc_loss': Array(0.09694151, dtype=float32), 'train/wer': 0.03730781576879227, 'validation/ctc_loss': Array(0.30519754, dtype=float32), 'validation/wer': 0.08692084149956071, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1626802, dtype=float32), 'test/wer': 0.053642881806918126, 'test/num_examples': 2472, 'score': 57672.292115449905, 'total_duration': 63235.09067606926, 'accumulated_submission_time': 57672.292115449905, 'accumulated_eval_time': 5557.467604875565, 'accumulated_logging_time': 2.247718572616577}
I0214 20:19:18.124145 139475384727296 logging_writer.py:48] [68876] accumulated_eval_time=5557.467605, accumulated_logging_time=2.247719, accumulated_submission_time=57672.292115, global_step=68876, preemption_count=0, score=57672.292115, test/ctc_loss=0.16268019378185272, test/num_examples=2472, test/wer=0.053643, total_duration=63235.090676, train/ctc_loss=0.0969415083527565, train/wer=0.037308, validation/ctc_loss=0.305197536945343, validation/num_examples=5348, validation/wer=0.086921
I0214 20:19:37.104967 139475376334592 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.2620337009429932, loss=0.911434531211853
I0214 20:20:53.472301 139475384727296 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.1727609634399414, loss=0.8970941305160522
I0214 20:22:13.496484 139475384727296 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.362176775932312, loss=0.9203969240188599
I0214 20:23:29.617534 139475376334592 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.1489723920822144, loss=0.9282252788543701
I0214 20:24:49.183536 139475384727296 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.35837721824646, loss=0.8896433711051941
I0214 20:26:11.066567 139475376334592 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.220218300819397, loss=0.8625361323356628
I0214 20:27:38.241087 139475384727296 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.55181884765625, loss=0.9074996113777161
I0214 20:29:05.251731 139475376334592 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.1018116474151611, loss=0.9133479595184326
I0214 20:30:31.324820 139475384727296 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.8361585140228271, loss=0.8908320665359497
I0214 20:32:01.663591 139475376334592 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.3638496398925781, loss=0.8188035488128662
I0214 20:33:31.032745 139475384727296 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.1598188877105713, loss=0.8927800059318542
I0214 20:34:58.117986 139475376334592 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.3510162830352783, loss=0.9454928040504456
I0214 20:36:24.206376 139474401687296 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.4424636363983154, loss=0.8779186606407166
I0214 20:37:40.390846 139474393294592 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.1584315299987793, loss=0.86551833152771
I0214 20:38:56.954959 139474401687296 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.138484001159668, loss=0.8966165781021118
I0214 20:40:18.609922 139474393294592 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.1685781478881836, loss=0.8821192383766174
I0214 20:41:45.485152 139474401687296 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.2646821737289429, loss=0.8838146328926086
I0214 20:43:12.959576 139474393294592 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.2408721446990967, loss=0.867603600025177
I0214 20:43:18.361258 139646656866112 spec.py:321] Evaluating on the training split.
I0214 20:44:11.667593 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 20:45:05.845854 139646656866112 spec.py:349] Evaluating on the test split.
I0214 20:45:32.492982 139646656866112 submission_runner.py:408] Time since start: 64809.50s, 	Step: 70607, 	{'train/ctc_loss': Array(0.07881192, dtype=float32), 'train/wer': 0.029981964460375522, 'validation/ctc_loss': Array(0.30443206, dtype=float32), 'validation/wer': 0.08557884472421484, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16090697, dtype=float32), 'test/wer': 0.054313163934759205, 'test/num_examples': 2472, 'score': 59112.44225859642, 'total_duration': 64809.5007917881, 'accumulated_submission_time': 59112.44225859642, 'accumulated_eval_time': 5691.591963291168, 'accumulated_logging_time': 2.307018518447876}
I0214 20:45:32.538405 139474401687296 logging_writer.py:48] [70607] accumulated_eval_time=5691.591963, accumulated_logging_time=2.307019, accumulated_submission_time=59112.442259, global_step=70607, preemption_count=0, score=59112.442259, test/ctc_loss=0.16090697050094604, test/num_examples=2472, test/wer=0.054313, total_duration=64809.500792, train/ctc_loss=0.07881192117929459, train/wer=0.029982, validation/ctc_loss=0.30443206429481506, validation/num_examples=5348, validation/wer=0.085579
I0214 20:46:44.035561 139474393294592 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.5190081596374512, loss=0.86460280418396
I0214 20:48:00.336759 139474401687296 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.3904067277908325, loss=0.8722624778747559
I0214 20:49:24.742120 139474393294592 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.0634088516235352, loss=0.8747431039810181
I0214 20:50:52.848831 139474401687296 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.161507248878479, loss=0.8944696187973022
I0214 20:52:21.217408 139475384727296 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.4158527851104736, loss=0.8800434470176697
I0214 20:53:40.030092 139475376334592 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.3753291368484497, loss=0.8596980571746826
I0214 20:54:59.907993 139475384727296 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.5512325763702393, loss=0.8950504660606384
I0214 20:56:22.858412 139475376334592 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.0961991548538208, loss=0.887721598148346
I0214 20:57:47.072632 139475384727296 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.2889434099197388, loss=0.920924961566925
I0214 20:59:13.652737 139475376334592 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.1409412622451782, loss=0.8716416358947754
I0214 21:00:43.095996 139475384727296 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.3996325731277466, loss=0.8655851483345032
I0214 21:02:13.507751 139475376334592 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.070914387702942, loss=0.806635320186615
I0214 21:03:39.876564 139475384727296 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.7052098512649536, loss=0.8927943110466003
I0214 21:05:07.454564 139475376334592 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.4223419427871704, loss=0.8652067184448242
I0214 21:06:38.322038 139475384727296 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.9013460874557495, loss=0.8923627138137817
I0214 21:07:56.541884 139475376334592 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.317624807357788, loss=0.8374151587486267
I0214 21:09:14.815329 139475384727296 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.3266685009002686, loss=0.8381320834159851
I0214 21:09:32.527625 139646656866112 spec.py:321] Evaluating on the training split.
I0214 21:10:26.090350 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 21:11:18.364886 139646656866112 spec.py:349] Evaluating on the test split.
I0214 21:11:45.060857 139646656866112 submission_runner.py:408] Time since start: 66382.07s, 	Step: 72323, 	{'train/ctc_loss': Array(0.07315466, dtype=float32), 'train/wer': 0.027833001988071572, 'validation/ctc_loss': Array(0.2974776, dtype=float32), 'validation/wer': 0.08360929549996621, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15847631, dtype=float32), 'test/wer': 0.051266426990027016, 'test/num_examples': 2472, 'score': 60552.34362244606, 'total_duration': 66382.07028150558, 'accumulated_submission_time': 60552.34362244606, 'accumulated_eval_time': 5824.119750261307, 'accumulated_logging_time': 2.368692636489868}
I0214 21:11:45.106490 139475384727296 logging_writer.py:48] [72323] accumulated_eval_time=5824.119750, accumulated_logging_time=2.368693, accumulated_submission_time=60552.343622, global_step=72323, preemption_count=0, score=60552.343622, test/ctc_loss=0.1584763079881668, test/num_examples=2472, test/wer=0.051266, total_duration=66382.070282, train/ctc_loss=0.07315465807914734, train/wer=0.027833, validation/ctc_loss=0.2974776029586792, validation/num_examples=5348, validation/wer=0.083609
I0214 21:12:44.069303 139475376334592 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.1387275457382202, loss=0.8670386672019958
I0214 21:14:00.476690 139475384727296 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.6547081470489502, loss=0.8474507331848145
I0214 21:15:17.020653 139475376334592 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.298776388168335, loss=0.8391807675361633
I0214 21:16:44.115778 139475384727296 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.3201113939285278, loss=0.8346279859542847
I0214 21:18:10.374217 139475376334592 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.8488112688064575, loss=0.8374824523925781
I0214 21:19:39.833570 139475384727296 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.429057240486145, loss=0.8366103172302246
I0214 21:20:21.384991 139475376334592 logging_writer.py:48] [72950] global_step=72950, preemption_count=0, score=61068.561848
I0214 21:20:22.267341 139646656866112 checkpoints.py:490] Saving checkpoint at step: 72950
I0214 21:20:23.799190 139646656866112 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_2/librispeech_conformer_jax/trial_1/checkpoint_72950
I0214 21:20:23.834526 139646656866112 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_2/librispeech_conformer_jax/trial_1/checkpoint_72950.
I0214 21:20:27.951660 139646656866112 submission_runner.py:583] Tuning trial 1/5
I0214 21:20:27.951922 139646656866112 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0214 21:20:27.977165 139646656866112 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.690332, dtype=float32), 'train/wer': 1.115215625845949, 'validation/ctc_loss': Array(30.757864, dtype=float32), 'validation/wer': 1.1779352558965794, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.871214, dtype=float32), 'test/wer': 1.190014827453131, 'test/num_examples': 2472, 'score': 62.33279633522034, 'total_duration': 239.9211916923523, 'accumulated_submission_time': 62.33279633522034, 'accumulated_eval_time': 177.5883333683014, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1748, {'train/ctc_loss': Array(5.9307604, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': Array(6.0122747, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.9551396, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1502.6152069568634, 'total_duration': 1789.3208968639374, 'accumulated_submission_time': 1502.6152069568634, 'accumulated_eval_time': 286.587749004364, 'accumulated_logging_time': 0.046248435974121094, 'global_step': 1748, 'preemption_count': 0}), (3538, {'train/ctc_loss': Array(3.3182616, dtype=float32), 'train/wer': 0.719244599525877, 'validation/ctc_loss': Array(3.7128878, dtype=float32), 'validation/wer': 0.7624858800698997, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.388314, dtype=float32), 'test/wer': 0.7071273332927102, 'test/num_examples': 2472, 'score': 2942.7661283016205, 'total_duration': 3349.6305611133575, 'accumulated_submission_time': 2942.7661283016205, 'accumulated_eval_time': 406.61941361427307, 'accumulated_logging_time': 0.0941307544708252, 'global_step': 3538, 'preemption_count': 0}), (5287, {'train/ctc_loss': Array(0.7781294, dtype=float32), 'train/wer': 0.2672749603344034, 'validation/ctc_loss': Array(1.1462481, dtype=float32), 'validation/wer': 0.3319366268573139, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.8411037, dtype=float32), 'test/wer': 0.2719517396867954, 'test/num_examples': 2472, 'score': 4383.109746217728, 'total_duration': 4925.651102542877, 'accumulated_submission_time': 4383.109746217728, 'accumulated_eval_time': 542.1664445400238, 'accumulated_logging_time': 0.1464216709136963, 'global_step': 5287, 'preemption_count': 0}), (7026, {'train/ctc_loss': Array(0.5286893, dtype=float32), 'train/wer': 0.1846181235534983, 'validation/ctc_loss': Array(0.84069234, dtype=float32), 'validation/wer': 0.25316431254042887, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.58280253, dtype=float32), 'test/wer': 0.1943208823350192, 'test/num_examples': 2472, 'score': 5823.235984802246, 'total_duration': 6501.536516189575, 'accumulated_submission_time': 5823.235984802246, 'accumulated_eval_time': 677.7943549156189, 'accumulated_logging_time': 0.19841790199279785, 'global_step': 7026, 'preemption_count': 0}), (8791, {'train/ctc_loss': Array(0.44677088, dtype=float32), 'train/wer': 0.15943486485898803, 'validation/ctc_loss': Array(0.7451185, dtype=float32), 'validation/wer': 0.2265271247477722, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4925594, dtype=float32), 'test/wer': 0.16637214876200923, 'test/num_examples': 2472, 'score': 7263.7350125312805, 'total_duration': 8076.360379934311, 'accumulated_submission_time': 7263.7350125312805, 'accumulated_eval_time': 811.9875221252441, 'accumulated_logging_time': 0.2500596046447754, 'global_step': 8791, 'preemption_count': 0}), (10539, {'train/ctc_loss': Array(0.4121761, dtype=float32), 'train/wer': 0.14543534043563386, 'validation/ctc_loss': Array(0.67398584, dtype=float32), 'validation/wer': 0.20539308919933963, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4373133, dtype=float32), 'test/wer': 0.1492088639733512, 'test/num_examples': 2472, 'score': 8703.784867048264, 'total_duration': 9653.374977111816, 'accumulated_submission_time': 8703.784867048264, 'accumulated_eval_time': 948.816588640213, 'accumulated_logging_time': 0.30726146697998047, 'global_step': 10539, 'preemption_count': 0}), (12248, {'train/ctc_loss': Array(0.36185822, dtype=float32), 'train/wer': 0.12938974326250186, 'validation/ctc_loss': Array(0.6489348, dtype=float32), 'validation/wer': 0.19732179924114426, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.411901, dtype=float32), 'test/wer': 0.1403936384132594, 'test/num_examples': 2472, 'score': 10143.811375379562, 'total_duration': 11232.879916191101, 'accumulated_submission_time': 10143.811375379562, 'accumulated_eval_time': 1088.169734954834, 'accumulated_logging_time': 0.355985164642334, 'global_step': 12248, 'preemption_count': 0}), (13977, {'train/ctc_loss': Array(0.30664057, dtype=float32), 'train/wer': 0.1145063574195897, 'validation/ctc_loss': Array(0.6024696, dtype=float32), 'validation/wer': 0.18286878361026096, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3806148, dtype=float32), 'test/wer': 0.1301362906993277, 'test/num_examples': 2472, 'score': 11584.330972671509, 'total_duration': 12809.234954595566, 'accumulated_submission_time': 11584.330972671509, 'accumulated_eval_time': 1223.8755309581757, 'accumulated_logging_time': 0.4083857536315918, 'global_step': 13977, 'preemption_count': 0}), (15712, {'train/ctc_loss': Array(0.28140113, dtype=float32), 'train/wer': 0.10300102676505168, 'validation/ctc_loss': Array(0.5813166, dtype=float32), 'validation/wer': 0.17756837908029774, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36267507, dtype=float32), 'test/wer': 0.12211321674486625, 'test/num_examples': 2472, 'score': 13024.305043458939, 'total_duration': 14383.716886281967, 'accumulated_submission_time': 13024.305043458939, 'accumulated_eval_time': 1358.2513551712036, 'accumulated_logging_time': 0.46280694007873535, 'global_step': 15712, 'preemption_count': 0}), (17428, {'train/ctc_loss': Array(0.27844656, dtype=float32), 'train/wer': 0.10552537446304172, 'validation/ctc_loss': Array(0.5747308, dtype=float32), 'validation/wer': 0.17573399499888973, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35867026, dtype=float32), 'test/wer': 0.12081327564844718, 'test/num_examples': 2472, 'score': 14464.859641313553, 'total_duration': 15959.115698099136, 'accumulated_submission_time': 14464.859641313553, 'accumulated_eval_time': 1492.9678757190704, 'accumulated_logging_time': 0.5146021842956543, 'global_step': 17428, 'preemption_count': 0}), (19133, {'train/ctc_loss': Array(0.2847909, dtype=float32), 'train/wer': 0.10319151749120813, 'validation/ctc_loss': Array(0.5575726, dtype=float32), 'validation/wer': 0.16925572279560133, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33965674, dtype=float32), 'test/wer': 0.11506509861271912, 'test/num_examples': 2472, 'score': 15904.858520269394, 'total_duration': 17536.534660100937, 'accumulated_submission_time': 15904.858520269394, 'accumulated_eval_time': 1630.2590498924255, 'accumulated_logging_time': 0.5669634342193604, 'global_step': 19133, 'preemption_count': 0}), (20853, {'train/ctc_loss': Array(0.27910382, dtype=float32), 'train/wer': 0.10145355739045558, 'validation/ctc_loss': Array(0.5391951, dtype=float32), 'validation/wer': 0.16466976259208124, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32976592, dtype=float32), 'test/wer': 0.11252615115877562, 'test/num_examples': 2472, 'score': 17345.28639960289, 'total_duration': 19115.42906999588, 'accumulated_submission_time': 17345.28639960289, 'accumulated_eval_time': 1768.594096660614, 'accumulated_logging_time': 0.622377872467041, 'global_step': 20853, 'preemption_count': 0}), (22545, {'train/ctc_loss': Array(0.2605467, dtype=float32), 'train/wer': 0.09435127070686285, 'validation/ctc_loss': Array(0.52861744, dtype=float32), 'validation/wer': 0.16012242100080135, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.313901, dtype=float32), 'test/wer': 0.1081591615379928, 'test/num_examples': 2472, 'score': 18785.17260837555, 'total_duration': 20691.28274512291, 'accumulated_submission_time': 18785.17260837555, 'accumulated_eval_time': 1904.4369497299194, 'accumulated_logging_time': 0.6713097095489502, 'global_step': 22545, 'preemption_count': 0}), (24263, {'train/ctc_loss': Array(0.2430587, dtype=float32), 'train/wer': 0.09053671719944259, 'validation/ctc_loss': Array(0.5190067, dtype=float32), 'validation/wer': 0.15627021442984446, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31107822, dtype=float32), 'test/wer': 0.10448276562468263, 'test/num_examples': 2472, 'score': 20225.373789548874, 'total_duration': 22267.058785676956, 'accumulated_submission_time': 20225.373789548874, 'accumulated_eval_time': 2039.88334441185, 'accumulated_logging_time': 0.7225484848022461, 'global_step': 24263, 'preemption_count': 0}), (25979, {'train/ctc_loss': Array(0.22930108, dtype=float32), 'train/wer': 0.08657502803933331, 'validation/ctc_loss': Array(0.51106757, dtype=float32), 'validation/wer': 0.15343174643019203, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3014969, dtype=float32), 'test/wer': 0.10139540552068735, 'test/num_examples': 2472, 'score': 21665.884313106537, 'total_duration': 23842.28617978096, 'accumulated_submission_time': 21665.884313106537, 'accumulated_eval_time': 2174.472556114197, 'accumulated_logging_time': 0.7736678123474121, 'global_step': 25979, 'preemption_count': 0}), (27691, {'train/ctc_loss': Array(0.22043964, dtype=float32), 'train/wer': 0.07960438166736558, 'validation/ctc_loss': Array(0.49129182, dtype=float32), 'validation/wer': 0.14787066626760767, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2954965, dtype=float32), 'test/wer': 0.09869396542969147, 'test/num_examples': 2472, 'score': 23106.38948559761, 'total_duration': 25422.112336874008, 'accumulated_submission_time': 23106.38948559761, 'accumulated_eval_time': 2313.663006067276, 'accumulated_logging_time': 0.8280179500579834, 'global_step': 27691, 'preemption_count': 0}), (29397, {'train/ctc_loss': Array(0.23616512, dtype=float32), 'train/wer': 0.08828978159126365, 'validation/ctc_loss': Array(0.48435923, dtype=float32), 'validation/wer': 0.14642246830860134, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28681365, dtype=float32), 'test/wer': 0.09666280746653667, 'test/num_examples': 2472, 'score': 24547.143973588943, 'total_duration': 26998.507881879807, 'accumulated_submission_time': 24547.143973588943, 'accumulated_eval_time': 2449.1744163036346, 'accumulated_logging_time': 0.8814880847930908, 'global_step': 29397, 'preemption_count': 0}), (31119, {'train/ctc_loss': Array(0.21892151, dtype=float32), 'train/wer': 0.07905631609504121, 'validation/ctc_loss': Array(0.47866896, dtype=float32), 'validation/wer': 0.14374812941096962, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28513002, dtype=float32), 'test/wer': 0.09580972112201167, 'test/num_examples': 2472, 'score': 25987.656771183014, 'total_duration': 28574.90484571457, 'accumulated_submission_time': 25987.656771183014, 'accumulated_eval_time': 2584.9278602600098, 'accumulated_logging_time': 0.9351787567138672, 'global_step': 31119, 'preemption_count': 0}), (32826, {'train/ctc_loss': Array(0.22863874, dtype=float32), 'train/wer': 0.08293051281919565, 'validation/ctc_loss': Array(0.47382867, dtype=float32), 'validation/wer': 0.14113171843169817, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27610952, dtype=float32), 'test/wer': 0.09363638210143603, 'test/num_examples': 2472, 'score': 27427.883678913116, 'total_duration': 30153.691357135773, 'accumulated_submission_time': 27427.883678913116, 'accumulated_eval_time': 2723.3589627742767, 'accumulated_logging_time': 0.9874227046966553, 'global_step': 32826, 'preemption_count': 0}), (34538, {'train/ctc_loss': Array(0.21834032, dtype=float32), 'train/wer': 0.07759732301757302, 'validation/ctc_loss': Array(0.46095514, dtype=float32), 'validation/wer': 0.1372602025546212, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26683512, dtype=float32), 'test/wer': 0.09014279040480978, 'test/num_examples': 2472, 'score': 28867.750519037247, 'total_duration': 31728.63215994835, 'accumulated_submission_time': 28867.750519037247, 'accumulated_eval_time': 2858.2907309532166, 'accumulated_logging_time': 1.050553798675537, 'global_step': 34538, 'preemption_count': 0}), (36249, {'train/ctc_loss': Array(0.17301418, dtype=float32), 'train/wer': 0.06496827690825416, 'validation/ctc_loss': Array(0.4529431, dtype=float32), 'validation/wer': 0.13483688463655058, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26671857, dtype=float32), 'test/wer': 0.08983811671033656, 'test/num_examples': 2472, 'score': 30308.221432209015, 'total_duration': 33309.32222747803, 'accumulated_submission_time': 30308.221432209015, 'accumulated_eval_time': 2998.3748049736023, 'accumulated_logging_time': 1.1091015338897705, 'global_step': 36249, 'preemption_count': 0}), (37952, {'train/ctc_loss': Array(0.19108309, dtype=float32), 'train/wer': 0.06909040991742518, 'validation/ctc_loss': Array(0.43196315, dtype=float32), 'validation/wer': 0.13027988839221064, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25399256, dtype=float32), 'test/wer': 0.08425243231166088, 'test/num_examples': 2472, 'score': 31748.563665390015, 'total_duration': 34886.196565151215, 'accumulated_submission_time': 31748.563665390015, 'accumulated_eval_time': 3134.772925376892, 'accumulated_logging_time': 1.1662836074829102, 'global_step': 37952, 'preemption_count': 0}), (39671, {'train/ctc_loss': Array(0.23817594, dtype=float32), 'train/wer': 0.08681242982594121, 'validation/ctc_loss': Array(0.43086866, dtype=float32), 'validation/wer': 0.12896685557604487, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24960111, dtype=float32), 'test/wer': 0.08329778806897813, 'test/num_examples': 2472, 'score': 33188.59181380272, 'total_duration': 36462.046426296234, 'accumulated_submission_time': 33188.59181380272, 'accumulated_eval_time': 3270.4538078308105, 'accumulated_logging_time': 1.2276201248168945, 'global_step': 39671, 'preemption_count': 0}), (41382, {'train/ctc_loss': Array(0.2418203, dtype=float32), 'train/wer': 0.08833832027845473, 'validation/ctc_loss': Array(0.4284742, dtype=float32), 'validation/wer': 0.1280110449231007, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24681735, dtype=float32), 'test/wer': 0.0825665712022424, 'test/num_examples': 2472, 'score': 34628.68599200249, 'total_duration': 38034.938487529755, 'accumulated_submission_time': 34628.68599200249, 'accumulated_eval_time': 3403.1156027317047, 'accumulated_logging_time': 1.2870965003967285, 'global_step': 41382, 'preemption_count': 0}), (43104, {'train/ctc_loss': Array(0.2755499, dtype=float32), 'train/wer': 0.10171335645886834, 'validation/ctc_loss': Array(0.4144993, dtype=float32), 'validation/wer': 0.12386919876034255, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23967984, dtype=float32), 'test/wer': 0.08124631852619178, 'test/num_examples': 2472, 'score': 36068.79007220268, 'total_duration': 39607.35124826431, 'accumulated_submission_time': 36068.79007220268, 'accumulated_eval_time': 3535.2889487743378, 'accumulated_logging_time': 1.3462481498718262, 'global_step': 43104, 'preemption_count': 0}), (44819, {'train/ctc_loss': Array(0.23959982, dtype=float32), 'train/wer': 0.08535817108137228, 'validation/ctc_loss': Array(0.40491655, dtype=float32), 'validation/wer': 0.12032594108730703, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23447508, dtype=float32), 'test/wer': 0.07935734162045782, 'test/num_examples': 2472, 'score': 37508.72058033943, 'total_duration': 41181.65663433075, 'accumulated_submission_time': 37508.72058033943, 'accumulated_eval_time': 3669.527673482895, 'accumulated_logging_time': 1.4051265716552734, 'global_step': 44819, 'preemption_count': 0}), (46534, {'train/ctc_loss': Array(0.22094382, dtype=float32), 'train/wer': 0.0819313462416354, 'validation/ctc_loss': Array(0.40488222, dtype=float32), 'validation/wer': 0.1207990190872491, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23017377, dtype=float32), 'test/wer': 0.07783397314809172, 'test/num_examples': 2472, 'score': 38949.01678466797, 'total_duration': 42756.07475566864, 'accumulated_submission_time': 38949.01678466797, 'accumulated_eval_time': 3803.5163989067078, 'accumulated_logging_time': 1.4624698162078857, 'global_step': 46534, 'preemption_count': 0}), (48243, {'train/ctc_loss': Array(0.18676206, dtype=float32), 'train/wer': 0.07017477079685477, 'validation/ctc_loss': Array(0.39040998, dtype=float32), 'validation/wer': 0.11653166243471041, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22285704, dtype=float32), 'test/wer': 0.07492941726078037, 'test/num_examples': 2472, 'score': 40389.19332933426, 'total_duration': 44331.88853049278, 'accumulated_submission_time': 40389.19332933426, 'accumulated_eval_time': 3939.021583557129, 'accumulated_logging_time': 1.5191435813903809, 'global_step': 48243, 'preemption_count': 0}), (49950, {'train/ctc_loss': Array(0.20000587, dtype=float32), 'train/wer': 0.07463326942816643, 'validation/ctc_loss': Array(0.3872141, dtype=float32), 'validation/wer': 0.11352906533303726, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21055159, dtype=float32), 'test/wer': 0.07072492027704995, 'test/num_examples': 2472, 'score': 41829.21695446968, 'total_duration': 45906.164083480835, 'accumulated_submission_time': 41829.21695446968, 'accumulated_eval_time': 4073.134298324585, 'accumulated_logging_time': 1.579833984375, 'global_step': 49950, 'preemption_count': 0}), (51667, {'train/ctc_loss': Array(0.17628975, dtype=float32), 'train/wer': 0.06667668785432056, 'validation/ctc_loss': Array(0.3812173, dtype=float32), 'validation/wer': 0.11206155806791084, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20984745, dtype=float32), 'test/wer': 0.07082647817520768, 'test/num_examples': 2472, 'score': 43269.77072787285, 'total_duration': 47481.20555949211, 'accumulated_submission_time': 43269.77072787285, 'accumulated_eval_time': 4207.490744113922, 'accumulated_logging_time': 1.6347463130950928, 'global_step': 51667, 'preemption_count': 0}), (53392, {'train/ctc_loss': Array(0.17431833, dtype=float32), 'train/wer': 0.06628950194729819, 'validation/ctc_loss': Array(0.36423317, dtype=float32), 'validation/wer': 0.10812245961941358, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20121741, dtype=float32), 'test/wer': 0.06851095809721122, 'test/num_examples': 2472, 'score': 44709.96631407738, 'total_duration': 49057.63871669769, 'accumulated_submission_time': 44709.96631407738, 'accumulated_eval_time': 4343.593742609024, 'accumulated_logging_time': 1.6926517486572266, 'global_step': 53392, 'preemption_count': 0}), (55121, {'train/ctc_loss': Array(0.16579323, dtype=float32), 'train/wer': 0.062470669427191164, 'validation/ctc_loss': Array(0.36580062, dtype=float32), 'validation/wer': 0.10823831545613409, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20039488, dtype=float32), 'test/wer': 0.06753600227489692, 'test/num_examples': 2472, 'score': 46150.562536239624, 'total_duration': 50635.04708957672, 'accumulated_submission_time': 46150.562536239624, 'accumulated_eval_time': 4480.270308256149, 'accumulated_logging_time': 1.7505762577056885, 'global_step': 55121, 'preemption_count': 0}), (56851, {'train/ctc_loss': Array(0.16412912, dtype=float32), 'train/wer': 0.061519178599636, 'validation/ctc_loss': Array(0.35546932, dtype=float32), 'validation/wer': 0.10464678451779835, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19666456, dtype=float32), 'test/wer': 0.06631730749700404, 'test/num_examples': 2472, 'score': 47590.833641052246, 'total_duration': 52208.10142946243, 'accumulated_submission_time': 47590.833641052246, 'accumulated_eval_time': 4612.914803504944, 'accumulated_logging_time': 1.8098342418670654, 'global_step': 56851, 'preemption_count': 0}), (58559, {'train/ctc_loss': Array(0.16294, dtype=float32), 'train/wer': 0.06126727583615592, 'validation/ctc_loss': Array(0.34518048, dtype=float32), 'validation/wer': 0.10165384206918525, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19008121, dtype=float32), 'test/wer': 0.06274246948185161, 'test/num_examples': 2472, 'score': 49030.73461127281, 'total_duration': 53784.438963890076, 'accumulated_submission_time': 49030.73461127281, 'accumulated_eval_time': 4749.212601184845, 'accumulated_logging_time': 1.8706691265106201, 'global_step': 58559, 'preemption_count': 0}), (60285, {'train/ctc_loss': Array(0.13095543, dtype=float32), 'train/wer': 0.05084403793126639, 'validation/ctc_loss': Array(0.33887717, dtype=float32), 'validation/wer': 0.09931741602865501, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18431841, dtype=float32), 'test/wer': 0.060386326244592045, 'test/num_examples': 2472, 'score': 50470.985605716705, 'total_duration': 55361.18985915184, 'accumulated_submission_time': 50470.985605716705, 'accumulated_eval_time': 4885.57377076149, 'accumulated_logging_time': 1.9312732219696045, 'global_step': 60285, 'preemption_count': 0}), (62016, {'train/ctc_loss': Array(0.12969963, dtype=float32), 'train/wer': 0.049176212861119985, 'validation/ctc_loss': Array(0.3313753, dtype=float32), 'validation/wer': 0.09697133533506473, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17523469, dtype=float32), 'test/wer': 0.05945199358154084, 'test/num_examples': 2472, 'score': 51911.11315703392, 'total_duration': 56937.3402159214, 'accumulated_submission_time': 51911.11315703392, 'accumulated_eval_time': 5021.4548535346985, 'accumulated_logging_time': 1.996178150177002, 'global_step': 62016, 'preemption_count': 0}), (63722, {'train/ctc_loss': Array(0.11556399, dtype=float32), 'train/wer': 0.044251885908137306, 'validation/ctc_loss': Array(0.32159543, dtype=float32), 'validation/wer': 0.09288741709066685, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17089857, dtype=float32), 'test/wer': 0.05750208193691223, 'test/num_examples': 2472, 'score': 53351.861990213394, 'total_duration': 58512.570706129074, 'accumulated_submission_time': 53351.861990213394, 'accumulated_eval_time': 5155.799608707428, 'accumulated_logging_time': 2.057328224182129, 'global_step': 63722, 'preemption_count': 0}), (65443, {'train/ctc_loss': Array(0.11451922, dtype=float32), 'train/wer': 0.04346464583202856, 'validation/ctc_loss': Array(0.3201817, dtype=float32), 'validation/wer': 0.09174816802958186, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1703582, dtype=float32), 'test/wer': 0.056384945057177095, 'test/num_examples': 2472, 'score': 54792.02154159546, 'total_duration': 60088.04986286163, 'accumulated_submission_time': 54792.02154159546, 'accumulated_eval_time': 5290.980396270752, 'accumulated_logging_time': 2.120563507080078, 'global_step': 65443, 'preemption_count': 0}), (67169, {'train/ctc_loss': Array(0.1024924, dtype=float32), 'train/wer': 0.03876814024731111, 'validation/ctc_loss': Array(0.31268138, dtype=float32), 'validation/wer': 0.08930554080539116, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16590431, dtype=float32), 'test/wer': 0.05441472183291694, 'test/num_examples': 2472, 'score': 56232.27780032158, 'total_duration': 61662.15571832657, 'accumulated_submission_time': 56232.27780032158, 'accumulated_eval_time': 5424.689416408539, 'accumulated_logging_time': 2.1815385818481445, 'global_step': 67169, 'preemption_count': 0}), (68876, {'train/ctc_loss': Array(0.09694151, dtype=float32), 'train/wer': 0.03730781576879227, 'validation/ctc_loss': Array(0.30519754, dtype=float32), 'validation/wer': 0.08692084149956071, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1626802, dtype=float32), 'test/wer': 0.053642881806918126, 'test/num_examples': 2472, 'score': 57672.292115449905, 'total_duration': 63235.09067606926, 'accumulated_submission_time': 57672.292115449905, 'accumulated_eval_time': 5557.467604875565, 'accumulated_logging_time': 2.247718572616577, 'global_step': 68876, 'preemption_count': 0}), (70607, {'train/ctc_loss': Array(0.07881192, dtype=float32), 'train/wer': 0.029981964460375522, 'validation/ctc_loss': Array(0.30443206, dtype=float32), 'validation/wer': 0.08557884472421484, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16090697, dtype=float32), 'test/wer': 0.054313163934759205, 'test/num_examples': 2472, 'score': 59112.44225859642, 'total_duration': 64809.5007917881, 'accumulated_submission_time': 59112.44225859642, 'accumulated_eval_time': 5691.591963291168, 'accumulated_logging_time': 2.307018518447876, 'global_step': 70607, 'preemption_count': 0}), (72323, {'train/ctc_loss': Array(0.07315466, dtype=float32), 'train/wer': 0.027833001988071572, 'validation/ctc_loss': Array(0.2974776, dtype=float32), 'validation/wer': 0.08360929549996621, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15847631, dtype=float32), 'test/wer': 0.051266426990027016, 'test/num_examples': 2472, 'score': 60552.34362244606, 'total_duration': 66382.07028150558, 'accumulated_submission_time': 60552.34362244606, 'accumulated_eval_time': 5824.119750261307, 'accumulated_logging_time': 2.368692636489868, 'global_step': 72323, 'preemption_count': 0})], 'global_step': 72950}
I0214 21:20:27.977493 139646656866112 submission_runner.py:586] Timing: 61068.561847925186
I0214 21:20:27.977574 139646656866112 submission_runner.py:588] Total number of evals: 43
I0214 21:20:27.977637 139646656866112 submission_runner.py:589] ====================
I0214 21:20:27.977700 139646656866112 submission_runner.py:542] Using RNG seed 1365630931
I0214 21:20:27.980152 139646656866112 submission_runner.py:551] --- Tuning run 2/5 ---
I0214 21:20:27.980288 139646656866112 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_2/librispeech_conformer_jax/trial_2.
I0214 21:20:27.982458 139646656866112 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_2/librispeech_conformer_jax/trial_2/hparams.json.
I0214 21:20:27.984585 139646656866112 submission_runner.py:206] Initializing dataset.
I0214 21:20:27.984724 139646656866112 submission_runner.py:213] Initializing model.
I0214 21:20:31.654158 139646656866112 submission_runner.py:255] Initializing optimizer.
I0214 21:20:32.202193 139646656866112 submission_runner.py:262] Initializing metrics bundle.
I0214 21:20:32.202407 139646656866112 submission_runner.py:280] Initializing checkpoint and logger.
I0214 21:20:32.313951 139646656866112 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_2/librispeech_conformer_jax/trial_2 with prefix checkpoint_
I0214 21:20:32.314116 139646656866112 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_2/librispeech_conformer_jax/trial_2/meta_data_0.json.
I0214 21:20:32.314366 139646656866112 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0214 21:20:32.314447 139646656866112 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0214 21:20:32.905250 139646656866112 logger_utils.py:220] Unable to record git information. Continuing without it.
I0214 21:20:33.385854 139646656866112 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_2/librispeech_conformer_jax/trial_2/flags_0.json.
I0214 21:20:33.499056 139646656866112 submission_runner.py:314] Starting training loop.
I0214 21:20:33.502222 139646656866112 input_pipeline.py:20] Loading split = train-clean-100
I0214 21:20:33.548753 139646656866112 input_pipeline.py:20] Loading split = train-clean-360
I0214 21:20:34.039968 139646656866112 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0214 21:21:10.256043 139474709726976 logging_writer.py:48] [0] global_step=0, grad_norm=42.58770751953125, loss=31.96015167236328
I0214 21:21:10.275364 139646656866112 spec.py:321] Evaluating on the training split.
I0214 21:22:05.516633 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 21:23:01.247339 139646656866112 spec.py:349] Evaluating on the test split.
I0214 21:23:29.071862 139646656866112 submission_runner.py:408] Time since start: 175.57s, 	Step: 1, 	{'train/ctc_loss': Array(32.058502, dtype=float32), 'train/wer': 1.1703602515325648, 'validation/ctc_loss': Array(30.757864, dtype=float32), 'validation/wer': 1.1779352558965794, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.871214, dtype=float32), 'test/wer': 1.1899945158734995, 'test/num_examples': 2472, 'score': 36.7762336730957, 'total_duration': 175.56985116004944, 'accumulated_submission_time': 36.7762336730957, 'accumulated_eval_time': 138.79355025291443, 'accumulated_logging_time': 0}
I0214 21:23:29.089702 139535991158528 logging_writer.py:48] [1] accumulated_eval_time=138.793550, accumulated_logging_time=0, accumulated_submission_time=36.776234, global_step=1, preemption_count=0, score=36.776234, test/ctc_loss=30.871213912963867, test/num_examples=2472, test/wer=1.189995, total_duration=175.569851, train/ctc_loss=32.058502197265625, train/wer=1.170360, validation/ctc_loss=30.757863998413086, validation/num_examples=5348, validation/wer=1.177935
I0214 21:25:11.130502 139474701334272 logging_writer.py:48] [100] global_step=100, grad_norm=2.1679136753082275, loss=6.536868572235107
I0214 21:26:28.076962 139474709726976 logging_writer.py:48] [200] global_step=200, grad_norm=0.6594303250312805, loss=5.916638374328613
I0214 21:27:44.772407 139474701334272 logging_writer.py:48] [300] global_step=300, grad_norm=0.5430813431739807, loss=5.84232759475708
I0214 21:29:01.952296 139474709726976 logging_writer.py:48] [400] global_step=400, grad_norm=5.532430171966553, loss=5.834542274475098
I0214 21:30:18.704298 139474701334272 logging_writer.py:48] [500] global_step=500, grad_norm=5.9279608726501465, loss=5.836406230926514
I0214 21:31:45.191955 139474709726976 logging_writer.py:48] [600] global_step=600, grad_norm=6.027343273162842, loss=5.844460964202881
I0214 21:33:13.218372 139474701334272 logging_writer.py:48] [700] global_step=700, grad_norm=0.44281265139579773, loss=5.794848918914795
I0214 21:34:41.974267 139474709726976 logging_writer.py:48] [800] global_step=800, grad_norm=0.7825108766555786, loss=5.798374176025391
I0214 21:36:09.010506 139474701334272 logging_writer.py:48] [900] global_step=900, grad_norm=1.7148659229278564, loss=5.7940521240234375
I0214 21:37:37.686283 139474709726976 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.6957574486732483, loss=5.763076305389404
I0214 21:39:02.298297 139535991158528 logging_writer.py:48] [1100] global_step=1100, grad_norm=3.003000497817993, loss=5.7290120124816895
I0214 21:40:18.990523 139535982765824 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.34562501311302185, loss=5.55514669418335
I0214 21:41:39.623338 139535991158528 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.2724117040634155, loss=5.470683574676514
I0214 21:43:04.615235 139535982765824 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.659446954727173, loss=5.317224979400635
I0214 21:44:30.106664 139535991158528 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.8308698534965515, loss=4.700091361999512
I0214 21:45:59.907846 139535982765824 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.649501323699951, loss=4.176154613494873
I0214 21:47:25.523876 139535991158528 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.4203461408615112, loss=3.8417553901672363
I0214 21:47:29.651350 139646656866112 spec.py:321] Evaluating on the training split.
I0214 21:48:08.977338 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 21:48:57.416434 139646656866112 spec.py:349] Evaluating on the test split.
I0214 21:49:22.682444 139646656866112 submission_runner.py:408] Time since start: 1729.18s, 	Step: 1706, 	{'train/ctc_loss': Array(6.2620625, dtype=float32), 'train/wer': 0.9306712172923777, 'validation/ctc_loss': Array(6.2103653, dtype=float32), 'validation/wer': 0.8912886065439238, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.156824, dtype=float32), 'test/wer': 0.8923486279527959, 'test/num_examples': 2472, 'score': 1477.254374742508, 'total_duration': 1729.1772775650024, 'accumulated_submission_time': 1477.254374742508, 'accumulated_eval_time': 251.8186011314392, 'accumulated_logging_time': 0.031685590744018555}
I0214 21:49:22.716865 139535991158528 logging_writer.py:48] [1706] accumulated_eval_time=251.818601, accumulated_logging_time=0.031686, accumulated_submission_time=1477.254375, global_step=1706, preemption_count=0, score=1477.254375, test/ctc_loss=6.156824111938477, test/num_examples=2472, test/wer=0.892349, total_duration=1729.177278, train/ctc_loss=6.2620625495910645, train/wer=0.930671, validation/ctc_loss=6.210365295410156, validation/num_examples=5348, validation/wer=0.891289
I0214 21:50:35.322703 139535982765824 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.604952335357666, loss=3.6524722576141357
I0214 21:51:52.032541 139535991158528 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.6441811323165894, loss=3.4518487453460693
I0214 21:53:18.412709 139535982765824 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.5833392143249512, loss=3.3148880004882812
I0214 21:54:43.969831 139535991158528 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.9745583534240723, loss=3.1989119052886963
I0214 21:56:01.824810 139535982765824 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.4019674062728882, loss=3.086174726486206
I0214 21:57:19.021190 139535991158528 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.191656231880188, loss=3.075038433074951
I0214 21:58:40.113434 139535982765824 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.6654436588287354, loss=2.882110357284546
I0214 22:00:07.114692 139535991158528 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.137773871421814, loss=2.817430257797241
I0214 22:01:36.108176 139535982765824 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.9515344500541687, loss=2.8154921531677246
I0214 22:03:04.588446 139535991158528 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.9500308036804199, loss=2.7183589935302734
I0214 22:04:35.127593 139535982765824 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.9759305715560913, loss=2.6775500774383545
I0214 22:06:08.127404 139535991158528 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.8720945715904236, loss=2.582252264022827
I0214 22:07:40.482012 139535982765824 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.9825709462165833, loss=2.6292243003845215
I0214 22:09:14.822967 139535991158528 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.3676620721817017, loss=2.4701058864593506
I0214 22:10:32.077149 139535982765824 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.9889243245124817, loss=2.5134425163269043
I0214 22:11:48.923379 139535991158528 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.7655982971191406, loss=2.4417388439178467
I0214 22:13:11.126359 139535982765824 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.8949382901191711, loss=2.3710379600524902
I0214 22:13:23.201428 139646656866112 spec.py:321] Evaluating on the training split.
I0214 22:14:15.939555 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 22:15:09.208456 139646656866112 spec.py:349] Evaluating on the test split.
I0214 22:15:36.526960 139646656866112 submission_runner.py:408] Time since start: 3303.03s, 	Step: 3416, 	{'train/ctc_loss': Array(2.681486, dtype=float32), 'train/wer': 0.573457583199163, 'validation/ctc_loss': Array(2.6309824, dtype=float32), 'validation/wer': 0.5461540689535321, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.3015385, dtype=float32), 'test/wer': 0.5043974569902301, 'test/num_examples': 2472, 'score': 2917.648540019989, 'total_duration': 3303.025156736374, 'accumulated_submission_time': 2917.648540019989, 'accumulated_eval_time': 385.1414303779602, 'accumulated_logging_time': 0.0872964859008789}
I0214 22:15:36.555636 139535991158528 logging_writer.py:48] [3416] accumulated_eval_time=385.141430, accumulated_logging_time=0.087296, accumulated_submission_time=2917.648540, global_step=3416, preemption_count=0, score=2917.648540, test/ctc_loss=2.3015384674072266, test/num_examples=2472, test/wer=0.504397, total_duration=3303.025157, train/ctc_loss=2.681485891342163, train/wer=0.573458, validation/ctc_loss=2.6309823989868164, validation/num_examples=5348, validation/wer=0.546154
I0214 22:16:41.168388 139535982765824 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.9309683442115784, loss=2.4096570014953613
I0214 22:17:57.636400 139535991158528 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.7850849032402039, loss=2.2699499130249023
I0214 22:19:20.360095 139535982765824 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.8606612086296082, loss=2.2079195976257324
I0214 22:20:47.869548 139535991158528 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.7224233150482178, loss=2.181234359741211
I0214 22:22:14.417692 139535982765824 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.9775075316429138, loss=2.2403318881988525
I0214 22:23:44.321626 139535991158528 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.8671827912330627, loss=2.159829616546631
I0214 22:25:11.397414 139535982765824 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.8003299832344055, loss=2.1578567028045654
I0214 22:26:35.433651 139535991158528 logging_writer.py:48] [4200] global_step=4200, grad_norm=1.2367799282073975, loss=2.049534320831299
I0214 22:27:53.328534 139535982765824 logging_writer.py:48] [4300] global_step=4300, grad_norm=1.1098723411560059, loss=1.9767427444458008
I0214 22:29:10.972507 139535991158528 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.9603734612464905, loss=1.9803502559661865
I0214 22:30:35.196413 139535982765824 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.4047383069992065, loss=1.9922749996185303
I0214 22:32:01.712836 139535991158528 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.7779719829559326, loss=1.91432785987854
I0214 22:33:29.576293 139535982765824 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.7659651041030884, loss=1.9188425540924072
I0214 22:34:55.876099 139535991158528 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.78035569190979, loss=1.8938493728637695
I0214 22:36:24.528472 139535982765824 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.7457156181335449, loss=1.9074311256408691
I0214 22:37:56.847164 139535991158528 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.8705865740776062, loss=1.892235279083252
I0214 22:39:25.904455 139535982765824 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.6556100249290466, loss=1.8980151414871216
I0214 22:39:36.599324 139646656866112 spec.py:321] Evaluating on the training split.
I0214 22:40:31.165055 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 22:41:25.304069 139646656866112 spec.py:349] Evaluating on the test split.
I0214 22:41:52.372906 139646656866112 submission_runner.py:408] Time since start: 4878.87s, 	Step: 5113, 	{'train/ctc_loss': Array(0.8880565, dtype=float32), 'train/wer': 0.2816791786266284, 'validation/ctc_loss': Array(0.9799351, dtype=float32), 'validation/wer': 0.29021887098487115, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.699066, dtype=float32), 'test/wer': 0.2274084455548108, 'test/num_examples': 2472, 'score': 4357.607877492905, 'total_duration': 4878.8678069114685, 'accumulated_submission_time': 4357.607877492905, 'accumulated_eval_time': 520.9090373516083, 'accumulated_logging_time': 0.13166284561157227}
I0214 22:41:52.406900 139535991158528 logging_writer.py:48] [5113] accumulated_eval_time=520.909037, accumulated_logging_time=0.131663, accumulated_submission_time=4357.607877, global_step=5113, preemption_count=0, score=4357.607877, test/ctc_loss=0.6990659832954407, test/num_examples=2472, test/wer=0.227408, total_duration=4878.867807, train/ctc_loss=0.8880565166473389, train/wer=0.281679, validation/ctc_loss=0.9799351096153259, validation/num_examples=5348, validation/wer=0.290219
I0214 22:43:04.897415 139535991158528 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.8707384467124939, loss=1.8995418548583984
I0214 22:44:23.528067 139535982765824 logging_writer.py:48] [5300] global_step=5300, grad_norm=1.0228811502456665, loss=1.853869915008545
I0214 22:45:43.540462 139535991158528 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.8296539783477783, loss=1.8065261840820312
I0214 22:47:06.076766 139535982765824 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.7612463235855103, loss=1.8096469640731812
I0214 22:48:30.108747 139535991158528 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.7657698392868042, loss=1.763886570930481
I0214 22:49:57.461213 139535982765824 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.6867442727088928, loss=1.8257684707641602
I0214 22:51:26.000062 139535991158528 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.9574745893478394, loss=1.8302167654037476
I0214 22:52:54.106127 139535982765824 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.8371715545654297, loss=1.7687498331069946
I0214 22:54:26.215328 139535991158528 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.9456377625465393, loss=1.7845072746276855
I0214 22:55:55.006442 139535982765824 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.6309157609939575, loss=1.6904568672180176
I0214 22:57:25.080163 139535991158528 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.7010780572891235, loss=1.7087959051132202
I0214 22:58:41.802296 139535982765824 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.5917444229125977, loss=1.7193114757537842
I0214 22:59:58.470991 139535991158528 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.6978045105934143, loss=1.7161167860031128
I0214 23:01:22.764909 139535982765824 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.6312142610549927, loss=1.720166802406311
I0214 23:02:45.819440 139535991158528 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.7468717098236084, loss=1.6409839391708374
I0214 23:04:13.442444 139535982765824 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.7662524580955505, loss=1.690390706062317
I0214 23:05:41.526520 139535991158528 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.669717013835907, loss=1.6497920751571655
I0214 23:05:52.868753 139646656866112 spec.py:321] Evaluating on the training split.
I0214 23:06:46.143569 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 23:07:40.567256 139646656866112 spec.py:349] Evaluating on the test split.
I0214 23:08:08.888638 139646656866112 submission_runner.py:408] Time since start: 6455.38s, 	Step: 6814, 	{'train/ctc_loss': Array(0.61878496, dtype=float32), 'train/wer': 0.21151884362014212, 'validation/ctc_loss': Array(0.7577799, dtype=float32), 'validation/wer': 0.22992556262490707, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5053952, dtype=float32), 'test/wer': 0.17023134889200334, 'test/num_examples': 2472, 'score': 5797.983711004257, 'total_duration': 6455.382749795914, 'accumulated_submission_time': 5797.983711004257, 'accumulated_eval_time': 656.9221382141113, 'accumulated_logging_time': 0.18206191062927246}
I0214 23:08:08.924628 139535991158528 logging_writer.py:48] [6814] accumulated_eval_time=656.922138, accumulated_logging_time=0.182062, accumulated_submission_time=5797.983711, global_step=6814, preemption_count=0, score=5797.983711, test/ctc_loss=0.5053951740264893, test/num_examples=2472, test/wer=0.170231, total_duration=6455.382750, train/ctc_loss=0.6187849640846252, train/wer=0.211519, validation/ctc_loss=0.7577798962593079, validation/num_examples=5348, validation/wer=0.229926
I0214 23:09:15.379543 139535982765824 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.7239517569541931, loss=1.6954107284545898
I0214 23:10:31.319347 139535991158528 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.5894424915313721, loss=1.6330994367599487
I0214 23:11:55.937039 139535982765824 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.6794280409812927, loss=1.6372066736221313
I0214 23:13:27.398015 139535991158528 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.6319640874862671, loss=1.6987709999084473
I0214 23:14:48.718729 139535991158528 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.5973013639450073, loss=1.6171029806137085
I0214 23:16:05.041695 139535982765824 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.758126974105835, loss=1.6425976753234863
I0214 23:17:26.143387 139535991158528 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.7328809499740601, loss=1.6036418676376343
I0214 23:18:49.888539 139535982765824 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.6209434270858765, loss=1.580259919166565
I0214 23:20:16.548404 139535991158528 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.7014625668525696, loss=1.6058317422866821
I0214 23:21:46.002416 139535982765824 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.5605932474136353, loss=1.5773530006408691
I0214 23:23:17.520509 139535991158528 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.808123767375946, loss=1.6732019186019897
I0214 23:24:48.400824 139535982765824 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.6585997343063354, loss=1.596293330192566
I0214 23:26:20.420606 139535991158528 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.6401792168617249, loss=1.6003108024597168
I0214 23:27:51.458617 139535982765824 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.6460670828819275, loss=1.5595611333847046
I0214 23:29:16.243270 139535991158528 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.6505008339881897, loss=1.5857913494110107
I0214 23:30:32.617851 139535982765824 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.553122341632843, loss=1.5322884321212769
I0214 23:31:48.602400 139535991158528 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.8582310080528259, loss=1.5823780298233032
I0214 23:32:08.929829 139646656866112 spec.py:321] Evaluating on the training split.
I0214 23:33:03.285106 139646656866112 spec.py:333] Evaluating on the validation split.
I0214 23:33:58.804646 139646656866112 spec.py:349] Evaluating on the test split.
I0214 23:34:26.458903 139646656866112 submission_runner.py:408] Time since start: 8032.96s, 	Step: 8528, 	{'train/ctc_loss': Array(0.5105811, dtype=float32), 'train/wer': 0.17519140059552968, 'validation/ctc_loss': Array(0.6722918, dtype=float32), 'validation/wer': 0.20403178311787365, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43440384, dtype=float32), 'test/wer': 0.14709645969167023, 'test/num_examples': 2472, 'score': 7237.903148651123, 'total_duration': 8032.956871747971, 'accumulated_submission_time': 7237.903148651123, 'accumulated_eval_time': 794.4483127593994, 'accumulated_logging_time': 0.23373818397521973}
I0214 23:34:26.494373 139535991158528 logging_writer.py:48] [8528] accumulated_eval_time=794.448313, accumulated_logging_time=0.233738, accumulated_submission_time=7237.903149, global_step=8528, preemption_count=0, score=7237.903149, test/ctc_loss=0.43440383672714233, test/num_examples=2472, test/wer=0.147096, total_duration=8032.956872, train/ctc_loss=0.5105810761451721, train/wer=0.175191, validation/ctc_loss=0.6722918152809143, validation/num_examples=5348, validation/wer=0.204032
I0214 23:35:22.011949 139535982765824 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.6801677942276001, loss=1.5628435611724854
I0214 23:36:37.959725 139535991158528 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.608916163444519, loss=1.5547528266906738
I0214 23:37:57.348425 139535982765824 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.567176103591919, loss=1.5781205892562866
I0214 23:39:27.844965 139535991158528 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.7299677729606628, loss=1.5753885507583618
I0214 23:40:56.167073 139535982765824 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.6996476650238037, loss=1.5768345594406128
I0214 23:42:26.934852 139535991158528 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.611624002456665, loss=1.5459917783737183
I0214 23:43:56.755821 139535982765824 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.6957777738571167, loss=1.5610655546188354
I0214 23:45:23.850494 139535991158528 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.5371684432029724, loss=1.5275273323059082
I0214 23:46:39.753698 139535982765824 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.6629652380943298, loss=1.5174751281738281
I0214 23:47:55.861201 139535991158528 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.8128657341003418, loss=1.5616698265075684
I0214 23:49:12.127653 139535982765824 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.943863570690155, loss=1.5116956233978271
I0214 23:50:30.305142 139535991158528 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.6780316829681396, loss=1.4941662549972534
I0214 23:51:58.217165 139535982765824 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.646468997001648, loss=1.454679012298584
I0214 23:53:28.530554 139535991158528 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.6197823882102966, loss=1.5395761728286743
I0214 23:54:57.913593 139535982765824 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.6768749356269836, loss=1.530210018157959
I0214 23:56:26.962520 139535991158528 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.5808559060096741, loss=1.4599679708480835
I0214 23:57:55.327408 139535982765824 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.7759920954704285, loss=1.5099457502365112
I0214 23:58:26.914383 139646656866112 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0214 23:59:47.525885 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 00:00:40.449163 139646656866112 spec.py:349] Evaluating on the test split.
I0215 00:01:07.229485 139646656866112 submission_runner.py:408] Time since start: 9633.72s, 	Step: 10237, 	{'train/ctc_loss': Array(0.30627134, dtype=float32), 'train/wer': 0.11301755653800374, 'validation/ctc_loss': Array(0.61254525, dtype=float32), 'validation/wer': 0.18793747646678316, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38682497, dtype=float32), 'test/wer': 0.13220807182174557, 'test/num_examples': 2472, 'score': 8678.238431215286, 'total_duration': 9633.724823951721, 'accumulated_submission_time': 8678.238431215286, 'accumulated_eval_time': 954.7578556537628, 'accumulated_logging_time': 0.2836422920227051}
I0215 00:01:07.265992 139535991158528 logging_writer.py:48] [10237] accumulated_eval_time=954.757856, accumulated_logging_time=0.283642, accumulated_submission_time=8678.238431, global_step=10237, preemption_count=0, score=8678.238431, test/ctc_loss=0.38682496547698975, test/num_examples=2472, test/wer=0.132208, total_duration=9633.724824, train/ctc_loss=0.30627134442329407, train/wer=0.113018, validation/ctc_loss=0.6125452518463135, validation/num_examples=5348, validation/wer=0.187937
I0215 00:01:59.426705 139535991158528 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.7065881490707397, loss=1.5131160020828247
I0215 00:03:16.812830 139535982765824 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.6832271218299866, loss=1.450029730796814
I0215 00:04:33.527786 139535991158528 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.5750352144241333, loss=1.4390157461166382
I0215 00:05:54.936808 139535982765824 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.5500687956809998, loss=1.4801719188690186
I0215 00:07:20.476649 139535991158528 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.6006468534469604, loss=1.5050204992294312
I0215 00:08:47.225563 139535982765824 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.77808678150177, loss=1.4871071577072144
I0215 00:10:16.468574 139535991158528 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.6836299896240234, loss=1.5226045846939087
I0215 00:11:47.315975 139535982765824 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.5484187602996826, loss=1.4514986276626587
I0215 00:13:16.229204 139535991158528 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.6254904270172119, loss=1.4590553045272827
I0215 00:14:40.991744 139535982765824 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.7301490306854248, loss=1.5073055028915405
I0215 00:16:09.700493 139535991158528 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.6173701882362366, loss=1.4920004606246948
I0215 00:17:32.885092 139535991158528 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.6257126927375793, loss=1.447361707687378
I0215 00:18:50.738802 139535982765824 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.5765413641929626, loss=1.4119584560394287
I0215 00:20:07.577327 139535991158528 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.5848878622055054, loss=1.3965102434158325
I0215 00:21:29.874782 139535982765824 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.672885000705719, loss=1.4267469644546509
I0215 00:22:56.819141 139535991158528 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.6948496699333191, loss=1.4198546409606934
I0215 00:24:21.738799 139535982765824 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.569460391998291, loss=1.456649661064148
I0215 00:25:07.531002 139646656866112 spec.py:321] Evaluating on the training split.
I0215 00:26:03.689331 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 00:26:57.317991 139646656866112 spec.py:349] Evaluating on the test split.
I0215 00:27:23.536167 139646656866112 submission_runner.py:408] Time since start: 11210.03s, 	Step: 11951, 	{'train/ctc_loss': Array(0.2763798, dtype=float32), 'train/wer': 0.10269464344157969, 'validation/ctc_loss': Array(0.579581, dtype=float32), 'validation/wer': 0.17735597671297681, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3629714, dtype=float32), 'test/wer': 0.12390063575244246, 'test/num_examples': 2472, 'score': 10118.417578220367, 'total_duration': 11210.029403686523, 'accumulated_submission_time': 10118.417578220367, 'accumulated_eval_time': 1090.7553856372833, 'accumulated_logging_time': 0.33667874336242676}
I0215 00:27:23.577196 139535991158528 logging_writer.py:48] [11951] accumulated_eval_time=1090.755386, accumulated_logging_time=0.336679, accumulated_submission_time=10118.417578, global_step=11951, preemption_count=0, score=10118.417578, test/ctc_loss=0.36297139525413513, test/num_examples=2472, test/wer=0.123901, total_duration=11210.029404, train/ctc_loss=0.27637979388237, train/wer=0.102695, validation/ctc_loss=0.5795810222625732, validation/num_examples=5348, validation/wer=0.177356
I0215 00:28:01.381085 139535982765824 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.5687713623046875, loss=1.3948252201080322
I0215 00:29:17.463962 139535991158528 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.7434194087982178, loss=1.4188214540481567
I0215 00:30:36.454454 139535982765824 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.5799126625061035, loss=1.4246641397476196
I0215 00:32:02.229091 139535991158528 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.6687334775924683, loss=1.424933671951294
I0215 00:33:30.537738 139535991158528 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.5260069370269775, loss=1.372286319732666
I0215 00:34:48.938369 139535982765824 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.765934407711029, loss=1.4110075235366821
I0215 00:36:07.655629 139535991158528 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.5831012725830078, loss=1.3997786045074463
I0215 00:37:30.614799 139535982765824 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.6242613196372986, loss=1.4115434885025024
I0215 00:38:55.190772 139535991158528 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.6374799609184265, loss=1.3506367206573486
I0215 00:40:23.857804 139535982765824 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.6704199910163879, loss=1.3870577812194824
I0215 00:41:55.123797 139535991158528 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.5756626725196838, loss=1.3999420404434204
I0215 00:43:26.286622 139535982765824 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.6581754684448242, loss=1.4608304500579834
I0215 00:44:53.653481 139535991158528 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.6678036451339722, loss=1.3970668315887451
I0215 00:46:19.985694 139535982765824 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.6473274827003479, loss=1.4114327430725098
I0215 00:47:52.459000 139535991158528 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.5810244679450989, loss=1.4027048349380493
I0215 00:49:11.523114 139535982765824 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.9792726039886475, loss=1.3650256395339966
I0215 00:50:28.234930 139535991158528 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.6196069121360779, loss=1.4029120206832886
I0215 00:51:23.673821 139646656866112 spec.py:321] Evaluating on the training split.
I0215 00:52:19.620048 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 00:53:12.654553 139646656866112 spec.py:349] Evaluating on the test split.
I0215 00:53:39.823719 139646656866112 submission_runner.py:408] Time since start: 12786.32s, 	Step: 13670, 	{'train/ctc_loss': Array(0.26991197, dtype=float32), 'train/wer': 0.09945294537382066, 'validation/ctc_loss': Array(0.5541362, dtype=float32), 'validation/wer': 0.16952605308128252, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3431652, dtype=float32), 'test/wer': 0.11863993662787155, 'test/num_examples': 2472, 'score': 11558.427836418152, 'total_duration': 12786.317202329636, 'accumulated_submission_time': 11558.427836418152, 'accumulated_eval_time': 1226.897890329361, 'accumulated_logging_time': 0.39374494552612305}
I0215 00:53:39.861554 139535991158528 logging_writer.py:48] [13670] accumulated_eval_time=1226.897890, accumulated_logging_time=0.393745, accumulated_submission_time=11558.427836, global_step=13670, preemption_count=0, score=11558.427836, test/ctc_loss=0.34316518902778625, test/num_examples=2472, test/wer=0.118640, total_duration=12786.317202, train/ctc_loss=0.2699119746685028, train/wer=0.099453, validation/ctc_loss=0.5541362166404724, validation/num_examples=5348, validation/wer=0.169526
I0215 00:54:03.753994 139535982765824 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.5588052868843079, loss=1.372267484664917
I0215 00:55:19.871421 139535991158528 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.5214335918426514, loss=1.38019597530365
I0215 00:56:35.898075 139535982765824 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.5985945463180542, loss=1.3907889127731323
I0215 00:58:03.612736 139535991158528 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.49199044704437256, loss=1.3982924222946167
I0215 00:59:33.379840 139535982765824 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.5619031190872192, loss=1.3308308124542236
I0215 01:01:03.131963 139535991158528 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.6402674913406372, loss=1.418548583984375
I0215 01:02:31.939395 139535982765824 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.587496817111969, loss=1.4138216972351074
I0215 01:04:01.082968 139535991158528 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.6160487532615662, loss=1.4182755947113037
I0215 01:05:24.112324 139535991158528 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.629219651222229, loss=1.3613289594650269
I0215 01:06:40.743894 139535982765824 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.539933443069458, loss=1.3379226922988892
I0215 01:08:00.781128 139535991158528 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.5901663303375244, loss=1.4094270467758179
I0215 01:09:22.810080 139535982765824 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.6462074518203735, loss=1.3220897912979126
I0215 01:10:49.289293 139535991158528 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.6948210000991821, loss=1.3406684398651123
I0215 01:12:18.437033 139535982765824 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.6516474485397339, loss=1.4011950492858887
I0215 01:13:49.380130 139535991158528 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.5935879349708557, loss=1.3734980821609497
I0215 01:15:20.467015 139535982765824 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.5435619950294495, loss=1.3161135911941528
I0215 01:16:52.412888 139535991158528 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.7534332275390625, loss=1.403186559677124
I0215 01:17:39.945381 139646656866112 spec.py:321] Evaluating on the training split.
I0215 01:18:36.645007 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 01:19:30.264853 139646656866112 spec.py:349] Evaluating on the test split.
I0215 01:19:57.696918 139646656866112 submission_runner.py:408] Time since start: 14364.19s, 	Step: 15354, 	{'train/ctc_loss': Array(0.2340234, dtype=float32), 'train/wer': 0.09087778612572, 'validation/ctc_loss': Array(0.5342885, dtype=float32), 'validation/wer': 0.16420633924519923, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32349566, dtype=float32), 'test/wer': 0.11061686267341012, 'test/num_examples': 2472, 'score': 12998.425715208054, 'total_duration': 14364.191707611084, 'accumulated_submission_time': 12998.425715208054, 'accumulated_eval_time': 1364.6433284282684, 'accumulated_logging_time': 0.4485950469970703}
I0215 01:19:57.731559 139535991158528 logging_writer.py:48] [15354] accumulated_eval_time=1364.643328, accumulated_logging_time=0.448595, accumulated_submission_time=12998.425715, global_step=15354, preemption_count=0, score=12998.425715, test/ctc_loss=0.32349565625190735, test/num_examples=2472, test/wer=0.110617, total_duration=14364.191708, train/ctc_loss=0.23402340710163116, train/wer=0.090878, validation/ctc_loss=0.5342885255813599, validation/num_examples=5348, validation/wer=0.164206
I0215 01:20:33.313760 139535982765824 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.5829387903213501, loss=1.3439676761627197
I0215 01:21:53.305183 139535991158528 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.6186060905456543, loss=1.3164739608764648
I0215 01:23:12.882378 139535982765824 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.6386776566505432, loss=1.2186610698699951
I0215 01:24:33.302380 139535991158528 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.7632829546928406, loss=1.3905831575393677
I0215 01:25:55.392183 139535982765824 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.6397918462753296, loss=1.3542088270187378
I0215 01:27:21.099572 139535991158528 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.6264479160308838, loss=1.2914458513259888
I0215 01:28:52.001296 139535982765824 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.6576693654060364, loss=1.3728275299072266
I0215 01:30:24.395445 139535991158528 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.6042109727859497, loss=1.2953981161117554
I0215 01:31:55.964217 139535982765824 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.5641908049583435, loss=1.290208101272583
I0215 01:33:27.626293 139535991158528 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.5159362554550171, loss=1.3898773193359375
I0215 01:34:55.859685 139535982765824 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.6223777532577515, loss=1.2810289859771729
I0215 01:36:27.464593 139535991158528 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.6858588457107544, loss=1.3059000968933105
I0215 01:37:44.751730 139535982765824 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.5601139068603516, loss=1.3175252676010132
I0215 01:39:02.003807 139535991158528 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.5243256092071533, loss=1.328953504562378
I0215 01:40:24.187318 139535982765824 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.5988259315490723, loss=1.3470544815063477
I0215 01:41:49.076169 139535991158528 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.5067787170410156, loss=1.3061630725860596
I0215 01:43:18.839340 139535982765824 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.6996454000473022, loss=1.3213163614273071
I0215 01:43:57.828788 139646656866112 spec.py:321] Evaluating on the training split.
I0215 01:44:52.601428 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 01:45:44.810081 139646656866112 spec.py:349] Evaluating on the test split.
I0215 01:46:11.710568 139646656866112 submission_runner.py:408] Time since start: 15938.21s, 	Step: 17048, 	{'train/ctc_loss': Array(0.23105614, dtype=float32), 'train/wer': 0.08641527741495221, 'validation/ctc_loss': Array(0.51330495, dtype=float32), 'validation/wer': 0.15523716655241993, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30940318, dtype=float32), 'test/wer': 0.10462494668210347, 'test/num_examples': 2472, 'score': 14438.438889980316, 'total_duration': 15938.205379962921, 'accumulated_submission_time': 14438.438889980316, 'accumulated_eval_time': 1498.5190238952637, 'accumulated_logging_time': 0.4984886646270752}
I0215 01:46:11.746409 139535991158528 logging_writer.py:48] [17048] accumulated_eval_time=1498.519024, accumulated_logging_time=0.498489, accumulated_submission_time=14438.438890, global_step=17048, preemption_count=0, score=14438.438890, test/ctc_loss=0.3094031810760498, test/num_examples=2472, test/wer=0.104625, total_duration=15938.205380, train/ctc_loss=0.23105613887310028, train/wer=0.086415, validation/ctc_loss=0.5133049488067627, validation/num_examples=5348, validation/wer=0.155237
I0215 01:46:51.849271 139535982765824 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.6291879415512085, loss=1.2903835773468018
I0215 01:48:07.737729 139535991158528 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.5757809281349182, loss=1.3689481019973755
I0215 01:49:26.452755 139535982765824 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.5922282934188843, loss=1.3413645029067993
I0215 01:50:53.776875 139535991158528 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.7322940826416016, loss=1.3123133182525635
I0215 01:52:22.602115 139535982765824 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.7139623165130615, loss=1.2927647829055786
I0215 01:53:45.676505 139535991158528 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.49930912256240845, loss=1.2877918481826782
I0215 01:55:01.815677 139535982765824 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.6208342909812927, loss=1.2830952405929565
I0215 01:56:20.620474 139535991158528 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.6469399333000183, loss=1.296540379524231
I0215 01:57:42.418474 139535982765824 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.5686935782432556, loss=1.328784465789795
I0215 01:59:10.415607 139535991158528 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.6315774321556091, loss=1.304046392440796
I0215 02:00:38.930826 139535982765824 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.836351215839386, loss=1.2906732559204102
I0215 02:02:09.033062 139535991158528 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.5975449681282043, loss=1.3195288181304932
I0215 02:03:36.311664 139535982765824 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.6068910360336304, loss=1.2728570699691772
I0215 02:05:04.095738 139535991158528 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.6530643105506897, loss=1.3548802137374878
I0215 02:06:32.258788 139535982765824 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.5621727705001831, loss=1.399114727973938
I0215 02:07:57.038431 139535991158528 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.656040608882904, loss=1.2716436386108398
I0215 02:09:13.268005 139535982765824 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.6131504774093628, loss=1.2948309183120728
I0215 02:10:12.176543 139646656866112 spec.py:321] Evaluating on the training split.
I0215 02:11:07.111398 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 02:11:59.726371 139646656866112 spec.py:349] Evaluating on the test split.
I0215 02:12:27.217479 139646656866112 submission_runner.py:408] Time since start: 17513.71s, 	Step: 18776, 	{'train/ctc_loss': Array(0.2122344, dtype=float32), 'train/wer': 0.08354792866530608, 'validation/ctc_loss': Array(0.5012511, dtype=float32), 'validation/wer': 0.15289108585882966, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29983425, dtype=float32), 'test/wer': 0.10060325391505698, 'test/num_examples': 2472, 'score': 15878.782112121582, 'total_duration': 17513.712234973907, 'accumulated_submission_time': 15878.782112121582, 'accumulated_eval_time': 1633.5538535118103, 'accumulated_logging_time': 0.5498590469360352}
I0215 02:12:27.256396 139535991158528 logging_writer.py:48] [18776] accumulated_eval_time=1633.553854, accumulated_logging_time=0.549859, accumulated_submission_time=15878.782112, global_step=18776, preemption_count=0, score=15878.782112, test/ctc_loss=0.2998342514038086, test/num_examples=2472, test/wer=0.100603, total_duration=17513.712235, train/ctc_loss=0.21223439276218414, train/wer=0.083548, validation/ctc_loss=0.5012511014938354, validation/num_examples=5348, validation/wer=0.152891
I0215 02:12:46.305682 139535982765824 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.7271440029144287, loss=1.2891473770141602
I0215 02:14:02.082877 139535991158528 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.6734229922294617, loss=1.239841103553772
I0215 02:15:18.281658 139535982765824 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.5949094295501709, loss=1.3121811151504517
I0215 02:16:40.148638 139535991158528 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.5773507356643677, loss=1.298954725265503
I0215 02:18:08.340048 139535982765824 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.6133703589439392, loss=1.2838048934936523
I0215 02:19:37.872146 139535991158528 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.5784434080123901, loss=1.3006162643432617
I0215 02:21:07.146499 139535982765824 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.5144638419151306, loss=1.3316802978515625
I0215 02:22:35.164649 139535991158528 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.7096050977706909, loss=1.2590097188949585
I0215 02:24:04.559977 139535991158528 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.5847251415252686, loss=1.2759088277816772
I0215 02:25:20.571954 139535982765824 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.7090084552764893, loss=1.2360119819641113
I0215 02:26:40.563925 139535991158528 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.5594015717506409, loss=1.2977818250656128
I0215 02:28:02.909283 139535982765824 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.6054242253303528, loss=1.2708877325057983
I0215 02:29:28.860968 139535991158528 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.5076782703399658, loss=1.2856218814849854
I0215 02:30:58.170730 139535982765824 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.520916223526001, loss=1.2302899360656738
I0215 02:32:26.199400 139535991158528 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.7063804268836975, loss=1.2864927053451538
I0215 02:33:59.843828 139535982765824 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.5636275410652161, loss=1.3207528591156006
I0215 02:35:28.955986 139535991158528 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.4898649454116821, loss=1.317152500152588
I0215 02:36:27.507469 139646656866112 spec.py:321] Evaluating on the training split.
I0215 02:37:21.541371 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 02:38:15.153753 139646656866112 spec.py:349] Evaluating on the test split.
I0215 02:38:41.873399 139646656866112 submission_runner.py:408] Time since start: 19088.37s, 	Step: 20469, 	{'train/ctc_loss': Array(0.22761327, dtype=float32), 'train/wer': 0.08249635161816465, 'validation/ctc_loss': Array(0.48038745, dtype=float32), 'validation/wer': 0.14615213802292015, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28335324, dtype=float32), 'test/wer': 0.0950175695163813, 'test/num_examples': 2472, 'score': 17318.944922447205, 'total_duration': 19088.36824822426, 'accumulated_submission_time': 17318.944922447205, 'accumulated_eval_time': 1767.913744688034, 'accumulated_logging_time': 0.6073198318481445}
I0215 02:38:41.911027 139535991158528 logging_writer.py:48] [20469] accumulated_eval_time=1767.913745, accumulated_logging_time=0.607320, accumulated_submission_time=17318.944922, global_step=20469, preemption_count=0, score=17318.944922, test/ctc_loss=0.2833532392978668, test/num_examples=2472, test/wer=0.095018, total_duration=19088.368248, train/ctc_loss=0.22761327028274536, train/wer=0.082496, validation/ctc_loss=0.48038744926452637, validation/num_examples=5348, validation/wer=0.146152
I0215 02:39:06.228308 139535982765824 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.7400771379470825, loss=1.2877483367919922
I0215 02:40:25.593395 139535991158528 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.5116305947303772, loss=1.2194552421569824
I0215 02:41:43.557985 139535982765824 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.7597873210906982, loss=1.232943058013916
I0215 02:43:02.718183 139535991158528 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.5467421412467957, loss=1.289246678352356
I0215 02:44:24.670974 139535982765824 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.7564501166343689, loss=1.2638673782348633
I0215 02:45:48.482512 139535991158528 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.5613136887550354, loss=1.2962627410888672
I0215 02:47:18.998955 139535982765824 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.9315202832221985, loss=1.2178051471710205
I0215 02:48:48.571281 139535991158528 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.5901528000831604, loss=1.267760992050171
I0215 02:50:18.020279 139535982765824 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.6993615031242371, loss=1.231626033782959
I0215 02:51:47.999493 139535991158528 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.6193437576293945, loss=1.2895193099975586
I0215 02:53:18.489669 139535982765824 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.6105860471725464, loss=1.270442247390747
I0215 02:54:47.915401 139535991158528 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.5071960687637329, loss=1.1972540616989136
I0215 02:56:10.750857 139535991158528 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.5809476971626282, loss=1.2268726825714111
I0215 02:57:27.044342 139535982765824 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.5465508103370667, loss=1.2712961435317993
I0215 02:58:46.961074 139535991158528 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.5688270926475525, loss=1.2733778953552246
I0215 03:00:09.783629 139535982765824 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.49676719307899475, loss=1.2330176830291748
I0215 03:01:39.295186 139535991158528 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.5520842671394348, loss=1.2751482725143433
I0215 03:02:42.783036 139646656866112 spec.py:321] Evaluating on the training split.
I0215 03:03:38.730773 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 03:04:33.082983 139646656866112 spec.py:349] Evaluating on the test split.
I0215 03:05:00.177682 139646656866112 submission_runner.py:408] Time since start: 20666.67s, 	Step: 22173, 	{'train/ctc_loss': Array(0.20327859, dtype=float32), 'train/wer': 0.07806933842239186, 'validation/ctc_loss': Array(0.4765248, dtype=float32), 'validation/wer': 0.14474255867615396, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27434734, dtype=float32), 'test/wer': 0.09390043263664616, 'test/num_examples': 2472, 'score': 18759.727390527725, 'total_duration': 20666.671327352524, 'accumulated_submission_time': 18759.727390527725, 'accumulated_eval_time': 1905.3011705875397, 'accumulated_logging_time': 0.6659457683563232}
I0215 03:05:00.217461 139535991158528 logging_writer.py:48] [22173] accumulated_eval_time=1905.301171, accumulated_logging_time=0.665946, accumulated_submission_time=18759.727391, global_step=22173, preemption_count=0, score=18759.727391, test/ctc_loss=0.27434733510017395, test/num_examples=2472, test/wer=0.093900, total_duration=20666.671327, train/ctc_loss=0.203278586268425, train/wer=0.078069, validation/ctc_loss=0.47652480006217957, validation/num_examples=5348, validation/wer=0.144743
I0215 03:05:21.411048 139535982765824 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.5732327103614807, loss=1.257383108139038
I0215 03:06:37.160602 139535991158528 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.5234703421592712, loss=1.2625494003295898
I0215 03:07:53.415444 139535982765824 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.5879068374633789, loss=1.2918646335601807
I0215 03:09:20.697889 139535991158528 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.6792726516723633, loss=1.2776496410369873
I0215 03:10:50.226120 139535982765824 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.5581993460655212, loss=1.3107926845550537
I0215 03:12:17.547290 139535991158528 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.619500994682312, loss=1.2246707677841187
I0215 03:13:33.673016 139535982765824 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.5238233804702759, loss=1.222485899925232
I0215 03:14:53.184230 139535991158528 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.5563604831695557, loss=1.2341928482055664
I0215 03:16:12.989039 139535982765824 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.5634700059890747, loss=1.2984459400177002
I0215 03:17:38.034828 139535991158528 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.7315693497657776, loss=1.2087379693984985
I0215 03:19:09.058070 139535982765824 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.5722804069519043, loss=1.2850335836410522
I0215 03:20:34.078145 139535991158528 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.6153872013092041, loss=1.2292085886001587
I0215 03:22:04.151516 139535982765824 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.5541970729827881, loss=1.231787085533142
I0215 03:23:31.786440 139535991158528 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.6719874143600464, loss=1.231957197189331
I0215 03:24:59.871157 139535982765824 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.5857843160629272, loss=1.212781310081482
I0215 03:26:34.409822 139535991158528 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.7064906358718872, loss=1.2394434213638306
I0215 03:27:51.552478 139535982765824 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.5312294960021973, loss=1.1969693899154663
I0215 03:29:00.876858 139646656866112 spec.py:321] Evaluating on the training split.
I0215 03:29:56.264953 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 03:30:49.635905 139646656866112 spec.py:349] Evaluating on the test split.
I0215 03:31:17.217692 139646656866112 submission_runner.py:408] Time since start: 22243.71s, 	Step: 23892, 	{'train/ctc_loss': Array(0.16723728, dtype=float32), 'train/wer': 0.06601133860240763, 'validation/ctc_loss': Array(0.4556515, dtype=float32), 'validation/wer': 0.13919113316662965, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26593608, dtype=float32), 'test/wer': 0.09089431885117706, 'test/num_examples': 2472, 'score': 20200.300904989243, 'total_duration': 22243.712045431137, 'accumulated_submission_time': 20200.300904989243, 'accumulated_eval_time': 2041.6354887485504, 'accumulated_logging_time': 0.7222182750701904}
I0215 03:31:17.257153 139535991158528 logging_writer.py:48] [23892] accumulated_eval_time=2041.635489, accumulated_logging_time=0.722218, accumulated_submission_time=20200.300905, global_step=23892, preemption_count=0, score=20200.300905, test/ctc_loss=0.26593607664108276, test/num_examples=2472, test/wer=0.090894, total_duration=22243.712045, train/ctc_loss=0.1672372817993164, train/wer=0.066011, validation/ctc_loss=0.45565149188041687, validation/num_examples=5348, validation/wer=0.139191
I0215 03:31:24.162215 139535982765824 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.5253540277481079, loss=1.17471444606781
I0215 03:32:40.281007 139535991158528 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.7221760749816895, loss=1.247666597366333
I0215 03:33:56.541251 139535982765824 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.5935400724411011, loss=1.2270605564117432
I0215 03:35:18.363270 139535991158528 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.6743701696395874, loss=1.2493396997451782
I0215 03:36:46.428106 139535982765824 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.6326305866241455, loss=1.1887762546539307
I0215 03:38:14.561338 139535991158528 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.624221920967102, loss=1.2370465993881226
I0215 03:39:43.336238 139535982765824 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.6184985041618347, loss=1.2247892618179321
I0215 03:41:11.116677 139535991158528 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.5937656164169312, loss=1.2221183776855469
I0215 03:42:39.980054 139535982765824 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.5371537208557129, loss=1.2791423797607422
I0215 03:44:03.155711 139535991158528 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.6196798086166382, loss=1.1828702688217163
I0215 03:45:22.213434 139535982765824 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.7282740473747253, loss=1.1792831420898438
I0215 03:46:43.984034 139535991158528 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.5793951749801636, loss=1.1959513425827026
I0215 03:48:05.774954 139535982765824 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.5511276721954346, loss=1.181896448135376
I0215 03:49:32.336802 139535991158528 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.5862909555435181, loss=1.2023593187332153
I0215 03:51:03.133314 139535982765824 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.5927562713623047, loss=1.2619773149490356
I0215 03:52:35.634502 139535991158528 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.8986930251121521, loss=1.1959282159805298
I0215 03:54:02.428137 139535982765824 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.5902777314186096, loss=1.191922903060913
I0215 03:55:18.245023 139646656866112 spec.py:321] Evaluating on the training split.
I0215 03:56:13.217917 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 03:57:06.753080 139646656866112 spec.py:349] Evaluating on the test split.
I0215 03:57:33.524424 139646656866112 submission_runner.py:408] Time since start: 23820.02s, 	Step: 25590, 	{'train/ctc_loss': Array(0.16852893, dtype=float32), 'train/wer': 0.06542295459881747, 'validation/ctc_loss': Array(0.45363906, dtype=float32), 'validation/wer': 0.1365747221873582, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26481235, dtype=float32), 'test/wer': 0.09065057989559848, 'test/num_examples': 2472, 'score': 21641.20259666443, 'total_duration': 23820.0189204216, 'accumulated_submission_time': 21641.20259666443, 'accumulated_eval_time': 2176.9085042476654, 'accumulated_logging_time': 0.7782742977142334}
I0215 03:57:33.560853 139535991158528 logging_writer.py:48] [25590] accumulated_eval_time=2176.908504, accumulated_logging_time=0.778274, accumulated_submission_time=21641.202597, global_step=25590, preemption_count=0, score=21641.202597, test/ctc_loss=0.2648123502731323, test/num_examples=2472, test/wer=0.090651, total_duration=23820.018920, train/ctc_loss=0.16852892935276031, train/wer=0.065423, validation/ctc_loss=0.45363906025886536, validation/num_examples=5348, validation/wer=0.136575
I0215 03:57:42.035565 139535982765824 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.6205109357833862, loss=1.2349390983581543
I0215 03:58:57.747338 139535991158528 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.6213465332984924, loss=1.2137196063995361
I0215 04:00:19.721297 139535991158528 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.5501575469970703, loss=1.1684600114822388
I0215 04:01:40.798609 139535982765824 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.6210623383522034, loss=1.2151237726211548
I0215 04:03:04.254634 139535991158528 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.635144054889679, loss=1.1920486688613892
I0215 04:04:30.053475 139535982765824 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.6137813925743103, loss=1.165282964706421
I0215 04:05:55.711563 139535991158528 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.6752107739448547, loss=1.2256639003753662
I0215 04:07:25.543412 139535982765824 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.5356194972991943, loss=1.2037227153778076
I0215 04:08:54.503055 139535991158528 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.5602829456329346, loss=1.1892075538635254
I0215 04:10:20.704399 139535982765824 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.6493510603904724, loss=1.2518579959869385
I0215 04:11:46.640011 139535991158528 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.6182039976119995, loss=1.1459769010543823
I0215 04:13:17.425312 139535982765824 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.2034947872161865, loss=1.2114309072494507
I0215 04:14:47.575421 139535991158528 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.5925285220146179, loss=1.1154835224151611
I0215 04:16:05.747918 139535982765824 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.6078697443008423, loss=1.1789391040802002
I0215 04:17:24.130521 139535991158528 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.5831115245819092, loss=1.2034626007080078
I0215 04:18:44.973539 139535982765824 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.6301530003547668, loss=1.1697951555252075
I0215 04:20:07.156898 139535991158528 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.5611696839332581, loss=1.2097092866897583
I0215 04:21:33.896012 139646656866112 spec.py:321] Evaluating on the training split.
I0215 04:22:28.816373 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 04:23:23.224993 139646656866112 spec.py:349] Evaluating on the test split.
I0215 04:23:50.243741 139646656866112 submission_runner.py:408] Time since start: 25396.74s, 	Step: 27297, 	{'train/ctc_loss': Array(0.15395947, dtype=float32), 'train/wer': 0.062930848772081, 'validation/ctc_loss': Array(0.4432258, dtype=float32), 'validation/wer': 0.13487550324879075, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25956357, dtype=float32), 'test/wer': 0.08630390185444722, 'test/num_examples': 2472, 'score': 23081.453466892242, 'total_duration': 25396.73770880699, 'accumulated_submission_time': 23081.453466892242, 'accumulated_eval_time': 2313.249319076538, 'accumulated_logging_time': 0.829658031463623}
I0215 04:23:50.280304 139535991158528 logging_writer.py:48] [27297] accumulated_eval_time=2313.249319, accumulated_logging_time=0.829658, accumulated_submission_time=23081.453467, global_step=27297, preemption_count=0, score=23081.453467, test/ctc_loss=0.2595635652542114, test/num_examples=2472, test/wer=0.086304, total_duration=25396.737709, train/ctc_loss=0.1539594680070877, train/wer=0.062931, validation/ctc_loss=0.44322580099105835, validation/num_examples=5348, validation/wer=0.134876
I0215 04:23:53.414927 139535982765824 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.6412235498428345, loss=1.1988211870193481
I0215 04:25:09.081289 139535991158528 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.5669470429420471, loss=1.1770023107528687
I0215 04:26:25.013100 139535982765824 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.6238619685173035, loss=1.1997787952423096
I0215 04:27:51.514483 139535991158528 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.5420658588409424, loss=1.1794096231460571
I0215 04:29:19.653131 139535982765824 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.5945653319358826, loss=1.2011009454727173
I0215 04:30:51.236025 139535991158528 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.6824491620063782, loss=1.1747101545333862
I0215 04:32:14.452611 139535991158528 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.6467540860176086, loss=1.137338399887085
I0215 04:33:34.896506 139535982765824 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.78998202085495, loss=1.1526036262512207
I0215 04:34:56.287939 139535991158528 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.5676608085632324, loss=1.1861493587493896
I0215 04:36:18.881860 139535982765824 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.5714844465255737, loss=1.2292890548706055
I0215 04:37:49.129556 139535991158528 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.5546318292617798, loss=1.204280972480774
I0215 04:39:18.777977 139535982765824 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.6398485898971558, loss=1.2375937700271606
I0215 04:40:47.757870 139535991158528 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.5989922881126404, loss=1.248726487159729
I0215 04:42:14.343606 139535982765824 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.5719384551048279, loss=1.137070655822754
I0215 04:43:42.747048 139535991158528 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.7084394693374634, loss=1.2163753509521484
I0215 04:45:09.502613 139535982765824 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.5540685057640076, loss=1.2075825929641724
I0215 04:46:32.340879 139535991158528 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.657463014125824, loss=1.1953768730163574
I0215 04:47:50.824532 139646656866112 spec.py:321] Evaluating on the training split.
I0215 04:48:46.471174 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 04:49:41.077170 139646656866112 spec.py:349] Evaluating on the test split.
I0215 04:50:08.567421 139646656866112 submission_runner.py:408] Time since start: 26975.06s, 	Step: 29000, 	{'train/ctc_loss': Array(0.16537838, dtype=float32), 'train/wer': 0.0638170060280613, 'validation/ctc_loss': Array(0.43855268, dtype=float32), 'validation/wer': 0.13271286096334128, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25002927, dtype=float32), 'test/wer': 0.08622265553592103, 'test/num_examples': 2472, 'score': 24521.913326740265, 'total_duration': 26975.06228876114, 'accumulated_submission_time': 24521.913326740265, 'accumulated_eval_time': 2450.9862122535706, 'accumulated_logging_time': 0.8819196224212646}
I0215 04:50:08.604964 139535991158528 logging_writer.py:48] [29000] accumulated_eval_time=2450.986212, accumulated_logging_time=0.881920, accumulated_submission_time=24521.913327, global_step=29000, preemption_count=0, score=24521.913327, test/ctc_loss=0.2500292658805847, test/num_examples=2472, test/wer=0.086223, total_duration=26975.062289, train/ctc_loss=0.1653783768415451, train/wer=0.063817, validation/ctc_loss=0.4385526776313782, validation/num_examples=5348, validation/wer=0.132713
I0215 04:50:09.490567 139535982765824 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.6257624626159668, loss=1.1611424684524536
I0215 04:51:25.622174 139535991158528 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.57930988073349, loss=1.1932153701782227
I0215 04:52:41.849194 139535982765824 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.5733080506324768, loss=1.1842124462127686
I0215 04:53:58.177717 139535991158528 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.5633721351623535, loss=1.2228546142578125
I0215 04:55:25.324027 139535982765824 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.5418928265571594, loss=1.134592056274414
I0215 04:56:56.568351 139535991158528 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.557686984539032, loss=1.1817082166671753
I0215 04:58:22.456701 139535982765824 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.5165751576423645, loss=1.1839599609375
I0215 04:59:49.074168 139535991158528 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.6235465407371521, loss=1.1773242950439453
I0215 05:01:20.124876 139535982765824 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.6598341464996338, loss=1.2022079229354858
I0215 05:02:47.601478 139535991158528 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.5339793562889099, loss=1.1486527919769287
I0215 05:04:04.195682 139535982765824 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.5526275038719177, loss=1.1347243785858154
I0215 05:05:22.626077 139535991158528 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.5928300619125366, loss=1.217347264289856
I0215 05:06:46.310022 139535982765824 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.5637393593788147, loss=1.166501522064209
I0215 05:08:09.312861 139535991158528 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.5089589953422546, loss=1.1453336477279663
I0215 05:09:38.029590 139535982765824 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.5480002164840698, loss=1.1322976350784302
I0215 05:11:09.401450 139535991158528 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.6847490668296814, loss=1.1503762006759644
I0215 05:12:38.439693 139535982765824 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.6783775687217712, loss=1.1557728052139282
I0215 05:14:08.059009 139535991158528 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.6590003967285156, loss=1.1533918380737305
I0215 05:14:09.466485 139646656866112 spec.py:321] Evaluating on the training split.
I0215 05:15:05.350710 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 05:15:57.508904 139646656866112 spec.py:349] Evaluating on the test split.
I0215 05:16:23.994642 139646656866112 submission_runner.py:408] Time since start: 28550.49s, 	Step: 30703, 	{'train/ctc_loss': Array(0.16171478, dtype=float32), 'train/wer': 0.0622558439090314, 'validation/ctc_loss': Array(0.43173057, dtype=float32), 'validation/wer': 0.12848412292304276, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24562462, dtype=float32), 'test/wer': 0.08281031015782098, 'test/num_examples': 2472, 'score': 25962.687898874283, 'total_duration': 28550.48955798149, 'accumulated_submission_time': 25962.687898874283, 'accumulated_eval_time': 2585.5083949565887, 'accumulated_logging_time': 0.9355514049530029}
I0215 05:16:24.033662 139535991158528 logging_writer.py:48] [30703] accumulated_eval_time=2585.508395, accumulated_logging_time=0.935551, accumulated_submission_time=25962.687899, global_step=30703, preemption_count=0, score=25962.687899, test/ctc_loss=0.2456246167421341, test/num_examples=2472, test/wer=0.082810, total_duration=28550.489558, train/ctc_loss=0.16171477735042572, train/wer=0.062256, validation/ctc_loss=0.43173056840896606, validation/num_examples=5348, validation/wer=0.128484
I0215 05:17:38.419312 139535982765824 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.5551892518997192, loss=1.1782654523849487
I0215 05:18:57.702115 139535991158528 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.6343778371810913, loss=1.1321083307266235
I0215 05:20:16.364458 139535982765824 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.6860947012901306, loss=1.1551578044891357
I0215 05:21:36.878769 139535991158528 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.6414583921432495, loss=1.1810929775238037
I0215 05:22:58.119413 139535982765824 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.7800759673118591, loss=1.219414472579956
I0215 05:24:21.285205 139535991158528 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.8686797022819519, loss=1.1798593997955322
I0215 05:25:48.969084 139535982765824 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.5983745455741882, loss=1.1397243738174438
I0215 05:27:15.337275 139535991158528 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.8013722896575928, loss=1.1631139516830444
I0215 05:28:44.786680 139535982765824 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.570914089679718, loss=1.125925898551941
I0215 05:30:10.810070 139535991158528 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.6482576727867126, loss=1.1810472011566162
I0215 05:31:38.898506 139535982765824 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.6344935894012451, loss=1.122701644897461
I0215 05:33:07.871560 139535991158528 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.7027572989463806, loss=1.1455281972885132
I0215 05:34:30.011825 139535991158528 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.6942394971847534, loss=1.1587350368499756
I0215 05:35:46.282531 139535982765824 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.6126607060432434, loss=1.146824598312378
I0215 05:37:06.106084 139535991158528 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.6955577731132507, loss=1.1797274351119995
I0215 05:38:31.082687 139535982765824 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.5718263387680054, loss=1.103481650352478
I0215 05:40:01.825311 139535991158528 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.6644083857536316, loss=1.1539640426635742
I0215 05:40:23.990647 139646656866112 spec.py:321] Evaluating on the training split.
I0215 05:41:20.081926 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 05:42:13.417281 139646656866112 spec.py:349] Evaluating on the test split.
I0215 05:42:41.948283 139646656866112 submission_runner.py:408] Time since start: 30128.44s, 	Step: 32426, 	{'train/ctc_loss': Array(0.14997573, dtype=float32), 'train/wer': 0.059448499174972975, 'validation/ctc_loss': Array(0.4245383, dtype=float32), 'validation/wer': 0.127624858800699, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24090798, dtype=float32), 'test/wer': 0.08309467227266265, 'test/num_examples': 2472, 'score': 27402.558747529984, 'total_duration': 30128.4415204525, 'accumulated_submission_time': 27402.558747529984, 'accumulated_eval_time': 2723.458404779434, 'accumulated_logging_time': 0.9899494647979736}
I0215 05:42:41.992616 139535991158528 logging_writer.py:48] [32426] accumulated_eval_time=2723.458405, accumulated_logging_time=0.989949, accumulated_submission_time=27402.558748, global_step=32426, preemption_count=0, score=27402.558748, test/ctc_loss=0.24090798199176788, test/num_examples=2472, test/wer=0.083095, total_duration=30128.441520, train/ctc_loss=0.1499757319688797, train/wer=0.059448, validation/ctc_loss=0.4245383143424988, validation/num_examples=5348, validation/wer=0.127625
I0215 05:43:39.183632 139535982765824 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.6264268159866333, loss=1.1797702312469482
I0215 05:44:55.538501 139535991158528 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.116477131843567, loss=1.1161396503448486
I0215 05:46:11.498780 139535982765824 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.6238384246826172, loss=1.148795485496521
I0215 05:47:36.802473 139535991158528 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.8073287606239319, loss=1.179628610610962
I0215 05:49:04.648432 139535982765824 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.6241998076438904, loss=1.1074718236923218
I0215 05:50:31.696591 139535991158528 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.633263349533081, loss=1.1860624551773071
I0215 05:51:47.830263 139535982765824 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.5391466021537781, loss=1.1432527303695679
I0215 05:53:07.543981 139535991158528 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.2828261852264404, loss=1.0627630949020386
I0215 05:54:32.110821 139535982765824 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.5594781637191772, loss=1.174349308013916
I0215 05:56:01.071903 139535991158528 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.6587621569633484, loss=1.119044303894043
I0215 05:57:34.721055 139535982765824 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.6498789191246033, loss=1.125058650970459
I0215 05:59:04.578348 139535991158528 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.67237389087677, loss=1.041774868965149
I0215 06:00:29.688839 139535982765824 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.5478636026382446, loss=1.1491150856018066
I0215 06:01:54.765009 139535991158528 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.5325120687484741, loss=1.1382691860198975
I0215 06:03:21.231383 139535982765824 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.5634764432907104, loss=1.1427489519119263
I0215 06:04:52.418848 139535991158528 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.5252664089202881, loss=1.1158329248428345
I0215 06:06:10.289442 139535982765824 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.563003420829773, loss=1.0998615026474
I0215 06:06:41.998053 139646656866112 spec.py:321] Evaluating on the training split.
I0215 06:07:39.658071 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 06:08:33.111510 139646656866112 spec.py:349] Evaluating on the test split.
I0215 06:09:00.276758 139646656866112 submission_runner.py:408] Time since start: 31706.77s, 	Step: 34142, 	{'train/ctc_loss': Array(0.14274807, dtype=float32), 'train/wer': 0.056221869416455766, 'validation/ctc_loss': Array(0.4185331, dtype=float32), 'validation/wer': 0.1253849792907692, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2346872, dtype=float32), 'test/wer': 0.0791339142445108, 'test/num_examples': 2472, 'score': 28842.477757692337, 'total_duration': 31706.7717628479, 'accumulated_submission_time': 28842.477757692337, 'accumulated_eval_time': 2861.7312412261963, 'accumulated_logging_time': 1.050461769104004}
I0215 06:09:00.317061 139535991158528 logging_writer.py:48] [34142] accumulated_eval_time=2861.731241, accumulated_logging_time=1.050462, accumulated_submission_time=28842.477758, global_step=34142, preemption_count=0, score=28842.477758, test/ctc_loss=0.2346871942281723, test/num_examples=2472, test/wer=0.079134, total_duration=31706.771763, train/ctc_loss=0.14274807274341583, train/wer=0.056222, validation/ctc_loss=0.4185330867767334, validation/num_examples=5348, validation/wer=0.125385
I0215 06:09:45.094056 139535982765824 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.7508580088615417, loss=1.0659414529800415
I0215 06:11:00.949793 139535991158528 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.6294090151786804, loss=1.168197512626648
I0215 06:12:17.265161 139535982765824 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.7373335957527161, loss=1.1674344539642334
I0215 06:13:39.213211 139535991158528 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.626638650894165, loss=1.177878975868225
I0215 06:15:05.987039 139535982765824 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.6186469197273254, loss=1.1295734643936157
I0215 06:16:37.992985 139535991158528 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.650836706161499, loss=1.1418098211288452
I0215 06:18:08.003113 139535982765824 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.6537351012229919, loss=1.0790523290634155
I0215 06:19:38.236403 139535991158528 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.5571590662002563, loss=1.0933945178985596
I0215 06:21:05.739124 139535982765824 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.6675577163696289, loss=1.197120189666748
I0215 06:22:28.344566 139535991158528 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.584488034248352, loss=1.114941954612732
I0215 06:23:45.204403 139535982765824 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.6277496218681335, loss=1.1081836223602295
I0215 06:25:07.342796 139535991158528 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.6121323108673096, loss=1.1202373504638672
I0215 06:26:30.727952 139535982765824 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.5648027062416077, loss=1.1679414510726929
I0215 06:27:59.348416 139535991158528 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.6060984134674072, loss=1.0981000661849976
I0215 06:29:30.717549 139535982765824 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.6262847185134888, loss=1.142760157585144
I0215 06:30:59.639433 139535991158528 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.7030660510063171, loss=1.089689016342163
I0215 06:32:27.108080 139535982765824 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.5614550709724426, loss=1.0878468751907349
I0215 06:33:00.700206 139646656866112 spec.py:321] Evaluating on the training split.
I0215 06:33:55.926476 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 06:34:48.929354 139646656866112 spec.py:349] Evaluating on the test split.
I0215 06:35:15.320757 139646656866112 submission_runner.py:408] Time since start: 33281.82s, 	Step: 35838, 	{'train/ctc_loss': Array(0.12617776, dtype=float32), 'train/wer': 0.05087757221715497, 'validation/ctc_loss': Array(0.40873176, dtype=float32), 'validation/wer': 0.120547998107688, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23062919, dtype=float32), 'test/wer': 0.07683870574614587, 'test/num_examples': 2472, 'score': 30282.776348114014, 'total_duration': 33281.81516170502, 'accumulated_submission_time': 30282.776348114014, 'accumulated_eval_time': 2996.345301389694, 'accumulated_logging_time': 1.1065824031829834}
I0215 06:35:15.369118 139535991158528 logging_writer.py:48] [35838] accumulated_eval_time=2996.345301, accumulated_logging_time=1.106582, accumulated_submission_time=30282.776348, global_step=35838, preemption_count=0, score=30282.776348, test/ctc_loss=0.23062919080257416, test/num_examples=2472, test/wer=0.076839, total_duration=33281.815162, train/ctc_loss=0.12617775797843933, train/wer=0.050878, validation/ctc_loss=0.40873175859451294, validation/num_examples=5348, validation/wer=0.120548
I0215 06:36:02.943552 139535982765824 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.6442965269088745, loss=1.1282864809036255
I0215 06:37:18.993394 139535991158528 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.5757429003715515, loss=1.0835041999816895
I0215 06:38:40.038002 139535991158528 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.6133877635002136, loss=1.065980315208435
I0215 06:39:57.797337 139535982765824 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.8644480109214783, loss=1.0967211723327637
I0215 06:41:17.159141 139535991158528 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.6535218358039856, loss=1.1470166444778442
I0215 06:42:39.768096 139535982765824 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.673989474773407, loss=1.1139564514160156
I0215 06:44:06.146740 139535991158528 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.695728600025177, loss=1.120078682899475
I0215 06:45:36.838698 139535982765824 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.5732880234718323, loss=1.1619027853012085
I0215 06:47:05.613445 139535991158528 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.7017576098442078, loss=1.1200029850006104
I0215 06:48:35.286675 139535982765824 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.6401480436325073, loss=1.1417075395584106
I0215 06:50:06.914628 139535991158528 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.7049895524978638, loss=1.1164456605911255
I0215 06:51:38.193078 139535982765824 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.5946534276008606, loss=1.1307282447814941
I0215 06:53:08.165025 139535991158528 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.5993680953979492, loss=1.0712686777114868
I0215 06:54:24.881638 139535982765824 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.8539305925369263, loss=1.0787508487701416
I0215 06:55:44.147416 139535991158528 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.6139090061187744, loss=1.094137191772461
I0215 06:57:04.291561 139535982765824 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.6862411499023438, loss=1.0833444595336914
I0215 06:58:31.014293 139535991158528 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.6293518543243408, loss=1.053451418876648
I0215 06:59:15.572358 139646656866112 spec.py:321] Evaluating on the training split.
I0215 07:00:11.957918 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 07:01:04.282819 139646656866112 spec.py:349] Evaluating on the test split.
I0215 07:01:32.237223 139646656866112 submission_runner.py:408] Time since start: 34858.73s, 	Step: 37553, 	{'train/ctc_loss': Array(0.1250977, dtype=float32), 'train/wer': 0.048614340540568166, 'validation/ctc_loss': Array(0.40274152, dtype=float32), 'validation/wer': 0.12092452957702965, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22272642, dtype=float32), 'test/wer': 0.07527471411451668, 'test/num_examples': 2472, 'score': 31722.89506983757, 'total_duration': 34858.73185968399, 'accumulated_submission_time': 31722.89506983757, 'accumulated_eval_time': 3133.0039348602295, 'accumulated_logging_time': 1.1698052883148193}
I0215 07:01:32.283618 139535991158528 logging_writer.py:48] [37553] accumulated_eval_time=3133.003935, accumulated_logging_time=1.169805, accumulated_submission_time=31722.895070, global_step=37553, preemption_count=0, score=31722.895070, test/ctc_loss=0.22272641956806183, test/num_examples=2472, test/wer=0.075275, total_duration=34858.731860, train/ctc_loss=0.12509770691394806, train/wer=0.048614, validation/ctc_loss=0.40274152159690857, validation/num_examples=5348, validation/wer=0.120925
I0215 07:02:08.744613 139535982765824 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.5818122029304504, loss=1.08342444896698
I0215 07:03:24.705447 139535991158528 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.5765405297279358, loss=1.089495062828064
I0215 07:04:41.075962 139535982765824 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.675050675868988, loss=1.155761480331421
I0215 07:06:09.104171 139535991158528 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.252366065979004, loss=1.108144760131836
I0215 07:07:39.118122 139535982765824 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.7170004844665527, loss=1.079757809638977
I0215 07:09:08.214633 139535991158528 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.5810790657997131, loss=1.0875083208084106
I0215 07:10:28.364435 139535991158528 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.6511435508728027, loss=1.0976428985595703
I0215 07:11:44.317123 139535982765824 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.6196107268333435, loss=1.1233458518981934
I0215 07:13:04.901617 139535991158528 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.6138679385185242, loss=1.061057448387146
I0215 07:14:30.432593 139535982765824 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.7082529067993164, loss=1.0940654277801514
I0215 07:15:56.193895 139535991158528 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.5477315187454224, loss=1.0644323825836182
I0215 07:17:24.914086 139535982765824 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.6629418134689331, loss=1.0731170177459717
I0215 07:18:52.440170 139535991158528 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.5819862484931946, loss=1.1059741973876953
I0215 07:20:23.325010 139535982765824 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.622558057308197, loss=1.0543992519378662
I0215 07:21:50.907441 139535991158528 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.6284264326095581, loss=1.1095188856124878
I0215 07:23:18.642184 139535982765824 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.7277114987373352, loss=1.1305170059204102
I0215 07:24:45.122506 139535991158528 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.6217953562736511, loss=1.0492321252822876
I0215 07:25:32.772591 139646656866112 spec.py:321] Evaluating on the training split.
I0215 07:26:27.634976 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 07:27:22.337601 139646656866112 spec.py:349] Evaluating on the test split.
I0215 07:27:49.722287 139646656866112 submission_runner.py:408] Time since start: 36436.22s, 	Step: 39264, 	{'train/ctc_loss': Array(0.12867208, dtype=float32), 'train/wer': 0.05037870590781216, 'validation/ctc_loss': Array(0.39843825, dtype=float32), 'validation/wer': 0.11827915463857806, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22493432, dtype=float32), 'test/wer': 0.07369041090325594, 'test/num_examples': 2472, 'score': 33163.29486012459, 'total_duration': 36436.21641421318, 'accumulated_submission_time': 33163.29486012459, 'accumulated_eval_time': 3269.946902036667, 'accumulated_logging_time': 1.2371857166290283}
I0215 07:27:49.759995 139535991158528 logging_writer.py:48] [39264] accumulated_eval_time=3269.946902, accumulated_logging_time=1.237186, accumulated_submission_time=33163.294860, global_step=39264, preemption_count=0, score=33163.294860, test/ctc_loss=0.22493432462215424, test/num_examples=2472, test/wer=0.073690, total_duration=36436.216414, train/ctc_loss=0.12867207825183868, train/wer=0.050379, validation/ctc_loss=0.3984382450580597, validation/num_examples=5348, validation/wer=0.118279
I0215 07:28:17.882052 139535982765824 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.769507110118866, loss=1.09470796585083
I0215 07:29:33.877832 139535991158528 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.5546677708625793, loss=1.0724596977233887
I0215 07:30:49.851608 139535982765824 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.6673988699913025, loss=1.1156609058380127
I0215 07:32:12.180119 139535991158528 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.6017025113105774, loss=1.0088777542114258
I0215 07:33:37.348886 139535982765824 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.6056860685348511, loss=1.0575686693191528
I0215 07:35:07.055448 139535991158528 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.5899761319160461, loss=1.0820728540420532
I0215 07:36:35.506110 139535982765824 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.5770412087440491, loss=1.087831974029541
I0215 07:38:06.843770 139535991158528 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.7395994663238525, loss=1.0981760025024414
I0215 07:39:34.034044 139535982765824 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.7809901833534241, loss=1.1011013984680176
I0215 07:41:03.824839 139535991158528 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.6242406368255615, loss=1.0535670518875122
I0215 07:42:21.239317 139535982765824 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.6029852032661438, loss=1.0433293581008911
I0215 07:43:40.076672 139535991158528 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.6380810737609863, loss=1.0389487743377686
I0215 07:45:02.687701 139535982765824 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.7308915257453918, loss=1.092856526374817
I0215 07:46:30.638473 139535991158528 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.6637421250343323, loss=1.00858736038208
I0215 07:48:02.238155 139535982765824 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.5884514451026917, loss=1.016908049583435
I0215 07:49:32.922351 139535991158528 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.6857954263687134, loss=1.0877869129180908
I0215 07:51:01.497546 139535982765824 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.6175293922424316, loss=1.0482163429260254
I0215 07:51:50.340951 139646656866112 spec.py:321] Evaluating on the training split.
I0215 07:52:45.420580 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 07:53:40.726821 139646656866112 spec.py:349] Evaluating on the test split.
I0215 07:54:08.519605 139646656866112 submission_runner.py:408] Time since start: 38015.01s, 	Step: 40958, 	{'train/ctc_loss': Array(0.11844589, dtype=float32), 'train/wer': 0.04667403892771734, 'validation/ctc_loss': Array(0.38874224, dtype=float32), 'validation/wer': 0.11543103198586559, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21982695, dtype=float32), 'test/wer': 0.07413726565515, 'test/num_examples': 2472, 'score': 34603.79007267952, 'total_duration': 38015.011687755585, 'accumulated_submission_time': 34603.79007267952, 'accumulated_eval_time': 3408.1167809963226, 'accumulated_logging_time': 1.2922863960266113}
I0215 07:54:08.560846 139535991158528 logging_writer.py:48] [40958] accumulated_eval_time=3408.116781, accumulated_logging_time=1.292286, accumulated_submission_time=34603.790073, global_step=40958, preemption_count=0, score=34603.790073, test/ctc_loss=0.21982695162296295, test/num_examples=2472, test/wer=0.074137, total_duration=38015.011688, train/ctc_loss=0.11844588816165924, train/wer=0.046674, validation/ctc_loss=0.38874223828315735, validation/num_examples=5348, validation/wer=0.115431
I0215 07:54:41.404552 139535982765824 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.6521465182304382, loss=1.0840753316879272
I0215 07:55:57.251763 139535991158528 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.578249990940094, loss=1.105965256690979
I0215 07:57:18.673734 139535991158528 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.6336261630058289, loss=1.095550775527954
I0215 07:58:36.463906 139535982765824 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.6183019280433655, loss=1.0800529718399048
I0215 07:59:53.086043 139535991158528 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.6691277027130127, loss=1.052812933921814
I0215 08:01:16.137425 139535982765824 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.8880831599235535, loss=1.0622988939285278
I0215 08:02:40.805390 139535991158528 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.6913226842880249, loss=1.0764198303222656
I0215 08:04:07.335550 139535982765824 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.6310688257217407, loss=1.1020128726959229
I0215 08:05:36.672171 139535991158528 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.6136253476142883, loss=1.0809208154678345
I0215 08:07:05.704936 139535982765824 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.5271835327148438, loss=1.063354730606079
I0215 08:08:32.125626 139535991158528 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.7745060920715332, loss=1.0415875911712646
I0215 08:10:01.390486 139535982765824 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.6864360570907593, loss=1.0662918090820312
I0215 08:11:31.686017 139535991158528 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.6192142963409424, loss=1.0947471857070923
I0215 08:12:56.311865 139535991158528 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.0758743286132812, loss=1.087827205657959
I0215 08:14:13.155031 139535982765824 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.5847140550613403, loss=1.0436002016067505
I0215 08:15:31.203559 139535991158528 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.5208755731582642, loss=1.0163724422454834
I0215 08:16:53.934931 139535982765824 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.539811909198761, loss=1.0257179737091064
I0215 08:18:09.251284 139646656866112 spec.py:321] Evaluating on the training split.
I0215 08:19:05.969126 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 08:19:59.046255 139646656866112 spec.py:349] Evaluating on the test split.
I0215 08:20:25.743019 139646656866112 submission_runner.py:408] Time since start: 39592.24s, 	Step: 42686, 	{'train/ctc_loss': Array(0.11153542, dtype=float32), 'train/wer': 0.04539858417974761, 'validation/ctc_loss': Array(0.38364777, dtype=float32), 'validation/wer': 0.11286289427189435, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2124593, dtype=float32), 'test/wer': 0.06989214551215649, 'test/num_examples': 2472, 'score': 36044.39460206032, 'total_duration': 39592.237605810165, 'accumulated_submission_time': 36044.39460206032, 'accumulated_eval_time': 3544.602229118347, 'accumulated_logging_time': 1.3495960235595703}
I0215 08:20:25.783746 139535991158528 logging_writer.py:48] [42686] accumulated_eval_time=3544.602229, accumulated_logging_time=1.349596, accumulated_submission_time=36044.394602, global_step=42686, preemption_count=0, score=36044.394602, test/ctc_loss=0.21245929598808289, test/num_examples=2472, test/wer=0.069892, total_duration=39592.237606, train/ctc_loss=0.11153542250394821, train/wer=0.045399, validation/ctc_loss=0.38364776968955994, validation/num_examples=5348, validation/wer=0.112863
I0215 08:20:37.269601 139535982765824 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.6609615087509155, loss=1.0649863481521606
I0215 08:21:53.356803 139535991158528 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.5933170318603516, loss=1.0433530807495117
I0215 08:23:09.383577 139535982765824 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.5888976454734802, loss=1.08217453956604
I0215 08:24:37.456256 139535991158528 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.6111531257629395, loss=1.054013967514038
I0215 08:26:08.654895 139535982765824 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.7831780910491943, loss=0.9841547608375549
I0215 08:27:40.456251 139535991158528 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.615858256816864, loss=1.036962628364563
I0215 08:29:08.410560 139535991158528 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.775667130947113, loss=1.0633351802825928
I0215 08:30:28.316515 139535982765824 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.6702748537063599, loss=1.0447709560394287
I0215 08:31:47.185345 139535991158528 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.5931841135025024, loss=1.0450373888015747
I0215 08:33:07.438678 139535982765824 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.254579782485962, loss=1.070467472076416
I0215 08:34:33.249001 139535991158528 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.5662582516670227, loss=1.024389624595642
I0215 08:36:03.146066 139535982765824 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.7478904128074646, loss=1.0159778594970703
I0215 08:37:32.739605 139535991158528 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.5751941204071045, loss=1.0242915153503418
I0215 08:39:04.375950 139535982765824 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.6286089420318604, loss=1.0299146175384521
I0215 08:40:34.420362 139535991158528 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.7844670414924622, loss=1.0314291715621948
I0215 08:42:04.166951 139535982765824 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.6717380285263062, loss=1.0499227046966553
I0215 08:43:37.190378 139535991158528 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.6998157501220703, loss=1.0795401334762573
I0215 08:44:26.535574 139646656866112 spec.py:321] Evaluating on the training split.
I0215 08:45:24.215489 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 08:46:18.572931 139646656866112 spec.py:349] Evaluating on the test split.
I0215 08:46:45.348766 139646656866112 submission_runner.py:408] Time since start: 41171.84s, 	Step: 44363, 	{'train/ctc_loss': Array(0.1322927, dtype=float32), 'train/wer': 0.047837093291638745, 'validation/ctc_loss': Array(0.37878135, dtype=float32), 'validation/wer': 0.11108643810884655, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20779921, dtype=float32), 'test/wer': 0.06956716023805172, 'test/num_examples': 2472, 'score': 37485.06210923195, 'total_duration': 41171.84330153465, 'accumulated_submission_time': 37485.06210923195, 'accumulated_eval_time': 3683.409082174301, 'accumulated_logging_time': 1.406475305557251}
I0215 08:46:45.390692 139535991158528 logging_writer.py:48] [44363] accumulated_eval_time=3683.409082, accumulated_logging_time=1.406475, accumulated_submission_time=37485.062109, global_step=44363, preemption_count=0, score=37485.062109, test/ctc_loss=0.20779921114444733, test/num_examples=2472, test/wer=0.069567, total_duration=41171.843302, train/ctc_loss=0.132292702794075, train/wer=0.047837, validation/ctc_loss=0.37878134846687317, validation/num_examples=5348, validation/wer=0.111086
I0215 08:47:14.177948 139535982765824 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.6582476496696472, loss=1.0385545492172241
I0215 08:48:30.004466 139535991158528 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.735464870929718, loss=1.0480660200119019
I0215 08:49:46.510954 139535982765824 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.6259916424751282, loss=1.0563642978668213
I0215 08:51:02.527063 139535991158528 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.6460903286933899, loss=1.0338783264160156
I0215 08:52:29.758783 139535982765824 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.61960369348526, loss=1.0404994487762451
I0215 08:53:58.688882 139535991158528 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.6338813900947571, loss=1.054162621498108
I0215 08:55:25.646118 139535982765824 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.9082579016685486, loss=1.0571106672286987
I0215 08:56:55.884506 139535991158528 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.616391122341156, loss=1.0489749908447266
I0215 08:58:26.432400 139535982765824 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.9227520227432251, loss=1.0612702369689941
I0215 08:59:57.703639 139535991158528 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.5863789916038513, loss=0.9736728072166443
I0215 09:01:19.855328 139535991158528 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.671803891658783, loss=1.0073350667953491
I0215 09:02:41.745356 139535982765824 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.7543874382972717, loss=1.0084757804870605
I0215 09:04:05.052314 139535991158528 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.6726055145263672, loss=1.0745669603347778
I0215 09:05:32.185685 139535982765824 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.6123639345169067, loss=1.0265015363693237
I0215 09:07:01.133281 139535991158528 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.6598681211471558, loss=1.086695671081543
I0215 09:08:30.911638 139535982765824 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.7427724599838257, loss=1.0280392169952393
I0215 09:10:02.351089 139535991158528 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.6276296377182007, loss=1.0375525951385498
I0215 09:10:45.439678 139646656866112 spec.py:321] Evaluating on the training split.
I0215 09:11:42.085762 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 09:12:34.927543 139646656866112 spec.py:349] Evaluating on the test split.
I0215 09:13:02.031632 139646656866112 submission_runner.py:408] Time since start: 42748.52s, 	Step: 46050, 	{'train/ctc_loss': Array(0.08896961, dtype=float32), 'train/wer': 0.035933301884857556, 'validation/ctc_loss': Array(0.37348276, dtype=float32), 'validation/wer': 0.1105457775374842, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20512715, dtype=float32), 'test/wer': 0.0687953202120529, 'test/num_examples': 2472, 'score': 38925.02504873276, 'total_duration': 42748.52485728264, 'accumulated_submission_time': 38925.02504873276, 'accumulated_eval_time': 3819.99338889122, 'accumulated_logging_time': 1.465172290802002}
I0215 09:13:02.082426 139535991158528 logging_writer.py:48] [46050] accumulated_eval_time=3819.993389, accumulated_logging_time=1.465172, accumulated_submission_time=38925.025049, global_step=46050, preemption_count=0, score=38925.025049, test/ctc_loss=0.20512714982032776, test/num_examples=2472, test/wer=0.068795, total_duration=42748.524857, train/ctc_loss=0.08896961063146591, train/wer=0.035933, validation/ctc_loss=0.37348276376724243, validation/num_examples=5348, validation/wer=0.110546
I0215 09:13:40.677449 139535982765824 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.6640685200691223, loss=1.015799880027771
I0215 09:14:56.764318 139535991158528 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.6894662380218506, loss=1.0465693473815918
I0215 09:16:13.975830 139535982765824 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.5371999144554138, loss=1.0457569360733032
I0215 09:17:42.133553 139535991158528 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.6717166304588318, loss=1.0162464380264282
I0215 09:19:03.635429 139535982765824 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.8277614712715149, loss=1.0319898128509521
I0215 09:20:25.668619 139535991158528 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.6058684587478638, loss=1.0432894229888916
I0215 09:21:46.781825 139535982765824 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.775861918926239, loss=0.9744527339935303
I0215 09:23:12.511111 139535991158528 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.6315370202064514, loss=1.025238037109375
I0215 09:24:43.368006 139535982765824 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.7774273753166199, loss=1.0161179304122925
I0215 09:26:12.423097 139535991158528 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.6580258011817932, loss=1.0157309770584106
I0215 09:27:44.710885 139535982765824 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.64597088098526, loss=1.0201936960220337
I0215 09:29:14.314694 139535991158528 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.6676772832870483, loss=1.0288892984390259
I0215 09:30:43.078744 139535982765824 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.7753569483757019, loss=1.0495247840881348
I0215 09:32:15.136639 139535991158528 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.6230446696281433, loss=1.0342857837677002
I0215 09:33:33.611034 139535982765824 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.6733389496803284, loss=0.9909076690673828
I0215 09:34:52.622617 139535991158528 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.6819855570793152, loss=0.9905329942703247
I0215 09:36:15.495987 139535982765824 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.8437823057174683, loss=1.0620352029800415
I0215 09:37:02.361719 139646656866112 spec.py:321] Evaluating on the training split.
I0215 09:37:58.102194 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 09:38:52.875711 139646656866112 spec.py:349] Evaluating on the test split.
I0215 09:39:21.158313 139646656866112 submission_runner.py:408] Time since start: 44327.65s, 	Step: 47755, 	{'train/ctc_loss': Array(0.09621476, dtype=float32), 'train/wer': 0.03858077130176123, 'validation/ctc_loss': Array(0.3708817, dtype=float32), 'validation/wer': 0.10841209921121485, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2026361, dtype=float32), 'test/wer': 0.0677391180712124, 'test/num_examples': 2472, 'score': 40365.21565961838, 'total_duration': 44327.65276861191, 'accumulated_submission_time': 40365.21565961838, 'accumulated_eval_time': 3958.7835640907288, 'accumulated_logging_time': 1.533278226852417}
I0215 09:39:21.203566 139535991158528 logging_writer.py:48] [47755] accumulated_eval_time=3958.783564, accumulated_logging_time=1.533278, accumulated_submission_time=40365.215660, global_step=47755, preemption_count=0, score=40365.215660, test/ctc_loss=0.20263609290122986, test/num_examples=2472, test/wer=0.067739, total_duration=44327.652769, train/ctc_loss=0.09621475636959076, train/wer=0.038581, validation/ctc_loss=0.37088170647621155, validation/num_examples=5348, validation/wer=0.108412
I0215 09:39:56.038888 139535982765824 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.7795956134796143, loss=1.0474869012832642
I0215 09:41:12.422456 139535991158528 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.8258997201919556, loss=0.9911020994186401
I0215 09:42:31.261352 139535982765824 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.1197673082351685, loss=1.0350667238235474
I0215 09:43:58.916181 139535991158528 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.763555645942688, loss=1.0525007247924805
I0215 09:45:29.832884 139535982765824 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.6067076325416565, loss=1.020763874053955
I0215 09:46:57.097721 139535991158528 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.0509305000305176, loss=1.045242190361023
I0215 09:48:25.073842 139535982765824 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.7049383521080017, loss=1.0273116827011108
I0215 09:49:48.978929 139535991158528 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.8651231527328491, loss=0.9871520400047302
I0215 09:51:09.784486 139535982765824 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.6300209760665894, loss=0.9701911211013794
I0215 09:52:28.454238 139535991158528 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.6772338151931763, loss=0.9841268062591553
I0215 09:53:52.071625 139535982765824 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.7200725674629211, loss=1.0136165618896484
I0215 09:55:22.958830 139535991158528 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.6316790580749512, loss=1.03011155128479
I0215 09:56:49.587056 139535982765824 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.681425929069519, loss=1.022287368774414
I0215 09:58:19.197256 139535991158528 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.5859584808349609, loss=0.9961325526237488
I0215 09:59:51.754615 139535982765824 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.0135271549224854, loss=1.0405640602111816
I0215 10:01:23.547208 139535991158528 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.855385959148407, loss=0.9695833325386047
I0215 10:02:54.893378 139535982765824 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.6851649880409241, loss=0.9662773609161377
I0215 10:03:21.605760 139646656866112 spec.py:321] Evaluating on the training split.
I0215 10:04:17.147455 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 10:05:11.834779 139646656866112 spec.py:349] Evaluating on the test split.
I0215 10:05:38.430156 139646656866112 submission_runner.py:408] Time since start: 45904.92s, 	Step: 49431, 	{'train/ctc_loss': Array(0.11491051, dtype=float32), 'train/wer': 0.045480027260535974, 'validation/ctc_loss': Array(0.3653458, dtype=float32), 'validation/wer': 0.10661633374204697, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1993707, dtype=float32), 'test/wer': 0.06562671378953142, 'test/num_examples': 2472, 'score': 41805.530272483826, 'total_duration': 45904.92424035072, 'accumulated_submission_time': 41805.530272483826, 'accumulated_eval_time': 4095.6011593341827, 'accumulated_logging_time': 1.597571611404419}
I0215 10:05:38.469490 139535991158528 logging_writer.py:48] [49431] accumulated_eval_time=4095.601159, accumulated_logging_time=1.597572, accumulated_submission_time=41805.530272, global_step=49431, preemption_count=0, score=41805.530272, test/ctc_loss=0.19937069714069366, test/num_examples=2472, test/wer=0.065627, total_duration=45904.924240, train/ctc_loss=0.11491050571203232, train/wer=0.045480, validation/ctc_loss=0.3653458058834076, validation/num_examples=5348, validation/wer=0.106616
I0215 10:06:35.922666 139535991158528 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.6506809592247009, loss=1.0191378593444824
I0215 10:07:52.649318 139535982765824 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.640352725982666, loss=0.9716525673866272
I0215 10:09:11.597524 139535991158528 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.6904790997505188, loss=1.0089163780212402
I0215 10:10:37.196028 139535982765824 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.7431176900863647, loss=1.029470682144165
I0215 10:12:04.714229 139535991158528 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.6731857657432556, loss=0.9924644231796265
I0215 10:13:33.570008 139535982765824 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.7078796625137329, loss=1.0001728534698486
I0215 10:15:03.001170 139535991158528 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.600517988204956, loss=1.0005970001220703
I0215 10:16:32.578814 139535982765824 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.805290699005127, loss=0.9684536457061768
I0215 10:18:01.132602 139535991158528 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.6897218823432922, loss=0.9774196147918701
I0215 10:19:29.350323 139535982765824 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.6750510931015015, loss=1.0423738956451416
I0215 10:20:57.811542 139535991158528 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.752578854560852, loss=0.9785338044166565
I0215 10:22:16.109727 139535982765824 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.6663105487823486, loss=0.9799451231956482
I0215 10:23:34.324628 139535991158528 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.6638560891151428, loss=0.9687007069587708
I0215 10:24:56.914829 139535982765824 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.7134482860565186, loss=1.0471386909484863
I0215 10:26:24.925276 139535991158528 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.9059783220291138, loss=0.9487596154212952
I0215 10:27:52.374937 139535982765824 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.6380541920661926, loss=1.011156439781189
I0215 10:29:22.148920 139535991158528 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.7044573426246643, loss=1.0026640892028809
I0215 10:29:39.369847 139646656866112 spec.py:321] Evaluating on the training split.
I0215 10:30:34.026935 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 10:31:27.393229 139646656866112 spec.py:349] Evaluating on the test split.
I0215 10:31:54.621973 139646656866112 submission_runner.py:408] Time since start: 47481.12s, 	Step: 51120, 	{'train/ctc_loss': Array(0.11720094, dtype=float32), 'train/wer': 0.04599116109105629, 'validation/ctc_loss': Array(0.35821724, dtype=float32), 'validation/wer': 0.10473367639533873, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19543357, dtype=float32), 'test/wer': 0.06438770743200699, 'test/num_examples': 2472, 'score': 43246.34427714348, 'total_duration': 47481.11702299118, 'accumulated_submission_time': 43246.34427714348, 'accumulated_eval_time': 4230.847477197647, 'accumulated_logging_time': 1.6536619663238525}
I0215 10:31:54.666518 139535991158528 logging_writer.py:48] [51120] accumulated_eval_time=4230.847477, accumulated_logging_time=1.653662, accumulated_submission_time=43246.344277, global_step=51120, preemption_count=0, score=43246.344277, test/ctc_loss=0.1954335719347, test/num_examples=2472, test/wer=0.064388, total_duration=47481.117023, train/ctc_loss=0.11720094084739685, train/wer=0.045991, validation/ctc_loss=0.3582172393798828, validation/num_examples=5348, validation/wer=0.104734
I0215 10:32:56.536695 139535982765824 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.9225605726242065, loss=1.0377416610717773
I0215 10:34:12.598887 139535991158528 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.7082559466362, loss=0.9453103542327881
I0215 10:35:34.332584 139535982765824 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.8071866035461426, loss=0.976720929145813
I0215 10:37:04.727301 139535991158528 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.6941675543785095, loss=0.9968497157096863
I0215 10:38:21.896543 139535982765824 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.6781038045883179, loss=0.9941659569740295
I0215 10:39:40.831466 139535991158528 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.6725320816040039, loss=1.0276918411254883
I0215 10:41:00.932039 139535982765824 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.6819831728935242, loss=0.9527295827865601
I0215 10:42:26.821948 139535991158528 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.6414867043495178, loss=0.944046139717102
I0215 10:43:54.797251 139535982765824 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.7060375213623047, loss=0.9875131845474243
I0215 10:45:26.056313 139535991158528 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.6450814604759216, loss=0.9963240027427673
I0215 10:46:55.557236 139535982765824 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.6727191209793091, loss=0.9828387498855591
I0215 10:48:26.259697 139535991158528 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.7252228260040283, loss=0.9756631255149841
I0215 10:49:58.434106 139535982765824 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.691781222820282, loss=0.9757876396179199
I0215 10:51:28.744097 139535991158528 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.7439761161804199, loss=0.9958245158195496
I0215 10:52:51.027169 139535991158528 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.6756546497344971, loss=0.9701147675514221
I0215 10:54:07.654581 139535982765824 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.6442874073982239, loss=0.9839662909507751
I0215 10:55:24.309879 139535991158528 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.6375092267990112, loss=0.9335083961486816
I0215 10:55:55.440963 139646656866112 spec.py:321] Evaluating on the training split.
I0215 10:56:48.000616 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 10:57:40.596241 139646656866112 spec.py:349] Evaluating on the test split.
I0215 10:58:07.332670 139646656866112 submission_runner.py:408] Time since start: 49053.83s, 	Step: 52839, 	{'train/ctc_loss': Array(0.1289726, dtype=float32), 'train/wer': 0.05273111193614256, 'validation/ctc_loss': Array(0.3622812, dtype=float32), 'validation/wer': 0.10279309113027023, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19422725, dtype=float32), 'test/wer': 0.06308776633558792, 'test/num_examples': 2472, 'score': 44687.03170180321, 'total_duration': 49053.82754635811, 'accumulated_submission_time': 44687.03170180321, 'accumulated_eval_time': 4362.7331802845, 'accumulated_logging_time': 1.71537446975708}
I0215 10:58:07.379479 139535991158528 logging_writer.py:48] [52839] accumulated_eval_time=4362.733180, accumulated_logging_time=1.715374, accumulated_submission_time=44687.031702, global_step=52839, preemption_count=0, score=44687.031702, test/ctc_loss=0.19422724843025208, test/num_examples=2472, test/wer=0.063088, total_duration=49053.827546, train/ctc_loss=0.1289726048707962, train/wer=0.052731, validation/ctc_loss=0.3622812032699585, validation/num_examples=5348, validation/wer=0.102793
I0215 10:58:54.652391 139535982765824 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.6265541315078735, loss=0.9135353565216064
I0215 11:00:10.699227 139535991158528 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.6154581904411316, loss=0.9446295499801636
I0215 11:01:31.908398 139535982765824 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.9288864135742188, loss=0.9721477031707764
I0215 11:03:03.787703 139535991158528 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.9652006030082703, loss=0.9507126212120056
I0215 11:04:32.429379 139535982765824 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.5995601415634155, loss=0.9597770571708679
I0215 11:05:59.224470 139535991158528 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.6212717890739441, loss=0.9733361005783081
I0215 11:07:27.983150 139535982765824 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.7128897309303284, loss=0.9816871881484985
I0215 11:08:56.809862 139535991158528 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.6284263730049133, loss=0.9577170014381409
I0215 11:10:14.648013 139535982765824 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.6395422220230103, loss=0.9454445242881775
I0215 11:11:36.047901 139535991158528 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.8143018484115601, loss=0.9216216206550598
I0215 11:12:56.850527 139535982765824 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.6643478274345398, loss=0.9951348900794983
I0215 11:14:22.036891 139535991158528 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.6297232508659363, loss=0.9517821669578552
I0215 11:15:51.530813 139535982765824 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.7886739373207092, loss=0.957079291343689
I0215 11:17:20.483224 139535991158528 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.7970831394195557, loss=1.004805088043213
I0215 11:18:50.200386 139535982765824 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.6728295683860779, loss=0.9733946919441223
I0215 11:20:20.015234 139535991158528 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.6613295674324036, loss=0.9403300285339355
I0215 11:21:48.081582 139535982765824 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.6477444767951965, loss=0.9858194589614868
I0215 11:22:07.405498 139646656866112 spec.py:321] Evaluating on the training split.
I0215 11:23:01.661126 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 11:23:56.641350 139646656866112 spec.py:349] Evaluating on the test split.
I0215 11:24:23.742846 139646656866112 submission_runner.py:408] Time since start: 50630.24s, 	Step: 54523, 	{'train/ctc_loss': Array(0.11021236, dtype=float32), 'train/wer': 0.04265171968540909, 'validation/ctc_loss': Array(0.34967783, dtype=float32), 'validation/wer': 0.10186624443650617, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19085734, dtype=float32), 'test/wer': 0.062417484207746836, 'test/num_examples': 2472, 'score': 46126.970742702484, 'total_duration': 50630.23694562912, 'accumulated_submission_time': 46126.970742702484, 'accumulated_eval_time': 4499.063733100891, 'accumulated_logging_time': 1.7805514335632324}
I0215 11:24:23.783448 139535991158528 logging_writer.py:48] [54523] accumulated_eval_time=4499.063733, accumulated_logging_time=1.780551, accumulated_submission_time=46126.970743, global_step=54523, preemption_count=0, score=46126.970743, test/ctc_loss=0.19085733592510223, test/num_examples=2472, test/wer=0.062417, total_duration=50630.236946, train/ctc_loss=0.11021236330270767, train/wer=0.042652, validation/ctc_loss=0.34967783093452454, validation/num_examples=5348, validation/wer=0.101866
I0215 11:25:27.389588 139535991158528 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.6580473184585571, loss=0.9901410341262817
I0215 11:26:47.116328 139535982765824 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.7066076993942261, loss=1.0109158754348755
I0215 11:28:05.245554 139535991158528 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.661037027835846, loss=0.9693772196769714
I0215 11:29:29.039934 139535982765824 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.778856635093689, loss=0.9812899827957153
I0215 11:30:54.325044 139535991158528 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.7462570667266846, loss=0.9548171758651733
I0215 11:32:23.049684 139535982765824 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.702403724193573, loss=0.9101089835166931
I0215 11:33:50.989943 139535991158528 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.7134643793106079, loss=0.9903199076652527
I0215 11:35:18.392034 139535982765824 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.7758930325508118, loss=0.9796631932258606
I0215 11:36:47.073793 139535991158528 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.7469805479049683, loss=0.9530642628669739
I0215 11:38:15.046456 139535982765824 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.7754617929458618, loss=0.9355710744857788
I0215 11:39:45.702872 139535991158528 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.8694112300872803, loss=0.9525769948959351
I0215 11:41:08.456992 139535991158528 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.720424234867096, loss=0.9809205532073975
I0215 11:42:26.775964 139535982765824 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.7228184342384338, loss=0.9736160635948181
I0215 11:43:47.161655 139535991158528 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.6174672842025757, loss=0.9090636372566223
I0215 11:45:11.092034 139535982765824 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.7464558482170105, loss=0.9651132822036743
I0215 11:46:35.439279 139535991158528 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.6735912561416626, loss=0.9045245051383972
I0215 11:48:05.341619 139535982765824 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.7158617377281189, loss=0.9343428015708923
I0215 11:48:25.118032 139646656866112 spec.py:321] Evaluating on the training split.
I0215 11:49:21.190855 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 11:50:16.503955 139646656866112 spec.py:349] Evaluating on the test split.
I0215 11:50:43.626941 139646656866112 submission_runner.py:408] Time since start: 52210.12s, 	Step: 56223, 	{'train/ctc_loss': Array(0.09992043, dtype=float32), 'train/wer': 0.04021150939261291, 'validation/ctc_loss': Array(0.3473121, dtype=float32), 'validation/wer': 0.10063044884482077, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1872067, dtype=float32), 'test/wer': 0.061056608372433124, 'test/num_examples': 2472, 'score': 47568.220660448074, 'total_duration': 52210.12111449242, 'accumulated_submission_time': 47568.220660448074, 'accumulated_eval_time': 4637.565958499908, 'accumulated_logging_time': 1.8369412422180176}
I0215 11:50:43.672778 139535991158528 logging_writer.py:48] [56223] accumulated_eval_time=4637.565958, accumulated_logging_time=1.836941, accumulated_submission_time=47568.220660, global_step=56223, preemption_count=0, score=47568.220660, test/ctc_loss=0.1872067004442215, test/num_examples=2472, test/wer=0.061057, total_duration=52210.121114, train/ctc_loss=0.09992042928934097, train/wer=0.040212, validation/ctc_loss=0.3473120927810669, validation/num_examples=5348, validation/wer=0.100630
I0215 11:51:42.758459 139535982765824 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.7965397834777832, loss=0.947866678237915
I0215 11:52:58.918603 139535991158528 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.9556820392608643, loss=0.9318566918373108
I0215 11:54:21.266914 139535982765824 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.8169664740562439, loss=0.9278914928436279
I0215 11:55:50.919804 139535991158528 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.5886433124542236, loss=0.9657691717147827
I0215 11:57:17.382576 139535991158528 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.6611285209655762, loss=0.9727345108985901
I0215 11:58:37.393343 139535982765824 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.9068896770477295, loss=0.9876830577850342
I0215 11:59:55.642019 139535991158528 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.6771665215492249, loss=0.9250322580337524
I0215 12:01:16.372488 139535982765824 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.7737610936164856, loss=0.9125081300735474
I0215 12:02:46.288660 139535991158528 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.0699081420898438, loss=0.9498071074485779
I0215 12:04:15.888851 139535982765824 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.7180641293525696, loss=0.905183732509613
I0215 12:05:45.986465 139535991158528 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.8021782040596008, loss=0.9126728773117065
I0215 12:07:18.100573 139535982765824 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.640790581703186, loss=0.8793976306915283
I0215 12:08:48.008216 139535991158528 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.6820711493492126, loss=0.9619870185852051
I0215 12:10:19.423862 139535982765824 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.716029167175293, loss=0.9719316363334656
I0215 12:11:49.671432 139535991158528 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.6469985842704773, loss=0.9365582466125488
I0215 12:13:06.307420 139535982765824 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.7761762142181396, loss=0.9286232590675354
I0215 12:14:26.096524 139535991158528 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.0950078964233398, loss=0.9260382652282715
I0215 12:14:43.775413 139646656866112 spec.py:321] Evaluating on the training split.
I0215 12:15:38.550179 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 12:16:30.951295 139646656866112 spec.py:349] Evaluating on the test split.
I0215 12:16:57.704761 139646656866112 submission_runner.py:408] Time since start: 53784.20s, 	Step: 57923, 	{'train/ctc_loss': Array(0.08032824, dtype=float32), 'train/wer': 0.03222073828152458, 'validation/ctc_loss': Array(0.34661222, dtype=float32), 'validation/wer': 0.09867055427363218, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18778616, dtype=float32), 'test/wer': 0.06146283996506408, 'test/num_examples': 2472, 'score': 49008.23717498779, 'total_duration': 53784.19967198372, 'accumulated_submission_time': 49008.23717498779, 'accumulated_eval_time': 4771.489331007004, 'accumulated_logging_time': 1.8994395732879639}
I0215 12:16:57.748611 139535991158528 logging_writer.py:48] [57923] accumulated_eval_time=4771.489331, accumulated_logging_time=1.899440, accumulated_submission_time=49008.237175, global_step=57923, preemption_count=0, score=49008.237175, test/ctc_loss=0.18778616189956665, test/num_examples=2472, test/wer=0.061463, total_duration=53784.199672, train/ctc_loss=0.08032824099063873, train/wer=0.032221, validation/ctc_loss=0.34661221504211426, validation/num_examples=5348, validation/wer=0.098671
I0215 12:17:56.764026 139535982765824 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.0880439281463623, loss=0.9004313349723816
I0215 12:19:12.673779 139535991158528 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.6823492050170898, loss=0.8976236581802368
I0215 12:20:35.230932 139535982765824 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.7776731252670288, loss=0.9305790066719055
I0215 12:22:04.795092 139535991158528 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.0645761489868164, loss=0.9221760630607605
I0215 12:23:34.889302 139535982765824 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.657029926776886, loss=0.8948423266410828
I0215 12:25:05.313243 139535991158528 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.7766405344009399, loss=0.9168710708618164
I0215 12:26:36.010802 139535982765824 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.7467787861824036, loss=0.9606886506080627
I0215 12:28:03.280868 139535991158528 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.8331455588340759, loss=0.9720454812049866
I0215 12:29:26.064943 139535991158528 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.7750941514968872, loss=0.945690393447876
I0215 12:30:44.683501 139535982765824 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.6473761200904846, loss=0.8985516428947449
I0215 12:32:03.320041 139535991158528 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.0544713735580444, loss=0.8853593468666077
I0215 12:33:28.862213 139535982765824 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.691484272480011, loss=0.9452285170555115
I0215 12:34:57.836051 139535991158528 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.0113829374313354, loss=0.9534292221069336
I0215 12:36:27.510758 139535982765824 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.6020086407661438, loss=0.9129936099052429
I0215 12:37:59.255920 139535991158528 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.7257047891616821, loss=0.9011244773864746
I0215 12:39:27.100468 139535982765824 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.7201738357543945, loss=0.8706251978874207
I0215 12:40:57.995003 139646656866112 spec.py:321] Evaluating on the training split.
I0215 12:41:52.916182 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 12:42:45.416047 139646656866112 spec.py:349] Evaluating on the test split.
I0215 12:43:13.021010 139646656866112 submission_runner.py:408] Time since start: 55359.52s, 	Step: 59600, 	{'train/ctc_loss': Array(0.09178022, dtype=float32), 'train/wer': 0.03688837503544862, 'validation/ctc_loss': Array(0.3443059, dtype=float32), 'validation/wer': 0.09844849725325121, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18204756, dtype=float32), 'test/wer': 0.058375479861068794, 'test/num_examples': 2472, 'score': 50448.39874601364, 'total_duration': 55359.5160779953, 'accumulated_submission_time': 50448.39874601364, 'accumulated_eval_time': 4906.509536027908, 'accumulated_logging_time': 1.959836483001709}
I0215 12:43:13.069651 139535991158528 logging_writer.py:48] [59600] accumulated_eval_time=4906.509536, accumulated_logging_time=1.959836, accumulated_submission_time=50448.398746, global_step=59600, preemption_count=0, score=50448.398746, test/ctc_loss=0.18204756081104279, test/num_examples=2472, test/wer=0.058375, total_duration=55359.516078, train/ctc_loss=0.09178022295236588, train/wer=0.036888, validation/ctc_loss=0.3443059027194977, validation/num_examples=5348, validation/wer=0.098448
I0215 12:43:13.951750 139535982765824 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.7837679386138916, loss=0.9321818947792053
I0215 12:44:30.347938 139535991158528 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.7268024682998657, loss=0.9519326090812683
I0215 12:45:53.064872 139535991158528 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.6124651432037354, loss=0.8920208811759949
I0215 12:47:10.555569 139535982765824 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.734007716178894, loss=0.876061201095581
I0215 12:48:32.055098 139535991158528 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.6872417330741882, loss=0.9486664533615112
I0215 12:49:58.966724 139535982765824 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.7109624147415161, loss=0.9309785962104797
I0215 12:51:29.149653 139535991158528 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.9952705502510071, loss=0.9043366312980652
I0215 12:52:58.298009 139535982765824 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.9215942621231079, loss=0.9145281910896301
I0215 12:54:26.650884 139535991158528 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.6566406488418579, loss=0.9499142169952393
I0215 12:55:53.050798 139535982765824 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.0115898847579956, loss=0.9179446697235107
I0215 12:57:20.922048 139535991158528 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.8021299242973328, loss=0.9013820290565491
I0215 12:58:49.276879 139535982765824 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.674483060836792, loss=0.9177310466766357
I0215 13:00:18.520367 139535991158528 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.9978954195976257, loss=0.8957700729370117
I0215 13:01:35.963634 139535982765824 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.7079424858093262, loss=0.8733119368553162
I0215 13:02:55.152096 139535991158528 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.7736042737960815, loss=0.9311749339103699
I0215 13:04:21.325381 139535982765824 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.7243999242782593, loss=0.9317305684089661
I0215 13:05:49.316139 139535991158528 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.7344373464584351, loss=0.9153254628181458
I0215 13:07:13.689187 139646656866112 spec.py:321] Evaluating on the training split.
I0215 13:08:11.504041 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 13:09:05.768857 139646656866112 spec.py:349] Evaluating on the test split.
I0215 13:09:34.276546 139646656866112 submission_runner.py:408] Time since start: 56940.77s, 	Step: 61297, 	{'train/ctc_loss': Array(0.076783, dtype=float32), 'train/wer': 0.029943321569885575, 'validation/ctc_loss': Array(0.34047252, dtype=float32), 'validation/wer': 0.09631481892698186, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18233697, dtype=float32), 'test/wer': 0.05872077671480511, 'test/num_examples': 2472, 'score': 51888.93256998062, 'total_duration': 56940.76946043968, 'accumulated_submission_time': 51888.93256998062, 'accumulated_eval_time': 5047.088932514191, 'accumulated_logging_time': 2.025007963180542}
I0215 13:09:34.323863 139535991158528 logging_writer.py:48] [61297] accumulated_eval_time=5047.088933, accumulated_logging_time=2.025008, accumulated_submission_time=51888.932570, global_step=61297, preemption_count=0, score=51888.932570, test/ctc_loss=0.1823369711637497, test/num_examples=2472, test/wer=0.058721, total_duration=56940.769460, train/ctc_loss=0.07678300142288208, train/wer=0.029943, validation/ctc_loss=0.3404725193977356, validation/num_examples=5348, validation/wer=0.096315
I0215 13:09:37.444032 139535982765824 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.756964385509491, loss=0.941416323184967
I0215 13:10:53.075863 139535991158528 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.692880392074585, loss=0.9047738313674927
I0215 13:12:09.465985 139535982765824 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.8767076730728149, loss=0.9282325506210327
I0215 13:13:37.722429 139535991158528 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.8426651954650879, loss=0.8257297873497009
I0215 13:15:05.197193 139535982765824 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.6933525800704956, loss=0.8984411954879761
I0215 13:16:39.844101 139535991158528 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.8482360243797302, loss=0.9040319323539734
I0215 13:17:55.813057 139535982765824 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.7033576965332031, loss=0.9243875741958618
I0215 13:19:16.035466 139535991158528 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.3766440153121948, loss=0.898176372051239
I0215 13:20:35.583810 139535982765824 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.7421606183052063, loss=0.907386839389801
I0215 13:21:59.275610 139535991158528 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.73839271068573, loss=0.896345317363739
I0215 13:23:26.070489 139535982765824 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.7059286236763, loss=0.8925812244415283
I0215 13:24:57.132405 139535991158528 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.7386777997016907, loss=0.9182129502296448
I0215 13:26:25.851540 139535982765824 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.8328242897987366, loss=0.9485647678375244
I0215 13:27:59.077212 139535991158528 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.6717548370361328, loss=0.8668318390846252
I0215 13:29:29.821578 139535982765824 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.7941353917121887, loss=0.9065044522285461
I0215 13:30:56.059314 139535991158528 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.7963467836380005, loss=0.9039045572280884
I0215 13:32:20.404683 139535991158528 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.6691566109657288, loss=0.8959915041923523
I0215 13:33:34.393936 139646656866112 spec.py:321] Evaluating on the training split.
I0215 13:34:29.028116 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 13:35:22.481426 139646656866112 spec.py:349] Evaluating on the test split.
I0215 13:35:49.119338 139646656866112 submission_runner.py:408] Time since start: 58515.61s, 	Step: 62996, 	{'train/ctc_loss': Array(0.07512333, dtype=float32), 'train/wer': 0.030618193308671705, 'validation/ctc_loss': Array(0.33476064, dtype=float32), 'validation/wer': 0.09451905345781399, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17784591, dtype=float32), 'test/wer': 0.05746145877764914, 'test/num_examples': 2472, 'score': 53328.91833233833, 'total_duration': 58515.613005161285, 'accumulated_submission_time': 53328.91833233833, 'accumulated_eval_time': 5181.807159900665, 'accumulated_logging_time': 2.0874433517456055}
I0215 13:35:49.161612 139535991158528 logging_writer.py:48] [62996] accumulated_eval_time=5181.807160, accumulated_logging_time=2.087443, accumulated_submission_time=53328.918332, global_step=62996, preemption_count=0, score=53328.918332, test/ctc_loss=0.17784591019153595, test/num_examples=2472, test/wer=0.057461, total_duration=58515.613005, train/ctc_loss=0.07512333244085312, train/wer=0.030618, validation/ctc_loss=0.3347606360912323, validation/num_examples=5348, validation/wer=0.094519
I0215 13:35:53.098173 139535982765824 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.8135477304458618, loss=0.8864759802818298
I0215 13:37:09.288015 139535991158528 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.8546322584152222, loss=0.8817985653877258
I0215 13:38:25.413390 139535982765824 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.7496492862701416, loss=0.9162754416465759
I0215 13:39:45.490374 139535991158528 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.699123740196228, loss=0.898775041103363
I0215 13:41:16.972590 139535982765824 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.7552502751350403, loss=0.8961310982704163
I0215 13:42:49.585449 139535991158528 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.8095394968986511, loss=0.9240947365760803
I0215 13:44:16.384458 139535982765824 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.688123345375061, loss=0.8629136681556702
I0215 13:45:43.826661 139535991158528 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.7872584462165833, loss=0.8748680949211121
I0215 13:47:10.509933 139535982765824 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.9842256903648376, loss=0.9111456871032715
I0215 13:48:36.436886 139535991158528 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.6890869140625, loss=0.858159601688385
I0215 13:49:55.310244 139535982765824 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.8492124676704407, loss=0.8704224228858948
I0215 13:51:13.541287 139535991158528 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.674311101436615, loss=0.8767179250717163
I0215 13:52:31.961572 139535982765824 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.6893150210380554, loss=0.9034922122955322
I0215 13:53:59.705847 139535991158528 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.6729949116706848, loss=0.9148184657096863
I0215 13:55:28.476284 139535982765824 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.6709926724433899, loss=0.8706539273262024
I0215 13:56:58.821384 139535991158528 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.8918271660804749, loss=0.8925624489784241
I0215 13:58:29.554905 139535982765824 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.6913586854934692, loss=0.8979925513267517
I0215 13:59:49.306595 139646656866112 spec.py:321] Evaluating on the training split.
I0215 14:00:43.815933 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 14:01:37.622467 139646656866112 spec.py:349] Evaluating on the test split.
I0215 14:02:04.508036 139646656866112 submission_runner.py:408] Time since start: 60091.00s, 	Step: 64691, 	{'train/ctc_loss': Array(0.06882256, dtype=float32), 'train/wer': 0.02745650481818082, 'validation/ctc_loss': Array(0.33348805, dtype=float32), 'validation/wer': 0.09349566023344952, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17620729, dtype=float32), 'test/wer': 0.05599902504417768, 'test/num_examples': 2472, 'score': 54768.97867703438, 'total_duration': 60091.00317811966, 'accumulated_submission_time': 54768.97867703438, 'accumulated_eval_time': 5317.002858400345, 'accumulated_logging_time': 2.147326707839966}
I0215 14:02:04.554475 139535991158528 logging_writer.py:48] [64691] accumulated_eval_time=5317.002858, accumulated_logging_time=2.147327, accumulated_submission_time=54768.978677, global_step=64691, preemption_count=0, score=54768.978677, test/ctc_loss=0.1762072890996933, test/num_examples=2472, test/wer=0.055999, total_duration=60091.003178, train/ctc_loss=0.06882255524396896, train/wer=0.027457, validation/ctc_loss=0.3334880471229553, validation/num_examples=5348, validation/wer=0.093496
I0215 14:02:12.264563 139535982765824 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.7505152821540833, loss=0.8631047606468201
I0215 14:03:28.333708 139535991158528 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.7213394641876221, loss=0.8416979908943176
I0215 14:04:47.908430 139535991158528 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.1025497913360596, loss=0.8751013875007629
I0215 14:06:07.053398 139535982765824 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.169816493988037, loss=0.8702248930931091
I0215 14:07:24.454180 139535991158528 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.8055768013000488, loss=0.9037042260169983
I0215 14:08:48.226473 139535982765824 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.8971742987632751, loss=0.8948739767074585
I0215 14:10:16.048215 139535991158528 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.7976653575897217, loss=0.8890856504440308
I0215 14:11:45.364130 139535982765824 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.7777087688446045, loss=0.8510754704475403
I0215 14:13:12.400733 139535991158528 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.8285824060440063, loss=0.8926031589508057
I0215 14:14:41.022293 139535982765824 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.6589597463607788, loss=0.8931082487106323
I0215 14:16:12.852121 139535991158528 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.8060632944107056, loss=0.8681073188781738
I0215 14:17:45.468492 139535982765824 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.7727525234222412, loss=0.827328085899353
I0215 14:19:13.292144 139535991158528 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.2514489889144897, loss=0.8807127475738525
I0215 14:20:35.716109 139535991158528 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.8780227899551392, loss=0.8673580288887024
I0215 14:21:54.919320 139535982765824 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.1575919389724731, loss=0.8705668449401855
I0215 14:23:15.402996 139535991158528 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.809677243232727, loss=0.8811393976211548
I0215 14:24:41.808561 139535982765824 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.6810929179191589, loss=0.8768535256385803
I0215 14:26:04.987330 139646656866112 spec.py:321] Evaluating on the training split.
I0215 14:26:57.843482 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 14:27:50.099736 139646656866112 spec.py:349] Evaluating on the test split.
I0215 14:28:17.291296 139646656866112 submission_runner.py:408] Time since start: 61663.79s, 	Step: 66393, 	{'train/ctc_loss': Array(0.07160717, dtype=float32), 'train/wer': 0.027959459071463435, 'validation/ctc_loss': Array(0.33296782, dtype=float32), 'validation/wer': 0.09302258223350744, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17585711, dtype=float32), 'test/wer': 0.05683179980907115, 'test/num_examples': 2472, 'score': 56209.32621002197, 'total_duration': 61663.78510403633, 'accumulated_submission_time': 56209.32621002197, 'accumulated_eval_time': 5449.299804925919, 'accumulated_logging_time': 2.2101027965545654}
I0215 14:28:17.339149 139535991158528 logging_writer.py:48] [66393] accumulated_eval_time=5449.299805, accumulated_logging_time=2.210103, accumulated_submission_time=56209.326210, global_step=66393, preemption_count=0, score=56209.326210, test/ctc_loss=0.17585711181163788, test/num_examples=2472, test/wer=0.056832, total_duration=61663.785104, train/ctc_loss=0.07160717248916626, train/wer=0.027959, validation/ctc_loss=0.3329678177833557, validation/num_examples=5348, validation/wer=0.093023
I0215 14:28:23.489215 139535982765824 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.7834923267364502, loss=0.9182563424110413
I0215 14:29:39.566357 139535991158528 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.2258342504501343, loss=0.8913165926933289
I0215 14:30:55.784363 139535982765824 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.671906054019928, loss=0.8181992769241333
I0215 14:32:23.574500 139535991158528 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.8035468459129333, loss=0.905762791633606
I0215 14:33:51.458431 139535982765824 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.7983408570289612, loss=0.8640552759170532
I0215 14:35:20.379082 139535991158528 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.8282290697097778, loss=0.8853825330734253
I0215 14:36:48.122532 139535991158528 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.8381839394569397, loss=0.8584254384040833
I0215 14:38:09.972443 139535982765824 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.8606043457984924, loss=0.8665460348129272
I0215 14:39:27.882690 139535991158528 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.9812805652618408, loss=0.9168500304222107
I0215 14:40:49.272060 139535982765824 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.7502012848854065, loss=0.8443765044212341
I0215 14:42:19.249316 139535991158528 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.7310793399810791, loss=0.8472205400466919
I0215 14:43:51.223695 139535982765824 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.8047181367874146, loss=0.8970472812652588
I0215 14:45:21.815403 139535991158528 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.9854027032852173, loss=0.851809024810791
I0215 14:46:47.752321 139535982765824 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.1110724210739136, loss=0.8707795739173889
I0215 14:48:19.305603 139535991158528 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.6863576173782349, loss=0.8462818264961243
I0215 14:49:46.826063 139535982765824 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.9399621486663818, loss=0.9166133403778076
I0215 14:51:17.285154 139535991158528 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.7872929573059082, loss=0.8143172264099121
I0215 14:52:17.389282 139646656866112 spec.py:321] Evaluating on the training split.
I0215 14:53:11.645103 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 14:54:04.432578 139646656866112 spec.py:349] Evaluating on the test split.
I0215 14:54:32.802290 139646656866112 submission_runner.py:408] Time since start: 63239.30s, 	Step: 68079, 	{'train/ctc_loss': Array(0.07398825, dtype=float32), 'train/wer': 0.028735978276936216, 'validation/ctc_loss': Array(0.33303624, dtype=float32), 'validation/wer': 0.09305154619268756, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17321478, dtype=float32), 'test/wer': 0.055897467146019945, 'test/num_examples': 2472, 'score': 57649.288610458374, 'total_duration': 63239.29641199112, 'accumulated_submission_time': 57649.288610458374, 'accumulated_eval_time': 5584.706095695496, 'accumulated_logging_time': 2.277143955230713}
I0215 14:54:32.845449 139535991158528 logging_writer.py:48] [68079] accumulated_eval_time=5584.706096, accumulated_logging_time=2.277144, accumulated_submission_time=57649.288610, global_step=68079, preemption_count=0, score=57649.288610, test/ctc_loss=0.17321477830410004, test/num_examples=2472, test/wer=0.055897, total_duration=63239.296412, train/ctc_loss=0.07398825138807297, train/wer=0.028736, validation/ctc_loss=0.33303624391555786, validation/num_examples=5348, validation/wer=0.093052
I0215 14:54:49.529154 139535982765824 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.6699633598327637, loss=0.8122666478157043
I0215 14:56:05.495485 139535991158528 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.773934543132782, loss=0.8363475799560547
I0215 14:57:21.578457 139535982765824 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.0007538795471191, loss=0.8383426070213318
I0215 14:58:37.526425 139535991158528 logging_writer.py:48] [68400] global_step=68400, grad_norm=2.07794451713562, loss=0.8608804941177368
I0215 15:00:06.833227 139535982765824 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.7329562306404114, loss=0.8733534812927246
I0215 15:01:38.422004 139535991158528 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.8590076565742493, loss=0.8913394212722778
I0215 15:03:06.651884 139535982765824 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.6261285543441772, loss=0.842944860458374
I0215 15:04:35.464649 139535991158528 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.9157126545906067, loss=0.8923310041427612
I0215 15:06:03.527018 139535982765824 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.8925061225891113, loss=0.8555009365081787
I0215 15:07:31.663719 139535991158528 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.730736494064331, loss=0.911434531211853
I0215 15:08:54.275951 139535991158528 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.9898931384086609, loss=0.8360425233840942
I0215 15:10:12.910517 139535982765824 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.8932787179946899, loss=0.8729568123817444
I0215 15:11:34.076658 139535991158528 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.9328048229217529, loss=0.8516736030578613
I0215 15:12:57.626693 139535982765824 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.7904210686683655, loss=0.8392246961593628
I0215 15:14:27.946514 139535991158528 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.9350727796554565, loss=0.8453261256217957
I0215 15:15:57.557869 139535982765824 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.817820131778717, loss=0.8876500129699707
I0215 15:17:22.790121 139535991158528 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.7437822818756104, loss=0.8581937551498413
I0215 15:18:34.104337 139646656866112 spec.py:321] Evaluating on the training split.
I0215 15:19:28.087997 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 15:20:22.420629 139646656866112 spec.py:349] Evaluating on the test split.
I0215 15:20:49.897926 139646656866112 submission_runner.py:408] Time since start: 64816.39s, 	Step: 69778, 	{'train/ctc_loss': Array(0.06125573, dtype=float32), 'train/wer': 0.023995495269475434, 'validation/ctc_loss': Array(0.32672223, dtype=float32), 'validation/wer': 0.09115923419291928, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17336021, dtype=float32), 'test/wer': 0.05553185871265208, 'test/num_examples': 2472, 'score': 59090.46134185791, 'total_duration': 64816.39191651344, 'accumulated_submission_time': 59090.46134185791, 'accumulated_eval_time': 5720.492845773697, 'accumulated_logging_time': 2.3368923664093018}
I0215 15:20:49.941199 139535991158528 logging_writer.py:48] [69778] accumulated_eval_time=5720.492846, accumulated_logging_time=2.336892, accumulated_submission_time=59090.461342, global_step=69778, preemption_count=0, score=59090.461342, test/ctc_loss=0.173360213637352, test/num_examples=2472, test/wer=0.055532, total_duration=64816.391917, train/ctc_loss=0.06125572696328163, train/wer=0.023995, validation/ctc_loss=0.32672223448753357, validation/num_examples=5348, validation/wer=0.091159
I0215 15:21:07.411523 139535982765824 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.7782195806503296, loss=0.8396793603897095
I0215 15:22:24.043499 139535991158528 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.8116747736930847, loss=0.8468159437179565
I0215 15:23:40.283383 139535982765824 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.9885286092758179, loss=0.8455473780632019
I0215 15:25:01.033415 139535991158528 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.8244330883026123, loss=0.873323380947113
I0215 15:26:21.556568 139535982765824 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.6989401578903198, loss=0.8739806413650513
I0215 15:27:40.904415 139535991158528 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.2151051759719849, loss=0.8577452301979065
I0215 15:29:04.324683 139535982765824 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.8721084594726562, loss=0.8155772089958191
I0215 15:30:29.942557 139535991158528 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.8186182975769043, loss=0.8859703540802002
I0215 15:32:01.959813 139535982765824 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.8013144731521606, loss=0.8161270618438721
I0215 15:33:31.755969 139535991158528 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.7733308672904968, loss=0.8755481839179993
I0215 15:34:56.931421 139535982765824 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.8326607942581177, loss=0.8817169070243835
I0215 15:36:25.034533 139535991158528 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.762688159942627, loss=0.8623343706130981
I0215 15:37:54.968490 139535982765824 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.7472167611122131, loss=0.8411685228347778
I0215 15:39:21.185867 139535991158528 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.6961542367935181, loss=0.8511379957199097
I0215 15:40:40.073591 139535982765824 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.7239543199539185, loss=0.852086067199707
I0215 15:42:00.934692 139535991158528 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.7190743088722229, loss=0.8857731819152832
I0215 15:43:25.080541 139535982765824 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.7625032067298889, loss=0.8382565379142761
I0215 15:44:50.047634 139646656866112 spec.py:321] Evaluating on the training split.
I0215 15:45:42.877017 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 15:46:36.790647 139646656866112 spec.py:349] Evaluating on the test split.
I0215 15:47:05.297360 139646656866112 submission_runner.py:408] Time since start: 66391.79s, 	Step: 71500, 	{'train/ctc_loss': Array(0.06221401, dtype=float32), 'train/wer': 0.024718881054426472, 'validation/ctc_loss': Array(0.32732484, dtype=float32), 'validation/wer': 0.09039651660117594, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17124374, dtype=float32), 'test/wer': 0.05437409867365385, 'test/num_examples': 2472, 'score': 60530.481770038605, 'total_duration': 66391.79093980789, 'accumulated_submission_time': 60530.481770038605, 'accumulated_eval_time': 5855.735311031342, 'accumulated_logging_time': 2.396700859069824}
I0215 15:47:05.347669 139535991158528 logging_writer.py:48] [71500] accumulated_eval_time=5855.735311, accumulated_logging_time=2.396701, accumulated_submission_time=60530.481770, global_step=71500, preemption_count=0, score=60530.481770, test/ctc_loss=0.17124374210834503, test/num_examples=2472, test/wer=0.054374, total_duration=66391.790940, train/ctc_loss=0.06221401318907738, train/wer=0.024719, validation/ctc_loss=0.32732483744621277, validation/num_examples=5348, validation/wer=0.090397
I0215 15:47:06.244540 139535982765824 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.0860437154769897, loss=0.8535536527633667
I0215 15:48:22.136772 139535991158528 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.7591263651847839, loss=0.8591040372848511
I0215 15:49:38.543766 139535982765824 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.8020655512809753, loss=0.8640919923782349
I0215 15:51:04.623101 139535991158528 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.8377435803413391, loss=0.8139533400535583
I0215 15:52:32.413896 139535982765824 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.7234861254692078, loss=0.8644874691963196
I0215 15:54:01.812661 139535991158528 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.0223612785339355, loss=0.8413327932357788
I0215 15:55:36.589355 139535991158528 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.8341501951217651, loss=0.8643280267715454
I0215 15:56:03.494313 139535982765824 logging_writer.py:48] [72135] global_step=72135, preemption_count=0, score=61068.561726
I0215 15:56:04.540928 139646656866112 checkpoints.py:490] Saving checkpoint at step: 72135
I0215 15:56:06.377870 139646656866112 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_2/librispeech_conformer_jax/trial_2/checkpoint_72135
I0215 15:56:06.412821 139646656866112 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_2/librispeech_conformer_jax/trial_2/checkpoint_72135.
I0215 15:56:09.560947 139646656866112 submission_runner.py:583] Tuning trial 2/5
I0215 15:56:09.561282 139646656866112 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.2, learning_rate=0.0008445074561975979, one_minus_beta1=0.11042418465, beta2=0.9978504782314613, weight_decay=0.08135402759553023, warmup_factor=0.05)
I0215 15:56:09.590045 139646656866112 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(32.058502, dtype=float32), 'train/wer': 1.1703602515325648, 'validation/ctc_loss': Array(30.757864, dtype=float32), 'validation/wer': 1.1779352558965794, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.871214, dtype=float32), 'test/wer': 1.1899945158734995, 'test/num_examples': 2472, 'score': 36.7762336730957, 'total_duration': 175.56985116004944, 'accumulated_submission_time': 36.7762336730957, 'accumulated_eval_time': 138.79355025291443, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1706, {'train/ctc_loss': Array(6.2620625, dtype=float32), 'train/wer': 0.9306712172923777, 'validation/ctc_loss': Array(6.2103653, dtype=float32), 'validation/wer': 0.8912886065439238, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.156824, dtype=float32), 'test/wer': 0.8923486279527959, 'test/num_examples': 2472, 'score': 1477.254374742508, 'total_duration': 1729.1772775650024, 'accumulated_submission_time': 1477.254374742508, 'accumulated_eval_time': 251.8186011314392, 'accumulated_logging_time': 0.031685590744018555, 'global_step': 1706, 'preemption_count': 0}), (3416, {'train/ctc_loss': Array(2.681486, dtype=float32), 'train/wer': 0.573457583199163, 'validation/ctc_loss': Array(2.6309824, dtype=float32), 'validation/wer': 0.5461540689535321, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.3015385, dtype=float32), 'test/wer': 0.5043974569902301, 'test/num_examples': 2472, 'score': 2917.648540019989, 'total_duration': 3303.025156736374, 'accumulated_submission_time': 2917.648540019989, 'accumulated_eval_time': 385.1414303779602, 'accumulated_logging_time': 0.0872964859008789, 'global_step': 3416, 'preemption_count': 0}), (5113, {'train/ctc_loss': Array(0.8880565, dtype=float32), 'train/wer': 0.2816791786266284, 'validation/ctc_loss': Array(0.9799351, dtype=float32), 'validation/wer': 0.29021887098487115, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.699066, dtype=float32), 'test/wer': 0.2274084455548108, 'test/num_examples': 2472, 'score': 4357.607877492905, 'total_duration': 4878.8678069114685, 'accumulated_submission_time': 4357.607877492905, 'accumulated_eval_time': 520.9090373516083, 'accumulated_logging_time': 0.13166284561157227, 'global_step': 5113, 'preemption_count': 0}), (6814, {'train/ctc_loss': Array(0.61878496, dtype=float32), 'train/wer': 0.21151884362014212, 'validation/ctc_loss': Array(0.7577799, dtype=float32), 'validation/wer': 0.22992556262490707, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5053952, dtype=float32), 'test/wer': 0.17023134889200334, 'test/num_examples': 2472, 'score': 5797.983711004257, 'total_duration': 6455.382749795914, 'accumulated_submission_time': 5797.983711004257, 'accumulated_eval_time': 656.9221382141113, 'accumulated_logging_time': 0.18206191062927246, 'global_step': 6814, 'preemption_count': 0}), (8528, {'train/ctc_loss': Array(0.5105811, dtype=float32), 'train/wer': 0.17519140059552968, 'validation/ctc_loss': Array(0.6722918, dtype=float32), 'validation/wer': 0.20403178311787365, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43440384, dtype=float32), 'test/wer': 0.14709645969167023, 'test/num_examples': 2472, 'score': 7237.903148651123, 'total_duration': 8032.956871747971, 'accumulated_submission_time': 7237.903148651123, 'accumulated_eval_time': 794.4483127593994, 'accumulated_logging_time': 0.23373818397521973, 'global_step': 8528, 'preemption_count': 0}), (10237, {'train/ctc_loss': Array(0.30627134, dtype=float32), 'train/wer': 0.11301755653800374, 'validation/ctc_loss': Array(0.61254525, dtype=float32), 'validation/wer': 0.18793747646678316, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38682497, dtype=float32), 'test/wer': 0.13220807182174557, 'test/num_examples': 2472, 'score': 8678.238431215286, 'total_duration': 9633.724823951721, 'accumulated_submission_time': 8678.238431215286, 'accumulated_eval_time': 954.7578556537628, 'accumulated_logging_time': 0.2836422920227051, 'global_step': 10237, 'preemption_count': 0}), (11951, {'train/ctc_loss': Array(0.2763798, dtype=float32), 'train/wer': 0.10269464344157969, 'validation/ctc_loss': Array(0.579581, dtype=float32), 'validation/wer': 0.17735597671297681, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3629714, dtype=float32), 'test/wer': 0.12390063575244246, 'test/num_examples': 2472, 'score': 10118.417578220367, 'total_duration': 11210.029403686523, 'accumulated_submission_time': 10118.417578220367, 'accumulated_eval_time': 1090.7553856372833, 'accumulated_logging_time': 0.33667874336242676, 'global_step': 11951, 'preemption_count': 0}), (13670, {'train/ctc_loss': Array(0.26991197, dtype=float32), 'train/wer': 0.09945294537382066, 'validation/ctc_loss': Array(0.5541362, dtype=float32), 'validation/wer': 0.16952605308128252, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3431652, dtype=float32), 'test/wer': 0.11863993662787155, 'test/num_examples': 2472, 'score': 11558.427836418152, 'total_duration': 12786.317202329636, 'accumulated_submission_time': 11558.427836418152, 'accumulated_eval_time': 1226.897890329361, 'accumulated_logging_time': 0.39374494552612305, 'global_step': 13670, 'preemption_count': 0}), (15354, {'train/ctc_loss': Array(0.2340234, dtype=float32), 'train/wer': 0.09087778612572, 'validation/ctc_loss': Array(0.5342885, dtype=float32), 'validation/wer': 0.16420633924519923, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32349566, dtype=float32), 'test/wer': 0.11061686267341012, 'test/num_examples': 2472, 'score': 12998.425715208054, 'total_duration': 14364.191707611084, 'accumulated_submission_time': 12998.425715208054, 'accumulated_eval_time': 1364.6433284282684, 'accumulated_logging_time': 0.4485950469970703, 'global_step': 15354, 'preemption_count': 0}), (17048, {'train/ctc_loss': Array(0.23105614, dtype=float32), 'train/wer': 0.08641527741495221, 'validation/ctc_loss': Array(0.51330495, dtype=float32), 'validation/wer': 0.15523716655241993, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30940318, dtype=float32), 'test/wer': 0.10462494668210347, 'test/num_examples': 2472, 'score': 14438.438889980316, 'total_duration': 15938.205379962921, 'accumulated_submission_time': 14438.438889980316, 'accumulated_eval_time': 1498.5190238952637, 'accumulated_logging_time': 0.4984886646270752, 'global_step': 17048, 'preemption_count': 0}), (18776, {'train/ctc_loss': Array(0.2122344, dtype=float32), 'train/wer': 0.08354792866530608, 'validation/ctc_loss': Array(0.5012511, dtype=float32), 'validation/wer': 0.15289108585882966, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29983425, dtype=float32), 'test/wer': 0.10060325391505698, 'test/num_examples': 2472, 'score': 15878.782112121582, 'total_duration': 17513.712234973907, 'accumulated_submission_time': 15878.782112121582, 'accumulated_eval_time': 1633.5538535118103, 'accumulated_logging_time': 0.5498590469360352, 'global_step': 18776, 'preemption_count': 0}), (20469, {'train/ctc_loss': Array(0.22761327, dtype=float32), 'train/wer': 0.08249635161816465, 'validation/ctc_loss': Array(0.48038745, dtype=float32), 'validation/wer': 0.14615213802292015, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28335324, dtype=float32), 'test/wer': 0.0950175695163813, 'test/num_examples': 2472, 'score': 17318.944922447205, 'total_duration': 19088.36824822426, 'accumulated_submission_time': 17318.944922447205, 'accumulated_eval_time': 1767.913744688034, 'accumulated_logging_time': 0.6073198318481445, 'global_step': 20469, 'preemption_count': 0}), (22173, {'train/ctc_loss': Array(0.20327859, dtype=float32), 'train/wer': 0.07806933842239186, 'validation/ctc_loss': Array(0.4765248, dtype=float32), 'validation/wer': 0.14474255867615396, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27434734, dtype=float32), 'test/wer': 0.09390043263664616, 'test/num_examples': 2472, 'score': 18759.727390527725, 'total_duration': 20666.671327352524, 'accumulated_submission_time': 18759.727390527725, 'accumulated_eval_time': 1905.3011705875397, 'accumulated_logging_time': 0.6659457683563232, 'global_step': 22173, 'preemption_count': 0}), (23892, {'train/ctc_loss': Array(0.16723728, dtype=float32), 'train/wer': 0.06601133860240763, 'validation/ctc_loss': Array(0.4556515, dtype=float32), 'validation/wer': 0.13919113316662965, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26593608, dtype=float32), 'test/wer': 0.09089431885117706, 'test/num_examples': 2472, 'score': 20200.300904989243, 'total_duration': 22243.712045431137, 'accumulated_submission_time': 20200.300904989243, 'accumulated_eval_time': 2041.6354887485504, 'accumulated_logging_time': 0.7222182750701904, 'global_step': 23892, 'preemption_count': 0}), (25590, {'train/ctc_loss': Array(0.16852893, dtype=float32), 'train/wer': 0.06542295459881747, 'validation/ctc_loss': Array(0.45363906, dtype=float32), 'validation/wer': 0.1365747221873582, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26481235, dtype=float32), 'test/wer': 0.09065057989559848, 'test/num_examples': 2472, 'score': 21641.20259666443, 'total_duration': 23820.0189204216, 'accumulated_submission_time': 21641.20259666443, 'accumulated_eval_time': 2176.9085042476654, 'accumulated_logging_time': 0.7782742977142334, 'global_step': 25590, 'preemption_count': 0}), (27297, {'train/ctc_loss': Array(0.15395947, dtype=float32), 'train/wer': 0.062930848772081, 'validation/ctc_loss': Array(0.4432258, dtype=float32), 'validation/wer': 0.13487550324879075, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25956357, dtype=float32), 'test/wer': 0.08630390185444722, 'test/num_examples': 2472, 'score': 23081.453466892242, 'total_duration': 25396.73770880699, 'accumulated_submission_time': 23081.453466892242, 'accumulated_eval_time': 2313.249319076538, 'accumulated_logging_time': 0.829658031463623, 'global_step': 27297, 'preemption_count': 0}), (29000, {'train/ctc_loss': Array(0.16537838, dtype=float32), 'train/wer': 0.0638170060280613, 'validation/ctc_loss': Array(0.43855268, dtype=float32), 'validation/wer': 0.13271286096334128, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25002927, dtype=float32), 'test/wer': 0.08622265553592103, 'test/num_examples': 2472, 'score': 24521.913326740265, 'total_duration': 26975.06228876114, 'accumulated_submission_time': 24521.913326740265, 'accumulated_eval_time': 2450.9862122535706, 'accumulated_logging_time': 0.8819196224212646, 'global_step': 29000, 'preemption_count': 0}), (30703, {'train/ctc_loss': Array(0.16171478, dtype=float32), 'train/wer': 0.0622558439090314, 'validation/ctc_loss': Array(0.43173057, dtype=float32), 'validation/wer': 0.12848412292304276, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24562462, dtype=float32), 'test/wer': 0.08281031015782098, 'test/num_examples': 2472, 'score': 25962.687898874283, 'total_duration': 28550.48955798149, 'accumulated_submission_time': 25962.687898874283, 'accumulated_eval_time': 2585.5083949565887, 'accumulated_logging_time': 0.9355514049530029, 'global_step': 30703, 'preemption_count': 0}), (32426, {'train/ctc_loss': Array(0.14997573, dtype=float32), 'train/wer': 0.059448499174972975, 'validation/ctc_loss': Array(0.4245383, dtype=float32), 'validation/wer': 0.127624858800699, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24090798, dtype=float32), 'test/wer': 0.08309467227266265, 'test/num_examples': 2472, 'score': 27402.558747529984, 'total_duration': 30128.4415204525, 'accumulated_submission_time': 27402.558747529984, 'accumulated_eval_time': 2723.458404779434, 'accumulated_logging_time': 0.9899494647979736, 'global_step': 32426, 'preemption_count': 0}), (34142, {'train/ctc_loss': Array(0.14274807, dtype=float32), 'train/wer': 0.056221869416455766, 'validation/ctc_loss': Array(0.4185331, dtype=float32), 'validation/wer': 0.1253849792907692, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2346872, dtype=float32), 'test/wer': 0.0791339142445108, 'test/num_examples': 2472, 'score': 28842.477757692337, 'total_duration': 31706.7717628479, 'accumulated_submission_time': 28842.477757692337, 'accumulated_eval_time': 2861.7312412261963, 'accumulated_logging_time': 1.050461769104004, 'global_step': 34142, 'preemption_count': 0}), (35838, {'train/ctc_loss': Array(0.12617776, dtype=float32), 'train/wer': 0.05087757221715497, 'validation/ctc_loss': Array(0.40873176, dtype=float32), 'validation/wer': 0.120547998107688, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23062919, dtype=float32), 'test/wer': 0.07683870574614587, 'test/num_examples': 2472, 'score': 30282.776348114014, 'total_duration': 33281.81516170502, 'accumulated_submission_time': 30282.776348114014, 'accumulated_eval_time': 2996.345301389694, 'accumulated_logging_time': 1.1065824031829834, 'global_step': 35838, 'preemption_count': 0}), (37553, {'train/ctc_loss': Array(0.1250977, dtype=float32), 'train/wer': 0.048614340540568166, 'validation/ctc_loss': Array(0.40274152, dtype=float32), 'validation/wer': 0.12092452957702965, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22272642, dtype=float32), 'test/wer': 0.07527471411451668, 'test/num_examples': 2472, 'score': 31722.89506983757, 'total_duration': 34858.73185968399, 'accumulated_submission_time': 31722.89506983757, 'accumulated_eval_time': 3133.0039348602295, 'accumulated_logging_time': 1.1698052883148193, 'global_step': 37553, 'preemption_count': 0}), (39264, {'train/ctc_loss': Array(0.12867208, dtype=float32), 'train/wer': 0.05037870590781216, 'validation/ctc_loss': Array(0.39843825, dtype=float32), 'validation/wer': 0.11827915463857806, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22493432, dtype=float32), 'test/wer': 0.07369041090325594, 'test/num_examples': 2472, 'score': 33163.29486012459, 'total_duration': 36436.21641421318, 'accumulated_submission_time': 33163.29486012459, 'accumulated_eval_time': 3269.946902036667, 'accumulated_logging_time': 1.2371857166290283, 'global_step': 39264, 'preemption_count': 0}), (40958, {'train/ctc_loss': Array(0.11844589, dtype=float32), 'train/wer': 0.04667403892771734, 'validation/ctc_loss': Array(0.38874224, dtype=float32), 'validation/wer': 0.11543103198586559, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21982695, dtype=float32), 'test/wer': 0.07413726565515, 'test/num_examples': 2472, 'score': 34603.79007267952, 'total_duration': 38015.011687755585, 'accumulated_submission_time': 34603.79007267952, 'accumulated_eval_time': 3408.1167809963226, 'accumulated_logging_time': 1.2922863960266113, 'global_step': 40958, 'preemption_count': 0}), (42686, {'train/ctc_loss': Array(0.11153542, dtype=float32), 'train/wer': 0.04539858417974761, 'validation/ctc_loss': Array(0.38364777, dtype=float32), 'validation/wer': 0.11286289427189435, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2124593, dtype=float32), 'test/wer': 0.06989214551215649, 'test/num_examples': 2472, 'score': 36044.39460206032, 'total_duration': 39592.237605810165, 'accumulated_submission_time': 36044.39460206032, 'accumulated_eval_time': 3544.602229118347, 'accumulated_logging_time': 1.3495960235595703, 'global_step': 42686, 'preemption_count': 0}), (44363, {'train/ctc_loss': Array(0.1322927, dtype=float32), 'train/wer': 0.047837093291638745, 'validation/ctc_loss': Array(0.37878135, dtype=float32), 'validation/wer': 0.11108643810884655, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20779921, dtype=float32), 'test/wer': 0.06956716023805172, 'test/num_examples': 2472, 'score': 37485.06210923195, 'total_duration': 41171.84330153465, 'accumulated_submission_time': 37485.06210923195, 'accumulated_eval_time': 3683.409082174301, 'accumulated_logging_time': 1.406475305557251, 'global_step': 44363, 'preemption_count': 0}), (46050, {'train/ctc_loss': Array(0.08896961, dtype=float32), 'train/wer': 0.035933301884857556, 'validation/ctc_loss': Array(0.37348276, dtype=float32), 'validation/wer': 0.1105457775374842, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20512715, dtype=float32), 'test/wer': 0.0687953202120529, 'test/num_examples': 2472, 'score': 38925.02504873276, 'total_duration': 42748.52485728264, 'accumulated_submission_time': 38925.02504873276, 'accumulated_eval_time': 3819.99338889122, 'accumulated_logging_time': 1.465172290802002, 'global_step': 46050, 'preemption_count': 0}), (47755, {'train/ctc_loss': Array(0.09621476, dtype=float32), 'train/wer': 0.03858077130176123, 'validation/ctc_loss': Array(0.3708817, dtype=float32), 'validation/wer': 0.10841209921121485, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2026361, dtype=float32), 'test/wer': 0.0677391180712124, 'test/num_examples': 2472, 'score': 40365.21565961838, 'total_duration': 44327.65276861191, 'accumulated_submission_time': 40365.21565961838, 'accumulated_eval_time': 3958.7835640907288, 'accumulated_logging_time': 1.533278226852417, 'global_step': 47755, 'preemption_count': 0}), (49431, {'train/ctc_loss': Array(0.11491051, dtype=float32), 'train/wer': 0.045480027260535974, 'validation/ctc_loss': Array(0.3653458, dtype=float32), 'validation/wer': 0.10661633374204697, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1993707, dtype=float32), 'test/wer': 0.06562671378953142, 'test/num_examples': 2472, 'score': 41805.530272483826, 'total_duration': 45904.92424035072, 'accumulated_submission_time': 41805.530272483826, 'accumulated_eval_time': 4095.6011593341827, 'accumulated_logging_time': 1.597571611404419, 'global_step': 49431, 'preemption_count': 0}), (51120, {'train/ctc_loss': Array(0.11720094, dtype=float32), 'train/wer': 0.04599116109105629, 'validation/ctc_loss': Array(0.35821724, dtype=float32), 'validation/wer': 0.10473367639533873, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19543357, dtype=float32), 'test/wer': 0.06438770743200699, 'test/num_examples': 2472, 'score': 43246.34427714348, 'total_duration': 47481.11702299118, 'accumulated_submission_time': 43246.34427714348, 'accumulated_eval_time': 4230.847477197647, 'accumulated_logging_time': 1.6536619663238525, 'global_step': 51120, 'preemption_count': 0}), (52839, {'train/ctc_loss': Array(0.1289726, dtype=float32), 'train/wer': 0.05273111193614256, 'validation/ctc_loss': Array(0.3622812, dtype=float32), 'validation/wer': 0.10279309113027023, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19422725, dtype=float32), 'test/wer': 0.06308776633558792, 'test/num_examples': 2472, 'score': 44687.03170180321, 'total_duration': 49053.82754635811, 'accumulated_submission_time': 44687.03170180321, 'accumulated_eval_time': 4362.7331802845, 'accumulated_logging_time': 1.71537446975708, 'global_step': 52839, 'preemption_count': 0}), (54523, {'train/ctc_loss': Array(0.11021236, dtype=float32), 'train/wer': 0.04265171968540909, 'validation/ctc_loss': Array(0.34967783, dtype=float32), 'validation/wer': 0.10186624443650617, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19085734, dtype=float32), 'test/wer': 0.062417484207746836, 'test/num_examples': 2472, 'score': 46126.970742702484, 'total_duration': 50630.23694562912, 'accumulated_submission_time': 46126.970742702484, 'accumulated_eval_time': 4499.063733100891, 'accumulated_logging_time': 1.7805514335632324, 'global_step': 54523, 'preemption_count': 0}), (56223, {'train/ctc_loss': Array(0.09992043, dtype=float32), 'train/wer': 0.04021150939261291, 'validation/ctc_loss': Array(0.3473121, dtype=float32), 'validation/wer': 0.10063044884482077, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1872067, dtype=float32), 'test/wer': 0.061056608372433124, 'test/num_examples': 2472, 'score': 47568.220660448074, 'total_duration': 52210.12111449242, 'accumulated_submission_time': 47568.220660448074, 'accumulated_eval_time': 4637.565958499908, 'accumulated_logging_time': 1.8369412422180176, 'global_step': 56223, 'preemption_count': 0}), (57923, {'train/ctc_loss': Array(0.08032824, dtype=float32), 'train/wer': 0.03222073828152458, 'validation/ctc_loss': Array(0.34661222, dtype=float32), 'validation/wer': 0.09867055427363218, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18778616, dtype=float32), 'test/wer': 0.06146283996506408, 'test/num_examples': 2472, 'score': 49008.23717498779, 'total_duration': 53784.19967198372, 'accumulated_submission_time': 49008.23717498779, 'accumulated_eval_time': 4771.489331007004, 'accumulated_logging_time': 1.8994395732879639, 'global_step': 57923, 'preemption_count': 0}), (59600, {'train/ctc_loss': Array(0.09178022, dtype=float32), 'train/wer': 0.03688837503544862, 'validation/ctc_loss': Array(0.3443059, dtype=float32), 'validation/wer': 0.09844849725325121, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18204756, dtype=float32), 'test/wer': 0.058375479861068794, 'test/num_examples': 2472, 'score': 50448.39874601364, 'total_duration': 55359.5160779953, 'accumulated_submission_time': 50448.39874601364, 'accumulated_eval_time': 4906.509536027908, 'accumulated_logging_time': 1.959836483001709, 'global_step': 59600, 'preemption_count': 0}), (61297, {'train/ctc_loss': Array(0.076783, dtype=float32), 'train/wer': 0.029943321569885575, 'validation/ctc_loss': Array(0.34047252, dtype=float32), 'validation/wer': 0.09631481892698186, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18233697, dtype=float32), 'test/wer': 0.05872077671480511, 'test/num_examples': 2472, 'score': 51888.93256998062, 'total_duration': 56940.76946043968, 'accumulated_submission_time': 51888.93256998062, 'accumulated_eval_time': 5047.088932514191, 'accumulated_logging_time': 2.025007963180542, 'global_step': 61297, 'preemption_count': 0}), (62996, {'train/ctc_loss': Array(0.07512333, dtype=float32), 'train/wer': 0.030618193308671705, 'validation/ctc_loss': Array(0.33476064, dtype=float32), 'validation/wer': 0.09451905345781399, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17784591, dtype=float32), 'test/wer': 0.05746145877764914, 'test/num_examples': 2472, 'score': 53328.91833233833, 'total_duration': 58515.613005161285, 'accumulated_submission_time': 53328.91833233833, 'accumulated_eval_time': 5181.807159900665, 'accumulated_logging_time': 2.0874433517456055, 'global_step': 62996, 'preemption_count': 0}), (64691, {'train/ctc_loss': Array(0.06882256, dtype=float32), 'train/wer': 0.02745650481818082, 'validation/ctc_loss': Array(0.33348805, dtype=float32), 'validation/wer': 0.09349566023344952, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17620729, dtype=float32), 'test/wer': 0.05599902504417768, 'test/num_examples': 2472, 'score': 54768.97867703438, 'total_duration': 60091.00317811966, 'accumulated_submission_time': 54768.97867703438, 'accumulated_eval_time': 5317.002858400345, 'accumulated_logging_time': 2.147326707839966, 'global_step': 64691, 'preemption_count': 0}), (66393, {'train/ctc_loss': Array(0.07160717, dtype=float32), 'train/wer': 0.027959459071463435, 'validation/ctc_loss': Array(0.33296782, dtype=float32), 'validation/wer': 0.09302258223350744, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17585711, dtype=float32), 'test/wer': 0.05683179980907115, 'test/num_examples': 2472, 'score': 56209.32621002197, 'total_duration': 61663.78510403633, 'accumulated_submission_time': 56209.32621002197, 'accumulated_eval_time': 5449.299804925919, 'accumulated_logging_time': 2.2101027965545654, 'global_step': 66393, 'preemption_count': 0}), (68079, {'train/ctc_loss': Array(0.07398825, dtype=float32), 'train/wer': 0.028735978276936216, 'validation/ctc_loss': Array(0.33303624, dtype=float32), 'validation/wer': 0.09305154619268756, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17321478, dtype=float32), 'test/wer': 0.055897467146019945, 'test/num_examples': 2472, 'score': 57649.288610458374, 'total_duration': 63239.29641199112, 'accumulated_submission_time': 57649.288610458374, 'accumulated_eval_time': 5584.706095695496, 'accumulated_logging_time': 2.277143955230713, 'global_step': 68079, 'preemption_count': 0}), (69778, {'train/ctc_loss': Array(0.06125573, dtype=float32), 'train/wer': 0.023995495269475434, 'validation/ctc_loss': Array(0.32672223, dtype=float32), 'validation/wer': 0.09115923419291928, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17336021, dtype=float32), 'test/wer': 0.05553185871265208, 'test/num_examples': 2472, 'score': 59090.46134185791, 'total_duration': 64816.39191651344, 'accumulated_submission_time': 59090.46134185791, 'accumulated_eval_time': 5720.492845773697, 'accumulated_logging_time': 2.3368923664093018, 'global_step': 69778, 'preemption_count': 0}), (71500, {'train/ctc_loss': Array(0.06221401, dtype=float32), 'train/wer': 0.024718881054426472, 'validation/ctc_loss': Array(0.32732484, dtype=float32), 'validation/wer': 0.09039651660117594, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17124374, dtype=float32), 'test/wer': 0.05437409867365385, 'test/num_examples': 2472, 'score': 60530.481770038605, 'total_duration': 66391.79093980789, 'accumulated_submission_time': 60530.481770038605, 'accumulated_eval_time': 5855.735311031342, 'accumulated_logging_time': 2.396700859069824, 'global_step': 71500, 'preemption_count': 0})], 'global_step': 72135}
I0215 15:56:09.590406 139646656866112 submission_runner.py:586] Timing: 61068.56172633171
I0215 15:56:09.590476 139646656866112 submission_runner.py:588] Total number of evals: 43
I0215 15:56:09.590530 139646656866112 submission_runner.py:589] ====================
I0215 15:56:09.590590 139646656866112 submission_runner.py:542] Using RNG seed 1365630931
I0215 15:56:09.593261 139646656866112 submission_runner.py:551] --- Tuning run 3/5 ---
I0215 15:56:09.593418 139646656866112 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_2/librispeech_conformer_jax/trial_3.
I0215 15:56:09.595433 139646656866112 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_2/librispeech_conformer_jax/trial_3/hparams.json.
I0215 15:56:09.596894 139646656866112 submission_runner.py:206] Initializing dataset.
I0215 15:56:09.597057 139646656866112 submission_runner.py:213] Initializing model.
I0215 15:56:13.363858 139646656866112 submission_runner.py:255] Initializing optimizer.
I0215 15:56:13.827529 139646656866112 submission_runner.py:262] Initializing metrics bundle.
I0215 15:56:13.827844 139646656866112 submission_runner.py:280] Initializing checkpoint and logger.
I0215 15:56:13.833642 139646656866112 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_2/librispeech_conformer_jax/trial_3 with prefix checkpoint_
I0215 15:56:13.833838 139646656866112 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_2/librispeech_conformer_jax/trial_3/meta_data_0.json.
I0215 15:56:13.834192 139646656866112 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0215 15:56:13.834289 139646656866112 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0215 15:56:14.412496 139646656866112 logger_utils.py:220] Unable to record git information. Continuing without it.
I0215 15:56:14.950948 139646656866112 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_2/librispeech_conformer_jax/trial_3/flags_0.json.
I0215 15:56:14.970225 139646656866112 submission_runner.py:314] Starting training loop.
I0215 15:56:14.974104 139646656866112 input_pipeline.py:20] Loading split = train-clean-100
I0215 15:56:15.023807 139646656866112 input_pipeline.py:20] Loading split = train-clean-360
I0215 15:56:15.656594 139646656866112 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0215 15:56:50.869203 139475292407552 logging_writer.py:48] [0] global_step=0, grad_norm=42.290122985839844, loss=31.438430786132812
I0215 15:56:50.892597 139646656866112 spec.py:321] Evaluating on the training split.
I0215 15:57:48.249835 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 15:58:43.074371 139646656866112 spec.py:349] Evaluating on the test split.
I0215 15:59:11.693542 139646656866112 submission_runner.py:408] Time since start: 176.72s, 	Step: 1, 	{'train/ctc_loss': Array(32.398228, dtype=float32), 'train/wer': 1.135258711138108, 'validation/ctc_loss': Array(30.757864, dtype=float32), 'validation/wer': 1.1779352558965794, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.871214, dtype=float32), 'test/wer': 1.190014827453131, 'test/num_examples': 2472, 'score': 35.92231845855713, 'total_duration': 176.72045016288757, 'accumulated_submission_time': 35.92231845855713, 'accumulated_eval_time': 140.7980694770813, 'accumulated_logging_time': 0}
I0215 15:59:11.712207 139535991158528 logging_writer.py:48] [1] accumulated_eval_time=140.798069, accumulated_logging_time=0, accumulated_submission_time=35.922318, global_step=1, preemption_count=0, score=35.922318, test/ctc_loss=30.871213912963867, test/num_examples=2472, test/wer=1.190015, total_duration=176.720450, train/ctc_loss=32.39822769165039, train/wer=1.135259, validation/ctc_loss=30.757863998413086, validation/num_examples=5348, validation/wer=1.177935
I0215 16:00:53.557320 139475772843776 logging_writer.py:48] [100] global_step=100, grad_norm=27.528732299804688, loss=7.3350510597229
I0215 16:02:10.531949 139475781236480 logging_writer.py:48] [200] global_step=200, grad_norm=3.050387382507324, loss=6.131450176239014
I0215 16:03:27.372005 139475772843776 logging_writer.py:48] [300] global_step=300, grad_norm=0.4086376428604126, loss=5.855576992034912
I0215 16:04:43.892398 139475781236480 logging_writer.py:48] [400] global_step=400, grad_norm=0.4015501141548157, loss=5.810764312744141
I0215 16:06:04.356986 139475772843776 logging_writer.py:48] [500] global_step=500, grad_norm=0.4725317060947418, loss=5.78285551071167
I0215 16:07:33.066268 139475781236480 logging_writer.py:48] [600] global_step=600, grad_norm=0.25514456629753113, loss=5.80399227142334
I0215 16:09:02.989061 139475772843776 logging_writer.py:48] [700] global_step=700, grad_norm=0.56923907995224, loss=5.79317045211792
I0215 16:10:31.843057 139475781236480 logging_writer.py:48] [800] global_step=800, grad_norm=0.33320730924606323, loss=5.788305759429932
I0215 16:12:04.069683 139475772843776 logging_writer.py:48] [900] global_step=900, grad_norm=0.3568946421146393, loss=5.809726238250732
I0215 16:13:32.196454 139475781236480 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.2920480966567993, loss=5.778970241546631
I0215 16:14:58.161595 139535991158528 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.4651395082473755, loss=5.7573323249816895
I0215 16:16:18.175514 139535982765824 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.4742976129055023, loss=5.777767658233643
I0215 16:17:38.533003 139535991158528 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.2788834869861603, loss=5.7864155769348145
I0215 16:19:03.286579 139535982765824 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.0346341133117676, loss=5.720396518707275
I0215 16:20:32.369381 139535991158528 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.41400575637817383, loss=5.556803226470947
I0215 16:22:02.628610 139535982765824 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.0933831930160522, loss=5.459134578704834
I0215 16:23:11.845275 139646656866112 spec.py:321] Evaluating on the training split.
I0215 16:23:50.597212 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 16:24:37.869997 139646656866112 spec.py:349] Evaluating on the test split.
I0215 16:25:02.598763 139646656866112 submission_runner.py:408] Time since start: 1727.62s, 	Step: 1679, 	{'train/ctc_loss': Array(6.0571713, dtype=float32), 'train/wer': 0.9391896477614642, 'validation/ctc_loss': Array(6.13424, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.0640683, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1475.9753279685974, 'total_duration': 1727.6220135688782, 'accumulated_submission_time': 1475.9753279685974, 'accumulated_eval_time': 251.5450747013092, 'accumulated_logging_time': 0.03110361099243164}
I0215 16:25:02.634201 139535991158528 logging_writer.py:48] [1679] accumulated_eval_time=251.545075, accumulated_logging_time=0.031104, accumulated_submission_time=1475.975328, global_step=1679, preemption_count=0, score=1475.975328, test/ctc_loss=6.06406831741333, test/num_examples=2472, test/wer=0.899580, total_duration=1727.622014, train/ctc_loss=6.05717134475708, train/wer=0.939190, validation/ctc_loss=6.13424015045166, validation/num_examples=5348, validation/wer=0.896618
I0215 16:25:19.613414 139535982765824 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.4329206943511963, loss=5.37596321105957
I0215 16:26:36.774827 139535991158528 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.4875903129577637, loss=4.982575416564941
I0215 16:27:53.927448 139535982765824 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.7727116346359253, loss=4.448923110961914
I0215 16:29:19.210871 139535991158528 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.3467507362365723, loss=4.018180847167969
I0215 16:30:45.991337 139535991158528 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.0312143564224243, loss=3.7500529289245605
I0215 16:32:05.331534 139535982765824 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.1669005155563354, loss=3.5217020511627197
I0215 16:33:27.952096 139535991158528 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.1415144205093384, loss=3.4305686950683594
I0215 16:34:50.870004 139535982765824 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.3135147094726562, loss=3.297069787979126
I0215 16:36:21.283413 139535991158528 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.546443223953247, loss=3.2183144092559814
I0215 16:37:54.226801 139535982765824 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.866876244544983, loss=3.0826218128204346
I0215 16:39:24.116163 139535991158528 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.1519429683685303, loss=3.0202012062072754
I0215 16:40:52.502176 139535982765824 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.1324734687805176, loss=2.9048871994018555
I0215 16:42:23.046435 139535991158528 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.3365038633346558, loss=2.8996167182922363
I0215 16:43:53.972832 139535982765824 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.1041830778121948, loss=2.8490848541259766
I0215 16:45:22.618767 139535991158528 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.1694279909133911, loss=2.8354110717773438
I0215 16:46:40.518607 139535982765824 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.9815049171447754, loss=2.6829748153686523
I0215 16:47:59.162832 139535991158528 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.0025094747543335, loss=2.7077243328094482
I0215 16:49:03.194334 139646656866112 spec.py:321] Evaluating on the training split.
I0215 16:49:45.502264 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 16:50:34.954885 139646656866112 spec.py:349] Evaluating on the test split.
I0215 16:51:00.924675 139646656866112 submission_runner.py:408] Time since start: 3285.95s, 	Step: 3379, 	{'train/ctc_loss': Array(3.853232, dtype=float32), 'train/wer': 0.8365734501347709, 'validation/ctc_loss': Array(3.802879, dtype=float32), 'validation/wer': 0.7978219102696544, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.4898498, dtype=float32), 'test/wer': 0.754087705400849, 'test/num_examples': 2472, 'score': 2916.449270963669, 'total_duration': 3285.9481086730957, 'accumulated_submission_time': 2916.449270963669, 'accumulated_eval_time': 369.26913595199585, 'accumulated_logging_time': 0.08405637741088867}
I0215 16:51:00.964818 139535991158528 logging_writer.py:48] [3379] accumulated_eval_time=369.269136, accumulated_logging_time=0.084056, accumulated_submission_time=2916.449271, global_step=3379, preemption_count=0, score=2916.449271, test/ctc_loss=3.489849805831909, test/num_examples=2472, test/wer=0.754088, total_duration=3285.948109, train/ctc_loss=3.853231906890869, train/wer=0.836573, validation/ctc_loss=3.8028790950775146, validation/num_examples=5348, validation/wer=0.797822
I0215 16:51:17.752039 139535982765824 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.1277499198913574, loss=2.632704257965088
I0215 16:52:34.181916 139535991158528 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.2178350687026978, loss=2.602989912033081
I0215 16:53:50.618677 139535982765824 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.9604911208152771, loss=2.6068310737609863
I0215 16:55:17.862993 139535991158528 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.4058291912078857, loss=2.513370990753174
I0215 16:56:47.613377 139535982765824 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.0332164764404297, loss=2.427440881729126
I0215 16:58:19.568327 139535991158528 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.9986699223518372, loss=2.452694892883301
I0215 16:59:51.718227 139535982765824 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.9403914213180542, loss=2.434877395629883
I0215 17:01:23.218474 139535991158528 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.9252661466598511, loss=2.3622100353240967
I0215 17:02:46.632843 139535991158528 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.8830220103263855, loss=2.2727208137512207
I0215 17:04:06.617348 139535982765824 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.9772026538848877, loss=2.3020269870758057
I0215 17:05:27.139279 139535991158528 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.9104305505752563, loss=2.2222166061401367
I0215 17:06:51.563881 139535982765824 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.8562313914299011, loss=2.210768461227417
I0215 17:08:21.015337 139535991158528 logging_writer.py:48] [4600] global_step=4600, grad_norm=1.232412576675415, loss=2.20719838142395
I0215 17:09:52.533486 139535982765824 logging_writer.py:48] [4700] global_step=4700, grad_norm=1.0589582920074463, loss=2.1753060817718506
I0215 17:11:20.206639 139535991158528 logging_writer.py:48] [4800] global_step=4800, grad_norm=1.0858362913131714, loss=2.0766408443450928
I0215 17:12:48.069422 139535982765824 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.8542726039886475, loss=2.1111555099487305
I0215 17:14:15.810601 139535991158528 logging_writer.py:48] [5000] global_step=5000, grad_norm=1.6755131483078003, loss=2.151160717010498
I0215 17:15:02.260625 139646656866112 spec.py:321] Evaluating on the training split.
I0215 17:16:01.139427 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 17:16:55.396386 139646656866112 spec.py:349] Evaluating on the test split.
I0215 17:17:23.643011 139646656866112 submission_runner.py:408] Time since start: 4868.67s, 	Step: 5053, 	{'train/ctc_loss': Array(1.3261416, dtype=float32), 'train/wer': 0.39291693782991527, 'validation/ctc_loss': Array(1.3397774, dtype=float32), 'validation/wer': 0.3746005387296408, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.0181005, dtype=float32), 'test/wer': 0.3211666971340361, 'test/num_examples': 2472, 'score': 4357.660209178925, 'total_duration': 4868.666774988174, 'accumulated_submission_time': 4357.660209178925, 'accumulated_eval_time': 510.6455659866333, 'accumulated_logging_time': 0.14217591285705566}
I0215 17:17:23.679528 139535991158528 logging_writer.py:48] [5053] accumulated_eval_time=510.645566, accumulated_logging_time=0.142176, accumulated_submission_time=4357.660209, global_step=5053, preemption_count=0, score=4357.660209, test/ctc_loss=1.0181005001068115, test/num_examples=2472, test/wer=0.321167, total_duration=4868.666775, train/ctc_loss=1.326141595840454, train/wer=0.392917, validation/ctc_loss=1.3397773504257202, validation/num_examples=5348, validation/wer=0.374601
I0215 17:18:00.454289 139535982765824 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.8170506358146667, loss=2.1330060958862305
I0215 17:19:23.496620 139535991158528 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.8427500128746033, loss=2.063662528991699
I0215 17:20:42.126886 139535982765824 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.9007355570793152, loss=1.9482600688934326
I0215 17:22:02.691428 139535991158528 logging_writer.py:48] [5400] global_step=5400, grad_norm=1.0403809547424316, loss=2.042844772338867
I0215 17:23:28.658432 139535982765824 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.785635769367218, loss=1.9888215065002441
I0215 17:24:58.367205 139535991158528 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.1071113348007202, loss=1.9667712450027466
I0215 17:26:29.311606 139535982765824 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.8218348026275635, loss=1.9298020601272583
I0215 17:27:54.696665 139535991158528 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.782249927520752, loss=1.9268494844436646
I0215 17:29:21.659072 139535982765824 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.7803175449371338, loss=1.8682912588119507
I0215 17:30:50.817335 139535991158528 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.7338767051696777, loss=1.869928002357483
I0215 17:32:21.303582 139535982765824 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.8273886442184448, loss=1.8623764514923096
I0215 17:33:51.150200 139535991158528 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.7037763595581055, loss=1.904876708984375
I0215 17:35:08.390677 139535982765824 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.7893164157867432, loss=1.8497565984725952
I0215 17:36:29.366132 139535991158528 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.9306887984275818, loss=1.7876008749008179
I0215 17:37:53.917625 139535982765824 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.7360010147094727, loss=1.8031997680664062
I0215 17:39:17.200848 139535991158528 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.6601068377494812, loss=1.7812069654464722
I0215 17:40:46.106678 139535982765824 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.8956347107887268, loss=1.832798719406128
I0215 17:41:24.477192 139646656866112 spec.py:321] Evaluating on the training split.
I0215 17:42:20.815343 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 17:43:15.251381 139646656866112 spec.py:349] Evaluating on the test split.
I0215 17:43:43.180505 139646656866112 submission_runner.py:408] Time since start: 6448.20s, 	Step: 6745, 	{'train/ctc_loss': Array(0.7336726, dtype=float32), 'train/wer': 0.24152971459282097, 'validation/ctc_loss': Array(0.87219507, dtype=float32), 'validation/wer': 0.25994187898857857, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.596215, dtype=float32), 'test/wer': 0.199540958300327, 'test/num_examples': 2472, 'score': 5798.374348640442, 'total_duration': 6448.203431367874, 'accumulated_submission_time': 5798.374348640442, 'accumulated_eval_time': 649.3420903682709, 'accumulated_logging_time': 0.19429850578308105}
I0215 17:43:43.218690 139535991158528 logging_writer.py:48] [6745] accumulated_eval_time=649.342090, accumulated_logging_time=0.194299, accumulated_submission_time=5798.374349, global_step=6745, preemption_count=0, score=5798.374349, test/ctc_loss=0.596215009689331, test/num_examples=2472, test/wer=0.199541, total_duration=6448.203431, train/ctc_loss=0.7336726188659668, train/wer=0.241530, validation/ctc_loss=0.8721950650215149, validation/num_examples=5348, validation/wer=0.259942
I0215 17:44:26.005493 139535982765824 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.7364721298217773, loss=1.7933419942855835
I0215 17:45:41.933411 139535991158528 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.7792091965675354, loss=1.7674373388290405
I0215 17:46:59.030929 139535982765824 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.7807924151420593, loss=1.7757867574691772
I0215 17:48:29.110722 139535991158528 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.7539055943489075, loss=1.7966700792312622
I0215 17:49:56.740412 139535982765824 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.6777661442756653, loss=1.771773099899292
I0215 17:51:20.469190 139535991158528 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.7392371892929077, loss=1.7574026584625244
I0215 17:52:40.843849 139535982765824 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.6945282816886902, loss=1.6667286157608032
I0215 17:54:04.130000 139535991158528 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.6842453479766846, loss=1.7261887788772583
I0215 17:55:28.542717 139535982765824 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.647077739238739, loss=1.7231413125991821
I0215 17:56:57.522922 139535991158528 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.709892988204956, loss=1.698000192642212
I0215 17:58:25.282688 139535982765824 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.738376259803772, loss=1.768646478652954
I0215 17:59:52.706848 139535991158528 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.7755701541900635, loss=1.758532166481018
I0215 18:01:20.704875 139535982765824 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.7543948292732239, loss=1.750153660774231
I0215 18:02:50.016586 139535991158528 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.6986130475997925, loss=1.6871837377548218
I0215 18:04:20.521411 139535982765824 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.7452908158302307, loss=1.727553129196167
I0215 18:05:47.788462 139535991158528 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.7630696892738342, loss=1.6674131155014038
I0215 18:07:06.463542 139535982765824 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.6648023128509521, loss=1.6818187236785889
I0215 18:07:43.451284 139646656866112 spec.py:321] Evaluating on the training split.
I0215 18:08:37.292004 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 18:09:30.798592 139646656866112 spec.py:349] Evaluating on the test split.
I0215 18:09:59.195779 139646656866112 submission_runner.py:408] Time since start: 8024.22s, 	Step: 8450, 	{'train/ctc_loss': Array(0.61011267, dtype=float32), 'train/wer': 0.20421158528917494, 'validation/ctc_loss': Array(0.7665455, dtype=float32), 'validation/wer': 0.23046622319626944, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.51278436, dtype=float32), 'test/wer': 0.1729734121422623, 'test/num_examples': 2472, 'score': 7238.520888566971, 'total_duration': 8024.218991994858, 'accumulated_submission_time': 7238.520888566971, 'accumulated_eval_time': 785.0801012516022, 'accumulated_logging_time': 0.24956583976745605}
I0215 18:09:59.236344 139535991158528 logging_writer.py:48] [8450] accumulated_eval_time=785.080101, accumulated_logging_time=0.249566, accumulated_submission_time=7238.520889, global_step=8450, preemption_count=0, score=7238.520889, test/ctc_loss=0.5127843618392944, test/num_examples=2472, test/wer=0.172973, total_duration=8024.218992, train/ctc_loss=0.6101126670837402, train/wer=0.204212, validation/ctc_loss=0.7665454745292664, validation/num_examples=5348, validation/wer=0.230466
I0215 18:10:38.081703 139535982765824 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.6745766401290894, loss=1.725112795829773
I0215 18:11:53.959948 139535991158528 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.6851993799209595, loss=1.658228874206543
I0215 18:13:11.039410 139535982765824 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.7024857401847839, loss=1.6750025749206543
I0215 18:14:40.246697 139535991158528 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.7176678776741028, loss=1.695173978805542
I0215 18:16:06.217319 139535982765824 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.6651528477668762, loss=1.6151387691497803
I0215 18:17:37.343059 139535991158528 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.6782934665679932, loss=1.619291067123413
I0215 18:19:04.745186 139535982765824 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.7198449969291687, loss=1.7002620697021484
I0215 18:20:34.834893 139535991158528 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.6816741824150085, loss=1.6427956819534302
I0215 18:22:05.042147 139535991158528 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.6311198472976685, loss=1.647549033164978
I0215 18:23:22.556565 139535982765824 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.6436275243759155, loss=1.6007963418960571
I0215 18:24:42.275691 139535991158528 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.711043119430542, loss=1.667758822441101
I0215 18:26:00.848826 139535982765824 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.6937711834907532, loss=1.6127417087554932
I0215 18:27:23.682748 139535991158528 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.6735476851463318, loss=1.6760890483856201
I0215 18:28:52.605629 139535982765824 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.7629145979881287, loss=1.5808539390563965
I0215 18:30:23.450885 139535991158528 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.7825250625610352, loss=1.6647528409957886
I0215 18:31:53.169613 139535982765824 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.8895164132118225, loss=1.6293044090270996
I0215 18:33:22.609351 139535991158528 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.7369457483291626, loss=1.6050974130630493
I0215 18:33:59.363214 139646656866112 spec.py:321] Evaluating on the training split.
I0215 18:34:53.460246 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 18:35:46.810049 139646656866112 spec.py:349] Evaluating on the test split.
I0215 18:36:14.192248 139646656866112 submission_runner.py:408] Time since start: 9599.22s, 	Step: 10145, 	{'train/ctc_loss': Array(0.5839462, dtype=float32), 'train/wer': 0.19995284792895754, 'validation/ctc_loss': Array(0.6855125, dtype=float32), 'validation/wer': 0.20873359915811426, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.44745144, dtype=float32), 'test/wer': 0.15215404301992566, 'test/num_examples': 2472, 'score': 8678.477567434311, 'total_duration': 9599.215919017792, 'accumulated_submission_time': 8678.477567434311, 'accumulated_eval_time': 919.9031093120575, 'accumulated_logging_time': 0.39224886894226074}
I0215 18:36:14.230667 139535991158528 logging_writer.py:48] [10145] accumulated_eval_time=919.903109, accumulated_logging_time=0.392249, accumulated_submission_time=8678.477567, global_step=10145, preemption_count=0, score=8678.477567, test/ctc_loss=0.4474514424800873, test/num_examples=2472, test/wer=0.152154, total_duration=9599.215919, train/ctc_loss=0.5839462280273438, train/wer=0.199953, validation/ctc_loss=0.6855124831199646, validation/num_examples=5348, validation/wer=0.208734
I0215 18:36:56.940546 139535982765824 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.7329074740409851, loss=1.6284857988357544
I0215 18:38:16.722867 139535991158528 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.6052306294441223, loss=1.5735218524932861
I0215 18:39:33.349474 139535982765824 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.669713020324707, loss=1.5800950527191162
I0215 18:40:49.866430 139535991158528 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.7074447274208069, loss=1.543893575668335
I0215 18:42:09.474970 139535982765824 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.6648802161216736, loss=1.5794659852981567
I0215 18:43:34.646282 139535991158528 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.6312190890312195, loss=1.571223497390747
I0215 18:45:03.801390 139535982765824 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.5903329253196716, loss=1.564743995666504
I0215 18:46:30.883348 139535991158528 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.5971906185150146, loss=1.608142375946045
I0215 18:47:59.250657 139535982765824 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.6044623851776123, loss=1.5523878335952759
I0215 18:49:29.002619 139535991158528 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.7548086047172546, loss=1.5111384391784668
I0215 18:50:58.806408 139535982765824 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.5978001952171326, loss=1.5725675821304321
I0215 18:52:25.685124 139535991158528 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.6524345874786377, loss=1.5544160604476929
I0215 18:53:49.559202 139535991158528 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.5878955125808716, loss=1.5302103757858276
I0215 18:55:10.279230 139535982765824 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.5721269249916077, loss=1.526608943939209
I0215 18:56:31.043956 139535991158528 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.6787415146827698, loss=1.4736173152923584
I0215 18:57:53.720948 139535982765824 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.6066799163818359, loss=1.486028790473938
I0215 18:59:21.297676 139535991158528 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.6561987996101379, loss=1.503194808959961
I0215 19:00:14.715651 139646656866112 spec.py:321] Evaluating on the training split.
I0215 19:01:10.734182 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 19:02:05.874017 139646656866112 spec.py:349] Evaluating on the test split.
I0215 19:02:33.369178 139646656866112 submission_runner.py:408] Time since start: 11178.39s, 	Step: 11861, 	{'train/ctc_loss': Array(0.50584203, dtype=float32), 'train/wer': 0.17452687315601126, 'validation/ctc_loss': Array(0.65349907, dtype=float32), 'validation/wer': 0.19818106336348804, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.41517326, dtype=float32), 'test/wer': 0.14240448479678264, 'test/num_examples': 2472, 'score': 10118.87825846672, 'total_duration': 11178.391873121262, 'accumulated_submission_time': 10118.87825846672, 'accumulated_eval_time': 1058.5496191978455, 'accumulated_logging_time': 0.4467132091522217}
I0215 19:02:33.405413 139535991158528 logging_writer.py:48] [11861] accumulated_eval_time=1058.549619, accumulated_logging_time=0.446713, accumulated_submission_time=10118.878258, global_step=11861, preemption_count=0, score=10118.878258, test/ctc_loss=0.4151732623577118, test/num_examples=2472, test/wer=0.142404, total_duration=11178.391873, train/ctc_loss=0.5058420300483704, train/wer=0.174527, validation/ctc_loss=0.6534990668296814, validation/num_examples=5348, validation/wer=0.198181
I0215 19:03:03.850989 139535982765824 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.5635231137275696, loss=1.5269713401794434
I0215 19:04:19.897695 139535991158528 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.583682656288147, loss=1.5352095365524292
I0215 19:05:35.927082 139535982765824 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.5491285920143127, loss=1.5592001676559448
I0215 19:06:59.395373 139535991158528 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.5588002800941467, loss=1.5318800210952759
I0215 19:08:28.368565 139535982765824 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.6168115139007568, loss=1.5496103763580322
I0215 19:09:55.372144 139535991158528 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.683025062084198, loss=1.475205659866333
I0215 19:11:12.477961 139535982765824 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.5749675035476685, loss=1.4991533756256104
I0215 19:12:31.028830 139535991158528 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.6715447306632996, loss=1.5389412641525269
I0215 19:13:55.634869 139535982765824 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.69316166639328, loss=1.4893735647201538
I0215 19:15:22.464823 139535991158528 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.5765326023101807, loss=1.4376298189163208
I0215 19:16:53.153622 139535982765824 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.6527336835861206, loss=1.5197768211364746
I0215 19:18:21.076871 139535991158528 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.6463582515716553, loss=1.5305562019348145
I0215 19:19:53.059435 139535982765824 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.618014931678772, loss=1.4828993082046509
I0215 19:21:22.757231 139535991158528 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.7153702974319458, loss=1.4368200302124023
I0215 19:22:52.380900 139535982765824 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.7547833919525146, loss=1.5334270000457764
I0215 19:24:23.066692 139535991158528 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.6194930076599121, loss=1.4028891324996948
I0215 19:25:39.257604 139535982765824 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.6426565051078796, loss=1.450700283050537
I0215 19:26:34.014398 139646656866112 spec.py:321] Evaluating on the training split.
I0215 19:27:29.173711 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 19:28:22.135417 139646656866112 spec.py:349] Evaluating on the test split.
I0215 19:28:50.406221 139646656866112 submission_runner.py:408] Time since start: 12755.43s, 	Step: 13572, 	{'train/ctc_loss': Array(0.49853423, dtype=float32), 'train/wer': 0.1681780210600365, 'validation/ctc_loss': Array(0.62621295, dtype=float32), 'validation/wer': 0.18898017899726774, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39062923, dtype=float32), 'test/wer': 0.13362988239595394, 'test/num_examples': 2472, 'score': 11559.40121126175, 'total_duration': 12755.429028272629, 'accumulated_submission_time': 11559.40121126175, 'accumulated_eval_time': 1194.934509754181, 'accumulated_logging_time': 0.5001969337463379}
I0215 19:28:50.443635 139535991158528 logging_writer.py:48] [13572] accumulated_eval_time=1194.934510, accumulated_logging_time=0.500197, accumulated_submission_time=11559.401211, global_step=13572, preemption_count=0, score=11559.401211, test/ctc_loss=0.39062923192977905, test/num_examples=2472, test/wer=0.133630, total_duration=12755.429028, train/ctc_loss=0.498534232378006, train/wer=0.168178, validation/ctc_loss=0.6262129545211792, validation/num_examples=5348, validation/wer=0.188980
I0215 19:29:12.486424 139535982765824 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.6686499714851379, loss=1.5175418853759766
I0215 19:30:28.310341 139535991158528 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.5770279169082642, loss=1.5031085014343262
I0215 19:31:44.746653 139535982765824 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.6434481143951416, loss=1.4880027770996094
I0215 19:33:07.328616 139535991158528 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.5710830688476562, loss=1.494403600692749
I0215 19:34:35.891164 139535982765824 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.6904600262641907, loss=1.4459673166275024
I0215 19:36:01.298862 139535991158528 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.6282361745834351, loss=1.5171412229537964
I0215 19:37:27.240220 139535982765824 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.6975856423377991, loss=1.471015453338623
I0215 19:38:53.669201 139535991158528 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.545802116394043, loss=1.5503123998641968
I0215 19:40:26.165025 139535982765824 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.5976815819740295, loss=1.4777024984359741
I0215 19:41:49.647430 139535991158528 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.7117529511451721, loss=1.45340096950531
I0215 19:43:07.304933 139535982765824 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.7238355278968811, loss=1.4554849863052368
I0215 19:44:27.858230 139535991158528 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.6474601626396179, loss=1.4399805068969727
I0215 19:45:52.646147 139535982765824 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.7199215888977051, loss=1.497908353805542
I0215 19:47:23.514223 139535991158528 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.6694257259368896, loss=1.4733505249023438
I0215 19:48:50.214643 139535982765824 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.5977421998977661, loss=1.4110006093978882
I0215 19:50:19.531232 139535991158528 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.6759942173957825, loss=1.4622561931610107
I0215 19:51:49.680859 139535982765824 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.7730090022087097, loss=1.4358012676239014
I0215 19:52:50.772579 139646656866112 spec.py:321] Evaluating on the training split.
I0215 19:53:45.873286 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 19:54:40.310552 139646656866112 spec.py:349] Evaluating on the test split.
I0215 19:55:07.748415 139646656866112 submission_runner.py:408] Time since start: 14332.77s, 	Step: 15270, 	{'train/ctc_loss': Array(0.41710535, dtype=float32), 'train/wer': 0.1461387617891493, 'validation/ctc_loss': Array(0.58882713, dtype=float32), 'validation/wer': 0.18003997026366858, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36876667, dtype=float32), 'test/wer': 0.12597241687486035, 'test/num_examples': 2472, 'score': 12999.642688512802, 'total_duration': 14332.771677017212, 'accumulated_submission_time': 12999.642688512802, 'accumulated_eval_time': 1331.903869152069, 'accumulated_logging_time': 0.5556769371032715}
I0215 19:55:07.791722 139535991158528 logging_writer.py:48] [15270] accumulated_eval_time=1331.903869, accumulated_logging_time=0.555677, accumulated_submission_time=12999.642689, global_step=15270, preemption_count=0, score=12999.642689, test/ctc_loss=0.3687666654586792, test/num_examples=2472, test/wer=0.125972, total_duration=14332.771677, train/ctc_loss=0.4171053469181061, train/wer=0.146139, validation/ctc_loss=0.5888271331787109, validation/num_examples=5348, validation/wer=0.180040
I0215 19:55:31.588424 139535982765824 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.7375122308731079, loss=1.5066697597503662
I0215 19:56:47.870498 139535991158528 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.595710813999176, loss=1.516250729560852
I0215 19:58:08.905344 139535991158528 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.579488217830658, loss=1.4485607147216797
I0215 19:59:29.019695 139535982765824 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.6795836687088013, loss=1.4795801639556885
I0215 20:00:48.581192 139535991158528 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.5311117172241211, loss=1.4681260585784912
I0215 20:02:13.986114 139535982765824 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.646652340888977, loss=1.4534763097763062
I0215 20:03:41.558043 139535991158528 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.6671010851860046, loss=1.481308102607727
I0215 20:05:10.551038 139535982765824 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.6954549551010132, loss=1.4834131002426147
I0215 20:06:38.339576 139535991158528 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.6289570927619934, loss=1.3872522115707397
I0215 20:08:08.678987 139535982765824 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.6305627226829529, loss=1.4431023597717285
I0215 20:09:36.597750 139535991158528 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.6607507467269897, loss=1.4594556093215942
I0215 20:11:05.060169 139535982765824 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.6174241304397583, loss=1.4408090114593506
I0215 20:12:35.615064 139535991158528 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.6183562874794006, loss=1.4299143552780151
I0215 20:13:53.360303 139535982765824 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.6677865386009216, loss=1.4104892015457153
I0215 20:15:13.866535 139535991158528 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.7475283741950989, loss=1.4550275802612305
I0215 20:16:35.743420 139535982765824 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.6456405520439148, loss=1.466679334640503
I0215 20:18:02.369332 139535991158528 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.5986582040786743, loss=1.4035297632217407
I0215 20:19:08.572696 139646656866112 spec.py:321] Evaluating on the training split.
I0215 20:20:01.697519 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 20:20:55.288743 139646656866112 spec.py:349] Evaluating on the test split.
I0215 20:21:22.805377 139646656866112 submission_runner.py:408] Time since start: 15907.83s, 	Step: 16979, 	{'train/ctc_loss': Array(0.4158837, dtype=float32), 'train/wer': 0.14749664016764993, 'validation/ctc_loss': Array(0.572075, dtype=float32), 'validation/wer': 0.17372582716240093, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36074033, dtype=float32), 'test/wer': 0.12347409258017997, 'test/num_examples': 2472, 'score': 14440.339807987213, 'total_duration': 15907.829645395279, 'accumulated_submission_time': 14440.339807987213, 'accumulated_eval_time': 1466.1310760974884, 'accumulated_logging_time': 0.6140539646148682}
I0215 20:21:22.840853 139535991158528 logging_writer.py:48] [16979] accumulated_eval_time=1466.131076, accumulated_logging_time=0.614054, accumulated_submission_time=14440.339808, global_step=16979, preemption_count=0, score=14440.339808, test/ctc_loss=0.3607403337955475, test/num_examples=2472, test/wer=0.123474, total_duration=15907.829645, train/ctc_loss=0.4158836901187897, train/wer=0.147497, validation/ctc_loss=0.5720750093460083, validation/num_examples=5348, validation/wer=0.173726
I0215 20:21:39.629501 139535982765824 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.5393335819244385, loss=1.4752554893493652
I0215 20:22:55.415064 139535991158528 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.5964779853820801, loss=1.4391605854034424
I0215 20:24:11.910039 139535982765824 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.8223938941955566, loss=1.4412387609481812
I0215 20:25:43.227070 139535991158528 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.7856852412223816, loss=1.3604363203048706
I0215 20:27:11.450526 139535982765824 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.5829436182975769, loss=1.4562643766403198
I0215 20:28:41.109788 139535991158528 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.8202401399612427, loss=1.4614520072937012
I0215 20:30:01.979362 139535991158528 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.7048166990280151, loss=1.4415607452392578
I0215 20:31:18.746712 139535982765824 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.6989746689796448, loss=1.4546732902526855
I0215 20:32:35.861683 139535991158528 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.8646393418312073, loss=1.3923699855804443
I0215 20:34:00.856639 139535982765824 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.6238632798194885, loss=1.4595316648483276
I0215 20:35:29.294556 139535991158528 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.6937502026557922, loss=1.4647350311279297
I0215 20:36:58.042083 139535982765824 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.7831233739852905, loss=1.4280462265014648
I0215 20:38:26.021563 139535991158528 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.8805501461029053, loss=1.4289604425430298
I0215 20:39:57.375682 139535982765824 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.6965489387512207, loss=1.416511058807373
I0215 20:41:27.255957 139535991158528 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.7234277725219727, loss=1.4685280323028564
I0215 20:42:56.250778 139535982765824 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.6533744931221008, loss=1.403537392616272
I0215 20:44:22.266052 139535991158528 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.5815396308898926, loss=1.3253223896026611
I0215 20:45:23.172664 139646656866112 spec.py:321] Evaluating on the training split.
I0215 20:46:16.764104 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 20:47:09.308452 139646656866112 spec.py:349] Evaluating on the test split.
I0215 20:47:35.890921 139646656866112 submission_runner.py:408] Time since start: 17480.91s, 	Step: 18678, 	{'train/ctc_loss': Array(0.39677075, dtype=float32), 'train/wer': 0.14022870301025028, 'validation/ctc_loss': Array(0.55821544, dtype=float32), 'validation/wer': 0.16983500197920387, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3428115, dtype=float32), 'test/wer': 0.11595880811650722, 'test/num_examples': 2472, 'score': 15880.586597919464, 'total_duration': 17480.91488957405, 'accumulated_submission_time': 15880.586597919464, 'accumulated_eval_time': 1598.8436088562012, 'accumulated_logging_time': 0.6656575202941895}
I0215 20:47:35.926944 139535991158528 logging_writer.py:48] [18678] accumulated_eval_time=1598.843609, accumulated_logging_time=0.665658, accumulated_submission_time=15880.586598, global_step=18678, preemption_count=0, score=15880.586598, test/ctc_loss=0.3428114950656891, test/num_examples=2472, test/wer=0.115959, total_duration=17480.914890, train/ctc_loss=0.39677074551582336, train/wer=0.140229, validation/ctc_loss=0.5582154393196106, validation/num_examples=5348, validation/wer=0.169835
I0215 20:47:53.324852 139535982765824 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.7047321200370789, loss=1.3955525159835815
I0215 20:49:09.086622 139535991158528 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.6887987852096558, loss=1.3970867395401
I0215 20:50:24.942280 139535982765824 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.6551441550254822, loss=1.3565503358840942
I0215 20:51:41.878802 139535991158528 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.7292318940162659, loss=1.4089792966842651
I0215 20:53:10.394787 139535982765824 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.8151728510856628, loss=1.4594424962997437
I0215 20:54:38.678930 139535991158528 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.5988373160362244, loss=1.4267226457595825
I0215 20:56:07.282142 139535982765824 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.6203703880310059, loss=1.373408317565918
I0215 20:57:38.057309 139535991158528 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.5910172462463379, loss=1.453546166419983
I0215 20:59:07.776305 139535982765824 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.7304297685623169, loss=1.3890132904052734
I0215 21:00:34.108172 139535991158528 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.661613941192627, loss=1.3652087450027466
I0215 21:01:50.919934 139535982765824 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.5557644367218018, loss=1.370079517364502
I0215 21:03:07.460676 139535991158528 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.7545497417449951, loss=1.3898358345031738
I0215 21:04:24.525467 139535982765824 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.695677638053894, loss=1.3808777332305908
I0215 21:05:47.095842 139535991158528 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.6879813075065613, loss=1.342802882194519
I0215 21:07:17.124288 139535982765824 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.759907603263855, loss=1.3826627731323242
I0215 21:08:43.851519 139535991158528 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.6327962875366211, loss=1.3413254022598267
I0215 21:10:13.371231 139535982765824 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.5970581769943237, loss=1.4365912675857544
I0215 21:11:36.510807 139646656866112 spec.py:321] Evaluating on the training split.
I0215 21:12:42.232438 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 21:13:34.603254 139646656866112 spec.py:349] Evaluating on the test split.
I0215 21:14:02.682673 139646656866112 submission_runner.py:408] Time since start: 19067.71s, 	Step: 20395, 	{'train/ctc_loss': Array(0.26701897, dtype=float32), 'train/wer': 0.09736891410829475, 'validation/ctc_loss': Array(0.53861165, dtype=float32), 'validation/wer': 0.16398428222481826, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3305695, dtype=float32), 'test/wer': 0.11165275323461905, 'test/num_examples': 2472, 'score': 17321.084460258484, 'total_duration': 19067.706463098526, 'accumulated_submission_time': 17321.084460258484, 'accumulated_eval_time': 1745.0095345973969, 'accumulated_logging_time': 0.7188072204589844}
I0215 21:14:02.720628 139535991158528 logging_writer.py:48] [20395] accumulated_eval_time=1745.009535, accumulated_logging_time=0.718807, accumulated_submission_time=17321.084460, global_step=20395, preemption_count=0, score=17321.084460, test/ctc_loss=0.3305695056915283, test/num_examples=2472, test/wer=0.111653, total_duration=19067.706463, train/ctc_loss=0.26701897382736206, train/wer=0.097369, validation/ctc_loss=0.538611650466919, validation/num_examples=5348, validation/wer=0.163984
I0215 21:14:07.388198 139535982765824 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.7400009036064148, loss=1.4209553003311157
I0215 21:15:23.131435 139535991158528 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.6950380802154541, loss=1.3531385660171509
I0215 21:16:42.585660 139535991158528 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.6577782034873962, loss=1.3218425512313843
I0215 21:18:00.245918 139535982765824 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.5575299263000488, loss=1.3482779264450073
I0215 21:19:16.598862 139535991158528 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.6775738000869751, loss=1.3702526092529297
I0215 21:20:38.765818 139535982765824 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.7011546492576599, loss=1.4317582845687866
I0215 21:22:02.924454 139535991158528 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.7137290835380554, loss=1.3884856700897217
I0215 21:23:31.173322 139535982765824 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.5709400177001953, loss=1.3298258781433105
I0215 21:24:58.338360 139535991158528 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.7171503901481628, loss=1.3494373559951782
I0215 21:26:25.211553 139535982765824 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.7660911083221436, loss=1.4032998085021973
I0215 21:27:55.773900 139535991158528 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.7031409740447998, loss=1.3882970809936523
I0215 21:29:26.884847 139535982765824 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.7075198292732239, loss=1.3820749521255493
I0215 21:30:54.530760 139535991158528 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.6276934742927551, loss=1.346165657043457
I0215 21:32:17.453243 139535991158528 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.6500349640846252, loss=1.3758591413497925
I0215 21:33:36.023199 139535982765824 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.7054725885391235, loss=1.3987021446228027
I0215 21:34:54.235646 139535991158528 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.6192654371261597, loss=1.3529678583145142
I0215 21:36:13.024745 139535982765824 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.6901060938835144, loss=1.3609960079193115
I0215 21:37:41.271314 139535991158528 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.7958335876464844, loss=1.4227938652038574
I0215 21:38:03.263568 139646656866112 spec.py:321] Evaluating on the training split.
I0215 21:38:58.886636 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 21:39:54.141971 139646656866112 spec.py:349] Evaluating on the test split.
I0215 21:40:21.554000 139646656866112 submission_runner.py:408] Time since start: 20646.58s, 	Step: 22125, 	{'train/ctc_loss': Array(0.25013727, dtype=float32), 'train/wer': 0.09088889577715095, 'validation/ctc_loss': Array(0.5297374, dtype=float32), 'validation/wer': 0.1605472257354432, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32201567, dtype=float32), 'test/wer': 0.10935754473625414, 'test/num_examples': 2472, 'score': 18761.54176592827, 'total_duration': 20646.577522277832, 'accumulated_submission_time': 18761.54176592827, 'accumulated_eval_time': 1883.2937757968903, 'accumulated_logging_time': 0.7721741199493408}
I0215 21:40:21.593879 139535991158528 logging_writer.py:48] [22125] accumulated_eval_time=1883.293776, accumulated_logging_time=0.772174, accumulated_submission_time=18761.541766, global_step=22125, preemption_count=0, score=18761.541766, test/ctc_loss=0.3220156729221344, test/num_examples=2472, test/wer=0.109358, total_duration=20646.577522, train/ctc_loss=0.2501372694969177, train/wer=0.090889, validation/ctc_loss=0.5297374129295349, validation/num_examples=5348, validation/wer=0.160547
I0215 21:41:19.321160 139535982765824 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.6609087586402893, loss=1.3450958728790283
I0215 21:42:35.282510 139535991158528 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.8073411583900452, loss=1.3432350158691406
I0215 21:43:56.224564 139535982765824 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.709683895111084, loss=1.4094600677490234
I0215 21:45:25.375033 139535991158528 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.7381148934364319, loss=1.354404330253601
I0215 21:46:51.281896 139535982765824 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.6102702021598816, loss=1.381707787513733
I0215 21:48:20.964643 139535991158528 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.6678850650787354, loss=1.3264731168746948
I0215 21:49:38.504325 139535982765824 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.6181120276451111, loss=1.3629262447357178
I0215 21:50:55.170822 139535991158528 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.72052001953125, loss=1.377870798110962
I0215 21:52:17.035566 139535982765824 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.7527909874916077, loss=1.4673833847045898
I0215 21:53:42.792324 139535991158528 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.7061305046081543, loss=1.318967342376709
I0215 21:55:10.091407 139535982765824 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.6655501127243042, loss=1.351110816001892
I0215 21:56:37.466022 139535991158528 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.6210246682167053, loss=1.3318986892700195
I0215 21:58:06.136448 139535982765824 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.8454889059066772, loss=1.2951139211654663
I0215 21:59:33.739344 139535991158528 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.6538089513778687, loss=1.3268342018127441
I0215 22:01:02.723146 139535982765824 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.6688059568405151, loss=1.3429217338562012
I0215 22:02:32.981701 139535991158528 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.7422300577163696, loss=1.3166509866714478
I0215 22:03:50.139436 139535982765824 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.6945260763168335, loss=1.3413833379745483
I0215 22:04:22.213819 139646656866112 spec.py:321] Evaluating on the training split.
I0215 22:05:19.908903 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 22:06:14.073249 139646656866112 spec.py:349] Evaluating on the test split.
I0215 22:06:42.107270 139646656866112 submission_runner.py:408] Time since start: 22227.13s, 	Step: 23843, 	{'train/ctc_loss': Array(0.24616796, dtype=float32), 'train/wer': 0.09029913923198671, 'validation/ctc_loss': Array(0.5182299, dtype=float32), 'validation/wer': 0.15712947855218823, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31246486, dtype=float32), 'test/wer': 0.1053561635488392, 'test/num_examples': 2472, 'score': 20202.07715177536, 'total_duration': 22227.13118314743, 'accumulated_submission_time': 20202.07715177536, 'accumulated_eval_time': 2023.181410074234, 'accumulated_logging_time': 0.8276913166046143}
I0215 22:06:42.142538 139535991158528 logging_writer.py:48] [23843] accumulated_eval_time=2023.181410, accumulated_logging_time=0.827691, accumulated_submission_time=20202.077152, global_step=23843, preemption_count=0, score=20202.077152, test/ctc_loss=0.3124648630619049, test/num_examples=2472, test/wer=0.105356, total_duration=22227.131183, train/ctc_loss=0.24616795778274536, train/wer=0.090299, validation/ctc_loss=0.5182299017906189, validation/num_examples=5348, validation/wer=0.157129
I0215 22:07:26.337468 139535982765824 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.7037926912307739, loss=1.281345009803772
I0215 22:08:42.343695 139535991158528 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.6543660163879395, loss=1.2846652269363403
I0215 22:09:58.299788 139535982765824 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.582182765007019, loss=1.3091404438018799
I0215 22:11:21.605688 139535991158528 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.7408484816551208, loss=1.4057557582855225
I0215 22:12:45.963399 139535982765824 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.764769434928894, loss=1.3876086473464966
I0215 22:14:16.010490 139535991158528 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.7816639542579651, loss=1.3350563049316406
I0215 22:15:46.453018 139535982765824 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.6583998203277588, loss=1.319122076034546
I0215 22:17:10.070960 139535991158528 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.6697754859924316, loss=1.3366636037826538
I0215 22:18:30.405407 139535982765824 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.657261848449707, loss=1.3462690114974976
I0215 22:19:50.337596 139535991158528 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.5511575937271118, loss=1.32656991481781
I0215 22:21:06.664864 139535982765824 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.660374104976654, loss=1.2940760850906372
I0215 22:22:22.814591 139535991158528 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.6341050267219543, loss=1.3016717433929443
I0215 22:23:38.823036 139535982765824 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.5710696578025818, loss=1.2673496007919312
I0215 22:24:54.985584 139535991158528 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.6399009823799133, loss=1.3546710014343262
I0215 22:26:10.966762 139535982765824 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.7066547274589539, loss=1.3478078842163086
I0215 22:27:26.930087 139535991158528 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.5926883220672607, loss=1.2735342979431152
I0215 22:28:42.946997 139535982765824 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.6195167303085327, loss=1.3444373607635498
I0215 22:29:58.983356 139535991158528 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.6361381411552429, loss=1.3831084966659546
I0215 22:30:42.688260 139646656866112 spec.py:321] Evaluating on the training split.
I0215 22:31:36.344102 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 22:32:26.085324 139646656866112 spec.py:349] Evaluating on the test split.
I0215 22:32:51.298708 139646656866112 submission_runner.py:408] Time since start: 23796.32s, 	Step: 25659, 	{'train/ctc_loss': Array(0.23114382, dtype=float32), 'train/wer': 0.08571623236410138, 'validation/ctc_loss': Array(0.5024098, dtype=float32), 'validation/wer': 0.15192562055282544, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30169642, dtype=float32), 'test/wer': 0.10139540552068735, 'test/num_examples': 2472, 'score': 21642.539937257767, 'total_duration': 23796.32274031639, 'accumulated_submission_time': 21642.539937257767, 'accumulated_eval_time': 2151.7861466407776, 'accumulated_logging_time': 0.880361795425415}
I0215 22:32:51.336870 139535991158528 logging_writer.py:48] [25659] accumulated_eval_time=2151.786147, accumulated_logging_time=0.880362, accumulated_submission_time=21642.539937, global_step=25659, preemption_count=0, score=21642.539937, test/ctc_loss=0.30169641971588135, test/num_examples=2472, test/wer=0.101395, total_duration=23796.322740, train/ctc_loss=0.23114381730556488, train/wer=0.085716, validation/ctc_loss=0.502409815788269, validation/num_examples=5348, validation/wer=0.151926
I0215 22:33:23.152798 139535982765824 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.6962107419967651, loss=1.2868179082870483
I0215 22:34:42.244975 139535991158528 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.6841632723808289, loss=1.2969906330108643
I0215 22:35:58.131924 139535982765824 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.6407851576805115, loss=1.3293814659118652
I0215 22:37:14.352487 139535991158528 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.6644642353057861, loss=1.296951413154602
I0215 22:38:30.330861 139535982765824 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.6377802491188049, loss=1.2850897312164307
I0215 22:39:46.384587 139535991158528 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.8450198769569397, loss=1.3663959503173828
I0215 22:41:02.409307 139535982765824 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.7107546925544739, loss=1.3164653778076172
I0215 22:42:18.527262 139535991158528 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.6516604423522949, loss=1.2922757863998413
I0215 22:43:34.594916 139535982765824 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.9406848549842834, loss=1.305469274520874
I0215 22:44:50.533806 139535991158528 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.7667030096054077, loss=1.2833583354949951
I0215 22:46:11.258413 139535982765824 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.5993785858154297, loss=1.3151909112930298
I0215 22:47:33.236475 139535991158528 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.5578880906105042, loss=1.2572407722473145
I0215 22:48:49.354792 139535982765824 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.7125419974327087, loss=1.3095637559890747
I0215 22:50:05.537334 139535991158528 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.6324024200439453, loss=1.254611611366272
I0215 22:51:21.629575 139535982765824 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.8970673084259033, loss=1.2970675230026245
I0215 22:52:38.002450 139535991158528 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.5787924528121948, loss=1.2712962627410889
I0215 22:53:54.206575 139535982765824 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.7501266598701477, loss=1.2905395030975342
I0215 22:55:10.463665 139535991158528 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.9794682860374451, loss=1.3072086572647095
I0215 22:56:26.658560 139535982765824 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.7221000790596008, loss=1.2762279510498047
I0215 22:56:51.441543 139646656866112 spec.py:321] Evaluating on the training split.
I0215 22:57:44.549020 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 22:58:34.907985 139646656866112 spec.py:349] Evaluating on the test split.
I0215 22:58:59.998944 139646656866112 submission_runner.py:408] Time since start: 25365.02s, 	Step: 27534, 	{'train/ctc_loss': Array(0.22649139, dtype=float32), 'train/wer': 0.08500884842540671, 'validation/ctc_loss': Array(0.49297327, dtype=float32), 'validation/wer': 0.150226401614258, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29015937, dtype=float32), 'test/wer': 0.09918144334084862, 'test/num_examples': 2472, 'score': 23082.5641746521, 'total_duration': 25365.021073818207, 'accumulated_submission_time': 23082.5641746521, 'accumulated_eval_time': 2280.3359375, 'accumulated_logging_time': 0.9355659484863281}
I0215 22:59:00.038503 139535991158528 logging_writer.py:48] [27534] accumulated_eval_time=2280.335938, accumulated_logging_time=0.935566, accumulated_submission_time=23082.564175, global_step=27534, preemption_count=0, score=23082.564175, test/ctc_loss=0.2901593744754791, test/num_examples=2472, test/wer=0.099181, total_duration=25365.021074, train/ctc_loss=0.22649139165878296, train/wer=0.085009, validation/ctc_loss=0.492973268032074, validation/num_examples=5348, validation/wer=0.150226
I0215 22:59:50.823729 139535982765824 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.6140020489692688, loss=1.2677167654037476
I0215 23:01:06.804629 139535991158528 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.6931194067001343, loss=1.2945300340652466
I0215 23:02:22.786020 139535982765824 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.5929701328277588, loss=1.2960222959518433
I0215 23:03:42.060102 139535991158528 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.8612044453620911, loss=1.2703806161880493
I0215 23:04:58.017392 139535982765824 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.6629316806793213, loss=1.250036358833313
I0215 23:06:14.083686 139535991158528 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.7613834738731384, loss=1.3599035739898682
I0215 23:07:30.108713 139535982765824 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.6740894913673401, loss=1.3584657907485962
I0215 23:08:46.456683 139535991158528 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.8697841763496399, loss=1.3224570751190186
I0215 23:10:02.495284 139535982765824 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.651515007019043, loss=1.3285053968429565
I0215 23:11:18.570111 139535991158528 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.7691136002540588, loss=1.3216822147369385
I0215 23:12:34.571757 139535982765824 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.645258903503418, loss=1.3106752634048462
I0215 23:13:50.749705 139535991158528 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.7310134768486023, loss=1.2959712743759155
I0215 23:15:09.741409 139535982765824 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.65891033411026, loss=1.291995644569397
I0215 23:16:30.326724 139535991158528 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.5890865325927734, loss=1.2228552103042603
I0215 23:17:46.348823 139535982765824 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.6720991134643555, loss=1.2941265106201172
I0215 23:19:02.399795 139535991158528 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.6521589159965515, loss=1.2838654518127441
I0215 23:20:18.512328 139535982765824 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.6808582544326782, loss=1.3164148330688477
I0215 23:21:34.631395 139535991158528 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.6265951991081238, loss=1.2858961820602417
I0215 23:22:50.976045 139535982765824 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.5847771167755127, loss=1.25688898563385
I0215 23:23:00.555012 139646656866112 spec.py:321] Evaluating on the training split.
I0215 23:23:53.781445 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 23:24:43.884653 139646656866112 spec.py:349] Evaluating on the test split.
I0215 23:25:09.779562 139646656866112 submission_runner.py:408] Time since start: 26934.80s, 	Step: 29414, 	{'train/ctc_loss': Array(0.20505889, dtype=float32), 'train/wer': 0.07636058815156133, 'validation/ctc_loss': Array(0.4759323, dtype=float32), 'validation/wer': 0.14279231875802542, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28078052, dtype=float32), 'test/wer': 0.09320983892917352, 'test/num_examples': 2472, 'score': 24523.00218605995, 'total_duration': 26934.803947925568, 'accumulated_submission_time': 24523.00218605995, 'accumulated_eval_time': 2409.5551302433014, 'accumulated_logging_time': 0.9896988868713379}
I0215 23:25:09.815597 139535991158528 logging_writer.py:48] [29414] accumulated_eval_time=2409.555130, accumulated_logging_time=0.989699, accumulated_submission_time=24523.002186, global_step=29414, preemption_count=0, score=24523.002186, test/ctc_loss=0.28078052401542664, test/num_examples=2472, test/wer=0.093210, total_duration=26934.803948, train/ctc_loss=0.20505888760089874, train/wer=0.076361, validation/ctc_loss=0.4759323000907898, validation/num_examples=5348, validation/wer=0.142792
I0215 23:26:15.772516 139535982765824 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.7013559341430664, loss=1.326535701751709
I0215 23:27:31.827779 139535991158528 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.7192511558532715, loss=1.287450909614563
I0215 23:28:47.927311 139535982765824 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.7214335203170776, loss=1.2462300062179565
I0215 23:30:04.248927 139535991158528 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.701420783996582, loss=1.31777822971344
I0215 23:31:23.742257 139535991158528 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.6478815674781799, loss=1.2388442754745483
I0215 23:32:39.750164 139535982765824 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.642708420753479, loss=1.274548888206482
I0215 23:33:55.730299 139535991158528 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.7079259157180786, loss=1.3106592893600464
I0215 23:35:11.764045 139535982765824 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.7189380526542664, loss=1.278128981590271
I0215 23:36:27.912487 139535991158528 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.7514758706092834, loss=1.3116830587387085
I0215 23:37:43.893232 139535982765824 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.6111921072006226, loss=1.28062105178833
I0215 23:38:59.976760 139535991158528 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.6516865491867065, loss=1.3277074098587036
I0215 23:40:16.290067 139535982765824 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.6204156279563904, loss=1.2878915071487427
I0215 23:41:32.280264 139535991158528 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.7193186283111572, loss=1.2722618579864502
I0215 23:42:51.491152 139535982765824 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.6920467615127563, loss=1.3137294054031372
I0215 23:44:14.718943 139535991158528 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.7075037360191345, loss=1.2603542804718018
I0215 23:45:30.579775 139535982765824 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.6546605229377747, loss=1.240456223487854
I0215 23:46:46.537050 139535991158528 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.6690621972084045, loss=1.2729233503341675
I0215 23:48:02.611335 139535982765824 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.7141404747962952, loss=1.305492877960205
I0215 23:49:09.826341 139646656866112 spec.py:321] Evaluating on the training split.
I0215 23:50:02.067621 139646656866112 spec.py:333] Evaluating on the validation split.
I0215 23:50:51.713537 139646656866112 spec.py:349] Evaluating on the test split.
I0215 23:51:16.769717 139646656866112 submission_runner.py:408] Time since start: 28501.79s, 	Step: 31290, 	{'train/ctc_loss': Array(0.24940795, dtype=float32), 'train/wer': 0.08863719162402736, 'validation/ctc_loss': Array(0.47313604, dtype=float32), 'validation/wer': 0.14384467594157005, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2828514, dtype=float32), 'test/wer': 0.09511912741453903, 'test/num_examples': 2472, 'score': 25962.93151783943, 'total_duration': 28501.7939991951, 'accumulated_submission_time': 25962.93151783943, 'accumulated_eval_time': 2536.493052005768, 'accumulated_logging_time': 1.041748285293579}
I0215 23:51:16.811380 139535991158528 logging_writer.py:48] [31290] accumulated_eval_time=2536.493052, accumulated_logging_time=1.041748, accumulated_submission_time=25962.931518, global_step=31290, preemption_count=0, score=25962.931518, test/ctc_loss=0.2828513979911804, test/num_examples=2472, test/wer=0.095119, total_duration=28501.793999, train/ctc_loss=0.24940794706344604, train/wer=0.088637, validation/ctc_loss=0.4731360375881195, validation/num_examples=5348, validation/wer=0.143845
I0215 23:51:25.236204 139535982765824 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.7157776951789856, loss=1.2947630882263184
I0215 23:52:41.166954 139535991158528 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.8390583395957947, loss=1.2594581842422485
I0215 23:53:57.187194 139535982765824 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.6385849714279175, loss=1.2068113088607788
I0215 23:55:13.197875 139535991158528 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.6919556856155396, loss=1.2668496370315552
I0215 23:56:29.513005 139535982765824 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.6722348928451538, loss=1.2863337993621826
I0215 23:57:45.504908 139535991158528 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.6266588568687439, loss=1.268467903137207
I0215 23:59:01.638185 139535982765824 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.603853166103363, loss=1.281947135925293
I0216 00:00:20.868396 139535991158528 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.6537635326385498, loss=1.225081443786621
I0216 00:01:36.927261 139535982765824 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.7937047481536865, loss=1.2390395402908325
I0216 00:02:52.932746 139535991158528 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.776817798614502, loss=1.2712864875793457
I0216 00:04:08.879772 139535982765824 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.5742006897926331, loss=1.2020680904388428
I0216 00:05:24.982283 139535991158528 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.5190185904502869, loss=1.21127188205719
I0216 00:06:41.033510 139535982765824 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.6791406869888306, loss=1.237081527709961
I0216 00:07:57.064579 139535991158528 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.6317014098167419, loss=1.273221492767334
I0216 00:09:13.053973 139535982765824 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.7122877836227417, loss=1.2500191926956177
I0216 00:10:29.217341 139535991158528 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.6531299948692322, loss=1.293017029762268
I0216 00:11:49.311102 139535982765824 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.6013259887695312, loss=1.2461519241333008
I0216 00:13:10.546163 139535991158528 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.721332848072052, loss=1.245998501777649
I0216 00:14:26.420770 139535982765824 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.7930845022201538, loss=1.300242304801941
I0216 00:15:16.969012 139646656866112 spec.py:321] Evaluating on the training split.
I0216 00:16:09.947113 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 00:16:59.297707 139646656866112 spec.py:349] Evaluating on the test split.
I0216 00:17:24.568520 139646656866112 submission_runner.py:408] Time since start: 30069.59s, 	Step: 33168, 	{'train/ctc_loss': Array(0.20455477, dtype=float32), 'train/wer': 0.07657436579432052, 'validation/ctc_loss': Array(0.46298337, dtype=float32), 'validation/wer': 0.13969317512575186, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26978076, dtype=float32), 'test/wer': 0.09087400727154551, 'test/num_examples': 2472, 'score': 27403.008170366287, 'total_duration': 30069.59238386154, 'accumulated_submission_time': 27403.008170366287, 'accumulated_eval_time': 2664.0866878032684, 'accumulated_logging_time': 1.0995814800262451}
I0216 00:17:24.605700 139535991158528 logging_writer.py:48] [33168] accumulated_eval_time=2664.086688, accumulated_logging_time=1.099581, accumulated_submission_time=27403.008170, global_step=33168, preemption_count=0, score=27403.008170, test/ctc_loss=0.2697807550430298, test/num_examples=2472, test/wer=0.090874, total_duration=30069.592384, train/ctc_loss=0.20455476641654968, train/wer=0.076574, validation/ctc_loss=0.4629833698272705, validation/num_examples=5348, validation/wer=0.139693
I0216 00:17:49.614249 139535982765824 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.7096338272094727, loss=1.1806203126907349
I0216 00:19:05.539226 139535991158528 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.700854480266571, loss=1.2647758722305298
I0216 00:20:21.539676 139535982765824 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.6684431433677673, loss=1.268589973449707
I0216 00:21:37.475597 139535991158528 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.6218902468681335, loss=1.2989931106567383
I0216 00:22:53.496861 139535982765824 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.7127593755722046, loss=1.2760089635849
I0216 00:24:09.494163 139535991158528 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.7304514050483704, loss=1.3045473098754883
I0216 00:25:25.541429 139535982765824 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.7046941518783569, loss=1.31636643409729
I0216 00:26:41.505163 139535991158528 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.6878707408905029, loss=1.246226191520691
I0216 00:28:02.668691 139535991158528 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.7308012843132019, loss=1.299599528312683
I0216 00:29:18.681129 139535982765824 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.8058815002441406, loss=1.2463833093643188
I0216 00:30:34.757704 139535991158528 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.6832108497619629, loss=1.1773171424865723
I0216 00:31:50.992712 139535982765824 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.6141526699066162, loss=1.2071675062179565
I0216 00:33:07.122977 139535991158528 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.7385443449020386, loss=1.2436308860778809
I0216 00:34:23.288203 139535982765824 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.7618009448051453, loss=1.2297841310501099
I0216 00:35:39.433662 139535991158528 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.8350381851196289, loss=1.2865768671035767
I0216 00:36:55.545206 139535982765824 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.7102624177932739, loss=1.2063555717468262
I0216 00:38:12.206057 139535991158528 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.6342452168464661, loss=1.2327141761779785
I0216 00:39:33.079084 139535982765824 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.6670668125152588, loss=1.2250787019729614
I0216 00:40:53.026726 139535991158528 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.7667670845985413, loss=1.26487398147583
I0216 00:41:25.247689 139646656866112 spec.py:321] Evaluating on the training split.
I0216 00:42:19.348752 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 00:43:09.538542 139646656866112 spec.py:349] Evaluating on the test split.
I0216 00:43:34.727287 139646656866112 submission_runner.py:408] Time since start: 31639.75s, 	Step: 35039, 	{'train/ctc_loss': Array(0.18673988, dtype=float32), 'train/wer': 0.07078146556924707, 'validation/ctc_loss': Array(0.44775584, dtype=float32), 'validation/wer': 0.13400658447338695, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26430643, dtype=float32), 'test/wer': 0.08847724087502284, 'test/num_examples': 2472, 'score': 28843.570236682892, 'total_duration': 31639.75147986412, 'accumulated_submission_time': 28843.570236682892, 'accumulated_eval_time': 2793.5609545707703, 'accumulated_logging_time': 1.1512136459350586}
I0216 00:43:34.763329 139535991158528 logging_writer.py:48] [35039] accumulated_eval_time=2793.560955, accumulated_logging_time=1.151214, accumulated_submission_time=28843.570237, global_step=35039, preemption_count=0, score=28843.570237, test/ctc_loss=0.2643064260482788, test/num_examples=2472, test/wer=0.088477, total_duration=31639.751480, train/ctc_loss=0.18673987686634064, train/wer=0.070781, validation/ctc_loss=0.4477558434009552, validation/num_examples=5348, validation/wer=0.134007
I0216 00:44:21.980464 139535982765824 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.6480618119239807, loss=1.2563344240188599
I0216 00:45:37.747655 139535991158528 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.6502941846847534, loss=1.219300627708435
I0216 00:46:53.783770 139535982765824 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.7519407868385315, loss=1.2228883504867554
I0216 00:48:09.802094 139535991158528 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.6891856789588928, loss=1.2921892404556274
I0216 00:49:25.826069 139535982765824 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.6926771402359009, loss=1.2043876647949219
I0216 00:50:41.924244 139535991158528 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.7368220090866089, loss=1.267517328262329
I0216 00:51:57.929024 139535982765824 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.6718229055404663, loss=1.1804401874542236
I0216 00:53:13.941837 139535991158528 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.698235034942627, loss=1.1950268745422363
I0216 00:54:30.045503 139535982765824 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.6291090250015259, loss=1.1904652118682861
I0216 00:55:46.373974 139535991158528 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.6688010096549988, loss=1.2469156980514526
I0216 00:57:07.671624 139535991158528 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.6698595881462097, loss=1.2107497453689575
I0216 00:58:23.671810 139535982765824 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.633261501789093, loss=1.223894715309143
I0216 00:59:39.978952 139535991158528 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.6873795986175537, loss=1.2011715173721313
I0216 01:00:56.094669 139535982765824 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.7423699498176575, loss=1.2288388013839722
I0216 01:02:12.268109 139535991158528 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.662718653678894, loss=1.224414348602295
I0216 01:03:28.367191 139535982765824 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.6235089898109436, loss=1.226243257522583
I0216 01:04:44.464123 139535991158528 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.7298347353935242, loss=1.2342954874038696
I0216 01:06:00.477969 139535982765824 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.6176459789276123, loss=1.1690442562103271
I0216 01:07:19.257922 139535991158528 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.731971025466919, loss=1.1840980052947998
I0216 01:07:35.469279 139646656866112 spec.py:321] Evaluating on the training split.
I0216 01:08:29.058080 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 01:09:19.013883 139646656866112 spec.py:349] Evaluating on the test split.
I0216 01:09:44.354131 139646656866112 submission_runner.py:408] Time since start: 33209.38s, 	Step: 36922, 	{'train/ctc_loss': Array(0.17774187, dtype=float32), 'train/wer': 0.06597390171950993, 'validation/ctc_loss': Array(0.44245794, dtype=float32), 'validation/wer': 0.1318246328818174, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25393718, dtype=float32), 'test/wer': 0.0839680701968192, 'test/num_examples': 2472, 'score': 30284.19582605362, 'total_duration': 33209.37820768356, 'accumulated_submission_time': 30284.19582605362, 'accumulated_eval_time': 2922.4401302337646, 'accumulated_logging_time': 1.2019224166870117}
I0216 01:09:44.395051 139535991158528 logging_writer.py:48] [36922] accumulated_eval_time=2922.440130, accumulated_logging_time=1.201922, accumulated_submission_time=30284.195826, global_step=36922, preemption_count=0, score=30284.195826, test/ctc_loss=0.2539371848106384, test/num_examples=2472, test/wer=0.083968, total_duration=33209.378208, train/ctc_loss=0.1777418702840805, train/wer=0.065974, validation/ctc_loss=0.4424579441547394, validation/num_examples=5348, validation/wer=0.131825
I0216 01:10:44.311309 139535982765824 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.6982783675193787, loss=1.2512236833572388
I0216 01:12:03.664877 139535991158528 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.6367335915565491, loss=1.19973623752594
I0216 01:13:19.663627 139535982765824 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.7380319237709045, loss=1.2537480592727661
I0216 01:14:35.658592 139535991158528 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.7301570773124695, loss=1.2628121376037598
I0216 01:15:52.094438 139535982765824 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.6815397143363953, loss=1.1905235052108765
I0216 01:17:08.276846 139535991158528 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.696853518486023, loss=1.2035671472549438
I0216 01:18:24.349785 139535982765824 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.6378319263458252, loss=1.1440318822860718
I0216 01:19:40.578271 139535991158528 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.7719327807426453, loss=1.1892057657241821
I0216 01:20:56.730141 139535982765824 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.8077152967453003, loss=1.1960307359695435
I0216 01:22:13.824480 139535991158528 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.6845904588699341, loss=1.2345290184020996
I0216 01:23:33.981397 139535982765824 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.8332292437553406, loss=1.2691118717193604
I0216 01:24:54.358297 139535991158528 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.8285446166992188, loss=1.257095456123352
I0216 01:26:13.660641 139535991158528 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.6982384920120239, loss=1.2086970806121826
I0216 01:27:29.821961 139535982765824 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.8223462700843811, loss=1.2355787754058838
I0216 01:28:46.004085 139535991158528 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.7735013365745544, loss=1.2422176599502563
I0216 01:30:02.163170 139535982765824 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.7732205986976624, loss=1.2311311960220337
I0216 01:31:18.484229 139535991158528 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.8258214592933655, loss=1.1973119974136353
I0216 01:32:34.653205 139535982765824 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.6716095805168152, loss=1.2137798070907593
I0216 01:33:45.048496 139646656866112 spec.py:321] Evaluating on the training split.
I0216 01:34:37.906002 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 01:35:28.187205 139646656866112 spec.py:349] Evaluating on the test split.
I0216 01:35:53.492685 139646656866112 submission_runner.py:408] Time since start: 34778.52s, 	Step: 38794, 	{'train/ctc_loss': Array(0.16964507, dtype=float32), 'train/wer': 0.06578919035314384, 'validation/ctc_loss': Array(0.43418646, dtype=float32), 'validation/wer': 0.1291116753719455, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24846429, dtype=float32), 'test/wer': 0.08337903438750431, 'test/num_examples': 2472, 'score': 31724.767939329147, 'total_duration': 34778.51620674133, 'accumulated_submission_time': 31724.767939329147, 'accumulated_eval_time': 3050.8781111240387, 'accumulated_logging_time': 1.257685661315918}
I0216 01:35:53.529513 139535991158528 logging_writer.py:48] [38794] accumulated_eval_time=3050.878111, accumulated_logging_time=1.257686, accumulated_submission_time=31724.767939, global_step=38794, preemption_count=0, score=31724.767939, test/ctc_loss=0.24846428632736206, test/num_examples=2472, test/wer=0.083379, total_duration=34778.516207, train/ctc_loss=0.16964507102966309, train/wer=0.065789, validation/ctc_loss=0.4341864585876465, validation/num_examples=5348, validation/wer=0.129112
I0216 01:35:58.901985 139535982765824 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.7339001297950745, loss=1.2601909637451172
I0216 01:37:14.752097 139535991158528 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.6887958645820618, loss=1.1737143993377686
I0216 01:38:30.702186 139535982765824 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.6727731227874756, loss=1.2263683080673218
I0216 01:39:46.675979 139535991158528 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.6561453342437744, loss=1.1721203327178955
I0216 01:41:05.876868 139535991158528 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.743423581123352, loss=1.2675893306732178
I0216 01:42:21.978951 139535982765824 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.6866369843482971, loss=1.2350651025772095
I0216 01:43:38.176864 139535991158528 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.696399986743927, loss=1.2061361074447632
I0216 01:44:54.221133 139535982765824 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.8411998152732849, loss=1.1634554862976074
I0216 01:46:10.322376 139535991158528 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.1357206106185913, loss=1.1828968524932861
I0216 01:47:26.646962 139535982765824 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.698186457157135, loss=1.193681001663208
I0216 01:48:42.790828 139535991158528 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.7318955063819885, loss=1.217807412147522
I0216 01:49:58.828970 139535982765824 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.7547628879547119, loss=1.2219716310501099
I0216 01:51:18.680198 139535991158528 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.820799708366394, loss=1.2406566143035889
I0216 01:52:39.143225 139535982765824 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.6392998099327087, loss=1.1285030841827393
I0216 01:54:00.894418 139535991158528 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.7393149733543396, loss=1.132643699645996
I0216 01:55:16.778707 139535982765824 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.6931353807449341, loss=1.183415412902832
I0216 01:56:32.767755 139535991158528 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.8864481449127197, loss=1.204324722290039
I0216 01:57:48.845606 139535982765824 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.640863299369812, loss=1.144506573677063
I0216 01:59:04.794248 139535991158528 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.0790354013442993, loss=1.1377061605453491
I0216 01:59:53.808939 139646656866112 spec.py:321] Evaluating on the training split.
I0216 02:00:46.944573 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 02:01:37.234666 139646656866112 spec.py:349] Evaluating on the test split.
I0216 02:02:03.424755 139646656866112 submission_runner.py:408] Time since start: 36348.45s, 	Step: 40666, 	{'train/ctc_loss': Array(0.17991582, dtype=float32), 'train/wer': 0.06738385587444276, 'validation/ctc_loss': Array(0.4272925, dtype=float32), 'validation/wer': 0.12789518908638017, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24406633, dtype=float32), 'test/wer': 0.08258688278187394, 'test/num_examples': 2472, 'score': 33164.96550655365, 'total_duration': 36348.4490673542, 'accumulated_submission_time': 33164.96550655365, 'accumulated_eval_time': 3180.488514661789, 'accumulated_logging_time': 1.3088583946228027}
I0216 02:02:03.466344 139535991158528 logging_writer.py:48] [40666] accumulated_eval_time=3180.488515, accumulated_logging_time=1.308858, accumulated_submission_time=33164.965507, global_step=40666, preemption_count=0, score=33164.965507, test/ctc_loss=0.24406632781028748, test/num_examples=2472, test/wer=0.082587, total_duration=36348.449067, train/ctc_loss=0.17991581559181213, train/wer=0.067384, validation/ctc_loss=0.42729249596595764, validation/num_examples=5348, validation/wer=0.127895
I0216 02:02:29.970885 139535982765824 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.751246452331543, loss=1.192905068397522
I0216 02:03:45.851350 139535991158528 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.7343214750289917, loss=1.2070375680923462
I0216 02:05:02.003491 139535982765824 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.7345369458198547, loss=1.2486355304718018
I0216 02:06:18.028705 139535991158528 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.7959285378456116, loss=1.227859377861023
I0216 02:07:34.020554 139535982765824 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.7231988310813904, loss=1.2426636219024658
I0216 02:08:53.344931 139535991158528 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.6745697855949402, loss=1.1827523708343506
I0216 02:10:09.461185 139535982765824 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.6845741271972656, loss=1.1794220209121704
I0216 02:11:25.609697 139535991158528 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.6905844211578369, loss=1.1577750444412231
I0216 02:12:41.787268 139535982765824 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.6674520373344421, loss=1.1579575538635254
I0216 02:13:57.902689 139535991158528 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.8356824517250061, loss=1.2382625341415405
I0216 02:15:14.007638 139535982765824 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.9103654623031616, loss=1.21039879322052
I0216 02:16:30.119319 139535991158528 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.8535659313201904, loss=1.1324445009231567
I0216 02:17:46.230764 139535982765824 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.6669129729270935, loss=1.178228497505188
I0216 02:19:03.210520 139535991158528 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.761821448802948, loss=1.1476538181304932
I0216 02:20:23.987247 139535982765824 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.7157598733901978, loss=1.207222819328308
I0216 02:21:44.711302 139535991158528 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.7088237404823303, loss=1.2029064893722534
I0216 02:23:05.261413 139535991158528 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.6903734803199768, loss=1.1913790702819824
I0216 02:24:21.307010 139535982765824 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.7818971276283264, loss=1.1666134595870972
I0216 02:25:37.266009 139535991158528 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.6829867959022522, loss=1.2063955068588257
I0216 02:26:03.526111 139646656866112 spec.py:321] Evaluating on the training split.
I0216 02:26:56.369617 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 02:27:46.892654 139646656866112 spec.py:349] Evaluating on the test split.
I0216 02:28:12.339257 139646656866112 submission_runner.py:408] Time since start: 37917.36s, 	Step: 42536, 	{'train/ctc_loss': Array(0.17591874, dtype=float32), 'train/wer': 0.06519827499960508, 'validation/ctc_loss': Array(0.42120722, dtype=float32), 'validation/wer': 0.12393678133176285, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24332765, dtype=float32), 'test/wer': 0.08041354376129832, 'test/num_examples': 2472, 'score': 34604.94077825546, 'total_duration': 37917.36299943924, 'accumulated_submission_time': 34604.94077825546, 'accumulated_eval_time': 3309.295679807663, 'accumulated_logging_time': 1.3665189743041992}
I0216 02:28:12.380493 139535991158528 logging_writer.py:48] [42536] accumulated_eval_time=3309.295680, accumulated_logging_time=1.366519, accumulated_submission_time=34604.940778, global_step=42536, preemption_count=0, score=34604.940778, test/ctc_loss=0.24332764744758606, test/num_examples=2472, test/wer=0.080414, total_duration=37917.362999, train/ctc_loss=0.17591874301433563, train/wer=0.065198, validation/ctc_loss=0.4212072193622589, validation/num_examples=5348, validation/wer=0.123937
I0216 02:29:01.604472 139535982765824 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.7837042808532715, loss=1.1752725839614868
I0216 02:30:17.487867 139535991158528 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.7473614811897278, loss=1.1322822570800781
I0216 02:31:33.631608 139535982765824 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.8413916230201721, loss=1.2012618780136108
I0216 02:32:49.667085 139535991158528 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.7717090845108032, loss=1.1679939031600952
I0216 02:34:05.852103 139535982765824 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.7251273989677429, loss=1.126496434211731
I0216 02:35:21.946182 139535991158528 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.093828558921814, loss=1.1666207313537598
I0216 02:36:40.132080 139535982765824 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.8917962908744812, loss=1.1357487440109253
I0216 02:38:02.177638 139535991158528 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.6780526041984558, loss=1.1780682802200317
I0216 02:39:18.207517 139535982765824 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.6684244871139526, loss=1.1770552396774292
I0216 02:40:34.305759 139535991158528 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.7817320823669434, loss=1.1553542613983154
I0216 02:41:50.416863 139535982765824 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.7174248099327087, loss=1.1549866199493408
I0216 02:43:06.596873 139535991158528 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.7221888899803162, loss=1.1429280042648315
I0216 02:44:22.754325 139535982765824 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.7934198975563049, loss=1.1760843992233276
I0216 02:45:38.831857 139535991158528 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.7823893427848816, loss=1.162445306777954
I0216 02:46:54.866325 139535982765824 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.903624951839447, loss=1.1763290166854858
I0216 02:48:14.781607 139535991158528 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.7960050106048584, loss=1.2181754112243652
I0216 02:49:35.808828 139535982765824 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.7760202288627625, loss=1.0992774963378906
I0216 02:50:59.018041 139535991158528 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.7871674299240112, loss=1.1628549098968506
I0216 02:52:12.382595 139646656866112 spec.py:321] Evaluating on the training split.
I0216 02:53:06.621420 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 02:53:57.044435 139646656866112 spec.py:349] Evaluating on the test split.
I0216 02:54:22.421440 139646656866112 submission_runner.py:408] Time since start: 39487.45s, 	Step: 44398, 	{'train/ctc_loss': Array(0.1598213, dtype=float32), 'train/wer': 0.060765059054097494, 'validation/ctc_loss': Array(0.4153037, dtype=float32), 'validation/wer': 0.1243615860664047, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23402373, dtype=float32), 'test/wer': 0.07771210367030244, 'test/num_examples': 2472, 'score': 36044.85700464249, 'total_duration': 39487.44536948204, 'accumulated_submission_time': 36044.85700464249, 'accumulated_eval_time': 3439.3287382125854, 'accumulated_logging_time': 1.4259233474731445}
I0216 02:54:22.459331 139535991158528 logging_writer.py:48] [44398] accumulated_eval_time=3439.328738, accumulated_logging_time=1.425923, accumulated_submission_time=36044.857005, global_step=44398, preemption_count=0, score=36044.857005, test/ctc_loss=0.23402373492717743, test/num_examples=2472, test/wer=0.077712, total_duration=39487.445369, train/ctc_loss=0.1598213016986847, train/wer=0.060765, validation/ctc_loss=0.41530370712280273, validation/num_examples=5348, validation/wer=0.124362
I0216 02:54:24.831269 139535982765824 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.6700384020805359, loss=1.1737979650497437
I0216 02:55:40.589789 139535991158528 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.9017268419265747, loss=1.1299397945404053
I0216 02:56:56.537775 139535982765824 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.8019010424613953, loss=1.1602591276168823
I0216 02:58:12.696240 139535991158528 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.7940263152122498, loss=1.182229995727539
I0216 02:59:28.739427 139535982765824 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.7995195984840393, loss=1.1838696002960205
I0216 03:00:44.891594 139535991158528 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.7994406223297119, loss=1.16082763671875
I0216 03:02:01.074490 139535982765824 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.6883268356323242, loss=1.1419577598571777
I0216 03:03:17.258036 139535991158528 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.776877224445343, loss=1.191511631011963
I0216 03:04:34.938228 139535982765824 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.7373601198196411, loss=1.1799296140670776
I0216 03:05:56.048892 139535991158528 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.8136827349662781, loss=1.1558834314346313
I0216 03:07:16.195071 139535991158528 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.7770931720733643, loss=1.1313451528549194
I0216 03:08:32.181839 139535982765824 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.8518889546394348, loss=1.0930874347686768
I0216 03:09:48.246358 139535991158528 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.9040870070457458, loss=1.1145594120025635
I0216 03:11:04.353681 139535982765824 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.7466495633125305, loss=1.1409755945205688
I0216 03:12:20.392839 139535991158528 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.9019361734390259, loss=1.1537342071533203
I0216 03:13:36.387075 139535982765824 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.8888932466506958, loss=1.1688910722732544
I0216 03:14:53.236355 139535991158528 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.8390382528305054, loss=1.147413730621338
I0216 03:16:14.876655 139535982765824 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.2533210515975952, loss=1.1240057945251465
I0216 03:17:36.368982 139535991158528 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.8486714363098145, loss=1.125913381576538
I0216 03:18:22.839991 139646656866112 spec.py:321] Evaluating on the training split.
I0216 03:19:17.191237 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 03:20:07.863917 139646656866112 spec.py:349] Evaluating on the test split.
I0216 03:20:33.223228 139646656866112 submission_runner.py:408] Time since start: 41058.25s, 	Step: 46259, 	{'train/ctc_loss': Array(0.15301697, dtype=float32), 'train/wer': 0.05895605283880826, 'validation/ctc_loss': Array(0.40488496, dtype=float32), 'validation/wer': 0.12075074582194889, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22412066, dtype=float32), 'test/wer': 0.07618873519793634, 'test/num_examples': 2472, 'score': 37485.154686927795, 'total_duration': 41058.24700117111, 'accumulated_submission_time': 37485.154686927795, 'accumulated_eval_time': 3569.7060022354126, 'accumulated_logging_time': 1.4790055751800537}
I0216 03:20:33.263027 139535991158528 logging_writer.py:48] [46259] accumulated_eval_time=3569.706002, accumulated_logging_time=1.479006, accumulated_submission_time=37485.154687, global_step=46259, preemption_count=0, score=37485.154687, test/ctc_loss=0.22412066161632538, test/num_examples=2472, test/wer=0.076189, total_duration=41058.247001, train/ctc_loss=0.15301696956157684, train/wer=0.058956, validation/ctc_loss=0.4048849642276764, validation/num_examples=5348, validation/wer=0.120751
I0216 03:21:05.076673 139535982765824 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.674449622631073, loss=1.0945641994476318
I0216 03:22:24.324122 139535991158528 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.845790684223175, loss=1.161062479019165
I0216 03:23:40.206033 139535982765824 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.7700271010398865, loss=1.1488114595413208
I0216 03:24:56.327625 139535991158528 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.7575427889823914, loss=1.2191107273101807
I0216 03:26:12.354343 139535982765824 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.7310183644294739, loss=1.0859858989715576
I0216 03:27:28.318866 139535991158528 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.8568288683891296, loss=1.143746256828308
I0216 03:28:44.417674 139535982765824 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.8344764709472656, loss=1.135787010192871
I0216 03:30:00.556378 139535991158528 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.8624994158744812, loss=1.2073100805282593
I0216 03:31:20.182698 139535982765824 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.7419462203979492, loss=1.1929051876068115
I0216 03:32:41.054187 139535991158528 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.8197152018547058, loss=1.1180871725082397
I0216 03:34:01.505857 139535982765824 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.7581027150154114, loss=1.1659951210021973
I0216 03:35:24.950593 139535991158528 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.8602352738380432, loss=1.1035560369491577
I0216 03:36:40.860055 139535982765824 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.7360924482345581, loss=1.1119722127914429
I0216 03:37:56.890523 139535991158528 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.7946654558181763, loss=1.1143994331359863
I0216 03:39:13.176661 139535982765824 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.7910406589508057, loss=1.1207911968231201
I0216 03:40:29.262269 139535991158528 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.7240883708000183, loss=1.131547212600708
I0216 03:41:45.259779 139535982765824 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.9956945776939392, loss=1.1143145561218262
I0216 03:43:01.177242 139535991158528 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.8376078605651855, loss=1.1126576662063599
I0216 03:44:18.917801 139535982765824 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.835087776184082, loss=1.1705734729766846
I0216 03:44:33.473539 139646656866112 spec.py:321] Evaluating on the training split.
I0216 03:45:27.669211 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 03:46:18.405638 139646656866112 spec.py:349] Evaluating on the test split.
I0216 03:46:43.871624 139646656866112 submission_runner.py:408] Time since start: 42628.90s, 	Step: 48120, 	{'train/ctc_loss': Array(0.13564813, dtype=float32), 'train/wer': 0.05087395046733907, 'validation/ctc_loss': Array(0.39561796, dtype=float32), 'validation/wer': 0.11742954516929434, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21684836, dtype=float32), 'test/wer': 0.07255296244388926, 'test/num_examples': 2472, 'score': 38925.27982378006, 'total_duration': 42628.89527773857, 'accumulated_submission_time': 38925.27982378006, 'accumulated_eval_time': 3700.0979936122894, 'accumulated_logging_time': 1.5350711345672607}
I0216 03:46:43.912132 139535991158528 logging_writer.py:48] [48120] accumulated_eval_time=3700.097994, accumulated_logging_time=1.535071, accumulated_submission_time=38925.279824, global_step=48120, preemption_count=0, score=38925.279824, test/ctc_loss=0.21684835851192474, test/num_examples=2472, test/wer=0.072553, total_duration=42628.895278, train/ctc_loss=0.13564813137054443, train/wer=0.050874, validation/ctc_loss=0.3956179618835449, validation/num_examples=5348, validation/wer=0.117430
I0216 03:47:45.293721 139535982765824 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.976768970489502, loss=1.084127426147461
I0216 03:49:01.317445 139535991158528 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.7603247761726379, loss=1.1095067262649536
I0216 03:50:17.379228 139535982765824 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.6928938031196594, loss=1.0840805768966675
I0216 03:51:36.734191 139535991158528 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.0426768064498901, loss=1.1354416608810425
I0216 03:52:52.758925 139535982765824 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.6846993565559387, loss=1.1020756959915161
I0216 03:54:08.840084 139535991158528 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.8473246097564697, loss=1.1079448461532593
I0216 03:55:24.861642 139535982765824 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.78505939245224, loss=1.0890276432037354
I0216 03:56:41.113863 139535991158528 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.7679810523986816, loss=1.1604644060134888
I0216 03:57:57.155513 139535982765824 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.8195077180862427, loss=1.0521312952041626
I0216 03:59:13.102627 139535991158528 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.964751124382019, loss=1.134890079498291
I0216 04:00:32.129682 139535982765824 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.7823062539100647, loss=1.1202987432479858
I0216 04:01:53.490121 139535991158528 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.7281623482704163, loss=1.1206470727920532
I0216 04:03:14.282235 139535982765824 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.7946369647979736, loss=1.0849837064743042
I0216 04:04:35.782526 139535991158528 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.73870450258255, loss=1.1749860048294067
I0216 04:05:51.779763 139535982765824 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.8808117508888245, loss=1.105636715888977
I0216 04:07:08.005513 139535991158528 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.8866866230964661, loss=1.061898946762085
I0216 04:08:24.029225 139535982765824 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.8931707739830017, loss=1.0939981937408447
I0216 04:09:39.917838 139535991158528 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.768182635307312, loss=1.1281458139419556
I0216 04:10:44.037726 139646656866112 spec.py:321] Evaluating on the training split.
I0216 04:11:39.896303 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 04:12:31.634993 139646656866112 spec.py:349] Evaluating on the test split.
I0216 04:12:57.597765 139646656866112 submission_runner.py:408] Time since start: 44202.62s, 	Step: 49986, 	{'train/ctc_loss': Array(0.13670734, dtype=float32), 'train/wer': 0.05112016917466058, 'validation/ctc_loss': Array(0.38398, dtype=float32), 'validation/wer': 0.11543103198586559, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21637918, dtype=float32), 'test/wer': 0.07174049925862734, 'test/num_examples': 2472, 'score': 40365.31974339485, 'total_duration': 44202.62142920494, 'accumulated_submission_time': 40365.31974339485, 'accumulated_eval_time': 3833.651977300644, 'accumulated_logging_time': 1.5921378135681152}
I0216 04:12:57.637778 139535991158528 logging_writer.py:48] [49986] accumulated_eval_time=3833.651977, accumulated_logging_time=1.592138, accumulated_submission_time=40365.319743, global_step=49986, preemption_count=0, score=40365.319743, test/ctc_loss=0.21637918055057526, test/num_examples=2472, test/wer=0.071740, total_duration=44202.621429, train/ctc_loss=0.1367073357105255, train/wer=0.051120, validation/ctc_loss=0.38398000597953796, validation/num_examples=5348, validation/wer=0.115431
I0216 04:13:09.243928 139535982765824 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.7605559825897217, loss=1.1011625528335571
I0216 04:14:25.131600 139535991158528 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.8186327219009399, loss=1.1572142839431763
I0216 04:15:41.095852 139535982765824 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.9263151288032532, loss=1.1501123905181885
I0216 04:16:57.162351 139535991158528 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.7712527513504028, loss=1.1215764284133911
I0216 04:18:13.294884 139535982765824 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.8218098282814026, loss=1.1250227689743042
I0216 04:19:32.731787 139535991158528 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.8525407314300537, loss=1.0976817607879639
I0216 04:20:48.745392 139535982765824 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.9530452489852905, loss=1.0927815437316895
I0216 04:22:04.821382 139535991158528 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.7572388648986816, loss=1.101484775543213
I0216 04:23:20.989458 139535982765824 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.8666033744812012, loss=1.1950955390930176
I0216 04:24:36.976097 139535991158528 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.8116938471794128, loss=1.078493595123291
I0216 04:25:53.146858 139535982765824 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.8305637240409851, loss=1.0841467380523682
I0216 04:27:09.192816 139535991158528 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.8651099801063538, loss=1.1061341762542725
I0216 04:28:27.158130 139535982765824 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.7977677583694458, loss=1.0979079008102417
I0216 04:29:48.647873 139535991158528 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.8263810873031616, loss=1.0834366083145142
I0216 04:31:09.677488 139535982765824 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.8326282501220703, loss=1.1145423650741577
I0216 04:32:34.609838 139535991158528 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.96653151512146, loss=1.0618305206298828
I0216 04:33:50.556568 139535982765824 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.8798803091049194, loss=1.158046841621399
I0216 04:35:06.568407 139535991158528 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.7727317214012146, loss=1.0698578357696533
I0216 04:36:22.617460 139535982765824 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.8617956638336182, loss=1.0602803230285645
I0216 04:36:57.994877 139646656866112 spec.py:321] Evaluating on the training split.
I0216 04:37:50.771396 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 04:38:41.186277 139646656866112 spec.py:349] Evaluating on the test split.
I0216 04:39:06.865217 139646656866112 submission_runner.py:408] Time since start: 45771.89s, 	Step: 51848, 	{'train/ctc_loss': Array(0.13435693, dtype=float32), 'train/wer': 0.05087923070689609, 'validation/ctc_loss': Array(0.37354812, dtype=float32), 'validation/wer': 0.10994718904776157, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21018149, dtype=float32), 'test/wer': 0.06954684865842016, 'test/num_examples': 2472, 'score': 41805.59288787842, 'total_duration': 45771.889213085175, 'accumulated_submission_time': 41805.59288787842, 'accumulated_eval_time': 3962.5165877342224, 'accumulated_logging_time': 1.646233320236206}
I0216 04:39:06.905235 139535991158528 logging_writer.py:48] [51848] accumulated_eval_time=3962.516588, accumulated_logging_time=1.646233, accumulated_submission_time=41805.592888, global_step=51848, preemption_count=0, score=41805.592888, test/ctc_loss=0.21018148958683014, test/num_examples=2472, test/wer=0.069547, total_duration=45771.889213, train/ctc_loss=0.13435693085193634, train/wer=0.050879, validation/ctc_loss=0.37354812026023865, validation/num_examples=5348, validation/wer=0.109947
I0216 04:39:47.139990 139535982765824 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.2669607400894165, loss=1.0586507320404053
I0216 04:41:03.421753 139535991158528 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.8240802884101868, loss=1.1183357238769531
I0216 04:42:19.716714 139535982765824 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.8803980350494385, loss=1.0961840152740479
I0216 04:43:36.110624 139535991158528 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.8549243211746216, loss=1.1133538484573364
I0216 04:44:52.490322 139535982765824 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.8343645930290222, loss=1.082728385925293
I0216 04:46:08.742245 139535991158528 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.7716172933578491, loss=1.078262448310852
I0216 04:47:27.598723 139535982765824 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.7515450716018677, loss=1.0663574934005737
I0216 04:48:48.078633 139535991158528 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.7960290312767029, loss=1.0605465173721313
I0216 04:50:04.201620 139535982765824 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.8607617616653442, loss=1.0766968727111816
I0216 04:51:20.107666 139535991158528 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.8145794868469238, loss=1.0466541051864624
I0216 04:52:36.231295 139535982765824 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.8878991007804871, loss=1.0751076936721802
I0216 04:53:52.275448 139535991158528 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.9427045583724976, loss=1.0453135967254639
I0216 04:55:08.298072 139535982765824 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.9260753393173218, loss=1.0544681549072266
I0216 04:56:24.486927 139535991158528 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.8481429815292358, loss=1.0329056978225708
I0216 04:57:44.011093 139535982765824 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.9464232921600342, loss=1.065548300743103
I0216 04:59:06.067424 139535991158528 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.9287192225456238, loss=1.1046881675720215
I0216 05:00:27.396822 139535982765824 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.8343110680580139, loss=1.099198341369629
I0216 05:01:49.693394 139535991158528 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.8577853441238403, loss=1.0618115663528442
I0216 05:03:05.671320 139535982765824 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.0190937519073486, loss=1.045820951461792
I0216 05:03:06.922863 139646656866112 spec.py:321] Evaluating on the training split.
I0216 05:03:59.725524 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 05:04:50.471572 139646656866112 spec.py:349] Evaluating on the test split.
I0216 05:05:16.489999 139646656866112 submission_runner.py:408] Time since start: 47341.51s, 	Step: 53703, 	{'train/ctc_loss': Array(0.1186261, dtype=float32), 'train/wer': 0.044654267796189744, 'validation/ctc_loss': Array(0.36797065, dtype=float32), 'validation/wer': 0.10780385606843218, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20350519, dtype=float32), 'test/wer': 0.06775942965084394, 'test/num_examples': 2472, 'score': 43245.52307033539, 'total_duration': 47341.51375102997, 'accumulated_submission_time': 43245.52307033539, 'accumulated_eval_time': 4092.0777394771576, 'accumulated_logging_time': 1.704374074935913}
I0216 05:05:16.528844 139535991158528 logging_writer.py:48] [53703] accumulated_eval_time=4092.077739, accumulated_logging_time=1.704374, accumulated_submission_time=43245.523070, global_step=53703, preemption_count=0, score=43245.523070, test/ctc_loss=0.20350518822669983, test/num_examples=2472, test/wer=0.067759, total_duration=47341.513751, train/ctc_loss=0.11862610280513763, train/wer=0.044654, validation/ctc_loss=0.36797064542770386, validation/num_examples=5348, validation/wer=0.107804
I0216 05:06:30.851716 139535982765824 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.8272871375083923, loss=1.0170072317123413
I0216 05:07:46.711963 139535991158528 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.9791693687438965, loss=1.1262553930282593
I0216 05:09:02.785662 139535982765824 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.8645288944244385, loss=1.110098958015442
I0216 05:10:18.806645 139535991158528 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.9137815833091736, loss=1.0716813802719116
I0216 05:11:34.889799 139535982765824 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.0219004154205322, loss=1.0852100849151611
I0216 05:12:50.945359 139535991158528 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.9583194851875305, loss=1.0624181032180786
I0216 05:14:07.008446 139535982765824 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.9146891236305237, loss=1.0579280853271484
I0216 05:15:27.278959 139535991158528 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.8518329858779907, loss=1.1375499963760376
I0216 05:16:51.240342 139535991158528 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.8857905268669128, loss=1.096182942390442
I0216 05:18:07.101491 139535982765824 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.8548325300216675, loss=1.0914210081100464
I0216 05:19:23.036415 139535991158528 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.9739787578582764, loss=1.0698341131210327
I0216 05:20:38.995754 139535982765824 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.8699609041213989, loss=1.1166938543319702
I0216 05:21:54.987496 139535991158528 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.9282275438308716, loss=1.0681648254394531
I0216 05:23:11.096304 139535982765824 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.9675531983375549, loss=1.0502318143844604
I0216 05:24:27.213094 139535991158528 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.0318843126296997, loss=1.0353606939315796
I0216 05:25:43.339077 139535982765824 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.7722485661506653, loss=1.0188877582550049
I0216 05:27:05.536897 139535991158528 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.7524043321609497, loss=0.9992951154708862
I0216 05:28:26.966652 139535982765824 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.8391271233558655, loss=1.0234493017196655
I0216 05:29:16.786705 139646656866112 spec.py:321] Evaluating on the training split.
I0216 05:30:11.602779 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 05:31:02.177754 139646656866112 spec.py:349] Evaluating on the test split.
I0216 05:31:27.922149 139646656866112 submission_runner.py:408] Time since start: 48912.95s, 	Step: 55563, 	{'train/ctc_loss': Array(0.11973902, dtype=float32), 'train/wer': 0.0472273716241127, 'validation/ctc_loss': Array(0.36250752, dtype=float32), 'validation/wer': 0.10651013255838651, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19598241, dtype=float32), 'test/wer': 0.06489549692279568, 'test/num_examples': 2472, 'score': 44685.69279670715, 'total_duration': 48912.9459066391, 'accumulated_submission_time': 44685.69279670715, 'accumulated_eval_time': 4223.207216978073, 'accumulated_logging_time': 1.7623958587646484}
I0216 05:31:27.961940 139535991158528 logging_writer.py:48] [55563] accumulated_eval_time=4223.207217, accumulated_logging_time=1.762396, accumulated_submission_time=44685.692797, global_step=55563, preemption_count=0, score=44685.692797, test/ctc_loss=0.1959824115037918, test/num_examples=2472, test/wer=0.064895, total_duration=48912.945907, train/ctc_loss=0.11973901838064194, train/wer=0.047227, validation/ctc_loss=0.36250752210617065, validation/num_examples=5348, validation/wer=0.106510
I0216 05:31:56.732205 139535982765824 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.9119576811790466, loss=1.0204298496246338
I0216 05:33:15.886116 139535991158528 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.0049173831939697, loss=1.0630664825439453
I0216 05:34:32.095833 139535982765824 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.8009220957756042, loss=1.0207219123840332
I0216 05:35:48.118736 139535991158528 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.905495285987854, loss=1.0190520286560059
I0216 05:37:04.237016 139535982765824 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.9920226335525513, loss=1.0684301853179932
I0216 05:38:20.338928 139535991158528 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.8378444314002991, loss=1.0173532962799072
I0216 05:39:36.351660 139535982765824 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.9845685362815857, loss=1.0399127006530762
I0216 05:40:57.048650 139535991158528 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.0424025058746338, loss=1.0012763738632202
I0216 05:42:18.042990 139535982765824 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.8354047536849976, loss=1.0241327285766602
I0216 05:43:39.110306 139535991158528 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.8546739816665649, loss=1.0200293064117432
I0216 05:45:00.729400 139535982765824 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.299857258796692, loss=1.0216697454452515
I0216 05:46:22.260462 139535991158528 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.9044252038002014, loss=1.0268144607543945
I0216 05:47:38.270600 139535982765824 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.828496515750885, loss=0.9822657704353333
I0216 05:48:54.565178 139535991158528 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.8358606100082397, loss=1.0120189189910889
I0216 05:50:10.781789 139535982765824 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.93253093957901, loss=1.0123790502548218
I0216 05:51:26.823951 139535991158528 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.0681341886520386, loss=1.0491870641708374
I0216 05:52:42.873784 139535982765824 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.958505392074585, loss=0.9800885319709778
I0216 05:53:58.967469 139535991158528 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.8217171430587769, loss=1.0317808389663696
I0216 05:55:19.897297 139535982765824 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.1020804643630981, loss=1.011120319366455
I0216 05:55:28.227792 139646656866112 spec.py:321] Evaluating on the training split.
I0216 05:56:22.234997 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 05:57:13.460757 139646656866112 spec.py:349] Evaluating on the test split.
I0216 05:57:38.937810 139646656866112 submission_runner.py:408] Time since start: 50483.96s, 	Step: 57412, 	{'train/ctc_loss': Array(0.13343582, dtype=float32), 'train/wer': 0.048016934638889036, 'validation/ctc_loss': Array(0.34958616, dtype=float32), 'validation/wer': 0.10371028317097425, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19371364, dtype=float32), 'test/wer': 0.0641439684764284, 'test/num_examples': 2472, 'score': 46125.87220811844, 'total_duration': 50483.96151971817, 'accumulated_submission_time': 46125.87220811844, 'accumulated_eval_time': 4353.911198377609, 'accumulated_logging_time': 1.8190128803253174}
I0216 05:57:38.977860 139535991158528 logging_writer.py:48] [57412] accumulated_eval_time=4353.911198, accumulated_logging_time=1.819013, accumulated_submission_time=46125.872208, global_step=57412, preemption_count=0, score=46125.872208, test/ctc_loss=0.19371363520622253, test/num_examples=2472, test/wer=0.064144, total_duration=50483.961520, train/ctc_loss=0.13343581557273865, train/wer=0.048017, validation/ctc_loss=0.34958615899086, validation/num_examples=5348, validation/wer=0.103710
I0216 05:58:46.443552 139535982765824 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.9739952087402344, loss=1.0671570301055908
I0216 06:00:02.435079 139535991158528 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.9435901641845703, loss=1.0290677547454834
I0216 06:01:21.677919 139535991158528 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.9204680919647217, loss=1.0050159692764282
I0216 06:02:37.631888 139535982765824 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.9644217491149902, loss=1.0360552072525024
I0216 06:03:53.683179 139535991158528 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.9954883456230164, loss=0.9988480806350708
I0216 06:05:09.774071 139535982765824 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.0074734687805176, loss=1.0109663009643555
I0216 06:06:25.747446 139535991158528 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.0320751667022705, loss=1.026440143585205
I0216 06:07:41.742719 139535982765824 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.878849983215332, loss=1.0041993856430054
I0216 06:08:57.901733 139535991158528 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.8505463004112244, loss=1.0152013301849365
I0216 06:10:18.921685 139535982765824 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.9882147908210754, loss=1.0109437704086304
I0216 06:11:40.414138 139535991158528 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.8730608224868774, loss=1.0176517963409424
I0216 06:13:01.961148 139535982765824 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.0962553024291992, loss=1.0481235980987549
I0216 06:14:22.888969 139535991158528 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.8329691886901855, loss=0.9898536205291748
I0216 06:15:42.514149 139535991158528 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.0935533046722412, loss=1.0280733108520508
I0216 06:16:58.619895 139535982765824 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.1268246173858643, loss=1.0279779434204102
I0216 06:18:14.595391 139535991158528 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.1929500102996826, loss=1.0227183103561401
I0216 06:19:30.679508 139535982765824 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.996342658996582, loss=1.0160975456237793
I0216 06:20:47.100584 139535991158528 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.9328141212463379, loss=1.0514206886291504
I0216 06:21:39.316280 139646656866112 spec.py:321] Evaluating on the training split.
I0216 06:22:34.004897 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 06:23:24.519623 139646656866112 spec.py:349] Evaluating on the test split.
I0216 06:23:50.201156 139646656866112 submission_runner.py:408] Time since start: 52055.23s, 	Step: 59270, 	{'train/ctc_loss': Array(0.08705975, dtype=float32), 'train/wer': 0.03397191080448442, 'validation/ctc_loss': Array(0.34289178, dtype=float32), 'validation/wer': 0.10056286627340047, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.184741, dtype=float32), 'test/wer': 0.06197062945585278, 'test/num_examples': 2472, 'score': 47566.122009038925, 'total_duration': 52055.225329875946, 'accumulated_submission_time': 47566.122009038925, 'accumulated_eval_time': 4484.790504455566, 'accumulated_logging_time': 1.8767881393432617}
I0216 06:23:50.244189 139535991158528 logging_writer.py:48] [59270] accumulated_eval_time=4484.790504, accumulated_logging_time=1.876788, accumulated_submission_time=47566.122009, global_step=59270, preemption_count=0, score=47566.122009, test/ctc_loss=0.18474100530147552, test/num_examples=2472, test/wer=0.061971, total_duration=52055.225330, train/ctc_loss=0.08705975115299225, train/wer=0.033972, validation/ctc_loss=0.34289178252220154, validation/num_examples=5348, validation/wer=0.100563
I0216 06:24:13.770690 139535982765824 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.8653508424758911, loss=0.9854122996330261
I0216 06:25:29.790650 139535991158528 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.9000240564346313, loss=1.0072170495986938
I0216 06:26:45.977774 139535982765824 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.9832512736320496, loss=0.9513952732086182
I0216 06:28:02.219351 139535991158528 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.9855430722236633, loss=1.033677577972412
I0216 06:29:18.393009 139535982765824 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.9406070113182068, loss=1.014760971069336
I0216 06:30:37.795764 139535991158528 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.0818215608596802, loss=0.9928024411201477
I0216 06:31:53.795210 139535982765824 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.0112953186035156, loss=0.9681106209754944
I0216 06:33:09.839111 139535991158528 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.8939407467842102, loss=1.0213772058486938
I0216 06:34:25.969547 139535982765824 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.9216882586479187, loss=0.9782865643501282
I0216 06:35:42.050851 139535991158528 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.9432334899902344, loss=0.9482564330101013
I0216 06:36:58.345601 139535982765824 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.0146759748458862, loss=0.9679245352745056
I0216 06:38:14.377144 139535991158528 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.1173053979873657, loss=0.9729353189468384
I0216 06:39:35.077121 139535982765824 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.0894988775253296, loss=0.9892479181289673
I0216 06:40:55.794215 139535991158528 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.9167110919952393, loss=0.9808140993118286
I0216 06:42:18.315722 139535982765824 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.9742898344993591, loss=0.9787735939025879
I0216 06:43:41.181210 139535991158528 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.9642918109893799, loss=0.9460952877998352
I0216 06:44:57.305777 139535982765824 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.0970298051834106, loss=0.9694060683250427
I0216 06:46:13.574761 139535991158528 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.9666674733161926, loss=0.979206383228302
I0216 06:47:29.749143 139535982765824 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.1133437156677246, loss=1.0480351448059082
I0216 06:47:50.811592 139646656866112 spec.py:321] Evaluating on the training split.
I0216 06:48:45.090948 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 06:49:36.250623 139646656866112 spec.py:349] Evaluating on the test split.
I0216 06:50:02.342111 139646656866112 submission_runner.py:408] Time since start: 53627.37s, 	Step: 61129, 	{'train/ctc_loss': Array(0.08885489, dtype=float32), 'train/wer': 0.03415380133993592, 'validation/ctc_loss': Array(0.33372676, dtype=float32), 'validation/wer': 0.0980816204369696, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17703222, dtype=float32), 'test/wer': 0.05900513882964678, 'test/num_examples': 2472, 'score': 49006.60219502449, 'total_duration': 53627.36594891548, 'accumulated_submission_time': 49006.60219502449, 'accumulated_eval_time': 4616.315147399902, 'accumulated_logging_time': 1.937981128692627}
I0216 06:50:02.384184 139535991158528 logging_writer.py:48] [61129] accumulated_eval_time=4616.315147, accumulated_logging_time=1.937981, accumulated_submission_time=49006.602195, global_step=61129, preemption_count=0, score=49006.602195, test/ctc_loss=0.1770322173833847, test/num_examples=2472, test/wer=0.059005, total_duration=53627.365949, train/ctc_loss=0.08885489404201508, train/wer=0.034154, validation/ctc_loss=0.33372676372528076, validation/num_examples=5348, validation/wer=0.098082
I0216 06:50:56.889353 139535982765824 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.2474737167358398, loss=0.9833773374557495
I0216 06:52:12.891472 139535991158528 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.957443118095398, loss=0.928040087223053
I0216 06:53:28.860041 139535982765824 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.967363715171814, loss=0.9483264684677124
I0216 06:54:45.253063 139535991158528 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.0905377864837646, loss=0.9920732378959656
I0216 06:56:01.407449 139535982765824 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.9118737578392029, loss=1.0222115516662598
I0216 06:57:18.572197 139535991158528 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.2391704320907593, loss=0.9644501209259033
I0216 06:58:42.930356 139535991158528 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.1043274402618408, loss=0.942517876625061
I0216 06:59:58.883224 139535982765824 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.07156240940094, loss=1.006056308746338
I0216 07:01:14.895359 139535991158528 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.0995006561279297, loss=0.9918034076690674
I0216 07:02:30.926131 139535982765824 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.1018472909927368, loss=0.9426019787788391
I0216 07:03:46.960454 139535991158528 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.1514931917190552, loss=0.9239185452461243
I0216 07:05:02.958139 139535982765824 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.9733824729919434, loss=0.9408398866653442
I0216 07:06:18.995341 139535991158528 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.0151276588439941, loss=0.9394443035125732
I0216 07:07:39.878294 139535982765824 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.1214559078216553, loss=0.9962428212165833
I0216 07:09:01.314029 139535991158528 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.0179314613342285, loss=0.9487842321395874
I0216 07:10:22.362091 139535982765824 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.0672916173934937, loss=0.9665634632110596
I0216 07:11:43.975051 139535991158528 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.0608948469161987, loss=0.9633688926696777
I0216 07:13:04.851998 139535991158528 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.0187947750091553, loss=0.9537842869758606
I0216 07:14:03.093197 139646656866112 spec.py:321] Evaluating on the training split.
I0216 07:14:56.542172 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 07:15:47.675305 139646656866112 spec.py:349] Evaluating on the test split.
I0216 07:16:14.036035 139646656866112 submission_runner.py:408] Time since start: 55199.06s, 	Step: 62978, 	{'train/ctc_loss': Array(0.10730325, dtype=float32), 'train/wer': 0.04166804184956599, 'validation/ctc_loss': Array(0.33158988, dtype=float32), 'validation/wer': 0.0967106597024436, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17833579, dtype=float32), 'test/wer': 0.05874108829443666, 'test/num_examples': 2472, 'score': 50447.224174022675, 'total_duration': 55199.06018662453, 'accumulated_submission_time': 50447.224174022675, 'accumulated_eval_time': 4747.2524383068085, 'accumulated_logging_time': 1.996875286102295}
I0216 07:16:14.074998 139535991158528 logging_writer.py:48] [62978] accumulated_eval_time=4747.252438, accumulated_logging_time=1.996875, accumulated_submission_time=50447.224174, global_step=62978, preemption_count=0, score=50447.224174, test/ctc_loss=0.1783357858657837, test/num_examples=2472, test/wer=0.058741, total_duration=55199.060187, train/ctc_loss=0.10730325430631638, train/wer=0.041668, validation/ctc_loss=0.33158987760543823, validation/num_examples=5348, validation/wer=0.096711
I0216 07:16:31.548981 139535982765824 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.1519720554351807, loss=0.9463169574737549
I0216 07:17:47.420837 139535991158528 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.1417629718780518, loss=0.9344741106033325
I0216 07:19:03.533170 139535982765824 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.0066955089569092, loss=0.9626098275184631
I0216 07:20:19.577413 139535991158528 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.249575138092041, loss=0.9205326437950134
I0216 07:21:35.680006 139535982765824 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.0728175640106201, loss=0.9920931458473206
I0216 07:22:51.752269 139535991158528 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.2962490320205688, loss=0.9516738653182983
I0216 07:24:07.901920 139535982765824 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.0476791858673096, loss=0.9318857192993164
I0216 07:25:26.477476 139535991158528 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.0523056983947754, loss=0.9201531410217285
I0216 07:26:47.743038 139535982765824 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.1751052141189575, loss=0.9266754388809204
I0216 07:28:10.481466 139535991158528 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.1976909637451172, loss=0.9233489036560059
I0216 07:29:26.565302 139535982765824 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.0630724430084229, loss=0.9527870416641235
I0216 07:30:42.665262 139535991158528 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.0458035469055176, loss=0.9362649917602539
I0216 07:31:58.814003 139535982765824 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.1450093984603882, loss=0.9677793979644775
I0216 07:33:14.859145 139535991158528 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.125653624534607, loss=0.944827675819397
I0216 07:34:30.964890 139535982765824 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.0680856704711914, loss=0.9206505417823792
I0216 07:35:47.150199 139535991158528 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.0689102411270142, loss=0.9763319492340088
I0216 07:37:08.883088 139535982765824 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.0974527597427368, loss=0.9561848640441895
I0216 07:38:31.979188 139535991158528 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.1115835905075073, loss=0.9438037276268005
I0216 07:39:54.109347 139535982765824 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.1877624988555908, loss=0.9177162647247314
I0216 07:40:14.066765 139646656866112 spec.py:321] Evaluating on the training split.
I0216 07:41:07.690025 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 07:41:59.005282 139646656866112 spec.py:349] Evaluating on the test split.
I0216 07:42:24.559700 139646656866112 submission_runner.py:408] Time since start: 56769.58s, 	Step: 64826, 	{'train/ctc_loss': Array(0.10190608, dtype=float32), 'train/wer': 0.03875721573180865, 'validation/ctc_loss': Array(0.31840345, dtype=float32), 'validation/wer': 0.09274259729476621, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16995475, dtype=float32), 'test/wer': 0.056019336623809236, 'test/num_examples': 2472, 'score': 51887.12818932533, 'total_duration': 56769.58395195007, 'accumulated_submission_time': 51887.12818932533, 'accumulated_eval_time': 4877.739913702011, 'accumulated_logging_time': 2.052243232727051}
I0216 07:42:24.599719 139535991158528 logging_writer.py:48] [64826] accumulated_eval_time=4877.739914, accumulated_logging_time=2.052243, accumulated_submission_time=51887.128189, global_step=64826, preemption_count=0, score=51887.128189, test/ctc_loss=0.16995474696159363, test/num_examples=2472, test/wer=0.056019, total_duration=56769.583952, train/ctc_loss=0.10190607607364655, train/wer=0.038757, validation/ctc_loss=0.3184034526348114, validation/num_examples=5348, validation/wer=0.092743
I0216 07:43:25.018766 139535991158528 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.2254033088684082, loss=0.9673161506652832
I0216 07:44:40.837958 139535982765824 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.2004835605621338, loss=0.9269421100616455
I0216 07:45:56.893419 139535991158528 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.0586159229278564, loss=0.908836305141449
I0216 07:47:13.005492 139535982765824 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.032167673110962, loss=0.8934703469276428
I0216 07:48:29.212734 139535991158528 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.3227143287658691, loss=0.986801028251648
I0216 07:49:45.309998 139535982765824 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.9569572806358337, loss=0.9098753929138184
I0216 07:51:01.252847 139535991158528 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.1628735065460205, loss=0.9383091926574707
I0216 07:52:21.674224 139535982765824 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.0731275081634521, loss=0.9200546741485596
I0216 07:53:43.213117 139535991158528 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.2976027727127075, loss=0.8908075094223022
I0216 07:55:04.331791 139535982765824 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.0079374313354492, loss=0.8978127241134644
I0216 07:56:25.856166 139535991158528 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.278917908668518, loss=0.9140205979347229
I0216 07:57:45.606063 139535991158528 logging_writer.py:48] [66000] global_step=66000, grad_norm=2.022264242172241, loss=0.9197799563407898
I0216 07:59:01.956960 139535982765824 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.1494591236114502, loss=0.9255675673484802
I0216 08:00:18.031841 139535991158528 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.1463344097137451, loss=0.9131947755813599
I0216 08:01:34.073672 139535982765824 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.0813541412353516, loss=0.9372724890708923
I0216 08:02:50.281624 139535991158528 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.0656027793884277, loss=0.9525871276855469
I0216 08:04:06.345648 139535982765824 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.1063933372497559, loss=0.9611344337463379
I0216 08:05:26.196940 139535991158528 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.1926130056381226, loss=0.9135751128196716
I0216 08:06:24.866086 139646656866112 spec.py:321] Evaluating on the training split.
I0216 08:07:16.771509 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 08:08:07.748609 139646656866112 spec.py:349] Evaluating on the test split.
I0216 08:08:33.538875 139646656866112 submission_runner.py:408] Time since start: 58338.56s, 	Step: 66672, 	{'train/ctc_loss': Array(0.11785042, dtype=float32), 'train/wer': 0.04573007873284251, 'validation/ctc_loss': Array(0.30807304, dtype=float32), 'validation/wer': 0.08892900933604951, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1668592, dtype=float32), 'test/wer': 0.0543944102532854, 'test/num_examples': 2472, 'score': 53327.30903625488, 'total_duration': 58338.56297969818, 'accumulated_submission_time': 53327.30903625488, 'accumulated_eval_time': 5006.407078027725, 'accumulated_logging_time': 2.1070005893707275}
I0216 08:08:33.580525 139535991158528 logging_writer.py:48] [66672] accumulated_eval_time=5006.407078, accumulated_logging_time=2.107001, accumulated_submission_time=53327.309036, global_step=66672, preemption_count=0, score=53327.309036, test/ctc_loss=0.1668591946363449, test/num_examples=2472, test/wer=0.054394, total_duration=58338.562980, train/ctc_loss=0.1178504228591919, train/wer=0.045730, validation/ctc_loss=0.3080730438232422, validation/num_examples=5348, validation/wer=0.088929
I0216 08:08:55.628368 139535982765824 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.0679450035095215, loss=0.9200781583786011
I0216 08:10:11.708621 139535991158528 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.127195119857788, loss=0.9178224802017212
I0216 08:11:27.816300 139535982765824 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.1316298246383667, loss=0.926732063293457
I0216 08:12:47.350078 139535991158528 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.265855073928833, loss=0.8751218914985657
I0216 08:14:03.389832 139535982765824 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.1856086254119873, loss=0.9369301199913025
I0216 08:15:19.862616 139535991158528 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.1432780027389526, loss=0.9312041401863098
I0216 08:16:35.968832 139535982765824 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.1063108444213867, loss=0.9808908700942993
I0216 08:17:51.981116 139535991158528 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.1810064315795898, loss=0.9391653537750244
I0216 08:19:08.072319 139535982765824 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.345853328704834, loss=0.9389377236366272
I0216 08:20:28.385379 139535991158528 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.3763843774795532, loss=0.8616239428520203
I0216 08:21:49.219452 139535982765824 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.1642569303512573, loss=0.8749896287918091
I0216 08:23:09.915086 139535991158528 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.1132341623306274, loss=0.8864795565605164
I0216 08:24:31.879544 139535982765824 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.105747103691101, loss=0.9378250241279602
I0216 08:25:55.989855 139535991158528 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.0783133506774902, loss=0.9028874635696411
I0216 08:27:11.980280 139535982765824 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.1258735656738281, loss=0.8563966155052185
I0216 08:28:28.066085 139535991158528 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.4677999019622803, loss=0.8915264010429382
I0216 08:29:44.182404 139535982765824 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.9945751428604126, loss=0.8561946153640747
I0216 08:31:00.382376 139535991158528 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.126112937927246, loss=0.8770565986633301
I0216 08:32:16.588585 139535982765824 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.339003562927246, loss=0.8763555884361267
I0216 08:32:33.748565 139646656866112 spec.py:321] Evaluating on the training split.
I0216 08:33:25.336207 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 08:34:16.863847 139646656866112 spec.py:349] Evaluating on the test split.
I0216 08:34:42.692516 139646656866112 submission_runner.py:408] Time since start: 59907.72s, 	Step: 68524, 	{'train/ctc_loss': Array(0.09267107, dtype=float32), 'train/wer': 0.03409489511961275, 'validation/ctc_loss': Array(0.30833793, dtype=float32), 'validation/wer': 0.08829180223408672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1638089, dtype=float32), 'test/wer': 0.05252574492718299, 'test/num_examples': 2472, 'score': 54767.389909267426, 'total_duration': 59907.71535515785, 'accumulated_submission_time': 54767.389909267426, 'accumulated_eval_time': 5135.344121932983, 'accumulated_logging_time': 2.1641712188720703}
I0216 08:34:42.735918 139535991158528 logging_writer.py:48] [68524] accumulated_eval_time=5135.344122, accumulated_logging_time=2.164171, accumulated_submission_time=54767.389909, global_step=68524, preemption_count=0, score=54767.389909, test/ctc_loss=0.1638088971376419, test/num_examples=2472, test/wer=0.052526, total_duration=59907.715355, train/ctc_loss=0.09267107397317886, train/wer=0.034095, validation/ctc_loss=0.308337926864624, validation/num_examples=5348, validation/wer=0.088292
I0216 08:35:41.090531 139535982765824 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.2948493957519531, loss=0.9319779276847839
I0216 08:36:57.254402 139535991158528 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.2723606824874878, loss=0.9001643061637878
I0216 08:38:13.302856 139535982765824 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.2750084400177002, loss=0.8850672245025635
I0216 08:39:29.347682 139535991158528 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.3512001037597656, loss=0.8827934265136719
I0216 08:40:45.408811 139535982765824 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.16064453125, loss=0.9175012707710266
I0216 08:42:04.796699 139535991158528 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.106208086013794, loss=0.8815678358078003
I0216 08:43:21.090754 139535982765824 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.155802845954895, loss=0.8697975277900696
I0216 08:44:37.218585 139535991158528 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.2690629959106445, loss=0.8877807855606079
I0216 08:45:53.490664 139535982765824 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.3759973049163818, loss=0.8831236362457275
I0216 08:47:09.831303 139535991158528 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.1513378620147705, loss=0.8657584190368652
I0216 08:48:25.927587 139535982765824 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.3198109865188599, loss=0.9105979204177856
I0216 08:49:47.378606 139535991158528 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.374258041381836, loss=0.8999183177947998
I0216 08:51:09.164322 139535982765824 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.0593234300613403, loss=0.8578190207481384
I0216 08:52:30.940092 139535991158528 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.0415390729904175, loss=0.8645543456077576
I0216 08:53:53.122002 139535982765824 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.9986474514007568, loss=0.9148316383361816
I0216 08:55:14.363910 139535991158528 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.624472737312317, loss=0.905494213104248
I0216 08:56:30.356833 139535982765824 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.1297553777694702, loss=0.9097046852111816
I0216 08:57:46.368710 139535991158528 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.2852822542190552, loss=0.892503559589386
I0216 08:58:43.031805 139646656866112 spec.py:321] Evaluating on the training split.
I0216 08:59:34.698861 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 09:00:25.467758 139646656866112 spec.py:349] Evaluating on the test split.
I0216 09:00:51.268473 139646656866112 submission_runner.py:408] Time since start: 61476.29s, 	Step: 70376, 	{'train/ctc_loss': Array(0.08907641, dtype=float32), 'train/wer': 0.034411574026683045, 'validation/ctc_loss': Array(0.30346766, dtype=float32), 'validation/wer': 0.08658292864245923, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1597395, dtype=float32), 'test/wer': 0.05165234700302643, 'test/num_examples': 2472, 'score': 56207.60083270073, 'total_duration': 61476.292707920074, 'accumulated_submission_time': 56207.60083270073, 'accumulated_eval_time': 5263.575304508209, 'accumulated_logging_time': 2.2221508026123047}
I0216 09:00:51.309849 139535991158528 logging_writer.py:48] [70376] accumulated_eval_time=5263.575305, accumulated_logging_time=2.222151, accumulated_submission_time=56207.600833, global_step=70376, preemption_count=0, score=56207.600833, test/ctc_loss=0.15973949432373047, test/num_examples=2472, test/wer=0.051652, total_duration=61476.292708, train/ctc_loss=0.08907640725374222, train/wer=0.034412, validation/ctc_loss=0.30346766114234924, validation/num_examples=5348, validation/wer=0.086583
I0216 09:01:10.377928 139535982765824 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.1181044578552246, loss=0.8804675936698914
I0216 09:02:26.471803 139535991158528 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.1155288219451904, loss=0.8972316980361938
I0216 09:03:42.797653 139535982765824 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.3951389789581299, loss=0.8421903252601624
I0216 09:04:59.049634 139535991158528 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.163803219795227, loss=0.8540772199630737
I0216 09:06:15.424551 139535982765824 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.0703891515731812, loss=0.8837581872940063
I0216 09:07:31.632906 139535991158528 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.134661316871643, loss=0.9244050979614258
I0216 09:08:53.064720 139535982765824 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.4208621978759766, loss=0.8908787965774536
I0216 09:10:16.549529 139535991158528 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.5084691047668457, loss=0.8718908429145813
I0216 09:11:32.544853 139535982765824 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.3791630268096924, loss=0.8623955249786377
I0216 09:12:48.556201 139535991158528 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.0931116342544556, loss=0.8595032691955566
I0216 09:14:04.630922 139535982765824 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.0288069248199463, loss=0.8760572671890259
I0216 09:15:20.758907 139535991158528 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.3846802711486816, loss=0.8569241762161255
I0216 09:16:36.877257 139535982765824 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.233680009841919, loss=0.8879693150520325
I0216 09:17:55.190840 139535991158528 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.1900634765625, loss=0.8554349541664124
I0216 09:19:17.390777 139535982765824 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.193312644958496, loss=0.8169888854026794
I0216 09:20:40.377642 139535991158528 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.5055546760559082, loss=0.8670423030853271
I0216 09:22:03.285537 139535982765824 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.3556289672851562, loss=0.8938689231872559
I0216 09:23:28.984245 139535991158528 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.2432024478912354, loss=0.8872042298316956
I0216 09:24:44.907720 139535982765824 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.2160910367965698, loss=0.8769452571868896
I0216 09:24:51.473886 139646656866112 spec.py:321] Evaluating on the training split.
I0216 09:25:45.407828 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 09:26:36.593579 139646656866112 spec.py:349] Evaluating on the test split.
I0216 09:27:02.438292 139646656866112 submission_runner.py:408] Time since start: 63047.46s, 	Step: 72210, 	{'train/ctc_loss': Array(0.06605408, dtype=float32), 'train/wer': 0.025010086881674155, 'validation/ctc_loss': Array(0.2977439, dtype=float32), 'validation/wer': 0.08529885978547361, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15752408, dtype=float32), 'test/wer': 0.05106331119371153, 'test/num_examples': 2472, 'score': 57647.67440390587, 'total_duration': 63047.46157693863, 'accumulated_submission_time': 57647.67440390587, 'accumulated_eval_time': 5394.533260583878, 'accumulated_logging_time': 2.2825567722320557}
I0216 09:27:02.485917 139535991158528 logging_writer.py:48] [72210] accumulated_eval_time=5394.533261, accumulated_logging_time=2.282557, accumulated_submission_time=57647.674404, global_step=72210, preemption_count=0, score=57647.674404, test/ctc_loss=0.15752407908439636, test/num_examples=2472, test/wer=0.051063, total_duration=63047.461577, train/ctc_loss=0.0660540759563446, train/wer=0.025010, validation/ctc_loss=0.29774388670921326, validation/num_examples=5348, validation/wer=0.085299
I0216 09:28:11.505416 139535982765824 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.3101698160171509, loss=0.8868781924247742
I0216 09:29:27.454299 139535991158528 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.1115645170211792, loss=0.8387385010719299
I0216 09:30:43.400630 139535982765824 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.2653172016143799, loss=0.9097033143043518
I0216 09:31:59.525674 139535991158528 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.2227203845977783, loss=0.9319261908531189
I0216 09:33:15.765166 139535982765824 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.3576011657714844, loss=0.8217965960502625
I0216 09:34:31.894264 139535991158528 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.330495834350586, loss=0.8722767233848572
I0216 09:35:50.573931 139535982765824 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.3187999725341797, loss=0.8827160000801086
I0216 09:37:14.242778 139535991158528 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.203142762184143, loss=0.9827153086662292
I0216 09:38:36.392360 139535982765824 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.2945613861083984, loss=0.8576827645301819
I0216 09:39:57.208742 139535991158528 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.2695331573486328, loss=0.8650412559509277
I0216 09:41:13.292758 139535982765824 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.2411431074142456, loss=0.8067213892936707
I0216 09:42:29.359042 139535991158528 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.3997859954833984, loss=0.8221306800842285
I0216 09:43:45.429603 139535982765824 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.3871608972549438, loss=0.8822938203811646
I0216 09:45:01.490471 139535991158528 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.5614713430404663, loss=0.8288331627845764
I0216 09:46:17.541588 139535982765824 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.436327338218689, loss=0.852033257484436
I0216 09:47:37.198950 139535991158528 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.336687445640564, loss=0.8700972199440002
I0216 09:48:59.481101 139535982765824 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.269579529762268, loss=0.8840065598487854
I0216 09:50:21.500842 139535991158528 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.398719310760498, loss=0.9051897525787354
I0216 09:51:02.933664 139646656866112 spec.py:321] Evaluating on the training split.
I0216 09:51:56.790323 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 09:52:49.231258 139646656866112 spec.py:349] Evaluating on the test split.
I0216 09:53:15.991254 139646656866112 submission_runner.py:408] Time since start: 64621.01s, 	Step: 74052, 	{'train/ctc_loss': Array(0.07841311, dtype=float32), 'train/wer': 0.030115272197834678, 'validation/ctc_loss': Array(0.29437685, dtype=float32), 'validation/wer': 0.08439132239782964, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15448758, dtype=float32), 'test/wer': 0.049600877460240084, 'test/num_examples': 2472, 'score': 59088.03487086296, 'total_duration': 64621.01479768753, 'accumulated_submission_time': 59088.03487086296, 'accumulated_eval_time': 5527.584671020508, 'accumulated_logging_time': 2.346060037612915}
I0216 09:53:16.046532 139535991158528 logging_writer.py:48] [74052] accumulated_eval_time=5527.584671, accumulated_logging_time=2.346060, accumulated_submission_time=59088.034871, global_step=74052, preemption_count=0, score=59088.034871, test/ctc_loss=0.15448758006095886, test/num_examples=2472, test/wer=0.049601, total_duration=64621.014798, train/ctc_loss=0.07841311395168304, train/wer=0.030115, validation/ctc_loss=0.29437685012817383, validation/num_examples=5348, validation/wer=0.084391
I0216 09:53:53.341976 139535982765824 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.1042828559875488, loss=0.7985620498657227
I0216 09:55:12.623287 139535991158528 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.168492078781128, loss=0.830141007900238
I0216 09:56:28.535189 139535982765824 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.1302284002304077, loss=0.8187664151191711
I0216 09:57:44.641754 139535991158528 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.309998869895935, loss=0.8453651070594788
I0216 09:59:00.693374 139535982765824 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.2838191986083984, loss=0.8365215063095093
I0216 10:00:16.750553 139535991158528 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.0784624814987183, loss=0.8133073449134827
I0216 10:01:32.737622 139535982765824 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.4476606845855713, loss=0.8887516856193542
I0216 10:02:53.425934 139535991158528 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.1731992959976196, loss=0.841317892074585
I0216 10:04:14.992983 139535982765824 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.179455041885376, loss=0.866077721118927
I0216 10:05:36.449417 139535991158528 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.318031668663025, loss=0.8586033582687378
I0216 10:06:57.675600 139535982765824 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.6303311586380005, loss=0.8666157722473145
I0216 10:08:22.653783 139535991158528 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.3394988775253296, loss=0.8186338543891907
I0216 10:09:38.940206 139535982765824 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.1899604797363281, loss=0.8876045942306519
I0216 10:10:55.034230 139535991158528 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.410333275794983, loss=0.8630677461624146
I0216 10:12:11.065228 139535982765824 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.1874186992645264, loss=0.8776262402534485
I0216 10:13:27.154127 139535991158528 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.3786224126815796, loss=0.8664212822914124
I0216 10:14:43.279774 139535982765824 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.2646706104278564, loss=0.805945634841919
I0216 10:16:01.408064 139535991158528 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.5457614660263062, loss=0.8609587550163269
I0216 10:17:16.427031 139646656866112 spec.py:321] Evaluating on the training split.
I0216 10:18:11.262862 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 10:19:03.126202 139646656866112 spec.py:349] Evaluating on the test split.
I0216 10:19:29.193265 139646656866112 submission_runner.py:408] Time since start: 66194.22s, 	Step: 75894, 	{'train/ctc_loss': Array(0.06854972, dtype=float32), 'train/wer': 0.025352037408149633, 'validation/ctc_loss': Array(0.29016793, dtype=float32), 'validation/wer': 0.08304932562248375, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15408525, dtype=float32), 'test/wer': 0.048768102695346614, 'test/num_examples': 2472, 'score': 60528.329884529114, 'total_duration': 66194.21731305122, 'accumulated_submission_time': 60528.329884529114, 'accumulated_eval_time': 5660.3452179431915, 'accumulated_logging_time': 2.4161722660064697}
I0216 10:19:29.238872 139535991158528 logging_writer.py:48] [75894] accumulated_eval_time=5660.345218, accumulated_logging_time=2.416172, accumulated_submission_time=60528.329885, global_step=75894, preemption_count=0, score=60528.329885, test/ctc_loss=0.15408524870872498, test/num_examples=2472, test/wer=0.048768, total_duration=66194.217313, train/ctc_loss=0.06854972243309021, train/wer=0.025352, validation/ctc_loss=0.2901679277420044, validation/num_examples=5348, validation/wer=0.083049
I0216 10:19:34.624929 139535982765824 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.146515130996704, loss=0.8365228772163391
I0216 10:20:50.408570 139535991158528 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.4513052701950073, loss=0.8286399841308594
I0216 10:22:06.726554 139535982765824 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.1530479192733765, loss=0.8400319218635559
I0216 10:23:22.806000 139535991158528 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.3088113069534302, loss=0.8385745882987976
I0216 10:24:42.143643 139535991158528 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.2691154479980469, loss=0.854690670967102
I0216 10:25:58.300326 139535982765824 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.256481409072876, loss=0.8140502572059631
I0216 10:27:14.367920 139535991158528 logging_writer.py:48] [76500] global_step=76500, grad_norm=1.155436635017395, loss=0.8469552397727966
I0216 10:28:29.230947 139535982765824 logging_writer.py:48] [76600] global_step=76600, preemption_count=0, score=61068.261309
I0216 10:28:30.104136 139646656866112 checkpoints.py:490] Saving checkpoint at step: 76600
I0216 10:28:31.629500 139646656866112 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_2/librispeech_conformer_jax/trial_3/checkpoint_76600
I0216 10:28:31.662744 139646656866112 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_2/librispeech_conformer_jax/trial_3/checkpoint_76600.
I0216 10:28:35.612805 139646656866112 submission_runner.py:583] Tuning trial 3/5
I0216 10:28:35.613124 139646656866112 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0216 10:28:35.645427 139646656866112 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(32.398228, dtype=float32), 'train/wer': 1.135258711138108, 'validation/ctc_loss': Array(30.757864, dtype=float32), 'validation/wer': 1.1779352558965794, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.871214, dtype=float32), 'test/wer': 1.190014827453131, 'test/num_examples': 2472, 'score': 35.92231845855713, 'total_duration': 176.72045016288757, 'accumulated_submission_time': 35.92231845855713, 'accumulated_eval_time': 140.7980694770813, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1679, {'train/ctc_loss': Array(6.0571713, dtype=float32), 'train/wer': 0.9391896477614642, 'validation/ctc_loss': Array(6.13424, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.0640683, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1475.9753279685974, 'total_duration': 1727.6220135688782, 'accumulated_submission_time': 1475.9753279685974, 'accumulated_eval_time': 251.5450747013092, 'accumulated_logging_time': 0.03110361099243164, 'global_step': 1679, 'preemption_count': 0}), (3379, {'train/ctc_loss': Array(3.853232, dtype=float32), 'train/wer': 0.8365734501347709, 'validation/ctc_loss': Array(3.802879, dtype=float32), 'validation/wer': 0.7978219102696544, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.4898498, dtype=float32), 'test/wer': 0.754087705400849, 'test/num_examples': 2472, 'score': 2916.449270963669, 'total_duration': 3285.9481086730957, 'accumulated_submission_time': 2916.449270963669, 'accumulated_eval_time': 369.26913595199585, 'accumulated_logging_time': 0.08405637741088867, 'global_step': 3379, 'preemption_count': 0}), (5053, {'train/ctc_loss': Array(1.3261416, dtype=float32), 'train/wer': 0.39291693782991527, 'validation/ctc_loss': Array(1.3397774, dtype=float32), 'validation/wer': 0.3746005387296408, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.0181005, dtype=float32), 'test/wer': 0.3211666971340361, 'test/num_examples': 2472, 'score': 4357.660209178925, 'total_duration': 4868.666774988174, 'accumulated_submission_time': 4357.660209178925, 'accumulated_eval_time': 510.6455659866333, 'accumulated_logging_time': 0.14217591285705566, 'global_step': 5053, 'preemption_count': 0}), (6745, {'train/ctc_loss': Array(0.7336726, dtype=float32), 'train/wer': 0.24152971459282097, 'validation/ctc_loss': Array(0.87219507, dtype=float32), 'validation/wer': 0.25994187898857857, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.596215, dtype=float32), 'test/wer': 0.199540958300327, 'test/num_examples': 2472, 'score': 5798.374348640442, 'total_duration': 6448.203431367874, 'accumulated_submission_time': 5798.374348640442, 'accumulated_eval_time': 649.3420903682709, 'accumulated_logging_time': 0.19429850578308105, 'global_step': 6745, 'preemption_count': 0}), (8450, {'train/ctc_loss': Array(0.61011267, dtype=float32), 'train/wer': 0.20421158528917494, 'validation/ctc_loss': Array(0.7665455, dtype=float32), 'validation/wer': 0.23046622319626944, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.51278436, dtype=float32), 'test/wer': 0.1729734121422623, 'test/num_examples': 2472, 'score': 7238.520888566971, 'total_duration': 8024.218991994858, 'accumulated_submission_time': 7238.520888566971, 'accumulated_eval_time': 785.0801012516022, 'accumulated_logging_time': 0.24956583976745605, 'global_step': 8450, 'preemption_count': 0}), (10145, {'train/ctc_loss': Array(0.5839462, dtype=float32), 'train/wer': 0.19995284792895754, 'validation/ctc_loss': Array(0.6855125, dtype=float32), 'validation/wer': 0.20873359915811426, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.44745144, dtype=float32), 'test/wer': 0.15215404301992566, 'test/num_examples': 2472, 'score': 8678.477567434311, 'total_duration': 9599.215919017792, 'accumulated_submission_time': 8678.477567434311, 'accumulated_eval_time': 919.9031093120575, 'accumulated_logging_time': 0.39224886894226074, 'global_step': 10145, 'preemption_count': 0}), (11861, {'train/ctc_loss': Array(0.50584203, dtype=float32), 'train/wer': 0.17452687315601126, 'validation/ctc_loss': Array(0.65349907, dtype=float32), 'validation/wer': 0.19818106336348804, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.41517326, dtype=float32), 'test/wer': 0.14240448479678264, 'test/num_examples': 2472, 'score': 10118.87825846672, 'total_duration': 11178.391873121262, 'accumulated_submission_time': 10118.87825846672, 'accumulated_eval_time': 1058.5496191978455, 'accumulated_logging_time': 0.4467132091522217, 'global_step': 11861, 'preemption_count': 0}), (13572, {'train/ctc_loss': Array(0.49853423, dtype=float32), 'train/wer': 0.1681780210600365, 'validation/ctc_loss': Array(0.62621295, dtype=float32), 'validation/wer': 0.18898017899726774, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39062923, dtype=float32), 'test/wer': 0.13362988239595394, 'test/num_examples': 2472, 'score': 11559.40121126175, 'total_duration': 12755.429028272629, 'accumulated_submission_time': 11559.40121126175, 'accumulated_eval_time': 1194.934509754181, 'accumulated_logging_time': 0.5001969337463379, 'global_step': 13572, 'preemption_count': 0}), (15270, {'train/ctc_loss': Array(0.41710535, dtype=float32), 'train/wer': 0.1461387617891493, 'validation/ctc_loss': Array(0.58882713, dtype=float32), 'validation/wer': 0.18003997026366858, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36876667, dtype=float32), 'test/wer': 0.12597241687486035, 'test/num_examples': 2472, 'score': 12999.642688512802, 'total_duration': 14332.771677017212, 'accumulated_submission_time': 12999.642688512802, 'accumulated_eval_time': 1331.903869152069, 'accumulated_logging_time': 0.5556769371032715, 'global_step': 15270, 'preemption_count': 0}), (16979, {'train/ctc_loss': Array(0.4158837, dtype=float32), 'train/wer': 0.14749664016764993, 'validation/ctc_loss': Array(0.572075, dtype=float32), 'validation/wer': 0.17372582716240093, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36074033, dtype=float32), 'test/wer': 0.12347409258017997, 'test/num_examples': 2472, 'score': 14440.339807987213, 'total_duration': 15907.829645395279, 'accumulated_submission_time': 14440.339807987213, 'accumulated_eval_time': 1466.1310760974884, 'accumulated_logging_time': 0.6140539646148682, 'global_step': 16979, 'preemption_count': 0}), (18678, {'train/ctc_loss': Array(0.39677075, dtype=float32), 'train/wer': 0.14022870301025028, 'validation/ctc_loss': Array(0.55821544, dtype=float32), 'validation/wer': 0.16983500197920387, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3428115, dtype=float32), 'test/wer': 0.11595880811650722, 'test/num_examples': 2472, 'score': 15880.586597919464, 'total_duration': 17480.91488957405, 'accumulated_submission_time': 15880.586597919464, 'accumulated_eval_time': 1598.8436088562012, 'accumulated_logging_time': 0.6656575202941895, 'global_step': 18678, 'preemption_count': 0}), (20395, {'train/ctc_loss': Array(0.26701897, dtype=float32), 'train/wer': 0.09736891410829475, 'validation/ctc_loss': Array(0.53861165, dtype=float32), 'validation/wer': 0.16398428222481826, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3305695, dtype=float32), 'test/wer': 0.11165275323461905, 'test/num_examples': 2472, 'score': 17321.084460258484, 'total_duration': 19067.706463098526, 'accumulated_submission_time': 17321.084460258484, 'accumulated_eval_time': 1745.0095345973969, 'accumulated_logging_time': 0.7188072204589844, 'global_step': 20395, 'preemption_count': 0}), (22125, {'train/ctc_loss': Array(0.25013727, dtype=float32), 'train/wer': 0.09088889577715095, 'validation/ctc_loss': Array(0.5297374, dtype=float32), 'validation/wer': 0.1605472257354432, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32201567, dtype=float32), 'test/wer': 0.10935754473625414, 'test/num_examples': 2472, 'score': 18761.54176592827, 'total_duration': 20646.577522277832, 'accumulated_submission_time': 18761.54176592827, 'accumulated_eval_time': 1883.2937757968903, 'accumulated_logging_time': 0.7721741199493408, 'global_step': 22125, 'preemption_count': 0}), (23843, {'train/ctc_loss': Array(0.24616796, dtype=float32), 'train/wer': 0.09029913923198671, 'validation/ctc_loss': Array(0.5182299, dtype=float32), 'validation/wer': 0.15712947855218823, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31246486, dtype=float32), 'test/wer': 0.1053561635488392, 'test/num_examples': 2472, 'score': 20202.07715177536, 'total_duration': 22227.13118314743, 'accumulated_submission_time': 20202.07715177536, 'accumulated_eval_time': 2023.181410074234, 'accumulated_logging_time': 0.8276913166046143, 'global_step': 23843, 'preemption_count': 0}), (25659, {'train/ctc_loss': Array(0.23114382, dtype=float32), 'train/wer': 0.08571623236410138, 'validation/ctc_loss': Array(0.5024098, dtype=float32), 'validation/wer': 0.15192562055282544, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30169642, dtype=float32), 'test/wer': 0.10139540552068735, 'test/num_examples': 2472, 'score': 21642.539937257767, 'total_duration': 23796.32274031639, 'accumulated_submission_time': 21642.539937257767, 'accumulated_eval_time': 2151.7861466407776, 'accumulated_logging_time': 0.880361795425415, 'global_step': 25659, 'preemption_count': 0}), (27534, {'train/ctc_loss': Array(0.22649139, dtype=float32), 'train/wer': 0.08500884842540671, 'validation/ctc_loss': Array(0.49297327, dtype=float32), 'validation/wer': 0.150226401614258, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29015937, dtype=float32), 'test/wer': 0.09918144334084862, 'test/num_examples': 2472, 'score': 23082.5641746521, 'total_duration': 25365.021073818207, 'accumulated_submission_time': 23082.5641746521, 'accumulated_eval_time': 2280.3359375, 'accumulated_logging_time': 0.9355659484863281, 'global_step': 27534, 'preemption_count': 0}), (29414, {'train/ctc_loss': Array(0.20505889, dtype=float32), 'train/wer': 0.07636058815156133, 'validation/ctc_loss': Array(0.4759323, dtype=float32), 'validation/wer': 0.14279231875802542, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28078052, dtype=float32), 'test/wer': 0.09320983892917352, 'test/num_examples': 2472, 'score': 24523.00218605995, 'total_duration': 26934.803947925568, 'accumulated_submission_time': 24523.00218605995, 'accumulated_eval_time': 2409.5551302433014, 'accumulated_logging_time': 0.9896988868713379, 'global_step': 29414, 'preemption_count': 0}), (31290, {'train/ctc_loss': Array(0.24940795, dtype=float32), 'train/wer': 0.08863719162402736, 'validation/ctc_loss': Array(0.47313604, dtype=float32), 'validation/wer': 0.14384467594157005, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2828514, dtype=float32), 'test/wer': 0.09511912741453903, 'test/num_examples': 2472, 'score': 25962.93151783943, 'total_duration': 28501.7939991951, 'accumulated_submission_time': 25962.93151783943, 'accumulated_eval_time': 2536.493052005768, 'accumulated_logging_time': 1.041748285293579, 'global_step': 31290, 'preemption_count': 0}), (33168, {'train/ctc_loss': Array(0.20455477, dtype=float32), 'train/wer': 0.07657436579432052, 'validation/ctc_loss': Array(0.46298337, dtype=float32), 'validation/wer': 0.13969317512575186, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26978076, dtype=float32), 'test/wer': 0.09087400727154551, 'test/num_examples': 2472, 'score': 27403.008170366287, 'total_duration': 30069.59238386154, 'accumulated_submission_time': 27403.008170366287, 'accumulated_eval_time': 2664.0866878032684, 'accumulated_logging_time': 1.0995814800262451, 'global_step': 33168, 'preemption_count': 0}), (35039, {'train/ctc_loss': Array(0.18673988, dtype=float32), 'train/wer': 0.07078146556924707, 'validation/ctc_loss': Array(0.44775584, dtype=float32), 'validation/wer': 0.13400658447338695, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26430643, dtype=float32), 'test/wer': 0.08847724087502284, 'test/num_examples': 2472, 'score': 28843.570236682892, 'total_duration': 31639.75147986412, 'accumulated_submission_time': 28843.570236682892, 'accumulated_eval_time': 2793.5609545707703, 'accumulated_logging_time': 1.1512136459350586, 'global_step': 35039, 'preemption_count': 0}), (36922, {'train/ctc_loss': Array(0.17774187, dtype=float32), 'train/wer': 0.06597390171950993, 'validation/ctc_loss': Array(0.44245794, dtype=float32), 'validation/wer': 0.1318246328818174, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25393718, dtype=float32), 'test/wer': 0.0839680701968192, 'test/num_examples': 2472, 'score': 30284.19582605362, 'total_duration': 33209.37820768356, 'accumulated_submission_time': 30284.19582605362, 'accumulated_eval_time': 2922.4401302337646, 'accumulated_logging_time': 1.2019224166870117, 'global_step': 36922, 'preemption_count': 0}), (38794, {'train/ctc_loss': Array(0.16964507, dtype=float32), 'train/wer': 0.06578919035314384, 'validation/ctc_loss': Array(0.43418646, dtype=float32), 'validation/wer': 0.1291116753719455, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24846429, dtype=float32), 'test/wer': 0.08337903438750431, 'test/num_examples': 2472, 'score': 31724.767939329147, 'total_duration': 34778.51620674133, 'accumulated_submission_time': 31724.767939329147, 'accumulated_eval_time': 3050.8781111240387, 'accumulated_logging_time': 1.257685661315918, 'global_step': 38794, 'preemption_count': 0}), (40666, {'train/ctc_loss': Array(0.17991582, dtype=float32), 'train/wer': 0.06738385587444276, 'validation/ctc_loss': Array(0.4272925, dtype=float32), 'validation/wer': 0.12789518908638017, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24406633, dtype=float32), 'test/wer': 0.08258688278187394, 'test/num_examples': 2472, 'score': 33164.96550655365, 'total_duration': 36348.4490673542, 'accumulated_submission_time': 33164.96550655365, 'accumulated_eval_time': 3180.488514661789, 'accumulated_logging_time': 1.3088583946228027, 'global_step': 40666, 'preemption_count': 0}), (42536, {'train/ctc_loss': Array(0.17591874, dtype=float32), 'train/wer': 0.06519827499960508, 'validation/ctc_loss': Array(0.42120722, dtype=float32), 'validation/wer': 0.12393678133176285, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24332765, dtype=float32), 'test/wer': 0.08041354376129832, 'test/num_examples': 2472, 'score': 34604.94077825546, 'total_duration': 37917.36299943924, 'accumulated_submission_time': 34604.94077825546, 'accumulated_eval_time': 3309.295679807663, 'accumulated_logging_time': 1.3665189743041992, 'global_step': 42536, 'preemption_count': 0}), (44398, {'train/ctc_loss': Array(0.1598213, dtype=float32), 'train/wer': 0.060765059054097494, 'validation/ctc_loss': Array(0.4153037, dtype=float32), 'validation/wer': 0.1243615860664047, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23402373, dtype=float32), 'test/wer': 0.07771210367030244, 'test/num_examples': 2472, 'score': 36044.85700464249, 'total_duration': 39487.44536948204, 'accumulated_submission_time': 36044.85700464249, 'accumulated_eval_time': 3439.3287382125854, 'accumulated_logging_time': 1.4259233474731445, 'global_step': 44398, 'preemption_count': 0}), (46259, {'train/ctc_loss': Array(0.15301697, dtype=float32), 'train/wer': 0.05895605283880826, 'validation/ctc_loss': Array(0.40488496, dtype=float32), 'validation/wer': 0.12075074582194889, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22412066, dtype=float32), 'test/wer': 0.07618873519793634, 'test/num_examples': 2472, 'score': 37485.154686927795, 'total_duration': 41058.24700117111, 'accumulated_submission_time': 37485.154686927795, 'accumulated_eval_time': 3569.7060022354126, 'accumulated_logging_time': 1.4790055751800537, 'global_step': 46259, 'preemption_count': 0}), (48120, {'train/ctc_loss': Array(0.13564813, dtype=float32), 'train/wer': 0.05087395046733907, 'validation/ctc_loss': Array(0.39561796, dtype=float32), 'validation/wer': 0.11742954516929434, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21684836, dtype=float32), 'test/wer': 0.07255296244388926, 'test/num_examples': 2472, 'score': 38925.27982378006, 'total_duration': 42628.89527773857, 'accumulated_submission_time': 38925.27982378006, 'accumulated_eval_time': 3700.0979936122894, 'accumulated_logging_time': 1.5350711345672607, 'global_step': 48120, 'preemption_count': 0}), (49986, {'train/ctc_loss': Array(0.13670734, dtype=float32), 'train/wer': 0.05112016917466058, 'validation/ctc_loss': Array(0.38398, dtype=float32), 'validation/wer': 0.11543103198586559, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21637918, dtype=float32), 'test/wer': 0.07174049925862734, 'test/num_examples': 2472, 'score': 40365.31974339485, 'total_duration': 44202.62142920494, 'accumulated_submission_time': 40365.31974339485, 'accumulated_eval_time': 3833.651977300644, 'accumulated_logging_time': 1.5921378135681152, 'global_step': 49986, 'preemption_count': 0}), (51848, {'train/ctc_loss': Array(0.13435693, dtype=float32), 'train/wer': 0.05087923070689609, 'validation/ctc_loss': Array(0.37354812, dtype=float32), 'validation/wer': 0.10994718904776157, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21018149, dtype=float32), 'test/wer': 0.06954684865842016, 'test/num_examples': 2472, 'score': 41805.59288787842, 'total_duration': 45771.889213085175, 'accumulated_submission_time': 41805.59288787842, 'accumulated_eval_time': 3962.5165877342224, 'accumulated_logging_time': 1.646233320236206, 'global_step': 51848, 'preemption_count': 0}), (53703, {'train/ctc_loss': Array(0.1186261, dtype=float32), 'train/wer': 0.044654267796189744, 'validation/ctc_loss': Array(0.36797065, dtype=float32), 'validation/wer': 0.10780385606843218, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20350519, dtype=float32), 'test/wer': 0.06775942965084394, 'test/num_examples': 2472, 'score': 43245.52307033539, 'total_duration': 47341.51375102997, 'accumulated_submission_time': 43245.52307033539, 'accumulated_eval_time': 4092.0777394771576, 'accumulated_logging_time': 1.704374074935913, 'global_step': 53703, 'preemption_count': 0}), (55563, {'train/ctc_loss': Array(0.11973902, dtype=float32), 'train/wer': 0.0472273716241127, 'validation/ctc_loss': Array(0.36250752, dtype=float32), 'validation/wer': 0.10651013255838651, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19598241, dtype=float32), 'test/wer': 0.06489549692279568, 'test/num_examples': 2472, 'score': 44685.69279670715, 'total_duration': 48912.9459066391, 'accumulated_submission_time': 44685.69279670715, 'accumulated_eval_time': 4223.207216978073, 'accumulated_logging_time': 1.7623958587646484, 'global_step': 55563, 'preemption_count': 0}), (57412, {'train/ctc_loss': Array(0.13343582, dtype=float32), 'train/wer': 0.048016934638889036, 'validation/ctc_loss': Array(0.34958616, dtype=float32), 'validation/wer': 0.10371028317097425, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19371364, dtype=float32), 'test/wer': 0.0641439684764284, 'test/num_examples': 2472, 'score': 46125.87220811844, 'total_duration': 50483.96151971817, 'accumulated_submission_time': 46125.87220811844, 'accumulated_eval_time': 4353.911198377609, 'accumulated_logging_time': 1.8190128803253174, 'global_step': 57412, 'preemption_count': 0}), (59270, {'train/ctc_loss': Array(0.08705975, dtype=float32), 'train/wer': 0.03397191080448442, 'validation/ctc_loss': Array(0.34289178, dtype=float32), 'validation/wer': 0.10056286627340047, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.184741, dtype=float32), 'test/wer': 0.06197062945585278, 'test/num_examples': 2472, 'score': 47566.122009038925, 'total_duration': 52055.225329875946, 'accumulated_submission_time': 47566.122009038925, 'accumulated_eval_time': 4484.790504455566, 'accumulated_logging_time': 1.8767881393432617, 'global_step': 59270, 'preemption_count': 0}), (61129, {'train/ctc_loss': Array(0.08885489, dtype=float32), 'train/wer': 0.03415380133993592, 'validation/ctc_loss': Array(0.33372676, dtype=float32), 'validation/wer': 0.0980816204369696, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17703222, dtype=float32), 'test/wer': 0.05900513882964678, 'test/num_examples': 2472, 'score': 49006.60219502449, 'total_duration': 53627.36594891548, 'accumulated_submission_time': 49006.60219502449, 'accumulated_eval_time': 4616.315147399902, 'accumulated_logging_time': 1.937981128692627, 'global_step': 61129, 'preemption_count': 0}), (62978, {'train/ctc_loss': Array(0.10730325, dtype=float32), 'train/wer': 0.04166804184956599, 'validation/ctc_loss': Array(0.33158988, dtype=float32), 'validation/wer': 0.0967106597024436, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17833579, dtype=float32), 'test/wer': 0.05874108829443666, 'test/num_examples': 2472, 'score': 50447.224174022675, 'total_duration': 55199.06018662453, 'accumulated_submission_time': 50447.224174022675, 'accumulated_eval_time': 4747.2524383068085, 'accumulated_logging_time': 1.996875286102295, 'global_step': 62978, 'preemption_count': 0}), (64826, {'train/ctc_loss': Array(0.10190608, dtype=float32), 'train/wer': 0.03875721573180865, 'validation/ctc_loss': Array(0.31840345, dtype=float32), 'validation/wer': 0.09274259729476621, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16995475, dtype=float32), 'test/wer': 0.056019336623809236, 'test/num_examples': 2472, 'score': 51887.12818932533, 'total_duration': 56769.58395195007, 'accumulated_submission_time': 51887.12818932533, 'accumulated_eval_time': 4877.739913702011, 'accumulated_logging_time': 2.052243232727051, 'global_step': 64826, 'preemption_count': 0}), (66672, {'train/ctc_loss': Array(0.11785042, dtype=float32), 'train/wer': 0.04573007873284251, 'validation/ctc_loss': Array(0.30807304, dtype=float32), 'validation/wer': 0.08892900933604951, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1668592, dtype=float32), 'test/wer': 0.0543944102532854, 'test/num_examples': 2472, 'score': 53327.30903625488, 'total_duration': 58338.56297969818, 'accumulated_submission_time': 53327.30903625488, 'accumulated_eval_time': 5006.407078027725, 'accumulated_logging_time': 2.1070005893707275, 'global_step': 66672, 'preemption_count': 0}), (68524, {'train/ctc_loss': Array(0.09267107, dtype=float32), 'train/wer': 0.03409489511961275, 'validation/ctc_loss': Array(0.30833793, dtype=float32), 'validation/wer': 0.08829180223408672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1638089, dtype=float32), 'test/wer': 0.05252574492718299, 'test/num_examples': 2472, 'score': 54767.389909267426, 'total_duration': 59907.71535515785, 'accumulated_submission_time': 54767.389909267426, 'accumulated_eval_time': 5135.344121932983, 'accumulated_logging_time': 2.1641712188720703, 'global_step': 68524, 'preemption_count': 0}), (70376, {'train/ctc_loss': Array(0.08907641, dtype=float32), 'train/wer': 0.034411574026683045, 'validation/ctc_loss': Array(0.30346766, dtype=float32), 'validation/wer': 0.08658292864245923, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1597395, dtype=float32), 'test/wer': 0.05165234700302643, 'test/num_examples': 2472, 'score': 56207.60083270073, 'total_duration': 61476.292707920074, 'accumulated_submission_time': 56207.60083270073, 'accumulated_eval_time': 5263.575304508209, 'accumulated_logging_time': 2.2221508026123047, 'global_step': 70376, 'preemption_count': 0}), (72210, {'train/ctc_loss': Array(0.06605408, dtype=float32), 'train/wer': 0.025010086881674155, 'validation/ctc_loss': Array(0.2977439, dtype=float32), 'validation/wer': 0.08529885978547361, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15752408, dtype=float32), 'test/wer': 0.05106331119371153, 'test/num_examples': 2472, 'score': 57647.67440390587, 'total_duration': 63047.46157693863, 'accumulated_submission_time': 57647.67440390587, 'accumulated_eval_time': 5394.533260583878, 'accumulated_logging_time': 2.2825567722320557, 'global_step': 72210, 'preemption_count': 0}), (74052, {'train/ctc_loss': Array(0.07841311, dtype=float32), 'train/wer': 0.030115272197834678, 'validation/ctc_loss': Array(0.29437685, dtype=float32), 'validation/wer': 0.08439132239782964, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15448758, dtype=float32), 'test/wer': 0.049600877460240084, 'test/num_examples': 2472, 'score': 59088.03487086296, 'total_duration': 64621.01479768753, 'accumulated_submission_time': 59088.03487086296, 'accumulated_eval_time': 5527.584671020508, 'accumulated_logging_time': 2.346060037612915, 'global_step': 74052, 'preemption_count': 0}), (75894, {'train/ctc_loss': Array(0.06854972, dtype=float32), 'train/wer': 0.025352037408149633, 'validation/ctc_loss': Array(0.29016793, dtype=float32), 'validation/wer': 0.08304932562248375, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15408525, dtype=float32), 'test/wer': 0.048768102695346614, 'test/num_examples': 2472, 'score': 60528.329884529114, 'total_duration': 66194.21731305122, 'accumulated_submission_time': 60528.329884529114, 'accumulated_eval_time': 5660.3452179431915, 'accumulated_logging_time': 2.4161722660064697, 'global_step': 75894, 'preemption_count': 0})], 'global_step': 76600}
I0216 10:28:35.645737 139646656866112 submission_runner.py:586] Timing: 61068.26130890846
I0216 10:28:35.645818 139646656866112 submission_runner.py:588] Total number of evals: 43
I0216 10:28:35.645872 139646656866112 submission_runner.py:589] ====================
I0216 10:28:35.645927 139646656866112 submission_runner.py:542] Using RNG seed 1365630931
I0216 10:28:35.648528 139646656866112 submission_runner.py:551] --- Tuning run 4/5 ---
I0216 10:28:35.648687 139646656866112 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_2/librispeech_conformer_jax/trial_4.
I0216 10:28:35.651991 139646656866112 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_2/librispeech_conformer_jax/trial_4/hparams.json.
I0216 10:28:35.654216 139646656866112 submission_runner.py:206] Initializing dataset.
I0216 10:28:35.654355 139646656866112 submission_runner.py:213] Initializing model.
I0216 10:28:39.385503 139646656866112 submission_runner.py:255] Initializing optimizer.
I0216 10:28:39.812641 139646656866112 submission_runner.py:262] Initializing metrics bundle.
I0216 10:28:39.812836 139646656866112 submission_runner.py:280] Initializing checkpoint and logger.
I0216 10:28:39.818661 139646656866112 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_2/librispeech_conformer_jax/trial_4 with prefix checkpoint_
I0216 10:28:39.818798 139646656866112 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_2/librispeech_conformer_jax/trial_4/meta_data_0.json.
I0216 10:28:39.819019 139646656866112 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0216 10:28:39.819092 139646656866112 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0216 10:28:40.434424 139646656866112 logger_utils.py:220] Unable to record git information. Continuing without it.
I0216 10:28:41.005947 139646656866112 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_2/librispeech_conformer_jax/trial_4/flags_0.json.
I0216 10:28:41.023647 139646656866112 submission_runner.py:314] Starting training loop.
I0216 10:28:41.027199 139646656866112 input_pipeline.py:20] Loading split = train-clean-100
I0216 10:28:41.077398 139646656866112 input_pipeline.py:20] Loading split = train-clean-360
I0216 10:28:41.591984 139646656866112 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0216 10:29:15.870540 139476339738368 logging_writer.py:48] [0] global_step=0, grad_norm=39.550594329833984, loss=32.219703674316406
I0216 10:29:15.891959 139646656866112 spec.py:321] Evaluating on the training split.
I0216 10:30:09.917982 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 10:31:01.926542 139646656866112 spec.py:349] Evaluating on the test split.
I0216 10:31:28.188385 139646656866112 submission_runner.py:408] Time since start: 167.16s, 	Step: 1, 	{'train/ctc_loss': Array(31.446594, dtype=float32), 'train/wer': 1.1269105188367123, 'validation/ctc_loss': Array(30.757864, dtype=float32), 'validation/wer': 1.1779352558965794, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.871214, dtype=float32), 'test/wer': 1.1899945158734995, 'test/num_examples': 2472, 'score': 34.868218183517456, 'total_duration': 167.16175413131714, 'accumulated_submission_time': 34.868218183517456, 'accumulated_eval_time': 132.29344940185547, 'accumulated_logging_time': 0}
I0216 10:31:28.205099 139535991158528 logging_writer.py:48] [1] accumulated_eval_time=132.293449, accumulated_logging_time=0, accumulated_submission_time=34.868218, global_step=1, preemption_count=0, score=34.868218, test/ctc_loss=30.871213912963867, test/num_examples=2472, test/wer=1.189995, total_duration=167.161754, train/ctc_loss=31.44659423828125, train/wer=1.126911, validation/ctc_loss=30.757863998413086, validation/num_examples=5348, validation/wer=1.177935
I0216 10:33:09.295677 139476289382144 logging_writer.py:48] [100] global_step=100, grad_norm=5.306451320648193, loss=5.8377485275268555
I0216 10:34:25.559350 139476297774848 logging_writer.py:48] [200] global_step=200, grad_norm=3.723012924194336, loss=5.76642370223999
I0216 10:35:42.031561 139476289382144 logging_writer.py:48] [300] global_step=300, grad_norm=1.3693208694458008, loss=5.597695350646973
I0216 10:36:58.202202 139476297774848 logging_writer.py:48] [400] global_step=400, grad_norm=1.543100118637085, loss=5.589056491851807
I0216 10:38:14.438165 139476289382144 logging_writer.py:48] [500] global_step=500, grad_norm=1.1457114219665527, loss=5.549233913421631
I0216 10:39:30.627016 139476297774848 logging_writer.py:48] [600] global_step=600, grad_norm=0.43792977929115295, loss=5.5002665519714355
I0216 10:40:46.682349 139476289382144 logging_writer.py:48] [700] global_step=700, grad_norm=3.7343125343322754, loss=5.558848857879639
I0216 10:42:02.678073 139476297774848 logging_writer.py:48] [800] global_step=800, grad_norm=0.362154483795166, loss=5.495203018188477
I0216 10:43:25.502782 139476289382144 logging_writer.py:48] [900] global_step=900, grad_norm=3.085757255554199, loss=5.485403060913086
I0216 10:44:48.279798 139476297774848 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.9484832286834717, loss=5.463416576385498
I0216 10:46:09.460570 139535991158528 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.5805904865264893, loss=5.3904876708984375
I0216 10:47:25.209819 139535982765824 logging_writer.py:48] [1200] global_step=1200, grad_norm=2.7703466415405273, loss=5.177056312561035
I0216 10:48:40.923201 139535991158528 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.8790433406829834, loss=4.729323863983154
I0216 10:49:56.533116 139535982765824 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.7646727561950684, loss=4.3928117752075195
I0216 10:51:12.522173 139535991158528 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.22085440158844, loss=4.222440242767334
I0216 10:52:28.326664 139535982765824 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.639976441860199, loss=4.070485591888428
I0216 10:53:48.887278 139535991158528 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.40970778465271, loss=3.991147041320801
I0216 10:55:11.208662 139535982765824 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.9645039439201355, loss=3.9017794132232666
I0216 10:55:28.244028 139646656866112 spec.py:321] Evaluating on the training split.
I0216 10:56:08.830911 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 10:56:55.425886 139646656866112 spec.py:349] Evaluating on the test split.
I0216 10:57:19.361582 139646656866112 submission_runner.py:408] Time since start: 1718.33s, 	Step: 1822, 	{'train/ctc_loss': Array(4.786604, dtype=float32), 'train/wer': 0.9100190787680567, 'validation/ctc_loss': Array(4.6146393, dtype=float32), 'validation/wer': 0.8706855769137936, 'validation/num_examples': 5348, 'test/ctc_loss': Array(4.465194, dtype=float32), 'test/wer': 0.8685231450449902, 'test/num_examples': 2472, 'score': 1474.8262186050415, 'total_duration': 1718.332086801529, 'accumulated_submission_time': 1474.8262186050415, 'accumulated_eval_time': 243.40519452095032, 'accumulated_logging_time': 0.027191877365112305}
I0216 10:57:19.392241 139535991158528 logging_writer.py:48] [1822] accumulated_eval_time=243.405195, accumulated_logging_time=0.027192, accumulated_submission_time=1474.826219, global_step=1822, preemption_count=0, score=1474.826219, test/ctc_loss=4.465194225311279, test/num_examples=2472, test/wer=0.868523, total_duration=1718.332087, train/ctc_loss=4.786603927612305, train/wer=0.910019, validation/ctc_loss=4.6146392822265625, validation/num_examples=5348, validation/wer=0.870686
I0216 10:58:19.136952 139535982765824 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.2285758256912231, loss=3.803875684738159
I0216 10:59:34.977732 139535991158528 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.7948020696640015, loss=3.766901731491089
I0216 11:00:54.336198 139535991158528 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.9494659900665283, loss=3.7195522785186768
I0216 11:02:10.134343 139535982765824 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.931989312171936, loss=3.5958263874053955
I0216 11:03:25.905441 139535991158528 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.7120741605758667, loss=3.580618143081665
I0216 11:04:41.720632 139535982765824 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.2646647691726685, loss=3.397362470626831
I0216 11:05:57.531247 139535991158528 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.6941887140274048, loss=3.315680503845215
I0216 11:07:13.281727 139535982765824 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.9639467000961304, loss=3.3295538425445557
I0216 11:08:34.678601 139535991158528 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.6150938272476196, loss=3.254511833190918
I0216 11:09:56.572543 139535982765824 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.8777021169662476, loss=3.1842496395111084
I0216 11:11:19.357663 139535991158528 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.09088134765625, loss=3.182619571685791
I0216 11:12:41.797378 139535982765824 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.1241616010665894, loss=3.159606695175171
I0216 11:14:07.339314 139535991158528 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.1309046745300293, loss=3.133333683013916
I0216 11:15:22.933521 139535982765824 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.9047578573226929, loss=3.100144386291504
I0216 11:16:38.773454 139535991158528 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.1693366765975952, loss=3.005894899368286
I0216 11:17:54.400996 139535982765824 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.6794441938400269, loss=3.0057473182678223
I0216 11:19:10.112912 139535991158528 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.6153716444969177, loss=3.016380786895752
I0216 11:20:25.777799 139535982765824 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.926198422908783, loss=2.982139825820923
I0216 11:21:19.482984 139646656866112 spec.py:321] Evaluating on the training split.
I0216 11:22:09.031989 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 11:22:59.113389 139646656866112 spec.py:349] Evaluating on the test split.
I0216 11:23:25.210800 139646656866112 submission_runner.py:408] Time since start: 3284.18s, 	Step: 3672, 	{'train/ctc_loss': Array(2.9874406, dtype=float32), 'train/wer': 0.6618090910126309, 'validation/ctc_loss': Array(2.8403525, dtype=float32), 'validation/wer': 0.6263166533110632, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.4753306, dtype=float32), 'test/wer': 0.5817845753864278, 'test/num_examples': 2472, 'score': 2914.8294603824615, 'total_duration': 3284.181474685669, 'accumulated_submission_time': 2914.8294603824615, 'accumulated_eval_time': 369.1273944377899, 'accumulated_logging_time': 0.07270073890686035}
I0216 11:23:25.243818 139535991158528 logging_writer.py:48] [3672] accumulated_eval_time=369.127394, accumulated_logging_time=0.072701, accumulated_submission_time=2914.829460, global_step=3672, preemption_count=0, score=2914.829460, test/ctc_loss=2.4753305912017822, test/num_examples=2472, test/wer=0.581785, total_duration=3284.181475, train/ctc_loss=2.987440586090088, train/wer=0.661809, validation/ctc_loss=2.8403525352478027, validation/num_examples=5348, validation/wer=0.626317
I0216 11:23:47.169274 139535982765824 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.2029263973236084, loss=2.969611644744873
I0216 11:25:03.130895 139535991158528 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.9390143156051636, loss=2.8918466567993164
I0216 11:26:18.865346 139535982765824 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.6752945184707642, loss=2.9048349857330322
I0216 11:27:34.615413 139535991158528 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.9396365284919739, loss=2.954732656478882
I0216 11:28:50.378065 139535982765824 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.6946488618850708, loss=2.8107895851135254
I0216 11:30:09.690649 139535991158528 logging_writer.py:48] [4200] global_step=4200, grad_norm=1.4341198205947876, loss=2.818159341812134
I0216 11:31:25.382868 139535982765824 logging_writer.py:48] [4300] global_step=4300, grad_norm=1.0151164531707764, loss=2.88395619392395
I0216 11:32:40.980667 139535991158528 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.7956751585006714, loss=2.714461088180542
I0216 11:33:56.653283 139535982765824 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.265182375907898, loss=2.771108627319336
I0216 11:35:12.394789 139535991158528 logging_writer.py:48] [4600] global_step=4600, grad_norm=1.3854820728302002, loss=2.8706891536712646
I0216 11:36:29.023555 139535982765824 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.7784961462020874, loss=2.8379228115081787
I0216 11:37:50.873177 139535991158528 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.5295031070709229, loss=2.728529214859009
I0216 11:39:13.243713 139535982765824 logging_writer.py:48] [4900] global_step=4900, grad_norm=1.4795260429382324, loss=2.706268310546875
I0216 11:40:35.033448 139535991158528 logging_writer.py:48] [5000] global_step=5000, grad_norm=1.7491403818130493, loss=2.8176839351654053
I0216 11:41:57.016861 139535982765824 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.5976361036300659, loss=2.7485527992248535
I0216 11:43:19.034356 139535991158528 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.1604479551315308, loss=2.772416591644287
I0216 11:44:34.679590 139535982765824 logging_writer.py:48] [5300] global_step=5300, grad_norm=2.332587480545044, loss=2.7343485355377197
I0216 11:45:50.377136 139535991158528 logging_writer.py:48] [5400] global_step=5400, grad_norm=1.29829740524292, loss=2.685750722885132
I0216 11:47:06.040248 139535982765824 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.6090199947357178, loss=2.749471426010132
I0216 11:47:25.360594 139646656866112 spec.py:321] Evaluating on the training split.
I0216 11:48:16.845648 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 11:49:08.183259 139646656866112 spec.py:349] Evaluating on the test split.
I0216 11:49:34.338201 139646656866112 submission_runner.py:408] Time since start: 4853.31s, 	Step: 5527, 	{'train/ctc_loss': Array(1.8403991, dtype=float32), 'train/wer': 0.5122606049758368, 'validation/ctc_loss': Array(1.9006361, dtype=float32), 'validation/wer': 0.4987593770817846, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.5014671, dtype=float32), 'test/wer': 0.43704425893201715, 'test/num_examples': 2472, 'score': 4354.856928110123, 'total_duration': 4853.3082637786865, 'accumulated_submission_time': 4354.856928110123, 'accumulated_eval_time': 498.09877157211304, 'accumulated_logging_time': 0.12321734428405762}
I0216 11:49:34.374659 139535991158528 logging_writer.py:48] [5527] accumulated_eval_time=498.098772, accumulated_logging_time=0.123217, accumulated_submission_time=4354.856928, global_step=5527, preemption_count=0, score=4354.856928, test/ctc_loss=1.5014671087265015, test/num_examples=2472, test/wer=0.437044, total_duration=4853.308264, train/ctc_loss=1.840399146080017, train/wer=0.512261, validation/ctc_loss=1.900636076927185, validation/num_examples=5348, validation/wer=0.498759
I0216 11:50:30.268115 139535982765824 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.4048495292663574, loss=2.665099859237671
I0216 11:51:46.131995 139535991158528 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.1954172849655151, loss=2.672879219055176
I0216 11:53:01.778875 139535982765824 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.8777981996536255, loss=2.674107551574707
I0216 11:54:17.635386 139535991158528 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.8164662718772888, loss=2.5999886989593506
I0216 11:55:33.426061 139535982765824 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.8480011820793152, loss=2.636176586151123
I0216 11:56:55.055701 139535991158528 logging_writer.py:48] [6100] global_step=6100, grad_norm=1.2391502857208252, loss=2.572526454925537
I0216 11:58:19.427321 139535991158528 logging_writer.py:48] [6200] global_step=6200, grad_norm=1.636936068534851, loss=2.673255205154419
I0216 11:59:35.159851 139535982765824 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.9100297689437866, loss=2.6320526599884033
I0216 12:00:50.852846 139535991158528 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.8164535164833069, loss=2.580359935760498
I0216 12:02:06.707163 139535982765824 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.4531651735305786, loss=2.5805041790008545
I0216 12:03:22.469703 139535991158528 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.7341853976249695, loss=2.5365567207336426
I0216 12:04:38.277846 139535982765824 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.5924627780914307, loss=2.5367541313171387
I0216 12:05:56.230589 139535991158528 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.5696154832839966, loss=2.600224733352661
I0216 12:07:18.354123 139535982765824 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.9694629907608032, loss=2.6064088344573975
I0216 12:08:39.673398 139535991158528 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.6801557540893555, loss=2.4932820796966553
I0216 12:10:01.247933 139535982765824 logging_writer.py:48] [7100] global_step=7100, grad_norm=1.2361164093017578, loss=2.463343858718872
I0216 12:11:23.333200 139535991158528 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.0439534187316895, loss=2.498838424682617
I0216 12:12:42.836680 139535991158528 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.2401998043060303, loss=2.4404637813568115
I0216 12:13:35.023605 139646656866112 spec.py:321] Evaluating on the training split.
I0216 12:14:27.508041 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 12:15:18.332256 139646656866112 spec.py:349] Evaluating on the test split.
I0216 12:15:44.286657 139646656866112 submission_runner.py:408] Time since start: 6423.26s, 	Step: 7370, 	{'train/ctc_loss': Array(1.4814537, dtype=float32), 'train/wer': 0.43512802264188416, 'validation/ctc_loss': Array(1.549442, dtype=float32), 'validation/wer': 0.42388754260115663, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.1917499, dtype=float32), 'test/wer': 0.36481628176223263, 'test/num_examples': 2472, 'score': 5795.416927576065, 'total_duration': 6423.257106304169, 'accumulated_submission_time': 5795.416927576065, 'accumulated_eval_time': 627.3559744358063, 'accumulated_logging_time': 0.17549872398376465}
I0216 12:15:44.322889 139535991158528 logging_writer.py:48] [7370] accumulated_eval_time=627.355974, accumulated_logging_time=0.175499, accumulated_submission_time=5795.416928, global_step=7370, preemption_count=0, score=5795.416928, test/ctc_loss=1.191749930381775, test/num_examples=2472, test/wer=0.364816, total_duration=6423.257106, train/ctc_loss=1.4814536571502686, train/wer=0.435128, validation/ctc_loss=1.5494420528411865, validation/num_examples=5348, validation/wer=0.423888
I0216 12:16:07.853646 139535982765824 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.283853530883789, loss=2.4395911693573
I0216 12:17:23.456458 139535991158528 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.4841845035552979, loss=2.5344629287719727
I0216 12:18:39.235562 139535982765824 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.5258479118347168, loss=2.4083423614501953
I0216 12:19:55.047408 139535991158528 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.6485866904258728, loss=2.4291067123413086
I0216 12:21:10.766044 139535982765824 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.3382346630096436, loss=2.450110912322998
I0216 12:22:26.466891 139535991158528 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.2627906799316406, loss=2.499915838241577
I0216 12:23:45.537025 139535982765824 logging_writer.py:48] [8000] global_step=8000, grad_norm=2.040156841278076, loss=2.4773142337799072
I0216 12:25:06.923559 139535991158528 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.0779101848602295, loss=2.404947519302368
I0216 12:26:28.653109 139535982765824 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.7794374227523804, loss=2.3888490200042725
I0216 12:27:50.541568 139535991158528 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.6518740653991699, loss=2.3953938484191895
I0216 12:29:06.342914 139535982765824 logging_writer.py:48] [8400] global_step=8400, grad_norm=2.2589921951293945, loss=2.5390968322753906
I0216 12:30:22.375338 139535991158528 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.2696365118026733, loss=2.444112777709961
I0216 12:31:38.137000 139535982765824 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.2406045198440552, loss=2.3597865104675293
I0216 12:32:53.907378 139535991158528 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.8876283764839172, loss=2.4102189540863037
I0216 12:34:09.511851 139535982765824 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.191583514213562, loss=2.4305295944213867
I0216 12:35:30.331871 139535991158528 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.9390555024147034, loss=2.284161329269409
I0216 12:36:53.069821 139535982765824 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.109918475151062, loss=2.324124336242676
I0216 12:38:16.346698 139535991158528 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.8242512941360474, loss=2.4013116359710693
I0216 12:39:38.201187 139535982765824 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.3095474243164062, loss=2.3578336238861084
I0216 12:39:45.150916 139646656866112 spec.py:321] Evaluating on the training split.
I0216 12:40:37.927806 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 12:41:29.523450 139646656866112 spec.py:349] Evaluating on the test split.
I0216 12:41:55.529137 139646656866112 submission_runner.py:408] Time since start: 7994.50s, 	Step: 9210, 	{'train/ctc_loss': Array(1.5415139, dtype=float32), 'train/wer': 0.4593235089347876, 'validation/ctc_loss': Array(1.4839153, dtype=float32), 'validation/wer': 0.41729341456114777, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.1388015, dtype=float32), 'test/wer': 0.3562854183169825, 'test/num_examples': 2472, 'score': 7236.158366441727, 'total_duration': 7994.499317407608, 'accumulated_submission_time': 7236.158366441727, 'accumulated_eval_time': 757.7280685901642, 'accumulated_logging_time': 0.2269151210784912}
I0216 12:41:55.560956 139535991158528 logging_writer.py:48] [9210] accumulated_eval_time=757.728069, accumulated_logging_time=0.226915, accumulated_submission_time=7236.158366, global_step=9210, preemption_count=0, score=7236.158366, test/ctc_loss=1.1388014554977417, test/num_examples=2472, test/wer=0.356285, total_duration=7994.499317, train/ctc_loss=1.5415139198303223, train/wer=0.459324, validation/ctc_loss=1.4839153289794922, validation/num_examples=5348, validation/wer=0.417293
I0216 12:43:07.809741 139535991158528 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.6610012054443359, loss=2.390753984451294
I0216 12:44:23.376980 139535982765824 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.2368197441101074, loss=2.3789308071136475
I0216 12:45:39.021748 139535991158528 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.9119992256164551, loss=2.401362419128418
I0216 12:46:54.726197 139535982765824 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.5750810503959656, loss=2.3337502479553223
I0216 12:48:10.849506 139535991158528 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.7989158630371094, loss=2.3300859928131104
I0216 12:49:27.883476 139535982765824 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.700742781162262, loss=2.282984972000122
I0216 12:50:49.956538 139535991158528 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.8383224010467529, loss=2.348607063293457
I0216 12:52:12.072609 139535982765824 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.1322046518325806, loss=2.435025930404663
I0216 12:53:35.492672 139535991158528 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.5605743527412415, loss=2.3318328857421875
I0216 12:54:58.152248 139535982765824 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.7394280433654785, loss=2.3640518188476562
I0216 12:56:23.864798 139535991158528 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.7911283373832703, loss=2.2809622287750244
I0216 12:57:39.459950 139535982765824 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.6506266593933105, loss=2.233546495437622
I0216 12:58:55.218533 139535991158528 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.7698324918746948, loss=2.327808141708374
I0216 13:00:10.954143 139535982765824 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.0002769231796265, loss=2.2929863929748535
I0216 13:01:26.736757 139535991158528 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.5820799469947815, loss=2.2968761920928955
I0216 13:02:42.404042 139535982765824 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.6590912342071533, loss=2.266812801361084
I0216 13:04:02.437664 139535991158528 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.5460468530654907, loss=2.2340924739837646
I0216 13:05:24.807409 139535982765824 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.32290518283844, loss=2.181546926498413
I0216 13:05:55.981299 139646656866112 spec.py:321] Evaluating on the training split.
I0216 13:06:49.188330 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 13:07:40.054968 139646656866112 spec.py:349] Evaluating on the test split.
I0216 13:08:06.254392 139646656866112 submission_runner.py:408] Time since start: 9565.22s, 	Step: 11039, 	{'train/ctc_loss': Array(1.1786427, dtype=float32), 'train/wer': 0.36860990982575104, 'validation/ctc_loss': Array(1.3118448, dtype=float32), 'validation/wer': 0.37750658930071346, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.9740689, dtype=float32), 'test/wer': 0.31235147157394433, 'test/num_examples': 2472, 'score': 8676.491871833801, 'total_duration': 9565.224183321, 'accumulated_submission_time': 8676.491871833801, 'accumulated_eval_time': 887.9946537017822, 'accumulated_logging_time': 0.2738659381866455}
I0216 13:08:06.293023 139535991158528 logging_writer.py:48] [11039] accumulated_eval_time=887.994654, accumulated_logging_time=0.273866, accumulated_submission_time=8676.491872, global_step=11039, preemption_count=0, score=8676.491872, test/ctc_loss=0.9740688800811768, test/num_examples=2472, test/wer=0.312351, total_duration=9565.224183, train/ctc_loss=1.178642749786377, train/wer=0.368610, validation/ctc_loss=1.311844825744629, validation/num_examples=5348, validation/wer=0.377507
I0216 13:08:53.035173 139535982765824 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.7212825417518616, loss=2.2749340534210205
I0216 13:10:08.617177 139535991158528 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.5662747621536255, loss=2.2690131664276123
I0216 13:11:24.237933 139535982765824 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.755012035369873, loss=2.259883165359497
I0216 13:12:43.530653 139535991158528 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.02419912815094, loss=2.2717297077178955
I0216 13:13:59.105596 139535982765824 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.588080108165741, loss=2.226361036300659
I0216 13:15:14.854970 139535991158528 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.138846516609192, loss=2.1622698307037354
I0216 13:16:30.651733 139535982765824 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.5263289213180542, loss=2.157029390335083
I0216 13:17:46.387194 139535991158528 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.9280150532722473, loss=2.204756021499634
I0216 13:19:03.897847 139535982765824 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.5056254267692566, loss=2.206974506378174
I0216 13:20:26.386006 139535991158528 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.700689435005188, loss=2.180854320526123
I0216 13:21:48.357474 139535982765824 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.2426340579986572, loss=2.250728130340576
I0216 13:23:10.413381 139535991158528 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.7514235377311707, loss=2.118727207183838
I0216 13:24:32.527742 139535982765824 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.8524521589279175, loss=2.1318936347961426
I0216 13:25:55.666097 139535991158528 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.5720305442810059, loss=2.1734838485717773
I0216 13:27:11.413173 139535982765824 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.9030835628509521, loss=2.120652437210083
I0216 13:28:27.120537 139535991158528 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.7473081946372986, loss=2.218351364135742
I0216 13:29:42.735893 139535982765824 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.697775661945343, loss=2.139054298400879
I0216 13:30:58.558553 139535991158528 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.61295485496521, loss=2.129694938659668
I0216 13:32:06.399060 139646656866112 spec.py:321] Evaluating on the training split.
I0216 13:32:58.658724 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 13:33:49.933869 139646656866112 spec.py:349] Evaluating on the test split.
I0216 13:34:16.131478 139646656866112 submission_runner.py:408] Time since start: 11135.10s, 	Step: 12891, 	{'train/ctc_loss': Array(1.1353468, dtype=float32), 'train/wer': 0.3522497808195933, 'validation/ctc_loss': Array(1.1930368, dtype=float32), 'validation/wer': 0.3462834413045367, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.8644295, dtype=float32), 'test/wer': 0.2827778116304105, 'test/num_examples': 2472, 'score': 10116.507513999939, 'total_duration': 11135.100909948349, 'accumulated_submission_time': 10116.507513999939, 'accumulated_eval_time': 1017.7202196121216, 'accumulated_logging_time': 0.32880735397338867}
I0216 13:34:16.172876 139535991158528 logging_writer.py:48] [12891] accumulated_eval_time=1017.720220, accumulated_logging_time=0.328807, accumulated_submission_time=10116.507514, global_step=12891, preemption_count=0, score=10116.507514, test/ctc_loss=0.8644294738769531, test/num_examples=2472, test/wer=0.282778, total_duration=11135.100910, train/ctc_loss=1.13534677028656, train/wer=0.352250, validation/ctc_loss=1.1930367946624756, validation/num_examples=5348, validation/wer=0.346283
I0216 13:34:23.820507 139535982765824 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.6459562182426453, loss=2.115389585494995
I0216 13:35:39.404408 139535991158528 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.9073275327682495, loss=2.081260919570923
I0216 13:36:55.201210 139535982765824 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.4563989639282227, loss=2.1553735733032227
I0216 13:38:11.419404 139535991158528 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.8423118591308594, loss=2.1011946201324463
I0216 13:39:27.220092 139535982765824 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.7070790529251099, loss=2.0954673290252686
I0216 13:40:49.315299 139535991158528 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.0591293573379517, loss=2.128495931625366
I0216 13:42:04.988329 139535982765824 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.9947137236595154, loss=2.11570143699646
I0216 13:43:20.835774 139535991158528 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.7198215126991272, loss=2.2325589656829834
I0216 13:44:36.613284 139535982765824 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.562652051448822, loss=2.1469404697418213
I0216 13:45:52.453819 139535991158528 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.9031014442443848, loss=2.1300830841064453
I0216 13:47:08.255336 139535982765824 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.0336281061172485, loss=2.1656546592712402
I0216 13:48:26.248568 139535991158528 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.7986820936203003, loss=2.128586530685425
I0216 13:49:47.915298 139535982765824 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.8948343992233276, loss=2.1220703125
I0216 13:51:10.355497 139535991158528 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.7101128101348877, loss=2.1088333129882812
I0216 13:52:32.547937 139535982765824 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.0185729265213013, loss=2.1404383182525635
I0216 13:53:54.251676 139535991158528 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.6491745710372925, loss=2.1779446601867676
I0216 13:55:14.632809 139535991158528 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.513413667678833, loss=2.0602643489837646
I0216 13:56:30.457058 139535982765824 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.9008967876434326, loss=2.061307668685913
I0216 13:57:46.386087 139535991158528 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.0537842512130737, loss=2.1140494346618652
I0216 13:58:16.463537 139646656866112 spec.py:321] Evaluating on the training split.
I0216 13:59:08.386776 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 13:59:59.044574 139646656866112 spec.py:349] Evaluating on the test split.
I0216 14:00:24.997908 139646656866112 submission_runner.py:408] Time since start: 12703.97s, 	Step: 14741, 	{'train/ctc_loss': Array(1.091014, dtype=float32), 'train/wer': 0.33997963444395773, 'validation/ctc_loss': Array(1.137504, dtype=float32), 'validation/wer': 0.3313090744084111, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.8089821, dtype=float32), 'test/wer': 0.2671785184733817, 'test/num_examples': 2472, 'score': 11556.709911346436, 'total_duration': 12703.967337846756, 'accumulated_submission_time': 11556.709911346436, 'accumulated_eval_time': 1146.2477378845215, 'accumulated_logging_time': 0.38587427139282227}
I0216 14:00:25.073529 139535991158528 logging_writer.py:48] [14741] accumulated_eval_time=1146.247738, accumulated_logging_time=0.385874, accumulated_submission_time=11556.709911, global_step=14741, preemption_count=0, score=11556.709911, test/ctc_loss=0.8089820742607117, test/num_examples=2472, test/wer=0.267179, total_duration=12703.967338, train/ctc_loss=1.09101402759552, train/wer=0.339980, validation/ctc_loss=1.137503981590271, validation/num_examples=5348, validation/wer=0.331309
I0216 14:01:10.293313 139535982765824 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.6537261605262756, loss=2.0069000720977783
I0216 14:02:26.110783 139535991158528 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.6813356876373291, loss=2.0830540657043457
I0216 14:03:42.072263 139535982765824 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.1304620504379272, loss=2.0840420722961426
I0216 14:04:57.780354 139535991158528 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.900943398475647, loss=2.0819997787475586
I0216 14:06:13.653671 139535982765824 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.9030213356018066, loss=2.0287373065948486
I0216 14:07:34.420675 139535991158528 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.5633662343025208, loss=2.0730855464935303
I0216 14:08:56.614369 139535982765824 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.1045482158660889, loss=2.0387163162231445
I0216 14:10:19.403971 139535991158528 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.743608832359314, loss=2.067439079284668
I0216 14:11:35.707239 139535982765824 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.9476808905601501, loss=2.0399837493896484
I0216 14:12:51.509603 139535991158528 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.5293267369270325, loss=2.0582761764526367
I0216 14:14:07.425281 139535982765824 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.8284812569618225, loss=2.068908452987671
I0216 14:15:23.367027 139535991158528 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.0068440437316895, loss=2.0850603580474854
I0216 14:16:39.318781 139535982765824 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.063021183013916, loss=2.034979820251465
I0216 14:18:01.593590 139535991158528 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.8149728178977966, loss=2.0666956901550293
I0216 14:19:25.494083 139535982765824 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.5917757749557495, loss=2.0374233722686768
I0216 14:20:48.244341 139535991158528 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.7626425623893738, loss=2.0682778358459473
I0216 14:22:11.369771 139535982765824 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.976517379283905, loss=2.0364861488342285
I0216 14:23:35.815523 139535991158528 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.582005500793457, loss=2.0344018936157227
I0216 14:24:25.512063 139646656866112 spec.py:321] Evaluating on the training split.
I0216 14:25:18.997191 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 14:26:10.510033 139646656866112 spec.py:349] Evaluating on the test split.
I0216 14:26:36.914435 139646656866112 submission_runner.py:408] Time since start: 14275.88s, 	Step: 16567, 	{'train/ctc_loss': Array(1.0301901, dtype=float32), 'train/wer': 0.32646981223341104, 'validation/ctc_loss': Array(1.10055, dtype=float32), 'validation/wer': 0.3198200372669608, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.77649456, dtype=float32), 'test/wer': 0.2576117644669226, 'test/num_examples': 2472, 'score': 12997.059435367584, 'total_duration': 14275.884579896927, 'accumulated_submission_time': 12997.059435367584, 'accumulated_eval_time': 1277.6439950466156, 'accumulated_logging_time': 0.4783322811126709}
I0216 14:26:36.949171 139535991158528 logging_writer.py:48] [16567] accumulated_eval_time=1277.643995, accumulated_logging_time=0.478332, accumulated_submission_time=12997.059435, global_step=16567, preemption_count=0, score=12997.059435, test/ctc_loss=0.776494562625885, test/num_examples=2472, test/wer=0.257612, total_duration=14275.884580, train/ctc_loss=1.030190110206604, train/wer=0.326470, validation/ctc_loss=1.1005500555038452, validation/num_examples=5348, validation/wer=0.319820
I0216 14:27:02.664229 139535982765824 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.6231996417045593, loss=2.004833698272705
I0216 14:28:18.408993 139535991158528 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.8710125684738159, loss=1.9687851667404175
I0216 14:29:34.456887 139535982765824 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.7291845679283142, loss=2.0598878860473633
I0216 14:30:50.199830 139535991158528 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.5476905703544617, loss=1.9753867387771606
I0216 14:32:06.046004 139535982765824 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.7091250419616699, loss=1.9949405193328857
I0216 14:33:21.799318 139535991158528 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.6984801292419434, loss=1.9968510866165161
I0216 14:34:37.541981 139535982765824 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.0462939739227295, loss=2.0556838512420654
I0216 14:35:58.389065 139535991158528 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.7165522575378418, loss=2.0136919021606445
I0216 14:37:20.380521 139535982765824 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.5002393126487732, loss=1.988459825515747
I0216 14:38:42.371870 139535991158528 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.7887223362922668, loss=1.943710207939148
I0216 14:40:02.025065 139535991158528 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.6140047311782837, loss=1.9892668724060059
I0216 14:41:17.939848 139535982765824 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.166002869606018, loss=2.028468132019043
I0216 14:42:33.902659 139535991158528 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.7121152281761169, loss=1.9786394834518433
I0216 14:43:49.783451 139535982765824 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.6803032159805298, loss=2.0234570503234863
I0216 14:45:05.974150 139535991158528 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.6031041145324707, loss=2.019869565963745
I0216 14:46:22.012823 139535982765824 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.5804751515388489, loss=1.9828400611877441
I0216 14:47:43.363386 139535991158528 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.3374614715576172, loss=2.060220956802368
I0216 14:49:06.104905 139535982765824 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.5515182614326477, loss=1.962029218673706
I0216 14:50:27.992848 139535991158528 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.6241440773010254, loss=1.9986034631729126
I0216 14:50:37.412317 139646656866112 spec.py:321] Evaluating on the training split.
I0216 14:51:30.027050 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 14:52:21.173637 139646656866112 spec.py:349] Evaluating on the test split.
I0216 14:52:47.356512 139646656866112 submission_runner.py:408] Time since start: 15846.33s, 	Step: 18413, 	{'train/ctc_loss': Array(1.1991059, dtype=float32), 'train/wer': 0.3634266049870247, 'validation/ctc_loss': Array(1.2338159, dtype=float32), 'validation/wer': 0.35353408575262846, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.8797922, dtype=float32), 'test/wer': 0.281091950520992, 'test/num_examples': 2472, 'score': 14437.434829473495, 'total_duration': 15846.326422691345, 'accumulated_submission_time': 14437.434829473495, 'accumulated_eval_time': 1407.5817940235138, 'accumulated_logging_time': 0.5285263061523438}
I0216 14:52:47.391098 139535991158528 logging_writer.py:48] [18413] accumulated_eval_time=1407.581794, accumulated_logging_time=0.528526, accumulated_submission_time=14437.434829, global_step=18413, preemption_count=0, score=14437.434829, test/ctc_loss=0.8797922134399414, test/num_examples=2472, test/wer=0.281092, total_duration=15846.326423, train/ctc_loss=1.1991058588027954, train/wer=0.363427, validation/ctc_loss=1.2338159084320068, validation/num_examples=5348, validation/wer=0.353534
I0216 14:53:53.807172 139535982765824 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.0240174531936646, loss=2.012113571166992
I0216 14:55:12.897272 139535991158528 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.6734064221382141, loss=1.9541906118392944
I0216 14:56:28.632860 139535982765824 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.7820144295692444, loss=1.9879541397094727
I0216 14:57:44.343808 139535991158528 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.572624921798706, loss=1.9647639989852905
I0216 14:59:00.069310 139535982765824 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.7410747408866882, loss=1.9302555322647095
I0216 15:00:15.863780 139535991158528 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.7460172176361084, loss=2.0433192253112793
I0216 15:01:31.897177 139535982765824 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.015913486480713, loss=1.9761555194854736
I0216 15:02:53.717761 139535991158528 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.7732462882995605, loss=1.9945387840270996
I0216 15:04:16.412589 139535982765824 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.9811272621154785, loss=1.979950189590454
I0216 15:05:37.845136 139535991158528 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.7325721383094788, loss=2.009162664413452
I0216 15:06:59.607615 139535982765824 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.7119279503822327, loss=1.9401426315307617
I0216 15:08:24.119887 139535991158528 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.9337177872657776, loss=1.946651816368103
I0216 15:09:39.854719 139535982765824 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.7741991281509399, loss=1.9000074863433838
I0216 15:10:55.708135 139535991158528 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.9231241345405579, loss=1.9385133981704712
I0216 15:12:11.593042 139535982765824 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.6484752297401428, loss=2.017516851425171
I0216 15:13:27.395204 139535991158528 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.8976197838783264, loss=1.9457252025604248
I0216 15:14:43.276075 139535982765824 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.9724290370941162, loss=1.9288594722747803
I0216 15:16:03.144508 139535991158528 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.6929891109466553, loss=1.9541195631027222
I0216 15:16:48.196886 139646656866112 spec.py:321] Evaluating on the training split.
I0216 15:17:40.461276 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 15:18:32.723819 139646656866112 spec.py:349] Evaluating on the test split.
I0216 15:18:59.279375 139646656866112 submission_runner.py:408] Time since start: 17418.25s, 	Step: 20256, 	{'train/ctc_loss': Array(0.88449436, dtype=float32), 'train/wer': 0.288827478657715, 'validation/ctc_loss': Array(1.0200983, dtype=float32), 'validation/wer': 0.3036098747791498, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.7080025, dtype=float32), 'test/wer': 0.2387626185688461, 'test/num_examples': 2472, 'score': 15878.150474071503, 'total_duration': 17418.24999141693, 'accumulated_submission_time': 15878.150474071503, 'accumulated_eval_time': 1538.6586077213287, 'accumulated_logging_time': 0.5804932117462158}
I0216 15:18:59.314926 139535991158528 logging_writer.py:48] [20256] accumulated_eval_time=1538.658608, accumulated_logging_time=0.580493, accumulated_submission_time=15878.150474, global_step=20256, preemption_count=0, score=15878.150474, test/ctc_loss=0.708002507686615, test/num_examples=2472, test/wer=0.238763, total_duration=17418.249991, train/ctc_loss=0.8844943642616272, train/wer=0.288827, validation/ctc_loss=1.020098328590393, validation/num_examples=5348, validation/wer=0.303610
I0216 15:19:33.571883 139535982765824 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.1731289625167847, loss=2.0331833362579346
I0216 15:20:49.394145 139535991158528 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.6914380192756653, loss=2.0113718509674072
I0216 15:22:05.302962 139535982765824 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.6892476081848145, loss=1.98539400100708
I0216 15:23:24.902047 139535991158528 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.8989006876945496, loss=1.9939684867858887
I0216 15:24:40.785799 139535982765824 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.8471918106079102, loss=1.9239546060562134
I0216 15:25:56.587798 139535991158528 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.006212830543518, loss=1.9871141910552979
I0216 15:27:12.394710 139535982765824 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.9032701849937439, loss=1.993283748626709
I0216 15:28:28.384839 139535991158528 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.7830394506454468, loss=1.941099762916565
I0216 15:29:44.272992 139535982765824 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.4874025583267212, loss=1.9378976821899414
I0216 15:31:04.274422 139535991158528 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.5374303460121155, loss=1.9795758724212646
I0216 15:32:26.892033 139535982765824 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.8467962145805359, loss=1.9636014699935913
I0216 15:33:49.089575 139535991158528 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.0054727792739868, loss=1.9730583429336548
I0216 15:35:11.477191 139535982765824 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.7485140562057495, loss=1.923552393913269
I0216 15:36:34.062227 139535991158528 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.9395734667778015, loss=1.9209678173065186
I0216 15:37:55.146991 139535991158528 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.8684414625167847, loss=1.9266835451126099
I0216 15:39:11.097535 139535982765824 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.6648872494697571, loss=1.8938018083572388
I0216 15:40:27.072132 139535991158528 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.7531136274337769, loss=1.9322162866592407
I0216 15:41:43.002456 139535982765824 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.6375026106834412, loss=1.8974206447601318
I0216 15:42:58.726384 139535991158528 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.6569458246231079, loss=1.8519542217254639
I0216 15:42:59.983592 139646656866112 spec.py:321] Evaluating on the training split.
I0216 15:43:52.904912 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 15:44:44.640919 139646656866112 spec.py:349] Evaluating on the test split.
I0216 15:45:11.032615 139646656866112 submission_runner.py:408] Time since start: 18990.00s, 	Step: 22103, 	{'train/ctc_loss': Array(0.90416443, dtype=float32), 'train/wer': 0.2953098019868589, 'validation/ctc_loss': Array(0.9769954, dtype=float32), 'validation/wer': 0.2892244417196868, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.67560107, dtype=float32), 'test/wer': 0.2254585339101822, 'test/num_examples': 2472, 'score': 17318.73151898384, 'total_duration': 18990.002960205078, 'accumulated_submission_time': 17318.73151898384, 'accumulated_eval_time': 1669.7016806602478, 'accumulated_logging_time': 0.6308321952819824}
I0216 15:45:11.068466 139535991158528 logging_writer.py:48] [22103] accumulated_eval_time=1669.701681, accumulated_logging_time=0.630832, accumulated_submission_time=17318.731519, global_step=22103, preemption_count=0, score=17318.731519, test/ctc_loss=0.675601065158844, test/num_examples=2472, test/wer=0.225459, total_duration=18990.002960, train/ctc_loss=0.9041644334793091, train/wer=0.295310, validation/ctc_loss=0.9769954085350037, validation/num_examples=5348, validation/wer=0.289224
I0216 15:46:25.072571 139535982765824 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.0011491775512695, loss=1.9303385019302368
I0216 15:47:40.800009 139535991158528 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.9431743621826172, loss=1.9572033882141113
I0216 15:48:56.569435 139535982765824 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.6072994470596313, loss=1.958103895187378
I0216 15:50:12.370044 139535991158528 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.542557954788208, loss=2.0011725425720215
I0216 15:51:30.505779 139535982765824 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.5867682695388794, loss=1.93342125415802
I0216 15:52:53.011020 139535991158528 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.0520875453948975, loss=1.870078444480896
I0216 15:54:08.786488 139535982765824 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.6292341351509094, loss=1.94369375705719
I0216 15:55:24.547623 139535991158528 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.6927154064178467, loss=1.939215898513794
I0216 15:56:40.225054 139535982765824 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.9820742607116699, loss=1.88409423828125
I0216 15:57:56.069802 139535991158528 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.7048829793930054, loss=1.898707628250122
I0216 15:59:11.956269 139535982765824 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.7003616690635681, loss=1.9016599655151367
I0216 16:00:32.268433 139535991158528 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.9785137176513672, loss=1.894845724105835
I0216 16:01:54.341560 139535982765824 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.7860100865364075, loss=1.9111402034759521
I0216 16:03:16.536408 139535991158528 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.5141233801841736, loss=1.9254299402236938
I0216 16:04:37.994942 139535982765824 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.5169927477836609, loss=1.919359564781189
I0216 16:06:02.543032 139535991158528 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.8412578105926514, loss=1.8876769542694092
I0216 16:07:18.283852 139535982765824 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.8783788681030273, loss=1.8849941492080688
I0216 16:08:34.415072 139535991158528 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.7242739200592041, loss=1.7969170808792114
I0216 16:09:11.234668 139646656866112 spec.py:321] Evaluating on the training split.
I0216 16:10:03.230816 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 16:10:54.955203 139646656866112 spec.py:349] Evaluating on the test split.
I0216 16:11:21.262159 139646656866112 submission_runner.py:408] Time since start: 20560.23s, 	Step: 23950, 	{'train/ctc_loss': Array(0.83565265, dtype=float32), 'train/wer': 0.2703343074093714, 'validation/ctc_loss': Array(0.94258255, dtype=float32), 'validation/wer': 0.2817903588634542, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.64889586, dtype=float32), 'test/wer': 0.21910100948550768, 'test/num_examples': 2472, 'score': 18758.80566763878, 'total_duration': 20560.23240017891, 'accumulated_submission_time': 18758.80566763878, 'accumulated_eval_time': 1799.7231032848358, 'accumulated_logging_time': 0.6856436729431152}
I0216 16:11:21.297487 139535991158528 logging_writer.py:48] [23950] accumulated_eval_time=1799.723103, accumulated_logging_time=0.685644, accumulated_submission_time=18758.805668, global_step=23950, preemption_count=0, score=18758.805668, test/ctc_loss=0.6488958597183228, test/num_examples=2472, test/wer=0.219101, total_duration=20560.232400, train/ctc_loss=0.8356526494026184, train/wer=0.270334, validation/ctc_loss=0.9425825476646423, validation/num_examples=5348, validation/wer=0.281790
I0216 16:11:59.856919 139535982765824 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.9350783824920654, loss=1.8588236570358276
I0216 16:13:15.597340 139535991158528 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.9515778422355652, loss=1.8933134078979492
I0216 16:14:31.445981 139535982765824 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.5396429300308228, loss=1.8295868635177612
I0216 16:15:47.207269 139535991158528 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.9455450773239136, loss=1.8889988660812378
I0216 16:17:02.986747 139535982765824 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.741123378276825, loss=1.898864507675171
I0216 16:18:23.898789 139535991158528 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.5903552174568176, loss=1.7738964557647705
I0216 16:19:45.709349 139535982765824 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.6430234313011169, loss=1.9215654134750366
I0216 16:21:08.649425 139535991158528 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.7174846529960632, loss=1.9112718105316162
I0216 16:22:29.112333 139535991158528 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.5905264616012573, loss=1.869511604309082
I0216 16:23:45.036211 139535982765824 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.519829273223877, loss=1.7937062978744507
I0216 16:25:01.230857 139535991158528 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.6159111857414246, loss=1.899653434753418
I0216 16:26:17.264292 139535982765824 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.6958839893341064, loss=1.7984559535980225
I0216 16:27:33.259834 139535991158528 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.6288899183273315, loss=1.820505976676941
I0216 16:28:49.829227 139535982765824 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.6624283194541931, loss=1.8113149404525757
I0216 16:30:12.072010 139535991158528 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.5339245796203613, loss=1.7683953046798706
I0216 16:31:34.991073 139535982765824 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.7470896244049072, loss=1.8808019161224365
I0216 16:32:57.852490 139535991158528 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.0131934881210327, loss=1.878548264503479
I0216 16:34:19.898682 139535982765824 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.7581015229225159, loss=1.8459184169769287
I0216 16:35:21.331124 139646656866112 spec.py:321] Evaluating on the training split.
I0216 16:36:15.484559 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 16:37:06.872364 139646656866112 spec.py:349] Evaluating on the test split.
I0216 16:37:33.058103 139646656866112 submission_runner.py:408] Time since start: 22132.03s, 	Step: 25774, 	{'train/ctc_loss': Array(0.83382195, dtype=float32), 'train/wer': 0.2674261006030476, 'validation/ctc_loss': Array(0.90188086, dtype=float32), 'validation/wer': 0.27060061596686524, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.62141186, dtype=float32), 'test/wer': 0.20888428493083908, 'test/num_examples': 2472, 'score': 20198.753033638, 'total_duration': 22132.027775764465, 'accumulated_submission_time': 20198.753033638, 'accumulated_eval_time': 1931.4437320232391, 'accumulated_logging_time': 0.7362205982208252}
I0216 16:37:33.096526 139535991158528 logging_writer.py:48] [25774] accumulated_eval_time=1931.443732, accumulated_logging_time=0.736221, accumulated_submission_time=20198.753034, global_step=25774, preemption_count=0, score=20198.753034, test/ctc_loss=0.6214118599891663, test/num_examples=2472, test/wer=0.208884, total_duration=22132.027776, train/ctc_loss=0.8338219523429871, train/wer=0.267426, validation/ctc_loss=0.9018808603286743, validation/num_examples=5348, validation/wer=0.270601
I0216 16:37:53.558770 139535982765824 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.6724137663841248, loss=1.833942174911499
I0216 16:39:09.371117 139535991158528 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.6306118369102478, loss=1.8011960983276367
I0216 16:40:25.270389 139535982765824 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.05640709400177, loss=1.870409369468689
I0216 16:41:41.175684 139535991158528 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.5374897718429565, loss=1.7537755966186523
I0216 16:42:57.329499 139535982765824 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.7043143510818481, loss=1.821936011314392
I0216 16:44:13.265088 139535991158528 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.5156593322753906, loss=1.8297144174575806
I0216 16:45:29.106599 139535982765824 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.46875429153442383, loss=1.7440710067749023
I0216 16:46:45.801529 139535991158528 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.7395375370979309, loss=1.79718816280365
I0216 16:48:08.331404 139535982765824 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.48272380232810974, loss=1.7839311361312866
I0216 16:49:30.914161 139535991158528 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.6116471290588379, loss=1.8175381422042847
I0216 16:50:54.811948 139535991158528 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.9767289161682129, loss=1.7992799282073975
I0216 16:52:10.705058 139535982765824 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.5634514093399048, loss=1.789628028869629
I0216 16:53:26.586748 139535991158528 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.5858449935913086, loss=1.7484006881713867
I0216 16:54:42.593415 139535982765824 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.5453530550003052, loss=1.859047770500183
I0216 16:55:58.578034 139535991158528 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.6084374785423279, loss=1.7994590997695923
I0216 16:57:14.396750 139535982765824 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.73477703332901, loss=1.8493566513061523
I0216 16:58:34.369039 139535991158528 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.5760980248451233, loss=1.7944896221160889
I0216 16:59:56.310679 139535982765824 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.8474094867706299, loss=1.7618411779403687
I0216 17:01:18.288523 139535991158528 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.7073307037353516, loss=1.8003023862838745
I0216 17:01:33.226097 139646656866112 spec.py:321] Evaluating on the training split.
I0216 17:02:26.962708 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 17:03:18.935166 139646656866112 spec.py:349] Evaluating on the test split.
I0216 17:03:44.800148 139646656866112 submission_runner.py:408] Time since start: 23703.77s, 	Step: 27620, 	{'train/ctc_loss': Array(0.7256441, dtype=float32), 'train/wer': 0.24243501468424766, 'validation/ctc_loss': Array(0.8796342, dtype=float32), 'validation/wer': 0.26477886017165975, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6013053, dtype=float32), 'test/wer': 0.20441573741189853, 'test/num_examples': 2472, 'score': 21638.790019273758, 'total_duration': 23703.770726919174, 'accumulated_submission_time': 21638.790019273758, 'accumulated_eval_time': 2063.012087583542, 'accumulated_logging_time': 0.7946665287017822}
I0216 17:03:44.841481 139535991158528 logging_writer.py:48] [27620] accumulated_eval_time=2063.012088, accumulated_logging_time=0.794667, accumulated_submission_time=21638.790019, global_step=27620, preemption_count=0, score=21638.790019, test/ctc_loss=0.6013053059577942, test/num_examples=2472, test/wer=0.204416, total_duration=23703.770727, train/ctc_loss=0.7256441116333008, train/wer=0.242435, validation/ctc_loss=0.8796342015266418, validation/num_examples=5348, validation/wer=0.264779
I0216 17:04:46.092872 139535982765824 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.8435534834861755, loss=1.7619996070861816
I0216 17:06:01.923551 139535991158528 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.7515245079994202, loss=1.772024393081665
I0216 17:07:21.496668 139535991158528 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.5711673498153687, loss=1.8034212589263916
I0216 17:08:37.536623 139535982765824 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.5426206588745117, loss=1.7974982261657715
I0216 17:09:53.536066 139535991158528 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.7447726130485535, loss=1.7891291379928589
I0216 17:11:09.465141 139535982765824 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.7575212121009827, loss=1.852518916130066
I0216 17:12:25.504461 139535991158528 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.543899416923523, loss=1.7883960008621216
I0216 17:13:44.109467 139535982765824 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.9553991556167603, loss=1.8187255859375
I0216 17:15:06.565340 139535991158528 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.5503792762756348, loss=1.7925841808319092
I0216 17:16:28.813079 139535982765824 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.6123296618461609, loss=1.7908360958099365
I0216 17:17:51.399456 139535991158528 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.6957226395606995, loss=1.7853379249572754
I0216 17:19:14.187323 139535982765824 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.7710604667663574, loss=1.7668675184249878
I0216 17:20:35.753968 139535991158528 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.719538688659668, loss=1.7027240991592407
I0216 17:21:51.664562 139535982765824 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.8248612284660339, loss=1.811102271080017
I0216 17:23:07.556200 139535991158528 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.749753475189209, loss=1.774531602859497
I0216 17:24:23.413005 139535982765824 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.8600160479545593, loss=1.7705416679382324
I0216 17:25:39.409298 139535991158528 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.7882124781608582, loss=1.7847024202346802
I0216 17:26:55.430141 139535982765824 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.7256404161453247, loss=1.7950109243392944
I0216 17:27:45.429338 139646656866112 spec.py:321] Evaluating on the training split.
I0216 17:28:38.857451 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 17:29:29.992429 139646656866112 spec.py:349] Evaluating on the test split.
I0216 17:29:56.208686 139646656866112 submission_runner.py:408] Time since start: 25275.18s, 	Step: 29464, 	{'train/ctc_loss': Array(0.7058413, dtype=float32), 'train/wer': 0.2416801166096091, 'validation/ctc_loss': Array(0.8496082, dtype=float32), 'validation/wer': 0.2589764136825743, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5760872, dtype=float32), 'test/wer': 0.19574269290922755, 'test/num_examples': 2472, 'score': 23079.289704084396, 'total_duration': 25275.1791806221, 'accumulated_submission_time': 23079.289704084396, 'accumulated_eval_time': 2193.7856407165527, 'accumulated_logging_time': 0.8524501323699951}
I0216 17:29:56.249100 139535991158528 logging_writer.py:48] [29464] accumulated_eval_time=2193.785641, accumulated_logging_time=0.852450, accumulated_submission_time=23079.289704, global_step=29464, preemption_count=0, score=23079.289704, test/ctc_loss=0.5760871767997742, test/num_examples=2472, test/wer=0.195743, total_duration=25275.179181, train/ctc_loss=0.7058413028717041, train/wer=0.241680, validation/ctc_loss=0.8496081829071045, validation/num_examples=5348, validation/wer=0.258976
I0216 17:30:24.267440 139535982765824 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.8925212025642395, loss=1.82584810256958
I0216 17:31:40.081468 139535991158528 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.6317857503890991, loss=1.7329243421554565
I0216 17:32:56.271174 139535982765824 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.8780535459518433, loss=1.8139723539352417
I0216 17:34:12.147797 139535991158528 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.6316626071929932, loss=1.7641791105270386
I0216 17:35:31.583379 139535991158528 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.8346552848815918, loss=1.787634015083313
I0216 17:36:47.412749 139535982765824 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.6840506196022034, loss=1.7570894956588745
I0216 17:38:03.445410 139535991158528 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.6650365591049194, loss=1.7742743492126465
I0216 17:39:19.258411 139535982765824 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.5545476078987122, loss=1.821563482284546
I0216 17:40:35.080817 139535991158528 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.5265660285949707, loss=1.731419324874878
I0216 17:41:50.961636 139535982765824 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.5346506237983704, loss=1.7333245277404785
I0216 17:43:06.836069 139535991158528 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.8478855490684509, loss=1.8348662853240967
I0216 17:44:27.047608 139535982765824 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.6876113414764404, loss=1.7826402187347412
I0216 17:45:48.900599 139535991158528 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.6359146237373352, loss=1.7681159973144531
I0216 17:47:10.928780 139535982765824 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.6663405895233154, loss=1.7880467176437378
I0216 17:48:37.121005 139535991158528 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.6292365193367004, loss=1.7265456914901733
I0216 17:49:52.990520 139535982765824 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.8152507543563843, loss=1.74064302444458
I0216 17:51:08.914285 139535991158528 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.982207179069519, loss=1.728621482849121
I0216 17:52:24.867620 139535982765824 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.5484023690223694, loss=1.7534092664718628
I0216 17:53:40.790659 139535991158528 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.6081240177154541, loss=1.6958718299865723
I0216 17:53:56.371073 139646656866112 spec.py:321] Evaluating on the training split.
I0216 17:54:48.137377 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 17:55:39.935663 139646656866112 spec.py:349] Evaluating on the test split.
I0216 17:56:06.350791 139646656866112 submission_runner.py:408] Time since start: 26845.32s, 	Step: 31322, 	{'train/ctc_loss': Array(0.71590084, dtype=float32), 'train/wer': 0.24235520597800395, 'validation/ctc_loss': Array(0.8296148, dtype=float32), 'validation/wer': 0.24962105486739333, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5572826, dtype=float32), 'test/wer': 0.19068510958097212, 'test/num_examples': 2472, 'score': 24519.32209968567, 'total_duration': 26845.321516752243, 'accumulated_submission_time': 24519.32209968567, 'accumulated_eval_time': 2323.7597975730896, 'accumulated_logging_time': 0.9073889255523682}
I0216 17:56:06.389644 139535991158528 logging_writer.py:48] [31322] accumulated_eval_time=2323.759798, accumulated_logging_time=0.907389, accumulated_submission_time=24519.322100, global_step=31322, preemption_count=0, score=24519.322100, test/ctc_loss=0.5572826266288757, test/num_examples=2472, test/wer=0.190685, total_duration=26845.321517, train/ctc_loss=0.7159008383750916, train/wer=0.242355, validation/ctc_loss=0.8296148180961609, validation/num_examples=5348, validation/wer=0.249621
I0216 17:57:06.146897 139535982765824 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.7904003262519836, loss=1.7697194814682007
I0216 17:58:21.997336 139535991158528 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.6428686380386353, loss=1.797017216682434
I0216 17:59:37.905313 139535982765824 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.6909441351890564, loss=1.7174793481826782
I0216 18:00:53.756695 139535991158528 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.6411222815513611, loss=1.7870392799377441
I0216 18:02:09.715917 139535982765824 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.686555802822113, loss=1.7549382448196411
I0216 18:03:30.841432 139535991158528 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.8704808354377747, loss=1.7438762187957764
I0216 18:04:51.836276 139535991158528 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.5940178036689758, loss=1.7977007627487183
I0216 18:06:07.971686 139535982765824 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.6758050322532654, loss=1.670376181602478
I0216 18:07:23.779974 139535991158528 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.5126352310180664, loss=1.7436695098876953
I0216 18:08:39.761954 139535982765824 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.8801637887954712, loss=1.7185016870498657
I0216 18:09:55.698667 139535991158528 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.6652185916900635, loss=1.754828929901123
I0216 18:11:11.825567 139535982765824 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.745562732219696, loss=1.7197411060333252
I0216 18:12:27.888951 139535991158528 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.6173228025436401, loss=1.7218098640441895
I0216 18:13:46.887979 139535982765824 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.5973719954490662, loss=1.6873902082443237
I0216 18:15:09.610767 139535991158528 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.6157814264297485, loss=1.8202632665634155
I0216 18:16:31.759734 139535982765824 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.5206734538078308, loss=1.7260382175445557
I0216 18:17:54.191161 139535991158528 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.6703654527664185, loss=1.742422103881836
I0216 18:19:10.184675 139535982765824 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.9016410708427429, loss=1.7698358297348022
I0216 18:20:06.778841 139646656866112 spec.py:321] Evaluating on the training split.
I0216 18:21:10.521970 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 18:22:01.917134 139646656866112 spec.py:349] Evaluating on the test split.
I0216 18:22:28.146904 139646656866112 submission_runner.py:408] Time since start: 28427.12s, 	Step: 33176, 	{'train/ctc_loss': Array(0.5189762, dtype=float32), 'train/wer': 0.18572372769332451, 'validation/ctc_loss': Array(0.810009, dtype=float32), 'validation/wer': 0.2471687729901426, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.54166, dtype=float32), 'test/wer': 0.18704933682692504, 'test/num_examples': 2472, 'score': 25959.6188287735, 'total_duration': 28427.117254018784, 'accumulated_submission_time': 25959.6188287735, 'accumulated_eval_time': 2465.121926546097, 'accumulated_logging_time': 0.965153694152832}
I0216 18:22:28.182362 139535991158528 logging_writer.py:48] [33176] accumulated_eval_time=2465.121927, accumulated_logging_time=0.965154, accumulated_submission_time=25959.618829, global_step=33176, preemption_count=0, score=25959.618829, test/ctc_loss=0.5416600108146667, test/num_examples=2472, test/wer=0.187049, total_duration=28427.117254, train/ctc_loss=0.5189762115478516, train/wer=0.185724, validation/ctc_loss=0.8100090026855469, validation/num_examples=5348, validation/wer=0.247169
I0216 18:22:47.344210 139535982765824 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.7121767401695251, loss=1.6565064191818237
I0216 18:24:03.227435 139535991158528 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.7688875794410706, loss=1.743672490119934
I0216 18:25:19.131337 139535982765824 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.5601227879524231, loss=1.7598192691802979
I0216 18:26:35.062497 139535991158528 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.5542472004890442, loss=1.717504858970642
I0216 18:27:50.937940 139535982765824 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.6769914031028748, loss=1.659569501876831
I0216 18:29:06.832214 139535991158528 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.7115833759307861, loss=1.784379482269287
I0216 18:30:22.702611 139535982765824 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.8613398671150208, loss=1.7061034440994263
I0216 18:31:44.017758 139535991158528 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.648362934589386, loss=1.6958738565444946
I0216 18:33:09.057345 139535991158528 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.7221702337265015, loss=1.6907531023025513
I0216 18:34:25.039181 139535982765824 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.700785219669342, loss=1.6960166692733765
I0216 18:35:41.105803 139535991158528 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.5798256993293762, loss=1.677011251449585
I0216 18:36:57.073616 139535982765824 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.5679701566696167, loss=1.74888014793396
I0216 18:38:13.407534 139535991158528 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.6364115476608276, loss=1.673446536064148
I0216 18:39:29.418005 139535982765824 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.6124007701873779, loss=1.6851820945739746
I0216 18:40:45.348339 139535991158528 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.6303085088729858, loss=1.696324110031128
I0216 18:42:06.536608 139535982765824 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.5533796548843384, loss=1.6785838603973389
I0216 18:43:28.532206 139535991158528 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.9866546392440796, loss=1.6661369800567627
I0216 18:44:51.682301 139535982765824 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.7938223481178284, loss=1.6365629434585571
I0216 18:46:13.685184 139535991158528 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.5938071012496948, loss=1.7767446041107178
I0216 18:46:28.819153 139646656866112 spec.py:321] Evaluating on the training split.
I0216 18:47:22.351660 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 18:48:14.623704 139646656866112 spec.py:349] Evaluating on the test split.
I0216 18:48:40.418622 139646656866112 submission_runner.py:408] Time since start: 29999.39s, 	Step: 35020, 	{'train/ctc_loss': Array(0.4539365, dtype=float32), 'train/wer': 0.16255994365555315, 'validation/ctc_loss': Array(0.78835493, dtype=float32), 'validation/wer': 0.24026569605221237, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5160036, dtype=float32), 'test/wer': 0.17756382913899213, 'test/num_examples': 2472, 'score': 27400.167891979218, 'total_duration': 29999.389021396637, 'accumulated_submission_time': 27400.167891979218, 'accumulated_eval_time': 2596.715493917465, 'accumulated_logging_time': 1.015394926071167}
I0216 18:48:40.459028 139535991158528 logging_writer.py:48] [35020] accumulated_eval_time=2596.715494, accumulated_logging_time=1.015395, accumulated_submission_time=27400.167892, global_step=35020, preemption_count=0, score=27400.167892, test/ctc_loss=0.5160036087036133, test/num_examples=2472, test/wer=0.177564, total_duration=29999.389021, train/ctc_loss=0.45393648743629456, train/wer=0.162560, validation/ctc_loss=0.7883549332618713, validation/num_examples=5348, validation/wer=0.240266
I0216 18:49:45.741561 139535991158528 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.7253954410552979, loss=1.6583346128463745
I0216 18:51:01.587514 139535982765824 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.6433034539222717, loss=1.6360222101211548
I0216 18:52:17.622783 139535991158528 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.6900602579116821, loss=1.6997383832931519
I0216 18:53:33.561874 139535982765824 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.8094482421875, loss=1.6766743659973145
I0216 18:54:49.452384 139535991158528 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.6232861280441284, loss=1.7115415334701538
I0216 18:56:05.637663 139535982765824 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.8602623343467712, loss=1.6522126197814941
I0216 18:57:24.319628 139535991158528 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.6251313090324402, loss=1.6043767929077148
I0216 18:58:45.839152 139535982765824 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.5794858932495117, loss=1.6502106189727783
I0216 19:00:07.834576 139535991158528 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.7213548421859741, loss=1.6613280773162842
I0216 19:01:29.828317 139535982765824 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.5503355860710144, loss=1.6735353469848633
I0216 19:02:52.406306 139535991158528 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.7113035917282104, loss=1.6200611591339111
I0216 19:04:08.301957 139535982765824 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.6933824419975281, loss=1.6331285238265991
I0216 19:05:24.184478 139535991158528 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.6063986420631409, loss=1.6404597759246826
I0216 19:06:40.082983 139535982765824 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.5779376029968262, loss=1.6661105155944824
I0216 19:07:56.063868 139535991158528 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.6027106046676636, loss=1.6454291343688965
I0216 19:09:11.963473 139535982765824 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.8173214793205261, loss=1.7214280366897583
I0216 19:10:28.048484 139535991158528 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.6439328193664551, loss=1.6828930377960205
I0216 19:11:50.527011 139535982765824 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.6670098304748535, loss=1.6306774616241455
I0216 19:12:40.742052 139646656866112 spec.py:321] Evaluating on the training split.
I0216 19:13:35.892475 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 19:14:28.145550 139646656866112 spec.py:349] Evaluating on the test split.
I0216 19:14:54.478695 139646656866112 submission_runner.py:408] Time since start: 31573.45s, 	Step: 36863, 	{'train/ctc_loss': Array(0.44536912, dtype=float32), 'train/wer': 0.15809184128399464, 'validation/ctc_loss': Array(0.7607473, dtype=float32), 'validation/wer': 0.23202062233893625, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5011267, dtype=float32), 'test/wer': 0.16980480571974083, 'test/num_examples': 2472, 'score': 28840.363377332687, 'total_duration': 31573.44949054718, 'accumulated_submission_time': 28840.363377332687, 'accumulated_eval_time': 2730.4466235637665, 'accumulated_logging_time': 1.0714967250823975}
I0216 19:14:54.520215 139535991158528 logging_writer.py:48] [36863] accumulated_eval_time=2730.446624, accumulated_logging_time=1.071497, accumulated_submission_time=28840.363377, global_step=36863, preemption_count=0, score=28840.363377, test/ctc_loss=0.5011267066001892, test/num_examples=2472, test/wer=0.169805, total_duration=31573.449491, train/ctc_loss=0.4453691244125366, train/wer=0.158092, validation/ctc_loss=0.7607473134994507, validation/num_examples=5348, validation/wer=0.232021
I0216 19:15:23.366688 139535982765824 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.7996556162834167, loss=1.623744010925293
I0216 19:16:39.250909 139535991158528 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.5888954401016235, loss=1.7332093715667725
I0216 19:17:58.679111 139535991158528 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.6941534876823425, loss=1.6166932582855225
I0216 19:19:14.483607 139535982765824 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.7248188853263855, loss=1.6203885078430176
I0216 19:20:30.316054 139535991158528 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.8974408507347107, loss=1.7136507034301758
I0216 19:21:46.153539 139535982765824 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.6337224841117859, loss=1.5870622396469116
I0216 19:23:02.068004 139535991158528 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.6896411776542664, loss=1.6488962173461914
I0216 19:24:18.043797 139535982765824 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.6650975346565247, loss=1.6349050998687744
I0216 19:25:34.395368 139535991158528 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.6169226765632629, loss=1.6558101177215576
I0216 19:26:56.568841 139535982765824 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.5918721556663513, loss=1.6369770765304565
I0216 19:28:19.524266 139535991158528 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.6531769037246704, loss=1.6701500415802002
I0216 19:29:41.652929 139535982765824 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.5767504572868347, loss=1.6613913774490356
I0216 19:31:03.395911 139535991158528 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.6326191425323486, loss=1.659769058227539
I0216 19:32:22.740804 139535991158528 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.5770906805992126, loss=1.6628490686416626
I0216 19:33:38.684237 139535982765824 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.5304195284843445, loss=1.7321134805679321
I0216 19:34:54.594757 139535991158528 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.6323227882385254, loss=1.6719692945480347
I0216 19:36:10.487043 139535982765824 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.8683109879493713, loss=1.709421992301941
I0216 19:37:26.347709 139535991158528 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.7916696071624756, loss=1.6567405462265015
I0216 19:38:42.169326 139535982765824 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.7346976399421692, loss=1.6619101762771606
I0216 19:38:54.744950 139646656866112 spec.py:321] Evaluating on the training split.
I0216 19:39:49.828280 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 19:40:41.040608 139646656866112 spec.py:349] Evaluating on the test split.
I0216 19:41:07.297789 139646656866112 submission_runner.py:408] Time since start: 33146.27s, 	Step: 38718, 	{'train/ctc_loss': Array(0.41135126, dtype=float32), 'train/wer': 0.15001940039835485, 'validation/ctc_loss': Array(0.7361761, dtype=float32), 'validation/wer': 0.2231962694420576, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4796191, dtype=float32), 'test/wer': 0.1635691507728556, 'test/num_examples': 2472, 'score': 30280.498059034348, 'total_duration': 33146.26831173897, 'accumulated_submission_time': 30280.498059034348, 'accumulated_eval_time': 2862.9936952590942, 'accumulated_logging_time': 1.1281471252441406}
I0216 19:41:07.339557 139535991158528 logging_writer.py:48] [38718] accumulated_eval_time=2862.993695, accumulated_logging_time=1.128147, accumulated_submission_time=30280.498059, global_step=38718, preemption_count=0, score=30280.498059, test/ctc_loss=0.4796190857887268, test/num_examples=2472, test/wer=0.163569, total_duration=33146.268312, train/ctc_loss=0.4113512635231018, train/wer=0.150019, validation/ctc_loss=0.736176073551178, validation/num_examples=5348, validation/wer=0.223196
I0216 19:42:10.107907 139535982765824 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.7100202441215515, loss=1.6405789852142334
I0216 19:43:25.973760 139535991158528 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.5877658128738403, loss=1.566049337387085
I0216 19:44:42.074931 139535982765824 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.582713782787323, loss=1.6280778646469116
I0216 19:45:58.393870 139535991158528 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.7381089925765991, loss=1.6286534070968628
I0216 19:47:17.940616 139535991158528 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.7719377875328064, loss=1.62155282497406
I0216 19:48:33.842518 139535982765824 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.6519159078598022, loss=1.625345230102539
I0216 19:49:49.713462 139535991158528 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.7107348442077637, loss=1.6369282007217407
I0216 19:51:05.639354 139535982765824 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.5687452554702759, loss=1.557539701461792
I0216 19:52:21.563425 139535991158528 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.6189988851547241, loss=1.6166062355041504
I0216 19:53:37.588038 139535982765824 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.6198739409446716, loss=1.6125895977020264
I0216 19:54:54.897435 139535991158528 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.6409886479377747, loss=1.5874627828598022
I0216 19:56:17.181465 139535982765824 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.5708188414573669, loss=1.6508433818817139
I0216 19:57:39.066066 139535991158528 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.6956150531768799, loss=1.5636706352233887
I0216 19:59:01.057035 139535982765824 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.5995330214500427, loss=1.5757907629013062
I0216 20:00:24.272044 139535991158528 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.9684629440307617, loss=1.6070215702056885
I0216 20:01:40.462990 139535982765824 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.7160320281982422, loss=1.6301555633544922
I0216 20:02:56.622236 139535991158528 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.6398480534553528, loss=1.5536293983459473
I0216 20:04:12.698682 139535982765824 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.5464473366737366, loss=1.5420078039169312
I0216 20:05:07.818492 139646656866112 spec.py:321] Evaluating on the training split.
I0216 20:06:00.860417 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 20:06:51.851580 139646656866112 spec.py:349] Evaluating on the test split.
I0216 20:07:17.685836 139646656866112 submission_runner.py:408] Time since start: 34716.66s, 	Step: 40574, 	{'train/ctc_loss': Array(0.40231338, dtype=float32), 'train/wer': 0.14547936483956425, 'validation/ctc_loss': Array(0.70659876, dtype=float32), 'validation/wer': 0.2144781177288394, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.45901236, dtype=float32), 'test/wer': 0.1562366705258668, 'test/num_examples': 2472, 'score': 31720.889266252518, 'total_duration': 34716.65667510033, 'accumulated_submission_time': 31720.889266252518, 'accumulated_eval_time': 2992.8555755615234, 'accumulated_logging_time': 1.185434341430664}
I0216 20:07:17.730514 139535991158528 logging_writer.py:48] [40574] accumulated_eval_time=2992.855576, accumulated_logging_time=1.185434, accumulated_submission_time=31720.889266, global_step=40574, preemption_count=0, score=31720.889266, test/ctc_loss=0.45901235938072205, test/num_examples=2472, test/wer=0.156237, total_duration=34716.656675, train/ctc_loss=0.40231338143348694, train/wer=0.145479, validation/ctc_loss=0.7065987586975098, validation/num_examples=5348, validation/wer=0.214478
I0216 20:07:38.230521 139535982765824 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.7000659704208374, loss=1.632521152496338
I0216 20:08:53.939529 139535991158528 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.605776846408844, loss=1.5089246034622192
I0216 20:10:09.878863 139535982765824 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.7138302326202393, loss=1.6946548223495483
I0216 20:11:25.837492 139535991158528 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.7196323871612549, loss=1.646092176437378
I0216 20:12:41.724169 139535982765824 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.5897372364997864, loss=1.593689203262329
I0216 20:13:59.733317 139535991158528 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.6933510303497314, loss=1.5997614860534668
I0216 20:15:24.682373 139535991158528 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.6160770654678345, loss=1.6224108934402466
I0216 20:16:40.516162 139535982765824 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.6992999315261841, loss=1.5845072269439697
I0216 20:17:56.656990 139535991158528 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.7806133031845093, loss=1.6002750396728516
I0216 20:19:12.834157 139535982765824 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.6711698770523071, loss=1.5764377117156982
I0216 20:20:28.689698 139535991158528 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.6549174785614014, loss=1.5679923295974731
I0216 20:21:44.652686 139535982765824 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.615151047706604, loss=1.5509883165359497
I0216 20:23:00.651824 139535991158528 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.6481159329414368, loss=1.5744928121566772
I0216 20:24:20.074634 139535982765824 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.5998097658157349, loss=1.5986227989196777
I0216 20:25:41.509243 139535991158528 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.5978445410728455, loss=1.5358929634094238
I0216 20:27:02.701181 139535982765824 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.6388639211654663, loss=1.6164100170135498
I0216 20:28:24.350234 139535991158528 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.5989912748336792, loss=1.634398102760315
I0216 20:29:44.892855 139535991158528 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.8416832685470581, loss=1.603400468826294
I0216 20:31:00.788915 139535982765824 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.7299497723579407, loss=1.618904948234558
I0216 20:31:17.965090 139646656866112 spec.py:321] Evaluating on the training split.
I0216 20:32:12.689777 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 20:33:04.005884 139646656866112 spec.py:349] Evaluating on the test split.
I0216 20:33:30.046769 139646656866112 submission_runner.py:408] Time since start: 36289.02s, 	Step: 42424, 	{'train/ctc_loss': Array(0.37856817, dtype=float32), 'train/wer': 0.13991521244110516, 'validation/ctc_loss': Array(0.6976467, dtype=float32), 'validation/wer': 0.214381571198239, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4475745, dtype=float32), 'test/wer': 0.15205248512176792, 'test/num_examples': 2472, 'score': 33161.03370857239, 'total_duration': 36289.01644325256, 'accumulated_submission_time': 33161.03370857239, 'accumulated_eval_time': 3124.9306325912476, 'accumulated_logging_time': 1.2468442916870117}
I0216 20:33:30.090476 139535991158528 logging_writer.py:48] [42424] accumulated_eval_time=3124.930633, accumulated_logging_time=1.246844, accumulated_submission_time=33161.033709, global_step=42424, preemption_count=0, score=33161.033709, test/ctc_loss=0.4475744962692261, test/num_examples=2472, test/wer=0.152052, total_duration=36289.016443, train/ctc_loss=0.378568172454834, train/wer=0.139915, validation/ctc_loss=0.6976466774940491, validation/num_examples=5348, validation/wer=0.214382
I0216 20:34:28.393187 139535982765824 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.8067472577095032, loss=1.545546054840088
I0216 20:35:44.523542 139535991158528 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.6522447466850281, loss=1.5776926279067993
I0216 20:37:00.521448 139535982765824 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.7209218144416809, loss=1.5416675806045532
I0216 20:38:16.385919 139535991158528 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.789078950881958, loss=1.5537101030349731
I0216 20:39:32.267715 139535982765824 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.6207953691482544, loss=1.5421797037124634
I0216 20:40:48.165385 139535991158528 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.7355024814605713, loss=1.6036630868911743
I0216 20:42:07.989027 139535982765824 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.7344759106636047, loss=1.560855746269226
I0216 20:43:30.831766 139535991158528 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.6964884400367737, loss=1.5618571043014526
I0216 20:44:53.138252 139535991158528 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.6968790292739868, loss=1.5652612447738647
I0216 20:46:08.991837 139535982765824 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.8741495609283447, loss=1.5397837162017822
I0216 20:47:24.924838 139535991158528 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.5966154336929321, loss=1.5396921634674072
I0216 20:48:40.970677 139535982765824 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.7209126353263855, loss=1.5864015817642212
I0216 20:49:57.003789 139535991158528 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.5873158574104309, loss=1.5480347871780396
I0216 20:51:13.303792 139535982765824 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.6832558512687683, loss=1.5482994318008423
I0216 20:52:30.487118 139535991158528 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.6667095422744751, loss=1.5152733325958252
I0216 20:53:52.368805 139535982765824 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.6343691349029541, loss=1.602808952331543
I0216 20:55:15.290784 139535991158528 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.7121725678443909, loss=1.496207356452942
I0216 20:56:36.747645 139535982765824 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.8332062363624573, loss=1.5210987329483032
I0216 20:57:30.436625 139646656866112 spec.py:321] Evaluating on the training split.
I0216 20:58:23.492139 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 20:59:14.702000 139646656866112 spec.py:349] Evaluating on the test split.
I0216 20:59:40.807443 139646656866112 submission_runner.py:408] Time since start: 37859.78s, 	Step: 44268, 	{'train/ctc_loss': Array(0.42395383, dtype=float32), 'train/wer': 0.1483157612824591, 'validation/ctc_loss': Array(0.67980665, dtype=float32), 'validation/wer': 0.207304710505228, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43433073, dtype=float32), 'test/wer': 0.1464464891434607, 'test/num_examples': 2472, 'score': 34601.290254592896, 'total_duration': 37859.777564525604, 'accumulated_submission_time': 34601.290254592896, 'accumulated_eval_time': 3255.2952768802643, 'accumulated_logging_time': 1.3067612648010254}
I0216 20:59:40.847607 139535991158528 logging_writer.py:48] [44268] accumulated_eval_time=3255.295277, accumulated_logging_time=1.306761, accumulated_submission_time=34601.290255, global_step=44268, preemption_count=0, score=34601.290255, test/ctc_loss=0.4343307316303253, test/num_examples=2472, test/wer=0.146446, total_duration=37859.777565, train/ctc_loss=0.4239538311958313, train/wer=0.148316, validation/ctc_loss=0.679806649684906, validation/num_examples=5348, validation/wer=0.207305
I0216 21:00:09.296507 139535991158528 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.7295846343040466, loss=1.5629558563232422
I0216 21:01:25.058692 139535982765824 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.6153621673583984, loss=1.5358734130859375
I0216 21:02:40.956290 139535991158528 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.5087456107139587, loss=1.5248616933822632
I0216 21:03:56.885110 139535982765824 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.6618101000785828, loss=1.6159961223602295
I0216 21:05:12.862626 139535991158528 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.6350189447402954, loss=1.520477294921875
I0216 21:06:28.769590 139535982765824 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.5114560723304749, loss=1.478843331336975
I0216 21:07:45.209639 139535991158528 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.6103347539901733, loss=1.5163328647613525
I0216 21:09:08.292178 139535982765824 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.6209821701049805, loss=1.5847417116165161
I0216 21:10:30.582728 139535991158528 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.6277794241905212, loss=1.5737065076828003
I0216 21:11:53.163347 139535982765824 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.6127972602844238, loss=1.5791616439819336
I0216 21:13:15.091388 139535991158528 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.6231479644775391, loss=1.498256802558899
I0216 21:14:35.168164 139535991158528 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.5407949090003967, loss=1.5382206439971924
I0216 21:15:51.127651 139535982765824 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.7670594453811646, loss=1.5333311557769775
I0216 21:17:07.012748 139535991158528 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.6949182152748108, loss=1.537063717842102
I0216 21:18:22.943471 139535982765824 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.8968581557273865, loss=1.5147125720977783
I0216 21:19:38.941293 139535991158528 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.8086463212966919, loss=1.5587084293365479
I0216 21:20:54.928755 139535982765824 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.7469555735588074, loss=1.5306432247161865
I0216 21:22:14.659840 139535991158528 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.5565276741981506, loss=1.4854516983032227
I0216 21:23:37.125210 139535982765824 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.6009767055511475, loss=1.496278166770935
I0216 21:23:41.451436 139646656866112 spec.py:321] Evaluating on the training split.
I0216 21:24:37.554783 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 21:25:28.961421 139646656866112 spec.py:349] Evaluating on the test split.
I0216 21:25:55.429859 139646656866112 submission_runner.py:408] Time since start: 39434.40s, 	Step: 46107, 	{'train/ctc_loss': Array(0.3622536, dtype=float32), 'train/wer': 0.13474557649446997, 'validation/ctc_loss': Array(0.65977854, dtype=float32), 'validation/wer': 0.2022360176487058, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4211136, dtype=float32), 'test/wer': 0.14396847642841185, 'test/num_examples': 2472, 'score': 36041.80644035339, 'total_duration': 39434.40048265457, 'accumulated_submission_time': 36041.80644035339, 'accumulated_eval_time': 3389.2680366039276, 'accumulated_logging_time': 1.362666368484497}
I0216 21:25:55.473397 139535991158528 logging_writer.py:48] [46107] accumulated_eval_time=3389.268037, accumulated_logging_time=1.362666, accumulated_submission_time=36041.806440, global_step=46107, preemption_count=0, score=36041.806440, test/ctc_loss=0.42111361026763916, test/num_examples=2472, test/wer=0.143968, total_duration=39434.400483, train/ctc_loss=0.3622536063194275, train/wer=0.134746, validation/ctc_loss=0.6597785353660583, validation/num_examples=5348, validation/wer=0.202236
I0216 21:27:06.472513 139535982765824 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.6643535494804382, loss=1.513905644416809
I0216 21:28:22.341685 139535991158528 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.7345199584960938, loss=1.535632848739624
I0216 21:29:41.766515 139535991158528 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.6413584351539612, loss=1.490654468536377
I0216 21:30:57.688700 139535982765824 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.6383321285247803, loss=1.5408086776733398
I0216 21:32:13.791988 139535991158528 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.819331705570221, loss=1.5655156373977661
I0216 21:33:29.838025 139535982765824 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.6274104714393616, loss=1.4788821935653687
I0216 21:34:45.819351 139535991158528 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.5983205437660217, loss=1.5743204355239868
I0216 21:36:01.841762 139535982765824 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.6655835509300232, loss=1.519495964050293
I0216 21:37:19.533840 139535991158528 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.7779537439346313, loss=1.5625165700912476
I0216 21:38:41.703880 139535982765824 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.6844050884246826, loss=1.5887281894683838
I0216 21:40:04.152945 139535991158528 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.6924693584442139, loss=1.5192065238952637
I0216 21:41:26.233598 139535982765824 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.6947530508041382, loss=1.5488874912261963
I0216 21:42:50.234444 139535991158528 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.5756181478500366, loss=1.5325530767440796
I0216 21:44:06.031315 139535982765824 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.6473038792610168, loss=1.4665392637252808
I0216 21:45:21.873656 139535991158528 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.655060350894928, loss=1.4562163352966309
I0216 21:46:37.699073 139535982765824 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.7473053336143494, loss=1.482165813446045
I0216 21:47:53.588569 139535991158528 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.6292380690574646, loss=1.4806253910064697
I0216 21:49:09.448884 139535982765824 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.6177922487258911, loss=1.4452197551727295
I0216 21:49:56.054426 139646656866112 spec.py:321] Evaluating on the training split.
I0216 21:50:50.719243 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 21:51:42.143637 139646656866112 spec.py:349] Evaluating on the test split.
I0216 21:52:08.189914 139646656866112 submission_runner.py:408] Time since start: 41007.16s, 	Step: 47963, 	{'train/ctc_loss': Array(0.33330858, dtype=float32), 'train/wer': 0.1254129063868123, 'validation/ctc_loss': Array(0.6382336, dtype=float32), 'validation/wer': 0.19520743022099502, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40312463, dtype=float32), 'test/wer': 0.13724534357036947, 'test/num_examples': 2472, 'score': 37482.298840522766, 'total_duration': 41007.160016059875, 'accumulated_submission_time': 37482.298840522766, 'accumulated_eval_time': 3521.3973503112793, 'accumulated_logging_time': 1.421417236328125}
I0216 21:52:08.228434 139535991158528 logging_writer.py:48] [47963] accumulated_eval_time=3521.397350, accumulated_logging_time=1.421417, accumulated_submission_time=37482.298841, global_step=47963, preemption_count=0, score=37482.298841, test/ctc_loss=0.4031246304512024, test/num_examples=2472, test/wer=0.137245, total_duration=41007.160016, train/ctc_loss=0.3333085775375366, train/wer=0.125413, validation/ctc_loss=0.6382336020469666, validation/num_examples=5348, validation/wer=0.195207
I0216 21:52:36.977624 139535982765824 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.7282966375350952, loss=1.5379691123962402
I0216 21:53:52.786646 139535991158528 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.7055977582931519, loss=1.5045708417892456
I0216 21:55:08.568764 139535982765824 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.6206104159355164, loss=1.477426528930664
I0216 21:56:24.498976 139535991158528 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.7306030988693237, loss=1.5273362398147583
I0216 21:57:40.475224 139535982765824 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.63132643699646, loss=1.5013134479522705
I0216 21:59:00.070936 139535991158528 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.575334370136261, loss=1.5255427360534668
I0216 22:00:15.945714 139535982765824 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.7633122801780701, loss=1.4890915155410767
I0216 22:01:31.903015 139535991158528 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.727313220500946, loss=1.5024603605270386
I0216 22:02:47.899674 139535982765824 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.6691606640815735, loss=1.484581708908081
I0216 22:04:03.912347 139535991158528 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.7402791380882263, loss=1.4836667776107788
I0216 22:05:19.807936 139535982765824 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.8425068855285645, loss=1.476378321647644
I0216 22:06:38.383926 139535991158528 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.5818823575973511, loss=1.45485520362854
I0216 22:08:00.222894 139535982765824 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.7763045430183411, loss=1.4827836751937866
I0216 22:09:22.314641 139535991158528 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.9052505493164062, loss=1.4953895807266235
I0216 22:10:43.851653 139535982765824 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.6069859266281128, loss=1.471348762512207
I0216 22:12:04.627124 139535991158528 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.7738264799118042, loss=1.4776923656463623
I0216 22:13:20.656692 139535982765824 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.7254801392555237, loss=1.476271390914917
I0216 22:14:36.889452 139535991158528 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.6167170405387878, loss=1.4686741828918457
I0216 22:15:52.818722 139535982765824 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.7064528465270996, loss=1.4914839267730713
I0216 22:16:08.455454 139646656866112 spec.py:321] Evaluating on the training split.
I0216 22:17:02.434802 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 22:17:54.068320 139646656866112 spec.py:349] Evaluating on the test split.
I0216 22:18:19.909482 139646656866112 submission_runner.py:408] Time since start: 42578.88s, 	Step: 49822, 	{'train/ctc_loss': Array(0.31033447, dtype=float32), 'train/wer': 0.1149222233388164, 'validation/ctc_loss': Array(0.6196946, dtype=float32), 'validation/wer': 0.1900711547930525, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38547108, dtype=float32), 'test/wer': 0.13143623179574676, 'test/num_examples': 2472, 'score': 38922.43569779396, 'total_duration': 42578.880200862885, 'accumulated_submission_time': 38922.43569779396, 'accumulated_eval_time': 3652.8457939624786, 'accumulated_logging_time': 1.4763050079345703}
I0216 22:18:19.953434 139535991158528 logging_writer.py:48] [49822] accumulated_eval_time=3652.845794, accumulated_logging_time=1.476305, accumulated_submission_time=38922.435698, global_step=49822, preemption_count=0, score=38922.435698, test/ctc_loss=0.38547107577323914, test/num_examples=2472, test/wer=0.131436, total_duration=42578.880201, train/ctc_loss=0.3103344738483429, train/wer=0.114922, validation/ctc_loss=0.6196945905685425, validation/num_examples=5348, validation/wer=0.190071
I0216 22:19:19.859933 139535982765824 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.7423766255378723, loss=1.5017141103744507
I0216 22:20:35.899515 139535991158528 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.648422360420227, loss=1.4668997526168823
I0216 22:21:51.901898 139535982765824 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.6400735378265381, loss=1.4785689115524292
I0216 22:23:08.031282 139535991158528 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.7629868984222412, loss=1.4453885555267334
I0216 22:24:23.990022 139535982765824 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.6742171049118042, loss=1.5072944164276123
I0216 22:25:44.169521 139535991158528 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.6395108103752136, loss=1.4861795902252197
I0216 22:27:08.281971 139535991158528 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.62715083360672, loss=1.4358844757080078
I0216 22:28:24.125809 139535982765824 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.6069661378860474, loss=1.475041389465332
I0216 22:29:40.099421 139535991158528 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.633177638053894, loss=1.4625930786132812
I0216 22:30:56.065323 139535982765824 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.7185238599777222, loss=1.490788221359253
I0216 22:32:12.434898 139535991158528 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.5938553214073181, loss=1.395477056503296
I0216 22:33:28.475280 139535982765824 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.6959801316261292, loss=1.4781744480133057
I0216 22:34:44.435681 139535991158528 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.6190861463546753, loss=1.5112119913101196
I0216 22:36:03.955427 139535982765824 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.6827610731124878, loss=1.4386427402496338
I0216 22:37:25.658667 139535991158528 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.7628414034843445, loss=1.422166109085083
I0216 22:38:48.062558 139535982765824 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.7376777529716492, loss=1.4973593950271606
I0216 22:40:13.095447 139535991158528 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.7448813319206238, loss=1.4044482707977295
I0216 22:41:28.945985 139535982765824 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.6869848966598511, loss=1.4634028673171997
I0216 22:42:20.336330 139646656866112 spec.py:321] Evaluating on the training split.
I0216 22:43:14.614050 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 22:44:06.490251 139646656866112 spec.py:349] Evaluating on the test split.
I0216 22:44:33.024037 139646656866112 submission_runner.py:408] Time since start: 44151.99s, 	Step: 51669, 	{'train/ctc_loss': Array(0.29647183, dtype=float32), 'train/wer': 0.11162165491166343, 'validation/ctc_loss': Array(0.6016999, dtype=float32), 'validation/wer': 0.18108267279415313, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3817462, dtype=float32), 'test/wer': 0.12897853066032947, 'test/num_examples': 2472, 'score': 40362.72778439522, 'total_duration': 44151.99367618561, 'accumulated_submission_time': 40362.72778439522, 'accumulated_eval_time': 3785.52684879303, 'accumulated_logging_time': 1.5382416248321533}
I0216 22:44:33.064664 139535991158528 logging_writer.py:48] [51669] accumulated_eval_time=3785.526849, accumulated_logging_time=1.538242, accumulated_submission_time=40362.727784, global_step=51669, preemption_count=0, score=40362.727784, test/ctc_loss=0.38174620270729065, test/num_examples=2472, test/wer=0.128979, total_duration=44151.993676, train/ctc_loss=0.29647183418273926, train/wer=0.111622, validation/ctc_loss=0.6016998887062073, validation/num_examples=5348, validation/wer=0.181083
I0216 22:44:57.355979 139535982765824 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.6983575224876404, loss=1.4124201536178589
I0216 22:46:13.183643 139535991158528 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.7006515860557556, loss=1.4618045091629028
I0216 22:47:29.138144 139535982765824 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.7821488380432129, loss=1.388548493385315
I0216 22:48:45.431088 139535991158528 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.7153400778770447, loss=1.366639256477356
I0216 22:50:01.328813 139535982765824 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.6569782495498657, loss=1.4992493391036987
I0216 22:51:17.292686 139535991158528 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.6623849272727966, loss=1.4504534006118774
I0216 22:52:33.653812 139535982765824 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.6573241949081421, loss=1.4168564081192017
I0216 22:53:55.342709 139535991158528 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.615212619304657, loss=1.4941911697387695
I0216 22:55:17.067356 139535982765824 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.7180823683738708, loss=1.4265578985214233
I0216 22:56:37.745903 139535991158528 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.699080765247345, loss=1.4337713718414307
I0216 22:57:53.681079 139535982765824 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.6946558952331543, loss=1.4602479934692383
I0216 22:59:09.657765 139535991158528 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.6384546756744385, loss=1.3629930019378662
I0216 23:00:25.583050 139535982765824 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.6476799249649048, loss=1.4302332401275635
I0216 23:01:41.557615 139535991158528 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.7081899046897888, loss=1.396454095840454
I0216 23:02:57.519088 139535982765824 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.6581387519836426, loss=1.4196991920471191
I0216 23:04:15.319328 139535991158528 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.9248074889183044, loss=1.3653346300125122
I0216 23:05:36.789079 139535982765824 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.6078082919120789, loss=1.3976010084152222
I0216 23:06:58.248534 139535991158528 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.6350057125091553, loss=1.4175776243209839
I0216 23:08:19.631470 139535982765824 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.6930178999900818, loss=1.420449137687683
I0216 23:08:33.404967 139646656866112 spec.py:321] Evaluating on the training split.
I0216 23:09:28.327983 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 23:10:19.962478 139646656866112 spec.py:349] Evaluating on the test split.
I0216 23:10:45.844799 139646656866112 submission_runner.py:408] Time since start: 45724.82s, 	Step: 53518, 	{'train/ctc_loss': Array(0.30620503, dtype=float32), 'train/wer': 0.11275602136665565, 'validation/ctc_loss': Array(0.57299715, dtype=float32), 'validation/wer': 0.17624569161107195, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35301486, dtype=float32), 'test/wer': 0.11983831982613288, 'test/num_examples': 2472, 'score': 41802.975400447845, 'total_duration': 45724.815037965775, 'accumulated_submission_time': 41802.975400447845, 'accumulated_eval_time': 3917.9606053829193, 'accumulated_logging_time': 1.5975022315979004}
I0216 23:10:45.885139 139535991158528 logging_writer.py:48] [53518] accumulated_eval_time=3917.960605, accumulated_logging_time=1.597502, accumulated_submission_time=41802.975400, global_step=53518, preemption_count=0, score=41802.975400, test/ctc_loss=0.35301485657691956, test/num_examples=2472, test/wer=0.119838, total_duration=45724.815038, train/ctc_loss=0.30620503425598145, train/wer=0.112756, validation/ctc_loss=0.5729971528053284, validation/num_examples=5348, validation/wer=0.176246
I0216 23:11:52.192056 139535991158528 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.6003255248069763, loss=1.3863475322723389
I0216 23:13:08.079389 139535982765824 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.8315815329551697, loss=1.4273695945739746
I0216 23:14:24.005359 139535991158528 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.6813149452209473, loss=1.4223620891571045
I0216 23:15:39.860430 139535982765824 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.6189230680465698, loss=1.365072250366211
I0216 23:16:55.822020 139535991158528 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.7389043569564819, loss=1.3928627967834473
I0216 23:18:11.746981 139535982765824 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.678462028503418, loss=1.4070823192596436
I0216 23:19:30.017502 139535991158528 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.8100922107696533, loss=1.4007291793823242
I0216 23:20:50.860623 139535982765824 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.6654132604598999, loss=1.333400011062622
I0216 23:22:12.925061 139535991158528 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.7224562168121338, loss=1.3745914697647095
I0216 23:23:34.343001 139535982765824 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.9149801135063171, loss=1.4571834802627563
I0216 23:24:58.737005 139535991158528 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.7878163456916809, loss=1.3993830680847168
I0216 23:26:14.631453 139535982765824 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.7157059907913208, loss=1.387436032295227
I0216 23:27:30.533664 139535991158528 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.9457055926322937, loss=1.393625020980835
I0216 23:28:46.453164 139535982765824 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.6165685057640076, loss=1.4171671867370605
I0216 23:30:02.372102 139535991158528 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.6870895624160767, loss=1.4158459901809692
I0216 23:31:18.175802 139535982765824 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.683470606803894, loss=1.3772221803665161
I0216 23:32:34.141925 139535991158528 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.7505146861076355, loss=1.4234546422958374
I0216 23:33:53.064114 139535982765824 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.9401795268058777, loss=1.3902981281280518
I0216 23:34:46.059033 139646656866112 spec.py:321] Evaluating on the training split.
I0216 23:35:41.797856 139646656866112 spec.py:333] Evaluating on the validation split.
I0216 23:36:33.792218 139646656866112 spec.py:349] Evaluating on the test split.
I0216 23:36:59.798605 139646656866112 submission_runner.py:408] Time since start: 47298.77s, 	Step: 55366, 	{'train/ctc_loss': Array(0.28244948, dtype=float32), 'train/wer': 0.10382260745166516, 'validation/ctc_loss': Array(0.55347747, dtype=float32), 'validation/wer': 0.17053979165258695, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33865485, dtype=float32), 'test/wer': 0.115654134422034, 'test/num_examples': 2472, 'score': 43243.05828857422, 'total_duration': 47298.76901054382, 'accumulated_submission_time': 43243.05828857422, 'accumulated_eval_time': 4051.69429731369, 'accumulated_logging_time': 1.6556503772735596}
I0216 23:36:59.838120 139535991158528 logging_writer.py:48] [55366] accumulated_eval_time=4051.694297, accumulated_logging_time=1.655650, accumulated_submission_time=43243.058289, global_step=55366, preemption_count=0, score=43243.058289, test/ctc_loss=0.33865484595298767, test/num_examples=2472, test/wer=0.115654, total_duration=47298.769011, train/ctc_loss=0.28244948387145996, train/wer=0.103823, validation/ctc_loss=0.5534774661064148, validation/num_examples=5348, validation/wer=0.170540
I0216 23:37:26.341311 139535982765824 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.8408464789390564, loss=1.4334238767623901
I0216 23:38:42.159173 139535991158528 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.6479647755622864, loss=1.351747751235962
I0216 23:39:58.360264 139535982765824 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.7005347609519958, loss=1.4110782146453857
I0216 23:41:17.777328 139535991158528 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.6346610188484192, loss=1.388571858406067
I0216 23:42:33.686770 139535982765824 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.571805477142334, loss=1.3904298543930054
I0216 23:43:49.755911 139535991158528 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.80394446849823, loss=1.3477463722229004
I0216 23:45:05.563282 139535982765824 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.7330284118652344, loss=1.3429224491119385
I0216 23:46:21.541235 139535991158528 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.7471326589584351, loss=1.3692893981933594
I0216 23:47:37.542864 139535982765824 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.8782452344894409, loss=1.4096765518188477
I0216 23:48:56.586556 139535991158528 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.6631093621253967, loss=1.3637961149215698
I0216 23:50:18.784088 139535982765824 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.6727141737937927, loss=1.4488433599472046
I0216 23:51:40.906845 139535991158528 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.733098030090332, loss=1.3479846715927124
I0216 23:53:02.679442 139535982765824 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.8795777559280396, loss=1.4011958837509155
I0216 23:54:24.742944 139535991158528 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.8336241245269775, loss=1.3270004987716675
I0216 23:55:40.771310 139535982765824 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.6707720756530762, loss=1.39625084400177
I0216 23:56:56.672768 139535991158528 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.6530787348747253, loss=1.3091648817062378
I0216 23:58:12.371653 139535982765824 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.7497009634971619, loss=1.3755583763122559
I0216 23:59:28.202754 139535991158528 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.6735600233078003, loss=1.3999526500701904
I0217 00:00:44.016095 139535982765824 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.6757816076278687, loss=1.343037486076355
I0217 00:01:00.373843 139646656866112 spec.py:321] Evaluating on the training split.
I0217 00:01:55.701864 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 00:02:46.993137 139646656866112 spec.py:349] Evaluating on the test split.
I0217 00:03:12.946962 139646656866112 submission_runner.py:408] Time since start: 48871.92s, 	Step: 57223, 	{'train/ctc_loss': Array(0.27888992, dtype=float32), 'train/wer': 0.10288397464705791, 'validation/ctc_loss': Array(0.54089737, dtype=float32), 'validation/wer': 0.16783648879577512, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3345989, dtype=float32), 'test/wer': 0.11490260597566673, 'test/num_examples': 2472, 'score': 44683.50492525101, 'total_duration': 48871.916709423065, 'accumulated_submission_time': 44683.50492525101, 'accumulated_eval_time': 4184.260848999023, 'accumulated_logging_time': 1.7116804122924805}
I0217 00:03:12.985282 139535991158528 logging_writer.py:48] [57223] accumulated_eval_time=4184.260849, accumulated_logging_time=1.711680, accumulated_submission_time=44683.504925, global_step=57223, preemption_count=0, score=44683.504925, test/ctc_loss=0.3345988988876343, test/num_examples=2472, test/wer=0.114903, total_duration=48871.916709, train/ctc_loss=0.278889924287796, train/wer=0.102884, validation/ctc_loss=0.5408973693847656, validation/num_examples=5348, validation/wer=0.167836
I0217 00:04:11.970272 139535982765824 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.7074818015098572, loss=1.3610397577285767
I0217 00:05:27.970124 139535991158528 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.6871235966682434, loss=1.344719409942627
I0217 00:06:44.017354 139535982765824 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.8380376696586609, loss=1.3963351249694824
I0217 00:07:59.914235 139535991158528 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.7160170078277588, loss=1.3866227865219116
I0217 00:09:19.468666 139535991158528 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.7894838452339172, loss=1.3270668983459473
I0217 00:10:35.399528 139535982765824 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.7546094059944153, loss=1.3252224922180176
I0217 00:11:51.671491 139535991158528 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.8288525938987732, loss=1.365526556968689
I0217 00:13:07.600553 139535982765824 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.7640464305877686, loss=1.3470863103866577
I0217 00:14:23.515973 139535991158528 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.7408880591392517, loss=1.3457467555999756
I0217 00:15:39.481316 139535982765824 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.814164936542511, loss=1.3826229572296143
I0217 00:16:55.713193 139535991158528 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.7223473191261292, loss=1.3176186084747314
I0217 00:18:17.904091 139535982765824 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.7545977830886841, loss=1.312134861946106
I0217 00:19:40.206455 139535991158528 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.8334565162658691, loss=1.3201195001602173
I0217 00:21:01.867561 139535982765824 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.731256365776062, loss=1.355384349822998
I0217 00:22:23.691972 139535991158528 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.6020318865776062, loss=1.3144502639770508
I0217 00:23:43.262435 139535991158528 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.8470879793167114, loss=1.3345309495925903
I0217 00:24:59.273118 139535982765824 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.6907705664634705, loss=1.304656744003296
I0217 00:26:15.261497 139535991158528 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.6767703294754028, loss=1.2947750091552734
I0217 00:27:13.640672 139646656866112 spec.py:321] Evaluating on the training split.
I0217 00:28:07.450545 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 00:28:58.603736 139646656866112 spec.py:349] Evaluating on the test split.
I0217 00:29:24.712563 139646656866112 submission_runner.py:408] Time since start: 50443.68s, 	Step: 59078, 	{'train/ctc_loss': Array(0.24648692, dtype=float32), 'train/wer': 0.09246029030067232, 'validation/ctc_loss': Array(0.51485807, dtype=float32), 'validation/wer': 0.15991967328654044, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31451392, dtype=float32), 'test/wer': 0.10895131314362318, 'test/num_examples': 2472, 'score': 46124.065954208374, 'total_duration': 50443.68307852745, 'accumulated_submission_time': 46124.065954208374, 'accumulated_eval_time': 4315.326943397522, 'accumulated_logging_time': 1.7696788311004639}
I0217 00:29:24.754781 139535991158528 logging_writer.py:48] [59078] accumulated_eval_time=4315.326943, accumulated_logging_time=1.769679, accumulated_submission_time=46124.065954, global_step=59078, preemption_count=0, score=46124.065954, test/ctc_loss=0.3145139217376709, test/num_examples=2472, test/wer=0.108951, total_duration=50443.683079, train/ctc_loss=0.24648691713809967, train/wer=0.092460, validation/ctc_loss=0.514858067035675, validation/num_examples=5348, validation/wer=0.159920
I0217 00:29:42.225014 139535982765824 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.7759803533554077, loss=1.286590814590454
I0217 00:30:57.958385 139535991158528 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.859619677066803, loss=1.365928292274475
I0217 00:32:13.919053 139535982765824 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.8176308274269104, loss=1.310390591621399
I0217 00:33:29.873140 139535991158528 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.9155068397521973, loss=1.348114013671875
I0217 00:34:45.919605 139535982765824 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.6605323553085327, loss=1.2813477516174316
I0217 00:36:01.909025 139535991158528 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.8075119256973267, loss=1.3280665874481201
I0217 00:37:22.671244 139535982765824 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.7931349277496338, loss=1.3549489974975586
I0217 00:38:43.720549 139535991158528 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.7847440838813782, loss=1.2434890270233154
I0217 00:39:59.595315 139535982765824 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.7866111397743225, loss=1.3217942714691162
I0217 00:41:15.555550 139535991158528 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.7682316303253174, loss=1.3215017318725586
I0217 00:42:31.578145 139535982765824 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.7171549201011658, loss=1.2898774147033691
I0217 00:43:47.818214 139535991158528 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.716343343257904, loss=1.2660744190216064
I0217 00:45:03.873614 139535982765824 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.7227204442024231, loss=1.311889410018921
I0217 00:46:19.902812 139535991158528 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.7853376269340515, loss=1.316402554512024
I0217 00:47:41.680874 139535982765824 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.8514710664749146, loss=1.349232792854309
I0217 00:49:02.842270 139535991158528 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.7217233180999756, loss=1.3126190900802612
I0217 00:50:24.322817 139535982765824 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.7223114967346191, loss=1.2979960441589355
I0217 00:51:47.061015 139535991158528 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.6817924380302429, loss=1.2834378480911255
I0217 00:53:02.977078 139535982765824 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.74882972240448, loss=1.2802577018737793
I0217 00:53:25.419018 139646656866112 spec.py:321] Evaluating on the training split.
I0217 00:54:19.200891 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 00:55:10.535925 139646656866112 spec.py:349] Evaluating on the test split.
I0217 00:55:36.612321 139646656866112 submission_runner.py:408] Time since start: 52015.58s, 	Step: 60931, 	{'train/ctc_loss': Array(0.22372997, dtype=float32), 'train/wer': 0.08363154616756922, 'validation/ctc_loss': Array(0.4961345, dtype=float32), 'validation/wer': 0.15193527520588548, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3011645, dtype=float32), 'test/wer': 0.1023500497633701, 'test/num_examples': 2472, 'score': 47564.641932964325, 'total_duration': 52015.58261036873, 'accumulated_submission_time': 47564.641932964325, 'accumulated_eval_time': 4446.514243841171, 'accumulated_logging_time': 1.8272485733032227}
I0217 00:55:36.655452 139535991158528 logging_writer.py:48] [60931] accumulated_eval_time=4446.514244, accumulated_logging_time=1.827249, accumulated_submission_time=47564.641933, global_step=60931, preemption_count=0, score=47564.641933, test/ctc_loss=0.30116450786590576, test/num_examples=2472, test/wer=0.102350, total_duration=52015.582610, train/ctc_loss=0.2237299680709839, train/wer=0.083632, validation/ctc_loss=0.496134489774704, validation/num_examples=5348, validation/wer=0.151935
I0217 00:56:29.563123 139535982765824 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.954862654209137, loss=1.27257239818573
I0217 00:57:45.262702 139535991158528 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.6642815470695496, loss=1.2961363792419434
I0217 00:59:01.157756 139535982765824 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.7719128727912903, loss=1.272463083267212
I0217 01:00:17.122404 139535991158528 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.8576540350914001, loss=1.2728996276855469
I0217 01:01:33.282848 139535982765824 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.8022361993789673, loss=1.2573885917663574
I0217 01:02:49.108847 139535991158528 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.7257493138313293, loss=1.3060500621795654
I0217 01:04:07.726932 139535982765824 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.8219820261001587, loss=1.2686210870742798
I0217 01:05:29.601336 139535991158528 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.809808075428009, loss=1.2921993732452393
I0217 01:06:54.840970 139535991158528 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.8522180318832397, loss=1.2692313194274902
I0217 01:08:10.863729 139535982765824 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.7535629868507385, loss=1.2467446327209473
I0217 01:09:26.836220 139535991158528 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.810338020324707, loss=1.286582112312317
I0217 01:10:42.912001 139535982765824 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.7008025646209717, loss=1.253345251083374
I0217 01:11:58.921060 139535991158528 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.7349610924720764, loss=1.2240592241287231
I0217 01:13:14.855089 139535982765824 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.689375102519989, loss=1.3109506368637085
I0217 01:14:30.988585 139535991158528 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.8104580640792847, loss=1.269726276397705
I0217 01:15:50.953212 139535982765824 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.8508029580116272, loss=1.3236037492752075
I0217 01:17:13.808903 139535991158528 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.7061586976051331, loss=1.2116402387619019
I0217 01:18:36.139322 139535982765824 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.692918062210083, loss=1.2555246353149414
I0217 01:19:37.144562 139646656866112 spec.py:321] Evaluating on the training split.
I0217 01:20:32.845512 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 01:21:24.477478 139646656866112 spec.py:349] Evaluating on the test split.
I0217 01:21:50.695384 139646656866112 submission_runner.py:408] Time since start: 53589.67s, 	Step: 62775, 	{'train/ctc_loss': Array(0.20581685, dtype=float32), 'train/wer': 0.07590667004818666, 'validation/ctc_loss': Array(0.47948486, dtype=float32), 'validation/wer': 0.14630661247188081, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2855554, dtype=float32), 'test/wer': 0.09950642861495339, 'test/num_examples': 2472, 'score': 49005.04101896286, 'total_duration': 53589.66577386856, 'accumulated_submission_time': 49005.04101896286, 'accumulated_eval_time': 4580.05917096138, 'accumulated_logging_time': 1.886429786682129}
I0217 01:21:50.738780 139535991158528 logging_writer.py:48] [62775] accumulated_eval_time=4580.059171, accumulated_logging_time=1.886430, accumulated_submission_time=49005.041019, global_step=62775, preemption_count=0, score=49005.041019, test/ctc_loss=0.2855553925037384, test/num_examples=2472, test/wer=0.099506, total_duration=53589.665774, train/ctc_loss=0.205816850066185, train/wer=0.075907, validation/ctc_loss=0.4794848561286926, validation/num_examples=5348, validation/wer=0.146307
I0217 01:22:10.476189 139535982765824 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.9490049481391907, loss=1.311781406402588
I0217 01:23:29.632457 139535991158528 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.8462753891944885, loss=1.2568470239639282
I0217 01:24:45.563290 139535982765824 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.73670494556427, loss=1.267110824584961
I0217 01:26:01.448388 139535991158528 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.7491948008537292, loss=1.2587621212005615
I0217 01:27:17.325669 139535982765824 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.8495112657546997, loss=1.2962009906768799
I0217 01:28:33.174025 139535991158528 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.8405752182006836, loss=1.2403258085250854
I0217 01:29:49.136019 139535982765824 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.7790614366531372, loss=1.2293559312820435
I0217 01:31:09.262869 139535991158528 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.8527961373329163, loss=1.2607924938201904
I0217 01:32:30.839099 139535982765824 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.8417361974716187, loss=1.2439817190170288
I0217 01:33:52.984503 139535991158528 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.7923113107681274, loss=1.2094987630844116
I0217 01:35:15.677181 139535982765824 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.8128862977027893, loss=1.2760776281356812
I0217 01:36:37.995063 139535991158528 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.0326330661773682, loss=1.2389692068099976
I0217 01:37:53.802902 139535982765824 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.0481529235839844, loss=1.2259784936904907
I0217 01:39:09.687969 139535991158528 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.8631454706192017, loss=1.2287650108337402
I0217 01:40:25.574346 139535982765824 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.8501891493797302, loss=1.2256025075912476
I0217 01:41:41.406556 139535991158528 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.698571503162384, loss=1.1952791213989258
I0217 01:42:57.316128 139535982765824 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.770137369632721, loss=1.2646831274032593
I0217 01:44:13.098441 139535991158528 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.6815495491027832, loss=1.235595941543579
I0217 01:45:33.516306 139535982765824 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.7538515329360962, loss=1.2420352697372437
I0217 01:45:50.909620 139646656866112 spec.py:321] Evaluating on the training split.
I0217 01:46:44.904145 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 01:47:35.922663 139646656866112 spec.py:349] Evaluating on the test split.
I0217 01:48:01.928121 139646656866112 submission_runner.py:408] Time since start: 55160.90s, 	Step: 64623, 	{'train/ctc_loss': Array(0.206059, dtype=float32), 'train/wer': 0.07740308378388618, 'validation/ctc_loss': Array(0.46793726, dtype=float32), 'validation/wer': 0.1434971084314085, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2775251, dtype=float32), 'test/wer': 0.09371762841996222, 'test/num_examples': 2472, 'score': 50445.12503552437, 'total_duration': 55160.89828634262, 'accumulated_submission_time': 50445.12503552437, 'accumulated_eval_time': 4711.07154917717, 'accumulated_logging_time': 1.944622278213501}
I0217 01:48:01.982202 139535991158528 logging_writer.py:48] [64623] accumulated_eval_time=4711.071549, accumulated_logging_time=1.944622, accumulated_submission_time=50445.125036, global_step=64623, preemption_count=0, score=50445.125036, test/ctc_loss=0.2775250971317291, test/num_examples=2472, test/wer=0.093718, total_duration=55160.898286, train/ctc_loss=0.20605899393558502, train/wer=0.077403, validation/ctc_loss=0.46793726086616516, validation/num_examples=5348, validation/wer=0.143497
I0217 01:49:00.924684 139535982765824 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.8114121556282043, loss=1.189414381980896
I0217 01:50:16.885400 139535991158528 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.9249159097671509, loss=1.2591274976730347
I0217 01:51:37.150620 139535991158528 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.858100175857544, loss=1.2339208126068115
I0217 01:52:53.001560 139535982765824 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.7077400088310242, loss=1.189877986907959
I0217 01:54:08.974019 139535991158528 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.6816277503967285, loss=1.1880717277526855
I0217 01:55:24.958324 139535982765824 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.9164705872535706, loss=1.220607042312622
I0217 01:56:40.930130 139535991158528 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.8592544794082642, loss=1.2289533615112305
I0217 01:57:56.770944 139535982765824 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.893039882183075, loss=1.2397923469543457
I0217 01:59:12.731636 139535991158528 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.8585427403450012, loss=1.2149865627288818
I0217 02:00:31.043779 139535982765824 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.7861037850379944, loss=1.2551218271255493
I0217 02:01:53.003028 139535991158528 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.6862159371376038, loss=1.1786479949951172
I0217 02:03:14.002854 139535982765824 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.8392031788825989, loss=1.1818450689315796
I0217 02:04:35.331159 139535991158528 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.8230587244033813, loss=1.2473227977752686
I0217 02:05:55.128266 139535991158528 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.7949668765068054, loss=1.1972519159317017
I0217 02:07:11.233699 139535982765824 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.8325950503349304, loss=1.2039623260498047
I0217 02:08:27.136981 139535991158528 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.7623010277748108, loss=1.1752219200134277
I0217 02:09:43.069876 139535982765824 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.83548504114151, loss=1.1810736656188965
I0217 02:10:58.853813 139535991158528 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.846781313419342, loss=1.2350088357925415
I0217 02:12:02.195902 139646656866112 spec.py:321] Evaluating on the training split.
I0217 02:12:55.569751 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 02:13:47.285939 139646656866112 spec.py:349] Evaluating on the test split.
I0217 02:14:13.631547 139646656866112 submission_runner.py:408] Time since start: 56732.60s, 	Step: 66485, 	{'train/ctc_loss': Array(0.17852962, dtype=float32), 'train/wer': 0.06609859684963174, 'validation/ctc_loss': Array(0.44406882, dtype=float32), 'validation/wer': 0.13647817565675777, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2626538, dtype=float32), 'test/wer': 0.09024434830296753, 'test/num_examples': 2472, 'score': 51885.24637913704, 'total_duration': 56732.60203003883, 'accumulated_submission_time': 51885.24637913704, 'accumulated_eval_time': 4842.50137925148, 'accumulated_logging_time': 2.0155465602874756}
I0217 02:14:13.678780 139535991158528 logging_writer.py:48] [66485] accumulated_eval_time=4842.501379, accumulated_logging_time=2.015547, accumulated_submission_time=51885.246379, global_step=66485, preemption_count=0, score=51885.246379, test/ctc_loss=0.26265379786491394, test/num_examples=2472, test/wer=0.090244, total_duration=56732.602030, train/ctc_loss=0.17852962017059326, train/wer=0.066099, validation/ctc_loss=0.4440688192844391, validation/num_examples=5348, validation/wer=0.136478
I0217 02:14:25.857236 139535982765824 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.8143144845962524, loss=1.2261780500411987
I0217 02:15:41.679194 139535991158528 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.823550820350647, loss=1.1537189483642578
I0217 02:16:57.562220 139535982765824 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.8976794481277466, loss=1.219871163368225
I0217 02:18:13.544580 139535991158528 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.8629843592643738, loss=1.2041738033294678
I0217 02:19:29.524027 139535982765824 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.8542017936706543, loss=1.1575756072998047
I0217 02:20:48.892771 139535991158528 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.7939006090164185, loss=1.1879504919052124
I0217 02:22:04.733926 139535982765824 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.8496114015579224, loss=1.1773027181625366
I0217 02:23:20.699543 139535991158528 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.8084697723388672, loss=1.1906408071517944
I0217 02:24:36.949990 139535982765824 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.8230544328689575, loss=1.190543532371521
I0217 02:25:52.848748 139535991158528 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.7917572855949402, loss=1.2052106857299805
I0217 02:27:08.644640 139535982765824 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.8438504934310913, loss=1.2013684511184692
I0217 02:28:24.616786 139535991158528 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.0430396795272827, loss=1.1274279356002808
I0217 02:29:44.723432 139535982765824 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.8557301759719849, loss=1.2075082063674927
I0217 02:31:06.783748 139535991158528 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.8780619502067566, loss=1.2121987342834473
I0217 02:32:27.797135 139535982765824 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.8657972812652588, loss=1.160605549812317
I0217 02:33:51.671674 139535991158528 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.821083664894104, loss=1.1379070281982422
I0217 02:35:07.487605 139535982765824 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.8504593372344971, loss=1.1953911781311035
I0217 02:36:23.299077 139535991158528 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.9364277720451355, loss=1.1957426071166992
I0217 02:37:39.191044 139535982765824 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.8366024494171143, loss=1.1363310813903809
I0217 02:38:13.806223 139646656866112 spec.py:321] Evaluating on the training split.
I0217 02:39:08.508563 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 02:40:00.529101 139646656866112 spec.py:349] Evaluating on the test split.
I0217 02:40:26.590251 139646656866112 submission_runner.py:408] Time since start: 58305.56s, 	Step: 68347, 	{'train/ctc_loss': Array(0.1791338, dtype=float32), 'train/wer': 0.0678948165873385, 'validation/ctc_loss': Array(0.4300354, dtype=float32), 'validation/wer': 0.13104260598395398, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25467312, dtype=float32), 'test/wer': 0.08520707655434363, 'test/num_examples': 2472, 'score': 53325.28448152542, 'total_duration': 58305.560836315155, 'accumulated_submission_time': 53325.28448152542, 'accumulated_eval_time': 4975.279711008072, 'accumulated_logging_time': 2.079103946685791}
I0217 02:40:26.636406 139535991158528 logging_writer.py:48] [68347] accumulated_eval_time=4975.279711, accumulated_logging_time=2.079104, accumulated_submission_time=53325.284482, global_step=68347, preemption_count=0, score=53325.284482, test/ctc_loss=0.2546731233596802, test/num_examples=2472, test/wer=0.085207, total_duration=58305.560836, train/ctc_loss=0.179133802652359, train/wer=0.067895, validation/ctc_loss=0.43003541231155396, validation/num_examples=5348, validation/wer=0.131043
I0217 02:41:07.534154 139535982765824 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.0559468269348145, loss=1.2044309377670288
I0217 02:42:23.774136 139535991158528 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.9775682687759399, loss=1.1715004444122314
I0217 02:43:39.734597 139535982765824 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.7786991596221924, loss=1.1543042659759521
I0217 02:44:55.661951 139535991158528 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.8217142820358276, loss=1.1421868801116943
I0217 02:46:11.571824 139535982765824 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.8226912617683411, loss=1.1315373182296753
I0217 02:47:28.320581 139535991158528 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.8277908563613892, loss=1.1865206956863403
I0217 02:48:49.526477 139535982765824 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.8625907897949219, loss=1.190532922744751
I0217 02:50:09.157148 139535991158528 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.737006664276123, loss=1.1474484205245972
I0217 02:51:25.130298 139535982765824 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.7681068778038025, loss=1.1158268451690674
I0217 02:52:41.148155 139535991158528 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.0330085754394531, loss=1.1282384395599365
I0217 02:53:57.177043 139535982765824 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.817717969417572, loss=1.1249899864196777
I0217 02:55:13.044030 139535991158528 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.8262796998023987, loss=1.1841334104537964
I0217 02:56:28.918279 139535982765824 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.8218091130256653, loss=1.139451503753662
I0217 02:57:46.775118 139535991158528 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.8967853784561157, loss=1.1714262962341309
I0217 02:59:07.590910 139535982765824 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.9407914280891418, loss=1.0873075723648071
I0217 03:00:29.697595 139535991158528 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.8299312591552734, loss=1.1151543855667114
I0217 03:01:51.568372 139535982765824 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.9017646908760071, loss=1.1355624198913574
I0217 03:03:13.414624 139535991158528 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.7725583910942078, loss=1.098698377609253
I0217 03:04:27.311164 139646656866112 spec.py:321] Evaluating on the training split.
I0217 03:05:21.503757 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 03:06:13.130964 139646656866112 spec.py:349] Evaluating on the test split.
I0217 03:06:39.165095 139646656866112 submission_runner.py:408] Time since start: 59878.14s, 	Step: 70199, 	{'train/ctc_loss': Array(0.1901347, dtype=float32), 'train/wer': 0.0665341131428934, 'validation/ctc_loss': Array(0.4159782, dtype=float32), 'validation/wer': 0.12617666084169266, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24319124, dtype=float32), 'test/wer': 0.08262750594113705, 'test/num_examples': 2472, 'score': 54765.86633563042, 'total_duration': 59878.135501384735, 'accumulated_submission_time': 54765.86633563042, 'accumulated_eval_time': 5107.127786159515, 'accumulated_logging_time': 2.1438803672790527}
I0217 03:06:39.210371 139535991158528 logging_writer.py:48] [70199] accumulated_eval_time=5107.127786, accumulated_logging_time=2.143880, accumulated_submission_time=54765.866336, global_step=70199, preemption_count=0, score=54765.866336, test/ctc_loss=0.24319124221801758, test/num_examples=2472, test/wer=0.082628, total_duration=59878.135501, train/ctc_loss=0.1901347041130066, train/wer=0.066534, validation/ctc_loss=0.41597819328308105, validation/num_examples=5348, validation/wer=0.126177
I0217 03:06:40.826916 139535982765824 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.8263013958930969, loss=1.1638500690460205
I0217 03:07:56.400046 139535991158528 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.8955203294754028, loss=1.1561026573181152
I0217 03:09:12.191092 139535982765824 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.9681805968284607, loss=1.1210232973098755
I0217 03:10:28.027154 139535991158528 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.7835597395896912, loss=1.1322041749954224
I0217 03:11:44.181527 139535982765824 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.8576491475105286, loss=1.0878349542617798
I0217 03:13:00.126547 139535991158528 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.9485182166099548, loss=1.1375327110290527
I0217 03:14:16.349384 139535982765824 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.9814954400062561, loss=1.1273775100708008
I0217 03:15:32.931144 139535991158528 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.8711439371109009, loss=1.1232043504714966
I0217 03:16:55.073752 139535982765824 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.9337177872657776, loss=1.1157159805297852
I0217 03:18:18.765710 139535991158528 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.0371204614639282, loss=1.0945669412612915
I0217 03:19:34.640286 139535982765824 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.7992281317710876, loss=1.098160982131958
I0217 03:20:50.576930 139535991158528 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.8892562985420227, loss=1.1327557563781738
I0217 03:22:06.431577 139535982765824 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.0237144231796265, loss=1.1409541368484497
I0217 03:23:22.345404 139535991158528 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.975174069404602, loss=1.1594213247299194
I0217 03:24:38.189628 139535982765824 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.9531388282775879, loss=1.1393803358078003
I0217 03:25:54.046777 139535991158528 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.019827961921692, loss=1.1643201112747192
I0217 03:27:15.017249 139535982765824 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.8323882222175598, loss=1.073819637298584
I0217 03:28:38.021831 139535991158528 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.8662435412406921, loss=1.1692372560501099
I0217 03:29:58.974362 139535982765824 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.0379431247711182, loss=1.1209172010421753
I0217 03:30:39.785127 139646656866112 spec.py:321] Evaluating on the training split.
I0217 03:31:35.979565 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 03:32:28.451383 139646656866112 spec.py:349] Evaluating on the test split.
I0217 03:32:55.003817 139646656866112 submission_runner.py:408] Time since start: 61453.97s, 	Step: 72051, 	{'train/ctc_loss': Array(0.13278739, dtype=float32), 'train/wer': 0.04944650206821238, 'validation/ctc_loss': Array(0.3995478, dtype=float32), 'validation/wer': 0.12247892871969646, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23223098, dtype=float32), 'test/wer': 0.0790932910852477, 'test/num_examples': 2472, 'score': 56206.35363817215, 'total_duration': 61453.973516225815, 'accumulated_submission_time': 56206.35363817215, 'accumulated_eval_time': 5242.33988404274, 'accumulated_logging_time': 2.204277276992798}
I0217 03:32:55.056146 139535991158528 logging_writer.py:48] [72051] accumulated_eval_time=5242.339884, accumulated_logging_time=2.204277, accumulated_submission_time=56206.353638, global_step=72051, preemption_count=0, score=56206.353638, test/ctc_loss=0.23223097622394562, test/num_examples=2472, test/wer=0.079093, total_duration=61453.973516, train/ctc_loss=0.13278739154338837, train/wer=0.049447, validation/ctc_loss=0.3995477855205536, validation/num_examples=5348, validation/wer=0.122479
I0217 03:33:36.467670 139535991158528 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.9226153492927551, loss=1.0694725513458252
I0217 03:34:52.219449 139535982765824 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.0118967294692993, loss=1.137555480003357
I0217 03:36:08.143763 139535991158528 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.9114291071891785, loss=1.104018211364746
I0217 03:37:24.155269 139535982765824 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.8167216181755066, loss=1.0784214735031128
I0217 03:38:40.171574 139535991158528 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.802332878112793, loss=1.1218570470809937
I0217 03:39:56.296506 139535982765824 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.9304152727127075, loss=1.1464760303497314
I0217 03:41:12.336656 139535991158528 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.9179664254188538, loss=1.034863829612732
I0217 03:42:33.170971 139535982765824 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.8742347359657288, loss=1.10768723487854
I0217 03:43:55.352550 139535991158528 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.8248040080070496, loss=1.1153136491775513
I0217 03:45:17.380450 139535982765824 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.05293869972229, loss=1.133868932723999
I0217 03:46:39.304932 139535991158528 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.9742206931114197, loss=1.138928771018982
I0217 03:48:00.575302 139535991158528 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.8463870882987976, loss=1.070702075958252
I0217 03:49:16.523748 139535982765824 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.7903537750244141, loss=1.0519806146621704
I0217 03:50:32.555366 139535991158528 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.9220399260520935, loss=1.0805082321166992
I0217 03:51:48.411669 139535982765824 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.9221400618553162, loss=1.1538068056106567
I0217 03:53:04.311197 139535991158528 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.9605646133422852, loss=1.0544010400772095
I0217 03:54:20.189147 139535982765824 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.8199202418327332, loss=1.1483923196792603
I0217 03:55:36.588968 139535991158528 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.0160130262374878, loss=1.1072988510131836
I0217 03:56:55.480731 139646656866112 spec.py:321] Evaluating on the training split.
I0217 03:57:50.992575 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 03:58:42.259360 139646656866112 spec.py:349] Evaluating on the test split.
I0217 03:59:09.019770 139646656866112 submission_runner.py:408] Time since start: 63027.99s, 	Step: 73899, 	{'train/ctc_loss': Array(0.13270302, dtype=float32), 'train/wer': 0.04926659246369541, 'validation/ctc_loss': Array(0.39282784, dtype=float32), 'validation/wer': 0.11941840369966306, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2275426, dtype=float32), 'test/wer': 0.07742774155546077, 'test/num_examples': 2472, 'score': 57646.68882584572, 'total_duration': 63027.989725112915, 'accumulated_submission_time': 57646.68882584572, 'accumulated_eval_time': 5375.872570037842, 'accumulated_logging_time': 2.272150993347168}
I0217 03:59:09.061874 139535991158528 logging_writer.py:48] [73899] accumulated_eval_time=5375.872570, accumulated_logging_time=2.272151, accumulated_submission_time=57646.688826, global_step=73899, preemption_count=0, score=57646.688826, test/ctc_loss=0.22754259407520294, test/num_examples=2472, test/wer=0.077428, total_duration=63027.989725, train/ctc_loss=0.1327030211687088, train/wer=0.049267, validation/ctc_loss=0.3928278386592865, validation/num_examples=5348, validation/wer=0.119418
I0217 03:59:10.679643 139535982765824 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.8968008160591125, loss=1.0545482635498047
I0217 04:00:26.298361 139535991158528 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.2025729417800903, loss=1.1270822286605835
I0217 04:01:42.079564 139535982765824 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.8878897428512573, loss=1.0607211589813232
I0217 04:03:01.492548 139535991158528 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.8483622670173645, loss=1.0169048309326172
I0217 04:04:17.412873 139535982765824 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.9623188972473145, loss=1.0870424509048462
I0217 04:05:33.537289 139535991158528 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.1308344602584839, loss=1.1077581644058228
I0217 04:06:49.438477 139535982765824 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.7341684699058533, loss=1.052085280418396
I0217 04:08:05.256054 139535991158528 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.9847225546836853, loss=1.07672119140625
I0217 04:09:21.215946 139535982765824 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.9041686058044434, loss=1.0726821422576904
I0217 04:10:37.251679 139535991158528 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.0549858808517456, loss=1.0393493175506592
I0217 04:11:58.083037 139535982765824 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.910023033618927, loss=1.058234691619873
I0217 04:13:20.111575 139535991158528 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.9715482592582703, loss=1.080890417098999
I0217 04:14:42.013626 139535982765824 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.7984574437141418, loss=1.114052176475525
I0217 04:16:07.290104 139535991158528 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.999762237071991, loss=1.0590860843658447
I0217 04:17:23.061244 139535982765824 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.2892926931381226, loss=1.06476891040802
I0217 04:18:38.952284 139535991158528 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.0862818956375122, loss=1.0657658576965332
I0217 04:19:55.110502 139535982765824 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.0328903198242188, loss=1.0169975757598877
I0217 04:21:11.067084 139535991158528 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.8431132435798645, loss=1.0371999740600586
I0217 04:22:27.082600 139535982765824 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.904154896736145, loss=1.0237984657287598
I0217 04:23:09.279034 139646656866112 spec.py:321] Evaluating on the training split.
I0217 04:24:03.146198 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 04:24:54.769834 139646656866112 spec.py:349] Evaluating on the test split.
I0217 04:25:20.961635 139646656866112 submission_runner.py:408] Time since start: 64599.93s, 	Step: 75757, 	{'train/ctc_loss': Array(0.16273986, dtype=float32), 'train/wer': 0.06216031307298651, 'validation/ctc_loss': Array(0.38568026, dtype=float32), 'validation/wer': 0.11660889965919075, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22376499, dtype=float32), 'test/wer': 0.07505128673856966, 'test/num_examples': 2472, 'score': 59086.815969944, 'total_duration': 64599.93062663078, 'accumulated_submission_time': 59086.815969944, 'accumulated_eval_time': 5507.54785490036, 'accumulated_logging_time': 2.3289687633514404}
I0217 04:25:21.008877 139535991158528 logging_writer.py:48] [75757] accumulated_eval_time=5507.547855, accumulated_logging_time=2.328969, accumulated_submission_time=59086.815970, global_step=75757, preemption_count=0, score=59086.815970, test/ctc_loss=0.22376498579978943, test/num_examples=2472, test/wer=0.075051, total_duration=64599.930627, train/ctc_loss=0.1627398580312729, train/wer=0.062160, validation/ctc_loss=0.38568025827407837, validation/num_examples=5348, validation/wer=0.116609
I0217 04:25:54.307363 139535982765824 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.2073352336883545, loss=1.0463054180145264
I0217 04:27:10.167277 139535991158528 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.9629313945770264, loss=1.0673989057540894
I0217 04:28:26.105242 139535982765824 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.0420584678649902, loss=1.0684276819229126
I0217 04:29:42.067428 139535991158528 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.91733318567276, loss=1.0530571937561035
I0217 04:30:57.952726 139535982765824 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.915142297744751, loss=1.052573800086975
I0217 04:32:17.440706 139535991158528 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.9604570269584656, loss=1.0434051752090454
I0217 04:33:33.495107 139535982765824 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.8906695246696472, loss=1.1093518733978271
I0217 04:34:49.447120 139535991158528 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.8405554294586182, loss=1.0634219646453857
I0217 04:36:05.533971 139535982765824 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.9361578822135925, loss=1.1467472314834595
I0217 04:37:21.735291 139535991158528 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.9106706380844116, loss=1.0821113586425781
I0217 04:38:37.844698 139535982765824 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.0272374153137207, loss=1.0717229843139648
I0217 04:39:54.332685 139535991158528 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.8450375199317932, loss=1.0857760906219482
I0217 04:41:16.545478 139535982765824 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.9028136134147644, loss=1.0709401369094849
I0217 04:42:37.890233 139535991158528 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.8963369727134705, loss=1.075864315032959
I0217 04:43:59.511654 139535982765824 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.9967113733291626, loss=1.102097749710083
I0217 04:45:21.304503 139535991158528 logging_writer.py:48] [77300] global_step=77300, grad_norm=1.0465936660766602, loss=1.0870699882507324
I0217 04:46:37.188759 139535982765824 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.8981620073318481, loss=1.0533926486968994
I0217 04:47:53.108327 139535991158528 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.9409304261207581, loss=1.0634621381759644
I0217 04:49:09.023072 139535982765824 logging_writer.py:48] [77600] global_step=77600, grad_norm=1.0263274908065796, loss=1.0873236656188965
I0217 04:49:21.629603 139646656866112 spec.py:321] Evaluating on the training split.
I0217 04:50:14.867830 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 04:51:06.758743 139646656866112 spec.py:349] Evaluating on the test split.
I0217 04:51:32.831831 139646656866112 submission_runner.py:408] Time since start: 66171.80s, 	Step: 77618, 	{'train/ctc_loss': Array(0.1638349, dtype=float32), 'train/wer': 0.06019522480349154, 'validation/ctc_loss': Array(0.3825297, dtype=float32), 'validation/wer': 0.1155468878225861, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21958135, dtype=float32), 'test/wer': 0.07427944671257083, 'test/num_examples': 2472, 'score': 60527.34537887573, 'total_duration': 66171.80233621597, 'accumulated_submission_time': 60527.34537887573, 'accumulated_eval_time': 5638.74435043335, 'accumulated_logging_time': 2.393007516860962}
I0217 04:51:32.878834 139535991158528 logging_writer.py:48] [77618] accumulated_eval_time=5638.744350, accumulated_logging_time=2.393008, accumulated_submission_time=60527.345379, global_step=77618, preemption_count=0, score=60527.345379, test/ctc_loss=0.21958135068416595, test/num_examples=2472, test/wer=0.074279, total_duration=66171.802336, train/ctc_loss=0.16383489966392517, train/wer=0.060195, validation/ctc_loss=0.38252970576286316, validation/num_examples=5348, validation/wer=0.115547
I0217 04:52:35.773794 139535982765824 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.8138797879219055, loss=1.0376806259155273
I0217 04:53:51.793326 139535991158528 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.8319488763809204, loss=1.051346778869629
I0217 04:55:08.128243 139535982765824 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.9257909655570984, loss=1.0668011903762817
I0217 04:56:24.157263 139535991158528 logging_writer.py:48] [78000] global_step=78000, grad_norm=1.318617582321167, loss=1.09024977684021
I0217 04:57:40.226270 139535982765824 logging_writer.py:48] [78100] global_step=78100, grad_norm=1.2648040056228638, loss=1.0981886386871338
I0217 04:58:56.548878 139535991158528 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.9028319716453552, loss=1.0593558549880981
I0217 05:00:19.531469 139535991158528 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.829371988773346, loss=1.0422230958938599
I0217 05:00:33.642811 139535982765824 logging_writer.py:48] [78320] global_step=78320, preemption_count=0, score=61068.045420
I0217 05:00:34.539771 139646656866112 checkpoints.py:490] Saving checkpoint at step: 78320
I0217 05:00:36.021673 139646656866112 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_2/librispeech_conformer_jax/trial_4/checkpoint_78320
I0217 05:00:36.049401 139646656866112 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_2/librispeech_conformer_jax/trial_4/checkpoint_78320.
I0217 05:00:39.160809 139646656866112 submission_runner.py:583] Tuning trial 4/5
I0217 05:00:39.161071 139646656866112 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.004958460849689891, one_minus_beta1=0.13625575743, beta2=0.6291854735396584, weight_decay=0.1147386261512052, warmup_factor=0.02)
I0217 05:00:39.208074 139646656866112 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.446594, dtype=float32), 'train/wer': 1.1269105188367123, 'validation/ctc_loss': Array(30.757864, dtype=float32), 'validation/wer': 1.1779352558965794, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.871214, dtype=float32), 'test/wer': 1.1899945158734995, 'test/num_examples': 2472, 'score': 34.868218183517456, 'total_duration': 167.16175413131714, 'accumulated_submission_time': 34.868218183517456, 'accumulated_eval_time': 132.29344940185547, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1822, {'train/ctc_loss': Array(4.786604, dtype=float32), 'train/wer': 0.9100190787680567, 'validation/ctc_loss': Array(4.6146393, dtype=float32), 'validation/wer': 0.8706855769137936, 'validation/num_examples': 5348, 'test/ctc_loss': Array(4.465194, dtype=float32), 'test/wer': 0.8685231450449902, 'test/num_examples': 2472, 'score': 1474.8262186050415, 'total_duration': 1718.332086801529, 'accumulated_submission_time': 1474.8262186050415, 'accumulated_eval_time': 243.40519452095032, 'accumulated_logging_time': 0.027191877365112305, 'global_step': 1822, 'preemption_count': 0}), (3672, {'train/ctc_loss': Array(2.9874406, dtype=float32), 'train/wer': 0.6618090910126309, 'validation/ctc_loss': Array(2.8403525, dtype=float32), 'validation/wer': 0.6263166533110632, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.4753306, dtype=float32), 'test/wer': 0.5817845753864278, 'test/num_examples': 2472, 'score': 2914.8294603824615, 'total_duration': 3284.181474685669, 'accumulated_submission_time': 2914.8294603824615, 'accumulated_eval_time': 369.1273944377899, 'accumulated_logging_time': 0.07270073890686035, 'global_step': 3672, 'preemption_count': 0}), (5527, {'train/ctc_loss': Array(1.8403991, dtype=float32), 'train/wer': 0.5122606049758368, 'validation/ctc_loss': Array(1.9006361, dtype=float32), 'validation/wer': 0.4987593770817846, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.5014671, dtype=float32), 'test/wer': 0.43704425893201715, 'test/num_examples': 2472, 'score': 4354.856928110123, 'total_duration': 4853.3082637786865, 'accumulated_submission_time': 4354.856928110123, 'accumulated_eval_time': 498.09877157211304, 'accumulated_logging_time': 0.12321734428405762, 'global_step': 5527, 'preemption_count': 0}), (7370, {'train/ctc_loss': Array(1.4814537, dtype=float32), 'train/wer': 0.43512802264188416, 'validation/ctc_loss': Array(1.549442, dtype=float32), 'validation/wer': 0.42388754260115663, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.1917499, dtype=float32), 'test/wer': 0.36481628176223263, 'test/num_examples': 2472, 'score': 5795.416927576065, 'total_duration': 6423.257106304169, 'accumulated_submission_time': 5795.416927576065, 'accumulated_eval_time': 627.3559744358063, 'accumulated_logging_time': 0.17549872398376465, 'global_step': 7370, 'preemption_count': 0}), (9210, {'train/ctc_loss': Array(1.5415139, dtype=float32), 'train/wer': 0.4593235089347876, 'validation/ctc_loss': Array(1.4839153, dtype=float32), 'validation/wer': 0.41729341456114777, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.1388015, dtype=float32), 'test/wer': 0.3562854183169825, 'test/num_examples': 2472, 'score': 7236.158366441727, 'total_duration': 7994.499317407608, 'accumulated_submission_time': 7236.158366441727, 'accumulated_eval_time': 757.7280685901642, 'accumulated_logging_time': 0.2269151210784912, 'global_step': 9210, 'preemption_count': 0}), (11039, {'train/ctc_loss': Array(1.1786427, dtype=float32), 'train/wer': 0.36860990982575104, 'validation/ctc_loss': Array(1.3118448, dtype=float32), 'validation/wer': 0.37750658930071346, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.9740689, dtype=float32), 'test/wer': 0.31235147157394433, 'test/num_examples': 2472, 'score': 8676.491871833801, 'total_duration': 9565.224183321, 'accumulated_submission_time': 8676.491871833801, 'accumulated_eval_time': 887.9946537017822, 'accumulated_logging_time': 0.2738659381866455, 'global_step': 11039, 'preemption_count': 0}), (12891, {'train/ctc_loss': Array(1.1353468, dtype=float32), 'train/wer': 0.3522497808195933, 'validation/ctc_loss': Array(1.1930368, dtype=float32), 'validation/wer': 0.3462834413045367, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.8644295, dtype=float32), 'test/wer': 0.2827778116304105, 'test/num_examples': 2472, 'score': 10116.507513999939, 'total_duration': 11135.100909948349, 'accumulated_submission_time': 10116.507513999939, 'accumulated_eval_time': 1017.7202196121216, 'accumulated_logging_time': 0.32880735397338867, 'global_step': 12891, 'preemption_count': 0}), (14741, {'train/ctc_loss': Array(1.091014, dtype=float32), 'train/wer': 0.33997963444395773, 'validation/ctc_loss': Array(1.137504, dtype=float32), 'validation/wer': 0.3313090744084111, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.8089821, dtype=float32), 'test/wer': 0.2671785184733817, 'test/num_examples': 2472, 'score': 11556.709911346436, 'total_duration': 12703.967337846756, 'accumulated_submission_time': 11556.709911346436, 'accumulated_eval_time': 1146.2477378845215, 'accumulated_logging_time': 0.38587427139282227, 'global_step': 14741, 'preemption_count': 0}), (16567, {'train/ctc_loss': Array(1.0301901, dtype=float32), 'train/wer': 0.32646981223341104, 'validation/ctc_loss': Array(1.10055, dtype=float32), 'validation/wer': 0.3198200372669608, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.77649456, dtype=float32), 'test/wer': 0.2576117644669226, 'test/num_examples': 2472, 'score': 12997.059435367584, 'total_duration': 14275.884579896927, 'accumulated_submission_time': 12997.059435367584, 'accumulated_eval_time': 1277.6439950466156, 'accumulated_logging_time': 0.4783322811126709, 'global_step': 16567, 'preemption_count': 0}), (18413, {'train/ctc_loss': Array(1.1991059, dtype=float32), 'train/wer': 0.3634266049870247, 'validation/ctc_loss': Array(1.2338159, dtype=float32), 'validation/wer': 0.35353408575262846, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.8797922, dtype=float32), 'test/wer': 0.281091950520992, 'test/num_examples': 2472, 'score': 14437.434829473495, 'total_duration': 15846.326422691345, 'accumulated_submission_time': 14437.434829473495, 'accumulated_eval_time': 1407.5817940235138, 'accumulated_logging_time': 0.5285263061523438, 'global_step': 18413, 'preemption_count': 0}), (20256, {'train/ctc_loss': Array(0.88449436, dtype=float32), 'train/wer': 0.288827478657715, 'validation/ctc_loss': Array(1.0200983, dtype=float32), 'validation/wer': 0.3036098747791498, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.7080025, dtype=float32), 'test/wer': 0.2387626185688461, 'test/num_examples': 2472, 'score': 15878.150474071503, 'total_duration': 17418.24999141693, 'accumulated_submission_time': 15878.150474071503, 'accumulated_eval_time': 1538.6586077213287, 'accumulated_logging_time': 0.5804932117462158, 'global_step': 20256, 'preemption_count': 0}), (22103, {'train/ctc_loss': Array(0.90416443, dtype=float32), 'train/wer': 0.2953098019868589, 'validation/ctc_loss': Array(0.9769954, dtype=float32), 'validation/wer': 0.2892244417196868, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.67560107, dtype=float32), 'test/wer': 0.2254585339101822, 'test/num_examples': 2472, 'score': 17318.73151898384, 'total_duration': 18990.002960205078, 'accumulated_submission_time': 17318.73151898384, 'accumulated_eval_time': 1669.7016806602478, 'accumulated_logging_time': 0.6308321952819824, 'global_step': 22103, 'preemption_count': 0}), (23950, {'train/ctc_loss': Array(0.83565265, dtype=float32), 'train/wer': 0.2703343074093714, 'validation/ctc_loss': Array(0.94258255, dtype=float32), 'validation/wer': 0.2817903588634542, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.64889586, dtype=float32), 'test/wer': 0.21910100948550768, 'test/num_examples': 2472, 'score': 18758.80566763878, 'total_duration': 20560.23240017891, 'accumulated_submission_time': 18758.80566763878, 'accumulated_eval_time': 1799.7231032848358, 'accumulated_logging_time': 0.6856436729431152, 'global_step': 23950, 'preemption_count': 0}), (25774, {'train/ctc_loss': Array(0.83382195, dtype=float32), 'train/wer': 0.2674261006030476, 'validation/ctc_loss': Array(0.90188086, dtype=float32), 'validation/wer': 0.27060061596686524, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.62141186, dtype=float32), 'test/wer': 0.20888428493083908, 'test/num_examples': 2472, 'score': 20198.753033638, 'total_duration': 22132.027775764465, 'accumulated_submission_time': 20198.753033638, 'accumulated_eval_time': 1931.4437320232391, 'accumulated_logging_time': 0.7362205982208252, 'global_step': 25774, 'preemption_count': 0}), (27620, {'train/ctc_loss': Array(0.7256441, dtype=float32), 'train/wer': 0.24243501468424766, 'validation/ctc_loss': Array(0.8796342, dtype=float32), 'validation/wer': 0.26477886017165975, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6013053, dtype=float32), 'test/wer': 0.20441573741189853, 'test/num_examples': 2472, 'score': 21638.790019273758, 'total_duration': 23703.770726919174, 'accumulated_submission_time': 21638.790019273758, 'accumulated_eval_time': 2063.012087583542, 'accumulated_logging_time': 0.7946665287017822, 'global_step': 27620, 'preemption_count': 0}), (29464, {'train/ctc_loss': Array(0.7058413, dtype=float32), 'train/wer': 0.2416801166096091, 'validation/ctc_loss': Array(0.8496082, dtype=float32), 'validation/wer': 0.2589764136825743, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5760872, dtype=float32), 'test/wer': 0.19574269290922755, 'test/num_examples': 2472, 'score': 23079.289704084396, 'total_duration': 25275.1791806221, 'accumulated_submission_time': 23079.289704084396, 'accumulated_eval_time': 2193.7856407165527, 'accumulated_logging_time': 0.8524501323699951, 'global_step': 29464, 'preemption_count': 0}), (31322, {'train/ctc_loss': Array(0.71590084, dtype=float32), 'train/wer': 0.24235520597800395, 'validation/ctc_loss': Array(0.8296148, dtype=float32), 'validation/wer': 0.24962105486739333, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5572826, dtype=float32), 'test/wer': 0.19068510958097212, 'test/num_examples': 2472, 'score': 24519.32209968567, 'total_duration': 26845.321516752243, 'accumulated_submission_time': 24519.32209968567, 'accumulated_eval_time': 2323.7597975730896, 'accumulated_logging_time': 0.9073889255523682, 'global_step': 31322, 'preemption_count': 0}), (33176, {'train/ctc_loss': Array(0.5189762, dtype=float32), 'train/wer': 0.18572372769332451, 'validation/ctc_loss': Array(0.810009, dtype=float32), 'validation/wer': 0.2471687729901426, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.54166, dtype=float32), 'test/wer': 0.18704933682692504, 'test/num_examples': 2472, 'score': 25959.6188287735, 'total_duration': 28427.117254018784, 'accumulated_submission_time': 25959.6188287735, 'accumulated_eval_time': 2465.121926546097, 'accumulated_logging_time': 0.965153694152832, 'global_step': 33176, 'preemption_count': 0}), (35020, {'train/ctc_loss': Array(0.4539365, dtype=float32), 'train/wer': 0.16255994365555315, 'validation/ctc_loss': Array(0.78835493, dtype=float32), 'validation/wer': 0.24026569605221237, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5160036, dtype=float32), 'test/wer': 0.17756382913899213, 'test/num_examples': 2472, 'score': 27400.167891979218, 'total_duration': 29999.389021396637, 'accumulated_submission_time': 27400.167891979218, 'accumulated_eval_time': 2596.715493917465, 'accumulated_logging_time': 1.015394926071167, 'global_step': 35020, 'preemption_count': 0}), (36863, {'train/ctc_loss': Array(0.44536912, dtype=float32), 'train/wer': 0.15809184128399464, 'validation/ctc_loss': Array(0.7607473, dtype=float32), 'validation/wer': 0.23202062233893625, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5011267, dtype=float32), 'test/wer': 0.16980480571974083, 'test/num_examples': 2472, 'score': 28840.363377332687, 'total_duration': 31573.44949054718, 'accumulated_submission_time': 28840.363377332687, 'accumulated_eval_time': 2730.4466235637665, 'accumulated_logging_time': 1.0714967250823975, 'global_step': 36863, 'preemption_count': 0}), (38718, {'train/ctc_loss': Array(0.41135126, dtype=float32), 'train/wer': 0.15001940039835485, 'validation/ctc_loss': Array(0.7361761, dtype=float32), 'validation/wer': 0.2231962694420576, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4796191, dtype=float32), 'test/wer': 0.1635691507728556, 'test/num_examples': 2472, 'score': 30280.498059034348, 'total_duration': 33146.26831173897, 'accumulated_submission_time': 30280.498059034348, 'accumulated_eval_time': 2862.9936952590942, 'accumulated_logging_time': 1.1281471252441406, 'global_step': 38718, 'preemption_count': 0}), (40574, {'train/ctc_loss': Array(0.40231338, dtype=float32), 'train/wer': 0.14547936483956425, 'validation/ctc_loss': Array(0.70659876, dtype=float32), 'validation/wer': 0.2144781177288394, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.45901236, dtype=float32), 'test/wer': 0.1562366705258668, 'test/num_examples': 2472, 'score': 31720.889266252518, 'total_duration': 34716.65667510033, 'accumulated_submission_time': 31720.889266252518, 'accumulated_eval_time': 2992.8555755615234, 'accumulated_logging_time': 1.185434341430664, 'global_step': 40574, 'preemption_count': 0}), (42424, {'train/ctc_loss': Array(0.37856817, dtype=float32), 'train/wer': 0.13991521244110516, 'validation/ctc_loss': Array(0.6976467, dtype=float32), 'validation/wer': 0.214381571198239, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4475745, dtype=float32), 'test/wer': 0.15205248512176792, 'test/num_examples': 2472, 'score': 33161.03370857239, 'total_duration': 36289.01644325256, 'accumulated_submission_time': 33161.03370857239, 'accumulated_eval_time': 3124.9306325912476, 'accumulated_logging_time': 1.2468442916870117, 'global_step': 42424, 'preemption_count': 0}), (44268, {'train/ctc_loss': Array(0.42395383, dtype=float32), 'train/wer': 0.1483157612824591, 'validation/ctc_loss': Array(0.67980665, dtype=float32), 'validation/wer': 0.207304710505228, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43433073, dtype=float32), 'test/wer': 0.1464464891434607, 'test/num_examples': 2472, 'score': 34601.290254592896, 'total_duration': 37859.777564525604, 'accumulated_submission_time': 34601.290254592896, 'accumulated_eval_time': 3255.2952768802643, 'accumulated_logging_time': 1.3067612648010254, 'global_step': 44268, 'preemption_count': 0}), (46107, {'train/ctc_loss': Array(0.3622536, dtype=float32), 'train/wer': 0.13474557649446997, 'validation/ctc_loss': Array(0.65977854, dtype=float32), 'validation/wer': 0.2022360176487058, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4211136, dtype=float32), 'test/wer': 0.14396847642841185, 'test/num_examples': 2472, 'score': 36041.80644035339, 'total_duration': 39434.40048265457, 'accumulated_submission_time': 36041.80644035339, 'accumulated_eval_time': 3389.2680366039276, 'accumulated_logging_time': 1.362666368484497, 'global_step': 46107, 'preemption_count': 0}), (47963, {'train/ctc_loss': Array(0.33330858, dtype=float32), 'train/wer': 0.1254129063868123, 'validation/ctc_loss': Array(0.6382336, dtype=float32), 'validation/wer': 0.19520743022099502, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40312463, dtype=float32), 'test/wer': 0.13724534357036947, 'test/num_examples': 2472, 'score': 37482.298840522766, 'total_duration': 41007.160016059875, 'accumulated_submission_time': 37482.298840522766, 'accumulated_eval_time': 3521.3973503112793, 'accumulated_logging_time': 1.421417236328125, 'global_step': 47963, 'preemption_count': 0}), (49822, {'train/ctc_loss': Array(0.31033447, dtype=float32), 'train/wer': 0.1149222233388164, 'validation/ctc_loss': Array(0.6196946, dtype=float32), 'validation/wer': 0.1900711547930525, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38547108, dtype=float32), 'test/wer': 0.13143623179574676, 'test/num_examples': 2472, 'score': 38922.43569779396, 'total_duration': 42578.880200862885, 'accumulated_submission_time': 38922.43569779396, 'accumulated_eval_time': 3652.8457939624786, 'accumulated_logging_time': 1.4763050079345703, 'global_step': 49822, 'preemption_count': 0}), (51669, {'train/ctc_loss': Array(0.29647183, dtype=float32), 'train/wer': 0.11162165491166343, 'validation/ctc_loss': Array(0.6016999, dtype=float32), 'validation/wer': 0.18108267279415313, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3817462, dtype=float32), 'test/wer': 0.12897853066032947, 'test/num_examples': 2472, 'score': 40362.72778439522, 'total_duration': 44151.99367618561, 'accumulated_submission_time': 40362.72778439522, 'accumulated_eval_time': 3785.52684879303, 'accumulated_logging_time': 1.5382416248321533, 'global_step': 51669, 'preemption_count': 0}), (53518, {'train/ctc_loss': Array(0.30620503, dtype=float32), 'train/wer': 0.11275602136665565, 'validation/ctc_loss': Array(0.57299715, dtype=float32), 'validation/wer': 0.17624569161107195, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35301486, dtype=float32), 'test/wer': 0.11983831982613288, 'test/num_examples': 2472, 'score': 41802.975400447845, 'total_duration': 45724.815037965775, 'accumulated_submission_time': 41802.975400447845, 'accumulated_eval_time': 3917.9606053829193, 'accumulated_logging_time': 1.5975022315979004, 'global_step': 53518, 'preemption_count': 0}), (55366, {'train/ctc_loss': Array(0.28244948, dtype=float32), 'train/wer': 0.10382260745166516, 'validation/ctc_loss': Array(0.55347747, dtype=float32), 'validation/wer': 0.17053979165258695, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33865485, dtype=float32), 'test/wer': 0.115654134422034, 'test/num_examples': 2472, 'score': 43243.05828857422, 'total_duration': 47298.76901054382, 'accumulated_submission_time': 43243.05828857422, 'accumulated_eval_time': 4051.69429731369, 'accumulated_logging_time': 1.6556503772735596, 'global_step': 55366, 'preemption_count': 0}), (57223, {'train/ctc_loss': Array(0.27888992, dtype=float32), 'train/wer': 0.10288397464705791, 'validation/ctc_loss': Array(0.54089737, dtype=float32), 'validation/wer': 0.16783648879577512, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3345989, dtype=float32), 'test/wer': 0.11490260597566673, 'test/num_examples': 2472, 'score': 44683.50492525101, 'total_duration': 48871.916709423065, 'accumulated_submission_time': 44683.50492525101, 'accumulated_eval_time': 4184.260848999023, 'accumulated_logging_time': 1.7116804122924805, 'global_step': 57223, 'preemption_count': 0}), (59078, {'train/ctc_loss': Array(0.24648692, dtype=float32), 'train/wer': 0.09246029030067232, 'validation/ctc_loss': Array(0.51485807, dtype=float32), 'validation/wer': 0.15991967328654044, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31451392, dtype=float32), 'test/wer': 0.10895131314362318, 'test/num_examples': 2472, 'score': 46124.065954208374, 'total_duration': 50443.68307852745, 'accumulated_submission_time': 46124.065954208374, 'accumulated_eval_time': 4315.326943397522, 'accumulated_logging_time': 1.7696788311004639, 'global_step': 59078, 'preemption_count': 0}), (60931, {'train/ctc_loss': Array(0.22372997, dtype=float32), 'train/wer': 0.08363154616756922, 'validation/ctc_loss': Array(0.4961345, dtype=float32), 'validation/wer': 0.15193527520588548, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3011645, dtype=float32), 'test/wer': 0.1023500497633701, 'test/num_examples': 2472, 'score': 47564.641932964325, 'total_duration': 52015.58261036873, 'accumulated_submission_time': 47564.641932964325, 'accumulated_eval_time': 4446.514243841171, 'accumulated_logging_time': 1.8272485733032227, 'global_step': 60931, 'preemption_count': 0}), (62775, {'train/ctc_loss': Array(0.20581685, dtype=float32), 'train/wer': 0.07590667004818666, 'validation/ctc_loss': Array(0.47948486, dtype=float32), 'validation/wer': 0.14630661247188081, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2855554, dtype=float32), 'test/wer': 0.09950642861495339, 'test/num_examples': 2472, 'score': 49005.04101896286, 'total_duration': 53589.66577386856, 'accumulated_submission_time': 49005.04101896286, 'accumulated_eval_time': 4580.05917096138, 'accumulated_logging_time': 1.886429786682129, 'global_step': 62775, 'preemption_count': 0}), (64623, {'train/ctc_loss': Array(0.206059, dtype=float32), 'train/wer': 0.07740308378388618, 'validation/ctc_loss': Array(0.46793726, dtype=float32), 'validation/wer': 0.1434971084314085, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2775251, dtype=float32), 'test/wer': 0.09371762841996222, 'test/num_examples': 2472, 'score': 50445.12503552437, 'total_duration': 55160.89828634262, 'accumulated_submission_time': 50445.12503552437, 'accumulated_eval_time': 4711.07154917717, 'accumulated_logging_time': 1.944622278213501, 'global_step': 64623, 'preemption_count': 0}), (66485, {'train/ctc_loss': Array(0.17852962, dtype=float32), 'train/wer': 0.06609859684963174, 'validation/ctc_loss': Array(0.44406882, dtype=float32), 'validation/wer': 0.13647817565675777, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2626538, dtype=float32), 'test/wer': 0.09024434830296753, 'test/num_examples': 2472, 'score': 51885.24637913704, 'total_duration': 56732.60203003883, 'accumulated_submission_time': 51885.24637913704, 'accumulated_eval_time': 4842.50137925148, 'accumulated_logging_time': 2.0155465602874756, 'global_step': 66485, 'preemption_count': 0}), (68347, {'train/ctc_loss': Array(0.1791338, dtype=float32), 'train/wer': 0.0678948165873385, 'validation/ctc_loss': Array(0.4300354, dtype=float32), 'validation/wer': 0.13104260598395398, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25467312, dtype=float32), 'test/wer': 0.08520707655434363, 'test/num_examples': 2472, 'score': 53325.28448152542, 'total_duration': 58305.560836315155, 'accumulated_submission_time': 53325.28448152542, 'accumulated_eval_time': 4975.279711008072, 'accumulated_logging_time': 2.079103946685791, 'global_step': 68347, 'preemption_count': 0}), (70199, {'train/ctc_loss': Array(0.1901347, dtype=float32), 'train/wer': 0.0665341131428934, 'validation/ctc_loss': Array(0.4159782, dtype=float32), 'validation/wer': 0.12617666084169266, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24319124, dtype=float32), 'test/wer': 0.08262750594113705, 'test/num_examples': 2472, 'score': 54765.86633563042, 'total_duration': 59878.135501384735, 'accumulated_submission_time': 54765.86633563042, 'accumulated_eval_time': 5107.127786159515, 'accumulated_logging_time': 2.1438803672790527, 'global_step': 70199, 'preemption_count': 0}), (72051, {'train/ctc_loss': Array(0.13278739, dtype=float32), 'train/wer': 0.04944650206821238, 'validation/ctc_loss': Array(0.3995478, dtype=float32), 'validation/wer': 0.12247892871969646, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23223098, dtype=float32), 'test/wer': 0.0790932910852477, 'test/num_examples': 2472, 'score': 56206.35363817215, 'total_duration': 61453.973516225815, 'accumulated_submission_time': 56206.35363817215, 'accumulated_eval_time': 5242.33988404274, 'accumulated_logging_time': 2.204277276992798, 'global_step': 72051, 'preemption_count': 0}), (73899, {'train/ctc_loss': Array(0.13270302, dtype=float32), 'train/wer': 0.04926659246369541, 'validation/ctc_loss': Array(0.39282784, dtype=float32), 'validation/wer': 0.11941840369966306, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2275426, dtype=float32), 'test/wer': 0.07742774155546077, 'test/num_examples': 2472, 'score': 57646.68882584572, 'total_duration': 63027.989725112915, 'accumulated_submission_time': 57646.68882584572, 'accumulated_eval_time': 5375.872570037842, 'accumulated_logging_time': 2.272150993347168, 'global_step': 73899, 'preemption_count': 0}), (75757, {'train/ctc_loss': Array(0.16273986, dtype=float32), 'train/wer': 0.06216031307298651, 'validation/ctc_loss': Array(0.38568026, dtype=float32), 'validation/wer': 0.11660889965919075, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22376499, dtype=float32), 'test/wer': 0.07505128673856966, 'test/num_examples': 2472, 'score': 59086.815969944, 'total_duration': 64599.93062663078, 'accumulated_submission_time': 59086.815969944, 'accumulated_eval_time': 5507.54785490036, 'accumulated_logging_time': 2.3289687633514404, 'global_step': 75757, 'preemption_count': 0}), (77618, {'train/ctc_loss': Array(0.1638349, dtype=float32), 'train/wer': 0.06019522480349154, 'validation/ctc_loss': Array(0.3825297, dtype=float32), 'validation/wer': 0.1155468878225861, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21958135, dtype=float32), 'test/wer': 0.07427944671257083, 'test/num_examples': 2472, 'score': 60527.34537887573, 'total_duration': 66171.80233621597, 'accumulated_submission_time': 60527.34537887573, 'accumulated_eval_time': 5638.74435043335, 'accumulated_logging_time': 2.393007516860962, 'global_step': 77618, 'preemption_count': 0})], 'global_step': 78320}
I0217 05:00:39.208367 139646656866112 submission_runner.py:586] Timing: 61068.04541969299
I0217 05:00:39.208441 139646656866112 submission_runner.py:588] Total number of evals: 43
I0217 05:00:39.208507 139646656866112 submission_runner.py:589] ====================
I0217 05:00:39.208576 139646656866112 submission_runner.py:542] Using RNG seed 1365630931
I0217 05:00:39.210886 139646656866112 submission_runner.py:551] --- Tuning run 5/5 ---
I0217 05:00:39.211017 139646656866112 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_2/librispeech_conformer_jax/trial_5.
I0217 05:00:39.213143 139646656866112 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_2/librispeech_conformer_jax/trial_5/hparams.json.
I0217 05:00:39.214394 139646656866112 submission_runner.py:206] Initializing dataset.
I0217 05:00:39.214520 139646656866112 submission_runner.py:213] Initializing model.
I0217 05:00:42.974509 139646656866112 submission_runner.py:255] Initializing optimizer.
I0217 05:00:43.435757 139646656866112 submission_runner.py:262] Initializing metrics bundle.
I0217 05:00:43.435963 139646656866112 submission_runner.py:280] Initializing checkpoint and logger.
I0217 05:00:43.440625 139646656866112 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_2/librispeech_conformer_jax/trial_5 with prefix checkpoint_
I0217 05:00:43.440773 139646656866112 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_2/librispeech_conformer_jax/trial_5/meta_data_0.json.
I0217 05:00:43.441028 139646656866112 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0217 05:00:43.441109 139646656866112 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0217 05:00:44.088671 139646656866112 logger_utils.py:220] Unable to record git information. Continuing without it.
I0217 05:00:44.692532 139646656866112 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_2/librispeech_conformer_jax/trial_5/flags_0.json.
I0217 05:00:44.710904 139646656866112 submission_runner.py:314] Starting training loop.
I0217 05:00:44.714352 139646656866112 input_pipeline.py:20] Loading split = train-clean-100
I0217 05:00:44.758372 139646656866112 input_pipeline.py:20] Loading split = train-clean-360
I0217 05:00:45.281953 139646656866112 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0217 05:01:21.344371 139475961702144 logging_writer.py:48] [0] global_step=0, grad_norm=40.05400466918945, loss=32.430747985839844
I0217 05:01:21.372122 139646656866112 spec.py:321] Evaluating on the training split.
I0217 05:02:14.678721 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 05:03:08.682218 139646656866112 spec.py:349] Evaluating on the test split.
I0217 05:03:35.788300 139646656866112 submission_runner.py:408] Time since start: 171.07s, 	Step: 1, 	{'train/ctc_loss': Array(32.05279, dtype=float32), 'train/wer': 1.151012912416401, 'validation/ctc_loss': Array(30.757864, dtype=float32), 'validation/wer': 1.1779352558965794, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.871214, dtype=float32), 'test/wer': 1.190014827453131, 'test/num_examples': 2472, 'score': 36.661099910736084, 'total_duration': 171.07474374771118, 'accumulated_submission_time': 36.661099910736084, 'accumulated_eval_time': 134.41353964805603, 'accumulated_logging_time': 0}
I0217 05:03:35.804725 139535991158528 logging_writer.py:48] [1] accumulated_eval_time=134.413540, accumulated_logging_time=0, accumulated_submission_time=36.661100, global_step=1, preemption_count=0, score=36.661100, test/ctc_loss=30.871213912963867, test/num_examples=2472, test/wer=1.190015, total_duration=171.074744, train/ctc_loss=32.052791595458984, train/wer=1.151013, validation/ctc_loss=30.757863998413086, validation/num_examples=5348, validation/wer=1.177935
I0217 05:05:19.437936 139476022208256 logging_writer.py:48] [100] global_step=100, grad_norm=0.8678711652755737, loss=5.921175479888916
I0217 05:06:36.350666 139476030600960 logging_writer.py:48] [200] global_step=200, grad_norm=1.399477243423462, loss=5.844816207885742
I0217 05:07:53.301423 139476022208256 logging_writer.py:48] [300] global_step=300, grad_norm=1.1309179067611694, loss=5.807119846343994
I0217 05:09:10.428815 139476030600960 logging_writer.py:48] [400] global_step=400, grad_norm=1.3421690464019775, loss=5.789923667907715
I0217 05:10:27.479542 139476022208256 logging_writer.py:48] [500] global_step=500, grad_norm=0.3804657757282257, loss=5.804637432098389
I0217 05:11:44.617988 139476030600960 logging_writer.py:48] [600] global_step=600, grad_norm=1.1254737377166748, loss=5.74806022644043
I0217 05:13:01.431259 139476022208256 logging_writer.py:48] [700] global_step=700, grad_norm=0.6958832740783691, loss=5.558237552642822
I0217 05:14:18.238107 139476030600960 logging_writer.py:48] [800] global_step=800, grad_norm=1.8781445026397705, loss=5.463981628417969
I0217 05:15:34.949018 139476022208256 logging_writer.py:48] [900] global_step=900, grad_norm=2.2855212688446045, loss=4.691917896270752
I0217 05:16:56.196009 139476030600960 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.0702142715454102, loss=3.781038284301758
I0217 05:18:17.401210 139535991158528 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.4159647226333618, loss=3.454434394836426
I0217 05:19:33.916312 139535982765824 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.2351468801498413, loss=3.197791576385498
I0217 05:20:50.652124 139535991158528 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.6949177980422974, loss=3.0547773838043213
I0217 05:22:07.093645 139535982765824 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.0988444089889526, loss=2.795135021209717
I0217 05:23:23.592172 139535991158528 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.7069332599639893, loss=2.6875314712524414
I0217 05:24:39.884055 139535982765824 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.6352713108062744, loss=2.6784796714782715
I0217 05:25:58.827109 139535991158528 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.8306547403335571, loss=2.52502179145813
I0217 05:27:21.284073 139535982765824 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.9893712997436523, loss=2.456894636154175
I0217 05:27:36.715460 139646656866112 spec.py:321] Evaluating on the training split.
I0217 05:28:24.019463 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 05:29:13.873622 139646656866112 spec.py:349] Evaluating on the test split.
I0217 05:29:39.810433 139646656866112 submission_runner.py:408] Time since start: 1735.09s, 	Step: 1820, 	{'train/ctc_loss': Array(3.4769754, dtype=float32), 'train/wer': 0.6580758933017746, 'validation/ctc_loss': Array(3.3963308, dtype=float32), 'validation/wer': 0.6373519217586916, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.0782773, dtype=float32), 'test/wer': 0.5873296366258404, 'test/num_examples': 2472, 'score': 1477.4896006584167, 'total_duration': 1735.0940651893616, 'accumulated_submission_time': 1477.4896006584167, 'accumulated_eval_time': 257.50309681892395, 'accumulated_logging_time': 0.027042150497436523}
I0217 05:29:39.846650 139535991158528 logging_writer.py:48] [1820] accumulated_eval_time=257.503097, accumulated_logging_time=0.027042, accumulated_submission_time=1477.489601, global_step=1820, preemption_count=0, score=1477.489601, test/ctc_loss=3.078277349472046, test/num_examples=2472, test/wer=0.587330, total_duration=1735.094065, train/ctc_loss=3.476975440979004, train/wer=0.658076, validation/ctc_loss=3.3963308334350586, validation/num_examples=5348, validation/wer=0.637352
I0217 05:30:41.350961 139535982765824 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.639613687992096, loss=2.369795083999634
I0217 05:31:57.668447 139535991158528 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.6139591932296753, loss=2.26991605758667
I0217 05:33:17.674072 139535991158528 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.0443202257156372, loss=2.260014772415161
I0217 05:34:33.923202 139535982765824 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.0483989715576172, loss=2.2689146995544434
I0217 05:35:50.281393 139535991158528 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.7374526262283325, loss=2.1951730251312256
I0217 05:37:06.573451 139535982765824 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.8675658702850342, loss=2.0685219764709473
I0217 05:38:22.996361 139535991158528 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.6594621539115906, loss=2.090059280395508
I0217 05:39:39.401926 139535982765824 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.570997953414917, loss=2.0793490409851074
I0217 05:40:57.429589 139535991158528 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.5505874156951904, loss=2.0413622856140137
I0217 05:42:20.290181 139535982765824 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.5560230612754822, loss=1.974942922592163
I0217 05:43:42.607905 139535991158528 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.5325471758842468, loss=1.9396660327911377
I0217 05:45:04.970022 139535982765824 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.5160131454467773, loss=1.972864031791687
I0217 05:46:29.800158 139535991158528 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.6586058735847473, loss=1.9553022384643555
I0217 05:47:46.022422 139535982765824 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.5328167080879211, loss=1.9048091173171997
I0217 05:49:02.121978 139535991158528 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.5376849174499512, loss=1.8488080501556396
I0217 05:50:18.444515 139535982765824 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.9076904654502869, loss=1.8863325119018555
I0217 05:51:34.787858 139535991158528 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.8665698766708374, loss=1.8516740798950195
I0217 05:52:51.109368 139535982765824 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.7695096731185913, loss=1.8010722398757935
I0217 05:53:39.946107 139646656866112 spec.py:321] Evaluating on the training split.
I0217 05:54:32.310904 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 05:55:24.539219 139646656866112 spec.py:349] Evaluating on the test split.
I0217 05:55:51.558339 139646656866112 submission_runner.py:408] Time since start: 3306.84s, 	Step: 3665, 	{'train/ctc_loss': Array(0.88411194, dtype=float32), 'train/wer': 0.27776975049271474, 'validation/ctc_loss': Array(0.9202193, dtype=float32), 'validation/wer': 0.26832211784469523, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.64782745, dtype=float32), 'test/wer': 0.2096155017975748, 'test/num_examples': 2472, 'score': 2917.4977061748505, 'total_duration': 3306.8418798446655, 'accumulated_submission_time': 2917.4977061748505, 'accumulated_eval_time': 389.10982155799866, 'accumulated_logging_time': 0.08170270919799805}
I0217 05:55:51.590203 139535991158528 logging_writer.py:48] [3665] accumulated_eval_time=389.109822, accumulated_logging_time=0.081703, accumulated_submission_time=2917.497706, global_step=3665, preemption_count=0, score=2917.497706, test/ctc_loss=0.6478274464607239, test/num_examples=2472, test/wer=0.209616, total_duration=3306.841880, train/ctc_loss=0.8841119408607483, train/wer=0.277770, validation/ctc_loss=0.9202193021774292, validation/num_examples=5348, validation/wer=0.268322
I0217 05:56:19.012576 139535982765824 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.5603813529014587, loss=1.8857011795043945
I0217 05:57:35.283698 139535991158528 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.4976743459701538, loss=1.8338881731033325
I0217 05:58:51.732278 139535982765824 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.5044698715209961, loss=1.8178126811981201
I0217 06:00:08.139292 139535991158528 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.577928364276886, loss=1.7818881273269653
I0217 06:01:24.558580 139535982765824 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.6451705694198608, loss=1.840824007987976
I0217 06:02:44.424695 139535991158528 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.6792080998420715, loss=1.8115078210830688
I0217 06:04:00.752491 139535982765824 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.6160039901733398, loss=1.7647157907485962
I0217 06:05:17.233681 139535991158528 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.5678123235702515, loss=1.7052522897720337
I0217 06:06:33.594722 139535982765824 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.5014483332633972, loss=1.7257274389266968
I0217 06:07:50.035570 139535991158528 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.49466732144355774, loss=1.7240711450576782
I0217 06:09:06.482993 139535982765824 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.6593073010444641, loss=1.739229440689087
I0217 06:10:27.279848 139535991158528 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.44986480474472046, loss=1.7571301460266113
I0217 06:11:49.803729 139535982765824 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.46477848291397095, loss=1.7649542093276978
I0217 06:13:11.712895 139535991158528 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.5092620253562927, loss=1.7440035343170166
I0217 06:14:34.328296 139535982765824 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.5862955451011658, loss=1.7226005792617798
I0217 06:15:56.750565 139535991158528 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.4567519724369049, loss=1.689218521118164
I0217 06:17:12.938852 139535982765824 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.41298994421958923, loss=1.626887321472168
I0217 06:18:29.284923 139535991158528 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.662205159664154, loss=1.7150800228118896
I0217 06:19:45.610642 139535982765824 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.5209040641784668, loss=1.6784260272979736
I0217 06:19:52.185695 139646656866112 spec.py:321] Evaluating on the training split.
I0217 06:20:46.208400 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 06:21:38.508612 139646656866112 spec.py:349] Evaluating on the test split.
I0217 06:22:05.120473 139646656866112 submission_runner.py:408] Time since start: 4880.40s, 	Step: 5510, 	{'train/ctc_loss': Array(0.5478368, dtype=float32), 'train/wer': 0.18741153359705068, 'validation/ctc_loss': Array(0.74291396, dtype=float32), 'validation/wer': 0.22239493323807408, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4876269, dtype=float32), 'test/wer': 0.16385351288769728, 'test/num_examples': 2472, 'score': 4358.005998134613, 'total_duration': 4880.403652906418, 'accumulated_submission_time': 4358.005998134613, 'accumulated_eval_time': 522.0387523174286, 'accumulated_logging_time': 0.12815475463867188}
I0217 06:22:05.153383 139535991158528 logging_writer.py:48] [5510] accumulated_eval_time=522.038752, accumulated_logging_time=0.128155, accumulated_submission_time=4358.005998, global_step=5510, preemption_count=0, score=4358.005998, test/ctc_loss=0.48762691020965576, test/num_examples=2472, test/wer=0.163854, total_duration=4880.403653, train/ctc_loss=0.5478367805480957, train/wer=0.187412, validation/ctc_loss=0.7429139614105225, validation/num_examples=5348, validation/wer=0.222395
I0217 06:23:14.343502 139535982765824 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.5193321108818054, loss=1.6462604999542236
I0217 06:24:30.550956 139535991158528 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.8145882487297058, loss=1.7225289344787598
I0217 06:25:46.714383 139535982765824 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.5255208611488342, loss=1.6848390102386475
I0217 06:27:02.869695 139535991158528 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.6946115493774414, loss=1.6771905422210693
I0217 06:28:20.592944 139535982765824 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.5947554707527161, loss=1.6350091695785522
I0217 06:29:43.686451 139535991158528 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.5491195917129517, loss=1.5891451835632324
I0217 06:31:08.065759 139535991158528 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.5759020447731018, loss=1.6771960258483887
I0217 06:32:24.132107 139535982765824 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.5292258262634277, loss=1.6636919975280762
I0217 06:33:40.423226 139535991158528 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.5267031788825989, loss=1.6371427774429321
I0217 06:34:56.669811 139535982765824 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.4488884508609772, loss=1.6131348609924316
I0217 06:36:12.903717 139535991158528 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.5000633597373962, loss=1.6090776920318604
I0217 06:37:29.126877 139535982765824 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.4804130792617798, loss=1.600354790687561
I0217 06:38:46.174023 139535991158528 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.6016054153442383, loss=1.6523373126983643
I0217 06:40:09.374676 139535982765824 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.49184733629226685, loss=1.5918939113616943
I0217 06:41:31.001646 139535991158528 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.5358269214630127, loss=1.5788990259170532
I0217 06:42:53.210515 139535982765824 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.4302276372909546, loss=1.5920034646987915
I0217 06:44:14.616682 139535991158528 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.5442089438438416, loss=1.615947961807251
I0217 06:45:34.922747 139535991158528 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.5036276578903198, loss=1.5633881092071533
I0217 06:46:05.832427 139646656866112 spec.py:321] Evaluating on the training split.
I0217 06:46:59.060701 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 06:47:50.903440 139646656866112 spec.py:349] Evaluating on the test split.
I0217 06:48:17.180297 139646656866112 submission_runner.py:408] Time since start: 6452.46s, 	Step: 7342, 	{'train/ctc_loss': Array(0.55028814, dtype=float32), 'train/wer': 0.18481111269599185, 'validation/ctc_loss': Array(0.6664708, dtype=float32), 'validation/wer': 0.20184017687324407, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42051786, dtype=float32), 'test/wer': 0.14327788272093922, 'test/num_examples': 2472, 'score': 5798.597426176071, 'total_duration': 6452.462328910828, 'accumulated_submission_time': 5798.597426176071, 'accumulated_eval_time': 653.3796038627625, 'accumulated_logging_time': 0.17641258239746094}
I0217 06:48:17.215610 139535991158528 logging_writer.py:48] [7342] accumulated_eval_time=653.379604, accumulated_logging_time=0.176413, accumulated_submission_time=5798.597426, global_step=7342, preemption_count=0, score=5798.597426, test/ctc_loss=0.42051786184310913, test/num_examples=2472, test/wer=0.143278, total_duration=6452.462329, train/ctc_loss=0.5502881407737732, train/wer=0.184811, validation/ctc_loss=0.6664708256721497, validation/num_examples=5348, validation/wer=0.201840
I0217 06:49:01.940084 139535982765824 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.5064025521278381, loss=1.5631448030471802
I0217 06:50:18.278346 139535991158528 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.4770142138004303, loss=1.5995910167694092
I0217 06:51:34.551970 139535982765824 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.7299109101295471, loss=1.5902377367019653
I0217 06:52:50.710497 139535991158528 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.6322739124298096, loss=1.5337337255477905
I0217 06:54:06.954310 139535982765824 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.5943479537963867, loss=1.559424877166748
I0217 06:55:23.194962 139535991158528 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.4939352869987488, loss=1.5999648571014404
I0217 06:56:40.600270 139535982765824 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.4688147008419037, loss=1.605318546295166
I0217 06:58:03.018271 139535991158528 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.7252369523048401, loss=1.5731971263885498
I0217 06:59:25.766145 139535982765824 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.5221072435379028, loss=1.5541428327560425
I0217 07:00:48.053061 139535991158528 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.7081268429756165, loss=1.5180931091308594
I0217 07:02:04.407910 139535982765824 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.46469447016716003, loss=1.5793498754501343
I0217 07:03:20.988353 139535991158528 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.5814555883407593, loss=1.5882083177566528
I0217 07:04:37.307671 139535982765824 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.4651067852973938, loss=1.526946783065796
I0217 07:05:53.659170 139535991158528 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.5943936705589294, loss=1.6139559745788574
I0217 07:07:10.117384 139535982765824 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.5679696798324585, loss=1.5567833185195923
I0217 07:08:29.011456 139535991158528 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.3879958391189575, loss=1.551493763923645
I0217 07:09:51.213155 139535982765824 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.5570431351661682, loss=1.5035037994384766
I0217 07:11:14.850868 139535991158528 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.46820569038391113, loss=1.589043140411377
I0217 07:12:17.540813 139646656866112 spec.py:321] Evaluating on the training split.
I0217 07:13:12.734385 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 07:14:05.259934 139646656866112 spec.py:349] Evaluating on the test split.
I0217 07:14:31.616663 139646656866112 submission_runner.py:408] Time since start: 8026.90s, 	Step: 9177, 	{'train/ctc_loss': Array(0.45161825, dtype=float32), 'train/wer': 0.15622523916638748, 'validation/ctc_loss': Array(0.6241024, dtype=float32), 'validation/wer': 0.18807264160962375, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39491493, dtype=float32), 'test/wer': 0.13308146974590213, 'test/num_examples': 2472, 'score': 7238.834161758423, 'total_duration': 8026.899383306503, 'accumulated_submission_time': 7238.834161758423, 'accumulated_eval_time': 787.4491305351257, 'accumulated_logging_time': 0.22736024856567383}
I0217 07:14:31.654494 139535991158528 logging_writer.py:48] [9177] accumulated_eval_time=787.449131, accumulated_logging_time=0.227360, accumulated_submission_time=7238.834162, global_step=9177, preemption_count=0, score=7238.834162, test/ctc_loss=0.3949149250984192, test/num_examples=2472, test/wer=0.133081, total_duration=8026.899383, train/ctc_loss=0.4516182541847229, train/wer=0.156225, validation/ctc_loss=0.6241024136543274, validation/num_examples=5348, validation/wer=0.188073
I0217 07:14:49.955942 139535982765824 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.47940075397491455, loss=1.5613925457000732
I0217 07:16:09.785980 139535991158528 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.5081961750984192, loss=1.5406215190887451
I0217 07:17:26.044161 139535982765824 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.4974510073661804, loss=1.4998605251312256
I0217 07:18:42.215452 139535991158528 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.6455264091491699, loss=1.5598413944244385
I0217 07:19:58.497478 139535982765824 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.48044103384017944, loss=1.5267000198364258
I0217 07:21:15.063777 139535991158528 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.47191303968429565, loss=1.568246841430664
I0217 07:22:31.444000 139535982765824 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.5239132642745972, loss=1.5057098865509033
I0217 07:23:50.641771 139535991158528 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.5313485264778137, loss=1.5635402202606201
I0217 07:25:13.380824 139535982765824 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.6323269605636597, loss=1.5692036151885986
I0217 07:26:36.794073 139535991158528 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.5131986737251282, loss=1.4932767152786255
I0217 07:27:59.401171 139535982765824 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.5522921085357666, loss=1.5522761344909668
I0217 07:29:25.420169 139535991158528 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.46875935792922974, loss=1.4968345165252686
I0217 07:30:41.542852 139535982765824 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.5704801678657532, loss=1.468915343284607
I0217 07:31:57.702646 139535991158528 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.541327714920044, loss=1.492659330368042
I0217 07:33:13.952996 139535982765824 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.44170117378234863, loss=1.4829062223434448
I0217 07:34:30.137578 139535991158528 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.5782831311225891, loss=1.5243785381317139
I0217 07:35:46.464714 139535982765824 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.48022785782814026, loss=1.4652856588363647
I0217 07:37:04.985327 139535991158528 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.46356621384620667, loss=1.4880419969558716
I0217 07:38:27.505409 139535982765824 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.5588077306747437, loss=1.4286887645721436
I0217 07:38:31.873429 139646656866112 spec.py:321] Evaluating on the training split.
I0217 07:39:26.395075 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 07:40:18.300201 139646656866112 spec.py:349] Evaluating on the test split.
I0217 07:40:44.538652 139646656866112 submission_runner.py:408] Time since start: 9599.82s, 	Step: 11007, 	{'train/ctc_loss': Array(0.43340543, dtype=float32), 'train/wer': 0.1545875990998041, 'validation/ctc_loss': Array(0.60133076, dtype=float32), 'validation/wer': 0.18090888903907237, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3724855, dtype=float32), 'test/wer': 0.1259114821359657, 'test/num_examples': 2472, 'score': 8678.961176633835, 'total_duration': 9599.821209192276, 'accumulated_submission_time': 8678.961176633835, 'accumulated_eval_time': 920.1078622341156, 'accumulated_logging_time': 0.2839655876159668}
I0217 07:40:44.580563 139535991158528 logging_writer.py:48] [11007] accumulated_eval_time=920.107862, accumulated_logging_time=0.283966, accumulated_submission_time=8678.961177, global_step=11007, preemption_count=0, score=8678.961177, test/ctc_loss=0.372485488653183, test/num_examples=2472, test/wer=0.125911, total_duration=9599.821209, train/ctc_loss=0.43340542912483215, train/wer=0.154588, validation/ctc_loss=0.6013307571411133, validation/num_examples=5348, validation/wer=0.180909
I0217 07:41:55.804646 139535982765824 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.5078652501106262, loss=1.4329774379730225
I0217 07:43:11.811627 139535991158528 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.6183547973632812, loss=1.5157427787780762
I0217 07:44:27.997328 139535982765824 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.4625067114830017, loss=1.5160578489303589
I0217 07:45:47.581562 139535991158528 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.5828083157539368, loss=1.4458038806915283
I0217 07:47:03.830720 139535982765824 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.5616337060928345, loss=1.5304478406906128
I0217 07:48:20.001272 139535991158528 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.5254890322685242, loss=1.4298022985458374
I0217 07:49:36.317638 139535982765824 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.49675047397613525, loss=1.4334590435028076
I0217 07:50:52.520091 139535991158528 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.4655342698097229, loss=1.4405995607376099
I0217 07:52:08.724531 139535982765824 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.5052406191825867, loss=1.4804750680923462
I0217 07:53:31.426125 139535991158528 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.6469964385032654, loss=1.4159482717514038
I0217 07:54:52.857242 139535982765824 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.5525739789009094, loss=1.4694405794143677
I0217 07:56:14.791338 139535991158528 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.4359297454357147, loss=1.443190097808838
I0217 07:57:37.258286 139535982765824 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.6061975955963135, loss=1.4459257125854492
I0217 07:58:59.986706 139535991158528 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.8112619519233704, loss=1.4797430038452148
I0217 08:00:16.126451 139535982765824 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.45261210203170776, loss=1.4485645294189453
I0217 08:01:32.303269 139535991158528 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.5185242295265198, loss=1.5006637573242188
I0217 08:02:48.527885 139535982765824 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.39087608456611633, loss=1.4388866424560547
I0217 08:04:04.743640 139535991158528 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.3750677704811096, loss=1.4186625480651855
I0217 08:04:44.721228 139646656866112 spec.py:321] Evaluating on the training split.
I0217 08:05:38.681144 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 08:06:29.965718 139646656866112 spec.py:349] Evaluating on the test split.
I0217 08:06:56.140945 139646656866112 submission_runner.py:408] Time since start: 11171.42s, 	Step: 12854, 	{'train/ctc_loss': Array(0.40785065, dtype=float32), 'train/wer': 0.1410327868852459, 'validation/ctc_loss': Array(0.5672202, dtype=float32), 'validation/wer': 0.17057841026482715, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35132352, dtype=float32), 'test/wer': 0.11996018930392216, 'test/num_examples': 2472, 'score': 10119.012127161026, 'total_duration': 11171.424641609192, 'accumulated_submission_time': 10119.012127161026, 'accumulated_eval_time': 1051.5222551822662, 'accumulated_logging_time': 0.34063029289245605}
I0217 08:06:56.173511 139535991158528 logging_writer.py:48] [12854] accumulated_eval_time=1051.522255, accumulated_logging_time=0.340630, accumulated_submission_time=10119.012127, global_step=12854, preemption_count=0, score=10119.012127, test/ctc_loss=0.35132351517677307, test/num_examples=2472, test/wer=0.119960, total_duration=11171.424642, train/ctc_loss=0.4078506529331207, train/wer=0.141033, validation/ctc_loss=0.5672202110290527, validation/num_examples=5348, validation/wer=0.170578
I0217 08:07:31.830218 139535982765824 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.520589292049408, loss=1.4511648416519165
I0217 08:08:47.818865 139535991158528 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.5002003908157349, loss=1.4651916027069092
I0217 08:10:03.948787 139535982765824 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.5929620265960693, loss=1.445821762084961
I0217 08:11:20.172771 139535991158528 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.6401119232177734, loss=1.4098235368728638
I0217 08:12:36.627573 139535982765824 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.5304757952690125, loss=1.4431161880493164
I0217 08:13:59.229972 139535991158528 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.5934094190597534, loss=1.4813898801803589
I0217 08:15:15.387541 139535982765824 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.5508968830108643, loss=1.3399699926376343
I0217 08:16:31.735612 139535991158528 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.528283417224884, loss=1.422975778579712
I0217 08:17:47.971106 139535982765824 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.6805934906005859, loss=1.4587806463241577
I0217 08:19:04.287836 139535991158528 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.5389116406440735, loss=1.468647837638855
I0217 08:20:20.647442 139535982765824 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.6127595901489258, loss=1.4393223524093628
I0217 08:21:39.807327 139535991158528 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.4615021347999573, loss=1.4840296506881714
I0217 08:23:02.794319 139535982765824 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.5930339097976685, loss=1.417099118232727
I0217 08:24:25.182805 139535991158528 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.6002119183540344, loss=1.4656933546066284
I0217 08:25:47.579252 139535982765824 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.6370006799697876, loss=1.458033561706543
I0217 08:27:10.164843 139535991158528 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.5016254186630249, loss=1.4224591255187988
I0217 08:28:30.931926 139535991158528 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.612903892993927, loss=1.4523491859436035
I0217 08:29:47.209767 139535982765824 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.46473801136016846, loss=1.3406494855880737
I0217 08:30:56.242425 139646656866112 spec.py:321] Evaluating on the training split.
I0217 08:31:48.658303 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 08:32:40.569586 139646656866112 spec.py:349] Evaluating on the test split.
I0217 08:33:06.888596 139646656866112 submission_runner.py:408] Time since start: 12742.17s, 	Step: 14692, 	{'train/ctc_loss': Array(0.4037296, dtype=float32), 'train/wer': 0.1419416377749473, 'validation/ctc_loss': Array(0.55786395, dtype=float32), 'validation/wer': 0.1679716539386157, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33908087, dtype=float32), 'test/wer': 0.11478073649787744, 'test/num_examples': 2472, 'score': 11558.991518497467, 'total_duration': 12742.170778512955, 'accumulated_submission_time': 11558.991518497467, 'accumulated_eval_time': 1182.161565065384, 'accumulated_logging_time': 0.389937162399292}
I0217 08:33:06.922126 139535991158528 logging_writer.py:48] [14692] accumulated_eval_time=1182.161565, accumulated_logging_time=0.389937, accumulated_submission_time=11558.991518, global_step=14692, preemption_count=0, score=11558.991518, test/ctc_loss=0.3390808701515198, test/num_examples=2472, test/wer=0.114781, total_duration=12742.170779, train/ctc_loss=0.4037295877933502, train/wer=0.141942, validation/ctc_loss=0.5578639507293701, validation/num_examples=5348, validation/wer=0.167972
I0217 08:33:13.834761 139535982765824 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.45596927404403687, loss=1.4087973833084106
I0217 08:34:29.718302 139535991158528 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.49403688311576843, loss=1.3767811059951782
I0217 08:35:45.934148 139535982765824 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.47884464263916016, loss=1.4428578615188599
I0217 08:37:02.114664 139535991158528 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.5187292695045471, loss=1.4367280006408691
I0217 08:38:18.258548 139535982765824 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.4865339398384094, loss=1.3962916135787964
I0217 08:39:35.296870 139535991158528 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.46298813819885254, loss=1.4379363059997559
I0217 08:40:57.761202 139535982765824 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.5531009435653687, loss=1.4879236221313477
I0217 08:42:20.138063 139535991158528 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.5220246911048889, loss=1.492990255355835
I0217 08:43:42.756716 139535991158528 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.4523641765117645, loss=1.3733171224594116
I0217 08:44:58.933273 139535982765824 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.4828914701938629, loss=1.4135240316390991
I0217 08:46:15.345834 139535991158528 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.540083110332489, loss=1.3960591554641724
I0217 08:47:31.480924 139535982765824 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.49243539571762085, loss=1.466705560684204
I0217 08:48:47.690302 139535991158528 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.4734610319137573, loss=1.4054603576660156
I0217 08:50:03.844405 139535982765824 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.5142978429794312, loss=1.4455476999282837
I0217 08:51:24.673865 139535991158528 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.4214228689670563, loss=1.4066632986068726
I0217 08:52:46.436557 139535982765824 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.6267056465148926, loss=1.414136290550232
I0217 08:54:10.269805 139535991158528 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.48140451312065125, loss=1.394024133682251
I0217 08:55:32.795469 139535982765824 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.5154960751533508, loss=1.3955591917037964
I0217 08:56:56.697408 139535991158528 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.49097758531570435, loss=1.4309701919555664
I0217 08:57:07.050073 139646656866112 spec.py:321] Evaluating on the training split.
I0217 08:58:00.638180 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 08:58:53.011662 139646656866112 spec.py:349] Evaluating on the test split.
I0217 08:59:19.489753 139646656866112 submission_runner.py:408] Time since start: 14314.77s, 	Step: 16515, 	{'train/ctc_loss': Array(0.399308, dtype=float32), 'train/wer': 0.13758047744419125, 'validation/ctc_loss': Array(0.535952, dtype=float32), 'validation/wer': 0.16143545381696708, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32374918, dtype=float32), 'test/wer': 0.11096215952714643, 'test/num_examples': 2472, 'score': 12999.028420209885, 'total_duration': 14314.772800445557, 'accumulated_submission_time': 12999.028420209885, 'accumulated_eval_time': 1314.5952589511871, 'accumulated_logging_time': 0.441178560256958}
I0217 08:59:19.525670 139535991158528 logging_writer.py:48] [16515] accumulated_eval_time=1314.595259, accumulated_logging_time=0.441179, accumulated_submission_time=12999.028420, global_step=16515, preemption_count=0, score=12999.028420, test/ctc_loss=0.3237491846084595, test/num_examples=2472, test/wer=0.110962, total_duration=14314.772800, train/ctc_loss=0.3993079960346222, train/wer=0.137580, validation/ctc_loss=0.5359519720077515, validation/num_examples=5348, validation/wer=0.161435
I0217 09:00:24.717145 139535982765824 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.5141962170600891, loss=1.3395438194274902
I0217 09:01:40.820976 139535991158528 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.4974871575832367, loss=1.4243831634521484
I0217 09:02:57.056840 139535982765824 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.4888937175273895, loss=1.397875189781189
I0217 09:04:13.551306 139535991158528 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.5118833184242249, loss=1.3935956954956055
I0217 09:05:29.755909 139535982765824 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.5669821500778198, loss=1.4172965288162231
I0217 09:06:45.810410 139535991158528 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.507490873336792, loss=1.3795055150985718
I0217 09:08:01.913588 139535982765824 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.5085744261741638, loss=1.4178391695022583
I0217 09:09:23.545238 139535991158528 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.5528053045272827, loss=1.4091520309448242
I0217 09:10:46.162228 139535982765824 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.47707676887512207, loss=1.391035795211792
I0217 09:12:09.518034 139535991158528 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.5136587023735046, loss=1.3801205158233643
I0217 09:13:29.546480 139535991158528 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.45491698384284973, loss=1.3757219314575195
I0217 09:14:45.640235 139535982765824 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.5614681839942932, loss=1.4584861993789673
I0217 09:16:01.729751 139535991158528 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.5431387424468994, loss=1.3367558717727661
I0217 09:17:17.968559 139535982765824 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.5186098217964172, loss=1.4485292434692383
I0217 09:18:34.207017 139535991158528 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.5503097176551819, loss=1.4033831357955933
I0217 09:19:50.788658 139535982765824 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.4552360475063324, loss=1.3325223922729492
I0217 09:21:13.266238 139535991158528 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.4985848069190979, loss=1.3934880495071411
I0217 09:22:35.789348 139535982765824 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.5109418034553528, loss=1.3324910402297974
I0217 09:23:19.734569 139646656866112 spec.py:321] Evaluating on the training split.
I0217 09:24:13.680947 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 09:25:06.091202 139646656866112 spec.py:349] Evaluating on the test split.
I0217 09:25:32.309408 139646656866112 submission_runner.py:408] Time since start: 15887.59s, 	Step: 18354, 	{'train/ctc_loss': Array(0.37713492, dtype=float32), 'train/wer': 0.13267483690018902, 'validation/ctc_loss': Array(0.5242192, dtype=float32), 'validation/wer': 0.15884800679687575, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31622794, dtype=float32), 'test/wer': 0.1068389088619422, 'test/num_examples': 2472, 'score': 14439.1499106884, 'total_duration': 15887.59229850769, 'accumulated_submission_time': 14439.1499106884, 'accumulated_eval_time': 1447.1639330387115, 'accumulated_logging_time': 0.4920070171356201}
I0217 09:25:32.342837 139535991158528 logging_writer.py:48] [18354] accumulated_eval_time=1447.163933, accumulated_logging_time=0.492007, accumulated_submission_time=14439.149911, global_step=18354, preemption_count=0, score=14439.149911, test/ctc_loss=0.3162279427051544, test/num_examples=2472, test/wer=0.106839, total_duration=15887.592299, train/ctc_loss=0.37713491916656494, train/wer=0.132675, validation/ctc_loss=0.5242192149162292, validation/num_examples=5348, validation/wer=0.158848
I0217 09:26:08.055525 139535982765824 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.5470941066741943, loss=1.392345666885376
I0217 09:27:24.308577 139535991158528 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.4884469211101532, loss=1.4054301977157593
I0217 09:28:44.012498 139535991158528 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.5077162384986877, loss=1.3576070070266724
I0217 09:30:00.158323 139535982765824 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.5709880590438843, loss=1.3641949892044067
I0217 09:31:16.352906 139535991158528 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.4686334431171417, loss=1.3569607734680176
I0217 09:32:32.569854 139535982765824 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.4989739656448364, loss=1.3113867044448853
I0217 09:33:48.795945 139535991158528 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.6019495725631714, loss=1.403855562210083
I0217 09:35:05.118730 139535982765824 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.6574062705039978, loss=1.4486455917358398
I0217 09:36:28.071077 139535991158528 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.555542528629303, loss=1.4201133251190186
I0217 09:37:51.039444 139535982765824 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.5926536321640015, loss=1.3665874004364014
I0217 09:39:13.731275 139535991158528 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.5539408922195435, loss=1.387617588043213
I0217 09:40:36.548269 139535982765824 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.46736863255500793, loss=1.3577197790145874
I0217 09:42:00.352214 139535991158528 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.5062728524208069, loss=1.3956897258758545
I0217 09:43:16.498552 139535982765824 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.5023241639137268, loss=1.368019938468933
I0217 09:44:32.638489 139535991158528 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.6635634899139404, loss=1.2956994771957397
I0217 09:45:48.783000 139535982765824 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.5759063363075256, loss=1.3974261283874512
I0217 09:47:04.972765 139535991158528 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.5429231524467468, loss=1.3235670328140259
I0217 09:48:21.081231 139535982765824 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.6263136267662048, loss=1.3666247129440308
I0217 09:49:32.853533 139646656866112 spec.py:321] Evaluating on the training split.
I0217 09:50:25.073095 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 09:51:17.537700 139646656866112 spec.py:349] Evaluating on the test split.
I0217 09:51:43.967860 139646656866112 submission_runner.py:408] Time since start: 17459.25s, 	Step: 20191, 	{'train/ctc_loss': Array(0.37280694, dtype=float32), 'train/wer': 0.13489396763806996, 'validation/ctc_loss': Array(0.50043136, dtype=float32), 'validation/wer': 0.15259179161396835, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3005866, dtype=float32), 'test/wer': 0.10297970873194809, 'test/num_examples': 2472, 'score': 15879.571325778961, 'total_duration': 17459.250715255737, 'accumulated_submission_time': 15879.571325778961, 'accumulated_eval_time': 1578.2720866203308, 'accumulated_logging_time': 0.5422773361206055}
I0217 09:51:44.003190 139535991158528 logging_writer.py:48] [20191] accumulated_eval_time=1578.272087, accumulated_logging_time=0.542277, accumulated_submission_time=15879.571326, global_step=20191, preemption_count=0, score=15879.571326, test/ctc_loss=0.30058661103248596, test/num_examples=2472, test/wer=0.102980, total_duration=17459.250715, train/ctc_loss=0.37280693650245667, train/wer=0.134894, validation/ctc_loss=0.5004313588142395, validation/num_examples=5348, validation/wer=0.152592
I0217 09:51:51.686177 139535982765824 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.4369511604309082, loss=1.3333269357681274
I0217 09:53:07.868412 139535991158528 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.5170655250549316, loss=1.3819812536239624
I0217 09:54:24.055972 139535982765824 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.6122274398803711, loss=1.4127378463745117
I0217 09:55:40.628187 139535991158528 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.4765465259552002, loss=1.354355812072754
I0217 09:57:00.173994 139535991158528 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.5132262706756592, loss=1.3269566297531128
I0217 09:58:16.311990 139535982765824 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.6560616493225098, loss=1.3079133033752441
I0217 09:59:32.391645 139535991158528 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.4905546009540558, loss=1.3527745008468628
I0217 10:00:48.549775 139535982765824 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.6409441828727722, loss=1.403265357017517
I0217 10:02:04.808819 139535991158528 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.5977622866630554, loss=1.3935660123825073
I0217 10:03:20.946715 139535982765824 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.4361473619937897, loss=1.3284811973571777
I0217 10:04:37.060254 139535991158528 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.5265272855758667, loss=1.3825225830078125
I0217 10:05:59.551822 139535982765824 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.4919973909854889, loss=1.3367425203323364
I0217 10:07:22.680669 139535991158528 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.5647937059402466, loss=1.3300989866256714
I0217 10:08:45.102658 139535982765824 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.5960510969161987, loss=1.3507040739059448
I0217 10:10:07.418754 139535991158528 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.44269806146621704, loss=1.3401923179626465
I0217 10:11:29.222469 139535991158528 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.5157749056816101, loss=1.2956573963165283
I0217 10:12:45.421481 139535982765824 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.45011818408966064, loss=1.3221362829208374
I0217 10:14:01.820903 139535991158528 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.4935878813266754, loss=1.328083872795105
I0217 10:15:18.041067 139535982765824 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.45736587047576904, loss=1.321819543838501
I0217 10:15:44.409209 139646656866112 spec.py:321] Evaluating on the training split.
I0217 10:16:37.892859 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 10:17:30.049398 139646656866112 spec.py:349] Evaluating on the test split.
I0217 10:17:56.644448 139646656866112 submission_runner.py:408] Time since start: 19031.93s, 	Step: 22036, 	{'train/ctc_loss': Array(0.31283918, dtype=float32), 'train/wer': 0.11125854066761731, 'validation/ctc_loss': Array(0.49085632, dtype=float32), 'validation/wer': 0.1479865221043282, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29445583, dtype=float32), 'test/wer': 0.10050169601689923, 'test/num_examples': 2472, 'score': 17319.888331651688, 'total_duration': 19031.927735090256, 'accumulated_submission_time': 17319.888331651688, 'accumulated_eval_time': 1710.5015604496002, 'accumulated_logging_time': 0.5935218334197998}
I0217 10:17:56.681213 139535991158528 logging_writer.py:48] [22036] accumulated_eval_time=1710.501560, accumulated_logging_time=0.593522, accumulated_submission_time=17319.888332, global_step=22036, preemption_count=0, score=17319.888332, test/ctc_loss=0.2944558262825012, test/num_examples=2472, test/wer=0.100502, total_duration=19031.927735, train/ctc_loss=0.31283918023109436, train/wer=0.111259, validation/ctc_loss=0.4908563196659088, validation/num_examples=5348, validation/wer=0.147987
I0217 10:18:46.034837 139535982765824 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.4376981854438782, loss=1.3195141553878784
I0217 10:20:02.292884 139535991158528 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.49739763140678406, loss=1.3366895914077759
I0217 10:21:18.575680 139535982765824 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.4839642345905304, loss=1.3178730010986328
I0217 10:22:34.920602 139535991158528 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.5964747071266174, loss=1.3603888750076294
I0217 10:23:51.865387 139535982765824 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.5167077779769897, loss=1.3976556062698364
I0217 10:25:14.385122 139535991158528 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.41800031065940857, loss=1.3360735177993774
I0217 10:26:37.008147 139535991158528 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.6918073296546936, loss=1.3457667827606201
I0217 10:27:53.050955 139535982765824 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.6198089122772217, loss=1.3106077909469604
I0217 10:29:09.464492 139535991158528 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.5398294925689697, loss=1.3597627878189087
I0217 10:30:25.623228 139535982765824 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.46060872077941895, loss=1.3271710872650146
I0217 10:31:41.848100 139535991158528 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.4981127977371216, loss=1.3190497159957886
I0217 10:32:58.096963 139535982765824 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.5494124889373779, loss=1.3813859224319458
I0217 10:34:19.432377 139535991158528 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.5974459648132324, loss=1.281304955482483
I0217 10:35:44.054121 139535982765824 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.4854156970977783, loss=1.298973798751831
I0217 10:37:07.178822 139535991158528 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.47787952423095703, loss=1.331233263015747
I0217 10:38:30.483385 139535982765824 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.5444864630699158, loss=1.3819998502731323
I0217 10:39:55.484361 139535991158528 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.5375461578369141, loss=1.3239502906799316
I0217 10:41:11.599580 139535982765824 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.4486549198627472, loss=1.2887117862701416
I0217 10:41:56.864205 139646656866112 spec.py:321] Evaluating on the training split.
I0217 10:42:49.895378 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 10:43:41.823251 139646656866112 spec.py:349] Evaluating on the test split.
I0217 10:44:08.446474 139646656866112 submission_runner.py:408] Time since start: 20603.73s, 	Step: 23861, 	{'train/ctc_loss': Array(0.32723373, dtype=float32), 'train/wer': 0.11497332248146139, 'validation/ctc_loss': Array(0.48551804, dtype=float32), 'validation/wer': 0.14419224345173157, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28720966, dtype=float32), 'test/wer': 0.09556598216643308, 'test/num_examples': 2472, 'score': 18759.982160806656, 'total_duration': 20603.729377031326, 'accumulated_submission_time': 18759.982160806656, 'accumulated_eval_time': 1842.077701330185, 'accumulated_logging_time': 0.6459090709686279}
I0217 10:44:08.480565 139535991158528 logging_writer.py:48] [23861] accumulated_eval_time=1842.077701, accumulated_logging_time=0.645909, accumulated_submission_time=18759.982161, global_step=23861, preemption_count=0, score=18759.982161, test/ctc_loss=0.2872096598148346, test/num_examples=2472, test/wer=0.095566, total_duration=20603.729377, train/ctc_loss=0.3272337317466736, train/wer=0.114973, validation/ctc_loss=0.48551803827285767, validation/num_examples=5348, validation/wer=0.144192
I0217 10:44:38.873958 139535982765824 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.5693399906158447, loss=1.2951550483703613
I0217 10:45:55.103569 139535991158528 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.5963728427886963, loss=1.3898015022277832
I0217 10:47:11.542893 139535982765824 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.5718746781349182, loss=1.2946685552597046
I0217 10:48:27.802845 139535991158528 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.599026083946228, loss=1.3048609495162964
I0217 10:49:44.104006 139535982765824 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.49704429507255554, loss=1.2969380617141724
I0217 10:51:00.376094 139535991158528 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.5786736607551575, loss=1.3138201236724854
I0217 10:52:21.664063 139535982765824 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.5818180441856384, loss=1.2999927997589111
I0217 10:53:44.592399 139535991158528 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.44142845273017883, loss=1.3256703615188599
I0217 10:55:07.090618 139535982765824 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.48408618569374084, loss=1.331206202507019
I0217 10:56:27.796949 139535991158528 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.4984864592552185, loss=1.338263988494873
I0217 10:57:44.059471 139535982765824 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.7310280799865723, loss=1.3258916139602661
I0217 10:59:00.268394 139535991158528 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.5589818358421326, loss=1.3238953351974487
I0217 11:00:16.530624 139535982765824 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.4861885905265808, loss=1.349463939666748
I0217 11:01:32.758310 139535991158528 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.6641186475753784, loss=1.264872431755066
I0217 11:02:49.358258 139535982765824 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.4573151469230652, loss=1.2965784072875977
I0217 11:04:10.599995 139535991158528 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.548067033290863, loss=1.3240249156951904
I0217 11:05:32.319354 139535982765824 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.49935439229011536, loss=1.3433473110198975
I0217 11:06:55.255622 139535991158528 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.436801940202713, loss=1.2946478128433228
I0217 11:08:08.615639 139646656866112 spec.py:321] Evaluating on the training split.
I0217 11:09:01.817733 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 11:09:53.856476 139646656866112 spec.py:349] Evaluating on the test split.
I0217 11:10:20.072603 139646656866112 submission_runner.py:408] Time since start: 22175.36s, 	Step: 25690, 	{'train/ctc_loss': Array(0.31858635, dtype=float32), 'train/wer': 0.11207038176328298, 'validation/ctc_loss': Array(0.47096306, dtype=float32), 'validation/wer': 0.14148894059491973, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27616546, dtype=float32), 'test/wer': 0.09513943899417057, 'test/num_examples': 2472, 'score': 20200.02801823616, 'total_duration': 22175.355345249176, 'accumulated_submission_time': 20200.02801823616, 'accumulated_eval_time': 1973.528375864029, 'accumulated_logging_time': 0.6977200508117676}
I0217 11:10:20.108808 139535991158528 logging_writer.py:48] [25690] accumulated_eval_time=1973.528376, accumulated_logging_time=0.697720, accumulated_submission_time=20200.028018, global_step=25690, preemption_count=0, score=20200.028018, test/ctc_loss=0.2761654555797577, test/num_examples=2472, test/wer=0.095139, total_duration=22175.355345, train/ctc_loss=0.3185863494873047, train/wer=0.112070, validation/ctc_loss=0.4709630608558655, validation/num_examples=5348, validation/wer=0.141489
I0217 11:10:28.559249 139535982765824 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.5346118211746216, loss=1.2679048776626587
I0217 11:11:48.245750 139535991158528 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.5538668036460876, loss=1.2787989377975464
I0217 11:13:04.368703 139535982765824 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.4741349220275879, loss=1.275292992591858
I0217 11:14:20.550217 139535991158528 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.5040650963783264, loss=1.2779356241226196
I0217 11:15:36.644287 139535982765824 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.5266498923301697, loss=1.2957932949066162
I0217 11:16:52.828311 139535991158528 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.5705999135971069, loss=1.3091610670089722
I0217 11:18:10.826994 139535982765824 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.6831458806991577, loss=1.3394056558609009
I0217 11:19:34.425744 139535991158528 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.5681620240211487, loss=1.2966245412826538
I0217 11:20:57.683405 139535982765824 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.48651620745658875, loss=1.337371826171875
I0217 11:22:21.088549 139535991158528 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.6475935578346252, loss=1.3023236989974976
I0217 11:23:43.229883 139535982765824 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.7633718252182007, loss=1.2886501550674438
I0217 11:25:07.470758 139535991158528 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.5576254725456238, loss=1.2167688608169556
I0217 11:26:23.711738 139535982765824 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.488980233669281, loss=1.2951750755310059
I0217 11:27:39.937408 139535991158528 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.5763582587242126, loss=1.2870007753372192
I0217 11:28:56.150866 139535982765824 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.6094978451728821, loss=1.2623357772827148
I0217 11:30:12.372553 139535991158528 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.5935603976249695, loss=1.3029265403747559
I0217 11:31:28.698094 139535982765824 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.5311097502708435, loss=1.295129418373108
I0217 11:32:46.204250 139535991158528 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.5203367471694946, loss=1.2477794885635376
I0217 11:34:09.327965 139535982765824 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.5210853815078735, loss=1.3329793214797974
I0217 11:34:20.577242 139646656866112 spec.py:321] Evaluating on the training split.
I0217 11:35:14.685850 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 11:36:06.746424 139646656866112 spec.py:349] Evaluating on the test split.
I0217 11:36:32.926498 139646656866112 submission_runner.py:408] Time since start: 23748.21s, 	Step: 27515, 	{'train/ctc_loss': Array(0.31003976, dtype=float32), 'train/wer': 0.10981249382119358, 'validation/ctc_loss': Array(0.46048242, dtype=float32), 'validation/wer': 0.13728916651380133, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27005804, dtype=float32), 'test/wer': 0.09160522413828123, 'test/num_examples': 2472, 'score': 21640.408204317093, 'total_duration': 23748.209998607635, 'accumulated_submission_time': 21640.408204317093, 'accumulated_eval_time': 2105.8720936775208, 'accumulated_logging_time': 0.7506282329559326}
I0217 11:36:32.965646 139535991158528 logging_writer.py:48] [27515] accumulated_eval_time=2105.872094, accumulated_logging_time=0.750628, accumulated_submission_time=21640.408204, global_step=27515, preemption_count=0, score=21640.408204, test/ctc_loss=0.2700580358505249, test/num_examples=2472, test/wer=0.091605, total_duration=23748.209999, train/ctc_loss=0.310039758682251, train/wer=0.109812, validation/ctc_loss=0.4604824185371399, validation/num_examples=5348, validation/wer=0.137289
I0217 11:37:38.216970 139535982765824 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.5572831034660339, loss=1.265230417251587
I0217 11:38:54.624616 139535991158528 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.5508173108100891, loss=1.3290270566940308
I0217 11:40:10.765047 139535982765824 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.5209931135177612, loss=1.314131259918213
I0217 11:41:30.310831 139535991158528 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.6256837844848633, loss=1.2246371507644653
I0217 11:42:46.457409 139535982765824 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.5726394653320312, loss=1.2308872938156128
I0217 11:44:02.532871 139535991158528 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.47205105423927307, loss=1.2994465827941895
I0217 11:45:18.773339 139535982765824 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.5999875068664551, loss=1.323481798171997
I0217 11:46:34.841433 139535991158528 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.5216020941734314, loss=1.2588706016540527
I0217 11:47:51.399975 139535982765824 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.5479086637496948, loss=1.2815582752227783
I0217 11:49:13.899419 139535991158528 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.5974798202514648, loss=1.318226933479309
I0217 11:50:36.640701 139535982765824 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.5836769938468933, loss=1.2883516550064087
I0217 11:51:59.310549 139535991158528 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.5893778204917908, loss=1.2574166059494019
I0217 11:53:21.751578 139535982765824 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.520538866519928, loss=1.273019790649414
I0217 11:54:43.497405 139535991158528 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.521294355392456, loss=1.2930288314819336
I0217 11:55:59.760027 139535982765824 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.4830082058906555, loss=1.2499016523361206
I0217 11:57:16.104489 139535991158528 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.5231854319572449, loss=1.2787126302719116
I0217 11:58:32.428508 139535982765824 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.5265552401542664, loss=1.2926522493362427
I0217 11:59:48.815460 139535991158528 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.5586227178573608, loss=1.3121970891952515
I0217 12:00:33.441414 139646656866112 spec.py:321] Evaluating on the training split.
I0217 12:01:26.642645 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 12:02:18.526395 139646656866112 spec.py:349] Evaluating on the test split.
I0217 12:02:44.843276 139646656866112 submission_runner.py:408] Time since start: 25320.13s, 	Step: 29360, 	{'train/ctc_loss': Array(0.27465776, dtype=float32), 'train/wer': 0.09899000135116875, 'validation/ctc_loss': Array(0.45722303, dtype=float32), 'validation/wer': 0.1353389265956728, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26472273, dtype=float32), 'test/wer': 0.08870066825096988, 'test/num_examples': 2472, 'score': 23080.796627759933, 'total_duration': 25320.125111341476, 'accumulated_submission_time': 23080.796627759933, 'accumulated_eval_time': 2237.2667417526245, 'accumulated_logging_time': 0.8045666217803955}
I0217 12:02:44.883051 139535991158528 logging_writer.py:48] [29360] accumulated_eval_time=2237.266742, accumulated_logging_time=0.804567, accumulated_submission_time=23080.796628, global_step=29360, preemption_count=0, score=23080.796628, test/ctc_loss=0.2647227346897125, test/num_examples=2472, test/wer=0.088701, total_duration=25320.125111, train/ctc_loss=0.2746577560901642, train/wer=0.098990, validation/ctc_loss=0.4572230279445648, validation/num_examples=5348, validation/wer=0.135339
I0217 12:03:16.095426 139535982765824 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.5228416323661804, loss=1.2546297311782837
I0217 12:04:32.184801 139535991158528 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.5865266919136047, loss=1.2961722612380981
I0217 12:05:48.484801 139535982765824 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.6161513328552246, loss=1.2242826223373413
I0217 12:07:04.788000 139535991158528 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.5395969152450562, loss=1.2778135538101196
I0217 12:08:21.079133 139535982765824 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.4775785803794861, loss=1.2707676887512207
I0217 12:09:44.741490 139535991158528 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.5904195308685303, loss=1.313805103302002
I0217 12:11:00.882680 139535982765824 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.5901737213134766, loss=1.277804970741272
I0217 12:12:17.350012 139535991158528 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.7576494216918945, loss=1.3077418804168701
I0217 12:13:33.637118 139535982765824 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.574831485748291, loss=1.2925077676773071
I0217 12:14:49.840563 139535991158528 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.538941502571106, loss=1.2708797454833984
I0217 12:16:06.086239 139535982765824 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.5028747320175171, loss=1.2216107845306396
I0217 12:17:25.133880 139535991158528 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.5121304392814636, loss=1.2985305786132812
I0217 12:18:47.639890 139535982765824 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.6835634708404541, loss=1.2695338726043701
I0217 12:20:10.296235 139535991158528 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.6482434272766113, loss=1.2622933387756348
I0217 12:21:34.913117 139535982765824 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.5562434792518616, loss=1.2261873483657837
I0217 12:23:01.340860 139535991158528 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.5849002599716187, loss=1.2744266986846924
I0217 12:24:17.585079 139535982765824 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.593274712562561, loss=1.2579827308654785
I0217 12:25:33.714430 139535991158528 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.5239964127540588, loss=1.310866117477417
I0217 12:26:44.925949 139646656866112 spec.py:321] Evaluating on the training split.
I0217 12:27:36.919183 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 12:28:28.919819 139646656866112 spec.py:349] Evaluating on the test split.
I0217 12:28:55.164236 139646656866112 submission_runner.py:408] Time since start: 26890.45s, 	Step: 31195, 	{'train/ctc_loss': Array(0.24588323, dtype=float32), 'train/wer': 0.08950538002682878, 'validation/ctc_loss': Array(0.4484083, dtype=float32), 'validation/wer': 0.13362039835098527, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25443456, dtype=float32), 'test/wer': 0.08573517762476388, 'test/num_examples': 2472, 'score': 24520.750933647156, 'total_duration': 26890.447347164154, 'accumulated_submission_time': 24520.750933647156, 'accumulated_eval_time': 2367.49915266037, 'accumulated_logging_time': 0.8600101470947266}
I0217 12:28:55.202151 139535991158528 logging_writer.py:48] [31195] accumulated_eval_time=2367.499153, accumulated_logging_time=0.860010, accumulated_submission_time=24520.750934, global_step=31195, preemption_count=0, score=24520.750934, test/ctc_loss=0.2544345557689667, test/num_examples=2472, test/wer=0.085735, total_duration=26890.447347, train/ctc_loss=0.24588322639465332, train/wer=0.089505, validation/ctc_loss=0.448408305644989, validation/num_examples=5348, validation/wer=0.133620
I0217 12:28:59.877936 139535982765824 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.4659603536128998, loss=1.2270746231079102
I0217 12:30:16.043096 139535991158528 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.5263718366622925, loss=1.2029285430908203
I0217 12:31:32.171639 139535982765824 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.5418323874473572, loss=1.2527116537094116
I0217 12:32:48.348147 139535991158528 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.7175146341323853, loss=1.3102247714996338
I0217 12:34:04.450668 139535982765824 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.5148313045501709, loss=1.2626538276672363
I0217 12:35:22.343634 139535991158528 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.5878864526748657, loss=1.2454191446304321
I0217 12:36:44.742841 139535982765824 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.5269298553466797, loss=1.2299952507019043
I0217 12:38:07.150168 139535991158528 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.4502434730529785, loss=1.260501503944397
I0217 12:39:28.171672 139535991158528 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.6790444850921631, loss=1.2608681917190552
I0217 12:40:44.316415 139535982765824 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.6619627475738525, loss=1.2459133863449097
I0217 12:42:00.611332 139535991158528 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.5473859310150146, loss=1.2809034585952759
I0217 12:43:16.951951 139535982765824 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.5884903073310852, loss=1.2316950559616089
I0217 12:44:33.227701 139535991158528 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.5091103315353394, loss=1.2605851888656616
I0217 12:45:49.449852 139535982765824 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.5369850397109985, loss=1.2398028373718262
I0217 12:47:11.325679 139535991158528 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.6624472737312317, loss=1.2787561416625977
I0217 12:48:33.887701 139535982765824 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.47982603311538696, loss=1.2210017442703247
I0217 12:49:56.881856 139535991158528 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.4865865707397461, loss=1.2318105697631836
I0217 12:51:19.357538 139535982765824 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.6367524862289429, loss=1.2416187524795532
I0217 12:52:43.108698 139535991158528 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.5250923037528992, loss=1.2406162023544312
I0217 12:52:55.749718 139646656866112 spec.py:321] Evaluating on the training split.
I0217 12:53:48.142689 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 12:54:39.800020 139646656866112 spec.py:349] Evaluating on the test split.
I0217 12:55:06.140377 139646656866112 submission_runner.py:408] Time since start: 28461.42s, 	Step: 33018, 	{'train/ctc_loss': Array(0.27327314, dtype=float32), 'train/wer': 0.09891458662096506, 'validation/ctc_loss': Array(0.44101134, dtype=float32), 'validation/wer': 0.13046332680035144, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24990562, dtype=float32), 'test/wer': 0.08211971645034834, 'test/num_examples': 2472, 'score': 25961.20705795288, 'total_duration': 28461.4237473011, 'accumulated_submission_time': 25961.20705795288, 'accumulated_eval_time': 2497.8841466903687, 'accumulated_logging_time': 0.9160919189453125}
I0217 12:55:06.177869 139535991158528 logging_writer.py:48] [33018] accumulated_eval_time=2497.884147, accumulated_logging_time=0.916092, accumulated_submission_time=25961.207058, global_step=33018, preemption_count=0, score=25961.207058, test/ctc_loss=0.24990561604499817, test/num_examples=2472, test/wer=0.082120, total_duration=28461.423747, train/ctc_loss=0.27327314019203186, train/wer=0.098915, validation/ctc_loss=0.44101133942604065, validation/num_examples=5348, validation/wer=0.130463
I0217 12:56:09.204673 139535982765824 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.5327646732330322, loss=1.2159663438796997
I0217 12:57:25.378359 139535991158528 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.48673397302627563, loss=1.2036867141723633
I0217 12:58:41.681782 139535982765824 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.638894259929657, loss=1.2814019918441772
I0217 12:59:57.918316 139535991158528 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.6629967093467712, loss=1.2497092485427856
I0217 13:01:14.025671 139535982765824 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.6024158000946045, loss=1.2457793951034546
I0217 13:02:30.255923 139535991158528 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.5958952903747559, loss=1.2435898780822754
I0217 13:03:46.529875 139535982765824 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.6180344820022583, loss=1.2767926454544067
I0217 13:05:07.826293 139535991158528 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.5553262233734131, loss=1.2718145847320557
I0217 13:06:31.579274 139535982765824 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.5371142625808716, loss=1.2514636516571045
I0217 13:07:57.470405 139535991158528 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.5222271084785461, loss=1.203235387802124
I0217 13:09:13.513106 139535982765824 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.5593178868293762, loss=1.2393466234207153
I0217 13:10:29.600194 139535991158528 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.6180486679077148, loss=1.2449934482574463
I0217 13:11:45.801960 139535982765824 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.47412824630737305, loss=1.2517218589782715
I0217 13:13:01.982513 139535991158528 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.4892843961715698, loss=1.2463107109069824
I0217 13:14:18.292273 139535982765824 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.6780932545661926, loss=1.195365309715271
I0217 13:15:36.085139 139535991158528 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.6452925801277161, loss=1.2523797750473022
I0217 13:16:58.804958 139535982765824 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.6877931356430054, loss=1.2534781694412231
I0217 13:18:22.304537 139535991158528 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.5523593425750732, loss=1.2419780492782593
I0217 13:19:06.489248 139646656866112 spec.py:321] Evaluating on the training split.
I0217 13:20:00.320474 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 13:20:52.302363 139646656866112 spec.py:349] Evaluating on the test split.
I0217 13:21:18.923252 139646656866112 submission_runner.py:408] Time since start: 30034.21s, 	Step: 34855, 	{'train/ctc_loss': Array(0.24580301, dtype=float32), 'train/wer': 0.0894540231484832, 'validation/ctc_loss': Array(0.42582294, dtype=float32), 'validation/wer': 0.12698765169873621, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24126007, dtype=float32), 'test/wer': 0.07996668900940426, 'test/num_examples': 2472, 'score': 27401.429488182068, 'total_duration': 30034.206517457962, 'accumulated_submission_time': 27401.429488182068, 'accumulated_eval_time': 2630.312391757965, 'accumulated_logging_time': 0.9704453945159912}
I0217 13:21:18.962304 139535991158528 logging_writer.py:48] [34855] accumulated_eval_time=2630.312392, accumulated_logging_time=0.970445, accumulated_submission_time=27401.429488, global_step=34855, preemption_count=0, score=27401.429488, test/ctc_loss=0.24126006662845612, test/num_examples=2472, test/wer=0.079967, total_duration=30034.206517, train/ctc_loss=0.24580301344394684, train/wer=0.089454, validation/ctc_loss=0.4258229434490204, validation/num_examples=5348, validation/wer=0.126988
I0217 13:21:53.905182 139535982765824 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.6286071538925171, loss=1.211156964302063
I0217 13:23:10.454312 139535991158528 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.5295340418815613, loss=1.2436703443527222
I0217 13:24:30.141346 139535991158528 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.5255296230316162, loss=1.227329969406128
I0217 13:25:46.319478 139535982765824 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.5261380672454834, loss=1.2294528484344482
I0217 13:27:02.582140 139535991158528 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.6128573417663574, loss=1.286833643913269
I0217 13:28:18.782444 139535982765824 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.5248562097549438, loss=1.2665761709213257
I0217 13:29:34.951421 139535991158528 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.5497029423713684, loss=1.2106720209121704
I0217 13:30:51.837100 139535982765824 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.5132848620414734, loss=1.2459875345230103
I0217 13:32:15.559208 139535991158528 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.5824278593063354, loss=1.1841058731079102
I0217 13:33:38.540194 139535982765824 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.5551074743270874, loss=1.1928852796554565
I0217 13:35:01.448716 139535991158528 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.6395889520645142, loss=1.1905736923217773
I0217 13:36:23.456372 139535982765824 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.5638377070426941, loss=1.2190871238708496
I0217 13:37:46.031857 139535991158528 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.5048701763153076, loss=1.2041277885437012
I0217 13:39:02.512457 139535982765824 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.6884997487068176, loss=1.2177155017852783
I0217 13:40:18.724034 139535991158528 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.6120229959487915, loss=1.2258566617965698
I0217 13:41:34.948613 139535982765824 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.5142492651939392, loss=1.210403323173523
I0217 13:42:51.260038 139535991158528 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.49630093574523926, loss=1.1549817323684692
I0217 13:44:07.542248 139535982765824 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.5620274543762207, loss=1.2206147909164429
I0217 13:45:19.544158 139646656866112 spec.py:321] Evaluating on the training split.
I0217 13:46:13.234194 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 13:47:04.872433 139646656866112 spec.py:349] Evaluating on the test split.
I0217 13:47:30.864425 139646656866112 submission_runner.py:408] Time since start: 31606.15s, 	Step: 36692, 	{'train/ctc_loss': Array(0.26628497, dtype=float32), 'train/wer': 0.09255809009069496, 'validation/ctc_loss': Array(0.42301342, dtype=float32), 'validation/wer': 0.1271903994129971, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24345164, dtype=float32), 'test/wer': 0.08142912274287571, 'test/num_examples': 2472, 'score': 28841.924087047577, 'total_duration': 31606.14753127098, 'accumulated_submission_time': 28841.924087047577, 'accumulated_eval_time': 2761.6267426013947, 'accumulated_logging_time': 1.024658441543579}
I0217 13:47:30.909492 139535991158528 logging_writer.py:48] [36692] accumulated_eval_time=2761.626743, accumulated_logging_time=1.024658, accumulated_submission_time=28841.924087, global_step=36692, preemption_count=0, score=28841.924087, test/ctc_loss=0.24345164000988007, test/num_examples=2472, test/wer=0.081429, total_duration=31606.147531, train/ctc_loss=0.2662849724292755, train/wer=0.092558, validation/ctc_loss=0.4230134189128876, validation/num_examples=5348, validation/wer=0.127190
I0217 13:47:37.828633 139535982765824 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.5813151001930237, loss=1.1864334344863892
I0217 13:48:54.037513 139535991158528 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.6063907146453857, loss=1.179918646812439
I0217 13:50:10.249086 139535982765824 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.6360657215118408, loss=1.1980408430099487
I0217 13:51:26.526803 139535991158528 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.5450829267501831, loss=1.2009479999542236
I0217 13:52:46.375617 139535991158528 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.5262162089347839, loss=1.141761064529419
I0217 13:54:02.590788 139535982765824 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.5110776424407959, loss=1.1811573505401611
I0217 13:55:18.868869 139535991158528 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.6124897003173828, loss=1.2376360893249512
I0217 13:56:35.447270 139535982765824 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.5354439616203308, loss=1.2047278881072998
I0217 13:57:51.649641 139535991158528 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.6126241087913513, loss=1.202652931213379
I0217 13:59:07.910315 139535982765824 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.5355609059333801, loss=1.1061010360717773
I0217 14:00:26.308877 139535991158528 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.5656407475471497, loss=1.2295993566513062
I0217 14:01:49.156414 139535982765824 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.5467492341995239, loss=1.243120789527893
I0217 14:03:12.429266 139535991158528 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.783972978591919, loss=1.189751148223877
I0217 14:04:35.025974 139535982765824 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.5244495272636414, loss=1.1731903553009033
I0217 14:05:57.724669 139535991158528 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.5395721793174744, loss=1.2359217405319214
I0217 14:07:17.751335 139535991158528 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.512377142906189, loss=1.1673930883407593
I0217 14:08:33.895121 139535982765824 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.6462599039077759, loss=1.2114523649215698
I0217 14:09:50.028091 139535991158528 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.6055871844291687, loss=1.1298469305038452
I0217 14:11:06.252379 139535982765824 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.5472921133041382, loss=1.1688920259475708
I0217 14:11:31.306873 139646656866112 spec.py:321] Evaluating on the training split.
I0217 14:12:24.869037 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 14:13:16.694741 139646656866112 spec.py:349] Evaluating on the test split.
I0217 14:13:42.745332 139646656866112 submission_runner.py:408] Time since start: 33178.03s, 	Step: 38534, 	{'train/ctc_loss': Array(0.2095532, dtype=float32), 'train/wer': 0.07635212007176301, 'validation/ctc_loss': Array(0.40643167, dtype=float32), 'validation/wer': 0.12010388406692606, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2337808, dtype=float32), 'test/wer': 0.07736680681656613, 'test/num_examples': 2472, 'score': 30282.230488061905, 'total_duration': 33178.02899599075, 'accumulated_submission_time': 30282.230488061905, 'accumulated_eval_time': 2893.059848546982, 'accumulated_logging_time': 1.0862317085266113}
I0217 14:13:42.788604 139535991158528 logging_writer.py:48] [38534] accumulated_eval_time=2893.059849, accumulated_logging_time=1.086232, accumulated_submission_time=30282.230488, global_step=38534, preemption_count=0, score=30282.230488, test/ctc_loss=0.23378080129623413, test/num_examples=2472, test/wer=0.077367, total_duration=33178.028996, train/ctc_loss=0.20955319702625275, train/wer=0.076352, validation/ctc_loss=0.4064316749572754, validation/num_examples=5348, validation/wer=0.120104
I0217 14:14:33.623032 139535982765824 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.5505613684654236, loss=1.1256513595581055
I0217 14:15:49.769228 139535991158528 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.6545293927192688, loss=1.1635907888412476
I0217 14:17:05.764233 139535982765824 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.6342906951904297, loss=1.220903992652893
I0217 14:18:21.847259 139535991158528 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.5829245448112488, loss=1.1665681600570679
I0217 14:19:38.581870 139535982765824 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.562249481678009, loss=1.1990926265716553
I0217 14:21:01.051437 139535991158528 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.5610411763191223, loss=1.1932508945465088
I0217 14:22:23.015862 139535991158528 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.5081326365470886, loss=1.1754589080810547
I0217 14:23:39.339814 139535982765824 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.6335940957069397, loss=1.215210199356079
I0217 14:24:55.741504 139535991158528 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.4882957339286804, loss=1.1592531204223633
I0217 14:26:12.045734 139535982765824 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.52260822057724, loss=1.1158419847488403
I0217 14:27:28.340641 139535991158528 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.5006088614463806, loss=1.1736983060836792
I0217 14:28:44.613333 139535982765824 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.5485665798187256, loss=1.1339466571807861
I0217 14:30:06.222292 139535991158528 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.6138592958450317, loss=1.1951080560684204
I0217 14:31:29.681596 139535982765824 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.6236417293548584, loss=1.143042802810669
I0217 14:32:52.148071 139535991158528 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.5783530473709106, loss=1.192053198814392
I0217 14:34:14.705213 139535982765824 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.49548596143722534, loss=1.1892470121383667
I0217 14:35:38.724460 139535991158528 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.5970433950424194, loss=1.1648814678192139
I0217 14:36:54.853313 139535982765824 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.7214042544364929, loss=1.1713603734970093
I0217 14:37:43.386288 139646656866112 spec.py:321] Evaluating on the training split.
I0217 14:38:36.232444 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 14:39:28.178850 139646656866112 spec.py:349] Evaluating on the test split.
I0217 14:39:54.402473 139646656866112 submission_runner.py:408] Time since start: 34749.69s, 	Step: 40365, 	{'train/ctc_loss': Array(0.19563828, dtype=float32), 'train/wer': 0.07309344373286569, 'validation/ctc_loss': Array(0.40345013, dtype=float32), 'validation/wer': 0.11903221757726136, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22827744, dtype=float32), 'test/wer': 0.07596530782198932, 'test/num_examples': 2472, 'score': 31722.741436958313, 'total_duration': 34749.68506407738, 'accumulated_submission_time': 31722.741436958313, 'accumulated_eval_time': 3024.0696020126343, 'accumulated_logging_time': 1.1442553997039795}
I0217 14:39:54.439632 139535991158528 logging_writer.py:48] [40365] accumulated_eval_time=3024.069602, accumulated_logging_time=1.144255, accumulated_submission_time=31722.741437, global_step=40365, preemption_count=0, score=31722.741437, test/ctc_loss=0.22827744483947754, test/num_examples=2472, test/wer=0.075965, total_duration=34749.685064, train/ctc_loss=0.1956382840871811, train/wer=0.073093, validation/ctc_loss=0.4034501314163208, validation/num_examples=5348, validation/wer=0.119032
I0217 14:40:21.797647 139535982765824 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.5153281092643738, loss=1.1318449974060059
I0217 14:41:37.796865 139535991158528 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.6660396456718445, loss=1.1216222047805786
I0217 14:42:53.904480 139535982765824 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.5364762544631958, loss=1.139164686203003
I0217 14:44:10.119422 139535991158528 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.5560407638549805, loss=1.1311112642288208
I0217 14:45:26.270154 139535982765824 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.5273990631103516, loss=1.1722441911697388
I0217 14:46:42.378802 139535991158528 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.7565968036651611, loss=1.1760905981063843
I0217 14:48:03.177506 139535982765824 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.4876103699207306, loss=1.1621003150939941
I0217 14:49:26.128846 139535991158528 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.5322868227958679, loss=1.14030921459198
I0217 14:50:52.572980 139535991158528 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.533576488494873, loss=1.2128733396530151
I0217 14:52:08.734800 139535982765824 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.7063630819320679, loss=1.1284209489822388
I0217 14:53:25.065190 139535991158528 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.5558522343635559, loss=1.125634789466858
I0217 14:54:41.420462 139535982765824 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.5574158430099487, loss=1.143688678741455
I0217 14:55:57.733024 139535991158528 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.5488333702087402, loss=1.2277107238769531
I0217 14:57:14.135651 139535982765824 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.6184176206588745, loss=1.1797116994857788
I0217 14:58:30.394584 139535991158528 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.4715912342071533, loss=1.1398428678512573
I0217 14:59:50.175737 139535982765824 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.6389608383178711, loss=1.1213606595993042
I0217 15:01:12.893112 139535991158528 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.5357954502105713, loss=1.1322342157363892
I0217 15:02:35.323189 139535982765824 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.6328409314155579, loss=1.1499747037887573
I0217 15:03:55.072645 139646656866112 spec.py:321] Evaluating on the training split.
I0217 15:04:47.096009 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 15:05:39.156167 139646656866112 spec.py:349] Evaluating on the test split.
I0217 15:06:05.549449 139646656866112 submission_runner.py:408] Time since start: 36320.83s, 	Step: 42198, 	{'train/ctc_loss': Array(0.21608451, dtype=float32), 'train/wer': 0.07941671660531348, 'validation/ctc_loss': Array(0.39799383, dtype=float32), 'validation/wer': 0.11731368933257383, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21821883, dtype=float32), 'test/wer': 0.0725732740235208, 'test/num_examples': 2472, 'score': 33163.28744673729, 'total_duration': 36320.83214735985, 'accumulated_submission_time': 33163.28744673729, 'accumulated_eval_time': 3154.5400528907776, 'accumulated_logging_time': 1.1971325874328613}
I0217 15:06:05.590597 139535991158528 logging_writer.py:48] [42198] accumulated_eval_time=3154.540053, accumulated_logging_time=1.197133, accumulated_submission_time=33163.287447, global_step=42198, preemption_count=0, score=33163.287447, test/ctc_loss=0.2182188332080841, test/num_examples=2472, test/wer=0.072573, total_duration=36320.832147, train/ctc_loss=0.21608451008796692, train/wer=0.079417, validation/ctc_loss=0.3979938328266144, validation/num_examples=5348, validation/wer=0.117314
I0217 15:06:07.972212 139535982765824 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.6267700791358948, loss=1.143683910369873
I0217 15:07:27.344187 139535991158528 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.6300361156463623, loss=1.1631190776824951
I0217 15:08:43.476744 139535982765824 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.536777675151825, loss=1.1538567543029785
I0217 15:09:59.706298 139535991158528 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.5908184051513672, loss=1.138087272644043
I0217 15:11:15.810166 139535982765824 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.5959622859954834, loss=1.1103090047836304
I0217 15:12:31.932855 139535991158528 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.5906137228012085, loss=1.1511653661727905
I0217 15:13:48.103651 139535982765824 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.5636152029037476, loss=1.1796679496765137
I0217 15:15:04.851969 139535991158528 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.7262730598449707, loss=1.1370387077331543
I0217 15:16:26.851757 139535982765824 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.667889416217804, loss=1.1357555389404297
I0217 15:17:49.799566 139535991158528 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.5493718981742859, loss=1.0879497528076172
I0217 15:19:12.275534 139535982765824 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.6304275393486023, loss=1.1439573764801025
I0217 15:20:35.196916 139535991158528 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.4969707131385803, loss=1.1249696016311646
I0217 15:21:51.622739 139535982765824 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.5672910213470459, loss=1.1403919458389282
I0217 15:23:07.709890 139535991158528 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.5467420816421509, loss=1.1213209629058838
I0217 15:24:23.945323 139535982765824 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.6998415589332581, loss=1.167834758758545
I0217 15:25:40.162685 139535991158528 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.6459317803382874, loss=1.114737629890442
I0217 15:26:56.388764 139535982765824 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.616000235080719, loss=1.1274000406265259
I0217 15:28:12.646531 139535991158528 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.4871288239955902, loss=1.1001800298690796
I0217 15:29:31.208627 139535982765824 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.9753747582435608, loss=1.1742738485336304
I0217 15:30:05.773168 139646656866112 spec.py:321] Evaluating on the training split.
I0217 15:31:09.339669 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 15:32:01.348951 139646656866112 spec.py:349] Evaluating on the test split.
I0217 15:32:28.141555 139646656866112 submission_runner.py:408] Time since start: 37903.42s, 	Step: 44043, 	{'train/ctc_loss': Array(0.14231463, dtype=float32), 'train/wer': 0.05326419344658345, 'validation/ctc_loss': Array(0.38264957, dtype=float32), 'validation/wer': 0.11266014655763346, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21474697, dtype=float32), 'test/wer': 0.07060305079926066, 'test/num_examples': 2472, 'score': 34603.37959957123, 'total_duration': 37903.424902677536, 'accumulated_submission_time': 34603.37959957123, 'accumulated_eval_time': 3296.9027593135834, 'accumulated_logging_time': 1.2538504600524902}
I0217 15:32:28.184250 139535991158528 logging_writer.py:48] [44043] accumulated_eval_time=3296.902759, accumulated_logging_time=1.253850, accumulated_submission_time=34603.379600, global_step=44043, preemption_count=0, score=34603.379600, test/ctc_loss=0.21474696695804596, test/num_examples=2472, test/wer=0.070603, total_duration=37903.424903, train/ctc_loss=0.1423146277666092, train/wer=0.053264, validation/ctc_loss=0.38264957070350647, validation/num_examples=5348, validation/wer=0.112660
I0217 15:33:12.188444 139535982765824 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.5925071239471436, loss=1.1089032888412476
I0217 15:34:28.324626 139535991158528 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.5367551445960999, loss=1.1470637321472168
I0217 15:35:48.149847 139535991158528 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.6378375887870789, loss=1.1335159540176392
I0217 15:37:04.253285 139535982765824 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.5506119132041931, loss=1.1593481302261353
I0217 15:38:20.415441 139535991158528 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.6361830830574036, loss=1.086768388748169
I0217 15:39:36.797989 139535982765824 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.5758165717124939, loss=1.1399189233779907
I0217 15:40:52.992801 139535991158528 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.5359216928482056, loss=1.1164004802703857
I0217 15:42:09.188548 139535982765824 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.5302180647850037, loss=1.1125643253326416
I0217 15:43:25.484493 139535991158528 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.5294907093048096, loss=1.1384025812149048
I0217 15:44:46.151174 139535982765824 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.5237835645675659, loss=1.102508544921875
I0217 15:46:08.352814 139535991158528 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.6067495346069336, loss=1.1768567562103271
I0217 15:47:30.672981 139535982765824 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.5877531170845032, loss=1.1615891456604004
I0217 15:48:53.097670 139535991158528 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.5332528948783875, loss=1.1134437322616577
I0217 15:50:13.694104 139535991158528 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.5361114144325256, loss=1.0880857706069946
I0217 15:51:29.894909 139535982765824 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.5608950257301331, loss=1.106367826461792
I0217 15:52:46.062398 139535991158528 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.7352896332740784, loss=1.2084847688674927
I0217 15:54:02.188625 139535982765824 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.5129863619804382, loss=1.0950571298599243
I0217 15:55:18.664786 139535991158528 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.6361665725708008, loss=1.1194757223129272
I0217 15:56:28.149197 139646656866112 spec.py:321] Evaluating on the training split.
I0217 15:57:22.597042 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 15:58:15.347480 139646656866112 spec.py:349] Evaluating on the test split.
I0217 15:58:41.275341 139646656866112 submission_runner.py:408] Time since start: 39476.56s, 	Step: 45893, 	{'train/ctc_loss': Array(0.12276159, dtype=float32), 'train/wer': 0.04592168596331469, 'validation/ctc_loss': Array(0.3812691, dtype=float32), 'validation/wer': 0.11052646823136411, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21339078, dtype=float32), 'test/wer': 0.06989214551215649, 'test/num_examples': 2472, 'score': 36043.25439476967, 'total_duration': 39476.55716729164, 'accumulated_submission_time': 36043.25439476967, 'accumulated_eval_time': 3430.021687746048, 'accumulated_logging_time': 1.3127069473266602}
I0217 15:58:41.322659 139535991158528 logging_writer.py:48] [45893] accumulated_eval_time=3430.021688, accumulated_logging_time=1.312707, accumulated_submission_time=36043.254395, global_step=45893, preemption_count=0, score=36043.254395, test/ctc_loss=0.2133907824754715, test/num_examples=2472, test/wer=0.069892, total_duration=39476.557167, train/ctc_loss=0.12276159226894379, train/wer=0.045922, validation/ctc_loss=0.38126909732818604, validation/num_examples=5348, validation/wer=0.110526
I0217 15:58:47.512607 139535982765824 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.6074008941650391, loss=1.1257977485656738
I0217 16:00:03.492019 139535991158528 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.6099343299865723, loss=1.1291613578796387
I0217 16:01:19.466730 139535982765824 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.6694968342781067, loss=1.121101975440979
I0217 16:02:35.637480 139535991158528 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.6505043506622314, loss=1.0958521366119385
I0217 16:03:51.735607 139535982765824 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.5873795747756958, loss=1.1469041109085083
I0217 16:05:11.702254 139535991158528 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.5858592987060547, loss=1.1178675889968872
I0217 16:06:27.918516 139535982765824 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.5965639352798462, loss=1.0841882228851318
I0217 16:07:44.129675 139535991158528 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.5509933233261108, loss=1.1039971113204956
I0217 16:09:00.276487 139535982765824 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.5736273527145386, loss=1.0827990770339966
I0217 16:10:16.493883 139535991158528 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.56981360912323, loss=1.0994927883148193
I0217 16:11:32.792760 139535982765824 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.705752968788147, loss=1.0944666862487793
I0217 16:12:55.001669 139535991158528 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.5584715008735657, loss=1.1285604238510132
I0217 16:14:18.124208 139535982765824 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.487034410238266, loss=1.1933691501617432
I0217 16:15:40.129843 139535991158528 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.5250772833824158, loss=1.0718474388122559
I0217 16:17:02.820105 139535982765824 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.5456566214561462, loss=1.1334733963012695
I0217 16:18:27.625808 139535991158528 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.7096623182296753, loss=1.1375802755355835
I0217 16:19:43.793528 139535982765824 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.49042466282844543, loss=1.0999411344528198
I0217 16:20:59.950121 139535991158528 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.6101661324501038, loss=1.0835429430007935
I0217 16:22:16.260151 139535982765824 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.6598334908485413, loss=1.11299467086792
I0217 16:22:41.846458 139646656866112 spec.py:321] Evaluating on the training split.
I0217 16:23:37.229717 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 16:24:29.666887 139646656866112 spec.py:349] Evaluating on the test split.
I0217 16:24:56.205558 139646656866112 submission_runner.py:408] Time since start: 41051.49s, 	Step: 47735, 	{'train/ctc_loss': Array(0.12371109, dtype=float32), 'train/wer': 0.047442438683276786, 'validation/ctc_loss': Array(0.36838114, dtype=float32), 'validation/wer': 0.1077845467623121, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2057504, dtype=float32), 'test/wer': 0.06780005281010704, 'test/num_examples': 2472, 'score': 37483.68469452858, 'total_duration': 41051.488491773605, 'accumulated_submission_time': 37483.68469452858, 'accumulated_eval_time': 3564.3747503757477, 'accumulated_logging_time': 1.379469633102417}
I0217 16:24:56.250984 139535991158528 logging_writer.py:48] [47735] accumulated_eval_time=3564.374750, accumulated_logging_time=1.379470, accumulated_submission_time=37483.684695, global_step=47735, preemption_count=0, score=37483.684695, test/ctc_loss=0.20575040578842163, test/num_examples=2472, test/wer=0.067800, total_duration=41051.488492, train/ctc_loss=0.12371108680963516, train/wer=0.047442, validation/ctc_loss=0.368381142616272, validation/num_examples=5348, validation/wer=0.107785
I0217 16:25:46.340906 139535982765824 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.5356411933898926, loss=1.1335018873214722
I0217 16:27:02.444586 139535991158528 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.5663411617279053, loss=1.144676685333252
I0217 16:28:18.692697 139535982765824 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.5655997395515442, loss=1.1174834966659546
I0217 16:29:34.901663 139535991158528 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.5646383166313171, loss=1.0898361206054688
I0217 16:30:51.455010 139535982765824 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.6069762706756592, loss=1.0648077726364136
I0217 16:32:11.168605 139535991158528 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.6715441346168518, loss=1.12044095993042
I0217 16:33:33.265179 139535982765824 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.5341808199882507, loss=1.0432037115097046
I0217 16:34:52.939506 139535991158528 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.5278670191764832, loss=1.1222984790802002
I0217 16:36:08.936502 139535982765824 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.5896041989326477, loss=1.114442229270935
I0217 16:37:25.076588 139535991158528 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.556926429271698, loss=1.053570032119751
I0217 16:38:41.234506 139535982765824 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.7082180976867676, loss=1.1388558149337769
I0217 16:39:57.524349 139535991158528 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.5547786355018616, loss=1.103896975517273
I0217 16:41:13.808128 139535982765824 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.5361840724945068, loss=1.093248724937439
I0217 16:42:34.572402 139535991158528 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.5497907996177673, loss=1.0618746280670166
I0217 16:43:56.322914 139535982765824 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.5856465101242065, loss=1.093375563621521
I0217 16:45:19.664134 139535991158528 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.5237269997596741, loss=1.0741045475006104
I0217 16:46:42.370446 139535982765824 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.5550244450569153, loss=1.074878454208374
I0217 16:48:03.902115 139535991158528 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.7539822459220886, loss=1.0979630947113037
I0217 16:48:56.863143 139646656866112 spec.py:321] Evaluating on the training split.
I0217 16:49:51.960388 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 16:50:43.922812 139646656866112 spec.py:349] Evaluating on the test split.
I0217 16:51:10.449708 139646656866112 submission_runner.py:408] Time since start: 42625.73s, 	Step: 49571, 	{'train/ctc_loss': Array(0.12066799, dtype=float32), 'train/wer': 0.04577250529184458, 'validation/ctc_loss': Array(0.36527857, dtype=float32), 'validation/wer': 0.1063460034563658, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20391922, dtype=float32), 'test/wer': 0.06710945910263441, 'test/num_examples': 2472, 'score': 38924.20606184006, 'total_duration': 42625.732850551605, 'accumulated_submission_time': 38924.20606184006, 'accumulated_eval_time': 3697.955447912216, 'accumulated_logging_time': 1.4431352615356445}
I0217 16:51:10.487815 139535991158528 logging_writer.py:48] [49571] accumulated_eval_time=3697.955448, accumulated_logging_time=1.443135, accumulated_submission_time=38924.206062, global_step=49571, preemption_count=0, score=38924.206062, test/ctc_loss=0.2039192169904709, test/num_examples=2472, test/wer=0.067109, total_duration=42625.732851, train/ctc_loss=0.12066798657178879, train/wer=0.045773, validation/ctc_loss=0.36527857184410095, validation/num_examples=5348, validation/wer=0.106346
I0217 16:51:33.329367 139535982765824 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.5757777690887451, loss=1.0693957805633545
I0217 16:52:49.565578 139535991158528 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.5870235562324524, loss=1.0277212858200073
I0217 16:54:05.871932 139535982765824 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.5401615500450134, loss=1.0820518732070923
I0217 16:55:22.201642 139535991158528 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.6091563105583191, loss=1.1225600242614746
I0217 16:56:38.501464 139535982765824 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.5525600910186768, loss=1.0853002071380615
I0217 16:57:54.895087 139535991158528 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.6430691480636597, loss=1.0406413078308105
I0217 16:59:11.168418 139535982765824 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.6727120280265808, loss=1.1297111511230469
I0217 17:00:33.884167 139535991158528 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.603823184967041, loss=1.0717209577560425
I0217 17:01:57.712283 139535982765824 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.5925501585006714, loss=1.1365453004837036
I0217 17:03:21.535387 139535991158528 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.5714537501335144, loss=1.0761611461639404
I0217 17:04:37.769227 139535982765824 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.5881162881851196, loss=1.0736500024795532
I0217 17:05:53.901935 139535991158528 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.5728353261947632, loss=1.0517234802246094
I0217 17:07:10.102835 139535982765824 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.6407558917999268, loss=1.0937254428863525
I0217 17:08:26.355990 139535991158528 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.592029869556427, loss=1.0040521621704102
I0217 17:09:42.538206 139535982765824 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.5637820363044739, loss=1.0742491483688354
I0217 17:11:00.307678 139535991158528 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.6483139991760254, loss=1.1113144159317017
I0217 17:12:23.399650 139535982765824 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.6823480129241943, loss=1.0765972137451172
I0217 17:13:46.913199 139535991158528 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.5796312093734741, loss=1.0861611366271973
I0217 17:15:09.326014 139535982765824 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.5497828722000122, loss=1.0952204465866089
I0217 17:15:10.570585 139646656866112 spec.py:321] Evaluating on the training split.
I0217 17:16:04.735434 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 17:16:56.167666 139646656866112 spec.py:349] Evaluating on the test split.
I0217 17:17:22.596579 139646656866112 submission_runner.py:408] Time since start: 44197.88s, 	Step: 51403, 	{'train/ctc_loss': Array(0.10996965, dtype=float32), 'train/wer': 0.042697279673189405, 'validation/ctc_loss': Array(0.3555751, dtype=float32), 'validation/wer': 0.1044054181912973, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19339007, dtype=float32), 'test/wer': 0.06550484431174212, 'test/num_examples': 2472, 'score': 40364.20062971115, 'total_duration': 44197.879747867584, 'accumulated_submission_time': 40364.20062971115, 'accumulated_eval_time': 3829.9755721092224, 'accumulated_logging_time': 1.4969408512115479}
I0217 17:17:22.634496 139535991158528 logging_writer.py:48] [51403] accumulated_eval_time=3829.975572, accumulated_logging_time=1.496941, accumulated_submission_time=40364.200630, global_step=51403, preemption_count=0, score=40364.200630, test/ctc_loss=0.19339007139205933, test/num_examples=2472, test/wer=0.065505, total_duration=44197.879748, train/ctc_loss=0.10996965318918228, train/wer=0.042697, validation/ctc_loss=0.3555751144886017, validation/num_examples=5348, validation/wer=0.104405
I0217 17:18:40.823386 139535991158528 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.5993868112564087, loss=1.0734044313430786
I0217 17:19:56.762945 139535982765824 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.5720319747924805, loss=1.0812476873397827
I0217 17:21:12.937493 139535991158528 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.4958464503288269, loss=1.0698084831237793
I0217 17:22:29.355937 139535982765824 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.526282012462616, loss=1.0737861394882202
I0217 17:23:45.548178 139535991158528 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.702569305896759, loss=1.0641652345657349
I0217 17:25:01.733350 139535982765824 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.689907968044281, loss=1.1047965288162231
I0217 17:26:19.715802 139535991158528 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.5794247984886169, loss=1.0788965225219727
I0217 17:27:42.279743 139535982765824 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.6326944231987, loss=1.0521184206008911
I0217 17:29:04.207417 139535991158528 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.5467519760131836, loss=1.0289760828018188
I0217 17:30:27.913635 139535982765824 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.5146632790565491, loss=1.0342048406600952
I0217 17:31:50.957692 139535991158528 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.6665291786193848, loss=1.0927075147628784
I0217 17:33:12.285007 139535991158528 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.6611464619636536, loss=0.988851010799408
I0217 17:34:28.592551 139535982765824 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.52447509765625, loss=1.0400928258895874
I0217 17:35:44.934557 139535991158528 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.6265783309936523, loss=0.9991751909255981
I0217 17:37:01.301341 139535982765824 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.7388991117477417, loss=1.0685031414031982
I0217 17:38:17.803411 139535991158528 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.6155366897583008, loss=1.0207393169403076
I0217 17:39:34.109335 139535982765824 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.7424439787864685, loss=1.0615267753601074
I0217 17:40:52.589769 139535991158528 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.6323820948600769, loss=1.064041018486023
I0217 17:41:22.908888 139646656866112 spec.py:321] Evaluating on the training split.
I0217 17:42:19.222707 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 17:43:11.583592 139646656866112 spec.py:349] Evaluating on the test split.
I0217 17:43:37.717164 139646656866112 submission_runner.py:408] Time since start: 45773.00s, 	Step: 53238, 	{'train/ctc_loss': Array(0.10054823, dtype=float32), 'train/wer': 0.039402895769823955, 'validation/ctc_loss': Array(0.35113725, dtype=float32), 'validation/wer': 0.10176004325284571, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18965232, dtype=float32), 'test/wer': 0.06260028842443077, 'test/num_examples': 2472, 'score': 41804.38695335388, 'total_duration': 45773.00025463104, 'accumulated_submission_time': 41804.38695335388, 'accumulated_eval_time': 3964.7778856754303, 'accumulated_logging_time': 1.5501763820648193}
I0217 17:43:37.757185 139535991158528 logging_writer.py:48] [53238] accumulated_eval_time=3964.777886, accumulated_logging_time=1.550176, accumulated_submission_time=41804.386953, global_step=53238, preemption_count=0, score=41804.386953, test/ctc_loss=0.18965232372283936, test/num_examples=2472, test/wer=0.062600, total_duration=45773.000255, train/ctc_loss=0.10054823011159897, train/wer=0.039403, validation/ctc_loss=0.35113725066185, validation/num_examples=5348, validation/wer=0.101760
I0217 17:44:25.613482 139535982765824 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.7137004137039185, loss=1.054540753364563
I0217 17:45:41.875375 139535991158528 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.5536286234855652, loss=1.0759350061416626
I0217 17:46:58.292244 139535982765824 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.6345923542976379, loss=1.0503228902816772
I0217 17:48:18.008923 139535991158528 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.6014695763587952, loss=1.0670533180236816
I0217 17:49:34.139270 139535982765824 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.7141221761703491, loss=1.0208775997161865
I0217 17:50:50.337359 139535991158528 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.6525428891181946, loss=1.0122867822647095
I0217 17:52:06.535948 139535982765824 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.5663944482803345, loss=1.0012052059173584
I0217 17:53:22.695305 139535991158528 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.5738021731376648, loss=1.0317963361740112
I0217 17:54:38.851303 139535982765824 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.6267462968826294, loss=1.0174890756607056
I0217 17:55:56.390722 139535991158528 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.5568228960037231, loss=1.0835269689559937
I0217 17:57:18.656860 139535982765824 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.5686618089675903, loss=1.03773033618927
I0217 17:58:40.810129 139535991158528 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.6938647031784058, loss=1.0112372636795044
I0217 18:00:02.420400 139535982765824 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.5492225885391235, loss=1.0712345838546753
I0217 18:01:27.926128 139535991158528 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.5767531991004944, loss=1.0299376249313354
I0217 18:02:44.099119 139535982765824 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.8569828271865845, loss=1.0647807121276855
I0217 18:04:00.260439 139535991158528 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.6346699595451355, loss=0.9939123392105103
I0217 18:05:16.486776 139535982765824 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.6312409043312073, loss=1.0635080337524414
I0217 18:06:32.700983 139535991158528 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.6383469700813293, loss=1.0687488317489624
I0217 18:07:37.827713 139646656866112 spec.py:321] Evaluating on the training split.
I0217 18:08:31.623786 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 18:09:22.943393 139646656866112 spec.py:349] Evaluating on the test split.
I0217 18:09:48.732076 139646656866112 submission_runner.py:408] Time since start: 47344.01s, 	Step: 55087, 	{'train/ctc_loss': Array(0.11465675, dtype=float32), 'train/wer': 0.04309293974236484, 'validation/ctc_loss': Array(0.34877175, dtype=float32), 'validation/wer': 0.09976153006941696, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18984833, dtype=float32), 'test/wer': 0.0627018463225885, 'test/num_examples': 2472, 'score': 43244.36760210991, 'total_duration': 47344.014533519745, 'accumulated_submission_time': 43244.36760210991, 'accumulated_eval_time': 4095.6756834983826, 'accumulated_logging_time': 1.6069636344909668}
I0217 18:09:48.773261 139535991158528 logging_writer.py:48] [55087] accumulated_eval_time=4095.675683, accumulated_logging_time=1.606964, accumulated_submission_time=43244.367602, global_step=55087, preemption_count=0, score=43244.367602, test/ctc_loss=0.18984833359718323, test/num_examples=2472, test/wer=0.062702, total_duration=47344.014534, train/ctc_loss=0.11465675383806229, train/wer=0.043093, validation/ctc_loss=0.34877175092697144, validation/num_examples=5348, validation/wer=0.099762
I0217 18:09:59.490530 139535982765824 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.6113394498825073, loss=0.9967848658561707
I0217 18:11:15.404950 139535991158528 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.5768078565597534, loss=1.016776442527771
I0217 18:12:31.541316 139535982765824 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.6610809564590454, loss=1.0393093824386597
I0217 18:13:47.906946 139535991158528 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.736348569393158, loss=1.0666944980621338
I0217 18:15:04.194123 139535982765824 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.6648989915847778, loss=1.0135457515716553
I0217 18:16:21.464593 139535991158528 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.6776126623153687, loss=1.048978328704834
I0217 18:17:41.886057 139535991158528 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.7756691575050354, loss=1.036684513092041
I0217 18:18:58.135715 139535982765824 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.579693078994751, loss=1.0539950132369995
I0217 18:20:14.376451 139535991158528 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.6060799360275269, loss=1.0036848783493042
I0217 18:21:30.625731 139535982765824 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.5255392789840698, loss=0.9821308255195618
I0217 18:22:46.983832 139535991158528 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.583565354347229, loss=1.0068562030792236
I0217 18:24:03.941996 139535982765824 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.5527827143669128, loss=0.9943800568580627
I0217 18:25:26.408167 139535991158528 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.6608352661132812, loss=1.0677515268325806
I0217 18:26:49.380317 139535982765824 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.6213452219963074, loss=1.0121780633926392
I0217 18:28:12.727931 139535991158528 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.5633267760276794, loss=1.0027661323547363
I0217 18:29:35.294310 139535982765824 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.5973848104476929, loss=1.0142500400543213
I0217 18:30:57.525231 139535991158528 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.7235301733016968, loss=1.014776587486267
I0217 18:32:13.583007 139535982765824 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.6617839336395264, loss=1.000920057296753
I0217 18:33:29.745432 139535991158528 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.6251652240753174, loss=0.978909432888031
I0217 18:33:49.217564 139646656866112 spec.py:321] Evaluating on the training split.
I0217 18:34:43.489016 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 18:35:34.884479 139646656866112 spec.py:349] Evaluating on the test split.
I0217 18:36:01.083446 139646656866112 submission_runner.py:408] Time since start: 48916.37s, 	Step: 56927, 	{'train/ctc_loss': Array(0.09401891, dtype=float32), 'train/wer': 0.035591196575403175, 'validation/ctc_loss': Array(0.33511576, dtype=float32), 'validation/wer': 0.09663342247796325, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18326183, dtype=float32), 'test/wer': 0.06002071781122418, 'test/num_examples': 2472, 'score': 44684.721542835236, 'total_duration': 48916.36607122421, 'accumulated_submission_time': 44684.721542835236, 'accumulated_eval_time': 4227.535162687302, 'accumulated_logging_time': 1.6658551692962646}
I0217 18:36:01.128102 139535991158528 logging_writer.py:48] [56927] accumulated_eval_time=4227.535163, accumulated_logging_time=1.665855, accumulated_submission_time=44684.721543, global_step=56927, preemption_count=0, score=44684.721543, test/ctc_loss=0.18326182663440704, test/num_examples=2472, test/wer=0.060021, total_duration=48916.366071, train/ctc_loss=0.09401891380548477, train/wer=0.035591, validation/ctc_loss=0.3351157605648041, validation/num_examples=5348, validation/wer=0.096633
I0217 18:36:57.278131 139535982765824 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.7540664076805115, loss=1.0036334991455078
I0217 18:38:13.388730 139535991158528 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.5922784805297852, loss=0.9835477471351624
I0217 18:39:29.505587 139535982765824 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.609040379524231, loss=1.0048953294754028
I0217 18:40:45.572200 139535991158528 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.59702467918396, loss=0.99603271484375
I0217 18:42:01.817766 139535982765824 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.6587327122688293, loss=0.9988902807235718
I0217 18:43:23.301143 139535991158528 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.5576745271682739, loss=0.9887859225273132
I0217 18:44:45.579940 139535982765824 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.0301686525344849, loss=1.0020010471343994
I0217 18:46:09.849855 139535991158528 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.6547654271125793, loss=1.024530053138733
I0217 18:47:26.287638 139535982765824 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.7075050473213196, loss=0.9714840650558472
I0217 18:48:42.497534 139535991158528 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.6761913895606995, loss=0.9752554297447205
I0217 18:49:58.709529 139535982765824 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.5328376293182373, loss=0.9667233228683472
I0217 18:51:15.122345 139535991158528 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.6660219430923462, loss=1.0038384199142456
I0217 18:52:31.376475 139535982765824 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.7702935934066772, loss=1.0004855394363403
I0217 18:53:50.952831 139535991158528 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.8254814147949219, loss=1.0177075862884521
I0217 18:55:13.618171 139535982765824 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.6503439545631409, loss=0.9946539998054504
I0217 18:56:37.440312 139535991158528 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.5562850832939148, loss=0.9908089637756348
I0217 18:57:59.811869 139535982765824 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.5624949336051941, loss=0.9663000702857971
I0217 18:59:22.614982 139535991158528 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.5918940901756287, loss=0.987578272819519
I0217 19:00:01.446106 139646656866112 spec.py:321] Evaluating on the training split.
I0217 19:00:56.255408 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 19:01:48.281829 139646656866112 spec.py:349] Evaluating on the test split.
I0217 19:02:14.418132 139646656866112 submission_runner.py:408] Time since start: 50489.70s, 	Step: 58747, 	{'train/ctc_loss': Array(0.09163833, dtype=float32), 'train/wer': 0.035968463669294695, 'validation/ctc_loss': Array(0.33312958, dtype=float32), 'validation/wer': 0.0941328673354123, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18139437, dtype=float32), 'test/wer': 0.057969248268437835, 'test/num_examples': 2472, 'score': 46124.952325344086, 'total_duration': 50489.70118045807, 'accumulated_submission_time': 46124.952325344086, 'accumulated_eval_time': 4360.501399755478, 'accumulated_logging_time': 1.725421667098999}
I0217 19:02:14.458987 139535991158528 logging_writer.py:48] [58747] accumulated_eval_time=4360.501400, accumulated_logging_time=1.725422, accumulated_submission_time=46124.952325, global_step=58747, preemption_count=0, score=46124.952325, test/ctc_loss=0.18139436841011047, test/num_examples=2472, test/wer=0.057969, total_duration=50489.701180, train/ctc_loss=0.09163832664489746, train/wer=0.035968, validation/ctc_loss=0.3331295847892761, validation/num_examples=5348, validation/wer=0.094133
I0217 19:02:55.444334 139535982765824 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.7520566582679749, loss=1.0050814151763916
I0217 19:04:11.483469 139535991158528 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.5183839797973633, loss=1.0083794593811035
I0217 19:05:27.964340 139535982765824 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.6675203442573547, loss=0.9588138461112976
I0217 19:06:44.133442 139535991158528 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.6202177405357361, loss=1.0154755115509033
I0217 19:08:00.387653 139535982765824 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.6082473993301392, loss=0.9962067008018494
I0217 19:09:16.627954 139535991158528 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.6637966632843018, loss=0.9737995266914368
I0217 19:10:32.899681 139535982765824 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.6872649788856506, loss=1.002906084060669
I0217 19:11:51.023529 139535991158528 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.6674052476882935, loss=0.9516844153404236
I0217 19:13:14.532979 139535982765824 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.6654520034790039, loss=1.0218552350997925
I0217 19:14:37.583364 139535991158528 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.8084716796875, loss=0.992699384689331
I0217 19:16:00.038069 139535991158528 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.5821781754493713, loss=0.956581175327301
I0217 19:17:16.249016 139535982765824 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.6076451539993286, loss=1.0045381784439087
I0217 19:18:32.475797 139535991158528 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.8119187951087952, loss=0.9645271897315979
I0217 19:19:48.618960 139535982765824 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.7653127908706665, loss=0.9910250902175903
I0217 19:21:05.132266 139535991158528 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.7713161706924438, loss=0.935546875
I0217 19:22:21.345591 139535982765824 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.5490177869796753, loss=0.9406951665878296
I0217 19:23:41.578368 139535991158528 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.6455273628234863, loss=1.018810749053955
I0217 19:25:05.339993 139535982765824 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.5755822658538818, loss=0.9914706945419312
I0217 19:26:14.655780 139646656866112 spec.py:321] Evaluating on the training split.
I0217 19:27:09.182333 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 19:28:00.723775 139646656866112 spec.py:349] Evaluating on the test split.
I0217 19:28:27.224389 139646656866112 submission_runner.py:408] Time since start: 52062.51s, 	Step: 60586, 	{'train/ctc_loss': Array(0.07575043, dtype=float32), 'train/wer': 0.030003598949953424, 'validation/ctc_loss': Array(0.32939577, dtype=float32), 'validation/wer': 0.09374668121301061, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17375073, dtype=float32), 'test/wer': 0.05784737879064855, 'test/num_examples': 2472, 'score': 47565.06146478653, 'total_duration': 52062.507381916046, 'accumulated_submission_time': 47565.06146478653, 'accumulated_eval_time': 4493.0639543533325, 'accumulated_logging_time': 1.7812588214874268}
I0217 19:28:27.268022 139535991158528 logging_writer.py:48] [60586] accumulated_eval_time=4493.063954, accumulated_logging_time=1.781259, accumulated_submission_time=47565.061465, global_step=60586, preemption_count=0, score=47565.061465, test/ctc_loss=0.17375072836875916, test/num_examples=2472, test/wer=0.057847, total_duration=52062.507382, train/ctc_loss=0.0757504254579544, train/wer=0.030004, validation/ctc_loss=0.32939577102661133, validation/num_examples=5348, validation/wer=0.093747
I0217 19:28:38.719683 139535982765824 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.6057668924331665, loss=0.9834392070770264
I0217 19:29:54.797903 139535991158528 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.6955155730247498, loss=0.9744658470153809
I0217 19:31:14.511577 139535991158528 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.5829778909683228, loss=0.9346555471420288
I0217 19:32:30.479093 139535982765824 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.7763420939445496, loss=0.9870502948760986
I0217 19:33:46.546879 139535991158528 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.707359254360199, loss=1.0045125484466553
I0217 19:35:02.762597 139535982765824 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.6047267317771912, loss=0.9666986465454102
I0217 19:36:18.908773 139535991158528 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.7838618755340576, loss=0.971779465675354
I0217 19:37:35.113948 139535982765824 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.6564443707466125, loss=0.924018144607544
I0217 19:38:55.282078 139535991158528 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.7687286138534546, loss=0.9653300642967224
I0217 19:40:17.761375 139535982765824 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.676378071308136, loss=0.9540282487869263
I0217 19:41:40.627107 139535991158528 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.7267661690711975, loss=0.9606220126152039
I0217 19:43:02.798166 139535982765824 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.7791313529014587, loss=0.9720820784568787
I0217 19:44:30.245889 139535991158528 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.6103460788726807, loss=0.9866961240768433
I0217 19:45:46.450703 139535982765824 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.8399325609207153, loss=0.9655745625495911
I0217 19:47:02.607596 139535991158528 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.6269415020942688, loss=0.9477476477622986
I0217 19:48:18.806982 139535982765824 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.6565825343132019, loss=0.9541568756103516
I0217 19:49:35.063370 139535991158528 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.611553966999054, loss=0.9184080958366394
I0217 19:50:51.220712 139535982765824 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.6214893460273743, loss=0.9289077520370483
I0217 19:52:09.429970 139535991158528 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.7264333367347717, loss=0.9512569904327393
I0217 19:52:27.755088 139646656866112 spec.py:321] Evaluating on the training split.
I0217 19:53:22.424485 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 19:54:14.454246 139646656866112 spec.py:349] Evaluating on the test split.
I0217 19:54:40.904635 139646656866112 submission_runner.py:408] Time since start: 53636.19s, 	Step: 62424, 	{'train/ctc_loss': Array(0.07041831, dtype=float32), 'train/wer': 0.028381517879350764, 'validation/ctc_loss': Array(0.3231744, dtype=float32), 'validation/wer': 0.09186402386630237, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17216443, dtype=float32), 'test/wer': 0.05646619137570329, 'test/num_examples': 2472, 'score': 49005.45624756813, 'total_duration': 53636.187504053116, 'accumulated_submission_time': 49005.45624756813, 'accumulated_eval_time': 4626.207343816757, 'accumulated_logging_time': 1.8439528942108154}
I0217 19:54:40.949825 139535991158528 logging_writer.py:48] [62424] accumulated_eval_time=4626.207344, accumulated_logging_time=1.843953, accumulated_submission_time=49005.456248, global_step=62424, preemption_count=0, score=49005.456248, test/ctc_loss=0.1721644252538681, test/num_examples=2472, test/wer=0.056466, total_duration=53636.187504, train/ctc_loss=0.07041830569505692, train/wer=0.028382, validation/ctc_loss=0.323174387216568, validation/num_examples=5348, validation/wer=0.091864
I0217 19:55:39.552720 139535982765824 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.050403118133545, loss=0.9558206796646118
I0217 19:56:56.128711 139535991158528 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.8023315072059631, loss=0.9469195604324341
I0217 19:58:12.258239 139535982765824 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.6020609140396118, loss=0.975044310092926
I0217 19:59:28.596459 139535991158528 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.5669863820075989, loss=0.9714900255203247
I0217 20:00:48.171470 139535991158528 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.7009345293045044, loss=0.9267346262931824
I0217 20:02:04.259650 139535982765824 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.7100157141685486, loss=0.9731208682060242
I0217 20:03:20.524943 139535991158528 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.6164388656616211, loss=0.9062258005142212
I0217 20:04:36.643038 139535982765824 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.6256793737411499, loss=0.9658766388893127
I0217 20:05:52.792670 139535991158528 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.6072985529899597, loss=0.9801959991455078
I0217 20:07:09.803827 139535982765824 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.6845012903213501, loss=0.938143253326416
I0217 20:08:32.397750 139535991158528 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.5825791358947754, loss=0.9555195569992065
I0217 20:09:54.275076 139535982765824 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.8241350054740906, loss=0.9689466953277588
I0217 20:11:17.120417 139535991158528 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.7355585098266602, loss=0.9524043798446655
I0217 20:12:40.883777 139535982765824 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.7201597094535828, loss=0.9564574360847473
I0217 20:14:03.961350 139535991158528 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.7353422045707703, loss=0.9349689483642578
I0217 20:15:20.017959 139535982765824 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.5670999884605408, loss=0.9517723917961121
I0217 20:16:36.205938 139535991158528 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.5984089374542236, loss=0.9333771467208862
I0217 20:17:52.329589 139535982765824 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.634766161441803, loss=0.9922751188278198
I0217 20:18:41.525255 139646656866112 spec.py:321] Evaluating on the training split.
I0217 20:19:35.114501 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 20:20:26.701963 139646656866112 spec.py:349] Evaluating on the test split.
I0217 20:20:52.939951 139646656866112 submission_runner.py:408] Time since start: 55208.22s, 	Step: 64266, 	{'train/ctc_loss': Array(0.07730179, dtype=float32), 'train/wer': 0.030322357468704732, 'validation/ctc_loss': Array(0.31241328, dtype=float32), 'validation/wer': 0.08855247786670786, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1681303, dtype=float32), 'test/wer': 0.05378506286433896, 'test/num_examples': 2472, 'score': 50445.885021448135, 'total_duration': 55208.22303843498, 'accumulated_submission_time': 50445.885021448135, 'accumulated_eval_time': 4757.616107225418, 'accumulated_logging_time': 1.9643621444702148}
I0217 20:20:52.989484 139535991158528 logging_writer.py:48] [64266] accumulated_eval_time=4757.616107, accumulated_logging_time=1.964362, accumulated_submission_time=50445.885021, global_step=64266, preemption_count=0, score=50445.885021, test/ctc_loss=0.1681302934885025, test/num_examples=2472, test/wer=0.053785, total_duration=55208.223038, train/ctc_loss=0.07730179280042648, train/wer=0.030322, validation/ctc_loss=0.3124132752418518, validation/num_examples=5348, validation/wer=0.088552
I0217 20:21:19.625100 139535982765824 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.7350194454193115, loss=0.9326766133308411
I0217 20:22:35.697514 139535991158528 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.6254742741584778, loss=0.9543161392211914
I0217 20:23:51.904251 139535982765824 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.659910261631012, loss=0.9291458129882812
I0217 20:25:08.093704 139535991158528 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.6069748401641846, loss=0.9790194630622864
I0217 20:26:24.281629 139535982765824 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.5810748934745789, loss=0.9028822183609009
I0217 20:27:44.383481 139535991158528 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.6562352180480957, loss=0.8901461362838745
I0217 20:29:09.406765 139535991158528 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.6355792880058289, loss=0.9637481570243835
I0217 20:30:25.790782 139535982765824 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.6637367010116577, loss=0.9597429633140564
I0217 20:31:41.795998 139535991158528 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.8259792327880859, loss=0.9353100657463074
I0217 20:32:57.875851 139535982765824 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.0326969623565674, loss=0.9050317406654358
I0217 20:34:14.033622 139535991158528 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.7349926829338074, loss=0.9687862992286682
I0217 20:35:30.275032 139535982765824 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.6471676826477051, loss=0.9217468500137329
I0217 20:36:47.991214 139535991158528 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.7047415971755981, loss=0.9809646010398865
I0217 20:38:11.462015 139535982765824 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.6211662888526917, loss=0.9362683296203613
I0217 20:39:34.383606 139535991158528 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.9676142334938049, loss=0.9196580052375793
I0217 20:40:56.914404 139535982765824 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.7028738856315613, loss=0.9319761395454407
I0217 20:42:19.739995 139535991158528 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.8048833608627319, loss=0.942991316318512
I0217 20:43:40.791475 139535991158528 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.6517860889434814, loss=0.9433645606040955
I0217 20:44:53.475426 139646656866112 spec.py:321] Evaluating on the training split.
I0217 20:45:47.689658 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 20:46:39.034630 139646656866112 spec.py:349] Evaluating on the test split.
I0217 20:47:05.055039 139646656866112 submission_runner.py:408] Time since start: 56780.34s, 	Step: 66097, 	{'train/ctc_loss': Array(0.07038693, dtype=float32), 'train/wer': 0.027615892312107062, 'validation/ctc_loss': Array(0.3174673, dtype=float32), 'validation/wer': 0.08771252305048419, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16740415, dtype=float32), 'test/wer': 0.053703816545812764, 'test/num_examples': 2472, 'score': 51886.28319978714, 'total_duration': 56780.337996959686, 'accumulated_submission_time': 51886.28319978714, 'accumulated_eval_time': 4889.189654827118, 'accumulated_logging_time': 2.029456615447998}
I0217 20:47:05.094042 139535991158528 logging_writer.py:48] [66097] accumulated_eval_time=4889.189655, accumulated_logging_time=2.029457, accumulated_submission_time=51886.283200, global_step=66097, preemption_count=0, score=51886.283200, test/ctc_loss=0.1674041450023651, test/num_examples=2472, test/wer=0.053704, total_duration=56780.337997, train/ctc_loss=0.07038693130016327, train/wer=0.027616, validation/ctc_loss=0.3174673020839691, validation/num_examples=5348, validation/wer=0.087713
I0217 20:47:08.229996 139535982765824 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.8116859197616577, loss=0.9135891795158386
I0217 20:48:24.525245 139535991158528 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.6817417740821838, loss=0.8859909176826477
I0217 20:49:40.683755 139535982765824 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.7010391354560852, loss=0.9344784617424011
I0217 20:50:56.990059 139535991158528 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.7598886489868164, loss=0.9791820645332336
I0217 20:52:13.343914 139535982765824 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.7519779205322266, loss=0.9479174017906189
I0217 20:53:29.764431 139535991158528 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.6505963206291199, loss=0.8945204019546509
I0217 20:54:47.114922 139535982765824 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.6441154479980469, loss=0.9277175068855286
I0217 20:56:09.839193 139535991158528 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.6036000847816467, loss=0.9463521242141724
I0217 20:57:31.952191 139535982765824 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.6749168634414673, loss=0.9238027930259705
I0217 20:58:55.372506 139535991158528 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.5989859700202942, loss=0.9451371431350708
I0217 21:00:11.634291 139535982765824 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.7037680745124817, loss=0.9508042931556702
I0217 21:01:27.921153 139535991158528 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.9315871000289917, loss=0.8986395001411438
I0217 21:02:44.205019 139535982765824 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.5879826545715332, loss=0.9412031769752502
I0217 21:04:00.797541 139535991158528 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.879448413848877, loss=0.9488013982772827
I0217 21:05:17.014111 139535982765824 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.7690281867980957, loss=0.9548806548118591
I0217 21:06:38.735197 139535991158528 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.8269346952438354, loss=0.9438679218292236
I0217 21:08:01.212551 139535982765824 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.6784446835517883, loss=0.9061240553855896
I0217 21:09:23.523325 139535991158528 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.678505003452301, loss=0.9199088215827942
I0217 21:10:45.643805 139535982765824 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.9271438121795654, loss=0.9550247192382812
I0217 21:11:05.299037 139646656866112 spec.py:321] Evaluating on the training split.
I0217 21:12:00.399285 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 21:12:52.174671 139646656866112 spec.py:349] Evaluating on the test split.
I0217 21:13:18.558875 139646656866112 submission_runner.py:408] Time since start: 58353.84s, 	Step: 67925, 	{'train/ctc_loss': Array(0.06680417, dtype=float32), 'train/wer': 0.026107204299398647, 'validation/ctc_loss': Array(0.30674285, dtype=float32), 'validation/wer': 0.08547264354055437, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16228499, dtype=float32), 'test/wer': 0.0524444986086568, 'test/num_examples': 2472, 'score': 53326.39934182167, 'total_duration': 58353.842539548874, 'accumulated_submission_time': 53326.39934182167, 'accumulated_eval_time': 5022.44410276413, 'accumulated_logging_time': 2.0852842330932617}
I0217 21:13:18.601636 139535991158528 logging_writer.py:48] [67925] accumulated_eval_time=5022.444103, accumulated_logging_time=2.085284, accumulated_submission_time=53326.399342, global_step=67925, preemption_count=0, score=53326.399342, test/ctc_loss=0.1622849851846695, test/num_examples=2472, test/wer=0.052444, total_duration=58353.842540, train/ctc_loss=0.06680417060852051, train/wer=0.026107, validation/ctc_loss=0.3067428469657898, validation/num_examples=5348, validation/wer=0.085473
I0217 21:14:19.994417 139535991158528 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.6303442120552063, loss=0.9159268736839294
I0217 21:15:36.059710 139535982765824 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.8451988697052002, loss=0.9111963510513306
I0217 21:16:52.208280 139535991158528 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.7522615790367126, loss=0.913642406463623
I0217 21:18:08.368947 139535982765824 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.7143001556396484, loss=0.9684969782829285
I0217 21:19:24.589338 139535991158528 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.7034463286399841, loss=0.9340352416038513
I0217 21:20:40.791857 139535982765824 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.9678170680999756, loss=0.9096461534500122
I0217 21:22:01.214689 139535991158528 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.7974924445152283, loss=0.913278341293335
I0217 21:23:24.177071 139535982765824 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.667539119720459, loss=0.9080210328102112
I0217 21:24:47.487065 139535991158528 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.6754823923110962, loss=0.9346721768379211
I0217 21:26:10.201662 139535982765824 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.7079614996910095, loss=0.8849319815635681
I0217 21:27:32.501808 139535991158528 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.0053610801696777, loss=0.9155726432800293
I0217 21:28:52.663426 139535991158528 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.6818965077400208, loss=0.8942040205001831
I0217 21:30:08.962785 139535982765824 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.6581243276596069, loss=0.8976333737373352
I0217 21:31:25.234299 139535991158528 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.8394690752029419, loss=0.8877358436584473
I0217 21:32:41.481666 139535982765824 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.6756373643875122, loss=0.8789858222007751
I0217 21:33:57.786892 139535991158528 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.0512713193893433, loss=0.9238112568855286
I0217 21:35:14.065034 139535982765824 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.7063981294631958, loss=0.9063907265663147
I0217 21:36:34.334212 139535991158528 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.8685356378555298, loss=0.8711315989494324
I0217 21:37:18.759995 139646656866112 spec.py:321] Evaluating on the training split.
I0217 21:38:13.853148 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 21:39:05.579995 139646656866112 spec.py:349] Evaluating on the test split.
I0217 21:39:31.923593 139646656866112 submission_runner.py:408] Time since start: 59927.21s, 	Step: 69755, 	{'train/ctc_loss': Array(0.05866672, dtype=float32), 'train/wer': 0.022810507040474836, 'validation/ctc_loss': Array(0.3071572, dtype=float32), 'validation/wer': 0.08567539125481526, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1607685, dtype=float32), 'test/wer': 0.05183515121971036, 'test/num_examples': 2472, 'score': 54766.46932411194, 'total_duration': 59927.20699119568, 'accumulated_submission_time': 54766.46932411194, 'accumulated_eval_time': 5155.602071285248, 'accumulated_logging_time': 2.1447830200195312}
I0217 21:39:31.971311 139535991158528 logging_writer.py:48] [69755] accumulated_eval_time=5155.602071, accumulated_logging_time=2.144783, accumulated_submission_time=54766.469324, global_step=69755, preemption_count=0, score=54766.469324, test/ctc_loss=0.16076849400997162, test/num_examples=2472, test/wer=0.051835, total_duration=59927.206991, train/ctc_loss=0.05866672098636627, train/wer=0.022811, validation/ctc_loss=0.3071571886539459, validation/num_examples=5348, validation/wer=0.085675
I0217 21:40:07.196799 139535982765824 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.7934327721595764, loss=0.8805585503578186
I0217 21:41:23.430928 139535991158528 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.7635939717292786, loss=0.893364429473877
I0217 21:42:39.749037 139535982765824 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.6765367984771729, loss=0.9240440726280212
I0217 21:43:59.325186 139535991158528 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.5850229263305664, loss=0.8530488610267639
I0217 21:45:15.465774 139535982765824 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.6819096207618713, loss=0.9341951012611389
I0217 21:46:31.553330 139535991158528 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.5744965076446533, loss=0.8797154426574707
I0217 21:47:47.691150 139535982765824 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.922294020652771, loss=0.9032217860221863
I0217 21:49:03.847429 139535991158528 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.7598606944084167, loss=0.9156303405761719
I0217 21:50:19.960314 139535982765824 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.7937551736831665, loss=0.8866316676139832
I0217 21:51:41.997577 139535991158528 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.8324725031852722, loss=0.8988621234893799
I0217 21:53:04.114210 139535982765824 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.7904171943664551, loss=0.8929275274276733
I0217 21:54:26.788087 139535991158528 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.7424851655960083, loss=0.9094371795654297
I0217 21:55:50.337934 139535982765824 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.8046034574508667, loss=0.9183231592178345
I0217 21:57:13.594680 139535991158528 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.7022746801376343, loss=0.8818811178207397
I0217 21:58:29.761717 139535982765824 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.7203735709190369, loss=0.9010800123214722
I0217 21:59:45.967302 139535991158528 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.6947776079177856, loss=0.8953155875205994
I0217 22:01:02.198089 139535982765824 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.9789494872093201, loss=0.8968398571014404
I0217 22:02:18.535523 139535991158528 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.2216821908950806, loss=0.8889622688293457
I0217 22:03:32.187580 139646656866112 spec.py:321] Evaluating on the training split.
I0217 22:04:27.316684 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 22:05:19.199307 139646656866112 spec.py:349] Evaluating on the test split.
I0217 22:05:45.609067 139646656866112 submission_runner.py:408] Time since start: 61500.89s, 	Step: 71598, 	{'train/ctc_loss': Array(0.05321724, dtype=float32), 'train/wer': 0.020219439003042555, 'validation/ctc_loss': Array(0.30390048, dtype=float32), 'validation/wer': 0.08402444558154802, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15960225, dtype=float32), 'test/wer': 0.05079926065850141, 'test/num_examples': 2472, 'score': 56206.59651684761, 'total_duration': 61500.89174199104, 'accumulated_submission_time': 56206.59651684761, 'accumulated_eval_time': 5289.01722574234, 'accumulated_logging_time': 2.2092363834381104}
I0217 22:05:45.656388 139535991158528 logging_writer.py:48] [71598] accumulated_eval_time=5289.017226, accumulated_logging_time=2.209236, accumulated_submission_time=56206.596517, global_step=71598, preemption_count=0, score=56206.596517, test/ctc_loss=0.15960225462913513, test/num_examples=2472, test/wer=0.050799, total_duration=61500.891742, train/ctc_loss=0.053217243403196335, train/wer=0.020219, validation/ctc_loss=0.30390048027038574, validation/num_examples=5348, validation/wer=0.084024
I0217 22:05:48.062625 139535982765824 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.6994013786315918, loss=0.8929705619812012
I0217 22:07:03.986858 139535991158528 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.8982944488525391, loss=0.9303972125053406
I0217 22:08:20.131438 139535982765824 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.9645461440086365, loss=0.901252269744873
I0217 22:09:36.316029 139535991158528 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.6399485468864441, loss=0.9321200251579285
I0217 22:10:52.379870 139535982765824 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.7011780738830566, loss=0.9240480661392212
I0217 22:12:14.614489 139535991158528 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.8360338807106018, loss=0.8452703952789307
I0217 22:13:30.932920 139535982765824 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.6692460179328918, loss=0.8963421583175659
I0217 22:14:47.555196 139535991158528 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.6223918199539185, loss=0.9047946929931641
I0217 22:16:03.896194 139535982765824 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.7752789258956909, loss=0.8740457892417908
I0217 22:17:20.303153 139535991158528 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.9455729126930237, loss=0.9234638810157776
I0217 22:18:36.661094 139535982765824 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.7422028183937073, loss=0.9360261559486389
I0217 22:19:53.079241 139535991158528 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.8135541677474976, loss=0.934097170829773
I0217 22:21:15.703732 139535982765824 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.6279973387718201, loss=0.8662140369415283
I0217 22:22:38.689278 139535991158528 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.7192912697792053, loss=0.9244973063468933
I0217 22:24:01.556262 139535982765824 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.873600423336029, loss=0.9270004034042358
I0217 22:25:23.736852 139535991158528 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.7900186777114868, loss=0.9242100119590759
I0217 22:26:45.004526 139535991158528 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.7943165898323059, loss=0.882205069065094
I0217 22:28:01.078854 139535982765824 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.877131462097168, loss=0.8267024159431458
I0217 22:29:17.309819 139535991158528 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.7345758676528931, loss=0.8381229639053345
I0217 22:29:46.140498 139646656866112 spec.py:321] Evaluating on the training split.
I0217 22:30:40.817997 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 22:31:32.710737 139646656866112 spec.py:349] Evaluating on the test split.
I0217 22:31:58.684543 139646656866112 submission_runner.py:408] Time since start: 63073.97s, 	Step: 73439, 	{'train/ctc_loss': Array(0.05957009, dtype=float32), 'train/wer': 0.021773615801862656, 'validation/ctc_loss': Array(0.30104917, dtype=float32), 'validation/wer': 0.08340654778570532, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15717694, dtype=float32), 'test/wer': 0.05023053642881807, 'test/num_examples': 2472, 'score': 57646.99338531494, 'total_duration': 63073.967522382736, 'accumulated_submission_time': 57646.99338531494, 'accumulated_eval_time': 5421.555196762085, 'accumulated_logging_time': 2.2709219455718994}
I0217 22:31:58.729374 139535991158528 logging_writer.py:48] [73439] accumulated_eval_time=5421.555197, accumulated_logging_time=2.270922, accumulated_submission_time=57646.993385, global_step=73439, preemption_count=0, score=57646.993385, test/ctc_loss=0.1571769416332245, test/num_examples=2472, test/wer=0.050231, total_duration=63073.967522, train/ctc_loss=0.05957008898258209, train/wer=0.021774, validation/ctc_loss=0.3010491728782654, validation/num_examples=5348, validation/wer=0.083407
I0217 22:32:45.823568 139535982765824 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.8320066928863525, loss=0.8997949957847595
I0217 22:34:02.025892 139535991158528 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.7374463677406311, loss=0.8938367962837219
I0217 22:35:18.425277 139535982765824 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.6923316717147827, loss=0.8901678323745728
I0217 22:36:34.833529 139535991158528 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.706461489200592, loss=0.889240562915802
I0217 22:37:51.164531 139535982765824 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.7038232684135437, loss=0.8978123068809509
I0217 22:39:11.264877 139535991158528 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.754002571105957, loss=0.9383217096328735
I0217 22:40:33.673850 139535982765824 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.8533186912536621, loss=0.8981954455375671
I0217 22:41:56.619013 139535991158528 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.61298006772995, loss=0.8642695546150208
I0217 22:43:12.833740 139535982765824 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.8095687627792358, loss=0.8817731738090515
I0217 22:44:29.188603 139535991158528 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.8266267776489258, loss=0.9002067446708679
I0217 22:45:45.443698 139535982765824 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.6113177537918091, loss=0.8740283250808716
I0217 22:47:01.800778 139535991158528 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.7340278029441833, loss=0.8983684778213501
I0217 22:48:18.503588 139535982765824 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.7372241020202637, loss=0.883997917175293
I0217 22:49:37.037748 139535991158528 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.0390512943267822, loss=0.8989870548248291
I0217 22:50:58.875210 139535982765824 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.8893454074859619, loss=0.8963034749031067
I0217 22:52:22.095005 139535991158528 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.6140759587287903, loss=0.8262876272201538
I0217 22:53:44.289777 139535982765824 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.7056270241737366, loss=0.9483829140663147
I0217 22:55:08.696720 139535991158528 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.7588470578193665, loss=0.8540951013565063
I0217 22:55:59.271566 139646656866112 spec.py:321] Evaluating on the training split.
I0217 22:56:52.781182 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 22:57:44.400871 139646656866112 spec.py:349] Evaluating on the test split.
I0217 22:58:10.695277 139646656866112 submission_runner.py:408] Time since start: 64645.98s, 	Step: 75268, 	{'train/ctc_loss': Array(0.05639435, dtype=float32), 'train/wer': 0.021032742192350257, 'validation/ctc_loss': Array(0.29899248, dtype=float32), 'validation/wer': 0.08247004643888121, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15624015, dtype=float32), 'test/wer': 0.04970243535839782, 'test/num_examples': 2472, 'score': 59087.447117090225, 'total_duration': 64645.97832798958, 'accumulated_submission_time': 59087.447117090225, 'accumulated_eval_time': 5552.972937345505, 'accumulated_logging_time': 2.3312904834747314}
I0217 22:58:10.738408 139535991158528 logging_writer.py:48] [75268] accumulated_eval_time=5552.972937, accumulated_logging_time=2.331290, accumulated_submission_time=59087.447117, global_step=75268, preemption_count=0, score=59087.447117, test/ctc_loss=0.15624015033245087, test/num_examples=2472, test/wer=0.049702, total_duration=64645.978328, train/ctc_loss=0.05639434605836868, train/wer=0.021033, validation/ctc_loss=0.29899248480796814, validation/num_examples=5348, validation/wer=0.082470
I0217 22:58:35.809962 139535982765824 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.6349876523017883, loss=0.856425404548645
I0217 22:59:51.946807 139535991158528 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.7396644949913025, loss=0.8670415878295898
I0217 23:01:08.302279 139535982765824 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.9577804803848267, loss=0.8929641842842102
I0217 23:02:24.607209 139535991158528 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.7105919718742371, loss=0.9046671390533447
I0217 23:03:40.889250 139535982765824 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.7948818802833557, loss=0.8920916318893433
I0217 23:04:57.218163 139535991158528 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.7317111492156982, loss=0.9154316186904907
I0217 23:06:13.955003 139535982765824 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.8259642124176025, loss=0.8875328302383423
I0217 23:07:31.213044 139535991158528 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.6123865246772766, loss=0.8940392732620239
I0217 23:08:53.940890 139535982765824 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.6336373686790466, loss=0.9144394993782043
I0217 23:10:16.153044 139535991158528 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.8853266835212708, loss=0.9282677173614502
I0217 23:11:36.493777 139535991158528 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.7669662833213806, loss=0.8228524923324585
I0217 23:12:52.741558 139535982765824 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.6755866408348083, loss=0.883115291595459
I0217 23:14:09.043270 139535991158528 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.8560798168182373, loss=0.8618738651275635
I0217 23:15:25.300411 139535982765824 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.6403120160102844, loss=0.8747957944869995
I0217 23:16:41.723787 139535991158528 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.6828025579452515, loss=0.9289016127586365
I0217 23:17:58.089226 139535982765824 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.6560624241828918, loss=0.8951740860939026
I0217 23:19:19.331096 139535991158528 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.7154675126075745, loss=0.8726063966751099
I0217 23:20:41.989990 139535982765824 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.6842659711837769, loss=0.856713056564331
I0217 23:22:04.889123 139535991158528 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.6274368762969971, loss=0.8594723343849182
I0217 23:22:11.061058 139646656866112 spec.py:321] Evaluating on the training split.
I0217 23:23:05.423400 139646656866112 spec.py:333] Evaluating on the validation split.
I0217 23:23:57.070568 139646656866112 spec.py:349] Evaluating on the test split.
I0217 23:24:23.611388 139646656866112 submission_runner.py:408] Time since start: 66218.89s, 	Step: 77109, 	{'train/ctc_loss': Array(0.05739345, dtype=float32), 'train/wer': 0.021495527344796185, 'validation/ctc_loss': Array(0.2971615, dtype=float32), 'validation/wer': 0.0818038753777383, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15573071, dtype=float32), 'test/wer': 0.049499319562082346, 'test/num_examples': 2472, 'score': 60527.67941856384, 'total_duration': 66218.89422011375, 'accumulated_submission_time': 60527.67941856384, 'accumulated_eval_time': 5685.517039775848, 'accumulated_logging_time': 2.3913497924804688}
I0217 23:24:23.657818 139535991158528 logging_writer.py:48] [77109] accumulated_eval_time=5685.517040, accumulated_logging_time=2.391350, accumulated_submission_time=60527.679419, global_step=77109, preemption_count=0, score=60527.679419, test/ctc_loss=0.1557307094335556, test/num_examples=2472, test/wer=0.049499, total_duration=66218.894220, train/ctc_loss=0.057393450289964676, train/wer=0.021496, validation/ctc_loss=0.2971614897251129, validation/num_examples=5348, validation/wer=0.081804
I0217 23:25:33.691071 139535982765824 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.8641009330749512, loss=0.9160482883453369
I0217 23:26:53.362381 139535991158528 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.735898494720459, loss=0.8820133209228516
I0217 23:28:09.463340 139535982765824 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.5924075245857239, loss=0.8645745515823364
I0217 23:29:25.535573 139535991158528 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.9036030173301697, loss=0.9108078479766846
I0217 23:30:41.728179 139535982765824 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.8682933449745178, loss=0.8661080598831177
I0217 23:31:57.984454 139535991158528 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.8519583344459534, loss=0.875768780708313
I0217 23:33:14.755190 139535982765824 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.8387049436569214, loss=0.8702855706214905
I0217 23:33:24.216269 139535991158528 logging_writer.py:48] [77813] global_step=77813, preemption_count=0, score=61068.168312
I0217 23:33:25.149416 139646656866112 checkpoints.py:490] Saving checkpoint at step: 77813
I0217 23:33:26.733305 139646656866112 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_2/librispeech_conformer_jax/trial_5/checkpoint_77813
I0217 23:33:26.767706 139646656866112 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_2/librispeech_conformer_jax/trial_5/checkpoint_77813.
I0217 23:33:29.312782 139646656866112 submission_runner.py:583] Tuning trial 5/5
I0217 23:33:29.313116 139646656866112 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0217 23:33:29.346647 139646656866112 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(32.05279, dtype=float32), 'train/wer': 1.151012912416401, 'validation/ctc_loss': Array(30.757864, dtype=float32), 'validation/wer': 1.1779352558965794, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.871214, dtype=float32), 'test/wer': 1.190014827453131, 'test/num_examples': 2472, 'score': 36.661099910736084, 'total_duration': 171.07474374771118, 'accumulated_submission_time': 36.661099910736084, 'accumulated_eval_time': 134.41353964805603, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1820, {'train/ctc_loss': Array(3.4769754, dtype=float32), 'train/wer': 0.6580758933017746, 'validation/ctc_loss': Array(3.3963308, dtype=float32), 'validation/wer': 0.6373519217586916, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.0782773, dtype=float32), 'test/wer': 0.5873296366258404, 'test/num_examples': 2472, 'score': 1477.4896006584167, 'total_duration': 1735.0940651893616, 'accumulated_submission_time': 1477.4896006584167, 'accumulated_eval_time': 257.50309681892395, 'accumulated_logging_time': 0.027042150497436523, 'global_step': 1820, 'preemption_count': 0}), (3665, {'train/ctc_loss': Array(0.88411194, dtype=float32), 'train/wer': 0.27776975049271474, 'validation/ctc_loss': Array(0.9202193, dtype=float32), 'validation/wer': 0.26832211784469523, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.64782745, dtype=float32), 'test/wer': 0.2096155017975748, 'test/num_examples': 2472, 'score': 2917.4977061748505, 'total_duration': 3306.8418798446655, 'accumulated_submission_time': 2917.4977061748505, 'accumulated_eval_time': 389.10982155799866, 'accumulated_logging_time': 0.08170270919799805, 'global_step': 3665, 'preemption_count': 0}), (5510, {'train/ctc_loss': Array(0.5478368, dtype=float32), 'train/wer': 0.18741153359705068, 'validation/ctc_loss': Array(0.74291396, dtype=float32), 'validation/wer': 0.22239493323807408, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4876269, dtype=float32), 'test/wer': 0.16385351288769728, 'test/num_examples': 2472, 'score': 4358.005998134613, 'total_duration': 4880.403652906418, 'accumulated_submission_time': 4358.005998134613, 'accumulated_eval_time': 522.0387523174286, 'accumulated_logging_time': 0.12815475463867188, 'global_step': 5510, 'preemption_count': 0}), (7342, {'train/ctc_loss': Array(0.55028814, dtype=float32), 'train/wer': 0.18481111269599185, 'validation/ctc_loss': Array(0.6664708, dtype=float32), 'validation/wer': 0.20184017687324407, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42051786, dtype=float32), 'test/wer': 0.14327788272093922, 'test/num_examples': 2472, 'score': 5798.597426176071, 'total_duration': 6452.462328910828, 'accumulated_submission_time': 5798.597426176071, 'accumulated_eval_time': 653.3796038627625, 'accumulated_logging_time': 0.17641258239746094, 'global_step': 7342, 'preemption_count': 0}), (9177, {'train/ctc_loss': Array(0.45161825, dtype=float32), 'train/wer': 0.15622523916638748, 'validation/ctc_loss': Array(0.6241024, dtype=float32), 'validation/wer': 0.18807264160962375, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39491493, dtype=float32), 'test/wer': 0.13308146974590213, 'test/num_examples': 2472, 'score': 7238.834161758423, 'total_duration': 8026.899383306503, 'accumulated_submission_time': 7238.834161758423, 'accumulated_eval_time': 787.4491305351257, 'accumulated_logging_time': 0.22736024856567383, 'global_step': 9177, 'preemption_count': 0}), (11007, {'train/ctc_loss': Array(0.43340543, dtype=float32), 'train/wer': 0.1545875990998041, 'validation/ctc_loss': Array(0.60133076, dtype=float32), 'validation/wer': 0.18090888903907237, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3724855, dtype=float32), 'test/wer': 0.1259114821359657, 'test/num_examples': 2472, 'score': 8678.961176633835, 'total_duration': 9599.821209192276, 'accumulated_submission_time': 8678.961176633835, 'accumulated_eval_time': 920.1078622341156, 'accumulated_logging_time': 0.2839655876159668, 'global_step': 11007, 'preemption_count': 0}), (12854, {'train/ctc_loss': Array(0.40785065, dtype=float32), 'train/wer': 0.1410327868852459, 'validation/ctc_loss': Array(0.5672202, dtype=float32), 'validation/wer': 0.17057841026482715, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35132352, dtype=float32), 'test/wer': 0.11996018930392216, 'test/num_examples': 2472, 'score': 10119.012127161026, 'total_duration': 11171.424641609192, 'accumulated_submission_time': 10119.012127161026, 'accumulated_eval_time': 1051.5222551822662, 'accumulated_logging_time': 0.34063029289245605, 'global_step': 12854, 'preemption_count': 0}), (14692, {'train/ctc_loss': Array(0.4037296, dtype=float32), 'train/wer': 0.1419416377749473, 'validation/ctc_loss': Array(0.55786395, dtype=float32), 'validation/wer': 0.1679716539386157, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33908087, dtype=float32), 'test/wer': 0.11478073649787744, 'test/num_examples': 2472, 'score': 11558.991518497467, 'total_duration': 12742.170778512955, 'accumulated_submission_time': 11558.991518497467, 'accumulated_eval_time': 1182.161565065384, 'accumulated_logging_time': 0.389937162399292, 'global_step': 14692, 'preemption_count': 0}), (16515, {'train/ctc_loss': Array(0.399308, dtype=float32), 'train/wer': 0.13758047744419125, 'validation/ctc_loss': Array(0.535952, dtype=float32), 'validation/wer': 0.16143545381696708, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32374918, dtype=float32), 'test/wer': 0.11096215952714643, 'test/num_examples': 2472, 'score': 12999.028420209885, 'total_duration': 14314.772800445557, 'accumulated_submission_time': 12999.028420209885, 'accumulated_eval_time': 1314.5952589511871, 'accumulated_logging_time': 0.441178560256958, 'global_step': 16515, 'preemption_count': 0}), (18354, {'train/ctc_loss': Array(0.37713492, dtype=float32), 'train/wer': 0.13267483690018902, 'validation/ctc_loss': Array(0.5242192, dtype=float32), 'validation/wer': 0.15884800679687575, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31622794, dtype=float32), 'test/wer': 0.1068389088619422, 'test/num_examples': 2472, 'score': 14439.1499106884, 'total_duration': 15887.59229850769, 'accumulated_submission_time': 14439.1499106884, 'accumulated_eval_time': 1447.1639330387115, 'accumulated_logging_time': 0.4920070171356201, 'global_step': 18354, 'preemption_count': 0}), (20191, {'train/ctc_loss': Array(0.37280694, dtype=float32), 'train/wer': 0.13489396763806996, 'validation/ctc_loss': Array(0.50043136, dtype=float32), 'validation/wer': 0.15259179161396835, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3005866, dtype=float32), 'test/wer': 0.10297970873194809, 'test/num_examples': 2472, 'score': 15879.571325778961, 'total_duration': 17459.250715255737, 'accumulated_submission_time': 15879.571325778961, 'accumulated_eval_time': 1578.2720866203308, 'accumulated_logging_time': 0.5422773361206055, 'global_step': 20191, 'preemption_count': 0}), (22036, {'train/ctc_loss': Array(0.31283918, dtype=float32), 'train/wer': 0.11125854066761731, 'validation/ctc_loss': Array(0.49085632, dtype=float32), 'validation/wer': 0.1479865221043282, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29445583, dtype=float32), 'test/wer': 0.10050169601689923, 'test/num_examples': 2472, 'score': 17319.888331651688, 'total_duration': 19031.927735090256, 'accumulated_submission_time': 17319.888331651688, 'accumulated_eval_time': 1710.5015604496002, 'accumulated_logging_time': 0.5935218334197998, 'global_step': 22036, 'preemption_count': 0}), (23861, {'train/ctc_loss': Array(0.32723373, dtype=float32), 'train/wer': 0.11497332248146139, 'validation/ctc_loss': Array(0.48551804, dtype=float32), 'validation/wer': 0.14419224345173157, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28720966, dtype=float32), 'test/wer': 0.09556598216643308, 'test/num_examples': 2472, 'score': 18759.982160806656, 'total_duration': 20603.729377031326, 'accumulated_submission_time': 18759.982160806656, 'accumulated_eval_time': 1842.077701330185, 'accumulated_logging_time': 0.6459090709686279, 'global_step': 23861, 'preemption_count': 0}), (25690, {'train/ctc_loss': Array(0.31858635, dtype=float32), 'train/wer': 0.11207038176328298, 'validation/ctc_loss': Array(0.47096306, dtype=float32), 'validation/wer': 0.14148894059491973, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27616546, dtype=float32), 'test/wer': 0.09513943899417057, 'test/num_examples': 2472, 'score': 20200.02801823616, 'total_duration': 22175.355345249176, 'accumulated_submission_time': 20200.02801823616, 'accumulated_eval_time': 1973.528375864029, 'accumulated_logging_time': 0.6977200508117676, 'global_step': 25690, 'preemption_count': 0}), (27515, {'train/ctc_loss': Array(0.31003976, dtype=float32), 'train/wer': 0.10981249382119358, 'validation/ctc_loss': Array(0.46048242, dtype=float32), 'validation/wer': 0.13728916651380133, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27005804, dtype=float32), 'test/wer': 0.09160522413828123, 'test/num_examples': 2472, 'score': 21640.408204317093, 'total_duration': 23748.209998607635, 'accumulated_submission_time': 21640.408204317093, 'accumulated_eval_time': 2105.8720936775208, 'accumulated_logging_time': 0.7506282329559326, 'global_step': 27515, 'preemption_count': 0}), (29360, {'train/ctc_loss': Array(0.27465776, dtype=float32), 'train/wer': 0.09899000135116875, 'validation/ctc_loss': Array(0.45722303, dtype=float32), 'validation/wer': 0.1353389265956728, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26472273, dtype=float32), 'test/wer': 0.08870066825096988, 'test/num_examples': 2472, 'score': 23080.796627759933, 'total_duration': 25320.125111341476, 'accumulated_submission_time': 23080.796627759933, 'accumulated_eval_time': 2237.2667417526245, 'accumulated_logging_time': 0.8045666217803955, 'global_step': 29360, 'preemption_count': 0}), (31195, {'train/ctc_loss': Array(0.24588323, dtype=float32), 'train/wer': 0.08950538002682878, 'validation/ctc_loss': Array(0.4484083, dtype=float32), 'validation/wer': 0.13362039835098527, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25443456, dtype=float32), 'test/wer': 0.08573517762476388, 'test/num_examples': 2472, 'score': 24520.750933647156, 'total_duration': 26890.447347164154, 'accumulated_submission_time': 24520.750933647156, 'accumulated_eval_time': 2367.49915266037, 'accumulated_logging_time': 0.8600101470947266, 'global_step': 31195, 'preemption_count': 0}), (33018, {'train/ctc_loss': Array(0.27327314, dtype=float32), 'train/wer': 0.09891458662096506, 'validation/ctc_loss': Array(0.44101134, dtype=float32), 'validation/wer': 0.13046332680035144, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24990562, dtype=float32), 'test/wer': 0.08211971645034834, 'test/num_examples': 2472, 'score': 25961.20705795288, 'total_duration': 28461.4237473011, 'accumulated_submission_time': 25961.20705795288, 'accumulated_eval_time': 2497.8841466903687, 'accumulated_logging_time': 0.9160919189453125, 'global_step': 33018, 'preemption_count': 0}), (34855, {'train/ctc_loss': Array(0.24580301, dtype=float32), 'train/wer': 0.0894540231484832, 'validation/ctc_loss': Array(0.42582294, dtype=float32), 'validation/wer': 0.12698765169873621, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24126007, dtype=float32), 'test/wer': 0.07996668900940426, 'test/num_examples': 2472, 'score': 27401.429488182068, 'total_duration': 30034.206517457962, 'accumulated_submission_time': 27401.429488182068, 'accumulated_eval_time': 2630.312391757965, 'accumulated_logging_time': 0.9704453945159912, 'global_step': 34855, 'preemption_count': 0}), (36692, {'train/ctc_loss': Array(0.26628497, dtype=float32), 'train/wer': 0.09255809009069496, 'validation/ctc_loss': Array(0.42301342, dtype=float32), 'validation/wer': 0.1271903994129971, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24345164, dtype=float32), 'test/wer': 0.08142912274287571, 'test/num_examples': 2472, 'score': 28841.924087047577, 'total_duration': 31606.14753127098, 'accumulated_submission_time': 28841.924087047577, 'accumulated_eval_time': 2761.6267426013947, 'accumulated_logging_time': 1.024658441543579, 'global_step': 36692, 'preemption_count': 0}), (38534, {'train/ctc_loss': Array(0.2095532, dtype=float32), 'train/wer': 0.07635212007176301, 'validation/ctc_loss': Array(0.40643167, dtype=float32), 'validation/wer': 0.12010388406692606, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2337808, dtype=float32), 'test/wer': 0.07736680681656613, 'test/num_examples': 2472, 'score': 30282.230488061905, 'total_duration': 33178.02899599075, 'accumulated_submission_time': 30282.230488061905, 'accumulated_eval_time': 2893.059848546982, 'accumulated_logging_time': 1.0862317085266113, 'global_step': 38534, 'preemption_count': 0}), (40365, {'train/ctc_loss': Array(0.19563828, dtype=float32), 'train/wer': 0.07309344373286569, 'validation/ctc_loss': Array(0.40345013, dtype=float32), 'validation/wer': 0.11903221757726136, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22827744, dtype=float32), 'test/wer': 0.07596530782198932, 'test/num_examples': 2472, 'score': 31722.741436958313, 'total_duration': 34749.68506407738, 'accumulated_submission_time': 31722.741436958313, 'accumulated_eval_time': 3024.0696020126343, 'accumulated_logging_time': 1.1442553997039795, 'global_step': 40365, 'preemption_count': 0}), (42198, {'train/ctc_loss': Array(0.21608451, dtype=float32), 'train/wer': 0.07941671660531348, 'validation/ctc_loss': Array(0.39799383, dtype=float32), 'validation/wer': 0.11731368933257383, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21821883, dtype=float32), 'test/wer': 0.0725732740235208, 'test/num_examples': 2472, 'score': 33163.28744673729, 'total_duration': 36320.83214735985, 'accumulated_submission_time': 33163.28744673729, 'accumulated_eval_time': 3154.5400528907776, 'accumulated_logging_time': 1.1971325874328613, 'global_step': 42198, 'preemption_count': 0}), (44043, {'train/ctc_loss': Array(0.14231463, dtype=float32), 'train/wer': 0.05326419344658345, 'validation/ctc_loss': Array(0.38264957, dtype=float32), 'validation/wer': 0.11266014655763346, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21474697, dtype=float32), 'test/wer': 0.07060305079926066, 'test/num_examples': 2472, 'score': 34603.37959957123, 'total_duration': 37903.424902677536, 'accumulated_submission_time': 34603.37959957123, 'accumulated_eval_time': 3296.9027593135834, 'accumulated_logging_time': 1.2538504600524902, 'global_step': 44043, 'preemption_count': 0}), (45893, {'train/ctc_loss': Array(0.12276159, dtype=float32), 'train/wer': 0.04592168596331469, 'validation/ctc_loss': Array(0.3812691, dtype=float32), 'validation/wer': 0.11052646823136411, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21339078, dtype=float32), 'test/wer': 0.06989214551215649, 'test/num_examples': 2472, 'score': 36043.25439476967, 'total_duration': 39476.55716729164, 'accumulated_submission_time': 36043.25439476967, 'accumulated_eval_time': 3430.021687746048, 'accumulated_logging_time': 1.3127069473266602, 'global_step': 45893, 'preemption_count': 0}), (47735, {'train/ctc_loss': Array(0.12371109, dtype=float32), 'train/wer': 0.047442438683276786, 'validation/ctc_loss': Array(0.36838114, dtype=float32), 'validation/wer': 0.1077845467623121, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2057504, dtype=float32), 'test/wer': 0.06780005281010704, 'test/num_examples': 2472, 'score': 37483.68469452858, 'total_duration': 41051.488491773605, 'accumulated_submission_time': 37483.68469452858, 'accumulated_eval_time': 3564.3747503757477, 'accumulated_logging_time': 1.379469633102417, 'global_step': 47735, 'preemption_count': 0}), (49571, {'train/ctc_loss': Array(0.12066799, dtype=float32), 'train/wer': 0.04577250529184458, 'validation/ctc_loss': Array(0.36527857, dtype=float32), 'validation/wer': 0.1063460034563658, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20391922, dtype=float32), 'test/wer': 0.06710945910263441, 'test/num_examples': 2472, 'score': 38924.20606184006, 'total_duration': 42625.732850551605, 'accumulated_submission_time': 38924.20606184006, 'accumulated_eval_time': 3697.955447912216, 'accumulated_logging_time': 1.4431352615356445, 'global_step': 49571, 'preemption_count': 0}), (51403, {'train/ctc_loss': Array(0.10996965, dtype=float32), 'train/wer': 0.042697279673189405, 'validation/ctc_loss': Array(0.3555751, dtype=float32), 'validation/wer': 0.1044054181912973, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19339007, dtype=float32), 'test/wer': 0.06550484431174212, 'test/num_examples': 2472, 'score': 40364.20062971115, 'total_duration': 44197.879747867584, 'accumulated_submission_time': 40364.20062971115, 'accumulated_eval_time': 3829.9755721092224, 'accumulated_logging_time': 1.4969408512115479, 'global_step': 51403, 'preemption_count': 0}), (53238, {'train/ctc_loss': Array(0.10054823, dtype=float32), 'train/wer': 0.039402895769823955, 'validation/ctc_loss': Array(0.35113725, dtype=float32), 'validation/wer': 0.10176004325284571, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18965232, dtype=float32), 'test/wer': 0.06260028842443077, 'test/num_examples': 2472, 'score': 41804.38695335388, 'total_duration': 45773.00025463104, 'accumulated_submission_time': 41804.38695335388, 'accumulated_eval_time': 3964.7778856754303, 'accumulated_logging_time': 1.5501763820648193, 'global_step': 53238, 'preemption_count': 0}), (55087, {'train/ctc_loss': Array(0.11465675, dtype=float32), 'train/wer': 0.04309293974236484, 'validation/ctc_loss': Array(0.34877175, dtype=float32), 'validation/wer': 0.09976153006941696, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18984833, dtype=float32), 'test/wer': 0.0627018463225885, 'test/num_examples': 2472, 'score': 43244.36760210991, 'total_duration': 47344.014533519745, 'accumulated_submission_time': 43244.36760210991, 'accumulated_eval_time': 4095.6756834983826, 'accumulated_logging_time': 1.6069636344909668, 'global_step': 55087, 'preemption_count': 0}), (56927, {'train/ctc_loss': Array(0.09401891, dtype=float32), 'train/wer': 0.035591196575403175, 'validation/ctc_loss': Array(0.33511576, dtype=float32), 'validation/wer': 0.09663342247796325, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18326183, dtype=float32), 'test/wer': 0.06002071781122418, 'test/num_examples': 2472, 'score': 44684.721542835236, 'total_duration': 48916.36607122421, 'accumulated_submission_time': 44684.721542835236, 'accumulated_eval_time': 4227.535162687302, 'accumulated_logging_time': 1.6658551692962646, 'global_step': 56927, 'preemption_count': 0}), (58747, {'train/ctc_loss': Array(0.09163833, dtype=float32), 'train/wer': 0.035968463669294695, 'validation/ctc_loss': Array(0.33312958, dtype=float32), 'validation/wer': 0.0941328673354123, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18139437, dtype=float32), 'test/wer': 0.057969248268437835, 'test/num_examples': 2472, 'score': 46124.952325344086, 'total_duration': 50489.70118045807, 'accumulated_submission_time': 46124.952325344086, 'accumulated_eval_time': 4360.501399755478, 'accumulated_logging_time': 1.725421667098999, 'global_step': 58747, 'preemption_count': 0}), (60586, {'train/ctc_loss': Array(0.07575043, dtype=float32), 'train/wer': 0.030003598949953424, 'validation/ctc_loss': Array(0.32939577, dtype=float32), 'validation/wer': 0.09374668121301061, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17375073, dtype=float32), 'test/wer': 0.05784737879064855, 'test/num_examples': 2472, 'score': 47565.06146478653, 'total_duration': 52062.507381916046, 'accumulated_submission_time': 47565.06146478653, 'accumulated_eval_time': 4493.0639543533325, 'accumulated_logging_time': 1.7812588214874268, 'global_step': 60586, 'preemption_count': 0}), (62424, {'train/ctc_loss': Array(0.07041831, dtype=float32), 'train/wer': 0.028381517879350764, 'validation/ctc_loss': Array(0.3231744, dtype=float32), 'validation/wer': 0.09186402386630237, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17216443, dtype=float32), 'test/wer': 0.05646619137570329, 'test/num_examples': 2472, 'score': 49005.45624756813, 'total_duration': 53636.187504053116, 'accumulated_submission_time': 49005.45624756813, 'accumulated_eval_time': 4626.207343816757, 'accumulated_logging_time': 1.8439528942108154, 'global_step': 62424, 'preemption_count': 0}), (64266, {'train/ctc_loss': Array(0.07730179, dtype=float32), 'train/wer': 0.030322357468704732, 'validation/ctc_loss': Array(0.31241328, dtype=float32), 'validation/wer': 0.08855247786670786, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1681303, dtype=float32), 'test/wer': 0.05378506286433896, 'test/num_examples': 2472, 'score': 50445.885021448135, 'total_duration': 55208.22303843498, 'accumulated_submission_time': 50445.885021448135, 'accumulated_eval_time': 4757.616107225418, 'accumulated_logging_time': 1.9643621444702148, 'global_step': 64266, 'preemption_count': 0}), (66097, {'train/ctc_loss': Array(0.07038693, dtype=float32), 'train/wer': 0.027615892312107062, 'validation/ctc_loss': Array(0.3174673, dtype=float32), 'validation/wer': 0.08771252305048419, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16740415, dtype=float32), 'test/wer': 0.053703816545812764, 'test/num_examples': 2472, 'score': 51886.28319978714, 'total_duration': 56780.337996959686, 'accumulated_submission_time': 51886.28319978714, 'accumulated_eval_time': 4889.189654827118, 'accumulated_logging_time': 2.029456615447998, 'global_step': 66097, 'preemption_count': 0}), (67925, {'train/ctc_loss': Array(0.06680417, dtype=float32), 'train/wer': 0.026107204299398647, 'validation/ctc_loss': Array(0.30674285, dtype=float32), 'validation/wer': 0.08547264354055437, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16228499, dtype=float32), 'test/wer': 0.0524444986086568, 'test/num_examples': 2472, 'score': 53326.39934182167, 'total_duration': 58353.842539548874, 'accumulated_submission_time': 53326.39934182167, 'accumulated_eval_time': 5022.44410276413, 'accumulated_logging_time': 2.0852842330932617, 'global_step': 67925, 'preemption_count': 0}), (69755, {'train/ctc_loss': Array(0.05866672, dtype=float32), 'train/wer': 0.022810507040474836, 'validation/ctc_loss': Array(0.3071572, dtype=float32), 'validation/wer': 0.08567539125481526, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1607685, dtype=float32), 'test/wer': 0.05183515121971036, 'test/num_examples': 2472, 'score': 54766.46932411194, 'total_duration': 59927.20699119568, 'accumulated_submission_time': 54766.46932411194, 'accumulated_eval_time': 5155.602071285248, 'accumulated_logging_time': 2.1447830200195312, 'global_step': 69755, 'preemption_count': 0}), (71598, {'train/ctc_loss': Array(0.05321724, dtype=float32), 'train/wer': 0.020219439003042555, 'validation/ctc_loss': Array(0.30390048, dtype=float32), 'validation/wer': 0.08402444558154802, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15960225, dtype=float32), 'test/wer': 0.05079926065850141, 'test/num_examples': 2472, 'score': 56206.59651684761, 'total_duration': 61500.89174199104, 'accumulated_submission_time': 56206.59651684761, 'accumulated_eval_time': 5289.01722574234, 'accumulated_logging_time': 2.2092363834381104, 'global_step': 71598, 'preemption_count': 0}), (73439, {'train/ctc_loss': Array(0.05957009, dtype=float32), 'train/wer': 0.021773615801862656, 'validation/ctc_loss': Array(0.30104917, dtype=float32), 'validation/wer': 0.08340654778570532, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15717694, dtype=float32), 'test/wer': 0.05023053642881807, 'test/num_examples': 2472, 'score': 57646.99338531494, 'total_duration': 63073.967522382736, 'accumulated_submission_time': 57646.99338531494, 'accumulated_eval_time': 5421.555196762085, 'accumulated_logging_time': 2.2709219455718994, 'global_step': 73439, 'preemption_count': 0}), (75268, {'train/ctc_loss': Array(0.05639435, dtype=float32), 'train/wer': 0.021032742192350257, 'validation/ctc_loss': Array(0.29899248, dtype=float32), 'validation/wer': 0.08247004643888121, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15624015, dtype=float32), 'test/wer': 0.04970243535839782, 'test/num_examples': 2472, 'score': 59087.447117090225, 'total_duration': 64645.97832798958, 'accumulated_submission_time': 59087.447117090225, 'accumulated_eval_time': 5552.972937345505, 'accumulated_logging_time': 2.3312904834747314, 'global_step': 75268, 'preemption_count': 0}), (77109, {'train/ctc_loss': Array(0.05739345, dtype=float32), 'train/wer': 0.021495527344796185, 'validation/ctc_loss': Array(0.2971615, dtype=float32), 'validation/wer': 0.0818038753777383, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15573071, dtype=float32), 'test/wer': 0.049499319562082346, 'test/num_examples': 2472, 'score': 60527.67941856384, 'total_duration': 66218.89422011375, 'accumulated_submission_time': 60527.67941856384, 'accumulated_eval_time': 5685.517039775848, 'accumulated_logging_time': 2.3913497924804688, 'global_step': 77109, 'preemption_count': 0})], 'global_step': 77813}
I0217 23:33:29.346980 139646656866112 submission_runner.py:586] Timing: 61068.168311834335
I0217 23:33:29.347060 139646656866112 submission_runner.py:588] Total number of evals: 43
I0217 23:33:29.347148 139646656866112 submission_runner.py:589] ====================
I0217 23:33:29.410871 139646656866112 submission_runner.py:673] Final librispeech_conformer score: 61068.04541969299
