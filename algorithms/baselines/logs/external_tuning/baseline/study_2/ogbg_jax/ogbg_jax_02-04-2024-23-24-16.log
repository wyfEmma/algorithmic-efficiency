python3 submission_runner.py --framework=jax --workload=ogbg --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification/study_2 --overwrite=true --save_checkpoints=false --num_tuning_trials=5 --rng_seed=449608868 --max_global_steps=80000 2>&1 | tee -a /logs/ogbg_jax_02-04-2024-23-24-16.log
I0204 23:24:36.789092 140451058161472 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification/study_2/ogbg_jax.
I0204 23:24:37.840034 140451058161472 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0204 23:24:37.840748 140451058161472 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0204 23:24:37.840880 140451058161472 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0204 23:24:37.842221 140451058161472 submission_runner.py:542] Using RNG seed 449608868
I0204 23:24:38.905663 140451058161472 submission_runner.py:551] --- Tuning run 1/5 ---
I0204 23:24:38.905876 140451058161472 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_2/ogbg_jax/trial_1.
I0204 23:24:38.906201 140451058161472 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_2/ogbg_jax/trial_1/hparams.json.
I0204 23:24:39.093453 140451058161472 submission_runner.py:206] Initializing dataset.
I0204 23:24:39.207214 140451058161472 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0204 23:24:39.215080 140451058161472 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
W0204 23:24:39.462460 140451058161472 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
I0204 23:24:39.523657 140451058161472 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0204 23:24:39.595237 140451058161472 submission_runner.py:213] Initializing model.
I0204 23:24:44.216239 140451058161472 submission_runner.py:255] Initializing optimizer.
I0204 23:24:44.868808 140451058161472 submission_runner.py:262] Initializing metrics bundle.
I0204 23:24:44.868998 140451058161472 submission_runner.py:280] Initializing checkpoint and logger.
I0204 23:24:44.870209 140451058161472 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_2/ogbg_jax/trial_1 with prefix checkpoint_
I0204 23:24:44.870385 140451058161472 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_2/ogbg_jax/trial_1/meta_data_0.json.
I0204 23:24:44.870634 140451058161472 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0204 23:24:44.870731 140451058161472 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0204 23:24:45.208172 140451058161472 logger_utils.py:220] Unable to record git information. Continuing without it.
I0204 23:24:45.515928 140451058161472 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_2/ogbg_jax/trial_1/flags_0.json.
I0204 23:24:45.526252 140451058161472 submission_runner.py:314] Starting training loop.
I0204 23:25:03.635581 140286164457216 logging_writer.py:48] [0] global_step=0, grad_norm=2.3059589862823486, loss=0.735933780670166
I0204 23:25:03.653011 140451058161472 spec.py:321] Evaluating on the training split.
I0204 23:25:03.659293 140451058161472 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0204 23:25:03.664105 140451058161472 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0204 23:25:03.734249 140451058161472 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0204 23:26:58.131959 140451058161472 spec.py:333] Evaluating on the validation split.
I0204 23:26:58.135738 140451058161472 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0204 23:26:58.139984 140451058161472 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0204 23:26:58.211858 140451058161472 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0204 23:28:31.445373 140451058161472 spec.py:349] Evaluating on the test split.
I0204 23:28:31.449003 140451058161472 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0204 23:28:31.453011 140451058161472 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0204 23:28:31.519719 140451058161472 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0204 23:30:09.116474 140451058161472 submission_runner.py:408] Time since start: 323.59s, 	Step: 1, 	{'train/accuracy': 0.5290504693984985, 'train/loss': 0.7363465428352356, 'train/mean_average_precision': 0.020846484877845774, 'validation/accuracy': 0.5270814895629883, 'validation/loss': 0.737440824508667, 'validation/mean_average_precision': 0.024032072784418074, 'validation/num_examples': 43793, 'test/accuracy': 0.5256852507591248, 'test/loss': 0.7376683354377747, 'test/mean_average_precision': 0.026026918574051882, 'test/num_examples': 43793, 'score': 18.12672519683838, 'total_duration': 323.5901656150818, 'accumulated_submission_time': 18.12672519683838, 'accumulated_eval_time': 305.463401556015, 'accumulated_logging_time': 0}
I0204 23:30:09.135687 140282347202304 logging_writer.py:48] [1] accumulated_eval_time=305.463402, accumulated_logging_time=0, accumulated_submission_time=18.126725, global_step=1, preemption_count=0, score=18.126725, test/accuracy=0.525685, test/loss=0.737668, test/mean_average_precision=0.026027, test/num_examples=43793, total_duration=323.590166, train/accuracy=0.529050, train/loss=0.736347, train/mean_average_precision=0.020846, validation/accuracy=0.527081, validation/loss=0.737441, validation/mean_average_precision=0.024032, validation/num_examples=43793
I0204 23:30:40.887372 140283689346816 logging_writer.py:48] [100] global_step=100, grad_norm=0.6025009751319885, loss=0.4509122967720032
I0204 23:31:12.753067 140282347202304 logging_writer.py:48] [200] global_step=200, grad_norm=0.36551421880722046, loss=0.3305782675743103
I0204 23:31:44.904058 140283689346816 logging_writer.py:48] [300] global_step=300, grad_norm=0.2652187943458557, loss=0.2370993047952652
I0204 23:32:17.575222 140282347202304 logging_writer.py:48] [400] global_step=400, grad_norm=0.17409509420394897, loss=0.16176588833332062
I0204 23:32:50.863883 140283689346816 logging_writer.py:48] [500] global_step=500, grad_norm=0.10639632493257523, loss=0.11636760830879211
I0204 23:33:23.862169 140282347202304 logging_writer.py:48] [600] global_step=600, grad_norm=0.06633882969617844, loss=0.08811673521995544
I0204 23:33:56.943397 140283689346816 logging_writer.py:48] [700] global_step=700, grad_norm=0.04361699894070625, loss=0.07043258845806122
I0204 23:34:09.124243 140451058161472 spec.py:321] Evaluating on the training split.
I0204 23:36:02.602244 140451058161472 spec.py:333] Evaluating on the validation split.
I0204 23:36:05.664662 140451058161472 spec.py:349] Evaluating on the test split.
I0204 23:36:08.587962 140451058161472 submission_runner.py:408] Time since start: 683.06s, 	Step: 738, 	{'train/accuracy': 0.9867269396781921, 'train/loss': 0.06961894780397415, 'train/mean_average_precision': 0.037541683577360145, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.0782666727900505, 'validation/mean_average_precision': 0.03982426540821902, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.08114508539438248, 'test/mean_average_precision': 0.04160785254877804, 'test/num_examples': 43793, 'score': 258.0806932449341, 'total_duration': 683.0615284442902, 'accumulated_submission_time': 258.0806932449341, 'accumulated_eval_time': 424.9269685745239, 'accumulated_logging_time': 0.032134294509887695}
I0204 23:36:08.605074 140283706132224 logging_writer.py:48] [738] accumulated_eval_time=424.926969, accumulated_logging_time=0.032134, accumulated_submission_time=258.080693, global_step=738, preemption_count=0, score=258.080693, test/accuracy=0.983142, test/loss=0.081145, test/mean_average_precision=0.041608, test/num_examples=43793, total_duration=683.061528, train/accuracy=0.986727, train/loss=0.069619, train/mean_average_precision=0.037542, validation/accuracy=0.984118, validation/loss=0.078267, validation/mean_average_precision=0.039824, validation/num_examples=43793
I0204 23:36:29.235625 140283714524928 logging_writer.py:48] [800] global_step=800, grad_norm=0.07282408326864243, loss=0.0689496248960495
I0204 23:37:01.570427 140283706132224 logging_writer.py:48] [900] global_step=900, grad_norm=0.3589112162590027, loss=0.05988559126853943
I0204 23:37:34.814745 140283714524928 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.1895676553249359, loss=0.05538078024983406
I0204 23:38:08.015813 140283706132224 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.22748130559921265, loss=0.05012716352939606
I0204 23:38:40.568166 140283714524928 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.1747264415025711, loss=0.05281250551342964
I0204 23:39:13.222209 140283706132224 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.16766150295734406, loss=0.047847263514995575
I0204 23:39:45.762134 140283714524928 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.11826427280902863, loss=0.052951373159885406
I0204 23:40:08.690073 140451058161472 spec.py:321] Evaluating on the training split.
I0204 23:42:03.665315 140451058161472 spec.py:333] Evaluating on the validation split.
I0204 23:42:06.693423 140451058161472 spec.py:349] Evaluating on the test split.
I0204 23:42:09.661495 140451058161472 submission_runner.py:408] Time since start: 1044.14s, 	Step: 1472, 	{'train/accuracy': 0.9868612289428711, 'train/loss': 0.0518169067800045, 'train/mean_average_precision': 0.07142127442439483, 'validation/accuracy': 0.9841642379760742, 'validation/loss': 0.06164403259754181, 'validation/mean_average_precision': 0.07514380538388722, 'validation/num_examples': 43793, 'test/accuracy': 0.9831724166870117, 'test/loss': 0.06509590893983841, 'test/mean_average_precision': 0.07412052245940108, 'test/num_examples': 43793, 'score': 498.13424348831177, 'total_duration': 1044.1351709365845, 'accumulated_submission_time': 498.13424348831177, 'accumulated_eval_time': 545.8983333110809, 'accumulated_logging_time': 0.06061291694641113}
I0204 23:42:09.677853 140283689346816 logging_writer.py:48] [1472] accumulated_eval_time=545.898333, accumulated_logging_time=0.060613, accumulated_submission_time=498.134243, global_step=1472, preemption_count=0, score=498.134243, test/accuracy=0.983172, test/loss=0.065096, test/mean_average_precision=0.074121, test/num_examples=43793, total_duration=1044.135171, train/accuracy=0.986861, train/loss=0.051817, train/mean_average_precision=0.071421, validation/accuracy=0.984164, validation/loss=0.061644, validation/mean_average_precision=0.075144, validation/num_examples=43793
I0204 23:42:19.055930 140283697739520 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.0920252650976181, loss=0.05878996104001999
I0204 23:42:51.191659 140283689346816 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.23107823729515076, loss=0.05096578225493431
I0204 23:43:23.296864 140283697739520 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.14124836027622223, loss=0.04955790191888809
I0204 23:43:55.389173 140283689346816 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.055839233100414276, loss=0.05114900320768356
I0204 23:44:27.396956 140283697739520 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.06984296441078186, loss=0.05170739069581032
I0204 23:44:59.234956 140283689346816 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.144905686378479, loss=0.04750045761466026
I0204 23:45:31.477424 140283697739520 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.17302438616752625, loss=0.04763580113649368
I0204 23:46:03.407369 140283689346816 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.11109030991792679, loss=0.04735685512423515
I0204 23:46:09.952938 140451058161472 spec.py:321] Evaluating on the training split.
I0204 23:48:06.726473 140451058161472 spec.py:333] Evaluating on the validation split.
I0204 23:48:09.815983 140451058161472 spec.py:349] Evaluating on the test split.
I0204 23:48:12.795893 140451058161472 submission_runner.py:408] Time since start: 1407.27s, 	Step: 2221, 	{'train/accuracy': 0.9878098964691162, 'train/loss': 0.04395898059010506, 'train/mean_average_precision': 0.13697456731632401, 'validation/accuracy': 0.9851465821266174, 'validation/loss': 0.05280575156211853, 'validation/mean_average_precision': 0.12950148084545254, 'validation/num_examples': 43793, 'test/accuracy': 0.9841352701187134, 'test/loss': 0.05575975030660629, 'test/mean_average_precision': 0.13042869335541193, 'test/num_examples': 43793, 'score': 738.378705739975, 'total_duration': 1407.2695829868317, 'accumulated_submission_time': 738.378705739975, 'accumulated_eval_time': 668.7412447929382, 'accumulated_logging_time': 0.08817148208618164}
I0204 23:48:12.812300 140283530442496 logging_writer.py:48] [2221] accumulated_eval_time=668.741245, accumulated_logging_time=0.088171, accumulated_submission_time=738.378706, global_step=2221, preemption_count=0, score=738.378706, test/accuracy=0.984135, test/loss=0.055760, test/mean_average_precision=0.130429, test/num_examples=43793, total_duration=1407.269583, train/accuracy=0.987810, train/loss=0.043959, train/mean_average_precision=0.136975, validation/accuracy=0.985147, validation/loss=0.052806, validation/mean_average_precision=0.129501, validation/num_examples=43793
I0204 23:48:38.773480 140283706132224 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.08574844896793365, loss=0.04384233430027962
I0204 23:49:11.450212 140283530442496 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.07295683771371841, loss=0.04717003181576729
I0204 23:49:43.406004 140283706132224 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.17090803384780884, loss=0.048069991171360016
I0204 23:50:15.439335 140283530442496 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.09688293933868408, loss=0.042467840015888214
I0204 23:50:47.554085 140283706132224 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.11433970183134079, loss=0.04778915271162987
I0204 23:51:19.501649 140283530442496 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.10518012940883636, loss=0.04677712917327881
I0204 23:51:51.271497 140283706132224 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.10108249634504318, loss=0.04361195117235184
I0204 23:52:12.849275 140451058161472 spec.py:321] Evaluating on the training split.
I0204 23:54:08.799262 140451058161472 spec.py:333] Evaluating on the validation split.
I0204 23:54:11.859814 140451058161472 spec.py:349] Evaluating on the test split.
I0204 23:54:14.797334 140451058161472 submission_runner.py:408] Time since start: 1769.27s, 	Step: 2968, 	{'train/accuracy': 0.9880942702293396, 'train/loss': 0.042098671197891235, 'train/mean_average_precision': 0.1608620839752452, 'validation/accuracy': 0.985255777835846, 'validation/loss': 0.051538657397031784, 'validation/mean_average_precision': 0.1466299149649001, 'validation/num_examples': 43793, 'test/accuracy': 0.9843475222587585, 'test/loss': 0.05426577478647232, 'test/mean_average_precision': 0.15363437850865844, 'test/num_examples': 43793, 'score': 978.384375333786, 'total_duration': 1769.2708656787872, 'accumulated_submission_time': 978.384375333786, 'accumulated_eval_time': 790.6891014575958, 'accumulated_logging_time': 0.11685395240783691}
I0204 23:54:14.812532 140283689346816 logging_writer.py:48] [2968] accumulated_eval_time=790.689101, accumulated_logging_time=0.116854, accumulated_submission_time=978.384375, global_step=2968, preemption_count=0, score=978.384375, test/accuracy=0.984348, test/loss=0.054266, test/mean_average_precision=0.153634, test/num_examples=43793, total_duration=1769.270866, train/accuracy=0.988094, train/loss=0.042099, train/mean_average_precision=0.160862, validation/accuracy=0.985256, validation/loss=0.051539, validation/mean_average_precision=0.146630, validation/num_examples=43793
I0204 23:54:25.746120 140283714524928 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.06115945428609848, loss=0.04121742025017738
I0204 23:54:58.234885 140283689346816 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.07749809324741364, loss=0.046617355197668076
I0204 23:55:30.347018 140283714524928 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.06094757840037346, loss=0.04062321409583092
I0204 23:56:02.424219 140283689346816 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.07369783520698547, loss=0.04383118450641632
I0204 23:56:34.441422 140283714524928 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.10774080455303192, loss=0.03982573747634888
I0204 23:57:06.733627 140283689346816 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.08223354071378708, loss=0.04664589837193489
I0204 23:57:38.612229 140283714524928 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.11345597356557846, loss=0.044174980372190475
I0204 23:58:10.809189 140283689346816 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.10969038307666779, loss=0.04126257076859474
I0204 23:58:14.995973 140451058161472 spec.py:321] Evaluating on the training split.
I0205 00:00:14.557973 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 00:00:17.571270 140451058161472 spec.py:349] Evaluating on the test split.
I0205 00:00:20.540871 140451058161472 submission_runner.py:408] Time since start: 2135.01s, 	Step: 3714, 	{'train/accuracy': 0.988378643989563, 'train/loss': 0.0402594655752182, 'train/mean_average_precision': 0.18233914361002074, 'validation/accuracy': 0.9854835271835327, 'validation/loss': 0.0498136468231678, 'validation/mean_average_precision': 0.1700115195174815, 'validation/num_examples': 43793, 'test/accuracy': 0.9845543503761292, 'test/loss': 0.05250402167439461, 'test/mean_average_precision': 0.16973306944287994, 'test/num_examples': 43793, 'score': 1218.5362486839294, 'total_duration': 2135.0145077705383, 'accumulated_submission_time': 1218.5362486839294, 'accumulated_eval_time': 916.2338988780975, 'accumulated_logging_time': 0.14468717575073242}
I0205 00:00:20.556246 140283530442496 logging_writer.py:48] [3714] accumulated_eval_time=916.233899, accumulated_logging_time=0.144687, accumulated_submission_time=1218.536249, global_step=3714, preemption_count=0, score=1218.536249, test/accuracy=0.984554, test/loss=0.052504, test/mean_average_precision=0.169733, test/num_examples=43793, total_duration=2135.014508, train/accuracy=0.988379, train/loss=0.040259, train/mean_average_precision=0.182339, validation/accuracy=0.985484, validation/loss=0.049814, validation/mean_average_precision=0.170012, validation/num_examples=43793
I0205 00:00:48.446010 140283706132224 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.053469590842723846, loss=0.04066072404384613
I0205 00:01:20.956979 140283530442496 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.0772903636097908, loss=0.04343610629439354
I0205 00:01:53.169578 140283706132224 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.0603700689971447, loss=0.04397764801979065
I0205 00:02:25.362311 140283530442496 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.0590389221906662, loss=0.03931832313537598
I0205 00:02:57.397996 140283706132224 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.11085990816354752, loss=0.04225066676735878
I0205 00:03:29.800939 140283530442496 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.04774928838014603, loss=0.04216022416949272
I0205 00:04:02.294478 140283706132224 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.04909496009349823, loss=0.04222875460982323
I0205 00:04:20.837912 140451058161472 spec.py:321] Evaluating on the training split.
I0205 00:06:24.485860 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 00:06:27.523094 140451058161472 spec.py:349] Evaluating on the test split.
I0205 00:06:30.534213 140451058161472 submission_runner.py:408] Time since start: 2505.01s, 	Step: 4458, 	{'train/accuracy': 0.9884053468704224, 'train/loss': 0.03992285206913948, 'train/mean_average_precision': 0.21001074872560668, 'validation/accuracy': 0.9856386184692383, 'validation/loss': 0.04938645660877228, 'validation/mean_average_precision': 0.18339889496720987, 'validation/num_examples': 43793, 'test/accuracy': 0.9847325086593628, 'test/loss': 0.05199676752090454, 'test/mean_average_precision': 0.18730380392712498, 'test/num_examples': 43793, 'score': 1458.7867727279663, 'total_duration': 2505.007899045944, 'accumulated_submission_time': 1458.7867727279663, 'accumulated_eval_time': 1045.9301965236664, 'accumulated_logging_time': 0.1723625659942627}
I0205 00:06:30.549870 140290188154624 logging_writer.py:48] [4458] accumulated_eval_time=1045.930197, accumulated_logging_time=0.172363, accumulated_submission_time=1458.786773, global_step=4458, preemption_count=0, score=1458.786773, test/accuracy=0.984733, test/loss=0.051997, test/mean_average_precision=0.187304, test/num_examples=43793, total_duration=2505.007899, train/accuracy=0.988405, train/loss=0.039923, train/mean_average_precision=0.210011, validation/accuracy=0.985639, validation/loss=0.049386, validation/mean_average_precision=0.183399, validation/num_examples=43793
I0205 00:06:44.364107 140388966844160 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.03436970338225365, loss=0.037279289215803146
I0205 00:07:16.629530 140290188154624 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.0453554131090641, loss=0.042503077536821365
I0205 00:07:48.633193 140388966844160 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.10628274083137512, loss=0.049495451152324677
I0205 00:08:21.586085 140290188154624 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.049862828105688095, loss=0.03776244446635246
I0205 00:08:53.856568 140388966844160 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.046844787895679474, loss=0.04222739860415459
I0205 00:09:27.104894 140290188154624 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.02569645829498768, loss=0.03891989588737488
I0205 00:10:00.210801 140388966844160 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.05682673677802086, loss=0.039586786180734634
I0205 00:10:30.571576 140451058161472 spec.py:321] Evaluating on the training split.
I0205 00:12:35.029414 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 00:12:38.094888 140451058161472 spec.py:349] Evaluating on the test split.
I0205 00:12:41.197924 140451058161472 submission_runner.py:408] Time since start: 2875.67s, 	Step: 5192, 	{'train/accuracy': 0.9888152480125427, 'train/loss': 0.03871508315205574, 'train/mean_average_precision': 0.2292613295651795, 'validation/accuracy': 0.9857624173164368, 'validation/loss': 0.048279769718647, 'validation/mean_average_precision': 0.1916392368220283, 'validation/num_examples': 43793, 'test/accuracy': 0.9848563075065613, 'test/loss': 0.05093342438340187, 'test/mean_average_precision': 0.19238571078258276, 'test/num_examples': 43793, 'score': 1698.7766785621643, 'total_duration': 2875.6716067790985, 'accumulated_submission_time': 1698.7766785621643, 'accumulated_eval_time': 1176.5565106868744, 'accumulated_logging_time': 0.1991899013519287}
I0205 00:12:41.215279 140283731310336 logging_writer.py:48] [5192] accumulated_eval_time=1176.556511, accumulated_logging_time=0.199190, accumulated_submission_time=1698.776679, global_step=5192, preemption_count=0, score=1698.776679, test/accuracy=0.984856, test/loss=0.050933, test/mean_average_precision=0.192386, test/num_examples=43793, total_duration=2875.671607, train/accuracy=0.988815, train/loss=0.038715, train/mean_average_precision=0.229261, validation/accuracy=0.985762, validation/loss=0.048280, validation/mean_average_precision=0.191639, validation/num_examples=43793
I0205 00:12:44.178506 140388975236864 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.03734681010246277, loss=0.040969423949718475
I0205 00:13:16.894223 140283731310336 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.06738582998514175, loss=0.04075546935200691
I0205 00:13:49.233619 140388975236864 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.06993628293275833, loss=0.04204820469021797
I0205 00:14:21.890483 140283731310336 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.0795578807592392, loss=0.04288097843527794
I0205 00:14:54.700427 140388975236864 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.06160643324255943, loss=0.040509823709726334
I0205 00:15:27.310731 140283731310336 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.052543699741363525, loss=0.042796313762664795
I0205 00:15:59.830636 140388975236864 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.03938698396086693, loss=0.039183683693408966
I0205 00:16:32.298910 140283731310336 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.039926644414663315, loss=0.039424579590559006
I0205 00:16:41.398512 140451058161472 spec.py:321] Evaluating on the training split.
I0205 00:18:42.685981 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 00:18:45.819328 140451058161472 spec.py:349] Evaluating on the test split.
I0205 00:18:48.842261 140451058161472 submission_runner.py:408] Time since start: 3243.32s, 	Step: 5929, 	{'train/accuracy': 0.9890048503875732, 'train/loss': 0.03766616806387901, 'train/mean_average_precision': 0.24921226230222537, 'validation/accuracy': 0.9859474897384644, 'validation/loss': 0.0476326122879982, 'validation/mean_average_precision': 0.20997313712472634, 'validation/num_examples': 43793, 'test/accuracy': 0.9850403666496277, 'test/loss': 0.05034620687365532, 'test/mean_average_precision': 0.20815691664234603, 'test/num_examples': 43793, 'score': 1938.92862033844, 'total_duration': 3243.3159506320953, 'accumulated_submission_time': 1938.92862033844, 'accumulated_eval_time': 1304.0002155303955, 'accumulated_logging_time': 0.22843122482299805}
I0205 00:18:48.858481 140290179761920 logging_writer.py:48] [5929] accumulated_eval_time=1304.000216, accumulated_logging_time=0.228431, accumulated_submission_time=1938.928620, global_step=5929, preemption_count=0, score=1938.928620, test/accuracy=0.985040, test/loss=0.050346, test/mean_average_precision=0.208157, test/num_examples=43793, total_duration=3243.315951, train/accuracy=0.989005, train/loss=0.037666, train/mean_average_precision=0.249212, validation/accuracy=0.985947, validation/loss=0.047633, validation/mean_average_precision=0.209973, validation/num_examples=43793
I0205 00:19:12.286453 140290188154624 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.046980950981378555, loss=0.038837119936943054
I0205 00:19:44.713667 140290179761920 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.06188108026981354, loss=0.041080720722675323
I0205 00:20:17.594338 140290188154624 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.05451570823788643, loss=0.04168801009654999
I0205 00:20:49.985800 140290179761920 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.03294317051768303, loss=0.03987417370080948
I0205 00:21:22.272573 140290188154624 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.06481031328439713, loss=0.038348473608493805
I0205 00:21:54.296357 140290179761920 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.033674124628305435, loss=0.04140754044055939
I0205 00:22:26.507440 140290188154624 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.12027325481176376, loss=0.04141954332590103
I0205 00:22:49.061488 140451058161472 spec.py:321] Evaluating on the training split.
I0205 00:24:50.839696 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 00:24:53.913367 140451058161472 spec.py:349] Evaluating on the test split.
I0205 00:24:56.867177 140451058161472 submission_runner.py:408] Time since start: 3611.34s, 	Step: 6671, 	{'train/accuracy': 0.9889360070228577, 'train/loss': 0.03777961805462837, 'train/mean_average_precision': 0.23797969611058023, 'validation/accuracy': 0.9859402179718018, 'validation/loss': 0.04762043431401253, 'validation/mean_average_precision': 0.20814223137437274, 'validation/num_examples': 43793, 'test/accuracy': 0.9851086139678955, 'test/loss': 0.0502110980451107, 'test/mean_average_precision': 0.20542590378598344, 'test/num_examples': 43793, 'score': 2179.0981862545013, 'total_duration': 3611.3408505916595, 'accumulated_submission_time': 2179.0981862545013, 'accumulated_eval_time': 1431.8058450222015, 'accumulated_logging_time': 0.2576940059661865}
I0205 00:24:56.882708 140388966844160 logging_writer.py:48] [6671] accumulated_eval_time=1431.805845, accumulated_logging_time=0.257694, accumulated_submission_time=2179.098186, global_step=6671, preemption_count=0, score=2179.098186, test/accuracy=0.985109, test/loss=0.050211, test/mean_average_precision=0.205426, test/num_examples=43793, total_duration=3611.340851, train/accuracy=0.988936, train/loss=0.037780, train/mean_average_precision=0.237980, validation/accuracy=0.985940, validation/loss=0.047620, validation/mean_average_precision=0.208142, validation/num_examples=43793
I0205 00:25:06.530052 140388975236864 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.034691061824560165, loss=0.03737270459532738
I0205 00:25:38.261529 140388966844160 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.02251572161912918, loss=0.03778139874339104
I0205 00:26:09.601728 140388975236864 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.02611994557082653, loss=0.04109303653240204
I0205 00:26:40.722290 140388966844160 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.030386731028556824, loss=0.039124950766563416
I0205 00:27:12.202003 140388975236864 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.030894337221980095, loss=0.04101058468222618
I0205 00:27:43.649065 140388966844160 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.04155469685792923, loss=0.04050669074058533
I0205 00:28:15.171884 140388975236864 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.027826528996229172, loss=0.04381310194730759
I0205 00:28:46.410573 140388966844160 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.04715244472026825, loss=0.04090278595685959
I0205 00:28:57.109968 140451058161472 spec.py:321] Evaluating on the training split.
I0205 00:31:00.945241 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 00:31:04.125247 140451058161472 spec.py:349] Evaluating on the test split.
I0205 00:31:07.045913 140451058161472 submission_runner.py:408] Time since start: 3981.52s, 	Step: 7435, 	{'train/accuracy': 0.9889921545982361, 'train/loss': 0.03744671493768692, 'train/mean_average_precision': 0.25876130041910667, 'validation/accuracy': 0.9860936403274536, 'validation/loss': 0.04684552922844887, 'validation/mean_average_precision': 0.2130963062876768, 'validation/num_examples': 43793, 'test/accuracy': 0.9852202534675598, 'test/loss': 0.049300067126750946, 'test/mean_average_precision': 0.21027840907231676, 'test/num_examples': 43793, 'score': 2419.294604063034, 'total_duration': 3981.5195965766907, 'accumulated_submission_time': 2419.294604063034, 'accumulated_eval_time': 1561.7417376041412, 'accumulated_logging_time': 0.28409743309020996}
I0205 00:31:07.062726 140283731310336 logging_writer.py:48] [7435] accumulated_eval_time=1561.741738, accumulated_logging_time=0.284097, accumulated_submission_time=2419.294604, global_step=7435, preemption_count=0, score=2419.294604, test/accuracy=0.985220, test/loss=0.049300, test/mean_average_precision=0.210278, test/num_examples=43793, total_duration=3981.519597, train/accuracy=0.988992, train/loss=0.037447, train/mean_average_precision=0.258761, validation/accuracy=0.986094, validation/loss=0.046846, validation/mean_average_precision=0.213096, validation/num_examples=43793
I0205 00:31:27.918189 140290188154624 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.022272499278187752, loss=0.03943812847137451
I0205 00:31:59.448086 140283731310336 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.04427028074860573, loss=0.04154033958911896
I0205 00:32:30.918506 140290188154624 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.03649139776825905, loss=0.04132434353232384
I0205 00:33:01.780576 140283731310336 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.029278071597218513, loss=0.040336839854717255
I0205 00:33:33.472739 140290188154624 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.04030269756913185, loss=0.03716561198234558
I0205 00:34:05.138453 140283731310336 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.020707450807094574, loss=0.037089575082063675
I0205 00:34:36.236940 140290188154624 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.028944121673703194, loss=0.042165957391262054
I0205 00:35:07.341648 140451058161472 spec.py:321] Evaluating on the training split.
I0205 00:37:09.370839 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 00:37:12.371446 140451058161472 spec.py:349] Evaluating on the test split.
I0205 00:37:15.295653 140451058161472 submission_runner.py:408] Time since start: 4349.77s, 	Step: 8200, 	{'train/accuracy': 0.9891512393951416, 'train/loss': 0.03659132122993469, 'train/mean_average_precision': 0.26106851781320495, 'validation/accuracy': 0.9860554933547974, 'validation/loss': 0.04685352370142937, 'validation/mean_average_precision': 0.2194025693009693, 'validation/num_examples': 43793, 'test/accuracy': 0.9851284027099609, 'test/loss': 0.04958391934633255, 'test/mean_average_precision': 0.21861140761688885, 'test/num_examples': 43793, 'score': 2659.541307926178, 'total_duration': 4349.7693428993225, 'accumulated_submission_time': 2659.541307926178, 'accumulated_eval_time': 1689.6956989765167, 'accumulated_logging_time': 0.3128774166107178}
I0205 00:37:15.315098 140290179761920 logging_writer.py:48] [8200] accumulated_eval_time=1689.695699, accumulated_logging_time=0.312877, accumulated_submission_time=2659.541308, global_step=8200, preemption_count=0, score=2659.541308, test/accuracy=0.985128, test/loss=0.049584, test/mean_average_precision=0.218611, test/num_examples=43793, total_duration=4349.769343, train/accuracy=0.989151, train/loss=0.036591, train/mean_average_precision=0.261069, validation/accuracy=0.986055, validation/loss=0.046854, validation/mean_average_precision=0.219403, validation/num_examples=43793
I0205 00:37:15.649570 140388966844160 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.029113641008734703, loss=0.03920082375407219
I0205 00:37:47.449678 140290179761920 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.023435581475496292, loss=0.037064824253320694
I0205 00:38:19.157585 140388966844160 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.022859947755932808, loss=0.03843768686056137
I0205 00:38:50.987974 140290179761920 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.02241424098610878, loss=0.03927794098854065
I0205 00:39:22.634990 140388966844160 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.025479553267359734, loss=0.036575112491846085
I0205 00:39:53.968395 140290179761920 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.024763986468315125, loss=0.038926344364881516
I0205 00:40:26.675073 140388966844160 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.01934800110757351, loss=0.03877055272459984
I0205 00:40:59.082273 140290179761920 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.023141052573919296, loss=0.04266683757305145
I0205 00:41:15.368976 140451058161472 spec.py:321] Evaluating on the training split.
I0205 00:43:18.212320 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 00:43:21.225128 140451058161472 spec.py:349] Evaluating on the test split.
I0205 00:43:24.157281 140451058161472 submission_runner.py:408] Time since start: 4718.63s, 	Step: 8952, 	{'train/accuracy': 0.9891273975372314, 'train/loss': 0.03664684295654297, 'train/mean_average_precision': 0.27911021454146734, 'validation/accuracy': 0.9861025810241699, 'validation/loss': 0.04621446877717972, 'validation/mean_average_precision': 0.22299958263479633, 'validation/num_examples': 43793, 'test/accuracy': 0.9851734638214111, 'test/loss': 0.048995938152074814, 'test/mean_average_precision': 0.22708903725360496, 'test/num_examples': 43793, 'score': 2899.5621926784515, 'total_duration': 4718.630972146988, 'accumulated_submission_time': 2899.5621926784515, 'accumulated_eval_time': 1818.4839661121368, 'accumulated_logging_time': 0.34474682807922363}
I0205 00:43:24.173685 140283731310336 logging_writer.py:48] [8952] accumulated_eval_time=1818.483966, accumulated_logging_time=0.344747, accumulated_submission_time=2899.562193, global_step=8952, preemption_count=0, score=2899.562193, test/accuracy=0.985173, test/loss=0.048996, test/mean_average_precision=0.227089, test/num_examples=43793, total_duration=4718.630972, train/accuracy=0.989127, train/loss=0.036647, train/mean_average_precision=0.279110, validation/accuracy=0.986103, validation/loss=0.046214, validation/mean_average_precision=0.223000, validation/num_examples=43793
I0205 00:43:39.461983 140388975236864 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.032405514270067215, loss=0.04136744141578674
I0205 00:44:10.466296 140283731310336 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.03276877477765083, loss=0.038375139236450195
I0205 00:44:41.592679 140388975236864 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.025958189740777016, loss=0.0357966274023056
I0205 00:45:13.250576 140283731310336 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.022824307903647423, loss=0.038895025849342346
I0205 00:45:45.599589 140388975236864 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.04191607981920242, loss=0.03941328078508377
I0205 00:46:18.228320 140283731310336 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.01960393227636814, loss=0.037712663412094116
I0205 00:46:50.177253 140388975236864 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.026229312643408775, loss=0.041768867522478104
I0205 00:47:22.545270 140283731310336 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.04241500794887543, loss=0.037701744586229324
I0205 00:47:24.160617 140451058161472 spec.py:321] Evaluating on the training split.
I0205 00:49:27.993813 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 00:49:31.033670 140451058161472 spec.py:349] Evaluating on the test split.
I0205 00:49:34.054468 140451058161472 submission_runner.py:408] Time since start: 5088.53s, 	Step: 9706, 	{'train/accuracy': 0.9896572232246399, 'train/loss': 0.03524595499038696, 'train/mean_average_precision': 0.2876529415246712, 'validation/accuracy': 0.9862491488456726, 'validation/loss': 0.04611955210566521, 'validation/mean_average_precision': 0.2190752261508913, 'validation/num_examples': 43793, 'test/accuracy': 0.9853731393814087, 'test/loss': 0.048670776188373566, 'test/mean_average_precision': 0.22655205116409696, 'test/num_examples': 43793, 'score': 3139.5162563323975, 'total_duration': 5088.528156280518, 'accumulated_submission_time': 3139.5162563323975, 'accumulated_eval_time': 1948.3777787685394, 'accumulated_logging_time': 0.3719336986541748}
I0205 00:49:34.072797 140290188154624 logging_writer.py:48] [9706] accumulated_eval_time=1948.377779, accumulated_logging_time=0.371934, accumulated_submission_time=3139.516256, global_step=9706, preemption_count=0, score=3139.516256, test/accuracy=0.985373, test/loss=0.048671, test/mean_average_precision=0.226552, test/num_examples=43793, total_duration=5088.528156, train/accuracy=0.989657, train/loss=0.035246, train/mean_average_precision=0.287653, validation/accuracy=0.986249, validation/loss=0.046120, validation/mean_average_precision=0.219075, validation/num_examples=43793
I0205 00:50:04.493171 140388966844160 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.030849983915686607, loss=0.03806646913290024
I0205 00:50:35.798503 140290188154624 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.035649996250867844, loss=0.040881648659706116
I0205 00:51:07.520096 140388966844160 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.023316191509366035, loss=0.03719159960746765
I0205 00:51:39.050796 140290188154624 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.02375379391014576, loss=0.03463267162442207
I0205 00:52:10.394793 140388966844160 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.021373005583882332, loss=0.035805411636829376
I0205 00:52:41.667936 140290188154624 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.0247582346200943, loss=0.038538455963134766
I0205 00:53:13.367382 140388966844160 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.0296724122017622, loss=0.036037564277648926
I0205 00:53:34.108055 140451058161472 spec.py:321] Evaluating on the training split.
I0205 00:55:34.171913 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 00:55:37.180796 140451058161472 spec.py:349] Evaluating on the test split.
I0205 00:55:40.142534 140451058161472 submission_runner.py:408] Time since start: 5454.62s, 	Step: 10468, 	{'train/accuracy': 0.9896705150604248, 'train/loss': 0.03482377529144287, 'train/mean_average_precision': 0.3068134647972326, 'validation/accuracy': 0.9862880706787109, 'validation/loss': 0.04639178141951561, 'validation/mean_average_precision': 0.24119614102253042, 'validation/num_examples': 43793, 'test/accuracy': 0.9854143857955933, 'test/loss': 0.04908290505409241, 'test/mean_average_precision': 0.2329932907320756, 'test/num_examples': 43793, 'score': 3379.5207164287567, 'total_duration': 5454.616222381592, 'accumulated_submission_time': 3379.5207164287567, 'accumulated_eval_time': 2074.4122228622437, 'accumulated_logging_time': 0.4013481140136719}
I0205 00:55:40.159774 140290179761920 logging_writer.py:48] [10468] accumulated_eval_time=2074.412223, accumulated_logging_time=0.401348, accumulated_submission_time=3379.520716, global_step=10468, preemption_count=0, score=3379.520716, test/accuracy=0.985414, test/loss=0.049083, test/mean_average_precision=0.232993, test/num_examples=43793, total_duration=5454.616222, train/accuracy=0.989671, train/loss=0.034824, train/mean_average_precision=0.306813, validation/accuracy=0.986288, validation/loss=0.046392, validation/mean_average_precision=0.241196, validation/num_examples=43793
I0205 00:55:50.595891 140388975236864 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.03207116574048996, loss=0.03834306448698044
I0205 00:56:22.378733 140290179761920 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.022290604189038277, loss=0.034742023795843124
I0205 00:56:53.778994 140388975236864 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.03502222150564194, loss=0.03838934749364853
I0205 00:57:25.765921 140290179761920 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.02601359225809574, loss=0.03661157190799713
I0205 00:57:57.232855 140388975236864 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.04577341675758362, loss=0.039725445210933685
I0205 00:58:28.782738 140290179761920 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.024538863450288773, loss=0.03825864940881729
I0205 00:59:00.550150 140388975236864 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.02814476564526558, loss=0.03850732743740082
I0205 00:59:32.497735 140290179761920 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.0241988692432642, loss=0.03973856940865517
I0205 00:59:40.408746 140451058161472 spec.py:321] Evaluating on the training split.
I0205 01:01:42.442731 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 01:01:45.444087 140451058161472 spec.py:349] Evaluating on the test split.
I0205 01:01:48.434236 140451058161472 submission_runner.py:408] Time since start: 5822.91s, 	Step: 11226, 	{'train/accuracy': 0.9897632002830505, 'train/loss': 0.03422287851572037, 'train/mean_average_precision': 0.3330268928440666, 'validation/accuracy': 0.9864967465400696, 'validation/loss': 0.04566303268074989, 'validation/mean_average_precision': 0.2484119453619031, 'validation/num_examples': 43793, 'test/accuracy': 0.9855487942695618, 'test/loss': 0.048500243574380875, 'test/mean_average_precision': 0.23948637983980706, 'test/num_examples': 43793, 'score': 3619.7391617298126, 'total_duration': 5822.907917261124, 'accumulated_submission_time': 3619.7391617298126, 'accumulated_eval_time': 2202.437658548355, 'accumulated_logging_time': 0.4295799732208252}
I0205 01:01:48.453326 140283731310336 logging_writer.py:48] [11226] accumulated_eval_time=2202.437659, accumulated_logging_time=0.429580, accumulated_submission_time=3619.739162, global_step=11226, preemption_count=0, score=3619.739162, test/accuracy=0.985549, test/loss=0.048500, test/mean_average_precision=0.239486, test/num_examples=43793, total_duration=5822.907917, train/accuracy=0.989763, train/loss=0.034223, train/mean_average_precision=0.333027, validation/accuracy=0.986497, validation/loss=0.045663, validation/mean_average_precision=0.248412, validation/num_examples=43793
I0205 01:02:12.416630 140290188154624 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.027082158252596855, loss=0.03688550367951393
I0205 01:02:43.481702 140283731310336 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.04088107869029045, loss=0.036772776395082474
I0205 01:03:15.021078 140290188154624 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.028293361887335777, loss=0.03864525631070137
I0205 01:03:46.393471 140283731310336 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.03631538152694702, loss=0.03880777582526207
I0205 01:04:17.883150 140290188154624 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.034311387687921524, loss=0.037464749068021774
I0205 01:04:49.981630 140283731310336 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.0333847813308239, loss=0.03639797493815422
I0205 01:05:22.328794 140290188154624 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.03403162583708763, loss=0.03660736232995987
I0205 01:05:48.483945 140451058161472 spec.py:321] Evaluating on the training split.
I0205 01:07:53.526917 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 01:07:56.501613 140451058161472 spec.py:349] Evaluating on the test split.
I0205 01:07:59.479145 140451058161472 submission_runner.py:408] Time since start: 6193.95s, 	Step: 11982, 	{'train/accuracy': 0.9901514053344727, 'train/loss': 0.03276079148054123, 'train/mean_average_precision': 0.3584733182330887, 'validation/accuracy': 0.9865888953208923, 'validation/loss': 0.04483211785554886, 'validation/mean_average_precision': 0.25001468516502967, 'validation/num_examples': 43793, 'test/accuracy': 0.9855656027793884, 'test/loss': 0.0477055162191391, 'test/mean_average_precision': 0.23895538587981185, 'test/num_examples': 43793, 'score': 3859.737271785736, 'total_duration': 6193.9528086185455, 'accumulated_submission_time': 3859.737271785736, 'accumulated_eval_time': 2333.4328026771545, 'accumulated_logging_time': 0.4608142375946045}
I0205 01:07:59.497622 140290179761920 logging_writer.py:48] [11982] accumulated_eval_time=2333.432803, accumulated_logging_time=0.460814, accumulated_submission_time=3859.737272, global_step=11982, preemption_count=0, score=3859.737272, test/accuracy=0.985566, test/loss=0.047706, test/mean_average_precision=0.238955, test/num_examples=43793, total_duration=6193.952809, train/accuracy=0.990151, train/loss=0.032761, train/mean_average_precision=0.358473, validation/accuracy=0.986589, validation/loss=0.044832, validation/mean_average_precision=0.250015, validation/num_examples=43793
I0205 01:08:05.760714 140388966844160 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.031674548983573914, loss=0.036167070269584656
I0205 01:08:37.198184 140290179761920 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.029071707278490067, loss=0.0365801639854908
I0205 01:09:08.663565 140388966844160 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.037451282143592834, loss=0.03710862249135971
I0205 01:09:39.980365 140290179761920 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.035550665110349655, loss=0.03610367700457573
I0205 01:10:12.321855 140388966844160 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.04593636840581894, loss=0.03769868239760399
I0205 01:10:44.000441 140290179761920 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.051345162093639374, loss=0.03479112312197685
I0205 01:11:15.935315 140388966844160 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.028449608013033867, loss=0.038673605769872665
I0205 01:11:47.621655 140290179761920 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.02877158485352993, loss=0.038124263286590576
I0205 01:11:59.621232 140451058161472 spec.py:321] Evaluating on the training split.
I0205 01:14:05.203513 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 01:14:08.190678 140451058161472 spec.py:349] Evaluating on the test split.
I0205 01:14:11.177794 140451058161472 submission_runner.py:408] Time since start: 6565.65s, 	Step: 12739, 	{'train/accuracy': 0.9903623461723328, 'train/loss': 0.03217914327979088, 'train/mean_average_precision': 0.3770886565685112, 'validation/accuracy': 0.9865893125534058, 'validation/loss': 0.04484793171286583, 'validation/mean_average_precision': 0.2501933144468553, 'validation/num_examples': 43793, 'test/accuracy': 0.985632598400116, 'test/loss': 0.04763798788189888, 'test/mean_average_precision': 0.2430544044938929, 'test/num_examples': 43793, 'score': 4099.829087495804, 'total_duration': 6565.651482105255, 'accumulated_submission_time': 4099.829087495804, 'accumulated_eval_time': 2464.989315032959, 'accumulated_logging_time': 0.49089860916137695}
I0205 01:14:11.195716 140283731310336 logging_writer.py:48] [12739] accumulated_eval_time=2464.989315, accumulated_logging_time=0.490899, accumulated_submission_time=4099.829087, global_step=12739, preemption_count=0, score=4099.829087, test/accuracy=0.985633, test/loss=0.047638, test/mean_average_precision=0.243054, test/num_examples=43793, total_duration=6565.651482, train/accuracy=0.990362, train/loss=0.032179, train/mean_average_precision=0.377089, validation/accuracy=0.986589, validation/loss=0.044848, validation/mean_average_precision=0.250193, validation/num_examples=43793
I0205 01:14:30.892919 140388975236864 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.03941936045885086, loss=0.03726309537887573
I0205 01:15:02.659801 140283731310336 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.033027030527591705, loss=0.03552131727337837
I0205 01:15:34.325424 140388975236864 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.030140981078147888, loss=0.032457511872053146
I0205 01:16:06.337968 140283731310336 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.045717280358076096, loss=0.0361018106341362
I0205 01:16:38.098820 140388975236864 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.03579717501997948, loss=0.033011339604854584
I0205 01:17:10.196495 140283731310336 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.030198685824871063, loss=0.03448621928691864
I0205 01:17:41.868124 140388975236864 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.03195594251155853, loss=0.03440765291452408
I0205 01:18:11.385267 140451058161472 spec.py:321] Evaluating on the training split.
I0205 01:20:14.869070 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 01:20:17.895587 140451058161472 spec.py:349] Evaluating on the test split.
I0205 01:20:20.895848 140451058161472 submission_runner.py:408] Time since start: 6935.37s, 	Step: 13493, 	{'train/accuracy': 0.9904111623764038, 'train/loss': 0.03199373930692673, 'train/mean_average_precision': 0.3685707194392107, 'validation/accuracy': 0.9865913391113281, 'validation/loss': 0.045087773352861404, 'validation/mean_average_precision': 0.25362827987968756, 'validation/num_examples': 43793, 'test/accuracy': 0.985736608505249, 'test/loss': 0.04794413596391678, 'test/mean_average_precision': 0.24486259259822446, 'test/num_examples': 43793, 'score': 4339.98765039444, 'total_duration': 6935.369527339935, 'accumulated_submission_time': 4339.98765039444, 'accumulated_eval_time': 2594.4998412132263, 'accumulated_logging_time': 0.5199365615844727}
I0205 01:20:20.914022 140290188154624 logging_writer.py:48] [13493] accumulated_eval_time=2594.499841, accumulated_logging_time=0.519937, accumulated_submission_time=4339.987650, global_step=13493, preemption_count=0, score=4339.987650, test/accuracy=0.985737, test/loss=0.047944, test/mean_average_precision=0.244863, test/num_examples=43793, total_duration=6935.369527, train/accuracy=0.990411, train/loss=0.031994, train/mean_average_precision=0.368571, validation/accuracy=0.986591, validation/loss=0.045088, validation/mean_average_precision=0.253628, validation/num_examples=43793
I0205 01:20:23.740957 140388966844160 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.04187304154038429, loss=0.035362835973501205
I0205 01:20:55.495002 140290188154624 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.034471478313207626, loss=0.036549605429172516
I0205 01:21:27.220546 140388966844160 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.032644689083099365, loss=0.035382650792598724
I0205 01:21:58.556215 140290188154624 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.0421890951693058, loss=0.032827191054821014
I0205 01:22:30.730702 140388966844160 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.04827038571238518, loss=0.03647526353597641
I0205 01:23:02.431240 140290188154624 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.03606412932276726, loss=0.0356697253882885
I0205 01:23:34.070991 140388966844160 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.03605698049068451, loss=0.03768389672040939
I0205 01:24:05.750101 140290188154624 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.04165775328874588, loss=0.03607538715004921
I0205 01:24:21.034129 140451058161472 spec.py:321] Evaluating on the training split.
I0205 01:26:24.890520 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 01:26:27.922539 140451058161472 spec.py:349] Evaluating on the test split.
I0205 01:26:30.882382 140451058161472 submission_runner.py:408] Time since start: 7305.36s, 	Step: 14249, 	{'train/accuracy': 0.9904477000236511, 'train/loss': 0.03179260343313217, 'train/mean_average_precision': 0.3795072526894285, 'validation/accuracy': 0.9866713285446167, 'validation/loss': 0.04471726343035698, 'validation/mean_average_precision': 0.25447894472058963, 'validation/num_examples': 43793, 'test/accuracy': 0.9857930541038513, 'test/loss': 0.04738032817840576, 'test/mean_average_precision': 0.24344730275635112, 'test/num_examples': 43793, 'score': 4580.077013969421, 'total_duration': 7305.356061458588, 'accumulated_submission_time': 4580.077013969421, 'accumulated_eval_time': 2724.3480422496796, 'accumulated_logging_time': 0.5486664772033691}
I0205 01:26:30.900988 140283731310336 logging_writer.py:48] [14249] accumulated_eval_time=2724.348042, accumulated_logging_time=0.548666, accumulated_submission_time=4580.077014, global_step=14249, preemption_count=0, score=4580.077014, test/accuracy=0.985793, test/loss=0.047380, test/mean_average_precision=0.243447, test/num_examples=43793, total_duration=7305.356061, train/accuracy=0.990448, train/loss=0.031793, train/mean_average_precision=0.379507, validation/accuracy=0.986671, validation/loss=0.044717, validation/mean_average_precision=0.254479, validation/num_examples=43793
I0205 01:26:47.327684 140388975236864 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.042247477918863297, loss=0.03664574772119522
I0205 01:27:19.137336 140283731310336 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.0485149584710598, loss=0.03701157867908478
I0205 01:27:51.108239 140388975236864 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.04839500039815903, loss=0.03377021849155426
I0205 01:28:22.940359 140283731310336 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.05142040178179741, loss=0.038120195269584656
I0205 01:28:54.852877 140388975236864 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.059999123215675354, loss=0.036509111523628235
I0205 01:29:26.691057 140283731310336 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.06606277078390121, loss=0.03273104503750801
I0205 01:29:58.259859 140388975236864 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.04513992369174957, loss=0.03475707396864891
I0205 01:30:30.138968 140283731310336 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.06140277162194252, loss=0.035359278321266174
I0205 01:30:31.058460 140451058161472 spec.py:321] Evaluating on the training split.
I0205 01:32:35.662508 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 01:32:38.698264 140451058161472 spec.py:349] Evaluating on the test split.
I0205 01:32:41.739606 140451058161472 submission_runner.py:408] Time since start: 7676.21s, 	Step: 15004, 	{'train/accuracy': 0.9902809858322144, 'train/loss': 0.032261211425065994, 'train/mean_average_precision': 0.3739686437256189, 'validation/accuracy': 0.9866579174995422, 'validation/loss': 0.04483366012573242, 'validation/mean_average_precision': 0.2554795498068286, 'validation/num_examples': 43793, 'test/accuracy': 0.9858074188232422, 'test/loss': 0.047604430466890335, 'test/mean_average_precision': 0.24544980175739897, 'test/num_examples': 43793, 'score': 4820.203650474548, 'total_duration': 7676.213287353516, 'accumulated_submission_time': 4820.203650474548, 'accumulated_eval_time': 2855.0291335582733, 'accumulated_logging_time': 0.5781416893005371}
I0205 01:32:41.757244 140290188154624 logging_writer.py:48] [15004] accumulated_eval_time=2855.029134, accumulated_logging_time=0.578142, accumulated_submission_time=4820.203650, global_step=15004, preemption_count=0, score=4820.203650, test/accuracy=0.985807, test/loss=0.047604, test/mean_average_precision=0.245450, test/num_examples=43793, total_duration=7676.213287, train/accuracy=0.990281, train/loss=0.032261, train/mean_average_precision=0.373969, validation/accuracy=0.986658, validation/loss=0.044834, validation/mean_average_precision=0.255480, validation/num_examples=43793
I0205 01:33:13.011384 140388966844160 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.04502413421869278, loss=0.03352026641368866
I0205 01:33:44.625871 140290188154624 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.043407753109931946, loss=0.035907965153455734
I0205 01:34:16.374958 140388966844160 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.04433697834610939, loss=0.03567745164036751
I0205 01:34:47.984207 140290188154624 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.04579557478427887, loss=0.03438732028007507
I0205 01:35:20.083684 140388966844160 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.042788777500391006, loss=0.036026731133461
I0205 01:35:52.080152 140290188154624 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.059710998088121414, loss=0.03641318157315254
I0205 01:36:24.210008 140388966844160 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.047633346170186996, loss=0.03523736074566841
I0205 01:36:41.924539 140451058161472 spec.py:321] Evaluating on the training split.
I0205 01:38:52.169537 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 01:38:55.577069 140451058161472 spec.py:349] Evaluating on the test split.
I0205 01:38:58.519770 140451058161472 submission_runner.py:408] Time since start: 8052.99s, 	Step: 15757, 	{'train/accuracy': 0.9904695749282837, 'train/loss': 0.03158517926931381, 'train/mean_average_precision': 0.37808878658834266, 'validation/accuracy': 0.9866855144500732, 'validation/loss': 0.044472336769104004, 'validation/mean_average_precision': 0.26377274406735646, 'validation/num_examples': 43793, 'test/accuracy': 0.98580402135849, 'test/loss': 0.047192614525556564, 'test/mean_average_precision': 0.2571249985640839, 'test/num_examples': 43793, 'score': 5060.340332508087, 'total_duration': 8052.993452072144, 'accumulated_submission_time': 5060.340332508087, 'accumulated_eval_time': 2991.6243121623993, 'accumulated_logging_time': 0.6067461967468262}
I0205 01:38:58.537700 140290179761920 logging_writer.py:48] [15757] accumulated_eval_time=2991.624312, accumulated_logging_time=0.606746, accumulated_submission_time=5060.340333, global_step=15757, preemption_count=0, score=5060.340333, test/accuracy=0.985804, test/loss=0.047193, test/mean_average_precision=0.257125, test/num_examples=43793, total_duration=8052.993452, train/accuracy=0.990470, train/loss=0.031585, train/mean_average_precision=0.378089, validation/accuracy=0.986686, validation/loss=0.044472, validation/mean_average_precision=0.263773, validation/num_examples=43793
I0205 01:39:12.723306 140388975236864 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.053671859204769135, loss=0.0360134094953537
I0205 01:39:44.684774 140290179761920 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.045829400420188904, loss=0.037073343992233276
I0205 01:40:16.887045 140388975236864 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.04472852870821953, loss=0.03385327756404877
I0205 01:40:48.756086 140290179761920 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.03829414024949074, loss=0.034463029354810715
I0205 01:41:20.562150 140388975236864 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.06795413792133331, loss=0.035741858184337616
I0205 01:41:52.241768 140290179761920 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.062385812401771545, loss=0.037334222346544266
I0205 01:42:24.069448 140388975236864 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.050453174859285355, loss=0.03591259568929672
I0205 01:42:55.772605 140290179761920 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.07720481604337692, loss=0.0380430668592453
I0205 01:42:58.625619 140451058161472 spec.py:321] Evaluating on the training split.
I0205 01:45:07.355967 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 01:45:10.394398 140451058161472 spec.py:349] Evaluating on the test split.
I0205 01:45:13.388557 140451058161472 submission_runner.py:408] Time since start: 8427.86s, 	Step: 16510, 	{'train/accuracy': 0.9904604554176331, 'train/loss': 0.031649067997932434, 'train/mean_average_precision': 0.3682752869847712, 'validation/accuracy': 0.9867634773254395, 'validation/loss': 0.044304266571998596, 'validation/mean_average_precision': 0.26255079006164844, 'validation/num_examples': 43793, 'test/accuracy': 0.9858672022819519, 'test/loss': 0.047187402844429016, 'test/mean_average_precision': 0.2516207197326865, 'test/num_examples': 43793, 'score': 5300.3963787555695, 'total_duration': 8427.862237930298, 'accumulated_submission_time': 5300.3963787555695, 'accumulated_eval_time': 3126.3872005939484, 'accumulated_logging_time': 0.6358902454376221}
I0205 01:45:13.408547 140283731310336 logging_writer.py:48] [16510] accumulated_eval_time=3126.387201, accumulated_logging_time=0.635890, accumulated_submission_time=5300.396379, global_step=16510, preemption_count=0, score=5300.396379, test/accuracy=0.985867, test/loss=0.047187, test/mean_average_precision=0.251621, test/num_examples=43793, total_duration=8427.862238, train/accuracy=0.990460, train/loss=0.031649, train/mean_average_precision=0.368275, validation/accuracy=0.986763, validation/loss=0.044304, validation/mean_average_precision=0.262551, validation/num_examples=43793
I0205 01:45:43.190231 140290188154624 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.05411357060074806, loss=0.03763354569673538
I0205 01:46:15.955080 140283731310336 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.05588987469673157, loss=0.03712166100740433
I0205 01:46:48.334976 140290188154624 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.07847563922405243, loss=0.034967463463544846
I0205 01:47:20.501371 140283731310336 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.06015618145465851, loss=0.03363910689949989
I0205 01:47:52.212193 140290188154624 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.06079139932990074, loss=0.036332666873931885
I0205 01:48:24.342909 140283731310336 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.07865645736455917, loss=0.034770626574754715
I0205 01:48:56.123838 140290188154624 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.05234165117144585, loss=0.03516687825322151
I0205 01:49:13.592641 140451058161472 spec.py:321] Evaluating on the training split.
I0205 01:51:15.155377 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 01:51:18.207608 140451058161472 spec.py:349] Evaluating on the test split.
I0205 01:51:21.194977 140451058161472 submission_runner.py:408] Time since start: 8795.67s, 	Step: 17255, 	{'train/accuracy': 0.9904911518096924, 'train/loss': 0.03133274242281914, 'train/mean_average_precision': 0.3850487983348396, 'validation/accuracy': 0.9867411255836487, 'validation/loss': 0.04437999799847603, 'validation/mean_average_precision': 0.2592182022289485, 'validation/num_examples': 43793, 'test/accuracy': 0.9859405159950256, 'test/loss': 0.047076329588890076, 'test/mean_average_precision': 0.2522397872860027, 'test/num_examples': 43793, 'score': 5540.545610666275, 'total_duration': 8795.66866350174, 'accumulated_submission_time': 5540.545610666275, 'accumulated_eval_time': 3253.9894936084747, 'accumulated_logging_time': 0.6687760353088379}
I0205 01:51:21.215390 140290179761920 logging_writer.py:48] [17255] accumulated_eval_time=3253.989494, accumulated_logging_time=0.668776, accumulated_submission_time=5540.545611, global_step=17255, preemption_count=0, score=5540.545611, test/accuracy=0.985941, test/loss=0.047076, test/mean_average_precision=0.252240, test/num_examples=43793, total_duration=8795.668664, train/accuracy=0.990491, train/loss=0.031333, train/mean_average_precision=0.385049, validation/accuracy=0.986741, validation/loss=0.044380, validation/mean_average_precision=0.259218, validation/num_examples=43793
I0205 01:51:36.210075 140388966844160 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.047234293073415756, loss=0.033653657883405685
I0205 01:52:08.007455 140290179761920 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.048665016889572144, loss=0.03354069963097572
I0205 01:52:39.433864 140388966844160 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.05845233425498009, loss=0.03595130145549774
I0205 01:53:11.283453 140290179761920 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.0829668641090393, loss=0.031531330198049545
I0205 01:53:43.105269 140388966844160 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.04401450604200363, loss=0.032319627702236176
I0205 01:54:14.899873 140290179761920 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.05067025125026703, loss=0.03400874510407448
I0205 01:54:46.560274 140388966844160 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.059663325548172, loss=0.0344276987016201
I0205 01:55:18.672532 140290179761920 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.054860517382621765, loss=0.03333093225955963
I0205 01:55:21.225333 140451058161472 spec.py:321] Evaluating on the training split.
I0205 01:57:28.917244 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 01:57:31.937960 140451058161472 spec.py:349] Evaluating on the test split.
I0205 01:57:34.886237 140451058161472 submission_runner.py:408] Time since start: 9169.36s, 	Step: 18009, 	{'train/accuracy': 0.9906929135322571, 'train/loss': 0.03085498698055744, 'train/mean_average_precision': 0.39717999532900616, 'validation/accuracy': 0.9866570830345154, 'validation/loss': 0.04496076703071594, 'validation/mean_average_precision': 0.2552737329943473, 'validation/num_examples': 43793, 'test/accuracy': 0.9858688712120056, 'test/loss': 0.04773515462875366, 'test/mean_average_precision': 0.24338506834360013, 'test/num_examples': 43793, 'score': 5780.521510839462, 'total_duration': 9169.35992527008, 'accumulated_submission_time': 5780.521510839462, 'accumulated_eval_time': 3387.6503591537476, 'accumulated_logging_time': 0.7024412155151367}
I0205 01:57:34.904995 140283731310336 logging_writer.py:48] [18009] accumulated_eval_time=3387.650359, accumulated_logging_time=0.702441, accumulated_submission_time=5780.521511, global_step=18009, preemption_count=0, score=5780.521511, test/accuracy=0.985869, test/loss=0.047735, test/mean_average_precision=0.243385, test/num_examples=43793, total_duration=9169.359925, train/accuracy=0.990693, train/loss=0.030855, train/mean_average_precision=0.397180, validation/accuracy=0.986657, validation/loss=0.044961, validation/mean_average_precision=0.255274, validation/num_examples=43793
I0205 01:58:04.604687 140290188154624 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.059314627200365067, loss=0.03905811160802841
I0205 01:58:36.554162 140283731310336 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.05180642753839493, loss=0.03261789679527283
I0205 01:59:08.954133 140290188154624 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.07254280149936676, loss=0.03513851389288902
I0205 01:59:40.680724 140283731310336 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.055210743099451065, loss=0.035659290850162506
I0205 02:00:12.663892 140290188154624 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.06760628521442413, loss=0.03471948578953743
I0205 02:00:44.408998 140283731310336 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.06414960324764252, loss=0.03484110161662102
I0205 02:01:16.478787 140290188154624 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.06964727491140366, loss=0.03657543286681175
I0205 02:01:34.928896 140451058161472 spec.py:321] Evaluating on the training split.
I0205 02:03:39.944516 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 02:03:42.977847 140451058161472 spec.py:349] Evaluating on the test split.
I0205 02:03:46.048181 140451058161472 submission_runner.py:408] Time since start: 9540.52s, 	Step: 18759, 	{'train/accuracy': 0.990839421749115, 'train/loss': 0.03034825250506401, 'train/mean_average_precision': 0.4093167248324422, 'validation/accuracy': 0.9866721034049988, 'validation/loss': 0.04450898617506027, 'validation/mean_average_precision': 0.2656968890905586, 'validation/num_examples': 43793, 'test/accuracy': 0.985846996307373, 'test/loss': 0.04725183919072151, 'test/mean_average_precision': 0.2473320580292265, 'test/num_examples': 43793, 'score': 6020.512645244598, 'total_duration': 9540.521861076355, 'accumulated_submission_time': 6020.512645244598, 'accumulated_eval_time': 3518.769593477249, 'accumulated_logging_time': 0.7336812019348145}
I0205 02:03:46.069736 140290179761920 logging_writer.py:48] [18759] accumulated_eval_time=3518.769593, accumulated_logging_time=0.733681, accumulated_submission_time=6020.512645, global_step=18759, preemption_count=0, score=6020.512645, test/accuracy=0.985847, test/loss=0.047252, test/mean_average_precision=0.247332, test/num_examples=43793, total_duration=9540.521861, train/accuracy=0.990839, train/loss=0.030348, train/mean_average_precision=0.409317, validation/accuracy=0.986672, validation/loss=0.044509, validation/mean_average_precision=0.265697, validation/num_examples=43793
I0205 02:03:59.434985 140388966844160 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.06225705146789551, loss=0.03154882788658142
I0205 02:04:31.295197 140290179761920 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.04961123317480087, loss=0.03536601737141609
I0205 02:05:03.130115 140388966844160 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.06244010105729103, loss=0.036952219903469086
I0205 02:05:34.885327 140290179761920 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.05741255730390549, loss=0.03017459623515606
I0205 02:06:06.810879 140388966844160 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.0594572089612484, loss=0.03539591655135155
I0205 02:06:38.532697 140290179761920 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.059496019035577774, loss=0.033453281968832016
I0205 02:07:10.575319 140388966844160 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.09476891160011292, loss=0.034088924527168274
I0205 02:07:41.985071 140290179761920 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.05832786485552788, loss=0.035078663378953934
I0205 02:07:46.089221 140451058161472 spec.py:321] Evaluating on the training split.
I0205 02:09:48.805323 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 02:09:51.837691 140451058161472 spec.py:349] Evaluating on the test split.
I0205 02:09:54.812935 140451058161472 submission_runner.py:408] Time since start: 9909.29s, 	Step: 19514, 	{'train/accuracy': 0.9907936453819275, 'train/loss': 0.030009375885128975, 'train/mean_average_precision': 0.42834513891776127, 'validation/accuracy': 0.9867143034934998, 'validation/loss': 0.04493936896324158, 'validation/mean_average_precision': 0.2623142473408286, 'validation/num_examples': 43793, 'test/accuracy': 0.9858187437057495, 'test/loss': 0.04760851338505745, 'test/mean_average_precision': 0.2507867971943465, 'test/num_examples': 43793, 'score': 6260.5007147789, 'total_duration': 9909.286611318588, 'accumulated_submission_time': 6260.5007147789, 'accumulated_eval_time': 3647.493248462677, 'accumulated_logging_time': 0.7661452293395996}
I0205 02:09:54.831785 140290188154624 logging_writer.py:48] [19514] accumulated_eval_time=3647.493248, accumulated_logging_time=0.766145, accumulated_submission_time=6260.500715, global_step=19514, preemption_count=0, score=6260.500715, test/accuracy=0.985819, test/loss=0.047609, test/mean_average_precision=0.250787, test/num_examples=43793, total_duration=9909.286611, train/accuracy=0.990794, train/loss=0.030009, train/mean_average_precision=0.428345, validation/accuracy=0.986714, validation/loss=0.044939, validation/mean_average_precision=0.262314, validation/num_examples=43793
I0205 02:10:22.357734 140388975236864 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.06186675280332565, loss=0.03563797473907471
I0205 02:10:53.913066 140290188154624 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.07498042285442352, loss=0.03722526878118515
I0205 02:11:25.635890 140388975236864 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.05758463963866234, loss=0.03353147581219673
I0205 02:11:57.481021 140290188154624 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.06107921153306961, loss=0.03416290134191513
I0205 02:12:29.169991 140388975236864 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.06669183075428009, loss=0.035559121519327164
I0205 02:13:00.693519 140290188154624 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.08147285878658295, loss=0.03573467582464218
I0205 02:13:32.688019 140388975236864 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.06415488570928574, loss=0.03719544783234596
I0205 02:13:55.005245 140451058161472 spec.py:321] Evaluating on the training split.
I0205 02:16:00.038379 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 02:16:03.310163 140451058161472 spec.py:349] Evaluating on the test split.
I0205 02:16:06.314271 140451058161472 submission_runner.py:408] Time since start: 10280.79s, 	Step: 20272, 	{'train/accuracy': 0.9910169243812561, 'train/loss': 0.029275966808199883, 'train/mean_average_precision': 0.43464146771933454, 'validation/accuracy': 0.9868003726005554, 'validation/loss': 0.04456842690706253, 'validation/mean_average_precision': 0.2706530486331538, 'validation/num_examples': 43793, 'test/accuracy': 0.9859198331832886, 'test/loss': 0.04731724411249161, 'test/mean_average_precision': 0.24841916065106492, 'test/num_examples': 43793, 'score': 6500.6417491436005, 'total_duration': 10280.787954807281, 'accumulated_submission_time': 6500.6417491436005, 'accumulated_eval_time': 3778.8022241592407, 'accumulated_logging_time': 0.7969620227813721}
I0205 02:16:06.334034 140283731310336 logging_writer.py:48] [20272] accumulated_eval_time=3778.802224, accumulated_logging_time=0.796962, accumulated_submission_time=6500.641749, global_step=20272, preemption_count=0, score=6500.641749, test/accuracy=0.985920, test/loss=0.047317, test/mean_average_precision=0.248419, test/num_examples=43793, total_duration=10280.787955, train/accuracy=0.991017, train/loss=0.029276, train/mean_average_precision=0.434641, validation/accuracy=0.986800, validation/loss=0.044568, validation/mean_average_precision=0.270653, validation/num_examples=43793
I0205 02:16:15.454018 140388966844160 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.08053258061408997, loss=0.033431995660066605
I0205 02:16:47.313163 140283731310336 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.0660354346036911, loss=0.032787274569272995
I0205 02:17:18.703226 140388966844160 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.07257827371358871, loss=0.03592855483293533
I0205 02:17:50.357484 140283731310336 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.07943595945835114, loss=0.03653246909379959
I0205 02:18:22.743435 140388966844160 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.11656121164560318, loss=0.03615979477763176
I0205 02:18:54.287706 140283731310336 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.06761182844638824, loss=0.035238515585660934
I0205 02:19:26.149477 140388966844160 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.060840655118227005, loss=0.03182381018996239
I0205 02:19:57.732285 140283731310336 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.07069084048271179, loss=0.041403621435165405
I0205 02:20:06.445490 140451058161472 spec.py:321] Evaluating on the training split.
I0205 02:22:08.222455 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 02:22:11.204214 140451058161472 spec.py:349] Evaluating on the test split.
I0205 02:22:14.160434 140451058161472 submission_runner.py:408] Time since start: 10648.63s, 	Step: 21028, 	{'train/accuracy': 0.9911913275718689, 'train/loss': 0.029066402465105057, 'train/mean_average_precision': 0.4319467944312771, 'validation/accuracy': 0.9865812063217163, 'validation/loss': 0.04467516019940376, 'validation/mean_average_precision': 0.26187619437584214, 'validation/num_examples': 43793, 'test/accuracy': 0.9858166575431824, 'test/loss': 0.04713984578847885, 'test/mean_average_precision': 0.2538452476150781, 'test/num_examples': 43793, 'score': 6740.721883535385, 'total_duration': 10648.634120225906, 'accumulated_submission_time': 6740.721883535385, 'accumulated_eval_time': 3906.517117500305, 'accumulated_logging_time': 0.8276827335357666}
I0205 02:22:14.179544 140290179761920 logging_writer.py:48] [21028] accumulated_eval_time=3906.517118, accumulated_logging_time=0.827683, accumulated_submission_time=6740.721884, global_step=21028, preemption_count=0, score=6740.721884, test/accuracy=0.985817, test/loss=0.047140, test/mean_average_precision=0.253845, test/num_examples=43793, total_duration=10648.634120, train/accuracy=0.991191, train/loss=0.029066, train/mean_average_precision=0.431947, validation/accuracy=0.986581, validation/loss=0.044675, validation/mean_average_precision=0.261876, validation/num_examples=43793
I0205 02:22:37.267872 140388975236864 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.08514216542243958, loss=0.036161139607429504
I0205 02:23:09.139651 140290179761920 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.09749502688646317, loss=0.033624645322561264
I0205 02:23:40.592744 140388975236864 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.07436078786849976, loss=0.033896785229444504
I0205 02:24:12.449530 140290179761920 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.06420852988958359, loss=0.03316628187894821
I0205 02:24:44.787699 140388975236864 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.06509911268949509, loss=0.03290359675884247
I0205 02:25:17.392856 140290179761920 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.0630071833729744, loss=0.03338543325662613
I0205 02:25:50.214581 140388975236864 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.08612926304340363, loss=0.03391542285680771
I0205 02:26:14.271179 140451058161472 spec.py:321] Evaluating on the training split.
I0205 02:28:20.800746 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 02:28:23.811229 140451058161472 spec.py:349] Evaluating on the test split.
I0205 02:28:26.801381 140451058161472 submission_runner.py:408] Time since start: 11021.28s, 	Step: 21774, 	{'train/accuracy': 0.991000235080719, 'train/loss': 0.02977318875491619, 'train/mean_average_precision': 0.4357048122346068, 'validation/accuracy': 0.9867650866508484, 'validation/loss': 0.044359687715768814, 'validation/mean_average_precision': 0.26043491822969284, 'validation/num_examples': 43793, 'test/accuracy': 0.9858962893486023, 'test/loss': 0.046945106238126755, 'test/mean_average_precision': 0.2553447340116264, 'test/num_examples': 43793, 'score': 6980.779272079468, 'total_duration': 11021.275067090988, 'accumulated_submission_time': 6980.779272079468, 'accumulated_eval_time': 4039.0473034381866, 'accumulated_logging_time': 0.8594973087310791}
I0205 02:28:26.820819 140283731310336 logging_writer.py:48] [21774] accumulated_eval_time=4039.047303, accumulated_logging_time=0.859497, accumulated_submission_time=6980.779272, global_step=21774, preemption_count=0, score=6980.779272, test/accuracy=0.985896, test/loss=0.046945, test/mean_average_precision=0.255345, test/num_examples=43793, total_duration=11021.275067, train/accuracy=0.991000, train/loss=0.029773, train/mean_average_precision=0.435705, validation/accuracy=0.986765, validation/loss=0.044360, validation/mean_average_precision=0.260435, validation/num_examples=43793
I0205 02:28:35.468692 140388966844160 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.10146695375442505, loss=0.03417155519127846
I0205 02:29:07.382086 140283731310336 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.08054326474666595, loss=0.03391030430793762
I0205 02:29:39.037595 140388966844160 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.07990755140781403, loss=0.03232402354478836
I0205 02:30:11.027144 140283731310336 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.08575788885354996, loss=0.035338398069143295
I0205 02:30:43.128130 140388966844160 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.06529698520898819, loss=0.03398074954748154
I0205 02:31:15.150689 140283731310336 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.07716060429811478, loss=0.03115685284137726
I0205 02:31:46.891192 140388966844160 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.058521002531051636, loss=0.03276001662015915
I0205 02:32:18.702653 140283731310336 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.06602763384580612, loss=0.036301158368587494
I0205 02:32:26.899364 140451058161472 spec.py:321] Evaluating on the training split.
I0205 02:34:29.499266 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 02:34:32.582886 140451058161472 spec.py:349] Evaluating on the test split.
I0205 02:34:35.611672 140451058161472 submission_runner.py:408] Time since start: 11390.09s, 	Step: 22527, 	{'train/accuracy': 0.990811824798584, 'train/loss': 0.03012635000050068, 'train/mean_average_precision': 0.3952604718650014, 'validation/accuracy': 0.9867122769355774, 'validation/loss': 0.044633541256189346, 'validation/mean_average_precision': 0.2590375873548066, 'validation/num_examples': 43793, 'test/accuracy': 0.9859253168106079, 'test/loss': 0.047483980655670166, 'test/mean_average_precision': 0.25613231337326164, 'test/num_examples': 43793, 'score': 7220.825093746185, 'total_duration': 11390.085256099701, 'accumulated_submission_time': 7220.825093746185, 'accumulated_eval_time': 4167.759459018707, 'accumulated_logging_time': 0.8915479183197021}
I0205 02:34:35.636244 140290188154624 logging_writer.py:48] [22527] accumulated_eval_time=4167.759459, accumulated_logging_time=0.891548, accumulated_submission_time=7220.825094, global_step=22527, preemption_count=0, score=7220.825094, test/accuracy=0.985925, test/loss=0.047484, test/mean_average_precision=0.256132, test/num_examples=43793, total_duration=11390.085256, train/accuracy=0.990812, train/loss=0.030126, train/mean_average_precision=0.395260, validation/accuracy=0.986712, validation/loss=0.044634, validation/mean_average_precision=0.259038, validation/num_examples=43793
I0205 02:34:59.750213 140388975236864 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.12188497930765152, loss=0.03652692586183548
I0205 02:35:31.642966 140290188154624 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.07724034786224365, loss=0.03181358054280281
I0205 02:36:03.442260 140388975236864 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.06851863116025925, loss=0.0315694622695446
I0205 02:36:35.247473 140290188154624 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.08025553077459335, loss=0.03155222907662392
I0205 02:37:07.239959 140388975236864 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.08505307137966156, loss=0.03711249306797981
I0205 02:37:38.691041 140290188154624 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.08006281405687332, loss=0.03271786496043205
I0205 02:38:10.683775 140388975236864 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.06920158118009567, loss=0.03592158481478691
I0205 02:38:35.661743 140451058161472 spec.py:321] Evaluating on the training split.
I0205 02:40:41.791268 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 02:40:44.849729 140451058161472 spec.py:349] Evaluating on the test split.
I0205 02:40:47.846038 140451058161472 submission_runner.py:408] Time since start: 11762.32s, 	Step: 23280, 	{'train/accuracy': 0.9908799529075623, 'train/loss': 0.0300232395529747, 'train/mean_average_precision': 0.4227056232213048, 'validation/accuracy': 0.9867395162582397, 'validation/loss': 0.04472553730010986, 'validation/mean_average_precision': 0.26406131902952434, 'validation/num_examples': 43793, 'test/accuracy': 0.985958993434906, 'test/loss': 0.04724575951695442, 'test/mean_average_precision': 0.2608490687676737, 'test/num_examples': 43793, 'score': 7460.819598436356, 'total_duration': 11762.319630622864, 'accumulated_submission_time': 7460.819598436356, 'accumulated_eval_time': 4299.94361448288, 'accumulated_logging_time': 0.9270291328430176}
I0205 02:40:47.865122 140283731310336 logging_writer.py:48] [23280] accumulated_eval_time=4299.943614, accumulated_logging_time=0.927029, accumulated_submission_time=7460.819598, global_step=23280, preemption_count=0, score=7460.819598, test/accuracy=0.985959, test/loss=0.047246, test/mean_average_precision=0.260849, test/num_examples=43793, total_duration=11762.319631, train/accuracy=0.990880, train/loss=0.030023, train/mean_average_precision=0.422706, validation/accuracy=0.986740, validation/loss=0.044726, validation/mean_average_precision=0.264061, validation/num_examples=43793
I0205 02:40:54.807937 140388966844160 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.09720095992088318, loss=0.03739285096526146
I0205 02:41:26.792809 140283731310336 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.059657350182533264, loss=0.03244788199663162
I0205 02:41:58.772951 140388966844160 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.06984686851501465, loss=0.03322199732065201
I0205 02:42:30.577179 140283731310336 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.09588093310594559, loss=0.033199187368154526
I0205 02:43:02.591516 140388966844160 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.09247708320617676, loss=0.03360650688409805
I0205 02:43:34.394550 140283731310336 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.06975483149290085, loss=0.031427718698978424
I0205 02:44:06.426088 140388966844160 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.06793735176324844, loss=0.03314441442489624
I0205 02:44:38.512400 140283731310336 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.07584875822067261, loss=0.03804818168282509
I0205 02:44:48.024384 140451058161472 spec.py:321] Evaluating on the training split.
I0205 02:46:54.602136 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 02:46:57.664766 140451058161472 spec.py:349] Evaluating on the test split.
I0205 02:47:00.672821 140451058161472 submission_runner.py:408] Time since start: 12135.15s, 	Step: 24031, 	{'train/accuracy': 0.9909188747406006, 'train/loss': 0.02971712313592434, 'train/mean_average_precision': 0.4331208281078121, 'validation/accuracy': 0.9867057800292969, 'validation/loss': 0.04461422935128212, 'validation/mean_average_precision': 0.2611251490209201, 'validation/num_examples': 43793, 'test/accuracy': 0.9859577417373657, 'test/loss': 0.04722660779953003, 'test/mean_average_precision': 0.25167957139764363, 'test/num_examples': 43793, 'score': 7700.661962509155, 'total_duration': 12135.146509170532, 'accumulated_submission_time': 7700.661962509155, 'accumulated_eval_time': 4432.592004299164, 'accumulated_logging_time': 1.2428011894226074}
I0205 02:47:00.692684 140290179761920 logging_writer.py:48] [24031] accumulated_eval_time=4432.592004, accumulated_logging_time=1.242801, accumulated_submission_time=7700.661963, global_step=24031, preemption_count=0, score=7700.661963, test/accuracy=0.985958, test/loss=0.047227, test/mean_average_precision=0.251680, test/num_examples=43793, total_duration=12135.146509, train/accuracy=0.990919, train/loss=0.029717, train/mean_average_precision=0.433121, validation/accuracy=0.986706, validation/loss=0.044614, validation/mean_average_precision=0.261125, validation/num_examples=43793
I0205 02:47:23.318879 140290188154624 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.06588969379663467, loss=0.03534159064292908
I0205 02:47:55.001233 140290179761920 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.07774942368268967, loss=0.034092094749212265
I0205 02:48:26.759048 140290188154624 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.09835466742515564, loss=0.0342918261885643
I0205 02:48:58.381808 140290179761920 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.09736616164445877, loss=0.033846672624349594
I0205 02:49:30.481227 140290188154624 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.09782400727272034, loss=0.03578422591090202
I0205 02:50:02.315405 140290179761920 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.09678921103477478, loss=0.03561180457472801
I0205 02:50:33.905744 140290188154624 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.08783776313066483, loss=0.03399757668375969
I0205 02:51:00.859719 140451058161472 spec.py:321] Evaluating on the training split.
I0205 02:53:08.317193 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 02:53:11.394759 140451058161472 spec.py:349] Evaluating on the test split.
I0205 02:53:14.392709 140451058161472 submission_runner.py:408] Time since start: 12508.87s, 	Step: 24785, 	{'train/accuracy': 0.9908357858657837, 'train/loss': 0.029875533655285835, 'train/mean_average_precision': 0.4064477900468354, 'validation/accuracy': 0.9866595268249512, 'validation/loss': 0.0450131893157959, 'validation/mean_average_precision': 0.25737166574734294, 'validation/num_examples': 43793, 'test/accuracy': 0.9858036041259766, 'test/loss': 0.04784903675317764, 'test/mean_average_precision': 0.25031988347804224, 'test/num_examples': 43793, 'score': 7940.79802775383, 'total_duration': 12508.866287469864, 'accumulated_submission_time': 7940.79802775383, 'accumulated_eval_time': 4566.1248388290405, 'accumulated_logging_time': 1.2736265659332275}
I0205 02:53:14.413161 140283731310336 logging_writer.py:48] [24785] accumulated_eval_time=4566.124839, accumulated_logging_time=1.273627, accumulated_submission_time=7940.798028, global_step=24785, preemption_count=0, score=7940.798028, test/accuracy=0.985804, test/loss=0.047849, test/mean_average_precision=0.250320, test/num_examples=43793, total_duration=12508.866287, train/accuracy=0.990836, train/loss=0.029876, train/mean_average_precision=0.406448, validation/accuracy=0.986660, validation/loss=0.045013, validation/mean_average_precision=0.257372, validation/num_examples=43793
I0205 02:53:19.615807 140388966844160 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.06618780642747879, loss=0.03249248117208481
I0205 02:53:51.938684 140283731310336 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.07399307191371918, loss=0.03319317102432251
I0205 02:54:24.110974 140388966844160 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.09597904235124588, loss=0.031050724908709526
I0205 02:54:55.552118 140283731310336 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.07271908968687057, loss=0.03321538493037224
I0205 02:55:27.844641 140388966844160 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.0758599266409874, loss=0.03312813490629196
I0205 02:55:59.775526 140283731310336 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.08596859872341156, loss=0.03555218502879143
I0205 02:56:31.595579 140388966844160 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.07099074125289917, loss=0.03663359209895134
I0205 02:57:03.232741 140283731310336 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.07154639065265656, loss=0.030783940106630325
I0205 02:57:14.534653 140451058161472 spec.py:321] Evaluating on the training split.
I0205 02:59:23.532129 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 02:59:26.931609 140451058161472 spec.py:349] Evaluating on the test split.
I0205 02:59:30.348375 140451058161472 submission_runner.py:408] Time since start: 12884.82s, 	Step: 25537, 	{'train/accuracy': 0.9911275506019592, 'train/loss': 0.029165217652916908, 'train/mean_average_precision': 0.44100879810799015, 'validation/accuracy': 0.9867557287216187, 'validation/loss': 0.04441393166780472, 'validation/mean_average_precision': 0.27166666396482503, 'validation/num_examples': 43793, 'test/accuracy': 0.9858790040016174, 'test/loss': 0.047144122421741486, 'test/mean_average_precision': 0.2556667009203458, 'test/num_examples': 43793, 'score': 8180.888298749924, 'total_duration': 12884.822046756744, 'accumulated_submission_time': 8180.888298749924, 'accumulated_eval_time': 4701.938496828079, 'accumulated_logging_time': 1.304905891418457}
I0205 02:59:30.370900 140290179761920 logging_writer.py:48] [25537] accumulated_eval_time=4701.938497, accumulated_logging_time=1.304906, accumulated_submission_time=8180.888299, global_step=25537, preemption_count=0, score=8180.888299, test/accuracy=0.985879, test/loss=0.047144, test/mean_average_precision=0.255667, test/num_examples=43793, total_duration=12884.822047, train/accuracy=0.991128, train/loss=0.029165, train/mean_average_precision=0.441009, validation/accuracy=0.986756, validation/loss=0.044414, validation/mean_average_precision=0.271667, validation/num_examples=43793
I0205 02:59:51.296574 140388975236864 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.09931754320859909, loss=0.033539898693561554
I0205 03:00:23.425398 140290179761920 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.06282258778810501, loss=0.03170676529407501
I0205 03:00:55.612440 140388975236864 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.06677588820457458, loss=0.03326147049665451
I0205 03:01:27.423347 140290179761920 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.07920568436384201, loss=0.03175732493400574
I0205 03:01:59.228791 140388975236864 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.08226459473371506, loss=0.03350779414176941
I0205 03:02:31.201089 140290179761920 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.10925739258527756, loss=0.03416699171066284
I0205 03:03:02.961386 140388975236864 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.07642108201980591, loss=0.03351840004324913
I0205 03:03:30.643081 140451058161472 spec.py:321] Evaluating on the training split.
I0205 03:05:37.682223 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 03:05:40.753638 140451058161472 spec.py:349] Evaluating on the test split.
I0205 03:05:43.757437 140451058161472 submission_runner.py:408] Time since start: 13258.23s, 	Step: 26288, 	{'train/accuracy': 0.9913343191146851, 'train/loss': 0.028332335874438286, 'train/mean_average_precision': 0.4558309209604774, 'validation/accuracy': 0.986750066280365, 'validation/loss': 0.04462895169854164, 'validation/mean_average_precision': 0.2663130309218301, 'validation/num_examples': 43793, 'test/accuracy': 0.9859998822212219, 'test/loss': 0.047167856246232986, 'test/mean_average_precision': 0.25527667639900953, 'test/num_examples': 43793, 'score': 8421.127045869827, 'total_duration': 13258.230972528458, 'accumulated_submission_time': 8421.127045869827, 'accumulated_eval_time': 4835.052654981613, 'accumulated_logging_time': 1.3399152755737305}
I0205 03:05:43.777333 140283731310336 logging_writer.py:48] [26288] accumulated_eval_time=4835.052655, accumulated_logging_time=1.339915, accumulated_submission_time=8421.127046, global_step=26288, preemption_count=0, score=8421.127046, test/accuracy=0.986000, test/loss=0.047168, test/mean_average_precision=0.255277, test/num_examples=43793, total_duration=13258.230973, train/accuracy=0.991334, train/loss=0.028332, train/mean_average_precision=0.455831, validation/accuracy=0.986750, validation/loss=0.044629, validation/mean_average_precision=0.266313, validation/num_examples=43793
I0205 03:05:48.004597 140290188154624 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.09335343539714813, loss=0.035641156136989594
I0205 03:06:19.892147 140283731310336 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.07047642022371292, loss=0.03108423948287964
I0205 03:06:51.407702 140290188154624 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.06919892877340317, loss=0.033196549862623215
I0205 03:07:23.207923 140283731310336 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.09663078933954239, loss=0.03782479465007782
I0205 03:07:54.662356 140290188154624 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.0765221044421196, loss=0.03279491141438484
I0205 03:08:26.626055 140283731310336 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.07863149046897888, loss=0.03053673543035984
I0205 03:08:58.483896 140290188154624 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.09481330215930939, loss=0.03577663376927376
I0205 03:09:30.353793 140283731310336 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.13303285837173462, loss=0.03323805332183838
I0205 03:09:43.876638 140451058161472 spec.py:321] Evaluating on the training split.
I0205 03:11:51.117799 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 03:11:54.211409 140451058161472 spec.py:349] Evaluating on the test split.
I0205 03:11:57.195937 140451058161472 submission_runner.py:408] Time since start: 13631.67s, 	Step: 27044, 	{'train/accuracy': 0.9913906455039978, 'train/loss': 0.02815435081720352, 'train/mean_average_precision': 0.4644263463446575, 'validation/accuracy': 0.9868559837341309, 'validation/loss': 0.044855065643787384, 'validation/mean_average_precision': 0.27040164502811026, 'validation/num_examples': 43793, 'test/accuracy': 0.9859737753868103, 'test/loss': 0.047707315534353256, 'test/mean_average_precision': 0.2546293091749866, 'test/num_examples': 43793, 'score': 8661.194565296173, 'total_duration': 13631.669520616531, 'accumulated_submission_time': 8661.194565296173, 'accumulated_eval_time': 4968.371803283691, 'accumulated_logging_time': 1.3710052967071533}
I0205 03:11:57.216207 140290179761920 logging_writer.py:48] [27044] accumulated_eval_time=4968.371803, accumulated_logging_time=1.371005, accumulated_submission_time=8661.194565, global_step=27044, preemption_count=0, score=8661.194565, test/accuracy=0.985974, test/loss=0.047707, test/mean_average_precision=0.254629, test/num_examples=43793, total_duration=13631.669521, train/accuracy=0.991391, train/loss=0.028154, train/mean_average_precision=0.464426, validation/accuracy=0.986856, validation/loss=0.044855, validation/mean_average_precision=0.270402, validation/num_examples=43793
I0205 03:12:15.331678 140388975236864 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.08728692680597305, loss=0.03090602345764637
I0205 03:12:46.739702 140290179761920 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.09667585790157318, loss=0.03698630630970001
I0205 03:13:18.333899 140388975236864 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.08632731437683105, loss=0.033738452941179276
I0205 03:13:49.892928 140290179761920 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.09385708719491959, loss=0.031869567930698395
I0205 03:14:21.658403 140388975236864 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.06766407191753387, loss=0.03014037013053894
I0205 03:14:53.348495 140290179761920 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.07709990441799164, loss=0.03208676725625992
I0205 03:15:25.336606 140388975236864 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.06648511439561844, loss=0.03279191255569458
I0205 03:15:57.333380 140290179761920 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.07762377709150314, loss=0.03189324587583542
I0205 03:15:57.338460 140451058161472 spec.py:321] Evaluating on the training split.
I0205 03:18:01.229625 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 03:18:04.743819 140451058161472 spec.py:349] Evaluating on the test split.
I0205 03:18:08.081908 140451058161472 submission_runner.py:408] Time since start: 14002.56s, 	Step: 27801, 	{'train/accuracy': 0.9913938045501709, 'train/loss': 0.02781856618821621, 'train/mean_average_precision': 0.47696753609613807, 'validation/accuracy': 0.986710250377655, 'validation/loss': 0.045109447091817856, 'validation/mean_average_precision': 0.26479174347512763, 'validation/num_examples': 43793, 'test/accuracy': 0.9859451055526733, 'test/loss': 0.0478653758764267, 'test/mean_average_precision': 0.2571030521372473, 'test/num_examples': 43793, 'score': 8901.284093856812, 'total_duration': 14002.555571317673, 'accumulated_submission_time': 8901.284093856812, 'accumulated_eval_time': 5099.115159749985, 'accumulated_logging_time': 1.403883695602417}
I0205 03:18:08.105221 140283731310336 logging_writer.py:48] [27801] accumulated_eval_time=5099.115160, accumulated_logging_time=1.403884, accumulated_submission_time=8901.284094, global_step=27801, preemption_count=0, score=8901.284094, test/accuracy=0.985945, test/loss=0.047865, test/mean_average_precision=0.257103, test/num_examples=43793, total_duration=14002.555571, train/accuracy=0.991394, train/loss=0.027819, train/mean_average_precision=0.476968, validation/accuracy=0.986710, validation/loss=0.045109, validation/mean_average_precision=0.264792, validation/num_examples=43793
I0205 03:18:40.667526 140388966844160 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.08921642601490021, loss=0.03260187804698944
I0205 03:19:13.574388 140283731310336 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.08217675983905792, loss=0.03363052383065224
I0205 03:19:46.018756 140388966844160 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.07538985460996628, loss=0.03340177983045578
I0205 03:20:17.882620 140283731310336 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.06878594309091568, loss=0.03050423040986061
I0205 03:20:49.649160 140388966844160 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.0685088038444519, loss=0.03164822235703468
I0205 03:21:21.702488 140283731310336 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.07909197360277176, loss=0.032359905540943146
I0205 03:21:53.645975 140388966844160 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.07125651091337204, loss=0.03221064805984497
I0205 03:22:08.175475 140451058161472 spec.py:321] Evaluating on the training split.
I0205 03:24:13.627809 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 03:24:16.654013 140451058161472 spec.py:349] Evaluating on the test split.
I0205 03:24:19.672390 140451058161472 submission_runner.py:408] Time since start: 14374.15s, 	Step: 28546, 	{'train/accuracy': 0.9915661811828613, 'train/loss': 0.027152186259627342, 'train/mean_average_precision': 0.48831206860766, 'validation/accuracy': 0.9869323372840881, 'validation/loss': 0.04443078488111496, 'validation/mean_average_precision': 0.2773346118327409, 'validation/num_examples': 43793, 'test/accuracy': 0.9859514236450195, 'test/loss': 0.04753367602825165, 'test/mean_average_precision': 0.25835264106118383, 'test/num_examples': 43793, 'score': 9141.320052146912, 'total_duration': 14374.145962715149, 'accumulated_submission_time': 9141.320052146912, 'accumulated_eval_time': 5230.611914157867, 'accumulated_logging_time': 1.4391045570373535}
I0205 03:24:19.692770 140290179761920 logging_writer.py:48] [28546] accumulated_eval_time=5230.611914, accumulated_logging_time=1.439105, accumulated_submission_time=9141.320052, global_step=28546, preemption_count=0, score=9141.320052, test/accuracy=0.985951, test/loss=0.047534, test/mean_average_precision=0.258353, test/num_examples=43793, total_duration=14374.145963, train/accuracy=0.991566, train/loss=0.027152, train/mean_average_precision=0.488312, validation/accuracy=0.986932, validation/loss=0.044431, validation/mean_average_precision=0.277335, validation/num_examples=43793
I0205 03:24:37.481158 140388975236864 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.08186930418014526, loss=0.030577434226870537
I0205 03:25:09.997420 140290179761920 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.08846496045589447, loss=0.034057363867759705
I0205 03:25:41.359193 140388975236864 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.0724978819489479, loss=0.031264740973711014
I0205 03:26:13.257980 140290179761920 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.08650623261928558, loss=0.029909132048487663
I0205 03:26:44.815749 140388975236864 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.08902540802955627, loss=0.03321634605526924
I0205 03:27:16.703699 140290179761920 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.080967977643013, loss=0.03131524473428726
I0205 03:27:48.795218 140388975236864 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.07556139677762985, loss=0.03214089199900627
I0205 03:28:19.741064 140451058161472 spec.py:321] Evaluating on the training split.
I0205 03:30:28.248750 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 03:30:31.635275 140451058161472 spec.py:349] Evaluating on the test split.
I0205 03:30:34.959926 140451058161472 submission_runner.py:408] Time since start: 14749.43s, 	Step: 29299, 	{'train/accuracy': 0.9914606809616089, 'train/loss': 0.02791007235646248, 'train/mean_average_precision': 0.45549788705928623, 'validation/accuracy': 0.9867382645606995, 'validation/loss': 0.044542621821165085, 'validation/mean_average_precision': 0.2711044475252064, 'validation/num_examples': 43793, 'test/accuracy': 0.9858566522598267, 'test/loss': 0.04740707948803902, 'test/mean_average_precision': 0.25263625499181763, 'test/num_examples': 43793, 'score': 9381.337366342545, 'total_duration': 14749.433589935303, 'accumulated_submission_time': 9381.337366342545, 'accumulated_eval_time': 5365.830714225769, 'accumulated_logging_time': 1.4701387882232666}
I0205 03:30:34.982238 140290188154624 logging_writer.py:48] [29299] accumulated_eval_time=5365.830714, accumulated_logging_time=1.470139, accumulated_submission_time=9381.337366, global_step=29299, preemption_count=0, score=9381.337366, test/accuracy=0.985857, test/loss=0.047407, test/mean_average_precision=0.252636, test/num_examples=43793, total_duration=14749.433590, train/accuracy=0.991461, train/loss=0.027910, train/mean_average_precision=0.455498, validation/accuracy=0.986738, validation/loss=0.044543, validation/mean_average_precision=0.271104, validation/num_examples=43793
I0205 03:30:35.676673 140388966844160 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.07723366469144821, loss=0.030284464359283447
I0205 03:31:08.443962 140290188154624 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.06830638647079468, loss=0.03205947205424309
I0205 03:31:40.956650 140388966844160 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.06667482107877731, loss=0.031052405014634132
I0205 03:32:13.316483 140290188154624 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.07932112365961075, loss=0.03210090100765228
I0205 03:32:45.398041 140388966844160 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.08212544023990631, loss=0.03178880736231804
I0205 03:33:17.628178 140290188154624 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.10591904073953629, loss=0.0359913632273674
I0205 03:33:49.119672 140388966844160 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.07639843970537186, loss=0.03332555294036865
I0205 03:34:21.247325 140290188154624 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.0971817746758461, loss=0.03476082533597946
I0205 03:34:35.145227 140451058161472 spec.py:321] Evaluating on the training split.
I0205 03:36:41.859880 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 03:36:44.975608 140451058161472 spec.py:349] Evaluating on the test split.
I0205 03:36:48.040358 140451058161472 submission_runner.py:408] Time since start: 15122.51s, 	Step: 30045, 	{'train/accuracy': 0.9913190603256226, 'train/loss': 0.02830742672085762, 'train/mean_average_precision': 0.4533270602357647, 'validation/accuracy': 0.9867492318153381, 'validation/loss': 0.044565699994564056, 'validation/mean_average_precision': 0.2735216258339702, 'validation/num_examples': 43793, 'test/accuracy': 0.9859737753868103, 'test/loss': 0.047178592532873154, 'test/mean_average_precision': 0.26105795087793254, 'test/num_examples': 43793, 'score': 9621.466262102127, 'total_duration': 15122.514045715332, 'accumulated_submission_time': 9621.466262102127, 'accumulated_eval_time': 5498.725798130035, 'accumulated_logging_time': 1.5060889720916748}
I0205 03:36:48.061728 140283731310336 logging_writer.py:48] [30045] accumulated_eval_time=5498.725798, accumulated_logging_time=1.506089, accumulated_submission_time=9621.466262, global_step=30045, preemption_count=0, score=9621.466262, test/accuracy=0.985974, test/loss=0.047179, test/mean_average_precision=0.261058, test/num_examples=43793, total_duration=15122.514046, train/accuracy=0.991319, train/loss=0.028307, train/mean_average_precision=0.453327, validation/accuracy=0.986749, validation/loss=0.044566, validation/mean_average_precision=0.273522, validation/num_examples=43793
I0205 03:37:06.491733 140290179761920 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.09051033854484558, loss=0.03227821737527847
I0205 03:37:38.338531 140283731310336 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.07768669724464417, loss=0.031425632536411285
I0205 03:38:10.084206 140290179761920 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.07282894849777222, loss=0.03203665092587471
I0205 03:38:42.340866 140283731310336 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.08881155401468277, loss=0.0316392220556736
I0205 03:39:14.433605 140290179761920 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.08018771559000015, loss=0.032292772084474564
I0205 03:39:46.761060 140283731310336 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.08184625208377838, loss=0.03242678940296173
I0205 03:40:18.598804 140290179761920 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.06004149466753006, loss=0.029972003772854805
I0205 03:40:48.295004 140451058161472 spec.py:321] Evaluating on the training split.
I0205 03:42:52.515890 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 03:42:55.603820 140451058161472 spec.py:349] Evaluating on the test split.
I0205 03:42:58.628144 140451058161472 submission_runner.py:408] Time since start: 15493.10s, 	Step: 30794, 	{'train/accuracy': 0.9913336038589478, 'train/loss': 0.02827261947095394, 'train/mean_average_precision': 0.4520790403454369, 'validation/accuracy': 0.986777663230896, 'validation/loss': 0.0448727048933506, 'validation/mean_average_precision': 0.2668618929323761, 'validation/num_examples': 43793, 'test/accuracy': 0.9858756065368652, 'test/loss': 0.047772377729415894, 'test/mean_average_precision': 0.2537742032421881, 'test/num_examples': 43793, 'score': 9861.666239261627, 'total_duration': 15493.101830720901, 'accumulated_submission_time': 9861.666239261627, 'accumulated_eval_time': 5629.058895349503, 'accumulated_logging_time': 1.5400199890136719}
I0205 03:42:58.649514 140290188154624 logging_writer.py:48] [30794] accumulated_eval_time=5629.058895, accumulated_logging_time=1.540020, accumulated_submission_time=9861.666239, global_step=30794, preemption_count=0, score=9861.666239, test/accuracy=0.985876, test/loss=0.047772, test/mean_average_precision=0.253774, test/num_examples=43793, total_duration=15493.101831, train/accuracy=0.991334, train/loss=0.028273, train/mean_average_precision=0.452079, validation/accuracy=0.986778, validation/loss=0.044873, validation/mean_average_precision=0.266862, validation/num_examples=43793
I0205 03:43:00.858385 140388966844160 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.097182497382164, loss=0.03279804065823555
I0205 03:43:32.751472 140290188154624 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.0773063525557518, loss=0.03233349695801735
I0205 03:44:04.661458 140388966844160 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.10874147713184357, loss=0.032622162252664566
I0205 03:44:36.393929 140290188154624 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.08671239018440247, loss=0.03184892609715462
I0205 03:45:08.635312 140388966844160 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.08588086068630219, loss=0.03318103402853012
I0205 03:45:40.144953 140290188154624 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.09070739150047302, loss=0.032230082899332047
I0205 03:46:11.803033 140388966844160 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.07821235805749893, loss=0.03249960020184517
I0205 03:46:43.375353 140290188154624 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.08282911777496338, loss=0.029434051364660263
I0205 03:46:58.747917 140451058161472 spec.py:321] Evaluating on the training split.
I0205 03:49:08.928683 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 03:49:11.977723 140451058161472 spec.py:349] Evaluating on the test split.
I0205 03:49:14.985847 140451058161472 submission_runner.py:408] Time since start: 15869.46s, 	Step: 31550, 	{'train/accuracy': 0.9914684891700745, 'train/loss': 0.0279049314558506, 'train/mean_average_precision': 0.46250529010033, 'validation/accuracy': 0.9867516756057739, 'validation/loss': 0.04472426325082779, 'validation/mean_average_precision': 0.2758591528506365, 'validation/num_examples': 43793, 'test/accuracy': 0.9859691262245178, 'test/loss': 0.04747837781906128, 'test/mean_average_precision': 0.263347388682702, 'test/num_examples': 43793, 'score': 10101.732964754105, 'total_duration': 15869.459534406662, 'accumulated_submission_time': 10101.732964754105, 'accumulated_eval_time': 5765.29677939415, 'accumulated_logging_time': 1.5725555419921875}
I0205 03:49:15.007323 140283731310336 logging_writer.py:48] [31550] accumulated_eval_time=5765.296779, accumulated_logging_time=1.572556, accumulated_submission_time=10101.732965, global_step=31550, preemption_count=0, score=10101.732965, test/accuracy=0.985969, test/loss=0.047478, test/mean_average_precision=0.263347, test/num_examples=43793, total_duration=15869.459534, train/accuracy=0.991468, train/loss=0.027905, train/mean_average_precision=0.462505, validation/accuracy=0.986752, validation/loss=0.044724, validation/mean_average_precision=0.275859, validation/num_examples=43793
I0205 03:49:31.192885 140290179761920 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.08366044610738754, loss=0.033822111785411835
I0205 03:50:03.325134 140283731310336 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.07404836267232895, loss=0.028612645342946053
I0205 03:50:36.256059 140290179761920 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.09136324375867844, loss=0.03484823554754257
I0205 03:51:09.475449 140283731310336 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.08704275637865067, loss=0.03458039462566376
I0205 03:51:42.315250 140290179761920 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.09569921344518661, loss=0.03048429824411869
I0205 03:52:15.450263 140283731310336 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.1274026781320572, loss=0.031281791627407074
I0205 03:52:48.581200 140290179761920 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.08311356604099274, loss=0.033217158168554306
I0205 03:53:15.011615 140451058161472 spec.py:321] Evaluating on the training split.
I0205 03:55:20.097645 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 03:55:23.143088 140451058161472 spec.py:349] Evaluating on the test split.
I0205 03:55:26.123194 140451058161472 submission_runner.py:408] Time since start: 16240.60s, 	Step: 32282, 	{'train/accuracy': 0.9915327429771423, 'train/loss': 0.027638010680675507, 'train/mean_average_precision': 0.4701164224800756, 'validation/accuracy': 0.9868023991584778, 'validation/loss': 0.04418288171291351, 'validation/mean_average_precision': 0.27248519758707684, 'validation/num_examples': 43793, 'test/accuracy': 0.985917329788208, 'test/loss': 0.046841979026794434, 'test/mean_average_precision': 0.26137567472069356, 'test/num_examples': 43793, 'score': 10341.703769683838, 'total_duration': 16240.596867084503, 'accumulated_submission_time': 10341.703769683838, 'accumulated_eval_time': 5896.408312559128, 'accumulated_logging_time': 1.6048774719238281}
I0205 03:55:26.144804 140290188154624 logging_writer.py:48] [32282] accumulated_eval_time=5896.408313, accumulated_logging_time=1.604877, accumulated_submission_time=10341.703770, global_step=32282, preemption_count=0, score=10341.703770, test/accuracy=0.985917, test/loss=0.046842, test/mean_average_precision=0.261376, test/num_examples=43793, total_duration=16240.596867, train/accuracy=0.991533, train/loss=0.027638, train/mean_average_precision=0.470116, validation/accuracy=0.986802, validation/loss=0.044183, validation/mean_average_precision=0.272485, validation/num_examples=43793
I0205 03:55:32.212525 140388975236864 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.08616044372320175, loss=0.0348348431289196
I0205 03:56:04.178664 140290188154624 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.09998764097690582, loss=0.035835009068250656
I0205 03:56:36.883718 140388975236864 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.10223624855279922, loss=0.03077472187578678
I0205 03:57:09.887104 140290188154624 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.09446823596954346, loss=0.034576695412397385
I0205 03:57:42.917316 140388975236864 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.09756433963775635, loss=0.03284710273146629
I0205 03:58:15.951952 140290188154624 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.08273942023515701, loss=0.032832320779561996
I0205 03:58:48.912436 140388975236864 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.09558078646659851, loss=0.03343596309423447
I0205 03:59:21.956328 140290188154624 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.07881523668766022, loss=0.03061005473136902
I0205 03:59:26.241628 140451058161472 spec.py:321] Evaluating on the training split.
I0205 04:01:34.720839 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 04:01:37.851713 140451058161472 spec.py:349] Evaluating on the test split.
I0205 04:01:40.879161 140451058161472 submission_runner.py:408] Time since start: 16615.35s, 	Step: 33014, 	{'train/accuracy': 0.9916948676109314, 'train/loss': 0.02696775458753109, 'train/mean_average_precision': 0.47629173604202346, 'validation/accuracy': 0.9868279695510864, 'validation/loss': 0.04445328935980797, 'validation/mean_average_precision': 0.27250767912286333, 'validation/num_examples': 43793, 'test/accuracy': 0.9860011339187622, 'test/loss': 0.047196511179208755, 'test/mean_average_precision': 0.25689540766324287, 'test/num_examples': 43793, 'score': 10581.765675783157, 'total_duration': 16615.352819919586, 'accumulated_submission_time': 10581.765675783157, 'accumulated_eval_time': 6031.045778036118, 'accumulated_logging_time': 1.6387574672698975}
I0205 04:01:40.901095 140290179761920 logging_writer.py:48] [33014] accumulated_eval_time=6031.045778, accumulated_logging_time=1.638757, accumulated_submission_time=10581.765676, global_step=33014, preemption_count=0, score=10581.765676, test/accuracy=0.986001, test/loss=0.047197, test/mean_average_precision=0.256895, test/num_examples=43793, total_duration=16615.352820, train/accuracy=0.991695, train/loss=0.026968, train/mean_average_precision=0.476292, validation/accuracy=0.986828, validation/loss=0.044453, validation/mean_average_precision=0.272508, validation/num_examples=43793
I0205 04:02:09.310591 140388966844160 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.09922140091657639, loss=0.03193022683262825
I0205 04:02:41.517823 140290179761920 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.07501914352178574, loss=0.03373052179813385
I0205 04:03:13.360408 140388966844160 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.11012420058250427, loss=0.033568475395441055
I0205 04:03:44.998747 140290179761920 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.08771635591983795, loss=0.03021945059299469
I0205 04:04:16.807812 140388966844160 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.08026814460754395, loss=0.028029724955558777
I0205 04:04:48.793140 140290179761920 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.07896368205547333, loss=0.03205568343400955
I0205 04:05:20.578881 140388966844160 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.07911641150712967, loss=0.03142926096916199
I0205 04:05:41.128233 140451058161472 spec.py:321] Evaluating on the training split.
I0205 04:07:44.982544 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 04:07:48.064609 140451058161472 spec.py:349] Evaluating on the test split.
I0205 04:07:51.132386 140451058161472 submission_runner.py:408] Time since start: 16985.61s, 	Step: 33766, 	{'train/accuracy': 0.9919641613960266, 'train/loss': 0.026328330859541893, 'train/mean_average_precision': 0.4970465806059808, 'validation/accuracy': 0.9867817163467407, 'validation/loss': 0.0448424257338047, 'validation/mean_average_precision': 0.27030136921882875, 'validation/num_examples': 43793, 'test/accuracy': 0.9859564900398254, 'test/loss': 0.04744219407439232, 'test/mean_average_precision': 0.2586274867507777, 'test/num_examples': 43793, 'score': 10821.961535930634, 'total_duration': 16985.606071949005, 'accumulated_submission_time': 10821.961535930634, 'accumulated_eval_time': 6161.0498831272125, 'accumulated_logging_time': 1.6721007823944092}
I0205 04:07:51.154949 140283731310336 logging_writer.py:48] [33766] accumulated_eval_time=6161.049883, accumulated_logging_time=1.672101, accumulated_submission_time=10821.961536, global_step=33766, preemption_count=0, score=10821.961536, test/accuracy=0.985956, test/loss=0.047442, test/mean_average_precision=0.258627, test/num_examples=43793, total_duration=16985.606072, train/accuracy=0.991964, train/loss=0.026328, train/mean_average_precision=0.497047, validation/accuracy=0.986782, validation/loss=0.044842, validation/mean_average_precision=0.270301, validation/num_examples=43793
I0205 04:08:02.525794 140290188154624 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.09849552810192108, loss=0.03357243910431862
I0205 04:08:34.351577 140283731310336 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.08582092076539993, loss=0.03177788853645325
I0205 04:09:06.316630 140290188154624 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.11896280199289322, loss=0.03546065092086792
I0205 04:09:37.935059 140283731310336 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.09708398580551147, loss=0.030267467722296715
I0205 04:10:09.842710 140290188154624 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.0945945680141449, loss=0.03307053819298744
I0205 04:10:41.340839 140283731310336 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.11988616734743118, loss=0.030158577486872673
I0205 04:11:13.084117 140290188154624 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.07699111849069595, loss=0.02770138345658779
I0205 04:11:44.713834 140283731310336 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.07961606979370117, loss=0.03232669085264206
I0205 04:11:51.374796 140451058161472 spec.py:321] Evaluating on the training split.
I0205 04:13:52.781245 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 04:13:55.783360 140451058161472 spec.py:349] Evaluating on the test split.
I0205 04:13:59.830005 140451058161472 submission_runner.py:408] Time since start: 17354.30s, 	Step: 34522, 	{'train/accuracy': 0.9921725392341614, 'train/loss': 0.025557050481438637, 'train/mean_average_precision': 0.527538363625475, 'validation/accuracy': 0.9869164824485779, 'validation/loss': 0.044772692024707794, 'validation/mean_average_precision': 0.27226595903886797, 'validation/num_examples': 43793, 'test/accuracy': 0.9859733581542969, 'test/loss': 0.0475214347243309, 'test/mean_average_precision': 0.26075940291263694, 'test/num_examples': 43793, 'score': 11062.150256633759, 'total_duration': 17354.303694963455, 'accumulated_submission_time': 11062.150256633759, 'accumulated_eval_time': 6289.505045890808, 'accumulated_logging_time': 1.7057373523712158}
I0205 04:13:59.851794 140290179761920 logging_writer.py:48] [34522] accumulated_eval_time=6289.505046, accumulated_logging_time=1.705737, accumulated_submission_time=11062.150257, global_step=34522, preemption_count=0, score=11062.150257, test/accuracy=0.985973, test/loss=0.047521, test/mean_average_precision=0.260759, test/num_examples=43793, total_duration=17354.303695, train/accuracy=0.992173, train/loss=0.025557, train/mean_average_precision=0.527538, validation/accuracy=0.986916, validation/loss=0.044773, validation/mean_average_precision=0.272266, validation/num_examples=43793
I0205 04:14:25.160239 140388975236864 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.07553768157958984, loss=0.030488332733511925
I0205 04:14:56.878150 140290179761920 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.0982532948255539, loss=0.03229739889502525
I0205 04:15:28.618937 140388975236864 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.08636513352394104, loss=0.031216936185956
I0205 04:16:00.087924 140290179761920 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.09298869222402573, loss=0.031660180538892746
I0205 04:16:31.828937 140388975236864 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.08519849926233292, loss=0.031066490337252617
I0205 04:17:03.564364 140290179761920 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.09423360973596573, loss=0.031683675944805145
I0205 04:17:35.714618 140388975236864 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.08609567582607269, loss=0.0291841272264719
I0205 04:17:59.836816 140451058161472 spec.py:321] Evaluating on the training split.
I0205 04:20:03.690107 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 04:20:06.791064 140451058161472 spec.py:349] Evaluating on the test split.
I0205 04:20:09.846247 140451058161472 submission_runner.py:408] Time since start: 17724.32s, 	Step: 35277, 	{'train/accuracy': 0.9921831488609314, 'train/loss': 0.025127580389380455, 'train/mean_average_precision': 0.532644755953247, 'validation/accuracy': 0.9869319200515747, 'validation/loss': 0.0451381579041481, 'validation/mean_average_precision': 0.2747726059200759, 'validation/num_examples': 43793, 'test/accuracy': 0.9859846830368042, 'test/loss': 0.04820175841450691, 'test/mean_average_precision': 0.2530558781775611, 'test/num_examples': 43793, 'score': 11302.103286266327, 'total_duration': 17724.31993317604, 'accumulated_submission_time': 11302.103286266327, 'accumulated_eval_time': 6419.514434099197, 'accumulated_logging_time': 1.7393901348114014}
I0205 04:20:09.868537 140283731310336 logging_writer.py:48] [35277] accumulated_eval_time=6419.514434, accumulated_logging_time=1.739390, accumulated_submission_time=11302.103286, global_step=35277, preemption_count=0, score=11302.103286, test/accuracy=0.985985, test/loss=0.048202, test/mean_average_precision=0.253056, test/num_examples=43793, total_duration=17724.319933, train/accuracy=0.992183, train/loss=0.025128, train/mean_average_precision=0.532645, validation/accuracy=0.986932, validation/loss=0.045138, validation/mean_average_precision=0.274773, validation/num_examples=43793
I0205 04:20:17.640418 140388966844160 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.1052132099866867, loss=0.03191561624407768
I0205 04:20:49.083070 140283731310336 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.09174184501171112, loss=0.03147219121456146
I0205 04:21:20.624444 140388966844160 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.0820998102426529, loss=0.03379824012517929
I0205 04:21:52.491963 140283731310336 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.14511938393115997, loss=0.03050766885280609
I0205 04:22:23.990297 140388966844160 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.09027136862277985, loss=0.03218429908156395
I0205 04:22:55.402522 140283731310336 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.112683966755867, loss=0.030164742842316628
I0205 04:23:27.375656 140388966844160 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.1222200095653534, loss=0.03303518518805504
I0205 04:23:59.046370 140283731310336 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.0907776802778244, loss=0.0302363820374012
I0205 04:24:09.909556 140451058161472 spec.py:321] Evaluating on the training split.
I0205 04:26:11.479171 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 04:26:14.554754 140451058161472 spec.py:349] Evaluating on the test split.
I0205 04:26:17.544738 140451058161472 submission_runner.py:408] Time since start: 18092.02s, 	Step: 36035, 	{'train/accuracy': 0.9919872879981995, 'train/loss': 0.02594996802508831, 'train/mean_average_precision': 0.5178907389430005, 'validation/accuracy': 0.9868706464767456, 'validation/loss': 0.044748783111572266, 'validation/mean_average_precision': 0.2739533269282222, 'validation/num_examples': 43793, 'test/accuracy': 0.9860104322433472, 'test/loss': 0.04768946394324303, 'test/mean_average_precision': 0.26107758368161565, 'test/num_examples': 43793, 'score': 11542.113516330719, 'total_duration': 18092.018426179886, 'accumulated_submission_time': 11542.113516330719, 'accumulated_eval_time': 6547.149572849274, 'accumulated_logging_time': 1.7725646495819092}
I0205 04:26:17.566836 140290179761920 logging_writer.py:48] [36035] accumulated_eval_time=6547.149573, accumulated_logging_time=1.772565, accumulated_submission_time=11542.113516, global_step=36035, preemption_count=0, score=11542.113516, test/accuracy=0.986010, test/loss=0.047689, test/mean_average_precision=0.261078, test/num_examples=43793, total_duration=18092.018426, train/accuracy=0.991987, train/loss=0.025950, train/mean_average_precision=0.517891, validation/accuracy=0.986871, validation/loss=0.044749, validation/mean_average_precision=0.273953, validation/num_examples=43793
I0205 04:26:38.440366 140388975236864 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.09534647315740585, loss=0.03244661167263985
I0205 04:27:10.417498 140290179761920 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.08988581597805023, loss=0.031757134944200516
I0205 04:27:42.133093 140388975236864 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.11166522651910782, loss=0.03154681995511055
I0205 04:28:13.970003 140290179761920 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.08876623213291168, loss=0.029843194410204887
I0205 04:28:45.687458 140388975236864 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.09790835529565811, loss=0.03400265425443649
I0205 04:29:17.413846 140290179761920 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.08508101105690002, loss=0.03133520111441612
I0205 04:29:49.567999 140388975236864 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.13053584098815918, loss=0.030704481527209282
I0205 04:30:17.669714 140451058161472 spec.py:321] Evaluating on the training split.
I0205 04:32:22.300542 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 04:32:25.394988 140451058161472 spec.py:349] Evaluating on the test split.
I0205 04:32:28.394123 140451058161472 submission_runner.py:408] Time since start: 18462.87s, 	Step: 36789, 	{'train/accuracy': 0.9918658137321472, 'train/loss': 0.026509465649724007, 'train/mean_average_precision': 0.4845961109288035, 'validation/accuracy': 0.9867196083068848, 'validation/loss': 0.045168228447437286, 'validation/mean_average_precision': 0.2734755096168028, 'validation/num_examples': 43793, 'test/accuracy': 0.9858894944190979, 'test/loss': 0.048092763870954514, 'test/mean_average_precision': 0.2523359419552278, 'test/num_examples': 43793, 'score': 11782.1854326725, 'total_duration': 18462.86779975891, 'accumulated_submission_time': 11782.1854326725, 'accumulated_eval_time': 6677.873930931091, 'accumulated_logging_time': 1.8053655624389648}
I0205 04:32:28.417098 140283530442496 logging_writer.py:48] [36789] accumulated_eval_time=6677.873931, accumulated_logging_time=1.805366, accumulated_submission_time=11782.185433, global_step=36789, preemption_count=0, score=11782.185433, test/accuracy=0.985889, test/loss=0.048093, test/mean_average_precision=0.252336, test/num_examples=43793, total_duration=18462.867800, train/accuracy=0.991866, train/loss=0.026509, train/mean_average_precision=0.484596, validation/accuracy=0.986720, validation/loss=0.045168, validation/mean_average_precision=0.273476, validation/num_examples=43793
I0205 04:32:32.309854 140283731310336 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.09597086906433105, loss=0.02946941927075386
I0205 04:33:04.226434 140283530442496 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.09487498551607132, loss=0.03176112473011017
I0205 04:33:36.348360 140283731310336 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.09818888455629349, loss=0.031726349145174026
I0205 04:34:12.239841 140283530442496 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.08129564672708511, loss=0.027280006557703018
I0205 04:34:44.902822 140283731310336 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.11350526660680771, loss=0.03112822026014328
I0205 04:35:17.383052 140283530442496 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.1786828190088272, loss=0.0355650931596756
I0205 04:35:48.877450 140283731310336 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.0956457257270813, loss=0.03125666454434395
I0205 04:36:20.576784 140283530442496 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.10149633139371872, loss=0.03090570867061615
I0205 04:36:28.496351 140451058161472 spec.py:321] Evaluating on the training split.
I0205 04:38:30.906522 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 04:38:33.899150 140451058161472 spec.py:349] Evaluating on the test split.
I0205 04:38:36.852184 140451058161472 submission_runner.py:408] Time since start: 18831.33s, 	Step: 37526, 	{'train/accuracy': 0.9918019771575928, 'train/loss': 0.026524484157562256, 'train/mean_average_precision': 0.48393254705927957, 'validation/accuracy': 0.9868669509887695, 'validation/loss': 0.04479661583900452, 'validation/mean_average_precision': 0.2795964258277546, 'validation/num_examples': 43793, 'test/accuracy': 0.9860280752182007, 'test/loss': 0.047872234135866165, 'test/mean_average_precision': 0.2616498358073807, 'test/num_examples': 43793, 'score': 12022.233164787292, 'total_duration': 18831.325871944427, 'accumulated_submission_time': 12022.233164787292, 'accumulated_eval_time': 6806.229717254639, 'accumulated_logging_time': 1.8392269611358643}
I0205 04:38:36.874697 140290179761920 logging_writer.py:48] [37526] accumulated_eval_time=6806.229717, accumulated_logging_time=1.839227, accumulated_submission_time=12022.233165, global_step=37526, preemption_count=0, score=12022.233165, test/accuracy=0.986028, test/loss=0.047872, test/mean_average_precision=0.261650, test/num_examples=43793, total_duration=18831.325872, train/accuracy=0.991802, train/loss=0.026524, train/mean_average_precision=0.483933, validation/accuracy=0.986867, validation/loss=0.044797, validation/mean_average_precision=0.279596, validation/num_examples=43793
I0205 04:39:00.915287 140388975236864 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.11184103786945343, loss=0.03275485336780548
I0205 04:39:33.275088 140290179761920 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.10994789749383926, loss=0.030090082436800003
I0205 04:40:04.961071 140388975236864 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.11073538661003113, loss=0.03151045739650726
I0205 04:40:37.195225 140290179761920 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.08435089886188507, loss=0.028887150809168816
I0205 04:41:09.269286 140388975236864 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.10683715343475342, loss=0.031751107424497604
I0205 04:41:41.488663 140290179761920 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.09749457985162735, loss=0.031167292967438698
I0205 04:42:13.520405 140388975236864 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.10230913758277893, loss=0.031510449945926666
I0205 04:42:36.952486 140451058161472 spec.py:321] Evaluating on the training split.
I0205 04:44:39.874510 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 04:44:44.088447 140451058161472 spec.py:349] Evaluating on the test split.
I0205 04:44:47.071281 140451058161472 submission_runner.py:408] Time since start: 19201.54s, 	Step: 38275, 	{'train/accuracy': 0.9918152093887329, 'train/loss': 0.026448266580700874, 'train/mean_average_precision': 0.4996172600643373, 'validation/accuracy': 0.9869120121002197, 'validation/loss': 0.04536278545856476, 'validation/mean_average_precision': 0.27353828843651545, 'validation/num_examples': 43793, 'test/accuracy': 0.9859615564346313, 'test/loss': 0.04834476113319397, 'test/mean_average_precision': 0.25540204635358305, 'test/num_examples': 43793, 'score': 12262.27923464775, 'total_duration': 19201.54480075836, 'accumulated_submission_time': 12262.27923464775, 'accumulated_eval_time': 6936.348298549652, 'accumulated_logging_time': 1.873117446899414}
I0205 04:44:47.094367 140283530442496 logging_writer.py:48] [38275] accumulated_eval_time=6936.348299, accumulated_logging_time=1.873117, accumulated_submission_time=12262.279235, global_step=38275, preemption_count=0, score=12262.279235, test/accuracy=0.985962, test/loss=0.048345, test/mean_average_precision=0.255402, test/num_examples=43793, total_duration=19201.544801, train/accuracy=0.991815, train/loss=0.026448, train/mean_average_precision=0.499617, validation/accuracy=0.986912, validation/loss=0.045363, validation/mean_average_precision=0.273538, validation/num_examples=43793
I0205 04:44:55.501989 140290188154624 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.08859124779701233, loss=0.03011566959321499
I0205 04:45:27.925919 140283530442496 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.10658007115125656, loss=0.029562434181571007
I0205 04:46:00.176078 140290188154624 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.10453693568706512, loss=0.027040161192417145
I0205 04:46:32.240691 140283530442496 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.0878123864531517, loss=0.03270316869020462
I0205 04:47:04.298959 140290188154624 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.08335383981466293, loss=0.027177181094884872
I0205 04:47:36.472159 140283530442496 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.09341610223054886, loss=0.027865365147590637
I0205 04:48:08.650075 140290188154624 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.09657476097345352, loss=0.029875420033931732
I0205 04:48:40.352754 140283530442496 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.0743810385465622, loss=0.028340723365545273
I0205 04:48:47.092393 140451058161472 spec.py:321] Evaluating on the training split.
I0205 04:50:48.320682 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 04:50:51.374440 140451058161472 spec.py:349] Evaluating on the test split.
I0205 04:50:54.394794 140451058161472 submission_runner.py:408] Time since start: 19568.87s, 	Step: 39022, 	{'train/accuracy': 0.9920036792755127, 'train/loss': 0.025848040357232094, 'train/mean_average_precision': 0.501135744574851, 'validation/accuracy': 0.9868023991584778, 'validation/loss': 0.0449364073574543, 'validation/mean_average_precision': 0.2725329770589841, 'validation/num_examples': 43793, 'test/accuracy': 0.9859805107116699, 'test/loss': 0.04781976342201233, 'test/mean_average_precision': 0.25807255582523925, 'test/num_examples': 43793, 'score': 12502.245649576187, 'total_duration': 19568.868483543396, 'accumulated_submission_time': 12502.245649576187, 'accumulated_eval_time': 7063.6506524086, 'accumulated_logging_time': 1.9074816703796387}
I0205 04:50:54.417801 140290179761920 logging_writer.py:48] [39022] accumulated_eval_time=7063.650652, accumulated_logging_time=1.907482, accumulated_submission_time=12502.245650, global_step=39022, preemption_count=0, score=12502.245650, test/accuracy=0.985981, test/loss=0.047820, test/mean_average_precision=0.258073, test/num_examples=43793, total_duration=19568.868484, train/accuracy=0.992004, train/loss=0.025848, train/mean_average_precision=0.501136, validation/accuracy=0.986802, validation/loss=0.044936, validation/mean_average_precision=0.272533, validation/num_examples=43793
I0205 04:51:19.718972 140388975236864 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.08804171532392502, loss=0.028951048851013184
I0205 04:51:51.523868 140290179761920 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.09282489120960236, loss=0.03177313879132271
I0205 04:52:23.585588 140388975236864 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.102279432117939, loss=0.03253094106912613
I0205 04:52:55.787207 140290179761920 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.1141076609492302, loss=0.029482213780283928
I0205 04:53:28.613392 140388975236864 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.09446939826011658, loss=0.02922973968088627
I0205 04:54:01.509730 140290179761920 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.09583308547735214, loss=0.0305583905428648
I0205 04:54:34.490399 140388975236864 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.14000654220581055, loss=0.02895045280456543
I0205 04:54:54.711957 140451058161472 spec.py:321] Evaluating on the training split.
I0205 04:57:00.153216 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 04:57:03.445060 140451058161472 spec.py:349] Evaluating on the test split.
I0205 04:57:06.503202 140451058161472 submission_runner.py:408] Time since start: 19940.98s, 	Step: 39763, 	{'train/accuracy': 0.9920294880867004, 'train/loss': 0.025563078001141548, 'train/mean_average_precision': 0.5300515347756143, 'validation/accuracy': 0.9869067668914795, 'validation/loss': 0.04511991888284683, 'validation/mean_average_precision': 0.2812240884105178, 'validation/num_examples': 43793, 'test/accuracy': 0.9859746098518372, 'test/loss': 0.048158034682273865, 'test/mean_average_precision': 0.25843331896107397, 'test/num_examples': 43793, 'score': 12742.505630731583, 'total_duration': 19940.97686815262, 'accumulated_submission_time': 12742.505630731583, 'accumulated_eval_time': 7195.441838502884, 'accumulated_logging_time': 1.942678689956665}
I0205 04:57:06.526447 140283530442496 logging_writer.py:48] [39763] accumulated_eval_time=7195.441839, accumulated_logging_time=1.942679, accumulated_submission_time=12742.505631, global_step=39763, preemption_count=0, score=12742.505631, test/accuracy=0.985975, test/loss=0.048158, test/mean_average_precision=0.258433, test/num_examples=43793, total_duration=19940.976868, train/accuracy=0.992029, train/loss=0.025563, train/mean_average_precision=0.530052, validation/accuracy=0.986907, validation/loss=0.045120, validation/mean_average_precision=0.281224, validation/num_examples=43793
I0205 04:57:18.712323 140290188154624 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.09369291365146637, loss=0.03075854666531086
I0205 04:57:51.112362 140283530442496 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.0858924463391304, loss=0.030168093740940094
I0205 04:58:23.133175 140290188154624 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.12109025567770004, loss=0.030997714027762413
I0205 04:58:55.051627 140283530442496 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.09138567000627518, loss=0.028750212863087654
I0205 04:59:27.169494 140290188154624 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.14983554184436798, loss=0.03164780139923096
I0205 04:59:58.995641 140283530442496 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.11572536826133728, loss=0.030090441927313805
I0205 05:00:31.705228 140290188154624 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.08274930715560913, loss=0.028730446472764015
I0205 05:01:04.274841 140283530442496 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.09714110195636749, loss=0.031957369297742844
I0205 05:01:06.508318 140451058161472 spec.py:321] Evaluating on the training split.
I0205 05:03:08.747969 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 05:03:11.788895 140451058161472 spec.py:349] Evaluating on the test split.
I0205 05:03:14.830024 140451058161472 submission_runner.py:408] Time since start: 20309.30s, 	Step: 40508, 	{'train/accuracy': 0.9923818707466125, 'train/loss': 0.024617204442620277, 'train/mean_average_precision': 0.531692979601829, 'validation/accuracy': 0.9868564009666443, 'validation/loss': 0.045163314789533615, 'validation/mean_average_precision': 0.27720460844275613, 'validation/num_examples': 43793, 'test/accuracy': 0.9860655665397644, 'test/loss': 0.04806293919682503, 'test/mean_average_precision': 0.2669514125648769, 'test/num_examples': 43793, 'score': 12982.455196619034, 'total_duration': 20309.30371117592, 'accumulated_submission_time': 12982.455196619034, 'accumulated_eval_time': 7323.763496160507, 'accumulated_logging_time': 1.9770872592926025}
I0205 05:03:14.853449 140290179761920 logging_writer.py:48] [40508] accumulated_eval_time=7323.763496, accumulated_logging_time=1.977087, accumulated_submission_time=12982.455197, global_step=40508, preemption_count=0, score=12982.455197, test/accuracy=0.986066, test/loss=0.048063, test/mean_average_precision=0.266951, test/num_examples=43793, total_duration=20309.303711, train/accuracy=0.992382, train/loss=0.024617, train/mean_average_precision=0.531693, validation/accuracy=0.986856, validation/loss=0.045163, validation/mean_average_precision=0.277205, validation/num_examples=43793
I0205 05:03:44.796591 140388975236864 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.12304197996854782, loss=0.032706502825021744
I0205 05:04:16.995455 140290179761920 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.10092341154813766, loss=0.02849891036748886
I0205 05:04:49.340168 140388975236864 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.10694681107997894, loss=0.02956208772957325
I0205 05:05:21.651531 140290179761920 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.09905735403299332, loss=0.030900662764906883
I0205 05:05:53.444612 140388975236864 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.09278912097215652, loss=0.028686178848147392
I0205 05:06:25.506918 140290179761920 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.10672822594642639, loss=0.0279343631118536
I0205 05:06:57.316089 140388975236864 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.11316460371017456, loss=0.027790488675236702
I0205 05:07:15.042112 140451058161472 spec.py:321] Evaluating on the training split.
I0205 05:09:20.109836 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 05:09:23.188596 140451058161472 spec.py:349] Evaluating on the test split.
I0205 05:09:26.272823 140451058161472 submission_runner.py:408] Time since start: 20680.75s, 	Step: 41256, 	{'train/accuracy': 0.9925772547721863, 'train/loss': 0.024118391796946526, 'train/mean_average_precision': 0.5373471320030823, 'validation/accuracy': 0.986707866191864, 'validation/loss': 0.04529014974832535, 'validation/mean_average_precision': 0.27485737752421924, 'validation/num_examples': 43793, 'test/accuracy': 0.9858545660972595, 'test/loss': 0.04817128926515579, 'test/mean_average_precision': 0.25741862070010346, 'test/num_examples': 43793, 'score': 13222.612513780594, 'total_duration': 20680.74651002884, 'accumulated_submission_time': 13222.612513780594, 'accumulated_eval_time': 7454.994160413742, 'accumulated_logging_time': 2.011096239089966}
I0205 05:09:26.295877 140283731310336 logging_writer.py:48] [41256] accumulated_eval_time=7454.994160, accumulated_logging_time=2.011096, accumulated_submission_time=13222.612514, global_step=41256, preemption_count=0, score=13222.612514, test/accuracy=0.985855, test/loss=0.048171, test/mean_average_precision=0.257419, test/num_examples=43793, total_duration=20680.746510, train/accuracy=0.992577, train/loss=0.024118, train/mean_average_precision=0.537347, validation/accuracy=0.986708, validation/loss=0.045290, validation/mean_average_precision=0.274857, validation/num_examples=43793
I0205 05:09:40.585765 140290188154624 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.09182367473840714, loss=0.03119642473757267
I0205 05:10:13.057590 140283731310336 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.0965588241815567, loss=0.03231615573167801
I0205 05:10:45.156888 140290188154624 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.10830951482057571, loss=0.030625561252236366
I0205 05:11:17.140761 140283731310336 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.09774460643529892, loss=0.0286245197057724
I0205 05:11:49.645670 140290188154624 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.11614898592233658, loss=0.028833461925387383
I0205 05:12:21.869136 140283731310336 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.13774481415748596, loss=0.027790740132331848
I0205 05:12:53.666907 140290188154624 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.09863603860139847, loss=0.03158213570713997
I0205 05:13:25.589167 140283731310336 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.11764039844274521, loss=0.029743708670139313
I0205 05:13:26.568786 140451058161472 spec.py:321] Evaluating on the training split.
I0205 05:15:31.567993 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 05:15:34.680957 140451058161472 spec.py:349] Evaluating on the test split.
I0205 05:15:37.736258 140451058161472 submission_runner.py:408] Time since start: 21052.21s, 	Step: 42004, 	{'train/accuracy': 0.99293053150177, 'train/loss': 0.023048240691423416, 'train/mean_average_precision': 0.5792572161274165, 'validation/accuracy': 0.9867269396781921, 'validation/loss': 0.04519191011786461, 'validation/mean_average_precision': 0.27773206183410876, 'validation/num_examples': 43793, 'test/accuracy': 0.9858461618423462, 'test/loss': 0.04808211699128151, 'test/mean_average_precision': 0.26245633353733167, 'test/num_examples': 43793, 'score': 13462.853985786438, 'total_duration': 21052.20994591713, 'accumulated_submission_time': 13462.853985786438, 'accumulated_eval_time': 7586.161582946777, 'accumulated_logging_time': 2.044940948486328}
I0205 05:15:37.759900 140283530442496 logging_writer.py:48] [42004] accumulated_eval_time=7586.161583, accumulated_logging_time=2.044941, accumulated_submission_time=13462.853986, global_step=42004, preemption_count=0, score=13462.853986, test/accuracy=0.985846, test/loss=0.048082, test/mean_average_precision=0.262456, test/num_examples=43793, total_duration=21052.209946, train/accuracy=0.992931, train/loss=0.023048, train/mean_average_precision=0.579257, validation/accuracy=0.986727, validation/loss=0.045192, validation/mean_average_precision=0.277732, validation/num_examples=43793
I0205 05:16:09.098459 140388975236864 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.0944928377866745, loss=0.029776133596897125
I0205 05:16:41.676509 140283530442496 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.09398756176233292, loss=0.029640033841133118
I0205 05:17:13.859006 140388975236864 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.11546482145786285, loss=0.02994811348617077
I0205 05:17:45.993561 140283530442496 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.11203163862228394, loss=0.028508111834526062
I0205 05:18:18.886744 140388975236864 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.10847079008817673, loss=0.02678973600268364
I0205 05:18:51.541478 140283530442496 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.10226138681173325, loss=0.029940901324152946
I0205 05:19:23.861212 140388975236864 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.09915038198232651, loss=0.02985134907066822
I0205 05:19:38.005475 140451058161472 spec.py:321] Evaluating on the training split.
I0205 05:21:42.688890 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 05:21:45.838002 140451058161472 spec.py:349] Evaluating on the test split.
I0205 05:21:48.930570 140451058161472 submission_runner.py:408] Time since start: 21423.40s, 	Step: 42744, 	{'train/accuracy': 0.9928200840950012, 'train/loss': 0.023367619141936302, 'train/mean_average_precision': 0.5596890151487883, 'validation/accuracy': 0.9868795275688171, 'validation/loss': 0.04527511075139046, 'validation/mean_average_precision': 0.2799710175790842, 'validation/num_examples': 43793, 'test/accuracy': 0.9859805107116699, 'test/loss': 0.04827718809247017, 'test/mean_average_precision': 0.2657243764067328, 'test/num_examples': 43793, 'score': 13703.067671060562, 'total_duration': 21423.404257774353, 'accumulated_submission_time': 13703.067671060562, 'accumulated_eval_time': 7717.086632013321, 'accumulated_logging_time': 2.079592227935791}
I0205 05:21:48.954420 140283731310336 logging_writer.py:48] [42744] accumulated_eval_time=7717.086632, accumulated_logging_time=2.079592, accumulated_submission_time=13703.067671, global_step=42744, preemption_count=0, score=13703.067671, test/accuracy=0.985981, test/loss=0.048277, test/mean_average_precision=0.265724, test/num_examples=43793, total_duration=21423.404258, train/accuracy=0.992820, train/loss=0.023368, train/mean_average_precision=0.559689, validation/accuracy=0.986880, validation/loss=0.045275, validation/mean_average_precision=0.279971, validation/num_examples=43793
I0205 05:22:07.582209 140290188154624 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.10655006766319275, loss=0.02967897057533264
I0205 05:22:39.618314 140283731310336 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.12554045021533966, loss=0.032976679503917694
I0205 05:23:11.550190 140290188154624 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.12091503292322159, loss=0.029100827872753143
I0205 05:23:43.385870 140283731310336 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.10149319469928741, loss=0.03100624680519104
I0205 05:24:15.518339 140290188154624 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.11186371743679047, loss=0.030123203992843628
I0205 05:24:47.625769 140283731310336 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.13248680531978607, loss=0.02828114666044712
I0205 05:25:19.905721 140290188154624 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.10168598592281342, loss=0.029366906732320786
I0205 05:25:49.155416 140451058161472 spec.py:321] Evaluating on the training split.
I0205 05:27:51.769833 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 05:27:54.831758 140451058161472 spec.py:349] Evaluating on the test split.
I0205 05:27:57.872034 140451058161472 submission_runner.py:408] Time since start: 21792.35s, 	Step: 43492, 	{'train/accuracy': 0.9925847053527832, 'train/loss': 0.023969653993844986, 'train/mean_average_precision': 0.5528363329090942, 'validation/accuracy': 0.9869071245193481, 'validation/loss': 0.04544226452708244, 'validation/mean_average_precision': 0.2818758340362011, 'validation/num_examples': 43793, 'test/accuracy': 0.9860390424728394, 'test/loss': 0.048316504806280136, 'test/mean_average_precision': 0.2653350767236811, 'test/num_examples': 43793, 'score': 13943.23703622818, 'total_duration': 21792.345589876175, 'accumulated_submission_time': 13943.23703622818, 'accumulated_eval_time': 7845.803071022034, 'accumulated_logging_time': 2.1141724586486816}
I0205 05:27:57.895530 140283530442496 logging_writer.py:48] [43492] accumulated_eval_time=7845.803071, accumulated_logging_time=2.114172, accumulated_submission_time=13943.237036, global_step=43492, preemption_count=0, score=13943.237036, test/accuracy=0.986039, test/loss=0.048317, test/mean_average_precision=0.265335, test/num_examples=43793, total_duration=21792.345590, train/accuracy=0.992585, train/loss=0.023970, train/mean_average_precision=0.552836, validation/accuracy=0.986907, validation/loss=0.045442, validation/mean_average_precision=0.281876, validation/num_examples=43793
I0205 05:28:00.900331 140290179761920 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.10047795623540878, loss=0.030017327517271042
I0205 05:28:33.435949 140283530442496 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.10073010623455048, loss=0.03026038035750389
I0205 05:29:05.602557 140290179761920 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.11754682660102844, loss=0.02842981554567814
I0205 05:29:37.101624 140283530442496 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.12726517021656036, loss=0.03100203536450863
I0205 05:30:08.776628 140290179761920 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.10582676529884338, loss=0.030123230069875717
I0205 05:30:40.627647 140283530442496 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.13318295776844025, loss=0.03149300813674927
I0205 05:31:12.452174 140290179761920 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.11582502722740173, loss=0.028817154467105865
I0205 05:31:44.261152 140283530442496 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.10474339127540588, loss=0.028076866641640663
I0205 05:31:58.161402 140451058161472 spec.py:321] Evaluating on the training split.
I0205 05:34:01.959461 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 05:34:05.063373 140451058161472 spec.py:349] Evaluating on the test split.
I0205 05:34:08.067028 140451058161472 submission_runner.py:408] Time since start: 22162.54s, 	Step: 44245, 	{'train/accuracy': 0.9922870993614197, 'train/loss': 0.024914905428886414, 'train/mean_average_precision': 0.5299943259670108, 'validation/accuracy': 0.9868738651275635, 'validation/loss': 0.045727986842393875, 'validation/mean_average_precision': 0.27316044796499767, 'validation/num_examples': 43793, 'test/accuracy': 0.9859662055969238, 'test/loss': 0.048688486218452454, 'test/mean_average_precision': 0.25902558919256746, 'test/num_examples': 43793, 'score': 14183.470051765442, 'total_duration': 22162.540717601776, 'accumulated_submission_time': 14183.470051765442, 'accumulated_eval_time': 7975.708652496338, 'accumulated_logging_time': 2.1499216556549072}
I0205 05:34:08.090621 140290188154624 logging_writer.py:48] [44245] accumulated_eval_time=7975.708652, accumulated_logging_time=2.149922, accumulated_submission_time=14183.470052, global_step=44245, preemption_count=0, score=14183.470052, test/accuracy=0.985966, test/loss=0.048688, test/mean_average_precision=0.259026, test/num_examples=43793, total_duration=22162.540718, train/accuracy=0.992287, train/loss=0.024915, train/mean_average_precision=0.529994, validation/accuracy=0.986874, validation/loss=0.045728, validation/mean_average_precision=0.273160, validation/num_examples=43793
I0205 05:34:26.180503 140388975236864 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.10205388069152832, loss=0.02750418893992901
I0205 05:34:57.832937 140290188154624 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.10701459646224976, loss=0.0318475067615509
I0205 05:35:29.562449 140388975236864 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.11124878376722336, loss=0.02981453761458397
I0205 05:36:01.690197 140290188154624 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.12028392404317856, loss=0.027524245902895927
I0205 05:36:33.487262 140388975236864 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.1027485579252243, loss=0.029657142236828804
I0205 05:37:05.250703 140290188154624 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.11636369675397873, loss=0.030535610392689705
I0205 05:37:36.719542 140388975236864 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.11235189437866211, loss=0.027903346344828606
I0205 05:38:08.315891 140290188154624 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.11915649473667145, loss=0.030515575781464577
I0205 05:38:08.321844 140451058161472 spec.py:321] Evaluating on the training split.
I0205 05:40:06.462536 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 05:40:09.503699 140451058161472 spec.py:349] Evaluating on the test split.
I0205 05:40:12.484030 140451058161472 submission_runner.py:408] Time since start: 22526.96s, 	Step: 45001, 	{'train/accuracy': 0.9922727942466736, 'train/loss': 0.024703234434127808, 'train/mean_average_precision': 0.5314834003758118, 'validation/accuracy': 0.9867748022079468, 'validation/loss': 0.04558992013335228, 'validation/mean_average_precision': 0.2845712141946099, 'validation/num_examples': 43793, 'test/accuracy': 0.9859042763710022, 'test/loss': 0.04851120710372925, 'test/mean_average_precision': 0.2633550920425738, 'test/num_examples': 43793, 'score': 14423.669402837753, 'total_duration': 22526.95771765709, 'accumulated_submission_time': 14423.669402837753, 'accumulated_eval_time': 8099.870770931244, 'accumulated_logging_time': 2.1851043701171875}
I0205 05:40:12.508188 140283530442496 logging_writer.py:48] [45001] accumulated_eval_time=8099.870771, accumulated_logging_time=2.185104, accumulated_submission_time=14423.669403, global_step=45001, preemption_count=0, score=14423.669403, test/accuracy=0.985904, test/loss=0.048511, test/mean_average_precision=0.263355, test/num_examples=43793, total_duration=22526.957718, train/accuracy=0.992273, train/loss=0.024703, train/mean_average_precision=0.531483, validation/accuracy=0.986775, validation/loss=0.045590, validation/mean_average_precision=0.284571, validation/num_examples=43793
I0205 05:40:44.902193 140283731310336 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.11336126178503036, loss=0.031212231144309044
I0205 05:41:17.525933 140283530442496 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.1122203841805458, loss=0.027411306276917458
I0205 05:41:49.257540 140283731310336 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.09951047599315643, loss=0.029144801199436188
I0205 05:42:21.128474 140283530442496 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.12354467064142227, loss=0.028508802875876427
I0205 05:42:52.880759 140283731310336 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.14233030378818512, loss=0.02978626638650894
I0205 05:43:24.895293 140283530442496 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.10821548849344254, loss=0.027560286223888397
I0205 05:43:56.554407 140283731310336 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.13746823370456696, loss=0.03010975383222103
I0205 05:44:12.692017 140451058161472 spec.py:321] Evaluating on the training split.
I0205 05:46:12.506453 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 05:46:15.576483 140451058161472 spec.py:349] Evaluating on the test split.
I0205 05:46:18.642145 140451058161472 submission_runner.py:408] Time since start: 22893.12s, 	Step: 45751, 	{'train/accuracy': 0.9925000667572021, 'train/loss': 0.023962808772921562, 'train/mean_average_precision': 0.5480373339681623, 'validation/accuracy': 0.9867601990699768, 'validation/loss': 0.04606109485030174, 'validation/mean_average_precision': 0.27701092953188267, 'validation/num_examples': 43793, 'test/accuracy': 0.9860045313835144, 'test/loss': 0.048934582620859146, 'test/mean_average_precision': 0.2583359577865653, 'test/num_examples': 43793, 'score': 14663.82266998291, 'total_duration': 22893.115824699402, 'accumulated_submission_time': 14663.82266998291, 'accumulated_eval_time': 8225.82084441185, 'accumulated_logging_time': 2.220059871673584}
I0205 05:46:18.666146 140290179761920 logging_writer.py:48] [45751] accumulated_eval_time=8225.820844, accumulated_logging_time=2.220060, accumulated_submission_time=14663.822670, global_step=45751, preemption_count=0, score=14663.822670, test/accuracy=0.986005, test/loss=0.048935, test/mean_average_precision=0.258336, test/num_examples=43793, total_duration=22893.115825, train/accuracy=0.992500, train/loss=0.023963, train/mean_average_precision=0.548037, validation/accuracy=0.986760, validation/loss=0.046061, validation/mean_average_precision=0.277011, validation/num_examples=43793
I0205 05:46:34.688423 140290188154624 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.12238146364688873, loss=0.027605390176177025
I0205 05:47:06.820556 140290179761920 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.12490188330411911, loss=0.029048435389995575
I0205 05:47:38.603661 140290188154624 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.11784008890390396, loss=0.029295580461621284
I0205 05:48:10.692888 140290179761920 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.10334254056215286, loss=0.02991102822124958
I0205 05:48:42.373223 140290188154624 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.10751648992300034, loss=0.02462160587310791
I0205 05:49:14.540559 140290179761920 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.11264495551586151, loss=0.029959995299577713
I0205 05:49:46.861075 140290188154624 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.11539909988641739, loss=0.02882547676563263
I0205 05:50:18.824213 140290179761920 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.11345452070236206, loss=0.02749590203166008
I0205 05:50:18.829371 140451058161472 spec.py:321] Evaluating on the training split.
I0205 05:52:21.933770 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 05:52:24.991075 140451058161472 spec.py:349] Evaluating on the test split.
I0205 05:52:27.968202 140451058161472 submission_runner.py:408] Time since start: 23262.44s, 	Step: 46501, 	{'train/accuracy': 0.9925301671028137, 'train/loss': 0.023875262588262558, 'train/mean_average_precision': 0.5455075725286589, 'validation/accuracy': 0.9867683053016663, 'validation/loss': 0.04635896533727646, 'validation/mean_average_precision': 0.2753539428337459, 'validation/num_examples': 43793, 'test/accuracy': 0.9858596324920654, 'test/loss': 0.04966329410672188, 'test/mean_average_precision': 0.2589811391864166, 'test/num_examples': 43793, 'score': 14903.953869581223, 'total_duration': 23262.441887378693, 'accumulated_submission_time': 14903.953869581223, 'accumulated_eval_time': 8354.959605455399, 'accumulated_logging_time': 2.2551698684692383}
I0205 05:52:27.992407 140283530442496 logging_writer.py:48] [46501] accumulated_eval_time=8354.959605, accumulated_logging_time=2.255170, accumulated_submission_time=14903.953870, global_step=46501, preemption_count=0, score=14903.953870, test/accuracy=0.985860, test/loss=0.049663, test/mean_average_precision=0.258981, test/num_examples=43793, total_duration=23262.441887, train/accuracy=0.992530, train/loss=0.023875, train/mean_average_precision=0.545508, validation/accuracy=0.986768, validation/loss=0.046359, validation/mean_average_precision=0.275354, validation/num_examples=43793
I0205 05:52:59.743041 140283731310336 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.1100669652223587, loss=0.029139548540115356
I0205 05:53:31.536757 140283530442496 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.12429305911064148, loss=0.02730856090784073
I0205 05:54:03.059673 140283731310336 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.10605540126562119, loss=0.027686357498168945
I0205 05:54:34.642647 140283530442496 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.11623810231685638, loss=0.028433190658688545
I0205 05:55:06.582709 140283731310336 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.13327383995056152, loss=0.029152430593967438
I0205 05:55:38.922357 140283530442496 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.12868598103523254, loss=0.029649049043655396
I0205 05:56:11.482259 140283731310336 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.11981620639562607, loss=0.028129419311881065
I0205 05:56:28.140876 140451058161472 spec.py:321] Evaluating on the training split.
I0205 05:58:33.025682 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 05:58:36.074986 140451058161472 spec.py:349] Evaluating on the test split.
I0205 05:58:39.092526 140451058161472 submission_runner.py:408] Time since start: 23633.57s, 	Step: 47254, 	{'train/accuracy': 0.9927741289138794, 'train/loss': 0.023025471717119217, 'train/mean_average_precision': 0.5728452454270465, 'validation/accuracy': 0.9868178367614746, 'validation/loss': 0.04605264961719513, 'validation/mean_average_precision': 0.2810175965239018, 'validation/num_examples': 43793, 'test/accuracy': 0.9858705401420593, 'test/loss': 0.04945741966366768, 'test/mean_average_precision': 0.254589348944634, 'test/num_examples': 43793, 'score': 15144.071580171585, 'total_duration': 23633.566210269928, 'accumulated_submission_time': 15144.071580171585, 'accumulated_eval_time': 8485.91120505333, 'accumulated_logging_time': 2.2899508476257324}
I0205 05:58:39.116940 140283262007040 logging_writer.py:48] [47254] accumulated_eval_time=8485.911205, accumulated_logging_time=2.289951, accumulated_submission_time=15144.071580, global_step=47254, preemption_count=0, score=15144.071580, test/accuracy=0.985871, test/loss=0.049457, test/mean_average_precision=0.254589, test/num_examples=43793, total_duration=23633.566210, train/accuracy=0.992774, train/loss=0.023025, train/mean_average_precision=0.572845, validation/accuracy=0.986818, validation/loss=0.046053, validation/mean_average_precision=0.281018, validation/num_examples=43793
I0205 05:58:54.376610 140290188154624 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.11030925065279007, loss=0.029788291081786156
I0205 05:59:26.047284 140283262007040 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.1129143014550209, loss=0.02769317477941513
I0205 05:59:57.398359 140290188154624 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.10830981284379959, loss=0.0287614855915308
I0205 06:00:29.237699 140283262007040 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.1109449490904808, loss=0.02655763179063797
I0205 06:01:00.630590 140290188154624 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.11546847969293594, loss=0.02650752104818821
I0205 06:01:32.276232 140283262007040 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.14484749734401703, loss=0.02922399714589119
I0205 06:02:03.874165 140290188154624 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.1353723406791687, loss=0.031265467405319214
I0205 06:02:35.198122 140283262007040 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.12364783883094788, loss=0.02651444636285305
I0205 06:02:39.362306 140451058161472 spec.py:321] Evaluating on the training split.
I0205 06:04:39.176013 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 06:04:42.216139 140451058161472 spec.py:349] Evaluating on the test split.
I0205 06:04:45.212574 140451058161472 submission_runner.py:408] Time since start: 23999.69s, 	Step: 48014, 	{'train/accuracy': 0.993111252784729, 'train/loss': 0.02199796587228775, 'train/mean_average_precision': 0.6028270478090696, 'validation/accuracy': 0.9867350459098816, 'validation/loss': 0.04650464653968811, 'validation/mean_average_precision': 0.27606598867261145, 'validation/num_examples': 43793, 'test/accuracy': 0.9859164953231812, 'test/loss': 0.04958193749189377, 'test/mean_average_precision': 0.2659727477785244, 'test/num_examples': 43793, 'score': 15384.28563117981, 'total_duration': 23999.686259269714, 'accumulated_submission_time': 15384.28563117981, 'accumulated_eval_time': 8611.761420249939, 'accumulated_logging_time': 2.3253207206726074}
I0205 06:04:45.237172 140283530442496 logging_writer.py:48] [48014] accumulated_eval_time=8611.761420, accumulated_logging_time=2.325321, accumulated_submission_time=15384.285631, global_step=48014, preemption_count=0, score=15384.285631, test/accuracy=0.985916, test/loss=0.049582, test/mean_average_precision=0.265973, test/num_examples=43793, total_duration=23999.686259, train/accuracy=0.993111, train/loss=0.021998, train/mean_average_precision=0.602827, validation/accuracy=0.986735, validation/loss=0.046505, validation/mean_average_precision=0.276066, validation/num_examples=43793
I0205 06:05:12.926752 140283731310336 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.11587297916412354, loss=0.02682872861623764
I0205 06:05:44.343431 140283530442496 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.12768618762493134, loss=0.028752457350492477
I0205 06:06:15.868862 140283731310336 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.12881235778331757, loss=0.028242679312825203
I0205 06:06:47.337212 140283530442496 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.14707544445991516, loss=0.026789655908942223
I0205 06:07:19.088903 140283731310336 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.1191881075501442, loss=0.02791954018175602
I0205 06:07:50.345327 140283530442496 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.13729418814182281, loss=0.028972528874874115
I0205 06:08:22.050862 140283731310336 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.13117454946041107, loss=0.027890946716070175
I0205 06:08:45.282125 140451058161472 spec.py:321] Evaluating on the training split.
I0205 06:10:41.887506 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 06:10:44.913326 140451058161472 spec.py:349] Evaluating on the test split.
I0205 06:10:47.883207 140451058161472 submission_runner.py:408] Time since start: 24362.36s, 	Step: 48776, 	{'train/accuracy': 0.9933006167411804, 'train/loss': 0.021312803030014038, 'train/mean_average_precision': 0.6074992806664145, 'validation/accuracy': 0.9868174195289612, 'validation/loss': 0.04697003215551376, 'validation/mean_average_precision': 0.27183009617920206, 'validation/num_examples': 43793, 'test/accuracy': 0.9858773350715637, 'test/loss': 0.05038075894117355, 'test/mean_average_precision': 0.2556186798312245, 'test/num_examples': 43793, 'score': 15624.298836946487, 'total_duration': 24362.35687804222, 'accumulated_submission_time': 15624.298836946487, 'accumulated_eval_time': 8734.362447023392, 'accumulated_logging_time': 2.3620529174804688}
I0205 06:10:47.907715 140283262007040 logging_writer.py:48] [48776] accumulated_eval_time=8734.362447, accumulated_logging_time=2.362053, accumulated_submission_time=15624.298837, global_step=48776, preemption_count=0, score=15624.298837, test/accuracy=0.985877, test/loss=0.050381, test/mean_average_precision=0.255619, test/num_examples=43793, total_duration=24362.356878, train/accuracy=0.993301, train/loss=0.021313, train/mean_average_precision=0.607499, validation/accuracy=0.986817, validation/loss=0.046970, validation/mean_average_precision=0.271830, validation/num_examples=43793
I0205 06:10:55.755960 140290188154624 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.12032272666692734, loss=0.026039572432637215
I0205 06:11:27.011928 140283262007040 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.1700810343027115, loss=0.03232695162296295
I0205 06:11:58.468540 140290188154624 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.12437357753515244, loss=0.027233798056840897
I0205 06:12:30.593478 140283262007040 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.1394672691822052, loss=0.029471229761838913
I0205 06:13:02.817021 140290188154624 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.12469816207885742, loss=0.02708461880683899
I0205 06:13:35.503226 140283262007040 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.12435104697942734, loss=0.029357071965932846
I0205 06:14:08.518163 140290188154624 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.13579519093036652, loss=0.028166724368929863
I0205 06:14:41.187412 140283262007040 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.12344687432050705, loss=0.026645272970199585
I0205 06:14:48.072970 140451058161472 spec.py:321] Evaluating on the training split.
I0205 06:16:48.568212 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 06:16:51.631374 140451058161472 spec.py:349] Evaluating on the test split.
I0205 06:16:54.631063 140451058161472 submission_runner.py:408] Time since start: 24729.10s, 	Step: 49522, 	{'train/accuracy': 0.9936766624450684, 'train/loss': 0.02044110931456089, 'train/mean_average_precision': 0.62012147547647, 'validation/accuracy': 0.9867147207260132, 'validation/loss': 0.04665745049715042, 'validation/mean_average_precision': 0.27573759577391055, 'validation/num_examples': 43793, 'test/accuracy': 0.9858027696609497, 'test/loss': 0.04974200949072838, 'test/mean_average_precision': 0.25762268551794937, 'test/num_examples': 43793, 'score': 15864.43145275116, 'total_duration': 24729.104751110077, 'accumulated_submission_time': 15864.43145275116, 'accumulated_eval_time': 8860.920503616333, 'accumulated_logging_time': 2.397468328475952}
I0205 06:16:54.656362 140283530442496 logging_writer.py:48] [49522] accumulated_eval_time=8860.920504, accumulated_logging_time=2.397468, accumulated_submission_time=15864.431453, global_step=49522, preemption_count=0, score=15864.431453, test/accuracy=0.985803, test/loss=0.049742, test/mean_average_precision=0.257623, test/num_examples=43793, total_duration=24729.104751, train/accuracy=0.993677, train/loss=0.020441, train/mean_average_precision=0.620121, validation/accuracy=0.986715, validation/loss=0.046657, validation/mean_average_precision=0.275738, validation/num_examples=43793
I0205 06:17:19.984739 140283731310336 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.15264081954956055, loss=0.027361063286662102
I0205 06:17:51.965759 140283530442496 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.13634836673736572, loss=0.02707890421152115
I0205 06:18:24.026898 140283731310336 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.1393723338842392, loss=0.028405027464032173
I0205 06:18:58.143847 140283530442496 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.1524057537317276, loss=0.02906675823032856
I0205 06:19:30.160979 140283731310336 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.1327144205570221, loss=0.028288239613175392
I0205 06:20:01.879522 140283530442496 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.13011445105075836, loss=0.027126815170049667
I0205 06:20:33.677856 140283731310336 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.11050071567296982, loss=0.02660919353365898
I0205 06:20:54.648102 140451058161472 spec.py:321] Evaluating on the training split.
I0205 06:22:57.042750 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 06:23:00.117273 140451058161472 spec.py:349] Evaluating on the test split.
I0205 06:23:03.351091 140451058161472 submission_runner.py:408] Time since start: 25097.82s, 	Step: 50268, 	{'train/accuracy': 0.9936593770980835, 'train/loss': 0.020675519481301308, 'train/mean_average_precision': 0.6200574946558495, 'validation/accuracy': 0.9865986108779907, 'validation/loss': 0.04682053625583649, 'validation/mean_average_precision': 0.2705907601746179, 'validation/num_examples': 43793, 'test/accuracy': 0.9857581257820129, 'test/loss': 0.04971133917570114, 'test/mean_average_precision': 0.25703920658683305, 'test/num_examples': 43793, 'score': 16104.392055511475, 'total_duration': 25097.82476592064, 'accumulated_submission_time': 16104.392055511475, 'accumulated_eval_time': 8989.623442411423, 'accumulated_logging_time': 2.433577060699463}
I0205 06:23:03.376883 140283262007040 logging_writer.py:48] [50268] accumulated_eval_time=8989.623442, accumulated_logging_time=2.433577, accumulated_submission_time=16104.392056, global_step=50268, preemption_count=0, score=16104.392056, test/accuracy=0.985758, test/loss=0.049711, test/mean_average_precision=0.257039, test/num_examples=43793, total_duration=25097.824766, train/accuracy=0.993659, train/loss=0.020676, train/mean_average_precision=0.620057, validation/accuracy=0.986599, validation/loss=0.046821, validation/mean_average_precision=0.270591, validation/num_examples=43793
I0205 06:23:13.946786 140290188154624 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.12223342806100845, loss=0.02525516413152218
I0205 06:23:45.403965 140283262007040 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.1255684345960617, loss=0.0260388795286417
I0205 06:24:17.340548 140290188154624 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.1700351983308792, loss=0.029224691912531853
I0205 06:24:48.820182 140283262007040 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.14246058464050293, loss=0.027697216719388962
I0205 06:25:20.514955 140290188154624 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.1271054446697235, loss=0.02786199562251568
I0205 06:25:52.132997 140283262007040 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.12307210266590118, loss=0.02674478106200695
I0205 06:26:24.393004 140290188154624 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.12585711479187012, loss=0.025823546573519707
I0205 06:26:56.533687 140283262007040 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.1110653281211853, loss=0.025869179517030716
I0205 06:27:03.432613 140451058161472 spec.py:321] Evaluating on the training split.
I0205 06:29:01.342526 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 06:29:04.548761 140451058161472 spec.py:349] Evaluating on the test split.
I0205 06:29:07.567936 140451058161472 submission_runner.py:408] Time since start: 25462.04s, 	Step: 51022, 	{'train/accuracy': 0.9933759570121765, 'train/loss': 0.02125820517539978, 'train/mean_average_precision': 0.6098886010964394, 'validation/accuracy': 0.9867504835128784, 'validation/loss': 0.04702604562044144, 'validation/mean_average_precision': 0.27266494475688524, 'validation/num_examples': 43793, 'test/accuracy': 0.9858331084251404, 'test/loss': 0.050172753632068634, 'test/mean_average_precision': 0.2515554863987931, 'test/num_examples': 43793, 'score': 16344.416038274765, 'total_duration': 25462.041620969772, 'accumulated_submission_time': 16344.416038274765, 'accumulated_eval_time': 9113.758713245392, 'accumulated_logging_time': 2.470756769180298}
I0205 06:29:07.593847 140283731310336 logging_writer.py:48] [51022] accumulated_eval_time=9113.758713, accumulated_logging_time=2.470757, accumulated_submission_time=16344.416038, global_step=51022, preemption_count=0, score=16344.416038, test/accuracy=0.985833, test/loss=0.050173, test/mean_average_precision=0.251555, test/num_examples=43793, total_duration=25462.041621, train/accuracy=0.993376, train/loss=0.021258, train/mean_average_precision=0.609889, validation/accuracy=0.986750, validation/loss=0.047026, validation/mean_average_precision=0.272665, validation/num_examples=43793
I0205 06:29:33.094218 140290179761920 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.12988747656345367, loss=0.026775717735290527
I0205 06:30:04.904109 140283731310336 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.12061265110969543, loss=0.023879464715719223
I0205 06:30:36.396211 140290179761920 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.14763399958610535, loss=0.026589253917336464
I0205 06:31:08.594610 140283731310336 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.1531493067741394, loss=0.026385387405753136
I0205 06:31:40.520610 140290179761920 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.16167959570884705, loss=0.02900789864361286
I0205 06:32:12.515015 140283731310336 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.13820065557956696, loss=0.025885330513119698
I0205 06:32:45.057718 140290179761920 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.14123831689357758, loss=0.025535712018609047
I0205 06:33:07.863544 140451058161472 spec.py:321] Evaluating on the training split.
I0205 06:35:09.590298 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 06:35:12.637776 140451058161472 spec.py:349] Evaluating on the test split.
I0205 06:35:15.658036 140451058161472 submission_runner.py:408] Time since start: 25830.13s, 	Step: 51772, 	{'train/accuracy': 0.9933111667633057, 'train/loss': 0.021427419036626816, 'train/mean_average_precision': 0.6009365828350484, 'validation/accuracy': 0.9867772459983826, 'validation/loss': 0.047151368111371994, 'validation/mean_average_precision': 0.2797023395820893, 'validation/num_examples': 43793, 'test/accuracy': 0.9858819246292114, 'test/loss': 0.050381388515233994, 'test/mean_average_precision': 0.257556443992949, 'test/num_examples': 43793, 'score': 16584.653742313385, 'total_duration': 25830.131724357605, 'accumulated_submission_time': 16584.653742313385, 'accumulated_eval_time': 9241.55316233635, 'accumulated_logging_time': 2.508890390396118}
I0205 06:35:15.683598 140283530442496 logging_writer.py:48] [51772] accumulated_eval_time=9241.553162, accumulated_logging_time=2.508890, accumulated_submission_time=16584.653742, global_step=51772, preemption_count=0, score=16584.653742, test/accuracy=0.985882, test/loss=0.050381, test/mean_average_precision=0.257556, test/num_examples=43793, total_duration=25830.131724, train/accuracy=0.993311, train/loss=0.021427, train/mean_average_precision=0.600937, validation/accuracy=0.986777, validation/loss=0.047151, validation/mean_average_precision=0.279702, validation/num_examples=43793
I0205 06:35:25.371925 140290188154624 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.1332569271326065, loss=0.026754675433039665
I0205 06:35:57.752887 140283530442496 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.15347126126289368, loss=0.026874924078583717
I0205 06:36:29.675429 140290188154624 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.13385836780071259, loss=0.027130665257573128
I0205 06:37:01.686262 140283530442496 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.15382510423660278, loss=0.027756692841649055
I0205 06:37:33.696611 140290188154624 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.15392406284809113, loss=0.0257407259196043
I0205 06:38:06.170053 140283530442496 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.12672843039035797, loss=0.024758460000157356
I0205 06:38:37.600439 140290188154624 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.14895984530448914, loss=0.024571280926465988
I0205 06:39:09.553517 140283530442496 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.14393466711044312, loss=0.026737727224826813
I0205 06:39:15.936347 140451058161472 spec.py:321] Evaluating on the training split.
I0205 06:41:19.343739 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 06:41:22.382166 140451058161472 spec.py:349] Evaluating on the test split.
I0205 06:41:25.389287 140451058161472 submission_runner.py:408] Time since start: 26199.86s, 	Step: 52521, 	{'train/accuracy': 0.9929652214050293, 'train/loss': 0.0222934540361166, 'train/mean_average_precision': 0.5837654219526973, 'validation/accuracy': 0.986805260181427, 'validation/loss': 0.04766381159424782, 'validation/mean_average_precision': 0.27680031738156924, 'validation/num_examples': 43793, 'test/accuracy': 0.9858347773551941, 'test/loss': 0.05096012353897095, 'test/mean_average_precision': 0.25749550432919716, 'test/num_examples': 43793, 'score': 16824.87526488304, 'total_duration': 26199.862973213196, 'accumulated_submission_time': 16824.87526488304, 'accumulated_eval_time': 9371.006050825119, 'accumulated_logging_time': 2.5452442169189453}
I0205 06:41:25.414560 140283262007040 logging_writer.py:48] [52521] accumulated_eval_time=9371.006051, accumulated_logging_time=2.545244, accumulated_submission_time=16824.875265, global_step=52521, preemption_count=0, score=16824.875265, test/accuracy=0.985835, test/loss=0.050960, test/mean_average_precision=0.257496, test/num_examples=43793, total_duration=26199.862973, train/accuracy=0.992965, train/loss=0.022293, train/mean_average_precision=0.583765, validation/accuracy=0.986805, validation/loss=0.047664, validation/mean_average_precision=0.276800, validation/num_examples=43793
I0205 06:41:51.036843 140283731310336 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.14020831882953644, loss=0.02418811433017254
I0205 06:42:22.895112 140283262007040 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.17316417396068573, loss=0.027221225202083588
I0205 06:42:54.566375 140283731310336 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.11856833845376968, loss=0.0249771848320961
I0205 06:43:26.452854 140283262007040 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.1460300236940384, loss=0.028669875115156174
I0205 06:43:58.237137 140283731310336 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.12633979320526123, loss=0.024291841313242912
I0205 06:44:29.838135 140283262007040 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.16914741694927216, loss=0.023810990154743195
I0205 06:45:01.569986 140283731310336 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.15254376828670502, loss=0.024709688499569893
I0205 06:45:25.434225 140451058161472 spec.py:321] Evaluating on the training split.
I0205 06:47:24.492726 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 06:47:27.573085 140451058161472 spec.py:349] Evaluating on the test split.
I0205 06:47:30.580946 140451058161472 submission_runner.py:408] Time since start: 26565.05s, 	Step: 53276, 	{'train/accuracy': 0.9930841326713562, 'train/loss': 0.021920325234532356, 'train/mean_average_precision': 0.5777379497760418, 'validation/accuracy': 0.9868068695068359, 'validation/loss': 0.04758194833993912, 'validation/mean_average_precision': 0.28020843587911254, 'validation/num_examples': 43793, 'test/accuracy': 0.9858364462852478, 'test/loss': 0.050853826105594635, 'test/mean_average_precision': 0.2604646324893419, 'test/num_examples': 43793, 'score': 17064.86261534691, 'total_duration': 26565.05463194847, 'accumulated_submission_time': 17064.86261534691, 'accumulated_eval_time': 9496.152726888657, 'accumulated_logging_time': 2.5827507972717285}
I0205 06:47:30.605978 140283530442496 logging_writer.py:48] [53276] accumulated_eval_time=9496.152727, accumulated_logging_time=2.582751, accumulated_submission_time=17064.862615, global_step=53276, preemption_count=0, score=17064.862615, test/accuracy=0.985836, test/loss=0.050854, test/mean_average_precision=0.260465, test/num_examples=43793, total_duration=26565.054632, train/accuracy=0.993084, train/loss=0.021920, train/mean_average_precision=0.577738, validation/accuracy=0.986807, validation/loss=0.047582, validation/mean_average_precision=0.280208, validation/num_examples=43793
I0205 06:47:38.575539 140290188154624 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.16019241511821747, loss=0.027061015367507935
I0205 06:48:10.467645 140283530442496 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.14600512385368347, loss=0.02634909749031067
I0205 06:48:43.387390 140290188154624 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.14697851240634918, loss=0.026653964072465897
I0205 06:49:15.983203 140283530442496 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.14020030200481415, loss=0.027396557852625847
I0205 06:49:47.825707 140290188154624 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.1387573480606079, loss=0.024381892755627632
I0205 06:50:19.738762 140283530442496 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.15593066811561584, loss=0.026133600622415543
I0205 06:50:51.351224 140290188154624 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.15180370211601257, loss=0.027768731117248535
I0205 06:51:23.566461 140283530442496 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.1621946543455124, loss=0.027708331122994423
I0205 06:51:30.611504 140451058161472 spec.py:321] Evaluating on the training split.
I0205 06:53:30.414216 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 06:53:33.502923 140451058161472 spec.py:349] Evaluating on the test split.
I0205 06:53:36.561449 140451058161472 submission_runner.py:408] Time since start: 26931.04s, 	Step: 54023, 	{'train/accuracy': 0.9934099316596985, 'train/loss': 0.020824594423174858, 'train/mean_average_precision': 0.6189938976762239, 'validation/accuracy': 0.9866924285888672, 'validation/loss': 0.048083286732435226, 'validation/mean_average_precision': 0.2718280150950659, 'validation/num_examples': 43793, 'test/accuracy': 0.985871434211731, 'test/loss': 0.050943344831466675, 'test/mean_average_precision': 0.26011650400011116, 'test/num_examples': 43793, 'score': 17304.836101531982, 'total_duration': 26931.035136938095, 'accumulated_submission_time': 17304.836101531982, 'accumulated_eval_time': 9622.102622747421, 'accumulated_logging_time': 2.6188244819641113}
I0205 06:53:36.587281 140283262007040 logging_writer.py:48] [54023] accumulated_eval_time=9622.102623, accumulated_logging_time=2.618824, accumulated_submission_time=17304.836102, global_step=54023, preemption_count=0, score=17304.836102, test/accuracy=0.985871, test/loss=0.050943, test/mean_average_precision=0.260117, test/num_examples=43793, total_duration=26931.035137, train/accuracy=0.993410, train/loss=0.020825, train/mean_average_precision=0.618994, validation/accuracy=0.986692, validation/loss=0.048083, validation/mean_average_precision=0.271828, validation/num_examples=43793
I0205 06:54:01.099494 140283731310336 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.14792245626449585, loss=0.027664585039019585
I0205 06:54:32.834284 140283262007040 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.1549517661333084, loss=0.027463486418128014
I0205 06:55:05.007342 140283731310336 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.1579616367816925, loss=0.02333722449839115
I0205 06:55:37.007401 140283262007040 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.18737196922302246, loss=0.025750569999217987
I0205 06:56:09.150743 140283731310336 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.14881843328475952, loss=0.02491685189306736
I0205 06:56:41.536156 140283262007040 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.14618748426437378, loss=0.02730056829750538
I0205 06:57:13.046988 140283731310336 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.15086577832698822, loss=0.02732929028570652
I0205 06:57:36.865507 140451058161472 spec.py:321] Evaluating on the training split.
I0205 06:59:39.047056 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 06:59:42.458980 140451058161472 spec.py:349] Evaluating on the test split.
I0205 06:59:45.872454 140451058161472 submission_runner.py:408] Time since start: 27300.35s, 	Step: 54777, 	{'train/accuracy': 0.9935857653617859, 'train/loss': 0.020347628742456436, 'train/mean_average_precision': 0.6222725297509415, 'validation/accuracy': 0.9866132736206055, 'validation/loss': 0.04827210679650307, 'validation/mean_average_precision': 0.2787696840403332, 'validation/num_examples': 43793, 'test/accuracy': 0.9857589602470398, 'test/loss': 0.051447078585624695, 'test/mean_average_precision': 0.25579638107437663, 'test/num_examples': 43793, 'score': 17545.082458496094, 'total_duration': 27300.34612417221, 'accumulated_submission_time': 17545.082458496094, 'accumulated_eval_time': 9751.109506607056, 'accumulated_logging_time': 2.6560540199279785}
I0205 06:59:45.902645 140283530442496 logging_writer.py:48] [54777] accumulated_eval_time=9751.109507, accumulated_logging_time=2.656054, accumulated_submission_time=17545.082458, global_step=54777, preemption_count=0, score=17545.082458, test/accuracy=0.985759, test/loss=0.051447, test/mean_average_precision=0.255796, test/num_examples=43793, total_duration=27300.346124, train/accuracy=0.993586, train/loss=0.020348, train/mean_average_precision=0.622273, validation/accuracy=0.986613, validation/loss=0.048272, validation/mean_average_precision=0.278770, validation/num_examples=43793
I0205 06:59:53.781267 140290179761920 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.17362123727798462, loss=0.025805173441767693
I0205 07:00:26.396571 140283530442496 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.14158625900745392, loss=0.025141455233097076
I0205 07:00:59.160245 140290179761920 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.15821510553359985, loss=0.024965234100818634
I0205 07:01:31.830515 140283530442496 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.14584419131278992, loss=0.0239628404378891
I0205 07:02:04.617388 140290179761920 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.1485808938741684, loss=0.02451803907752037
I0205 07:02:37.009642 140283530442496 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.16587211191654205, loss=0.025340285152196884
I0205 07:03:09.683235 140290179761920 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.15723957121372223, loss=0.0242251418530941
I0205 07:03:42.616999 140283530442496 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.16204847395420074, loss=0.027247579768300056
I0205 07:03:45.923923 140451058161472 spec.py:321] Evaluating on the training split.
I0205 07:05:46.501794 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 07:05:49.598668 140451058161472 spec.py:349] Evaluating on the test split.
I0205 07:05:52.617071 140451058161472 submission_runner.py:408] Time since start: 27667.09s, 	Step: 55511, 	{'train/accuracy': 0.9937710165977478, 'train/loss': 0.019580619409680367, 'train/mean_average_precision': 0.6551641993310239, 'validation/accuracy': 0.9865986108779907, 'validation/loss': 0.04900636896491051, 'validation/mean_average_precision': 0.27371629621791915, 'validation/num_examples': 43793, 'test/accuracy': 0.985806941986084, 'test/loss': 0.052032433450222015, 'test/mean_average_precision': 0.25322293049016803, 'test/num_examples': 43793, 'score': 17785.06728363037, 'total_duration': 27667.090751171112, 'accumulated_submission_time': 17785.06728363037, 'accumulated_eval_time': 9877.802606344223, 'accumulated_logging_time': 2.69838547706604}
I0205 07:05:52.643061 140283262007040 logging_writer.py:48] [55511] accumulated_eval_time=9877.802606, accumulated_logging_time=2.698385, accumulated_submission_time=17785.067284, global_step=55511, preemption_count=0, score=17785.067284, test/accuracy=0.985807, test/loss=0.052032, test/mean_average_precision=0.253223, test/num_examples=43793, total_duration=27667.090751, train/accuracy=0.993771, train/loss=0.019581, train/mean_average_precision=0.655164, validation/accuracy=0.986599, validation/loss=0.049006, validation/mean_average_precision=0.273716, validation/num_examples=43793
I0205 07:06:21.585364 140283731310336 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.15302062034606934, loss=0.02528882957994938
I0205 07:06:53.285582 140283262007040 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.17099879682064056, loss=0.024412162601947784
I0205 07:07:25.110908 140283731310336 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.16619104146957397, loss=0.026682648807764053
I0205 07:07:56.824042 140283262007040 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.14307235181331635, loss=0.022738343104720116
I0205 07:08:28.823928 140283731310336 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.14032354950904846, loss=0.02582969143986702
I0205 07:09:00.863431 140283262007040 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.16738638281822205, loss=0.023612141609191895
I0205 07:09:32.921510 140283731310336 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.18048477172851562, loss=0.02605864405632019
I0205 07:09:52.909074 140451058161472 spec.py:321] Evaluating on the training split.
I0205 07:11:45.382146 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 07:11:48.752640 140451058161472 spec.py:349] Evaluating on the test split.
I0205 07:11:52.020369 140451058161472 submission_runner.py:408] Time since start: 28026.49s, 	Step: 56263, 	{'train/accuracy': 0.994708776473999, 'train/loss': 0.017203541472554207, 'train/mean_average_precision': 0.6975938120108676, 'validation/accuracy': 0.9865763187408447, 'validation/loss': 0.049066293984651566, 'validation/mean_average_precision': 0.2698884790954958, 'validation/num_examples': 43793, 'test/accuracy': 0.9857622981071472, 'test/loss': 0.05213439092040062, 'test/mean_average_precision': 0.2585453414761834, 'test/num_examples': 43793, 'score': 18025.301979780197, 'total_duration': 28026.49404001236, 'accumulated_submission_time': 18025.301979780197, 'accumulated_eval_time': 9996.913838386536, 'accumulated_logging_time': 2.7351033687591553}
I0205 07:11:52.049726 140283530442496 logging_writer.py:48] [56263] accumulated_eval_time=9996.913838, accumulated_logging_time=2.735103, accumulated_submission_time=18025.301980, global_step=56263, preemption_count=0, score=18025.301980, test/accuracy=0.985762, test/loss=0.052134, test/mean_average_precision=0.258545, test/num_examples=43793, total_duration=28026.494040, train/accuracy=0.994709, train/loss=0.017204, train/mean_average_precision=0.697594, validation/accuracy=0.986576, validation/loss=0.049066, validation/mean_average_precision=0.269888, validation/num_examples=43793
I0205 07:12:04.548736 140290179761920 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.15273964405059814, loss=0.022529400885105133
I0205 07:12:37.138033 140283530442496 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.1649942398071289, loss=0.023369494825601578
I0205 07:13:09.816057 140290179761920 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.16058655083179474, loss=0.02488730102777481
I0205 07:13:42.251006 140283530442496 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.1803644299507141, loss=0.023746028542518616
I0205 07:14:15.329899 140290179761920 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.18681606650352478, loss=0.02462669089436531
I0205 07:14:47.612637 140283530442496 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.18146534264087677, loss=0.022927986457943916
I0205 07:15:20.356017 140290179761920 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.1672789752483368, loss=0.02379634417593479
I0205 07:15:52.032583 140451058161472 spec.py:321] Evaluating on the training split.
I0205 07:17:57.768506 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 07:18:00.905659 140451058161472 spec.py:349] Evaluating on the test split.
I0205 07:18:04.150010 140451058161472 submission_runner.py:408] Time since start: 28398.62s, 	Step: 56999, 	{'train/accuracy': 0.994670569896698, 'train/loss': 0.01724269986152649, 'train/mean_average_precision': 0.6762894919081208, 'validation/accuracy': 0.9865738749504089, 'validation/loss': 0.04920833185315132, 'validation/mean_average_precision': 0.2711901601753668, 'validation/num_examples': 43793, 'test/accuracy': 0.9856982827186584, 'test/loss': 0.05246062949299812, 'test/mean_average_precision': 0.2558035961147663, 'test/num_examples': 43793, 'score': 18265.25046825409, 'total_duration': 28398.62367272377, 'accumulated_submission_time': 18265.25046825409, 'accumulated_eval_time': 10129.031205415726, 'accumulated_logging_time': 2.776209592819214}
I0205 07:18:04.176490 140283262007040 logging_writer.py:48] [56999] accumulated_eval_time=10129.031205, accumulated_logging_time=2.776210, accumulated_submission_time=18265.250468, global_step=56999, preemption_count=0, score=18265.250468, test/accuracy=0.985698, test/loss=0.052461, test/mean_average_precision=0.255804, test/num_examples=43793, total_duration=28398.623673, train/accuracy=0.994671, train/loss=0.017243, train/mean_average_precision=0.676289, validation/accuracy=0.986574, validation/loss=0.049208, validation/mean_average_precision=0.271190, validation/num_examples=43793
I0205 07:18:04.857635 140290188154624 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.1736198514699936, loss=0.023832298815250397
I0205 07:18:36.480963 140283262007040 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.1596565842628479, loss=0.024900954216718674
I0205 07:19:08.079475 140290188154624 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.17345359921455383, loss=0.023346716538071632
I0205 07:19:39.809549 140283262007040 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.16132447123527527, loss=0.023713000118732452
I0205 07:20:11.737174 140290188154624 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.19128641486167908, loss=0.022553905844688416
I0205 07:20:43.699502 140283262007040 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.1778784543275833, loss=0.025158081203699112
I0205 07:21:15.616394 140290188154624 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.15945909917354584, loss=0.02384226955473423
I0205 07:21:36.200750 140283262007040 logging_writer.py:48] [57666] global_step=57666, preemption_count=0, score=18477.228970
I0205 07:21:36.255098 140451058161472 checkpoints.py:490] Saving checkpoint at step: 57666
I0205 07:21:36.371376 140451058161472 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_2/ogbg_jax/trial_1/checkpoint_57666
I0205 07:21:36.372517 140451058161472 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_2/ogbg_jax/trial_1/checkpoint_57666.
I0205 07:21:36.562902 140451058161472 submission_runner.py:583] Tuning trial 1/5
I0205 07:21:36.563188 140451058161472 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0205 07:21:36.567742 140451058161472 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5290504693984985, 'train/loss': 0.7363465428352356, 'train/mean_average_precision': 0.020846484877845774, 'validation/accuracy': 0.5270814895629883, 'validation/loss': 0.737440824508667, 'validation/mean_average_precision': 0.024032072784418074, 'validation/num_examples': 43793, 'test/accuracy': 0.5256852507591248, 'test/loss': 0.7376683354377747, 'test/mean_average_precision': 0.026026918574051882, 'test/num_examples': 43793, 'score': 18.12672519683838, 'total_duration': 323.5901656150818, 'accumulated_submission_time': 18.12672519683838, 'accumulated_eval_time': 305.463401556015, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (738, {'train/accuracy': 0.9867269396781921, 'train/loss': 0.06961894780397415, 'train/mean_average_precision': 0.037541683577360145, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.0782666727900505, 'validation/mean_average_precision': 0.03982426540821902, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.08114508539438248, 'test/mean_average_precision': 0.04160785254877804, 'test/num_examples': 43793, 'score': 258.0806932449341, 'total_duration': 683.0615284442902, 'accumulated_submission_time': 258.0806932449341, 'accumulated_eval_time': 424.9269685745239, 'accumulated_logging_time': 0.032134294509887695, 'global_step': 738, 'preemption_count': 0}), (1472, {'train/accuracy': 0.9868612289428711, 'train/loss': 0.0518169067800045, 'train/mean_average_precision': 0.07142127442439483, 'validation/accuracy': 0.9841642379760742, 'validation/loss': 0.06164403259754181, 'validation/mean_average_precision': 0.07514380538388722, 'validation/num_examples': 43793, 'test/accuracy': 0.9831724166870117, 'test/loss': 0.06509590893983841, 'test/mean_average_precision': 0.07412052245940108, 'test/num_examples': 43793, 'score': 498.13424348831177, 'total_duration': 1044.1351709365845, 'accumulated_submission_time': 498.13424348831177, 'accumulated_eval_time': 545.8983333110809, 'accumulated_logging_time': 0.06061291694641113, 'global_step': 1472, 'preemption_count': 0}), (2221, {'train/accuracy': 0.9878098964691162, 'train/loss': 0.04395898059010506, 'train/mean_average_precision': 0.13697456731632401, 'validation/accuracy': 0.9851465821266174, 'validation/loss': 0.05280575156211853, 'validation/mean_average_precision': 0.12950148084545254, 'validation/num_examples': 43793, 'test/accuracy': 0.9841352701187134, 'test/loss': 0.05575975030660629, 'test/mean_average_precision': 0.13042869335541193, 'test/num_examples': 43793, 'score': 738.378705739975, 'total_duration': 1407.2695829868317, 'accumulated_submission_time': 738.378705739975, 'accumulated_eval_time': 668.7412447929382, 'accumulated_logging_time': 0.08817148208618164, 'global_step': 2221, 'preemption_count': 0}), (2968, {'train/accuracy': 0.9880942702293396, 'train/loss': 0.042098671197891235, 'train/mean_average_precision': 0.1608620839752452, 'validation/accuracy': 0.985255777835846, 'validation/loss': 0.051538657397031784, 'validation/mean_average_precision': 0.1466299149649001, 'validation/num_examples': 43793, 'test/accuracy': 0.9843475222587585, 'test/loss': 0.05426577478647232, 'test/mean_average_precision': 0.15363437850865844, 'test/num_examples': 43793, 'score': 978.384375333786, 'total_duration': 1769.2708656787872, 'accumulated_submission_time': 978.384375333786, 'accumulated_eval_time': 790.6891014575958, 'accumulated_logging_time': 0.11685395240783691, 'global_step': 2968, 'preemption_count': 0}), (3714, {'train/accuracy': 0.988378643989563, 'train/loss': 0.0402594655752182, 'train/mean_average_precision': 0.18233914361002074, 'validation/accuracy': 0.9854835271835327, 'validation/loss': 0.0498136468231678, 'validation/mean_average_precision': 0.1700115195174815, 'validation/num_examples': 43793, 'test/accuracy': 0.9845543503761292, 'test/loss': 0.05250402167439461, 'test/mean_average_precision': 0.16973306944287994, 'test/num_examples': 43793, 'score': 1218.5362486839294, 'total_duration': 2135.0145077705383, 'accumulated_submission_time': 1218.5362486839294, 'accumulated_eval_time': 916.2338988780975, 'accumulated_logging_time': 0.14468717575073242, 'global_step': 3714, 'preemption_count': 0}), (4458, {'train/accuracy': 0.9884053468704224, 'train/loss': 0.03992285206913948, 'train/mean_average_precision': 0.21001074872560668, 'validation/accuracy': 0.9856386184692383, 'validation/loss': 0.04938645660877228, 'validation/mean_average_precision': 0.18339889496720987, 'validation/num_examples': 43793, 'test/accuracy': 0.9847325086593628, 'test/loss': 0.05199676752090454, 'test/mean_average_precision': 0.18730380392712498, 'test/num_examples': 43793, 'score': 1458.7867727279663, 'total_duration': 2505.007899045944, 'accumulated_submission_time': 1458.7867727279663, 'accumulated_eval_time': 1045.9301965236664, 'accumulated_logging_time': 0.1723625659942627, 'global_step': 4458, 'preemption_count': 0}), (5192, {'train/accuracy': 0.9888152480125427, 'train/loss': 0.03871508315205574, 'train/mean_average_precision': 0.2292613295651795, 'validation/accuracy': 0.9857624173164368, 'validation/loss': 0.048279769718647, 'validation/mean_average_precision': 0.1916392368220283, 'validation/num_examples': 43793, 'test/accuracy': 0.9848563075065613, 'test/loss': 0.05093342438340187, 'test/mean_average_precision': 0.19238571078258276, 'test/num_examples': 43793, 'score': 1698.7766785621643, 'total_duration': 2875.6716067790985, 'accumulated_submission_time': 1698.7766785621643, 'accumulated_eval_time': 1176.5565106868744, 'accumulated_logging_time': 0.1991899013519287, 'global_step': 5192, 'preemption_count': 0}), (5929, {'train/accuracy': 0.9890048503875732, 'train/loss': 0.03766616806387901, 'train/mean_average_precision': 0.24921226230222537, 'validation/accuracy': 0.9859474897384644, 'validation/loss': 0.0476326122879982, 'validation/mean_average_precision': 0.20997313712472634, 'validation/num_examples': 43793, 'test/accuracy': 0.9850403666496277, 'test/loss': 0.05034620687365532, 'test/mean_average_precision': 0.20815691664234603, 'test/num_examples': 43793, 'score': 1938.92862033844, 'total_duration': 3243.3159506320953, 'accumulated_submission_time': 1938.92862033844, 'accumulated_eval_time': 1304.0002155303955, 'accumulated_logging_time': 0.22843122482299805, 'global_step': 5929, 'preemption_count': 0}), (6671, {'train/accuracy': 0.9889360070228577, 'train/loss': 0.03777961805462837, 'train/mean_average_precision': 0.23797969611058023, 'validation/accuracy': 0.9859402179718018, 'validation/loss': 0.04762043431401253, 'validation/mean_average_precision': 0.20814223137437274, 'validation/num_examples': 43793, 'test/accuracy': 0.9851086139678955, 'test/loss': 0.0502110980451107, 'test/mean_average_precision': 0.20542590378598344, 'test/num_examples': 43793, 'score': 2179.0981862545013, 'total_duration': 3611.3408505916595, 'accumulated_submission_time': 2179.0981862545013, 'accumulated_eval_time': 1431.8058450222015, 'accumulated_logging_time': 0.2576940059661865, 'global_step': 6671, 'preemption_count': 0}), (7435, {'train/accuracy': 0.9889921545982361, 'train/loss': 0.03744671493768692, 'train/mean_average_precision': 0.25876130041910667, 'validation/accuracy': 0.9860936403274536, 'validation/loss': 0.04684552922844887, 'validation/mean_average_precision': 0.2130963062876768, 'validation/num_examples': 43793, 'test/accuracy': 0.9852202534675598, 'test/loss': 0.049300067126750946, 'test/mean_average_precision': 0.21027840907231676, 'test/num_examples': 43793, 'score': 2419.294604063034, 'total_duration': 3981.5195965766907, 'accumulated_submission_time': 2419.294604063034, 'accumulated_eval_time': 1561.7417376041412, 'accumulated_logging_time': 0.28409743309020996, 'global_step': 7435, 'preemption_count': 0}), (8200, {'train/accuracy': 0.9891512393951416, 'train/loss': 0.03659132122993469, 'train/mean_average_precision': 0.26106851781320495, 'validation/accuracy': 0.9860554933547974, 'validation/loss': 0.04685352370142937, 'validation/mean_average_precision': 0.2194025693009693, 'validation/num_examples': 43793, 'test/accuracy': 0.9851284027099609, 'test/loss': 0.04958391934633255, 'test/mean_average_precision': 0.21861140761688885, 'test/num_examples': 43793, 'score': 2659.541307926178, 'total_duration': 4349.7693428993225, 'accumulated_submission_time': 2659.541307926178, 'accumulated_eval_time': 1689.6956989765167, 'accumulated_logging_time': 0.3128774166107178, 'global_step': 8200, 'preemption_count': 0}), (8952, {'train/accuracy': 0.9891273975372314, 'train/loss': 0.03664684295654297, 'train/mean_average_precision': 0.27911021454146734, 'validation/accuracy': 0.9861025810241699, 'validation/loss': 0.04621446877717972, 'validation/mean_average_precision': 0.22299958263479633, 'validation/num_examples': 43793, 'test/accuracy': 0.9851734638214111, 'test/loss': 0.048995938152074814, 'test/mean_average_precision': 0.22708903725360496, 'test/num_examples': 43793, 'score': 2899.5621926784515, 'total_duration': 4718.630972146988, 'accumulated_submission_time': 2899.5621926784515, 'accumulated_eval_time': 1818.4839661121368, 'accumulated_logging_time': 0.34474682807922363, 'global_step': 8952, 'preemption_count': 0}), (9706, {'train/accuracy': 0.9896572232246399, 'train/loss': 0.03524595499038696, 'train/mean_average_precision': 0.2876529415246712, 'validation/accuracy': 0.9862491488456726, 'validation/loss': 0.04611955210566521, 'validation/mean_average_precision': 0.2190752261508913, 'validation/num_examples': 43793, 'test/accuracy': 0.9853731393814087, 'test/loss': 0.048670776188373566, 'test/mean_average_precision': 0.22655205116409696, 'test/num_examples': 43793, 'score': 3139.5162563323975, 'total_duration': 5088.528156280518, 'accumulated_submission_time': 3139.5162563323975, 'accumulated_eval_time': 1948.3777787685394, 'accumulated_logging_time': 0.3719336986541748, 'global_step': 9706, 'preemption_count': 0}), (10468, {'train/accuracy': 0.9896705150604248, 'train/loss': 0.03482377529144287, 'train/mean_average_precision': 0.3068134647972326, 'validation/accuracy': 0.9862880706787109, 'validation/loss': 0.04639178141951561, 'validation/mean_average_precision': 0.24119614102253042, 'validation/num_examples': 43793, 'test/accuracy': 0.9854143857955933, 'test/loss': 0.04908290505409241, 'test/mean_average_precision': 0.2329932907320756, 'test/num_examples': 43793, 'score': 3379.5207164287567, 'total_duration': 5454.616222381592, 'accumulated_submission_time': 3379.5207164287567, 'accumulated_eval_time': 2074.4122228622437, 'accumulated_logging_time': 0.4013481140136719, 'global_step': 10468, 'preemption_count': 0}), (11226, {'train/accuracy': 0.9897632002830505, 'train/loss': 0.03422287851572037, 'train/mean_average_precision': 0.3330268928440666, 'validation/accuracy': 0.9864967465400696, 'validation/loss': 0.04566303268074989, 'validation/mean_average_precision': 0.2484119453619031, 'validation/num_examples': 43793, 'test/accuracy': 0.9855487942695618, 'test/loss': 0.048500243574380875, 'test/mean_average_precision': 0.23948637983980706, 'test/num_examples': 43793, 'score': 3619.7391617298126, 'total_duration': 5822.907917261124, 'accumulated_submission_time': 3619.7391617298126, 'accumulated_eval_time': 2202.437658548355, 'accumulated_logging_time': 0.4295799732208252, 'global_step': 11226, 'preemption_count': 0}), (11982, {'train/accuracy': 0.9901514053344727, 'train/loss': 0.03276079148054123, 'train/mean_average_precision': 0.3584733182330887, 'validation/accuracy': 0.9865888953208923, 'validation/loss': 0.04483211785554886, 'validation/mean_average_precision': 0.25001468516502967, 'validation/num_examples': 43793, 'test/accuracy': 0.9855656027793884, 'test/loss': 0.0477055162191391, 'test/mean_average_precision': 0.23895538587981185, 'test/num_examples': 43793, 'score': 3859.737271785736, 'total_duration': 6193.9528086185455, 'accumulated_submission_time': 3859.737271785736, 'accumulated_eval_time': 2333.4328026771545, 'accumulated_logging_time': 0.4608142375946045, 'global_step': 11982, 'preemption_count': 0}), (12739, {'train/accuracy': 0.9903623461723328, 'train/loss': 0.03217914327979088, 'train/mean_average_precision': 0.3770886565685112, 'validation/accuracy': 0.9865893125534058, 'validation/loss': 0.04484793171286583, 'validation/mean_average_precision': 0.2501933144468553, 'validation/num_examples': 43793, 'test/accuracy': 0.985632598400116, 'test/loss': 0.04763798788189888, 'test/mean_average_precision': 0.2430544044938929, 'test/num_examples': 43793, 'score': 4099.829087495804, 'total_duration': 6565.651482105255, 'accumulated_submission_time': 4099.829087495804, 'accumulated_eval_time': 2464.989315032959, 'accumulated_logging_time': 0.49089860916137695, 'global_step': 12739, 'preemption_count': 0}), (13493, {'train/accuracy': 0.9904111623764038, 'train/loss': 0.03199373930692673, 'train/mean_average_precision': 0.3685707194392107, 'validation/accuracy': 0.9865913391113281, 'validation/loss': 0.045087773352861404, 'validation/mean_average_precision': 0.25362827987968756, 'validation/num_examples': 43793, 'test/accuracy': 0.985736608505249, 'test/loss': 0.04794413596391678, 'test/mean_average_precision': 0.24486259259822446, 'test/num_examples': 43793, 'score': 4339.98765039444, 'total_duration': 6935.369527339935, 'accumulated_submission_time': 4339.98765039444, 'accumulated_eval_time': 2594.4998412132263, 'accumulated_logging_time': 0.5199365615844727, 'global_step': 13493, 'preemption_count': 0}), (14249, {'train/accuracy': 0.9904477000236511, 'train/loss': 0.03179260343313217, 'train/mean_average_precision': 0.3795072526894285, 'validation/accuracy': 0.9866713285446167, 'validation/loss': 0.04471726343035698, 'validation/mean_average_precision': 0.25447894472058963, 'validation/num_examples': 43793, 'test/accuracy': 0.9857930541038513, 'test/loss': 0.04738032817840576, 'test/mean_average_precision': 0.24344730275635112, 'test/num_examples': 43793, 'score': 4580.077013969421, 'total_duration': 7305.356061458588, 'accumulated_submission_time': 4580.077013969421, 'accumulated_eval_time': 2724.3480422496796, 'accumulated_logging_time': 0.5486664772033691, 'global_step': 14249, 'preemption_count': 0}), (15004, {'train/accuracy': 0.9902809858322144, 'train/loss': 0.032261211425065994, 'train/mean_average_precision': 0.3739686437256189, 'validation/accuracy': 0.9866579174995422, 'validation/loss': 0.04483366012573242, 'validation/mean_average_precision': 0.2554795498068286, 'validation/num_examples': 43793, 'test/accuracy': 0.9858074188232422, 'test/loss': 0.047604430466890335, 'test/mean_average_precision': 0.24544980175739897, 'test/num_examples': 43793, 'score': 4820.203650474548, 'total_duration': 7676.213287353516, 'accumulated_submission_time': 4820.203650474548, 'accumulated_eval_time': 2855.0291335582733, 'accumulated_logging_time': 0.5781416893005371, 'global_step': 15004, 'preemption_count': 0}), (15757, {'train/accuracy': 0.9904695749282837, 'train/loss': 0.03158517926931381, 'train/mean_average_precision': 0.37808878658834266, 'validation/accuracy': 0.9866855144500732, 'validation/loss': 0.044472336769104004, 'validation/mean_average_precision': 0.26377274406735646, 'validation/num_examples': 43793, 'test/accuracy': 0.98580402135849, 'test/loss': 0.047192614525556564, 'test/mean_average_precision': 0.2571249985640839, 'test/num_examples': 43793, 'score': 5060.340332508087, 'total_duration': 8052.993452072144, 'accumulated_submission_time': 5060.340332508087, 'accumulated_eval_time': 2991.6243121623993, 'accumulated_logging_time': 0.6067461967468262, 'global_step': 15757, 'preemption_count': 0}), (16510, {'train/accuracy': 0.9904604554176331, 'train/loss': 0.031649067997932434, 'train/mean_average_precision': 0.3682752869847712, 'validation/accuracy': 0.9867634773254395, 'validation/loss': 0.044304266571998596, 'validation/mean_average_precision': 0.26255079006164844, 'validation/num_examples': 43793, 'test/accuracy': 0.9858672022819519, 'test/loss': 0.047187402844429016, 'test/mean_average_precision': 0.2516207197326865, 'test/num_examples': 43793, 'score': 5300.3963787555695, 'total_duration': 8427.862237930298, 'accumulated_submission_time': 5300.3963787555695, 'accumulated_eval_time': 3126.3872005939484, 'accumulated_logging_time': 0.6358902454376221, 'global_step': 16510, 'preemption_count': 0}), (17255, {'train/accuracy': 0.9904911518096924, 'train/loss': 0.03133274242281914, 'train/mean_average_precision': 0.3850487983348396, 'validation/accuracy': 0.9867411255836487, 'validation/loss': 0.04437999799847603, 'validation/mean_average_precision': 0.2592182022289485, 'validation/num_examples': 43793, 'test/accuracy': 0.9859405159950256, 'test/loss': 0.047076329588890076, 'test/mean_average_precision': 0.2522397872860027, 'test/num_examples': 43793, 'score': 5540.545610666275, 'total_duration': 8795.66866350174, 'accumulated_submission_time': 5540.545610666275, 'accumulated_eval_time': 3253.9894936084747, 'accumulated_logging_time': 0.6687760353088379, 'global_step': 17255, 'preemption_count': 0}), (18009, {'train/accuracy': 0.9906929135322571, 'train/loss': 0.03085498698055744, 'train/mean_average_precision': 0.39717999532900616, 'validation/accuracy': 0.9866570830345154, 'validation/loss': 0.04496076703071594, 'validation/mean_average_precision': 0.2552737329943473, 'validation/num_examples': 43793, 'test/accuracy': 0.9858688712120056, 'test/loss': 0.04773515462875366, 'test/mean_average_precision': 0.24338506834360013, 'test/num_examples': 43793, 'score': 5780.521510839462, 'total_duration': 9169.35992527008, 'accumulated_submission_time': 5780.521510839462, 'accumulated_eval_time': 3387.6503591537476, 'accumulated_logging_time': 0.7024412155151367, 'global_step': 18009, 'preemption_count': 0}), (18759, {'train/accuracy': 0.990839421749115, 'train/loss': 0.03034825250506401, 'train/mean_average_precision': 0.4093167248324422, 'validation/accuracy': 0.9866721034049988, 'validation/loss': 0.04450898617506027, 'validation/mean_average_precision': 0.2656968890905586, 'validation/num_examples': 43793, 'test/accuracy': 0.985846996307373, 'test/loss': 0.04725183919072151, 'test/mean_average_precision': 0.2473320580292265, 'test/num_examples': 43793, 'score': 6020.512645244598, 'total_duration': 9540.521861076355, 'accumulated_submission_time': 6020.512645244598, 'accumulated_eval_time': 3518.769593477249, 'accumulated_logging_time': 0.7336812019348145, 'global_step': 18759, 'preemption_count': 0}), (19514, {'train/accuracy': 0.9907936453819275, 'train/loss': 0.030009375885128975, 'train/mean_average_precision': 0.42834513891776127, 'validation/accuracy': 0.9867143034934998, 'validation/loss': 0.04493936896324158, 'validation/mean_average_precision': 0.2623142473408286, 'validation/num_examples': 43793, 'test/accuracy': 0.9858187437057495, 'test/loss': 0.04760851338505745, 'test/mean_average_precision': 0.2507867971943465, 'test/num_examples': 43793, 'score': 6260.5007147789, 'total_duration': 9909.286611318588, 'accumulated_submission_time': 6260.5007147789, 'accumulated_eval_time': 3647.493248462677, 'accumulated_logging_time': 0.7661452293395996, 'global_step': 19514, 'preemption_count': 0}), (20272, {'train/accuracy': 0.9910169243812561, 'train/loss': 0.029275966808199883, 'train/mean_average_precision': 0.43464146771933454, 'validation/accuracy': 0.9868003726005554, 'validation/loss': 0.04456842690706253, 'validation/mean_average_precision': 0.2706530486331538, 'validation/num_examples': 43793, 'test/accuracy': 0.9859198331832886, 'test/loss': 0.04731724411249161, 'test/mean_average_precision': 0.24841916065106492, 'test/num_examples': 43793, 'score': 6500.6417491436005, 'total_duration': 10280.787954807281, 'accumulated_submission_time': 6500.6417491436005, 'accumulated_eval_time': 3778.8022241592407, 'accumulated_logging_time': 0.7969620227813721, 'global_step': 20272, 'preemption_count': 0}), (21028, {'train/accuracy': 0.9911913275718689, 'train/loss': 0.029066402465105057, 'train/mean_average_precision': 0.4319467944312771, 'validation/accuracy': 0.9865812063217163, 'validation/loss': 0.04467516019940376, 'validation/mean_average_precision': 0.26187619437584214, 'validation/num_examples': 43793, 'test/accuracy': 0.9858166575431824, 'test/loss': 0.04713984578847885, 'test/mean_average_precision': 0.2538452476150781, 'test/num_examples': 43793, 'score': 6740.721883535385, 'total_duration': 10648.634120225906, 'accumulated_submission_time': 6740.721883535385, 'accumulated_eval_time': 3906.517117500305, 'accumulated_logging_time': 0.8276827335357666, 'global_step': 21028, 'preemption_count': 0}), (21774, {'train/accuracy': 0.991000235080719, 'train/loss': 0.02977318875491619, 'train/mean_average_precision': 0.4357048122346068, 'validation/accuracy': 0.9867650866508484, 'validation/loss': 0.044359687715768814, 'validation/mean_average_precision': 0.26043491822969284, 'validation/num_examples': 43793, 'test/accuracy': 0.9858962893486023, 'test/loss': 0.046945106238126755, 'test/mean_average_precision': 0.2553447340116264, 'test/num_examples': 43793, 'score': 6980.779272079468, 'total_duration': 11021.275067090988, 'accumulated_submission_time': 6980.779272079468, 'accumulated_eval_time': 4039.0473034381866, 'accumulated_logging_time': 0.8594973087310791, 'global_step': 21774, 'preemption_count': 0}), (22527, {'train/accuracy': 0.990811824798584, 'train/loss': 0.03012635000050068, 'train/mean_average_precision': 0.3952604718650014, 'validation/accuracy': 0.9867122769355774, 'validation/loss': 0.044633541256189346, 'validation/mean_average_precision': 0.2590375873548066, 'validation/num_examples': 43793, 'test/accuracy': 0.9859253168106079, 'test/loss': 0.047483980655670166, 'test/mean_average_precision': 0.25613231337326164, 'test/num_examples': 43793, 'score': 7220.825093746185, 'total_duration': 11390.085256099701, 'accumulated_submission_time': 7220.825093746185, 'accumulated_eval_time': 4167.759459018707, 'accumulated_logging_time': 0.8915479183197021, 'global_step': 22527, 'preemption_count': 0}), (23280, {'train/accuracy': 0.9908799529075623, 'train/loss': 0.0300232395529747, 'train/mean_average_precision': 0.4227056232213048, 'validation/accuracy': 0.9867395162582397, 'validation/loss': 0.04472553730010986, 'validation/mean_average_precision': 0.26406131902952434, 'validation/num_examples': 43793, 'test/accuracy': 0.985958993434906, 'test/loss': 0.04724575951695442, 'test/mean_average_precision': 0.2608490687676737, 'test/num_examples': 43793, 'score': 7460.819598436356, 'total_duration': 11762.319630622864, 'accumulated_submission_time': 7460.819598436356, 'accumulated_eval_time': 4299.94361448288, 'accumulated_logging_time': 0.9270291328430176, 'global_step': 23280, 'preemption_count': 0}), (24031, {'train/accuracy': 0.9909188747406006, 'train/loss': 0.02971712313592434, 'train/mean_average_precision': 0.4331208281078121, 'validation/accuracy': 0.9867057800292969, 'validation/loss': 0.04461422935128212, 'validation/mean_average_precision': 0.2611251490209201, 'validation/num_examples': 43793, 'test/accuracy': 0.9859577417373657, 'test/loss': 0.04722660779953003, 'test/mean_average_precision': 0.25167957139764363, 'test/num_examples': 43793, 'score': 7700.661962509155, 'total_duration': 12135.146509170532, 'accumulated_submission_time': 7700.661962509155, 'accumulated_eval_time': 4432.592004299164, 'accumulated_logging_time': 1.2428011894226074, 'global_step': 24031, 'preemption_count': 0}), (24785, {'train/accuracy': 0.9908357858657837, 'train/loss': 0.029875533655285835, 'train/mean_average_precision': 0.4064477900468354, 'validation/accuracy': 0.9866595268249512, 'validation/loss': 0.0450131893157959, 'validation/mean_average_precision': 0.25737166574734294, 'validation/num_examples': 43793, 'test/accuracy': 0.9858036041259766, 'test/loss': 0.04784903675317764, 'test/mean_average_precision': 0.25031988347804224, 'test/num_examples': 43793, 'score': 7940.79802775383, 'total_duration': 12508.866287469864, 'accumulated_submission_time': 7940.79802775383, 'accumulated_eval_time': 4566.1248388290405, 'accumulated_logging_time': 1.2736265659332275, 'global_step': 24785, 'preemption_count': 0}), (25537, {'train/accuracy': 0.9911275506019592, 'train/loss': 0.029165217652916908, 'train/mean_average_precision': 0.44100879810799015, 'validation/accuracy': 0.9867557287216187, 'validation/loss': 0.04441393166780472, 'validation/mean_average_precision': 0.27166666396482503, 'validation/num_examples': 43793, 'test/accuracy': 0.9858790040016174, 'test/loss': 0.047144122421741486, 'test/mean_average_precision': 0.2556667009203458, 'test/num_examples': 43793, 'score': 8180.888298749924, 'total_duration': 12884.822046756744, 'accumulated_submission_time': 8180.888298749924, 'accumulated_eval_time': 4701.938496828079, 'accumulated_logging_time': 1.304905891418457, 'global_step': 25537, 'preemption_count': 0}), (26288, {'train/accuracy': 0.9913343191146851, 'train/loss': 0.028332335874438286, 'train/mean_average_precision': 0.4558309209604774, 'validation/accuracy': 0.986750066280365, 'validation/loss': 0.04462895169854164, 'validation/mean_average_precision': 0.2663130309218301, 'validation/num_examples': 43793, 'test/accuracy': 0.9859998822212219, 'test/loss': 0.047167856246232986, 'test/mean_average_precision': 0.25527667639900953, 'test/num_examples': 43793, 'score': 8421.127045869827, 'total_duration': 13258.230972528458, 'accumulated_submission_time': 8421.127045869827, 'accumulated_eval_time': 4835.052654981613, 'accumulated_logging_time': 1.3399152755737305, 'global_step': 26288, 'preemption_count': 0}), (27044, {'train/accuracy': 0.9913906455039978, 'train/loss': 0.02815435081720352, 'train/mean_average_precision': 0.4644263463446575, 'validation/accuracy': 0.9868559837341309, 'validation/loss': 0.044855065643787384, 'validation/mean_average_precision': 0.27040164502811026, 'validation/num_examples': 43793, 'test/accuracy': 0.9859737753868103, 'test/loss': 0.047707315534353256, 'test/mean_average_precision': 0.2546293091749866, 'test/num_examples': 43793, 'score': 8661.194565296173, 'total_duration': 13631.669520616531, 'accumulated_submission_time': 8661.194565296173, 'accumulated_eval_time': 4968.371803283691, 'accumulated_logging_time': 1.3710052967071533, 'global_step': 27044, 'preemption_count': 0}), (27801, {'train/accuracy': 0.9913938045501709, 'train/loss': 0.02781856618821621, 'train/mean_average_precision': 0.47696753609613807, 'validation/accuracy': 0.986710250377655, 'validation/loss': 0.045109447091817856, 'validation/mean_average_precision': 0.26479174347512763, 'validation/num_examples': 43793, 'test/accuracy': 0.9859451055526733, 'test/loss': 0.0478653758764267, 'test/mean_average_precision': 0.2571030521372473, 'test/num_examples': 43793, 'score': 8901.284093856812, 'total_duration': 14002.555571317673, 'accumulated_submission_time': 8901.284093856812, 'accumulated_eval_time': 5099.115159749985, 'accumulated_logging_time': 1.403883695602417, 'global_step': 27801, 'preemption_count': 0}), (28546, {'train/accuracy': 0.9915661811828613, 'train/loss': 0.027152186259627342, 'train/mean_average_precision': 0.48831206860766, 'validation/accuracy': 0.9869323372840881, 'validation/loss': 0.04443078488111496, 'validation/mean_average_precision': 0.2773346118327409, 'validation/num_examples': 43793, 'test/accuracy': 0.9859514236450195, 'test/loss': 0.04753367602825165, 'test/mean_average_precision': 0.25835264106118383, 'test/num_examples': 43793, 'score': 9141.320052146912, 'total_duration': 14374.145962715149, 'accumulated_submission_time': 9141.320052146912, 'accumulated_eval_time': 5230.611914157867, 'accumulated_logging_time': 1.4391045570373535, 'global_step': 28546, 'preemption_count': 0}), (29299, {'train/accuracy': 0.9914606809616089, 'train/loss': 0.02791007235646248, 'train/mean_average_precision': 0.45549788705928623, 'validation/accuracy': 0.9867382645606995, 'validation/loss': 0.044542621821165085, 'validation/mean_average_precision': 0.2711044475252064, 'validation/num_examples': 43793, 'test/accuracy': 0.9858566522598267, 'test/loss': 0.04740707948803902, 'test/mean_average_precision': 0.25263625499181763, 'test/num_examples': 43793, 'score': 9381.337366342545, 'total_duration': 14749.433589935303, 'accumulated_submission_time': 9381.337366342545, 'accumulated_eval_time': 5365.830714225769, 'accumulated_logging_time': 1.4701387882232666, 'global_step': 29299, 'preemption_count': 0}), (30045, {'train/accuracy': 0.9913190603256226, 'train/loss': 0.02830742672085762, 'train/mean_average_precision': 0.4533270602357647, 'validation/accuracy': 0.9867492318153381, 'validation/loss': 0.044565699994564056, 'validation/mean_average_precision': 0.2735216258339702, 'validation/num_examples': 43793, 'test/accuracy': 0.9859737753868103, 'test/loss': 0.047178592532873154, 'test/mean_average_precision': 0.26105795087793254, 'test/num_examples': 43793, 'score': 9621.466262102127, 'total_duration': 15122.514045715332, 'accumulated_submission_time': 9621.466262102127, 'accumulated_eval_time': 5498.725798130035, 'accumulated_logging_time': 1.5060889720916748, 'global_step': 30045, 'preemption_count': 0}), (30794, {'train/accuracy': 0.9913336038589478, 'train/loss': 0.02827261947095394, 'train/mean_average_precision': 0.4520790403454369, 'validation/accuracy': 0.986777663230896, 'validation/loss': 0.0448727048933506, 'validation/mean_average_precision': 0.2668618929323761, 'validation/num_examples': 43793, 'test/accuracy': 0.9858756065368652, 'test/loss': 0.047772377729415894, 'test/mean_average_precision': 0.2537742032421881, 'test/num_examples': 43793, 'score': 9861.666239261627, 'total_duration': 15493.101830720901, 'accumulated_submission_time': 9861.666239261627, 'accumulated_eval_time': 5629.058895349503, 'accumulated_logging_time': 1.5400199890136719, 'global_step': 30794, 'preemption_count': 0}), (31550, {'train/accuracy': 0.9914684891700745, 'train/loss': 0.0279049314558506, 'train/mean_average_precision': 0.46250529010033, 'validation/accuracy': 0.9867516756057739, 'validation/loss': 0.04472426325082779, 'validation/mean_average_precision': 0.2758591528506365, 'validation/num_examples': 43793, 'test/accuracy': 0.9859691262245178, 'test/loss': 0.04747837781906128, 'test/mean_average_precision': 0.263347388682702, 'test/num_examples': 43793, 'score': 10101.732964754105, 'total_duration': 15869.459534406662, 'accumulated_submission_time': 10101.732964754105, 'accumulated_eval_time': 5765.29677939415, 'accumulated_logging_time': 1.5725555419921875, 'global_step': 31550, 'preemption_count': 0}), (32282, {'train/accuracy': 0.9915327429771423, 'train/loss': 0.027638010680675507, 'train/mean_average_precision': 0.4701164224800756, 'validation/accuracy': 0.9868023991584778, 'validation/loss': 0.04418288171291351, 'validation/mean_average_precision': 0.27248519758707684, 'validation/num_examples': 43793, 'test/accuracy': 0.985917329788208, 'test/loss': 0.046841979026794434, 'test/mean_average_precision': 0.26137567472069356, 'test/num_examples': 43793, 'score': 10341.703769683838, 'total_duration': 16240.596867084503, 'accumulated_submission_time': 10341.703769683838, 'accumulated_eval_time': 5896.408312559128, 'accumulated_logging_time': 1.6048774719238281, 'global_step': 32282, 'preemption_count': 0}), (33014, {'train/accuracy': 0.9916948676109314, 'train/loss': 0.02696775458753109, 'train/mean_average_precision': 0.47629173604202346, 'validation/accuracy': 0.9868279695510864, 'validation/loss': 0.04445328935980797, 'validation/mean_average_precision': 0.27250767912286333, 'validation/num_examples': 43793, 'test/accuracy': 0.9860011339187622, 'test/loss': 0.047196511179208755, 'test/mean_average_precision': 0.25689540766324287, 'test/num_examples': 43793, 'score': 10581.765675783157, 'total_duration': 16615.352819919586, 'accumulated_submission_time': 10581.765675783157, 'accumulated_eval_time': 6031.045778036118, 'accumulated_logging_time': 1.6387574672698975, 'global_step': 33014, 'preemption_count': 0}), (33766, {'train/accuracy': 0.9919641613960266, 'train/loss': 0.026328330859541893, 'train/mean_average_precision': 0.4970465806059808, 'validation/accuracy': 0.9867817163467407, 'validation/loss': 0.0448424257338047, 'validation/mean_average_precision': 0.27030136921882875, 'validation/num_examples': 43793, 'test/accuracy': 0.9859564900398254, 'test/loss': 0.04744219407439232, 'test/mean_average_precision': 0.2586274867507777, 'test/num_examples': 43793, 'score': 10821.961535930634, 'total_duration': 16985.606071949005, 'accumulated_submission_time': 10821.961535930634, 'accumulated_eval_time': 6161.0498831272125, 'accumulated_logging_time': 1.6721007823944092, 'global_step': 33766, 'preemption_count': 0}), (34522, {'train/accuracy': 0.9921725392341614, 'train/loss': 0.025557050481438637, 'train/mean_average_precision': 0.527538363625475, 'validation/accuracy': 0.9869164824485779, 'validation/loss': 0.044772692024707794, 'validation/mean_average_precision': 0.27226595903886797, 'validation/num_examples': 43793, 'test/accuracy': 0.9859733581542969, 'test/loss': 0.0475214347243309, 'test/mean_average_precision': 0.26075940291263694, 'test/num_examples': 43793, 'score': 11062.150256633759, 'total_duration': 17354.303694963455, 'accumulated_submission_time': 11062.150256633759, 'accumulated_eval_time': 6289.505045890808, 'accumulated_logging_time': 1.7057373523712158, 'global_step': 34522, 'preemption_count': 0}), (35277, {'train/accuracy': 0.9921831488609314, 'train/loss': 0.025127580389380455, 'train/mean_average_precision': 0.532644755953247, 'validation/accuracy': 0.9869319200515747, 'validation/loss': 0.0451381579041481, 'validation/mean_average_precision': 0.2747726059200759, 'validation/num_examples': 43793, 'test/accuracy': 0.9859846830368042, 'test/loss': 0.04820175841450691, 'test/mean_average_precision': 0.2530558781775611, 'test/num_examples': 43793, 'score': 11302.103286266327, 'total_duration': 17724.31993317604, 'accumulated_submission_time': 11302.103286266327, 'accumulated_eval_time': 6419.514434099197, 'accumulated_logging_time': 1.7393901348114014, 'global_step': 35277, 'preemption_count': 0}), (36035, {'train/accuracy': 0.9919872879981995, 'train/loss': 0.02594996802508831, 'train/mean_average_precision': 0.5178907389430005, 'validation/accuracy': 0.9868706464767456, 'validation/loss': 0.044748783111572266, 'validation/mean_average_precision': 0.2739533269282222, 'validation/num_examples': 43793, 'test/accuracy': 0.9860104322433472, 'test/loss': 0.04768946394324303, 'test/mean_average_precision': 0.26107758368161565, 'test/num_examples': 43793, 'score': 11542.113516330719, 'total_duration': 18092.018426179886, 'accumulated_submission_time': 11542.113516330719, 'accumulated_eval_time': 6547.149572849274, 'accumulated_logging_time': 1.7725646495819092, 'global_step': 36035, 'preemption_count': 0}), (36789, {'train/accuracy': 0.9918658137321472, 'train/loss': 0.026509465649724007, 'train/mean_average_precision': 0.4845961109288035, 'validation/accuracy': 0.9867196083068848, 'validation/loss': 0.045168228447437286, 'validation/mean_average_precision': 0.2734755096168028, 'validation/num_examples': 43793, 'test/accuracy': 0.9858894944190979, 'test/loss': 0.048092763870954514, 'test/mean_average_precision': 0.2523359419552278, 'test/num_examples': 43793, 'score': 11782.1854326725, 'total_duration': 18462.86779975891, 'accumulated_submission_time': 11782.1854326725, 'accumulated_eval_time': 6677.873930931091, 'accumulated_logging_time': 1.8053655624389648, 'global_step': 36789, 'preemption_count': 0}), (37526, {'train/accuracy': 0.9918019771575928, 'train/loss': 0.026524484157562256, 'train/mean_average_precision': 0.48393254705927957, 'validation/accuracy': 0.9868669509887695, 'validation/loss': 0.04479661583900452, 'validation/mean_average_precision': 0.2795964258277546, 'validation/num_examples': 43793, 'test/accuracy': 0.9860280752182007, 'test/loss': 0.047872234135866165, 'test/mean_average_precision': 0.2616498358073807, 'test/num_examples': 43793, 'score': 12022.233164787292, 'total_duration': 18831.325871944427, 'accumulated_submission_time': 12022.233164787292, 'accumulated_eval_time': 6806.229717254639, 'accumulated_logging_time': 1.8392269611358643, 'global_step': 37526, 'preemption_count': 0}), (38275, {'train/accuracy': 0.9918152093887329, 'train/loss': 0.026448266580700874, 'train/mean_average_precision': 0.4996172600643373, 'validation/accuracy': 0.9869120121002197, 'validation/loss': 0.04536278545856476, 'validation/mean_average_precision': 0.27353828843651545, 'validation/num_examples': 43793, 'test/accuracy': 0.9859615564346313, 'test/loss': 0.04834476113319397, 'test/mean_average_precision': 0.25540204635358305, 'test/num_examples': 43793, 'score': 12262.27923464775, 'total_duration': 19201.54480075836, 'accumulated_submission_time': 12262.27923464775, 'accumulated_eval_time': 6936.348298549652, 'accumulated_logging_time': 1.873117446899414, 'global_step': 38275, 'preemption_count': 0}), (39022, {'train/accuracy': 0.9920036792755127, 'train/loss': 0.025848040357232094, 'train/mean_average_precision': 0.501135744574851, 'validation/accuracy': 0.9868023991584778, 'validation/loss': 0.0449364073574543, 'validation/mean_average_precision': 0.2725329770589841, 'validation/num_examples': 43793, 'test/accuracy': 0.9859805107116699, 'test/loss': 0.04781976342201233, 'test/mean_average_precision': 0.25807255582523925, 'test/num_examples': 43793, 'score': 12502.245649576187, 'total_duration': 19568.868483543396, 'accumulated_submission_time': 12502.245649576187, 'accumulated_eval_time': 7063.6506524086, 'accumulated_logging_time': 1.9074816703796387, 'global_step': 39022, 'preemption_count': 0}), (39763, {'train/accuracy': 0.9920294880867004, 'train/loss': 0.025563078001141548, 'train/mean_average_precision': 0.5300515347756143, 'validation/accuracy': 0.9869067668914795, 'validation/loss': 0.04511991888284683, 'validation/mean_average_precision': 0.2812240884105178, 'validation/num_examples': 43793, 'test/accuracy': 0.9859746098518372, 'test/loss': 0.048158034682273865, 'test/mean_average_precision': 0.25843331896107397, 'test/num_examples': 43793, 'score': 12742.505630731583, 'total_duration': 19940.97686815262, 'accumulated_submission_time': 12742.505630731583, 'accumulated_eval_time': 7195.441838502884, 'accumulated_logging_time': 1.942678689956665, 'global_step': 39763, 'preemption_count': 0}), (40508, {'train/accuracy': 0.9923818707466125, 'train/loss': 0.024617204442620277, 'train/mean_average_precision': 0.531692979601829, 'validation/accuracy': 0.9868564009666443, 'validation/loss': 0.045163314789533615, 'validation/mean_average_precision': 0.27720460844275613, 'validation/num_examples': 43793, 'test/accuracy': 0.9860655665397644, 'test/loss': 0.04806293919682503, 'test/mean_average_precision': 0.2669514125648769, 'test/num_examples': 43793, 'score': 12982.455196619034, 'total_duration': 20309.30371117592, 'accumulated_submission_time': 12982.455196619034, 'accumulated_eval_time': 7323.763496160507, 'accumulated_logging_time': 1.9770872592926025, 'global_step': 40508, 'preemption_count': 0}), (41256, {'train/accuracy': 0.9925772547721863, 'train/loss': 0.024118391796946526, 'train/mean_average_precision': 0.5373471320030823, 'validation/accuracy': 0.986707866191864, 'validation/loss': 0.04529014974832535, 'validation/mean_average_precision': 0.27485737752421924, 'validation/num_examples': 43793, 'test/accuracy': 0.9858545660972595, 'test/loss': 0.04817128926515579, 'test/mean_average_precision': 0.25741862070010346, 'test/num_examples': 43793, 'score': 13222.612513780594, 'total_duration': 20680.74651002884, 'accumulated_submission_time': 13222.612513780594, 'accumulated_eval_time': 7454.994160413742, 'accumulated_logging_time': 2.011096239089966, 'global_step': 41256, 'preemption_count': 0}), (42004, {'train/accuracy': 0.99293053150177, 'train/loss': 0.023048240691423416, 'train/mean_average_precision': 0.5792572161274165, 'validation/accuracy': 0.9867269396781921, 'validation/loss': 0.04519191011786461, 'validation/mean_average_precision': 0.27773206183410876, 'validation/num_examples': 43793, 'test/accuracy': 0.9858461618423462, 'test/loss': 0.04808211699128151, 'test/mean_average_precision': 0.26245633353733167, 'test/num_examples': 43793, 'score': 13462.853985786438, 'total_duration': 21052.20994591713, 'accumulated_submission_time': 13462.853985786438, 'accumulated_eval_time': 7586.161582946777, 'accumulated_logging_time': 2.044940948486328, 'global_step': 42004, 'preemption_count': 0}), (42744, {'train/accuracy': 0.9928200840950012, 'train/loss': 0.023367619141936302, 'train/mean_average_precision': 0.5596890151487883, 'validation/accuracy': 0.9868795275688171, 'validation/loss': 0.04527511075139046, 'validation/mean_average_precision': 0.2799710175790842, 'validation/num_examples': 43793, 'test/accuracy': 0.9859805107116699, 'test/loss': 0.04827718809247017, 'test/mean_average_precision': 0.2657243764067328, 'test/num_examples': 43793, 'score': 13703.067671060562, 'total_duration': 21423.404257774353, 'accumulated_submission_time': 13703.067671060562, 'accumulated_eval_time': 7717.086632013321, 'accumulated_logging_time': 2.079592227935791, 'global_step': 42744, 'preemption_count': 0}), (43492, {'train/accuracy': 0.9925847053527832, 'train/loss': 0.023969653993844986, 'train/mean_average_precision': 0.5528363329090942, 'validation/accuracy': 0.9869071245193481, 'validation/loss': 0.04544226452708244, 'validation/mean_average_precision': 0.2818758340362011, 'validation/num_examples': 43793, 'test/accuracy': 0.9860390424728394, 'test/loss': 0.048316504806280136, 'test/mean_average_precision': 0.2653350767236811, 'test/num_examples': 43793, 'score': 13943.23703622818, 'total_duration': 21792.345589876175, 'accumulated_submission_time': 13943.23703622818, 'accumulated_eval_time': 7845.803071022034, 'accumulated_logging_time': 2.1141724586486816, 'global_step': 43492, 'preemption_count': 0}), (44245, {'train/accuracy': 0.9922870993614197, 'train/loss': 0.024914905428886414, 'train/mean_average_precision': 0.5299943259670108, 'validation/accuracy': 0.9868738651275635, 'validation/loss': 0.045727986842393875, 'validation/mean_average_precision': 0.27316044796499767, 'validation/num_examples': 43793, 'test/accuracy': 0.9859662055969238, 'test/loss': 0.048688486218452454, 'test/mean_average_precision': 0.25902558919256746, 'test/num_examples': 43793, 'score': 14183.470051765442, 'total_duration': 22162.540717601776, 'accumulated_submission_time': 14183.470051765442, 'accumulated_eval_time': 7975.708652496338, 'accumulated_logging_time': 2.1499216556549072, 'global_step': 44245, 'preemption_count': 0}), (45001, {'train/accuracy': 0.9922727942466736, 'train/loss': 0.024703234434127808, 'train/mean_average_precision': 0.5314834003758118, 'validation/accuracy': 0.9867748022079468, 'validation/loss': 0.04558992013335228, 'validation/mean_average_precision': 0.2845712141946099, 'validation/num_examples': 43793, 'test/accuracy': 0.9859042763710022, 'test/loss': 0.04851120710372925, 'test/mean_average_precision': 0.2633550920425738, 'test/num_examples': 43793, 'score': 14423.669402837753, 'total_duration': 22526.95771765709, 'accumulated_submission_time': 14423.669402837753, 'accumulated_eval_time': 8099.870770931244, 'accumulated_logging_time': 2.1851043701171875, 'global_step': 45001, 'preemption_count': 0}), (45751, {'train/accuracy': 0.9925000667572021, 'train/loss': 0.023962808772921562, 'train/mean_average_precision': 0.5480373339681623, 'validation/accuracy': 0.9867601990699768, 'validation/loss': 0.04606109485030174, 'validation/mean_average_precision': 0.27701092953188267, 'validation/num_examples': 43793, 'test/accuracy': 0.9860045313835144, 'test/loss': 0.048934582620859146, 'test/mean_average_precision': 0.2583359577865653, 'test/num_examples': 43793, 'score': 14663.82266998291, 'total_duration': 22893.115824699402, 'accumulated_submission_time': 14663.82266998291, 'accumulated_eval_time': 8225.82084441185, 'accumulated_logging_time': 2.220059871673584, 'global_step': 45751, 'preemption_count': 0}), (46501, {'train/accuracy': 0.9925301671028137, 'train/loss': 0.023875262588262558, 'train/mean_average_precision': 0.5455075725286589, 'validation/accuracy': 0.9867683053016663, 'validation/loss': 0.04635896533727646, 'validation/mean_average_precision': 0.2753539428337459, 'validation/num_examples': 43793, 'test/accuracy': 0.9858596324920654, 'test/loss': 0.04966329410672188, 'test/mean_average_precision': 0.2589811391864166, 'test/num_examples': 43793, 'score': 14903.953869581223, 'total_duration': 23262.441887378693, 'accumulated_submission_time': 14903.953869581223, 'accumulated_eval_time': 8354.959605455399, 'accumulated_logging_time': 2.2551698684692383, 'global_step': 46501, 'preemption_count': 0}), (47254, {'train/accuracy': 0.9927741289138794, 'train/loss': 0.023025471717119217, 'train/mean_average_precision': 0.5728452454270465, 'validation/accuracy': 0.9868178367614746, 'validation/loss': 0.04605264961719513, 'validation/mean_average_precision': 0.2810175965239018, 'validation/num_examples': 43793, 'test/accuracy': 0.9858705401420593, 'test/loss': 0.04945741966366768, 'test/mean_average_precision': 0.254589348944634, 'test/num_examples': 43793, 'score': 15144.071580171585, 'total_duration': 23633.566210269928, 'accumulated_submission_time': 15144.071580171585, 'accumulated_eval_time': 8485.91120505333, 'accumulated_logging_time': 2.2899508476257324, 'global_step': 47254, 'preemption_count': 0}), (48014, {'train/accuracy': 0.993111252784729, 'train/loss': 0.02199796587228775, 'train/mean_average_precision': 0.6028270478090696, 'validation/accuracy': 0.9867350459098816, 'validation/loss': 0.04650464653968811, 'validation/mean_average_precision': 0.27606598867261145, 'validation/num_examples': 43793, 'test/accuracy': 0.9859164953231812, 'test/loss': 0.04958193749189377, 'test/mean_average_precision': 0.2659727477785244, 'test/num_examples': 43793, 'score': 15384.28563117981, 'total_duration': 23999.686259269714, 'accumulated_submission_time': 15384.28563117981, 'accumulated_eval_time': 8611.761420249939, 'accumulated_logging_time': 2.3253207206726074, 'global_step': 48014, 'preemption_count': 0}), (48776, {'train/accuracy': 0.9933006167411804, 'train/loss': 0.021312803030014038, 'train/mean_average_precision': 0.6074992806664145, 'validation/accuracy': 0.9868174195289612, 'validation/loss': 0.04697003215551376, 'validation/mean_average_precision': 0.27183009617920206, 'validation/num_examples': 43793, 'test/accuracy': 0.9858773350715637, 'test/loss': 0.05038075894117355, 'test/mean_average_precision': 0.2556186798312245, 'test/num_examples': 43793, 'score': 15624.298836946487, 'total_duration': 24362.35687804222, 'accumulated_submission_time': 15624.298836946487, 'accumulated_eval_time': 8734.362447023392, 'accumulated_logging_time': 2.3620529174804688, 'global_step': 48776, 'preemption_count': 0}), (49522, {'train/accuracy': 0.9936766624450684, 'train/loss': 0.02044110931456089, 'train/mean_average_precision': 0.62012147547647, 'validation/accuracy': 0.9867147207260132, 'validation/loss': 0.04665745049715042, 'validation/mean_average_precision': 0.27573759577391055, 'validation/num_examples': 43793, 'test/accuracy': 0.9858027696609497, 'test/loss': 0.04974200949072838, 'test/mean_average_precision': 0.25762268551794937, 'test/num_examples': 43793, 'score': 15864.43145275116, 'total_duration': 24729.104751110077, 'accumulated_submission_time': 15864.43145275116, 'accumulated_eval_time': 8860.920503616333, 'accumulated_logging_time': 2.397468328475952, 'global_step': 49522, 'preemption_count': 0}), (50268, {'train/accuracy': 0.9936593770980835, 'train/loss': 0.020675519481301308, 'train/mean_average_precision': 0.6200574946558495, 'validation/accuracy': 0.9865986108779907, 'validation/loss': 0.04682053625583649, 'validation/mean_average_precision': 0.2705907601746179, 'validation/num_examples': 43793, 'test/accuracy': 0.9857581257820129, 'test/loss': 0.04971133917570114, 'test/mean_average_precision': 0.25703920658683305, 'test/num_examples': 43793, 'score': 16104.392055511475, 'total_duration': 25097.82476592064, 'accumulated_submission_time': 16104.392055511475, 'accumulated_eval_time': 8989.623442411423, 'accumulated_logging_time': 2.433577060699463, 'global_step': 50268, 'preemption_count': 0}), (51022, {'train/accuracy': 0.9933759570121765, 'train/loss': 0.02125820517539978, 'train/mean_average_precision': 0.6098886010964394, 'validation/accuracy': 0.9867504835128784, 'validation/loss': 0.04702604562044144, 'validation/mean_average_precision': 0.27266494475688524, 'validation/num_examples': 43793, 'test/accuracy': 0.9858331084251404, 'test/loss': 0.050172753632068634, 'test/mean_average_precision': 0.2515554863987931, 'test/num_examples': 43793, 'score': 16344.416038274765, 'total_duration': 25462.041620969772, 'accumulated_submission_time': 16344.416038274765, 'accumulated_eval_time': 9113.758713245392, 'accumulated_logging_time': 2.470756769180298, 'global_step': 51022, 'preemption_count': 0}), (51772, {'train/accuracy': 0.9933111667633057, 'train/loss': 0.021427419036626816, 'train/mean_average_precision': 0.6009365828350484, 'validation/accuracy': 0.9867772459983826, 'validation/loss': 0.047151368111371994, 'validation/mean_average_precision': 0.2797023395820893, 'validation/num_examples': 43793, 'test/accuracy': 0.9858819246292114, 'test/loss': 0.050381388515233994, 'test/mean_average_precision': 0.257556443992949, 'test/num_examples': 43793, 'score': 16584.653742313385, 'total_duration': 25830.131724357605, 'accumulated_submission_time': 16584.653742313385, 'accumulated_eval_time': 9241.55316233635, 'accumulated_logging_time': 2.508890390396118, 'global_step': 51772, 'preemption_count': 0}), (52521, {'train/accuracy': 0.9929652214050293, 'train/loss': 0.0222934540361166, 'train/mean_average_precision': 0.5837654219526973, 'validation/accuracy': 0.986805260181427, 'validation/loss': 0.04766381159424782, 'validation/mean_average_precision': 0.27680031738156924, 'validation/num_examples': 43793, 'test/accuracy': 0.9858347773551941, 'test/loss': 0.05096012353897095, 'test/mean_average_precision': 0.25749550432919716, 'test/num_examples': 43793, 'score': 16824.87526488304, 'total_duration': 26199.862973213196, 'accumulated_submission_time': 16824.87526488304, 'accumulated_eval_time': 9371.006050825119, 'accumulated_logging_time': 2.5452442169189453, 'global_step': 52521, 'preemption_count': 0}), (53276, {'train/accuracy': 0.9930841326713562, 'train/loss': 0.021920325234532356, 'train/mean_average_precision': 0.5777379497760418, 'validation/accuracy': 0.9868068695068359, 'validation/loss': 0.04758194833993912, 'validation/mean_average_precision': 0.28020843587911254, 'validation/num_examples': 43793, 'test/accuracy': 0.9858364462852478, 'test/loss': 0.050853826105594635, 'test/mean_average_precision': 0.2604646324893419, 'test/num_examples': 43793, 'score': 17064.86261534691, 'total_duration': 26565.05463194847, 'accumulated_submission_time': 17064.86261534691, 'accumulated_eval_time': 9496.152726888657, 'accumulated_logging_time': 2.5827507972717285, 'global_step': 53276, 'preemption_count': 0}), (54023, {'train/accuracy': 0.9934099316596985, 'train/loss': 0.020824594423174858, 'train/mean_average_precision': 0.6189938976762239, 'validation/accuracy': 0.9866924285888672, 'validation/loss': 0.048083286732435226, 'validation/mean_average_precision': 0.2718280150950659, 'validation/num_examples': 43793, 'test/accuracy': 0.985871434211731, 'test/loss': 0.050943344831466675, 'test/mean_average_precision': 0.26011650400011116, 'test/num_examples': 43793, 'score': 17304.836101531982, 'total_duration': 26931.035136938095, 'accumulated_submission_time': 17304.836101531982, 'accumulated_eval_time': 9622.102622747421, 'accumulated_logging_time': 2.6188244819641113, 'global_step': 54023, 'preemption_count': 0}), (54777, {'train/accuracy': 0.9935857653617859, 'train/loss': 0.020347628742456436, 'train/mean_average_precision': 0.6222725297509415, 'validation/accuracy': 0.9866132736206055, 'validation/loss': 0.04827210679650307, 'validation/mean_average_precision': 0.2787696840403332, 'validation/num_examples': 43793, 'test/accuracy': 0.9857589602470398, 'test/loss': 0.051447078585624695, 'test/mean_average_precision': 0.25579638107437663, 'test/num_examples': 43793, 'score': 17545.082458496094, 'total_duration': 27300.34612417221, 'accumulated_submission_time': 17545.082458496094, 'accumulated_eval_time': 9751.109506607056, 'accumulated_logging_time': 2.6560540199279785, 'global_step': 54777, 'preemption_count': 0}), (55511, {'train/accuracy': 0.9937710165977478, 'train/loss': 0.019580619409680367, 'train/mean_average_precision': 0.6551641993310239, 'validation/accuracy': 0.9865986108779907, 'validation/loss': 0.04900636896491051, 'validation/mean_average_precision': 0.27371629621791915, 'validation/num_examples': 43793, 'test/accuracy': 0.985806941986084, 'test/loss': 0.052032433450222015, 'test/mean_average_precision': 0.25322293049016803, 'test/num_examples': 43793, 'score': 17785.06728363037, 'total_duration': 27667.090751171112, 'accumulated_submission_time': 17785.06728363037, 'accumulated_eval_time': 9877.802606344223, 'accumulated_logging_time': 2.69838547706604, 'global_step': 55511, 'preemption_count': 0}), (56263, {'train/accuracy': 0.994708776473999, 'train/loss': 0.017203541472554207, 'train/mean_average_precision': 0.6975938120108676, 'validation/accuracy': 0.9865763187408447, 'validation/loss': 0.049066293984651566, 'validation/mean_average_precision': 0.2698884790954958, 'validation/num_examples': 43793, 'test/accuracy': 0.9857622981071472, 'test/loss': 0.05213439092040062, 'test/mean_average_precision': 0.2585453414761834, 'test/num_examples': 43793, 'score': 18025.301979780197, 'total_duration': 28026.49404001236, 'accumulated_submission_time': 18025.301979780197, 'accumulated_eval_time': 9996.913838386536, 'accumulated_logging_time': 2.7351033687591553, 'global_step': 56263, 'preemption_count': 0}), (56999, {'train/accuracy': 0.994670569896698, 'train/loss': 0.01724269986152649, 'train/mean_average_precision': 0.6762894919081208, 'validation/accuracy': 0.9865738749504089, 'validation/loss': 0.04920833185315132, 'validation/mean_average_precision': 0.2711901601753668, 'validation/num_examples': 43793, 'test/accuracy': 0.9856982827186584, 'test/loss': 0.05246062949299812, 'test/mean_average_precision': 0.2558035961147663, 'test/num_examples': 43793, 'score': 18265.25046825409, 'total_duration': 28398.62367272377, 'accumulated_submission_time': 18265.25046825409, 'accumulated_eval_time': 10129.031205415726, 'accumulated_logging_time': 2.776209592819214, 'global_step': 56999, 'preemption_count': 0})], 'global_step': 57666}
I0205 07:21:36.567952 140451058161472 submission_runner.py:586] Timing: 18477.228969812393
I0205 07:21:36.568019 140451058161472 submission_runner.py:588] Total number of evals: 77
I0205 07:21:36.568070 140451058161472 submission_runner.py:589] ====================
I0205 07:21:36.568125 140451058161472 submission_runner.py:542] Using RNG seed 449608868
I0205 07:21:36.641507 140451058161472 submission_runner.py:551] --- Tuning run 2/5 ---
I0205 07:21:36.641695 140451058161472 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_2/ogbg_jax/trial_2.
I0205 07:21:36.642051 140451058161472 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_2/ogbg_jax/trial_2/hparams.json.
I0205 07:21:36.797354 140451058161472 submission_runner.py:206] Initializing dataset.
I0205 07:21:36.904592 140451058161472 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0205 07:21:36.909062 140451058161472 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0205 07:21:37.341857 140451058161472 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0205 07:21:37.384098 140451058161472 submission_runner.py:213] Initializing model.
I0205 07:21:40.263075 140451058161472 submission_runner.py:255] Initializing optimizer.
I0205 07:21:40.909929 140451058161472 submission_runner.py:262] Initializing metrics bundle.
I0205 07:21:40.910131 140451058161472 submission_runner.py:280] Initializing checkpoint and logger.
I0205 07:21:40.910820 140451058161472 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_2/ogbg_jax/trial_2 with prefix checkpoint_
I0205 07:21:40.910954 140451058161472 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_2/ogbg_jax/trial_2/meta_data_0.json.
I0205 07:21:40.911182 140451058161472 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0205 07:21:40.911312 140451058161472 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0205 07:21:42.303451 140451058161472 logger_utils.py:220] Unable to record git information. Continuing without it.
I0205 07:21:43.683651 140451058161472 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_2/ogbg_jax/trial_2/flags_0.json.
I0205 07:21:43.692468 140451058161472 submission_runner.py:314] Starting training loop.
I0205 07:21:55.806467 140266943043328 logging_writer.py:48] [0] global_step=0, grad_norm=2.3050804138183594, loss=0.7357997298240662
I0205 07:21:55.818800 140451058161472 spec.py:321] Evaluating on the training split.
I0205 07:23:56.122340 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 07:23:59.611778 140451058161472 spec.py:349] Evaluating on the test split.
I0205 07:24:03.023449 140451058161472 submission_runner.py:408] Time since start: 139.33s, 	Step: 1, 	{'train/accuracy': 0.5289902091026306, 'train/loss': 0.7364099025726318, 'train/mean_average_precision': 0.020686487925188634, 'validation/accuracy': 0.5270814895629883, 'validation/loss': 0.737440824508667, 'validation/mean_average_precision': 0.024062548442236216, 'validation/num_examples': 43793, 'test/accuracy': 0.5256850719451904, 'test/loss': 0.7376683354377747, 'test/mean_average_precision': 0.026021847524319627, 'test/num_examples': 43793, 'score': 12.126285314559937, 'total_duration': 139.33090662956238, 'accumulated_submission_time': 12.126285314559937, 'accumulated_eval_time': 127.20457720756531, 'accumulated_logging_time': 0}
I0205 07:24:03.032860 140283262007040 logging_writer.py:48] [1] accumulated_eval_time=127.204577, accumulated_logging_time=0, accumulated_submission_time=12.126285, global_step=1, preemption_count=0, score=12.126285, test/accuracy=0.525685, test/loss=0.737668, test/mean_average_precision=0.026022, test/num_examples=43793, total_duration=139.330907, train/accuracy=0.528990, train/loss=0.736410, train/mean_average_precision=0.020686, validation/accuracy=0.527081, validation/loss=0.737441, validation/mean_average_precision=0.024063, validation/num_examples=43793
I0205 07:24:35.207265 140283731310336 logging_writer.py:48] [100] global_step=100, grad_norm=0.4555138647556305, loss=0.40146493911743164
I0205 07:25:07.654093 140283262007040 logging_writer.py:48] [200] global_step=200, grad_norm=0.3276410698890686, loss=0.29363417625427246
I0205 07:25:39.666118 140283731310336 logging_writer.py:48] [300] global_step=300, grad_norm=0.23025992512702942, loss=0.20027761161327362
I0205 07:26:11.795707 140283262007040 logging_writer.py:48] [400] global_step=400, grad_norm=0.1474514752626419, loss=0.13419878482818604
I0205 07:26:43.655253 140283731310336 logging_writer.py:48] [500] global_step=500, grad_norm=0.08871684968471527, loss=0.0991334542632103
I0205 07:27:15.530902 140283262007040 logging_writer.py:48] [600] global_step=600, grad_norm=0.07955796271562576, loss=0.07777417451143265
I0205 07:27:47.384741 140283731310336 logging_writer.py:48] [700] global_step=700, grad_norm=0.03794269263744354, loss=0.06396359205245972
I0205 07:28:03.234880 140451058161472 spec.py:321] Evaluating on the training split.
I0205 07:30:03.066694 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 07:30:06.118527 140451058161472 spec.py:349] Evaluating on the test split.
I0205 07:30:09.122214 140451058161472 submission_runner.py:408] Time since start: 505.43s, 	Step: 750, 	{'train/accuracy': 0.986754298210144, 'train/loss': 0.06418555229902267, 'train/mean_average_precision': 0.04357131042408905, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.07305673509836197, 'validation/mean_average_precision': 0.04533270888916812, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.07600926607847214, 'test/mean_average_precision': 0.04617481438971092, 'test/num_examples': 43793, 'score': 252.2958436012268, 'total_duration': 505.4296803474426, 'accumulated_submission_time': 252.2958436012268, 'accumulated_eval_time': 253.09186100959778, 'accumulated_logging_time': 0.021273136138916016}
I0205 07:30:09.138306 140266960250624 logging_writer.py:48] [750] accumulated_eval_time=253.091861, accumulated_logging_time=0.021273, accumulated_submission_time=252.295844, global_step=750, preemption_count=0, score=252.295844, test/accuracy=0.983142, test/loss=0.076009, test/mean_average_precision=0.046175, test/num_examples=43793, total_duration=505.429680, train/accuracy=0.986754, train/loss=0.064186, train/mean_average_precision=0.043571, validation/accuracy=0.984118, validation/loss=0.073057, validation/mean_average_precision=0.045333, validation/num_examples=43793
I0205 07:30:25.517676 140266968643328 logging_writer.py:48] [800] global_step=800, grad_norm=0.1069227010011673, loss=0.06539791077375412
I0205 07:30:57.576410 140266960250624 logging_writer.py:48] [900] global_step=900, grad_norm=0.10761505365371704, loss=0.056760381907224655
I0205 07:31:30.161496 140266968643328 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.05383116006851196, loss=0.05364513024687767
I0205 07:32:03.205508 140266960250624 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.0570744127035141, loss=0.04842069745063782
I0205 07:32:35.527697 140266968643328 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.03885037079453468, loss=0.05324530228972435
I0205 07:33:07.802908 140266960250624 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.04625007137656212, loss=0.04968944564461708
I0205 07:33:40.250844 140266968643328 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.04151066765189171, loss=0.05337604507803917
I0205 07:34:09.385319 140451058161472 spec.py:321] Evaluating on the training split.
I0205 07:36:12.074895 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 07:36:15.148528 140451058161472 spec.py:349] Evaluating on the test split.
I0205 07:36:18.211558 140451058161472 submission_runner.py:408] Time since start: 874.52s, 	Step: 1491, 	{'train/accuracy': 0.9871442317962646, 'train/loss': 0.048594001680612564, 'train/mean_average_precision': 0.0847953123676511, 'validation/accuracy': 0.9844759702682495, 'validation/loss': 0.05792314186692238, 'validation/mean_average_precision': 0.08769234356015555, 'validation/num_examples': 43793, 'test/accuracy': 0.9834765195846558, 'test/loss': 0.0612092986702919, 'test/mean_average_precision': 0.08469940992907997, 'test/num_examples': 43793, 'score': 492.5110273361206, 'total_duration': 874.5190241336823, 'accumulated_submission_time': 492.5110273361206, 'accumulated_eval_time': 381.91805124282837, 'accumulated_logging_time': 0.04872751235961914}
I0205 07:36:18.227780 140283530442496 logging_writer.py:48] [1491] accumulated_eval_time=381.918051, accumulated_logging_time=0.048728, accumulated_submission_time=492.511027, global_step=1491, preemption_count=0, score=492.511027, test/accuracy=0.983477, test/loss=0.061209, test/mean_average_precision=0.084699, test/num_examples=43793, total_duration=874.519024, train/accuracy=0.987144, train/loss=0.048594, train/mean_average_precision=0.084795, validation/accuracy=0.984476, validation/loss=0.057923, validation/mean_average_precision=0.087692, validation/num_examples=43793
I0205 07:36:21.485660 140283731310336 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.03424351289868355, loss=0.056716740131378174
I0205 07:36:53.669220 140283530442496 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.05296691879630089, loss=0.050951190292835236
I0205 07:37:26.202570 140283731310336 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.023551711812615395, loss=0.05385222285985947
I0205 07:37:58.466049 140283530442496 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.03180937469005585, loss=0.05275146663188934
I0205 07:38:30.513781 140283731310336 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.027549484744668007, loss=0.05316790193319321
I0205 07:39:02.582408 140283530442496 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.03321225941181183, loss=0.04996723681688309
I0205 07:39:34.722066 140283731310336 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.02639002539217472, loss=0.05018375813961029
I0205 07:40:07.326671 140283530442496 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.033986903727054596, loss=0.05034229904413223
I0205 07:40:18.490850 140451058161472 spec.py:321] Evaluating on the training split.
I0205 07:42:20.577380 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 07:42:23.622213 140451058161472 spec.py:349] Evaluating on the test split.
I0205 07:42:26.625490 140451058161472 submission_runner.py:408] Time since start: 1242.93s, 	Step: 2236, 	{'train/accuracy': 0.9874035120010376, 'train/loss': 0.04586554691195488, 'train/mean_average_precision': 0.11386966374838156, 'validation/accuracy': 0.9847410321235657, 'validation/loss': 0.05491954833269119, 'validation/mean_average_precision': 0.11226110042255129, 'validation/num_examples': 43793, 'test/accuracy': 0.9837313294410706, 'test/loss': 0.058089207857847214, 'test/mean_average_precision': 0.1110899480813678, 'test/num_examples': 43793, 'score': 732.7416000366211, 'total_duration': 1242.9329607486725, 'accumulated_submission_time': 732.7416000366211, 'accumulated_eval_time': 510.0526442527771, 'accumulated_logging_time': 0.07758808135986328}
I0205 07:42:26.641042 140266968643328 logging_writer.py:48] [2236] accumulated_eval_time=510.052644, accumulated_logging_time=0.077588, accumulated_submission_time=732.741600, global_step=2236, preemption_count=0, score=732.741600, test/accuracy=0.983731, test/loss=0.058089, test/mean_average_precision=0.111090, test/num_examples=43793, total_duration=1242.932961, train/accuracy=0.987404, train/loss=0.045866, train/mean_average_precision=0.113870, validation/accuracy=0.984741, validation/loss=0.054920, validation/mean_average_precision=0.112261, validation/num_examples=43793
I0205 07:42:47.347122 140283262007040 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.0364576056599617, loss=0.04617200046777725
I0205 07:43:19.385102 140266968643328 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.026548445224761963, loss=0.04918290674686432
I0205 07:43:51.419542 140283262007040 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.05809967592358589, loss=0.05060227960348129
I0205 07:44:23.484009 140266968643328 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.0748424306511879, loss=0.045715562999248505
I0205 07:44:55.399232 140283262007040 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.04894024878740311, loss=0.05003473162651062
I0205 07:45:28.073575 140266968643328 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.02936411276459694, loss=0.04882827401161194
I0205 07:45:59.712809 140283262007040 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.06668375432491302, loss=0.0479397289454937
I0205 07:46:26.697378 140451058161472 spec.py:321] Evaluating on the training split.
I0205 07:48:27.830956 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 07:48:30.951046 140451058161472 spec.py:349] Evaluating on the test split.
I0205 07:48:33.985305 140451058161472 submission_runner.py:408] Time since start: 1610.29s, 	Step: 2985, 	{'train/accuracy': 0.9875199794769287, 'train/loss': 0.044625088572502136, 'train/mean_average_precision': 0.14252135502158822, 'validation/accuracy': 0.9848595857620239, 'validation/loss': 0.05364485830068588, 'validation/mean_average_precision': 0.13451959985440934, 'validation/num_examples': 43793, 'test/accuracy': 0.9838774800300598, 'test/loss': 0.05656570568680763, 'test/mean_average_precision': 0.13298944726020429, 'test/num_examples': 43793, 'score': 972.7662193775177, 'total_duration': 1610.2927725315094, 'accumulated_submission_time': 972.7662193775177, 'accumulated_eval_time': 637.340523481369, 'accumulated_logging_time': 0.10427021980285645}
I0205 07:48:34.000643 140266960250624 logging_writer.py:48] [2985] accumulated_eval_time=637.340523, accumulated_logging_time=0.104270, accumulated_submission_time=972.766219, global_step=2985, preemption_count=0, score=972.766219, test/accuracy=0.983877, test/loss=0.056566, test/mean_average_precision=0.132989, test/num_examples=43793, total_duration=1610.292773, train/accuracy=0.987520, train/loss=0.044625, train/mean_average_precision=0.142521, validation/accuracy=0.984860, validation/loss=0.053645, validation/mean_average_precision=0.134520, validation/num_examples=43793
I0205 07:48:39.141904 140283530442496 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.026698123663663864, loss=0.04471588879823685
I0205 07:49:11.113211 140266960250624 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.03181428089737892, loss=0.04886237904429436
I0205 07:49:43.134370 140283530442496 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.042315371334552765, loss=0.04429994896054268
I0205 07:50:15.258029 140266960250624 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.024576440453529358, loss=0.04688386619091034
I0205 07:50:47.723620 140283530442496 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.043453387916088104, loss=0.04438594728708267
I0205 07:51:20.004719 140266960250624 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.021044038236141205, loss=0.04824135825037956
I0205 07:51:52.063892 140283530442496 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.04171188548207283, loss=0.04690766707062721
I0205 07:52:24.768328 140266960250624 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.032050721347332, loss=0.04396970197558403
I0205 07:52:34.109164 140451058161472 spec.py:321] Evaluating on the training split.
I0205 07:54:31.221443 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 07:54:34.260085 140451058161472 spec.py:349] Evaluating on the test split.
I0205 07:54:37.277135 140451058161472 submission_runner.py:408] Time since start: 1973.58s, 	Step: 3730, 	{'train/accuracy': 0.9874981641769409, 'train/loss': 0.04415629431605339, 'train/mean_average_precision': 0.16691992686249002, 'validation/accuracy': 0.9848372340202332, 'validation/loss': 0.053268492221832275, 'validation/mean_average_precision': 0.1536197460118933, 'validation/num_examples': 43793, 'test/accuracy': 0.983906090259552, 'test/loss': 0.05640191584825516, 'test/mean_average_precision': 0.1527795488106242, 'test/num_examples': 43793, 'score': 1212.8425545692444, 'total_duration': 1973.5845963954926, 'accumulated_submission_time': 1212.8425545692444, 'accumulated_eval_time': 760.508437871933, 'accumulated_logging_time': 0.13177275657653809}
I0205 07:54:37.293196 140266968643328 logging_writer.py:48] [3730] accumulated_eval_time=760.508438, accumulated_logging_time=0.131773, accumulated_submission_time=1212.842555, global_step=3730, preemption_count=0, score=1212.842555, test/accuracy=0.983906, test/loss=0.056402, test/mean_average_precision=0.152780, test/num_examples=43793, total_duration=1973.584596, train/accuracy=0.987498, train/loss=0.044156, train/mean_average_precision=0.166920, validation/accuracy=0.984837, validation/loss=0.053268, validation/mean_average_precision=0.153620, validation/num_examples=43793
I0205 07:55:00.343171 140283262007040 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.054954852908849716, loss=0.05945798382163048
I0205 07:55:32.897572 140266968643328 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.048689935356378555, loss=0.047674134373664856
I0205 07:56:04.812372 140283262007040 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.03214997053146362, loss=0.04718509316444397
I0205 07:56:36.537446 140266968643328 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.027059894055128098, loss=0.04363024979829788
I0205 07:57:08.908351 140283262007040 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.047976285219192505, loss=0.04538612440228462
I0205 07:57:40.679033 140266968643328 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.05442890524864197, loss=0.04912840202450752
I0205 07:58:12.562162 140283262007040 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.02176918275654316, loss=0.04558293893933296
I0205 07:58:37.546696 140451058161472 spec.py:321] Evaluating on the training split.
I0205 08:00:33.420494 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 08:00:36.426132 140451058161472 spec.py:349] Evaluating on the test split.
I0205 08:00:39.388069 140451058161472 submission_runner.py:408] Time since start: 2335.70s, 	Step: 4480, 	{'train/accuracy': 0.9881991744041443, 'train/loss': 0.04134407266974449, 'train/mean_average_precision': 0.19705664648587554, 'validation/accuracy': 0.9853300452232361, 'validation/loss': 0.050343018025159836, 'validation/mean_average_precision': 0.17024784114180394, 'validation/num_examples': 43793, 'test/accuracy': 0.9844414591789246, 'test/loss': 0.05320088937878609, 'test/mean_average_precision': 0.16817030768870864, 'test/num_examples': 43793, 'score': 1453.0647163391113, 'total_duration': 2335.695539712906, 'accumulated_submission_time': 1453.0647163391113, 'accumulated_eval_time': 882.3497655391693, 'accumulated_logging_time': 0.15921521186828613}
I0205 08:00:39.403865 140266960250624 logging_writer.py:48] [4480] accumulated_eval_time=882.349766, accumulated_logging_time=0.159215, accumulated_submission_time=1453.064716, global_step=4480, preemption_count=0, score=1453.064716, test/accuracy=0.984441, test/loss=0.053201, test/mean_average_precision=0.168170, test/num_examples=43793, total_duration=2335.695540, train/accuracy=0.988199, train/loss=0.041344, train/mean_average_precision=0.197057, validation/accuracy=0.985330, validation/loss=0.050343, validation/mean_average_precision=0.170248, validation/num_examples=43793
I0205 08:00:46.696649 140283731310336 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.011402606964111328, loss=0.040306344628334045
I0205 08:01:19.391752 140266960250624 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.016126278787851334, loss=0.044627849012613297
I0205 08:01:51.098039 140283731310336 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.01991857960820198, loss=0.05089184641838074
I0205 08:02:22.902469 140266960250624 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.016101352870464325, loss=0.04011064022779465
I0205 08:02:54.448305 140283731310336 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.024194613099098206, loss=0.04525668919086456
I0205 08:03:26.026616 140266960250624 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.010730063542723656, loss=0.04151616245508194
I0205 08:03:57.790718 140283731310336 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.018281886354088783, loss=0.042351190000772476
I0205 08:04:29.691316 140266960250624 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.015959693118929863, loss=0.04322698712348938
I0205 08:04:39.536669 140451058161472 spec.py:321] Evaluating on the training split.
I0205 08:06:35.361323 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 08:06:38.809448 140451058161472 spec.py:349] Evaluating on the test split.
I0205 08:06:43.920645 140451058161472 submission_runner.py:408] Time since start: 2700.23s, 	Step: 5232, 	{'train/accuracy': 0.9880143404006958, 'train/loss': 0.04127421975135803, 'train/mean_average_precision': 0.20600171914672058, 'validation/accuracy': 0.9851628541946411, 'validation/loss': 0.05104755610227585, 'validation/mean_average_precision': 0.18876825638804642, 'validation/num_examples': 43793, 'test/accuracy': 0.9842683672904968, 'test/loss': 0.054004017263650894, 'test/mean_average_precision': 0.18970394779650487, 'test/num_examples': 43793, 'score': 1693.1650228500366, 'total_duration': 2700.2280929088593, 'accumulated_submission_time': 1693.1650228500366, 'accumulated_eval_time': 1006.7336690425873, 'accumulated_logging_time': 0.18717479705810547}
I0205 08:06:43.938980 140266968643328 logging_writer.py:48] [5232] accumulated_eval_time=1006.733669, accumulated_logging_time=0.187175, accumulated_submission_time=1693.165023, global_step=5232, preemption_count=0, score=1693.165023, test/accuracy=0.984268, test/loss=0.054004, test/mean_average_precision=0.189704, test/num_examples=43793, total_duration=2700.228093, train/accuracy=0.988014, train/loss=0.041274, train/mean_average_precision=0.206002, validation/accuracy=0.985163, validation/loss=0.051048, validation/mean_average_precision=0.188768, validation/num_examples=43793
I0205 08:07:07.268208 140283530442496 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.02729363925755024, loss=0.043042294681072235
I0205 08:07:40.026966 140266968643328 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.018740352243185043, loss=0.043929845094680786
I0205 08:08:12.722939 140283530442496 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.030723949894309044, loss=0.044984690845012665
I0205 08:08:44.576001 140266968643328 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.028725875541567802, loss=0.04180368036031723
I0205 08:09:16.386641 140283530442496 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.018485892564058304, loss=0.04478856921195984
I0205 08:09:48.598863 140266968643328 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.0201309472322464, loss=0.041711337864398956
I0205 08:10:20.596897 140283530442496 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.022949811071157455, loss=0.042256955057382584
I0205 08:10:44.210490 140451058161472 spec.py:321] Evaluating on the training split.
I0205 08:12:46.475554 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 08:12:49.520551 140451058161472 spec.py:349] Evaluating on the test split.
I0205 08:12:52.583093 140451058161472 submission_runner.py:408] Time since start: 3068.89s, 	Step: 5975, 	{'train/accuracy': 0.9888017773628235, 'train/loss': 0.038616690784692764, 'train/mean_average_precision': 0.23812575126523344, 'validation/accuracy': 0.9857409000396729, 'validation/loss': 0.04839784651994705, 'validation/mean_average_precision': 0.20119131121826642, 'validation/num_examples': 43793, 'test/accuracy': 0.9848635196685791, 'test/loss': 0.051224883645772934, 'test/mean_average_precision': 0.20399773425762774, 'test/num_examples': 43793, 'score': 1933.4031188488007, 'total_duration': 3068.8905482292175, 'accumulated_submission_time': 1933.4031188488007, 'accumulated_eval_time': 1135.106214761734, 'accumulated_logging_time': 0.21738028526306152}
I0205 08:12:52.600039 140266960250624 logging_writer.py:48] [5975] accumulated_eval_time=1135.106215, accumulated_logging_time=0.217380, accumulated_submission_time=1933.403119, global_step=5975, preemption_count=0, score=1933.403119, test/accuracy=0.984864, test/loss=0.051225, test/mean_average_precision=0.203998, test/num_examples=43793, total_duration=3068.890548, train/accuracy=0.988802, train/loss=0.038617, train/mean_average_precision=0.238126, validation/accuracy=0.985741, validation/loss=0.048398, validation/mean_average_precision=0.201191, validation/num_examples=43793
I0205 08:13:01.048624 140283530442496 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.016170138493180275, loss=0.0409335196018219
I0205 08:13:33.218821 140266960250624 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.029766634106636047, loss=0.04341309890151024
I0205 08:14:05.809086 140283530442496 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.02013685368001461, loss=0.043415218591690063
I0205 08:14:37.795506 140266960250624 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.018818175420165062, loss=0.04202452674508095
I0205 08:15:10.196096 140283530442496 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.024010123685002327, loss=0.0406939871609211
I0205 08:15:42.073724 140266960250624 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.014920604415237904, loss=0.04263138025999069
I0205 08:16:14.377706 140283530442496 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.03738579526543617, loss=0.04287238419055939
I0205 08:16:46.515864 140266960250624 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.01770811900496483, loss=0.03889291733503342
I0205 08:16:52.820301 140451058161472 spec.py:321] Evaluating on the training split.
I0205 08:18:49.120240 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 08:18:52.204828 140451058161472 spec.py:349] Evaluating on the test split.
I0205 08:18:55.185670 140451058161472 submission_runner.py:408] Time since start: 3431.49s, 	Step: 6721, 	{'train/accuracy': 0.9884495735168457, 'train/loss': 0.03919443488121033, 'train/mean_average_precision': 0.24130529244074025, 'validation/accuracy': 0.9856499433517456, 'validation/loss': 0.04877936840057373, 'validation/mean_average_precision': 0.20440969556304164, 'validation/num_examples': 43793, 'test/accuracy': 0.9846937656402588, 'test/loss': 0.05163053795695305, 'test/mean_average_precision': 0.2059085480747178, 'test/num_examples': 43793, 'score': 2173.590786933899, 'total_duration': 3431.493139743805, 'accumulated_submission_time': 2173.590786933899, 'accumulated_eval_time': 1257.4715340137482, 'accumulated_logging_time': 0.24632859230041504}
I0205 08:18:55.202016 140283262007040 logging_writer.py:48] [6721] accumulated_eval_time=1257.471534, accumulated_logging_time=0.246329, accumulated_submission_time=2173.590787, global_step=6721, preemption_count=0, score=2173.590787, test/accuracy=0.984694, test/loss=0.051631, test/mean_average_precision=0.205909, test/num_examples=43793, total_duration=3431.493140, train/accuracy=0.988450, train/loss=0.039194, train/mean_average_precision=0.241305, validation/accuracy=0.985650, validation/loss=0.048779, validation/mean_average_precision=0.204410, validation/num_examples=43793
I0205 08:19:20.981235 140290213332736 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.01136095728725195, loss=0.040328867733478546
I0205 08:19:52.491008 140283262007040 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.011673983186483383, loss=0.0428200326859951
I0205 08:20:24.140302 140290213332736 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.014704464934766293, loss=0.04103747382760048
I0205 08:20:55.632368 140283262007040 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.017889274284243584, loss=0.04327468201518059
I0205 08:21:27.444524 140290213332736 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.014752509072422981, loss=0.04149049147963524
I0205 08:21:59.507377 140283262007040 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.01522959116846323, loss=0.04595009610056877
I0205 08:22:31.143846 140290213332736 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.018138786777853966, loss=0.04210062325000763
I0205 08:22:55.255811 140451058161472 spec.py:321] Evaluating on the training split.
I0205 08:24:51.054594 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 08:24:54.105747 140451058161472 spec.py:349] Evaluating on the test split.
I0205 08:24:57.161782 140451058161472 submission_runner.py:408] Time since start: 3793.47s, 	Step: 7477, 	{'train/accuracy': 0.9889029264450073, 'train/loss': 0.03751550242304802, 'train/mean_average_precision': 0.27572406125737625, 'validation/accuracy': 0.9859803915023804, 'validation/loss': 0.04746478796005249, 'validation/mean_average_precision': 0.2206059988797787, 'validation/num_examples': 43793, 'test/accuracy': 0.9850782752037048, 'test/loss': 0.050132304430007935, 'test/mean_average_precision': 0.21980030579954665, 'test/num_examples': 43793, 'score': 2413.612592458725, 'total_duration': 3793.469251871109, 'accumulated_submission_time': 2413.612592458725, 'accumulated_eval_time': 1379.377456188202, 'accumulated_logging_time': 0.27362728118896484}
I0205 08:24:57.178545 140266968643328 logging_writer.py:48] [7477] accumulated_eval_time=1379.377456, accumulated_logging_time=0.273627, accumulated_submission_time=2413.612592, global_step=7477, preemption_count=0, score=2413.612592, test/accuracy=0.985078, test/loss=0.050132, test/mean_average_precision=0.219800, test/num_examples=43793, total_duration=3793.469252, train/accuracy=0.988903, train/loss=0.037516, train/mean_average_precision=0.275724, validation/accuracy=0.985980, validation/loss=0.047465, validation/mean_average_precision=0.220606, validation/num_examples=43793
I0205 08:25:05.222229 140283731310336 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.014963685534894466, loss=0.04117051139473915
I0205 08:25:37.354902 140266968643328 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.022542066872119904, loss=0.04329361394047737
I0205 08:26:09.591343 140283731310336 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.017289264127612114, loss=0.04299803450703621
I0205 08:26:41.730306 140266968643328 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.018044322729110718, loss=0.042449623346328735
I0205 08:27:13.849164 140283731310336 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.016710177063941956, loss=0.03847995027899742
I0205 08:27:45.819861 140266968643328 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.013356858864426613, loss=0.03879588842391968
I0205 08:28:17.768257 140283731310336 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.01992386020720005, loss=0.04315818101167679
I0205 08:28:49.678290 140266968643328 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.018227586522698402, loss=0.04117390885949135
I0205 08:28:57.391750 140451058161472 spec.py:321] Evaluating on the training split.
I0205 08:30:52.754579 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 08:30:55.790583 140451058161472 spec.py:349] Evaluating on the test split.
I0205 08:30:58.797323 140451058161472 submission_runner.py:408] Time since start: 4155.10s, 	Step: 8225, 	{'train/accuracy': 0.9894757270812988, 'train/loss': 0.03590020909905434, 'train/mean_average_precision': 0.3072628440876122, 'validation/accuracy': 0.9861716032028198, 'validation/loss': 0.04675177484750748, 'validation/mean_average_precision': 0.2247504688285445, 'validation/num_examples': 43793, 'test/accuracy': 0.9852569103240967, 'test/loss': 0.04944462701678276, 'test/mean_average_precision': 0.22522695849827612, 'test/num_examples': 43793, 'score': 2653.793397426605, 'total_duration': 4155.104791641235, 'accumulated_submission_time': 2653.793397426605, 'accumulated_eval_time': 1500.782978773117, 'accumulated_logging_time': 0.30237483978271484}
I0205 08:30:58.814433 140283262007040 logging_writer.py:48] [8225] accumulated_eval_time=1500.782979, accumulated_logging_time=0.302375, accumulated_submission_time=2653.793397, global_step=8225, preemption_count=0, score=2653.793397, test/accuracy=0.985257, test/loss=0.049445, test/mean_average_precision=0.225227, test/num_examples=43793, total_duration=4155.104792, train/accuracy=0.989476, train/loss=0.035900, train/mean_average_precision=0.307263, validation/accuracy=0.986172, validation/loss=0.046752, validation/mean_average_precision=0.224750, validation/num_examples=43793
I0205 08:31:23.174784 140290213332736 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.013498155400156975, loss=0.03824300318956375
I0205 08:31:55.029862 140283262007040 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.015046779066324234, loss=0.04036945104598999
I0205 08:32:26.749287 140290213332736 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.01945110410451889, loss=0.04157226160168648
I0205 08:32:58.022071 140283262007040 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.017865736037492752, loss=0.039242617785930634
I0205 08:33:29.502037 140290213332736 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.013131845742464066, loss=0.04165089502930641
I0205 08:34:01.371386 140283262007040 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.01461503654718399, loss=0.04047762602567673
I0205 08:34:32.828618 140290213332736 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.01812409609556198, loss=0.045514605939388275
I0205 08:34:58.808446 140451058161472 spec.py:321] Evaluating on the training split.
I0205 08:36:57.625256 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 08:37:00.698020 140451058161472 spec.py:349] Evaluating on the test split.
I0205 08:37:03.991668 140451058161472 submission_runner.py:408] Time since start: 4520.30s, 	Step: 8984, 	{'train/accuracy': 0.9894742965698242, 'train/loss': 0.035279128700494766, 'train/mean_average_precision': 0.3132704878213992, 'validation/accuracy': 0.986291766166687, 'validation/loss': 0.04616290330886841, 'validation/mean_average_precision': 0.23599086679248701, 'validation/num_examples': 43793, 'test/accuracy': 0.9854257702827454, 'test/loss': 0.048779260367155075, 'test/mean_average_precision': 0.2395137327731072, 'test/num_examples': 43793, 'score': 2893.755373477936, 'total_duration': 4520.299135923386, 'accumulated_submission_time': 2893.755373477936, 'accumulated_eval_time': 1625.9661529064178, 'accumulated_logging_time': 0.33057332038879395}
I0205 08:37:04.009062 140266968643328 logging_writer.py:48] [8984] accumulated_eval_time=1625.966153, accumulated_logging_time=0.330573, accumulated_submission_time=2893.755373, global_step=8984, preemption_count=0, score=2893.755373, test/accuracy=0.985426, test/loss=0.048779, test/mean_average_precision=0.239514, test/num_examples=43793, total_duration=4520.299136, train/accuracy=0.989474, train/loss=0.035279, train/mean_average_precision=0.313270, validation/accuracy=0.986292, validation/loss=0.046163, validation/mean_average_precision=0.235991, validation/num_examples=43793
I0205 08:37:09.491346 140283530442496 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.018101559951901436, loss=0.042578741908073425
I0205 08:37:41.416008 140266968643328 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.02366812713444233, loss=0.0407538004219532
I0205 08:38:13.685770 140283530442496 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.018664520233869553, loss=0.03760087490081787
I0205 08:38:45.626018 140266968643328 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.018210696056485176, loss=0.0401768796145916
I0205 08:39:17.822548 140283530442496 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.023688556626439095, loss=0.04111379012465477
I0205 08:39:49.596654 140266968643328 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.016786828637123108, loss=0.039084307849407196
I0205 08:40:21.756094 140283530442496 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.016641737893223763, loss=0.04406661540269852
I0205 08:40:53.824826 140266968643328 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.020552191883325577, loss=0.03908591344952583
I0205 08:41:04.079859 140451058161472 spec.py:321] Evaluating on the training split.
I0205 08:43:00.825744 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 08:43:04.347561 140451058161472 spec.py:349] Evaluating on the test split.
I0205 08:43:07.675200 140451058161472 submission_runner.py:408] Time since start: 4883.98s, 	Step: 9733, 	{'train/accuracy': 0.9896693229675293, 'train/loss': 0.03427824005484581, 'train/mean_average_precision': 0.3510216167629603, 'validation/accuracy': 0.9862142205238342, 'validation/loss': 0.04642321169376373, 'validation/mean_average_precision': 0.2376936664874003, 'validation/num_examples': 43793, 'test/accuracy': 0.9853495359420776, 'test/loss': 0.04912646859884262, 'test/mean_average_precision': 0.2371165836262182, 'test/num_examples': 43793, 'score': 3133.7946379184723, 'total_duration': 4883.982634544373, 'accumulated_submission_time': 3133.7946379184723, 'accumulated_eval_time': 1749.5614104270935, 'accumulated_logging_time': 0.358844518661499}
I0205 08:43:07.694939 140283731310336 logging_writer.py:48] [9733] accumulated_eval_time=1749.561410, accumulated_logging_time=0.358845, accumulated_submission_time=3133.794638, global_step=9733, preemption_count=0, score=3133.794638, test/accuracy=0.985350, test/loss=0.049126, test/mean_average_precision=0.237117, test/num_examples=43793, total_duration=4883.982635, train/accuracy=0.989669, train/loss=0.034278, train/mean_average_precision=0.351022, validation/accuracy=0.986214, validation/loss=0.046423, validation/mean_average_precision=0.237694, validation/num_examples=43793
I0205 08:43:29.839210 140290213332736 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.013894429430365562, loss=0.03934911638498306
I0205 08:44:02.243246 140283731310336 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.02978687360882759, loss=0.043522246181964874
I0205 08:44:35.310952 140290213332736 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.015353219583630562, loss=0.03898785635828972
I0205 08:45:08.222624 140283731310336 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.014292835257947445, loss=0.0367240272462368
I0205 08:45:39.634027 140290213332736 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.01747349463403225, loss=0.038587700575590134
I0205 08:46:11.407991 140283731310336 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.015619233250617981, loss=0.04046715423464775
I0205 08:46:42.674212 140290213332736 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.013233174569904804, loss=0.03740311041474342
I0205 08:47:07.938662 140451058161472 spec.py:321] Evaluating on the training split.
I0205 08:49:04.213735 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 08:49:07.266290 140451058161472 spec.py:349] Evaluating on the test split.
I0205 08:49:10.253752 140451058161472 submission_runner.py:408] Time since start: 5246.56s, 	Step: 10481, 	{'train/accuracy': 0.9896564483642578, 'train/loss': 0.033921267837285995, 'train/mean_average_precision': 0.3735877274136109, 'validation/accuracy': 0.9863014817237854, 'validation/loss': 0.04627609625458717, 'validation/mean_average_precision': 0.2454329941619501, 'validation/num_examples': 43793, 'test/accuracy': 0.9854468703269958, 'test/loss': 0.049061618745326996, 'test/mean_average_precision': 0.24721478526615945, 'test/num_examples': 43793, 'score': 3374.0041666030884, 'total_duration': 5246.561218261719, 'accumulated_submission_time': 3374.0041666030884, 'accumulated_eval_time': 1871.8764510154724, 'accumulated_logging_time': 0.39055585861206055}
I0205 08:49:10.271789 140283262007040 logging_writer.py:48] [10481] accumulated_eval_time=1871.876451, accumulated_logging_time=0.390556, accumulated_submission_time=3374.004167, global_step=10481, preemption_count=0, score=3374.004167, test/accuracy=0.985447, test/loss=0.049062, test/mean_average_precision=0.247215, test/num_examples=43793, total_duration=5246.561218, train/accuracy=0.989656, train/loss=0.033921, train/mean_average_precision=0.373588, validation/accuracy=0.986301, validation/loss=0.046276, validation/mean_average_precision=0.245433, validation/num_examples=43793
I0205 08:49:16.670404 140283530442496 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.015522065572440624, loss=0.039854876697063446
I0205 08:49:48.470661 140283262007040 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.012316769920289516, loss=0.036504510790109634
I0205 08:50:20.350982 140283530442496 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.015834586694836617, loss=0.0390285849571228
I0205 08:50:51.899682 140283262007040 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.014680378139019012, loss=0.03827495500445366
I0205 08:51:23.546212 140283530442496 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.01709592342376709, loss=0.04064306616783142
I0205 08:51:55.229681 140283262007040 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.015916122123599052, loss=0.03964211419224739
I0205 08:52:27.484272 140283530442496 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.01998800039291382, loss=0.04019813612103462
I0205 08:52:59.172894 140283262007040 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.018562251701951027, loss=0.0417061522603035
I0205 08:53:10.526434 140451058161472 spec.py:321] Evaluating on the training split.
I0205 08:55:06.245446 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 08:55:09.353160 140451058161472 spec.py:349] Evaluating on the test split.
I0205 08:55:12.333737 140451058161472 submission_runner.py:408] Time since start: 5608.64s, 	Step: 11236, 	{'train/accuracy': 0.9896635413169861, 'train/loss': 0.03401583433151245, 'train/mean_average_precision': 0.3581437473261031, 'validation/accuracy': 0.9863518476486206, 'validation/loss': 0.04625559598207474, 'validation/mean_average_precision': 0.2520758598756026, 'validation/num_examples': 43793, 'test/accuracy': 0.9854127168655396, 'test/loss': 0.049329109489917755, 'test/mean_average_precision': 0.24701030213277722, 'test/num_examples': 43793, 'score': 3614.2272896766663, 'total_duration': 5608.641200304031, 'accumulated_submission_time': 3614.2272896766663, 'accumulated_eval_time': 1993.6837046146393, 'accumulated_logging_time': 0.41956257820129395}
I0205 08:55:12.352068 140266968643328 logging_writer.py:48] [11236] accumulated_eval_time=1993.683705, accumulated_logging_time=0.419563, accumulated_submission_time=3614.227290, global_step=11236, preemption_count=0, score=3614.227290, test/accuracy=0.985413, test/loss=0.049329, test/mean_average_precision=0.247010, test/num_examples=43793, total_duration=5608.641200, train/accuracy=0.989664, train/loss=0.034016, train/mean_average_precision=0.358144, validation/accuracy=0.986352, validation/loss=0.046256, validation/mean_average_precision=0.252076, validation/num_examples=43793
I0205 08:55:32.850704 140290213332736 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.01847076788544655, loss=0.038990650326013565
I0205 08:56:04.284528 140266968643328 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.018623484298586845, loss=0.03909974545240402
I0205 08:56:36.001226 140290213332736 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.017598500475287437, loss=0.04036524519324303
I0205 08:57:07.927583 140266968643328 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.017066940665245056, loss=0.04001709446310997
I0205 08:57:39.875439 140290213332736 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.015059776604175568, loss=0.038980741053819656
I0205 08:58:11.714044 140266968643328 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.018523095175623894, loss=0.0387430265545845
I0205 08:58:43.592871 140290213332736 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.01883389987051487, loss=0.03872688114643097
I0205 08:59:12.636846 140451058161472 spec.py:321] Evaluating on the training split.
I0205 09:01:06.866628 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 09:01:09.924160 140451058161472 spec.py:349] Evaluating on the test split.
I0205 09:01:12.945974 140451058161472 submission_runner.py:408] Time since start: 5969.25s, 	Step: 11992, 	{'train/accuracy': 0.9899739623069763, 'train/loss': 0.03308432921767235, 'train/mean_average_precision': 0.37030113663788566, 'validation/accuracy': 0.9864541292190552, 'validation/loss': 0.04605422168970108, 'validation/mean_average_precision': 0.25101093124939455, 'validation/num_examples': 43793, 'test/accuracy': 0.9854961037635803, 'test/loss': 0.04903990775346756, 'test/mean_average_precision': 0.2538872922350826, 'test/num_examples': 43793, 'score': 3854.4807589054108, 'total_duration': 5969.253441572189, 'accumulated_submission_time': 3854.4807589054108, 'accumulated_eval_time': 2113.992784023285, 'accumulated_logging_time': 0.4489624500274658}
I0205 09:01:12.963908 140283262007040 logging_writer.py:48] [11992] accumulated_eval_time=2113.992784, accumulated_logging_time=0.448962, accumulated_submission_time=3854.480759, global_step=11992, preemption_count=0, score=3854.480759, test/accuracy=0.985496, test/loss=0.049040, test/mean_average_precision=0.253887, test/num_examples=43793, total_duration=5969.253442, train/accuracy=0.989974, train/loss=0.033084, train/mean_average_precision=0.370301, validation/accuracy=0.986454, validation/loss=0.046054, validation/mean_average_precision=0.251011, validation/num_examples=43793
I0205 09:01:15.877483 140283731310336 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.020818807184696198, loss=0.03783916309475899
I0205 09:01:48.011060 140283262007040 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.01876719295978546, loss=0.03844541683793068
I0205 09:02:19.840106 140283731310336 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.02039623260498047, loss=0.03806502744555473
I0205 09:02:51.626735 140283262007040 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.01819234900176525, loss=0.03758624568581581
I0205 09:03:24.326030 140283731310336 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.014686275273561478, loss=0.03903253749012947
I0205 09:03:56.504128 140283262007040 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.016265517100691795, loss=0.03677070513367653
I0205 09:04:28.295098 140283731310336 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.0184123907238245, loss=0.04007139056921005
I0205 09:04:59.830401 140283262007040 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.01943395473062992, loss=0.03955583646893501
I0205 09:05:13.143453 140451058161472 spec.py:321] Evaluating on the training split.
I0205 09:07:05.368860 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 09:07:08.482874 140451058161472 spec.py:349] Evaluating on the test split.
I0205 09:07:11.496158 140451058161472 submission_runner.py:408] Time since start: 6327.80s, 	Step: 12742, 	{'train/accuracy': 0.9902027249336243, 'train/loss': 0.03249253332614899, 'train/mean_average_precision': 0.38287722729232115, 'validation/accuracy': 0.9865673780441284, 'validation/loss': 0.04554583132266998, 'validation/mean_average_precision': 0.2547032048499188, 'validation/num_examples': 43793, 'test/accuracy': 0.9856456518173218, 'test/loss': 0.048312168568372726, 'test/mean_average_precision': 0.2498462761455225, 'test/num_examples': 43793, 'score': 4094.6281826496124, 'total_duration': 6327.803616523743, 'accumulated_submission_time': 4094.6281826496124, 'accumulated_eval_time': 2232.3454310894012, 'accumulated_logging_time': 0.4781973361968994}
I0205 09:07:11.514469 140283530442496 logging_writer.py:48] [12742] accumulated_eval_time=2232.345431, accumulated_logging_time=0.478197, accumulated_submission_time=4094.628183, global_step=12742, preemption_count=0, score=4094.628183, test/accuracy=0.985646, test/loss=0.048312, test/mean_average_precision=0.249846, test/num_examples=43793, total_duration=6327.803617, train/accuracy=0.990203, train/loss=0.032493, train/mean_average_precision=0.382877, validation/accuracy=0.986567, validation/loss=0.045546, validation/mean_average_precision=0.254703, validation/num_examples=43793
I0205 09:07:30.292275 140290213332736 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.020100805908441544, loss=0.03944634273648262
I0205 09:08:02.964870 140283530442496 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.019316110759973526, loss=0.03720699995756149
I0205 09:08:34.747680 140290213332736 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.015200508758425713, loss=0.03449421375989914
I0205 09:09:06.720192 140283530442496 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.01593613065779209, loss=0.03733554482460022
I0205 09:09:38.467280 140290213332736 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.015508544631302357, loss=0.0352812334895134
I0205 09:10:10.391216 140283530442496 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.01525238435715437, loss=0.03740232437849045
I0205 09:10:42.357709 140290213332736 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.01667560264468193, loss=0.03610256686806679
I0205 09:11:11.545421 140451058161472 spec.py:321] Evaluating on the training split.
I0205 09:13:04.482259 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 09:13:07.924479 140451058161472 spec.py:349] Evaluating on the test split.
I0205 09:13:11.289242 140451058161472 submission_runner.py:408] Time since start: 6687.60s, 	Step: 13491, 	{'train/accuracy': 0.9905490279197693, 'train/loss': 0.03170744329690933, 'train/mean_average_precision': 0.39984651207485566, 'validation/accuracy': 0.9866088032722473, 'validation/loss': 0.045085567981004715, 'validation/mean_average_precision': 0.25580061287290407, 'validation/num_examples': 43793, 'test/accuracy': 0.9857509732246399, 'test/loss': 0.04791153594851494, 'test/mean_average_precision': 0.2501149207185102, 'test/num_examples': 43793, 'score': 4334.628254652023, 'total_duration': 6687.5966901779175, 'accumulated_submission_time': 4334.628254652023, 'accumulated_eval_time': 2352.0891876220703, 'accumulated_logging_time': 0.5074634552001953}
I0205 09:13:11.311307 140266968643328 logging_writer.py:48] [13491] accumulated_eval_time=2352.089188, accumulated_logging_time=0.507463, accumulated_submission_time=4334.628255, global_step=13491, preemption_count=0, score=4334.628255, test/accuracy=0.985751, test/loss=0.047912, test/mean_average_precision=0.250115, test/num_examples=43793, total_duration=6687.596690, train/accuracy=0.990549, train/loss=0.031707, train/mean_average_precision=0.399847, validation/accuracy=0.986609, validation/loss=0.045086, validation/mean_average_precision=0.255801, validation/num_examples=43793
I0205 09:13:14.673875 140283262007040 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.02003808319568634, loss=0.03765475004911423
I0205 09:13:47.226614 140266968643328 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.018968813121318817, loss=0.038634054362773895
I0205 09:14:19.564493 140283262007040 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.01757119596004486, loss=0.036826036870479584
I0205 09:14:51.255632 140266968643328 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.018071677535772324, loss=0.03405618667602539
I0205 09:15:22.837592 140283262007040 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.015926649793982506, loss=0.037539560347795486
I0205 09:15:54.444066 140266968643328 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.01938461884856224, loss=0.0371767021715641
I0205 09:16:26.284463 140283262007040 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.01672688126564026, loss=0.038977667689323425
I0205 09:16:57.871101 140266968643328 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.021077079698443413, loss=0.03658416122198105
I0205 09:17:11.425861 140451058161472 spec.py:321] Evaluating on the training split.
I0205 09:19:06.407000 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 09:19:09.479592 140451058161472 spec.py:349] Evaluating on the test split.
I0205 09:19:12.481457 140451058161472 submission_runner.py:408] Time since start: 7048.79s, 	Step: 14243, 	{'train/accuracy': 0.9906753897666931, 'train/loss': 0.03099401853978634, 'train/mean_average_precision': 0.42887643696747557, 'validation/accuracy': 0.9867565631866455, 'validation/loss': 0.04504191875457764, 'validation/mean_average_precision': 0.2566420787519, 'validation/num_examples': 43793, 'test/accuracy': 0.9858794212341309, 'test/loss': 0.04778122156858444, 'test/mean_average_precision': 0.2592707509360255, 'test/num_examples': 43793, 'score': 4574.7080709934235, 'total_duration': 7048.78892493248, 'accumulated_submission_time': 4574.7080709934235, 'accumulated_eval_time': 2473.1447324752808, 'accumulated_logging_time': 0.5425519943237305}
I0205 09:19:12.499413 140283530442496 logging_writer.py:48] [14243] accumulated_eval_time=2473.144732, accumulated_logging_time=0.542552, accumulated_submission_time=4574.708071, global_step=14243, preemption_count=0, score=4574.708071, test/accuracy=0.985879, test/loss=0.047781, test/mean_average_precision=0.259271, test/num_examples=43793, total_duration=7048.788925, train/accuracy=0.990675, train/loss=0.030994, train/mean_average_precision=0.428876, validation/accuracy=0.986757, validation/loss=0.045042, validation/mean_average_precision=0.256642, validation/num_examples=43793
I0205 09:19:31.415696 140290213332736 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.01956835575401783, loss=0.03796185180544853
I0205 09:20:04.183825 140283530442496 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.017068088054656982, loss=0.0374661460518837
I0205 09:20:36.331409 140290213332736 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.015561919659376144, loss=0.034511856734752655
I0205 09:21:08.362173 140283530442496 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.01789660006761551, loss=0.037517473101615906
I0205 09:21:40.287187 140290213332736 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.022212034091353416, loss=0.03692222759127617
I0205 09:22:12.779912 140283530442496 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.018780244514346123, loss=0.033434584736824036
I0205 09:22:44.878447 140290213332736 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.02011238783597946, loss=0.03572315350174904
I0205 09:23:12.518254 140451058161472 spec.py:321] Evaluating on the training split.
I0205 09:25:07.207458 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 09:25:10.676677 140451058161472 spec.py:349] Evaluating on the test split.
I0205 09:25:14.047586 140451058161472 submission_runner.py:408] Time since start: 7410.36s, 	Step: 14988, 	{'train/accuracy': 0.9906166195869446, 'train/loss': 0.0303849708288908, 'train/mean_average_precision': 0.42623232653305093, 'validation/accuracy': 0.9866489768028259, 'validation/loss': 0.045590728521347046, 'validation/mean_average_precision': 0.25932788192480427, 'validation/num_examples': 43793, 'test/accuracy': 0.9857316017150879, 'test/loss': 0.048439156264066696, 'test/mean_average_precision': 0.2535050643187877, 'test/num_examples': 43793, 'score': 4814.695953845978, 'total_duration': 7410.355037212372, 'accumulated_submission_time': 4814.695953845978, 'accumulated_eval_time': 2594.674001932144, 'accumulated_logging_time': 0.5714969635009766}
I0205 09:25:14.067936 140266968643328 logging_writer.py:48] [14988] accumulated_eval_time=2594.674002, accumulated_logging_time=0.571497, accumulated_submission_time=4814.695954, global_step=14988, preemption_count=0, score=4814.695954, test/accuracy=0.985732, test/loss=0.048439, test/mean_average_precision=0.253505, test/num_examples=43793, total_duration=7410.355037, train/accuracy=0.990617, train/loss=0.030385, train/mean_average_precision=0.426232, validation/accuracy=0.986649, validation/loss=0.045591, validation/mean_average_precision=0.259328, validation/num_examples=43793
I0205 09:25:18.422961 140283731310336 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.020786678418517113, loss=0.03726428374648094
I0205 09:25:51.038754 140266968643328 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.016851022839546204, loss=0.034956786781549454
I0205 09:26:23.796236 140283731310336 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.022459276020526886, loss=0.03812817111611366
I0205 09:26:56.086426 140266968643328 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.022974777966737747, loss=0.03557557612657547
I0205 09:27:28.793549 140283731310336 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.019130831584334373, loss=0.035530831664800644
I0205 09:28:01.437488 140266968643328 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.019201671704649925, loss=0.03654228150844574
I0205 09:28:33.904993 140283731310336 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.02225416526198387, loss=0.03720372915267944
I0205 09:29:06.513150 140266968643328 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.023206161335110664, loss=0.03596135228872299
I0205 09:29:14.181614 140451058161472 spec.py:321] Evaluating on the training split.
I0205 09:31:08.788738 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 09:31:11.891118 140451058161472 spec.py:349] Evaluating on the test split.
I0205 09:31:14.906521 140451058161472 submission_runner.py:408] Time since start: 7771.21s, 	Step: 15725, 	{'train/accuracy': 0.9909983277320862, 'train/loss': 0.02954896353185177, 'train/mean_average_precision': 0.454112727750228, 'validation/accuracy': 0.9866875410079956, 'validation/loss': 0.045338816940784454, 'validation/mean_average_precision': 0.25527725304688215, 'validation/num_examples': 43793, 'test/accuracy': 0.985811173915863, 'test/loss': 0.048067834228277206, 'test/mean_average_precision': 0.25743244548876176, 'test/num_examples': 43793, 'score': 5054.773876190186, 'total_duration': 7771.213989019394, 'accumulated_submission_time': 5054.773876190186, 'accumulated_eval_time': 2715.3988678455353, 'accumulated_logging_time': 0.6034946441650391}
I0205 09:31:14.924875 140283262007040 logging_writer.py:48] [15725] accumulated_eval_time=2715.398868, accumulated_logging_time=0.603495, accumulated_submission_time=5054.773876, global_step=15725, preemption_count=0, score=5054.773876, test/accuracy=0.985811, test/loss=0.048068, test/mean_average_precision=0.257432, test/num_examples=43793, total_duration=7771.213989, train/accuracy=0.990998, train/loss=0.029549, train/mean_average_precision=0.454113, validation/accuracy=0.986688, validation/loss=0.045339, validation/mean_average_precision=0.255277, validation/num_examples=43793
I0205 09:31:40.592512 140290213332736 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.017015691846609116, loss=0.03589281812310219
I0205 09:32:12.927039 140283262007040 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.019306769594550133, loss=0.03836476802825928
I0205 09:32:44.721719 140290213332736 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.01800413802266121, loss=0.03597727045416832
I0205 09:33:16.977070 140283262007040 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.01979241520166397, loss=0.036893080919981
I0205 09:33:48.670849 140290213332736 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.023861613124608994, loss=0.03677952289581299
I0205 09:34:20.381559 140283262007040 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.023680439218878746, loss=0.037431009113788605
I0205 09:34:52.216556 140290213332736 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.026805279776453972, loss=0.0374886691570282
I0205 09:35:15.000486 140451058161472 spec.py:321] Evaluating on the training split.
I0205 09:37:08.626839 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 09:37:11.727397 140451058161472 spec.py:349] Evaluating on the test split.
I0205 09:37:14.733160 140451058161472 submission_runner.py:408] Time since start: 8131.04s, 	Step: 16472, 	{'train/accuracy': 0.991483211517334, 'train/loss': 0.027779418975114822, 'train/mean_average_precision': 0.48929640670339225, 'validation/accuracy': 0.9867638349533081, 'validation/loss': 0.04555807635188103, 'validation/mean_average_precision': 0.2630052954278285, 'validation/num_examples': 43793, 'test/accuracy': 0.985897958278656, 'test/loss': 0.04825650900602341, 'test/mean_average_precision': 0.256947923922455, 'test/num_examples': 43793, 'score': 5294.818076848984, 'total_duration': 8131.040618658066, 'accumulated_submission_time': 5294.818076848984, 'accumulated_eval_time': 2835.131489276886, 'accumulated_logging_time': 0.632915735244751}
I0205 09:37:14.751797 140266968643328 logging_writer.py:48] [16472] accumulated_eval_time=2835.131489, accumulated_logging_time=0.632916, accumulated_submission_time=5294.818077, global_step=16472, preemption_count=0, score=5294.818077, test/accuracy=0.985898, test/loss=0.048257, test/mean_average_precision=0.256948, test/num_examples=43793, total_duration=8131.040619, train/accuracy=0.991483, train/loss=0.027779, train/mean_average_precision=0.489296, validation/accuracy=0.986764, validation/loss=0.045558, validation/mean_average_precision=0.263005, validation/num_examples=43793
I0205 09:37:24.074574 140283731310336 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.023231249302625656, loss=0.03883541002869606
I0205 09:37:55.771788 140266968643328 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.02078813873231411, loss=0.03828940540552139
I0205 09:38:27.948770 140283731310336 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.024964306503534317, loss=0.037725768983364105
I0205 09:39:00.579775 140266968643328 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.019690480083227158, loss=0.03585933521389961
I0205 09:39:32.731803 140283731310336 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.026373255997896194, loss=0.03534887358546257
I0205 09:40:05.571118 140266968643328 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.02805228903889656, loss=0.036709100008010864
I0205 09:40:37.836227 140283731310336 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.020546650514006615, loss=0.034437838941812515
I0205 09:41:10.178246 140266968643328 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.01993078924715519, loss=0.03574956953525543
I0205 09:41:15.011382 140451058161472 spec.py:321] Evaluating on the training split.
I0205 09:43:06.940910 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 09:43:09.987220 140451058161472 spec.py:349] Evaluating on the test split.
I0205 09:43:13.045092 140451058161472 submission_runner.py:408] Time since start: 8489.35s, 	Step: 17216, 	{'train/accuracy': 0.9915978908538818, 'train/loss': 0.0274420864880085, 'train/mean_average_precision': 0.5071297348406607, 'validation/accuracy': 0.9867764711380005, 'validation/loss': 0.04545893147587776, 'validation/mean_average_precision': 0.25982477374850405, 'validation/num_examples': 43793, 'test/accuracy': 0.9859485030174255, 'test/loss': 0.048284754157066345, 'test/mean_average_precision': 0.25943890002891323, 'test/num_examples': 43793, 'score': 5535.046224355698, 'total_duration': 8489.352545261383, 'accumulated_submission_time': 5535.046224355698, 'accumulated_eval_time': 2953.16513299942, 'accumulated_logging_time': 0.6627569198608398}
I0205 09:43:13.063167 140283262007040 logging_writer.py:48] [17216] accumulated_eval_time=2953.165133, accumulated_logging_time=0.662757, accumulated_submission_time=5535.046224, global_step=17216, preemption_count=0, score=5535.046224, test/accuracy=0.985949, test/loss=0.048285, test/mean_average_precision=0.259439, test/num_examples=43793, total_duration=8489.352545, train/accuracy=0.991598, train/loss=0.027442, train/mean_average_precision=0.507130, validation/accuracy=0.986776, validation/loss=0.045459, validation/mean_average_precision=0.259825, validation/num_examples=43793
I0205 09:43:40.107076 140290213332736 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.018983734771609306, loss=0.03423149883747101
I0205 09:44:12.089261 140283262007040 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.0208102036267519, loss=0.033941831439733505
I0205 09:44:43.563440 140290213332736 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.02358945459127426, loss=0.0370635911822319
I0205 09:45:15.497502 140283262007040 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.02816755697131157, loss=0.033554043620824814
I0205 09:45:47.326784 140290213332736 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.01870979182422161, loss=0.033542096614837646
I0205 09:46:19.207645 140283262007040 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.029454492032527924, loss=0.035019174218177795
I0205 09:46:51.371041 140290213332736 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.023690616711974144, loss=0.03543786332011223
I0205 09:47:13.347434 140451058161472 spec.py:321] Evaluating on the training split.
I0205 09:49:04.068246 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 09:49:07.114544 140451058161472 spec.py:349] Evaluating on the test split.
I0205 09:49:10.130023 140451058161472 submission_runner.py:408] Time since start: 8846.44s, 	Step: 17970, 	{'train/accuracy': 0.991574227809906, 'train/loss': 0.027520490810275078, 'train/mean_average_precision': 0.5063725356236961, 'validation/accuracy': 0.9866810441017151, 'validation/loss': 0.045769862830638885, 'validation/mean_average_precision': 0.25497578991680975, 'validation/num_examples': 43793, 'test/accuracy': 0.985820472240448, 'test/loss': 0.04843926429748535, 'test/mean_average_precision': 0.25534367132578084, 'test/num_examples': 43793, 'score': 5775.2991716861725, 'total_duration': 8846.437492847443, 'accumulated_submission_time': 5775.2991716861725, 'accumulated_eval_time': 3069.9476771354675, 'accumulated_logging_time': 0.691856861114502}
I0205 09:49:10.148753 140266968643328 logging_writer.py:48] [17970] accumulated_eval_time=3069.947677, accumulated_logging_time=0.691857, accumulated_submission_time=5775.299172, global_step=17970, preemption_count=0, score=5775.299172, test/accuracy=0.985820, test/loss=0.048439, test/mean_average_precision=0.255344, test/num_examples=43793, total_duration=8846.437493, train/accuracy=0.991574, train/loss=0.027520, train/mean_average_precision=0.506373, validation/accuracy=0.986681, validation/loss=0.045770, validation/mean_average_precision=0.254976, validation/num_examples=43793
I0205 09:49:20.136805 140283731310336 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.02260163612663746, loss=0.03420732915401459
I0205 09:49:52.092370 140266968643328 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.028824863955378532, loss=0.03776489943265915
I0205 09:50:24.029818 140283731310336 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.02062126435339451, loss=0.033774275332689285
I0205 09:50:55.748403 140266968643328 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.027622943744063377, loss=0.035217203199863434
I0205 09:51:28.625759 140283731310336 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.019695598632097244, loss=0.03540584817528725
I0205 09:52:01.364875 140266968643328 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.020854758098721504, loss=0.035161081701517105
I0205 09:52:34.032882 140283731310336 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.027689587324857712, loss=0.03541313856840134
I0205 09:53:06.601868 140266968643328 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.02836359478533268, loss=0.03629567474126816
I0205 09:53:10.132025 140451058161472 spec.py:321] Evaluating on the training split.
I0205 09:55:07.934871 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 09:55:11.046243 140451058161472 spec.py:349] Evaluating on the test split.
I0205 09:55:14.144868 140451058161472 submission_runner.py:408] Time since start: 9210.45s, 	Step: 18712, 	{'train/accuracy': 0.9916909337043762, 'train/loss': 0.027664432302117348, 'train/mean_average_precision': 0.4879586830864612, 'validation/accuracy': 0.9867812991142273, 'validation/loss': 0.04561914876103401, 'validation/mean_average_precision': 0.2659859739068194, 'validation/num_examples': 43793, 'test/accuracy': 0.9859510064125061, 'test/loss': 0.048301104456186295, 'test/mean_average_precision': 0.2602272453178065, 'test/num_examples': 43793, 'score': 6015.247898340225, 'total_duration': 9210.452338218689, 'accumulated_submission_time': 6015.247898340225, 'accumulated_eval_time': 3193.9604799747467, 'accumulated_logging_time': 0.723224401473999}
I0205 09:55:14.163632 140283530442496 logging_writer.py:48] [18712] accumulated_eval_time=3193.960480, accumulated_logging_time=0.723224, accumulated_submission_time=6015.247898, global_step=18712, preemption_count=0, score=6015.247898, test/accuracy=0.985951, test/loss=0.048301, test/mean_average_precision=0.260227, test/num_examples=43793, total_duration=9210.452338, train/accuracy=0.991691, train/loss=0.027664, train/mean_average_precision=0.487959, validation/accuracy=0.986781, validation/loss=0.045619, validation/mean_average_precision=0.265986, validation/num_examples=43793
I0205 09:55:42.716246 140290213332736 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.020306166261434555, loss=0.03196559101343155
I0205 09:56:15.017728 140283530442496 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.022401010617613792, loss=0.035772792994976044
I0205 09:56:46.740726 140290213332736 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.025389589369297028, loss=0.03708745539188385
I0205 09:57:18.716019 140283530442496 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.02285742573440075, loss=0.03119676373898983
I0205 09:57:50.812782 140290213332736 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.021789804100990295, loss=0.03593295067548752
I0205 09:58:22.816908 140283530442496 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.021835550665855408, loss=0.03385427966713905
I0205 09:58:54.688980 140290213332736 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.027497481554746628, loss=0.033295538276433945
I0205 09:59:14.270877 140451058161472 spec.py:321] Evaluating on the training split.
I0205 10:01:11.231150 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 10:01:14.331525 140451058161472 spec.py:349] Evaluating on the test split.
I0205 10:01:17.359207 140451058161472 submission_runner.py:408] Time since start: 9573.67s, 	Step: 19462, 	{'train/accuracy': 0.9915679693222046, 'train/loss': 0.027529949322342873, 'train/mean_average_precision': 0.49315032014147686, 'validation/accuracy': 0.9867374897003174, 'validation/loss': 0.04580539092421532, 'validation/mean_average_precision': 0.26231222515437447, 'validation/num_examples': 43793, 'test/accuracy': 0.9858710169792175, 'test/loss': 0.04866298288106918, 'test/mean_average_precision': 0.2597261426049351, 'test/num_examples': 43793, 'score': 6255.323559045792, 'total_duration': 9573.666657924652, 'accumulated_submission_time': 6255.323559045792, 'accumulated_eval_time': 3317.048745393753, 'accumulated_logging_time': 0.7530508041381836}
I0205 10:01:17.378906 140266968643328 logging_writer.py:48] [19462] accumulated_eval_time=3317.048745, accumulated_logging_time=0.753051, accumulated_submission_time=6255.323559, global_step=19462, preemption_count=0, score=6255.323559, test/accuracy=0.985871, test/loss=0.048663, test/mean_average_precision=0.259726, test/num_examples=43793, total_duration=9573.666658, train/accuracy=0.991568, train/loss=0.027530, train/mean_average_precision=0.493150, validation/accuracy=0.986737, validation/loss=0.045805, validation/mean_average_precision=0.262312, validation/num_examples=43793
I0205 10:01:29.763338 140283731310336 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.024119893088936806, loss=0.03429177403450012
I0205 10:02:01.846924 140266968643328 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.027609489858150482, loss=0.036050885915756226
I0205 10:02:33.317287 140283731310336 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.028253301978111267, loss=0.03630254790186882
I0205 10:03:05.340498 140266968643328 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.024177193641662598, loss=0.03417966514825821
I0205 10:03:36.982682 140283731310336 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.022409358993172646, loss=0.03428642079234123
I0205 10:04:08.871683 140266968643328 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.0279062669724226, loss=0.03494006022810936
I0205 10:04:40.507375 140283731310336 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.030531350523233414, loss=0.0360422320663929
I0205 10:05:12.533329 140266968643328 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.026764970272779465, loss=0.0369950532913208
I0205 10:05:17.663712 140451058161472 spec.py:321] Evaluating on the training split.
I0205 10:07:09.623712 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 10:07:12.676753 140451058161472 spec.py:349] Evaluating on the test split.
I0205 10:07:15.817930 140451058161472 submission_runner.py:408] Time since start: 9932.13s, 	Step: 20217, 	{'train/accuracy': 0.9917171597480774, 'train/loss': 0.027245081961154938, 'train/mean_average_precision': 0.5011942091495836, 'validation/accuracy': 0.9866205453872681, 'validation/loss': 0.04634416103363037, 'validation/mean_average_precision': 0.2602996367965026, 'validation/num_examples': 43793, 'test/accuracy': 0.9857703447341919, 'test/loss': 0.04912406578660011, 'test/mean_average_precision': 0.25388159446065756, 'test/num_examples': 43793, 'score': 6495.576878070831, 'total_duration': 9932.125399589539, 'accumulated_submission_time': 6495.576878070831, 'accumulated_eval_time': 3435.202912569046, 'accumulated_logging_time': 0.7837088108062744}
I0205 10:07:15.839326 140283530442496 logging_writer.py:48] [20217] accumulated_eval_time=3435.202913, accumulated_logging_time=0.783709, accumulated_submission_time=6495.576878, global_step=20217, preemption_count=0, score=6495.576878, test/accuracy=0.985770, test/loss=0.049124, test/mean_average_precision=0.253882, test/num_examples=43793, total_duration=9932.125400, train/accuracy=0.991717, train/loss=0.027245, train/mean_average_precision=0.501194, validation/accuracy=0.986621, validation/loss=0.046344, validation/mean_average_precision=0.260300, validation/num_examples=43793
I0205 10:07:42.868899 140290213332736 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.026351211592555046, loss=0.032940253615379333
I0205 10:08:15.146223 140283530442496 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.027200153097510338, loss=0.033906254917383194
I0205 10:08:47.168579 140290213332736 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.03007030114531517, loss=0.03542131930589676
I0205 10:09:19.569663 140283530442496 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.02615690603852272, loss=0.036158714443445206
I0205 10:09:51.747279 140290213332736 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.032303132116794586, loss=0.035075150430202484
I0205 10:10:23.970623 140283530442496 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.029517192393541336, loss=0.03477747365832329
I0205 10:10:55.801522 140290213332736 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.029492666944861412, loss=0.03287050873041153
I0205 10:11:16.132054 140451058161472 spec.py:321] Evaluating on the training split.
I0205 10:13:08.108958 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 10:13:11.170801 140451058161472 spec.py:349] Evaluating on the test split.
I0205 10:13:14.199961 140451058161472 submission_runner.py:408] Time since start: 10290.51s, 	Step: 20964, 	{'train/accuracy': 0.9916337728500366, 'train/loss': 0.02696966752409935, 'train/mean_average_precision': 0.5147379353860131, 'validation/accuracy': 0.9867159724235535, 'validation/loss': 0.046382952481508255, 'validation/mean_average_precision': 0.2640476810927308, 'validation/num_examples': 43793, 'test/accuracy': 0.985842764377594, 'test/loss': 0.049242448061704636, 'test/mean_average_precision': 0.25052299615250756, 'test/num_examples': 43793, 'score': 6735.836624145508, 'total_duration': 10290.507292747498, 'accumulated_submission_time': 6735.836624145508, 'accumulated_eval_time': 3553.2706336975098, 'accumulated_logging_time': 0.817469596862793}
I0205 10:13:14.218980 140266968643328 logging_writer.py:48] [20964] accumulated_eval_time=3553.270634, accumulated_logging_time=0.817470, accumulated_submission_time=6735.836624, global_step=20964, preemption_count=0, score=6735.836624, test/accuracy=0.985843, test/loss=0.049242, test/mean_average_precision=0.250523, test/num_examples=43793, total_duration=10290.507293, train/accuracy=0.991634, train/loss=0.026970, train/mean_average_precision=0.514738, validation/accuracy=0.986716, validation/loss=0.046383, validation/mean_average_precision=0.264048, validation/num_examples=43793
I0205 10:13:26.085051 140283262007040 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.034115228801965714, loss=0.03993876278400421
I0205 10:13:57.873307 140266968643328 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.028889907523989677, loss=0.03542480245232582
I0205 10:14:29.759905 140283262007040 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.032442957162857056, loss=0.03280366584658623
I0205 10:15:01.450832 140266968643328 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.026334291324019432, loss=0.034052859991788864
I0205 10:15:34.804911 140283262007040 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.0318438783288002, loss=0.03396480530500412
I0205 10:16:07.785073 140266968643328 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.03173886612057686, loss=0.034029826521873474
I0205 10:16:39.886526 140283262007040 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.02538328245282173, loss=0.032369375228881836
I0205 10:17:11.728732 140266968643328 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.027360688894987106, loss=0.03258912265300751
I0205 10:17:14.256545 140451058161472 spec.py:321] Evaluating on the training split.
I0205 10:19:15.380879 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 10:19:18.376662 140451058161472 spec.py:349] Evaluating on the test split.
I0205 10:19:21.431713 140451058161472 submission_runner.py:408] Time since start: 10657.74s, 	Step: 21709, 	{'train/accuracy': 0.992107629776001, 'train/loss': 0.025918900966644287, 'train/mean_average_precision': 0.5224654679997216, 'validation/accuracy': 0.9866802096366882, 'validation/loss': 0.04617874696850777, 'validation/mean_average_precision': 0.2573069386431443, 'validation/num_examples': 43793, 'test/accuracy': 0.985846996307373, 'test/loss': 0.04880925267934799, 'test/mean_average_precision': 0.2529326603844326, 'test/num_examples': 43793, 'score': 6975.8414607048035, 'total_duration': 10657.739182472229, 'accumulated_submission_time': 6975.8414607048035, 'accumulated_eval_time': 3680.4457519054413, 'accumulated_logging_time': 0.8478615283966064}
I0205 10:19:21.451839 140283530442496 logging_writer.py:48] [21709] accumulated_eval_time=3680.445752, accumulated_logging_time=0.847862, accumulated_submission_time=6975.841461, global_step=21709, preemption_count=0, score=6975.841461, test/accuracy=0.985847, test/loss=0.048809, test/mean_average_precision=0.252933, test/num_examples=43793, total_duration=10657.739182, train/accuracy=0.992108, train/loss=0.025919, train/mean_average_precision=0.522465, validation/accuracy=0.986680, validation/loss=0.046179, validation/mean_average_precision=0.257307, validation/num_examples=43793
I0205 10:19:51.440866 140290213332736 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.027851127088069916, loss=0.03328685089945793
I0205 10:20:23.606446 140283530442496 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.029831793159246445, loss=0.03292274475097656
I0205 10:20:55.278152 140290213332736 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.028004122897982597, loss=0.03287345543503761
I0205 10:21:27.412708 140283530442496 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.032993316650390625, loss=0.03436191380023956
I0205 10:21:59.727601 140290213332736 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.03207831084728241, loss=0.0331193245947361
I0205 10:22:31.716463 140283530442496 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.030998844653367996, loss=0.0308634452521801
I0205 10:23:03.806765 140290213332736 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.029721803963184357, loss=0.03324202448129654
I0205 10:23:21.699176 140451058161472 spec.py:321] Evaluating on the training split.
I0205 10:25:10.949928 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 10:25:14.014839 140451058161472 spec.py:349] Evaluating on the test split.
I0205 10:25:17.071554 140451058161472 submission_runner.py:408] Time since start: 11013.38s, 	Step: 22456, 	{'train/accuracy': 0.9921752214431763, 'train/loss': 0.02509389817714691, 'train/mean_average_precision': 0.5612627482316623, 'validation/accuracy': 0.9866765737533569, 'validation/loss': 0.04679597169160843, 'validation/mean_average_precision': 0.25370349778591433, 'validation/num_examples': 43793, 'test/accuracy': 0.9858326315879822, 'test/loss': 0.04969538748264313, 'test/mean_average_precision': 0.2527972181515401, 'test/num_examples': 43793, 'score': 7216.057116985321, 'total_duration': 11013.379020690918, 'accumulated_submission_time': 7216.057116985321, 'accumulated_eval_time': 3795.8180980682373, 'accumulated_logging_time': 0.8788893222808838}
I0205 10:25:17.091900 140283262007040 logging_writer.py:48] [22456] accumulated_eval_time=3795.818098, accumulated_logging_time=0.878889, accumulated_submission_time=7216.057117, global_step=22456, preemption_count=0, score=7216.057117, test/accuracy=0.985833, test/loss=0.049695, test/mean_average_precision=0.252797, test/num_examples=43793, total_duration=11013.379021, train/accuracy=0.992175, train/loss=0.025094, train/mean_average_precision=0.561263, validation/accuracy=0.986677, validation/loss=0.046796, validation/mean_average_precision=0.253703, validation/num_examples=43793
I0205 10:25:31.665351 140283731310336 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.03154571354389191, loss=0.034913305193185806
I0205 10:26:04.036985 140283262007040 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.0364898145198822, loss=0.03625433146953583
I0205 10:26:35.850365 140283731310336 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.028068818151950836, loss=0.03264709562063217
I0205 10:27:07.919980 140283262007040 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.027882985770702362, loss=0.030983421951532364
I0205 10:27:39.773142 140283731310336 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.029630400240421295, loss=0.031203728169202805
I0205 10:28:11.777298 140283262007040 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.035935766994953156, loss=0.03571934998035431
I0205 10:28:44.231252 140283731310336 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.03450087457895279, loss=0.03276989236474037
I0205 10:29:16.546007 140283262007040 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.0352250412106514, loss=0.03484504297375679
I0205 10:29:17.224117 140451058161472 spec.py:321] Evaluating on the training split.
I0205 10:31:14.703179 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 10:31:17.815090 140451058161472 spec.py:349] Evaluating on the test split.
I0205 10:31:20.847141 140451058161472 submission_runner.py:408] Time since start: 11377.15s, 	Step: 23203, 	{'train/accuracy': 0.9926006197929382, 'train/loss': 0.024097805842757225, 'train/mean_average_precision': 0.5666674766675607, 'validation/accuracy': 0.9866806268692017, 'validation/loss': 0.04645010828971863, 'validation/mean_average_precision': 0.25767473144249486, 'validation/num_examples': 43793, 'test/accuracy': 0.9858015179634094, 'test/loss': 0.04949909821152687, 'test/mean_average_precision': 0.2530047210423624, 'test/num_examples': 43793, 'score': 7456.1574766635895, 'total_duration': 11377.154458284378, 'accumulated_submission_time': 7456.1574766635895, 'accumulated_eval_time': 3919.440933704376, 'accumulated_logging_time': 0.9106287956237793}
I0205 10:31:20.866631 140266968643328 logging_writer.py:48] [23203] accumulated_eval_time=3919.440934, accumulated_logging_time=0.910629, accumulated_submission_time=7456.157477, global_step=23203, preemption_count=0, score=7456.157477, test/accuracy=0.985802, test/loss=0.049499, test/mean_average_precision=0.253005, test/num_examples=43793, total_duration=11377.154458, train/accuracy=0.992601, train/loss=0.024098, train/mean_average_precision=0.566667, validation/accuracy=0.986681, validation/loss=0.046450, validation/mean_average_precision=0.257675, validation/num_examples=43793
I0205 10:31:52.585288 140290213332736 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.039763763546943665, loss=0.03604966029524803
I0205 10:32:24.797851 140266968643328 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.029384469613432884, loss=0.03178638592362404
I0205 10:32:56.613621 140290213332736 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.03247804567217827, loss=0.03118795156478882
I0205 10:33:28.882687 140266968643328 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.03220377489924431, loss=0.03192198649048805
I0205 10:34:00.639575 140290213332736 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.033581435680389404, loss=0.03236670792102814
I0205 10:34:32.544963 140266968643328 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.03364703804254532, loss=0.03197328746318817
I0205 10:35:04.600127 140290213332736 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.038288939744234085, loss=0.033271919935941696
I0205 10:35:21.091019 140451058161472 spec.py:321] Evaluating on the training split.
I0205 10:37:14.236339 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 10:37:17.830551 140451058161472 spec.py:349] Evaluating on the test split.
I0205 10:37:21.317782 140451058161472 submission_runner.py:408] Time since start: 11737.63s, 	Step: 23953, 	{'train/accuracy': 0.9929395318031311, 'train/loss': 0.022926853969693184, 'train/mean_average_precision': 0.6068382807187294, 'validation/accuracy': 0.9866469502449036, 'validation/loss': 0.046744298189878464, 'validation/mean_average_precision': 0.25062150247287623, 'validation/num_examples': 43793, 'test/accuracy': 0.9857783317565918, 'test/loss': 0.04991532862186432, 'test/mean_average_precision': 0.2435153176655401, 'test/num_examples': 43793, 'score': 7696.350813627243, 'total_duration': 11737.6252348423, 'accumulated_submission_time': 7696.350813627243, 'accumulated_eval_time': 4039.667632818222, 'accumulated_logging_time': 0.9410545825958252}
I0205 10:37:21.341001 140283262007040 logging_writer.py:48] [23953] accumulated_eval_time=4039.667633, accumulated_logging_time=0.941055, accumulated_submission_time=7696.350814, global_step=23953, preemption_count=0, score=7696.350814, test/accuracy=0.985778, test/loss=0.049915, test/mean_average_precision=0.243515, test/num_examples=43793, total_duration=11737.625235, train/accuracy=0.992940, train/loss=0.022927, train/mean_average_precision=0.606838, validation/accuracy=0.986647, validation/loss=0.046744, validation/mean_average_precision=0.250622, validation/num_examples=43793
I0205 10:37:37.350132 140283731310336 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.03956934064626694, loss=0.03623224422335625
I0205 10:38:10.691533 140283262007040 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.03148213028907776, loss=0.033421263098716736
I0205 10:38:42.605884 140283731310336 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.032426632940769196, loss=0.033068086951971054
I0205 10:39:14.731603 140283262007040 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.03948783501982689, loss=0.033418383449316025
I0205 10:39:46.683347 140283731310336 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.03771844506263733, loss=0.03225242346525192
I0205 10:40:18.754390 140283262007040 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.042189497500658035, loss=0.03480776771903038
I0205 10:40:50.649091 140283731310336 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.03516523540019989, loss=0.03303477168083191
I0205 10:41:21.497429 140451058161472 spec.py:321] Evaluating on the training split.
I0205 10:43:18.424156 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 10:43:21.496384 140451058161472 spec.py:349] Evaluating on the test split.
I0205 10:43:24.520052 140451058161472 submission_runner.py:408] Time since start: 12100.83s, 	Step: 24697, 	{'train/accuracy': 0.9930366277694702, 'train/loss': 0.022799205034971237, 'train/mean_average_precision': 0.6002876704147919, 'validation/accuracy': 0.9866778254508972, 'validation/loss': 0.04691381752490997, 'validation/mean_average_precision': 0.25802253076312165, 'validation/num_examples': 43793, 'test/accuracy': 0.9857467412948608, 'test/loss': 0.050121892243623734, 'test/mean_average_precision': 0.2449126202635368, 'test/num_examples': 43793, 'score': 7936.4745626449585, 'total_duration': 12100.827523469925, 'accumulated_submission_time': 7936.4745626449585, 'accumulated_eval_time': 4162.690213441849, 'accumulated_logging_time': 0.9763381481170654}
I0205 10:43:24.540123 140283530442496 logging_writer.py:48] [24697] accumulated_eval_time=4162.690213, accumulated_logging_time=0.976338, accumulated_submission_time=7936.474563, global_step=24697, preemption_count=0, score=7936.474563, test/accuracy=0.985747, test/loss=0.050122, test/mean_average_precision=0.244913, test/num_examples=43793, total_duration=12100.827523, train/accuracy=0.993037, train/loss=0.022799, train/mean_average_precision=0.600288, validation/accuracy=0.986678, validation/loss=0.046914, validation/mean_average_precision=0.258023, validation/num_examples=43793
I0205 10:43:25.835922 140290213332736 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.03426678478717804, loss=0.03317762166261673
I0205 10:43:57.724946 140283530442496 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.033358633518218994, loss=0.03242633119225502
I0205 10:44:29.871097 140290213332736 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.05449265241622925, loss=0.03220028057694435
I0205 10:45:02.338637 140283530442496 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.037921350449323654, loss=0.03061523661017418
I0205 10:45:34.440247 140290213332736 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.038014840334653854, loss=0.0327933207154274
I0205 10:46:06.461555 140283530442496 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.03566382825374603, loss=0.03270077332854271
I0205 10:46:38.863478 140290213332736 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.03489338606595993, loss=0.034092191606760025
I0205 10:47:10.995283 140283530442496 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.03832962363958359, loss=0.03475141152739525
I0205 10:47:24.594010 140451058161472 spec.py:321] Evaluating on the training split.
I0205 10:49:18.123785 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 10:49:21.230940 140451058161472 spec.py:349] Evaluating on the test split.
I0205 10:49:24.191052 140451058161472 submission_runner.py:408] Time since start: 12460.50s, 	Step: 25443, 	{'train/accuracy': 0.9928334951400757, 'train/loss': 0.02330951765179634, 'train/mean_average_precision': 0.5824658225387833, 'validation/accuracy': 0.9866371750831604, 'validation/loss': 0.047194041311740875, 'validation/mean_average_precision': 0.25858392894004656, 'validation/num_examples': 43793, 'test/accuracy': 0.9858074188232422, 'test/loss': 0.05031436309218407, 'test/mean_average_precision': 0.25117618069757147, 'test/num_examples': 43793, 'score': 8176.495717287064, 'total_duration': 12460.498522043228, 'accumulated_submission_time': 8176.495717287064, 'accumulated_eval_time': 4282.287209033966, 'accumulated_logging_time': 1.0089423656463623}
I0205 10:49:24.211284 140266968643328 logging_writer.py:48] [25443] accumulated_eval_time=4282.287209, accumulated_logging_time=1.008942, accumulated_submission_time=8176.495717, global_step=25443, preemption_count=0, score=8176.495717, test/accuracy=0.985807, test/loss=0.050314, test/mean_average_precision=0.251176, test/num_examples=43793, total_duration=12460.498522, train/accuracy=0.992833, train/loss=0.023310, train/mean_average_precision=0.582466, validation/accuracy=0.986637, validation/loss=0.047194, validation/mean_average_precision=0.258584, validation/num_examples=43793
I0205 10:49:42.817351 140283262007040 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.03206639364361763, loss=0.02950352057814598
I0205 10:50:14.834481 140266968643328 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.038706015795469284, loss=0.03215793892741203
I0205 10:50:46.573575 140283262007040 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.034388672560453415, loss=0.03144122660160065
I0205 10:51:18.701036 140266968643328 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.03528787940740585, loss=0.032172974199056625
I0205 10:51:51.360040 140283262007040 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.0424942784011364, loss=0.031295400112867355
I0205 10:52:24.500689 140266968643328 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.04661093279719353, loss=0.03233868256211281
I0205 10:52:57.700570 140283262007040 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.03892948850989342, loss=0.03216957673430443
I0205 10:53:24.203797 140451058161472 spec.py:321] Evaluating on the training split.
I0205 10:55:18.070330 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 10:55:21.168682 140451058161472 spec.py:349] Evaluating on the test split.
I0205 10:55:24.228122 140451058161472 submission_runner.py:408] Time since start: 12820.54s, 	Step: 26181, 	{'train/accuracy': 0.9925329089164734, 'train/loss': 0.024043401703238487, 'train/mean_average_precision': 0.5762234810496102, 'validation/accuracy': 0.986629068851471, 'validation/loss': 0.04753634333610535, 'validation/mean_average_precision': 0.25586160400905505, 'validation/num_examples': 43793, 'test/accuracy': 0.9857892990112305, 'test/loss': 0.05049561336636543, 'test/mean_average_precision': 0.2471158389841096, 'test/num_examples': 43793, 'score': 8416.455756187439, 'total_duration': 12820.535581588745, 'accumulated_submission_time': 8416.455756187439, 'accumulated_eval_time': 4402.311491250992, 'accumulated_logging_time': 1.0403468608856201}
I0205 10:55:24.248552 140283530442496 logging_writer.py:48] [26181] accumulated_eval_time=4402.311491, accumulated_logging_time=1.040347, accumulated_submission_time=8416.455756, global_step=26181, preemption_count=0, score=8416.455756, test/accuracy=0.985789, test/loss=0.050496, test/mean_average_precision=0.247116, test/num_examples=43793, total_duration=12820.535582, train/accuracy=0.992533, train/loss=0.024043, train/mean_average_precision=0.576223, validation/accuracy=0.986629, validation/loss=0.047536, validation/mean_average_precision=0.255862, validation/num_examples=43793
I0205 10:55:30.679576 140283731310336 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.04112539067864418, loss=0.03282885625958443
I0205 10:56:02.810192 140283530442496 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.05154078081250191, loss=0.03456604480743408
I0205 10:56:34.505498 140283731310336 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.03663108870387077, loss=0.03049986995756626
I0205 10:57:06.743846 140283530442496 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.04400629550218582, loss=0.03276352211833
I0205 10:57:39.106921 140283731310336 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.04555429890751839, loss=0.03478820621967316
I0205 10:58:11.221807 140283530442496 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.038844965398311615, loss=0.03189222887158394
I0205 10:58:43.401448 140283731310336 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.03383069112896919, loss=0.029602501541376114
I0205 10:59:15.299365 140283530442496 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.039339058101177216, loss=0.03345758095383644
I0205 10:59:24.473738 140451058161472 spec.py:321] Evaluating on the training split.
I0205 11:01:16.649670 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 11:01:19.695575 140451058161472 spec.py:349] Evaluating on the test split.
I0205 11:01:22.684747 140451058161472 submission_runner.py:408] Time since start: 13178.99s, 	Step: 26930, 	{'train/accuracy': 0.9923691749572754, 'train/loss': 0.024398958310484886, 'train/mean_average_precision': 0.5442668976545151, 'validation/accuracy': 0.986559271812439, 'validation/loss': 0.04842165485024452, 'validation/mean_average_precision': 0.2523562677872928, 'validation/num_examples': 43793, 'test/accuracy': 0.9857602119445801, 'test/loss': 0.051313646137714386, 'test/mean_average_precision': 0.2501128544462677, 'test/num_examples': 43793, 'score': 8656.649072170258, 'total_duration': 13178.992216587067, 'accumulated_submission_time': 8656.649072170258, 'accumulated_eval_time': 4520.522451400757, 'accumulated_logging_time': 1.071984052658081}
I0205 11:01:22.705130 140266968643328 logging_writer.py:48] [26930] accumulated_eval_time=4520.522451, accumulated_logging_time=1.071984, accumulated_submission_time=8656.649072, global_step=26930, preemption_count=0, score=8656.649072, test/accuracy=0.985760, test/loss=0.051314, test/mean_average_precision=0.250113, test/num_examples=43793, total_duration=13178.992217, train/accuracy=0.992369, train/loss=0.024399, train/mean_average_precision=0.544267, validation/accuracy=0.986559, validation/loss=0.048422, validation/mean_average_precision=0.252356, validation/num_examples=43793
I0205 11:01:45.484777 140283262007040 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.04470866918563843, loss=0.031790971755981445
I0205 11:02:17.444363 140266968643328 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.039671771228313446, loss=0.0303754061460495
I0205 11:02:49.488584 140283262007040 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.041348040103912354, loss=0.03447527810931206
I0205 11:03:21.471739 140266968643328 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.04520522803068161, loss=0.03218523785471916
I0205 11:03:53.329620 140283262007040 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.03636832907795906, loss=0.030284663662314415
I0205 11:04:25.263282 140266968643328 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.03368650749325752, loss=0.02984493039548397
I0205 11:04:56.936898 140283262007040 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.039265938103199005, loss=0.030117740854620934
I0205 11:05:22.753357 140451058161472 spec.py:321] Evaluating on the training split.
I0205 11:07:19.187643 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 11:07:22.263222 140451058161472 spec.py:349] Evaluating on the test split.
I0205 11:07:25.307899 140451058161472 submission_runner.py:408] Time since start: 13541.62s, 	Step: 27681, 	{'train/accuracy': 0.992576003074646, 'train/loss': 0.023915134370326996, 'train/mean_average_precision': 0.5780123560081694, 'validation/accuracy': 0.9865665435791016, 'validation/loss': 0.04806533455848694, 'validation/mean_average_precision': 0.25048311568385967, 'validation/num_examples': 43793, 'test/accuracy': 0.9857446551322937, 'test/loss': 0.05106044188141823, 'test/mean_average_precision': 0.2448402843569965, 'test/num_examples': 43793, 'score': 8896.665783882141, 'total_duration': 13541.615369558334, 'accumulated_submission_time': 8896.665783882141, 'accumulated_eval_time': 4643.076948165894, 'accumulated_logging_time': 1.1032321453094482}
I0205 11:07:25.328344 140283731310336 logging_writer.py:48] [27681] accumulated_eval_time=4643.076948, accumulated_logging_time=1.103232, accumulated_submission_time=8896.665784, global_step=27681, preemption_count=0, score=8896.665784, test/accuracy=0.985745, test/loss=0.051060, test/mean_average_precision=0.244840, test/num_examples=43793, total_duration=13541.615370, train/accuracy=0.992576, train/loss=0.023915, train/mean_average_precision=0.578012, validation/accuracy=0.986567, validation/loss=0.048065, validation/mean_average_precision=0.250483, validation/num_examples=43793
I0205 11:07:31.826020 140290213332736 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.03542177751660347, loss=0.031077291816473007
I0205 11:08:04.464156 140283731310336 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.03986421972513199, loss=0.029795005917549133
I0205 11:08:36.122854 140290213332736 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.05042523890733719, loss=0.031603217124938965
I0205 11:09:08.126385 140283731310336 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.04370822757482529, loss=0.03262326866388321
I0205 11:09:40.276088 140290213332736 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.04637787118554115, loss=0.032342635095119476
I0205 11:10:12.271950 140283731310336 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.04054776579141617, loss=0.030161933973431587
I0205 11:10:44.147655 140290213332736 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.038509074598550797, loss=0.029638424515724182
I0205 11:11:16.785170 140283731310336 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.04498141631484032, loss=0.031144175678491592
I0205 11:11:25.436495 140451058161472 spec.py:321] Evaluating on the training split.
I0205 11:13:20.484679 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 11:13:23.526735 140451058161472 spec.py:349] Evaluating on the test split.
I0205 11:13:26.514936 140451058161472 submission_runner.py:408] Time since start: 13902.82s, 	Step: 28428, 	{'train/accuracy': 0.9928958415985107, 'train/loss': 0.022750936448574066, 'train/mean_average_precision': 0.5812031928977817, 'validation/accuracy': 0.9866591095924377, 'validation/loss': 0.048344485461711884, 'validation/mean_average_precision': 0.258367335005388, 'validation/num_examples': 43793, 'test/accuracy': 0.9858065247535706, 'test/loss': 0.051526401191949844, 'test/mean_average_precision': 0.24750431426551517, 'test/num_examples': 43793, 'score': 9136.741748094559, 'total_duration': 13902.822404623032, 'accumulated_submission_time': 9136.741748094559, 'accumulated_eval_time': 4764.155343532562, 'accumulated_logging_time': 1.1362247467041016}
I0205 11:13:26.535726 140266968643328 logging_writer.py:48] [28428] accumulated_eval_time=4764.155344, accumulated_logging_time=1.136225, accumulated_submission_time=9136.741748, global_step=28428, preemption_count=0, score=9136.741748, test/accuracy=0.985807, test/loss=0.051526, test/mean_average_precision=0.247504, test/num_examples=43793, total_duration=13902.822405, train/accuracy=0.992896, train/loss=0.022751, train/mean_average_precision=0.581203, validation/accuracy=0.986659, validation/loss=0.048344, validation/mean_average_precision=0.258367, validation/num_examples=43793
I0205 11:13:50.036999 140283262007040 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.03540251776576042, loss=0.030273418873548508
I0205 11:14:22.441122 140266968643328 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.039832741022109985, loss=0.03089994005858898
I0205 11:14:54.865195 140283262007040 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.04760662466287613, loss=0.03101952373981476
I0205 11:15:27.065577 140266968643328 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.04226357117295265, loss=0.031146854162216187
I0205 11:15:59.310143 140283262007040 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.04639096558094025, loss=0.029319003224372864
I0205 11:16:31.280890 140266968643328 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.046153124421834946, loss=0.03126343712210655
I0205 11:17:03.542693 140283262007040 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.04343586787581444, loss=0.03049410693347454
I0205 11:17:26.756376 140451058161472 spec.py:321] Evaluating on the training split.
I0205 11:19:20.299856 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 11:19:23.379345 140451058161472 spec.py:349] Evaluating on the test split.
I0205 11:19:26.387604 140451058161472 submission_runner.py:408] Time since start: 14262.70s, 	Step: 29172, 	{'train/accuracy': 0.9930545687675476, 'train/loss': 0.022334126755595207, 'train/mean_average_precision': 0.615808792548507, 'validation/accuracy': 0.9865154027938843, 'validation/loss': 0.04860911890864372, 'validation/mean_average_precision': 0.24820526200687404, 'validation/num_examples': 43793, 'test/accuracy': 0.9856435656547546, 'test/loss': 0.05174456164240837, 'test/mean_average_precision': 0.2452964177865293, 'test/num_examples': 43793, 'score': 9376.930389642715, 'total_duration': 14262.695072174072, 'accumulated_submission_time': 9376.930389642715, 'accumulated_eval_time': 4883.786524772644, 'accumulated_logging_time': 1.1681454181671143}
I0205 11:19:26.410240 140283530442496 logging_writer.py:48] [29172] accumulated_eval_time=4883.786525, accumulated_logging_time=1.168145, accumulated_submission_time=9376.930390, global_step=29172, preemption_count=0, score=9376.930390, test/accuracy=0.985644, test/loss=0.051745, test/mean_average_precision=0.245296, test/num_examples=43793, total_duration=14262.695072, train/accuracy=0.993055, train/loss=0.022334, train/mean_average_precision=0.615809, validation/accuracy=0.986515, validation/loss=0.048609, validation/mean_average_precision=0.248205, validation/num_examples=43793
I0205 11:19:35.888327 140283731310336 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.04055124893784523, loss=0.030690673738718033
I0205 11:20:08.110027 140283530442496 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.05055732652544975, loss=0.030266758054494858
I0205 11:20:40.148195 140283731310336 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.04518546536564827, loss=0.03025057725608349
I0205 11:21:12.152164 140283530442496 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.04510052874684334, loss=0.0304286926984787
I0205 11:21:43.962728 140283731310336 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.048704829066991806, loss=0.03051486425101757
I0205 11:22:15.818760 140283530442496 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.04882928356528282, loss=0.030199173837900162
I0205 11:22:48.151847 140283731310336 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.05051511153578758, loss=0.03247655928134918
I0205 11:23:19.921588 140283530442496 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.043838731944561005, loss=0.031270645558834076
I0205 11:23:26.659501 140451058161472 spec.py:321] Evaluating on the training split.
I0205 11:25:17.740151 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 11:25:20.808601 140451058161472 spec.py:349] Evaluating on the test split.
I0205 11:25:23.879693 140451058161472 submission_runner.py:408] Time since start: 14620.19s, 	Step: 29922, 	{'train/accuracy': 0.9933610558509827, 'train/loss': 0.021280016750097275, 'train/mean_average_precision': 0.6281966779933524, 'validation/accuracy': 0.9865202903747559, 'validation/loss': 0.04879826307296753, 'validation/mean_average_precision': 0.2524698596022855, 'validation/num_examples': 43793, 'test/accuracy': 0.9856886267662048, 'test/loss': 0.05191462114453316, 'test/mean_average_precision': 0.24138551391030635, 'test/num_examples': 43793, 'score': 9617.14767241478, 'total_duration': 14620.18716263771, 'accumulated_submission_time': 9617.14767241478, 'accumulated_eval_time': 5001.006668329239, 'accumulated_logging_time': 1.2020776271820068}
I0205 11:25:23.900624 140266968643328 logging_writer.py:48] [29922] accumulated_eval_time=5001.006668, accumulated_logging_time=1.202078, accumulated_submission_time=9617.147672, global_step=29922, preemption_count=0, score=9617.147672, test/accuracy=0.985689, test/loss=0.051915, test/mean_average_precision=0.241386, test/num_examples=43793, total_duration=14620.187163, train/accuracy=0.993361, train/loss=0.021280, train/mean_average_precision=0.628197, validation/accuracy=0.986520, validation/loss=0.048798, validation/mean_average_precision=0.252470, validation/num_examples=43793
I0205 11:25:49.350469 140290213332736 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.044609084725379944, loss=0.03157292678952217
I0205 11:26:21.838694 140266968643328 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.051271941512823105, loss=0.030413968488574028
I0205 11:26:54.074520 140290213332736 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.056610822677612305, loss=0.03121517226099968
I0205 11:27:26.926339 140266968643328 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.04234413430094719, loss=0.02997305430471897
I0205 11:27:59.374084 140290213332736 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.046904075890779495, loss=0.029690541326999664
I0205 11:28:31.516097 140266968643328 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.05212632194161415, loss=0.03194177895784378
I0205 11:29:03.614115 140290213332736 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.04902084171772003, loss=0.03142374008893967
I0205 11:29:24.126991 140451058161472 spec.py:321] Evaluating on the training split.
I0205 11:31:21.631970 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 11:31:24.756235 140451058161472 spec.py:349] Evaluating on the test split.
I0205 11:31:27.763323 140451058161472 submission_runner.py:408] Time since start: 14984.07s, 	Step: 30664, 	{'train/accuracy': 0.9941012263298035, 'train/loss': 0.01936311647295952, 'train/mean_average_precision': 0.6699394839401119, 'validation/accuracy': 0.9864890575408936, 'validation/loss': 0.04925333335995674, 'validation/mean_average_precision': 0.2432590919025162, 'validation/num_examples': 43793, 'test/accuracy': 0.9856064915657043, 'test/loss': 0.05230368301272392, 'test/mean_average_precision': 0.2375251315964577, 'test/num_examples': 43793, 'score': 9857.341331720352, 'total_duration': 14984.070784330368, 'accumulated_submission_time': 9857.341331720352, 'accumulated_eval_time': 5124.642949104309, 'accumulated_logging_time': 1.2355139255523682}
I0205 11:31:27.784239 140283262007040 logging_writer.py:48] [30664] accumulated_eval_time=5124.642949, accumulated_logging_time=1.235514, accumulated_submission_time=9857.341332, global_step=30664, preemption_count=0, score=9857.341332, test/accuracy=0.985606, test/loss=0.052304, test/mean_average_precision=0.237525, test/num_examples=43793, total_duration=14984.070784, train/accuracy=0.994101, train/loss=0.019363, train/mean_average_precision=0.669939, validation/accuracy=0.986489, validation/loss=0.049253, validation/mean_average_precision=0.243259, validation/num_examples=43793
I0205 11:31:39.739499 140283530442496 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.050462257117033005, loss=0.029234448447823524
I0205 11:32:11.990397 140283262007040 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.04833504557609558, loss=0.030360987409949303
I0205 11:32:44.001854 140283530442496 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.04788351431488991, loss=0.030466372147202492
I0205 11:33:16.033923 140283262007040 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.04943041875958443, loss=0.031133875250816345
I0205 11:33:48.204163 140283530442496 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.051660165190696716, loss=0.029374979436397552
I0205 11:34:20.334936 140283262007040 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.050629548728466034, loss=0.03136945888400078
I0205 11:34:52.362363 140283530442496 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.04696083441376686, loss=0.02913970872759819
I0205 11:35:24.429994 140283262007040 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.05056040734052658, loss=0.030171046033501625
I0205 11:35:27.892011 140451058161472 spec.py:321] Evaluating on the training split.
I0205 11:37:18.787591 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 11:37:21.851792 140451058161472 spec.py:349] Evaluating on the test split.
I0205 11:37:24.902802 140451058161472 submission_runner.py:408] Time since start: 15341.21s, 	Step: 31412, 	{'train/accuracy': 0.9941602349281311, 'train/loss': 0.019281893968582153, 'train/mean_average_precision': 0.6668670900993225, 'validation/accuracy': 0.9864894151687622, 'validation/loss': 0.04892243072390556, 'validation/mean_average_precision': 0.2506274620268583, 'validation/num_examples': 43793, 'test/accuracy': 0.9856364130973816, 'test/loss': 0.05200303718447685, 'test/mean_average_precision': 0.24071333250409857, 'test/num_examples': 43793, 'score': 10097.41719198227, 'total_duration': 15341.210273504257, 'accumulated_submission_time': 10097.41719198227, 'accumulated_eval_time': 5241.653692960739, 'accumulated_logging_time': 1.2681159973144531}
I0205 11:37:24.924210 140266968643328 logging_writer.py:48] [31412] accumulated_eval_time=5241.653693, accumulated_logging_time=1.268116, accumulated_submission_time=10097.417192, global_step=31412, preemption_count=0, score=10097.417192, test/accuracy=0.985636, test/loss=0.052003, test/mean_average_precision=0.240713, test/num_examples=43793, total_duration=15341.210274, train/accuracy=0.994160, train/loss=0.019282, train/mean_average_precision=0.666867, validation/accuracy=0.986489, validation/loss=0.048922, validation/mean_average_precision=0.250627, validation/num_examples=43793
I0205 11:37:53.919622 140290213332736 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.05334119126200676, loss=0.028173178434371948
I0205 11:38:26.260555 140266968643328 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.049223508685827255, loss=0.031107045710086823
I0205 11:38:58.605083 140290213332736 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.04445694386959076, loss=0.0271538607776165
I0205 11:39:32.142575 140266968643328 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.05421890318393707, loss=0.032101456075906754
I0205 11:40:05.607265 140290213332736 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.048373859375715256, loss=0.03153560310602188
I0205 11:40:38.116979 140266968643328 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.046112269163131714, loss=0.028371084481477737
I0205 11:41:10.599177 140290213332736 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.05269579961895943, loss=0.02991711162030697
I0205 11:41:25.181064 140451058161472 spec.py:321] Evaluating on the training split.
I0205 11:43:21.035513 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 11:43:24.030713 140451058161472 spec.py:349] Evaluating on the test split.
I0205 11:43:26.997308 140451058161472 submission_runner.py:408] Time since start: 15703.30s, 	Step: 32146, 	{'train/accuracy': 0.9937401413917542, 'train/loss': 0.02024981938302517, 'train/mean_average_precision': 0.6502916140923689, 'validation/accuracy': 0.9865487217903137, 'validation/loss': 0.049678266048431396, 'validation/mean_average_precision': 0.2499031904886978, 'validation/num_examples': 43793, 'test/accuracy': 0.9856595396995544, 'test/loss': 0.05282976105809212, 'test/mean_average_precision': 0.23891806109517563, 'test/num_examples': 43793, 'score': 10337.642447710037, 'total_duration': 15703.304780244827, 'accumulated_submission_time': 10337.642447710037, 'accumulated_eval_time': 5363.4699013233185, 'accumulated_logging_time': 1.3005335330963135}
I0205 11:43:27.018720 140283262007040 logging_writer.py:48] [32146] accumulated_eval_time=5363.469901, accumulated_logging_time=1.300534, accumulated_submission_time=10337.642448, global_step=32146, preemption_count=0, score=10337.642448, test/accuracy=0.985660, test/loss=0.052830, test/mean_average_precision=0.238918, test/num_examples=43793, total_duration=15703.304780, train/accuracy=0.993740, train/loss=0.020250, train/mean_average_precision=0.650292, validation/accuracy=0.986549, validation/loss=0.049678, validation/mean_average_precision=0.249903, validation/num_examples=43793
I0205 11:43:44.354941 140283530442496 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.05053887143731117, loss=0.030438942834734917
I0205 11:44:16.520568 140283262007040 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.05674057826399803, loss=0.0329376757144928
I0205 11:44:48.180243 140283530442496 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.06898654252290726, loss=0.03290051221847534
I0205 11:45:19.826678 140283262007040 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.054084762930870056, loss=0.029198672622442245
I0205 11:45:51.534785 140283530442496 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.05156364664435387, loss=0.031091338023543358
I0205 11:46:23.468988 140283262007040 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.05462189391255379, loss=0.030644528567790985
I0205 11:46:55.354004 140283530442496 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.04795180261135101, loss=0.029926080256700516
I0205 11:47:26.934217 140283262007040 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.05691489949822426, loss=0.03054381161928177
I0205 11:47:27.259875 140451058161472 spec.py:321] Evaluating on the training split.
I0205 11:49:25.748709 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 11:49:28.809782 140451058161472 spec.py:349] Evaluating on the test split.
I0205 11:49:31.816250 140451058161472 submission_runner.py:408] Time since start: 16068.12s, 	Step: 32902, 	{'train/accuracy': 0.9936198592185974, 'train/loss': 0.020604152232408524, 'train/mean_average_precision': 0.6400930897954951, 'validation/accuracy': 0.986531674861908, 'validation/loss': 0.04952919855713844, 'validation/mean_average_precision': 0.24534358330526443, 'validation/num_examples': 43793, 'test/accuracy': 0.9856982827186584, 'test/loss': 0.05269746482372284, 'test/mean_average_precision': 0.24266124610204737, 'test/num_examples': 43793, 'score': 10577.850949764252, 'total_duration': 16068.123711824417, 'accumulated_submission_time': 10577.850949764252, 'accumulated_eval_time': 5488.026214838028, 'accumulated_logging_time': 1.334458589553833}
I0205 11:49:31.839480 140283731310336 logging_writer.py:48] [32902] accumulated_eval_time=5488.026215, accumulated_logging_time=1.334459, accumulated_submission_time=10577.850950, global_step=32902, preemption_count=0, score=10577.850950, test/accuracy=0.985698, test/loss=0.052697, test/mean_average_precision=0.242661, test/num_examples=43793, total_duration=16068.123712, train/accuracy=0.993620, train/loss=0.020604, train/mean_average_precision=0.640093, validation/accuracy=0.986532, validation/loss=0.049529, validation/mean_average_precision=0.245344, validation/num_examples=43793
I0205 11:50:03.528629 140290213332736 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.05901425704360008, loss=0.02968410588800907
I0205 11:50:35.153236 140283731310336 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.048299480229616165, loss=0.029138654470443726
I0205 11:51:07.238982 140290213332736 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.07015564292669296, loss=0.031026380136609077
I0205 11:51:39.441789 140283731310336 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.06486305594444275, loss=0.03125622496008873
I0205 11:52:11.792874 140290213332736 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.06066112965345383, loss=0.02957286313176155
I0205 11:52:43.503913 140283731310336 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.05534673482179642, loss=0.0272531658411026
I0205 11:53:15.378467 140290213332736 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.058224260807037354, loss=0.029651517048478127
I0205 11:53:31.969723 140451058161472 spec.py:321] Evaluating on the training split.
I0205 11:55:23.096746 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 11:55:26.184180 140451058161472 spec.py:349] Evaluating on the test split.
I0205 11:55:29.171982 140451058161472 submission_runner.py:408] Time since start: 16425.48s, 	Step: 33652, 	{'train/accuracy': 0.9934507012367249, 'train/loss': 0.020942265167832375, 'train/mean_average_precision': 0.6310022435507462, 'validation/accuracy': 0.9864732027053833, 'validation/loss': 0.049740687012672424, 'validation/mean_average_precision': 0.24596346589723075, 'validation/num_examples': 43793, 'test/accuracy': 0.9855508804321289, 'test/loss': 0.052817679941654205, 'test/mean_average_precision': 0.2418377184419295, 'test/num_examples': 43793, 'score': 10817.950261116028, 'total_duration': 16425.47945213318, 'accumulated_submission_time': 10817.950261116028, 'accumulated_eval_time': 5605.228426933289, 'accumulated_logging_time': 1.3684918880462646}
I0205 11:55:29.193629 140283262007040 logging_writer.py:48] [33652] accumulated_eval_time=5605.228427, accumulated_logging_time=1.368492, accumulated_submission_time=10817.950261, global_step=33652, preemption_count=0, score=10817.950261, test/accuracy=0.985551, test/loss=0.052818, test/mean_average_precision=0.241838, test/num_examples=43793, total_duration=16425.479452, train/accuracy=0.993451, train/loss=0.020942, train/mean_average_precision=0.631002, validation/accuracy=0.986473, validation/loss=0.049741, validation/mean_average_precision=0.245963, validation/num_examples=43793
I0205 11:55:45.060562 140283530442496 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.08160218596458435, loss=0.030467918142676353
I0205 11:56:17.556870 140283262007040 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.05724325776100159, loss=0.030655499547719955
I0205 11:56:50.542878 140283530442496 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.053377412259578705, loss=0.028444968163967133
I0205 11:57:23.582871 140283262007040 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.04909650608897209, loss=0.03113741986453533
I0205 11:57:56.841123 140283530442496 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.06043672934174538, loss=0.02927313558757305
I0205 11:58:29.253489 140283262007040 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.05480993911623955, loss=0.02955922670662403
I0205 11:59:01.274108 140283530442496 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.061658211052417755, loss=0.02821696735918522
I0205 11:59:29.405347 140451058161472 spec.py:321] Evaluating on the training split.
I0205 12:01:24.539438 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 12:01:27.597877 140451058161472 spec.py:349] Evaluating on the test split.
I0205 12:01:30.549230 140451058161472 submission_runner.py:408] Time since start: 16786.86s, 	Step: 34387, 	{'train/accuracy': 0.9934825897216797, 'train/loss': 0.02074396423995495, 'train/mean_average_precision': 0.6385106217173827, 'validation/accuracy': 0.9863489866256714, 'validation/loss': 0.050209540873765945, 'validation/mean_average_precision': 0.24648465404572378, 'validation/num_examples': 43793, 'test/accuracy': 0.9855243563652039, 'test/loss': 0.05341925099492073, 'test/mean_average_precision': 0.23595149152631772, 'test/num_examples': 43793, 'score': 11058.128161430359, 'total_duration': 16786.856697797775, 'accumulated_submission_time': 11058.128161430359, 'accumulated_eval_time': 5726.372263431549, 'accumulated_logging_time': 1.402308702468872}
I0205 12:01:30.571307 140283731310336 logging_writer.py:48] [34387] accumulated_eval_time=5726.372263, accumulated_logging_time=1.402309, accumulated_submission_time=11058.128161, global_step=34387, preemption_count=0, score=11058.128161, test/accuracy=0.985524, test/loss=0.053419, test/mean_average_precision=0.235951, test/num_examples=43793, total_duration=16786.856698, train/accuracy=0.993483, train/loss=0.020744, train/mean_average_precision=0.638511, validation/accuracy=0.986349, validation/loss=0.050210, validation/mean_average_precision=0.246485, validation/num_examples=43793
I0205 12:01:35.239463 140290213332736 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.051622405648231506, loss=0.026754844933748245
I0205 12:02:07.276845 140283731310336 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.06558248400688171, loss=0.030161626636981964
I0205 12:02:39.191631 140290213332736 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.06230485811829567, loss=0.029454268515110016
I0205 12:03:11.028615 140283731310336 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.06882857531309128, loss=0.029394928365945816
I0205 12:03:42.836290 140290213332736 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.05693363770842552, loss=0.028637399896979332
I0205 12:04:15.018329 140283731310336 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.05987658351659775, loss=0.028888672590255737
I0205 12:04:47.301470 140290213332736 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.05544547364115715, loss=0.02896537259221077
I0205 12:05:19.173581 140283731310336 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.06874684244394302, loss=0.029898282140493393
I0205 12:05:30.839898 140451058161472 spec.py:321] Evaluating on the training split.
I0205 12:07:18.885171 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 12:07:21.874778 140451058161472 spec.py:349] Evaluating on the test split.
I0205 12:07:24.895669 140451058161472 submission_runner.py:408] Time since start: 17141.20s, 	Step: 35138, 	{'train/accuracy': 0.9936950206756592, 'train/loss': 0.020330367609858513, 'train/mean_average_precision': 0.6362148804038725, 'validation/accuracy': 0.9863278865814209, 'validation/loss': 0.050283726304769516, 'validation/mean_average_precision': 0.24305085441601995, 'validation/num_examples': 43793, 'test/accuracy': 0.9854733943939209, 'test/loss': 0.05352490022778511, 'test/mean_average_precision': 0.2315473117127911, 'test/num_examples': 43793, 'score': 11298.363315105438, 'total_duration': 17141.203130722046, 'accumulated_submission_time': 11298.363315105438, 'accumulated_eval_time': 5840.427979707718, 'accumulated_logging_time': 1.437326431274414}
I0205 12:07:24.918615 140266968643328 logging_writer.py:48] [35138] accumulated_eval_time=5840.427980, accumulated_logging_time=1.437326, accumulated_submission_time=11298.363315, global_step=35138, preemption_count=0, score=11298.363315, test/accuracy=0.985473, test/loss=0.053525, test/mean_average_precision=0.231547, test/num_examples=43793, total_duration=17141.203131, train/accuracy=0.993695, train/loss=0.020330, train/mean_average_precision=0.636215, validation/accuracy=0.986328, validation/loss=0.050284, validation/mean_average_precision=0.243051, validation/num_examples=43793
I0205 12:07:45.066577 140283530442496 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.05383395403623581, loss=0.026801984757184982
I0205 12:08:17.216504 140266968643328 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.05928749218583107, loss=0.030433926731348038
I0205 12:08:49.199423 140283530442496 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.05716661736369133, loss=0.02824161760509014
I0205 12:09:21.313184 140266968643328 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.07038773596286774, loss=0.03099447675049305
I0205 12:09:53.330646 140283530442496 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.06940658390522003, loss=0.027031805366277695
I0205 12:10:25.207208 140266968643328 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.061892129480838776, loss=0.029921645298600197
I0205 12:10:57.078212 140283530442496 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.055685050785541534, loss=0.027499284595251083
I0205 12:11:25.169059 140451058161472 spec.py:321] Evaluating on the training split.
I0205 12:13:19.697376 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 12:13:22.720153 140451058161472 spec.py:349] Evaluating on the test split.
I0205 12:13:25.706733 140451058161472 submission_runner.py:408] Time since start: 17502.01s, 	Step: 35887, 	{'train/accuracy': 0.993681788444519, 'train/loss': 0.019934918731451035, 'train/mean_average_precision': 0.6535335108823093, 'validation/accuracy': 0.9863148927688599, 'validation/loss': 0.05142214149236679, 'validation/mean_average_precision': 0.24302966324393094, 'validation/num_examples': 43793, 'test/accuracy': 0.9854927659034729, 'test/loss': 0.05451366677880287, 'test/mean_average_precision': 0.2386775963454686, 'test/num_examples': 43793, 'score': 11538.582442760468, 'total_duration': 17502.014199733734, 'accumulated_submission_time': 11538.582442760468, 'accumulated_eval_time': 5960.965605020523, 'accumulated_logging_time': 1.4715502262115479}
I0205 12:13:25.729009 140283262007040 logging_writer.py:48] [35887] accumulated_eval_time=5960.965605, accumulated_logging_time=1.471550, accumulated_submission_time=11538.582443, global_step=35887, preemption_count=0, score=11538.582443, test/accuracy=0.985493, test/loss=0.054514, test/mean_average_precision=0.238678, test/num_examples=43793, total_duration=17502.014200, train/accuracy=0.993682, train/loss=0.019935, train/mean_average_precision=0.653534, validation/accuracy=0.986315, validation/loss=0.051422, validation/mean_average_precision=0.243030, validation/num_examples=43793
I0205 12:13:30.252122 140283731310336 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.062381256371736526, loss=0.029602527618408203
I0205 12:14:03.102323 140283262007040 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.06530127674341202, loss=0.028588062152266502
I0205 12:14:35.162703 140283731310336 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.06913521885871887, loss=0.028858346864581108
I0205 12:15:07.265446 140283262007040 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.05997997522354126, loss=0.02899201028048992
I0205 12:15:39.096781 140283731310336 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.056789740920066833, loss=0.02869940735399723
I0205 12:16:12.272467 140283262007040 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.06569359451532364, loss=0.0283063855022192
I0205 12:16:45.550251 140283731310336 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.06915579736232758, loss=0.031161317601799965
I0205 12:17:18.978169 140283262007040 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.05995297059416771, loss=0.029295194894075394
I0205 12:17:25.739613 140451058161472 spec.py:321] Evaluating on the training split.
I0205 12:19:23.455201 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 12:19:26.530459 140451058161472 spec.py:349] Evaluating on the test split.
I0205 12:19:29.511037 140451058161472 submission_runner.py:408] Time since start: 17865.82s, 	Step: 36621, 	{'train/accuracy': 0.9945728182792664, 'train/loss': 0.017682744190096855, 'train/mean_average_precision': 0.688483399506173, 'validation/accuracy': 0.986333966255188, 'validation/loss': 0.05144191533327103, 'validation/mean_average_precision': 0.24413099951531606, 'validation/num_examples': 43793, 'test/accuracy': 0.9854645133018494, 'test/loss': 0.054535992443561554, 'test/mean_average_precision': 0.2343355525502071, 'test/num_examples': 43793, 'score': 11778.559102535248, 'total_duration': 17865.818506240845, 'accumulated_submission_time': 11778.559102535248, 'accumulated_eval_time': 6084.737009048462, 'accumulated_logging_time': 1.5058512687683105}
I0205 12:19:29.533677 140266968643328 logging_writer.py:48] [36621] accumulated_eval_time=6084.737009, accumulated_logging_time=1.505851, accumulated_submission_time=11778.559103, global_step=36621, preemption_count=0, score=11778.559103, test/accuracy=0.985465, test/loss=0.054536, test/mean_average_precision=0.234336, test/num_examples=43793, total_duration=17865.818506, train/accuracy=0.994573, train/loss=0.017683, train/mean_average_precision=0.688483, validation/accuracy=0.986334, validation/loss=0.051442, validation/mean_average_precision=0.244131, validation/num_examples=43793
I0205 12:19:55.123890 140290213332736 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.0567680187523365, loss=0.027798136696219444
I0205 12:20:26.794763 140266968643328 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.0637543648481369, loss=0.027563346549868584
I0205 12:20:58.794454 140290213332736 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.06308863312005997, loss=0.029627420008182526
I0205 12:21:31.021749 140266968643328 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.06016211956739426, loss=0.027648193761706352
I0205 12:22:03.486733 140290213332736 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.06090962514281273, loss=0.026952210813760757
I0205 12:22:35.227586 140266968643328 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.06393839418888092, loss=0.029168011620640755
I0205 12:23:07.078257 140290213332736 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.09607777744531631, loss=0.030303115025162697
I0205 12:23:29.651467 140451058161472 spec.py:321] Evaluating on the training split.
I0205 12:25:23.939841 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 12:25:26.985695 140451058161472 spec.py:349] Evaluating on the test split.
I0205 12:25:29.990371 140451058161472 submission_runner.py:408] Time since start: 18226.30s, 	Step: 37372, 	{'train/accuracy': 0.9952541589736938, 'train/loss': 0.01587018556892872, 'train/mean_average_precision': 0.7463612898447716, 'validation/accuracy': 0.9863250255584717, 'validation/loss': 0.05211950093507767, 'validation/mean_average_precision': 0.23930383596515317, 'validation/num_examples': 43793, 'test/accuracy': 0.9855479598045349, 'test/loss': 0.055206358432769775, 'test/mean_average_precision': 0.23536206192428505, 'test/num_examples': 43793, 'score': 12018.643469572067, 'total_duration': 18226.29783797264, 'accumulated_submission_time': 12018.643469572067, 'accumulated_eval_time': 6205.075866937637, 'accumulated_logging_time': 1.5408875942230225}
I0205 12:25:30.013745 140283262007040 logging_writer.py:48] [37372] accumulated_eval_time=6205.075867, accumulated_logging_time=1.540888, accumulated_submission_time=12018.643470, global_step=37372, preemption_count=0, score=12018.643470, test/accuracy=0.985548, test/loss=0.055206, test/mean_average_precision=0.235362, test/num_examples=43793, total_duration=18226.297838, train/accuracy=0.995254, train/loss=0.015870, train/mean_average_precision=0.746361, validation/accuracy=0.986325, validation/loss=0.052120, validation/mean_average_precision=0.239304, validation/num_examples=43793
I0205 12:25:39.322369 140283731310336 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.06847923994064331, loss=0.02926005609333515
I0205 12:26:11.458704 140283262007040 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.06083017960190773, loss=0.02804170735180378
I0205 12:26:43.454112 140283731310336 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.06645199656486511, loss=0.028495941311120987
I0205 12:27:15.890511 140283262007040 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.05834074690937996, loss=0.027537601068615913
I0205 12:27:48.094522 140283731310336 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.07775352895259857, loss=0.028990305960178375
I0205 12:28:20.070248 140283262007040 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.06421922892332077, loss=0.026974499225616455
I0205 12:28:51.833882 140283731310336 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.06970971077680588, loss=0.02913537062704563
I0205 12:29:23.750112 140283262007040 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.06797526776790619, loss=0.028132565319538116
I0205 12:29:30.126979 140451058161472 spec.py:321] Evaluating on the training split.
I0205 12:31:19.518290 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 12:31:22.546408 140451058161472 spec.py:349] Evaluating on the test split.
I0205 12:31:25.573983 140451058161472 submission_runner.py:408] Time since start: 18581.88s, 	Step: 38121, 	{'train/accuracy': 0.9949754476547241, 'train/loss': 0.016621870920062065, 'train/mean_average_precision': 0.7227896262201021, 'validation/accuracy': 0.9864480495452881, 'validation/loss': 0.05189632624387741, 'validation/mean_average_precision': 0.23987123868166987, 'validation/num_examples': 43793, 'test/accuracy': 0.9855525493621826, 'test/loss': 0.05509902909398079, 'test/mean_average_precision': 0.2385763605440669, 'test/num_examples': 43793, 'score': 12258.72385263443, 'total_duration': 18581.881452083588, 'accumulated_submission_time': 12258.72385263443, 'accumulated_eval_time': 6320.522824525833, 'accumulated_logging_time': 1.5767641067504883}
I0205 12:31:25.596176 140266968643328 logging_writer.py:48] [38121] accumulated_eval_time=6320.522825, accumulated_logging_time=1.576764, accumulated_submission_time=12258.723853, global_step=38121, preemption_count=0, score=12258.723853, test/accuracy=0.985553, test/loss=0.055099, test/mean_average_precision=0.238576, test/num_examples=43793, total_duration=18581.881452, train/accuracy=0.994975, train/loss=0.016622, train/mean_average_precision=0.722790, validation/accuracy=0.986448, validation/loss=0.051896, validation/mean_average_precision=0.239871, validation/num_examples=43793
I0205 12:31:51.237120 140283530442496 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.07369253039360046, loss=0.028874937444925308
I0205 12:32:23.058159 140266968643328 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.07111519575119019, loss=0.02728673256933689
I0205 12:32:54.594523 140283530442496 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.06470240652561188, loss=0.027426840737462044
I0205 12:33:26.459006 140266968643328 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.06530871242284775, loss=0.02544442005455494
I0205 12:33:58.075465 140283530442496 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.07083332538604736, loss=0.02825910784304142
I0205 12:34:29.952632 140266968643328 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.07554509490728378, loss=0.026103518903255463
I0205 12:35:02.450682 140283530442496 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.06307194381952286, loss=0.026560431346297264
I0205 12:35:25.701027 140451058161472 spec.py:321] Evaluating on the training split.
I0205 12:37:16.711052 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 12:37:19.825671 140451058161472 spec.py:349] Evaluating on the test split.
I0205 12:37:22.869029 140451058161472 submission_runner.py:408] Time since start: 18939.18s, 	Step: 38874, 	{'train/accuracy': 0.9947724938392639, 'train/loss': 0.016733374446630478, 'train/mean_average_precision': 0.714931953560515, 'validation/accuracy': 0.986291766166687, 'validation/loss': 0.05320531502366066, 'validation/mean_average_precision': 0.23560947523729037, 'validation/num_examples': 43793, 'test/accuracy': 0.9855129718780518, 'test/loss': 0.05614528805017471, 'test/mean_average_precision': 0.23769189452529768, 'test/num_examples': 43793, 'score': 12498.797527074814, 'total_duration': 18939.176498413086, 'accumulated_submission_time': 12498.797527074814, 'accumulated_eval_time': 6437.69078040123, 'accumulated_logging_time': 1.6097221374511719}
I0205 12:37:22.897669 140283262007040 logging_writer.py:48] [38874] accumulated_eval_time=6437.690780, accumulated_logging_time=1.609722, accumulated_submission_time=12498.797527, global_step=38874, preemption_count=0, score=12498.797527, test/accuracy=0.985513, test/loss=0.056145, test/mean_average_precision=0.237692, test/num_examples=43793, total_duration=18939.176498, train/accuracy=0.994772, train/loss=0.016733, train/mean_average_precision=0.714932, validation/accuracy=0.986292, validation/loss=0.053205, validation/mean_average_precision=0.235609, validation/num_examples=43793
I0205 12:37:31.516233 140290213332736 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.07720717787742615, loss=0.027196815237402916
I0205 12:38:03.337311 140283262007040 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.07268667966127396, loss=0.026808494701981544
I0205 12:38:35.377954 140290213332736 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.06803102046251297, loss=0.026946134865283966
I0205 12:39:07.925968 140283262007040 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.07214292883872986, loss=0.027666984125971794
I0205 12:39:39.655517 140290213332736 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.0731215700507164, loss=0.028406456112861633
I0205 12:40:11.869943 140283262007040 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.07710599899291992, loss=0.027043858543038368
I0205 12:40:43.300766 140290213332736 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.07394126057624817, loss=0.026616062968969345
I0205 12:41:15.380329 140283262007040 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.07623689621686935, loss=0.02857884019613266
I0205 12:41:22.882172 140451058161472 spec.py:321] Evaluating on the training split.
I0205 12:43:12.592498 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 12:43:15.661794 140451058161472 spec.py:349] Evaluating on the test split.
I0205 12:43:18.692274 140451058161472 submission_runner.py:408] Time since start: 19295.00s, 	Step: 39625, 	{'train/accuracy': 0.994320273399353, 'train/loss': 0.01782328262925148, 'train/mean_average_precision': 0.705524199624799, 'validation/accuracy': 0.986319363117218, 'validation/loss': 0.05309564247727394, 'validation/mean_average_precision': 0.24038347614439076, 'validation/num_examples': 43793, 'test/accuracy': 0.9854274988174438, 'test/loss': 0.05641566216945648, 'test/mean_average_precision': 0.23018162124236663, 'test/num_examples': 43793, 'score': 12738.750935316086, 'total_duration': 19294.999740600586, 'accumulated_submission_time': 12738.750935316086, 'accumulated_eval_time': 6553.50083398819, 'accumulated_logging_time': 1.6491947174072266}
I0205 12:43:18.714900 140266968643328 logging_writer.py:48] [39625] accumulated_eval_time=6553.500834, accumulated_logging_time=1.649195, accumulated_submission_time=12738.750935, global_step=39625, preemption_count=0, score=12738.750935, test/accuracy=0.985427, test/loss=0.056416, test/mean_average_precision=0.230182, test/num_examples=43793, total_duration=19294.999741, train/accuracy=0.994320, train/loss=0.017823, train/mean_average_precision=0.705524, validation/accuracy=0.986319, validation/loss=0.053096, validation/mean_average_precision=0.240383, validation/num_examples=43793
I0205 12:43:43.048738 140283530442496 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.08214690536260605, loss=0.026981914415955544
I0205 12:44:14.732689 140266968643328 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.1530539095401764, loss=0.03016539476811886
I0205 12:44:46.568106 140283530442496 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.07256931811571121, loss=0.0276152603328228
I0205 12:45:18.598176 140266968643328 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.07071108371019363, loss=0.02716602198779583
I0205 12:45:50.817299 140283530442496 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.0703093409538269, loss=0.026736728847026825
I0205 12:46:22.851372 140266968643328 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.07248562574386597, loss=0.028355766087770462
I0205 12:46:54.651123 140283530442496 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.06281086802482605, loss=0.026637308299541473
I0205 12:47:18.823751 140451058161472 spec.py:321] Evaluating on the training split.
I0205 12:49:03.610289 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 12:49:06.673461 140451058161472 spec.py:349] Evaluating on the test split.
I0205 12:49:09.689757 140451058161472 submission_runner.py:408] Time since start: 19646.00s, 	Step: 40377, 	{'train/accuracy': 0.9941571950912476, 'train/loss': 0.018354937434196472, 'train/mean_average_precision': 0.6876441021637686, 'validation/accuracy': 0.9862978458404541, 'validation/loss': 0.053439173847436905, 'validation/mean_average_precision': 0.23703639568501791, 'validation/num_examples': 43793, 'test/accuracy': 0.9854485392570496, 'test/loss': 0.05665614455938339, 'test/mean_average_precision': 0.23156396099689264, 'test/num_examples': 43793, 'score': 12978.82855129242, 'total_duration': 19645.997228384018, 'accumulated_submission_time': 12978.82855129242, 'accumulated_eval_time': 6664.366796016693, 'accumulated_logging_time': 1.6828031539916992}
I0205 12:49:09.712897 140283731310336 logging_writer.py:48] [40377] accumulated_eval_time=6664.366796, accumulated_logging_time=1.682803, accumulated_submission_time=12978.828551, global_step=40377, preemption_count=0, score=12978.828551, test/accuracy=0.985449, test/loss=0.056656, test/mean_average_precision=0.231564, test/num_examples=43793, total_duration=19645.997228, train/accuracy=0.994157, train/loss=0.018355, train/mean_average_precision=0.687644, validation/accuracy=0.986298, validation/loss=0.053439, validation/mean_average_precision=0.237036, validation/num_examples=43793
I0205 12:49:17.273162 140290213332736 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.0735112726688385, loss=0.026923298835754395
I0205 12:49:48.766249 140283731310336 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.0743202343583107, loss=0.02803114801645279
I0205 12:50:20.609636 140290213332736 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.0763484314084053, loss=0.028763016685843468
I0205 12:50:52.100190 140283731310336 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.08117068558931351, loss=0.02634783647954464
I0205 12:51:23.621325 140290213332736 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.07086651772260666, loss=0.026764530688524246
I0205 12:51:55.158251 140283731310336 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.0709904357790947, loss=0.028197703883051872
I0205 12:52:26.756327 140290213332736 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.07959488034248352, loss=0.02638377994298935
I0205 12:52:58.665824 140283731310336 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.07897371798753738, loss=0.02545461244881153
I0205 12:53:09.928261 140451058161472 spec.py:321] Evaluating on the training split.
I0205 12:55:01.181067 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 12:55:04.351924 140451058161472 spec.py:349] Evaluating on the test split.
I0205 12:55:07.321210 140451058161472 submission_runner.py:408] Time since start: 20003.63s, 	Step: 41136, 	{'train/accuracy': 0.9942678809165955, 'train/loss': 0.018299048766493797, 'train/mean_average_precision': 0.6918804230446601, 'validation/accuracy': 0.9859828352928162, 'validation/loss': 0.05258406326174736, 'validation/mean_average_precision': 0.23572478720522083, 'validation/num_examples': 43793, 'test/accuracy': 0.9851621389389038, 'test/loss': 0.05586322024464607, 'test/mean_average_precision': 0.23260640344393388, 'test/num_examples': 43793, 'score': 13219.013098239899, 'total_duration': 20003.628672599792, 'accumulated_submission_time': 13219.013098239899, 'accumulated_eval_time': 6781.759689092636, 'accumulated_logging_time': 1.7166087627410889}
I0205 12:55:07.343812 140266968643328 logging_writer.py:48] [41136] accumulated_eval_time=6781.759689, accumulated_logging_time=1.716609, accumulated_submission_time=13219.013098, global_step=41136, preemption_count=0, score=13219.013098, test/accuracy=0.985162, test/loss=0.055863, test/mean_average_precision=0.232606, test/num_examples=43793, total_duration=20003.628673, train/accuracy=0.994268, train/loss=0.018299, train/mean_average_precision=0.691880, validation/accuracy=0.985983, validation/loss=0.052584, validation/mean_average_precision=0.235725, validation/num_examples=43793
I0205 12:55:28.057780 140283262007040 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.07352398335933685, loss=0.025533024221658707
I0205 12:56:00.013956 140266968643328 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.08000224828720093, loss=0.027719872072339058
I0205 12:56:32.054915 140283262007040 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.09808817505836487, loss=0.02893073670566082
I0205 12:57:03.982077 140266968643328 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.08097520470619202, loss=0.02742396481335163
I0205 12:57:35.850593 140283262007040 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.08485518395900726, loss=0.026100540533661842
I0205 12:58:07.569956 140266968643328 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.07566961646080017, loss=0.026658911257982254
I0205 12:58:39.358270 140283262007040 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.07954736799001694, loss=0.026212794706225395
I0205 12:59:07.626641 140451058161472 spec.py:321] Evaluating on the training split.
I0205 13:00:52.690504 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 13:00:55.736929 140451058161472 spec.py:349] Evaluating on the test split.
I0205 13:01:00.370628 140451058161472 submission_runner.py:408] Time since start: 20356.68s, 	Step: 41890, 	{'train/accuracy': 0.9942914843559265, 'train/loss': 0.017965376377105713, 'train/mean_average_precision': 0.701986704581986, 'validation/accuracy': 0.9862337112426758, 'validation/loss': 0.05374501273036003, 'validation/mean_average_precision': 0.23481378091598354, 'validation/num_examples': 43793, 'test/accuracy': 0.9853798747062683, 'test/loss': 0.056821972131729126, 'test/mean_average_precision': 0.23249823551142412, 'test/num_examples': 43793, 'score': 13459.264628887177, 'total_duration': 20356.678092479706, 'accumulated_submission_time': 13459.264628887177, 'accumulated_eval_time': 6894.503623247147, 'accumulated_logging_time': 1.7502388954162598}
I0205 13:01:00.395876 140283530442496 logging_writer.py:48] [41890] accumulated_eval_time=6894.503623, accumulated_logging_time=1.750239, accumulated_submission_time=13459.264629, global_step=41890, preemption_count=0, score=13459.264629, test/accuracy=0.985380, test/loss=0.056822, test/mean_average_precision=0.232498, test/num_examples=43793, total_duration=20356.678092, train/accuracy=0.994291, train/loss=0.017965, train/mean_average_precision=0.701987, validation/accuracy=0.986234, validation/loss=0.053745, validation/mean_average_precision=0.234814, validation/num_examples=43793
I0205 13:01:04.098834 140290213332736 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.0767471194267273, loss=0.02799360267817974
I0205 13:01:36.142889 140283530442496 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.07841340452432632, loss=0.026041073724627495
I0205 13:02:08.404115 140290213332736 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.07088299840688705, loss=0.027300577610731125
I0205 13:02:39.960752 140283530442496 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.0756615623831749, loss=0.027091791853308678
I0205 13:03:11.661633 140290213332736 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.08215933293104172, loss=0.02752651460468769
I0205 13:03:43.293282 140283530442496 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.0723821371793747, loss=0.026010699570178986
I0205 13:04:15.615003 140290213332736 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.07826079428195953, loss=0.025301093235611916
I0205 13:04:49.517809 140283530442496 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.08238442987203598, loss=0.026600169017910957
I0205 13:05:00.527344 140451058161472 spec.py:321] Evaluating on the training split.
I0205 13:06:49.749098 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 13:06:52.875846 140451058161472 spec.py:349] Evaluating on the test split.
I0205 13:06:55.959386 140451058161472 submission_runner.py:408] Time since start: 20712.27s, 	Step: 42636, 	{'train/accuracy': 0.9941781163215637, 'train/loss': 0.017894301563501358, 'train/mean_average_precision': 0.6990086529737705, 'validation/accuracy': 0.9864057898521423, 'validation/loss': 0.05441704019904137, 'validation/mean_average_precision': 0.23981925742551147, 'validation/num_examples': 43793, 'test/accuracy': 0.9854514598846436, 'test/loss': 0.058204285800457, 'test/mean_average_precision': 0.23092372916024198, 'test/num_examples': 43793, 'score': 13699.364458560944, 'total_duration': 20712.2668569088, 'accumulated_submission_time': 13699.364458560944, 'accumulated_eval_time': 7009.9356207847595, 'accumulated_logging_time': 1.7863051891326904}
I0205 13:06:55.982877 140266968643328 logging_writer.py:48] [42636] accumulated_eval_time=7009.935621, accumulated_logging_time=1.786305, accumulated_submission_time=13699.364459, global_step=42636, preemption_count=0, score=13699.364459, test/accuracy=0.985451, test/loss=0.058204, test/mean_average_precision=0.230924, test/num_examples=43793, total_duration=20712.266857, train/accuracy=0.994178, train/loss=0.017894, train/mean_average_precision=0.699009, validation/accuracy=0.986406, validation/loss=0.054417, validation/mean_average_precision=0.239819, validation/num_examples=43793
I0205 13:07:16.385100 140283262007040 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.0747092217206955, loss=0.026382844895124435
I0205 13:07:47.830760 140266968643328 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.08311963081359863, loss=0.026719385758042336
I0205 13:08:19.391561 140283262007040 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.0841449648141861, loss=0.028447771444916725
I0205 13:08:50.737518 140266968643328 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.07421603798866272, loss=0.026140308007597923
I0205 13:09:22.685295 140283262007040 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.10352311283349991, loss=0.027572311460971832
I0205 13:09:54.448765 140266968643328 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.07524328678846359, loss=0.026599910110235214
I0205 13:10:26.488147 140283262007040 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.08050834387540817, loss=0.026042921468615532
I0205 13:10:55.995644 140451058161472 spec.py:321] Evaluating on the training split.
I0205 13:12:46.432527 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 13:12:49.553905 140451058161472 spec.py:349] Evaluating on the test split.
I0205 13:12:52.605952 140451058161472 submission_runner.py:408] Time since start: 21068.91s, 	Step: 43393, 	{'train/accuracy': 0.9946008920669556, 'train/loss': 0.016835426911711693, 'train/mean_average_precision': 0.7236670943979682, 'validation/accuracy': 0.9863424897193909, 'validation/loss': 0.054734326899051666, 'validation/mean_average_precision': 0.2367951793593295, 'validation/num_examples': 43793, 'test/accuracy': 0.9854030609130859, 'test/loss': 0.058230116963386536, 'test/mean_average_precision': 0.22809934313288432, 'test/num_examples': 43793, 'score': 13939.344955205917, 'total_duration': 21068.913420438766, 'accumulated_submission_time': 13939.344955205917, 'accumulated_eval_time': 7126.545880794525, 'accumulated_logging_time': 1.8220069408416748}
I0205 13:12:52.638839 140283530442496 logging_writer.py:48] [43393] accumulated_eval_time=7126.545881, accumulated_logging_time=1.822007, accumulated_submission_time=13939.344955, global_step=43393, preemption_count=0, score=13939.344955, test/accuracy=0.985403, test/loss=0.058230, test/mean_average_precision=0.228099, test/num_examples=43793, total_duration=21068.913420, train/accuracy=0.994601, train/loss=0.016835, train/mean_average_precision=0.723667, validation/accuracy=0.986342, validation/loss=0.054734, validation/mean_average_precision=0.236795, validation/num_examples=43793
I0205 13:12:55.218358 140290213332736 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.09484633803367615, loss=0.026384206488728523
I0205 13:13:27.041869 140283530442496 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.07921433448791504, loss=0.026844751089811325
I0205 13:13:58.939661 140290213332736 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.09327523410320282, loss=0.02772149071097374
I0205 13:14:30.498965 140283530442496 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.07545819878578186, loss=0.02497701905667782
I0205 13:15:02.243753 140290213332736 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.08458179235458374, loss=0.027570253238081932
I0205 13:15:34.192206 140283530442496 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.09090638160705566, loss=0.02711878903210163
I0205 13:16:06.263704 140290213332736 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.10377313941717148, loss=0.0267917700111866
I0205 13:16:38.038138 140283530442496 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.07874482125043869, loss=0.0257695484906435
I0205 13:16:52.904006 140451058161472 spec.py:321] Evaluating on the training split.
I0205 13:18:36.036552 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 13:18:39.229440 140451058161472 spec.py:349] Evaluating on the test split.
I0205 13:18:42.247220 140451058161472 submission_runner.py:408] Time since start: 21418.55s, 	Step: 44147, 	{'train/accuracy': 0.9953388571739197, 'train/loss': 0.015141450800001621, 'train/mean_average_precision': 0.7568423681622725, 'validation/accuracy': 0.9860754013061523, 'validation/loss': 0.05479004979133606, 'validation/mean_average_precision': 0.2288448749028145, 'validation/num_examples': 43793, 'test/accuracy': 0.9852429628372192, 'test/loss': 0.05832561478018761, 'test/mean_average_precision': 0.22534321390666928, 'test/num_examples': 43793, 'score': 14179.578085184097, 'total_duration': 21418.554672002792, 'accumulated_submission_time': 14179.578085184097, 'accumulated_eval_time': 7235.889031648636, 'accumulated_logging_time': 1.866541862487793}
I0205 13:18:42.270768 140266968643328 logging_writer.py:48] [44147] accumulated_eval_time=7235.889032, accumulated_logging_time=1.866542, accumulated_submission_time=14179.578085, global_step=44147, preemption_count=0, score=14179.578085, test/accuracy=0.985243, test/loss=0.058326, test/mean_average_precision=0.225343, test/num_examples=43793, total_duration=21418.554672, train/accuracy=0.995339, train/loss=0.015141, train/mean_average_precision=0.756842, validation/accuracy=0.986075, validation/loss=0.054790, validation/mean_average_precision=0.228845, validation/num_examples=43793
I0205 13:18:59.669291 140283731310336 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.07511848956346512, loss=0.02542203478515148
I0205 13:19:32.591117 140266968643328 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.09172794222831726, loss=0.025806555524468422
I0205 13:20:06.002117 140283731310336 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.08286909759044647, loss=0.027515623718500137
I0205 13:20:38.374663 140266968643328 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.0886913537979126, loss=0.026495663449168205
I0205 13:21:10.575577 140283731310336 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.07987134903669357, loss=0.025130709633231163
I0205 13:21:43.028647 140266968643328 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.0787777528166771, loss=0.02587164007127285
I0205 13:22:15.468150 140283731310336 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.09153913706541061, loss=0.026899633929133415
I0205 13:22:42.540019 140451058161472 spec.py:321] Evaluating on the training split.
I0205 13:24:31.890383 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 13:24:34.993959 140451058161472 spec.py:349] Evaluating on the test split.
I0205 13:24:38.044794 140451058161472 submission_runner.py:408] Time since start: 21774.35s, 	Step: 44884, 	{'train/accuracy': 0.996155858039856, 'train/loss': 0.013371159322559834, 'train/mean_average_precision': 0.7954491680973936, 'validation/accuracy': 0.9861951470375061, 'validation/loss': 0.05493079498410225, 'validation/mean_average_precision': 0.23999916244612557, 'validation/num_examples': 43793, 'test/accuracy': 0.9854169487953186, 'test/loss': 0.058216750621795654, 'test/mean_average_precision': 0.22741323348126463, 'test/num_examples': 43793, 'score': 14419.814072847366, 'total_duration': 21774.352262735367, 'accumulated_submission_time': 14419.814072847366, 'accumulated_eval_time': 7351.393758773804, 'accumulated_logging_time': 1.9024038314819336}
I0205 13:24:38.068091 140283262007040 logging_writer.py:48] [44884] accumulated_eval_time=7351.393759, accumulated_logging_time=1.902404, accumulated_submission_time=14419.814073, global_step=44884, preemption_count=0, score=14419.814073, test/accuracy=0.985417, test/loss=0.058217, test/mean_average_precision=0.227413, test/num_examples=43793, total_duration=21774.352263, train/accuracy=0.996156, train/loss=0.013371, train/mean_average_precision=0.795449, validation/accuracy=0.986195, validation/loss=0.054931, validation/mean_average_precision=0.239999, validation/num_examples=43793
I0205 13:24:43.615634 140290213332736 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.06956591457128525, loss=0.025197317823767662
I0205 13:25:16.176149 140283262007040 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.10746994614601135, loss=0.027783146128058434
I0205 13:25:49.131300 140290213332736 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.08388228714466095, loss=0.027366191148757935
I0205 13:26:22.087975 140283262007040 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.08300630748271942, loss=0.025651995092630386
I0205 13:26:55.098825 140290213332736 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.09632407128810883, loss=0.025609899312257767
I0205 13:27:27.717661 140283262007040 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.08608835190534592, loss=0.02570037916302681
I0205 13:28:00.002260 140290213332736 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.09378766268491745, loss=0.025189900770783424
I0205 13:28:32.021375 140283262007040 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.0851646140217781, loss=0.024262860417366028
I0205 13:28:38.081637 140451058161472 spec.py:321] Evaluating on the training split.
I0205 13:30:26.174769 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 13:30:29.264204 140451058161472 spec.py:349] Evaluating on the test split.
I0205 13:30:32.244888 140451058161472 submission_runner.py:408] Time since start: 22128.55s, 	Step: 45620, 	{'train/accuracy': 0.9964368939399719, 'train/loss': 0.01280219480395317, 'train/mean_average_precision': 0.7997349957824464, 'validation/accuracy': 0.986088752746582, 'validation/loss': 0.05589419975876808, 'validation/mean_average_precision': 0.23350083450891645, 'validation/num_examples': 43793, 'test/accuracy': 0.9852564930915833, 'test/loss': 0.059393975883722305, 'test/mean_average_precision': 0.22205511168090772, 'test/num_examples': 43793, 'score': 14659.792273283005, 'total_duration': 22128.552355766296, 'accumulated_submission_time': 14659.792273283005, 'accumulated_eval_time': 7465.55695939064, 'accumulated_logging_time': 1.9380853176116943}
I0205 13:30:32.269584 140266968643328 logging_writer.py:48] [45620] accumulated_eval_time=7465.556959, accumulated_logging_time=1.938085, accumulated_submission_time=14659.792273, global_step=45620, preemption_count=0, score=14659.792273, test/accuracy=0.985256, test/loss=0.059394, test/mean_average_precision=0.222055, test/num_examples=43793, total_duration=22128.552356, train/accuracy=0.996437, train/loss=0.012802, train/mean_average_precision=0.799735, validation/accuracy=0.986089, validation/loss=0.055894, validation/mean_average_precision=0.233501, validation/num_examples=43793
I0205 13:30:57.746234 140283731310336 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.09125164151191711, loss=0.02569110319018364
I0205 13:31:29.326225 140266968643328 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.08179397881031036, loss=0.025114547461271286
I0205 13:32:01.102937 140283731310336 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.09176661819219589, loss=0.02488410845398903
I0205 13:32:32.544210 140266968643328 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.08105780929327011, loss=0.02568085305392742
I0205 13:33:04.033926 140283731310336 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.08210016787052155, loss=0.025718336924910545
I0205 13:33:35.165344 140266968643328 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.08578083664178848, loss=0.023431535810232162
I0205 13:34:06.959625 140283731310336 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.07817081362009048, loss=0.026396755129098892
I0205 13:34:32.370200 140451058161472 spec.py:321] Evaluating on the training split.
I0205 13:36:22.507160 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 13:36:25.541923 140451058161472 spec.py:349] Evaluating on the test split.
I0205 13:36:28.506085 140451058161472 submission_runner.py:408] Time since start: 22484.81s, 	Step: 46380, 	{'train/accuracy': 0.995314359664917, 'train/loss': 0.015161000192165375, 'train/mean_average_precision': 0.7500370780843769, 'validation/accuracy': 0.9860250353813171, 'validation/loss': 0.056016840040683746, 'validation/mean_average_precision': 0.2258241513802438, 'validation/num_examples': 43793, 'test/accuracy': 0.9850963950157166, 'test/loss': 0.05946921184659004, 'test/mean_average_precision': 0.2228981628786851, 'test/num_examples': 43793, 'score': 14899.860787391663, 'total_duration': 22484.813533067703, 'accumulated_submission_time': 14899.860787391663, 'accumulated_eval_time': 7581.6927881240845, 'accumulated_logging_time': 1.9741096496582031}
I0205 13:36:28.531877 140283530442496 logging_writer.py:48] [46380] accumulated_eval_time=7581.692788, accumulated_logging_time=1.974110, accumulated_submission_time=14899.860787, global_step=46380, preemption_count=0, score=14899.860787, test/accuracy=0.985096, test/loss=0.059469, test/mean_average_precision=0.222898, test/num_examples=43793, total_duration=22484.813533, train/accuracy=0.995314, train/loss=0.015161, train/mean_average_precision=0.750037, validation/accuracy=0.986025, validation/loss=0.056017, validation/mean_average_precision=0.225824, validation/num_examples=43793
I0205 13:36:35.304624 140290213332736 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.08717627078294754, loss=0.025483787059783936
I0205 13:37:07.539547 140283530442496 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.08693468570709229, loss=0.0248065497726202
I0205 13:37:39.575040 140290213332736 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.08026772737503052, loss=0.025116760283708572
I0205 13:38:11.864325 140283530442496 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.0879962220788002, loss=0.025183064863085747
I0205 13:38:43.994154 140290213332736 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.08035992085933685, loss=0.024716947227716446
I0205 13:39:16.018093 140283530442496 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.08105120807886124, loss=0.025143850594758987
I0205 13:39:47.609087 140290213332736 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.08289508521556854, loss=0.02550182119011879
I0205 13:40:19.721202 140283530442496 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.08610910177230835, loss=0.025294499471783638
I0205 13:40:28.527680 140451058161472 spec.py:321] Evaluating on the training split.
I0205 13:42:15.268465 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 13:42:18.241978 140451058161472 spec.py:349] Evaluating on the test split.
I0205 13:42:21.252392 140451058161472 submission_runner.py:408] Time since start: 22837.56s, 	Step: 47129, 	{'train/accuracy': 0.9956495761871338, 'train/loss': 0.014239811338484287, 'train/mean_average_precision': 0.7698141151166427, 'validation/accuracy': 0.9861208200454712, 'validation/loss': 0.056531015783548355, 'validation/mean_average_precision': 0.2288986155071729, 'validation/num_examples': 43793, 'test/accuracy': 0.9853002429008484, 'test/loss': 0.05993714556097984, 'test/mean_average_precision': 0.23162354453084574, 'test/num_examples': 43793, 'score': 15139.825226068497, 'total_duration': 22837.559860944748, 'accumulated_submission_time': 15139.825226068497, 'accumulated_eval_time': 7694.4174518585205, 'accumulated_logging_time': 2.0112147331237793}
I0205 13:42:21.276627 140283262007040 logging_writer.py:48] [47129] accumulated_eval_time=7694.417452, accumulated_logging_time=2.011215, accumulated_submission_time=15139.825226, global_step=47129, preemption_count=0, score=15139.825226, test/accuracy=0.985300, test/loss=0.059937, test/mean_average_precision=0.231624, test/num_examples=43793, total_duration=22837.559861, train/accuracy=0.995650, train/loss=0.014240, train/mean_average_precision=0.769814, validation/accuracy=0.986121, validation/loss=0.056531, validation/mean_average_precision=0.228899, validation/num_examples=43793
I0205 13:42:43.766691 140283731310336 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.0855465680360794, loss=0.024658823385834694
I0205 13:43:15.664258 140283262007040 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.09210719168186188, loss=0.026046928018331528
I0205 13:43:47.656717 140283731310336 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.08912134170532227, loss=0.024915698915719986
I0205 13:44:19.728073 140283262007040 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.07800409197807312, loss=0.025453872978687286
I0205 13:44:51.923660 140283731310336 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.08964256197214127, loss=0.024213356897234917
I0205 13:45:24.254395 140283262007040 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.09536482393741608, loss=0.024532940238714218
I0205 13:45:56.424657 140283731310336 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.08737710863351822, loss=0.025019364431500435
I0205 13:46:21.431020 140451058161472 spec.py:321] Evaluating on the training split.
I0205 13:48:10.492450 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 13:48:13.595992 140451058161472 spec.py:349] Evaluating on the test split.
I0205 13:48:16.606896 140451058161472 submission_runner.py:408] Time since start: 23192.91s, 	Step: 47879, 	{'train/accuracy': 0.9952120780944824, 'train/loss': 0.015117822214961052, 'train/mean_average_precision': 0.7573643665429037, 'validation/accuracy': 0.9860530495643616, 'validation/loss': 0.05645659193396568, 'validation/mean_average_precision': 0.2272830550792519, 'validation/num_examples': 43793, 'test/accuracy': 0.9852320551872253, 'test/loss': 0.05987373739480972, 'test/mean_average_precision': 0.22779078451152113, 'test/num_examples': 43793, 'score': 15379.947883367538, 'total_duration': 23192.914344787598, 'accumulated_submission_time': 15379.947883367538, 'accumulated_eval_time': 7809.593261241913, 'accumulated_logging_time': 2.0465872287750244}
I0205 13:48:16.639899 140266968643328 logging_writer.py:48] [47879] accumulated_eval_time=7809.593261, accumulated_logging_time=2.046587, accumulated_submission_time=15379.947883, global_step=47879, preemption_count=0, score=15379.947883, test/accuracy=0.985232, test/loss=0.059874, test/mean_average_precision=0.227791, test/num_examples=43793, total_duration=23192.914345, train/accuracy=0.995212, train/loss=0.015118, train/mean_average_precision=0.757364, validation/accuracy=0.986053, validation/loss=0.056457, validation/mean_average_precision=0.227283, validation/num_examples=43793
I0205 13:48:23.730210 140290213332736 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.09157740324735641, loss=0.026477891951799393
I0205 13:48:55.805383 140266968643328 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.0798671767115593, loss=0.023829195648431778
I0205 13:49:28.212537 140290213332736 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.10211986303329468, loss=0.024739492684602737
I0205 13:50:00.391283 140266968643328 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.09896483272314072, loss=0.025071775540709496
I0205 13:50:32.815470 140290213332736 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.094872385263443, loss=0.025721022859215736
I0205 13:51:04.772222 140266968643328 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.0818956196308136, loss=0.02403266169130802
I0205 13:51:36.694538 140290213332736 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.0817398875951767, loss=0.024797091260552406
I0205 13:52:08.976359 140266968643328 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.09529244154691696, loss=0.02538078837096691
I0205 13:52:16.729651 140451058161472 spec.py:321] Evaluating on the training split.
I0205 13:54:03.433131 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 13:54:06.510346 140451058161472 spec.py:349] Evaluating on the test split.
I0205 13:54:09.525352 140451058161472 submission_runner.py:408] Time since start: 23545.83s, 	Step: 48625, 	{'train/accuracy': 0.9950177669525146, 'train/loss': 0.01554893609136343, 'train/mean_average_precision': 0.7415567667563101, 'validation/accuracy': 0.9860798716545105, 'validation/loss': 0.05699317157268524, 'validation/mean_average_precision': 0.23002389334434853, 'validation/num_examples': 43793, 'test/accuracy': 0.9852830171585083, 'test/loss': 0.060362961143255234, 'test/mean_average_precision': 0.22806503587802487, 'test/num_examples': 43793, 'score': 15620.005237102509, 'total_duration': 23545.832807779312, 'accumulated_submission_time': 15620.005237102509, 'accumulated_eval_time': 7922.388898611069, 'accumulated_logging_time': 2.0906825065612793}
I0205 13:54:09.550066 140283262007040 logging_writer.py:48] [48625] accumulated_eval_time=7922.388899, accumulated_logging_time=2.090683, accumulated_submission_time=15620.005237, global_step=48625, preemption_count=0, score=15620.005237, test/accuracy=0.985283, test/loss=0.060363, test/mean_average_precision=0.228065, test/num_examples=43793, total_duration=23545.832808, train/accuracy=0.995018, train/loss=0.015549, train/mean_average_precision=0.741557, validation/accuracy=0.986080, validation/loss=0.056993, validation/mean_average_precision=0.230024, validation/num_examples=43793
I0205 13:54:34.886533 140283530442496 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.0791352316737175, loss=0.025005042552947998
I0205 13:55:07.695999 140283262007040 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.07711374759674072, loss=0.024208232760429382
I0205 13:55:39.208756 140283530442496 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.09728167206048965, loss=0.027561023831367493
I0205 13:56:11.457071 140283262007040 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.08616992086172104, loss=0.024763211607933044
I0205 13:56:43.723537 140283530442496 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.0899086743593216, loss=0.026042936369776726
I0205 13:57:15.935073 140283262007040 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.08959636092185974, loss=0.024456290528178215
I0205 13:57:47.997500 140283530442496 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.09921509772539139, loss=0.025274408981204033
I0205 13:58:09.592611 140451058161472 spec.py:321] Evaluating on the training split.
I0205 14:00:03.225612 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 14:00:06.236820 140451058161472 spec.py:349] Evaluating on the test split.
I0205 14:00:09.172526 140451058161472 submission_runner.py:408] Time since start: 23905.48s, 	Step: 49368, 	{'train/accuracy': 0.9946566820144653, 'train/loss': 0.016457218676805496, 'train/mean_average_precision': 0.7364141469888428, 'validation/accuracy': 0.9861857891082764, 'validation/loss': 0.057639967650175095, 'validation/mean_average_precision': 0.22420575743211074, 'validation/num_examples': 43793, 'test/accuracy': 0.9852412939071655, 'test/loss': 0.061149779707193375, 'test/mean_average_precision': 0.22574784946939636, 'test/num_examples': 43793, 'score': 15860.014887571335, 'total_duration': 23905.479996204376, 'accumulated_submission_time': 15860.014887571335, 'accumulated_eval_time': 8041.968768358231, 'accumulated_logging_time': 2.127811908721924}
I0205 14:00:09.205968 140266968643328 logging_writer.py:48] [49368] accumulated_eval_time=8041.968768, accumulated_logging_time=2.127812, accumulated_submission_time=15860.014888, global_step=49368, preemption_count=0, score=15860.014888, test/accuracy=0.985241, test/loss=0.061150, test/mean_average_precision=0.225748, test/num_examples=43793, total_duration=23905.479996, train/accuracy=0.994657, train/loss=0.016457, train/mean_average_precision=0.736414, validation/accuracy=0.986186, validation/loss=0.057640, validation/mean_average_precision=0.224206, validation/num_examples=43793
I0205 14:00:19.524279 140290213332736 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.08763561397790909, loss=0.024790214374661446
I0205 14:00:51.320350 140266968643328 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.09397705644369125, loss=0.024651503190398216
I0205 14:01:22.830509 140290213332736 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.10687462985515594, loss=0.024220434948801994
I0205 14:01:55.199410 140266968643328 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.08409430086612701, loss=0.024143459275364876
I0205 14:02:27.159908 140290213332736 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.08580417186021805, loss=0.02448154054582119
I0205 14:02:58.940938 140266968643328 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.08493313193321228, loss=0.025241103023290634
I0205 14:03:31.161292 140290213332736 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.08513394743204117, loss=0.024187644943594933
I0205 14:04:02.963099 140266968643328 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.09720325469970703, loss=0.024409646168351173
I0205 14:04:09.228717 140451058161472 spec.py:321] Evaluating on the training split.
I0205 14:06:01.607315 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 14:06:04.754301 140451058161472 spec.py:349] Evaluating on the test split.
I0205 14:06:07.755395 140451058161472 submission_runner.py:408] Time since start: 24264.06s, 	Step: 50121, 	{'train/accuracy': 0.9957500696182251, 'train/loss': 0.01392839290201664, 'train/mean_average_precision': 0.7854547276249997, 'validation/accuracy': 0.9860916137695312, 'validation/loss': 0.057477932423353195, 'validation/mean_average_precision': 0.2286573693495823, 'validation/num_examples': 43793, 'test/accuracy': 0.9851734638214111, 'test/loss': 0.06093522161245346, 'test/mean_average_precision': 0.2285361877579136, 'test/num_examples': 43793, 'score': 16100.006301641464, 'total_duration': 24264.062856197357, 'accumulated_submission_time': 16100.006301641464, 'accumulated_eval_time': 8160.495388507843, 'accumulated_logging_time': 2.1722218990325928}
I0205 14:06:07.780220 140283262007040 logging_writer.py:48] [50121] accumulated_eval_time=8160.495389, accumulated_logging_time=2.172222, accumulated_submission_time=16100.006302, global_step=50121, preemption_count=0, score=16100.006302, test/accuracy=0.985173, test/loss=0.060935, test/mean_average_precision=0.228536, test/num_examples=43793, total_duration=24264.062856, train/accuracy=0.995750, train/loss=0.013928, train/mean_average_precision=0.785455, validation/accuracy=0.986092, validation/loss=0.057478, validation/mean_average_precision=0.228657, validation/num_examples=43793
I0205 14:06:33.439053 140283530442496 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.09780633449554443, loss=0.023637033998966217
I0205 14:07:05.656216 140283262007040 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.09120374172925949, loss=0.02333720028400421
I0205 14:07:38.040750 140283530442496 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.09166184812784195, loss=0.02282252535223961
I0205 14:08:10.177670 140283262007040 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.07959361374378204, loss=0.02533741667866707
I0205 14:08:42.147014 140283530442496 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.0942884162068367, loss=0.02478538453578949
I0205 14:09:14.169426 140283262007040 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.08621185272932053, loss=0.02484055981040001
I0205 14:09:45.912343 140283530442496 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.09310930967330933, loss=0.024264996871352196
I0205 14:10:08.041681 140451058161472 spec.py:321] Evaluating on the training split.
I0205 14:11:55.201556 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 14:11:58.200026 140451058161472 spec.py:349] Evaluating on the test split.
I0205 14:12:01.140811 140451058161472 submission_runner.py:408] Time since start: 24617.45s, 	Step: 50869, 	{'train/accuracy': 0.9962384104728699, 'train/loss': 0.012546772137284279, 'train/mean_average_precision': 0.8122539594282422, 'validation/accuracy': 0.9861472249031067, 'validation/loss': 0.058172766119241714, 'validation/mean_average_precision': 0.22581597638840017, 'validation/num_examples': 43793, 'test/accuracy': 0.9852408766746521, 'test/loss': 0.061664335429668427, 'test/mean_average_precision': 0.22632950553906886, 'test/num_examples': 43793, 'score': 16340.236045360565, 'total_duration': 24617.448274374008, 'accumulated_submission_time': 16340.236045360565, 'accumulated_eval_time': 8273.594465494156, 'accumulated_logging_time': 2.208230495452881}
I0205 14:12:01.165734 140266968643328 logging_writer.py:48] [50869] accumulated_eval_time=8273.594465, accumulated_logging_time=2.208230, accumulated_submission_time=16340.236045, global_step=50869, preemption_count=0, score=16340.236045, test/accuracy=0.985241, test/loss=0.061664, test/mean_average_precision=0.226330, test/num_examples=43793, total_duration=24617.448274, train/accuracy=0.996238, train/loss=0.012547, train/mean_average_precision=0.812254, validation/accuracy=0.986147, validation/loss=0.058173, validation/mean_average_precision=0.225816, validation/num_examples=43793
I0205 14:12:11.494861 140283731310336 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.07966487854719162, loss=0.023101523518562317
I0205 14:12:43.467606 140266968643328 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.08418195694684982, loss=0.02447747066617012
I0205 14:13:15.654961 140283731310336 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.07778789848089218, loss=0.023514943197369576
I0205 14:13:48.484734 140266968643328 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.0774494856595993, loss=0.022287677973508835
I0205 14:14:20.566346 140283731310336 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.07637906819581985, loss=0.024439090862870216
I0205 14:14:52.106313 140266968643328 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.08279912918806076, loss=0.023515909910202026
I0205 14:15:23.904249 140283731310336 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.1005156934261322, loss=0.024540934711694717
I0205 14:15:55.457185 140266968643328 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.09539559483528137, loss=0.024025335907936096
I0205 14:16:01.523996 140451058161472 spec.py:321] Evaluating on the training split.
I0205 14:17:48.582817 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 14:17:51.626260 140451058161472 spec.py:349] Evaluating on the test split.
I0205 14:17:54.594874 140451058161472 submission_runner.py:408] Time since start: 24970.90s, 	Step: 51620, 	{'train/accuracy': 0.9967470765113831, 'train/loss': 0.011496426537632942, 'train/mean_average_precision': 0.8367234392536422, 'validation/accuracy': 0.986141562461853, 'validation/loss': 0.05932892858982086, 'validation/mean_average_precision': 0.22748976931870318, 'validation/num_examples': 43793, 'test/accuracy': 0.9852067828178406, 'test/loss': 0.06297004967927933, 'test/mean_average_precision': 0.2218210113962008, 'test/num_examples': 43793, 'score': 16580.56310081482, 'total_duration': 24970.902344703674, 'accumulated_submission_time': 16580.56310081482, 'accumulated_eval_time': 8386.665300130844, 'accumulated_logging_time': 2.2444097995758057}
I0205 14:17:54.619694 140283262007040 logging_writer.py:48] [51620] accumulated_eval_time=8386.665300, accumulated_logging_time=2.244410, accumulated_submission_time=16580.563101, global_step=51620, preemption_count=0, score=16580.563101, test/accuracy=0.985207, test/loss=0.062970, test/mean_average_precision=0.221821, test/num_examples=43793, total_duration=24970.902345, train/accuracy=0.996747, train/loss=0.011496, train/mean_average_precision=0.836723, validation/accuracy=0.986142, validation/loss=0.059329, validation/mean_average_precision=0.227490, validation/num_examples=43793
I0205 14:18:20.505178 140283530442496 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.09981086105108261, loss=0.02344968542456627
I0205 14:18:52.110539 140283262007040 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.08156552910804749, loss=0.02310902811586857
I0205 14:19:23.790390 140283530442496 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.09649786353111267, loss=0.023985520005226135
I0205 14:19:55.151490 140283262007040 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.09558176249265671, loss=0.024285050109028816
I0205 14:20:26.485751 140283530442496 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.08942554146051407, loss=0.024020612239837646
I0205 14:20:58.003618 140283262007040 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.08423053473234177, loss=0.02266952022910118
I0205 14:21:29.675111 140283530442496 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.10466006398200989, loss=0.02301502600312233
I0205 14:21:54.861791 140451058161472 spec.py:321] Evaluating on the training split.
I0205 14:23:37.938499 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 14:23:40.944793 140451058161472 spec.py:349] Evaluating on the test split.
I0205 14:23:43.868354 140451058161472 submission_runner.py:408] Time since start: 25320.18s, 	Step: 52381, 	{'train/accuracy': 0.9970282316207886, 'train/loss': 0.010879489593207836, 'train/mean_average_precision': 0.8544680011223145, 'validation/accuracy': 0.9861334562301636, 'validation/loss': 0.059530146420001984, 'validation/mean_average_precision': 0.22775425386405748, 'validation/num_examples': 43793, 'test/accuracy': 0.9852383732795715, 'test/loss': 0.0631648600101471, 'test/mean_average_precision': 0.22353977631998603, 'test/num_examples': 43793, 'score': 16820.774089574814, 'total_duration': 25320.17582321167, 'accumulated_submission_time': 16820.774089574814, 'accumulated_eval_time': 8495.671817302704, 'accumulated_logging_time': 2.2801904678344727}
I0205 14:23:43.894053 140266968643328 logging_writer.py:48] [52381] accumulated_eval_time=8495.671817, accumulated_logging_time=2.280190, accumulated_submission_time=16820.774090, global_step=52381, preemption_count=0, score=16820.774090, test/accuracy=0.985238, test/loss=0.063165, test/mean_average_precision=0.223540, test/num_examples=43793, total_duration=25320.175823, train/accuracy=0.997028, train/loss=0.010879, train/mean_average_precision=0.854468, validation/accuracy=0.986133, validation/loss=0.059530, validation/mean_average_precision=0.227754, validation/num_examples=43793
I0205 14:23:50.591082 140290213332736 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.08624983578920364, loss=0.02279365248978138
I0205 14:24:23.193313 140266968643328 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.10051535815000534, loss=0.023855840787291527
I0205 14:24:55.238920 140290213332736 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.14096173644065857, loss=0.02361084520816803
I0205 14:25:27.189625 140266968643328 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.07673818618059158, loss=0.023502282798290253
I0205 14:25:59.172100 140290213332736 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.0813472643494606, loss=0.022572262212634087
I0205 14:26:31.405589 140266968643328 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.0995556190609932, loss=0.025859937071800232
I0205 14:27:03.632151 140290213332736 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.07799416035413742, loss=0.022436359897255898
I0205 14:27:35.779022 140266968643328 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.08335928618907928, loss=0.021908367052674294
I0205 14:27:43.912024 140451058161472 spec.py:321] Evaluating on the training split.
I0205 14:29:30.580085 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 14:29:33.909749 140451058161472 spec.py:349] Evaluating on the test split.
I0205 14:29:37.206103 140451058161472 submission_runner.py:408] Time since start: 25673.51s, 	Step: 53126, 	{'train/accuracy': 0.9972330331802368, 'train/loss': 0.010528430342674255, 'train/mean_average_precision': 0.8540056525446815, 'validation/accuracy': 0.9861581921577454, 'validation/loss': 0.059389643371105194, 'validation/mean_average_precision': 0.22787074654849107, 'validation/num_examples': 43793, 'test/accuracy': 0.9852033853530884, 'test/loss': 0.06320707499980927, 'test/mean_average_precision': 0.22194294565332634, 'test/num_examples': 43793, 'score': 17060.759781360626, 'total_duration': 25673.513555049896, 'accumulated_submission_time': 17060.759781360626, 'accumulated_eval_time': 8608.965829610825, 'accumulated_logging_time': 2.3182740211486816}
I0205 14:29:37.233482 140283262007040 logging_writer.py:48] [53126] accumulated_eval_time=8608.965830, accumulated_logging_time=2.318274, accumulated_submission_time=17060.759781, global_step=53126, preemption_count=0, score=17060.759781, test/accuracy=0.985203, test/loss=0.063207, test/mean_average_precision=0.221943, test/num_examples=43793, total_duration=25673.513555, train/accuracy=0.997233, train/loss=0.010528, train/mean_average_precision=0.854006, validation/accuracy=0.986158, validation/loss=0.059390, validation/mean_average_precision=0.227871, validation/num_examples=43793
I0205 14:30:01.440796 140283731310336 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.0854567363858223, loss=0.022557945922017097
I0205 14:30:33.384871 140283262007040 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.12083102017641068, loss=0.024232078343629837
I0205 14:31:05.703498 140283731310336 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.1017291471362114, loss=0.02428947202861309
I0205 14:31:37.896207 140283262007040 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.0864410325884819, loss=0.023839343339204788
I0205 14:32:09.916794 140283731310336 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.08560362458229065, loss=0.02423989400267601
I0205 14:32:41.937229 140283262007040 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.08616508543491364, loss=0.02242174744606018
I0205 14:33:13.977464 140283731310336 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.08303786814212799, loss=0.023537050932645798
I0205 14:33:37.400715 140451058161472 spec.py:321] Evaluating on the training split.
I0205 14:35:25.610746 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 14:35:28.696525 140451058161472 spec.py:349] Evaluating on the test split.
I0205 14:35:31.650238 140451058161472 submission_runner.py:408] Time since start: 26027.96s, 	Step: 53873, 	{'train/accuracy': 0.9966502785682678, 'train/loss': 0.011681037954986095, 'train/mean_average_precision': 0.835379206205047, 'validation/accuracy': 0.9860693216323853, 'validation/loss': 0.06027856469154358, 'validation/mean_average_precision': 0.22494664746660062, 'validation/num_examples': 43793, 'test/accuracy': 0.9851911664009094, 'test/loss': 0.06380055844783783, 'test/mean_average_precision': 0.21495218237258368, 'test/num_examples': 43793, 'score': 17300.893683433533, 'total_duration': 26027.957573890686, 'accumulated_submission_time': 17300.893683433533, 'accumulated_eval_time': 8723.215174674988, 'accumulated_logging_time': 2.3571720123291016}
I0205 14:35:31.681307 140266968643328 logging_writer.py:48] [53873] accumulated_eval_time=8723.215175, accumulated_logging_time=2.357172, accumulated_submission_time=17300.893683, global_step=53873, preemption_count=0, score=17300.893683, test/accuracy=0.985191, test/loss=0.063801, test/mean_average_precision=0.214952, test/num_examples=43793, total_duration=26027.957574, train/accuracy=0.996650, train/loss=0.011681, train/mean_average_precision=0.835379, validation/accuracy=0.986069, validation/loss=0.060279, validation/mean_average_precision=0.224947, validation/num_examples=43793
I0205 14:35:40.775315 140283530442496 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.08420102298259735, loss=0.02384878322482109
I0205 14:36:13.158184 140266968643328 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.1021202951669693, loss=0.024169771000742912
I0205 14:36:46.011565 140283530442496 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.08715388923883438, loss=0.023523658514022827
I0205 14:37:18.264343 140266968643328 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.09495542198419571, loss=0.024251583963632584
I0205 14:37:50.061197 140283530442496 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.09147169440984726, loss=0.021747594699263573
I0205 14:38:22.186583 140266968643328 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.09503354877233505, loss=0.02323690801858902
I0205 14:38:53.635330 140283530442496 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.08100127428770065, loss=0.02344641275703907
I0205 14:39:25.531298 140266968643328 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.09852863103151321, loss=0.023257993161678314
I0205 14:39:31.665835 140451058161472 spec.py:321] Evaluating on the training split.
I0205 14:41:19.114752 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 14:41:22.141335 140451058161472 spec.py:349] Evaluating on the test split.
I0205 14:41:25.106354 140451058161472 submission_runner.py:408] Time since start: 26381.41s, 	Step: 54620, 	{'train/accuracy': 0.9948896765708923, 'train/loss': 0.015510822646319866, 'train/mean_average_precision': 0.7515258127315602, 'validation/accuracy': 0.9860928654670715, 'validation/loss': 0.060128357261419296, 'validation/mean_average_precision': 0.22509716084751005, 'validation/num_examples': 43793, 'test/accuracy': 0.9852109551429749, 'test/loss': 0.06393587589263916, 'test/mean_average_precision': 0.2158959223624439, 'test/num_examples': 43793, 'score': 17540.84679889679, 'total_duration': 26381.413821458817, 'accumulated_submission_time': 17540.84679889679, 'accumulated_eval_time': 8836.655643463135, 'accumulated_logging_time': 2.3990213871002197}
I0205 14:41:25.133351 140283262007040 logging_writer.py:48] [54620] accumulated_eval_time=8836.655643, accumulated_logging_time=2.399021, accumulated_submission_time=17540.846799, global_step=54620, preemption_count=0, score=17540.846799, test/accuracy=0.985211, test/loss=0.063936, test/mean_average_precision=0.215896, test/num_examples=43793, total_duration=26381.413821, train/accuracy=0.994890, train/loss=0.015511, train/mean_average_precision=0.751526, validation/accuracy=0.986093, validation/loss=0.060128, validation/mean_average_precision=0.225097, validation/num_examples=43793
I0205 14:41:51.212873 140283731310336 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.09782593697309494, loss=0.023754959926009178
I0205 14:42:23.558155 140283262007040 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.09374600648880005, loss=0.022324806079268456
I0205 14:42:55.462571 140283731310336 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.09435087442398071, loss=0.022061511874198914
I0205 14:43:27.704316 140283262007040 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.09312722086906433, loss=0.023152893409132957
I0205 14:43:59.918432 140283731310336 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.08306822925806046, loss=0.02226712927222252
I0205 14:44:31.954870 140283262007040 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.08290863782167435, loss=0.02232297696173191
I0205 14:45:04.054496 140283731310336 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.08781382441520691, loss=0.02290944755077362
I0205 14:45:25.154692 140451058161472 spec.py:321] Evaluating on the training split.
I0205 14:47:08.088933 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 14:47:11.084700 140451058161472 spec.py:349] Evaluating on the test split.
I0205 14:47:14.053967 140451058161472 submission_runner.py:408] Time since start: 26730.36s, 	Step: 55367, 	{'train/accuracy': 0.9954902529716492, 'train/loss': 0.014068689197301865, 'train/mean_average_precision': 0.7815939752419798, 'validation/accuracy': 0.9859552383422852, 'validation/loss': 0.05988990142941475, 'validation/mean_average_precision': 0.2264501280562338, 'validation/num_examples': 43793, 'test/accuracy': 0.9850273132324219, 'test/loss': 0.0636545941233635, 'test/mean_average_precision': 0.21549331518327594, 'test/num_examples': 43793, 'score': 17780.836613178253, 'total_duration': 26730.361434221268, 'accumulated_submission_time': 17780.836613178253, 'accumulated_eval_time': 8945.55486869812, 'accumulated_logging_time': 2.4369421005249023}
I0205 14:47:14.080828 140266968643328 logging_writer.py:48] [55367] accumulated_eval_time=8945.554869, accumulated_logging_time=2.436942, accumulated_submission_time=17780.836613, global_step=55367, preemption_count=0, score=17780.836613, test/accuracy=0.985027, test/loss=0.063655, test/mean_average_precision=0.215493, test/num_examples=43793, total_duration=26730.361434, train/accuracy=0.995490, train/loss=0.014069, train/mean_average_precision=0.781594, validation/accuracy=0.985955, validation/loss=0.059890, validation/mean_average_precision=0.226450, validation/num_examples=43793
I0205 14:47:24.790771 140290213332736 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.08207026869058609, loss=0.02169467695057392
I0205 14:47:56.313977 140266968643328 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.08713837713003159, loss=0.024121489375829697
I0205 14:48:28.094443 140290213332736 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.0835040807723999, loss=0.022846342995762825
I0205 14:48:59.573944 140266968643328 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.08152761310338974, loss=0.022125527262687683
I0205 14:49:31.490299 140290213332736 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.09505552798509598, loss=0.023573104292154312
I0205 14:50:03.367315 140266968643328 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.08921535313129425, loss=0.021943148225545883
I0205 14:50:34.748306 140290213332736 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.09410922974348068, loss=0.02430192194879055
I0205 14:51:06.280094 140266968643328 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.07957370579242706, loss=0.022032931447029114
I0205 14:51:14.291018 140451058161472 spec.py:321] Evaluating on the training split.
I0205 14:53:02.025908 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 14:53:05.129138 140451058161472 spec.py:349] Evaluating on the test split.
I0205 14:53:08.199822 140451058161472 submission_runner.py:408] Time since start: 27084.51s, 	Step: 56126, 	{'train/accuracy': 0.9959081411361694, 'train/loss': 0.012733520939946175, 'train/mean_average_precision': 0.8261949531777921, 'validation/accuracy': 0.986108660697937, 'validation/loss': 0.06120892986655235, 'validation/mean_average_precision': 0.22277356853215763, 'validation/num_examples': 43793, 'test/accuracy': 0.9852176904678345, 'test/loss': 0.06481973081827164, 'test/mean_average_precision': 0.21762757762418214, 'test/num_examples': 43793, 'score': 18021.015577316284, 'total_duration': 27084.507292032242, 'accumulated_submission_time': 18021.015577316284, 'accumulated_eval_time': 9059.463624238968, 'accumulated_logging_time': 2.475022077560425}
I0205 14:53:08.225720 140283530442496 logging_writer.py:48] [56126] accumulated_eval_time=9059.463624, accumulated_logging_time=2.475022, accumulated_submission_time=18021.015577, global_step=56126, preemption_count=0, score=18021.015577, test/accuracy=0.985218, test/loss=0.064820, test/mean_average_precision=0.217628, test/num_examples=43793, total_duration=27084.507292, train/accuracy=0.995908, train/loss=0.012734, train/mean_average_precision=0.826195, validation/accuracy=0.986109, validation/loss=0.061209, validation/mean_average_precision=0.222774, validation/num_examples=43793
I0205 14:53:31.829957 140283731310336 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.08959690481424332, loss=0.02320386841893196
I0205 14:54:03.914761 140283530442496 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.09437663108110428, loss=0.02209361642599106
I0205 14:54:36.690844 140283731310336 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.07769370079040527, loss=0.021611018106341362
I0205 14:55:09.326485 140283530442496 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.10447027534246445, loss=0.023929204791784286
I0205 14:55:41.912078 140283731310336 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.0898623913526535, loss=0.021840784698724747
I0205 14:56:14.769284 140283530442496 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.08366496860980988, loss=0.022377047687768936
I0205 14:56:47.191749 140283731310336 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.06712424010038376, loss=0.020538711920380592
I0205 14:57:08.463608 140451058161472 spec.py:321] Evaluating on the training split.
I0205 14:58:54.633348 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 14:58:57.742576 140451058161472 spec.py:349] Evaluating on the test split.
I0205 14:59:00.793140 140451058161472 submission_runner.py:408] Time since start: 27437.10s, 	Step: 56866, 	{'train/accuracy': 0.9954772591590881, 'train/loss': 0.013762353919446468, 'train/mean_average_precision': 0.8133653761740869, 'validation/accuracy': 0.9861249327659607, 'validation/loss': 0.06183765456080437, 'validation/mean_average_precision': 0.2242386545209901, 'validation/num_examples': 43793, 'test/accuracy': 0.985187828540802, 'test/loss': 0.06561759114265442, 'test/mean_average_precision': 0.2167473575816602, 'test/num_examples': 43793, 'score': 18261.217477321625, 'total_duration': 27437.100608110428, 'accumulated_submission_time': 18261.217477321625, 'accumulated_eval_time': 9171.793118715286, 'accumulated_logging_time': 2.5136139392852783}
I0205 14:59:00.820978 140266968643328 logging_writer.py:48] [56866] accumulated_eval_time=9171.793119, accumulated_logging_time=2.513614, accumulated_submission_time=18261.217477, global_step=56866, preemption_count=0, score=18261.217477, test/accuracy=0.985188, test/loss=0.065618, test/mean_average_precision=0.216747, test/num_examples=43793, total_duration=27437.100608, train/accuracy=0.995477, train/loss=0.013762, train/mean_average_precision=0.813365, validation/accuracy=0.986125, validation/loss=0.061838, validation/mean_average_precision=0.224239, validation/num_examples=43793
I0205 14:59:11.903808 140290213332736 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.08254216611385345, loss=0.022126389667391777
I0205 14:59:43.306747 140266968643328 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.08734201639890671, loss=0.021934110671281815
I0205 15:00:15.062947 140290213332736 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.09662248194217682, loss=0.02264227718114853
I0205 15:00:46.707483 140266968643328 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.0782720223069191, loss=0.02158384397625923
I0205 15:01:19.028549 140290213332736 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.08814766258001328, loss=0.02167343534529209
I0205 15:01:50.976559 140266968643328 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.1047433391213417, loss=0.02097715251147747
I0205 15:02:22.745882 140290213332736 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.08637113124132156, loss=0.02251623570919037
I0205 15:02:36.961171 140266968643328 logging_writer.py:48] [57545] global_step=57545, preemption_count=0, score=18477.311545
I0205 15:02:37.034359 140451058161472 checkpoints.py:490] Saving checkpoint at step: 57545
I0205 15:02:37.173279 140451058161472 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_2/ogbg_jax/trial_2/checkpoint_57545
I0205 15:02:37.174186 140451058161472 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_2/ogbg_jax/trial_2/checkpoint_57545.
I0205 15:02:37.337931 140451058161472 submission_runner.py:583] Tuning trial 2/5
I0205 15:02:37.338177 140451058161472 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.2, learning_rate=0.0008445074561975979, one_minus_beta1=0.11042418465, beta2=0.9978504782314613, weight_decay=0.08135402759553023, warmup_factor=0.05)
I0205 15:02:37.343178 140451058161472 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5289902091026306, 'train/loss': 0.7364099025726318, 'train/mean_average_precision': 0.020686487925188634, 'validation/accuracy': 0.5270814895629883, 'validation/loss': 0.737440824508667, 'validation/mean_average_precision': 0.024062548442236216, 'validation/num_examples': 43793, 'test/accuracy': 0.5256850719451904, 'test/loss': 0.7376683354377747, 'test/mean_average_precision': 0.026021847524319627, 'test/num_examples': 43793, 'score': 12.126285314559937, 'total_duration': 139.33090662956238, 'accumulated_submission_time': 12.126285314559937, 'accumulated_eval_time': 127.20457720756531, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (750, {'train/accuracy': 0.986754298210144, 'train/loss': 0.06418555229902267, 'train/mean_average_precision': 0.04357131042408905, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.07305673509836197, 'validation/mean_average_precision': 0.04533270888916812, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.07600926607847214, 'test/mean_average_precision': 0.04617481438971092, 'test/num_examples': 43793, 'score': 252.2958436012268, 'total_duration': 505.4296803474426, 'accumulated_submission_time': 252.2958436012268, 'accumulated_eval_time': 253.09186100959778, 'accumulated_logging_time': 0.021273136138916016, 'global_step': 750, 'preemption_count': 0}), (1491, {'train/accuracy': 0.9871442317962646, 'train/loss': 0.048594001680612564, 'train/mean_average_precision': 0.0847953123676511, 'validation/accuracy': 0.9844759702682495, 'validation/loss': 0.05792314186692238, 'validation/mean_average_precision': 0.08769234356015555, 'validation/num_examples': 43793, 'test/accuracy': 0.9834765195846558, 'test/loss': 0.0612092986702919, 'test/mean_average_precision': 0.08469940992907997, 'test/num_examples': 43793, 'score': 492.5110273361206, 'total_duration': 874.5190241336823, 'accumulated_submission_time': 492.5110273361206, 'accumulated_eval_time': 381.91805124282837, 'accumulated_logging_time': 0.04872751235961914, 'global_step': 1491, 'preemption_count': 0}), (2236, {'train/accuracy': 0.9874035120010376, 'train/loss': 0.04586554691195488, 'train/mean_average_precision': 0.11386966374838156, 'validation/accuracy': 0.9847410321235657, 'validation/loss': 0.05491954833269119, 'validation/mean_average_precision': 0.11226110042255129, 'validation/num_examples': 43793, 'test/accuracy': 0.9837313294410706, 'test/loss': 0.058089207857847214, 'test/mean_average_precision': 0.1110899480813678, 'test/num_examples': 43793, 'score': 732.7416000366211, 'total_duration': 1242.9329607486725, 'accumulated_submission_time': 732.7416000366211, 'accumulated_eval_time': 510.0526442527771, 'accumulated_logging_time': 0.07758808135986328, 'global_step': 2236, 'preemption_count': 0}), (2985, {'train/accuracy': 0.9875199794769287, 'train/loss': 0.044625088572502136, 'train/mean_average_precision': 0.14252135502158822, 'validation/accuracy': 0.9848595857620239, 'validation/loss': 0.05364485830068588, 'validation/mean_average_precision': 0.13451959985440934, 'validation/num_examples': 43793, 'test/accuracy': 0.9838774800300598, 'test/loss': 0.05656570568680763, 'test/mean_average_precision': 0.13298944726020429, 'test/num_examples': 43793, 'score': 972.7662193775177, 'total_duration': 1610.2927725315094, 'accumulated_submission_time': 972.7662193775177, 'accumulated_eval_time': 637.340523481369, 'accumulated_logging_time': 0.10427021980285645, 'global_step': 2985, 'preemption_count': 0}), (3730, {'train/accuracy': 0.9874981641769409, 'train/loss': 0.04415629431605339, 'train/mean_average_precision': 0.16691992686249002, 'validation/accuracy': 0.9848372340202332, 'validation/loss': 0.053268492221832275, 'validation/mean_average_precision': 0.1536197460118933, 'validation/num_examples': 43793, 'test/accuracy': 0.983906090259552, 'test/loss': 0.05640191584825516, 'test/mean_average_precision': 0.1527795488106242, 'test/num_examples': 43793, 'score': 1212.8425545692444, 'total_duration': 1973.5845963954926, 'accumulated_submission_time': 1212.8425545692444, 'accumulated_eval_time': 760.508437871933, 'accumulated_logging_time': 0.13177275657653809, 'global_step': 3730, 'preemption_count': 0}), (4480, {'train/accuracy': 0.9881991744041443, 'train/loss': 0.04134407266974449, 'train/mean_average_precision': 0.19705664648587554, 'validation/accuracy': 0.9853300452232361, 'validation/loss': 0.050343018025159836, 'validation/mean_average_precision': 0.17024784114180394, 'validation/num_examples': 43793, 'test/accuracy': 0.9844414591789246, 'test/loss': 0.05320088937878609, 'test/mean_average_precision': 0.16817030768870864, 'test/num_examples': 43793, 'score': 1453.0647163391113, 'total_duration': 2335.695539712906, 'accumulated_submission_time': 1453.0647163391113, 'accumulated_eval_time': 882.3497655391693, 'accumulated_logging_time': 0.15921521186828613, 'global_step': 4480, 'preemption_count': 0}), (5232, {'train/accuracy': 0.9880143404006958, 'train/loss': 0.04127421975135803, 'train/mean_average_precision': 0.20600171914672058, 'validation/accuracy': 0.9851628541946411, 'validation/loss': 0.05104755610227585, 'validation/mean_average_precision': 0.18876825638804642, 'validation/num_examples': 43793, 'test/accuracy': 0.9842683672904968, 'test/loss': 0.054004017263650894, 'test/mean_average_precision': 0.18970394779650487, 'test/num_examples': 43793, 'score': 1693.1650228500366, 'total_duration': 2700.2280929088593, 'accumulated_submission_time': 1693.1650228500366, 'accumulated_eval_time': 1006.7336690425873, 'accumulated_logging_time': 0.18717479705810547, 'global_step': 5232, 'preemption_count': 0}), (5975, {'train/accuracy': 0.9888017773628235, 'train/loss': 0.038616690784692764, 'train/mean_average_precision': 0.23812575126523344, 'validation/accuracy': 0.9857409000396729, 'validation/loss': 0.04839784651994705, 'validation/mean_average_precision': 0.20119131121826642, 'validation/num_examples': 43793, 'test/accuracy': 0.9848635196685791, 'test/loss': 0.051224883645772934, 'test/mean_average_precision': 0.20399773425762774, 'test/num_examples': 43793, 'score': 1933.4031188488007, 'total_duration': 3068.8905482292175, 'accumulated_submission_time': 1933.4031188488007, 'accumulated_eval_time': 1135.106214761734, 'accumulated_logging_time': 0.21738028526306152, 'global_step': 5975, 'preemption_count': 0}), (6721, {'train/accuracy': 0.9884495735168457, 'train/loss': 0.03919443488121033, 'train/mean_average_precision': 0.24130529244074025, 'validation/accuracy': 0.9856499433517456, 'validation/loss': 0.04877936840057373, 'validation/mean_average_precision': 0.20440969556304164, 'validation/num_examples': 43793, 'test/accuracy': 0.9846937656402588, 'test/loss': 0.05163053795695305, 'test/mean_average_precision': 0.2059085480747178, 'test/num_examples': 43793, 'score': 2173.590786933899, 'total_duration': 3431.493139743805, 'accumulated_submission_time': 2173.590786933899, 'accumulated_eval_time': 1257.4715340137482, 'accumulated_logging_time': 0.24632859230041504, 'global_step': 6721, 'preemption_count': 0}), (7477, {'train/accuracy': 0.9889029264450073, 'train/loss': 0.03751550242304802, 'train/mean_average_precision': 0.27572406125737625, 'validation/accuracy': 0.9859803915023804, 'validation/loss': 0.04746478796005249, 'validation/mean_average_precision': 0.2206059988797787, 'validation/num_examples': 43793, 'test/accuracy': 0.9850782752037048, 'test/loss': 0.050132304430007935, 'test/mean_average_precision': 0.21980030579954665, 'test/num_examples': 43793, 'score': 2413.612592458725, 'total_duration': 3793.469251871109, 'accumulated_submission_time': 2413.612592458725, 'accumulated_eval_time': 1379.377456188202, 'accumulated_logging_time': 0.27362728118896484, 'global_step': 7477, 'preemption_count': 0}), (8225, {'train/accuracy': 0.9894757270812988, 'train/loss': 0.03590020909905434, 'train/mean_average_precision': 0.3072628440876122, 'validation/accuracy': 0.9861716032028198, 'validation/loss': 0.04675177484750748, 'validation/mean_average_precision': 0.2247504688285445, 'validation/num_examples': 43793, 'test/accuracy': 0.9852569103240967, 'test/loss': 0.04944462701678276, 'test/mean_average_precision': 0.22522695849827612, 'test/num_examples': 43793, 'score': 2653.793397426605, 'total_duration': 4155.104791641235, 'accumulated_submission_time': 2653.793397426605, 'accumulated_eval_time': 1500.782978773117, 'accumulated_logging_time': 0.30237483978271484, 'global_step': 8225, 'preemption_count': 0}), (8984, {'train/accuracy': 0.9894742965698242, 'train/loss': 0.035279128700494766, 'train/mean_average_precision': 0.3132704878213992, 'validation/accuracy': 0.986291766166687, 'validation/loss': 0.04616290330886841, 'validation/mean_average_precision': 0.23599086679248701, 'validation/num_examples': 43793, 'test/accuracy': 0.9854257702827454, 'test/loss': 0.048779260367155075, 'test/mean_average_precision': 0.2395137327731072, 'test/num_examples': 43793, 'score': 2893.755373477936, 'total_duration': 4520.299135923386, 'accumulated_submission_time': 2893.755373477936, 'accumulated_eval_time': 1625.9661529064178, 'accumulated_logging_time': 0.33057332038879395, 'global_step': 8984, 'preemption_count': 0}), (9733, {'train/accuracy': 0.9896693229675293, 'train/loss': 0.03427824005484581, 'train/mean_average_precision': 0.3510216167629603, 'validation/accuracy': 0.9862142205238342, 'validation/loss': 0.04642321169376373, 'validation/mean_average_precision': 0.2376936664874003, 'validation/num_examples': 43793, 'test/accuracy': 0.9853495359420776, 'test/loss': 0.04912646859884262, 'test/mean_average_precision': 0.2371165836262182, 'test/num_examples': 43793, 'score': 3133.7946379184723, 'total_duration': 4883.982634544373, 'accumulated_submission_time': 3133.7946379184723, 'accumulated_eval_time': 1749.5614104270935, 'accumulated_logging_time': 0.358844518661499, 'global_step': 9733, 'preemption_count': 0}), (10481, {'train/accuracy': 0.9896564483642578, 'train/loss': 0.033921267837285995, 'train/mean_average_precision': 0.3735877274136109, 'validation/accuracy': 0.9863014817237854, 'validation/loss': 0.04627609625458717, 'validation/mean_average_precision': 0.2454329941619501, 'validation/num_examples': 43793, 'test/accuracy': 0.9854468703269958, 'test/loss': 0.049061618745326996, 'test/mean_average_precision': 0.24721478526615945, 'test/num_examples': 43793, 'score': 3374.0041666030884, 'total_duration': 5246.561218261719, 'accumulated_submission_time': 3374.0041666030884, 'accumulated_eval_time': 1871.8764510154724, 'accumulated_logging_time': 0.39055585861206055, 'global_step': 10481, 'preemption_count': 0}), (11236, {'train/accuracy': 0.9896635413169861, 'train/loss': 0.03401583433151245, 'train/mean_average_precision': 0.3581437473261031, 'validation/accuracy': 0.9863518476486206, 'validation/loss': 0.04625559598207474, 'validation/mean_average_precision': 0.2520758598756026, 'validation/num_examples': 43793, 'test/accuracy': 0.9854127168655396, 'test/loss': 0.049329109489917755, 'test/mean_average_precision': 0.24701030213277722, 'test/num_examples': 43793, 'score': 3614.2272896766663, 'total_duration': 5608.641200304031, 'accumulated_submission_time': 3614.2272896766663, 'accumulated_eval_time': 1993.6837046146393, 'accumulated_logging_time': 0.41956257820129395, 'global_step': 11236, 'preemption_count': 0}), (11992, {'train/accuracy': 0.9899739623069763, 'train/loss': 0.03308432921767235, 'train/mean_average_precision': 0.37030113663788566, 'validation/accuracy': 0.9864541292190552, 'validation/loss': 0.04605422168970108, 'validation/mean_average_precision': 0.25101093124939455, 'validation/num_examples': 43793, 'test/accuracy': 0.9854961037635803, 'test/loss': 0.04903990775346756, 'test/mean_average_precision': 0.2538872922350826, 'test/num_examples': 43793, 'score': 3854.4807589054108, 'total_duration': 5969.253441572189, 'accumulated_submission_time': 3854.4807589054108, 'accumulated_eval_time': 2113.992784023285, 'accumulated_logging_time': 0.4489624500274658, 'global_step': 11992, 'preemption_count': 0}), (12742, {'train/accuracy': 0.9902027249336243, 'train/loss': 0.03249253332614899, 'train/mean_average_precision': 0.38287722729232115, 'validation/accuracy': 0.9865673780441284, 'validation/loss': 0.04554583132266998, 'validation/mean_average_precision': 0.2547032048499188, 'validation/num_examples': 43793, 'test/accuracy': 0.9856456518173218, 'test/loss': 0.048312168568372726, 'test/mean_average_precision': 0.2498462761455225, 'test/num_examples': 43793, 'score': 4094.6281826496124, 'total_duration': 6327.803616523743, 'accumulated_submission_time': 4094.6281826496124, 'accumulated_eval_time': 2232.3454310894012, 'accumulated_logging_time': 0.4781973361968994, 'global_step': 12742, 'preemption_count': 0}), (13491, {'train/accuracy': 0.9905490279197693, 'train/loss': 0.03170744329690933, 'train/mean_average_precision': 0.39984651207485566, 'validation/accuracy': 0.9866088032722473, 'validation/loss': 0.045085567981004715, 'validation/mean_average_precision': 0.25580061287290407, 'validation/num_examples': 43793, 'test/accuracy': 0.9857509732246399, 'test/loss': 0.04791153594851494, 'test/mean_average_precision': 0.2501149207185102, 'test/num_examples': 43793, 'score': 4334.628254652023, 'total_duration': 6687.5966901779175, 'accumulated_submission_time': 4334.628254652023, 'accumulated_eval_time': 2352.0891876220703, 'accumulated_logging_time': 0.5074634552001953, 'global_step': 13491, 'preemption_count': 0}), (14243, {'train/accuracy': 0.9906753897666931, 'train/loss': 0.03099401853978634, 'train/mean_average_precision': 0.42887643696747557, 'validation/accuracy': 0.9867565631866455, 'validation/loss': 0.04504191875457764, 'validation/mean_average_precision': 0.2566420787519, 'validation/num_examples': 43793, 'test/accuracy': 0.9858794212341309, 'test/loss': 0.04778122156858444, 'test/mean_average_precision': 0.2592707509360255, 'test/num_examples': 43793, 'score': 4574.7080709934235, 'total_duration': 7048.78892493248, 'accumulated_submission_time': 4574.7080709934235, 'accumulated_eval_time': 2473.1447324752808, 'accumulated_logging_time': 0.5425519943237305, 'global_step': 14243, 'preemption_count': 0}), (14988, {'train/accuracy': 0.9906166195869446, 'train/loss': 0.0303849708288908, 'train/mean_average_precision': 0.42623232653305093, 'validation/accuracy': 0.9866489768028259, 'validation/loss': 0.045590728521347046, 'validation/mean_average_precision': 0.25932788192480427, 'validation/num_examples': 43793, 'test/accuracy': 0.9857316017150879, 'test/loss': 0.048439156264066696, 'test/mean_average_precision': 0.2535050643187877, 'test/num_examples': 43793, 'score': 4814.695953845978, 'total_duration': 7410.355037212372, 'accumulated_submission_time': 4814.695953845978, 'accumulated_eval_time': 2594.674001932144, 'accumulated_logging_time': 0.5714969635009766, 'global_step': 14988, 'preemption_count': 0}), (15725, {'train/accuracy': 0.9909983277320862, 'train/loss': 0.02954896353185177, 'train/mean_average_precision': 0.454112727750228, 'validation/accuracy': 0.9866875410079956, 'validation/loss': 0.045338816940784454, 'validation/mean_average_precision': 0.25527725304688215, 'validation/num_examples': 43793, 'test/accuracy': 0.985811173915863, 'test/loss': 0.048067834228277206, 'test/mean_average_precision': 0.25743244548876176, 'test/num_examples': 43793, 'score': 5054.773876190186, 'total_duration': 7771.213989019394, 'accumulated_submission_time': 5054.773876190186, 'accumulated_eval_time': 2715.3988678455353, 'accumulated_logging_time': 0.6034946441650391, 'global_step': 15725, 'preemption_count': 0}), (16472, {'train/accuracy': 0.991483211517334, 'train/loss': 0.027779418975114822, 'train/mean_average_precision': 0.48929640670339225, 'validation/accuracy': 0.9867638349533081, 'validation/loss': 0.04555807635188103, 'validation/mean_average_precision': 0.2630052954278285, 'validation/num_examples': 43793, 'test/accuracy': 0.985897958278656, 'test/loss': 0.04825650900602341, 'test/mean_average_precision': 0.256947923922455, 'test/num_examples': 43793, 'score': 5294.818076848984, 'total_duration': 8131.040618658066, 'accumulated_submission_time': 5294.818076848984, 'accumulated_eval_time': 2835.131489276886, 'accumulated_logging_time': 0.632915735244751, 'global_step': 16472, 'preemption_count': 0}), (17216, {'train/accuracy': 0.9915978908538818, 'train/loss': 0.0274420864880085, 'train/mean_average_precision': 0.5071297348406607, 'validation/accuracy': 0.9867764711380005, 'validation/loss': 0.04545893147587776, 'validation/mean_average_precision': 0.25982477374850405, 'validation/num_examples': 43793, 'test/accuracy': 0.9859485030174255, 'test/loss': 0.048284754157066345, 'test/mean_average_precision': 0.25943890002891323, 'test/num_examples': 43793, 'score': 5535.046224355698, 'total_duration': 8489.352545261383, 'accumulated_submission_time': 5535.046224355698, 'accumulated_eval_time': 2953.16513299942, 'accumulated_logging_time': 0.6627569198608398, 'global_step': 17216, 'preemption_count': 0}), (17970, {'train/accuracy': 0.991574227809906, 'train/loss': 0.027520490810275078, 'train/mean_average_precision': 0.5063725356236961, 'validation/accuracy': 0.9866810441017151, 'validation/loss': 0.045769862830638885, 'validation/mean_average_precision': 0.25497578991680975, 'validation/num_examples': 43793, 'test/accuracy': 0.985820472240448, 'test/loss': 0.04843926429748535, 'test/mean_average_precision': 0.25534367132578084, 'test/num_examples': 43793, 'score': 5775.2991716861725, 'total_duration': 8846.437492847443, 'accumulated_submission_time': 5775.2991716861725, 'accumulated_eval_time': 3069.9476771354675, 'accumulated_logging_time': 0.691856861114502, 'global_step': 17970, 'preemption_count': 0}), (18712, {'train/accuracy': 0.9916909337043762, 'train/loss': 0.027664432302117348, 'train/mean_average_precision': 0.4879586830864612, 'validation/accuracy': 0.9867812991142273, 'validation/loss': 0.04561914876103401, 'validation/mean_average_precision': 0.2659859739068194, 'validation/num_examples': 43793, 'test/accuracy': 0.9859510064125061, 'test/loss': 0.048301104456186295, 'test/mean_average_precision': 0.2602272453178065, 'test/num_examples': 43793, 'score': 6015.247898340225, 'total_duration': 9210.452338218689, 'accumulated_submission_time': 6015.247898340225, 'accumulated_eval_time': 3193.9604799747467, 'accumulated_logging_time': 0.723224401473999, 'global_step': 18712, 'preemption_count': 0}), (19462, {'train/accuracy': 0.9915679693222046, 'train/loss': 0.027529949322342873, 'train/mean_average_precision': 0.49315032014147686, 'validation/accuracy': 0.9867374897003174, 'validation/loss': 0.04580539092421532, 'validation/mean_average_precision': 0.26231222515437447, 'validation/num_examples': 43793, 'test/accuracy': 0.9858710169792175, 'test/loss': 0.04866298288106918, 'test/mean_average_precision': 0.2597261426049351, 'test/num_examples': 43793, 'score': 6255.323559045792, 'total_duration': 9573.666657924652, 'accumulated_submission_time': 6255.323559045792, 'accumulated_eval_time': 3317.048745393753, 'accumulated_logging_time': 0.7530508041381836, 'global_step': 19462, 'preemption_count': 0}), (20217, {'train/accuracy': 0.9917171597480774, 'train/loss': 0.027245081961154938, 'train/mean_average_precision': 0.5011942091495836, 'validation/accuracy': 0.9866205453872681, 'validation/loss': 0.04634416103363037, 'validation/mean_average_precision': 0.2602996367965026, 'validation/num_examples': 43793, 'test/accuracy': 0.9857703447341919, 'test/loss': 0.04912406578660011, 'test/mean_average_precision': 0.25388159446065756, 'test/num_examples': 43793, 'score': 6495.576878070831, 'total_duration': 9932.125399589539, 'accumulated_submission_time': 6495.576878070831, 'accumulated_eval_time': 3435.202912569046, 'accumulated_logging_time': 0.7837088108062744, 'global_step': 20217, 'preemption_count': 0}), (20964, {'train/accuracy': 0.9916337728500366, 'train/loss': 0.02696966752409935, 'train/mean_average_precision': 0.5147379353860131, 'validation/accuracy': 0.9867159724235535, 'validation/loss': 0.046382952481508255, 'validation/mean_average_precision': 0.2640476810927308, 'validation/num_examples': 43793, 'test/accuracy': 0.985842764377594, 'test/loss': 0.049242448061704636, 'test/mean_average_precision': 0.25052299615250756, 'test/num_examples': 43793, 'score': 6735.836624145508, 'total_duration': 10290.507292747498, 'accumulated_submission_time': 6735.836624145508, 'accumulated_eval_time': 3553.2706336975098, 'accumulated_logging_time': 0.817469596862793, 'global_step': 20964, 'preemption_count': 0}), (21709, {'train/accuracy': 0.992107629776001, 'train/loss': 0.025918900966644287, 'train/mean_average_precision': 0.5224654679997216, 'validation/accuracy': 0.9866802096366882, 'validation/loss': 0.04617874696850777, 'validation/mean_average_precision': 0.2573069386431443, 'validation/num_examples': 43793, 'test/accuracy': 0.985846996307373, 'test/loss': 0.04880925267934799, 'test/mean_average_precision': 0.2529326603844326, 'test/num_examples': 43793, 'score': 6975.8414607048035, 'total_duration': 10657.739182472229, 'accumulated_submission_time': 6975.8414607048035, 'accumulated_eval_time': 3680.4457519054413, 'accumulated_logging_time': 0.8478615283966064, 'global_step': 21709, 'preemption_count': 0}), (22456, {'train/accuracy': 0.9921752214431763, 'train/loss': 0.02509389817714691, 'train/mean_average_precision': 0.5612627482316623, 'validation/accuracy': 0.9866765737533569, 'validation/loss': 0.04679597169160843, 'validation/mean_average_precision': 0.25370349778591433, 'validation/num_examples': 43793, 'test/accuracy': 0.9858326315879822, 'test/loss': 0.04969538748264313, 'test/mean_average_precision': 0.2527972181515401, 'test/num_examples': 43793, 'score': 7216.057116985321, 'total_duration': 11013.379020690918, 'accumulated_submission_time': 7216.057116985321, 'accumulated_eval_time': 3795.8180980682373, 'accumulated_logging_time': 0.8788893222808838, 'global_step': 22456, 'preemption_count': 0}), (23203, {'train/accuracy': 0.9926006197929382, 'train/loss': 0.024097805842757225, 'train/mean_average_precision': 0.5666674766675607, 'validation/accuracy': 0.9866806268692017, 'validation/loss': 0.04645010828971863, 'validation/mean_average_precision': 0.25767473144249486, 'validation/num_examples': 43793, 'test/accuracy': 0.9858015179634094, 'test/loss': 0.04949909821152687, 'test/mean_average_precision': 0.2530047210423624, 'test/num_examples': 43793, 'score': 7456.1574766635895, 'total_duration': 11377.154458284378, 'accumulated_submission_time': 7456.1574766635895, 'accumulated_eval_time': 3919.440933704376, 'accumulated_logging_time': 0.9106287956237793, 'global_step': 23203, 'preemption_count': 0}), (23953, {'train/accuracy': 0.9929395318031311, 'train/loss': 0.022926853969693184, 'train/mean_average_precision': 0.6068382807187294, 'validation/accuracy': 0.9866469502449036, 'validation/loss': 0.046744298189878464, 'validation/mean_average_precision': 0.25062150247287623, 'validation/num_examples': 43793, 'test/accuracy': 0.9857783317565918, 'test/loss': 0.04991532862186432, 'test/mean_average_precision': 0.2435153176655401, 'test/num_examples': 43793, 'score': 7696.350813627243, 'total_duration': 11737.6252348423, 'accumulated_submission_time': 7696.350813627243, 'accumulated_eval_time': 4039.667632818222, 'accumulated_logging_time': 0.9410545825958252, 'global_step': 23953, 'preemption_count': 0}), (24697, {'train/accuracy': 0.9930366277694702, 'train/loss': 0.022799205034971237, 'train/mean_average_precision': 0.6002876704147919, 'validation/accuracy': 0.9866778254508972, 'validation/loss': 0.04691381752490997, 'validation/mean_average_precision': 0.25802253076312165, 'validation/num_examples': 43793, 'test/accuracy': 0.9857467412948608, 'test/loss': 0.050121892243623734, 'test/mean_average_precision': 0.2449126202635368, 'test/num_examples': 43793, 'score': 7936.4745626449585, 'total_duration': 12100.827523469925, 'accumulated_submission_time': 7936.4745626449585, 'accumulated_eval_time': 4162.690213441849, 'accumulated_logging_time': 0.9763381481170654, 'global_step': 24697, 'preemption_count': 0}), (25443, {'train/accuracy': 0.9928334951400757, 'train/loss': 0.02330951765179634, 'train/mean_average_precision': 0.5824658225387833, 'validation/accuracy': 0.9866371750831604, 'validation/loss': 0.047194041311740875, 'validation/mean_average_precision': 0.25858392894004656, 'validation/num_examples': 43793, 'test/accuracy': 0.9858074188232422, 'test/loss': 0.05031436309218407, 'test/mean_average_precision': 0.25117618069757147, 'test/num_examples': 43793, 'score': 8176.495717287064, 'total_duration': 12460.498522043228, 'accumulated_submission_time': 8176.495717287064, 'accumulated_eval_time': 4282.287209033966, 'accumulated_logging_time': 1.0089423656463623, 'global_step': 25443, 'preemption_count': 0}), (26181, {'train/accuracy': 0.9925329089164734, 'train/loss': 0.024043401703238487, 'train/mean_average_precision': 0.5762234810496102, 'validation/accuracy': 0.986629068851471, 'validation/loss': 0.04753634333610535, 'validation/mean_average_precision': 0.25586160400905505, 'validation/num_examples': 43793, 'test/accuracy': 0.9857892990112305, 'test/loss': 0.05049561336636543, 'test/mean_average_precision': 0.2471158389841096, 'test/num_examples': 43793, 'score': 8416.455756187439, 'total_duration': 12820.535581588745, 'accumulated_submission_time': 8416.455756187439, 'accumulated_eval_time': 4402.311491250992, 'accumulated_logging_time': 1.0403468608856201, 'global_step': 26181, 'preemption_count': 0}), (26930, {'train/accuracy': 0.9923691749572754, 'train/loss': 0.024398958310484886, 'train/mean_average_precision': 0.5442668976545151, 'validation/accuracy': 0.986559271812439, 'validation/loss': 0.04842165485024452, 'validation/mean_average_precision': 0.2523562677872928, 'validation/num_examples': 43793, 'test/accuracy': 0.9857602119445801, 'test/loss': 0.051313646137714386, 'test/mean_average_precision': 0.2501128544462677, 'test/num_examples': 43793, 'score': 8656.649072170258, 'total_duration': 13178.992216587067, 'accumulated_submission_time': 8656.649072170258, 'accumulated_eval_time': 4520.522451400757, 'accumulated_logging_time': 1.071984052658081, 'global_step': 26930, 'preemption_count': 0}), (27681, {'train/accuracy': 0.992576003074646, 'train/loss': 0.023915134370326996, 'train/mean_average_precision': 0.5780123560081694, 'validation/accuracy': 0.9865665435791016, 'validation/loss': 0.04806533455848694, 'validation/mean_average_precision': 0.25048311568385967, 'validation/num_examples': 43793, 'test/accuracy': 0.9857446551322937, 'test/loss': 0.05106044188141823, 'test/mean_average_precision': 0.2448402843569965, 'test/num_examples': 43793, 'score': 8896.665783882141, 'total_duration': 13541.615369558334, 'accumulated_submission_time': 8896.665783882141, 'accumulated_eval_time': 4643.076948165894, 'accumulated_logging_time': 1.1032321453094482, 'global_step': 27681, 'preemption_count': 0}), (28428, {'train/accuracy': 0.9928958415985107, 'train/loss': 0.022750936448574066, 'train/mean_average_precision': 0.5812031928977817, 'validation/accuracy': 0.9866591095924377, 'validation/loss': 0.048344485461711884, 'validation/mean_average_precision': 0.258367335005388, 'validation/num_examples': 43793, 'test/accuracy': 0.9858065247535706, 'test/loss': 0.051526401191949844, 'test/mean_average_precision': 0.24750431426551517, 'test/num_examples': 43793, 'score': 9136.741748094559, 'total_duration': 13902.822404623032, 'accumulated_submission_time': 9136.741748094559, 'accumulated_eval_time': 4764.155343532562, 'accumulated_logging_time': 1.1362247467041016, 'global_step': 28428, 'preemption_count': 0}), (29172, {'train/accuracy': 0.9930545687675476, 'train/loss': 0.022334126755595207, 'train/mean_average_precision': 0.615808792548507, 'validation/accuracy': 0.9865154027938843, 'validation/loss': 0.04860911890864372, 'validation/mean_average_precision': 0.24820526200687404, 'validation/num_examples': 43793, 'test/accuracy': 0.9856435656547546, 'test/loss': 0.05174456164240837, 'test/mean_average_precision': 0.2452964177865293, 'test/num_examples': 43793, 'score': 9376.930389642715, 'total_duration': 14262.695072174072, 'accumulated_submission_time': 9376.930389642715, 'accumulated_eval_time': 4883.786524772644, 'accumulated_logging_time': 1.1681454181671143, 'global_step': 29172, 'preemption_count': 0}), (29922, {'train/accuracy': 0.9933610558509827, 'train/loss': 0.021280016750097275, 'train/mean_average_precision': 0.6281966779933524, 'validation/accuracy': 0.9865202903747559, 'validation/loss': 0.04879826307296753, 'validation/mean_average_precision': 0.2524698596022855, 'validation/num_examples': 43793, 'test/accuracy': 0.9856886267662048, 'test/loss': 0.05191462114453316, 'test/mean_average_precision': 0.24138551391030635, 'test/num_examples': 43793, 'score': 9617.14767241478, 'total_duration': 14620.18716263771, 'accumulated_submission_time': 9617.14767241478, 'accumulated_eval_time': 5001.006668329239, 'accumulated_logging_time': 1.2020776271820068, 'global_step': 29922, 'preemption_count': 0}), (30664, {'train/accuracy': 0.9941012263298035, 'train/loss': 0.01936311647295952, 'train/mean_average_precision': 0.6699394839401119, 'validation/accuracy': 0.9864890575408936, 'validation/loss': 0.04925333335995674, 'validation/mean_average_precision': 0.2432590919025162, 'validation/num_examples': 43793, 'test/accuracy': 0.9856064915657043, 'test/loss': 0.05230368301272392, 'test/mean_average_precision': 0.2375251315964577, 'test/num_examples': 43793, 'score': 9857.341331720352, 'total_duration': 14984.070784330368, 'accumulated_submission_time': 9857.341331720352, 'accumulated_eval_time': 5124.642949104309, 'accumulated_logging_time': 1.2355139255523682, 'global_step': 30664, 'preemption_count': 0}), (31412, {'train/accuracy': 0.9941602349281311, 'train/loss': 0.019281893968582153, 'train/mean_average_precision': 0.6668670900993225, 'validation/accuracy': 0.9864894151687622, 'validation/loss': 0.04892243072390556, 'validation/mean_average_precision': 0.2506274620268583, 'validation/num_examples': 43793, 'test/accuracy': 0.9856364130973816, 'test/loss': 0.05200303718447685, 'test/mean_average_precision': 0.24071333250409857, 'test/num_examples': 43793, 'score': 10097.41719198227, 'total_duration': 15341.210273504257, 'accumulated_submission_time': 10097.41719198227, 'accumulated_eval_time': 5241.653692960739, 'accumulated_logging_time': 1.2681159973144531, 'global_step': 31412, 'preemption_count': 0}), (32146, {'train/accuracy': 0.9937401413917542, 'train/loss': 0.02024981938302517, 'train/mean_average_precision': 0.6502916140923689, 'validation/accuracy': 0.9865487217903137, 'validation/loss': 0.049678266048431396, 'validation/mean_average_precision': 0.2499031904886978, 'validation/num_examples': 43793, 'test/accuracy': 0.9856595396995544, 'test/loss': 0.05282976105809212, 'test/mean_average_precision': 0.23891806109517563, 'test/num_examples': 43793, 'score': 10337.642447710037, 'total_duration': 15703.304780244827, 'accumulated_submission_time': 10337.642447710037, 'accumulated_eval_time': 5363.4699013233185, 'accumulated_logging_time': 1.3005335330963135, 'global_step': 32146, 'preemption_count': 0}), (32902, {'train/accuracy': 0.9936198592185974, 'train/loss': 0.020604152232408524, 'train/mean_average_precision': 0.6400930897954951, 'validation/accuracy': 0.986531674861908, 'validation/loss': 0.04952919855713844, 'validation/mean_average_precision': 0.24534358330526443, 'validation/num_examples': 43793, 'test/accuracy': 0.9856982827186584, 'test/loss': 0.05269746482372284, 'test/mean_average_precision': 0.24266124610204737, 'test/num_examples': 43793, 'score': 10577.850949764252, 'total_duration': 16068.123711824417, 'accumulated_submission_time': 10577.850949764252, 'accumulated_eval_time': 5488.026214838028, 'accumulated_logging_time': 1.334458589553833, 'global_step': 32902, 'preemption_count': 0}), (33652, {'train/accuracy': 0.9934507012367249, 'train/loss': 0.020942265167832375, 'train/mean_average_precision': 0.6310022435507462, 'validation/accuracy': 0.9864732027053833, 'validation/loss': 0.049740687012672424, 'validation/mean_average_precision': 0.24596346589723075, 'validation/num_examples': 43793, 'test/accuracy': 0.9855508804321289, 'test/loss': 0.052817679941654205, 'test/mean_average_precision': 0.2418377184419295, 'test/num_examples': 43793, 'score': 10817.950261116028, 'total_duration': 16425.47945213318, 'accumulated_submission_time': 10817.950261116028, 'accumulated_eval_time': 5605.228426933289, 'accumulated_logging_time': 1.3684918880462646, 'global_step': 33652, 'preemption_count': 0}), (34387, {'train/accuracy': 0.9934825897216797, 'train/loss': 0.02074396423995495, 'train/mean_average_precision': 0.6385106217173827, 'validation/accuracy': 0.9863489866256714, 'validation/loss': 0.050209540873765945, 'validation/mean_average_precision': 0.24648465404572378, 'validation/num_examples': 43793, 'test/accuracy': 0.9855243563652039, 'test/loss': 0.05341925099492073, 'test/mean_average_precision': 0.23595149152631772, 'test/num_examples': 43793, 'score': 11058.128161430359, 'total_duration': 16786.856697797775, 'accumulated_submission_time': 11058.128161430359, 'accumulated_eval_time': 5726.372263431549, 'accumulated_logging_time': 1.402308702468872, 'global_step': 34387, 'preemption_count': 0}), (35138, {'train/accuracy': 0.9936950206756592, 'train/loss': 0.020330367609858513, 'train/mean_average_precision': 0.6362148804038725, 'validation/accuracy': 0.9863278865814209, 'validation/loss': 0.050283726304769516, 'validation/mean_average_precision': 0.24305085441601995, 'validation/num_examples': 43793, 'test/accuracy': 0.9854733943939209, 'test/loss': 0.05352490022778511, 'test/mean_average_precision': 0.2315473117127911, 'test/num_examples': 43793, 'score': 11298.363315105438, 'total_duration': 17141.203130722046, 'accumulated_submission_time': 11298.363315105438, 'accumulated_eval_time': 5840.427979707718, 'accumulated_logging_time': 1.437326431274414, 'global_step': 35138, 'preemption_count': 0}), (35887, {'train/accuracy': 0.993681788444519, 'train/loss': 0.019934918731451035, 'train/mean_average_precision': 0.6535335108823093, 'validation/accuracy': 0.9863148927688599, 'validation/loss': 0.05142214149236679, 'validation/mean_average_precision': 0.24302966324393094, 'validation/num_examples': 43793, 'test/accuracy': 0.9854927659034729, 'test/loss': 0.05451366677880287, 'test/mean_average_precision': 0.2386775963454686, 'test/num_examples': 43793, 'score': 11538.582442760468, 'total_duration': 17502.014199733734, 'accumulated_submission_time': 11538.582442760468, 'accumulated_eval_time': 5960.965605020523, 'accumulated_logging_time': 1.4715502262115479, 'global_step': 35887, 'preemption_count': 0}), (36621, {'train/accuracy': 0.9945728182792664, 'train/loss': 0.017682744190096855, 'train/mean_average_precision': 0.688483399506173, 'validation/accuracy': 0.986333966255188, 'validation/loss': 0.05144191533327103, 'validation/mean_average_precision': 0.24413099951531606, 'validation/num_examples': 43793, 'test/accuracy': 0.9854645133018494, 'test/loss': 0.054535992443561554, 'test/mean_average_precision': 0.2343355525502071, 'test/num_examples': 43793, 'score': 11778.559102535248, 'total_duration': 17865.818506240845, 'accumulated_submission_time': 11778.559102535248, 'accumulated_eval_time': 6084.737009048462, 'accumulated_logging_time': 1.5058512687683105, 'global_step': 36621, 'preemption_count': 0}), (37372, {'train/accuracy': 0.9952541589736938, 'train/loss': 0.01587018556892872, 'train/mean_average_precision': 0.7463612898447716, 'validation/accuracy': 0.9863250255584717, 'validation/loss': 0.05211950093507767, 'validation/mean_average_precision': 0.23930383596515317, 'validation/num_examples': 43793, 'test/accuracy': 0.9855479598045349, 'test/loss': 0.055206358432769775, 'test/mean_average_precision': 0.23536206192428505, 'test/num_examples': 43793, 'score': 12018.643469572067, 'total_duration': 18226.29783797264, 'accumulated_submission_time': 12018.643469572067, 'accumulated_eval_time': 6205.075866937637, 'accumulated_logging_time': 1.5408875942230225, 'global_step': 37372, 'preemption_count': 0}), (38121, {'train/accuracy': 0.9949754476547241, 'train/loss': 0.016621870920062065, 'train/mean_average_precision': 0.7227896262201021, 'validation/accuracy': 0.9864480495452881, 'validation/loss': 0.05189632624387741, 'validation/mean_average_precision': 0.23987123868166987, 'validation/num_examples': 43793, 'test/accuracy': 0.9855525493621826, 'test/loss': 0.05509902909398079, 'test/mean_average_precision': 0.2385763605440669, 'test/num_examples': 43793, 'score': 12258.72385263443, 'total_duration': 18581.881452083588, 'accumulated_submission_time': 12258.72385263443, 'accumulated_eval_time': 6320.522824525833, 'accumulated_logging_time': 1.5767641067504883, 'global_step': 38121, 'preemption_count': 0}), (38874, {'train/accuracy': 0.9947724938392639, 'train/loss': 0.016733374446630478, 'train/mean_average_precision': 0.714931953560515, 'validation/accuracy': 0.986291766166687, 'validation/loss': 0.05320531502366066, 'validation/mean_average_precision': 0.23560947523729037, 'validation/num_examples': 43793, 'test/accuracy': 0.9855129718780518, 'test/loss': 0.05614528805017471, 'test/mean_average_precision': 0.23769189452529768, 'test/num_examples': 43793, 'score': 12498.797527074814, 'total_duration': 18939.176498413086, 'accumulated_submission_time': 12498.797527074814, 'accumulated_eval_time': 6437.69078040123, 'accumulated_logging_time': 1.6097221374511719, 'global_step': 38874, 'preemption_count': 0}), (39625, {'train/accuracy': 0.994320273399353, 'train/loss': 0.01782328262925148, 'train/mean_average_precision': 0.705524199624799, 'validation/accuracy': 0.986319363117218, 'validation/loss': 0.05309564247727394, 'validation/mean_average_precision': 0.24038347614439076, 'validation/num_examples': 43793, 'test/accuracy': 0.9854274988174438, 'test/loss': 0.05641566216945648, 'test/mean_average_precision': 0.23018162124236663, 'test/num_examples': 43793, 'score': 12738.750935316086, 'total_duration': 19294.999740600586, 'accumulated_submission_time': 12738.750935316086, 'accumulated_eval_time': 6553.50083398819, 'accumulated_logging_time': 1.6491947174072266, 'global_step': 39625, 'preemption_count': 0}), (40377, {'train/accuracy': 0.9941571950912476, 'train/loss': 0.018354937434196472, 'train/mean_average_precision': 0.6876441021637686, 'validation/accuracy': 0.9862978458404541, 'validation/loss': 0.053439173847436905, 'validation/mean_average_precision': 0.23703639568501791, 'validation/num_examples': 43793, 'test/accuracy': 0.9854485392570496, 'test/loss': 0.05665614455938339, 'test/mean_average_precision': 0.23156396099689264, 'test/num_examples': 43793, 'score': 12978.82855129242, 'total_duration': 19645.997228384018, 'accumulated_submission_time': 12978.82855129242, 'accumulated_eval_time': 6664.366796016693, 'accumulated_logging_time': 1.6828031539916992, 'global_step': 40377, 'preemption_count': 0}), (41136, {'train/accuracy': 0.9942678809165955, 'train/loss': 0.018299048766493797, 'train/mean_average_precision': 0.6918804230446601, 'validation/accuracy': 0.9859828352928162, 'validation/loss': 0.05258406326174736, 'validation/mean_average_precision': 0.23572478720522083, 'validation/num_examples': 43793, 'test/accuracy': 0.9851621389389038, 'test/loss': 0.05586322024464607, 'test/mean_average_precision': 0.23260640344393388, 'test/num_examples': 43793, 'score': 13219.013098239899, 'total_duration': 20003.628672599792, 'accumulated_submission_time': 13219.013098239899, 'accumulated_eval_time': 6781.759689092636, 'accumulated_logging_time': 1.7166087627410889, 'global_step': 41136, 'preemption_count': 0}), (41890, {'train/accuracy': 0.9942914843559265, 'train/loss': 0.017965376377105713, 'train/mean_average_precision': 0.701986704581986, 'validation/accuracy': 0.9862337112426758, 'validation/loss': 0.05374501273036003, 'validation/mean_average_precision': 0.23481378091598354, 'validation/num_examples': 43793, 'test/accuracy': 0.9853798747062683, 'test/loss': 0.056821972131729126, 'test/mean_average_precision': 0.23249823551142412, 'test/num_examples': 43793, 'score': 13459.264628887177, 'total_duration': 20356.678092479706, 'accumulated_submission_time': 13459.264628887177, 'accumulated_eval_time': 6894.503623247147, 'accumulated_logging_time': 1.7502388954162598, 'global_step': 41890, 'preemption_count': 0}), (42636, {'train/accuracy': 0.9941781163215637, 'train/loss': 0.017894301563501358, 'train/mean_average_precision': 0.6990086529737705, 'validation/accuracy': 0.9864057898521423, 'validation/loss': 0.05441704019904137, 'validation/mean_average_precision': 0.23981925742551147, 'validation/num_examples': 43793, 'test/accuracy': 0.9854514598846436, 'test/loss': 0.058204285800457, 'test/mean_average_precision': 0.23092372916024198, 'test/num_examples': 43793, 'score': 13699.364458560944, 'total_duration': 20712.2668569088, 'accumulated_submission_time': 13699.364458560944, 'accumulated_eval_time': 7009.9356207847595, 'accumulated_logging_time': 1.7863051891326904, 'global_step': 42636, 'preemption_count': 0}), (43393, {'train/accuracy': 0.9946008920669556, 'train/loss': 0.016835426911711693, 'train/mean_average_precision': 0.7236670943979682, 'validation/accuracy': 0.9863424897193909, 'validation/loss': 0.054734326899051666, 'validation/mean_average_precision': 0.2367951793593295, 'validation/num_examples': 43793, 'test/accuracy': 0.9854030609130859, 'test/loss': 0.058230116963386536, 'test/mean_average_precision': 0.22809934313288432, 'test/num_examples': 43793, 'score': 13939.344955205917, 'total_duration': 21068.913420438766, 'accumulated_submission_time': 13939.344955205917, 'accumulated_eval_time': 7126.545880794525, 'accumulated_logging_time': 1.8220069408416748, 'global_step': 43393, 'preemption_count': 0}), (44147, {'train/accuracy': 0.9953388571739197, 'train/loss': 0.015141450800001621, 'train/mean_average_precision': 0.7568423681622725, 'validation/accuracy': 0.9860754013061523, 'validation/loss': 0.05479004979133606, 'validation/mean_average_precision': 0.2288448749028145, 'validation/num_examples': 43793, 'test/accuracy': 0.9852429628372192, 'test/loss': 0.05832561478018761, 'test/mean_average_precision': 0.22534321390666928, 'test/num_examples': 43793, 'score': 14179.578085184097, 'total_duration': 21418.554672002792, 'accumulated_submission_time': 14179.578085184097, 'accumulated_eval_time': 7235.889031648636, 'accumulated_logging_time': 1.866541862487793, 'global_step': 44147, 'preemption_count': 0}), (44884, {'train/accuracy': 0.996155858039856, 'train/loss': 0.013371159322559834, 'train/mean_average_precision': 0.7954491680973936, 'validation/accuracy': 0.9861951470375061, 'validation/loss': 0.05493079498410225, 'validation/mean_average_precision': 0.23999916244612557, 'validation/num_examples': 43793, 'test/accuracy': 0.9854169487953186, 'test/loss': 0.058216750621795654, 'test/mean_average_precision': 0.22741323348126463, 'test/num_examples': 43793, 'score': 14419.814072847366, 'total_duration': 21774.352262735367, 'accumulated_submission_time': 14419.814072847366, 'accumulated_eval_time': 7351.393758773804, 'accumulated_logging_time': 1.9024038314819336, 'global_step': 44884, 'preemption_count': 0}), (45620, {'train/accuracy': 0.9964368939399719, 'train/loss': 0.01280219480395317, 'train/mean_average_precision': 0.7997349957824464, 'validation/accuracy': 0.986088752746582, 'validation/loss': 0.05589419975876808, 'validation/mean_average_precision': 0.23350083450891645, 'validation/num_examples': 43793, 'test/accuracy': 0.9852564930915833, 'test/loss': 0.059393975883722305, 'test/mean_average_precision': 0.22205511168090772, 'test/num_examples': 43793, 'score': 14659.792273283005, 'total_duration': 22128.552355766296, 'accumulated_submission_time': 14659.792273283005, 'accumulated_eval_time': 7465.55695939064, 'accumulated_logging_time': 1.9380853176116943, 'global_step': 45620, 'preemption_count': 0}), (46380, {'train/accuracy': 0.995314359664917, 'train/loss': 0.015161000192165375, 'train/mean_average_precision': 0.7500370780843769, 'validation/accuracy': 0.9860250353813171, 'validation/loss': 0.056016840040683746, 'validation/mean_average_precision': 0.2258241513802438, 'validation/num_examples': 43793, 'test/accuracy': 0.9850963950157166, 'test/loss': 0.05946921184659004, 'test/mean_average_precision': 0.2228981628786851, 'test/num_examples': 43793, 'score': 14899.860787391663, 'total_duration': 22484.813533067703, 'accumulated_submission_time': 14899.860787391663, 'accumulated_eval_time': 7581.6927881240845, 'accumulated_logging_time': 1.9741096496582031, 'global_step': 46380, 'preemption_count': 0}), (47129, {'train/accuracy': 0.9956495761871338, 'train/loss': 0.014239811338484287, 'train/mean_average_precision': 0.7698141151166427, 'validation/accuracy': 0.9861208200454712, 'validation/loss': 0.056531015783548355, 'validation/mean_average_precision': 0.2288986155071729, 'validation/num_examples': 43793, 'test/accuracy': 0.9853002429008484, 'test/loss': 0.05993714556097984, 'test/mean_average_precision': 0.23162354453084574, 'test/num_examples': 43793, 'score': 15139.825226068497, 'total_duration': 22837.559860944748, 'accumulated_submission_time': 15139.825226068497, 'accumulated_eval_time': 7694.4174518585205, 'accumulated_logging_time': 2.0112147331237793, 'global_step': 47129, 'preemption_count': 0}), (47879, {'train/accuracy': 0.9952120780944824, 'train/loss': 0.015117822214961052, 'train/mean_average_precision': 0.7573643665429037, 'validation/accuracy': 0.9860530495643616, 'validation/loss': 0.05645659193396568, 'validation/mean_average_precision': 0.2272830550792519, 'validation/num_examples': 43793, 'test/accuracy': 0.9852320551872253, 'test/loss': 0.05987373739480972, 'test/mean_average_precision': 0.22779078451152113, 'test/num_examples': 43793, 'score': 15379.947883367538, 'total_duration': 23192.914344787598, 'accumulated_submission_time': 15379.947883367538, 'accumulated_eval_time': 7809.593261241913, 'accumulated_logging_time': 2.0465872287750244, 'global_step': 47879, 'preemption_count': 0}), (48625, {'train/accuracy': 0.9950177669525146, 'train/loss': 0.01554893609136343, 'train/mean_average_precision': 0.7415567667563101, 'validation/accuracy': 0.9860798716545105, 'validation/loss': 0.05699317157268524, 'validation/mean_average_precision': 0.23002389334434853, 'validation/num_examples': 43793, 'test/accuracy': 0.9852830171585083, 'test/loss': 0.060362961143255234, 'test/mean_average_precision': 0.22806503587802487, 'test/num_examples': 43793, 'score': 15620.005237102509, 'total_duration': 23545.832807779312, 'accumulated_submission_time': 15620.005237102509, 'accumulated_eval_time': 7922.388898611069, 'accumulated_logging_time': 2.0906825065612793, 'global_step': 48625, 'preemption_count': 0}), (49368, {'train/accuracy': 0.9946566820144653, 'train/loss': 0.016457218676805496, 'train/mean_average_precision': 0.7364141469888428, 'validation/accuracy': 0.9861857891082764, 'validation/loss': 0.057639967650175095, 'validation/mean_average_precision': 0.22420575743211074, 'validation/num_examples': 43793, 'test/accuracy': 0.9852412939071655, 'test/loss': 0.061149779707193375, 'test/mean_average_precision': 0.22574784946939636, 'test/num_examples': 43793, 'score': 15860.014887571335, 'total_duration': 23905.479996204376, 'accumulated_submission_time': 15860.014887571335, 'accumulated_eval_time': 8041.968768358231, 'accumulated_logging_time': 2.127811908721924, 'global_step': 49368, 'preemption_count': 0}), (50121, {'train/accuracy': 0.9957500696182251, 'train/loss': 0.01392839290201664, 'train/mean_average_precision': 0.7854547276249997, 'validation/accuracy': 0.9860916137695312, 'validation/loss': 0.057477932423353195, 'validation/mean_average_precision': 0.2286573693495823, 'validation/num_examples': 43793, 'test/accuracy': 0.9851734638214111, 'test/loss': 0.06093522161245346, 'test/mean_average_precision': 0.2285361877579136, 'test/num_examples': 43793, 'score': 16100.006301641464, 'total_duration': 24264.062856197357, 'accumulated_submission_time': 16100.006301641464, 'accumulated_eval_time': 8160.495388507843, 'accumulated_logging_time': 2.1722218990325928, 'global_step': 50121, 'preemption_count': 0}), (50869, {'train/accuracy': 0.9962384104728699, 'train/loss': 0.012546772137284279, 'train/mean_average_precision': 0.8122539594282422, 'validation/accuracy': 0.9861472249031067, 'validation/loss': 0.058172766119241714, 'validation/mean_average_precision': 0.22581597638840017, 'validation/num_examples': 43793, 'test/accuracy': 0.9852408766746521, 'test/loss': 0.061664335429668427, 'test/mean_average_precision': 0.22632950553906886, 'test/num_examples': 43793, 'score': 16340.236045360565, 'total_duration': 24617.448274374008, 'accumulated_submission_time': 16340.236045360565, 'accumulated_eval_time': 8273.594465494156, 'accumulated_logging_time': 2.208230495452881, 'global_step': 50869, 'preemption_count': 0}), (51620, {'train/accuracy': 0.9967470765113831, 'train/loss': 0.011496426537632942, 'train/mean_average_precision': 0.8367234392536422, 'validation/accuracy': 0.986141562461853, 'validation/loss': 0.05932892858982086, 'validation/mean_average_precision': 0.22748976931870318, 'validation/num_examples': 43793, 'test/accuracy': 0.9852067828178406, 'test/loss': 0.06297004967927933, 'test/mean_average_precision': 0.2218210113962008, 'test/num_examples': 43793, 'score': 16580.56310081482, 'total_duration': 24970.902344703674, 'accumulated_submission_time': 16580.56310081482, 'accumulated_eval_time': 8386.665300130844, 'accumulated_logging_time': 2.2444097995758057, 'global_step': 51620, 'preemption_count': 0}), (52381, {'train/accuracy': 0.9970282316207886, 'train/loss': 0.010879489593207836, 'train/mean_average_precision': 0.8544680011223145, 'validation/accuracy': 0.9861334562301636, 'validation/loss': 0.059530146420001984, 'validation/mean_average_precision': 0.22775425386405748, 'validation/num_examples': 43793, 'test/accuracy': 0.9852383732795715, 'test/loss': 0.0631648600101471, 'test/mean_average_precision': 0.22353977631998603, 'test/num_examples': 43793, 'score': 16820.774089574814, 'total_duration': 25320.17582321167, 'accumulated_submission_time': 16820.774089574814, 'accumulated_eval_time': 8495.671817302704, 'accumulated_logging_time': 2.2801904678344727, 'global_step': 52381, 'preemption_count': 0}), (53126, {'train/accuracy': 0.9972330331802368, 'train/loss': 0.010528430342674255, 'train/mean_average_precision': 0.8540056525446815, 'validation/accuracy': 0.9861581921577454, 'validation/loss': 0.059389643371105194, 'validation/mean_average_precision': 0.22787074654849107, 'validation/num_examples': 43793, 'test/accuracy': 0.9852033853530884, 'test/loss': 0.06320707499980927, 'test/mean_average_precision': 0.22194294565332634, 'test/num_examples': 43793, 'score': 17060.759781360626, 'total_duration': 25673.513555049896, 'accumulated_submission_time': 17060.759781360626, 'accumulated_eval_time': 8608.965829610825, 'accumulated_logging_time': 2.3182740211486816, 'global_step': 53126, 'preemption_count': 0}), (53873, {'train/accuracy': 0.9966502785682678, 'train/loss': 0.011681037954986095, 'train/mean_average_precision': 0.835379206205047, 'validation/accuracy': 0.9860693216323853, 'validation/loss': 0.06027856469154358, 'validation/mean_average_precision': 0.22494664746660062, 'validation/num_examples': 43793, 'test/accuracy': 0.9851911664009094, 'test/loss': 0.06380055844783783, 'test/mean_average_precision': 0.21495218237258368, 'test/num_examples': 43793, 'score': 17300.893683433533, 'total_duration': 26027.957573890686, 'accumulated_submission_time': 17300.893683433533, 'accumulated_eval_time': 8723.215174674988, 'accumulated_logging_time': 2.3571720123291016, 'global_step': 53873, 'preemption_count': 0}), (54620, {'train/accuracy': 0.9948896765708923, 'train/loss': 0.015510822646319866, 'train/mean_average_precision': 0.7515258127315602, 'validation/accuracy': 0.9860928654670715, 'validation/loss': 0.060128357261419296, 'validation/mean_average_precision': 0.22509716084751005, 'validation/num_examples': 43793, 'test/accuracy': 0.9852109551429749, 'test/loss': 0.06393587589263916, 'test/mean_average_precision': 0.2158959223624439, 'test/num_examples': 43793, 'score': 17540.84679889679, 'total_duration': 26381.413821458817, 'accumulated_submission_time': 17540.84679889679, 'accumulated_eval_time': 8836.655643463135, 'accumulated_logging_time': 2.3990213871002197, 'global_step': 54620, 'preemption_count': 0}), (55367, {'train/accuracy': 0.9954902529716492, 'train/loss': 0.014068689197301865, 'train/mean_average_precision': 0.7815939752419798, 'validation/accuracy': 0.9859552383422852, 'validation/loss': 0.05988990142941475, 'validation/mean_average_precision': 0.2264501280562338, 'validation/num_examples': 43793, 'test/accuracy': 0.9850273132324219, 'test/loss': 0.0636545941233635, 'test/mean_average_precision': 0.21549331518327594, 'test/num_examples': 43793, 'score': 17780.836613178253, 'total_duration': 26730.361434221268, 'accumulated_submission_time': 17780.836613178253, 'accumulated_eval_time': 8945.55486869812, 'accumulated_logging_time': 2.4369421005249023, 'global_step': 55367, 'preemption_count': 0}), (56126, {'train/accuracy': 0.9959081411361694, 'train/loss': 0.012733520939946175, 'train/mean_average_precision': 0.8261949531777921, 'validation/accuracy': 0.986108660697937, 'validation/loss': 0.06120892986655235, 'validation/mean_average_precision': 0.22277356853215763, 'validation/num_examples': 43793, 'test/accuracy': 0.9852176904678345, 'test/loss': 0.06481973081827164, 'test/mean_average_precision': 0.21762757762418214, 'test/num_examples': 43793, 'score': 18021.015577316284, 'total_duration': 27084.507292032242, 'accumulated_submission_time': 18021.015577316284, 'accumulated_eval_time': 9059.463624238968, 'accumulated_logging_time': 2.475022077560425, 'global_step': 56126, 'preemption_count': 0}), (56866, {'train/accuracy': 0.9954772591590881, 'train/loss': 0.013762353919446468, 'train/mean_average_precision': 0.8133653761740869, 'validation/accuracy': 0.9861249327659607, 'validation/loss': 0.06183765456080437, 'validation/mean_average_precision': 0.2242386545209901, 'validation/num_examples': 43793, 'test/accuracy': 0.985187828540802, 'test/loss': 0.06561759114265442, 'test/mean_average_precision': 0.2167473575816602, 'test/num_examples': 43793, 'score': 18261.217477321625, 'total_duration': 27437.100608110428, 'accumulated_submission_time': 18261.217477321625, 'accumulated_eval_time': 9171.793118715286, 'accumulated_logging_time': 2.5136139392852783, 'global_step': 56866, 'preemption_count': 0})], 'global_step': 57545}
I0205 15:02:37.343399 140451058161472 submission_runner.py:586] Timing: 18477.311544656754
I0205 15:02:37.343465 140451058161472 submission_runner.py:588] Total number of evals: 77
I0205 15:02:37.343515 140451058161472 submission_runner.py:589] ====================
I0205 15:02:37.343570 140451058161472 submission_runner.py:542] Using RNG seed 449608868
I0205 15:02:37.408856 140451058161472 submission_runner.py:551] --- Tuning run 3/5 ---
I0205 15:02:37.409009 140451058161472 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_2/ogbg_jax/trial_3.
I0205 15:02:37.410022 140451058161472 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_2/ogbg_jax/trial_3/hparams.json.
I0205 15:02:37.546181 140451058161472 submission_runner.py:206] Initializing dataset.
I0205 15:02:37.639537 140451058161472 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0205 15:02:37.645306 140451058161472 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0205 15:02:37.779188 140451058161472 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0205 15:02:37.815965 140451058161472 submission_runner.py:213] Initializing model.
I0205 15:02:42.349069 140451058161472 submission_runner.py:255] Initializing optimizer.
I0205 15:02:42.937584 140451058161472 submission_runner.py:262] Initializing metrics bundle.
I0205 15:02:42.937788 140451058161472 submission_runner.py:280] Initializing checkpoint and logger.
I0205 15:02:42.938448 140451058161472 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_2/ogbg_jax/trial_3 with prefix checkpoint_
I0205 15:02:42.938576 140451058161472 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_2/ogbg_jax/trial_3/meta_data_0.json.
I0205 15:02:42.938775 140451058161472 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0205 15:02:42.938837 140451058161472 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0205 15:02:44.667366 140451058161472 logger_utils.py:220] Unable to record git information. Continuing without it.
I0205 15:02:46.390807 140451058161472 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_2/ogbg_jax/trial_3/flags_0.json.
I0205 15:02:46.394626 140451058161472 submission_runner.py:314] Starting training loop.
I0205 15:02:58.126680 140248389748480 logging_writer.py:48] [0] global_step=0, grad_norm=2.306910276412964, loss=0.7360677719116211
I0205 15:02:58.138127 140451058161472 spec.py:321] Evaluating on the training split.
I0205 15:04:46.755895 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 15:04:49.838054 140451058161472 spec.py:349] Evaluating on the test split.
I0205 15:04:52.874714 140451058161472 submission_runner.py:408] Time since start: 126.48s, 	Step: 1, 	{'train/accuracy': 0.5291368365287781, 'train/loss': 0.7363722324371338, 'train/mean_average_precision': 0.021802974706375825, 'validation/accuracy': 0.5270814895629883, 'validation/loss': 0.737440824508667, 'validation/mean_average_precision': 0.024047021074292817, 'validation/num_examples': 43793, 'test/accuracy': 0.5256850719451904, 'test/loss': 0.7376683354377747, 'test/mean_average_precision': 0.026058016819722727, 'test/num_examples': 43793, 'score': 11.743421077728271, 'total_duration': 126.48002123832703, 'accumulated_submission_time': 11.743421077728271, 'accumulated_eval_time': 114.73655438423157, 'accumulated_logging_time': 0}
I0205 15:04:52.884280 140266968643328 logging_writer.py:48] [1] accumulated_eval_time=114.736554, accumulated_logging_time=0, accumulated_submission_time=11.743421, global_step=1, preemption_count=0, score=11.743421, test/accuracy=0.525685, test/loss=0.737668, test/mean_average_precision=0.026058, test/num_examples=43793, total_duration=126.480021, train/accuracy=0.529137, train/loss=0.736372, train/mean_average_precision=0.021803, validation/accuracy=0.527081, validation/loss=0.737441, validation/mean_average_precision=0.024047, validation/num_examples=43793
I0205 15:05:25.084054 140283530442496 logging_writer.py:48] [100] global_step=100, grad_norm=0.5977209210395813, loss=0.45140156149864197
I0205 15:05:56.859723 140266968643328 logging_writer.py:48] [200] global_step=200, grad_norm=0.36484256386756897, loss=0.33138102293014526
I0205 15:06:29.737046 140283530442496 logging_writer.py:48] [300] global_step=300, grad_norm=0.26505953073501587, loss=0.23799435794353485
I0205 15:07:02.461934 140266968643328 logging_writer.py:48] [400] global_step=400, grad_norm=0.17411020398139954, loss=0.1627078354358673
I0205 15:07:33.926360 140283530442496 logging_writer.py:48] [500] global_step=500, grad_norm=0.1065734401345253, loss=0.1177062839269638
I0205 15:08:05.985010 140266968643328 logging_writer.py:48] [600] global_step=600, grad_norm=0.0671573355793953, loss=0.089049331843853
I0205 15:08:37.791365 140283530442496 logging_writer.py:48] [700] global_step=700, grad_norm=0.045562926679849625, loss=0.0708996057510376
I0205 15:08:53.196149 140451058161472 spec.py:321] Evaluating on the training split.
I0205 15:10:43.987409 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 15:10:46.990938 140451058161472 spec.py:349] Evaluating on the test split.
I0205 15:10:49.990446 140451058161472 submission_runner.py:408] Time since start: 483.60s, 	Step: 749, 	{'train/accuracy': 0.9866331815719604, 'train/loss': 0.06894971430301666, 'train/mean_average_precision': 0.03819908784707897, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.07708906382322311, 'validation/mean_average_precision': 0.041322128947657716, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.07995471358299255, 'test/mean_average_precision': 0.043379390131197515, 'test/num_examples': 43793, 'score': 252.02333188056946, 'total_duration': 483.5957524776459, 'accumulated_submission_time': 252.02333188056946, 'accumulated_eval_time': 231.53080677986145, 'accumulated_logging_time': 0.02034306526184082}
I0205 15:10:50.005609 140248415348480 logging_writer.py:48] [749] accumulated_eval_time=231.530807, accumulated_logging_time=0.020343, accumulated_submission_time=252.023332, global_step=749, preemption_count=0, score=252.023332, test/accuracy=0.983142, test/loss=0.079955, test/mean_average_precision=0.043379, test/num_examples=43793, total_duration=483.595752, train/accuracy=0.986633, train/loss=0.068950, train/mean_average_precision=0.038199, validation/accuracy=0.984118, validation/loss=0.077089, validation/mean_average_precision=0.041322, validation/num_examples=43793
I0205 15:11:07.318731 140266960250624 logging_writer.py:48] [800] global_step=800, grad_norm=0.13538679480552673, loss=0.06981777399778366
I0205 15:11:38.984138 140248415348480 logging_writer.py:48] [900] global_step=900, grad_norm=0.5647830367088318, loss=0.06056622043251991
I0205 15:12:10.817961 140266960250624 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.16200195252895355, loss=0.05518084019422531
I0205 15:12:42.603585 140248415348480 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.21091370284557343, loss=0.049450360238552094
I0205 15:13:14.413258 140266960250624 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.18658803403377533, loss=0.05226367712020874
I0205 15:13:46.555184 140248415348480 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.23013348877429962, loss=0.04766508936882019
I0205 15:14:18.802981 140266960250624 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.15933093428611755, loss=0.05227743461728096
I0205 15:14:50.292823 140451058161472 spec.py:321] Evaluating on the training split.
I0205 15:16:38.159574 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 15:16:41.263769 140451058161472 spec.py:349] Evaluating on the test split.
I0205 15:16:44.383352 140451058161472 submission_runner.py:408] Time since start: 837.99s, 	Step: 1499, 	{'train/accuracy': 0.9871978163719177, 'train/loss': 0.04832090809941292, 'train/mean_average_precision': 0.08593510077412683, 'validation/accuracy': 0.984529972076416, 'validation/loss': 0.057636987417936325, 'validation/mean_average_precision': 0.08721207535649246, 'validation/num_examples': 43793, 'test/accuracy': 0.9835198521614075, 'test/loss': 0.061021484434604645, 'test/mean_average_precision': 0.08584278558918844, 'test/num_examples': 43793, 'score': 492.2781195640564, 'total_duration': 837.9886548519135, 'accumulated_submission_time': 492.2781195640564, 'accumulated_eval_time': 345.62128949165344, 'accumulated_logging_time': 0.0478668212890625}
I0205 15:16:44.399605 140266968643328 logging_writer.py:48] [1499] accumulated_eval_time=345.621289, accumulated_logging_time=0.047867, accumulated_submission_time=492.278120, global_step=1499, preemption_count=0, score=492.278120, test/accuracy=0.983520, test/loss=0.061021, test/mean_average_precision=0.085843, test/num_examples=43793, total_duration=837.988655, train/accuracy=0.987198, train/loss=0.048321, train/mean_average_precision=0.085935, validation/accuracy=0.984530, validation/loss=0.057637, validation/mean_average_precision=0.087212, validation/num_examples=43793
I0205 15:16:45.081690 140283262007040 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.2664279341697693, loss=0.05724143236875534
I0205 15:17:17.546780 140266968643328 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.3022077679634094, loss=0.0502440519630909
I0205 15:17:49.258986 140283262007040 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.17959517240524292, loss=0.047968681901693344
I0205 15:18:21.129391 140266968643328 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.10014963150024414, loss=0.05022774264216423
I0205 15:18:52.500235 140283262007040 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.08796598762273788, loss=0.05078839883208275
I0205 15:19:24.406684 140266968643328 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.12183870375156403, loss=0.04525522142648697
I0205 15:19:55.931918 140283262007040 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.24418488144874573, loss=0.04629741981625557
I0205 15:20:28.888108 140266968643328 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.1463705450296402, loss=0.04553384333848953
I0205 15:20:44.536488 140451058161472 spec.py:321] Evaluating on the training split.
I0205 15:22:38.710549 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 15:22:42.151484 140451058161472 spec.py:349] Evaluating on the test split.
I0205 15:22:45.516517 140451058161472 submission_runner.py:408] Time since start: 1199.12s, 	Step: 2248, 	{'train/accuracy': 0.9878924489021301, 'train/loss': 0.04351016506552696, 'train/mean_average_precision': 0.1318084182003098, 'validation/accuracy': 0.9850869178771973, 'validation/loss': 0.053109265863895416, 'validation/mean_average_precision': 0.1295160089173986, 'validation/num_examples': 43793, 'test/accuracy': 0.9840531349182129, 'test/loss': 0.0561034195125103, 'test/mean_average_precision': 0.13673173042026274, 'test/num_examples': 43793, 'score': 732.3831737041473, 'total_duration': 1199.1216876506805, 'accumulated_submission_time': 732.3831737041473, 'accumulated_eval_time': 466.60114884376526, 'accumulated_logging_time': 0.07543468475341797}
I0205 15:22:45.533865 140248415348480 logging_writer.py:48] [2248] accumulated_eval_time=466.601149, accumulated_logging_time=0.075435, accumulated_submission_time=732.383174, global_step=2248, preemption_count=0, score=732.383174, test/accuracy=0.984053, test/loss=0.056103, test/mean_average_precision=0.136732, test/num_examples=43793, total_duration=1199.121688, train/accuracy=0.987892, train/loss=0.043510, train/mean_average_precision=0.131808, validation/accuracy=0.985087, validation/loss=0.053109, validation/mean_average_precision=0.129516, validation/num_examples=43793
I0205 15:23:02.987405 140283530442496 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.0993487685918808, loss=0.04188650846481323
I0205 15:23:35.322109 140248415348480 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.08451622724533081, loss=0.044854212552309036
I0205 15:24:07.943281 140283530442496 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.18704475462436676, loss=0.04669507220387459
I0205 15:24:39.301225 140248415348480 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.09026503562927246, loss=0.0396859347820282
I0205 15:25:11.079294 140283530442496 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.13227061927318573, loss=0.04605916142463684
I0205 15:25:43.081678 140248415348480 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.10641513019800186, loss=0.04502009227871895
I0205 15:26:14.969685 140283530442496 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.11909341812133789, loss=0.04157077521085739
I0205 15:26:45.516598 140451058161472 spec.py:321] Evaluating on the training split.
I0205 15:28:28.071964 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 15:28:31.077106 140451058161472 spec.py:349] Evaluating on the test split.
I0205 15:28:34.025797 140451058161472 submission_runner.py:408] Time since start: 1547.63s, 	Step: 2997, 	{'train/accuracy': 0.9880146980285645, 'train/loss': 0.0421840101480484, 'train/mean_average_precision': 0.1713988685245202, 'validation/accuracy': 0.9852290153503418, 'validation/loss': 0.05124799907207489, 'validation/mean_average_precision': 0.15495950949841802, 'validation/num_examples': 43793, 'test/accuracy': 0.9842986464500427, 'test/loss': 0.05396828427910805, 'test/mean_average_precision': 0.1536918482128193, 'test/num_examples': 43793, 'score': 972.3318812847137, 'total_duration': 1547.6310951709747, 'accumulated_submission_time': 972.3318812847137, 'accumulated_eval_time': 575.1102995872498, 'accumulated_logging_time': 0.10525918006896973}
I0205 15:28:34.041474 140266960250624 logging_writer.py:48] [2997] accumulated_eval_time=575.110300, accumulated_logging_time=0.105259, accumulated_submission_time=972.331881, global_step=2997, preemption_count=0, score=972.331881, test/accuracy=0.984299, test/loss=0.053968, test/mean_average_precision=0.153692, test/num_examples=43793, total_duration=1547.631095, train/accuracy=0.988015, train/loss=0.042184, train/mean_average_precision=0.171399, validation/accuracy=0.985229, validation/loss=0.051248, validation/mean_average_precision=0.154960, validation/num_examples=43793
I0205 15:28:35.313973 140266968643328 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.0593985877931118, loss=0.03824162110686302
I0205 15:29:06.925631 140266960250624 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.0884992927312851, loss=0.044341642409563065
I0205 15:29:39.166425 140266968643328 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.08163880556821823, loss=0.03878369554877281
I0205 15:30:11.278552 140266960250624 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.07707485556602478, loss=0.04100784659385681
I0205 15:30:43.219432 140266968643328 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.12794817984104156, loss=0.0382537804543972
I0205 15:31:15.114413 140266960250624 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.07550827413797379, loss=0.04455408453941345
I0205 15:31:46.835218 140266968643328 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.143336221575737, loss=0.04283009096980095
I0205 15:32:18.534782 140266960250624 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.10270188003778458, loss=0.03788788616657257
I0205 15:32:34.116111 140451058161472 spec.py:321] Evaluating on the training split.
I0205 15:34:22.576536 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 15:34:25.621277 140451058161472 spec.py:349] Evaluating on the test split.
I0205 15:34:28.594672 140451058161472 submission_runner.py:408] Time since start: 1902.20s, 	Step: 3750, 	{'train/accuracy': 0.9884123802185059, 'train/loss': 0.04051417484879494, 'train/mean_average_precision': 0.17195161098369188, 'validation/accuracy': 0.9853410124778748, 'validation/loss': 0.050396572798490524, 'validation/mean_average_precision': 0.1698219331341683, 'validation/num_examples': 43793, 'test/accuracy': 0.9845029711723328, 'test/loss': 0.05332661792635918, 'test/mean_average_precision': 0.16829727798683972, 'test/num_examples': 43793, 'score': 1212.3737390041351, 'total_duration': 1902.1999762058258, 'accumulated_submission_time': 1212.3737390041351, 'accumulated_eval_time': 689.58882188797, 'accumulated_logging_time': 0.1334693431854248}
I0205 15:34:28.611994 140248415348480 logging_writer.py:48] [3750] accumulated_eval_time=689.588822, accumulated_logging_time=0.133469, accumulated_submission_time=1212.373739, global_step=3750, preemption_count=0, score=1212.373739, test/accuracy=0.984503, test/loss=0.053327, test/mean_average_precision=0.168297, test/num_examples=43793, total_duration=1902.199976, train/accuracy=0.988412, train/loss=0.040514, train/mean_average_precision=0.171952, validation/accuracy=0.985341, validation/loss=0.050397, validation/mean_average_precision=0.169822, validation/num_examples=43793
I0205 15:34:44.859013 140283262007040 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.06476213037967682, loss=0.03866513818502426
I0205 15:35:16.555691 140248415348480 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.12148536741733551, loss=0.04153880104422569
I0205 15:35:47.940598 140283262007040 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.0675908625125885, loss=0.042332932353019714
I0205 15:36:19.961856 140248415348480 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.06940226256847382, loss=0.03651962801814079
I0205 15:36:51.920380 140283262007040 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.14266683161258698, loss=0.03987988457083702
I0205 15:37:23.919727 140248415348480 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.04691513627767563, loss=0.03947696089744568
I0205 15:37:56.296066 140283262007040 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.05757581442594528, loss=0.040199220180511475
I0205 15:38:28.109978 140248415348480 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.044264692813158035, loss=0.034529827535152435
I0205 15:38:28.743750 140451058161472 spec.py:321] Evaluating on the training split.
I0205 15:40:21.206294 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 15:40:24.254102 140451058161472 spec.py:349] Evaluating on the test split.
I0205 15:40:27.230321 140451058161472 submission_runner.py:408] Time since start: 2260.84s, 	Step: 4503, 	{'train/accuracy': 0.9884085059165955, 'train/loss': 0.04010861739516258, 'train/mean_average_precision': 0.2116189163327401, 'validation/accuracy': 0.9854055643081665, 'validation/loss': 0.049385201185941696, 'validation/mean_average_precision': 0.18087929617954912, 'validation/num_examples': 43793, 'test/accuracy': 0.9845539331436157, 'test/loss': 0.05209474638104439, 'test/mean_average_precision': 0.17693838182309646, 'test/num_examples': 43793, 'score': 1452.4729554653168, 'total_duration': 2260.835620880127, 'accumulated_submission_time': 1452.4729554653168, 'accumulated_eval_time': 808.075345993042, 'accumulated_logging_time': 0.16302108764648438}
I0205 15:40:27.246855 140266968643328 logging_writer.py:48] [4503] accumulated_eval_time=808.075346, accumulated_logging_time=0.163021, accumulated_submission_time=1452.472955, global_step=4503, preemption_count=0, score=1452.472955, test/accuracy=0.984554, test/loss=0.052095, test/mean_average_precision=0.176938, test/num_examples=43793, total_duration=2260.835621, train/accuracy=0.988409, train/loss=0.040109, train/mean_average_precision=0.211619, validation/accuracy=0.985406, validation/loss=0.049385, validation/mean_average_precision=0.180879, validation/num_examples=43793
I0205 15:40:58.644424 140283530442496 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.05642688274383545, loss=0.04037037864327431
I0205 15:41:30.930258 140266968643328 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.11499166488647461, loss=0.04685813933610916
I0205 15:42:03.600401 140283530442496 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.059208959341049194, loss=0.03375180438160896
I0205 15:42:35.380740 140266968643328 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.0618717297911644, loss=0.03936467692255974
I0205 15:43:07.273335 140283530442496 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.033192090690135956, loss=0.03668986260890961
I0205 15:43:38.877631 140266968643328 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.0617535226047039, loss=0.03678750991821289
I0205 15:44:10.797877 140283530442496 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.038576025515794754, loss=0.037744563072919846
I0205 15:44:27.314460 140451058161472 spec.py:321] Evaluating on the training split.
I0205 15:46:17.138983 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 15:46:20.668107 140451058161472 spec.py:349] Evaluating on the test split.
I0205 15:46:24.010942 140451058161472 submission_runner.py:408] Time since start: 2617.62s, 	Step: 5253, 	{'train/accuracy': 0.9887412786483765, 'train/loss': 0.038450874388217926, 'train/mean_average_precision': 0.2200065332386008, 'validation/accuracy': 0.9858241081237793, 'validation/loss': 0.04824988916516304, 'validation/mean_average_precision': 0.19572114582471295, 'validation/num_examples': 43793, 'test/accuracy': 0.9849397540092468, 'test/loss': 0.050983134657144547, 'test/mean_average_precision': 0.19612095671217766, 'test/num_examples': 43793, 'score': 1692.5093190670013, 'total_duration': 2617.616218805313, 'accumulated_submission_time': 1692.5093190670013, 'accumulated_eval_time': 924.7717523574829, 'accumulated_logging_time': 0.190324068069458}
I0205 15:46:24.028729 140248415348480 logging_writer.py:48] [5253] accumulated_eval_time=924.771752, accumulated_logging_time=0.190324, accumulated_submission_time=1692.509319, global_step=5253, preemption_count=0, score=1692.509319, test/accuracy=0.984940, test/loss=0.050983, test/mean_average_precision=0.196121, test/num_examples=43793, total_duration=2617.616219, train/accuracy=0.988741, train/loss=0.038451, train/mean_average_precision=0.220007, validation/accuracy=0.985824, validation/loss=0.048250, validation/mean_average_precision=0.195721, validation/num_examples=43793
I0205 15:46:39.707045 140266960250624 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.06978901475667953, loss=0.03798267990350723
I0205 15:47:12.440200 140248415348480 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.07864340394735336, loss=0.03895105794072151
I0205 15:47:44.560363 140266960250624 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.09193181246519089, loss=0.04009834676980972
I0205 15:48:16.487359 140248415348480 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.0788947269320488, loss=0.03730408102273941
I0205 15:48:48.051726 140266960250624 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.07031622529029846, loss=0.040654800832271576
I0205 15:49:19.865458 140248415348480 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.04235111176967621, loss=0.036421000957489014
I0205 15:49:51.568219 140266960250624 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.05087054520845413, loss=0.0365988090634346
I0205 15:50:23.400717 140248415348480 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.0676109716296196, loss=0.03607773780822754
I0205 15:50:24.028328 140451058161472 spec.py:321] Evaluating on the training split.
I0205 15:52:07.635328 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 15:52:12.549681 140451058161472 spec.py:349] Evaluating on the test split.
I0205 15:52:15.523647 140451058161472 submission_runner.py:408] Time since start: 2969.13s, 	Step: 6003, 	{'train/accuracy': 0.9889976382255554, 'train/loss': 0.03763619810342789, 'train/mean_average_precision': 0.2488204824404136, 'validation/accuracy': 0.9858983755111694, 'validation/loss': 0.047921206802129745, 'validation/mean_average_precision': 0.20578790123543542, 'validation/num_examples': 43793, 'test/accuracy': 0.9850517511367798, 'test/loss': 0.05053393170237541, 'test/mean_average_precision': 0.21110773477493158, 'test/num_examples': 43793, 'score': 1932.4763493537903, 'total_duration': 2969.128954410553, 'accumulated_submission_time': 1932.4763493537903, 'accumulated_eval_time': 1036.267024755478, 'accumulated_logging_time': 0.21976399421691895}
I0205 15:52:15.545624 140266968643328 logging_writer.py:48] [6003] accumulated_eval_time=1036.267025, accumulated_logging_time=0.219764, accumulated_submission_time=1932.476349, global_step=6003, preemption_count=0, score=1932.476349, test/accuracy=0.985052, test/loss=0.050534, test/mean_average_precision=0.211108, test/num_examples=43793, total_duration=2969.128954, train/accuracy=0.988998, train/loss=0.037636, train/mean_average_precision=0.248820, validation/accuracy=0.985898, validation/loss=0.047921, validation/mean_average_precision=0.205788, validation/num_examples=43793
I0205 15:52:46.540063 140388950066944 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.04153847694396973, loss=0.03764327988028526
I0205 15:53:19.380084 140266968643328 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.05451071634888649, loss=0.04006269946694374
I0205 15:53:51.735604 140388950066944 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.0387205146253109, loss=0.035960473120212555
I0205 15:54:23.913755 140266968643328 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.08844911307096481, loss=0.035577405244112015
I0205 15:54:55.479411 140388950066944 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.0392906479537487, loss=0.038600314408540726
I0205 15:55:27.137240 140266968643328 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.15978100895881653, loss=0.03997962176799774
I0205 15:55:58.598415 140388950066944 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.03835213929414749, loss=0.03410416841506958
I0205 15:56:15.653446 140451058161472 spec.py:321] Evaluating on the training split.
I0205 15:58:03.037598 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 15:58:06.016698 140451058161472 spec.py:349] Evaluating on the test split.
I0205 15:58:08.936096 140451058161472 submission_runner.py:408] Time since start: 3322.54s, 	Step: 6755, 	{'train/accuracy': 0.9892213940620422, 'train/loss': 0.03666960075497627, 'train/mean_average_precision': 0.26341637862221823, 'validation/accuracy': 0.9860619902610779, 'validation/loss': 0.046762146055698395, 'validation/mean_average_precision': 0.2146012390955433, 'validation/num_examples': 43793, 'test/accuracy': 0.9851566553115845, 'test/loss': 0.0493510365486145, 'test/mean_average_precision': 0.22187402768276412, 'test/num_examples': 43793, 'score': 2172.550704717636, 'total_duration': 3322.5414004325867, 'accumulated_submission_time': 2172.550704717636, 'accumulated_eval_time': 1149.54962849617, 'accumulated_logging_time': 0.2551395893096924}
I0205 15:58:08.952383 140283262007040 logging_writer.py:48] [6755] accumulated_eval_time=1149.549628, accumulated_logging_time=0.255140, accumulated_submission_time=2172.550705, global_step=6755, preemption_count=0, score=2172.550705, test/accuracy=0.985157, test/loss=0.049351, test/mean_average_precision=0.221874, test/num_examples=43793, total_duration=3322.541400, train/accuracy=0.989221, train/loss=0.036670, train/mean_average_precision=0.263416, validation/accuracy=0.986062, validation/loss=0.046762, validation/mean_average_precision=0.214601, validation/num_examples=43793
I0205 15:58:23.472572 140290204940032 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.02802346833050251, loss=0.03542204946279526
I0205 15:58:54.871602 140283262007040 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.03380614519119263, loss=0.03919530287384987
I0205 15:59:26.558364 140290204940032 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.02832832559943199, loss=0.03592991083860397
I0205 15:59:58.101902 140283262007040 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.03468213602900505, loss=0.03810173645615578
I0205 16:00:29.722353 140290204940032 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.051441505551338196, loss=0.03700965642929077
I0205 16:01:01.381219 140283262007040 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.03686903044581413, loss=0.04161160811781883
I0205 16:01:32.650889 140290204940032 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.03964469954371452, loss=0.038187168538570404
I0205 16:02:04.419918 140283262007040 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.02959134429693222, loss=0.036055851727724075
I0205 16:02:09.172805 140451058161472 spec.py:321] Evaluating on the training split.
I0205 16:04:00.551025 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 16:04:03.934645 140451058161472 spec.py:349] Evaluating on the test split.
I0205 16:04:06.907536 140451058161472 submission_runner.py:408] Time since start: 3680.51s, 	Step: 7516, 	{'train/accuracy': 0.9892968535423279, 'train/loss': 0.03595465421676636, 'train/mean_average_precision': 0.27810517075809804, 'validation/accuracy': 0.9860575199127197, 'validation/loss': 0.04683062434196472, 'validation/mean_average_precision': 0.21252932044261078, 'validation/num_examples': 43793, 'test/accuracy': 0.9853032231330872, 'test/loss': 0.04935446009039879, 'test/mean_average_precision': 0.2166553914108559, 'test/num_examples': 43793, 'score': 2412.738084077835, 'total_duration': 3680.512838125229, 'accumulated_submission_time': 2412.738084077835, 'accumulated_eval_time': 1267.2843120098114, 'accumulated_logging_time': 0.2840569019317627}
I0205 16:04:06.924323 140266960250624 logging_writer.py:48] [7516] accumulated_eval_time=1267.284312, accumulated_logging_time=0.284057, accumulated_submission_time=2412.738084, global_step=7516, preemption_count=0, score=2412.738084, test/accuracy=0.985303, test/loss=0.049354, test/mean_average_precision=0.216655, test/num_examples=43793, total_duration=3680.512838, train/accuracy=0.989297, train/loss=0.035955, train/mean_average_precision=0.278105, validation/accuracy=0.986058, validation/loss=0.046831, validation/mean_average_precision=0.212529, validation/num_examples=43793
I0205 16:04:33.944787 140283530442496 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.036217235028743744, loss=0.03803379461169243
I0205 16:05:05.822325 140266960250624 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.03493823856115341, loss=0.03850255534052849
I0205 16:05:37.222134 140283530442496 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.040455881506204605, loss=0.037976235151290894
I0205 16:06:09.076466 140266960250624 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.05070428550243378, loss=0.033576954156160355
I0205 16:06:40.826140 140283530442496 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.02693648263812065, loss=0.03398679941892624
I0205 16:07:12.718208 140266960250624 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.036686647683382034, loss=0.03990557789802551
I0205 16:07:44.268764 140283530442496 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.029528604820370674, loss=0.03581684082746506
I0205 16:08:07.061454 140451058161472 spec.py:321] Evaluating on the training split.
I0205 16:09:51.164960 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 16:09:54.214392 140451058161472 spec.py:349] Evaluating on the test split.
I0205 16:09:57.206856 140451058161472 submission_runner.py:408] Time since start: 4030.81s, 	Step: 8273, 	{'train/accuracy': 0.9894575476646423, 'train/loss': 0.03549367934465408, 'train/mean_average_precision': 0.280584196580796, 'validation/accuracy': 0.9862965941429138, 'validation/loss': 0.046224649995565414, 'validation/mean_average_precision': 0.22483497796773647, 'validation/num_examples': 43793, 'test/accuracy': 0.9853959083557129, 'test/loss': 0.0489659383893013, 'test/mean_average_precision': 0.2248276800619779, 'test/num_examples': 43793, 'score': 2652.844264984131, 'total_duration': 4030.8121387958527, 'accumulated_submission_time': 2652.844264984131, 'accumulated_eval_time': 1377.429646730423, 'accumulated_logging_time': 0.31173205375671387}
I0205 16:09:57.223798 140283262007040 logging_writer.py:48] [8273] accumulated_eval_time=1377.429647, accumulated_logging_time=0.311732, accumulated_submission_time=2652.844265, global_step=8273, preemption_count=0, score=2652.844265, test/accuracy=0.985396, test/loss=0.048966, test/mean_average_precision=0.224828, test/num_examples=43793, total_duration=4030.812139, train/accuracy=0.989458, train/loss=0.035494, train/mean_average_precision=0.280584, validation/accuracy=0.986297, validation/loss=0.046225, validation/mean_average_precision=0.224835, validation/num_examples=43793
I0205 16:10:06.256580 140290204940032 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.0307066161185503, loss=0.032958630472421646
I0205 16:10:38.118848 140283262007040 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.024898914620280266, loss=0.03476389870047569
I0205 16:11:10.215766 140290204940032 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.03414252772927284, loss=0.03692057356238365
I0205 16:11:42.357538 140283262007040 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.024388067424297333, loss=0.033744558691978455
I0205 16:12:14.605644 140290204940032 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.032131828367710114, loss=0.0358634889125824
I0205 16:12:46.346661 140283262007040 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.030580349266529083, loss=0.03561899811029434
I0205 16:13:18.315422 140290204940032 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.029497526586055756, loss=0.03982491418719292
I0205 16:13:50.006113 140283262007040 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.046747349202632904, loss=0.038691964000463486
I0205 16:13:57.308983 140451058161472 spec.py:321] Evaluating on the training split.
I0205 16:15:43.180100 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 16:15:46.269446 140451058161472 spec.py:349] Evaluating on the test split.
I0205 16:15:49.367958 140451058161472 submission_runner.py:408] Time since start: 4382.97s, 	Step: 9024, 	{'train/accuracy': 0.9894021153450012, 'train/loss': 0.0356401763856411, 'train/mean_average_precision': 0.2816182237666848, 'validation/accuracy': 0.9863436818122864, 'validation/loss': 0.045850545167922974, 'validation/mean_average_precision': 0.22994952997389329, 'validation/num_examples': 43793, 'test/accuracy': 0.9855276942253113, 'test/loss': 0.048610664904117584, 'test/mean_average_precision': 0.23025746755184726, 'test/num_examples': 43793, 'score': 2892.8968245983124, 'total_duration': 4382.973264217377, 'accumulated_submission_time': 2892.8968245983124, 'accumulated_eval_time': 1489.488579750061, 'accumulated_logging_time': 0.3412954807281494}
I0205 16:15:49.384771 140266968643328 logging_writer.py:48] [9024] accumulated_eval_time=1489.488580, accumulated_logging_time=0.341295, accumulated_submission_time=2892.896825, global_step=9024, preemption_count=0, score=2892.896825, test/accuracy=0.985528, test/loss=0.048611, test/mean_average_precision=0.230257, test/num_examples=43793, total_duration=4382.973264, train/accuracy=0.989402, train/loss=0.035640, train/mean_average_precision=0.281618, validation/accuracy=0.986344, validation/loss=0.045851, validation/mean_average_precision=0.229950, validation/num_examples=43793
I0205 16:16:14.082380 140283530442496 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.04085078462958336, loss=0.03486967086791992
I0205 16:16:46.079022 140266968643328 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.032166119664907455, loss=0.031904589384794235
I0205 16:17:18.376239 140283530442496 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.025859447196125984, loss=0.035444971174001694
I0205 16:17:50.000250 140266968643328 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.05087671056389809, loss=0.035673633217811584
I0205 16:18:22.014987 140283530442496 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.033457908779382706, loss=0.034142982214689255
I0205 16:18:53.892848 140266968643328 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.032468460500240326, loss=0.038826026022434235
I0205 16:19:25.647616 140283530442496 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.03618832305073738, loss=0.03365582227706909
I0205 16:19:49.424067 140451058161472 spec.py:321] Evaluating on the training split.
I0205 16:21:36.209126 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 16:21:39.575414 140451058161472 spec.py:349] Evaluating on the test split.
I0205 16:21:42.785870 140451058161472 submission_runner.py:408] Time since start: 4736.39s, 	Step: 9776, 	{'train/accuracy': 0.989449679851532, 'train/loss': 0.035480014979839325, 'train/mean_average_precision': 0.2902229885165782, 'validation/accuracy': 0.9861695766448975, 'validation/loss': 0.04601406678557396, 'validation/mean_average_precision': 0.22814390693573108, 'validation/num_examples': 43793, 'test/accuracy': 0.985284686088562, 'test/loss': 0.0488116517663002, 'test/mean_average_precision': 0.22754404799916472, 'test/num_examples': 43793, 'score': 3132.904645681381, 'total_duration': 4736.391147851944, 'accumulated_submission_time': 3132.904645681381, 'accumulated_eval_time': 1602.8503172397614, 'accumulated_logging_time': 0.36899900436401367}
I0205 16:21:42.805908 140266960250624 logging_writer.py:48] [9776] accumulated_eval_time=1602.850317, accumulated_logging_time=0.368999, accumulated_submission_time=3132.904646, global_step=9776, preemption_count=0, score=3132.904646, test/accuracy=0.985285, test/loss=0.048812, test/mean_average_precision=0.227544, test/num_examples=43793, total_duration=4736.391148, train/accuracy=0.989450, train/loss=0.035480, train/mean_average_precision=0.290223, validation/accuracy=0.986170, validation/loss=0.046014, validation/mean_average_precision=0.228144, validation/num_examples=43793
I0205 16:21:50.955954 140290204940032 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.044925980269908905, loss=0.03540511429309845
I0205 16:22:22.984123 140266960250624 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.058550313115119934, loss=0.03795721381902695
I0205 16:22:54.773114 140290204940032 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.029795357957482338, loss=0.03369980305433273
I0205 16:23:26.549290 140266960250624 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.030292028561234474, loss=0.031270235776901245
I0205 16:23:58.015868 140290204940032 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.03336929902434349, loss=0.03198500722646713
I0205 16:24:29.637653 140266960250624 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.030211228877305984, loss=0.035597775131464005
I0205 16:25:01.196666 140290204940032 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.03367723897099495, loss=0.03256732597947121
I0205 16:25:33.968762 140266960250624 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.029636526480317116, loss=0.035028230398893356
I0205 16:25:43.084007 140451058161472 spec.py:321] Evaluating on the training split.
I0205 16:27:31.600112 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 16:27:36.535168 140451058161472 spec.py:349] Evaluating on the test split.
I0205 16:27:39.570657 140451058161472 submission_runner.py:408] Time since start: 5093.18s, 	Step: 10529, 	{'train/accuracy': 0.9898571372032166, 'train/loss': 0.0340229794383049, 'train/mean_average_precision': 0.31486855756027643, 'validation/accuracy': 0.9864293336868286, 'validation/loss': 0.04547018185257912, 'validation/mean_average_precision': 0.2401039852908579, 'validation/num_examples': 43793, 'test/accuracy': 0.985558032989502, 'test/loss': 0.048242755234241486, 'test/mean_average_precision': 0.23295535644020104, 'test/num_examples': 43793, 'score': 3373.1482582092285, 'total_duration': 5093.1759622097015, 'accumulated_submission_time': 3373.1482582092285, 'accumulated_eval_time': 1719.3369302749634, 'accumulated_logging_time': 0.4020838737487793}
I0205 16:27:39.588483 140266968643328 logging_writer.py:48] [10529] accumulated_eval_time=1719.336930, accumulated_logging_time=0.402084, accumulated_submission_time=3373.148258, global_step=10529, preemption_count=0, score=3373.148258, test/accuracy=0.985558, test/loss=0.048243, test/mean_average_precision=0.232955, test/num_examples=43793, total_duration=5093.175962, train/accuracy=0.989857, train/loss=0.034023, train/mean_average_precision=0.314869, validation/accuracy=0.986429, validation/loss=0.045470, validation/mean_average_precision=0.240104, validation/num_examples=43793
I0205 16:28:02.945707 140283262007040 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.026613721624016762, loss=0.030955150723457336
I0205 16:28:34.483301 140266968643328 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.04033253714442253, loss=0.03506684675812721
I0205 16:29:06.387634 140283262007040 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.03327997773885727, loss=0.03239875286817551
I0205 16:29:38.101522 140266968643328 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.04453613609075546, loss=0.036034561693668365
I0205 16:30:10.114248 140283262007040 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.03165649622678757, loss=0.03445425257086754
I0205 16:30:41.761712 140266968643328 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.039132606238126755, loss=0.03525659814476967
I0205 16:31:13.648583 140283262007040 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.03549859672784805, loss=0.0366847962141037
I0205 16:31:39.705575 140451058161472 spec.py:321] Evaluating on the training split.
I0205 16:33:22.796864 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 16:33:25.960318 140451058161472 spec.py:349] Evaluating on the test split.
I0205 16:33:29.026669 140451058161472 submission_runner.py:408] Time since start: 5442.63s, 	Step: 11283, 	{'train/accuracy': 0.9897293448448181, 'train/loss': 0.03409227356314659, 'train/mean_average_precision': 0.31628149151282303, 'validation/accuracy': 0.9861927032470703, 'validation/loss': 0.045579321682453156, 'validation/mean_average_precision': 0.2445057253378528, 'validation/num_examples': 43793, 'test/accuracy': 0.9854072332382202, 'test/loss': 0.048310309648513794, 'test/mean_average_precision': 0.23691313713075418, 'test/num_examples': 43793, 'score': 3613.232980489731, 'total_duration': 5442.6319761276245, 'accumulated_submission_time': 3613.232980489731, 'accumulated_eval_time': 1828.657984495163, 'accumulated_logging_time': 0.43224501609802246}
I0205 16:33:29.043893 140283530442496 logging_writer.py:48] [11283] accumulated_eval_time=1828.657984, accumulated_logging_time=0.432245, accumulated_submission_time=3613.232980, global_step=11283, preemption_count=0, score=3613.232980, test/accuracy=0.985407, test/loss=0.048310, test/mean_average_precision=0.236913, test/num_examples=43793, total_duration=5442.631976, train/accuracy=0.989729, train/loss=0.034092, train/mean_average_precision=0.316281, validation/accuracy=0.986193, validation/loss=0.045579, validation/mean_average_precision=0.244506, validation/num_examples=43793
I0205 16:33:34.876428 140290204940032 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.04638389125466347, loss=0.03350969776511192
I0205 16:34:06.905820 140283530442496 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.03651003539562225, loss=0.03261975944042206
I0205 16:34:38.310700 140290204940032 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.048639457672834396, loss=0.035530176013708115
I0205 16:35:09.986500 140283530442496 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.03386349976062775, loss=0.03496572747826576
I0205 16:35:41.384075 140290204940032 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.03274986147880554, loss=0.033895596861839294
I0205 16:36:13.079497 140283530442496 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.053227782249450684, loss=0.033216726034879684
I0205 16:36:44.209771 140290204940032 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.05018538609147072, loss=0.03378134220838547
I0205 16:37:15.801195 140283530442496 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.0326584130525589, loss=0.03184480965137482
I0205 16:37:29.084459 140451058161472 spec.py:321] Evaluating on the training split.
I0205 16:39:15.985747 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 16:39:20.891987 140451058161472 spec.py:349] Evaluating on the test split.
I0205 16:39:23.917133 140451058161472 submission_runner.py:408] Time since start: 5797.52s, 	Step: 12043, 	{'train/accuracy': 0.9900323748588562, 'train/loss': 0.03316228464245796, 'train/mean_average_precision': 0.34131822994914207, 'validation/accuracy': 0.9864557385444641, 'validation/loss': 0.045129720121622086, 'validation/mean_average_precision': 0.24222951530329723, 'validation/num_examples': 43793, 'test/accuracy': 0.985649049282074, 'test/loss': 0.04773968458175659, 'test/mean_average_precision': 0.24178979968372194, 'test/num_examples': 43793, 'score': 3853.240893602371, 'total_duration': 5797.5224277973175, 'accumulated_submission_time': 3853.240893602371, 'accumulated_eval_time': 1943.4906041622162, 'accumulated_logging_time': 0.46197009086608887}
I0205 16:39:23.934719 140266960250624 logging_writer.py:48] [12043] accumulated_eval_time=1943.490604, accumulated_logging_time=0.461970, accumulated_submission_time=3853.240894, global_step=12043, preemption_count=0, score=3853.240894, test/accuracy=0.985649, test/loss=0.047740, test/mean_average_precision=0.241790, test/num_examples=43793, total_duration=5797.522428, train/accuracy=0.990032, train/loss=0.033162, train/mean_average_precision=0.341318, validation/accuracy=0.986456, validation/loss=0.045130, validation/mean_average_precision=0.242230, validation/num_examples=43793
I0205 16:39:42.862374 140283262007040 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.04509727656841278, loss=0.03310557082295418
I0205 16:40:14.989324 140266960250624 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.05145081877708435, loss=0.03356526792049408
I0205 16:40:46.699013 140283262007040 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.05335579067468643, loss=0.03252876177430153
I0205 16:41:18.516157 140266960250624 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.04191537946462631, loss=0.0336688794195652
I0205 16:41:51.129980 140283262007040 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.046439118683338165, loss=0.030820181593298912
I0205 16:42:23.045182 140266960250624 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.03689883276820183, loss=0.034337807446718216
I0205 16:42:55.010403 140283262007040 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.03421089053153992, loss=0.034607384353876114
I0205 16:43:24.007772 140451058161472 spec.py:321] Evaluating on the training split.
I0205 16:45:09.920360 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 16:45:12.961396 140451058161472 spec.py:349] Evaluating on the test split.
I0205 16:45:15.974133 140451058161472 submission_runner.py:408] Time since start: 6149.58s, 	Step: 12792, 	{'train/accuracy': 0.9902786612510681, 'train/loss': 0.03209632635116577, 'train/mean_average_precision': 0.3793567029038292, 'validation/accuracy': 0.9866027235984802, 'validation/loss': 0.04466933012008667, 'validation/mean_average_precision': 0.2507951353642788, 'validation/num_examples': 43793, 'test/accuracy': 0.9857446551322937, 'test/loss': 0.04757782816886902, 'test/mean_average_precision': 0.24555612273623034, 'test/num_examples': 43793, 'score': 4092.884313106537, 'total_duration': 6149.5794270038605, 'accumulated_submission_time': 4092.884313106537, 'accumulated_eval_time': 2055.456913471222, 'accumulated_logging_time': 0.8892319202423096}
I0205 16:45:15.992245 140283530442496 logging_writer.py:48] [12792] accumulated_eval_time=2055.456913, accumulated_logging_time=0.889232, accumulated_submission_time=4092.884313, global_step=12792, preemption_count=0, score=4092.884313, test/accuracy=0.985745, test/loss=0.047578, test/mean_average_precision=0.245556, test/num_examples=43793, total_duration=6149.579427, train/accuracy=0.990279, train/loss=0.032096, train/mean_average_precision=0.379357, validation/accuracy=0.986603, validation/loss=0.044669, validation/mean_average_precision=0.250795, validation/num_examples=43793
I0205 16:45:18.895838 140290204940032 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.0523456335067749, loss=0.03395429626107216
I0205 16:45:51.170896 140283530442496 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.038049228489398956, loss=0.032270289957523346
I0205 16:46:23.290757 140290204940032 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.03441895917057991, loss=0.028723135590553284
I0205 16:46:55.154774 140283530442496 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.04917991906404495, loss=0.03139254078269005
I0205 16:47:27.284558 140290204940032 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.038384296000003815, loss=0.029348867014050484
I0205 16:47:59.193391 140283530442496 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.03984226658940315, loss=0.030900150537490845
I0205 16:48:31.618328 140290204940032 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.04323120415210724, loss=0.030964141711592674
I0205 16:49:03.595515 140283530442496 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.043231409043073654, loss=0.030659634619951248
I0205 16:49:16.218675 140451058161472 spec.py:321] Evaluating on the training split.
I0205 16:50:59.108864 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 16:51:02.344993 140451058161472 spec.py:349] Evaluating on the test split.
I0205 16:51:07.203085 140451058161472 submission_runner.py:408] Time since start: 6500.81s, 	Step: 13540, 	{'train/accuracy': 0.9903260469436646, 'train/loss': 0.0318269245326519, 'train/mean_average_precision': 0.3630988382990801, 'validation/accuracy': 0.986552357673645, 'validation/loss': 0.04456004500389099, 'validation/mean_average_precision': 0.25936007800393945, 'validation/num_examples': 43793, 'test/accuracy': 0.9855934381484985, 'test/loss': 0.04757139831781387, 'test/mean_average_precision': 0.24682646676603034, 'test/num_examples': 43793, 'score': 4333.080197811127, 'total_duration': 6500.808378696442, 'accumulated_submission_time': 4333.080197811127, 'accumulated_eval_time': 2166.441266775131, 'accumulated_logging_time': 0.9181814193725586}
I0205 16:51:07.221597 140266960250624 logging_writer.py:48] [13540] accumulated_eval_time=2166.441267, accumulated_logging_time=0.918181, accumulated_submission_time=4333.080198, global_step=13540, preemption_count=0, score=4333.080198, test/accuracy=0.985593, test/loss=0.047571, test/mean_average_precision=0.246826, test/num_examples=43793, total_duration=6500.808379, train/accuracy=0.990326, train/loss=0.031827, train/mean_average_precision=0.363099, validation/accuracy=0.986552, validation/loss=0.044560, validation/mean_average_precision=0.259360, validation/num_examples=43793
I0205 16:51:26.726284 140283262007040 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.04702150821685791, loss=0.032684098929166794
I0205 16:51:58.595003 140266960250624 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.038971953094005585, loss=0.030985824763774872
I0205 16:52:30.971413 140283262007040 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.05169442296028137, loss=0.028505805879831314
I0205 16:53:03.171517 140266960250624 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.04855902120471001, loss=0.031592827290296555
I0205 16:53:36.542053 140283262007040 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.045064691454172134, loss=0.0318894162774086
I0205 16:54:08.666384 140266960250624 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.042500074952840805, loss=0.03399726375937462
I0205 16:54:40.572350 140283262007040 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.0496070422232151, loss=0.03147372603416443
I0205 16:55:07.366718 140451058161472 spec.py:321] Evaluating on the training split.
I0205 16:56:54.865225 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 16:56:58.423750 140451058161472 spec.py:349] Evaluating on the test split.
I0205 16:57:01.897182 140451058161472 submission_runner.py:408] Time since start: 6855.50s, 	Step: 14285, 	{'train/accuracy': 0.990658700466156, 'train/loss': 0.03080630674958229, 'train/mean_average_precision': 0.39338222098189163, 'validation/accuracy': 0.9864890575408936, 'validation/loss': 0.0447542667388916, 'validation/mean_average_precision': 0.25668576147233724, 'validation/num_examples': 43793, 'test/accuracy': 0.9856544733047485, 'test/loss': 0.04733927175402641, 'test/mean_average_precision': 0.2469929637029501, 'test/num_examples': 43793, 'score': 4573.193847417831, 'total_duration': 6855.502467632294, 'accumulated_submission_time': 4573.193847417831, 'accumulated_eval_time': 2280.971666574478, 'accumulated_logging_time': 0.9479734897613525}
I0205 16:57:01.915386 140283530442496 logging_writer.py:48] [14285] accumulated_eval_time=2280.971667, accumulated_logging_time=0.947973, accumulated_submission_time=4573.193847, global_step=14285, preemption_count=0, score=4573.193847, test/accuracy=0.985654, test/loss=0.047339, test/mean_average_precision=0.246993, test/num_examples=43793, total_duration=6855.502468, train/accuracy=0.990659, train/loss=0.030806, train/mean_average_precision=0.393382, validation/accuracy=0.986489, validation/loss=0.044754, validation/mean_average_precision=0.256686, validation/num_examples=43793
I0205 16:57:07.011605 140290204940032 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.054102156311273575, loss=0.03329901024699211
I0205 16:57:38.856536 140283530442496 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.06673990935087204, loss=0.033112332224845886
I0205 16:58:11.139735 140290204940032 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.04704691097140312, loss=0.0292831938713789
I0205 16:58:44.196670 140283530442496 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.08140309154987335, loss=0.03249223902821541
I0205 16:59:16.893208 140290204940032 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.0509057380259037, loss=0.031111333519220352
I0205 16:59:48.857999 140283530442496 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.08062568306922913, loss=0.028205690905451775
I0205 17:00:20.731308 140290204940032 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.0681883916258812, loss=0.030681511387228966
I0205 17:00:52.666960 140283530442496 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.06062120944261551, loss=0.031525105237960815
I0205 17:01:02.049844 140451058161472 spec.py:321] Evaluating on the training split.
I0205 17:02:46.433987 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 17:02:49.617598 140451058161472 spec.py:349] Evaluating on the test split.
I0205 17:02:52.719718 140451058161472 submission_runner.py:408] Time since start: 7206.33s, 	Step: 15030, 	{'train/accuracy': 0.9908473491668701, 'train/loss': 0.03013112209737301, 'train/mean_average_precision': 0.39935623585281727, 'validation/accuracy': 0.9865454435348511, 'validation/loss': 0.04472634196281433, 'validation/mean_average_precision': 0.25874011864608715, 'validation/num_examples': 43793, 'test/accuracy': 0.9856852293014526, 'test/loss': 0.04769224673509598, 'test/mean_average_precision': 0.24728610833237274, 'test/num_examples': 43793, 'score': 4813.296638011932, 'total_duration': 7206.325021743774, 'accumulated_submission_time': 4813.296638011932, 'accumulated_eval_time': 2391.64150595665, 'accumulated_logging_time': 0.9773366451263428}
I0205 17:02:52.737994 140266968643328 logging_writer.py:48] [15030] accumulated_eval_time=2391.641506, accumulated_logging_time=0.977337, accumulated_submission_time=4813.296638, global_step=15030, preemption_count=0, score=4813.296638, test/accuracy=0.985685, test/loss=0.047692, test/mean_average_precision=0.247286, test/num_examples=43793, total_duration=7206.325022, train/accuracy=0.990847, train/loss=0.030131, train/mean_average_precision=0.399356, validation/accuracy=0.986545, validation/loss=0.044726, validation/mean_average_precision=0.258740, validation/num_examples=43793
I0205 17:03:15.662420 140283262007040 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.05186234042048454, loss=0.03034565970301628
I0205 17:03:47.546746 140266968643328 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.052472736686468124, loss=0.03220124915242195
I0205 17:04:19.716542 140283262007040 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.11372655630111694, loss=0.030752025544643402
I0205 17:04:51.791105 140266968643328 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.07179314643144608, loss=0.030212515965104103
I0205 17:05:24.342720 140283262007040 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.05509348213672638, loss=0.03191312029957771
I0205 17:05:57.288106 140266968643328 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.06392265856266022, loss=0.0329400859773159
I0205 17:06:29.602285 140283262007040 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.05852202698588371, loss=0.031114887446165085
I0205 17:06:52.887046 140451058161472 spec.py:321] Evaluating on the training split.
I0205 17:08:36.869353 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 17:08:39.922654 140451058161472 spec.py:349] Evaluating on the test split.
I0205 17:08:42.857534 140451058161472 submission_runner.py:408] Time since start: 7556.46s, 	Step: 15774, 	{'train/accuracy': 0.990815281867981, 'train/loss': 0.030350325629115105, 'train/mean_average_precision': 0.39717195598107635, 'validation/accuracy': 0.9866440892219543, 'validation/loss': 0.044659607112407684, 'validation/mean_average_precision': 0.265979384637311, 'validation/num_examples': 43793, 'test/accuracy': 0.985791802406311, 'test/loss': 0.04754490405321121, 'test/mean_average_precision': 0.2527092747568265, 'test/num_examples': 43793, 'score': 5053.413534879684, 'total_duration': 7556.462839126587, 'accumulated_submission_time': 5053.413534879684, 'accumulated_eval_time': 2501.61195063591, 'accumulated_logging_time': 1.0068624019622803}
I0205 17:08:42.875984 140266960250624 logging_writer.py:48] [15774] accumulated_eval_time=2501.611951, accumulated_logging_time=1.006862, accumulated_submission_time=5053.413535, global_step=15774, preemption_count=0, score=5053.413535, test/accuracy=0.985792, test/loss=0.047545, test/mean_average_precision=0.252709, test/num_examples=43793, total_duration=7556.462839, train/accuracy=0.990815, train/loss=0.030350, train/mean_average_precision=0.397172, validation/accuracy=0.986644, validation/loss=0.044660, validation/mean_average_precision=0.265979, validation/num_examples=43793
I0205 17:08:51.548356 140283530442496 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.05473819375038147, loss=0.031060941517353058
I0205 17:09:23.351661 140266960250624 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.057729341089725494, loss=0.03270022198557854
I0205 17:09:55.065252 140283530442496 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.06283040344715118, loss=0.03015906922519207
I0205 17:10:26.815936 140266960250624 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.059624407440423965, loss=0.029771171510219574
I0205 17:10:58.451702 140283530442496 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.07248818874359131, loss=0.032569948583841324
I0205 17:11:30.387319 140266960250624 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.06266089528799057, loss=0.03348340466618538
I0205 17:12:02.615696 140283530442496 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.06128817796707153, loss=0.03163907304406166
I0205 17:12:34.450211 140266960250624 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.12982629239559174, loss=0.03476666286587715
I0205 17:12:42.909929 140451058161472 spec.py:321] Evaluating on the training split.
I0205 17:14:22.807707 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 17:14:27.759339 140451058161472 spec.py:349] Evaluating on the test split.
I0205 17:14:30.732225 140451058161472 submission_runner.py:408] Time since start: 7904.34s, 	Step: 16528, 	{'train/accuracy': 0.9906279444694519, 'train/loss': 0.030868563801050186, 'train/mean_average_precision': 0.39136064694115513, 'validation/accuracy': 0.9864423274993896, 'validation/loss': 0.04503690078854561, 'validation/mean_average_precision': 0.2502382753996813, 'validation/num_examples': 43793, 'test/accuracy': 0.9855433106422424, 'test/loss': 0.04803470894694328, 'test/mean_average_precision': 0.24411684659740535, 'test/num_examples': 43793, 'score': 5293.414917469025, 'total_duration': 7904.33753156662, 'accumulated_submission_time': 5293.414917469025, 'accumulated_eval_time': 2609.4342000484467, 'accumulated_logging_time': 1.0378234386444092}
I0205 17:14:30.751166 140266968643328 logging_writer.py:48] [16528] accumulated_eval_time=2609.434200, accumulated_logging_time=1.037823, accumulated_submission_time=5293.414917, global_step=16528, preemption_count=0, score=5293.414917, test/accuracy=0.985543, test/loss=0.048035, test/mean_average_precision=0.244117, test/num_examples=43793, total_duration=7904.337532, train/accuracy=0.990628, train/loss=0.030869, train/mean_average_precision=0.391361, validation/accuracy=0.986442, validation/loss=0.045037, validation/mean_average_precision=0.250238, validation/num_examples=43793
I0205 17:14:54.214155 140290204940032 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.056959476321935654, loss=0.033055905252695084
I0205 17:15:26.440612 140266968643328 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.07486167550086975, loss=0.03388367220759392
I0205 17:15:58.223501 140290204940032 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.05243604630231857, loss=0.030758168548345566
I0205 17:16:30.236430 140266968643328 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.09952594339847565, loss=0.02961057238280773
I0205 17:17:02.105687 140290204940032 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.09160678833723068, loss=0.0325864814221859
I0205 17:17:33.772172 140266968643328 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.07490917295217514, loss=0.028978077694773674
I0205 17:18:05.518271 140290204940032 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.05910041928291321, loss=0.03031201846897602
I0205 17:18:30.859048 140451058161472 spec.py:321] Evaluating on the training split.
I0205 17:20:15.124449 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 17:20:18.141652 140451058161472 spec.py:349] Evaluating on the test split.
I0205 17:20:21.096369 140451058161472 submission_runner.py:408] Time since start: 8254.70s, 	Step: 17281, 	{'train/accuracy': 0.9905187487602234, 'train/loss': 0.03111601248383522, 'train/mean_average_precision': 0.3874007734505545, 'validation/accuracy': 0.9865190982818604, 'validation/loss': 0.04453900828957558, 'validation/mean_average_precision': 0.2598120879231033, 'validation/num_examples': 43793, 'test/accuracy': 0.9857496619224548, 'test/loss': 0.04720601066946983, 'test/mean_average_precision': 0.25298960599179354, 'test/num_examples': 43793, 'score': 5533.490139722824, 'total_duration': 8254.701673984528, 'accumulated_submission_time': 5533.490139722824, 'accumulated_eval_time': 2719.6714749336243, 'accumulated_logging_time': 1.0690538883209229}
I0205 17:20:21.116373 140266960250624 logging_writer.py:48] [17281] accumulated_eval_time=2719.671475, accumulated_logging_time=1.069054, accumulated_submission_time=5533.490140, global_step=17281, preemption_count=0, score=5533.490140, test/accuracy=0.985750, test/loss=0.047206, test/mean_average_precision=0.252990, test/num_examples=43793, total_duration=8254.701674, train/accuracy=0.990519, train/loss=0.031116, train/mean_average_precision=0.387401, validation/accuracy=0.986519, validation/loss=0.044539, validation/mean_average_precision=0.259812, validation/num_examples=43793
I0205 17:20:27.532346 140283262007040 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.06749697029590607, loss=0.029294703155755997
I0205 17:20:59.437532 140266960250624 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.06223943084478378, loss=0.029184026643633842
I0205 17:21:31.721236 140283262007040 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.06523787975311279, loss=0.031647730618715286
I0205 17:22:04.442348 140266960250624 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.13020741939544678, loss=0.028228189796209335
I0205 17:22:36.218601 140283262007040 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.05892544984817505, loss=0.02953210100531578
I0205 17:23:08.232498 140266960250624 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.06438744813203812, loss=0.030481042340397835
I0205 17:23:40.406164 140283262007040 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.07080074399709702, loss=0.031182734295725822
I0205 17:24:12.945511 140266960250624 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.06679936498403549, loss=0.02928275614976883
I0205 17:24:21.217972 140451058161472 spec.py:321] Evaluating on the training split.
I0205 17:26:06.924941 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 17:26:11.945190 140451058161472 spec.py:349] Evaluating on the test split.
I0205 17:26:14.956995 140451058161472 submission_runner.py:408] Time since start: 8608.56s, 	Step: 18027, 	{'train/accuracy': 0.990691065788269, 'train/loss': 0.03058709017932415, 'train/mean_average_precision': 0.3921542590010484, 'validation/accuracy': 0.9866855144500732, 'validation/loss': 0.04439421743154526, 'validation/mean_average_precision': 0.2646887457085707, 'validation/num_examples': 43793, 'test/accuracy': 0.9858617186546326, 'test/loss': 0.04723426327109337, 'test/mean_average_precision': 0.25909809108275444, 'test/num_examples': 43793, 'score': 5773.5592222213745, 'total_duration': 8608.562299251556, 'accumulated_submission_time': 5773.5592222213745, 'accumulated_eval_time': 2833.4104483127594, 'accumulated_logging_time': 1.101609468460083}
I0205 17:26:14.976101 140266968643328 logging_writer.py:48] [18027] accumulated_eval_time=2833.410448, accumulated_logging_time=1.101609, accumulated_submission_time=5773.559222, global_step=18027, preemption_count=0, score=5773.559222, test/accuracy=0.985862, test/loss=0.047234, test/mean_average_precision=0.259098, test/num_examples=43793, total_duration=8608.562299, train/accuracy=0.990691, train/loss=0.030587, train/mean_average_precision=0.392154, validation/accuracy=0.986686, validation/loss=0.044394, validation/mean_average_precision=0.264689, validation/num_examples=43793
I0205 17:26:39.052783 140283530442496 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.08193996548652649, loss=0.0339202843606472
I0205 17:27:11.703181 140266968643328 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.06428918987512589, loss=0.029091142117977142
I0205 17:27:44.282168 140283530442496 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.06601522117853165, loss=0.029829563573002815
I0205 17:28:16.185444 140266968643328 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.06452878564596176, loss=0.031073791906237602
I0205 17:28:48.283290 140283530442496 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.08416219800710678, loss=0.03110571578145027
I0205 17:29:20.392509 140266968643328 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.09709059447050095, loss=0.03086182288825512
I0205 17:29:52.282068 140283530442496 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.0877407118678093, loss=0.033618781715631485
I0205 17:30:15.084405 140451058161472 spec.py:321] Evaluating on the training split.
I0205 17:31:58.306400 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 17:32:01.342734 140451058161472 spec.py:349] Evaluating on the test split.
I0205 17:32:04.518477 140451058161472 submission_runner.py:408] Time since start: 8958.12s, 	Step: 18772, 	{'train/accuracy': 0.9907752275466919, 'train/loss': 0.030265526846051216, 'train/mean_average_precision': 0.3949178652503551, 'validation/accuracy': 0.98653244972229, 'validation/loss': 0.04530401900410652, 'validation/mean_average_precision': 0.25429649453010816, 'validation/num_examples': 43793, 'test/accuracy': 0.9856511354446411, 'test/loss': 0.04803209751844406, 'test/mean_average_precision': 0.24621501443393456, 'test/num_examples': 43793, 'score': 6013.634890794754, 'total_duration': 8958.123777866364, 'accumulated_submission_time': 6013.634890794754, 'accumulated_eval_time': 2942.8444719314575, 'accumulated_logging_time': 1.133225440979004}
I0205 17:32:04.537579 140266960250624 logging_writer.py:48] [18772] accumulated_eval_time=2942.844472, accumulated_logging_time=1.133225, accumulated_submission_time=6013.634891, global_step=18772, preemption_count=0, score=6013.634891, test/accuracy=0.985651, test/loss=0.048032, test/mean_average_precision=0.246215, test/num_examples=43793, total_duration=8958.123778, train/accuracy=0.990775, train/loss=0.030266, train/mean_average_precision=0.394918, validation/accuracy=0.986532, validation/loss=0.045304, validation/mean_average_precision=0.254296, validation/num_examples=43793
I0205 17:32:13.681984 140290204940032 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.10108816623687744, loss=0.027383655309677124
I0205 17:32:45.389075 140266960250624 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.06691009551286697, loss=0.030538171529769897
I0205 17:33:17.536203 140290204940032 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.07251914590597153, loss=0.03323376551270485
I0205 17:33:49.383367 140266960250624 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.0627569928765297, loss=0.024380255490541458
I0205 17:34:21.481666 140290204940032 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.08270277827978134, loss=0.03163187950849533
I0205 17:34:53.054899 140266960250624 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.08745536208152771, loss=0.029610250145196915
I0205 17:35:25.118094 140290204940032 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.10547147691249847, loss=0.029382379725575447
I0205 17:35:57.157299 140266960250624 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.08866342902183533, loss=0.029700975865125656
I0205 17:36:04.620169 140451058161472 spec.py:321] Evaluating on the training split.
I0205 17:37:47.620874 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 17:37:50.645869 140451058161472 spec.py:349] Evaluating on the test split.
I0205 17:37:55.486486 140451058161472 submission_runner.py:408] Time since start: 9309.09s, 	Step: 19524, 	{'train/accuracy': 0.990888774394989, 'train/loss': 0.02983536384999752, 'train/mean_average_precision': 0.41505419043593395, 'validation/accuracy': 0.9865884780883789, 'validation/loss': 0.044611282646656036, 'validation/mean_average_precision': 0.257674980871371, 'validation/num_examples': 43793, 'test/accuracy': 0.9857951998710632, 'test/loss': 0.04735570773482323, 'test/mean_average_precision': 0.25049877040342133, 'test/num_examples': 43793, 'score': 6253.68617773056, 'total_duration': 9309.091790914536, 'accumulated_submission_time': 6253.68617773056, 'accumulated_eval_time': 3053.710742712021, 'accumulated_logging_time': 1.1633639335632324}
I0205 17:37:55.505706 140266968643328 logging_writer.py:48] [19524] accumulated_eval_time=3053.710743, accumulated_logging_time=1.163364, accumulated_submission_time=6253.686178, global_step=19524, preemption_count=0, score=6253.686178, test/accuracy=0.985795, test/loss=0.047356, test/mean_average_precision=0.250499, test/num_examples=43793, total_duration=9309.091791, train/accuracy=0.990889, train/loss=0.029835, train/mean_average_precision=0.415054, validation/accuracy=0.986588, validation/loss=0.044611, validation/mean_average_precision=0.257675, validation/num_examples=43793
I0205 17:38:19.831702 140283262007040 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.09712119400501251, loss=0.03141239285469055
I0205 17:38:51.283327 140266968643328 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.08843190968036652, loss=0.03275126591324806
I0205 17:39:22.781634 140283262007040 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.07880127429962158, loss=0.028809618204832077
I0205 17:39:54.518984 140266968643328 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.07605704665184021, loss=0.030203906819224358
I0205 17:40:26.056746 140283262007040 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.0913761630654335, loss=0.031200002878904343
I0205 17:40:58.371557 140266968643328 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.08571849763393402, loss=0.03229732811450958
I0205 17:41:30.309965 140283262007040 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.08783947676420212, loss=0.03303804248571396
I0205 17:41:55.549681 140451058161472 spec.py:321] Evaluating on the training split.
I0205 17:43:38.053097 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 17:43:41.112632 140451058161472 spec.py:349] Evaluating on the test split.
I0205 17:43:44.017343 140451058161472 submission_runner.py:408] Time since start: 9657.62s, 	Step: 20281, 	{'train/accuracy': 0.99105304479599, 'train/loss': 0.029099976643919945, 'train/mean_average_precision': 0.43728197588999584, 'validation/accuracy': 0.986669659614563, 'validation/loss': 0.044859033077955246, 'validation/mean_average_precision': 0.2627159908102456, 'validation/num_examples': 43793, 'test/accuracy': 0.985710084438324, 'test/loss': 0.04760020226240158, 'test/mean_average_precision': 0.2503044683984956, 'test/num_examples': 43793, 'score': 6493.699534893036, 'total_duration': 9657.622642278671, 'accumulated_submission_time': 6493.699534893036, 'accumulated_eval_time': 3162.1783707141876, 'accumulated_logging_time': 1.1930761337280273}
I0205 17:43:44.035934 140283530442496 logging_writer.py:48] [20281] accumulated_eval_time=3162.178371, accumulated_logging_time=1.193076, accumulated_submission_time=6493.699535, global_step=20281, preemption_count=0, score=6493.699535, test/accuracy=0.985710, test/loss=0.047600, test/mean_average_precision=0.250304, test/num_examples=43793, total_duration=9657.622642, train/accuracy=0.991053, train/loss=0.029100, train/mean_average_precision=0.437282, validation/accuracy=0.986670, validation/loss=0.044859, validation/mean_average_precision=0.262716, validation/num_examples=43793
I0205 17:43:50.389549 140290204940032 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.07984065264463425, loss=0.029351646080613136
I0205 17:44:21.885730 140283530442496 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.08477995544672012, loss=0.028589937835931778
I0205 17:44:53.236459 140290204940032 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.09204746782779694, loss=0.03280601650476456
I0205 17:45:24.838376 140283530442496 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.08272574841976166, loss=0.030614037066698074
I0205 17:45:56.396137 140290204940032 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.1406184434890747, loss=0.03166933357715607
I0205 17:46:28.152863 140283530442496 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.10342667996883392, loss=0.031184783205389977
I0205 17:46:59.588599 140290204940032 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.07850201427936554, loss=0.027346031740307808
I0205 17:47:31.192891 140283530442496 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.08788689225912094, loss=0.037761200219392776
I0205 17:47:44.138172 140451058161472 spec.py:321] Evaluating on the training split.
I0205 17:49:26.191956 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 17:49:29.609883 140451058161472 spec.py:349] Evaluating on the test split.
I0205 17:49:33.006214 140451058161472 submission_runner.py:408] Time since start: 10006.61s, 	Step: 21042, 	{'train/accuracy': 0.9911764860153198, 'train/loss': 0.02850949950516224, 'train/mean_average_precision': 0.44081136446997216, 'validation/accuracy': 0.9867488145828247, 'validation/loss': 0.044682178646326065, 'validation/mean_average_precision': 0.26651221630376165, 'validation/num_examples': 43793, 'test/accuracy': 0.9857838153839111, 'test/loss': 0.04751453176140785, 'test/mean_average_precision': 0.2609902777836348, 'test/num_examples': 43793, 'score': 6733.768709421158, 'total_duration': 10006.611500024796, 'accumulated_submission_time': 6733.768709421158, 'accumulated_eval_time': 3271.0463457107544, 'accumulated_logging_time': 1.224475383758545}
I0205 17:49:33.029325 140266960250624 logging_writer.py:48] [21042] accumulated_eval_time=3271.046346, accumulated_logging_time=1.224475, accumulated_submission_time=6733.768709, global_step=21042, preemption_count=0, score=6733.768709, test/accuracy=0.985784, test/loss=0.047515, test/mean_average_precision=0.260990, test/num_examples=43793, total_duration=10006.611500, train/accuracy=0.991176, train/loss=0.028509, train/mean_average_precision=0.440811, validation/accuracy=0.986749, validation/loss=0.044682, validation/mean_average_precision=0.266512, validation/num_examples=43793
I0205 17:49:52.517448 140283262007040 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.0846852958202362, loss=0.030586158856749535
I0205 17:50:25.640149 140266960250624 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.08804631233215332, loss=0.029469197615981102
I0205 17:50:58.363563 140283262007040 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.0832233726978302, loss=0.02976822666823864
I0205 17:51:31.392019 140266960250624 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.08485589176416397, loss=0.029260678216814995
I0205 17:52:04.061762 140283262007040 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.07452687621116638, loss=0.02909797616302967
I0205 17:52:36.842726 140266960250624 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.0824715793132782, loss=0.028988145291805267
I0205 17:53:09.616853 140283262007040 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.09687512367963791, loss=0.030337711796164513
I0205 17:53:33.309778 140451058161472 spec.py:321] Evaluating on the training split.
I0205 17:55:17.906675 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 17:55:21.076064 140451058161472 spec.py:349] Evaluating on the test split.
I0205 17:55:24.122972 140451058161472 submission_runner.py:408] Time since start: 10357.73s, 	Step: 21776, 	{'train/accuracy': 0.991248607635498, 'train/loss': 0.02826027013361454, 'train/mean_average_precision': 0.45386688447746715, 'validation/accuracy': 0.9866753816604614, 'validation/loss': 0.04443187266588211, 'validation/mean_average_precision': 0.2672081493576128, 'validation/num_examples': 43793, 'test/accuracy': 0.985862135887146, 'test/loss': 0.047269124537706375, 'test/mean_average_precision': 0.2573478156117922, 'test/num_examples': 43793, 'score': 6974.013297080994, 'total_duration': 10357.728278398514, 'accumulated_submission_time': 6974.013297080994, 'accumulated_eval_time': 3381.8595008850098, 'accumulated_logging_time': 1.2595610618591309}
I0205 17:55:24.142172 140283530442496 logging_writer.py:48] [21776] accumulated_eval_time=3381.859501, accumulated_logging_time=1.259561, accumulated_submission_time=6974.013297, global_step=21776, preemption_count=0, score=6974.013297, test/accuracy=0.985862, test/loss=0.047269, test/mean_average_precision=0.257348, test/num_examples=43793, total_duration=10357.728278, train/accuracy=0.991249, train/loss=0.028260, train/mean_average_precision=0.453867, validation/accuracy=0.986675, validation/loss=0.044432, validation/mean_average_precision=0.267208, validation/num_examples=43793
I0205 17:55:32.136768 140290204940032 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.06761489808559418, loss=0.02892998605966568
I0205 17:56:04.018834 140283530442496 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.12029121816158295, loss=0.028749840334057808
I0205 17:56:35.881030 140290204940032 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.08913657069206238, loss=0.028543898835778236
I0205 17:57:07.955226 140283530442496 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.09528722614049911, loss=0.030418643727898598
I0205 17:57:39.666519 140290204940032 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.11726758629083633, loss=0.02928021550178528
I0205 17:58:11.458293 140283530442496 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.07187597453594208, loss=0.026843560859560966
I0205 17:58:43.284836 140290204940032 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.07776488363742828, loss=0.027807539328932762
I0205 17:59:15.230087 140283530442496 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.08523522317409515, loss=0.032061897218227386
I0205 17:59:24.148905 140451058161472 spec.py:321] Evaluating on the training split.
I0205 18:01:06.505567 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 18:01:12.482668 140451058161472 spec.py:349] Evaluating on the test split.
I0205 18:01:15.782589 140451058161472 submission_runner.py:408] Time since start: 10709.39s, 	Step: 22529, 	{'train/accuracy': 0.9914287328720093, 'train/loss': 0.028035998344421387, 'train/mean_average_precision': 0.4523613957712486, 'validation/accuracy': 0.9865572452545166, 'validation/loss': 0.04490554332733154, 'validation/mean_average_precision': 0.2573994457154161, 'validation/num_examples': 43793, 'test/accuracy': 0.9856717586517334, 'test/loss': 0.04791056364774704, 'test/mean_average_precision': 0.2484504127180709, 'test/num_examples': 43793, 'score': 7213.988672018051, 'total_duration': 10709.387873411179, 'accumulated_submission_time': 7213.988672018051, 'accumulated_eval_time': 3493.4931180477142, 'accumulated_logging_time': 1.289865255355835}
I0205 18:01:15.805786 140266960250624 logging_writer.py:48] [22529] accumulated_eval_time=3493.493118, accumulated_logging_time=1.289865, accumulated_submission_time=7213.988672, global_step=22529, preemption_count=0, score=7213.988672, test/accuracy=0.985672, test/loss=0.047911, test/mean_average_precision=0.248450, test/num_examples=43793, total_duration=10709.387873, train/accuracy=0.991429, train/loss=0.028036, train/mean_average_precision=0.452361, validation/accuracy=0.986557, validation/loss=0.044906, validation/mean_average_precision=0.257399, validation/num_examples=43793
I0205 18:01:41.463580 140266968643328 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.11768284440040588, loss=0.03394798934459686
I0205 18:02:15.489888 140266960250624 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.07822185009717941, loss=0.027940567582845688
I0205 18:02:47.471834 140266968643328 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.07784418016672134, loss=0.026841329410672188
I0205 18:03:19.968934 140266960250624 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.08008981496095657, loss=0.02744949609041214
I0205 18:03:51.810956 140266968643328 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.103435218334198, loss=0.03229568526148796
I0205 18:04:23.978126 140266960250624 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.1030539721250534, loss=0.02849925309419632
I0205 18:04:55.923160 140266968643328 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.11415696889162064, loss=0.03220027685165405
I0205 18:05:15.795272 140451058161472 spec.py:321] Evaluating on the training split.
I0205 18:07:00.490356 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 18:07:03.766302 140451058161472 spec.py:349] Evaluating on the test split.
I0205 18:07:06.848606 140451058161472 submission_runner.py:408] Time since start: 11060.45s, 	Step: 23263, 	{'train/accuracy': 0.9912769198417664, 'train/loss': 0.028356939554214478, 'train/mean_average_precision': 0.4405505491753948, 'validation/accuracy': 0.9866635799407959, 'validation/loss': 0.04467831179499626, 'validation/mean_average_precision': 0.2679886704370666, 'validation/num_examples': 43793, 'test/accuracy': 0.9859034419059753, 'test/loss': 0.04734523221850395, 'test/mean_average_precision': 0.2580197824040367, 'test/num_examples': 43793, 'score': 7453.943880081177, 'total_duration': 11060.453907966614, 'accumulated_submission_time': 7453.943880081177, 'accumulated_eval_time': 3604.5464034080505, 'accumulated_logging_time': 1.3261759281158447}
I0205 18:07:06.869186 140283262007040 logging_writer.py:48] [23263] accumulated_eval_time=3604.546403, accumulated_logging_time=1.326176, accumulated_submission_time=7453.943880, global_step=23263, preemption_count=0, score=7453.943880, test/accuracy=0.985903, test/loss=0.047345, test/mean_average_precision=0.258020, test/num_examples=43793, total_duration=11060.453908, train/accuracy=0.991277, train/loss=0.028357, train/mean_average_precision=0.440551, validation/accuracy=0.986664, validation/loss=0.044678, validation/mean_average_precision=0.267989, validation/num_examples=43793
I0205 18:07:18.967647 140283530442496 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.09830500930547714, loss=0.03261086344718933
I0205 18:07:51.021783 140283262007040 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.11442901939153671, loss=0.02907521277666092
I0205 18:08:23.103277 140283530442496 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.07600175589323044, loss=0.028540106490254402
I0205 18:08:57.015372 140283262007040 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.0982649102807045, loss=0.027825944125652313
I0205 18:09:30.269620 140283530442496 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.11477630585432053, loss=0.030018622055649757
I0205 18:10:03.271538 140283262007040 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.07399344444274902, loss=0.026946306228637695
I0205 18:10:35.966766 140283530442496 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.12101346999406815, loss=0.029954394325613976
I0205 18:11:07.135539 140451058161472 spec.py:321] Evaluating on the training split.
I0205 18:12:46.423791 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 18:12:49.553778 140451058161472 spec.py:349] Evaluating on the test split.
I0205 18:12:54.748148 140451058161472 submission_runner.py:408] Time since start: 11408.35s, 	Step: 23997, 	{'train/accuracy': 0.9911851286888123, 'train/loss': 0.02885875478386879, 'train/mean_average_precision': 0.4346443075859728, 'validation/accuracy': 0.9867857694625854, 'validation/loss': 0.04451743885874748, 'validation/mean_average_precision': 0.2709604029205279, 'validation/num_examples': 43793, 'test/accuracy': 0.9859076142311096, 'test/loss': 0.047397810965776443, 'test/mean_average_precision': 0.2571818691088361, 'test/num_examples': 43793, 'score': 7694.173835515976, 'total_duration': 11408.353337526321, 'accumulated_submission_time': 7694.173835515976, 'accumulated_eval_time': 3712.1588644981384, 'accumulated_logging_time': 1.3611788749694824}
I0205 18:12:54.768507 140266960250624 logging_writer.py:48] [23997] accumulated_eval_time=3712.158864, accumulated_logging_time=1.361179, accumulated_submission_time=7694.173836, global_step=23997, preemption_count=0, score=7694.173836, test/accuracy=0.985908, test/loss=0.047398, test/mean_average_precision=0.257182, test/num_examples=43793, total_duration=11408.353338, train/accuracy=0.991185, train/loss=0.028859, train/mean_average_precision=0.434644, validation/accuracy=0.986786, validation/loss=0.044517, validation/mean_average_precision=0.270960, validation/num_examples=43793
I0205 18:12:56.151329 140290204940032 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.08563211560249329, loss=0.03297971934080124
I0205 18:13:28.352667 140266960250624 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.10942138731479645, loss=0.030254961922764778
I0205 18:13:59.881834 140290204940032 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.09650137275457382, loss=0.029304230585694313
I0205 18:14:32.003639 140266960250624 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.08807418495416641, loss=0.029363155364990234
I0205 18:15:03.610284 140290204940032 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.08302353322505951, loss=0.028463149443268776
I0205 18:15:34.783365 140266960250624 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.0946405827999115, loss=0.030702361837029457
I0205 18:16:06.901371 140290204940032 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.10048982501029968, loss=0.03073977492749691
I0205 18:16:38.522977 140266960250624 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.07443061470985413, loss=0.027856770902872086
I0205 18:16:54.998607 140451058161472 spec.py:321] Evaluating on the training split.
I0205 18:18:35.313461 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 18:18:38.301035 140451058161472 spec.py:349] Evaluating on the test split.
I0205 18:18:41.294975 140451058161472 submission_runner.py:408] Time since start: 11754.90s, 	Step: 24753, 	{'train/accuracy': 0.9910281896591187, 'train/loss': 0.029097726568579674, 'train/mean_average_precision': 0.43235437727377957, 'validation/accuracy': 0.9866266250610352, 'validation/loss': 0.04503720998764038, 'validation/mean_average_precision': 0.26164825516651796, 'validation/num_examples': 43793, 'test/accuracy': 0.9857720136642456, 'test/loss': 0.04777917265892029, 'test/mean_average_precision': 0.2502596400985658, 'test/num_examples': 43793, 'score': 7934.371356964111, 'total_duration': 11754.900280475616, 'accumulated_submission_time': 7934.371356964111, 'accumulated_eval_time': 3818.4551911354065, 'accumulated_logging_time': 1.3941354751586914}
I0205 18:18:41.315012 140266968643328 logging_writer.py:48] [24753] accumulated_eval_time=3818.455191, accumulated_logging_time=1.394135, accumulated_submission_time=7934.371357, global_step=24753, preemption_count=0, score=7934.371357, test/accuracy=0.985772, test/loss=0.047779, test/mean_average_precision=0.250260, test/num_examples=43793, total_duration=11754.900280, train/accuracy=0.991028, train/loss=0.029098, train/mean_average_precision=0.432354, validation/accuracy=0.986627, validation/loss=0.045037, validation/mean_average_precision=0.261648, validation/num_examples=43793
I0205 18:18:56.509355 140283262007040 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.08364568650722504, loss=0.028507765382528305
I0205 18:19:28.768391 140266968643328 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.10482166707515717, loss=0.02923574484884739
I0205 18:20:00.346897 140283262007040 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.11800131946802139, loss=0.027301644906401634
I0205 18:20:32.161807 140266968643328 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.09431962668895721, loss=0.02948569506406784
I0205 18:21:03.992863 140283262007040 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.08470065891742706, loss=0.028431514278054237
I0205 18:21:36.216490 140266968643328 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.1395721733570099, loss=0.029753748327493668
I0205 18:22:08.234278 140283262007040 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.11986179649829865, loss=0.031970877200365067
I0205 18:22:39.756611 140266968643328 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.07992967963218689, loss=0.02656758390367031
I0205 18:22:41.331584 140451058161472 spec.py:321] Evaluating on the training split.
I0205 18:24:21.747441 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 18:24:24.769438 140451058161472 spec.py:349] Evaluating on the test split.
I0205 18:24:27.741738 140451058161472 submission_runner.py:408] Time since start: 12101.35s, 	Step: 25506, 	{'train/accuracy': 0.9909423589706421, 'train/loss': 0.029259320348501205, 'train/mean_average_precision': 0.430151878859003, 'validation/accuracy': 0.986642062664032, 'validation/loss': 0.04556584730744362, 'validation/mean_average_precision': 0.257154364719039, 'validation/num_examples': 43793, 'test/accuracy': 0.9857766628265381, 'test/loss': 0.048466894775629044, 'test/mean_average_precision': 0.24619635459370018, 'test/num_examples': 43793, 'score': 8174.356585264206, 'total_duration': 12101.346946954727, 'accumulated_submission_time': 8174.356585264206, 'accumulated_eval_time': 3924.8651978969574, 'accumulated_logging_time': 1.4252452850341797}
I0205 18:24:27.761337 140283530442496 logging_writer.py:48] [25506] accumulated_eval_time=3924.865198, accumulated_logging_time=1.425245, accumulated_submission_time=8174.356585, global_step=25506, preemption_count=0, score=8174.356585, test/accuracy=0.985777, test/loss=0.048467, test/mean_average_precision=0.246196, test/num_examples=43793, total_duration=12101.346947, train/accuracy=0.990942, train/loss=0.029259, train/mean_average_precision=0.430152, validation/accuracy=0.986642, validation/loss=0.045566, validation/mean_average_precision=0.257154, validation/num_examples=43793
I0205 18:24:57.973761 140290204940032 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.11283668130636215, loss=0.02789587900042534
I0205 18:25:30.311833 140283530442496 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.08425331860780716, loss=0.026963859796524048
I0205 18:26:02.224671 140290204940032 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.08459946513175964, loss=0.028591472655534744
I0205 18:26:34.021718 140283530442496 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.10437493771314621, loss=0.026983506977558136
I0205 18:27:05.961864 140290204940032 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.09955604374408722, loss=0.028145287185907364
I0205 18:27:37.841499 140283530442496 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.12069519609212875, loss=0.028972458094358444
I0205 18:28:10.834114 140290204940032 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.08849189430475235, loss=0.02775575965642929
I0205 18:28:27.970473 140451058161472 spec.py:321] Evaluating on the training split.
I0205 18:30:14.480653 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 18:30:17.482548 140451058161472 spec.py:349] Evaluating on the test split.
I0205 18:30:20.470525 140451058161472 submission_runner.py:408] Time since start: 12454.08s, 	Step: 26254, 	{'train/accuracy': 0.9912765026092529, 'train/loss': 0.02838318794965744, 'train/mean_average_precision': 0.4462405974678892, 'validation/accuracy': 0.9867240786552429, 'validation/loss': 0.04525268077850342, 'validation/mean_average_precision': 0.2645874325719716, 'validation/num_examples': 43793, 'test/accuracy': 0.9858777523040771, 'test/loss': 0.04805606231093407, 'test/mean_average_precision': 0.25816406230441513, 'test/num_examples': 43793, 'score': 8414.532056331635, 'total_duration': 12454.075824022293, 'accumulated_submission_time': 8414.532056331635, 'accumulated_eval_time': 4037.365210533142, 'accumulated_logging_time': 1.4570987224578857}
I0205 18:30:20.491327 140266968643328 logging_writer.py:48] [26254] accumulated_eval_time=4037.365211, accumulated_logging_time=1.457099, accumulated_submission_time=8414.532056, global_step=26254, preemption_count=0, score=8414.532056, test/accuracy=0.985878, test/loss=0.048056, test/mean_average_precision=0.258164, test/num_examples=43793, total_duration=12454.075824, train/accuracy=0.991277, train/loss=0.028383, train/mean_average_precision=0.446241, validation/accuracy=0.986724, validation/loss=0.045253, validation/mean_average_precision=0.264587, validation/num_examples=43793
I0205 18:30:35.408207 140283262007040 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.11813148111104965, loss=0.03171198442578316
I0205 18:31:07.323979 140266968643328 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.1092366948723793, loss=0.025696637108922005
I0205 18:31:39.366705 140283262007040 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.08848096430301666, loss=0.02809327282011509
I0205 18:32:11.061865 140266968643328 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.09339671581983566, loss=0.03312550485134125
I0205 18:32:42.941905 140283262007040 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.07629174739122391, loss=0.028736896812915802
I0205 18:33:14.725648 140266968643328 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.08255243301391602, loss=0.025163665413856506
I0205 18:33:46.701385 140283262007040 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.13656537234783173, loss=0.03246741741895676
I0205 18:34:18.485683 140266968643328 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.12728269398212433, loss=0.02877787873148918
I0205 18:34:20.738591 140451058161472 spec.py:321] Evaluating on the training split.
I0205 18:35:58.156748 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 18:36:01.165868 140451058161472 spec.py:349] Evaluating on the test split.
I0205 18:36:04.313108 140451058161472 submission_runner.py:408] Time since start: 12797.92s, 	Step: 27008, 	{'train/accuracy': 0.99150550365448, 'train/loss': 0.02743266150355339, 'train/mean_average_precision': 0.47418810057435634, 'validation/accuracy': 0.9866209626197815, 'validation/loss': 0.04493384435772896, 'validation/mean_average_precision': 0.2674430369710101, 'validation/num_examples': 43793, 'test/accuracy': 0.985755980014801, 'test/loss': 0.04790718853473663, 'test/mean_average_precision': 0.26249672644052296, 'test/num_examples': 43793, 'score': 8654.747866868973, 'total_duration': 12797.918403863907, 'accumulated_submission_time': 8654.747866868973, 'accumulated_eval_time': 4140.939670324326, 'accumulated_logging_time': 1.4889216423034668}
I0205 18:36:04.333343 140266960250624 logging_writer.py:48] [27008] accumulated_eval_time=4140.939670, accumulated_logging_time=1.488922, accumulated_submission_time=8654.747867, global_step=27008, preemption_count=0, score=8654.747867, test/accuracy=0.985756, test/loss=0.047907, test/mean_average_precision=0.262497, test/num_examples=43793, total_duration=12797.918404, train/accuracy=0.991506, train/loss=0.027433, train/mean_average_precision=0.474188, validation/accuracy=0.986621, validation/loss=0.044934, validation/mean_average_precision=0.267443, validation/num_examples=43793
I0205 18:36:33.644070 140290204940032 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.1108001247048378, loss=0.026279786601662636
I0205 18:37:05.052660 140266960250624 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.09530774503946304, loss=0.0324932336807251
I0205 18:37:38.184196 140290204940032 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.10720524936914444, loss=0.029657989740371704
I0205 18:38:10.117502 140266960250624 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.09162046015262604, loss=0.026899976655840874
I0205 18:38:41.585688 140290204940032 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.09043712913990021, loss=0.02619091235101223
I0205 18:39:13.405198 140266960250624 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.09267240017652512, loss=0.027341946959495544
I0205 18:39:44.955728 140290204940032 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.08672258257865906, loss=0.028306562453508377
I0205 18:40:04.443184 140451058161472 spec.py:321] Evaluating on the training split.
I0205 18:41:44.501309 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 18:41:47.487743 140451058161472 spec.py:349] Evaluating on the test split.
I0205 18:41:50.449661 140451058161472 submission_runner.py:408] Time since start: 13144.05s, 	Step: 27762, 	{'train/accuracy': 0.9915337562561035, 'train/loss': 0.0273089949041605, 'train/mean_average_precision': 0.4780035881167493, 'validation/accuracy': 0.9865288138389587, 'validation/loss': 0.04487583041191101, 'validation/mean_average_precision': 0.26343613950845807, 'validation/num_examples': 43793, 'test/accuracy': 0.9856725931167603, 'test/loss': 0.047843076288700104, 'test/mean_average_precision': 0.25814367993191484, 'test/num_examples': 43793, 'score': 8894.826835393906, 'total_duration': 13144.054966926575, 'accumulated_submission_time': 8894.826835393906, 'accumulated_eval_time': 4246.946130990982, 'accumulated_logging_time': 1.519975185394287}
I0205 18:41:50.469768 140266968643328 logging_writer.py:48] [27762] accumulated_eval_time=4246.946131, accumulated_logging_time=1.519975, accumulated_submission_time=8894.826835, global_step=27762, preemption_count=0, score=8894.826835, test/accuracy=0.985673, test/loss=0.047843, test/mean_average_precision=0.258144, test/num_examples=43793, total_duration=13144.054967, train/accuracy=0.991534, train/loss=0.027309, train/mean_average_precision=0.478004, validation/accuracy=0.986529, validation/loss=0.044876, validation/mean_average_precision=0.263436, validation/num_examples=43793
I0205 18:42:03.154565 140283262007040 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.08590713888406754, loss=0.02635031007230282
I0205 18:42:34.666747 140266968643328 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.11190541833639145, loss=0.02768627367913723
I0205 18:43:07.036790 140283262007040 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.118624746799469, loss=0.029112813994288445
I0205 18:43:38.232949 140266968643328 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.09368148446083069, loss=0.028858082368969917
I0205 18:44:10.078103 140283262007040 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.08700169622898102, loss=0.02542301081120968
I0205 18:44:41.628114 140266968643328 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.08273743093013763, loss=0.027624541893601418
I0205 18:45:13.498264 140283262007040 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.10959003120660782, loss=0.02827460691332817
I0205 18:45:45.042601 140266968643328 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.10129915922880173, loss=0.027330320328474045
I0205 18:45:50.623582 140451058161472 spec.py:321] Evaluating on the training split.
I0205 18:47:31.926718 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 18:47:34.956254 140451058161472 spec.py:349] Evaluating on the test split.
I0205 18:47:37.884802 140451058161472 submission_runner.py:408] Time since start: 13491.49s, 	Step: 28519, 	{'train/accuracy': 0.9918634295463562, 'train/loss': 0.02625834196805954, 'train/mean_average_precision': 0.4980736352194875, 'validation/accuracy': 0.9867614507675171, 'validation/loss': 0.04515032842755318, 'validation/mean_average_precision': 0.2686315066777437, 'validation/num_examples': 43793, 'test/accuracy': 0.9858773350715637, 'test/loss': 0.048234663903713226, 'test/mean_average_precision': 0.25259174588429933, 'test/num_examples': 43793, 'score': 9134.948380470276, 'total_duration': 13491.490109682083, 'accumulated_submission_time': 9134.948380470276, 'accumulated_eval_time': 4354.20730304718, 'accumulated_logging_time': 1.55245041847229}
I0205 18:47:37.905320 140283530442496 logging_writer.py:48] [28519] accumulated_eval_time=4354.207303, accumulated_logging_time=1.552450, accumulated_submission_time=9134.948380, global_step=28519, preemption_count=0, score=9134.948380, test/accuracy=0.985877, test/loss=0.048235, test/mean_average_precision=0.252592, test/num_examples=43793, total_duration=13491.490110, train/accuracy=0.991863, train/loss=0.026258, train/mean_average_precision=0.498074, validation/accuracy=0.986761, validation/loss=0.045150, validation/mean_average_precision=0.268632, validation/num_examples=43793
I0205 18:48:05.313761 140290204940032 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.09450729936361313, loss=0.02583738975226879
I0205 18:48:37.076014 140283530442496 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.10076583176851273, loss=0.029286513105034828
I0205 18:49:08.782030 140290204940032 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.08073777705430984, loss=0.025825094431638718
I0205 18:49:40.323514 140283530442496 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.09672669321298599, loss=0.02550516463816166
I0205 18:50:12.267356 140290204940032 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.10142309963703156, loss=0.030250681564211845
I0205 18:50:44.623982 140283530442496 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.09671839326620102, loss=0.026611849665641785
I0205 18:51:16.984132 140290204940032 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.0887458398938179, loss=0.02734566293656826
I0205 18:51:38.017062 140451058161472 spec.py:321] Evaluating on the training split.
I0205 18:53:18.747601 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 18:53:21.766075 140451058161472 spec.py:349] Evaluating on the test split.
I0205 18:53:24.717379 140451058161472 submission_runner.py:408] Time since start: 13838.32s, 	Step: 29267, 	{'train/accuracy': 0.9919530749320984, 'train/loss': 0.025827709585428238, 'train/mean_average_precision': 0.5062438255238915, 'validation/accuracy': 0.9866485595703125, 'validation/loss': 0.045173633843660355, 'validation/mean_average_precision': 0.26228965582438557, 'validation/num_examples': 43793, 'test/accuracy': 0.9858684539794922, 'test/loss': 0.04821714013814926, 'test/mean_average_precision': 0.25379463137131003, 'test/num_examples': 43793, 'score': 9375.02670264244, 'total_duration': 13838.322686195374, 'accumulated_submission_time': 9375.02670264244, 'accumulated_eval_time': 4460.9075763225555, 'accumulated_logging_time': 1.5855414867401123}
I0205 18:53:24.739300 140266960250624 logging_writer.py:48] [29267] accumulated_eval_time=4460.907576, accumulated_logging_time=1.585541, accumulated_submission_time=9375.026703, global_step=29267, preemption_count=0, score=9375.026703, test/accuracy=0.985868, test/loss=0.048217, test/mean_average_precision=0.253795, test/num_examples=43793, total_duration=13838.322686, train/accuracy=0.991953, train/loss=0.025828, train/mean_average_precision=0.506244, validation/accuracy=0.986649, validation/loss=0.045174, validation/mean_average_precision=0.262290, validation/num_examples=43793
I0205 18:53:35.478026 140283262007040 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.10007170587778091, loss=0.02667432837188244
I0205 18:54:07.488596 140266960250624 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.08886558562517166, loss=0.02828046679496765
I0205 18:54:38.718036 140283262007040 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.09269502013921738, loss=0.026866191998124123
I0205 18:55:10.716955 140266960250624 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.10559870302677155, loss=0.027530577033758163
I0205 18:55:42.154551 140283262007040 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.09024262428283691, loss=0.02635735273361206
I0205 18:56:13.836103 140266960250624 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.1270439475774765, loss=0.030588705092668533
I0205 18:56:45.288589 140283262007040 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.09158185869455338, loss=0.02846957929432392
I0205 18:57:16.661695 140266960250624 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.1480538547039032, loss=0.029896264895796776
I0205 18:57:24.809796 140451058161472 spec.py:321] Evaluating on the training split.
I0205 18:59:02.450803 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 18:59:05.545140 140451058161472 spec.py:349] Evaluating on the test split.
I0205 18:59:08.502643 140451058161472 submission_runner.py:408] Time since start: 14182.11s, 	Step: 30027, 	{'train/accuracy': 0.9917265176773071, 'train/loss': 0.026603862643241882, 'train/mean_average_precision': 0.4825484781293242, 'validation/accuracy': 0.9867070317268372, 'validation/loss': 0.045193303376436234, 'validation/mean_average_precision': 0.2677666322082109, 'validation/num_examples': 43793, 'test/accuracy': 0.9858120083808899, 'test/loss': 0.047961484640836716, 'test/mean_average_precision': 0.2577138416244921, 'test/num_examples': 43793, 'score': 9615.066091775894, 'total_duration': 14182.107945919037, 'accumulated_submission_time': 9615.066091775894, 'accumulated_eval_time': 4564.600377559662, 'accumulated_logging_time': 1.6183860301971436}
I0205 18:59:08.524193 140283530442496 logging_writer.py:48] [30027] accumulated_eval_time=4564.600378, accumulated_logging_time=1.618386, accumulated_submission_time=9615.066092, global_step=30027, preemption_count=0, score=9615.066092, test/accuracy=0.985812, test/loss=0.047961, test/mean_average_precision=0.257714, test/num_examples=43793, total_duration=14182.107946, train/accuracy=0.991727, train/loss=0.026604, train/mean_average_precision=0.482548, validation/accuracy=0.986707, validation/loss=0.045193, validation/mean_average_precision=0.267767, validation/num_examples=43793
I0205 18:59:31.633265 140290204940032 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.09292037785053253, loss=0.027366841211915016
I0205 19:00:03.121150 140283530442496 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.09696996957063675, loss=0.025322262197732925
I0205 19:00:34.792760 140290204940032 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.09475639462471008, loss=0.027216710150241852
I0205 19:01:06.179387 140283530442496 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.11073782294988632, loss=0.027948621660470963
I0205 19:01:37.547230 140290204940032 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.09722612053155899, loss=0.02829609625041485
I0205 19:02:09.324251 140283530442496 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.10126926749944687, loss=0.028468258678913116
I0205 19:02:40.977981 140290204940032 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.10166354477405548, loss=0.025737091898918152
I0205 19:03:08.641471 140451058161472 spec.py:321] Evaluating on the training split.
I0205 19:04:50.321654 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 19:04:53.360883 140451058161472 spec.py:349] Evaluating on the test split.
I0205 19:04:56.274742 140451058161472 submission_runner.py:408] Time since start: 14529.88s, 	Step: 30789, 	{'train/accuracy': 0.9917187094688416, 'train/loss': 0.026825791224837303, 'train/mean_average_precision': 0.47531528965594083, 'validation/accuracy': 0.9866449236869812, 'validation/loss': 0.04516757279634476, 'validation/mean_average_precision': 0.27096467932786544, 'validation/num_examples': 43793, 'test/accuracy': 0.9856843948364258, 'test/loss': 0.048130933195352554, 'test/mean_average_precision': 0.2610056338665237, 'test/num_examples': 43793, 'score': 9855.15208029747, 'total_duration': 14529.88004231453, 'accumulated_submission_time': 9855.15208029747, 'accumulated_eval_time': 4672.233599424362, 'accumulated_logging_time': 1.6509528160095215}
I0205 19:04:56.296603 140266960250624 logging_writer.py:48] [30789] accumulated_eval_time=4672.233599, accumulated_logging_time=1.650953, accumulated_submission_time=9855.152080, global_step=30789, preemption_count=0, score=9855.152080, test/accuracy=0.985684, test/loss=0.048131, test/mean_average_precision=0.261006, test/num_examples=43793, total_duration=14529.880042, train/accuracy=0.991719, train/loss=0.026826, train/mean_average_precision=0.475315, validation/accuracy=0.986645, validation/loss=0.045168, validation/mean_average_precision=0.270965, validation/num_examples=43793
I0205 19:05:00.230172 140283262007040 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.09884079545736313, loss=0.027769075706601143
I0205 19:05:31.936554 140266960250624 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.15667474269866943, loss=0.02751859650015831
I0205 19:06:03.482239 140283262007040 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.10230837762355804, loss=0.02774682268500328
I0205 19:06:34.890983 140266960250624 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.14120590686798096, loss=0.027284668758511543
I0205 19:07:06.245758 140283262007040 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.10144151002168655, loss=0.028901802375912666
I0205 19:07:38.492722 140266960250624 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.1059858575463295, loss=0.027491532266139984
I0205 19:08:09.832364 140283262007040 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.09942764788866043, loss=0.028571350499987602
I0205 19:08:41.325799 140266960250624 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.116689033806324, loss=0.024562565609812737
I0205 19:08:56.333733 140451058161472 spec.py:321] Evaluating on the training split.
I0205 19:10:34.409744 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 19:10:37.383934 140451058161472 spec.py:349] Evaluating on the test split.
I0205 19:10:40.301152 140451058161472 submission_runner.py:408] Time since start: 14873.91s, 	Step: 31549, 	{'train/accuracy': 0.9915143251419067, 'train/loss': 0.027306945994496346, 'train/mean_average_precision': 0.4664824860010296, 'validation/accuracy': 0.9866051077842712, 'validation/loss': 0.045458097010850906, 'validation/mean_average_precision': 0.27000922246488557, 'validation/num_examples': 43793, 'test/accuracy': 0.9858478307723999, 'test/loss': 0.048325251787900925, 'test/mean_average_precision': 0.26212106268357166, 'test/num_examples': 43793, 'score': 10095.158047437668, 'total_duration': 14873.906448364258, 'accumulated_submission_time': 10095.158047437668, 'accumulated_eval_time': 4776.200966119766, 'accumulated_logging_time': 1.6840364933013916}
I0205 19:10:40.322180 140283530442496 logging_writer.py:48] [31549] accumulated_eval_time=4776.200966, accumulated_logging_time=1.684036, accumulated_submission_time=10095.158047, global_step=31549, preemption_count=0, score=10095.158047, test/accuracy=0.985848, test/loss=0.048325, test/mean_average_precision=0.262121, test/num_examples=43793, total_duration=14873.906448, train/accuracy=0.991514, train/loss=0.027307, train/mean_average_precision=0.466482, validation/accuracy=0.986605, validation/loss=0.045458, validation/mean_average_precision=0.270009, validation/num_examples=43793
I0205 19:10:56.735611 140290204940032 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.12272127717733383, loss=0.031032560393214226
I0205 19:11:28.793876 140283530442496 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.08826357871294022, loss=0.02460174262523651
I0205 19:12:00.710799 140290204940032 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.11099015921354294, loss=0.03014749102294445
I0205 19:12:32.765522 140283530442496 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.0874251127243042, loss=0.028972072526812553
I0205 19:13:04.661860 140290204940032 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.12467171251773834, loss=0.025441229343414307
I0205 19:13:36.482089 140283530442496 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.09581071138381958, loss=0.025974228978157043
I0205 19:14:08.672785 140290204940032 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.10690536350011826, loss=0.02853429690003395
I0205 19:14:40.226595 140283530442496 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.10367480665445328, loss=0.02981013059616089
I0205 19:14:40.536578 140451058161472 spec.py:321] Evaluating on the training split.
I0205 19:16:20.522725 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 19:16:23.554222 140451058161472 spec.py:349] Evaluating on the test split.
I0205 19:16:26.560319 140451058161472 submission_runner.py:408] Time since start: 15220.17s, 	Step: 32302, 	{'train/accuracy': 0.9916272759437561, 'train/loss': 0.026981551200151443, 'train/mean_average_precision': 0.48471532963114583, 'validation/accuracy': 0.9866538643836975, 'validation/loss': 0.04501298442482948, 'validation/mean_average_precision': 0.27844504873588466, 'validation/num_examples': 43793, 'test/accuracy': 0.9857808351516724, 'test/loss': 0.04796469211578369, 'test/mean_average_precision': 0.2607573388347303, 'test/num_examples': 43793, 'score': 10335.340276241302, 'total_duration': 15220.165620326996, 'accumulated_submission_time': 10335.340276241302, 'accumulated_eval_time': 4882.224649429321, 'accumulated_logging_time': 1.7169504165649414}
I0205 19:16:26.582152 140266960250624 logging_writer.py:48] [32302] accumulated_eval_time=4882.224649, accumulated_logging_time=1.716950, accumulated_submission_time=10335.340276, global_step=32302, preemption_count=0, score=10335.340276, test/accuracy=0.985781, test/loss=0.047965, test/mean_average_precision=0.260757, test/num_examples=43793, total_duration=15220.165620, train/accuracy=0.991627, train/loss=0.026982, train/mean_average_precision=0.484715, validation/accuracy=0.986654, validation/loss=0.045013, validation/mean_average_precision=0.278445, validation/num_examples=43793
I0205 19:16:57.754095 140266968643328 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.12771379947662354, loss=0.03076764941215515
I0205 19:17:29.332437 140266960250624 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.09876351058483124, loss=0.025497807189822197
I0205 19:18:01.155627 140266968643328 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.1038341149687767, loss=0.028824657201766968
I0205 19:18:33.257082 140266960250624 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.13197845220565796, loss=0.027661766856908798
I0205 19:19:05.038750 140266968643328 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.10789532959461212, loss=0.02868586778640747
I0205 19:19:36.952383 140266960250624 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.12675145268440247, loss=0.028967153280973434
I0205 19:20:08.501639 140266968643328 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.13015593588352203, loss=0.026005815714597702
I0205 19:20:26.750114 140451058161472 spec.py:321] Evaluating on the training split.
I0205 19:22:08.012814 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 19:22:11.047688 140451058161472 spec.py:349] Evaluating on the test split.
I0205 19:22:14.038509 140451058161472 submission_runner.py:408] Time since start: 15567.64s, 	Step: 33058, 	{'train/accuracy': 0.9915615320205688, 'train/loss': 0.02697215974330902, 'train/mean_average_precision': 0.4788424207599614, 'validation/accuracy': 0.9867366552352905, 'validation/loss': 0.04553863778710365, 'validation/mean_average_precision': 0.26934329264440693, 'validation/num_examples': 43793, 'test/accuracy': 0.9859063625335693, 'test/loss': 0.048527345061302185, 'test/mean_average_precision': 0.2565631284387224, 'test/num_examples': 43793, 'score': 10575.47698545456, 'total_duration': 15567.64379477501, 'accumulated_submission_time': 10575.47698545456, 'accumulated_eval_time': 4989.512982130051, 'accumulated_logging_time': 1.749946117401123}
I0205 19:22:14.060645 140283262007040 logging_writer.py:48] [33058] accumulated_eval_time=4989.512982, accumulated_logging_time=1.749946, accumulated_submission_time=10575.476985, global_step=33058, preemption_count=0, score=10575.476985, test/accuracy=0.985906, test/loss=0.048527, test/mean_average_precision=0.256563, test/num_examples=43793, total_duration=15567.643795, train/accuracy=0.991562, train/loss=0.026972, train/mean_average_precision=0.478842, validation/accuracy=0.986737, validation/loss=0.045539, validation/mean_average_precision=0.269343, validation/num_examples=43793
I0205 19:22:27.725812 140283530442496 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.10354965925216675, loss=0.02687213383615017
I0205 19:22:59.777440 140283262007040 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.13128678500652313, loss=0.029566772282123566
I0205 19:23:31.871470 140283530442496 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.12247961014509201, loss=0.02825794741511345
I0205 19:24:04.170191 140283262007040 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.09864955395460129, loss=0.024799257516860962
I0205 19:24:36.102594 140283530442496 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.10400909185409546, loss=0.023169834166765213
I0205 19:25:08.735649 140283262007040 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.10379090905189514, loss=0.02656782791018486
I0205 19:25:41.018939 140283530442496 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.09773328900337219, loss=0.02645036205649376
I0205 19:26:13.247543 140283262007040 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.10066785663366318, loss=0.027503568679094315
I0205 19:26:14.200011 140451058161472 spec.py:321] Evaluating on the training split.
I0205 19:27:59.496906 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 19:28:02.713559 140451058161472 spec.py:349] Evaluating on the test split.
I0205 19:28:05.676258 140451058161472 submission_runner.py:408] Time since start: 15919.28s, 	Step: 33804, 	{'train/accuracy': 0.991463303565979, 'train/loss': 0.027266643941402435, 'train/mean_average_precision': 0.47013349861063486, 'validation/accuracy': 0.986585259437561, 'validation/loss': 0.04563891142606735, 'validation/mean_average_precision': 0.2751634706236099, 'validation/num_examples': 43793, 'test/accuracy': 0.9856860637664795, 'test/loss': 0.0485675148665905, 'test/mean_average_precision': 0.2549646396946223, 'test/num_examples': 43793, 'score': 10815.58456158638, 'total_duration': 15919.281561613083, 'accumulated_submission_time': 10815.58456158638, 'accumulated_eval_time': 5100.989178657532, 'accumulated_logging_time': 1.7831127643585205}
I0205 19:28:05.697910 140266960250624 logging_writer.py:48] [33804] accumulated_eval_time=5100.989179, accumulated_logging_time=1.783113, accumulated_submission_time=10815.584562, global_step=33804, preemption_count=0, score=10815.584562, test/accuracy=0.985686, test/loss=0.048568, test/mean_average_precision=0.254965, test/num_examples=43793, total_duration=15919.281562, train/accuracy=0.991463, train/loss=0.027267, train/mean_average_precision=0.470133, validation/accuracy=0.986585, validation/loss=0.045639, validation/mean_average_precision=0.275163, validation/num_examples=43793
I0205 19:28:36.628534 140266968643328 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.09851677715778351, loss=0.026350747793912888
I0205 19:29:08.516099 140266960250624 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.13750334084033966, loss=0.031018510460853577
I0205 19:29:40.256525 140266968643328 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.09718377143144608, loss=0.02534390054643154
I0205 19:30:12.102697 140266960250624 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.09253952652215958, loss=0.026542838662862778
I0205 19:30:43.473533 140266968643328 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.1143559068441391, loss=0.025542806833982468
I0205 19:31:14.982787 140266960250624 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.08569829165935516, loss=0.02312811277806759
I0205 19:31:46.668554 140266968643328 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.12875358760356903, loss=0.02803044021129608
I0205 19:32:05.905622 140451058161472 spec.py:321] Evaluating on the training split.
I0205 19:33:45.359045 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 19:33:48.445860 140451058161472 spec.py:349] Evaluating on the test split.
I0205 19:33:51.496694 140451058161472 submission_runner.py:408] Time since start: 16265.10s, 	Step: 34562, 	{'train/accuracy': 0.9917646646499634, 'train/loss': 0.026229219511151314, 'train/mean_average_precision': 0.4959750658599631, 'validation/accuracy': 0.9866027235984802, 'validation/loss': 0.04598342999815941, 'validation/mean_average_precision': 0.26992419653193517, 'validation/num_examples': 43793, 'test/accuracy': 0.9858149886131287, 'test/loss': 0.048810895532369614, 'test/mean_average_precision': 0.2672977632378215, 'test/num_examples': 43793, 'score': 11055.760778903961, 'total_duration': 16265.101999282837, 'accumulated_submission_time': 11055.760778903961, 'accumulated_eval_time': 5206.580208301544, 'accumulated_logging_time': 1.8159537315368652}
I0205 19:33:51.518651 140283262007040 logging_writer.py:48] [34562] accumulated_eval_time=5206.580208, accumulated_logging_time=1.815954, accumulated_submission_time=11055.760779, global_step=34562, preemption_count=0, score=11055.760779, test/accuracy=0.985815, test/loss=0.048811, test/mean_average_precision=0.267298, test/num_examples=43793, total_duration=16265.101999, train/accuracy=0.991765, train/loss=0.026229, train/mean_average_precision=0.495975, validation/accuracy=0.986603, validation/loss=0.045983, validation/mean_average_precision=0.269924, validation/num_examples=43793
I0205 19:34:03.951510 140290204940032 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.07953859865665436, loss=0.024537529796361923
I0205 19:34:35.345836 140283262007040 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.10780410468578339, loss=0.027870506048202515
I0205 19:35:06.696887 140290204940032 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.11736315488815308, loss=0.026186171919107437
I0205 19:35:37.858389 140283262007040 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.1000586450099945, loss=0.026216134428977966
I0205 19:36:09.432780 140290204940032 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.10428933054208755, loss=0.026633821427822113
I0205 19:36:41.858690 140283262007040 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.1336628645658493, loss=0.027233028784394264
I0205 19:37:13.689923 140290204940032 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.10156219452619553, loss=0.023945322260260582
I0205 19:37:45.044545 140283262007040 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.14504098892211914, loss=0.02787359431385994
I0205 19:37:51.745109 140451058161472 spec.py:321] Evaluating on the training split.
I0205 19:39:33.019088 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 19:39:36.097970 140451058161472 spec.py:349] Evaluating on the test split.
I0205 19:39:39.085622 140451058161472 submission_runner.py:408] Time since start: 16612.69s, 	Step: 35322, 	{'train/accuracy': 0.9921203255653381, 'train/loss': 0.025237414985895157, 'train/mean_average_precision': 0.5261174470746846, 'validation/accuracy': 0.9866863489151001, 'validation/loss': 0.045371729880571365, 'validation/mean_average_precision': 0.2695367499370574, 'validation/num_examples': 43793, 'test/accuracy': 0.9858158230781555, 'test/loss': 0.048591263592243195, 'test/mean_average_precision': 0.26026593774116824, 'test/num_examples': 43793, 'score': 11295.954808950424, 'total_duration': 16612.69092822075, 'accumulated_submission_time': 11295.954808950424, 'accumulated_eval_time': 5313.920674800873, 'accumulated_logging_time': 1.8492765426635742}
I0205 19:39:39.107716 140266960250624 logging_writer.py:48] [35322] accumulated_eval_time=5313.920675, accumulated_logging_time=1.849277, accumulated_submission_time=11295.954809, global_step=35322, preemption_count=0, score=11295.954809, test/accuracy=0.985816, test/loss=0.048591, test/mean_average_precision=0.260266, test/num_examples=43793, total_duration=16612.690928, train/accuracy=0.992120, train/loss=0.025237, train/mean_average_precision=0.526117, validation/accuracy=0.986686, validation/loss=0.045372, validation/mean_average_precision=0.269537, validation/num_examples=43793
I0205 19:40:04.672863 140283530442496 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.11179623007774353, loss=0.02661013789474964
I0205 19:40:36.317702 140266960250624 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.1236477866768837, loss=0.029509134590625763
I0205 19:41:08.342206 140283530442496 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.1062464788556099, loss=0.0253012552857399
I0205 19:41:40.141843 140266960250624 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.10089898109436035, loss=0.02666608989238739
I0205 19:42:11.981054 140283530442496 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.10071497410535812, loss=0.024565989151597023
I0205 19:42:43.871623 140266960250624 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.1238783597946167, loss=0.027135564014315605
I0205 19:43:15.526237 140283530442496 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.1076696366071701, loss=0.024314643815159798
I0205 19:43:39.172480 140451058161472 spec.py:321] Evaluating on the training split.
I0205 19:45:16.424090 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 19:45:19.436869 140451058161472 spec.py:349] Evaluating on the test split.
I0205 19:45:22.436492 140451058161472 submission_runner.py:408] Time since start: 16956.04s, 	Step: 36076, 	{'train/accuracy': 0.9921783804893494, 'train/loss': 0.024977702647447586, 'train/mean_average_precision': 0.5172506535598527, 'validation/accuracy': 0.9864655137062073, 'validation/loss': 0.04580044001340866, 'validation/mean_average_precision': 0.27123029214973643, 'validation/num_examples': 43793, 'test/accuracy': 0.9856621026992798, 'test/loss': 0.048594508320093155, 'test/mean_average_precision': 0.2601393911286229, 'test/num_examples': 43793, 'score': 11535.98846077919, 'total_duration': 16956.04179906845, 'accumulated_submission_time': 11535.98846077919, 'accumulated_eval_time': 5417.184643983841, 'accumulated_logging_time': 1.882709264755249}
I0205 19:45:22.458806 140266968643328 logging_writer.py:48] [36076] accumulated_eval_time=5417.184644, accumulated_logging_time=1.882709, accumulated_submission_time=11535.988461, global_step=36076, preemption_count=0, score=11535.988461, test/accuracy=0.985662, test/loss=0.048595, test/mean_average_precision=0.260139, test/num_examples=43793, total_duration=16956.041799, train/accuracy=0.992178, train/loss=0.024978, train/mean_average_precision=0.517251, validation/accuracy=0.986466, validation/loss=0.045800, validation/mean_average_precision=0.271230, validation/num_examples=43793
I0205 19:45:30.429762 140283262007040 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.12672804296016693, loss=0.026168499141931534
I0205 19:46:02.179700 140266968643328 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.1421373337507248, loss=0.026402005925774574
I0205 19:46:34.010019 140283262007040 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.13173314929008484, loss=0.026830734685063362
I0205 19:47:05.886868 140266968643328 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.12516562640666962, loss=0.025675863027572632
I0205 19:47:37.170138 140283262007040 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.12412289530038834, loss=0.028188787400722504
I0205 19:48:08.747005 140266968643328 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.11356222629547119, loss=0.024908825755119324
I0205 19:48:40.077549 140283262007040 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.09584326297044754, loss=0.024596301838755608
I0205 19:49:12.058579 140266968643328 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.11529377102851868, loss=0.024936091154813766
I0205 19:49:22.735362 140451058161472 spec.py:321] Evaluating on the training split.
I0205 19:51:01.109341 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 19:51:04.362938 140451058161472 spec.py:349] Evaluating on the test split.
I0205 19:51:07.415623 140451058161472 submission_runner.py:408] Time since start: 17301.02s, 	Step: 36835, 	{'train/accuracy': 0.9925355911254883, 'train/loss': 0.023661969229578972, 'train/mean_average_precision': 0.5555088251116154, 'validation/accuracy': 0.9866177439689636, 'validation/loss': 0.046089913696050644, 'validation/mean_average_precision': 0.2748257784845323, 'validation/num_examples': 43793, 'test/accuracy': 0.9857437610626221, 'test/loss': 0.04906424134969711, 'test/mean_average_precision': 0.2600800066140546, 'test/num_examples': 43793, 'score': 11776.233745574951, 'total_duration': 17301.020915985107, 'accumulated_submission_time': 11776.233745574951, 'accumulated_eval_time': 5521.864846467972, 'accumulated_logging_time': 1.9163761138916016}
I0205 19:51:07.445250 140266960250624 logging_writer.py:48] [36835] accumulated_eval_time=5521.864846, accumulated_logging_time=1.916376, accumulated_submission_time=11776.233746, global_step=36835, preemption_count=0, score=11776.233746, test/accuracy=0.985744, test/loss=0.049064, test/mean_average_precision=0.260080, test/num_examples=43793, total_duration=17301.020916, train/accuracy=0.992536, train/loss=0.023662, train/mean_average_precision=0.555509, validation/accuracy=0.986618, validation/loss=0.046090, validation/mean_average_precision=0.274826, validation/num_examples=43793
I0205 19:51:28.636223 140290204940032 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.1286420077085495, loss=0.026761965826153755
I0205 19:52:00.997354 140266960250624 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.10968410968780518, loss=0.026958849281072617
I0205 19:52:32.888260 140290204940032 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.11105558276176453, loss=0.021667061373591423
I0205 19:53:04.406754 140266960250624 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.17927202582359314, loss=0.027805708348751068
I0205 19:53:35.829540 140290204940032 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.16629646718502045, loss=0.028722640126943588
I0205 19:54:08.066356 140266960250624 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.12538166344165802, loss=0.026167580857872963
I0205 19:54:39.774586 140290204940032 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.11941110342741013, loss=0.0262395441532135
I0205 19:55:07.469882 140451058161472 spec.py:321] Evaluating on the training split.
I0205 19:56:52.324000 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 19:56:55.596347 140451058161472 spec.py:349] Evaluating on the test split.
I0205 19:56:58.908377 140451058161472 submission_runner.py:408] Time since start: 17652.51s, 	Step: 37588, 	{'train/accuracy': 0.9923935532569885, 'train/loss': 0.024036496877670288, 'train/mean_average_precision': 0.550742001196731, 'validation/accuracy': 0.9867110848426819, 'validation/loss': 0.04626239836215973, 'validation/mean_average_precision': 0.2691423838406784, 'validation/num_examples': 43793, 'test/accuracy': 0.9858764410018921, 'test/loss': 0.04935387894511223, 'test/mean_average_precision': 0.2592533850321455, 'test/num_examples': 43793, 'score': 12016.22732925415, 'total_duration': 17652.513543844223, 'accumulated_submission_time': 12016.22732925415, 'accumulated_eval_time': 5633.3031594753265, 'accumulated_logging_time': 1.9569110870361328}
I0205 19:56:58.934174 140266968643328 logging_writer.py:48] [37588] accumulated_eval_time=5633.303159, accumulated_logging_time=1.956911, accumulated_submission_time=12016.227329, global_step=37588, preemption_count=0, score=12016.227329, test/accuracy=0.985876, test/loss=0.049354, test/mean_average_precision=0.259253, test/num_examples=43793, total_duration=17652.513544, train/accuracy=0.992394, train/loss=0.024036, train/mean_average_precision=0.550742, validation/accuracy=0.986711, validation/loss=0.046262, validation/mean_average_precision=0.269142, validation/num_examples=43793
I0205 19:57:03.314183 140283530442496 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.11160369217395782, loss=0.028069524094462395
I0205 19:57:35.331587 140266968643328 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.09764792770147324, loss=0.02438366785645485
I0205 19:58:07.376499 140283530442496 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.15691323578357697, loss=0.02625671960413456
I0205 19:58:39.585998 140266968643328 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.13072045147418976, loss=0.024381903931498528
I0205 19:59:11.963281 140283530442496 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.11132942885160446, loss=0.026143060997128487
I0205 19:59:44.199259 140266968643328 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.11669960618019104, loss=0.027033606544137
I0205 20:00:16.631545 140283530442496 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.11972089111804962, loss=0.02632818929851055
I0205 20:00:48.409727 140266968643328 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.113092340528965, loss=0.024243250489234924
I0205 20:00:59.196729 140451058161472 spec.py:321] Evaluating on the training split.
I0205 20:02:44.208425 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 20:02:47.404780 140451058161472 spec.py:349] Evaluating on the test split.
I0205 20:02:50.554611 140451058161472 submission_runner.py:408] Time since start: 18004.16s, 	Step: 38335, 	{'train/accuracy': 0.9923391938209534, 'train/loss': 0.024496717378497124, 'train/mean_average_precision': 0.5252410837212944, 'validation/accuracy': 0.986572265625, 'validation/loss': 0.045990101993083954, 'validation/mean_average_precision': 0.2755666996524653, 'validation/num_examples': 43793, 'test/accuracy': 0.9856947064399719, 'test/loss': 0.04922757297754288, 'test/mean_average_precision': 0.264713702862164, 'test/num_examples': 43793, 'score': 12256.457571268082, 'total_duration': 18004.159906864166, 'accumulated_submission_time': 12256.457571268082, 'accumulated_eval_time': 5744.66099023819, 'accumulated_logging_time': 1.9948585033416748}
I0205 20:02:50.577457 140266960250624 logging_writer.py:48] [38335] accumulated_eval_time=5744.660990, accumulated_logging_time=1.994859, accumulated_submission_time=12256.457571, global_step=38335, preemption_count=0, score=12256.457571, test/accuracy=0.985695, test/loss=0.049228, test/mean_average_precision=0.264714, test/num_examples=43793, total_duration=18004.159907, train/accuracy=0.992339, train/loss=0.024497, train/mean_average_precision=0.525241, validation/accuracy=0.986572, validation/loss=0.045990, validation/mean_average_precision=0.275567, validation/num_examples=43793
I0205 20:03:11.953560 140290204940032 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.16479800641536713, loss=0.024609742686152458
I0205 20:03:43.579386 140266960250624 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.13739049434661865, loss=0.021495386958122253
I0205 20:04:15.726599 140290204940032 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.11828345060348511, loss=0.02753535285592079
I0205 20:04:47.654012 140266960250624 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.11855073273181915, loss=0.022374268621206284
I0205 20:05:19.713938 140290204940032 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.10234465450048447, loss=0.022737981751561165
I0205 20:05:51.086029 140266960250624 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.1122485101222992, loss=0.024923600256443024
I0205 20:06:23.289294 140290204940032 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.11732254177331924, loss=0.023280350491404533
I0205 20:06:50.841161 140451058161472 spec.py:321] Evaluating on the training split.
I0205 20:08:27.814568 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 20:08:30.829073 140451058161472 spec.py:349] Evaluating on the test split.
I0205 20:08:33.793667 140451058161472 submission_runner.py:408] Time since start: 18347.40s, 	Step: 39088, 	{'train/accuracy': 0.9921321868896484, 'train/loss': 0.025087174028158188, 'train/mean_average_precision': 0.5257421666684701, 'validation/accuracy': 0.9865466952323914, 'validation/loss': 0.046366214752197266, 'validation/mean_average_precision': 0.27294682753135996, 'validation/num_examples': 43793, 'test/accuracy': 0.985687792301178, 'test/loss': 0.04935147985816002, 'test/mean_average_precision': 0.26225860070596524, 'test/num_examples': 43793, 'score': 12496.689729452133, 'total_duration': 18347.398972272873, 'accumulated_submission_time': 12496.689729452133, 'accumulated_eval_time': 5847.613451719284, 'accumulated_logging_time': 2.028700351715088}
I0205 20:08:33.816117 140266968643328 logging_writer.py:48] [39088] accumulated_eval_time=5847.613452, accumulated_logging_time=2.028700, accumulated_submission_time=12496.689729, global_step=39088, preemption_count=0, score=12496.689729, test/accuracy=0.985688, test/loss=0.049351, test/mean_average_precision=0.262259, test/num_examples=43793, total_duration=18347.398972, train/accuracy=0.992132, train/loss=0.025087, train/mean_average_precision=0.525742, validation/accuracy=0.986547, validation/loss=0.046366, validation/mean_average_precision=0.272947, validation/num_examples=43793
I0205 20:08:38.023696 140283530442496 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.11156799644231796, loss=0.02411266788840294
I0205 20:09:09.822439 140266968643328 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.10919570922851562, loss=0.026675499975681305
I0205 20:09:41.406510 140283530442496 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.13041551411151886, loss=0.028467562049627304
I0205 20:10:13.170228 140266968643328 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.12656307220458984, loss=0.024999497458338737
I0205 20:10:44.605282 140283530442496 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.1333104521036148, loss=0.024074804037809372
I0205 20:11:16.178804 140266968643328 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.14435485005378723, loss=0.02571963332593441
I0205 20:11:47.686202 140283530442496 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.1511157751083374, loss=0.022359561175107956
I0205 20:12:19.457104 140266968643328 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.13947027921676636, loss=0.02535964362323284
I0205 20:12:33.964550 140451058161472 spec.py:321] Evaluating on the training split.
I0205 20:14:16.391180 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 20:14:19.351229 140451058161472 spec.py:349] Evaluating on the test split.
I0205 20:14:22.339385 140451058161472 submission_runner.py:408] Time since start: 18695.94s, 	Step: 39848, 	{'train/accuracy': 0.992060124874115, 'train/loss': 0.025317229330539703, 'train/mean_average_precision': 0.5124504340697817, 'validation/accuracy': 0.9864711761474609, 'validation/loss': 0.04643158242106438, 'validation/mean_average_precision': 0.27471431599247587, 'validation/num_examples': 43793, 'test/accuracy': 0.985620379447937, 'test/loss': 0.04942890629172325, 'test/mean_average_precision': 0.25711138588610166, 'test/num_examples': 43793, 'score': 12736.806010484695, 'total_duration': 18695.944691181183, 'accumulated_submission_time': 12736.806010484695, 'accumulated_eval_time': 5955.988241195679, 'accumulated_logging_time': 2.06325101852417}
I0205 20:14:22.362242 140266960250624 logging_writer.py:48] [39848] accumulated_eval_time=5955.988241, accumulated_logging_time=2.063251, accumulated_submission_time=12736.806010, global_step=39848, preemption_count=0, score=12736.806010, test/accuracy=0.985620, test/loss=0.049429, test/mean_average_precision=0.257111, test/num_examples=43793, total_duration=18695.944691, train/accuracy=0.992060, train/loss=0.025317, train/mean_average_precision=0.512450, validation/accuracy=0.986471, validation/loss=0.046432, validation/mean_average_precision=0.274714, validation/num_examples=43793
I0205 20:14:39.393501 140290204940032 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.11385318636894226, loss=0.024739040061831474
I0205 20:15:11.442646 140266960250624 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.17131833732128143, loss=0.025870000943541527
I0205 20:15:44.205178 140290204940032 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.14648142457008362, loss=0.025539912283420563
I0205 20:16:17.636320 140266960250624 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.1575419157743454, loss=0.027267808094620705
I0205 20:16:50.790622 140290204940032 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.15105889737606049, loss=0.0243625957518816
I0205 20:17:23.066709 140266960250624 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.11431055516004562, loss=0.022913675755262375
I0205 20:17:54.907908 140290204940032 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.12455775588750839, loss=0.026979144662618637
I0205 20:18:22.630871 140451058161472 spec.py:321] Evaluating on the training split.
I0205 20:19:58.716747 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 20:20:01.717087 140451058161472 spec.py:349] Evaluating on the test split.
I0205 20:20:04.750600 140451058161472 submission_runner.py:408] Time since start: 19038.36s, 	Step: 40588, 	{'train/accuracy': 0.9921411871910095, 'train/loss': 0.025021405890583992, 'train/mean_average_precision': 0.5354935396143286, 'validation/accuracy': 0.9864374995231628, 'validation/loss': 0.04617885500192642, 'validation/mean_average_precision': 0.2703810326860346, 'validation/num_examples': 43793, 'test/accuracy': 0.9857370257377625, 'test/loss': 0.04916870966553688, 'test/mean_average_precision': 0.26094914867589253, 'test/num_examples': 43793, 'score': 12977.04095864296, 'total_duration': 19038.35589647293, 'accumulated_submission_time': 12977.04095864296, 'accumulated_eval_time': 6058.107916593552, 'accumulated_logging_time': 2.097200632095337}
I0205 20:20:04.780816 140266968643328 logging_writer.py:48] [40588] accumulated_eval_time=6058.107917, accumulated_logging_time=2.097201, accumulated_submission_time=12977.040959, global_step=40588, preemption_count=0, score=12977.040959, test/accuracy=0.985737, test/loss=0.049169, test/mean_average_precision=0.260949, test/num_examples=43793, total_duration=19038.355896, train/accuracy=0.992141, train/loss=0.025021, train/mean_average_precision=0.535494, validation/accuracy=0.986437, validation/loss=0.046179, validation/mean_average_precision=0.270381, validation/num_examples=43793
I0205 20:20:09.031148 140283262007040 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.1312776803970337, loss=0.026719098910689354
I0205 20:20:40.667149 140266968643328 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.1218733936548233, loss=0.023234035819768906
I0205 20:21:12.484045 140283262007040 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.14874179661273956, loss=0.024711990728974342
I0205 20:21:44.084965 140266968643328 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.11327993124723434, loss=0.025291725993156433
I0205 20:22:15.568198 140283262007040 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.11546061187982559, loss=0.02400095760822296
I0205 20:22:46.947363 140266968643328 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.11387760937213898, loss=0.022541943937540054
I0205 20:23:18.651116 140283262007040 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.1321220099925995, loss=0.02273239940404892
I0205 20:23:50.169559 140266968643328 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.13584354519844055, loss=0.026379507035017014
I0205 20:24:04.999909 140451058161472 spec.py:321] Evaluating on the training split.
I0205 20:25:47.052011 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 20:25:50.074044 140451058161472 spec.py:349] Evaluating on the test split.
I0205 20:25:53.001110 140451058161472 submission_runner.py:408] Time since start: 19386.61s, 	Step: 41348, 	{'train/accuracy': 0.992215633392334, 'train/loss': 0.024663478136062622, 'train/mean_average_precision': 0.5279078039982317, 'validation/accuracy': 0.9865621328353882, 'validation/loss': 0.04639315977692604, 'validation/mean_average_precision': 0.27348047653523444, 'validation/num_examples': 43793, 'test/accuracy': 0.9857257008552551, 'test/loss': 0.04914647340774536, 'test/mean_average_precision': 0.26264856029123873, 'test/num_examples': 43793, 'score': 13217.228226184845, 'total_duration': 19386.606401205063, 'accumulated_submission_time': 13217.228226184845, 'accumulated_eval_time': 6166.109060764313, 'accumulated_logging_time': 2.139084815979004}
I0205 20:25:53.024242 140266960250624 logging_writer.py:48] [41348] accumulated_eval_time=6166.109061, accumulated_logging_time=2.139085, accumulated_submission_time=13217.228226, global_step=41348, preemption_count=0, score=13217.228226, test/accuracy=0.985726, test/loss=0.049146, test/mean_average_precision=0.262649, test/num_examples=43793, total_duration=19386.606401, train/accuracy=0.992216, train/loss=0.024663, train/mean_average_precision=0.527908, validation/accuracy=0.986562, validation/loss=0.046393, validation/mean_average_precision=0.273480, validation/num_examples=43793
I0205 20:26:10.023242 140283530442496 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.13013334572315216, loss=0.026663849130272865
I0205 20:26:41.609135 140266960250624 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.13312295079231262, loss=0.025633065029978752
I0205 20:27:13.319879 140283530442496 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.12411979585886002, loss=0.023086868226528168
I0205 20:27:45.314220 140266960250624 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.12427901476621628, loss=0.02244533784687519
I0205 20:28:17.385812 140283530442496 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.12356909364461899, loss=0.0216864962130785
I0205 20:28:50.429141 140266960250624 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.10431718081235886, loss=0.02512398362159729
I0205 20:29:23.694118 140283530442496 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.11993184685707092, loss=0.023767152801156044
I0205 20:29:53.268794 140451058161472 spec.py:321] Evaluating on the training split.
I0205 20:31:37.542160 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 20:31:40.555034 140451058161472 spec.py:349] Evaluating on the test split.
I0205 20:31:43.565096 140451058161472 submission_runner.py:408] Time since start: 19737.17s, 	Step: 42093, 	{'train/accuracy': 0.9923676252365112, 'train/loss': 0.024189790710806847, 'train/mean_average_precision': 0.5447008755982579, 'validation/accuracy': 0.9866648316383362, 'validation/loss': 0.046636130660772324, 'validation/mean_average_precision': 0.27019773042277423, 'validation/num_examples': 43793, 'test/accuracy': 0.9857661128044128, 'test/loss': 0.049860917031764984, 'test/mean_average_precision': 0.25911789760313175, 'test/num_examples': 43793, 'score': 13457.440511703491, 'total_duration': 19737.170392751694, 'accumulated_submission_time': 13457.440511703491, 'accumulated_eval_time': 6276.40532040596, 'accumulated_logging_time': 2.1731395721435547}
I0205 20:31:43.588818 140266968643328 logging_writer.py:48] [42093] accumulated_eval_time=6276.405320, accumulated_logging_time=2.173140, accumulated_submission_time=13457.440512, global_step=42093, preemption_count=0, score=13457.440512, test/accuracy=0.985766, test/loss=0.049861, test/mean_average_precision=0.259118, test/num_examples=43793, total_duration=19737.170393, train/accuracy=0.992368, train/loss=0.024190, train/mean_average_precision=0.544701, validation/accuracy=0.986665, validation/loss=0.046636, validation/mean_average_precision=0.270198, validation/num_examples=43793
I0205 20:31:46.110508 140290204940032 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.1376691609621048, loss=0.024966128170490265
I0205 20:32:17.770863 140266968643328 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.12362028658390045, loss=0.025118382647633553
I0205 20:32:49.097808 140290204940032 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.1198456659913063, loss=0.023997755721211433
I0205 20:33:20.740789 140266968643328 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.12144511938095093, loss=0.0237092487514019
I0205 20:33:52.631880 140290204940032 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.13480916619300842, loss=0.021585555747151375
I0205 20:34:24.387121 140266968643328 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.13566239178180695, loss=0.02432367391884327
I0205 20:34:55.893688 140290204940032 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.14701765775680542, loss=0.0240729171782732
I0205 20:35:27.351716 140266968643328 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.14617711305618286, loss=0.02405865490436554
I0205 20:35:43.599663 140451058161472 spec.py:321] Evaluating on the training split.
I0205 20:37:29.368288 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 20:37:32.409852 140451058161472 spec.py:349] Evaluating on the test split.
I0205 20:37:35.333582 140451058161472 submission_runner.py:408] Time since start: 20088.94s, 	Step: 42853, 	{'train/accuracy': 0.9926583170890808, 'train/loss': 0.023178569972515106, 'train/mean_average_precision': 0.5682554598312384, 'validation/accuracy': 0.9865312576293945, 'validation/loss': 0.046705130487680435, 'validation/mean_average_precision': 0.27624668941734537, 'validation/num_examples': 43793, 'test/accuracy': 0.9856052398681641, 'test/loss': 0.04985882714390755, 'test/mean_average_precision': 0.26457020850963203, 'test/num_examples': 43793, 'score': 13697.419059038162, 'total_duration': 20088.938884973526, 'accumulated_submission_time': 13697.419059038162, 'accumulated_eval_time': 6388.139190912247, 'accumulated_logging_time': 2.2093663215637207}
I0205 20:37:35.357734 140283262007040 logging_writer.py:48] [42853] accumulated_eval_time=6388.139191, accumulated_logging_time=2.209366, accumulated_submission_time=13697.419059, global_step=42853, preemption_count=0, score=13697.419059, test/accuracy=0.985605, test/loss=0.049859, test/mean_average_precision=0.264570, test/num_examples=43793, total_duration=20088.938885, train/accuracy=0.992658, train/loss=0.023179, train/mean_average_precision=0.568255, validation/accuracy=0.986531, validation/loss=0.046705, validation/mean_average_precision=0.276247, validation/num_examples=43793
I0205 20:37:50.719320 140283530442496 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.16447535157203674, loss=0.02675493061542511
I0205 20:38:22.461429 140283262007040 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.16336765885353088, loss=0.024568967521190643
I0205 20:38:53.923418 140283530442496 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.12301532179117203, loss=0.02568366564810276
I0205 20:39:25.796132 140283262007040 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.13748686015605927, loss=0.02404133602976799
I0205 20:39:57.599934 140283530442496 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.13679145276546478, loss=0.02259010449051857
I0205 20:40:29.468163 140283262007040 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.143594428896904, loss=0.02309117093682289
I0205 20:41:01.434027 140283530442496 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.1256217211484909, loss=0.023860925808548927
I0205 20:41:33.248733 140283262007040 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.12924955785274506, loss=0.02463669516146183
I0205 20:41:35.589149 140451058161472 spec.py:321] Evaluating on the training split.
I0205 20:43:12.457767 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 20:43:15.460219 140451058161472 spec.py:349] Evaluating on the test split.
I0205 20:43:18.379332 140451058161472 submission_runner.py:408] Time since start: 20431.98s, 	Step: 43608, 	{'train/accuracy': 0.9930390119552612, 'train/loss': 0.022058961912989616, 'train/mean_average_precision': 0.610813742315912, 'validation/accuracy': 0.986449658870697, 'validation/loss': 0.04667244106531143, 'validation/mean_average_precision': 0.277682788359816, 'validation/num_examples': 43793, 'test/accuracy': 0.9856485724449158, 'test/loss': 0.04956782981753349, 'test/mean_average_precision': 0.2643410263914481, 'test/num_examples': 43793, 'score': 13937.618394374847, 'total_duration': 20431.984637498856, 'accumulated_submission_time': 13937.618394374847, 'accumulated_eval_time': 6490.929327249527, 'accumulated_logging_time': 2.2449228763580322}
I0205 20:43:18.402975 140266960250624 logging_writer.py:48] [43608] accumulated_eval_time=6490.929327, accumulated_logging_time=2.244923, accumulated_submission_time=13937.618394, global_step=43608, preemption_count=0, score=13937.618394, test/accuracy=0.985649, test/loss=0.049568, test/mean_average_precision=0.264341, test/num_examples=43793, total_duration=20431.984637, train/accuracy=0.993039, train/loss=0.022059, train/mean_average_precision=0.610814, validation/accuracy=0.986450, validation/loss=0.046672, validation/mean_average_precision=0.277683, validation/num_examples=43793
I0205 20:43:47.929894 140290204940032 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.1412249058485031, loss=0.023688599467277527
I0205 20:44:19.218888 140266960250624 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.14702264964580536, loss=0.027299880981445312
I0205 20:44:50.485381 140290204940032 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.12652765214443207, loss=0.02416108176112175
I0205 20:45:21.860284 140266960250624 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.1653115302324295, loss=0.024775756523013115
I0205 20:45:53.085473 140290204940032 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.134707510471344, loss=0.024534843862056732
I0205 20:46:25.515053 140266960250624 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.12202838808298111, loss=0.022940514609217644
I0205 20:46:58.224508 140290204940032 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.13298186659812927, loss=0.0214300025254488
I0205 20:47:18.555476 140451058161472 spec.py:321] Evaluating on the training split.
I0205 20:48:57.296071 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 20:49:00.285273 140451058161472 spec.py:349] Evaluating on the test split.
I0205 20:49:03.544419 140451058161472 submission_runner.py:408] Time since start: 20777.15s, 	Step: 44364, 	{'train/accuracy': 0.9931018352508545, 'train/loss': 0.021798377856612206, 'train/mean_average_precision': 0.5935135327960487, 'validation/accuracy': 0.9863875508308411, 'validation/loss': 0.04711327329277992, 'validation/mean_average_precision': 0.2702936182597056, 'validation/num_examples': 43793, 'test/accuracy': 0.9856389164924622, 'test/loss': 0.04985194280743599, 'test/mean_average_precision': 0.25856992277601665, 'test/num_examples': 43793, 'score': 14177.737513303757, 'total_duration': 20777.149724960327, 'accumulated_submission_time': 14177.737513303757, 'accumulated_eval_time': 6595.918229103088, 'accumulated_logging_time': 2.280789852142334}
I0205 20:49:03.573633 140266968643328 logging_writer.py:48] [44364] accumulated_eval_time=6595.918229, accumulated_logging_time=2.280790, accumulated_submission_time=14177.737513, global_step=44364, preemption_count=0, score=14177.737513, test/accuracy=0.985639, test/loss=0.049852, test/mean_average_precision=0.258570, test/num_examples=43793, total_duration=20777.149725, train/accuracy=0.993102, train/loss=0.021798, train/mean_average_precision=0.593514, validation/accuracy=0.986388, validation/loss=0.047113, validation/mean_average_precision=0.270294, validation/num_examples=43793
I0205 20:49:15.468211 140283530442496 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.16960178315639496, loss=0.027203911915421486
I0205 20:49:47.492282 140266968643328 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.13392117619514465, loss=0.023749694228172302
I0205 20:50:19.480360 140283530442496 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.12845087051391602, loss=0.021369874477386475
I0205 20:50:51.137501 140266968643328 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.12090745568275452, loss=0.023625878617167473
I0205 20:51:23.038675 140283530442496 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.13481444120407104, loss=0.02482328563928604
I0205 20:51:54.531210 140266968643328 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.12976707518100739, loss=0.021361511200666428
I0205 20:52:26.434331 140283530442496 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.15428726375102997, loss=0.02473134733736515
I0205 20:52:57.664550 140266968643328 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.15365850925445557, loss=0.026300812140107155
I0205 20:53:03.591530 140451058161472 spec.py:321] Evaluating on the training split.
I0205 20:54:42.572634 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 20:54:45.539552 140451058161472 spec.py:349] Evaluating on the test split.
I0205 20:54:48.459975 140451058161472 submission_runner.py:408] Time since start: 21122.07s, 	Step: 45119, 	{'train/accuracy': 0.9935007691383362, 'train/loss': 0.02077432908117771, 'train/mean_average_precision': 0.6282470077284659, 'validation/accuracy': 0.986407458782196, 'validation/loss': 0.047308359295129776, 'validation/mean_average_precision': 0.2746351986985576, 'validation/num_examples': 43793, 'test/accuracy': 0.9856823086738586, 'test/loss': 0.05051201581954956, 'test/mean_average_precision': 0.2607793049964524, 'test/num_examples': 43793, 'score': 14417.723826646805, 'total_duration': 21122.0652821064, 'accumulated_submission_time': 14417.723826646805, 'accumulated_eval_time': 6700.786629199982, 'accumulated_logging_time': 2.320936918258667}
I0205 20:54:48.483982 140266960250624 logging_writer.py:48] [45119] accumulated_eval_time=6700.786629, accumulated_logging_time=2.320937, accumulated_submission_time=14417.723827, global_step=45119, preemption_count=0, score=14417.723827, test/accuracy=0.985682, test/loss=0.050512, test/mean_average_precision=0.260779, test/num_examples=43793, total_duration=21122.065282, train/accuracy=0.993501, train/loss=0.020774, train/mean_average_precision=0.628247, validation/accuracy=0.986407, validation/loss=0.047308, validation/mean_average_precision=0.274635, validation/num_examples=43793
I0205 20:55:14.434286 140290204940032 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.14199374616146088, loss=0.02108137309551239
I0205 20:55:46.014684 140266960250624 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.139852374792099, loss=0.023611461743712425
I0205 20:56:17.636986 140290204940032 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.12342511117458344, loss=0.022027164697647095
I0205 20:56:49.474372 140266960250624 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.16798797249794006, loss=0.02400323376059532
I0205 20:57:21.975001 140290204940032 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.13447117805480957, loss=0.021751025691628456
I0205 20:57:54.798023 140266960250624 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.15976296365261078, loss=0.024267734959721565
I0205 20:58:26.518017 140290204940032 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.1562958061695099, loss=0.02255297638475895
I0205 20:58:48.621917 140451058161472 spec.py:321] Evaluating on the training split.
I0205 21:00:25.458250 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 21:00:28.471333 140451058161472 spec.py:349] Evaluating on the test split.
I0205 21:00:31.396214 140451058161472 submission_runner.py:408] Time since start: 21465.00s, 	Step: 45872, 	{'train/accuracy': 0.993179976940155, 'train/loss': 0.02161386050283909, 'train/mean_average_precision': 0.5867398796278861, 'validation/accuracy': 0.9865272045135498, 'validation/loss': 0.04743587598204613, 'validation/mean_average_precision': 0.27080341575819306, 'validation/num_examples': 43793, 'test/accuracy': 0.9856418371200562, 'test/loss': 0.050838593393564224, 'test/mean_average_precision': 0.2581153166498968, 'test/num_examples': 43793, 'score': 14657.829684019089, 'total_duration': 21465.00151848793, 'accumulated_submission_time': 14657.829684019089, 'accumulated_eval_time': 6803.560876607895, 'accumulated_logging_time': 2.3559634685516357}
I0205 21:00:31.420045 140283262007040 logging_writer.py:48] [45872] accumulated_eval_time=6803.560877, accumulated_logging_time=2.355963, accumulated_submission_time=14657.829684, global_step=45872, preemption_count=0, score=14657.829684, test/accuracy=0.985642, test/loss=0.050839, test/mean_average_precision=0.258115, test/num_examples=43793, total_duration=21465.001518, train/accuracy=0.993180, train/loss=0.021614, train/mean_average_precision=0.586740, validation/accuracy=0.986527, validation/loss=0.047436, validation/mean_average_precision=0.270803, validation/num_examples=43793
I0205 21:00:40.794993 140283530442496 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.16530008614063263, loss=0.02339889667928219
I0205 21:01:13.071710 140283262007040 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.12333355844020844, loss=0.022684482857584953
I0205 21:01:45.117434 140283530442496 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.14591875672340393, loss=0.02385842800140381
I0205 21:02:17.369610 140283262007040 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.12999151647090912, loss=0.018735382705926895
I0205 21:02:48.831418 140283530442496 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.15172892808914185, loss=0.02490885555744171
I0205 21:03:20.922311 140283262007040 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.15506482124328613, loss=0.02164522185921669
I0205 21:03:52.271736 140283530442496 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.1651839315891266, loss=0.020629793405532837
I0205 21:04:24.199170 140283262007040 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.1551680713891983, loss=0.02333296462893486
I0205 21:04:31.523300 140451058161472 spec.py:321] Evaluating on the training split.
I0205 21:06:09.485976 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 21:06:12.469533 140451058161472 spec.py:349] Evaluating on the test split.
I0205 21:06:15.479068 140451058161472 submission_runner.py:408] Time since start: 21809.08s, 	Step: 46624, 	{'train/accuracy': 0.9928723573684692, 'train/loss': 0.022404048591852188, 'train/mean_average_precision': 0.5792297219554194, 'validation/accuracy': 0.9865121841430664, 'validation/loss': 0.047870516777038574, 'validation/mean_average_precision': 0.2754371891996536, 'validation/num_examples': 43793, 'test/accuracy': 0.9857025146484375, 'test/loss': 0.05104123055934906, 'test/mean_average_precision': 0.26245024778889864, 'test/num_examples': 43793, 'score': 14897.901216506958, 'total_duration': 21809.08435201645, 'accumulated_submission_time': 14897.901216506958, 'accumulated_eval_time': 6907.516575574875, 'accumulated_logging_time': 2.390967845916748}
I0205 21:06:15.504276 140266960250624 logging_writer.py:48] [46624] accumulated_eval_time=6907.516576, accumulated_logging_time=2.390968, accumulated_submission_time=14897.901217, global_step=46624, preemption_count=0, score=14897.901217, test/accuracy=0.985703, test/loss=0.051041, test/mean_average_precision=0.262450, test/num_examples=43793, total_duration=21809.084352, train/accuracy=0.992872, train/loss=0.022404, train/mean_average_precision=0.579230, validation/accuracy=0.986512, validation/loss=0.047871, validation/mean_average_precision=0.275437, validation/num_examples=43793
I0205 21:06:40.283540 140290204940032 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.1297522634267807, loss=0.02261851169168949
I0205 21:07:12.193770 140266960250624 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.13447418808937073, loss=0.022270191460847855
I0205 21:07:43.592323 140290204940032 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.14151008427143097, loss=0.022522250190377235
I0205 21:08:15.335874 140266960250624 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.14424200356006622, loss=0.02190004661679268
I0205 21:08:47.207922 140290204940032 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.1692584902048111, loss=0.024692043662071228
I0205 21:09:18.683947 140266960250624 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.15780901908874512, loss=0.022040192037820816
I0205 21:09:50.341092 140290204940032 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.15511658787727356, loss=0.0243291724473238
I0205 21:10:15.604104 140451058161472 spec.py:321] Evaluating on the training split.
I0205 21:11:53.611272 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 21:11:56.582313 140451058161472 spec.py:349] Evaluating on the test split.
I0205 21:11:59.501605 140451058161472 submission_runner.py:408] Time since start: 22153.11s, 	Step: 47381, 	{'train/accuracy': 0.9928503632545471, 'train/loss': 0.022338924929499626, 'train/mean_average_precision': 0.5829663625984829, 'validation/accuracy': 0.9865494966506958, 'validation/loss': 0.0480351597070694, 'validation/mean_average_precision': 0.27176249077532527, 'validation/num_examples': 43793, 'test/accuracy': 0.9857825636863708, 'test/loss': 0.05094132199883461, 'test/mean_average_precision': 0.2635603124742287, 'test/num_examples': 43793, 'score': 15137.97015786171, 'total_duration': 22153.10691165924, 'accumulated_submission_time': 15137.97015786171, 'accumulated_eval_time': 7011.414033174515, 'accumulated_logging_time': 2.427185297012329}
I0205 21:11:59.525397 140266968643328 logging_writer.py:48] [47381] accumulated_eval_time=7011.414033, accumulated_logging_time=2.427185, accumulated_submission_time=15137.970158, global_step=47381, preemption_count=0, score=15137.970158, test/accuracy=0.985783, test/loss=0.050941, test/mean_average_precision=0.263560, test/num_examples=43793, total_duration=22153.106912, train/accuracy=0.992850, train/loss=0.022339, train/mean_average_precision=0.582966, validation/accuracy=0.986549, validation/loss=0.048035, validation/mean_average_precision=0.271762, validation/num_examples=43793
I0205 21:12:06.134667 140283530442496 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.15494558215141296, loss=0.02222752571105957
I0205 21:12:38.169929 140266968643328 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.18015187978744507, loss=0.02517647109925747
I0205 21:13:10.330662 140283530442496 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.15434813499450684, loss=0.020384015515446663
I0205 21:13:42.230118 140266968643328 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.15293459594249725, loss=0.02067430689930916
I0205 21:14:14.207007 140283530442496 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.15573568642139435, loss=0.02272464893758297
I0205 21:14:45.730203 140266968643328 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.17088399827480316, loss=0.025183580815792084
I0205 21:15:17.570379 140283530442496 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.13257363438606262, loss=0.020329549908638
I0205 21:15:49.374503 140266968643328 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.14012210071086884, loss=0.021324865520000458
I0205 21:15:59.773826 140451058161472 spec.py:321] Evaluating on the training split.
I0205 21:17:43.758481 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 21:17:46.797149 140451058161472 spec.py:349] Evaluating on the test split.
I0205 21:17:49.813068 140451058161472 submission_runner.py:408] Time since start: 22503.42s, 	Step: 48134, 	{'train/accuracy': 0.9928312301635742, 'train/loss': 0.022473499178886414, 'train/mean_average_precision': 0.581797263235984, 'validation/accuracy': 0.9864045977592468, 'validation/loss': 0.04809046536684036, 'validation/mean_average_precision': 0.27633013277789387, 'validation/num_examples': 43793, 'test/accuracy': 0.9855150580406189, 'test/loss': 0.05132876709103584, 'test/mean_average_precision': 0.25643907958030315, 'test/num_examples': 43793, 'score': 15378.187016963959, 'total_duration': 22503.418353796005, 'accumulated_submission_time': 15378.187016963959, 'accumulated_eval_time': 7121.453207015991, 'accumulated_logging_time': 2.461944580078125}
I0205 21:17:49.837983 140266960250624 logging_writer.py:48] [48134] accumulated_eval_time=7121.453207, accumulated_logging_time=2.461945, accumulated_submission_time=15378.187017, global_step=48134, preemption_count=0, score=15378.187017, test/accuracy=0.985515, test/loss=0.051329, test/mean_average_precision=0.256439, test/num_examples=43793, total_duration=22503.418354, train/accuracy=0.992831, train/loss=0.022473, train/mean_average_precision=0.581797, validation/accuracy=0.986405, validation/loss=0.048090, validation/mean_average_precision=0.276330, validation/num_examples=43793
I0205 21:18:11.561041 140290204940032 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.15978823602199554, loss=0.022091759368777275
I0205 21:18:43.093830 140266960250624 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.1541798710823059, loss=0.0212253425270319
I0205 21:19:14.655196 140290204940032 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.18201158940792084, loss=0.022097425535321236
I0205 21:19:46.237839 140266960250624 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.16306275129318237, loss=0.021691612899303436
I0205 21:20:18.271640 140290204940032 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.16634531319141388, loss=0.02461550571024418
I0205 21:20:50.554750 140266960250624 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.15179535746574402, loss=0.021731501445174217
I0205 21:21:22.404124 140290204940032 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.15830104053020477, loss=0.020207861438393593
I0205 21:21:49.909038 140451058161472 spec.py:321] Evaluating on the training split.
I0205 21:23:33.644791 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 21:23:36.642666 140451058161472 spec.py:349] Evaluating on the test split.
I0205 21:23:39.613847 140451058161472 submission_runner.py:408] Time since start: 22853.22s, 	Step: 48888, 	{'train/accuracy': 0.9929319024085999, 'train/loss': 0.022101666778326035, 'train/mean_average_precision': 0.5932613878542279, 'validation/accuracy': 0.9862564206123352, 'validation/loss': 0.048245687037706375, 'validation/mean_average_precision': 0.26514776540965757, 'validation/num_examples': 43793, 'test/accuracy': 0.9853625893592834, 'test/loss': 0.05148543789982796, 'test/mean_average_precision': 0.255840481288772, 'test/num_examples': 43793, 'score': 15618.224890708923, 'total_duration': 22853.219148159027, 'accumulated_submission_time': 15618.224890708923, 'accumulated_eval_time': 7231.157967567444, 'accumulated_logging_time': 2.4997594356536865}
I0205 21:23:39.638683 140283262007040 logging_writer.py:48] [48888] accumulated_eval_time=7231.157968, accumulated_logging_time=2.499759, accumulated_submission_time=15618.224891, global_step=48888, preemption_count=0, score=15618.224891, test/accuracy=0.985363, test/loss=0.051485, test/mean_average_precision=0.255840, test/num_examples=43793, total_duration=22853.219148, train/accuracy=0.992932, train/loss=0.022102, train/mean_average_precision=0.593261, validation/accuracy=0.986256, validation/loss=0.048246, validation/mean_average_precision=0.265148, validation/num_examples=43793
I0205 21:23:44.372987 140283530442496 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.2365361601114273, loss=0.02671537920832634
I0205 21:24:16.232557 140283262007040 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.1642499566078186, loss=0.02148357219994068
I0205 21:24:47.871429 140283530442496 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.16800202429294586, loss=0.022675683721899986
I0205 21:25:19.738081 140283262007040 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.149506077170372, loss=0.0212413277477026
I0205 21:25:51.175719 140283530442496 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.16747960448265076, loss=0.02305784821510315
I0205 21:26:23.123061 140283262007040 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.1631929576396942, loss=0.021297112107276917
I0205 21:26:54.653827 140283530442496 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.1515612155199051, loss=0.0203672144562006
I0205 21:27:26.356387 140283262007040 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.14303719997406006, loss=0.02081165462732315
I0205 21:27:39.619674 140451058161472 spec.py:321] Evaluating on the training split.
I0205 21:29:17.620386 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 21:29:20.699799 140451058161472 spec.py:349] Evaluating on the test split.
I0205 21:29:23.659168 140451058161472 submission_runner.py:408] Time since start: 23197.26s, 	Step: 49643, 	{'train/accuracy': 0.9931454062461853, 'train/loss': 0.021327124908566475, 'train/mean_average_precision': 0.5954668398015054, 'validation/accuracy': 0.9864569902420044, 'validation/loss': 0.04879399761557579, 'validation/mean_average_precision': 0.27577099883949685, 'validation/num_examples': 43793, 'test/accuracy': 0.9856106638908386, 'test/loss': 0.05225291848182678, 'test/mean_average_precision': 0.2538569310413312, 'test/num_examples': 43793, 'score': 15857.749560117722, 'total_duration': 23197.264449834824, 'accumulated_submission_time': 15857.749560117722, 'accumulated_eval_time': 7335.19739151001, 'accumulated_logging_time': 2.960538148880005}
I0205 21:29:23.684106 140266960250624 logging_writer.py:48] [49643] accumulated_eval_time=7335.197392, accumulated_logging_time=2.960538, accumulated_submission_time=15857.749560, global_step=49643, preemption_count=0, score=15857.749560, test/accuracy=0.985611, test/loss=0.052253, test/mean_average_precision=0.253857, test/num_examples=43793, total_duration=23197.264450, train/accuracy=0.993145, train/loss=0.021327, train/mean_average_precision=0.595467, validation/accuracy=0.986457, validation/loss=0.048794, validation/mean_average_precision=0.275771, validation/num_examples=43793
I0205 21:29:41.969504 140290204940032 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.1508890986442566, loss=0.02025279961526394
I0205 21:30:14.122500 140266960250624 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.1691262274980545, loss=0.022335192188620567
I0205 21:30:46.385313 140290204940032 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.19982856512069702, loss=0.02192782424390316
I0205 21:31:18.595346 140266960250624 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.17098358273506165, loss=0.021445900201797485
I0205 21:31:50.736060 140290204940032 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.21808873116970062, loss=0.02189846709370613
I0205 21:32:22.508007 140266960250624 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.1671314537525177, loss=0.020688991993665695
I0205 21:32:54.044568 140290204940032 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.17432458698749542, loss=0.01938336342573166
I0205 21:33:23.662690 140451058161472 spec.py:321] Evaluating on the training split.
I0205 21:35:06.808766 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 21:35:09.826530 140451058161472 spec.py:349] Evaluating on the test split.
I0205 21:35:12.824443 140451058161472 submission_runner.py:408] Time since start: 23546.43s, 	Step: 50393, 	{'train/accuracy': 0.9933032393455505, 'train/loss': 0.020673012360930443, 'train/mean_average_precision': 0.6219227461660122, 'validation/accuracy': 0.9864910840988159, 'validation/loss': 0.049216095358133316, 'validation/mean_average_precision': 0.271150679389319, 'validation/num_examples': 43793, 'test/accuracy': 0.985623300075531, 'test/loss': 0.05265863239765167, 'test/mean_average_precision': 0.25557388236241485, 'test/num_examples': 43793, 'score': 16097.697563171387, 'total_duration': 23546.429747581482, 'accumulated_submission_time': 16097.697563171387, 'accumulated_eval_time': 7444.359100818634, 'accumulated_logging_time': 2.996345281600952}
I0205 21:35:12.856861 140266968643328 logging_writer.py:48] [50393] accumulated_eval_time=7444.359101, accumulated_logging_time=2.996345, accumulated_submission_time=16097.697563, global_step=50393, preemption_count=0, score=16097.697563, test/accuracy=0.985623, test/loss=0.052659, test/mean_average_precision=0.255574, test/num_examples=43793, total_duration=23546.429748, train/accuracy=0.993303, train/loss=0.020673, train/mean_average_precision=0.621923, validation/accuracy=0.986491, validation/loss=0.049216, validation/mean_average_precision=0.271151, validation/num_examples=43793
I0205 21:35:15.573289 140283262007040 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.14779558777809143, loss=0.01957624964416027
I0205 21:35:47.127298 140266968643328 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.2207781970500946, loss=0.023409346118569374
I0205 21:36:18.688913 140283262007040 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.16628462076187134, loss=0.021987391635775566
I0205 21:36:50.087272 140266968643328 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.1622510850429535, loss=0.022141151130199432
I0205 21:37:21.925726 140283262007040 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.2007334977388382, loss=0.020907552912831306
I0205 21:37:53.529389 140266968643328 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.1616392880678177, loss=0.02012387290596962
I0205 21:38:25.427289 140283262007040 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.16488973796367645, loss=0.01971130445599556
I0205 21:38:57.006968 140266968643328 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.1639397144317627, loss=0.020410463213920593
I0205 21:39:12.891444 140451058161472 spec.py:321] Evaluating on the training split.
I0205 21:40:48.789662 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 21:40:51.876433 140451058161472 spec.py:349] Evaluating on the test split.
I0205 21:40:54.937396 140451058161472 submission_runner.py:408] Time since start: 23888.54s, 	Step: 51151, 	{'train/accuracy': 0.9937353134155273, 'train/loss': 0.019665347412228584, 'train/mean_average_precision': 0.649720284927805, 'validation/accuracy': 0.9862414002418518, 'validation/loss': 0.04925694689154625, 'validation/mean_average_precision': 0.2753046787889687, 'validation/num_examples': 43793, 'test/accuracy': 0.9853950142860413, 'test/loss': 0.0527043342590332, 'test/mean_average_precision': 0.2526182830014674, 'test/num_examples': 43793, 'score': 16337.699612855911, 'total_duration': 23888.542701005936, 'accumulated_submission_time': 16337.699612855911, 'accumulated_eval_time': 7546.4050052165985, 'accumulated_logging_time': 3.041268825531006}
I0205 21:40:54.961996 140266960250624 logging_writer.py:48] [51151] accumulated_eval_time=7546.405005, accumulated_logging_time=3.041269, accumulated_submission_time=16337.699613, global_step=51151, preemption_count=0, score=16337.699613, test/accuracy=0.985395, test/loss=0.052704, test/mean_average_precision=0.252618, test/num_examples=43793, total_duration=23888.542701, train/accuracy=0.993735, train/loss=0.019665, train/mean_average_precision=0.649720, validation/accuracy=0.986241, validation/loss=0.049257, validation/mean_average_precision=0.275305, validation/num_examples=43793
I0205 21:41:11.098844 140290204940032 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.14239823818206787, loss=0.01736932434141636
I0205 21:41:42.528398 140266960250624 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.19251559674739838, loss=0.019731206819415092
I0205 21:42:14.365489 140290204940032 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.17883244156837463, loss=0.02051294967532158
I0205 21:42:45.982058 140266960250624 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.15325556695461273, loss=0.021199103444814682
I0205 21:43:17.600992 140290204940032 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.17682413756847382, loss=0.020437879487872124
I0205 21:43:49.221048 140266960250624 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.17506512999534607, loss=0.01918771304190159
I0205 21:44:20.738748 140290204940032 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.1661900132894516, loss=0.02104310132563114
I0205 21:44:52.248374 140266960250624 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.17574091255664825, loss=0.02059529349207878
I0205 21:44:55.108196 140451058161472 spec.py:321] Evaluating on the training split.
I0205 21:46:36.997710 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 21:46:40.128232 140451058161472 spec.py:349] Evaluating on the test split.
I0205 21:46:43.193814 140451058161472 submission_runner.py:408] Time since start: 24236.80s, 	Step: 51910, 	{'train/accuracy': 0.9942708015441895, 'train/loss': 0.017842639237642288, 'train/mean_average_precision': 0.6942757210473101, 'validation/accuracy': 0.9864622354507446, 'validation/loss': 0.049424007534980774, 'validation/mean_average_precision': 0.2712660328003022, 'validation/num_examples': 43793, 'test/accuracy': 0.9855466485023499, 'test/loss': 0.05311064422130585, 'test/mean_average_precision': 0.25182722983540423, 'test/num_examples': 43793, 'score': 16577.81442141533, 'total_duration': 24236.799120903015, 'accumulated_submission_time': 16577.81442141533, 'accumulated_eval_time': 7654.490574836731, 'accumulated_logging_time': 3.0768375396728516}
I0205 21:46:43.218782 140266968643328 logging_writer.py:48] [51910] accumulated_eval_time=7654.490575, accumulated_logging_time=3.076838, accumulated_submission_time=16577.814421, global_step=51910, preemption_count=0, score=16577.814421, test/accuracy=0.985547, test/loss=0.053111, test/mean_average_precision=0.251827, test/num_examples=43793, total_duration=24236.799121, train/accuracy=0.994271, train/loss=0.017843, train/mean_average_precision=0.694276, validation/accuracy=0.986462, validation/loss=0.049424, validation/mean_average_precision=0.271266, validation/num_examples=43793
I0205 21:47:12.618577 140283530442496 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.16486194729804993, loss=0.02008630521595478
I0205 21:47:44.390728 140266968643328 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.1842951774597168, loss=0.02152331918478012
I0205 21:48:16.383001 140283530442496 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.17237617075443268, loss=0.018818356096744537
I0205 21:48:47.967612 140266968643328 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.1515900194644928, loss=0.018071938306093216
I0205 21:49:20.384799 140283530442496 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.1956174671649933, loss=0.017718974500894547
I0205 21:49:51.952267 140266968643328 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.18883800506591797, loss=0.020221546292304993
I0205 21:50:23.952701 140283530442496 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.20074738562107086, loss=0.01850745640695095
I0205 21:50:43.291730 140451058161472 spec.py:321] Evaluating on the training split.
I0205 21:52:26.452952 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 21:52:29.604616 140451058161472 spec.py:349] Evaluating on the test split.
I0205 21:52:32.700164 140451058161472 submission_runner.py:408] Time since start: 24586.31s, 	Step: 52662, 	{'train/accuracy': 0.994036078453064, 'train/loss': 0.01852259784936905, 'train/mean_average_precision': 0.6743744810593322, 'validation/accuracy': 0.9860635995864868, 'validation/loss': 0.05013353377580643, 'validation/mean_average_precision': 0.26632934442485806, 'validation/num_examples': 43793, 'test/accuracy': 0.9852699637413025, 'test/loss': 0.05378008633852005, 'test/mean_average_precision': 0.25007046616990775, 'test/num_examples': 43793, 'score': 16817.855093955994, 'total_duration': 24586.30546784401, 'accumulated_submission_time': 16817.855093955994, 'accumulated_eval_time': 7763.898961544037, 'accumulated_logging_time': 3.1142618656158447}
I0205 21:52:32.726362 140283262007040 logging_writer.py:48] [52662] accumulated_eval_time=7763.898962, accumulated_logging_time=3.114262, accumulated_submission_time=16817.855094, global_step=52662, preemption_count=0, score=16817.855094, test/accuracy=0.985270, test/loss=0.053780, test/mean_average_precision=0.250070, test/num_examples=43793, total_duration=24586.305468, train/accuracy=0.994036, train/loss=0.018523, train/mean_average_precision=0.674374, validation/accuracy=0.986064, validation/loss=0.050134, validation/mean_average_precision=0.266329, validation/num_examples=43793
I0205 21:52:45.085031 140290204940032 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.1836937814950943, loss=0.020766606554389
I0205 21:53:16.980583 140283262007040 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.14959895610809326, loss=0.018357254564762115
I0205 21:53:49.046748 140290204940032 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.18991661071777344, loss=0.02185586281120777
I0205 21:54:21.568088 140283262007040 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.16704653203487396, loss=0.018628155812621117
I0205 21:54:53.303099 140290204940032 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.18559253215789795, loss=0.01700933277606964
I0205 21:55:25.567925 140283262007040 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.18265187740325928, loss=0.017542913556098938
I0205 21:55:59.250084 140290204940032 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.20863142609596252, loss=0.021000757813453674
I0205 21:56:31.324803 140283262007040 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.20309774577617645, loss=0.019943365827202797
I0205 21:56:32.897678 140451058161472 spec.py:321] Evaluating on the training split.
I0205 21:58:13.694004 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 21:58:16.812504 140451058161472 spec.py:349] Evaluating on the test split.
I0205 21:58:19.910646 140451058161472 submission_runner.py:408] Time since start: 24933.52s, 	Step: 53406, 	{'train/accuracy': 0.9941920042037964, 'train/loss': 0.017919816076755524, 'train/mean_average_precision': 0.6768201170825074, 'validation/accuracy': 0.986491858959198, 'validation/loss': 0.05078452080488205, 'validation/mean_average_precision': 0.26381657423925053, 'validation/num_examples': 43793, 'test/accuracy': 0.985526442527771, 'test/loss': 0.05450359359383583, 'test/mean_average_precision': 0.2508009860190624, 'test/num_examples': 43793, 'score': 17057.9955971241, 'total_duration': 24933.515946626663, 'accumulated_submission_time': 17057.9955971241, 'accumulated_eval_time': 7870.911877632141, 'accumulated_logging_time': 3.1511764526367188}
I0205 21:58:19.936018 140266968643328 logging_writer.py:48] [53406] accumulated_eval_time=7870.911878, accumulated_logging_time=3.151176, accumulated_submission_time=17057.995597, global_step=53406, preemption_count=0, score=17057.995597, test/accuracy=0.985526, test/loss=0.054504, test/mean_average_precision=0.250801, test/num_examples=43793, total_duration=24933.515947, train/accuracy=0.994192, train/loss=0.017920, train/mean_average_precision=0.676820, validation/accuracy=0.986492, validation/loss=0.050785, validation/mean_average_precision=0.263817, validation/num_examples=43793
I0205 21:58:52.017638 140283530442496 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.17608603835105896, loss=0.0207755658775568
I0205 21:59:24.122473 140266968643328 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.19802086055278778, loss=0.02120405249297619
I0205 21:59:56.481152 140283530442496 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.17071405053138733, loss=0.017456166446208954
I0205 22:00:27.902251 140266968643328 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.19478031992912292, loss=0.020703377202153206
I0205 22:00:59.021244 140283530442496 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.20239955186843872, loss=0.020637445151805878
I0205 22:01:31.112721 140266968643328 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.1985233575105667, loss=0.02076658606529236
I0205 22:02:03.219172 140283530442496 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.20194247364997864, loss=0.021267123520374298
I0205 22:02:20.074345 140451058161472 spec.py:321] Evaluating on the training split.
I0205 22:04:01.770792 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 22:04:04.838152 140451058161472 spec.py:349] Evaluating on the test split.
I0205 22:04:07.803024 140451058161472 submission_runner.py:408] Time since start: 25281.41s, 	Step: 54155, 	{'train/accuracy': 0.9939531683921814, 'train/loss': 0.018817074596881866, 'train/mean_average_precision': 0.660413799421119, 'validation/accuracy': 0.9861224889755249, 'validation/loss': 0.05127255991101265, 'validation/mean_average_precision': 0.2609240909249675, 'validation/num_examples': 43793, 'test/accuracy': 0.9853095412254333, 'test/loss': 0.05473443865776062, 'test/mean_average_precision': 0.25348312832829123, 'test/num_examples': 43793, 'score': 17298.101543664932, 'total_duration': 25281.408328294754, 'accumulated_submission_time': 17298.101543664932, 'accumulated_eval_time': 7978.640509128571, 'accumulated_logging_time': 3.1888413429260254}
I0205 22:04:07.828214 140283262007040 logging_writer.py:48] [54155] accumulated_eval_time=7978.640509, accumulated_logging_time=3.188841, accumulated_submission_time=17298.101544, global_step=54155, preemption_count=0, score=17298.101544, test/accuracy=0.985310, test/loss=0.054734, test/mean_average_precision=0.253483, test/num_examples=43793, total_duration=25281.408328, train/accuracy=0.993953, train/loss=0.018817, train/mean_average_precision=0.660414, validation/accuracy=0.986122, validation/loss=0.051273, validation/mean_average_precision=0.260924, validation/num_examples=43793
I0205 22:04:22.353358 140290204940032 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.2102544605731964, loss=0.019611962139606476
I0205 22:04:53.904452 140283262007040 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.17088748514652252, loss=0.016736119985580444
I0205 22:05:25.843940 140290204940032 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.19701041281223297, loss=0.019092582166194916
I0205 22:05:57.484729 140283262007040 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.23332619667053223, loss=0.01864074543118477
I0205 22:06:29.468873 140290204940032 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.19809433817863464, loss=0.01974567212164402
I0205 22:07:01.057242 140283262007040 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.2026859074831009, loss=0.02151353284716606
I0205 22:07:32.860381 140290204940032 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.22537586092948914, loss=0.018746843561530113
I0205 22:08:05.114050 140283262007040 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.21358215808868408, loss=0.020014764741063118
I0205 22:08:07.971390 140451058161472 spec.py:321] Evaluating on the training split.
I0205 22:09:46.473550 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 22:09:49.520729 140451058161472 spec.py:349] Evaluating on the test split.
I0205 22:09:52.549119 140451058161472 submission_runner.py:408] Time since start: 25626.15s, 	Step: 54910, 	{'train/accuracy': 0.9937155842781067, 'train/loss': 0.01926986500620842, 'train/mean_average_precision': 0.6451781842926773, 'validation/accuracy': 0.9864431619644165, 'validation/loss': 0.05154697969555855, 'validation/mean_average_precision': 0.26383317846192306, 'validation/num_examples': 43793, 'test/accuracy': 0.9854388236999512, 'test/loss': 0.055534858256578445, 'test/mean_average_precision': 0.25176556668036687, 'test/num_examples': 43793, 'score': 17538.212922811508, 'total_duration': 25626.154417037964, 'accumulated_submission_time': 17538.212922811508, 'accumulated_eval_time': 8083.218180656433, 'accumulated_logging_time': 3.224980354309082}
I0205 22:09:52.574713 140266968643328 logging_writer.py:48] [54910] accumulated_eval_time=8083.218181, accumulated_logging_time=3.224980, accumulated_submission_time=17538.212923, global_step=54910, preemption_count=0, score=17538.212923, test/accuracy=0.985439, test/loss=0.055535, test/mean_average_precision=0.251766, test/num_examples=43793, total_duration=25626.154417, train/accuracy=0.993716, train/loss=0.019270, train/mean_average_precision=0.645178, validation/accuracy=0.986443, validation/loss=0.051547, validation/mean_average_precision=0.263833, validation/num_examples=43793
I0205 22:10:21.764585 140283530442496 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.2506861388683319, loss=0.01868070662021637
I0205 22:10:53.200357 140266968643328 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.200190931558609, loss=0.018308036029338837
I0205 22:11:24.787668 140283530442496 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.19046814739704132, loss=0.018446527421474457
I0205 22:11:56.615682 140266968643328 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.2194150984287262, loss=0.019099494442343712
I0205 22:12:28.737255 140283530442496 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.17969104647636414, loss=0.016672134399414062
I0205 22:13:00.297044 140266968643328 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.20238104462623596, loss=0.020607737824320793
I0205 22:13:31.923988 140283530442496 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.19124965369701385, loss=0.017958421260118484
I0205 22:13:52.707899 140451058161472 spec.py:321] Evaluating on the training split.
I0205 22:15:32.839280 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 22:15:35.820377 140451058161472 spec.py:349] Evaluating on the test split.
I0205 22:15:38.769573 140451058161472 submission_runner.py:408] Time since start: 25972.37s, 	Step: 55667, 	{'train/accuracy': 0.9937472343444824, 'train/loss': 0.019045617431402206, 'train/mean_average_precision': 0.666710952563394, 'validation/accuracy': 0.9862653613090515, 'validation/loss': 0.05202322080731392, 'validation/mean_average_precision': 0.2650312759193009, 'validation/num_examples': 43793, 'test/accuracy': 0.9853293299674988, 'test/loss': 0.05590364709496498, 'test/mean_average_precision': 0.2508333407109575, 'test/num_examples': 43793, 'score': 17778.314692020416, 'total_duration': 25972.37487721443, 'accumulated_submission_time': 17778.314692020416, 'accumulated_eval_time': 8189.279809951782, 'accumulated_logging_time': 3.261626958847046}
I0205 22:15:38.794956 140266960250624 logging_writer.py:48] [55667] accumulated_eval_time=8189.279810, accumulated_logging_time=3.261627, accumulated_submission_time=17778.314692, global_step=55667, preemption_count=0, score=17778.314692, test/accuracy=0.985329, test/loss=0.055904, test/mean_average_precision=0.250833, test/num_examples=43793, total_duration=25972.374877, train/accuracy=0.993747, train/loss=0.019046, train/mean_average_precision=0.666711, validation/accuracy=0.986265, validation/loss=0.052023, validation/mean_average_precision=0.265031, validation/num_examples=43793
I0205 22:15:49.606039 140290204940032 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.21659128367900848, loss=0.017809560522437096
I0205 22:16:21.281240 140266960250624 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.21080602705478668, loss=0.01938428357243538
I0205 22:16:52.505752 140290204940032 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.17786063253879547, loss=0.016453133895993233
I0205 22:17:24.155072 140266960250624 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.21803590655326843, loss=0.020533816888928413
I0205 22:17:55.569720 140290204940032 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.21469491720199585, loss=0.018145369365811348
I0205 22:18:27.154266 140266960250624 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.23408138751983643, loss=0.018635032698512077
I0205 22:18:58.648759 140290204940032 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.2213616520166397, loss=0.014911694452166557
I0205 22:19:30.559615 140266960250624 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.19714266061782837, loss=0.016029682010412216
I0205 22:19:39.035584 140451058161472 spec.py:321] Evaluating on the training split.
I0205 22:21:20.918345 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 22:21:24.020660 140451058161472 spec.py:349] Evaluating on the test split.
I0205 22:21:27.034437 140451058161472 submission_runner.py:408] Time since start: 26320.64s, 	Step: 56428, 	{'train/accuracy': 0.9937534928321838, 'train/loss': 0.019039291888475418, 'train/mean_average_precision': 0.653744196203391, 'validation/accuracy': 0.9862361550331116, 'validation/loss': 0.0522097609937191, 'validation/mean_average_precision': 0.26560886923810517, 'validation/num_examples': 43793, 'test/accuracy': 0.9853870272636414, 'test/loss': 0.05601897090673447, 'test/mean_average_precision': 0.2533901667486278, 'test/num_examples': 43793, 'score': 18018.523869276047, 'total_duration': 26320.63973712921, 'accumulated_submission_time': 18018.523869276047, 'accumulated_eval_time': 8297.278610229492, 'accumulated_logging_time': 3.2980382442474365}
I0205 22:21:27.061794 140266968643328 logging_writer.py:48] [56428] accumulated_eval_time=8297.278610, accumulated_logging_time=3.298038, accumulated_submission_time=18018.523869, global_step=56428, preemption_count=0, score=18018.523869, test/accuracy=0.985387, test/loss=0.056019, test/mean_average_precision=0.253390, test/num_examples=43793, total_duration=26320.639737, train/accuracy=0.993753, train/loss=0.019039, train/mean_average_precision=0.653744, validation/accuracy=0.986236, validation/loss=0.052210, validation/mean_average_precision=0.265609, validation/num_examples=43793
I0205 22:21:50.770172 140283530442496 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.20265042781829834, loss=0.018596474081277847
I0205 22:22:23.199769 140266968643328 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.2247995287179947, loss=0.016931045800447464
I0205 22:22:55.476531 140283530442496 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.21152469515800476, loss=0.017407484352588654
I0205 22:23:27.704330 140266968643328 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.21537692844867706, loss=0.016403894871473312
I0205 22:23:59.890781 140283530442496 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.19630646705627441, loss=0.01621464267373085
I0205 22:24:32.213819 140266968643328 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.21703147888183594, loss=0.016661889851093292
I0205 22:25:04.712613 140283530442496 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.22520971298217773, loss=0.018191758543252945
I0205 22:25:27.157796 140451058161472 spec.py:321] Evaluating on the training split.
I0205 22:27:11.462800 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 22:27:14.505566 140451058161472 spec.py:349] Evaluating on the test split.
I0205 22:27:17.512627 140451058161472 submission_runner.py:408] Time since start: 26671.12s, 	Step: 57170, 	{'train/accuracy': 0.993995189666748, 'train/loss': 0.018129203468561172, 'train/mean_average_precision': 0.6793042671348181, 'validation/accuracy': 0.9860672354698181, 'validation/loss': 0.05306480824947357, 'validation/mean_average_precision': 0.25908438815895707, 'validation/num_examples': 43793, 'test/accuracy': 0.9852640628814697, 'test/loss': 0.05669456720352173, 'test/mean_average_precision': 0.24714238903356686, 'test/num_examples': 43793, 'score': 18258.58660340309, 'total_duration': 26671.117931365967, 'accumulated_submission_time': 18258.58660340309, 'accumulated_eval_time': 8407.633393764496, 'accumulated_logging_time': 3.338200807571411}
I0205 22:27:17.539226 140283262007040 logging_writer.py:48] [57170] accumulated_eval_time=8407.633394, accumulated_logging_time=3.338201, accumulated_submission_time=18258.586603, global_step=57170, preemption_count=0, score=18258.586603, test/accuracy=0.985264, test/loss=0.056695, test/mean_average_precision=0.247142, test/num_examples=43793, total_duration=26671.117931, train/accuracy=0.993995, train/loss=0.018129, train/mean_average_precision=0.679304, validation/accuracy=0.986067, validation/loss=0.053065, validation/mean_average_precision=0.259084, validation/num_examples=43793
I0205 22:27:28.062515 140290204940032 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.21258416771888733, loss=0.015832489356398582
I0205 22:27:59.920464 140283262007040 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.21638983488082886, loss=0.016934016719460487
I0205 22:28:32.213361 140290204940032 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.21658994257450104, loss=0.015880916267633438
I0205 22:29:04.245644 140283262007040 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.2156466543674469, loss=0.016746656969189644
I0205 22:29:35.708573 140290204940032 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.22056952118873596, loss=0.016352932900190353
I0205 22:30:07.681862 140283262007040 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.22676783800125122, loss=0.017997238785028458
I0205 22:30:39.744673 140290204940032 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.25957319140434265, loss=0.01691722497344017
I0205 22:30:56.127286 140283262007040 logging_writer.py:48] [57853] global_step=57853, preemption_count=0, score=18477.127919
I0205 22:30:56.184200 140451058161472 checkpoints.py:490] Saving checkpoint at step: 57853
I0205 22:30:56.303071 140451058161472 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_2/ogbg_jax/trial_3/checkpoint_57853
I0205 22:30:56.304324 140451058161472 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_2/ogbg_jax/trial_3/checkpoint_57853.
I0205 22:30:56.485714 140451058161472 submission_runner.py:583] Tuning trial 3/5
I0205 22:30:56.485943 140451058161472 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0205 22:30:56.490206 140451058161472 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5291368365287781, 'train/loss': 0.7363722324371338, 'train/mean_average_precision': 0.021802974706375825, 'validation/accuracy': 0.5270814895629883, 'validation/loss': 0.737440824508667, 'validation/mean_average_precision': 0.024047021074292817, 'validation/num_examples': 43793, 'test/accuracy': 0.5256850719451904, 'test/loss': 0.7376683354377747, 'test/mean_average_precision': 0.026058016819722727, 'test/num_examples': 43793, 'score': 11.743421077728271, 'total_duration': 126.48002123832703, 'accumulated_submission_time': 11.743421077728271, 'accumulated_eval_time': 114.73655438423157, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (749, {'train/accuracy': 0.9866331815719604, 'train/loss': 0.06894971430301666, 'train/mean_average_precision': 0.03819908784707897, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.07708906382322311, 'validation/mean_average_precision': 0.041322128947657716, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.07995471358299255, 'test/mean_average_precision': 0.043379390131197515, 'test/num_examples': 43793, 'score': 252.02333188056946, 'total_duration': 483.5957524776459, 'accumulated_submission_time': 252.02333188056946, 'accumulated_eval_time': 231.53080677986145, 'accumulated_logging_time': 0.02034306526184082, 'global_step': 749, 'preemption_count': 0}), (1499, {'train/accuracy': 0.9871978163719177, 'train/loss': 0.04832090809941292, 'train/mean_average_precision': 0.08593510077412683, 'validation/accuracy': 0.984529972076416, 'validation/loss': 0.057636987417936325, 'validation/mean_average_precision': 0.08721207535649246, 'validation/num_examples': 43793, 'test/accuracy': 0.9835198521614075, 'test/loss': 0.061021484434604645, 'test/mean_average_precision': 0.08584278558918844, 'test/num_examples': 43793, 'score': 492.2781195640564, 'total_duration': 837.9886548519135, 'accumulated_submission_time': 492.2781195640564, 'accumulated_eval_time': 345.62128949165344, 'accumulated_logging_time': 0.0478668212890625, 'global_step': 1499, 'preemption_count': 0}), (2248, {'train/accuracy': 0.9878924489021301, 'train/loss': 0.04351016506552696, 'train/mean_average_precision': 0.1318084182003098, 'validation/accuracy': 0.9850869178771973, 'validation/loss': 0.053109265863895416, 'validation/mean_average_precision': 0.1295160089173986, 'validation/num_examples': 43793, 'test/accuracy': 0.9840531349182129, 'test/loss': 0.0561034195125103, 'test/mean_average_precision': 0.13673173042026274, 'test/num_examples': 43793, 'score': 732.3831737041473, 'total_duration': 1199.1216876506805, 'accumulated_submission_time': 732.3831737041473, 'accumulated_eval_time': 466.60114884376526, 'accumulated_logging_time': 0.07543468475341797, 'global_step': 2248, 'preemption_count': 0}), (2997, {'train/accuracy': 0.9880146980285645, 'train/loss': 0.0421840101480484, 'train/mean_average_precision': 0.1713988685245202, 'validation/accuracy': 0.9852290153503418, 'validation/loss': 0.05124799907207489, 'validation/mean_average_precision': 0.15495950949841802, 'validation/num_examples': 43793, 'test/accuracy': 0.9842986464500427, 'test/loss': 0.05396828427910805, 'test/mean_average_precision': 0.1536918482128193, 'test/num_examples': 43793, 'score': 972.3318812847137, 'total_duration': 1547.6310951709747, 'accumulated_submission_time': 972.3318812847137, 'accumulated_eval_time': 575.1102995872498, 'accumulated_logging_time': 0.10525918006896973, 'global_step': 2997, 'preemption_count': 0}), (3750, {'train/accuracy': 0.9884123802185059, 'train/loss': 0.04051417484879494, 'train/mean_average_precision': 0.17195161098369188, 'validation/accuracy': 0.9853410124778748, 'validation/loss': 0.050396572798490524, 'validation/mean_average_precision': 0.1698219331341683, 'validation/num_examples': 43793, 'test/accuracy': 0.9845029711723328, 'test/loss': 0.05332661792635918, 'test/mean_average_precision': 0.16829727798683972, 'test/num_examples': 43793, 'score': 1212.3737390041351, 'total_duration': 1902.1999762058258, 'accumulated_submission_time': 1212.3737390041351, 'accumulated_eval_time': 689.58882188797, 'accumulated_logging_time': 0.1334693431854248, 'global_step': 3750, 'preemption_count': 0}), (4503, {'train/accuracy': 0.9884085059165955, 'train/loss': 0.04010861739516258, 'train/mean_average_precision': 0.2116189163327401, 'validation/accuracy': 0.9854055643081665, 'validation/loss': 0.049385201185941696, 'validation/mean_average_precision': 0.18087929617954912, 'validation/num_examples': 43793, 'test/accuracy': 0.9845539331436157, 'test/loss': 0.05209474638104439, 'test/mean_average_precision': 0.17693838182309646, 'test/num_examples': 43793, 'score': 1452.4729554653168, 'total_duration': 2260.835620880127, 'accumulated_submission_time': 1452.4729554653168, 'accumulated_eval_time': 808.075345993042, 'accumulated_logging_time': 0.16302108764648438, 'global_step': 4503, 'preemption_count': 0}), (5253, {'train/accuracy': 0.9887412786483765, 'train/loss': 0.038450874388217926, 'train/mean_average_precision': 0.2200065332386008, 'validation/accuracy': 0.9858241081237793, 'validation/loss': 0.04824988916516304, 'validation/mean_average_precision': 0.19572114582471295, 'validation/num_examples': 43793, 'test/accuracy': 0.9849397540092468, 'test/loss': 0.050983134657144547, 'test/mean_average_precision': 0.19612095671217766, 'test/num_examples': 43793, 'score': 1692.5093190670013, 'total_duration': 2617.616218805313, 'accumulated_submission_time': 1692.5093190670013, 'accumulated_eval_time': 924.7717523574829, 'accumulated_logging_time': 0.190324068069458, 'global_step': 5253, 'preemption_count': 0}), (6003, {'train/accuracy': 0.9889976382255554, 'train/loss': 0.03763619810342789, 'train/mean_average_precision': 0.2488204824404136, 'validation/accuracy': 0.9858983755111694, 'validation/loss': 0.047921206802129745, 'validation/mean_average_precision': 0.20578790123543542, 'validation/num_examples': 43793, 'test/accuracy': 0.9850517511367798, 'test/loss': 0.05053393170237541, 'test/mean_average_precision': 0.21110773477493158, 'test/num_examples': 43793, 'score': 1932.4763493537903, 'total_duration': 2969.128954410553, 'accumulated_submission_time': 1932.4763493537903, 'accumulated_eval_time': 1036.267024755478, 'accumulated_logging_time': 0.21976399421691895, 'global_step': 6003, 'preemption_count': 0}), (6755, {'train/accuracy': 0.9892213940620422, 'train/loss': 0.03666960075497627, 'train/mean_average_precision': 0.26341637862221823, 'validation/accuracy': 0.9860619902610779, 'validation/loss': 0.046762146055698395, 'validation/mean_average_precision': 0.2146012390955433, 'validation/num_examples': 43793, 'test/accuracy': 0.9851566553115845, 'test/loss': 0.0493510365486145, 'test/mean_average_precision': 0.22187402768276412, 'test/num_examples': 43793, 'score': 2172.550704717636, 'total_duration': 3322.5414004325867, 'accumulated_submission_time': 2172.550704717636, 'accumulated_eval_time': 1149.54962849617, 'accumulated_logging_time': 0.2551395893096924, 'global_step': 6755, 'preemption_count': 0}), (7516, {'train/accuracy': 0.9892968535423279, 'train/loss': 0.03595465421676636, 'train/mean_average_precision': 0.27810517075809804, 'validation/accuracy': 0.9860575199127197, 'validation/loss': 0.04683062434196472, 'validation/mean_average_precision': 0.21252932044261078, 'validation/num_examples': 43793, 'test/accuracy': 0.9853032231330872, 'test/loss': 0.04935446009039879, 'test/mean_average_precision': 0.2166553914108559, 'test/num_examples': 43793, 'score': 2412.738084077835, 'total_duration': 3680.512838125229, 'accumulated_submission_time': 2412.738084077835, 'accumulated_eval_time': 1267.2843120098114, 'accumulated_logging_time': 0.2840569019317627, 'global_step': 7516, 'preemption_count': 0}), (8273, {'train/accuracy': 0.9894575476646423, 'train/loss': 0.03549367934465408, 'train/mean_average_precision': 0.280584196580796, 'validation/accuracy': 0.9862965941429138, 'validation/loss': 0.046224649995565414, 'validation/mean_average_precision': 0.22483497796773647, 'validation/num_examples': 43793, 'test/accuracy': 0.9853959083557129, 'test/loss': 0.0489659383893013, 'test/mean_average_precision': 0.2248276800619779, 'test/num_examples': 43793, 'score': 2652.844264984131, 'total_duration': 4030.8121387958527, 'accumulated_submission_time': 2652.844264984131, 'accumulated_eval_time': 1377.429646730423, 'accumulated_logging_time': 0.31173205375671387, 'global_step': 8273, 'preemption_count': 0}), (9024, {'train/accuracy': 0.9894021153450012, 'train/loss': 0.0356401763856411, 'train/mean_average_precision': 0.2816182237666848, 'validation/accuracy': 0.9863436818122864, 'validation/loss': 0.045850545167922974, 'validation/mean_average_precision': 0.22994952997389329, 'validation/num_examples': 43793, 'test/accuracy': 0.9855276942253113, 'test/loss': 0.048610664904117584, 'test/mean_average_precision': 0.23025746755184726, 'test/num_examples': 43793, 'score': 2892.8968245983124, 'total_duration': 4382.973264217377, 'accumulated_submission_time': 2892.8968245983124, 'accumulated_eval_time': 1489.488579750061, 'accumulated_logging_time': 0.3412954807281494, 'global_step': 9024, 'preemption_count': 0}), (9776, {'train/accuracy': 0.989449679851532, 'train/loss': 0.035480014979839325, 'train/mean_average_precision': 0.2902229885165782, 'validation/accuracy': 0.9861695766448975, 'validation/loss': 0.04601406678557396, 'validation/mean_average_precision': 0.22814390693573108, 'validation/num_examples': 43793, 'test/accuracy': 0.985284686088562, 'test/loss': 0.0488116517663002, 'test/mean_average_precision': 0.22754404799916472, 'test/num_examples': 43793, 'score': 3132.904645681381, 'total_duration': 4736.391147851944, 'accumulated_submission_time': 3132.904645681381, 'accumulated_eval_time': 1602.8503172397614, 'accumulated_logging_time': 0.36899900436401367, 'global_step': 9776, 'preemption_count': 0}), (10529, {'train/accuracy': 0.9898571372032166, 'train/loss': 0.0340229794383049, 'train/mean_average_precision': 0.31486855756027643, 'validation/accuracy': 0.9864293336868286, 'validation/loss': 0.04547018185257912, 'validation/mean_average_precision': 0.2401039852908579, 'validation/num_examples': 43793, 'test/accuracy': 0.985558032989502, 'test/loss': 0.048242755234241486, 'test/mean_average_precision': 0.23295535644020104, 'test/num_examples': 43793, 'score': 3373.1482582092285, 'total_duration': 5093.1759622097015, 'accumulated_submission_time': 3373.1482582092285, 'accumulated_eval_time': 1719.3369302749634, 'accumulated_logging_time': 0.4020838737487793, 'global_step': 10529, 'preemption_count': 0}), (11283, {'train/accuracy': 0.9897293448448181, 'train/loss': 0.03409227356314659, 'train/mean_average_precision': 0.31628149151282303, 'validation/accuracy': 0.9861927032470703, 'validation/loss': 0.045579321682453156, 'validation/mean_average_precision': 0.2445057253378528, 'validation/num_examples': 43793, 'test/accuracy': 0.9854072332382202, 'test/loss': 0.048310309648513794, 'test/mean_average_precision': 0.23691313713075418, 'test/num_examples': 43793, 'score': 3613.232980489731, 'total_duration': 5442.6319761276245, 'accumulated_submission_time': 3613.232980489731, 'accumulated_eval_time': 1828.657984495163, 'accumulated_logging_time': 0.43224501609802246, 'global_step': 11283, 'preemption_count': 0}), (12043, {'train/accuracy': 0.9900323748588562, 'train/loss': 0.03316228464245796, 'train/mean_average_precision': 0.34131822994914207, 'validation/accuracy': 0.9864557385444641, 'validation/loss': 0.045129720121622086, 'validation/mean_average_precision': 0.24222951530329723, 'validation/num_examples': 43793, 'test/accuracy': 0.985649049282074, 'test/loss': 0.04773968458175659, 'test/mean_average_precision': 0.24178979968372194, 'test/num_examples': 43793, 'score': 3853.240893602371, 'total_duration': 5797.5224277973175, 'accumulated_submission_time': 3853.240893602371, 'accumulated_eval_time': 1943.4906041622162, 'accumulated_logging_time': 0.46197009086608887, 'global_step': 12043, 'preemption_count': 0}), (12792, {'train/accuracy': 0.9902786612510681, 'train/loss': 0.03209632635116577, 'train/mean_average_precision': 0.3793567029038292, 'validation/accuracy': 0.9866027235984802, 'validation/loss': 0.04466933012008667, 'validation/mean_average_precision': 0.2507951353642788, 'validation/num_examples': 43793, 'test/accuracy': 0.9857446551322937, 'test/loss': 0.04757782816886902, 'test/mean_average_precision': 0.24555612273623034, 'test/num_examples': 43793, 'score': 4092.884313106537, 'total_duration': 6149.5794270038605, 'accumulated_submission_time': 4092.884313106537, 'accumulated_eval_time': 2055.456913471222, 'accumulated_logging_time': 0.8892319202423096, 'global_step': 12792, 'preemption_count': 0}), (13540, {'train/accuracy': 0.9903260469436646, 'train/loss': 0.0318269245326519, 'train/mean_average_precision': 0.3630988382990801, 'validation/accuracy': 0.986552357673645, 'validation/loss': 0.04456004500389099, 'validation/mean_average_precision': 0.25936007800393945, 'validation/num_examples': 43793, 'test/accuracy': 0.9855934381484985, 'test/loss': 0.04757139831781387, 'test/mean_average_precision': 0.24682646676603034, 'test/num_examples': 43793, 'score': 4333.080197811127, 'total_duration': 6500.808378696442, 'accumulated_submission_time': 4333.080197811127, 'accumulated_eval_time': 2166.441266775131, 'accumulated_logging_time': 0.9181814193725586, 'global_step': 13540, 'preemption_count': 0}), (14285, {'train/accuracy': 0.990658700466156, 'train/loss': 0.03080630674958229, 'train/mean_average_precision': 0.39338222098189163, 'validation/accuracy': 0.9864890575408936, 'validation/loss': 0.0447542667388916, 'validation/mean_average_precision': 0.25668576147233724, 'validation/num_examples': 43793, 'test/accuracy': 0.9856544733047485, 'test/loss': 0.04733927175402641, 'test/mean_average_precision': 0.2469929637029501, 'test/num_examples': 43793, 'score': 4573.193847417831, 'total_duration': 6855.502467632294, 'accumulated_submission_time': 4573.193847417831, 'accumulated_eval_time': 2280.971666574478, 'accumulated_logging_time': 0.9479734897613525, 'global_step': 14285, 'preemption_count': 0}), (15030, {'train/accuracy': 0.9908473491668701, 'train/loss': 0.03013112209737301, 'train/mean_average_precision': 0.39935623585281727, 'validation/accuracy': 0.9865454435348511, 'validation/loss': 0.04472634196281433, 'validation/mean_average_precision': 0.25874011864608715, 'validation/num_examples': 43793, 'test/accuracy': 0.9856852293014526, 'test/loss': 0.04769224673509598, 'test/mean_average_precision': 0.24728610833237274, 'test/num_examples': 43793, 'score': 4813.296638011932, 'total_duration': 7206.325021743774, 'accumulated_submission_time': 4813.296638011932, 'accumulated_eval_time': 2391.64150595665, 'accumulated_logging_time': 0.9773366451263428, 'global_step': 15030, 'preemption_count': 0}), (15774, {'train/accuracy': 0.990815281867981, 'train/loss': 0.030350325629115105, 'train/mean_average_precision': 0.39717195598107635, 'validation/accuracy': 0.9866440892219543, 'validation/loss': 0.044659607112407684, 'validation/mean_average_precision': 0.265979384637311, 'validation/num_examples': 43793, 'test/accuracy': 0.985791802406311, 'test/loss': 0.04754490405321121, 'test/mean_average_precision': 0.2527092747568265, 'test/num_examples': 43793, 'score': 5053.413534879684, 'total_duration': 7556.462839126587, 'accumulated_submission_time': 5053.413534879684, 'accumulated_eval_time': 2501.61195063591, 'accumulated_logging_time': 1.0068624019622803, 'global_step': 15774, 'preemption_count': 0}), (16528, {'train/accuracy': 0.9906279444694519, 'train/loss': 0.030868563801050186, 'train/mean_average_precision': 0.39136064694115513, 'validation/accuracy': 0.9864423274993896, 'validation/loss': 0.04503690078854561, 'validation/mean_average_precision': 0.2502382753996813, 'validation/num_examples': 43793, 'test/accuracy': 0.9855433106422424, 'test/loss': 0.04803470894694328, 'test/mean_average_precision': 0.24411684659740535, 'test/num_examples': 43793, 'score': 5293.414917469025, 'total_duration': 7904.33753156662, 'accumulated_submission_time': 5293.414917469025, 'accumulated_eval_time': 2609.4342000484467, 'accumulated_logging_time': 1.0378234386444092, 'global_step': 16528, 'preemption_count': 0}), (17281, {'train/accuracy': 0.9905187487602234, 'train/loss': 0.03111601248383522, 'train/mean_average_precision': 0.3874007734505545, 'validation/accuracy': 0.9865190982818604, 'validation/loss': 0.04453900828957558, 'validation/mean_average_precision': 0.2598120879231033, 'validation/num_examples': 43793, 'test/accuracy': 0.9857496619224548, 'test/loss': 0.04720601066946983, 'test/mean_average_precision': 0.25298960599179354, 'test/num_examples': 43793, 'score': 5533.490139722824, 'total_duration': 8254.701673984528, 'accumulated_submission_time': 5533.490139722824, 'accumulated_eval_time': 2719.6714749336243, 'accumulated_logging_time': 1.0690538883209229, 'global_step': 17281, 'preemption_count': 0}), (18027, {'train/accuracy': 0.990691065788269, 'train/loss': 0.03058709017932415, 'train/mean_average_precision': 0.3921542590010484, 'validation/accuracy': 0.9866855144500732, 'validation/loss': 0.04439421743154526, 'validation/mean_average_precision': 0.2646887457085707, 'validation/num_examples': 43793, 'test/accuracy': 0.9858617186546326, 'test/loss': 0.04723426327109337, 'test/mean_average_precision': 0.25909809108275444, 'test/num_examples': 43793, 'score': 5773.5592222213745, 'total_duration': 8608.562299251556, 'accumulated_submission_time': 5773.5592222213745, 'accumulated_eval_time': 2833.4104483127594, 'accumulated_logging_time': 1.101609468460083, 'global_step': 18027, 'preemption_count': 0}), (18772, {'train/accuracy': 0.9907752275466919, 'train/loss': 0.030265526846051216, 'train/mean_average_precision': 0.3949178652503551, 'validation/accuracy': 0.98653244972229, 'validation/loss': 0.04530401900410652, 'validation/mean_average_precision': 0.25429649453010816, 'validation/num_examples': 43793, 'test/accuracy': 0.9856511354446411, 'test/loss': 0.04803209751844406, 'test/mean_average_precision': 0.24621501443393456, 'test/num_examples': 43793, 'score': 6013.634890794754, 'total_duration': 8958.123777866364, 'accumulated_submission_time': 6013.634890794754, 'accumulated_eval_time': 2942.8444719314575, 'accumulated_logging_time': 1.133225440979004, 'global_step': 18772, 'preemption_count': 0}), (19524, {'train/accuracy': 0.990888774394989, 'train/loss': 0.02983536384999752, 'train/mean_average_precision': 0.41505419043593395, 'validation/accuracy': 0.9865884780883789, 'validation/loss': 0.044611282646656036, 'validation/mean_average_precision': 0.257674980871371, 'validation/num_examples': 43793, 'test/accuracy': 0.9857951998710632, 'test/loss': 0.04735570773482323, 'test/mean_average_precision': 0.25049877040342133, 'test/num_examples': 43793, 'score': 6253.68617773056, 'total_duration': 9309.091790914536, 'accumulated_submission_time': 6253.68617773056, 'accumulated_eval_time': 3053.710742712021, 'accumulated_logging_time': 1.1633639335632324, 'global_step': 19524, 'preemption_count': 0}), (20281, {'train/accuracy': 0.99105304479599, 'train/loss': 0.029099976643919945, 'train/mean_average_precision': 0.43728197588999584, 'validation/accuracy': 0.986669659614563, 'validation/loss': 0.044859033077955246, 'validation/mean_average_precision': 0.2627159908102456, 'validation/num_examples': 43793, 'test/accuracy': 0.985710084438324, 'test/loss': 0.04760020226240158, 'test/mean_average_precision': 0.2503044683984956, 'test/num_examples': 43793, 'score': 6493.699534893036, 'total_duration': 9657.622642278671, 'accumulated_submission_time': 6493.699534893036, 'accumulated_eval_time': 3162.1783707141876, 'accumulated_logging_time': 1.1930761337280273, 'global_step': 20281, 'preemption_count': 0}), (21042, {'train/accuracy': 0.9911764860153198, 'train/loss': 0.02850949950516224, 'train/mean_average_precision': 0.44081136446997216, 'validation/accuracy': 0.9867488145828247, 'validation/loss': 0.044682178646326065, 'validation/mean_average_precision': 0.26651221630376165, 'validation/num_examples': 43793, 'test/accuracy': 0.9857838153839111, 'test/loss': 0.04751453176140785, 'test/mean_average_precision': 0.2609902777836348, 'test/num_examples': 43793, 'score': 6733.768709421158, 'total_duration': 10006.611500024796, 'accumulated_submission_time': 6733.768709421158, 'accumulated_eval_time': 3271.0463457107544, 'accumulated_logging_time': 1.224475383758545, 'global_step': 21042, 'preemption_count': 0}), (21776, {'train/accuracy': 0.991248607635498, 'train/loss': 0.02826027013361454, 'train/mean_average_precision': 0.45386688447746715, 'validation/accuracy': 0.9866753816604614, 'validation/loss': 0.04443187266588211, 'validation/mean_average_precision': 0.2672081493576128, 'validation/num_examples': 43793, 'test/accuracy': 0.985862135887146, 'test/loss': 0.047269124537706375, 'test/mean_average_precision': 0.2573478156117922, 'test/num_examples': 43793, 'score': 6974.013297080994, 'total_duration': 10357.728278398514, 'accumulated_submission_time': 6974.013297080994, 'accumulated_eval_time': 3381.8595008850098, 'accumulated_logging_time': 1.2595610618591309, 'global_step': 21776, 'preemption_count': 0}), (22529, {'train/accuracy': 0.9914287328720093, 'train/loss': 0.028035998344421387, 'train/mean_average_precision': 0.4523613957712486, 'validation/accuracy': 0.9865572452545166, 'validation/loss': 0.04490554332733154, 'validation/mean_average_precision': 0.2573994457154161, 'validation/num_examples': 43793, 'test/accuracy': 0.9856717586517334, 'test/loss': 0.04791056364774704, 'test/mean_average_precision': 0.2484504127180709, 'test/num_examples': 43793, 'score': 7213.988672018051, 'total_duration': 10709.387873411179, 'accumulated_submission_time': 7213.988672018051, 'accumulated_eval_time': 3493.4931180477142, 'accumulated_logging_time': 1.289865255355835, 'global_step': 22529, 'preemption_count': 0}), (23263, {'train/accuracy': 0.9912769198417664, 'train/loss': 0.028356939554214478, 'train/mean_average_precision': 0.4405505491753948, 'validation/accuracy': 0.9866635799407959, 'validation/loss': 0.04467831179499626, 'validation/mean_average_precision': 0.2679886704370666, 'validation/num_examples': 43793, 'test/accuracy': 0.9859034419059753, 'test/loss': 0.04734523221850395, 'test/mean_average_precision': 0.2580197824040367, 'test/num_examples': 43793, 'score': 7453.943880081177, 'total_duration': 11060.453907966614, 'accumulated_submission_time': 7453.943880081177, 'accumulated_eval_time': 3604.5464034080505, 'accumulated_logging_time': 1.3261759281158447, 'global_step': 23263, 'preemption_count': 0}), (23997, {'train/accuracy': 0.9911851286888123, 'train/loss': 0.02885875478386879, 'train/mean_average_precision': 0.4346443075859728, 'validation/accuracy': 0.9867857694625854, 'validation/loss': 0.04451743885874748, 'validation/mean_average_precision': 0.2709604029205279, 'validation/num_examples': 43793, 'test/accuracy': 0.9859076142311096, 'test/loss': 0.047397810965776443, 'test/mean_average_precision': 0.2571818691088361, 'test/num_examples': 43793, 'score': 7694.173835515976, 'total_duration': 11408.353337526321, 'accumulated_submission_time': 7694.173835515976, 'accumulated_eval_time': 3712.1588644981384, 'accumulated_logging_time': 1.3611788749694824, 'global_step': 23997, 'preemption_count': 0}), (24753, {'train/accuracy': 0.9910281896591187, 'train/loss': 0.029097726568579674, 'train/mean_average_precision': 0.43235437727377957, 'validation/accuracy': 0.9866266250610352, 'validation/loss': 0.04503720998764038, 'validation/mean_average_precision': 0.26164825516651796, 'validation/num_examples': 43793, 'test/accuracy': 0.9857720136642456, 'test/loss': 0.04777917265892029, 'test/mean_average_precision': 0.2502596400985658, 'test/num_examples': 43793, 'score': 7934.371356964111, 'total_duration': 11754.900280475616, 'accumulated_submission_time': 7934.371356964111, 'accumulated_eval_time': 3818.4551911354065, 'accumulated_logging_time': 1.3941354751586914, 'global_step': 24753, 'preemption_count': 0}), (25506, {'train/accuracy': 0.9909423589706421, 'train/loss': 0.029259320348501205, 'train/mean_average_precision': 0.430151878859003, 'validation/accuracy': 0.986642062664032, 'validation/loss': 0.04556584730744362, 'validation/mean_average_precision': 0.257154364719039, 'validation/num_examples': 43793, 'test/accuracy': 0.9857766628265381, 'test/loss': 0.048466894775629044, 'test/mean_average_precision': 0.24619635459370018, 'test/num_examples': 43793, 'score': 8174.356585264206, 'total_duration': 12101.346946954727, 'accumulated_submission_time': 8174.356585264206, 'accumulated_eval_time': 3924.8651978969574, 'accumulated_logging_time': 1.4252452850341797, 'global_step': 25506, 'preemption_count': 0}), (26254, {'train/accuracy': 0.9912765026092529, 'train/loss': 0.02838318794965744, 'train/mean_average_precision': 0.4462405974678892, 'validation/accuracy': 0.9867240786552429, 'validation/loss': 0.04525268077850342, 'validation/mean_average_precision': 0.2645874325719716, 'validation/num_examples': 43793, 'test/accuracy': 0.9858777523040771, 'test/loss': 0.04805606231093407, 'test/mean_average_precision': 0.25816406230441513, 'test/num_examples': 43793, 'score': 8414.532056331635, 'total_duration': 12454.075824022293, 'accumulated_submission_time': 8414.532056331635, 'accumulated_eval_time': 4037.365210533142, 'accumulated_logging_time': 1.4570987224578857, 'global_step': 26254, 'preemption_count': 0}), (27008, {'train/accuracy': 0.99150550365448, 'train/loss': 0.02743266150355339, 'train/mean_average_precision': 0.47418810057435634, 'validation/accuracy': 0.9866209626197815, 'validation/loss': 0.04493384435772896, 'validation/mean_average_precision': 0.2674430369710101, 'validation/num_examples': 43793, 'test/accuracy': 0.985755980014801, 'test/loss': 0.04790718853473663, 'test/mean_average_precision': 0.26249672644052296, 'test/num_examples': 43793, 'score': 8654.747866868973, 'total_duration': 12797.918403863907, 'accumulated_submission_time': 8654.747866868973, 'accumulated_eval_time': 4140.939670324326, 'accumulated_logging_time': 1.4889216423034668, 'global_step': 27008, 'preemption_count': 0}), (27762, {'train/accuracy': 0.9915337562561035, 'train/loss': 0.0273089949041605, 'train/mean_average_precision': 0.4780035881167493, 'validation/accuracy': 0.9865288138389587, 'validation/loss': 0.04487583041191101, 'validation/mean_average_precision': 0.26343613950845807, 'validation/num_examples': 43793, 'test/accuracy': 0.9856725931167603, 'test/loss': 0.047843076288700104, 'test/mean_average_precision': 0.25814367993191484, 'test/num_examples': 43793, 'score': 8894.826835393906, 'total_duration': 13144.054966926575, 'accumulated_submission_time': 8894.826835393906, 'accumulated_eval_time': 4246.946130990982, 'accumulated_logging_time': 1.519975185394287, 'global_step': 27762, 'preemption_count': 0}), (28519, {'train/accuracy': 0.9918634295463562, 'train/loss': 0.02625834196805954, 'train/mean_average_precision': 0.4980736352194875, 'validation/accuracy': 0.9867614507675171, 'validation/loss': 0.04515032842755318, 'validation/mean_average_precision': 0.2686315066777437, 'validation/num_examples': 43793, 'test/accuracy': 0.9858773350715637, 'test/loss': 0.048234663903713226, 'test/mean_average_precision': 0.25259174588429933, 'test/num_examples': 43793, 'score': 9134.948380470276, 'total_duration': 13491.490109682083, 'accumulated_submission_time': 9134.948380470276, 'accumulated_eval_time': 4354.20730304718, 'accumulated_logging_time': 1.55245041847229, 'global_step': 28519, 'preemption_count': 0}), (29267, {'train/accuracy': 0.9919530749320984, 'train/loss': 0.025827709585428238, 'train/mean_average_precision': 0.5062438255238915, 'validation/accuracy': 0.9866485595703125, 'validation/loss': 0.045173633843660355, 'validation/mean_average_precision': 0.26228965582438557, 'validation/num_examples': 43793, 'test/accuracy': 0.9858684539794922, 'test/loss': 0.04821714013814926, 'test/mean_average_precision': 0.25379463137131003, 'test/num_examples': 43793, 'score': 9375.02670264244, 'total_duration': 13838.322686195374, 'accumulated_submission_time': 9375.02670264244, 'accumulated_eval_time': 4460.9075763225555, 'accumulated_logging_time': 1.5855414867401123, 'global_step': 29267, 'preemption_count': 0}), (30027, {'train/accuracy': 0.9917265176773071, 'train/loss': 0.026603862643241882, 'train/mean_average_precision': 0.4825484781293242, 'validation/accuracy': 0.9867070317268372, 'validation/loss': 0.045193303376436234, 'validation/mean_average_precision': 0.2677666322082109, 'validation/num_examples': 43793, 'test/accuracy': 0.9858120083808899, 'test/loss': 0.047961484640836716, 'test/mean_average_precision': 0.2577138416244921, 'test/num_examples': 43793, 'score': 9615.066091775894, 'total_duration': 14182.107945919037, 'accumulated_submission_time': 9615.066091775894, 'accumulated_eval_time': 4564.600377559662, 'accumulated_logging_time': 1.6183860301971436, 'global_step': 30027, 'preemption_count': 0}), (30789, {'train/accuracy': 0.9917187094688416, 'train/loss': 0.026825791224837303, 'train/mean_average_precision': 0.47531528965594083, 'validation/accuracy': 0.9866449236869812, 'validation/loss': 0.04516757279634476, 'validation/mean_average_precision': 0.27096467932786544, 'validation/num_examples': 43793, 'test/accuracy': 0.9856843948364258, 'test/loss': 0.048130933195352554, 'test/mean_average_precision': 0.2610056338665237, 'test/num_examples': 43793, 'score': 9855.15208029747, 'total_duration': 14529.88004231453, 'accumulated_submission_time': 9855.15208029747, 'accumulated_eval_time': 4672.233599424362, 'accumulated_logging_time': 1.6509528160095215, 'global_step': 30789, 'preemption_count': 0}), (31549, {'train/accuracy': 0.9915143251419067, 'train/loss': 0.027306945994496346, 'train/mean_average_precision': 0.4664824860010296, 'validation/accuracy': 0.9866051077842712, 'validation/loss': 0.045458097010850906, 'validation/mean_average_precision': 0.27000922246488557, 'validation/num_examples': 43793, 'test/accuracy': 0.9858478307723999, 'test/loss': 0.048325251787900925, 'test/mean_average_precision': 0.26212106268357166, 'test/num_examples': 43793, 'score': 10095.158047437668, 'total_duration': 14873.906448364258, 'accumulated_submission_time': 10095.158047437668, 'accumulated_eval_time': 4776.200966119766, 'accumulated_logging_time': 1.6840364933013916, 'global_step': 31549, 'preemption_count': 0}), (32302, {'train/accuracy': 0.9916272759437561, 'train/loss': 0.026981551200151443, 'train/mean_average_precision': 0.48471532963114583, 'validation/accuracy': 0.9866538643836975, 'validation/loss': 0.04501298442482948, 'validation/mean_average_precision': 0.27844504873588466, 'validation/num_examples': 43793, 'test/accuracy': 0.9857808351516724, 'test/loss': 0.04796469211578369, 'test/mean_average_precision': 0.2607573388347303, 'test/num_examples': 43793, 'score': 10335.340276241302, 'total_duration': 15220.165620326996, 'accumulated_submission_time': 10335.340276241302, 'accumulated_eval_time': 4882.224649429321, 'accumulated_logging_time': 1.7169504165649414, 'global_step': 32302, 'preemption_count': 0}), (33058, {'train/accuracy': 0.9915615320205688, 'train/loss': 0.02697215974330902, 'train/mean_average_precision': 0.4788424207599614, 'validation/accuracy': 0.9867366552352905, 'validation/loss': 0.04553863778710365, 'validation/mean_average_precision': 0.26934329264440693, 'validation/num_examples': 43793, 'test/accuracy': 0.9859063625335693, 'test/loss': 0.048527345061302185, 'test/mean_average_precision': 0.2565631284387224, 'test/num_examples': 43793, 'score': 10575.47698545456, 'total_duration': 15567.64379477501, 'accumulated_submission_time': 10575.47698545456, 'accumulated_eval_time': 4989.512982130051, 'accumulated_logging_time': 1.749946117401123, 'global_step': 33058, 'preemption_count': 0}), (33804, {'train/accuracy': 0.991463303565979, 'train/loss': 0.027266643941402435, 'train/mean_average_precision': 0.47013349861063486, 'validation/accuracy': 0.986585259437561, 'validation/loss': 0.04563891142606735, 'validation/mean_average_precision': 0.2751634706236099, 'validation/num_examples': 43793, 'test/accuracy': 0.9856860637664795, 'test/loss': 0.0485675148665905, 'test/mean_average_precision': 0.2549646396946223, 'test/num_examples': 43793, 'score': 10815.58456158638, 'total_duration': 15919.281561613083, 'accumulated_submission_time': 10815.58456158638, 'accumulated_eval_time': 5100.989178657532, 'accumulated_logging_time': 1.7831127643585205, 'global_step': 33804, 'preemption_count': 0}), (34562, {'train/accuracy': 0.9917646646499634, 'train/loss': 0.026229219511151314, 'train/mean_average_precision': 0.4959750658599631, 'validation/accuracy': 0.9866027235984802, 'validation/loss': 0.04598342999815941, 'validation/mean_average_precision': 0.26992419653193517, 'validation/num_examples': 43793, 'test/accuracy': 0.9858149886131287, 'test/loss': 0.048810895532369614, 'test/mean_average_precision': 0.2672977632378215, 'test/num_examples': 43793, 'score': 11055.760778903961, 'total_duration': 16265.101999282837, 'accumulated_submission_time': 11055.760778903961, 'accumulated_eval_time': 5206.580208301544, 'accumulated_logging_time': 1.8159537315368652, 'global_step': 34562, 'preemption_count': 0}), (35322, {'train/accuracy': 0.9921203255653381, 'train/loss': 0.025237414985895157, 'train/mean_average_precision': 0.5261174470746846, 'validation/accuracy': 0.9866863489151001, 'validation/loss': 0.045371729880571365, 'validation/mean_average_precision': 0.2695367499370574, 'validation/num_examples': 43793, 'test/accuracy': 0.9858158230781555, 'test/loss': 0.048591263592243195, 'test/mean_average_precision': 0.26026593774116824, 'test/num_examples': 43793, 'score': 11295.954808950424, 'total_duration': 16612.69092822075, 'accumulated_submission_time': 11295.954808950424, 'accumulated_eval_time': 5313.920674800873, 'accumulated_logging_time': 1.8492765426635742, 'global_step': 35322, 'preemption_count': 0}), (36076, {'train/accuracy': 0.9921783804893494, 'train/loss': 0.024977702647447586, 'train/mean_average_precision': 0.5172506535598527, 'validation/accuracy': 0.9864655137062073, 'validation/loss': 0.04580044001340866, 'validation/mean_average_precision': 0.27123029214973643, 'validation/num_examples': 43793, 'test/accuracy': 0.9856621026992798, 'test/loss': 0.048594508320093155, 'test/mean_average_precision': 0.2601393911286229, 'test/num_examples': 43793, 'score': 11535.98846077919, 'total_duration': 16956.04179906845, 'accumulated_submission_time': 11535.98846077919, 'accumulated_eval_time': 5417.184643983841, 'accumulated_logging_time': 1.882709264755249, 'global_step': 36076, 'preemption_count': 0}), (36835, {'train/accuracy': 0.9925355911254883, 'train/loss': 0.023661969229578972, 'train/mean_average_precision': 0.5555088251116154, 'validation/accuracy': 0.9866177439689636, 'validation/loss': 0.046089913696050644, 'validation/mean_average_precision': 0.2748257784845323, 'validation/num_examples': 43793, 'test/accuracy': 0.9857437610626221, 'test/loss': 0.04906424134969711, 'test/mean_average_precision': 0.2600800066140546, 'test/num_examples': 43793, 'score': 11776.233745574951, 'total_duration': 17301.020915985107, 'accumulated_submission_time': 11776.233745574951, 'accumulated_eval_time': 5521.864846467972, 'accumulated_logging_time': 1.9163761138916016, 'global_step': 36835, 'preemption_count': 0}), (37588, {'train/accuracy': 0.9923935532569885, 'train/loss': 0.024036496877670288, 'train/mean_average_precision': 0.550742001196731, 'validation/accuracy': 0.9867110848426819, 'validation/loss': 0.04626239836215973, 'validation/mean_average_precision': 0.2691423838406784, 'validation/num_examples': 43793, 'test/accuracy': 0.9858764410018921, 'test/loss': 0.04935387894511223, 'test/mean_average_precision': 0.2592533850321455, 'test/num_examples': 43793, 'score': 12016.22732925415, 'total_duration': 17652.513543844223, 'accumulated_submission_time': 12016.22732925415, 'accumulated_eval_time': 5633.3031594753265, 'accumulated_logging_time': 1.9569110870361328, 'global_step': 37588, 'preemption_count': 0}), (38335, {'train/accuracy': 0.9923391938209534, 'train/loss': 0.024496717378497124, 'train/mean_average_precision': 0.5252410837212944, 'validation/accuracy': 0.986572265625, 'validation/loss': 0.045990101993083954, 'validation/mean_average_precision': 0.2755666996524653, 'validation/num_examples': 43793, 'test/accuracy': 0.9856947064399719, 'test/loss': 0.04922757297754288, 'test/mean_average_precision': 0.264713702862164, 'test/num_examples': 43793, 'score': 12256.457571268082, 'total_duration': 18004.159906864166, 'accumulated_submission_time': 12256.457571268082, 'accumulated_eval_time': 5744.66099023819, 'accumulated_logging_time': 1.9948585033416748, 'global_step': 38335, 'preemption_count': 0}), (39088, {'train/accuracy': 0.9921321868896484, 'train/loss': 0.025087174028158188, 'train/mean_average_precision': 0.5257421666684701, 'validation/accuracy': 0.9865466952323914, 'validation/loss': 0.046366214752197266, 'validation/mean_average_precision': 0.27294682753135996, 'validation/num_examples': 43793, 'test/accuracy': 0.985687792301178, 'test/loss': 0.04935147985816002, 'test/mean_average_precision': 0.26225860070596524, 'test/num_examples': 43793, 'score': 12496.689729452133, 'total_duration': 18347.398972272873, 'accumulated_submission_time': 12496.689729452133, 'accumulated_eval_time': 5847.613451719284, 'accumulated_logging_time': 2.028700351715088, 'global_step': 39088, 'preemption_count': 0}), (39848, {'train/accuracy': 0.992060124874115, 'train/loss': 0.025317229330539703, 'train/mean_average_precision': 0.5124504340697817, 'validation/accuracy': 0.9864711761474609, 'validation/loss': 0.04643158242106438, 'validation/mean_average_precision': 0.27471431599247587, 'validation/num_examples': 43793, 'test/accuracy': 0.985620379447937, 'test/loss': 0.04942890629172325, 'test/mean_average_precision': 0.25711138588610166, 'test/num_examples': 43793, 'score': 12736.806010484695, 'total_duration': 18695.944691181183, 'accumulated_submission_time': 12736.806010484695, 'accumulated_eval_time': 5955.988241195679, 'accumulated_logging_time': 2.06325101852417, 'global_step': 39848, 'preemption_count': 0}), (40588, {'train/accuracy': 0.9921411871910095, 'train/loss': 0.025021405890583992, 'train/mean_average_precision': 0.5354935396143286, 'validation/accuracy': 0.9864374995231628, 'validation/loss': 0.04617885500192642, 'validation/mean_average_precision': 0.2703810326860346, 'validation/num_examples': 43793, 'test/accuracy': 0.9857370257377625, 'test/loss': 0.04916870966553688, 'test/mean_average_precision': 0.26094914867589253, 'test/num_examples': 43793, 'score': 12977.04095864296, 'total_duration': 19038.35589647293, 'accumulated_submission_time': 12977.04095864296, 'accumulated_eval_time': 6058.107916593552, 'accumulated_logging_time': 2.097200632095337, 'global_step': 40588, 'preemption_count': 0}), (41348, {'train/accuracy': 0.992215633392334, 'train/loss': 0.024663478136062622, 'train/mean_average_precision': 0.5279078039982317, 'validation/accuracy': 0.9865621328353882, 'validation/loss': 0.04639315977692604, 'validation/mean_average_precision': 0.27348047653523444, 'validation/num_examples': 43793, 'test/accuracy': 0.9857257008552551, 'test/loss': 0.04914647340774536, 'test/mean_average_precision': 0.26264856029123873, 'test/num_examples': 43793, 'score': 13217.228226184845, 'total_duration': 19386.606401205063, 'accumulated_submission_time': 13217.228226184845, 'accumulated_eval_time': 6166.109060764313, 'accumulated_logging_time': 2.139084815979004, 'global_step': 41348, 'preemption_count': 0}), (42093, {'train/accuracy': 0.9923676252365112, 'train/loss': 0.024189790710806847, 'train/mean_average_precision': 0.5447008755982579, 'validation/accuracy': 0.9866648316383362, 'validation/loss': 0.046636130660772324, 'validation/mean_average_precision': 0.27019773042277423, 'validation/num_examples': 43793, 'test/accuracy': 0.9857661128044128, 'test/loss': 0.049860917031764984, 'test/mean_average_precision': 0.25911789760313175, 'test/num_examples': 43793, 'score': 13457.440511703491, 'total_duration': 19737.170392751694, 'accumulated_submission_time': 13457.440511703491, 'accumulated_eval_time': 6276.40532040596, 'accumulated_logging_time': 2.1731395721435547, 'global_step': 42093, 'preemption_count': 0}), (42853, {'train/accuracy': 0.9926583170890808, 'train/loss': 0.023178569972515106, 'train/mean_average_precision': 0.5682554598312384, 'validation/accuracy': 0.9865312576293945, 'validation/loss': 0.046705130487680435, 'validation/mean_average_precision': 0.27624668941734537, 'validation/num_examples': 43793, 'test/accuracy': 0.9856052398681641, 'test/loss': 0.04985882714390755, 'test/mean_average_precision': 0.26457020850963203, 'test/num_examples': 43793, 'score': 13697.419059038162, 'total_duration': 20088.938884973526, 'accumulated_submission_time': 13697.419059038162, 'accumulated_eval_time': 6388.139190912247, 'accumulated_logging_time': 2.2093663215637207, 'global_step': 42853, 'preemption_count': 0}), (43608, {'train/accuracy': 0.9930390119552612, 'train/loss': 0.022058961912989616, 'train/mean_average_precision': 0.610813742315912, 'validation/accuracy': 0.986449658870697, 'validation/loss': 0.04667244106531143, 'validation/mean_average_precision': 0.277682788359816, 'validation/num_examples': 43793, 'test/accuracy': 0.9856485724449158, 'test/loss': 0.04956782981753349, 'test/mean_average_precision': 0.2643410263914481, 'test/num_examples': 43793, 'score': 13937.618394374847, 'total_duration': 20431.984637498856, 'accumulated_submission_time': 13937.618394374847, 'accumulated_eval_time': 6490.929327249527, 'accumulated_logging_time': 2.2449228763580322, 'global_step': 43608, 'preemption_count': 0}), (44364, {'train/accuracy': 0.9931018352508545, 'train/loss': 0.021798377856612206, 'train/mean_average_precision': 0.5935135327960487, 'validation/accuracy': 0.9863875508308411, 'validation/loss': 0.04711327329277992, 'validation/mean_average_precision': 0.2702936182597056, 'validation/num_examples': 43793, 'test/accuracy': 0.9856389164924622, 'test/loss': 0.04985194280743599, 'test/mean_average_precision': 0.25856992277601665, 'test/num_examples': 43793, 'score': 14177.737513303757, 'total_duration': 20777.149724960327, 'accumulated_submission_time': 14177.737513303757, 'accumulated_eval_time': 6595.918229103088, 'accumulated_logging_time': 2.280789852142334, 'global_step': 44364, 'preemption_count': 0}), (45119, {'train/accuracy': 0.9935007691383362, 'train/loss': 0.02077432908117771, 'train/mean_average_precision': 0.6282470077284659, 'validation/accuracy': 0.986407458782196, 'validation/loss': 0.047308359295129776, 'validation/mean_average_precision': 0.2746351986985576, 'validation/num_examples': 43793, 'test/accuracy': 0.9856823086738586, 'test/loss': 0.05051201581954956, 'test/mean_average_precision': 0.2607793049964524, 'test/num_examples': 43793, 'score': 14417.723826646805, 'total_duration': 21122.0652821064, 'accumulated_submission_time': 14417.723826646805, 'accumulated_eval_time': 6700.786629199982, 'accumulated_logging_time': 2.320936918258667, 'global_step': 45119, 'preemption_count': 0}), (45872, {'train/accuracy': 0.993179976940155, 'train/loss': 0.02161386050283909, 'train/mean_average_precision': 0.5867398796278861, 'validation/accuracy': 0.9865272045135498, 'validation/loss': 0.04743587598204613, 'validation/mean_average_precision': 0.27080341575819306, 'validation/num_examples': 43793, 'test/accuracy': 0.9856418371200562, 'test/loss': 0.050838593393564224, 'test/mean_average_precision': 0.2581153166498968, 'test/num_examples': 43793, 'score': 14657.829684019089, 'total_duration': 21465.00151848793, 'accumulated_submission_time': 14657.829684019089, 'accumulated_eval_time': 6803.560876607895, 'accumulated_logging_time': 2.3559634685516357, 'global_step': 45872, 'preemption_count': 0}), (46624, {'train/accuracy': 0.9928723573684692, 'train/loss': 0.022404048591852188, 'train/mean_average_precision': 0.5792297219554194, 'validation/accuracy': 0.9865121841430664, 'validation/loss': 0.047870516777038574, 'validation/mean_average_precision': 0.2754371891996536, 'validation/num_examples': 43793, 'test/accuracy': 0.9857025146484375, 'test/loss': 0.05104123055934906, 'test/mean_average_precision': 0.26245024778889864, 'test/num_examples': 43793, 'score': 14897.901216506958, 'total_duration': 21809.08435201645, 'accumulated_submission_time': 14897.901216506958, 'accumulated_eval_time': 6907.516575574875, 'accumulated_logging_time': 2.390967845916748, 'global_step': 46624, 'preemption_count': 0}), (47381, {'train/accuracy': 0.9928503632545471, 'train/loss': 0.022338924929499626, 'train/mean_average_precision': 0.5829663625984829, 'validation/accuracy': 0.9865494966506958, 'validation/loss': 0.0480351597070694, 'validation/mean_average_precision': 0.27176249077532527, 'validation/num_examples': 43793, 'test/accuracy': 0.9857825636863708, 'test/loss': 0.05094132199883461, 'test/mean_average_precision': 0.2635603124742287, 'test/num_examples': 43793, 'score': 15137.97015786171, 'total_duration': 22153.10691165924, 'accumulated_submission_time': 15137.97015786171, 'accumulated_eval_time': 7011.414033174515, 'accumulated_logging_time': 2.427185297012329, 'global_step': 47381, 'preemption_count': 0}), (48134, {'train/accuracy': 0.9928312301635742, 'train/loss': 0.022473499178886414, 'train/mean_average_precision': 0.581797263235984, 'validation/accuracy': 0.9864045977592468, 'validation/loss': 0.04809046536684036, 'validation/mean_average_precision': 0.27633013277789387, 'validation/num_examples': 43793, 'test/accuracy': 0.9855150580406189, 'test/loss': 0.05132876709103584, 'test/mean_average_precision': 0.25643907958030315, 'test/num_examples': 43793, 'score': 15378.187016963959, 'total_duration': 22503.418353796005, 'accumulated_submission_time': 15378.187016963959, 'accumulated_eval_time': 7121.453207015991, 'accumulated_logging_time': 2.461944580078125, 'global_step': 48134, 'preemption_count': 0}), (48888, {'train/accuracy': 0.9929319024085999, 'train/loss': 0.022101666778326035, 'train/mean_average_precision': 0.5932613878542279, 'validation/accuracy': 0.9862564206123352, 'validation/loss': 0.048245687037706375, 'validation/mean_average_precision': 0.26514776540965757, 'validation/num_examples': 43793, 'test/accuracy': 0.9853625893592834, 'test/loss': 0.05148543789982796, 'test/mean_average_precision': 0.255840481288772, 'test/num_examples': 43793, 'score': 15618.224890708923, 'total_duration': 22853.219148159027, 'accumulated_submission_time': 15618.224890708923, 'accumulated_eval_time': 7231.157967567444, 'accumulated_logging_time': 2.4997594356536865, 'global_step': 48888, 'preemption_count': 0}), (49643, {'train/accuracy': 0.9931454062461853, 'train/loss': 0.021327124908566475, 'train/mean_average_precision': 0.5954668398015054, 'validation/accuracy': 0.9864569902420044, 'validation/loss': 0.04879399761557579, 'validation/mean_average_precision': 0.27577099883949685, 'validation/num_examples': 43793, 'test/accuracy': 0.9856106638908386, 'test/loss': 0.05225291848182678, 'test/mean_average_precision': 0.2538569310413312, 'test/num_examples': 43793, 'score': 15857.749560117722, 'total_duration': 23197.264449834824, 'accumulated_submission_time': 15857.749560117722, 'accumulated_eval_time': 7335.19739151001, 'accumulated_logging_time': 2.960538148880005, 'global_step': 49643, 'preemption_count': 0}), (50393, {'train/accuracy': 0.9933032393455505, 'train/loss': 0.020673012360930443, 'train/mean_average_precision': 0.6219227461660122, 'validation/accuracy': 0.9864910840988159, 'validation/loss': 0.049216095358133316, 'validation/mean_average_precision': 0.271150679389319, 'validation/num_examples': 43793, 'test/accuracy': 0.985623300075531, 'test/loss': 0.05265863239765167, 'test/mean_average_precision': 0.25557388236241485, 'test/num_examples': 43793, 'score': 16097.697563171387, 'total_duration': 23546.429747581482, 'accumulated_submission_time': 16097.697563171387, 'accumulated_eval_time': 7444.359100818634, 'accumulated_logging_time': 2.996345281600952, 'global_step': 50393, 'preemption_count': 0}), (51151, {'train/accuracy': 0.9937353134155273, 'train/loss': 0.019665347412228584, 'train/mean_average_precision': 0.649720284927805, 'validation/accuracy': 0.9862414002418518, 'validation/loss': 0.04925694689154625, 'validation/mean_average_precision': 0.2753046787889687, 'validation/num_examples': 43793, 'test/accuracy': 0.9853950142860413, 'test/loss': 0.0527043342590332, 'test/mean_average_precision': 0.2526182830014674, 'test/num_examples': 43793, 'score': 16337.699612855911, 'total_duration': 23888.542701005936, 'accumulated_submission_time': 16337.699612855911, 'accumulated_eval_time': 7546.4050052165985, 'accumulated_logging_time': 3.041268825531006, 'global_step': 51151, 'preemption_count': 0}), (51910, {'train/accuracy': 0.9942708015441895, 'train/loss': 0.017842639237642288, 'train/mean_average_precision': 0.6942757210473101, 'validation/accuracy': 0.9864622354507446, 'validation/loss': 0.049424007534980774, 'validation/mean_average_precision': 0.2712660328003022, 'validation/num_examples': 43793, 'test/accuracy': 0.9855466485023499, 'test/loss': 0.05311064422130585, 'test/mean_average_precision': 0.25182722983540423, 'test/num_examples': 43793, 'score': 16577.81442141533, 'total_duration': 24236.799120903015, 'accumulated_submission_time': 16577.81442141533, 'accumulated_eval_time': 7654.490574836731, 'accumulated_logging_time': 3.0768375396728516, 'global_step': 51910, 'preemption_count': 0}), (52662, {'train/accuracy': 0.994036078453064, 'train/loss': 0.01852259784936905, 'train/mean_average_precision': 0.6743744810593322, 'validation/accuracy': 0.9860635995864868, 'validation/loss': 0.05013353377580643, 'validation/mean_average_precision': 0.26632934442485806, 'validation/num_examples': 43793, 'test/accuracy': 0.9852699637413025, 'test/loss': 0.05378008633852005, 'test/mean_average_precision': 0.25007046616990775, 'test/num_examples': 43793, 'score': 16817.855093955994, 'total_duration': 24586.30546784401, 'accumulated_submission_time': 16817.855093955994, 'accumulated_eval_time': 7763.898961544037, 'accumulated_logging_time': 3.1142618656158447, 'global_step': 52662, 'preemption_count': 0}), (53406, {'train/accuracy': 0.9941920042037964, 'train/loss': 0.017919816076755524, 'train/mean_average_precision': 0.6768201170825074, 'validation/accuracy': 0.986491858959198, 'validation/loss': 0.05078452080488205, 'validation/mean_average_precision': 0.26381657423925053, 'validation/num_examples': 43793, 'test/accuracy': 0.985526442527771, 'test/loss': 0.05450359359383583, 'test/mean_average_precision': 0.2508009860190624, 'test/num_examples': 43793, 'score': 17057.9955971241, 'total_duration': 24933.515946626663, 'accumulated_submission_time': 17057.9955971241, 'accumulated_eval_time': 7870.911877632141, 'accumulated_logging_time': 3.1511764526367188, 'global_step': 53406, 'preemption_count': 0}), (54155, {'train/accuracy': 0.9939531683921814, 'train/loss': 0.018817074596881866, 'train/mean_average_precision': 0.660413799421119, 'validation/accuracy': 0.9861224889755249, 'validation/loss': 0.05127255991101265, 'validation/mean_average_precision': 0.2609240909249675, 'validation/num_examples': 43793, 'test/accuracy': 0.9853095412254333, 'test/loss': 0.05473443865776062, 'test/mean_average_precision': 0.25348312832829123, 'test/num_examples': 43793, 'score': 17298.101543664932, 'total_duration': 25281.408328294754, 'accumulated_submission_time': 17298.101543664932, 'accumulated_eval_time': 7978.640509128571, 'accumulated_logging_time': 3.1888413429260254, 'global_step': 54155, 'preemption_count': 0}), (54910, {'train/accuracy': 0.9937155842781067, 'train/loss': 0.01926986500620842, 'train/mean_average_precision': 0.6451781842926773, 'validation/accuracy': 0.9864431619644165, 'validation/loss': 0.05154697969555855, 'validation/mean_average_precision': 0.26383317846192306, 'validation/num_examples': 43793, 'test/accuracy': 0.9854388236999512, 'test/loss': 0.055534858256578445, 'test/mean_average_precision': 0.25176556668036687, 'test/num_examples': 43793, 'score': 17538.212922811508, 'total_duration': 25626.154417037964, 'accumulated_submission_time': 17538.212922811508, 'accumulated_eval_time': 8083.218180656433, 'accumulated_logging_time': 3.224980354309082, 'global_step': 54910, 'preemption_count': 0}), (55667, {'train/accuracy': 0.9937472343444824, 'train/loss': 0.019045617431402206, 'train/mean_average_precision': 0.666710952563394, 'validation/accuracy': 0.9862653613090515, 'validation/loss': 0.05202322080731392, 'validation/mean_average_precision': 0.2650312759193009, 'validation/num_examples': 43793, 'test/accuracy': 0.9853293299674988, 'test/loss': 0.05590364709496498, 'test/mean_average_precision': 0.2508333407109575, 'test/num_examples': 43793, 'score': 17778.314692020416, 'total_duration': 25972.37487721443, 'accumulated_submission_time': 17778.314692020416, 'accumulated_eval_time': 8189.279809951782, 'accumulated_logging_time': 3.261626958847046, 'global_step': 55667, 'preemption_count': 0}), (56428, {'train/accuracy': 0.9937534928321838, 'train/loss': 0.019039291888475418, 'train/mean_average_precision': 0.653744196203391, 'validation/accuracy': 0.9862361550331116, 'validation/loss': 0.0522097609937191, 'validation/mean_average_precision': 0.26560886923810517, 'validation/num_examples': 43793, 'test/accuracy': 0.9853870272636414, 'test/loss': 0.05601897090673447, 'test/mean_average_precision': 0.2533901667486278, 'test/num_examples': 43793, 'score': 18018.523869276047, 'total_duration': 26320.63973712921, 'accumulated_submission_time': 18018.523869276047, 'accumulated_eval_time': 8297.278610229492, 'accumulated_logging_time': 3.2980382442474365, 'global_step': 56428, 'preemption_count': 0}), (57170, {'train/accuracy': 0.993995189666748, 'train/loss': 0.018129203468561172, 'train/mean_average_precision': 0.6793042671348181, 'validation/accuracy': 0.9860672354698181, 'validation/loss': 0.05306480824947357, 'validation/mean_average_precision': 0.25908438815895707, 'validation/num_examples': 43793, 'test/accuracy': 0.9852640628814697, 'test/loss': 0.05669456720352173, 'test/mean_average_precision': 0.24714238903356686, 'test/num_examples': 43793, 'score': 18258.58660340309, 'total_duration': 26671.117931365967, 'accumulated_submission_time': 18258.58660340309, 'accumulated_eval_time': 8407.633393764496, 'accumulated_logging_time': 3.338200807571411, 'global_step': 57170, 'preemption_count': 0})], 'global_step': 57853}
I0205 22:30:56.490429 140451058161472 submission_runner.py:586] Timing: 18477.1279194355
I0205 22:30:56.490487 140451058161472 submission_runner.py:588] Total number of evals: 77
I0205 22:30:56.490529 140451058161472 submission_runner.py:589] ====================
I0205 22:30:56.490578 140451058161472 submission_runner.py:542] Using RNG seed 449608868
I0205 22:30:56.555093 140451058161472 submission_runner.py:551] --- Tuning run 4/5 ---
I0205 22:30:56.555278 140451058161472 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_2/ogbg_jax/trial_4.
I0205 22:30:56.558279 140451058161472 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_2/ogbg_jax/trial_4/hparams.json.
I0205 22:30:56.691668 140451058161472 submission_runner.py:206] Initializing dataset.
I0205 22:30:56.783074 140451058161472 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0205 22:30:56.788682 140451058161472 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0205 22:30:56.921768 140451058161472 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0205 22:30:56.960974 140451058161472 submission_runner.py:213] Initializing model.
I0205 22:30:59.288183 140451058161472 submission_runner.py:255] Initializing optimizer.
I0205 22:30:59.854138 140451058161472 submission_runner.py:262] Initializing metrics bundle.
I0205 22:30:59.854337 140451058161472 submission_runner.py:280] Initializing checkpoint and logger.
I0205 22:30:59.854997 140451058161472 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_2/ogbg_jax/trial_4 with prefix checkpoint_
I0205 22:30:59.855157 140451058161472 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_2/ogbg_jax/trial_4/meta_data_0.json.
I0205 22:30:59.855370 140451058161472 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0205 22:30:59.855435 140451058161472 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0205 22:31:01.848510 140451058161472 logger_utils.py:220] Unable to record git information. Continuing without it.
I0205 22:31:03.712634 140451058161472 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_2/ogbg_jax/trial_4/flags_0.json.
I0205 22:31:03.716780 140451058161472 submission_runner.py:314] Starting training loop.
I0205 22:31:15.600538 140229849954048 logging_writer.py:48] [0] global_step=0, grad_norm=2.306910753250122, loss=0.7360677719116211
I0205 22:31:15.611162 140451058161472 spec.py:321] Evaluating on the training split.
I0205 22:32:58.637418 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 22:33:02.166347 140451058161472 spec.py:349] Evaluating on the test split.
I0205 22:33:05.163865 140451058161472 submission_runner.py:408] Time since start: 121.45s, 	Step: 1, 	{'train/accuracy': 0.5288742184638977, 'train/loss': 0.7363438606262207, 'train/mean_average_precision': 0.02097083927118521, 'validation/accuracy': 0.5270814895629883, 'validation/loss': 0.7374407649040222, 'validation/mean_average_precision': 0.024088420370142846, 'validation/num_examples': 43793, 'test/accuracy': 0.5256850719451904, 'test/loss': 0.7376683354377747, 'test/mean_average_precision': 0.02604572037436443, 'test/num_examples': 43793, 'score': 11.894293308258057, 'total_duration': 121.44691801071167, 'accumulated_submission_time': 11.894293308258057, 'accumulated_eval_time': 109.55257105827332, 'accumulated_logging_time': 0}
I0205 22:33:05.174385 140248415348480 logging_writer.py:48] [1] accumulated_eval_time=109.552571, accumulated_logging_time=0, accumulated_submission_time=11.894293, global_step=1, preemption_count=0, score=11.894293, test/accuracy=0.525685, test/loss=0.737668, test/mean_average_precision=0.026046, test/num_examples=43793, total_duration=121.446918, train/accuracy=0.528874, train/loss=0.736344, train/mean_average_precision=0.020971, validation/accuracy=0.527081, validation/loss=0.737441, validation/mean_average_precision=0.024088, validation/num_examples=43793
I0205 22:33:38.197957 140266968643328 logging_writer.py:48] [100] global_step=100, grad_norm=0.10308685153722763, loss=0.11775341629981995
I0205 22:34:12.150682 140248415348480 logging_writer.py:48] [200] global_step=200, grad_norm=0.010197889059782028, loss=0.05845620855689049
I0205 22:34:44.435265 140266968643328 logging_writer.py:48] [300] global_step=300, grad_norm=0.010209920816123486, loss=0.05393816530704498
I0205 22:35:16.724880 140248415348480 logging_writer.py:48] [400] global_step=400, grad_norm=0.010257045738399029, loss=0.053276177495718
I0205 22:35:48.479689 140266968643328 logging_writer.py:48] [500] global_step=500, grad_norm=0.016969168558716774, loss=0.06217367574572563
I0205 22:36:19.929661 140248415348480 logging_writer.py:48] [600] global_step=600, grad_norm=0.042544253170490265, loss=0.05675369128584862
I0205 22:36:51.767635 140266968643328 logging_writer.py:48] [700] global_step=700, grad_norm=0.01207001879811287, loss=0.05042210966348648
I0205 22:37:05.224514 140451058161472 spec.py:321] Evaluating on the training split.
I0205 22:38:44.842754 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 22:38:47.853719 140451058161472 spec.py:349] Evaluating on the test split.
I0205 22:38:50.816119 140451058161472 submission_runner.py:408] Time since start: 467.10s, 	Step: 743, 	{'train/accuracy': 0.9868398904800415, 'train/loss': 0.05368730053305626, 'train/mean_average_precision': 0.042395540089543944, 'validation/accuracy': 0.9841423034667969, 'validation/loss': 0.06374505907297134, 'validation/mean_average_precision': 0.04093739695877839, 'validation/num_examples': 43793, 'test/accuracy': 0.9831504821777344, 'test/loss': 0.066874660551548, 'test/mean_average_precision': 0.04238444251262984, 'test/num_examples': 43793, 'score': 251.91315054893494, 'total_duration': 467.0992650985718, 'accumulated_submission_time': 251.91315054893494, 'accumulated_eval_time': 215.14411735534668, 'accumulated_logging_time': 0.021558761596679688}
I0205 22:38:50.831420 140229871392512 logging_writer.py:48] [743] accumulated_eval_time=215.144117, accumulated_logging_time=0.021559, accumulated_submission_time=251.913151, global_step=743, preemption_count=0, score=251.913151, test/accuracy=0.983150, test/loss=0.066875, test/mean_average_precision=0.042384, test/num_examples=43793, total_duration=467.099265, train/accuracy=0.986840, train/loss=0.053687, train/mean_average_precision=0.042396, validation/accuracy=0.984142, validation/loss=0.063745, validation/mean_average_precision=0.040937, validation/num_examples=43793
I0205 22:39:09.330307 140266960250624 logging_writer.py:48] [800] global_step=800, grad_norm=0.02152750827372074, loss=0.060305386781692505
I0205 22:39:41.032632 140229871392512 logging_writer.py:48] [900] global_step=900, grad_norm=0.014284472912549973, loss=0.05146458372473717
I0205 22:40:12.749510 140266960250624 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.004363859537988901, loss=0.051817383617162704
I0205 22:40:44.341826 140229871392512 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.010431801900267601, loss=0.04612726345658302
I0205 22:41:16.264244 140266960250624 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.01440026331692934, loss=0.051810406148433685
I0205 22:41:48.418116 140229871392512 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.008682701736688614, loss=0.047563984990119934
I0205 22:42:20.474018 140266960250624 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.015221537090837955, loss=0.05514167994260788
I0205 22:42:51.093195 140451058161472 spec.py:321] Evaluating on the training split.
I0205 22:44:29.941565 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 22:44:32.975397 140451058161472 spec.py:349] Evaluating on the test split.
I0205 22:44:35.966215 140451058161472 submission_runner.py:408] Time since start: 812.25s, 	Step: 1499, 	{'train/accuracy': 0.9869152903556824, 'train/loss': 0.05036809667944908, 'train/mean_average_precision': 0.06419398640662428, 'validation/accuracy': 0.9842705726623535, 'validation/loss': 0.06056094542145729, 'validation/mean_average_precision': 0.06255241673381669, 'validation/num_examples': 43793, 'test/accuracy': 0.9832835793495178, 'test/loss': 0.06384151428937912, 'test/mean_average_precision': 0.06262509468053194, 'test/num_examples': 43793, 'score': 492.1413643360138, 'total_duration': 812.2493708133698, 'accumulated_submission_time': 492.1413643360138, 'accumulated_eval_time': 320.01709842681885, 'accumulated_logging_time': 0.04915928840637207}
I0205 22:44:35.982069 140229879785216 logging_writer.py:48] [1499] accumulated_eval_time=320.017098, accumulated_logging_time=0.049159, accumulated_submission_time=492.141364, global_step=1499, preemption_count=0, score=492.141364, test/accuracy=0.983284, test/loss=0.063842, test/mean_average_precision=0.062625, test/num_examples=43793, total_duration=812.249371, train/accuracy=0.986915, train/loss=0.050368, train/mean_average_precision=0.064194, validation/accuracy=0.984271, validation/loss=0.060561, validation/mean_average_precision=0.062552, validation/num_examples=43793
I0205 22:44:36.660396 140266968643328 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.0066469814628362656, loss=0.05897717922925949
I0205 22:45:08.517307 140229879785216 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.0151940593495965, loss=0.05178048461675644
I0205 22:45:39.852651 140266968643328 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.008079142309725285, loss=0.05295688658952713
I0205 22:46:11.680925 140229879785216 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.008006399497389793, loss=0.052581701427698135
I0205 22:46:42.866296 140266968643328 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.006575239356607199, loss=0.05304410681128502
I0205 22:47:14.490311 140229879785216 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.024478338658809662, loss=0.049947723746299744
I0205 22:47:46.374273 140266968643328 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.01955094374716282, loss=0.04880133643746376
I0205 22:48:18.262145 140229879785216 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.009019425138831139, loss=0.0495748333632946
I0205 22:48:36.032933 140451058161472 spec.py:321] Evaluating on the training split.
I0205 22:50:21.712048 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 22:50:24.768228 140451058161472 spec.py:349] Evaluating on the test split.
I0205 22:50:27.762064 140451058161472 submission_runner.py:408] Time since start: 1164.05s, 	Step: 2256, 	{'train/accuracy': 0.9873570799827576, 'train/loss': 0.046902839094400406, 'train/mean_average_precision': 0.09434357553499763, 'validation/accuracy': 0.9845478534698486, 'validation/loss': 0.057609282433986664, 'validation/mean_average_precision': 0.0900378077916038, 'validation/num_examples': 43793, 'test/accuracy': 0.9835636615753174, 'test/loss': 0.0608636774122715, 'test/mean_average_precision': 0.09006096953725061, 'test/num_examples': 43793, 'score': 732.1600904464722, 'total_duration': 1164.0452189445496, 'accumulated_submission_time': 732.1600904464722, 'accumulated_eval_time': 431.74619340896606, 'accumulated_logging_time': 0.07654666900634766}
I0205 22:50:27.777659 140229871392512 logging_writer.py:48] [2256] accumulated_eval_time=431.746193, accumulated_logging_time=0.076547, accumulated_submission_time=732.160090, global_step=2256, preemption_count=0, score=732.160090, test/accuracy=0.983564, test/loss=0.060864, test/mean_average_precision=0.090061, test/num_examples=43793, total_duration=1164.045219, train/accuracy=0.987357, train/loss=0.046903, train/mean_average_precision=0.094344, validation/accuracy=0.984548, validation/loss=0.057609, validation/mean_average_precision=0.090038, validation/num_examples=43793
I0205 22:50:42.302687 140248415348480 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.03442699834704399, loss=0.046837061643600464
I0205 22:51:14.421219 140229871392512 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.015382050536572933, loss=0.049922455102205276
I0205 22:51:45.882024 140248415348480 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.03070886805653572, loss=0.04909371957182884
I0205 22:52:17.457736 140229871392512 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.016006583347916603, loss=0.043368540704250336
I0205 22:52:49.303451 140248415348480 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.015501457266509533, loss=0.048007041215896606
I0205 22:53:20.838650 140229871392512 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.0147878872230649, loss=0.04972120746970177
I0205 22:53:52.073944 140248415348480 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.04042289778590202, loss=0.04706524312496185
I0205 22:54:23.700352 140229871392512 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.020678166300058365, loss=0.04290721192955971
I0205 22:54:27.762910 140451058161472 spec.py:321] Evaluating on the training split.
I0205 22:56:04.497798 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 22:56:07.483885 140451058161472 spec.py:349] Evaluating on the test split.
I0205 22:56:10.453305 140451058161472 submission_runner.py:408] Time since start: 1506.74s, 	Step: 3014, 	{'train/accuracy': 0.9874885678291321, 'train/loss': 0.04553155601024628, 'train/mean_average_precision': 0.11648724945300393, 'validation/accuracy': 0.9847686290740967, 'validation/loss': 0.055045951157808304, 'validation/mean_average_precision': 0.10606262900438587, 'validation/num_examples': 43793, 'test/accuracy': 0.9837780594825745, 'test/loss': 0.058156948536634445, 'test/mean_average_precision': 0.10731129227853359, 'test/num_examples': 43793, 'score': 972.1134994029999, 'total_duration': 1506.7364542484283, 'accumulated_submission_time': 972.1134994029999, 'accumulated_eval_time': 534.4365320205688, 'accumulated_logging_time': 0.10349726676940918}
I0205 22:56:10.473173 140266960250624 logging_writer.py:48] [3014] accumulated_eval_time=534.436532, accumulated_logging_time=0.103497, accumulated_submission_time=972.113499, global_step=3014, preemption_count=0, score=972.113499, test/accuracy=0.983778, test/loss=0.058157, test/mean_average_precision=0.107311, test/num_examples=43793, total_duration=1506.736454, train/accuracy=0.987489, train/loss=0.045532, train/mean_average_precision=0.116487, validation/accuracy=0.984769, validation/loss=0.055046, validation/mean_average_precision=0.106063, validation/num_examples=43793
I0205 22:56:37.991273 140266968643328 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.02947859652340412, loss=0.047931794077157974
I0205 22:57:09.510963 140266960250624 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.04349537566304207, loss=0.04380393400788307
I0205 22:57:41.390514 140266968643328 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.02608235739171505, loss=0.04603572562336922
I0205 22:58:13.525712 140266960250624 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.01934078335762024, loss=0.04044817388057709
I0205 22:58:45.035316 140266968643328 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.019337886944413185, loss=0.04822557419538498
I0205 22:59:16.802946 140266960250624 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.033712904900312424, loss=0.04576987028121948
I0205 22:59:48.536161 140266968643328 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.035341132432222366, loss=0.04096505790948868
I0205 23:00:10.747860 140451058161472 spec.py:321] Evaluating on the training split.
I0205 23:01:53.102251 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 23:01:56.089881 140451058161472 spec.py:349] Evaluating on the test split.
I0205 23:01:59.038339 140451058161472 submission_runner.py:408] Time since start: 1855.32s, 	Step: 3771, 	{'train/accuracy': 0.9876707196235657, 'train/loss': 0.04354364797472954, 'train/mean_average_precision': 0.1393367840583442, 'validation/accuracy': 0.9849566221237183, 'validation/loss': 0.05245377495884895, 'validation/mean_average_precision': 0.1315201944459623, 'validation/num_examples': 43793, 'test/accuracy': 0.9840333461761475, 'test/loss': 0.055392779409885406, 'test/mean_average_precision': 0.1286881339106711, 'test/num_examples': 43793, 'score': 1212.3574872016907, 'total_duration': 1855.3214933872223, 'accumulated_submission_time': 1212.3574872016907, 'accumulated_eval_time': 642.7269651889801, 'accumulated_logging_time': 0.13416767120361328}
I0205 23:01:59.054870 140229871392512 logging_writer.py:48] [3771] accumulated_eval_time=642.726965, accumulated_logging_time=0.134168, accumulated_submission_time=1212.357487, global_step=3771, preemption_count=0, score=1212.357487, test/accuracy=0.984033, test/loss=0.055393, test/mean_average_precision=0.128688, test/num_examples=43793, total_duration=1855.321493, train/accuracy=0.987671, train/loss=0.043544, train/mean_average_precision=0.139337, validation/accuracy=0.984957, validation/loss=0.052454, validation/mean_average_precision=0.131520, validation/num_examples=43793
I0205 23:02:08.680781 140248415348480 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.022872479632496834, loss=0.04132058843970299
I0205 23:02:40.032527 140229871392512 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.038449399173259735, loss=0.04526819288730621
I0205 23:03:11.855846 140248415348480 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.03165258467197418, loss=0.045505814254283905
I0205 23:03:43.394771 140229871392512 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.021919092163443565, loss=0.04096086695790291
I0205 23:04:15.120691 140248415348480 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.02436790056526661, loss=0.04362517595291138
I0205 23:04:47.002360 140229871392512 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.02289617620408535, loss=0.04225347563624382
I0205 23:05:19.035673 140248415348480 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.04672966152429581, loss=0.043266329914331436
I0205 23:05:50.485200 140229871392512 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.02590791881084442, loss=0.03832065314054489
I0205 23:05:59.246569 140451058161472 spec.py:321] Evaluating on the training split.
I0205 23:07:37.870246 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 23:07:40.864264 140451058161472 spec.py:349] Evaluating on the test split.
I0205 23:07:43.802528 140451058161472 submission_runner.py:408] Time since start: 2200.09s, 	Step: 4529, 	{'train/accuracy': 0.9878915548324585, 'train/loss': 0.04272441565990448, 'train/mean_average_precision': 0.15881659907429796, 'validation/accuracy': 0.9850808382034302, 'validation/loss': 0.053034041076898575, 'validation/mean_average_precision': 0.14061921402636388, 'validation/num_examples': 43793, 'test/accuracy': 0.9841015338897705, 'test/loss': 0.05617416650056839, 'test/mean_average_precision': 0.13804253794928967, 'test/num_examples': 43793, 'score': 1452.5180218219757, 'total_duration': 2200.0856885910034, 'accumulated_submission_time': 1452.5180218219757, 'accumulated_eval_time': 747.282881975174, 'accumulated_logging_time': 0.16181612014770508}
I0205 23:07:43.818649 140266960250624 logging_writer.py:48] [4529] accumulated_eval_time=747.282882, accumulated_logging_time=0.161816, accumulated_submission_time=1452.518022, global_step=4529, preemption_count=0, score=1452.518022, test/accuracy=0.984102, test/loss=0.056174, test/mean_average_precision=0.138043, test/num_examples=43793, total_duration=2200.085689, train/accuracy=0.987892, train/loss=0.042724, train/mean_average_precision=0.158817, validation/accuracy=0.985081, validation/loss=0.053034, validation/mean_average_precision=0.140619, validation/num_examples=43793
I0205 23:08:06.697672 140266968643328 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.031122688204050064, loss=0.043808408081531525
I0205 23:08:38.486155 140266960250624 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.10813326388597488, loss=0.05188935622572899
I0205 23:09:10.744988 140266968643328 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.025785939767956734, loss=0.037119533866643906
I0205 23:09:42.524914 140266960250624 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.03793108090758324, loss=0.04259321838617325
I0205 23:10:14.359806 140266968643328 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.022719627246260643, loss=0.03967708349227905
I0205 23:10:46.028152 140266960250624 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.04373152181506157, loss=0.039862342178821564
I0205 23:11:18.316833 140266968643328 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.05521708354353905, loss=0.0427328459918499
I0205 23:11:43.972092 140451058161472 spec.py:321] Evaluating on the training split.
I0205 23:13:27.434630 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 23:13:30.452113 140451058161472 spec.py:349] Evaluating on the test split.
I0205 23:13:33.456911 140451058161472 submission_runner.py:408] Time since start: 2549.74s, 	Step: 5280, 	{'train/accuracy': 0.9880184531211853, 'train/loss': 0.04215643182396889, 'train/mean_average_precision': 0.1746008317249671, 'validation/accuracy': 0.9848993420600891, 'validation/loss': 0.051810652017593384, 'validation/mean_average_precision': 0.14567380892209825, 'validation/num_examples': 43793, 'test/accuracy': 0.9839562177658081, 'test/loss': 0.054508913308382034, 'test/mean_average_precision': 0.14418011181236254, 'test/num_examples': 43793, 'score': 1692.6388084888458, 'total_duration': 2549.740065097809, 'accumulated_submission_time': 1692.6388084888458, 'accumulated_eval_time': 856.7676658630371, 'accumulated_logging_time': 0.19035816192626953}
I0205 23:13:33.473489 140229871392512 logging_writer.py:48] [5280] accumulated_eval_time=856.767666, accumulated_logging_time=0.190358, accumulated_submission_time=1692.638808, global_step=5280, preemption_count=0, score=1692.638808, test/accuracy=0.983956, test/loss=0.054509, test/mean_average_precision=0.144180, test/num_examples=43793, total_duration=2549.740065, train/accuracy=0.988018, train/loss=0.042156, train/mean_average_precision=0.174601, validation/accuracy=0.984899, validation/loss=0.051811, validation/mean_average_precision=0.145674, validation/num_examples=43793
I0205 23:13:40.204858 140248415348480 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.07668884843587875, loss=0.04188605397939682
I0205 23:14:12.194378 140229871392512 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.04179134964942932, loss=0.042741112411022186
I0205 23:14:43.663516 140248415348480 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.0897417739033699, loss=0.04352353513240814
I0205 23:15:15.530222 140229871392512 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.037758469581604004, loss=0.040391430258750916
I0205 23:15:47.645164 140248415348480 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.025505557656288147, loss=0.04302872344851494
I0205 23:16:19.354781 140229871392512 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.018364254385232925, loss=0.0397413969039917
I0205 23:16:51.666098 140248415348480 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.03637787327170372, loss=0.040752556174993515
I0205 23:17:23.710546 140229871392512 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.040665727108716965, loss=0.037868231534957886
I0205 23:17:33.577173 140451058161472 spec.py:321] Evaluating on the training split.
I0205 23:19:11.971244 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 23:19:15.003193 140451058161472 spec.py:349] Evaluating on the test split.
I0205 23:19:18.026094 140451058161472 submission_runner.py:408] Time since start: 2894.31s, 	Step: 6032, 	{'train/accuracy': 0.9882644414901733, 'train/loss': 0.04114021733403206, 'train/mean_average_precision': 0.17060031998227768, 'validation/accuracy': 0.9852550029754639, 'validation/loss': 0.05112120509147644, 'validation/mean_average_precision': 0.15067751522048584, 'validation/num_examples': 43793, 'test/accuracy': 0.9843584895133972, 'test/loss': 0.053869158029556274, 'test/mean_average_precision': 0.14989309555874172, 'test/num_examples': 43793, 'score': 1932.7112345695496, 'total_duration': 2894.309236764908, 'accumulated_submission_time': 1932.7112345695496, 'accumulated_eval_time': 961.2165246009827, 'accumulated_logging_time': 0.21789264678955078}
I0205 23:19:18.042850 140290196547328 logging_writer.py:48] [6032] accumulated_eval_time=961.216525, accumulated_logging_time=0.217893, accumulated_submission_time=1932.711235, global_step=6032, preemption_count=0, score=1932.711235, test/accuracy=0.984358, test/loss=0.053869, test/mean_average_precision=0.149893, test/num_examples=43793, total_duration=2894.309237, train/accuracy=0.988264, train/loss=0.041140, train/mean_average_precision=0.170600, validation/accuracy=0.985255, validation/loss=0.051121, validation/mean_average_precision=0.150678, validation/num_examples=43793
I0205 23:19:39.851805 140388941678336 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.060696281492710114, loss=0.04336227849125862
I0205 23:20:11.488490 140290196547328 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.034110330045223236, loss=0.04185766726732254
I0205 23:20:43.330100 140388941678336 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.041157402098178864, loss=0.04095597192645073
I0205 23:21:15.443004 140290196547328 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.06560417264699936, loss=0.03796780854463577
I0205 23:21:47.313365 140388941678336 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.04272313788533211, loss=0.04085546359419823
I0205 23:22:19.301485 140290196547328 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.07177208364009857, loss=0.03910956159234047
I0205 23:22:50.874049 140388941678336 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.027615098282694817, loss=0.03630119189620018
I0205 23:23:18.332441 140451058161472 spec.py:321] Evaluating on the training split.
I0205 23:24:58.140781 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 23:25:01.201023 140451058161472 spec.py:349] Evaluating on the test split.
I0205 23:25:04.399994 140451058161472 submission_runner.py:408] Time since start: 3240.68s, 	Step: 6787, 	{'train/accuracy': 0.9882400631904602, 'train/loss': 0.04097194969654083, 'train/mean_average_precision': 0.18827192917678587, 'validation/accuracy': 0.9853280186653137, 'validation/loss': 0.05018118396401405, 'validation/mean_average_precision': 0.1575803344208639, 'validation/num_examples': 43793, 'test/accuracy': 0.9844317436218262, 'test/loss': 0.05277862772345543, 'test/mean_average_precision': 0.1560394778303248, 'test/num_examples': 43793, 'score': 2172.968799352646, 'total_duration': 3240.6831436157227, 'accumulated_submission_time': 2172.968799352646, 'accumulated_eval_time': 1067.2840287685394, 'accumulated_logging_time': 0.24642467498779297}
I0205 23:25:04.416373 140229879785216 logging_writer.py:48] [6787] accumulated_eval_time=1067.284029, accumulated_logging_time=0.246425, accumulated_submission_time=2172.968799, global_step=6787, preemption_count=0, score=2172.968799, test/accuracy=0.984432, test/loss=0.052779, test/mean_average_precision=0.156039, test/num_examples=43793, total_duration=3240.683144, train/accuracy=0.988240, train/loss=0.040972, train/mean_average_precision=0.188272, validation/accuracy=0.985328, validation/loss=0.050181, validation/mean_average_precision=0.157580, validation/num_examples=43793
I0205 23:25:08.921355 140266968643328 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.021529102697968483, loss=0.037555158138275146
I0205 23:25:40.767168 140229879785216 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.053475167602300644, loss=0.04328899458050728
I0205 23:26:13.177218 140266968643328 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.03291470184922218, loss=0.03772386536002159
I0205 23:26:44.729172 140229879785216 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.025500286370515823, loss=0.04158233478665352
I0205 23:27:16.163716 140266968643328 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.07943519204854965, loss=0.04081016406416893
I0205 23:27:47.898644 140229879785216 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.06962063163518906, loss=0.04712294042110443
I0205 23:28:19.546070 140266968643328 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.10229808837175369, loss=0.04191453382372856
I0205 23:28:51.492559 140229879785216 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.10194337368011475, loss=0.03986286744475365
I0205 23:29:04.452219 140451058161472 spec.py:321] Evaluating on the training split.
I0205 23:30:43.440509 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 23:30:46.835958 140451058161472 spec.py:349] Evaluating on the test split.
I0205 23:30:50.259274 140451058161472 submission_runner.py:408] Time since start: 3586.54s, 	Step: 7541, 	{'train/accuracy': 0.9883561730384827, 'train/loss': 0.04039285331964493, 'train/mean_average_precision': 0.18365727493677925, 'validation/accuracy': 0.9855983853340149, 'validation/loss': 0.04950226470828056, 'validation/mean_average_precision': 0.1682590390890557, 'validation/num_examples': 43793, 'test/accuracy': 0.9845829606056213, 'test/loss': 0.05229341983795166, 'test/mean_average_precision': 0.16385811315475504, 'test/num_examples': 43793, 'score': 2412.973567724228, 'total_duration': 3586.542414188385, 'accumulated_submission_time': 2412.973567724228, 'accumulated_eval_time': 1173.091017961502, 'accumulated_logging_time': 0.2735772132873535}
I0205 23:30:50.278368 140290196547328 logging_writer.py:48] [7541] accumulated_eval_time=1173.091018, accumulated_logging_time=0.273577, accumulated_submission_time=2412.973568, global_step=7541, preemption_count=0, score=2412.973568, test/accuracy=0.984583, test/loss=0.052293, test/mean_average_precision=0.163858, test/num_examples=43793, total_duration=3586.542414, train/accuracy=0.988356, train/loss=0.040393, train/mean_average_precision=0.183657, validation/accuracy=0.985598, validation/loss=0.049502, validation/mean_average_precision=0.168259, validation/num_examples=43793
I0205 23:31:09.757271 140388941678336 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.07168322801589966, loss=0.042207181453704834
I0205 23:31:42.195915 140290196547328 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.05068283528089523, loss=0.04514727741479874
I0205 23:32:14.265505 140388941678336 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.030969932675361633, loss=0.040904007852077484
I0205 23:32:46.316833 140290196547328 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.052315738052129745, loss=0.03725497052073479
I0205 23:33:18.415057 140388941678336 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.02399677038192749, loss=0.03898652270436287
I0205 23:33:50.191705 140290196547328 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.051015302538871765, loss=0.04400601610541344
I0205 23:34:22.105885 140388941678336 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.02534497156739235, loss=0.04032494127750397
I0205 23:34:50.517860 140451058161472 spec.py:321] Evaluating on the training split.
I0205 23:36:29.096375 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 23:36:34.393718 140451058161472 spec.py:349] Evaluating on the test split.
I0205 23:36:37.500798 140451058161472 submission_runner.py:408] Time since start: 3933.78s, 	Step: 8290, 	{'train/accuracy': 0.9883766174316406, 'train/loss': 0.040410179644823074, 'train/mean_average_precision': 0.18653734470643504, 'validation/accuracy': 0.9854250550270081, 'validation/loss': 0.050055332481861115, 'validation/mean_average_precision': 0.1676451153843103, 'validation/num_examples': 43793, 'test/accuracy': 0.9844376444816589, 'test/loss': 0.05293816328048706, 'test/mean_average_precision': 0.1628862646202331, 'test/num_examples': 43793, 'score': 2653.181145429611, 'total_duration': 3933.7839448451996, 'accumulated_submission_time': 2653.181145429611, 'accumulated_eval_time': 1280.0739023685455, 'accumulated_logging_time': 0.3043797016143799}
I0205 23:36:37.518371 140266960250624 logging_writer.py:48] [8290] accumulated_eval_time=1280.073902, accumulated_logging_time=0.304380, accumulated_submission_time=2653.181145, global_step=8290, preemption_count=0, score=2653.181145, test/accuracy=0.984438, test/loss=0.052938, test/mean_average_precision=0.162886, test/num_examples=43793, total_duration=3933.783945, train/accuracy=0.988377, train/loss=0.040410, train/mean_average_precision=0.186537, validation/accuracy=0.985425, validation/loss=0.050055, validation/mean_average_precision=0.167645, validation/num_examples=43793
I0205 23:36:41.078033 140266968643328 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.031672701239585876, loss=0.036564115434885025
I0205 23:37:13.102910 140266960250624 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.04674558714032173, loss=0.038619574159383774
I0205 23:37:45.017596 140266968643328 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.03496598079800606, loss=0.0412079356610775
I0205 23:38:16.802692 140266960250624 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.03688792884349823, loss=0.03633372485637665
I0205 23:38:48.590675 140266968643328 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.03966712951660156, loss=0.04079463332891464
I0205 23:39:20.685475 140266960250624 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.042257409542798996, loss=0.03941250964999199
I0205 23:39:52.377729 140266968643328 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.040177956223487854, loss=0.04581121355295181
I0205 23:40:24.280087 140266960250624 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.07804495841264725, loss=0.04349846392869949
I0205 23:40:37.551024 140451058161472 spec.py:321] Evaluating on the training split.
I0205 23:42:13.530879 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 23:42:16.789017 140451058161472 spec.py:349] Evaluating on the test split.
I0205 23:42:19.838101 140451058161472 submission_runner.py:408] Time since start: 4276.12s, 	Step: 9043, 	{'train/accuracy': 0.9884548783302307, 'train/loss': 0.04007907584309578, 'train/mean_average_precision': 0.2033323095092237, 'validation/accuracy': 0.985439658164978, 'validation/loss': 0.049570098519325256, 'validation/mean_average_precision': 0.17320713215950936, 'validation/num_examples': 43793, 'test/accuracy': 0.9845362305641174, 'test/loss': 0.05233263969421387, 'test/mean_average_precision': 0.16886620021256832, 'test/num_examples': 43793, 'score': 2893.182389497757, 'total_duration': 4276.1212503910065, 'accumulated_submission_time': 2893.182389497757, 'accumulated_eval_time': 1382.3609237670898, 'accumulated_logging_time': 0.33313870429992676}
I0205 23:42:19.854786 140248415348480 logging_writer.py:48] [9043] accumulated_eval_time=1382.360924, accumulated_logging_time=0.333139, accumulated_submission_time=2893.182389, global_step=9043, preemption_count=0, score=2893.182389, test/accuracy=0.984536, test/loss=0.052333, test/mean_average_precision=0.168866, test/num_examples=43793, total_duration=4276.121250, train/accuracy=0.988455, train/loss=0.040079, train/mean_average_precision=0.203332, validation/accuracy=0.985440, validation/loss=0.049570, validation/mean_average_precision=0.173207, validation/num_examples=43793
I0205 23:42:38.168412 140290196547328 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.05794793367385864, loss=0.039373356848955154
I0205 23:43:10.017809 140248415348480 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.07621278613805771, loss=0.03654370829463005
I0205 23:43:41.430811 140290196547328 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.09682619571685791, loss=0.03981252759695053
I0205 23:44:13.277274 140248415348480 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.028042567893862724, loss=0.039867278188467026
I0205 23:44:44.955690 140290196547328 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.02049802429974079, loss=0.039578329771757126
I0205 23:45:16.964404 140248415348480 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.047718748450279236, loss=0.04692168906331062
I0205 23:45:48.739560 140290196547328 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.06214194744825363, loss=0.0396183617413044
I0205 23:46:20.045939 140451058161472 spec.py:321] Evaluating on the training split.
I0205 23:47:58.339985 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 23:48:01.532819 140451058161472 spec.py:349] Evaluating on the test split.
I0205 23:48:04.816109 140451058161472 submission_runner.py:408] Time since start: 4621.10s, 	Step: 9800, 	{'train/accuracy': 0.9884865283966064, 'train/loss': 0.03967352584004402, 'train/mean_average_precision': 0.20581174027788657, 'validation/accuracy': 0.9856600761413574, 'validation/loss': 0.049448948353528976, 'validation/mean_average_precision': 0.17003765094657858, 'validation/num_examples': 43793, 'test/accuracy': 0.9847678542137146, 'test/loss': 0.05241949483752251, 'test/mean_average_precision': 0.17062260994694053, 'test/num_examples': 43793, 'score': 3133.3432919979095, 'total_duration': 4621.099257946014, 'accumulated_submission_time': 3133.3432919979095, 'accumulated_eval_time': 1487.1310422420502, 'accumulated_logging_time': 0.36079931259155273}
I0205 23:48:04.833390 140266968643328 logging_writer.py:48] [9800] accumulated_eval_time=1487.131042, accumulated_logging_time=0.360799, accumulated_submission_time=3133.343292, global_step=9800, preemption_count=0, score=3133.343292, test/accuracy=0.984768, test/loss=0.052419, test/mean_average_precision=0.170623, test/num_examples=43793, total_duration=4621.099258, train/accuracy=0.988487, train/loss=0.039674, train/mean_average_precision=0.205812, validation/accuracy=0.985660, validation/loss=0.049449, validation/mean_average_precision=0.170038, validation/num_examples=43793
I0205 23:48:05.172837 140388941678336 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.022473923861980438, loss=0.03713042289018631
I0205 23:48:36.751605 140266968643328 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.06635717302560806, loss=0.041879892349243164
I0205 23:49:08.357085 140388941678336 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.034397635608911514, loss=0.03874088451266289
I0205 23:49:39.839556 140266968643328 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.08438931405544281, loss=0.03714265301823616
I0205 23:50:10.975186 140388941678336 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.10415614396333694, loss=0.039614468812942505
I0205 23:50:42.504105 140266968643328 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.04369257763028145, loss=0.040761373937129974
I0205 23:51:14.180768 140388941678336 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.030567780137062073, loss=0.037130020558834076
I0205 23:51:45.435873 140266968643328 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.041668038815259933, loss=0.03949814289808273
I0205 23:52:04.948718 140451058161472 spec.py:321] Evaluating on the training split.
I0205 23:53:46.020808 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 23:53:49.163735 140451058161472 spec.py:349] Evaluating on the test split.
I0205 23:53:52.197034 140451058161472 submission_runner.py:408] Time since start: 4968.48s, 	Step: 10563, 	{'train/accuracy': 0.9885579347610474, 'train/loss': 0.039471160620450974, 'train/mean_average_precision': 0.19193969492430596, 'validation/accuracy': 0.9855850338935852, 'validation/loss': 0.049318790435791016, 'validation/mean_average_precision': 0.1746983318028851, 'validation/num_examples': 43793, 'test/accuracy': 0.9847211241722107, 'test/loss': 0.05216002091765404, 'test/mean_average_precision': 0.1650606484348421, 'test/num_examples': 43793, 'score': 3373.4267842769623, 'total_duration': 4968.480097293854, 'accumulated_submission_time': 3373.4267842769623, 'accumulated_eval_time': 1594.379231929779, 'accumulated_logging_time': 0.39026618003845215}
I0205 23:53:52.213687 140248415348480 logging_writer.py:48] [10563] accumulated_eval_time=1594.379232, accumulated_logging_time=0.390266, accumulated_submission_time=3373.426784, global_step=10563, preemption_count=0, score=3373.426784, test/accuracy=0.984721, test/loss=0.052160, test/mean_average_precision=0.165061, test/num_examples=43793, total_duration=4968.480097, train/accuracy=0.988558, train/loss=0.039471, train/mean_average_precision=0.191940, validation/accuracy=0.985585, validation/loss=0.049319, validation/mean_average_precision=0.174698, validation/num_examples=43793
I0205 23:54:04.076608 140266960250624 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.030292022973299026, loss=0.03600074350833893
I0205 23:54:35.048938 140248415348480 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.06631210446357727, loss=0.040452782064676285
I0205 23:55:06.110652 140266960250624 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.0453982837498188, loss=0.03991467505693436
I0205 23:55:37.170666 140248415348480 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.05244683474302292, loss=0.04334099963307381
I0205 23:56:08.278938 140266960250624 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.0787062719464302, loss=0.039882488548755646
I0205 23:56:39.240169 140248415348480 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.10176512598991394, loss=0.04104049503803253
I0205 23:57:10.450301 140266960250624 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.059928249567747116, loss=0.04355452209711075
I0205 23:57:41.597491 140248415348480 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.05436289310455322, loss=0.04034743830561638
I0205 23:57:52.333788 140451058161472 spec.py:321] Evaluating on the training split.
I0205 23:59:27.802775 140451058161472 spec.py:333] Evaluating on the validation split.
I0205 23:59:31.013466 140451058161472 spec.py:349] Evaluating on the test split.
I0205 23:59:34.026495 140451058161472 submission_runner.py:408] Time since start: 5310.31s, 	Step: 11336, 	{'train/accuracy': 0.9885560870170593, 'train/loss': 0.039330873638391495, 'train/mean_average_precision': 0.2063082084535478, 'validation/accuracy': 0.9856479167938232, 'validation/loss': 0.04939792677760124, 'validation/mean_average_precision': 0.18426530556965534, 'validation/num_examples': 43793, 'test/accuracy': 0.9846722483634949, 'test/loss': 0.05250081792473793, 'test/mean_average_precision': 0.1774870521222304, 'test/num_examples': 43793, 'score': 3613.5157945156097, 'total_duration': 5310.309653043747, 'accumulated_submission_time': 3613.5157945156097, 'accumulated_eval_time': 1696.071894645691, 'accumulated_logging_time': 0.41802167892456055}
I0205 23:59:34.043940 140229879785216 logging_writer.py:48] [11336] accumulated_eval_time=1696.071895, accumulated_logging_time=0.418022, accumulated_submission_time=3613.515795, global_step=11336, preemption_count=0, score=3613.515795, test/accuracy=0.984672, test/loss=0.052501, test/mean_average_precision=0.177487, test/num_examples=43793, total_duration=5310.309653, train/accuracy=0.988556, train/loss=0.039331, train/mean_average_precision=0.206308, validation/accuracy=0.985648, validation/loss=0.049398, validation/mean_average_precision=0.184265, validation/num_examples=43793
I0205 23:59:54.313556 140290196547328 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.04462230205535889, loss=0.03911133483052254
I0206 00:00:25.566543 140229879785216 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.06647734344005585, loss=0.04169534519314766
I0206 00:00:56.591035 140290196547328 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.032311372458934784, loss=0.04084097221493721
I0206 00:01:27.880133 140229879785216 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.026011498644948006, loss=0.03899218887090683
I0206 00:01:59.041588 140290196547328 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.08294189721345901, loss=0.04041990265250206
I0206 00:02:30.331080 140229879785216 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.04640195146203041, loss=0.03963437303900719
I0206 00:03:01.994043 140290196547328 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.03907463327050209, loss=0.03933835029602051
I0206 00:03:33.559283 140229879785216 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.04683775082230568, loss=0.03951584920287132
I0206 00:03:34.200079 140451058161472 spec.py:321] Evaluating on the training split.
I0206 00:05:13.444749 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 00:05:16.460100 140451058161472 spec.py:349] Evaluating on the test split.
I0206 00:05:19.416962 140451058161472 submission_runner.py:408] Time since start: 5655.70s, 	Step: 12103, 	{'train/accuracy': 0.9886201024055481, 'train/loss': 0.03922673314809799, 'train/mean_average_precision': 0.2103504437889654, 'validation/accuracy': 0.9856515526771545, 'validation/loss': 0.048678528517484665, 'validation/mean_average_precision': 0.17721198220892478, 'validation/num_examples': 43793, 'test/accuracy': 0.9847754836082458, 'test/loss': 0.05141591280698776, 'test/mean_average_precision': 0.17533691284810193, 'test/num_examples': 43793, 'score': 3853.6414008140564, 'total_duration': 5655.700120210648, 'accumulated_submission_time': 3853.6414008140564, 'accumulated_eval_time': 1801.2887377738953, 'accumulated_logging_time': 0.44663023948669434}
I0206 00:05:19.433980 140248415348480 logging_writer.py:48] [12103] accumulated_eval_time=1801.288738, accumulated_logging_time=0.446630, accumulated_submission_time=3853.641401, global_step=12103, preemption_count=0, score=3853.641401, test/accuracy=0.984775, test/loss=0.051416, test/mean_average_precision=0.175337, test/num_examples=43793, total_duration=5655.700120, train/accuracy=0.988620, train/loss=0.039227, train/mean_average_precision=0.210350, validation/accuracy=0.985652, validation/loss=0.048679, validation/mean_average_precision=0.177212, validation/num_examples=43793
I0206 00:05:50.336398 140266968643328 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.05022299662232399, loss=0.03834902495145798
I0206 00:06:21.736793 140248415348480 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.04476761072874069, loss=0.03854917362332344
I0206 00:06:53.134080 140266968643328 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.03551090136170387, loss=0.03939703479409218
I0206 00:07:24.708855 140248415348480 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.07030016928911209, loss=0.03833439573645592
I0206 00:07:56.061287 140266968643328 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.028419647365808487, loss=0.042756348848342896
I0206 00:08:27.584834 140248415348480 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.03179685026407242, loss=0.0410236194729805
I0206 00:08:58.795597 140266968643328 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.032861948013305664, loss=0.04146252200007439
I0206 00:09:19.480863 140451058161472 spec.py:321] Evaluating on the training split.
I0206 00:10:54.588111 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 00:10:57.594499 140451058161472 spec.py:349] Evaluating on the test split.
I0206 00:11:00.538583 140451058161472 submission_runner.py:408] Time since start: 5996.82s, 	Step: 12866, 	{'train/accuracy': 0.988673210144043, 'train/loss': 0.03916774317622185, 'train/mean_average_precision': 0.21065034903441726, 'validation/accuracy': 0.9855707883834839, 'validation/loss': 0.04928121343255043, 'validation/mean_average_precision': 0.1662361976664409, 'validation/num_examples': 43793, 'test/accuracy': 0.9846440553665161, 'test/loss': 0.052109234035015106, 'test/mean_average_precision': 0.16405105374278703, 'test/num_examples': 43793, 'score': 4093.658228158951, 'total_duration': 5996.82173871994, 'accumulated_submission_time': 4093.658228158951, 'accumulated_eval_time': 1902.3464109897614, 'accumulated_logging_time': 0.4743480682373047}
I0206 00:11:00.557108 140229879785216 logging_writer.py:48] [12866] accumulated_eval_time=1902.346411, accumulated_logging_time=0.474348, accumulated_submission_time=4093.658228, global_step=12866, preemption_count=0, score=4093.658228, test/accuracy=0.984644, test/loss=0.052109, test/mean_average_precision=0.164051, test/num_examples=43793, total_duration=5996.821739, train/accuracy=0.988673, train/loss=0.039168, train/mean_average_precision=0.210650, validation/accuracy=0.985571, validation/loss=0.049281, validation/mean_average_precision=0.166236, validation/num_examples=43793
I0206 00:11:11.525614 140290196547328 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.03741813823580742, loss=0.03706984221935272
I0206 00:11:42.497382 140229879785216 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.02762400172650814, loss=0.033971015363931656
I0206 00:12:13.791687 140290196547328 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.046429116278886795, loss=0.038498248904943466
I0206 00:12:45.242228 140229879785216 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.05348270386457443, loss=0.03535067290067673
I0206 00:13:16.979497 140290196547328 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.0717734694480896, loss=0.036013808101415634
I0206 00:13:48.642886 140229879785216 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.0488586463034153, loss=0.03694586828351021
I0206 00:14:20.287672 140290196547328 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.0759652853012085, loss=0.03689643368124962
I0206 00:14:51.574944 140229879785216 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.03486929461359978, loss=0.03880180045962334
I0206 00:15:00.635381 140451058161472 spec.py:321] Evaluating on the training split.
I0206 00:16:42.662401 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 00:16:45.666009 140451058161472 spec.py:349] Evaluating on the test split.
I0206 00:16:48.682660 140451058161472 submission_runner.py:408] Time since start: 6344.97s, 	Step: 13629, 	{'train/accuracy': 0.98865807056427, 'train/loss': 0.03865594416856766, 'train/mean_average_precision': 0.21663015439800132, 'validation/accuracy': 0.9856945872306824, 'validation/loss': 0.049088768661022186, 'validation/mean_average_precision': 0.1791409499424729, 'validation/num_examples': 43793, 'test/accuracy': 0.9846861958503723, 'test/loss': 0.05202307179570198, 'test/mean_average_precision': 0.17551811385025048, 'test/num_examples': 43793, 'score': 4333.704358816147, 'total_duration': 6344.965814590454, 'accumulated_submission_time': 4333.704358816147, 'accumulated_eval_time': 2010.3936491012573, 'accumulated_logging_time': 0.5054383277893066}
I0206 00:16:48.700595 140248415348480 logging_writer.py:48] [13629] accumulated_eval_time=2010.393649, accumulated_logging_time=0.505438, accumulated_submission_time=4333.704359, global_step=13629, preemption_count=0, score=4333.704359, test/accuracy=0.984686, test/loss=0.052023, test/mean_average_precision=0.175518, test/num_examples=43793, total_duration=6344.965815, train/accuracy=0.988658, train/loss=0.038656, train/mean_average_precision=0.216630, validation/accuracy=0.985695, validation/loss=0.049089, validation/mean_average_precision=0.179141, validation/num_examples=43793
I0206 00:17:11.697000 140266960250624 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.04501296579837799, loss=0.03927645459771156
I0206 00:17:43.366525 140248415348480 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.03170904889702797, loss=0.03473041206598282
I0206 00:18:14.739648 140266960250624 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.02832914888858795, loss=0.04081418365240097
I0206 00:18:46.082990 140248415348480 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.03027694672346115, loss=0.038937777280807495
I0206 00:19:18.089463 140266960250624 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.029761122539639473, loss=0.04053815081715584
I0206 00:19:49.657507 140248415348480 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.05651154741644859, loss=0.040531840175390244
I0206 00:20:21.068885 140266960250624 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.027466431260108948, loss=0.03994635492563248
I0206 00:20:48.728106 140451058161472 spec.py:321] Evaluating on the training split.
I0206 00:22:28.313311 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 00:22:31.371978 140451058161472 spec.py:349] Evaluating on the test split.
I0206 00:22:34.452831 140451058161472 submission_runner.py:408] Time since start: 6690.74s, 	Step: 14389, 	{'train/accuracy': 0.9884803295135498, 'train/loss': 0.04013616219162941, 'train/mean_average_precision': 0.19236986708748655, 'validation/accuracy': 0.9853804111480713, 'validation/loss': 0.049872953444719315, 'validation/mean_average_precision': 0.17261028306643447, 'validation/num_examples': 43793, 'test/accuracy': 0.9843934178352356, 'test/loss': 0.05279448628425598, 'test/mean_average_precision': 0.16882090915140036, 'test/num_examples': 43793, 'score': 4573.699645042419, 'total_duration': 6690.735986948013, 'accumulated_submission_time': 4573.699645042419, 'accumulated_eval_time': 2116.118327856064, 'accumulated_logging_time': 0.5356552600860596}
I0206 00:22:34.471657 140229879785216 logging_writer.py:48] [14389] accumulated_eval_time=2116.118328, accumulated_logging_time=0.535655, accumulated_submission_time=4573.699645, global_step=14389, preemption_count=0, score=4573.699645, test/accuracy=0.984393, test/loss=0.052794, test/mean_average_precision=0.168821, test/num_examples=43793, total_duration=6690.735987, train/accuracy=0.988480, train/loss=0.040136, train/mean_average_precision=0.192370, validation/accuracy=0.985380, validation/loss=0.049873, validation/mean_average_precision=0.172610, validation/num_examples=43793
I0206 00:22:38.305598 140290196547328 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.057286690920591354, loss=0.04281507059931755
I0206 00:23:10.017882 140229879785216 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.062214821577072144, loss=0.036197975277900696
I0206 00:23:41.571827 140290196547328 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.11396109312772751, loss=0.04240679740905762
I0206 00:24:12.898889 140229879785216 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.07099813967943192, loss=0.04056151956319809
I0206 00:24:44.630488 140290196547328 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.07424832880496979, loss=0.03340265154838562
I0206 00:25:15.751741 140229879785216 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.07641685754060745, loss=0.03665123134851456
I0206 00:25:47.260027 140290196547328 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.0487113855779171, loss=0.041737619787454605
I0206 00:26:18.646505 140229879785216 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.030667385086417198, loss=0.035930633544921875
I0206 00:26:34.680063 140451058161472 spec.py:321] Evaluating on the training split.
I0206 00:28:18.289012 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 00:28:21.764037 140451058161472 spec.py:349] Evaluating on the test split.
I0206 00:28:25.225335 140451058161472 submission_runner.py:408] Time since start: 7041.51s, 	Step: 15152, 	{'train/accuracy': 0.9884754419326782, 'train/loss': 0.03947283327579498, 'train/mean_average_precision': 0.20115746473315604, 'validation/accuracy': 0.9856117963790894, 'validation/loss': 0.049600694328546524, 'validation/mean_average_precision': 0.1733149277449493, 'validation/num_examples': 43793, 'test/accuracy': 0.984649121761322, 'test/loss': 0.05258084833621979, 'test/mean_average_precision': 0.17377426183102065, 'test/num_examples': 43793, 'score': 4813.877220630646, 'total_duration': 7041.50847029686, 'accumulated_submission_time': 4813.877220630646, 'accumulated_eval_time': 2226.663529396057, 'accumulated_logging_time': 0.5656332969665527}
I0206 00:28:25.246429 140248415348480 logging_writer.py:48] [15152] accumulated_eval_time=2226.663529, accumulated_logging_time=0.565633, accumulated_submission_time=4813.877221, global_step=15152, preemption_count=0, score=4813.877221, test/accuracy=0.984649, test/loss=0.052581, test/mean_average_precision=0.173774, test/num_examples=43793, total_duration=7041.508470, train/accuracy=0.988475, train/loss=0.039473, train/mean_average_precision=0.201157, validation/accuracy=0.985612, validation/loss=0.049601, validation/mean_average_precision=0.173315, validation/num_examples=43793
I0206 00:28:41.228886 140266968643328 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.05792344734072685, loss=0.042373720556497574
I0206 00:29:13.798922 140248415348480 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.055747125297784805, loss=0.038580272346735
I0206 00:29:46.902385 140266968643328 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.03811411187052727, loss=0.03798492252826691
I0206 00:30:19.105042 140248415348480 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.09753965586423874, loss=0.03896338865160942
I0206 00:30:50.311725 140266968643328 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.034471504390239716, loss=0.041661836206912994
I0206 00:31:21.695359 140248415348480 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.05819133669137955, loss=0.03838178515434265
I0206 00:31:53.243374 140266968643328 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.03331456333398819, loss=0.03831569850444794
I0206 00:32:24.870379 140248415348480 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.043941058218479156, loss=0.040180910378694534
I0206 00:32:25.524666 140451058161472 spec.py:321] Evaluating on the training split.
I0206 00:33:59.577933 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 00:34:02.818731 140451058161472 spec.py:349] Evaluating on the test split.
I0206 00:34:05.791277 140451058161472 submission_runner.py:408] Time since start: 7382.07s, 	Step: 15903, 	{'train/accuracy': 0.9885097742080688, 'train/loss': 0.03935691714286804, 'train/mean_average_precision': 0.20351287691145448, 'validation/accuracy': 0.9857429265975952, 'validation/loss': 0.04877891391515732, 'validation/mean_average_precision': 0.18361947724181774, 'validation/num_examples': 43793, 'test/accuracy': 0.9847834706306458, 'test/loss': 0.051879771053791046, 'test/mean_average_precision': 0.1804877903662326, 'test/num_examples': 43793, 'score': 5054.121758937836, 'total_duration': 7382.07443356514, 'accumulated_submission_time': 5054.121758937836, 'accumulated_eval_time': 2326.9300923347473, 'accumulated_logging_time': 0.5987019538879395}
I0206 00:34:05.809423 140229879785216 logging_writer.py:48] [15903] accumulated_eval_time=2326.930092, accumulated_logging_time=0.598702, accumulated_submission_time=5054.121759, global_step=15903, preemption_count=0, score=5054.121759, test/accuracy=0.984783, test/loss=0.051880, test/mean_average_precision=0.180488, test/num_examples=43793, total_duration=7382.074434, train/accuracy=0.988510, train/loss=0.039357, train/mean_average_precision=0.203513, validation/accuracy=0.985743, validation/loss=0.048779, validation/mean_average_precision=0.183619, validation/num_examples=43793
I0206 00:34:36.722030 140290196547328 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.041481006890535355, loss=0.03773486241698265
I0206 00:35:08.302761 140229879785216 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.03364254906773567, loss=0.039708416908979416
I0206 00:35:39.661087 140290196547328 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.0651034265756607, loss=0.040136758238077164
I0206 00:36:10.997829 140229879785216 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.031571172177791595, loss=0.04225794970989227
I0206 00:36:42.530594 140290196547328 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.055680155754089355, loss=0.039572421461343765
I0206 00:37:14.066273 140229879785216 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.07244236767292023, loss=0.04367050528526306
I0206 00:37:45.681354 140290196547328 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.026304351165890694, loss=0.04142732545733452
I0206 00:38:05.983338 140451058161472 spec.py:321] Evaluating on the training split.
I0206 00:39:44.570430 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 00:39:49.794234 140451058161472 spec.py:349] Evaluating on the test split.
I0206 00:39:52.824092 140451058161472 submission_runner.py:408] Time since start: 7729.11s, 	Step: 16665, 	{'train/accuracy': 0.9887725710868835, 'train/loss': 0.038911666721105576, 'train/mean_average_precision': 0.20659298565540096, 'validation/accuracy': 0.9857416749000549, 'validation/loss': 0.0486304834485054, 'validation/mean_average_precision': 0.18239847980590773, 'validation/num_examples': 43793, 'test/accuracy': 0.9848668575286865, 'test/loss': 0.05157385393977165, 'test/mean_average_precision': 0.17525405956295786, 'test/num_examples': 43793, 'score': 5294.265455007553, 'total_duration': 7729.107240438461, 'accumulated_submission_time': 5294.265455007553, 'accumulated_eval_time': 2433.7707934379578, 'accumulated_logging_time': 0.6277382373809814}
I0206 00:39:52.849131 140266960250624 logging_writer.py:48] [16665] accumulated_eval_time=2433.770793, accumulated_logging_time=0.627738, accumulated_submission_time=5294.265455, global_step=16665, preemption_count=0, score=5294.265455, test/accuracy=0.984867, test/loss=0.051574, test/mean_average_precision=0.175254, test/num_examples=43793, total_duration=7729.107240, train/accuracy=0.988773, train/loss=0.038912, train/mean_average_precision=0.206593, validation/accuracy=0.985742, validation/loss=0.048630, validation/mean_average_precision=0.182398, validation/num_examples=43793
I0206 00:40:04.189440 140266968643328 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.0641842931509018, loss=0.043024059385061264
I0206 00:40:35.457041 140266960250624 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.06976346671581268, loss=0.03865643963217735
I0206 00:41:06.674030 140266968643328 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.055519986897706985, loss=0.03774971142411232
I0206 00:41:38.258189 140266960250624 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.09124297648668289, loss=0.04253213480114937
I0206 00:42:09.733288 140266968643328 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.07829272747039795, loss=0.03788158297538757
I0206 00:42:41.117466 140266960250624 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.03297928720712662, loss=0.0386449471116066
I0206 00:43:12.985187 140266968643328 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.03733540698885918, loss=0.03564911335706711
I0206 00:43:45.115653 140266960250624 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.031567856669425964, loss=0.03708169236779213
I0206 00:43:52.872136 140451058161472 spec.py:321] Evaluating on the training split.
I0206 00:45:34.236829 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 00:45:37.237180 140451058161472 spec.py:349] Evaluating on the test split.
I0206 00:45:40.179369 140451058161472 submission_runner.py:408] Time since start: 8076.46s, 	Step: 17425, 	{'train/accuracy': 0.9885989427566528, 'train/loss': 0.03919602930545807, 'train/mean_average_precision': 0.20203607163677134, 'validation/accuracy': 0.9855594038963318, 'validation/loss': 0.049065567553043365, 'validation/mean_average_precision': 0.1677639326494566, 'validation/num_examples': 43793, 'test/accuracy': 0.9847135543823242, 'test/loss': 0.052009761333465576, 'test/mean_average_precision': 0.1743933827146704, 'test/num_examples': 43793, 'score': 5534.256805181503, 'total_duration': 8076.462526798248, 'accumulated_submission_time': 5534.256805181503, 'accumulated_eval_time': 2541.077990293503, 'accumulated_logging_time': 0.6633737087249756}
I0206 00:45:40.198525 140229879785216 logging_writer.py:48] [17425] accumulated_eval_time=2541.077990, accumulated_logging_time=0.663374, accumulated_submission_time=5534.256805, global_step=17425, preemption_count=0, score=5534.256805, test/accuracy=0.984714, test/loss=0.052010, test/mean_average_precision=0.174393, test/num_examples=43793, total_duration=8076.462527, train/accuracy=0.988599, train/loss=0.039196, train/mean_average_precision=0.202036, validation/accuracy=0.985559, validation/loss=0.049066, validation/mean_average_precision=0.167764, validation/num_examples=43793
I0206 00:46:04.561515 140290196547328 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.03283205255866051, loss=0.04149129241704941
I0206 00:46:35.879554 140229879785216 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.09248675405979156, loss=0.03614456579089165
I0206 00:47:07.459314 140290196547328 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.032492902129888535, loss=0.03542046993970871
I0206 00:47:39.038137 140229879785216 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.04183220863342285, loss=0.037660930305719376
I0206 00:48:10.615952 140290196547328 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.05640990287065506, loss=0.0387750118970871
I0206 00:48:42.965839 140229879785216 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.032808996737003326, loss=0.039038319140672684
I0206 00:49:14.468716 140290196547328 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.029965752735733986, loss=0.04341225326061249
I0206 00:49:40.375553 140451058161472 spec.py:321] Evaluating on the training split.
I0206 00:51:19.346521 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 00:51:22.832260 140451058161472 spec.py:349] Evaluating on the test split.
I0206 00:51:26.296778 140451058161472 submission_runner.py:408] Time since start: 8422.58s, 	Step: 18183, 	{'train/accuracy': 0.9885631799697876, 'train/loss': 0.03920503705739975, 'train/mean_average_precision': 0.21115823829716834, 'validation/accuracy': 0.9856604933738708, 'validation/loss': 0.049062520265579224, 'validation/mean_average_precision': 0.1755662687161741, 'validation/num_examples': 43793, 'test/accuracy': 0.9847097396850586, 'test/loss': 0.051849689334630966, 'test/mean_average_precision': 0.17605731491423418, 'test/num_examples': 43793, 'score': 5774.4022517204285, 'total_duration': 8422.579911470413, 'accumulated_submission_time': 5774.4022517204285, 'accumulated_eval_time': 2646.9991524219513, 'accumulated_logging_time': 0.6947915554046631}
I0206 00:51:26.319502 140248415348480 logging_writer.py:48] [18183] accumulated_eval_time=2646.999152, accumulated_logging_time=0.694792, accumulated_submission_time=5774.402252, global_step=18183, preemption_count=0, score=5774.402252, test/accuracy=0.984710, test/loss=0.051850, test/mean_average_precision=0.176057, test/num_examples=43793, total_duration=8422.579911, train/accuracy=0.988563, train/loss=0.039205, train/mean_average_precision=0.211158, validation/accuracy=0.985660, validation/loss=0.049063, validation/mean_average_precision=0.175566, validation/num_examples=43793
I0206 00:51:32.209617 140266960250624 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.04385088384151459, loss=0.03660165145993233
I0206 00:52:04.558960 140248415348480 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.04290872439742088, loss=0.04113717004656792
I0206 00:52:35.880477 140266960250624 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.05985003709793091, loss=0.039632756263017654
I0206 00:53:07.395406 140248415348480 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.047554198652505875, loss=0.03832618519663811
I0206 00:53:38.948038 140266960250624 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.12546466290950775, loss=0.04091409221291542
I0206 00:54:10.860272 140248415348480 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.0582575760781765, loss=0.04328252375125885
I0206 00:54:43.165277 140266960250624 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.04314982891082764, loss=0.03515864163637161
I0206 00:55:15.351164 140248415348480 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.05662073194980621, loss=0.03855863958597183
I0206 00:55:26.366475 140451058161472 spec.py:321] Evaluating on the training split.
I0206 00:57:05.029545 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 00:57:08.056933 140451058161472 spec.py:349] Evaluating on the test split.
I0206 00:57:11.045258 140451058161472 submission_runner.py:408] Time since start: 8767.33s, 	Step: 18935, 	{'train/accuracy': 0.9886395931243896, 'train/loss': 0.0389874204993248, 'train/mean_average_precision': 0.20706087524820685, 'validation/accuracy': 0.9856272339820862, 'validation/loss': 0.049088090658187866, 'validation/mean_average_precision': 0.17807781206289738, 'validation/num_examples': 43793, 'test/accuracy': 0.9846933484077454, 'test/loss': 0.051995303481817245, 'test/mean_average_precision': 0.1730764760968453, 'test/num_examples': 43793, 'score': 6014.416128873825, 'total_duration': 8767.328412532806, 'accumulated_submission_time': 6014.416128873825, 'accumulated_eval_time': 2751.677888393402, 'accumulated_logging_time': 0.729445219039917}
I0206 00:57:11.064729 140229879785216 logging_writer.py:48] [18935] accumulated_eval_time=2751.677888, accumulated_logging_time=0.729445, accumulated_submission_time=6014.416129, global_step=18935, preemption_count=0, score=6014.416129, test/accuracy=0.984693, test/loss=0.051995, test/mean_average_precision=0.173076, test/num_examples=43793, total_duration=8767.328413, train/accuracy=0.988640, train/loss=0.038987, train/mean_average_precision=0.207061, validation/accuracy=0.985627, validation/loss=0.049088, validation/mean_average_precision=0.178078, validation/num_examples=43793
I0206 00:57:31.949388 140266968643328 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.04623746499419212, loss=0.041136108338832855
I0206 00:58:03.819192 140229879785216 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.03763855621218681, loss=0.034426186233758926
I0206 00:58:35.844322 140266968643328 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.11748135834932327, loss=0.040596939623355865
I0206 00:59:07.854396 140229879785216 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.03437856584787369, loss=0.03658544644713402
I0206 00:59:40.308321 140266968643328 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.05623587593436241, loss=0.03698105365037918
I0206 01:00:12.416555 140229879785216 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.09719602763652802, loss=0.04054209217429161
I0206 01:00:43.912624 140266968643328 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.10988536477088928, loss=0.03978215530514717
I0206 01:01:11.207542 140451058161472 spec.py:321] Evaluating on the training split.
I0206 01:02:46.011877 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 01:02:48.975286 140451058161472 spec.py:349] Evaluating on the test split.
I0206 01:02:51.966188 140451058161472 submission_runner.py:408] Time since start: 9108.25s, 	Step: 19686, 	{'train/accuracy': 0.9886992573738098, 'train/loss': 0.038554951548576355, 'train/mean_average_precision': 0.21994364760224747, 'validation/accuracy': 0.9856789708137512, 'validation/loss': 0.04852884262800217, 'validation/mean_average_precision': 0.1826200142812672, 'validation/num_examples': 43793, 'test/accuracy': 0.984761118888855, 'test/loss': 0.05136070027947426, 'test/mean_average_precision': 0.1776437460243953, 'test/num_examples': 43793, 'score': 6254.527894496918, 'total_duration': 9108.249343633652, 'accumulated_submission_time': 6254.527894496918, 'accumulated_eval_time': 2852.4364881515503, 'accumulated_logging_time': 0.760200023651123}
I0206 01:02:51.985280 140248415348480 logging_writer.py:48] [19686] accumulated_eval_time=2852.436488, accumulated_logging_time=0.760200, accumulated_submission_time=6254.527894, global_step=19686, preemption_count=0, score=6254.527894, test/accuracy=0.984761, test/loss=0.051361, test/mean_average_precision=0.177644, test/num_examples=43793, total_duration=9108.249344, train/accuracy=0.988699, train/loss=0.038555, train/mean_average_precision=0.219944, validation/accuracy=0.985679, validation/loss=0.048529, validation/mean_average_precision=0.182620, validation/num_examples=43793
I0206 01:02:56.973264 140266960250624 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.08363413065671921, loss=0.04550473764538765
I0206 01:03:28.462807 140248415348480 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.025586772710084915, loss=0.038015082478523254
I0206 01:03:59.691813 140266960250624 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.03675607964396477, loss=0.038129452615976334
I0206 01:04:31.268226 140248415348480 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.041334573179483414, loss=0.03757871687412262
I0206 01:05:02.805709 140266960250624 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.07500207424163818, loss=0.04151332005858421
I0206 01:05:33.978635 140248415348480 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.04018070548772812, loss=0.041732802987098694
I0206 01:06:05.501882 140266960250624 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.03839986026287079, loss=0.03608468174934387
I0206 01:06:37.572520 140248415348480 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.05197789520025253, loss=0.03772379085421562
I0206 01:06:52.078493 140451058161472 spec.py:321] Evaluating on the training split.
I0206 01:08:32.101161 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 01:08:35.217698 140451058161472 spec.py:349] Evaluating on the test split.
I0206 01:08:38.249965 140451058161472 submission_runner.py:408] Time since start: 9454.53s, 	Step: 20446, 	{'train/accuracy': 0.9888023138046265, 'train/loss': 0.038551561534404755, 'train/mean_average_precision': 0.21080826995503685, 'validation/accuracy': 0.9857754111289978, 'validation/loss': 0.048722174018621445, 'validation/mean_average_precision': 0.1826559964014653, 'validation/num_examples': 43793, 'test/accuracy': 0.9848723411560059, 'test/loss': 0.0517776682972908, 'test/mean_average_precision': 0.17850474728096502, 'test/num_examples': 43793, 'score': 6494.588192939758, 'total_duration': 9454.533122062683, 'accumulated_submission_time': 6494.588192939758, 'accumulated_eval_time': 2958.607923746109, 'accumulated_logging_time': 0.7917554378509521}
I0206 01:08:38.269953 140229879785216 logging_writer.py:48] [20446] accumulated_eval_time=2958.607924, accumulated_logging_time=0.791755, accumulated_submission_time=6494.588193, global_step=20446, preemption_count=0, score=6494.588193, test/accuracy=0.984872, test/loss=0.051778, test/mean_average_precision=0.178505, test/num_examples=43793, total_duration=9454.533122, train/accuracy=0.988802, train/loss=0.038552, train/mean_average_precision=0.210808, validation/accuracy=0.985775, validation/loss=0.048722, validation/mean_average_precision=0.182656, validation/num_examples=43793
I0206 01:08:55.694410 140266968643328 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.05959748849272728, loss=0.04023875296115875
I0206 01:09:27.412738 140229879785216 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.03128144517540932, loss=0.04132205620408058
I0206 01:09:59.081740 140266968643328 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.10821286588907242, loss=0.04279660806059837
I0206 01:10:31.202021 140229879785216 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.051685772836208344, loss=0.040508076548576355
I0206 01:11:03.159873 140266968643328 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.08505464345216751, loss=0.038002364337444305
I0206 01:11:35.148461 140229879785216 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.07897712290287018, loss=0.0470735989511013
I0206 01:12:07.148409 140266968643328 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.04662725329399109, loss=0.04210919886827469
I0206 01:12:38.544705 140229879785216 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.034727517515420914, loss=0.03939471021294594
I0206 01:12:38.549665 140451058161472 spec.py:321] Evaluating on the training split.
I0206 01:14:16.562933 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 01:14:19.586220 140451058161472 spec.py:349] Evaluating on the test split.
I0206 01:14:22.653657 140451058161472 submission_runner.py:408] Time since start: 9798.94s, 	Step: 21201, 	{'train/accuracy': 0.988953709602356, 'train/loss': 0.037700071930885315, 'train/mean_average_precision': 0.230811797165299, 'validation/accuracy': 0.9858095049858093, 'validation/loss': 0.04827570170164108, 'validation/mean_average_precision': 0.187712481522891, 'validation/num_examples': 43793, 'test/accuracy': 0.9848698377609253, 'test/loss': 0.05133536458015442, 'test/mean_average_precision': 0.18289813381186903, 'test/num_examples': 43793, 'score': 6734.835546016693, 'total_duration': 9798.936812639236, 'accumulated_submission_time': 6734.835546016693, 'accumulated_eval_time': 3062.7118458747864, 'accumulated_logging_time': 0.8235526084899902}
I0206 01:14:22.673683 140266960250624 logging_writer.py:48] [21201] accumulated_eval_time=3062.711846, accumulated_logging_time=0.823553, accumulated_submission_time=6734.835546, global_step=21201, preemption_count=0, score=6734.835546, test/accuracy=0.984870, test/loss=0.051335, test/mean_average_precision=0.182898, test/num_examples=43793, total_duration=9798.936813, train/accuracy=0.988954, train/loss=0.037700, train/mean_average_precision=0.230812, validation/accuracy=0.985810, validation/loss=0.048276, validation/mean_average_precision=0.187712, validation/num_examples=43793
I0206 01:14:54.626246 140290196547328 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.06324209272861481, loss=0.03866183012723923
I0206 01:15:26.758229 140266960250624 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.04620128124952316, loss=0.03829747438430786
I0206 01:15:58.311511 140290196547328 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.13028329610824585, loss=0.03893629461526871
I0206 01:16:30.336070 140266960250624 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.08352373540401459, loss=0.03686818853020668
I0206 01:17:01.800871 140290196547328 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.04379807785153389, loss=0.03906603902578354
I0206 01:17:33.546694 140266960250624 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.035027001053094864, loss=0.03607442229986191
I0206 01:18:05.352100 140290196547328 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.046258505433797836, loss=0.03809073194861412
I0206 01:18:22.965157 140451058161472 spec.py:321] Evaluating on the training split.
I0206 01:19:59.083383 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 01:20:02.396419 140451058161472 spec.py:349] Evaluating on the test split.
I0206 01:20:05.461947 140451058161472 submission_runner.py:408] Time since start: 10141.75s, 	Step: 21957, 	{'train/accuracy': 0.9888525605201721, 'train/loss': 0.03817529231309891, 'train/mean_average_precision': 0.21947451632087114, 'validation/accuracy': 0.985712468624115, 'validation/loss': 0.048748038709163666, 'validation/mean_average_precision': 0.18646979911386335, 'validation/num_examples': 43793, 'test/accuracy': 0.9847274422645569, 'test/loss': 0.05180920287966728, 'test/mean_average_precision': 0.1798832976796487, 'test/num_examples': 43793, 'score': 6975.096371412277, 'total_duration': 10141.745084524155, 'accumulated_submission_time': 6975.096371412277, 'accumulated_eval_time': 3165.2085721492767, 'accumulated_logging_time': 0.85434889793396}
I0206 01:20:05.481860 140229879785216 logging_writer.py:48] [21957] accumulated_eval_time=3165.208572, accumulated_logging_time=0.854349, accumulated_submission_time=6975.096371, global_step=21957, preemption_count=0, score=6975.096371, test/accuracy=0.984727, test/loss=0.051809, test/mean_average_precision=0.179883, test/num_examples=43793, total_duration=10141.745085, train/accuracy=0.988853, train/loss=0.038175, train/mean_average_precision=0.219475, validation/accuracy=0.985712, validation/loss=0.048748, validation/mean_average_precision=0.186470, validation/num_examples=43793
I0206 01:20:19.362239 140248415348480 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.12931180000305176, loss=0.0374378003180027
I0206 01:20:50.597997 140229879785216 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.09352771192789078, loss=0.039418771862983704
I0206 01:21:22.178583 140248415348480 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.036178767681121826, loss=0.03752901777625084
I0206 01:21:53.427772 140229879785216 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.05686390399932861, loss=0.03537066653370857
I0206 01:22:24.981319 140248415348480 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.08187608420848846, loss=0.03637484461069107
I0206 01:22:56.332238 140229879785216 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.06694534420967102, loss=0.04123261943459511
I0206 01:23:27.831278 140248415348480 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.0337020568549633, loss=0.041483502835035324
I0206 01:23:59.245782 140229879785216 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.031218232586979866, loss=0.03524395823478699
I0206 01:24:05.614808 140451058161472 spec.py:321] Evaluating on the training split.
I0206 01:25:44.989295 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 01:25:48.074819 140451058161472 spec.py:349] Evaluating on the test split.
I0206 01:25:51.190335 140451058161472 submission_runner.py:408] Time since start: 10487.47s, 	Step: 22721, 	{'train/accuracy': 0.9887853860855103, 'train/loss': 0.03829365223646164, 'train/mean_average_precision': 0.22578721979965902, 'validation/accuracy': 0.9858614802360535, 'validation/loss': 0.048403218388557434, 'validation/mean_average_precision': 0.18717810663629783, 'validation/num_examples': 43793, 'test/accuracy': 0.9849422574043274, 'test/loss': 0.051137737929821014, 'test/mean_average_precision': 0.18069880982036599, 'test/num_examples': 43793, 'score': 7215.198048114777, 'total_duration': 10487.473489522934, 'accumulated_submission_time': 7215.198048114777, 'accumulated_eval_time': 3270.7840468883514, 'accumulated_logging_time': 0.8855433464050293}
I0206 01:25:51.210862 140266968643328 logging_writer.py:48] [22721] accumulated_eval_time=3270.784047, accumulated_logging_time=0.885543, accumulated_submission_time=7215.198048, global_step=22721, preemption_count=0, score=7215.198048, test/accuracy=0.984942, test/loss=0.051138, test/mean_average_precision=0.180699, test/num_examples=43793, total_duration=10487.473490, train/accuracy=0.988785, train/loss=0.038294, train/mean_average_precision=0.225787, validation/accuracy=0.985861, validation/loss=0.048403, validation/mean_average_precision=0.187178, validation/num_examples=43793
I0206 01:26:16.733218 140290196547328 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.05236457660794258, loss=0.03586459904909134
I0206 01:26:48.761947 140266968643328 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.028403913602232933, loss=0.03545641899108887
I0206 01:27:20.252773 140290196547328 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.05682156980037689, loss=0.04211520776152611
I0206 01:27:51.825615 140266968643328 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.039538830518722534, loss=0.037856005132198334
I0206 01:28:23.099631 140290196547328 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.041816022247076035, loss=0.04291805997490883
I0206 01:28:54.409565 140266968643328 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.0402517132461071, loss=0.041989050805568695
I0206 01:29:26.012115 140290196547328 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.08043725788593292, loss=0.037063006311655045
I0206 01:29:51.302867 140451058161472 spec.py:321] Evaluating on the training split.
I0206 01:31:31.646962 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 01:31:34.873529 140451058161472 spec.py:349] Evaluating on the test split.
I0206 01:31:37.977869 140451058161472 submission_runner.py:408] Time since start: 10834.26s, 	Step: 23481, 	{'train/accuracy': 0.9887216091156006, 'train/loss': 0.0385473296046257, 'train/mean_average_precision': 0.21614784740970006, 'validation/accuracy': 0.9858009815216064, 'validation/loss': 0.048311781138181686, 'validation/mean_average_precision': 0.18691507637863614, 'validation/num_examples': 43793, 'test/accuracy': 0.9848819971084595, 'test/loss': 0.051236700266599655, 'test/mean_average_precision': 0.18170816291909414, 'test/num_examples': 43793, 'score': 7455.2598967552185, 'total_duration': 10834.261020421982, 'accumulated_submission_time': 7455.2598967552185, 'accumulated_eval_time': 3377.4589943885803, 'accumulated_logging_time': 0.9166724681854248}
I0206 01:31:37.998545 140248415348480 logging_writer.py:48] [23481] accumulated_eval_time=3377.458994, accumulated_logging_time=0.916672, accumulated_submission_time=7455.259897, global_step=23481, preemption_count=0, score=7455.259897, test/accuracy=0.984882, test/loss=0.051237, test/mean_average_precision=0.181708, test/num_examples=43793, total_duration=10834.261020, train/accuracy=0.988722, train/loss=0.038547, train/mean_average_precision=0.216148, validation/accuracy=0.985801, validation/loss=0.048312, validation/mean_average_precision=0.186915, validation/num_examples=43793
I0206 01:31:44.575344 140266960250624 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.0333237461745739, loss=0.03675335273146629
I0206 01:32:16.212081 140248415348480 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.05362233147025108, loss=0.038277480751276016
I0206 01:32:47.404633 140266960250624 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.03361590951681137, loss=0.03750372305512428
I0206 01:33:19.697948 140248415348480 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.04450087621808052, loss=0.03550121560692787
I0206 01:33:51.759982 140266960250624 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.04466218128800392, loss=0.03872572258114815
I0206 01:34:23.131886 140248415348480 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.1201072633266449, loss=0.04500620812177658
I0206 01:34:54.408400 140266960250624 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.059926826506853104, loss=0.04122438281774521
I0206 01:35:26.112530 140248415348480 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.04587215930223465, loss=0.03909548372030258
I0206 01:35:38.144097 140451058161472 spec.py:321] Evaluating on the training split.
I0206 01:37:13.309818 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 01:37:16.426399 140451058161472 spec.py:349] Evaluating on the test split.
I0206 01:37:19.459825 140451058161472 submission_runner.py:408] Time since start: 11175.74s, 	Step: 24238, 	{'train/accuracy': 0.9888251423835754, 'train/loss': 0.038296639919281006, 'train/mean_average_precision': 0.21755191098602358, 'validation/accuracy': 0.9858407378196716, 'validation/loss': 0.04817565158009529, 'validation/mean_average_precision': 0.19022482647230404, 'validation/num_examples': 43793, 'test/accuracy': 0.9849464893341064, 'test/loss': 0.051013898104429245, 'test/mean_average_precision': 0.1909638553261999, 'test/num_examples': 43793, 'score': 7695.373823165894, 'total_duration': 11175.742981672287, 'accumulated_submission_time': 7695.373823165894, 'accumulated_eval_time': 3478.77467918396, 'accumulated_logging_time': 0.9493060111999512}
I0206 01:37:19.479805 140229879785216 logging_writer.py:48] [24238] accumulated_eval_time=3478.774679, accumulated_logging_time=0.949306, accumulated_submission_time=7695.373823, global_step=24238, preemption_count=0, score=7695.373823, test/accuracy=0.984946, test/loss=0.051014, test/mean_average_precision=0.190964, test/num_examples=43793, total_duration=11175.742982, train/accuracy=0.988825, train/loss=0.038297, train/mean_average_precision=0.217552, validation/accuracy=0.985841, validation/loss=0.048176, validation/mean_average_precision=0.190225, validation/num_examples=43793
I0206 01:37:39.385603 140266968643328 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.056088585406541824, loss=0.03936200588941574
I0206 01:38:10.974602 140229879785216 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.07615084201097488, loss=0.03820854797959328
I0206 01:38:42.473217 140266968643328 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.10098621994256973, loss=0.04388584569096565
I0206 01:39:13.912288 140229879785216 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.07833368331193924, loss=0.042549826204776764
I0206 01:39:45.008926 140266968643328 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.09476277232170105, loss=0.03924113139510155
I0206 01:40:17.339071 140229879785216 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.033997174352407455, loss=0.037844467908144
I0206 01:40:49.406407 140266968643328 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.05162353068590164, loss=0.03849124163389206
I0206 01:41:19.703452 140451058161472 spec.py:321] Evaluating on the training split.
I0206 01:42:56.121975 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 01:42:59.170427 140451058161472 spec.py:349] Evaluating on the test split.
I0206 01:43:02.391668 140451058161472 submission_runner.py:408] Time since start: 11518.67s, 	Step: 24996, 	{'train/accuracy': 0.9887945652008057, 'train/loss': 0.03828536346554756, 'train/mean_average_precision': 0.22411569355984684, 'validation/accuracy': 0.9858716130256653, 'validation/loss': 0.048033662140369415, 'validation/mean_average_precision': 0.19200558721430855, 'validation/num_examples': 43793, 'test/accuracy': 0.9849563837051392, 'test/loss': 0.05095234513282776, 'test/mean_average_precision': 0.18763869259624225, 'test/num_examples': 43793, 'score': 7935.566670894623, 'total_duration': 11518.674816608429, 'accumulated_submission_time': 7935.566670894623, 'accumulated_eval_time': 3581.462842464447, 'accumulated_logging_time': 0.980283260345459}
I0206 01:43:02.414921 140248415348480 logging_writer.py:48] [24996] accumulated_eval_time=3581.462842, accumulated_logging_time=0.980283, accumulated_submission_time=7935.566671, global_step=24996, preemption_count=0, score=7935.566671, test/accuracy=0.984956, test/loss=0.050952, test/mean_average_precision=0.187639, test/num_examples=43793, total_duration=11518.674817, train/accuracy=0.988795, train/loss=0.038285, train/mean_average_precision=0.224116, validation/accuracy=0.985872, validation/loss=0.048034, validation/mean_average_precision=0.192006, validation/num_examples=43793
I0206 01:43:04.013203 140266960250624 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.042757950723171234, loss=0.036212533712387085
I0206 01:43:35.400895 140248415348480 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.05122796446084976, loss=0.039908330887556076
I0206 01:44:06.713053 140266960250624 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.08077122271060944, loss=0.04109429568052292
I0206 01:44:38.017570 140248415348480 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.053658805787563324, loss=0.04141603410243988
I0206 01:45:09.485829 140266960250624 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.04021753370761871, loss=0.04243479669094086
I0206 01:45:41.198287 140248415348480 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.061905164271593094, loss=0.03499598428606987
I0206 01:46:12.650779 140266960250624 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.04975646361708641, loss=0.03942530229687691
I0206 01:46:44.067433 140248415348480 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.03768443688750267, loss=0.03670455142855644
I0206 01:47:02.644224 140451058161472 spec.py:321] Evaluating on the training split.
I0206 01:48:42.338182 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 01:48:45.353286 140451058161472 spec.py:349] Evaluating on the test split.
I0206 01:48:48.318867 140451058161472 submission_runner.py:408] Time since start: 11864.60s, 	Step: 25759, 	{'train/accuracy': 0.9889598488807678, 'train/loss': 0.038259729743003845, 'train/mean_average_precision': 0.21395748378791343, 'validation/accuracy': 0.9858368635177612, 'validation/loss': 0.04825982078909874, 'validation/mean_average_precision': 0.1868899929087874, 'validation/num_examples': 43793, 'test/accuracy': 0.9849376082420349, 'test/loss': 0.05120475962758064, 'test/mean_average_precision': 0.17947322257862497, 'test/num_examples': 43793, 'score': 8175.762567043304, 'total_duration': 11864.60201716423, 'accumulated_submission_time': 8175.762567043304, 'accumulated_eval_time': 3687.1374304294586, 'accumulated_logging_time': 1.0166258811950684}
I0206 01:48:48.338911 140229879785216 logging_writer.py:48] [25759] accumulated_eval_time=3687.137430, accumulated_logging_time=1.016626, accumulated_submission_time=8175.762567, global_step=25759, preemption_count=0, score=8175.762567, test/accuracy=0.984938, test/loss=0.051205, test/mean_average_precision=0.179473, test/num_examples=43793, total_duration=11864.602017, train/accuracy=0.988960, train/loss=0.038260, train/mean_average_precision=0.213957, validation/accuracy=0.985837, validation/loss=0.048260, validation/mean_average_precision=0.186890, validation/num_examples=43793
I0206 01:49:01.684691 140266968643328 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.05138664320111275, loss=0.03840653970837593
I0206 01:49:33.078127 140229879785216 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.042011041194200516, loss=0.03617594391107559
I0206 01:50:04.410209 140266968643328 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.02743658237159252, loss=0.03632558509707451
I0206 01:50:35.454442 140229879785216 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.055128805339336395, loss=0.03916042298078537
I0206 01:51:06.736503 140266968643328 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.03852689638733864, loss=0.03947724401950836
I0206 01:51:38.218913 140229879785216 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.05430232360959053, loss=0.04288099333643913
I0206 01:52:09.845332 140266968643328 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.03420579060912132, loss=0.03522397577762604
I0206 01:52:41.136315 140229879785216 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.03711631894111633, loss=0.037774696946144104
I0206 01:52:48.568399 140451058161472 spec.py:321] Evaluating on the training split.
I0206 01:54:26.308185 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 01:54:29.341768 140451058161472 spec.py:349] Evaluating on the test split.
I0206 01:54:32.313432 140451058161472 submission_runner.py:408] Time since start: 12208.60s, 	Step: 26525, 	{'train/accuracy': 0.9887128472328186, 'train/loss': 0.03872840106487274, 'train/mean_average_precision': 0.212833312132566, 'validation/accuracy': 0.9856792092323303, 'validation/loss': 0.048808928579092026, 'validation/mean_average_precision': 0.1811582477184639, 'validation/num_examples': 43793, 'test/accuracy': 0.9847586154937744, 'test/loss': 0.05191972106695175, 'test/mean_average_precision': 0.17592324247629057, 'test/num_examples': 43793, 'score': 8415.960547924042, 'total_duration': 12208.59658241272, 'accumulated_submission_time': 8415.960547924042, 'accumulated_eval_time': 3790.8824088573456, 'accumulated_logging_time': 1.0475599765777588}
I0206 01:54:32.333764 140248415348480 logging_writer.py:48] [26525] accumulated_eval_time=3790.882409, accumulated_logging_time=1.047560, accumulated_submission_time=8415.960548, global_step=26525, preemption_count=0, score=8415.960548, test/accuracy=0.984759, test/loss=0.051920, test/mean_average_precision=0.175923, test/num_examples=43793, total_duration=12208.596582, train/accuracy=0.988713, train/loss=0.038728, train/mean_average_precision=0.212833, validation/accuracy=0.985679, validation/loss=0.048809, validation/mean_average_precision=0.181158, validation/num_examples=43793
I0206 01:54:56.240704 140266960250624 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.09452299028635025, loss=0.04609498381614685
I0206 01:55:27.609822 140248415348480 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.03408336266875267, loss=0.03857128322124481
I0206 01:55:58.914830 140266960250624 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.0430322103202343, loss=0.03437897562980652
I0206 01:56:30.635810 140248415348480 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.06490977853536606, loss=0.04076268896460533
I0206 01:57:01.870067 140266960250624 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.059416960924863815, loss=0.03897034004330635
I0206 01:57:33.401420 140248415348480 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.046414125710725784, loss=0.035369109362363815
I0206 01:58:04.966749 140266960250624 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.07425863295793533, loss=0.04319971799850464
I0206 01:58:32.359989 140451058161472 spec.py:321] Evaluating on the training split.
I0206 02:00:14.384932 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 02:00:17.860511 140451058161472 spec.py:349] Evaluating on the test split.
I0206 02:00:21.251408 140451058161472 submission_runner.py:408] Time since start: 12557.53s, 	Step: 27288, 	{'train/accuracy': 0.9887645244598389, 'train/loss': 0.03830304741859436, 'train/mean_average_precision': 0.2247528497563965, 'validation/accuracy': 0.9856621623039246, 'validation/loss': 0.0485771968960762, 'validation/mean_average_precision': 0.18338578642084308, 'validation/num_examples': 43793, 'test/accuracy': 0.9847981929779053, 'test/loss': 0.05146558955311775, 'test/mean_average_precision': 0.17754872683397158, 'test/num_examples': 43793, 'score': 8655.956233978271, 'total_duration': 12557.534547328949, 'accumulated_submission_time': 8655.956233978271, 'accumulated_eval_time': 3899.773766040802, 'accumulated_logging_time': 1.078413486480713}
I0206 02:00:21.274328 140229879785216 logging_writer.py:48] [27288] accumulated_eval_time=3899.773766, accumulated_logging_time=1.078413, accumulated_submission_time=8655.956234, global_step=27288, preemption_count=0, score=8655.956234, test/accuracy=0.984798, test/loss=0.051466, test/mean_average_precision=0.177549, test/num_examples=43793, total_duration=12557.534547, train/accuracy=0.988765, train/loss=0.038303, train/mean_average_precision=0.224753, validation/accuracy=0.985662, validation/loss=0.048577, validation/mean_average_precision=0.183386, validation/num_examples=43793
I0206 02:00:25.537525 140266968643328 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.07173500955104828, loss=0.03999937325716019
I0206 02:00:58.108900 140229879785216 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.08950888365507126, loss=0.037623949348926544
I0206 02:01:30.689881 140266968643328 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.060391031205654144, loss=0.034919142723083496
I0206 02:02:02.976619 140229879785216 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.04971649497747421, loss=0.03915511816740036
I0206 02:02:35.430575 140266968643328 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.08405084162950516, loss=0.039222199469804764
I0206 02:03:08.064327 140229879785216 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.061233386397361755, loss=0.03673110902309418
I0206 02:03:39.364223 140266968643328 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.03418359160423279, loss=0.038986556231975555
I0206 02:04:10.667227 140229879785216 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.05601879209280014, loss=0.0396985225379467
I0206 02:04:21.367420 140451058161472 spec.py:321] Evaluating on the training split.
I0206 02:06:00.668252 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 02:06:03.922893 140451058161472 spec.py:349] Evaluating on the test split.
I0206 02:06:06.952963 140451058161472 submission_runner.py:408] Time since start: 12903.24s, 	Step: 28035, 	{'train/accuracy': 0.9887883067131042, 'train/loss': 0.03838672488927841, 'train/mean_average_precision': 0.2266030029702564, 'validation/accuracy': 0.9857709407806396, 'validation/loss': 0.0483795665204525, 'validation/mean_average_precision': 0.18527124744371645, 'validation/num_examples': 43793, 'test/accuracy': 0.9848756790161133, 'test/loss': 0.05123374983668327, 'test/mean_average_precision': 0.18248757231048227, 'test/num_examples': 43793, 'score': 8896.014830827713, 'total_duration': 12903.236117601395, 'accumulated_submission_time': 8896.014830827713, 'accumulated_eval_time': 4005.3592603206635, 'accumulated_logging_time': 1.1128215789794922}
I0206 02:06:06.974352 140248415348480 logging_writer.py:48] [28035] accumulated_eval_time=4005.359260, accumulated_logging_time=1.112822, accumulated_submission_time=8896.014831, global_step=28035, preemption_count=0, score=8896.014831, test/accuracy=0.984876, test/loss=0.051234, test/mean_average_precision=0.182488, test/num_examples=43793, total_duration=12903.236118, train/accuracy=0.988788, train/loss=0.038387, train/mean_average_precision=0.226603, validation/accuracy=0.985771, validation/loss=0.048380, validation/mean_average_precision=0.185271, validation/num_examples=43793
I0206 02:06:28.091839 140266960250624 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.09149188548326492, loss=0.03961861878633499
I0206 02:07:00.011757 140248415348480 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.11817343533039093, loss=0.0364929661154747
I0206 02:07:32.198175 140266960250624 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.07097896933555603, loss=0.03570450469851494
I0206 02:08:04.081034 140248415348480 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.03784963861107826, loss=0.03886684402823448
I0206 02:08:35.794582 140266960250624 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.06907158344984055, loss=0.03764788806438446
I0206 02:09:07.734375 140248415348480 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.03498995676636696, loss=0.03618495166301727
I0206 02:09:39.271771 140266960250624 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.07183583080768585, loss=0.04060337319970131
I0206 02:10:07.238527 140451058161472 spec.py:321] Evaluating on the training split.
I0206 02:11:44.812686 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 02:11:47.803478 140451058161472 spec.py:349] Evaluating on the test split.
I0206 02:11:50.815951 140451058161472 submission_runner.py:408] Time since start: 13247.10s, 	Step: 28789, 	{'train/accuracy': 0.9888795018196106, 'train/loss': 0.037977900356054306, 'train/mean_average_precision': 0.22512584744264968, 'validation/accuracy': 0.9859036803245544, 'validation/loss': 0.04835958778858185, 'validation/mean_average_precision': 0.19535043027927054, 'validation/num_examples': 43793, 'test/accuracy': 0.9850125908851624, 'test/loss': 0.05125235766172409, 'test/mean_average_precision': 0.18447898547369126, 'test/num_examples': 43793, 'score': 9136.247852563858, 'total_duration': 13247.099108457565, 'accumulated_submission_time': 9136.247852563858, 'accumulated_eval_time': 4108.936638832092, 'accumulated_logging_time': 1.145425796508789}
I0206 02:11:50.837458 140229879785216 logging_writer.py:48] [28789] accumulated_eval_time=4108.936639, accumulated_logging_time=1.145426, accumulated_submission_time=9136.247853, global_step=28789, preemption_count=0, score=9136.247853, test/accuracy=0.985013, test/loss=0.051252, test/mean_average_precision=0.184479, test/num_examples=43793, total_duration=13247.099108, train/accuracy=0.988880, train/loss=0.037978, train/mean_average_precision=0.225126, validation/accuracy=0.985904, validation/loss=0.048360, validation/mean_average_precision=0.195350, validation/num_examples=43793
I0206 02:11:54.583289 140290196547328 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.0459761805832386, loss=0.03559169918298721
I0206 02:12:26.026949 140229879785216 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.11966047435998917, loss=0.034681133925914764
I0206 02:12:57.320876 140290196547328 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.060262251645326614, loss=0.03998051583766937
I0206 02:13:28.689074 140229879785216 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.1168157160282135, loss=0.035997651517391205
I0206 02:14:00.137977 140290196547328 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.051627643406391144, loss=0.036278996616601944
I0206 02:14:31.834433 140229879785216 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.03911517560482025, loss=0.03488345071673393
I0206 02:15:02.953323 140290196547328 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.05874097719788551, loss=0.03653237596154213
I0206 02:15:33.882297 140229879785216 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.04826270043849945, loss=0.03634035214781761
I0206 02:15:50.871321 140451058161472 spec.py:321] Evaluating on the training split.
I0206 02:17:25.399570 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 02:17:28.449765 140451058161472 spec.py:349] Evaluating on the test split.
I0206 02:17:31.452678 140451058161472 submission_runner.py:408] Time since start: 13587.74s, 	Step: 29556, 	{'train/accuracy': 0.9890247583389282, 'train/loss': 0.037338387221097946, 'train/mean_average_precision': 0.2374429432051383, 'validation/accuracy': 0.9858587980270386, 'validation/loss': 0.04812519997358322, 'validation/mean_average_precision': 0.18958315642744727, 'validation/num_examples': 43793, 'test/accuracy': 0.9849578142166138, 'test/loss': 0.05109774321317673, 'test/mean_average_precision': 0.18668369635083729, 'test/num_examples': 43793, 'score': 9376.250790834427, 'total_duration': 13587.735835075378, 'accumulated_submission_time': 9376.250790834427, 'accumulated_eval_time': 4209.517950057983, 'accumulated_logging_time': 1.1778452396392822}
I0206 02:17:31.473313 140266960250624 logging_writer.py:48] [29556] accumulated_eval_time=4209.517950, accumulated_logging_time=1.177845, accumulated_submission_time=9376.250791, global_step=29556, preemption_count=0, score=9376.250791, test/accuracy=0.984958, test/loss=0.051098, test/mean_average_precision=0.186684, test/num_examples=43793, total_duration=13587.735835, train/accuracy=0.989025, train/loss=0.037338, train/mean_average_precision=0.237443, validation/accuracy=0.985859, validation/loss=0.048125, validation/mean_average_precision=0.189583, validation/num_examples=43793
I0206 02:17:45.531389 140266968643328 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.056654803454875946, loss=0.03758813068270683
I0206 02:18:17.325958 140266960250624 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.06815675646066666, loss=0.03714967146515846
I0206 02:18:49.424828 140266968643328 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.06576688587665558, loss=0.04497760161757469
I0206 02:19:20.593184 140266960250624 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.032729726284742355, loss=0.037043530493974686
I0206 02:19:52.076880 140266968643328 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.04448186233639717, loss=0.040303587913513184
I0206 02:20:23.716019 140266960250624 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.03401489928364754, loss=0.03712509945034981
I0206 02:20:54.812695 140266968643328 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.08938068896532059, loss=0.036895446479320526
I0206 02:21:26.321231 140266960250624 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.07278870046138763, loss=0.03627960756421089
I0206 02:21:31.736331 140451058161472 spec.py:321] Evaluating on the training split.
I0206 02:23:07.260290 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 02:23:12.612077 140451058161472 spec.py:349] Evaluating on the test split.
I0206 02:23:15.657696 140451058161472 submission_runner.py:408] Time since start: 13931.94s, 	Step: 30318, 	{'train/accuracy': 0.9889353513717651, 'train/loss': 0.03758455440402031, 'train/mean_average_precision': 0.24313854729822296, 'validation/accuracy': 0.9859227538108826, 'validation/loss': 0.04829683527350426, 'validation/mean_average_precision': 0.1986930130203873, 'validation/num_examples': 43793, 'test/accuracy': 0.9849645495414734, 'test/loss': 0.05136340111494064, 'test/mean_average_precision': 0.1865411181259032, 'test/num_examples': 43793, 'score': 9616.483031272888, 'total_duration': 13931.94085597992, 'accumulated_submission_time': 9616.483031272888, 'accumulated_eval_time': 4313.439270734787, 'accumulated_logging_time': 1.2094299793243408}
I0206 02:23:15.679404 140248415348480 logging_writer.py:48] [30318] accumulated_eval_time=4313.439271, accumulated_logging_time=1.209430, accumulated_submission_time=9616.483031, global_step=30318, preemption_count=0, score=9616.483031, test/accuracy=0.984965, test/loss=0.051363, test/mean_average_precision=0.186541, test/num_examples=43793, total_duration=13931.940856, train/accuracy=0.988935, train/loss=0.037585, train/mean_average_precision=0.243139, validation/accuracy=0.985923, validation/loss=0.048297, validation/mean_average_precision=0.198693, validation/num_examples=43793
I0206 02:23:41.752222 140290196547328 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.06869497150182724, loss=0.037608008831739426
I0206 02:24:13.838636 140248415348480 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.03993495926260948, loss=0.03742873668670654
I0206 02:24:45.497157 140290196547328 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.08646807819604874, loss=0.03687065839767456
I0206 02:25:17.210711 140248415348480 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.055675189942121506, loss=0.0352000817656517
I0206 02:25:48.804387 140290196547328 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.043981071561574936, loss=0.03804929926991463
I0206 02:26:20.565626 140248415348480 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.05736237391829491, loss=0.03811907395720482
I0206 02:26:51.934587 140290196547328 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.08289781212806702, loss=0.038841281086206436
I0206 02:27:15.856585 140451058161472 spec.py:321] Evaluating on the training split.
I0206 02:28:54.317039 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 02:28:57.789408 140451058161472 spec.py:349] Evaluating on the test split.
I0206 02:29:01.220861 140451058161472 submission_runner.py:408] Time since start: 14277.50s, 	Step: 31076, 	{'train/accuracy': 0.9885992407798767, 'train/loss': 0.03864162787795067, 'train/mean_average_precision': 0.22234282131653765, 'validation/accuracy': 0.9853118062019348, 'validation/loss': 0.0490008220076561, 'validation/mean_average_precision': 0.1907836688823532, 'validation/num_examples': 43793, 'test/accuracy': 0.9844574332237244, 'test/loss': 0.05172889307141304, 'test/mean_average_precision': 0.18330393134426415, 'test/num_examples': 43793, 'score': 9856.629340171814, 'total_duration': 14277.503986597061, 'accumulated_submission_time': 9856.629340171814, 'accumulated_eval_time': 4418.8034727573395, 'accumulated_logging_time': 1.241889476776123}
I0206 02:29:01.247117 140229879785216 logging_writer.py:48] [31076] accumulated_eval_time=4418.803473, accumulated_logging_time=1.241889, accumulated_submission_time=9856.629340, global_step=31076, preemption_count=0, score=9856.629340, test/accuracy=0.984457, test/loss=0.051729, test/mean_average_precision=0.183304, test/num_examples=43793, total_duration=14277.503987, train/accuracy=0.988599, train/loss=0.038642, train/mean_average_precision=0.222343, validation/accuracy=0.985312, validation/loss=0.049001, validation/mean_average_precision=0.190784, validation/num_examples=43793
I0206 02:29:09.409063 140266960250624 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.04506333917379379, loss=0.03643052652478218
I0206 02:29:40.454028 140229879785216 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.06067539006471634, loss=0.04035418853163719
I0206 02:30:11.821105 140266960250624 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.09172569215297699, loss=0.0389661006629467
I0206 02:30:42.994107 140229879785216 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.06056535243988037, loss=0.03880621865391731
I0206 02:31:14.635692 140266960250624 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.08470720797777176, loss=0.03331950679421425
I0206 02:31:46.238819 140229879785216 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.057984139770269394, loss=0.042866285890340805
I0206 02:32:17.665223 140266960250624 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.04731355607509613, loss=0.03279104456305504
I0206 02:32:49.185236 140229879785216 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.05439820513129234, loss=0.04130828380584717
I0206 02:33:01.258506 140451058161472 spec.py:321] Evaluating on the training split.
I0206 02:34:42.787758 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 02:34:46.190891 140451058161472 spec.py:349] Evaluating on the test split.
I0206 02:34:49.621518 140451058161472 submission_runner.py:408] Time since start: 14625.90s, 	Step: 31840, 	{'train/accuracy': 0.9889352321624756, 'train/loss': 0.03788282349705696, 'train/mean_average_precision': 0.23030380929346733, 'validation/accuracy': 0.9858736395835876, 'validation/loss': 0.04784974083304405, 'validation/mean_average_precision': 0.19357178850933154, 'validation/num_examples': 43793, 'test/accuracy': 0.9849127531051636, 'test/loss': 0.05064694955945015, 'test/mean_average_precision': 0.1879387186659543, 'test/num_examples': 43793, 'score': 10096.608315229416, 'total_duration': 14625.904658794403, 'accumulated_submission_time': 10096.608315229416, 'accumulated_eval_time': 4527.166420221329, 'accumulated_logging_time': 1.280625581741333}
I0206 02:34:49.645450 140248415348480 logging_writer.py:48] [31840] accumulated_eval_time=4527.166420, accumulated_logging_time=1.280626, accumulated_submission_time=10096.608315, global_step=31840, preemption_count=0, score=10096.608315, test/accuracy=0.984913, test/loss=0.050647, test/mean_average_precision=0.187939, test/num_examples=43793, total_duration=14625.904659, train/accuracy=0.988935, train/loss=0.037883, train/mean_average_precision=0.230304, validation/accuracy=0.985874, validation/loss=0.047850, validation/mean_average_precision=0.193572, validation/num_examples=43793
I0206 02:35:09.574673 140290196547328 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.04506253823637962, loss=0.04190918058156967
I0206 02:35:41.027763 140248415348480 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.06800870597362518, loss=0.03503721207380295
I0206 02:36:12.600698 140290196547328 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.05097367614507675, loss=0.03498459607362747
I0206 02:36:44.360696 140248415348480 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.0534144788980484, loss=0.03770159184932709
I0206 02:37:16.211593 140290196547328 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.04412204772233963, loss=0.04278996214270592
I0206 02:37:47.510350 140248415348480 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.108492910861969, loss=0.04444972053170204
I0206 02:38:19.929634 140290196547328 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.04148758202791214, loss=0.03491358086466789
I0206 02:38:49.748675 140451058161472 spec.py:321] Evaluating on the training split.
I0206 02:40:30.882109 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 02:40:33.934232 140451058161472 spec.py:349] Evaluating on the test split.
I0206 02:40:36.918828 140451058161472 submission_runner.py:408] Time since start: 14973.20s, 	Step: 32593, 	{'train/accuracy': 0.9887871742248535, 'train/loss': 0.038394100964069366, 'train/mean_average_precision': 0.22740842182914375, 'validation/accuracy': 0.9858419895172119, 'validation/loss': 0.04838836193084717, 'validation/mean_average_precision': 0.1839510717008973, 'validation/num_examples': 43793, 'test/accuracy': 0.9849587082862854, 'test/loss': 0.051338449120521545, 'test/mean_average_precision': 0.18290179246589217, 'test/num_examples': 43793, 'score': 10336.679065942764, 'total_duration': 14973.201986551285, 'accumulated_submission_time': 10336.679065942764, 'accumulated_eval_time': 4634.336527824402, 'accumulated_logging_time': 1.3167879581451416}
I0206 02:40:36.940191 140266960250624 logging_writer.py:48] [32593] accumulated_eval_time=4634.336528, accumulated_logging_time=1.316788, accumulated_submission_time=10336.679066, global_step=32593, preemption_count=0, score=10336.679066, test/accuracy=0.984959, test/loss=0.051338, test/mean_average_precision=0.182902, test/num_examples=43793, total_duration=14973.201987, train/accuracy=0.988787, train/loss=0.038394, train/mean_average_precision=0.227408, validation/accuracy=0.985842, validation/loss=0.048388, validation/mean_average_precision=0.183951, validation/num_examples=43793
I0206 02:40:39.611581 140266968643328 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.04660214111208916, loss=0.04022645205259323
I0206 02:41:11.445095 140266960250624 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.04827028885483742, loss=0.039724744856357574
I0206 02:41:43.478100 140266968643328 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.05613549053668976, loss=0.03994753956794739
I0206 02:42:15.582864 140266960250624 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.06718427687883377, loss=0.039787594228982925
I0206 02:42:47.360564 140266968643328 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.06689324975013733, loss=0.035461556166410446
I0206 02:43:19.167864 140266960250624 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.04101071134209633, loss=0.036549683660268784
I0206 02:43:50.986165 140266968643328 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.04201582819223404, loss=0.03940354287624359
I0206 02:44:22.652781 140266960250624 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.09432399272918701, loss=0.04099142178893089
I0206 02:44:37.038997 140451058161472 spec.py:321] Evaluating on the training split.
I0206 02:46:14.246250 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 02:46:17.239308 140451058161472 spec.py:349] Evaluating on the test split.
I0206 02:46:20.205585 140451058161472 submission_runner.py:408] Time since start: 15316.49s, 	Step: 33347, 	{'train/accuracy': 0.9890311360359192, 'train/loss': 0.03757062554359436, 'train/mean_average_precision': 0.23133906465316256, 'validation/accuracy': 0.9859755039215088, 'validation/loss': 0.048174526542425156, 'validation/mean_average_precision': 0.19412180114186225, 'validation/num_examples': 43793, 'test/accuracy': 0.9849936366081238, 'test/loss': 0.051304273307323456, 'test/mean_average_precision': 0.18574911694512755, 'test/num_examples': 43793, 'score': 10576.746666193008, 'total_duration': 15316.488739967346, 'accumulated_submission_time': 10576.746666193008, 'accumulated_eval_time': 4737.503067016602, 'accumulated_logging_time': 1.3489949703216553}
I0206 02:46:20.227696 140229879785216 logging_writer.py:48] [33347] accumulated_eval_time=4737.503067, accumulated_logging_time=1.348995, accumulated_submission_time=10576.746666, global_step=33347, preemption_count=0, score=10576.746666, test/accuracy=0.984994, test/loss=0.051304, test/mean_average_precision=0.185749, test/num_examples=43793, total_duration=15316.488740, train/accuracy=0.989031, train/loss=0.037571, train/mean_average_precision=0.231339, validation/accuracy=0.985976, validation/loss=0.048175, validation/mean_average_precision=0.194122, validation/num_examples=43793
I0206 02:46:37.786433 140290196547328 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.03794155269861221, loss=0.03707848861813545
I0206 02:47:10.089515 140229879785216 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.06210032105445862, loss=0.03226204216480255
I0206 02:47:41.585862 140290196547328 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.03290575370192528, loss=0.0376201868057251
I0206 02:48:13.077005 140229879785216 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.04281923919916153, loss=0.039037756621837616
I0206 02:48:44.756114 140290196547328 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.09685900062322617, loss=0.04005539417266846
I0206 02:49:16.892661 140229879785216 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.03506892919540405, loss=0.037065539509058
I0206 02:49:48.683084 140290196547328 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.09381832927465439, loss=0.042306702584028244
I0206 02:50:20.196138 140229879785216 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.08055379986763, loss=0.038066502660512924
I0206 02:50:20.528253 140451058161472 spec.py:321] Evaluating on the training split.
I0206 02:52:01.485943 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 02:52:04.691690 140451058161472 spec.py:349] Evaluating on the test split.
I0206 02:52:07.670424 140451058161472 submission_runner.py:408] Time since start: 15663.95s, 	Step: 34102, 	{'train/accuracy': 0.9890766143798828, 'train/loss': 0.0374147966504097, 'train/mean_average_precision': 0.22170499395294838, 'validation/accuracy': 0.9859133958816528, 'validation/loss': 0.047539807856082916, 'validation/mean_average_precision': 0.19530376395857477, 'validation/num_examples': 43793, 'test/accuracy': 0.9850218892097473, 'test/loss': 0.05039689689874649, 'test/mean_average_precision': 0.19320288726833304, 'test/num_examples': 43793, 'score': 10817.016341924667, 'total_duration': 15663.953579187393, 'accumulated_submission_time': 10817.016341924667, 'accumulated_eval_time': 4844.645184755325, 'accumulated_logging_time': 1.3820130825042725}
I0206 02:52:07.691843 140248415348480 logging_writer.py:48] [34102] accumulated_eval_time=4844.645185, accumulated_logging_time=1.382013, accumulated_submission_time=10817.016342, global_step=34102, preemption_count=0, score=10817.016342, test/accuracy=0.985022, test/loss=0.050397, test/mean_average_precision=0.193203, test/num_examples=43793, total_duration=15663.953579, train/accuracy=0.989077, train/loss=0.037415, train/mean_average_precision=0.221705, validation/accuracy=0.985913, validation/loss=0.047540, validation/mean_average_precision=0.195304, validation/num_examples=43793
I0206 02:52:38.489418 140266960250624 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.043344881385564804, loss=0.0380803681910038
I0206 02:53:09.808711 140248415348480 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.06491460651159286, loss=0.03487659618258476
I0206 02:53:40.744724 140266960250624 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.059202708303928375, loss=0.03301385045051575
I0206 02:54:12.501799 140248415348480 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.039029814302921295, loss=0.03781276196241379
I0206 02:54:44.831347 140266960250624 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.055777713656425476, loss=0.03690226003527641
I0206 02:55:17.134881 140248415348480 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.0887572392821312, loss=0.03877534717321396
I0206 02:55:49.000559 140266960250624 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.05720941722393036, loss=0.03767681121826172
I0206 02:56:07.815315 140451058161472 spec.py:321] Evaluating on the training split.
I0206 02:57:42.007174 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 02:57:45.117632 140451058161472 spec.py:349] Evaluating on the test split.
I0206 02:57:48.146121 140451058161472 submission_runner.py:408] Time since start: 16004.43s, 	Step: 34861, 	{'train/accuracy': 0.9888927936553955, 'train/loss': 0.037979718297719955, 'train/mean_average_precision': 0.23677103873994235, 'validation/accuracy': 0.9859304428100586, 'validation/loss': 0.048315953463315964, 'validation/mean_average_precision': 0.1910359411764158, 'validation/num_examples': 43793, 'test/accuracy': 0.9849645495414734, 'test/loss': 0.05137333273887634, 'test/mean_average_precision': 0.1845622175386213, 'test/num_examples': 43793, 'score': 11057.10607290268, 'total_duration': 16004.42927980423, 'accumulated_submission_time': 11057.10607290268, 'accumulated_eval_time': 4944.97594666481, 'accumulated_logging_time': 1.415419578552246}
I0206 02:57:48.167918 140266968643328 logging_writer.py:48] [34861] accumulated_eval_time=4944.975947, accumulated_logging_time=1.415420, accumulated_submission_time=11057.106073, global_step=34861, preemption_count=0, score=11057.106073, test/accuracy=0.984965, test/loss=0.051373, test/mean_average_precision=0.184562, test/num_examples=43793, total_duration=16004.429280, train/accuracy=0.988893, train/loss=0.037980, train/mean_average_precision=0.236771, validation/accuracy=0.985930, validation/loss=0.048316, validation/mean_average_precision=0.191036, validation/num_examples=43793
I0206 02:58:00.862600 140290196547328 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.07237197458744049, loss=0.03701728209853172
I0206 02:58:32.712042 140266968643328 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.042412105947732925, loss=0.036177024245262146
I0206 02:59:04.553990 140290196547328 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.06476518511772156, loss=0.040662799030542374
I0206 02:59:36.169617 140266968643328 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.057943861931562424, loss=0.03343333303928375
I0206 03:00:08.086977 140290196547328 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.10692819952964783, loss=0.040434639900922775
I0206 03:00:39.289641 140266968643328 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.05113983899354935, loss=0.036958225071430206
I0206 03:01:11.060690 140290196547328 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.04135545715689659, loss=0.04125290364027023
I0206 03:01:42.714465 140266968643328 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.0788094773888588, loss=0.03657558187842369
I0206 03:01:48.379162 140451058161472 spec.py:321] Evaluating on the training split.
I0206 03:03:25.480192 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 03:03:28.587062 140451058161472 spec.py:349] Evaluating on the test split.
I0206 03:03:31.633340 140451058161472 submission_runner.py:408] Time since start: 16347.92s, 	Step: 35619, 	{'train/accuracy': 0.9888798594474792, 'train/loss': 0.03776901960372925, 'train/mean_average_precision': 0.24197418865759052, 'validation/accuracy': 0.9859288334846497, 'validation/loss': 0.047665756195783615, 'validation/mean_average_precision': 0.19524556600481408, 'validation/num_examples': 43793, 'test/accuracy': 0.9850631356239319, 'test/loss': 0.05058734491467476, 'test/mean_average_precision': 0.1908041635108264, 'test/num_examples': 43793, 'score': 11297.285032272339, 'total_duration': 16347.916496515274, 'accumulated_submission_time': 11297.285032272339, 'accumulated_eval_time': 5048.230092048645, 'accumulated_logging_time': 1.4496972560882568}
I0206 03:03:31.655213 140229879785216 logging_writer.py:48] [35619] accumulated_eval_time=5048.230092, accumulated_logging_time=1.449697, accumulated_submission_time=11297.285032, global_step=35619, preemption_count=0, score=11297.285032, test/accuracy=0.985063, test/loss=0.050587, test/mean_average_precision=0.190804, test/num_examples=43793, total_duration=16347.916497, train/accuracy=0.988880, train/loss=0.037769, train/mean_average_precision=0.241974, validation/accuracy=0.985929, validation/loss=0.047666, validation/mean_average_precision=0.195246, validation/num_examples=43793
I0206 03:03:57.603713 140248415348480 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.05244836583733559, loss=0.0380450040102005
I0206 03:04:29.539386 140229879785216 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.04575062170624733, loss=0.037109971046447754
I0206 03:05:01.400950 140248415348480 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.042063310742378235, loss=0.03948049992322922
I0206 03:05:33.235017 140229879785216 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.06885891407728195, loss=0.0371222123503685
I0206 03:06:04.461590 140248415348480 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.057111237198114395, loss=0.03803666681051254
I0206 03:06:38.165421 140229879785216 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.08769913017749786, loss=0.0394071564078331
I0206 03:07:11.623942 140248415348480 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.0766843855381012, loss=0.04037937521934509
I0206 03:07:31.655515 140451058161472 spec.py:321] Evaluating on the training split.
I0206 03:09:08.374575 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 03:09:11.498337 140451058161472 spec.py:349] Evaluating on the test split.
I0206 03:09:14.523317 140451058161472 submission_runner.py:408] Time since start: 16690.81s, 	Step: 36364, 	{'train/accuracy': 0.9889218807220459, 'train/loss': 0.037400249391794205, 'train/mean_average_precision': 0.23931580360512697, 'validation/accuracy': 0.9859093427658081, 'validation/loss': 0.04826940596103668, 'validation/mean_average_precision': 0.1960000096477773, 'validation/num_examples': 43793, 'test/accuracy': 0.9849388599395752, 'test/loss': 0.05115370452404022, 'test/mean_average_precision': 0.18467875618708884, 'test/num_examples': 43793, 'score': 11537.255279302597, 'total_duration': 16690.80647611618, 'accumulated_submission_time': 11537.255279302597, 'accumulated_eval_time': 5151.097851753235, 'accumulated_logging_time': 1.4821081161499023}
I0206 03:09:14.545189 140266968643328 logging_writer.py:48] [36364] accumulated_eval_time=5151.097852, accumulated_logging_time=1.482108, accumulated_submission_time=11537.255279, global_step=36364, preemption_count=0, score=11537.255279, test/accuracy=0.984939, test/loss=0.051154, test/mean_average_precision=0.184679, test/num_examples=43793, total_duration=16690.806476, train/accuracy=0.988922, train/loss=0.037400, train/mean_average_precision=0.239316, validation/accuracy=0.985909, validation/loss=0.048269, validation/mean_average_precision=0.196000, validation/num_examples=43793
I0206 03:09:26.447350 140290196547328 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.030490901321172714, loss=0.0353609137237072
I0206 03:09:58.614517 140266968643328 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.08649731427431107, loss=0.04237939044833183
I0206 03:10:30.369550 140290196547328 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.11227863281965256, loss=0.04034743458032608
I0206 03:11:01.792461 140266968643328 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.05727972090244293, loss=0.037393201142549515
I0206 03:11:33.233021 140290196547328 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.0641712099313736, loss=0.03678711876273155
I0206 03:12:04.770350 140266968643328 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.07025985419750214, loss=0.03899167850613594
I0206 03:12:36.287064 140290196547328 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.08170904964208603, loss=0.03928403928875923
I0206 03:13:08.111622 140266968643328 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.03899090737104416, loss=0.03269962593913078
I0206 03:13:14.624521 140451058161472 spec.py:321] Evaluating on the training split.
I0206 03:14:50.341212 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 03:14:53.449942 140451058161472 spec.py:349] Evaluating on the test split.
I0206 03:14:56.501403 140451058161472 submission_runner.py:408] Time since start: 17032.78s, 	Step: 37122, 	{'train/accuracy': 0.9891963601112366, 'train/loss': 0.03668893128633499, 'train/mean_average_precision': 0.2415403166293018, 'validation/accuracy': 0.9859312772750854, 'validation/loss': 0.04741903394460678, 'validation/mean_average_precision': 0.19278137679949098, 'validation/num_examples': 43793, 'test/accuracy': 0.9850353598594666, 'test/loss': 0.05028887465596199, 'test/mean_average_precision': 0.185580799291037, 'test/num_examples': 43793, 'score': 11777.304328680038, 'total_duration': 17032.784563302994, 'accumulated_submission_time': 11777.304328680038, 'accumulated_eval_time': 5252.974688053131, 'accumulated_logging_time': 1.514685869216919}
I0206 03:14:56.523490 140229879785216 logging_writer.py:48] [37122] accumulated_eval_time=5252.974688, accumulated_logging_time=1.514686, accumulated_submission_time=11777.304329, global_step=37122, preemption_count=0, score=11777.304329, test/accuracy=0.985035, test/loss=0.050289, test/mean_average_precision=0.185581, test/num_examples=43793, total_duration=17032.784563, train/accuracy=0.989196, train/loss=0.036689, train/mean_average_precision=0.241540, validation/accuracy=0.985931, validation/loss=0.047419, validation/mean_average_precision=0.192781, validation/num_examples=43793
I0206 03:15:21.601605 140248415348480 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.06166194751858711, loss=0.03548244386911392
I0206 03:15:52.918365 140229879785216 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.1417917013168335, loss=0.04404179006814957
I0206 03:16:24.406583 140248415348480 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.048240628093481064, loss=0.03917200118303299
I0206 03:16:55.736640 140229879785216 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.040433503687381744, loss=0.036220699548721313
I0206 03:17:27.419161 140248415348480 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.051411133259534836, loss=0.038147859275341034
I0206 03:17:58.893881 140229879785216 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.07771938294172287, loss=0.03567986562848091
I0206 03:18:30.461415 140248415348480 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.07053700089454651, loss=0.04145658388733864
I0206 03:18:56.515828 140451058161472 spec.py:321] Evaluating on the training split.
I0206 03:20:34.330433 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 03:20:37.431502 140451058161472 spec.py:349] Evaluating on the test split.
I0206 03:20:40.473047 140451058161472 submission_runner.py:408] Time since start: 17376.76s, 	Step: 37884, 	{'train/accuracy': 0.9892786145210266, 'train/loss': 0.036347683519124985, 'train/mean_average_precision': 0.2639222874446258, 'validation/accuracy': 0.9859499335289001, 'validation/loss': 0.04764634370803833, 'validation/mean_average_precision': 0.1962621683676298, 'validation/num_examples': 43793, 'test/accuracy': 0.9851309657096863, 'test/loss': 0.05036367475986481, 'test/mean_average_precision': 0.19165741451932264, 'test/num_examples': 43793, 'score': 12017.266623020172, 'total_duration': 17376.75620341301, 'accumulated_submission_time': 12017.266623020172, 'accumulated_eval_time': 5356.93186378479, 'accumulated_logging_time': 1.5475928783416748}
I0206 03:20:40.496410 140266960250624 logging_writer.py:48] [37884] accumulated_eval_time=5356.931864, accumulated_logging_time=1.547593, accumulated_submission_time=12017.266623, global_step=37884, preemption_count=0, score=12017.266623, test/accuracy=0.985131, test/loss=0.050364, test/mean_average_precision=0.191657, test/num_examples=43793, total_duration=17376.756203, train/accuracy=0.989279, train/loss=0.036348, train/mean_average_precision=0.263922, validation/accuracy=0.985950, validation/loss=0.047646, validation/mean_average_precision=0.196262, validation/num_examples=43793
I0206 03:20:45.964312 140290196547328 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.03503597900271416, loss=0.035309869796037674
I0206 03:21:17.967640 140266960250624 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.03901831805706024, loss=0.039519693702459335
I0206 03:21:49.632283 140290196547328 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.05563948675990105, loss=0.03915739804506302
I0206 03:22:21.053994 140266960250624 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.04054676741361618, loss=0.038385383784770966
I0206 03:22:52.806740 140290196547328 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.10332927852869034, loss=0.03517431393265724
I0206 03:23:24.868082 140266960250624 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.1124446839094162, loss=0.038283996284008026
I0206 03:23:56.946841 140290196547328 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.03260798752307892, loss=0.03241725638508797
I0206 03:24:28.589707 140266960250624 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.054764363914728165, loss=0.04051831364631653
I0206 03:24:40.570031 140451058161472 spec.py:321] Evaluating on the training split.
I0206 03:26:18.056238 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 03:26:21.085432 140451058161472 spec.py:349] Evaluating on the test split.
I0206 03:26:24.115179 140451058161472 submission_runner.py:408] Time since start: 17720.40s, 	Step: 38639, 	{'train/accuracy': 0.9892585873603821, 'train/loss': 0.0369451642036438, 'train/mean_average_precision': 0.24027067192509202, 'validation/accuracy': 0.9859738945960999, 'validation/loss': 0.04767146706581116, 'validation/mean_average_precision': 0.1958262650743369, 'validation/num_examples': 43793, 'test/accuracy': 0.9850825071334839, 'test/loss': 0.05038914829492569, 'test/mean_average_precision': 0.19137357498871682, 'test/num_examples': 43793, 'score': 12257.309017419815, 'total_duration': 17720.39829516411, 'accumulated_submission_time': 12257.309017419815, 'accumulated_eval_time': 5460.476921081543, 'accumulated_logging_time': 1.582322359085083}
I0206 03:26:24.139010 140248415348480 logging_writer.py:48] [38639] accumulated_eval_time=5460.476921, accumulated_logging_time=1.582322, accumulated_submission_time=12257.309017, global_step=38639, preemption_count=0, score=12257.309017, test/accuracy=0.985083, test/loss=0.050389, test/mean_average_precision=0.191374, test/num_examples=43793, total_duration=17720.398295, train/accuracy=0.989259, train/loss=0.036945, train/mean_average_precision=0.240271, validation/accuracy=0.985974, validation/loss=0.047671, validation/mean_average_precision=0.195826, validation/num_examples=43793
I0206 03:26:43.683469 140266968643328 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.07323122024536133, loss=0.033264074474573135
I0206 03:27:15.238321 140248415348480 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.07068543136119843, loss=0.03591819852590561
I0206 03:27:46.818365 140266968643328 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.043360017240047455, loss=0.03514760732650757
I0206 03:28:18.609657 140248415348480 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.038617685437202454, loss=0.03382286801934242
I0206 03:28:50.106889 140266968643328 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.0626247376203537, loss=0.03438812866806984
I0206 03:29:21.644900 140248415348480 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.04168909415602684, loss=0.03673830255866051
I0206 03:29:52.940285 140266968643328 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.04958250746130943, loss=0.0410035103559494
I0206 03:30:24.257445 140451058161472 spec.py:321] Evaluating on the training split.
I0206 03:32:01.627941 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 03:32:05.107399 140451058161472 spec.py:349] Evaluating on the test split.
I0206 03:32:08.435715 140451058161472 submission_runner.py:408] Time since start: 18064.72s, 	Step: 39400, 	{'train/accuracy': 0.9892501831054688, 'train/loss': 0.036724213510751724, 'train/mean_average_precision': 0.2511990354285915, 'validation/accuracy': 0.9859767556190491, 'validation/loss': 0.04746464267373085, 'validation/mean_average_precision': 0.2000518758374433, 'validation/num_examples': 43793, 'test/accuracy': 0.9850707054138184, 'test/loss': 0.05035984143614769, 'test/mean_average_precision': 0.19696708578381983, 'test/num_examples': 43793, 'score': 12497.395084142685, 'total_duration': 18064.71885228157, 'accumulated_submission_time': 12497.395084142685, 'accumulated_eval_time': 5564.655128240585, 'accumulated_logging_time': 1.6185622215270996}
I0206 03:32:08.460921 140229879785216 logging_writer.py:48] [39400] accumulated_eval_time=5564.655128, accumulated_logging_time=1.618562, accumulated_submission_time=12497.395084, global_step=39400, preemption_count=0, score=12497.395084, test/accuracy=0.985071, test/loss=0.050360, test/mean_average_precision=0.196967, test/num_examples=43793, total_duration=18064.718852, train/accuracy=0.989250, train/loss=0.036724, train/mean_average_precision=0.251199, validation/accuracy=0.985977, validation/loss=0.047465, validation/mean_average_precision=0.200052, validation/num_examples=43793
I0206 03:32:08.819357 140290196547328 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.06580944359302521, loss=0.0365673191845417
I0206 03:32:42.059824 140229879785216 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.0908290296792984, loss=0.0350007601082325
I0206 03:33:14.865433 140290196547328 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.04301832243800163, loss=0.03722946718335152
I0206 03:33:47.443935 140229879785216 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.04996803402900696, loss=0.03559006005525589
I0206 03:34:19.718915 140290196547328 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.04178965836763382, loss=0.03813228756189346
I0206 03:34:52.289650 140229879785216 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.09027063101530075, loss=0.03654371201992035
I0206 03:35:23.901298 140290196547328 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.1447211503982544, loss=0.03809347376227379
I0206 03:35:55.294386 140229879785216 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.0813828855752945, loss=0.03664864972233772
I0206 03:36:08.622392 140451058161472 spec.py:321] Evaluating on the training split.
I0206 03:37:45.862211 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 03:37:48.893424 140451058161472 spec.py:349] Evaluating on the test split.
I0206 03:37:51.843919 140451058161472 submission_runner.py:408] Time since start: 18408.13s, 	Step: 40142, 	{'train/accuracy': 0.9890367984771729, 'train/loss': 0.037310272455215454, 'train/mean_average_precision': 0.23788077565488858, 'validation/accuracy': 0.9860019087791443, 'validation/loss': 0.047450996935367584, 'validation/mean_average_precision': 0.19823682262756082, 'validation/num_examples': 43793, 'test/accuracy': 0.9850610494613647, 'test/loss': 0.05041808262467384, 'test/mean_average_precision': 0.19414910349240583, 'test/num_examples': 43793, 'score': 12737.522000074387, 'total_duration': 18408.12707543373, 'accumulated_submission_time': 12737.522000074387, 'accumulated_eval_time': 5667.876608371735, 'accumulated_logging_time': 1.6552250385284424}
I0206 03:37:51.866850 140266960250624 logging_writer.py:48] [40142] accumulated_eval_time=5667.876608, accumulated_logging_time=1.655225, accumulated_submission_time=12737.522000, global_step=40142, preemption_count=0, score=12737.522000, test/accuracy=0.985061, test/loss=0.050418, test/mean_average_precision=0.194149, test/num_examples=43793, total_duration=18408.127075, train/accuracy=0.989037, train/loss=0.037310, train/mean_average_precision=0.237881, validation/accuracy=0.986002, validation/loss=0.047451, validation/mean_average_precision=0.198237, validation/num_examples=43793
I0206 03:38:10.521828 140266968643328 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.1685318648815155, loss=0.04221804440021515
I0206 03:38:41.374466 140266960250624 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.038997288793325424, loss=0.03473212569952011
I0206 03:39:12.949647 140266968643328 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.07286358624696732, loss=0.03490292653441429
I0206 03:39:44.313343 140266960250624 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.06347589194774628, loss=0.03949674963951111
I0206 03:40:15.980637 140266968643328 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.04553629085421562, loss=0.040362242609262466
I0206 03:40:47.281561 140266960250624 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.048680391162633896, loss=0.03567756339907646
I0206 03:41:19.233695 140266968643328 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.05066719278693199, loss=0.03703946992754936
I0206 03:41:51.786692 140266960250624 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.11166436225175858, loss=0.0389084666967392
I0206 03:41:52.120974 140451058161472 spec.py:321] Evaluating on the training split.
I0206 03:43:33.130493 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 03:43:36.158326 140451058161472 spec.py:349] Evaluating on the test split.
I0206 03:43:39.166575 140451058161472 submission_runner.py:408] Time since start: 18755.45s, 	Step: 40902, 	{'train/accuracy': 0.9892304539680481, 'train/loss': 0.036914996802806854, 'train/mean_average_precision': 0.24487599262648516, 'validation/accuracy': 0.9860546588897705, 'validation/loss': 0.047015801072120667, 'validation/mean_average_precision': 0.19984733758230472, 'validation/num_examples': 43793, 'test/accuracy': 0.985190749168396, 'test/loss': 0.049587149173021317, 'test/mean_average_precision': 0.1979586610062238, 'test/num_examples': 43793, 'score': 12977.745255231857, 'total_duration': 18755.449722766876, 'accumulated_submission_time': 12977.745255231857, 'accumulated_eval_time': 5774.92215013504, 'accumulated_logging_time': 1.6892304420471191}
I0206 03:43:39.189841 140229879785216 logging_writer.py:48] [40902] accumulated_eval_time=5774.922150, accumulated_logging_time=1.689230, accumulated_submission_time=12977.745255, global_step=40902, preemption_count=0, score=12977.745255, test/accuracy=0.985191, test/loss=0.049587, test/mean_average_precision=0.197959, test/num_examples=43793, total_duration=18755.449723, train/accuracy=0.989230, train/loss=0.036915, train/mean_average_precision=0.244876, validation/accuracy=0.986055, validation/loss=0.047016, validation/mean_average_precision=0.199847, validation/num_examples=43793
I0206 03:44:11.070370 140290196547328 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.05328725278377533, loss=0.03441299870610237
I0206 03:44:42.646453 140229879785216 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.07657140493392944, loss=0.03374643996357918
I0206 03:45:14.098896 140290196547328 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.10766652971506119, loss=0.03349796682596207
I0206 03:45:45.352195 140229879785216 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.05887138471007347, loss=0.03912043198943138
I0206 03:46:16.850279 140290196547328 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.07695555686950684, loss=0.04062382131814957
I0206 03:46:48.419516 140229879785216 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.05418146401643753, loss=0.03798219934105873
I0206 03:47:19.673592 140290196547328 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.0973893404006958, loss=0.034448523074388504
I0206 03:47:39.380152 140451058161472 spec.py:321] Evaluating on the training split.
I0206 03:49:14.311647 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 03:49:17.362268 140451058161472 spec.py:349] Evaluating on the test split.
I0206 03:49:20.307088 140451058161472 submission_runner.py:408] Time since start: 19096.59s, 	Step: 41664, 	{'train/accuracy': 0.9892215132713318, 'train/loss': 0.03669707104563713, 'train/mean_average_precision': 0.25355743928034935, 'validation/accuracy': 0.9860948920249939, 'validation/loss': 0.04706144705414772, 'validation/mean_average_precision': 0.19683720692700446, 'validation/num_examples': 43793, 'test/accuracy': 0.9852290749549866, 'test/loss': 0.049860768020153046, 'test/mean_average_precision': 0.1985576710004122, 'test/num_examples': 43793, 'score': 13217.90337395668, 'total_duration': 19096.59023118019, 'accumulated_submission_time': 13217.90337395668, 'accumulated_eval_time': 5875.849026441574, 'accumulated_logging_time': 1.7248921394348145}
I0206 03:49:20.331822 140248415348480 logging_writer.py:48] [41664] accumulated_eval_time=5875.849026, accumulated_logging_time=1.724892, accumulated_submission_time=13217.903374, global_step=41664, preemption_count=0, score=13217.903374, test/accuracy=0.985229, test/loss=0.049861, test/mean_average_precision=0.198558, test/num_examples=43793, total_duration=19096.590231, train/accuracy=0.989222, train/loss=0.036697, train/mean_average_precision=0.253557, validation/accuracy=0.986095, validation/loss=0.047061, validation/mean_average_precision=0.196837, validation/num_examples=43793
I0206 03:49:32.096697 140266968643328 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.03807230293750763, loss=0.03515252843499184
I0206 03:50:03.835155 140248415348480 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.08000560104846954, loss=0.033703695982694626
I0206 03:50:35.296258 140266968643328 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.06269693374633789, loss=0.03961586952209473
I0206 03:51:07.232161 140248415348480 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.1309359073638916, loss=0.03538181632757187
I0206 03:51:39.024984 140266968643328 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.08591978251934052, loss=0.03841222822666168
I0206 03:52:10.589371 140248415348480 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.04573601111769676, loss=0.03600306063890457
I0206 03:52:42.096437 140266968643328 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.04111006483435631, loss=0.03842771053314209
I0206 03:53:13.832557 140248415348480 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.03595662862062454, loss=0.03734520450234413
I0206 03:53:20.373851 140451058161472 spec.py:321] Evaluating on the training split.
I0206 03:54:57.269844 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 03:55:00.262116 140451058161472 spec.py:349] Evaluating on the test split.
I0206 03:55:03.434947 140451058161472 submission_runner.py:408] Time since start: 19439.72s, 	Step: 42422, 	{'train/accuracy': 0.9893390536308289, 'train/loss': 0.036265041679143906, 'train/mean_average_precision': 0.2505960639193763, 'validation/accuracy': 0.9860355854034424, 'validation/loss': 0.046855002641677856, 'validation/mean_average_precision': 0.21005827711503167, 'validation/num_examples': 43793, 'test/accuracy': 0.9852202534675598, 'test/loss': 0.049669425934553146, 'test/mean_average_precision': 0.20277958623412873, 'test/num_examples': 43793, 'score': 13457.914669513702, 'total_duration': 19439.718099355698, 'accumulated_submission_time': 13457.914669513702, 'accumulated_eval_time': 5978.910071611404, 'accumulated_logging_time': 1.7608411312103271}
I0206 03:55:03.464961 140229879785216 logging_writer.py:48] [42422] accumulated_eval_time=5978.910072, accumulated_logging_time=1.760841, accumulated_submission_time=13457.914670, global_step=42422, preemption_count=0, score=13457.914670, test/accuracy=0.985220, test/loss=0.049669, test/mean_average_precision=0.202780, test/num_examples=43793, total_duration=19439.718099, train/accuracy=0.989339, train/loss=0.036265, train/mean_average_precision=0.250596, validation/accuracy=0.986036, validation/loss=0.046855, validation/mean_average_precision=0.210058, validation/num_examples=43793
I0206 03:55:27.985711 140290196547328 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.05306524038314819, loss=0.03222508355975151
I0206 03:55:59.090471 140229879785216 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.056034334003925323, loss=0.036884427070617676
I0206 03:56:30.453518 140290196547328 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.05540517717599869, loss=0.03705539554357529
I0206 03:57:01.384229 140229879785216 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.0928814709186554, loss=0.03640800714492798
I0206 03:57:32.843896 140290196547328 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.05592145398259163, loss=0.044016674160957336
I0206 03:58:05.736526 140229879785216 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.13709689676761627, loss=0.0401381216943264
I0206 03:58:37.033652 140290196547328 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.045535191893577576, loss=0.03831420838832855
I0206 03:59:03.558751 140451058161472 spec.py:321] Evaluating on the training split.
I0206 04:00:40.271008 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 04:00:43.358286 140451058161472 spec.py:349] Evaluating on the test split.
I0206 04:00:46.399246 140451058161472 submission_runner.py:408] Time since start: 19782.68s, 	Step: 43185, 	{'train/accuracy': 0.9892221689224243, 'train/loss': 0.03665018826723099, 'train/mean_average_precision': 0.2538185712336535, 'validation/accuracy': 0.9859641790390015, 'validation/loss': 0.04728439077734947, 'validation/mean_average_precision': 0.20269953349993863, 'validation/num_examples': 43793, 'test/accuracy': 0.9850420951843262, 'test/loss': 0.05029060319066048, 'test/mean_average_precision': 0.18731922219462746, 'test/num_examples': 43793, 'score': 13697.976322174072, 'total_duration': 19782.68240070343, 'accumulated_submission_time': 13697.976322174072, 'accumulated_eval_time': 6081.750519990921, 'accumulated_logging_time': 1.8031601905822754}
I0206 04:00:46.423706 140248415348480 logging_writer.py:48] [43185] accumulated_eval_time=6081.750520, accumulated_logging_time=1.803160, accumulated_submission_time=13697.976322, global_step=43185, preemption_count=0, score=13697.976322, test/accuracy=0.985042, test/loss=0.050291, test/mean_average_precision=0.187319, test/num_examples=43793, total_duration=19782.682401, train/accuracy=0.989222, train/loss=0.036650, train/mean_average_precision=0.253819, validation/accuracy=0.985964, validation/loss=0.047284, validation/mean_average_precision=0.202700, validation/num_examples=43793
I0206 04:00:51.674933 140266968643328 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.1331566870212555, loss=0.04009036719799042
I0206 04:01:23.561322 140248415348480 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.042239781469106674, loss=0.0367882065474987
I0206 04:01:56.162883 140266968643328 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.06600581109523773, loss=0.03589589148759842
I0206 04:02:29.232805 140248415348480 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.08249431103467941, loss=0.03627786412835121
I0206 04:03:01.985520 140266968643328 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.07070181518793106, loss=0.03785686567425728
I0206 04:03:33.779283 140248415348480 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.15584680438041687, loss=0.0379287488758564
I0206 04:04:05.723735 140266968643328 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.06093604490160942, loss=0.039855021983385086
I0206 04:04:37.796856 140248415348480 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.06854598224163055, loss=0.0391821525990963
I0206 04:04:46.600272 140451058161472 spec.py:321] Evaluating on the training split.
I0206 04:06:20.608711 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 04:06:23.632851 140451058161472 spec.py:349] Evaluating on the test split.
I0206 04:06:26.618674 140451058161472 submission_runner.py:408] Time since start: 20122.90s, 	Step: 43929, 	{'train/accuracy': 0.9891306161880493, 'train/loss': 0.03650067374110222, 'train/mean_average_precision': 0.2575394367368832, 'validation/accuracy': 0.9861127138137817, 'validation/loss': 0.046978097409009933, 'validation/mean_average_precision': 0.2011696650770483, 'validation/num_examples': 43793, 'test/accuracy': 0.9852215051651001, 'test/loss': 0.04968126118183136, 'test/mean_average_precision': 0.19707071287609873, 'test/num_examples': 43793, 'score': 13938.120023965836, 'total_duration': 20122.901830911636, 'accumulated_submission_time': 13938.120023965836, 'accumulated_eval_time': 6181.76887345314, 'accumulated_logging_time': 1.839141607284546}
I0206 04:06:26.642678 140266960250624 logging_writer.py:48] [43929] accumulated_eval_time=6181.768873, accumulated_logging_time=1.839142, accumulated_submission_time=13938.120024, global_step=43929, preemption_count=0, score=13938.120024, test/accuracy=0.985222, test/loss=0.049681, test/mean_average_precision=0.197071, test/num_examples=43793, total_duration=20122.901831, train/accuracy=0.989131, train/loss=0.036501, train/mean_average_precision=0.257539, validation/accuracy=0.986113, validation/loss=0.046978, validation/mean_average_precision=0.201170, validation/num_examples=43793
I0206 04:06:49.662022 140290196547328 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.08208858966827393, loss=0.03929338976740837
I0206 04:07:21.572521 140266960250624 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.12821689248085022, loss=0.03456166014075279
I0206 04:07:52.957684 140290196547328 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.07781360298395157, loss=0.03427088260650635
I0206 04:08:24.482269 140266960250624 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.049885768443346024, loss=0.03395891189575195
I0206 04:08:55.851747 140290196547328 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.08572917431592941, loss=0.04156738892197609
I0206 04:09:28.019540 140266960250624 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.0584954209625721, loss=0.03948989510536194
I0206 04:10:00.209391 140290196547328 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.10255274921655655, loss=0.032909248024225235
I0206 04:10:26.886949 140451058161472 spec.py:321] Evaluating on the training split.
I0206 04:12:09.780851 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 04:12:13.166238 140451058161472 spec.py:349] Evaluating on the test split.
I0206 04:12:16.503345 140451058161472 submission_runner.py:408] Time since start: 20472.79s, 	Step: 44683, 	{'train/accuracy': 0.9893518686294556, 'train/loss': 0.03623374551534653, 'train/mean_average_precision': 0.25651968428844657, 'validation/accuracy': 0.9861443638801575, 'validation/loss': 0.04691162332892418, 'validation/mean_average_precision': 0.20618330719853067, 'validation/num_examples': 43793, 'test/accuracy': 0.9852408766746521, 'test/loss': 0.04987991228699684, 'test/mean_average_precision': 0.1917108982459069, 'test/num_examples': 43793, 'score': 14178.33187842369, 'total_duration': 20472.786482810974, 'accumulated_submission_time': 14178.33187842369, 'accumulated_eval_time': 6291.385230064392, 'accumulated_logging_time': 1.8742420673370361}
I0206 04:12:16.529192 140229879785216 logging_writer.py:48] [44683] accumulated_eval_time=6291.385230, accumulated_logging_time=1.874242, accumulated_submission_time=14178.331878, global_step=44683, preemption_count=0, score=14178.331878, test/accuracy=0.985241, test/loss=0.049880, test/mean_average_precision=0.191711, test/num_examples=43793, total_duration=20472.786483, train/accuracy=0.989352, train/loss=0.036234, train/mean_average_precision=0.256520, validation/accuracy=0.986144, validation/loss=0.046912, validation/mean_average_precision=0.206183, validation/num_examples=43793
I0206 04:12:22.405829 140248415348480 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.056735966354608536, loss=0.03698265179991722
I0206 04:12:54.470442 140229879785216 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.06546196341514587, loss=0.038099877536296844
I0206 04:13:26.099833 140248415348480 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.07054285705089569, loss=0.03719406574964523
I0206 04:13:57.745554 140229879785216 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.056077465415000916, loss=0.039652079343795776
I0206 04:14:29.448926 140248415348480 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.07191596180200577, loss=0.04112973436713219
I0206 04:15:00.659225 140229879785216 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.06499606370925903, loss=0.034206755459308624
I0206 04:15:32.269124 140248415348480 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.08702671527862549, loss=0.0380224734544754
I0206 04:16:03.821880 140229879785216 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.049566175788640976, loss=0.0362267903983593
I0206 04:16:16.509891 140451058161472 spec.py:321] Evaluating on the training split.
I0206 04:17:48.345766 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 04:17:51.371522 140451058161472 spec.py:349] Evaluating on the test split.
I0206 04:17:54.360470 140451058161472 submission_runner.py:408] Time since start: 20810.64s, 	Step: 45441, 	{'train/accuracy': 0.9894936084747314, 'train/loss': 0.03576960042119026, 'train/mean_average_precision': 0.2657211170336563, 'validation/accuracy': 0.9861301779747009, 'validation/loss': 0.04697982221841812, 'validation/mean_average_precision': 0.2082862425106177, 'validation/num_examples': 43793, 'test/accuracy': 0.9853171110153198, 'test/loss': 0.049627721309661865, 'test/mean_average_precision': 0.19841860972032793, 'test/num_examples': 43793, 'score': 14418.279221534729, 'total_duration': 20810.643618822098, 'accumulated_submission_time': 14418.279221534729, 'accumulated_eval_time': 6389.235752105713, 'accumulated_logging_time': 1.9128201007843018}
I0206 04:17:54.384510 140266968643328 logging_writer.py:48] [45441] accumulated_eval_time=6389.235752, accumulated_logging_time=1.912820, accumulated_submission_time=14418.279222, global_step=45441, preemption_count=0, score=14418.279222, test/accuracy=0.985317, test/loss=0.049628, test/mean_average_precision=0.198419, test/num_examples=43793, total_duration=20810.643619, train/accuracy=0.989494, train/loss=0.035770, train/mean_average_precision=0.265721, validation/accuracy=0.986130, validation/loss=0.046980, validation/mean_average_precision=0.208286, validation/num_examples=43793
I0206 04:18:13.306250 140290196547328 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.05565245449542999, loss=0.034747425466775894
I0206 04:18:44.710213 140266968643328 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.05704623460769653, loss=0.033871833235025406
I0206 04:19:15.853463 140290196547328 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.04906807839870453, loss=0.037512753158807755
I0206 04:19:47.003869 140266968643328 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.056087326258420944, loss=0.0357525534927845
I0206 04:20:18.160576 140290196547328 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.08003012835979462, loss=0.03527139872312546
I0206 04:20:49.462621 140266968643328 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.05712110176682472, loss=0.036494240164756775
I0206 04:21:20.636306 140290196547328 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.0423126146197319, loss=0.03782469779253006
I0206 04:21:52.024957 140266968643328 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.07370896637439728, loss=0.02936723083257675
I0206 04:21:54.546837 140451058161472 spec.py:321] Evaluating on the training split.
I0206 04:23:34.665754 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 04:23:37.684935 140451058161472 spec.py:349] Evaluating on the test split.
I0206 04:23:40.636085 140451058161472 submission_runner.py:408] Time since start: 21156.92s, 	Step: 46209, 	{'train/accuracy': 0.9894861578941345, 'train/loss': 0.03554147481918335, 'train/mean_average_precision': 0.26567322678175076, 'validation/accuracy': 0.9860729575157166, 'validation/loss': 0.04676460102200508, 'validation/mean_average_precision': 0.20342372193775465, 'validation/num_examples': 43793, 'test/accuracy': 0.985183596611023, 'test/loss': 0.049697089940309525, 'test/mean_average_precision': 0.19757847741367224, 'test/num_examples': 43793, 'score': 14658.41047000885, 'total_duration': 21156.919238328934, 'accumulated_submission_time': 14658.41047000885, 'accumulated_eval_time': 6495.324959516525, 'accumulated_logging_time': 1.9479403495788574}
I0206 04:23:40.660062 140248415348480 logging_writer.py:48] [46209] accumulated_eval_time=6495.324960, accumulated_logging_time=1.947940, accumulated_submission_time=14658.410470, global_step=46209, preemption_count=0, score=14658.410470, test/accuracy=0.985184, test/loss=0.049697, test/mean_average_precision=0.197578, test/num_examples=43793, total_duration=21156.919238, train/accuracy=0.989486, train/loss=0.035541, train/mean_average_precision=0.265673, validation/accuracy=0.986073, validation/loss=0.046765, validation/mean_average_precision=0.203424, validation/num_examples=43793
I0206 04:24:09.615945 140266960250624 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.1213378757238388, loss=0.038655221462249756
I0206 04:24:40.809697 140248415348480 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.04775245487689972, loss=0.035933807492256165
I0206 04:25:12.020448 140266960250624 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.11199849843978882, loss=0.036692626774311066
I0206 04:25:43.173777 140248415348480 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.05856296047568321, loss=0.036693789064884186
I0206 04:26:14.562915 140266960250624 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.08868686109781265, loss=0.03812440484762192
I0206 04:26:45.680034 140248415348480 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.06726062297821045, loss=0.033804431557655334
I0206 04:27:17.193449 140266960250624 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.06136658787727356, loss=0.03580955043435097
I0206 04:27:40.825314 140451058161472 spec.py:321] Evaluating on the training split.
I0206 04:29:18.747898 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 04:29:21.737619 140451058161472 spec.py:349] Evaluating on the test split.
I0206 04:29:24.732482 140451058161472 submission_runner.py:408] Time since start: 21501.02s, 	Step: 46976, 	{'train/accuracy': 0.9894858598709106, 'train/loss': 0.03554527461528778, 'train/mean_average_precision': 0.27140540957010273, 'validation/accuracy': 0.9861480593681335, 'validation/loss': 0.04689713567495346, 'validation/mean_average_precision': 0.20850976515983013, 'validation/num_examples': 43793, 'test/accuracy': 0.9852185845375061, 'test/loss': 0.049553755670785904, 'test/mean_average_precision': 0.19979763567894623, 'test/num_examples': 43793, 'score': 14898.544739961624, 'total_duration': 21501.015640735626, 'accumulated_submission_time': 14898.544739961624, 'accumulated_eval_time': 6599.232083559036, 'accumulated_logging_time': 1.9829089641571045}
I0206 04:29:24.756691 140229879785216 logging_writer.py:48] [46976] accumulated_eval_time=6599.232084, accumulated_logging_time=1.982909, accumulated_submission_time=14898.544740, global_step=46976, preemption_count=0, score=14898.544740, test/accuracy=0.985219, test/loss=0.049554, test/mean_average_precision=0.199798, test/num_examples=43793, total_duration=21501.015641, train/accuracy=0.989486, train/loss=0.035545, train/mean_average_precision=0.271405, validation/accuracy=0.986148, validation/loss=0.046897, validation/mean_average_precision=0.208510, validation/num_examples=43793
I0206 04:29:32.621715 140266968643328 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.05664339289069176, loss=0.035148344933986664
I0206 04:30:06.262454 140229879785216 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.08915732800960541, loss=0.03771994262933731
I0206 04:30:37.361292 140266968643328 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.05647426098585129, loss=0.03698784485459328
I0206 04:31:08.418425 140229879785216 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.10680845379829407, loss=0.039437320083379745
I0206 04:31:40.339497 140266968643328 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.05371349677443504, loss=0.03523148223757744
I0206 04:32:11.878811 140229879785216 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.07652828097343445, loss=0.03922990337014198
I0206 04:32:43.715026 140266968643328 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.05063215270638466, loss=0.03307459130883217
I0206 04:33:15.060453 140229879785216 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.06398855894804001, loss=0.034837134182453156
I0206 04:33:25.026978 140451058161472 spec.py:321] Evaluating on the training split.
I0206 04:35:03.757357 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 04:35:06.772395 140451058161472 spec.py:349] Evaluating on the test split.
I0206 04:35:09.742711 140451058161472 submission_runner.py:408] Time since start: 21846.03s, 	Step: 47733, 	{'train/accuracy': 0.9893301129341125, 'train/loss': 0.03604253754019737, 'train/mean_average_precision': 0.2642958725828656, 'validation/accuracy': 0.9861845970153809, 'validation/loss': 0.04685523360967636, 'validation/mean_average_precision': 0.2086522317130877, 'validation/num_examples': 43793, 'test/accuracy': 0.9853790402412415, 'test/loss': 0.049420733004808426, 'test/mean_average_precision': 0.20462316028592084, 'test/num_examples': 43793, 'score': 15138.784644126892, 'total_duration': 21846.0258705616, 'accumulated_submission_time': 15138.784644126892, 'accumulated_eval_time': 6703.9477796554565, 'accumulated_logging_time': 2.0181338787078857}
I0206 04:35:09.766549 140266960250624 logging_writer.py:48] [47733] accumulated_eval_time=6703.947780, accumulated_logging_time=2.018134, accumulated_submission_time=15138.784644, global_step=47733, preemption_count=0, score=15138.784644, test/accuracy=0.985379, test/loss=0.049421, test/mean_average_precision=0.204623, test/num_examples=43793, total_duration=21846.025871, train/accuracy=0.989330, train/loss=0.036043, train/mean_average_precision=0.264296, validation/accuracy=0.986185, validation/loss=0.046855, validation/mean_average_precision=0.208652, validation/num_examples=43793
I0206 04:35:31.195031 140290196547328 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.10006427764892578, loss=0.0369332991540432
I0206 04:36:03.024457 140266960250624 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.11679551750421524, loss=0.04282626509666443
I0206 04:36:34.893225 140290196547328 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.0833766758441925, loss=0.033282775431871414
I0206 04:37:06.828876 140266960250624 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.06101423501968384, loss=0.035325516015291214
I0206 04:37:38.514624 140290196547328 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.07749729603528976, loss=0.03751079738140106
I0206 04:38:10.698442 140266960250624 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.06281373649835587, loss=0.038965221494436264
I0206 04:38:42.123113 140290196547328 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.0891028568148613, loss=0.03508894890546799
I0206 04:39:09.895296 140451058161472 spec.py:321] Evaluating on the training split.
I0206 04:40:45.449044 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 04:40:48.500633 140451058161472 spec.py:349] Evaluating on the test split.
I0206 04:40:51.535582 140451058161472 submission_runner.py:408] Time since start: 22187.82s, 	Step: 48488, 	{'train/accuracy': 0.9893452525138855, 'train/loss': 0.0367339625954628, 'train/mean_average_precision': 0.25762161900276714, 'validation/accuracy': 0.9859787821769714, 'validation/loss': 0.047152355313301086, 'validation/mean_average_precision': 0.20892928456741483, 'validation/num_examples': 43793, 'test/accuracy': 0.9852316379547119, 'test/loss': 0.049615755677223206, 'test/mean_average_precision': 0.20043036230925274, 'test/num_examples': 43793, 'score': 15378.88271021843, 'total_duration': 22187.8186275959, 'accumulated_submission_time': 15378.88271021843, 'accumulated_eval_time': 6805.587907791138, 'accumulated_logging_time': 2.05293345451355}
I0206 04:40:51.559899 140248415348480 logging_writer.py:48] [48488] accumulated_eval_time=6805.587908, accumulated_logging_time=2.052933, accumulated_submission_time=15378.882710, global_step=48488, preemption_count=0, score=15378.882710, test/accuracy=0.985232, test/loss=0.049616, test/mean_average_precision=0.200430, test/num_examples=43793, total_duration=22187.818628, train/accuracy=0.989345, train/loss=0.036734, train/mean_average_precision=0.257622, validation/accuracy=0.985979, validation/loss=0.047152, validation/mean_average_precision=0.208929, validation/num_examples=43793
I0206 04:40:55.671306 140266968643328 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.09046557545661926, loss=0.036665525287389755
I0206 04:41:27.752331 140248415348480 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.07029349356889725, loss=0.040920473635196686
I0206 04:41:59.946279 140266968643328 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.06644408404827118, loss=0.03644225373864174
I0206 04:42:32.039123 140248415348480 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.08010071516036987, loss=0.03763362765312195
I0206 04:43:04.443732 140266968643328 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.14973169565200806, loss=0.042891841381788254
I0206 04:43:36.555361 140248415348480 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.06451820582151413, loss=0.03527519851922989
I0206 04:44:08.478608 140266968643328 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.07150585949420929, loss=0.03923223167657852
I0206 04:44:40.321577 140248415348480 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.0895606204867363, loss=0.036248210817575455
I0206 04:44:51.671144 140451058161472 spec.py:321] Evaluating on the training split.
I0206 04:46:29.668984 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 04:46:32.712187 140451058161472 spec.py:349] Evaluating on the test split.
I0206 04:46:35.703161 140451058161472 submission_runner.py:408] Time since start: 22531.99s, 	Step: 49237, 	{'train/accuracy': 0.989425539970398, 'train/loss': 0.03569924458861351, 'train/mean_average_precision': 0.26737582420810835, 'validation/accuracy': 0.9861699938774109, 'validation/loss': 0.04659583792090416, 'validation/mean_average_precision': 0.2079270938996903, 'validation/num_examples': 43793, 'test/accuracy': 0.9853769540786743, 'test/loss': 0.04939282685518265, 'test/mean_average_precision': 0.1981841083007525, 'test/num_examples': 43793, 'score': 15618.961620092392, 'total_duration': 22531.986197948456, 'accumulated_submission_time': 15618.961620092392, 'accumulated_eval_time': 6909.61977314949, 'accumulated_logging_time': 2.088233470916748}
I0206 04:46:35.728460 140229879785216 logging_writer.py:48] [49237] accumulated_eval_time=6909.619773, accumulated_logging_time=2.088233, accumulated_submission_time=15618.961620, global_step=49237, preemption_count=0, score=15618.961620, test/accuracy=0.985377, test/loss=0.049393, test/mean_average_precision=0.198184, test/num_examples=43793, total_duration=22531.986198, train/accuracy=0.989426, train/loss=0.035699, train/mean_average_precision=0.267376, validation/accuracy=0.986170, validation/loss=0.046596, validation/mean_average_precision=0.207927, validation/num_examples=43793
I0206 04:46:55.962721 140290196547328 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.10272978991270065, loss=0.03816509246826172
I0206 04:47:27.803840 140229879785216 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.2162211537361145, loss=0.03675617277622223
I0206 04:47:59.525601 140290196547328 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.054575420916080475, loss=0.036979664117097855
I0206 04:48:31.153587 140229879785216 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.11951369792222977, loss=0.03487517312169075
I0206 04:49:02.693690 140290196547328 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.10796082019805908, loss=0.03298245370388031
I0206 04:49:34.094593 140229879785216 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.09542281925678253, loss=0.03710814565420151
I0206 04:50:05.541083 140290196547328 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.07313002645969391, loss=0.03974662348628044
I0206 04:50:35.992483 140451058161472 spec.py:321] Evaluating on the training split.
I0206 04:52:15.552300 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 04:52:18.562577 140451058161472 spec.py:349] Evaluating on the test split.
I0206 04:52:21.579879 140451058161472 submission_runner.py:408] Time since start: 22877.86s, 	Step: 49998, 	{'train/accuracy': 0.9894838929176331, 'train/loss': 0.03577679023146629, 'train/mean_average_precision': 0.26919955506192716, 'validation/accuracy': 0.9862085580825806, 'validation/loss': 0.046243514865636826, 'validation/mean_average_precision': 0.21020474569661, 'validation/num_examples': 43793, 'test/accuracy': 0.9853028059005737, 'test/loss': 0.04925117641687393, 'test/mean_average_precision': 0.20060108943621363, 'test/num_examples': 43793, 'score': 15859.194140434265, 'total_duration': 22877.863034963608, 'accumulated_submission_time': 15859.194140434265, 'accumulated_eval_time': 7015.207123994827, 'accumulated_logging_time': 2.1260316371917725}
I0206 04:52:21.604820 140248415348480 logging_writer.py:48] [49998] accumulated_eval_time=7015.207124, accumulated_logging_time=2.126032, accumulated_submission_time=15859.194140, global_step=49998, preemption_count=0, score=15859.194140, test/accuracy=0.985303, test/loss=0.049251, test/mean_average_precision=0.200601, test/num_examples=43793, total_duration=22877.863035, train/accuracy=0.989484, train/loss=0.035777, train/mean_average_precision=0.269200, validation/accuracy=0.986209, validation/loss=0.046244, validation/mean_average_precision=0.210205, validation/num_examples=43793
I0206 04:52:22.575961 140266968643328 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.05642654374241829, loss=0.038057368248701096
I0206 04:52:54.024493 140248415348480 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.09863061457872391, loss=0.03698838874697685
I0206 04:53:26.620045 140266968643328 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.07372990250587463, loss=0.033002883195877075
I0206 04:53:59.654700 140248415348480 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.05079515278339386, loss=0.03553284704685211
I0206 04:54:32.339627 140266968643328 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.15263690054416656, loss=0.033765967935323715
I0206 04:55:04.872385 140248415348480 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.08038831502199173, loss=0.04033469781279564
I0206 04:55:36.325429 140266968643328 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.06021016463637352, loss=0.038505669683218
I0206 04:56:07.572727 140248415348480 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.0719393938779831, loss=0.038056228309869766
I0206 04:56:21.613858 140451058161472 spec.py:321] Evaluating on the training split.
I0206 04:57:57.354470 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 04:58:00.464300 140451058161472 spec.py:349] Evaluating on the test split.
I0206 04:58:03.760779 140451058161472 submission_runner.py:408] Time since start: 23220.04s, 	Step: 50746, 	{'train/accuracy': 0.9894869923591614, 'train/loss': 0.03578002378344536, 'train/mean_average_precision': 0.2574080154233004, 'validation/accuracy': 0.9860761761665344, 'validation/loss': 0.046711672097444534, 'validation/mean_average_precision': 0.2097283898741605, 'validation/num_examples': 43793, 'test/accuracy': 0.9852480292320251, 'test/loss': 0.04947970062494278, 'test/mean_average_precision': 0.2006904166219487, 'test/num_examples': 43793, 'score': 16099.170221567154, 'total_duration': 23220.043934583664, 'accumulated_submission_time': 16099.170221567154, 'accumulated_eval_time': 7117.35399389267, 'accumulated_logging_time': 2.1619765758514404}
I0206 04:58:03.785295 140229879785216 logging_writer.py:48] [50746] accumulated_eval_time=7117.353994, accumulated_logging_time=2.161977, accumulated_submission_time=16099.170222, global_step=50746, preemption_count=0, score=16099.170222, test/accuracy=0.985248, test/loss=0.049480, test/mean_average_precision=0.200690, test/num_examples=43793, total_duration=23220.043935, train/accuracy=0.989487, train/loss=0.035780, train/mean_average_precision=0.257408, validation/accuracy=0.986076, validation/loss=0.046712, validation/mean_average_precision=0.209728, validation/num_examples=43793
I0206 04:58:20.964057 140266960250624 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.06560707837343216, loss=0.035528797656297684
I0206 04:58:52.200115 140229879785216 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.057929281145334244, loss=0.03447924554347992
I0206 04:59:23.700685 140266960250624 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.08831135183572769, loss=0.03539307042956352
I0206 04:59:55.129805 140229879785216 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.06086253747344017, loss=0.03568075969815254
I0206 05:00:26.497504 140266960250624 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.09432317316532135, loss=0.031088868156075478
I0206 05:00:58.078096 140229879785216 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.08166249096393585, loss=0.036054570227861404
I0206 05:01:29.828994 140266960250624 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.09024345874786377, loss=0.03582610562443733
I0206 05:02:02.032403 140229879785216 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.08723187446594238, loss=0.03859815001487732
I0206 05:02:03.867473 140451058161472 spec.py:321] Evaluating on the training split.
I0206 05:03:42.879923 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 05:03:46.223331 140451058161472 spec.py:349] Evaluating on the test split.
I0206 05:03:49.524391 140451058161472 submission_runner.py:408] Time since start: 23565.81s, 	Step: 51507, 	{'train/accuracy': 0.9896237254142761, 'train/loss': 0.03497027978301048, 'train/mean_average_precision': 0.28959290790469516, 'validation/accuracy': 0.9863124489784241, 'validation/loss': 0.04612846300005913, 'validation/mean_average_precision': 0.2125804910287261, 'validation/num_examples': 43793, 'test/accuracy': 0.9854000806808472, 'test/loss': 0.049058105796575546, 'test/mean_average_precision': 0.20585747115689745, 'test/num_examples': 43793, 'score': 16339.22156381607, 'total_duration': 23565.807424545288, 'accumulated_submission_time': 16339.22156381607, 'accumulated_eval_time': 7223.010741472244, 'accumulated_logging_time': 2.1973283290863037}
I0206 05:03:49.550910 140266968643328 logging_writer.py:48] [51507] accumulated_eval_time=7223.010741, accumulated_logging_time=2.197328, accumulated_submission_time=16339.221564, global_step=51507, preemption_count=0, score=16339.221564, test/accuracy=0.985400, test/loss=0.049058, test/mean_average_precision=0.205857, test/num_examples=43793, total_duration=23565.807425, train/accuracy=0.989624, train/loss=0.034970, train/mean_average_precision=0.289593, validation/accuracy=0.986312, validation/loss=0.046128, validation/mean_average_precision=0.212580, validation/num_examples=43793
I0206 05:04:19.985344 140290196547328 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.08176343888044357, loss=0.03478802740573883
I0206 05:04:52.022048 140266968643328 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.08435575664043427, loss=0.03317830711603165
I0206 05:05:24.549013 140290196547328 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.11530352383852005, loss=0.03410886228084564
I0206 05:05:56.590761 140266968643328 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.1010294035077095, loss=0.03630296140909195
I0206 05:06:28.763231 140290196547328 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.09047103673219681, loss=0.03910277783870697
I0206 05:07:00.383311 140266968643328 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.05824797973036766, loss=0.03897659108042717
I0206 05:07:31.971582 140290196547328 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.05810139700770378, loss=0.032453108578920364
I0206 05:07:49.683670 140451058161472 spec.py:321] Evaluating on the training split.
I0206 05:09:25.072522 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 05:09:28.217137 140451058161472 spec.py:349] Evaluating on the test split.
I0206 05:09:31.324789 140451058161472 submission_runner.py:408] Time since start: 23907.61s, 	Step: 52257, 	{'train/accuracy': 0.9895951151847839, 'train/loss': 0.03497644141316414, 'train/mean_average_precision': 0.27689385115770665, 'validation/accuracy': 0.986238956451416, 'validation/loss': 0.04646912217140198, 'validation/mean_average_precision': 0.2151909716914597, 'validation/num_examples': 43793, 'test/accuracy': 0.9854320883750916, 'test/loss': 0.04927407577633858, 'test/mean_average_precision': 0.20773597079881576, 'test/num_examples': 43793, 'score': 16579.318229198456, 'total_duration': 23907.607943296432, 'accumulated_submission_time': 16579.318229198456, 'accumulated_eval_time': 7324.651810646057, 'accumulated_logging_time': 2.2361605167388916}
I0206 05:09:31.351247 140229879785216 logging_writer.py:48] [52257] accumulated_eval_time=7324.651811, accumulated_logging_time=2.236161, accumulated_submission_time=16579.318229, global_step=52257, preemption_count=0, score=16579.318229, test/accuracy=0.985432, test/loss=0.049274, test/mean_average_precision=0.207736, test/num_examples=43793, total_duration=23907.607943, train/accuracy=0.989595, train/loss=0.034976, train/mean_average_precision=0.276894, validation/accuracy=0.986239, validation/loss=0.046469, validation/mean_average_precision=0.215191, validation/num_examples=43793
I0206 05:09:45.210232 140266960250624 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.07737774401903152, loss=0.0313900001347065
I0206 05:10:16.690595 140229879785216 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.05245096981525421, loss=0.03228331729769707
I0206 05:10:48.281071 140266960250624 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.06902957707643509, loss=0.035135962069034576
I0206 05:11:19.750846 140229879785216 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.11142394691705704, loss=0.030546866357326508
I0206 05:11:51.263660 140266960250624 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.08107829838991165, loss=0.03763711079955101
I0206 05:12:23.166797 140229879785216 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.09922578185796738, loss=0.032239899039268494
I0206 05:12:54.360687 140266960250624 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.07584401965141296, loss=0.040832336992025375
I0206 05:13:26.007183 140229879785216 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.06511542946100235, loss=0.03294762223958969
I0206 05:13:31.351915 140451058161472 spec.py:321] Evaluating on the training split.
I0206 05:15:07.009702 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 05:15:10.069247 140451058161472 spec.py:349] Evaluating on the test split.
I0206 05:15:13.042522 140451058161472 submission_runner.py:408] Time since start: 24249.33s, 	Step: 53018, 	{'train/accuracy': 0.9896343350410461, 'train/loss': 0.0349128283560276, 'train/mean_average_precision': 0.2853589611694491, 'validation/accuracy': 0.9862856864929199, 'validation/loss': 0.04632439464330673, 'validation/mean_average_precision': 0.21648895201171497, 'validation/num_examples': 43793, 'test/accuracy': 0.9853954315185547, 'test/loss': 0.04904608428478241, 'test/mean_average_precision': 0.20804087235297986, 'test/num_examples': 43793, 'score': 16819.287693738937, 'total_duration': 24249.325675964355, 'accumulated_submission_time': 16819.287693738937, 'accumulated_eval_time': 7426.342364788055, 'accumulated_logging_time': 2.273671865463257}
I0206 05:15:13.068180 140248415348480 logging_writer.py:48] [53018] accumulated_eval_time=7426.342365, accumulated_logging_time=2.273672, accumulated_submission_time=16819.287694, global_step=53018, preemption_count=0, score=16819.287694, test/accuracy=0.985395, test/loss=0.049046, test/mean_average_precision=0.208041, test/num_examples=43793, total_duration=24249.325676, train/accuracy=0.989634, train/loss=0.034913, train/mean_average_precision=0.285359, validation/accuracy=0.986286, validation/loss=0.046324, validation/mean_average_precision=0.216489, validation/num_examples=43793
I0206 05:15:39.559077 140266968643328 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.06426112353801727, loss=0.03183874487876892
I0206 05:16:11.948512 140248415348480 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.09339050203561783, loss=0.03214206546545029
I0206 05:16:43.719239 140266968643328 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.09617607295513153, loss=0.03494774177670479
I0206 05:17:15.781162 140248415348480 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.1111530289053917, loss=0.03754190355539322
I0206 05:17:47.796888 140266968643328 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.07863570004701614, loss=0.03727486729621887
I0206 05:18:20.004714 140248415348480 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.09202931076288223, loss=0.040294233709573746
I0206 05:18:51.680512 140266968643328 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.14632578194141388, loss=0.03188377246260643
I0206 05:19:13.142457 140451058161472 spec.py:321] Evaluating on the training split.
I0206 05:20:48.830106 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 05:20:51.894165 140451058161472 spec.py:349] Evaluating on the test split.
I0206 05:20:54.876548 140451058161472 submission_runner.py:408] Time since start: 24591.16s, 	Step: 53768, 	{'train/accuracy': 0.9898422956466675, 'train/loss': 0.034334566444158554, 'train/mean_average_precision': 0.2906300054524572, 'validation/accuracy': 0.9863603711128235, 'validation/loss': 0.04618864133954048, 'validation/mean_average_precision': 0.21715397523875626, 'validation/num_examples': 43793, 'test/accuracy': 0.9854085445404053, 'test/loss': 0.0492413267493248, 'test/mean_average_precision': 0.21076632327964406, 'test/num_examples': 43793, 'score': 17059.330296278, 'total_duration': 24591.159697771072, 'accumulated_submission_time': 17059.330296278, 'accumulated_eval_time': 7528.076402425766, 'accumulated_logging_time': 2.310476779937744}
I0206 05:20:54.902298 140229879785216 logging_writer.py:48] [53768] accumulated_eval_time=7528.076402, accumulated_logging_time=2.310477, accumulated_submission_time=17059.330296, global_step=53768, preemption_count=0, score=17059.330296, test/accuracy=0.985409, test/loss=0.049241, test/mean_average_precision=0.210766, test/num_examples=43793, total_duration=24591.159698, train/accuracy=0.989842, train/loss=0.034335, train/mean_average_precision=0.290630, validation/accuracy=0.986360, validation/loss=0.046189, validation/mean_average_precision=0.217154, validation/num_examples=43793
I0206 05:21:05.676390 140290196547328 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.1527932733297348, loss=0.03634611517190933
I0206 05:21:38.682079 140229879785216 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.09676354378461838, loss=0.03703198581933975
I0206 05:22:10.599529 140290196547328 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.08598361909389496, loss=0.03739957883954048
I0206 05:22:42.563692 140229879785216 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.06295094639062881, loss=0.03935087099671364
I0206 05:23:14.786782 140290196547328 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.0729270726442337, loss=0.03706659376621246
I0206 05:23:46.517775 140229879785216 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.0920451357960701, loss=0.03126165643334389
I0206 05:24:18.909673 140290196547328 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.0859573557972908, loss=0.03422762081027031
I0206 05:24:50.383261 140229879785216 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.08441129326820374, loss=0.035183943808078766
I0206 05:24:55.127661 140451058161472 spec.py:321] Evaluating on the training split.
I0206 05:26:33.062442 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 05:26:36.090609 140451058161472 spec.py:349] Evaluating on the test split.
I0206 05:26:39.022241 140451058161472 submission_runner.py:408] Time since start: 24935.31s, 	Step: 54516, 	{'train/accuracy': 0.9898531436920166, 'train/loss': 0.03418021276593208, 'train/mean_average_precision': 0.2880437519159216, 'validation/accuracy': 0.9863904118537903, 'validation/loss': 0.04603935778141022, 'validation/mean_average_precision': 0.2220284243850361, 'validation/num_examples': 43793, 'test/accuracy': 0.9854624271392822, 'test/loss': 0.049133043736219406, 'test/mean_average_precision': 0.21115627677974935, 'test/num_examples': 43793, 'score': 17299.522981405258, 'total_duration': 24935.30538392067, 'accumulated_submission_time': 17299.522981405258, 'accumulated_eval_time': 7631.970919847488, 'accumulated_logging_time': 2.348414182662964}
I0206 05:26:39.049186 140248415348480 logging_writer.py:48] [54516] accumulated_eval_time=7631.970920, accumulated_logging_time=2.348414, accumulated_submission_time=17299.522981, global_step=54516, preemption_count=0, score=17299.522981, test/accuracy=0.985462, test/loss=0.049133, test/mean_average_precision=0.211156, test/num_examples=43793, total_duration=24935.305384, train/accuracy=0.989853, train/loss=0.034180, train/mean_average_precision=0.288044, validation/accuracy=0.986390, validation/loss=0.046039, validation/mean_average_precision=0.222028, validation/num_examples=43793
I0206 05:27:05.984134 140266960250624 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.0682167336344719, loss=0.039576414972543716
I0206 05:27:37.019231 140248415348480 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.08068358898162842, loss=0.03728962689638138
I0206 05:28:08.393611 140266960250624 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.085043765604496, loss=0.03471190109848976
I0206 05:28:39.670858 140248415348480 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.06574007123708725, loss=0.03329960256814957
I0206 05:29:11.158832 140266960250624 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.13046574592590332, loss=0.035758402198553085
I0206 05:29:42.631967 140248415348480 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.07791744917631149, loss=0.03270970284938812
I0206 05:30:14.243435 140266960250624 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.0588989220559597, loss=0.031870149075984955
I0206 05:30:39.026321 140451058161472 spec.py:321] Evaluating on the training split.
I0206 05:32:19.584990 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 05:32:22.930096 140451058161472 spec.py:349] Evaluating on the test split.
I0206 05:32:26.273802 140451058161472 submission_runner.py:408] Time since start: 25282.56s, 	Step: 55281, 	{'train/accuracy': 0.9897654056549072, 'train/loss': 0.03450881317257881, 'train/mean_average_precision': 0.284018912475966, 'validation/accuracy': 0.9863700866699219, 'validation/loss': 0.04589037969708443, 'validation/mean_average_precision': 0.22173952586280968, 'validation/num_examples': 43793, 'test/accuracy': 0.9854293465614319, 'test/loss': 0.04875707998871803, 'test/mean_average_precision': 0.2053719159509973, 'test/num_examples': 43793, 'score': 17539.468029499054, 'total_duration': 25282.556941986084, 'accumulated_submission_time': 17539.468029499054, 'accumulated_eval_time': 7739.218340873718, 'accumulated_logging_time': 2.3874459266662598}
I0206 05:32:26.303126 140229879785216 logging_writer.py:48] [55281] accumulated_eval_time=7739.218341, accumulated_logging_time=2.387446, accumulated_submission_time=17539.468029, global_step=55281, preemption_count=0, score=17539.468029, test/accuracy=0.985429, test/loss=0.048757, test/mean_average_precision=0.205372, test/num_examples=43793, total_duration=25282.556942, train/accuracy=0.989765, train/loss=0.034509, train/mean_average_precision=0.284019, validation/accuracy=0.986370, validation/loss=0.045890, validation/mean_average_precision=0.221740, validation/num_examples=43793
I0206 05:32:32.700805 140266968643328 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.14616712927818298, loss=0.038151517510414124
I0206 05:33:04.919469 140229879785216 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.14080755412578583, loss=0.031125521287322044
I0206 05:33:36.665936 140266968643328 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.07280639559030533, loss=0.03744858503341675
I0206 05:34:08.488325 140229879785216 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.059921156615018845, loss=0.03532398119568825
I0206 05:34:40.564932 140266968643328 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.09285090118646622, loss=0.03403814882040024
I0206 05:35:12.126981 140229879785216 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.07945390045642853, loss=0.03687675669789314
I0206 05:35:43.452761 140266968643328 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.1205630674958229, loss=0.030476758256554604
I0206 05:36:14.858048 140229879785216 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.08710696548223495, loss=0.037468548864126205
I0206 05:36:26.368781 140451058161472 spec.py:321] Evaluating on the training split.
I0206 05:38:02.384315 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 05:38:05.404243 140451058161472 spec.py:349] Evaluating on the test split.
I0206 05:38:08.464693 140451058161472 submission_runner.py:408] Time since start: 25624.75s, 	Step: 56038, 	{'train/accuracy': 0.989662766456604, 'train/loss': 0.034886036068201065, 'train/mean_average_precision': 0.2843685079643878, 'validation/accuracy': 0.9862491488456726, 'validation/loss': 0.04602076858282089, 'validation/mean_average_precision': 0.21583840603793586, 'validation/num_examples': 43793, 'test/accuracy': 0.9853495359420776, 'test/loss': 0.04895078018307686, 'test/mean_average_precision': 0.2030077043569287, 'test/num_examples': 43793, 'score': 17779.500022172928, 'total_duration': 25624.747854471207, 'accumulated_submission_time': 17779.500022172928, 'accumulated_eval_time': 7841.314211845398, 'accumulated_logging_time': 2.4286468029022217}
I0206 05:38:08.490620 140248415348480 logging_writer.py:48] [56038] accumulated_eval_time=7841.314212, accumulated_logging_time=2.428647, accumulated_submission_time=17779.500022, global_step=56038, preemption_count=0, score=17779.500022, test/accuracy=0.985350, test/loss=0.048951, test/mean_average_precision=0.203008, test/num_examples=43793, total_duration=25624.747854, train/accuracy=0.989663, train/loss=0.034886, train/mean_average_precision=0.284369, validation/accuracy=0.986249, validation/loss=0.046021, validation/mean_average_precision=0.215838, validation/num_examples=43793
I0206 05:38:28.510952 140266960250624 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.07180529832839966, loss=0.03462376073002815
I0206 05:39:00.037809 140248415348480 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.12215197831392288, loss=0.03788905590772629
I0206 05:39:31.429809 140266960250624 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.1008002907037735, loss=0.03230183571577072
I0206 05:40:03.502736 140248415348480 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.07444164901971817, loss=0.0342094711959362
I0206 05:40:34.983858 140266960250624 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.10535594820976257, loss=0.0347331166267395
I0206 05:41:06.732048 140248415348480 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.09641390293836594, loss=0.03569447249174118
I0206 05:41:38.565147 140266960250624 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.12052880972623825, loss=0.03600342944264412
I0206 05:42:08.479643 140451058161472 spec.py:321] Evaluating on the training split.
I0206 05:43:46.125640 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 05:43:49.136362 140451058161472 spec.py:349] Evaluating on the test split.
I0206 05:43:52.106771 140451058161472 submission_runner.py:408] Time since start: 25968.39s, 	Step: 56795, 	{'train/accuracy': 0.9896244406700134, 'train/loss': 0.03496367484331131, 'train/mean_average_precision': 0.27953008690565206, 'validation/accuracy': 0.9862730503082275, 'validation/loss': 0.045942336320877075, 'validation/mean_average_precision': 0.21640935875464556, 'validation/num_examples': 43793, 'test/accuracy': 0.9854097962379456, 'test/loss': 0.04870106652379036, 'test/mean_average_precision': 0.20893250288086945, 'test/num_examples': 43793, 'score': 18019.45709347725, 'total_duration': 25968.38992166519, 'accumulated_submission_time': 18019.45709347725, 'accumulated_eval_time': 7944.941290616989, 'accumulated_logging_time': 2.4669902324676514}
I0206 05:43:52.132266 140266968643328 logging_writer.py:48] [56795] accumulated_eval_time=7944.941291, accumulated_logging_time=2.466990, accumulated_submission_time=18019.457093, global_step=56795, preemption_count=0, score=18019.457093, test/accuracy=0.985410, test/loss=0.048701, test/mean_average_precision=0.208933, test/num_examples=43793, total_duration=25968.389922, train/accuracy=0.989624, train/loss=0.034964, train/mean_average_precision=0.279530, validation/accuracy=0.986273, validation/loss=0.045942, validation/mean_average_precision=0.216409, validation/num_examples=43793
I0206 05:43:54.094961 140290196547328 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.08827565610408783, loss=0.03182714059948921
I0206 05:44:25.751273 140266968643328 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.10643354058265686, loss=0.034594908356666565
I0206 05:44:57.370121 140290196547328 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.08662337809801102, loss=0.035979293286800385
I0206 05:45:28.876845 140266968643328 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.10945864021778107, loss=0.03390678018331528
I0206 05:45:59.970118 140290196547328 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.1019849181175232, loss=0.033562883734703064
I0206 05:46:31.093387 140266968643328 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.10080200433731079, loss=0.03431360796093941
I0206 05:47:02.461120 140290196547328 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.10321621596813202, loss=0.03175728768110275
I0206 05:47:33.648915 140266968643328 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.14496742188930511, loss=0.03849337622523308
I0206 05:47:52.395745 140451058161472 spec.py:321] Evaluating on the training split.
I0206 05:49:27.978431 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 05:49:30.998827 140451058161472 spec.py:349] Evaluating on the test split.
I0206 05:49:33.952671 140451058161472 submission_runner.py:408] Time since start: 26310.24s, 	Step: 57561, 	{'train/accuracy': 0.9898160099983215, 'train/loss': 0.03474228456616402, 'train/mean_average_precision': 0.28430747955029, 'validation/accuracy': 0.9861565828323364, 'validation/loss': 0.046272970736026764, 'validation/mean_average_precision': 0.22056289875778998, 'validation/num_examples': 43793, 'test/accuracy': 0.985299825668335, 'test/loss': 0.048982568085193634, 'test/mean_average_precision': 0.2117912201186689, 'test/num_examples': 43793, 'score': 18259.68850851059, 'total_duration': 26310.2358212471, 'accumulated_submission_time': 18259.68850851059, 'accumulated_eval_time': 8046.498164176941, 'accumulated_logging_time': 2.5046470165252686}
I0206 05:49:33.978383 140229879785216 logging_writer.py:48] [57561] accumulated_eval_time=8046.498164, accumulated_logging_time=2.504647, accumulated_submission_time=18259.688509, global_step=57561, preemption_count=0, score=18259.688509, test/accuracy=0.985300, test/loss=0.048983, test/mean_average_precision=0.211791, test/num_examples=43793, total_duration=26310.235821, train/accuracy=0.989816, train/loss=0.034742, train/mean_average_precision=0.284307, validation/accuracy=0.986157, validation/loss=0.046273, validation/mean_average_precision=0.220563, validation/num_examples=43793
I0206 05:49:46.695246 140248415348480 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.11436214298009872, loss=0.03444787114858627
I0206 05:50:18.322112 140229879785216 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.08023582398891449, loss=0.03723151609301567
I0206 05:50:49.387688 140248415348480 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.10681933164596558, loss=0.03410572558641434
I0206 05:51:20.729523 140229879785216 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.15417346358299255, loss=0.036309368908405304
I0206 05:51:51.991477 140248415348480 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.16075098514556885, loss=0.03668728098273277
I0206 05:52:23.608408 140229879785216 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.11085169017314911, loss=0.03178330138325691
I0206 05:52:55.082295 140248415348480 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.10300038754940033, loss=0.034529708325862885
I0206 05:53:11.622497 140229879785216 logging_writer.py:48] [58253] global_step=58253, preemption_count=0, score=18477.287541
I0206 05:53:11.715990 140451058161472 checkpoints.py:490] Saving checkpoint at step: 58253
I0206 05:53:11.830484 140451058161472 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_2/ogbg_jax/trial_4/checkpoint_58253
I0206 05:53:11.831617 140451058161472 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_2/ogbg_jax/trial_4/checkpoint_58253.
I0206 05:53:11.992695 140451058161472 submission_runner.py:583] Tuning trial 4/5
I0206 05:53:11.992944 140451058161472 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.004958460849689891, one_minus_beta1=0.13625575743, beta2=0.6291854735396584, weight_decay=0.1147386261512052, warmup_factor=0.02)
I0206 05:53:11.997354 140451058161472 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5288742184638977, 'train/loss': 0.7363438606262207, 'train/mean_average_precision': 0.02097083927118521, 'validation/accuracy': 0.5270814895629883, 'validation/loss': 0.7374407649040222, 'validation/mean_average_precision': 0.024088420370142846, 'validation/num_examples': 43793, 'test/accuracy': 0.5256850719451904, 'test/loss': 0.7376683354377747, 'test/mean_average_precision': 0.02604572037436443, 'test/num_examples': 43793, 'score': 11.894293308258057, 'total_duration': 121.44691801071167, 'accumulated_submission_time': 11.894293308258057, 'accumulated_eval_time': 109.55257105827332, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (743, {'train/accuracy': 0.9868398904800415, 'train/loss': 0.05368730053305626, 'train/mean_average_precision': 0.042395540089543944, 'validation/accuracy': 0.9841423034667969, 'validation/loss': 0.06374505907297134, 'validation/mean_average_precision': 0.04093739695877839, 'validation/num_examples': 43793, 'test/accuracy': 0.9831504821777344, 'test/loss': 0.066874660551548, 'test/mean_average_precision': 0.04238444251262984, 'test/num_examples': 43793, 'score': 251.91315054893494, 'total_duration': 467.0992650985718, 'accumulated_submission_time': 251.91315054893494, 'accumulated_eval_time': 215.14411735534668, 'accumulated_logging_time': 0.021558761596679688, 'global_step': 743, 'preemption_count': 0}), (1499, {'train/accuracy': 0.9869152903556824, 'train/loss': 0.05036809667944908, 'train/mean_average_precision': 0.06419398640662428, 'validation/accuracy': 0.9842705726623535, 'validation/loss': 0.06056094542145729, 'validation/mean_average_precision': 0.06255241673381669, 'validation/num_examples': 43793, 'test/accuracy': 0.9832835793495178, 'test/loss': 0.06384151428937912, 'test/mean_average_precision': 0.06262509468053194, 'test/num_examples': 43793, 'score': 492.1413643360138, 'total_duration': 812.2493708133698, 'accumulated_submission_time': 492.1413643360138, 'accumulated_eval_time': 320.01709842681885, 'accumulated_logging_time': 0.04915928840637207, 'global_step': 1499, 'preemption_count': 0}), (2256, {'train/accuracy': 0.9873570799827576, 'train/loss': 0.046902839094400406, 'train/mean_average_precision': 0.09434357553499763, 'validation/accuracy': 0.9845478534698486, 'validation/loss': 0.057609282433986664, 'validation/mean_average_precision': 0.0900378077916038, 'validation/num_examples': 43793, 'test/accuracy': 0.9835636615753174, 'test/loss': 0.0608636774122715, 'test/mean_average_precision': 0.09006096953725061, 'test/num_examples': 43793, 'score': 732.1600904464722, 'total_duration': 1164.0452189445496, 'accumulated_submission_time': 732.1600904464722, 'accumulated_eval_time': 431.74619340896606, 'accumulated_logging_time': 0.07654666900634766, 'global_step': 2256, 'preemption_count': 0}), (3014, {'train/accuracy': 0.9874885678291321, 'train/loss': 0.04553155601024628, 'train/mean_average_precision': 0.11648724945300393, 'validation/accuracy': 0.9847686290740967, 'validation/loss': 0.055045951157808304, 'validation/mean_average_precision': 0.10606262900438587, 'validation/num_examples': 43793, 'test/accuracy': 0.9837780594825745, 'test/loss': 0.058156948536634445, 'test/mean_average_precision': 0.10731129227853359, 'test/num_examples': 43793, 'score': 972.1134994029999, 'total_duration': 1506.7364542484283, 'accumulated_submission_time': 972.1134994029999, 'accumulated_eval_time': 534.4365320205688, 'accumulated_logging_time': 0.10349726676940918, 'global_step': 3014, 'preemption_count': 0}), (3771, {'train/accuracy': 0.9876707196235657, 'train/loss': 0.04354364797472954, 'train/mean_average_precision': 0.1393367840583442, 'validation/accuracy': 0.9849566221237183, 'validation/loss': 0.05245377495884895, 'validation/mean_average_precision': 0.1315201944459623, 'validation/num_examples': 43793, 'test/accuracy': 0.9840333461761475, 'test/loss': 0.055392779409885406, 'test/mean_average_precision': 0.1286881339106711, 'test/num_examples': 43793, 'score': 1212.3574872016907, 'total_duration': 1855.3214933872223, 'accumulated_submission_time': 1212.3574872016907, 'accumulated_eval_time': 642.7269651889801, 'accumulated_logging_time': 0.13416767120361328, 'global_step': 3771, 'preemption_count': 0}), (4529, {'train/accuracy': 0.9878915548324585, 'train/loss': 0.04272441565990448, 'train/mean_average_precision': 0.15881659907429796, 'validation/accuracy': 0.9850808382034302, 'validation/loss': 0.053034041076898575, 'validation/mean_average_precision': 0.14061921402636388, 'validation/num_examples': 43793, 'test/accuracy': 0.9841015338897705, 'test/loss': 0.05617416650056839, 'test/mean_average_precision': 0.13804253794928967, 'test/num_examples': 43793, 'score': 1452.5180218219757, 'total_duration': 2200.0856885910034, 'accumulated_submission_time': 1452.5180218219757, 'accumulated_eval_time': 747.282881975174, 'accumulated_logging_time': 0.16181612014770508, 'global_step': 4529, 'preemption_count': 0}), (5280, {'train/accuracy': 0.9880184531211853, 'train/loss': 0.04215643182396889, 'train/mean_average_precision': 0.1746008317249671, 'validation/accuracy': 0.9848993420600891, 'validation/loss': 0.051810652017593384, 'validation/mean_average_precision': 0.14567380892209825, 'validation/num_examples': 43793, 'test/accuracy': 0.9839562177658081, 'test/loss': 0.054508913308382034, 'test/mean_average_precision': 0.14418011181236254, 'test/num_examples': 43793, 'score': 1692.6388084888458, 'total_duration': 2549.740065097809, 'accumulated_submission_time': 1692.6388084888458, 'accumulated_eval_time': 856.7676658630371, 'accumulated_logging_time': 0.19035816192626953, 'global_step': 5280, 'preemption_count': 0}), (6032, {'train/accuracy': 0.9882644414901733, 'train/loss': 0.04114021733403206, 'train/mean_average_precision': 0.17060031998227768, 'validation/accuracy': 0.9852550029754639, 'validation/loss': 0.05112120509147644, 'validation/mean_average_precision': 0.15067751522048584, 'validation/num_examples': 43793, 'test/accuracy': 0.9843584895133972, 'test/loss': 0.053869158029556274, 'test/mean_average_precision': 0.14989309555874172, 'test/num_examples': 43793, 'score': 1932.7112345695496, 'total_duration': 2894.309236764908, 'accumulated_submission_time': 1932.7112345695496, 'accumulated_eval_time': 961.2165246009827, 'accumulated_logging_time': 0.21789264678955078, 'global_step': 6032, 'preemption_count': 0}), (6787, {'train/accuracy': 0.9882400631904602, 'train/loss': 0.04097194969654083, 'train/mean_average_precision': 0.18827192917678587, 'validation/accuracy': 0.9853280186653137, 'validation/loss': 0.05018118396401405, 'validation/mean_average_precision': 0.1575803344208639, 'validation/num_examples': 43793, 'test/accuracy': 0.9844317436218262, 'test/loss': 0.05277862772345543, 'test/mean_average_precision': 0.1560394778303248, 'test/num_examples': 43793, 'score': 2172.968799352646, 'total_duration': 3240.6831436157227, 'accumulated_submission_time': 2172.968799352646, 'accumulated_eval_time': 1067.2840287685394, 'accumulated_logging_time': 0.24642467498779297, 'global_step': 6787, 'preemption_count': 0}), (7541, {'train/accuracy': 0.9883561730384827, 'train/loss': 0.04039285331964493, 'train/mean_average_precision': 0.18365727493677925, 'validation/accuracy': 0.9855983853340149, 'validation/loss': 0.04950226470828056, 'validation/mean_average_precision': 0.1682590390890557, 'validation/num_examples': 43793, 'test/accuracy': 0.9845829606056213, 'test/loss': 0.05229341983795166, 'test/mean_average_precision': 0.16385811315475504, 'test/num_examples': 43793, 'score': 2412.973567724228, 'total_duration': 3586.542414188385, 'accumulated_submission_time': 2412.973567724228, 'accumulated_eval_time': 1173.091017961502, 'accumulated_logging_time': 0.2735772132873535, 'global_step': 7541, 'preemption_count': 0}), (8290, {'train/accuracy': 0.9883766174316406, 'train/loss': 0.040410179644823074, 'train/mean_average_precision': 0.18653734470643504, 'validation/accuracy': 0.9854250550270081, 'validation/loss': 0.050055332481861115, 'validation/mean_average_precision': 0.1676451153843103, 'validation/num_examples': 43793, 'test/accuracy': 0.9844376444816589, 'test/loss': 0.05293816328048706, 'test/mean_average_precision': 0.1628862646202331, 'test/num_examples': 43793, 'score': 2653.181145429611, 'total_duration': 3933.7839448451996, 'accumulated_submission_time': 2653.181145429611, 'accumulated_eval_time': 1280.0739023685455, 'accumulated_logging_time': 0.3043797016143799, 'global_step': 8290, 'preemption_count': 0}), (9043, {'train/accuracy': 0.9884548783302307, 'train/loss': 0.04007907584309578, 'train/mean_average_precision': 0.2033323095092237, 'validation/accuracy': 0.985439658164978, 'validation/loss': 0.049570098519325256, 'validation/mean_average_precision': 0.17320713215950936, 'validation/num_examples': 43793, 'test/accuracy': 0.9845362305641174, 'test/loss': 0.05233263969421387, 'test/mean_average_precision': 0.16886620021256832, 'test/num_examples': 43793, 'score': 2893.182389497757, 'total_duration': 4276.1212503910065, 'accumulated_submission_time': 2893.182389497757, 'accumulated_eval_time': 1382.3609237670898, 'accumulated_logging_time': 0.33313870429992676, 'global_step': 9043, 'preemption_count': 0}), (9800, {'train/accuracy': 0.9884865283966064, 'train/loss': 0.03967352584004402, 'train/mean_average_precision': 0.20581174027788657, 'validation/accuracy': 0.9856600761413574, 'validation/loss': 0.049448948353528976, 'validation/mean_average_precision': 0.17003765094657858, 'validation/num_examples': 43793, 'test/accuracy': 0.9847678542137146, 'test/loss': 0.05241949483752251, 'test/mean_average_precision': 0.17062260994694053, 'test/num_examples': 43793, 'score': 3133.3432919979095, 'total_duration': 4621.099257946014, 'accumulated_submission_time': 3133.3432919979095, 'accumulated_eval_time': 1487.1310422420502, 'accumulated_logging_time': 0.36079931259155273, 'global_step': 9800, 'preemption_count': 0}), (10563, {'train/accuracy': 0.9885579347610474, 'train/loss': 0.039471160620450974, 'train/mean_average_precision': 0.19193969492430596, 'validation/accuracy': 0.9855850338935852, 'validation/loss': 0.049318790435791016, 'validation/mean_average_precision': 0.1746983318028851, 'validation/num_examples': 43793, 'test/accuracy': 0.9847211241722107, 'test/loss': 0.05216002091765404, 'test/mean_average_precision': 0.1650606484348421, 'test/num_examples': 43793, 'score': 3373.4267842769623, 'total_duration': 4968.480097293854, 'accumulated_submission_time': 3373.4267842769623, 'accumulated_eval_time': 1594.379231929779, 'accumulated_logging_time': 0.39026618003845215, 'global_step': 10563, 'preemption_count': 0}), (11336, {'train/accuracy': 0.9885560870170593, 'train/loss': 0.039330873638391495, 'train/mean_average_precision': 0.2063082084535478, 'validation/accuracy': 0.9856479167938232, 'validation/loss': 0.04939792677760124, 'validation/mean_average_precision': 0.18426530556965534, 'validation/num_examples': 43793, 'test/accuracy': 0.9846722483634949, 'test/loss': 0.05250081792473793, 'test/mean_average_precision': 0.1774870521222304, 'test/num_examples': 43793, 'score': 3613.5157945156097, 'total_duration': 5310.309653043747, 'accumulated_submission_time': 3613.5157945156097, 'accumulated_eval_time': 1696.071894645691, 'accumulated_logging_time': 0.41802167892456055, 'global_step': 11336, 'preemption_count': 0}), (12103, {'train/accuracy': 0.9886201024055481, 'train/loss': 0.03922673314809799, 'train/mean_average_precision': 0.2103504437889654, 'validation/accuracy': 0.9856515526771545, 'validation/loss': 0.048678528517484665, 'validation/mean_average_precision': 0.17721198220892478, 'validation/num_examples': 43793, 'test/accuracy': 0.9847754836082458, 'test/loss': 0.05141591280698776, 'test/mean_average_precision': 0.17533691284810193, 'test/num_examples': 43793, 'score': 3853.6414008140564, 'total_duration': 5655.700120210648, 'accumulated_submission_time': 3853.6414008140564, 'accumulated_eval_time': 1801.2887377738953, 'accumulated_logging_time': 0.44663023948669434, 'global_step': 12103, 'preemption_count': 0}), (12866, {'train/accuracy': 0.988673210144043, 'train/loss': 0.03916774317622185, 'train/mean_average_precision': 0.21065034903441726, 'validation/accuracy': 0.9855707883834839, 'validation/loss': 0.04928121343255043, 'validation/mean_average_precision': 0.1662361976664409, 'validation/num_examples': 43793, 'test/accuracy': 0.9846440553665161, 'test/loss': 0.052109234035015106, 'test/mean_average_precision': 0.16405105374278703, 'test/num_examples': 43793, 'score': 4093.658228158951, 'total_duration': 5996.82173871994, 'accumulated_submission_time': 4093.658228158951, 'accumulated_eval_time': 1902.3464109897614, 'accumulated_logging_time': 0.4743480682373047, 'global_step': 12866, 'preemption_count': 0}), (13629, {'train/accuracy': 0.98865807056427, 'train/loss': 0.03865594416856766, 'train/mean_average_precision': 0.21663015439800132, 'validation/accuracy': 0.9856945872306824, 'validation/loss': 0.049088768661022186, 'validation/mean_average_precision': 0.1791409499424729, 'validation/num_examples': 43793, 'test/accuracy': 0.9846861958503723, 'test/loss': 0.05202307179570198, 'test/mean_average_precision': 0.17551811385025048, 'test/num_examples': 43793, 'score': 4333.704358816147, 'total_duration': 6344.965814590454, 'accumulated_submission_time': 4333.704358816147, 'accumulated_eval_time': 2010.3936491012573, 'accumulated_logging_time': 0.5054383277893066, 'global_step': 13629, 'preemption_count': 0}), (14389, {'train/accuracy': 0.9884803295135498, 'train/loss': 0.04013616219162941, 'train/mean_average_precision': 0.19236986708748655, 'validation/accuracy': 0.9853804111480713, 'validation/loss': 0.049872953444719315, 'validation/mean_average_precision': 0.17261028306643447, 'validation/num_examples': 43793, 'test/accuracy': 0.9843934178352356, 'test/loss': 0.05279448628425598, 'test/mean_average_precision': 0.16882090915140036, 'test/num_examples': 43793, 'score': 4573.699645042419, 'total_duration': 6690.735986948013, 'accumulated_submission_time': 4573.699645042419, 'accumulated_eval_time': 2116.118327856064, 'accumulated_logging_time': 0.5356552600860596, 'global_step': 14389, 'preemption_count': 0}), (15152, {'train/accuracy': 0.9884754419326782, 'train/loss': 0.03947283327579498, 'train/mean_average_precision': 0.20115746473315604, 'validation/accuracy': 0.9856117963790894, 'validation/loss': 0.049600694328546524, 'validation/mean_average_precision': 0.1733149277449493, 'validation/num_examples': 43793, 'test/accuracy': 0.984649121761322, 'test/loss': 0.05258084833621979, 'test/mean_average_precision': 0.17377426183102065, 'test/num_examples': 43793, 'score': 4813.877220630646, 'total_duration': 7041.50847029686, 'accumulated_submission_time': 4813.877220630646, 'accumulated_eval_time': 2226.663529396057, 'accumulated_logging_time': 0.5656332969665527, 'global_step': 15152, 'preemption_count': 0}), (15903, {'train/accuracy': 0.9885097742080688, 'train/loss': 0.03935691714286804, 'train/mean_average_precision': 0.20351287691145448, 'validation/accuracy': 0.9857429265975952, 'validation/loss': 0.04877891391515732, 'validation/mean_average_precision': 0.18361947724181774, 'validation/num_examples': 43793, 'test/accuracy': 0.9847834706306458, 'test/loss': 0.051879771053791046, 'test/mean_average_precision': 0.1804877903662326, 'test/num_examples': 43793, 'score': 5054.121758937836, 'total_duration': 7382.07443356514, 'accumulated_submission_time': 5054.121758937836, 'accumulated_eval_time': 2326.9300923347473, 'accumulated_logging_time': 0.5987019538879395, 'global_step': 15903, 'preemption_count': 0}), (16665, {'train/accuracy': 0.9887725710868835, 'train/loss': 0.038911666721105576, 'train/mean_average_precision': 0.20659298565540096, 'validation/accuracy': 0.9857416749000549, 'validation/loss': 0.0486304834485054, 'validation/mean_average_precision': 0.18239847980590773, 'validation/num_examples': 43793, 'test/accuracy': 0.9848668575286865, 'test/loss': 0.05157385393977165, 'test/mean_average_precision': 0.17525405956295786, 'test/num_examples': 43793, 'score': 5294.265455007553, 'total_duration': 7729.107240438461, 'accumulated_submission_time': 5294.265455007553, 'accumulated_eval_time': 2433.7707934379578, 'accumulated_logging_time': 0.6277382373809814, 'global_step': 16665, 'preemption_count': 0}), (17425, {'train/accuracy': 0.9885989427566528, 'train/loss': 0.03919602930545807, 'train/mean_average_precision': 0.20203607163677134, 'validation/accuracy': 0.9855594038963318, 'validation/loss': 0.049065567553043365, 'validation/mean_average_precision': 0.1677639326494566, 'validation/num_examples': 43793, 'test/accuracy': 0.9847135543823242, 'test/loss': 0.052009761333465576, 'test/mean_average_precision': 0.1743933827146704, 'test/num_examples': 43793, 'score': 5534.256805181503, 'total_duration': 8076.462526798248, 'accumulated_submission_time': 5534.256805181503, 'accumulated_eval_time': 2541.077990293503, 'accumulated_logging_time': 0.6633737087249756, 'global_step': 17425, 'preemption_count': 0}), (18183, {'train/accuracy': 0.9885631799697876, 'train/loss': 0.03920503705739975, 'train/mean_average_precision': 0.21115823829716834, 'validation/accuracy': 0.9856604933738708, 'validation/loss': 0.049062520265579224, 'validation/mean_average_precision': 0.1755662687161741, 'validation/num_examples': 43793, 'test/accuracy': 0.9847097396850586, 'test/loss': 0.051849689334630966, 'test/mean_average_precision': 0.17605731491423418, 'test/num_examples': 43793, 'score': 5774.4022517204285, 'total_duration': 8422.579911470413, 'accumulated_submission_time': 5774.4022517204285, 'accumulated_eval_time': 2646.9991524219513, 'accumulated_logging_time': 0.6947915554046631, 'global_step': 18183, 'preemption_count': 0}), (18935, {'train/accuracy': 0.9886395931243896, 'train/loss': 0.0389874204993248, 'train/mean_average_precision': 0.20706087524820685, 'validation/accuracy': 0.9856272339820862, 'validation/loss': 0.049088090658187866, 'validation/mean_average_precision': 0.17807781206289738, 'validation/num_examples': 43793, 'test/accuracy': 0.9846933484077454, 'test/loss': 0.051995303481817245, 'test/mean_average_precision': 0.1730764760968453, 'test/num_examples': 43793, 'score': 6014.416128873825, 'total_duration': 8767.328412532806, 'accumulated_submission_time': 6014.416128873825, 'accumulated_eval_time': 2751.677888393402, 'accumulated_logging_time': 0.729445219039917, 'global_step': 18935, 'preemption_count': 0}), (19686, {'train/accuracy': 0.9886992573738098, 'train/loss': 0.038554951548576355, 'train/mean_average_precision': 0.21994364760224747, 'validation/accuracy': 0.9856789708137512, 'validation/loss': 0.04852884262800217, 'validation/mean_average_precision': 0.1826200142812672, 'validation/num_examples': 43793, 'test/accuracy': 0.984761118888855, 'test/loss': 0.05136070027947426, 'test/mean_average_precision': 0.1776437460243953, 'test/num_examples': 43793, 'score': 6254.527894496918, 'total_duration': 9108.249343633652, 'accumulated_submission_time': 6254.527894496918, 'accumulated_eval_time': 2852.4364881515503, 'accumulated_logging_time': 0.760200023651123, 'global_step': 19686, 'preemption_count': 0}), (20446, {'train/accuracy': 0.9888023138046265, 'train/loss': 0.038551561534404755, 'train/mean_average_precision': 0.21080826995503685, 'validation/accuracy': 0.9857754111289978, 'validation/loss': 0.048722174018621445, 'validation/mean_average_precision': 0.1826559964014653, 'validation/num_examples': 43793, 'test/accuracy': 0.9848723411560059, 'test/loss': 0.0517776682972908, 'test/mean_average_precision': 0.17850474728096502, 'test/num_examples': 43793, 'score': 6494.588192939758, 'total_duration': 9454.533122062683, 'accumulated_submission_time': 6494.588192939758, 'accumulated_eval_time': 2958.607923746109, 'accumulated_logging_time': 0.7917554378509521, 'global_step': 20446, 'preemption_count': 0}), (21201, {'train/accuracy': 0.988953709602356, 'train/loss': 0.037700071930885315, 'train/mean_average_precision': 0.230811797165299, 'validation/accuracy': 0.9858095049858093, 'validation/loss': 0.04827570170164108, 'validation/mean_average_precision': 0.187712481522891, 'validation/num_examples': 43793, 'test/accuracy': 0.9848698377609253, 'test/loss': 0.05133536458015442, 'test/mean_average_precision': 0.18289813381186903, 'test/num_examples': 43793, 'score': 6734.835546016693, 'total_duration': 9798.936812639236, 'accumulated_submission_time': 6734.835546016693, 'accumulated_eval_time': 3062.7118458747864, 'accumulated_logging_time': 0.8235526084899902, 'global_step': 21201, 'preemption_count': 0}), (21957, {'train/accuracy': 0.9888525605201721, 'train/loss': 0.03817529231309891, 'train/mean_average_precision': 0.21947451632087114, 'validation/accuracy': 0.985712468624115, 'validation/loss': 0.048748038709163666, 'validation/mean_average_precision': 0.18646979911386335, 'validation/num_examples': 43793, 'test/accuracy': 0.9847274422645569, 'test/loss': 0.05180920287966728, 'test/mean_average_precision': 0.1798832976796487, 'test/num_examples': 43793, 'score': 6975.096371412277, 'total_duration': 10141.745084524155, 'accumulated_submission_time': 6975.096371412277, 'accumulated_eval_time': 3165.2085721492767, 'accumulated_logging_time': 0.85434889793396, 'global_step': 21957, 'preemption_count': 0}), (22721, {'train/accuracy': 0.9887853860855103, 'train/loss': 0.03829365223646164, 'train/mean_average_precision': 0.22578721979965902, 'validation/accuracy': 0.9858614802360535, 'validation/loss': 0.048403218388557434, 'validation/mean_average_precision': 0.18717810663629783, 'validation/num_examples': 43793, 'test/accuracy': 0.9849422574043274, 'test/loss': 0.051137737929821014, 'test/mean_average_precision': 0.18069880982036599, 'test/num_examples': 43793, 'score': 7215.198048114777, 'total_duration': 10487.473489522934, 'accumulated_submission_time': 7215.198048114777, 'accumulated_eval_time': 3270.7840468883514, 'accumulated_logging_time': 0.8855433464050293, 'global_step': 22721, 'preemption_count': 0}), (23481, {'train/accuracy': 0.9887216091156006, 'train/loss': 0.0385473296046257, 'train/mean_average_precision': 0.21614784740970006, 'validation/accuracy': 0.9858009815216064, 'validation/loss': 0.048311781138181686, 'validation/mean_average_precision': 0.18691507637863614, 'validation/num_examples': 43793, 'test/accuracy': 0.9848819971084595, 'test/loss': 0.051236700266599655, 'test/mean_average_precision': 0.18170816291909414, 'test/num_examples': 43793, 'score': 7455.2598967552185, 'total_duration': 10834.261020421982, 'accumulated_submission_time': 7455.2598967552185, 'accumulated_eval_time': 3377.4589943885803, 'accumulated_logging_time': 0.9166724681854248, 'global_step': 23481, 'preemption_count': 0}), (24238, {'train/accuracy': 0.9888251423835754, 'train/loss': 0.038296639919281006, 'train/mean_average_precision': 0.21755191098602358, 'validation/accuracy': 0.9858407378196716, 'validation/loss': 0.04817565158009529, 'validation/mean_average_precision': 0.19022482647230404, 'validation/num_examples': 43793, 'test/accuracy': 0.9849464893341064, 'test/loss': 0.051013898104429245, 'test/mean_average_precision': 0.1909638553261999, 'test/num_examples': 43793, 'score': 7695.373823165894, 'total_duration': 11175.742981672287, 'accumulated_submission_time': 7695.373823165894, 'accumulated_eval_time': 3478.77467918396, 'accumulated_logging_time': 0.9493060111999512, 'global_step': 24238, 'preemption_count': 0}), (24996, {'train/accuracy': 0.9887945652008057, 'train/loss': 0.03828536346554756, 'train/mean_average_precision': 0.22411569355984684, 'validation/accuracy': 0.9858716130256653, 'validation/loss': 0.048033662140369415, 'validation/mean_average_precision': 0.19200558721430855, 'validation/num_examples': 43793, 'test/accuracy': 0.9849563837051392, 'test/loss': 0.05095234513282776, 'test/mean_average_precision': 0.18763869259624225, 'test/num_examples': 43793, 'score': 7935.566670894623, 'total_duration': 11518.674816608429, 'accumulated_submission_time': 7935.566670894623, 'accumulated_eval_time': 3581.462842464447, 'accumulated_logging_time': 0.980283260345459, 'global_step': 24996, 'preemption_count': 0}), (25759, {'train/accuracy': 0.9889598488807678, 'train/loss': 0.038259729743003845, 'train/mean_average_precision': 0.21395748378791343, 'validation/accuracy': 0.9858368635177612, 'validation/loss': 0.04825982078909874, 'validation/mean_average_precision': 0.1868899929087874, 'validation/num_examples': 43793, 'test/accuracy': 0.9849376082420349, 'test/loss': 0.05120475962758064, 'test/mean_average_precision': 0.17947322257862497, 'test/num_examples': 43793, 'score': 8175.762567043304, 'total_duration': 11864.60201716423, 'accumulated_submission_time': 8175.762567043304, 'accumulated_eval_time': 3687.1374304294586, 'accumulated_logging_time': 1.0166258811950684, 'global_step': 25759, 'preemption_count': 0}), (26525, {'train/accuracy': 0.9887128472328186, 'train/loss': 0.03872840106487274, 'train/mean_average_precision': 0.212833312132566, 'validation/accuracy': 0.9856792092323303, 'validation/loss': 0.048808928579092026, 'validation/mean_average_precision': 0.1811582477184639, 'validation/num_examples': 43793, 'test/accuracy': 0.9847586154937744, 'test/loss': 0.05191972106695175, 'test/mean_average_precision': 0.17592324247629057, 'test/num_examples': 43793, 'score': 8415.960547924042, 'total_duration': 12208.59658241272, 'accumulated_submission_time': 8415.960547924042, 'accumulated_eval_time': 3790.8824088573456, 'accumulated_logging_time': 1.0475599765777588, 'global_step': 26525, 'preemption_count': 0}), (27288, {'train/accuracy': 0.9887645244598389, 'train/loss': 0.03830304741859436, 'train/mean_average_precision': 0.2247528497563965, 'validation/accuracy': 0.9856621623039246, 'validation/loss': 0.0485771968960762, 'validation/mean_average_precision': 0.18338578642084308, 'validation/num_examples': 43793, 'test/accuracy': 0.9847981929779053, 'test/loss': 0.05146558955311775, 'test/mean_average_precision': 0.17754872683397158, 'test/num_examples': 43793, 'score': 8655.956233978271, 'total_duration': 12557.534547328949, 'accumulated_submission_time': 8655.956233978271, 'accumulated_eval_time': 3899.773766040802, 'accumulated_logging_time': 1.078413486480713, 'global_step': 27288, 'preemption_count': 0}), (28035, {'train/accuracy': 0.9887883067131042, 'train/loss': 0.03838672488927841, 'train/mean_average_precision': 0.2266030029702564, 'validation/accuracy': 0.9857709407806396, 'validation/loss': 0.0483795665204525, 'validation/mean_average_precision': 0.18527124744371645, 'validation/num_examples': 43793, 'test/accuracy': 0.9848756790161133, 'test/loss': 0.05123374983668327, 'test/mean_average_precision': 0.18248757231048227, 'test/num_examples': 43793, 'score': 8896.014830827713, 'total_duration': 12903.236117601395, 'accumulated_submission_time': 8896.014830827713, 'accumulated_eval_time': 4005.3592603206635, 'accumulated_logging_time': 1.1128215789794922, 'global_step': 28035, 'preemption_count': 0}), (28789, {'train/accuracy': 0.9888795018196106, 'train/loss': 0.037977900356054306, 'train/mean_average_precision': 0.22512584744264968, 'validation/accuracy': 0.9859036803245544, 'validation/loss': 0.04835958778858185, 'validation/mean_average_precision': 0.19535043027927054, 'validation/num_examples': 43793, 'test/accuracy': 0.9850125908851624, 'test/loss': 0.05125235766172409, 'test/mean_average_precision': 0.18447898547369126, 'test/num_examples': 43793, 'score': 9136.247852563858, 'total_duration': 13247.099108457565, 'accumulated_submission_time': 9136.247852563858, 'accumulated_eval_time': 4108.936638832092, 'accumulated_logging_time': 1.145425796508789, 'global_step': 28789, 'preemption_count': 0}), (29556, {'train/accuracy': 0.9890247583389282, 'train/loss': 0.037338387221097946, 'train/mean_average_precision': 0.2374429432051383, 'validation/accuracy': 0.9858587980270386, 'validation/loss': 0.04812519997358322, 'validation/mean_average_precision': 0.18958315642744727, 'validation/num_examples': 43793, 'test/accuracy': 0.9849578142166138, 'test/loss': 0.05109774321317673, 'test/mean_average_precision': 0.18668369635083729, 'test/num_examples': 43793, 'score': 9376.250790834427, 'total_duration': 13587.735835075378, 'accumulated_submission_time': 9376.250790834427, 'accumulated_eval_time': 4209.517950057983, 'accumulated_logging_time': 1.1778452396392822, 'global_step': 29556, 'preemption_count': 0}), (30318, {'train/accuracy': 0.9889353513717651, 'train/loss': 0.03758455440402031, 'train/mean_average_precision': 0.24313854729822296, 'validation/accuracy': 0.9859227538108826, 'validation/loss': 0.04829683527350426, 'validation/mean_average_precision': 0.1986930130203873, 'validation/num_examples': 43793, 'test/accuracy': 0.9849645495414734, 'test/loss': 0.05136340111494064, 'test/mean_average_precision': 0.1865411181259032, 'test/num_examples': 43793, 'score': 9616.483031272888, 'total_duration': 13931.94085597992, 'accumulated_submission_time': 9616.483031272888, 'accumulated_eval_time': 4313.439270734787, 'accumulated_logging_time': 1.2094299793243408, 'global_step': 30318, 'preemption_count': 0}), (31076, {'train/accuracy': 0.9885992407798767, 'train/loss': 0.03864162787795067, 'train/mean_average_precision': 0.22234282131653765, 'validation/accuracy': 0.9853118062019348, 'validation/loss': 0.0490008220076561, 'validation/mean_average_precision': 0.1907836688823532, 'validation/num_examples': 43793, 'test/accuracy': 0.9844574332237244, 'test/loss': 0.05172889307141304, 'test/mean_average_precision': 0.18330393134426415, 'test/num_examples': 43793, 'score': 9856.629340171814, 'total_duration': 14277.503986597061, 'accumulated_submission_time': 9856.629340171814, 'accumulated_eval_time': 4418.8034727573395, 'accumulated_logging_time': 1.241889476776123, 'global_step': 31076, 'preemption_count': 0}), (31840, {'train/accuracy': 0.9889352321624756, 'train/loss': 0.03788282349705696, 'train/mean_average_precision': 0.23030380929346733, 'validation/accuracy': 0.9858736395835876, 'validation/loss': 0.04784974083304405, 'validation/mean_average_precision': 0.19357178850933154, 'validation/num_examples': 43793, 'test/accuracy': 0.9849127531051636, 'test/loss': 0.05064694955945015, 'test/mean_average_precision': 0.1879387186659543, 'test/num_examples': 43793, 'score': 10096.608315229416, 'total_duration': 14625.904658794403, 'accumulated_submission_time': 10096.608315229416, 'accumulated_eval_time': 4527.166420221329, 'accumulated_logging_time': 1.280625581741333, 'global_step': 31840, 'preemption_count': 0}), (32593, {'train/accuracy': 0.9887871742248535, 'train/loss': 0.038394100964069366, 'train/mean_average_precision': 0.22740842182914375, 'validation/accuracy': 0.9858419895172119, 'validation/loss': 0.04838836193084717, 'validation/mean_average_precision': 0.1839510717008973, 'validation/num_examples': 43793, 'test/accuracy': 0.9849587082862854, 'test/loss': 0.051338449120521545, 'test/mean_average_precision': 0.18290179246589217, 'test/num_examples': 43793, 'score': 10336.679065942764, 'total_duration': 14973.201986551285, 'accumulated_submission_time': 10336.679065942764, 'accumulated_eval_time': 4634.336527824402, 'accumulated_logging_time': 1.3167879581451416, 'global_step': 32593, 'preemption_count': 0}), (33347, {'train/accuracy': 0.9890311360359192, 'train/loss': 0.03757062554359436, 'train/mean_average_precision': 0.23133906465316256, 'validation/accuracy': 0.9859755039215088, 'validation/loss': 0.048174526542425156, 'validation/mean_average_precision': 0.19412180114186225, 'validation/num_examples': 43793, 'test/accuracy': 0.9849936366081238, 'test/loss': 0.051304273307323456, 'test/mean_average_precision': 0.18574911694512755, 'test/num_examples': 43793, 'score': 10576.746666193008, 'total_duration': 15316.488739967346, 'accumulated_submission_time': 10576.746666193008, 'accumulated_eval_time': 4737.503067016602, 'accumulated_logging_time': 1.3489949703216553, 'global_step': 33347, 'preemption_count': 0}), (34102, {'train/accuracy': 0.9890766143798828, 'train/loss': 0.0374147966504097, 'train/mean_average_precision': 0.22170499395294838, 'validation/accuracy': 0.9859133958816528, 'validation/loss': 0.047539807856082916, 'validation/mean_average_precision': 0.19530376395857477, 'validation/num_examples': 43793, 'test/accuracy': 0.9850218892097473, 'test/loss': 0.05039689689874649, 'test/mean_average_precision': 0.19320288726833304, 'test/num_examples': 43793, 'score': 10817.016341924667, 'total_duration': 15663.953579187393, 'accumulated_submission_time': 10817.016341924667, 'accumulated_eval_time': 4844.645184755325, 'accumulated_logging_time': 1.3820130825042725, 'global_step': 34102, 'preemption_count': 0}), (34861, {'train/accuracy': 0.9888927936553955, 'train/loss': 0.037979718297719955, 'train/mean_average_precision': 0.23677103873994235, 'validation/accuracy': 0.9859304428100586, 'validation/loss': 0.048315953463315964, 'validation/mean_average_precision': 0.1910359411764158, 'validation/num_examples': 43793, 'test/accuracy': 0.9849645495414734, 'test/loss': 0.05137333273887634, 'test/mean_average_precision': 0.1845622175386213, 'test/num_examples': 43793, 'score': 11057.10607290268, 'total_duration': 16004.42927980423, 'accumulated_submission_time': 11057.10607290268, 'accumulated_eval_time': 4944.97594666481, 'accumulated_logging_time': 1.415419578552246, 'global_step': 34861, 'preemption_count': 0}), (35619, {'train/accuracy': 0.9888798594474792, 'train/loss': 0.03776901960372925, 'train/mean_average_precision': 0.24197418865759052, 'validation/accuracy': 0.9859288334846497, 'validation/loss': 0.047665756195783615, 'validation/mean_average_precision': 0.19524556600481408, 'validation/num_examples': 43793, 'test/accuracy': 0.9850631356239319, 'test/loss': 0.05058734491467476, 'test/mean_average_precision': 0.1908041635108264, 'test/num_examples': 43793, 'score': 11297.285032272339, 'total_duration': 16347.916496515274, 'accumulated_submission_time': 11297.285032272339, 'accumulated_eval_time': 5048.230092048645, 'accumulated_logging_time': 1.4496972560882568, 'global_step': 35619, 'preemption_count': 0}), (36364, {'train/accuracy': 0.9889218807220459, 'train/loss': 0.037400249391794205, 'train/mean_average_precision': 0.23931580360512697, 'validation/accuracy': 0.9859093427658081, 'validation/loss': 0.04826940596103668, 'validation/mean_average_precision': 0.1960000096477773, 'validation/num_examples': 43793, 'test/accuracy': 0.9849388599395752, 'test/loss': 0.05115370452404022, 'test/mean_average_precision': 0.18467875618708884, 'test/num_examples': 43793, 'score': 11537.255279302597, 'total_duration': 16690.80647611618, 'accumulated_submission_time': 11537.255279302597, 'accumulated_eval_time': 5151.097851753235, 'accumulated_logging_time': 1.4821081161499023, 'global_step': 36364, 'preemption_count': 0}), (37122, {'train/accuracy': 0.9891963601112366, 'train/loss': 0.03668893128633499, 'train/mean_average_precision': 0.2415403166293018, 'validation/accuracy': 0.9859312772750854, 'validation/loss': 0.04741903394460678, 'validation/mean_average_precision': 0.19278137679949098, 'validation/num_examples': 43793, 'test/accuracy': 0.9850353598594666, 'test/loss': 0.05028887465596199, 'test/mean_average_precision': 0.185580799291037, 'test/num_examples': 43793, 'score': 11777.304328680038, 'total_duration': 17032.784563302994, 'accumulated_submission_time': 11777.304328680038, 'accumulated_eval_time': 5252.974688053131, 'accumulated_logging_time': 1.514685869216919, 'global_step': 37122, 'preemption_count': 0}), (37884, {'train/accuracy': 0.9892786145210266, 'train/loss': 0.036347683519124985, 'train/mean_average_precision': 0.2639222874446258, 'validation/accuracy': 0.9859499335289001, 'validation/loss': 0.04764634370803833, 'validation/mean_average_precision': 0.1962621683676298, 'validation/num_examples': 43793, 'test/accuracy': 0.9851309657096863, 'test/loss': 0.05036367475986481, 'test/mean_average_precision': 0.19165741451932264, 'test/num_examples': 43793, 'score': 12017.266623020172, 'total_duration': 17376.75620341301, 'accumulated_submission_time': 12017.266623020172, 'accumulated_eval_time': 5356.93186378479, 'accumulated_logging_time': 1.5475928783416748, 'global_step': 37884, 'preemption_count': 0}), (38639, {'train/accuracy': 0.9892585873603821, 'train/loss': 0.0369451642036438, 'train/mean_average_precision': 0.24027067192509202, 'validation/accuracy': 0.9859738945960999, 'validation/loss': 0.04767146706581116, 'validation/mean_average_precision': 0.1958262650743369, 'validation/num_examples': 43793, 'test/accuracy': 0.9850825071334839, 'test/loss': 0.05038914829492569, 'test/mean_average_precision': 0.19137357498871682, 'test/num_examples': 43793, 'score': 12257.309017419815, 'total_duration': 17720.39829516411, 'accumulated_submission_time': 12257.309017419815, 'accumulated_eval_time': 5460.476921081543, 'accumulated_logging_time': 1.582322359085083, 'global_step': 38639, 'preemption_count': 0}), (39400, {'train/accuracy': 0.9892501831054688, 'train/loss': 0.036724213510751724, 'train/mean_average_precision': 0.2511990354285915, 'validation/accuracy': 0.9859767556190491, 'validation/loss': 0.04746464267373085, 'validation/mean_average_precision': 0.2000518758374433, 'validation/num_examples': 43793, 'test/accuracy': 0.9850707054138184, 'test/loss': 0.05035984143614769, 'test/mean_average_precision': 0.19696708578381983, 'test/num_examples': 43793, 'score': 12497.395084142685, 'total_duration': 18064.71885228157, 'accumulated_submission_time': 12497.395084142685, 'accumulated_eval_time': 5564.655128240585, 'accumulated_logging_time': 1.6185622215270996, 'global_step': 39400, 'preemption_count': 0}), (40142, {'train/accuracy': 0.9890367984771729, 'train/loss': 0.037310272455215454, 'train/mean_average_precision': 0.23788077565488858, 'validation/accuracy': 0.9860019087791443, 'validation/loss': 0.047450996935367584, 'validation/mean_average_precision': 0.19823682262756082, 'validation/num_examples': 43793, 'test/accuracy': 0.9850610494613647, 'test/loss': 0.05041808262467384, 'test/mean_average_precision': 0.19414910349240583, 'test/num_examples': 43793, 'score': 12737.522000074387, 'total_duration': 18408.12707543373, 'accumulated_submission_time': 12737.522000074387, 'accumulated_eval_time': 5667.876608371735, 'accumulated_logging_time': 1.6552250385284424, 'global_step': 40142, 'preemption_count': 0}), (40902, {'train/accuracy': 0.9892304539680481, 'train/loss': 0.036914996802806854, 'train/mean_average_precision': 0.24487599262648516, 'validation/accuracy': 0.9860546588897705, 'validation/loss': 0.047015801072120667, 'validation/mean_average_precision': 0.19984733758230472, 'validation/num_examples': 43793, 'test/accuracy': 0.985190749168396, 'test/loss': 0.049587149173021317, 'test/mean_average_precision': 0.1979586610062238, 'test/num_examples': 43793, 'score': 12977.745255231857, 'total_duration': 18755.449722766876, 'accumulated_submission_time': 12977.745255231857, 'accumulated_eval_time': 5774.92215013504, 'accumulated_logging_time': 1.6892304420471191, 'global_step': 40902, 'preemption_count': 0}), (41664, {'train/accuracy': 0.9892215132713318, 'train/loss': 0.03669707104563713, 'train/mean_average_precision': 0.25355743928034935, 'validation/accuracy': 0.9860948920249939, 'validation/loss': 0.04706144705414772, 'validation/mean_average_precision': 0.19683720692700446, 'validation/num_examples': 43793, 'test/accuracy': 0.9852290749549866, 'test/loss': 0.049860768020153046, 'test/mean_average_precision': 0.1985576710004122, 'test/num_examples': 43793, 'score': 13217.90337395668, 'total_duration': 19096.59023118019, 'accumulated_submission_time': 13217.90337395668, 'accumulated_eval_time': 5875.849026441574, 'accumulated_logging_time': 1.7248921394348145, 'global_step': 41664, 'preemption_count': 0}), (42422, {'train/accuracy': 0.9893390536308289, 'train/loss': 0.036265041679143906, 'train/mean_average_precision': 0.2505960639193763, 'validation/accuracy': 0.9860355854034424, 'validation/loss': 0.046855002641677856, 'validation/mean_average_precision': 0.21005827711503167, 'validation/num_examples': 43793, 'test/accuracy': 0.9852202534675598, 'test/loss': 0.049669425934553146, 'test/mean_average_precision': 0.20277958623412873, 'test/num_examples': 43793, 'score': 13457.914669513702, 'total_duration': 19439.718099355698, 'accumulated_submission_time': 13457.914669513702, 'accumulated_eval_time': 5978.910071611404, 'accumulated_logging_time': 1.7608411312103271, 'global_step': 42422, 'preemption_count': 0}), (43185, {'train/accuracy': 0.9892221689224243, 'train/loss': 0.03665018826723099, 'train/mean_average_precision': 0.2538185712336535, 'validation/accuracy': 0.9859641790390015, 'validation/loss': 0.04728439077734947, 'validation/mean_average_precision': 0.20269953349993863, 'validation/num_examples': 43793, 'test/accuracy': 0.9850420951843262, 'test/loss': 0.05029060319066048, 'test/mean_average_precision': 0.18731922219462746, 'test/num_examples': 43793, 'score': 13697.976322174072, 'total_duration': 19782.68240070343, 'accumulated_submission_time': 13697.976322174072, 'accumulated_eval_time': 6081.750519990921, 'accumulated_logging_time': 1.8031601905822754, 'global_step': 43185, 'preemption_count': 0}), (43929, {'train/accuracy': 0.9891306161880493, 'train/loss': 0.03650067374110222, 'train/mean_average_precision': 0.2575394367368832, 'validation/accuracy': 0.9861127138137817, 'validation/loss': 0.046978097409009933, 'validation/mean_average_precision': 0.2011696650770483, 'validation/num_examples': 43793, 'test/accuracy': 0.9852215051651001, 'test/loss': 0.04968126118183136, 'test/mean_average_precision': 0.19707071287609873, 'test/num_examples': 43793, 'score': 13938.120023965836, 'total_duration': 20122.901830911636, 'accumulated_submission_time': 13938.120023965836, 'accumulated_eval_time': 6181.76887345314, 'accumulated_logging_time': 1.839141607284546, 'global_step': 43929, 'preemption_count': 0}), (44683, {'train/accuracy': 0.9893518686294556, 'train/loss': 0.03623374551534653, 'train/mean_average_precision': 0.25651968428844657, 'validation/accuracy': 0.9861443638801575, 'validation/loss': 0.04691162332892418, 'validation/mean_average_precision': 0.20618330719853067, 'validation/num_examples': 43793, 'test/accuracy': 0.9852408766746521, 'test/loss': 0.04987991228699684, 'test/mean_average_precision': 0.1917108982459069, 'test/num_examples': 43793, 'score': 14178.33187842369, 'total_duration': 20472.786482810974, 'accumulated_submission_time': 14178.33187842369, 'accumulated_eval_time': 6291.385230064392, 'accumulated_logging_time': 1.8742420673370361, 'global_step': 44683, 'preemption_count': 0}), (45441, {'train/accuracy': 0.9894936084747314, 'train/loss': 0.03576960042119026, 'train/mean_average_precision': 0.2657211170336563, 'validation/accuracy': 0.9861301779747009, 'validation/loss': 0.04697982221841812, 'validation/mean_average_precision': 0.2082862425106177, 'validation/num_examples': 43793, 'test/accuracy': 0.9853171110153198, 'test/loss': 0.049627721309661865, 'test/mean_average_precision': 0.19841860972032793, 'test/num_examples': 43793, 'score': 14418.279221534729, 'total_duration': 20810.643618822098, 'accumulated_submission_time': 14418.279221534729, 'accumulated_eval_time': 6389.235752105713, 'accumulated_logging_time': 1.9128201007843018, 'global_step': 45441, 'preemption_count': 0}), (46209, {'train/accuracy': 0.9894861578941345, 'train/loss': 0.03554147481918335, 'train/mean_average_precision': 0.26567322678175076, 'validation/accuracy': 0.9860729575157166, 'validation/loss': 0.04676460102200508, 'validation/mean_average_precision': 0.20342372193775465, 'validation/num_examples': 43793, 'test/accuracy': 0.985183596611023, 'test/loss': 0.049697089940309525, 'test/mean_average_precision': 0.19757847741367224, 'test/num_examples': 43793, 'score': 14658.41047000885, 'total_duration': 21156.919238328934, 'accumulated_submission_time': 14658.41047000885, 'accumulated_eval_time': 6495.324959516525, 'accumulated_logging_time': 1.9479403495788574, 'global_step': 46209, 'preemption_count': 0}), (46976, {'train/accuracy': 0.9894858598709106, 'train/loss': 0.03554527461528778, 'train/mean_average_precision': 0.27140540957010273, 'validation/accuracy': 0.9861480593681335, 'validation/loss': 0.04689713567495346, 'validation/mean_average_precision': 0.20850976515983013, 'validation/num_examples': 43793, 'test/accuracy': 0.9852185845375061, 'test/loss': 0.049553755670785904, 'test/mean_average_precision': 0.19979763567894623, 'test/num_examples': 43793, 'score': 14898.544739961624, 'total_duration': 21501.015640735626, 'accumulated_submission_time': 14898.544739961624, 'accumulated_eval_time': 6599.232083559036, 'accumulated_logging_time': 1.9829089641571045, 'global_step': 46976, 'preemption_count': 0}), (47733, {'train/accuracy': 0.9893301129341125, 'train/loss': 0.03604253754019737, 'train/mean_average_precision': 0.2642958725828656, 'validation/accuracy': 0.9861845970153809, 'validation/loss': 0.04685523360967636, 'validation/mean_average_precision': 0.2086522317130877, 'validation/num_examples': 43793, 'test/accuracy': 0.9853790402412415, 'test/loss': 0.049420733004808426, 'test/mean_average_precision': 0.20462316028592084, 'test/num_examples': 43793, 'score': 15138.784644126892, 'total_duration': 21846.0258705616, 'accumulated_submission_time': 15138.784644126892, 'accumulated_eval_time': 6703.9477796554565, 'accumulated_logging_time': 2.0181338787078857, 'global_step': 47733, 'preemption_count': 0}), (48488, {'train/accuracy': 0.9893452525138855, 'train/loss': 0.0367339625954628, 'train/mean_average_precision': 0.25762161900276714, 'validation/accuracy': 0.9859787821769714, 'validation/loss': 0.047152355313301086, 'validation/mean_average_precision': 0.20892928456741483, 'validation/num_examples': 43793, 'test/accuracy': 0.9852316379547119, 'test/loss': 0.049615755677223206, 'test/mean_average_precision': 0.20043036230925274, 'test/num_examples': 43793, 'score': 15378.88271021843, 'total_duration': 22187.8186275959, 'accumulated_submission_time': 15378.88271021843, 'accumulated_eval_time': 6805.587907791138, 'accumulated_logging_time': 2.05293345451355, 'global_step': 48488, 'preemption_count': 0}), (49237, {'train/accuracy': 0.989425539970398, 'train/loss': 0.03569924458861351, 'train/mean_average_precision': 0.26737582420810835, 'validation/accuracy': 0.9861699938774109, 'validation/loss': 0.04659583792090416, 'validation/mean_average_precision': 0.2079270938996903, 'validation/num_examples': 43793, 'test/accuracy': 0.9853769540786743, 'test/loss': 0.04939282685518265, 'test/mean_average_precision': 0.1981841083007525, 'test/num_examples': 43793, 'score': 15618.961620092392, 'total_duration': 22531.986197948456, 'accumulated_submission_time': 15618.961620092392, 'accumulated_eval_time': 6909.61977314949, 'accumulated_logging_time': 2.088233470916748, 'global_step': 49237, 'preemption_count': 0}), (49998, {'train/accuracy': 0.9894838929176331, 'train/loss': 0.03577679023146629, 'train/mean_average_precision': 0.26919955506192716, 'validation/accuracy': 0.9862085580825806, 'validation/loss': 0.046243514865636826, 'validation/mean_average_precision': 0.21020474569661, 'validation/num_examples': 43793, 'test/accuracy': 0.9853028059005737, 'test/loss': 0.04925117641687393, 'test/mean_average_precision': 0.20060108943621363, 'test/num_examples': 43793, 'score': 15859.194140434265, 'total_duration': 22877.863034963608, 'accumulated_submission_time': 15859.194140434265, 'accumulated_eval_time': 7015.207123994827, 'accumulated_logging_time': 2.1260316371917725, 'global_step': 49998, 'preemption_count': 0}), (50746, {'train/accuracy': 0.9894869923591614, 'train/loss': 0.03578002378344536, 'train/mean_average_precision': 0.2574080154233004, 'validation/accuracy': 0.9860761761665344, 'validation/loss': 0.046711672097444534, 'validation/mean_average_precision': 0.2097283898741605, 'validation/num_examples': 43793, 'test/accuracy': 0.9852480292320251, 'test/loss': 0.04947970062494278, 'test/mean_average_precision': 0.2006904166219487, 'test/num_examples': 43793, 'score': 16099.170221567154, 'total_duration': 23220.043934583664, 'accumulated_submission_time': 16099.170221567154, 'accumulated_eval_time': 7117.35399389267, 'accumulated_logging_time': 2.1619765758514404, 'global_step': 50746, 'preemption_count': 0}), (51507, {'train/accuracy': 0.9896237254142761, 'train/loss': 0.03497027978301048, 'train/mean_average_precision': 0.28959290790469516, 'validation/accuracy': 0.9863124489784241, 'validation/loss': 0.04612846300005913, 'validation/mean_average_precision': 0.2125804910287261, 'validation/num_examples': 43793, 'test/accuracy': 0.9854000806808472, 'test/loss': 0.049058105796575546, 'test/mean_average_precision': 0.20585747115689745, 'test/num_examples': 43793, 'score': 16339.22156381607, 'total_duration': 23565.807424545288, 'accumulated_submission_time': 16339.22156381607, 'accumulated_eval_time': 7223.010741472244, 'accumulated_logging_time': 2.1973283290863037, 'global_step': 51507, 'preemption_count': 0}), (52257, {'train/accuracy': 0.9895951151847839, 'train/loss': 0.03497644141316414, 'train/mean_average_precision': 0.27689385115770665, 'validation/accuracy': 0.986238956451416, 'validation/loss': 0.04646912217140198, 'validation/mean_average_precision': 0.2151909716914597, 'validation/num_examples': 43793, 'test/accuracy': 0.9854320883750916, 'test/loss': 0.04927407577633858, 'test/mean_average_precision': 0.20773597079881576, 'test/num_examples': 43793, 'score': 16579.318229198456, 'total_duration': 23907.607943296432, 'accumulated_submission_time': 16579.318229198456, 'accumulated_eval_time': 7324.651810646057, 'accumulated_logging_time': 2.2361605167388916, 'global_step': 52257, 'preemption_count': 0}), (53018, {'train/accuracy': 0.9896343350410461, 'train/loss': 0.0349128283560276, 'train/mean_average_precision': 0.2853589611694491, 'validation/accuracy': 0.9862856864929199, 'validation/loss': 0.04632439464330673, 'validation/mean_average_precision': 0.21648895201171497, 'validation/num_examples': 43793, 'test/accuracy': 0.9853954315185547, 'test/loss': 0.04904608428478241, 'test/mean_average_precision': 0.20804087235297986, 'test/num_examples': 43793, 'score': 16819.287693738937, 'total_duration': 24249.325675964355, 'accumulated_submission_time': 16819.287693738937, 'accumulated_eval_time': 7426.342364788055, 'accumulated_logging_time': 2.273671865463257, 'global_step': 53018, 'preemption_count': 0}), (53768, {'train/accuracy': 0.9898422956466675, 'train/loss': 0.034334566444158554, 'train/mean_average_precision': 0.2906300054524572, 'validation/accuracy': 0.9863603711128235, 'validation/loss': 0.04618864133954048, 'validation/mean_average_precision': 0.21715397523875626, 'validation/num_examples': 43793, 'test/accuracy': 0.9854085445404053, 'test/loss': 0.0492413267493248, 'test/mean_average_precision': 0.21076632327964406, 'test/num_examples': 43793, 'score': 17059.330296278, 'total_duration': 24591.159697771072, 'accumulated_submission_time': 17059.330296278, 'accumulated_eval_time': 7528.076402425766, 'accumulated_logging_time': 2.310476779937744, 'global_step': 53768, 'preemption_count': 0}), (54516, {'train/accuracy': 0.9898531436920166, 'train/loss': 0.03418021276593208, 'train/mean_average_precision': 0.2880437519159216, 'validation/accuracy': 0.9863904118537903, 'validation/loss': 0.04603935778141022, 'validation/mean_average_precision': 0.2220284243850361, 'validation/num_examples': 43793, 'test/accuracy': 0.9854624271392822, 'test/loss': 0.049133043736219406, 'test/mean_average_precision': 0.21115627677974935, 'test/num_examples': 43793, 'score': 17299.522981405258, 'total_duration': 24935.30538392067, 'accumulated_submission_time': 17299.522981405258, 'accumulated_eval_time': 7631.970919847488, 'accumulated_logging_time': 2.348414182662964, 'global_step': 54516, 'preemption_count': 0}), (55281, {'train/accuracy': 0.9897654056549072, 'train/loss': 0.03450881317257881, 'train/mean_average_precision': 0.284018912475966, 'validation/accuracy': 0.9863700866699219, 'validation/loss': 0.04589037969708443, 'validation/mean_average_precision': 0.22173952586280968, 'validation/num_examples': 43793, 'test/accuracy': 0.9854293465614319, 'test/loss': 0.04875707998871803, 'test/mean_average_precision': 0.2053719159509973, 'test/num_examples': 43793, 'score': 17539.468029499054, 'total_duration': 25282.556941986084, 'accumulated_submission_time': 17539.468029499054, 'accumulated_eval_time': 7739.218340873718, 'accumulated_logging_time': 2.3874459266662598, 'global_step': 55281, 'preemption_count': 0}), (56038, {'train/accuracy': 0.989662766456604, 'train/loss': 0.034886036068201065, 'train/mean_average_precision': 0.2843685079643878, 'validation/accuracy': 0.9862491488456726, 'validation/loss': 0.04602076858282089, 'validation/mean_average_precision': 0.21583840603793586, 'validation/num_examples': 43793, 'test/accuracy': 0.9853495359420776, 'test/loss': 0.04895078018307686, 'test/mean_average_precision': 0.2030077043569287, 'test/num_examples': 43793, 'score': 17779.500022172928, 'total_duration': 25624.747854471207, 'accumulated_submission_time': 17779.500022172928, 'accumulated_eval_time': 7841.314211845398, 'accumulated_logging_time': 2.4286468029022217, 'global_step': 56038, 'preemption_count': 0}), (56795, {'train/accuracy': 0.9896244406700134, 'train/loss': 0.03496367484331131, 'train/mean_average_precision': 0.27953008690565206, 'validation/accuracy': 0.9862730503082275, 'validation/loss': 0.045942336320877075, 'validation/mean_average_precision': 0.21640935875464556, 'validation/num_examples': 43793, 'test/accuracy': 0.9854097962379456, 'test/loss': 0.04870106652379036, 'test/mean_average_precision': 0.20893250288086945, 'test/num_examples': 43793, 'score': 18019.45709347725, 'total_duration': 25968.38992166519, 'accumulated_submission_time': 18019.45709347725, 'accumulated_eval_time': 7944.941290616989, 'accumulated_logging_time': 2.4669902324676514, 'global_step': 56795, 'preemption_count': 0}), (57561, {'train/accuracy': 0.9898160099983215, 'train/loss': 0.03474228456616402, 'train/mean_average_precision': 0.28430747955029, 'validation/accuracy': 0.9861565828323364, 'validation/loss': 0.046272970736026764, 'validation/mean_average_precision': 0.22056289875778998, 'validation/num_examples': 43793, 'test/accuracy': 0.985299825668335, 'test/loss': 0.048982568085193634, 'test/mean_average_precision': 0.2117912201186689, 'test/num_examples': 43793, 'score': 18259.68850851059, 'total_duration': 26310.2358212471, 'accumulated_submission_time': 18259.68850851059, 'accumulated_eval_time': 8046.498164176941, 'accumulated_logging_time': 2.5046470165252686, 'global_step': 57561, 'preemption_count': 0})], 'global_step': 58253}
I0206 05:53:11.997564 140451058161472 submission_runner.py:586] Timing: 18477.287540912628
I0206 05:53:11.997630 140451058161472 submission_runner.py:588] Total number of evals: 77
I0206 05:53:11.997679 140451058161472 submission_runner.py:589] ====================
I0206 05:53:11.997735 140451058161472 submission_runner.py:542] Using RNG seed 449608868
I0206 05:53:12.059103 140451058161472 submission_runner.py:551] --- Tuning run 5/5 ---
I0206 05:53:12.059297 140451058161472 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_2/ogbg_jax/trial_5.
I0206 05:53:12.059641 140451058161472 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_2/ogbg_jax/trial_5/hparams.json.
I0206 05:53:12.189526 140451058161472 submission_runner.py:206] Initializing dataset.
I0206 05:53:12.275273 140451058161472 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0206 05:53:12.281193 140451058161472 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0206 05:53:12.414949 140451058161472 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0206 05:53:12.451225 140451058161472 submission_runner.py:213] Initializing model.
I0206 05:53:14.662857 140451058161472 submission_runner.py:255] Initializing optimizer.
I0206 05:53:15.234846 140451058161472 submission_runner.py:262] Initializing metrics bundle.
I0206 05:53:15.235046 140451058161472 submission_runner.py:280] Initializing checkpoint and logger.
I0206 05:53:15.235751 140451058161472 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_2/ogbg_jax/trial_5 with prefix checkpoint_
I0206 05:53:15.235887 140451058161472 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_2/ogbg_jax/trial_5/meta_data_0.json.
I0206 05:53:15.236102 140451058161472 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0206 05:53:15.236168 140451058161472 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0206 05:53:17.525934 140451058161472 logger_utils.py:220] Unable to record git information. Continuing without it.
I0206 05:53:19.820739 140451058161472 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_2/ogbg_jax/trial_5/flags_0.json.
I0206 05:53:19.824618 140451058161472 submission_runner.py:314] Starting training loop.
I0206 05:53:32.946522 140210975590144 logging_writer.py:48] [0] global_step=0, grad_norm=1.887237548828125, loss=0.7344204783439636
I0206 05:53:32.957381 140451058161472 spec.py:321] Evaluating on the training split.
I0206 05:55:10.062231 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 05:55:13.074322 140451058161472 spec.py:349] Evaluating on the test split.
I0206 05:55:15.999450 140451058161472 submission_runner.py:408] Time since start: 116.17s, 	Step: 1, 	{'train/accuracy': 0.5288358926773071, 'train/loss': 0.7364843487739563, 'train/mean_average_precision': 0.020878767422651344, 'validation/accuracy': 0.5270814895629883, 'validation/loss': 0.737440824508667, 'validation/mean_average_precision': 0.024115497491982076, 'validation/num_examples': 43793, 'test/accuracy': 0.5256850719451904, 'test/loss': 0.7376683354377747, 'test/mean_average_precision': 0.02603311504248907, 'test/num_examples': 43793, 'score': 13.132712841033936, 'total_duration': 116.17477297782898, 'accumulated_submission_time': 13.132712841033936, 'accumulated_eval_time': 103.04201483726501, 'accumulated_logging_time': 0}
I0206 05:55:16.008756 140211004475136 logging_writer.py:48] [1] accumulated_eval_time=103.042015, accumulated_logging_time=0, accumulated_submission_time=13.132713, global_step=1, preemption_count=0, score=13.132713, test/accuracy=0.525685, test/loss=0.737668, test/mean_average_precision=0.026033, test/num_examples=43793, total_duration=116.174773, train/accuracy=0.528836, train/loss=0.736484, train/mean_average_precision=0.020879, validation/accuracy=0.527081, validation/loss=0.737441, validation/mean_average_precision=0.024115, validation/num_examples=43793
I0206 05:55:47.631489 140229871392512 logging_writer.py:48] [100] global_step=100, grad_norm=0.2822999358177185, loss=0.2644096910953522
I0206 05:56:19.556276 140211004475136 logging_writer.py:48] [200] global_step=200, grad_norm=0.09104836732149124, loss=0.10725577175617218
I0206 05:56:51.280932 140229871392512 logging_writer.py:48] [300] global_step=300, grad_norm=0.03161377087235451, loss=0.06723124533891678
I0206 05:57:22.969536 140211004475136 logging_writer.py:48] [400] global_step=400, grad_norm=0.021831676363945007, loss=0.05780736356973648
I0206 05:57:55.330680 140229871392512 logging_writer.py:48] [500] global_step=500, grad_norm=0.03703959286212921, loss=0.061670295894145966
I0206 05:58:28.500285 140211004475136 logging_writer.py:48] [600] global_step=600, grad_norm=0.019142314791679382, loss=0.05476304888725281
I0206 05:59:00.632764 140229871392512 logging_writer.py:48] [700] global_step=700, grad_norm=0.016728859394788742, loss=0.04902956634759903
I0206 05:59:16.160717 140451058161472 spec.py:321] Evaluating on the training split.
I0206 06:00:52.742100 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 06:00:55.841496 140451058161472 spec.py:349] Evaluating on the test split.
I0206 06:00:58.829914 140451058161472 submission_runner.py:408] Time since start: 459.01s, 	Step: 749, 	{'train/accuracy': 0.9869271516799927, 'train/loss': 0.05065993219614029, 'train/mean_average_precision': 0.05997201030197283, 'validation/accuracy': 0.984121561050415, 'validation/loss': 0.060095831751823425, 'validation/mean_average_precision': 0.056914372746511545, 'validation/num_examples': 43793, 'test/accuracy': 0.9831327795982361, 'test/loss': 0.06328555196523666, 'test/mean_average_precision': 0.058255329912699876, 'test/num_examples': 43793, 'score': 252.96980333328247, 'total_duration': 459.00523805618286, 'accumulated_submission_time': 252.96980333328247, 'accumulated_eval_time': 205.71116495132446, 'accumulated_logging_time': 0.3030984401702881}
I0206 06:00:58.845574 140229879785216 logging_writer.py:48] [749] accumulated_eval_time=205.711165, accumulated_logging_time=0.303098, accumulated_submission_time=252.969803, global_step=749, preemption_count=0, score=252.969803, test/accuracy=0.983133, test/loss=0.063286, test/mean_average_precision=0.058255, test/num_examples=43793, total_duration=459.005238, train/accuracy=0.986927, train/loss=0.050660, train/mean_average_precision=0.059972, validation/accuracy=0.984122, validation/loss=0.060096, validation/mean_average_precision=0.056914, validation/num_examples=43793
I0206 06:01:15.917974 140248415348480 logging_writer.py:48] [800] global_step=800, grad_norm=0.03211605176329613, loss=0.055559441447257996
I0206 06:01:48.309528 140229879785216 logging_writer.py:48] [900] global_step=900, grad_norm=0.020857352763414383, loss=0.04917189106345177
I0206 06:02:20.139765 140248415348480 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.02234669215977192, loss=0.04992345720529556
I0206 06:02:51.717302 140229879785216 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.022653060033917427, loss=0.04355573654174805
I0206 06:03:23.510493 140248415348480 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.04848414659500122, loss=0.04871333763003349
I0206 06:03:55.018075 140229879785216 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.03575672209262848, loss=0.044976960867643356
I0206 06:04:26.877896 140248415348480 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.022501248866319656, loss=0.049350254237651825
I0206 06:04:58.253661 140229879785216 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.024555128067731857, loss=0.05350736156105995
I0206 06:04:58.929232 140451058161472 spec.py:321] Evaluating on the training split.
I0206 06:06:40.514158 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 06:06:43.492673 140451058161472 spec.py:349] Evaluating on the test split.
I0206 06:06:46.447451 140451058161472 submission_runner.py:408] Time since start: 806.62s, 	Step: 1503, 	{'train/accuracy': 0.9873338937759399, 'train/loss': 0.04714243486523628, 'train/mean_average_precision': 0.10909406662670157, 'validation/accuracy': 0.9847106337547302, 'validation/loss': 0.056974757462739944, 'validation/mean_average_precision': 0.10810284037495257, 'validation/num_examples': 43793, 'test/accuracy': 0.983751118183136, 'test/loss': 0.06052108854055405, 'test/mean_average_precision': 0.10710254149319327, 'test/num_examples': 43793, 'score': 493.0213305950165, 'total_duration': 806.6227686405182, 'accumulated_submission_time': 493.0213305950165, 'accumulated_eval_time': 313.22933530807495, 'accumulated_logging_time': 0.33067798614501953}
I0206 06:06:46.462829 140210965407488 logging_writer.py:48] [1503] accumulated_eval_time=313.229335, accumulated_logging_time=0.330678, accumulated_submission_time=493.021331, global_step=1503, preemption_count=0, score=493.021331, test/accuracy=0.983751, test/loss=0.060521, test/mean_average_precision=0.107103, test/num_examples=43793, total_duration=806.622769, train/accuracy=0.987334, train/loss=0.047142, train/mean_average_precision=0.109094, validation/accuracy=0.984711, validation/loss=0.056975, validation/mean_average_precision=0.108103, validation/num_examples=43793
I0206 06:07:17.566964 140211004475136 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.026684926822781563, loss=0.04845459759235382
I0206 06:07:48.887548 140210965407488 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.018571533262729645, loss=0.04742388799786568
I0206 06:08:20.252612 140211004475136 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.017066936939954758, loss=0.04927707836031914
I0206 06:08:51.646489 140210965407488 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.015868302434682846, loss=0.05110817402601242
I0206 06:09:23.127621 140211004475136 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.015154433436691761, loss=0.04493747651576996
I0206 06:09:55.036110 140210965407488 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.013139375485479832, loss=0.045363787561655045
I0206 06:10:27.778990 140211004475136 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.014787029474973679, loss=0.04467250779271126
I0206 06:10:46.636829 140451058161472 spec.py:321] Evaluating on the training split.
I0206 06:12:24.162030 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 06:12:27.193053 140451058161472 spec.py:349] Evaluating on the test split.
I0206 06:12:30.270370 140451058161472 submission_runner.py:408] Time since start: 1150.45s, 	Step: 2259, 	{'train/accuracy': 0.9877690076828003, 'train/loss': 0.04325583949685097, 'train/mean_average_precision': 0.14656782099532822, 'validation/accuracy': 0.9849586486816406, 'validation/loss': 0.05234578624367714, 'validation/mean_average_precision': 0.13774467362244358, 'validation/num_examples': 43793, 'test/accuracy': 0.9840215444564819, 'test/loss': 0.055156830698251724, 'test/mean_average_precision': 0.14165930443343672, 'test/num_examples': 43793, 'score': 733.1634314060211, 'total_duration': 1150.4456820487976, 'accumulated_submission_time': 733.1634314060211, 'accumulated_eval_time': 416.8628304004669, 'accumulated_logging_time': 0.3571460247039795}
I0206 06:12:30.286112 140229879785216 logging_writer.py:48] [2259] accumulated_eval_time=416.862830, accumulated_logging_time=0.357146, accumulated_submission_time=733.163431, global_step=2259, preemption_count=0, score=733.163431, test/accuracy=0.984022, test/loss=0.055157, test/mean_average_precision=0.141659, test/num_examples=43793, total_duration=1150.445682, train/accuracy=0.987769, train/loss=0.043256, train/mean_average_precision=0.146568, validation/accuracy=0.984959, validation/loss=0.052346, validation/mean_average_precision=0.137745, validation/num_examples=43793
I0206 06:12:43.449563 140248415348480 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.014691582880914211, loss=0.042257051914930344
I0206 06:13:15.235376 140229879785216 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.016463959589600563, loss=0.046212486922740936
I0206 06:13:46.507152 140248415348480 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.021930981427431107, loss=0.047135233879089355
I0206 06:14:18.635647 140229879785216 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.028436627238988876, loss=0.040099624544382095
I0206 06:14:51.051375 140248415348480 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.021845867857336998, loss=0.04713143780827522
I0206 06:15:23.267228 140229879785216 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.0170893631875515, loss=0.04587812349200249
I0206 06:15:54.614851 140248415348480 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.015437417663633823, loss=0.04212529584765434
I0206 06:16:26.273868 140229879785216 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.01031632162630558, loss=0.03834964334964752
I0206 06:16:30.375536 140451058161472 spec.py:321] Evaluating on the training split.
I0206 06:18:04.233180 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 06:18:07.252964 140451058161472 spec.py:349] Evaluating on the test split.
I0206 06:18:12.745732 140451058161472 submission_runner.py:408] Time since start: 1492.92s, 	Step: 3014, 	{'train/accuracy': 0.9881166219711304, 'train/loss': 0.04161359742283821, 'train/mean_average_precision': 0.17624069387477692, 'validation/accuracy': 0.9850816130638123, 'validation/loss': 0.05104980617761612, 'validation/mean_average_precision': 0.15291850797555867, 'validation/num_examples': 43793, 'test/accuracy': 0.9841373562812805, 'test/loss': 0.053809214383363724, 'test/mean_average_precision': 0.15615814952137944, 'test/num_examples': 43793, 'score': 973.2205414772034, 'total_duration': 1492.9210562705994, 'accumulated_submission_time': 973.2205414772034, 'accumulated_eval_time': 519.2329788208008, 'accumulated_logging_time': 0.38398075103759766}
I0206 06:18:12.761257 140211004475136 logging_writer.py:48] [3014] accumulated_eval_time=519.232979, accumulated_logging_time=0.383981, accumulated_submission_time=973.220541, global_step=3014, preemption_count=0, score=973.220541, test/accuracy=0.984137, test/loss=0.053809, test/mean_average_precision=0.156158, test/num_examples=43793, total_duration=1492.921056, train/accuracy=0.988117, train/loss=0.041614, train/mean_average_precision=0.176241, validation/accuracy=0.985082, validation/loss=0.051050, validation/mean_average_precision=0.152919, validation/num_examples=43793
I0206 06:18:40.966098 140229871392512 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.015926022082567215, loss=0.0445416122674942
I0206 06:19:12.825837 140211004475136 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.02626432478427887, loss=0.03989829868078232
I0206 06:19:44.760351 140229871392512 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.012014005333185196, loss=0.04209933429956436
I0206 06:20:16.458043 140211004475136 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.012565530836582184, loss=0.03772192448377609
I0206 06:20:48.086655 140229871392512 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.01530400663614273, loss=0.04526735842227936
I0206 06:21:20.733662 140211004475136 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.01902521215379238, loss=0.042266495525836945
I0206 06:21:53.074416 140229871392512 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.019871538504958153, loss=0.038601987063884735
I0206 06:22:12.874290 140451058161472 spec.py:321] Evaluating on the training split.
I0206 06:23:46.925246 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 06:23:49.929256 140451058161472 spec.py:349] Evaluating on the test split.
I0206 06:23:52.909065 140451058161472 submission_runner.py:408] Time since start: 1833.08s, 	Step: 3762, 	{'train/accuracy': 0.9884961247444153, 'train/loss': 0.03974774479866028, 'train/mean_average_precision': 0.19839875751331684, 'validation/accuracy': 0.9855943322181702, 'validation/loss': 0.049146659672260284, 'validation/mean_average_precision': 0.17655218056886918, 'validation/num_examples': 43793, 'test/accuracy': 0.984641969203949, 'test/loss': 0.05208675563335419, 'test/mean_average_precision': 0.18236919829215972, 'test/num_examples': 43793, 'score': 1213.301376581192, 'total_duration': 1833.0843858718872, 'accumulated_submission_time': 1213.301376581192, 'accumulated_eval_time': 619.2677090167999, 'accumulated_logging_time': 0.4104807376861572}
I0206 06:23:52.925270 140229879785216 logging_writer.py:48] [3762] accumulated_eval_time=619.267709, accumulated_logging_time=0.410481, accumulated_submission_time=1213.301377, global_step=3762, preemption_count=0, score=1213.301377, test/accuracy=0.984642, test/loss=0.052087, test/mean_average_precision=0.182369, test/num_examples=43793, total_duration=1833.084386, train/accuracy=0.988496, train/loss=0.039748, train/mean_average_precision=0.198399, validation/accuracy=0.985594, validation/loss=0.049147, validation/mean_average_precision=0.176552, validation/num_examples=43793
I0206 06:24:05.461007 140248415348480 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.01147552765905857, loss=0.03916580229997635
I0206 06:24:37.688940 140229879785216 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.013387884944677353, loss=0.04072917252779007
I0206 06:25:09.909989 140248415348480 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.011713068000972271, loss=0.041350238025188446
I0206 06:25:42.143239 140229879785216 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.012952226214110851, loss=0.03685285523533821
I0206 06:26:14.031320 140248415348480 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.020642908290028572, loss=0.039762888103723526
I0206 06:26:46.196635 140229879785216 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.012498500756919384, loss=0.040130164474248886
I0206 06:27:18.309781 140248415348480 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.010406244546175003, loss=0.04026992991566658
I0206 06:27:50.157279 140229879785216 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.010003535076975822, loss=0.03475768119096756
I0206 06:27:53.036142 140451058161472 spec.py:321] Evaluating on the training split.
I0206 06:29:27.392366 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 06:29:30.517962 140451058161472 spec.py:349] Evaluating on the test split.
I0206 06:29:33.669022 140451058161472 submission_runner.py:408] Time since start: 2173.84s, 	Step: 4510, 	{'train/accuracy': 0.9887267351150513, 'train/loss': 0.03883034363389015, 'train/mean_average_precision': 0.2203952174799551, 'validation/accuracy': 0.9857790470123291, 'validation/loss': 0.04811367392539978, 'validation/mean_average_precision': 0.1902895326825564, 'validation/num_examples': 43793, 'test/accuracy': 0.9848470687866211, 'test/loss': 0.050781041383743286, 'test/mean_average_precision': 0.19348862270885217, 'test/num_examples': 43793, 'score': 1453.3801944255829, 'total_duration': 2173.8443462848663, 'accumulated_submission_time': 1453.3801944255829, 'accumulated_eval_time': 719.900552034378, 'accumulated_logging_time': 0.438244104385376}
I0206 06:29:33.684887 140210965407488 logging_writer.py:48] [4510] accumulated_eval_time=719.900552, accumulated_logging_time=0.438244, accumulated_submission_time=1453.380194, global_step=4510, preemption_count=0, score=1453.380194, test/accuracy=0.984847, test/loss=0.050781, test/mean_average_precision=0.193489, test/num_examples=43793, total_duration=2173.844346, train/accuracy=0.988727, train/loss=0.038830, train/mean_average_precision=0.220395, validation/accuracy=0.985779, validation/loss=0.048114, validation/mean_average_precision=0.190290, validation/num_examples=43793
I0206 06:30:03.019490 140211004475136 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.011205384507775307, loss=0.040026988834142685
I0206 06:30:35.058308 140210965407488 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.019208023324608803, loss=0.04658735170960426
I0206 06:31:06.745314 140211004475136 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.010484419763088226, loss=0.033851295709609985
I0206 06:31:38.945441 140210965407488 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.014246771112084389, loss=0.04004497081041336
I0206 06:32:11.028023 140211004475136 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.010414838790893555, loss=0.03671766445040703
I0206 06:32:42.801697 140210965407488 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.012728003785014153, loss=0.036787524819374084
I0206 06:33:14.693030 140211004475136 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.01214249525219202, loss=0.03760799393057823
I0206 06:33:33.891826 140451058161472 spec.py:321] Evaluating on the training split.
I0206 06:35:11.255102 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 06:35:14.347170 140451058161472 spec.py:349] Evaluating on the test split.
I0206 06:35:17.430717 140451058161472 submission_runner.py:408] Time since start: 2517.61s, 	Step: 5261, 	{'train/accuracy': 0.9889106750488281, 'train/loss': 0.03774059936404228, 'train/mean_average_precision': 0.237520259220091, 'validation/accuracy': 0.985815167427063, 'validation/loss': 0.047681551426649094, 'validation/mean_average_precision': 0.2013130685938968, 'validation/num_examples': 43793, 'test/accuracy': 0.984906017780304, 'test/loss': 0.0503714494407177, 'test/mean_average_precision': 0.20533108427856772, 'test/num_examples': 43793, 'score': 1693.5554299354553, 'total_duration': 2517.606040239334, 'accumulated_submission_time': 1693.5554299354553, 'accumulated_eval_time': 823.4393961429596, 'accumulated_logging_time': 0.4656791687011719}
I0206 06:35:17.446730 140229879785216 logging_writer.py:48] [5261] accumulated_eval_time=823.439396, accumulated_logging_time=0.465679, accumulated_submission_time=1693.555430, global_step=5261, preemption_count=0, score=1693.555430, test/accuracy=0.984906, test/loss=0.050371, test/mean_average_precision=0.205331, test/num_examples=43793, total_duration=2517.606040, train/accuracy=0.988911, train/loss=0.037741, train/mean_average_precision=0.237520, validation/accuracy=0.985815, validation/loss=0.047682, validation/mean_average_precision=0.201313, validation/num_examples=43793
I0206 06:35:30.096574 140248415348480 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.015345511958003044, loss=0.03776562213897705
I0206 06:36:02.288398 140229879785216 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.014786913990974426, loss=0.0393393412232399
I0206 06:36:34.511632 140248415348480 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.019658230245113373, loss=0.03857165575027466
I0206 06:37:07.059272 140229879785216 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.011317030526697636, loss=0.03614554926753044
I0206 06:37:38.498502 140248415348480 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.011030999943614006, loss=0.04002205282449722
I0206 06:38:10.708421 140229879785216 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.018066128715872765, loss=0.03662768751382828
I0206 06:38:42.548921 140248415348480 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.013783630914986134, loss=0.035639408975839615
I0206 06:39:14.611027 140229879785216 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.014025751501321793, loss=0.03466333448886871
I0206 06:39:17.495256 140451058161472 spec.py:321] Evaluating on the training split.
I0206 06:40:58.245353 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 06:41:01.577316 140451058161472 spec.py:349] Evaluating on the test split.
I0206 06:41:04.995013 140451058161472 submission_runner.py:408] Time since start: 2865.17s, 	Step: 6010, 	{'train/accuracy': 0.9891675114631653, 'train/loss': 0.03660612180829048, 'train/mean_average_precision': 0.25422043343603723, 'validation/accuracy': 0.9861252903938293, 'validation/loss': 0.04656600579619408, 'validation/mean_average_precision': 0.216148982664498, 'validation/num_examples': 43793, 'test/accuracy': 0.985258162021637, 'test/loss': 0.04918884485960007, 'test/mean_average_precision': 0.21554773051913081, 'test/num_examples': 43793, 'score': 1933.570939540863, 'total_duration': 2865.1703193187714, 'accumulated_submission_time': 1933.570939540863, 'accumulated_eval_time': 930.9390976428986, 'accumulated_logging_time': 0.49407386779785156}
I0206 06:41:05.013565 140210965407488 logging_writer.py:48] [6010] accumulated_eval_time=930.939098, accumulated_logging_time=0.494074, accumulated_submission_time=1933.570940, global_step=6010, preemption_count=0, score=1933.570940, test/accuracy=0.985258, test/loss=0.049189, test/mean_average_precision=0.215548, test/num_examples=43793, total_duration=2865.170319, train/accuracy=0.989168, train/loss=0.036606, train/mean_average_precision=0.254220, validation/accuracy=0.986125, validation/loss=0.046566, validation/mean_average_precision=0.216149, validation/num_examples=43793
I0206 06:41:34.296795 140229871392512 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.014624959789216518, loss=0.03754551336169243
I0206 06:42:07.276589 140210965407488 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.01575041376054287, loss=0.039153002202510834
I0206 06:42:39.591394 140229871392512 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.011553226970136166, loss=0.0359436497092247
I0206 06:43:12.053750 140210965407488 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.015157121233642101, loss=0.03406412526965141
I0206 06:43:44.245310 140229871392512 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.012778344564139843, loss=0.03919975832104683
I0206 06:44:16.547830 140210965407488 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.04723338782787323, loss=0.0378691628575325
I0206 06:44:48.798462 140229871392512 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.01093943603336811, loss=0.03311362862586975
I0206 06:45:05.302619 140451058161472 spec.py:321] Evaluating on the training split.
I0206 06:46:43.209354 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 06:46:46.324589 140451058161472 spec.py:349] Evaluating on the test split.
I0206 06:46:49.337824 140451058161472 submission_runner.py:408] Time since start: 3209.51s, 	Step: 6751, 	{'train/accuracy': 0.9891411662101746, 'train/loss': 0.036478422582149506, 'train/mean_average_precision': 0.27995736027791956, 'validation/accuracy': 0.9861935377120972, 'validation/loss': 0.04652482271194458, 'validation/mean_average_precision': 0.22070827583285108, 'validation/num_examples': 43793, 'test/accuracy': 0.9853482842445374, 'test/loss': 0.049075283110141754, 'test/mean_average_precision': 0.22745468948360154, 'test/num_examples': 43793, 'score': 2173.825298309326, 'total_duration': 3209.513146877289, 'accumulated_submission_time': 2173.825298309326, 'accumulated_eval_time': 1034.9742851257324, 'accumulated_logging_time': 0.5242447853088379}
I0206 06:46:49.354864 140290179761920 logging_writer.py:48] [6751] accumulated_eval_time=1034.974285, accumulated_logging_time=0.524245, accumulated_submission_time=2173.825298, global_step=6751, preemption_count=0, score=2173.825298, test/accuracy=0.985348, test/loss=0.049075, test/mean_average_precision=0.227455, test/num_examples=43793, total_duration=3209.513147, train/accuracy=0.989141, train/loss=0.036478, train/mean_average_precision=0.279957, validation/accuracy=0.986194, validation/loss=0.046525, validation/mean_average_precision=0.220708, validation/num_examples=43793
I0206 06:47:05.527392 140290188154624 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.010812061838805676, loss=0.03481162339448929
I0206 06:47:37.540238 140290179761920 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.016273340210318565, loss=0.039404213428497314
I0206 06:48:09.484286 140290188154624 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.010254026390612125, loss=0.03546004369854927
I0206 06:48:41.425359 140290179761920 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.01977071352303028, loss=0.03750554844737053
I0206 06:49:13.411169 140290188154624 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.02374921925365925, loss=0.03678213059902191
I0206 06:49:44.899807 140290179761920 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.016348768025636673, loss=0.04125622287392616
I0206 06:50:16.600058 140290188154624 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.021271128207445145, loss=0.037961266934871674
I0206 06:50:48.026660 140290179761920 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.015106305480003357, loss=0.034948401153087616
I0206 06:50:49.594102 140451058161472 spec.py:321] Evaluating on the training split.
I0206 06:52:29.878019 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 06:52:33.324764 140451058161472 spec.py:349] Evaluating on the test split.
I0206 06:52:36.654268 140451058161472 submission_runner.py:408] Time since start: 3556.83s, 	Step: 7506, 	{'train/accuracy': 0.9896240234375, 'train/loss': 0.03505132719874382, 'train/mean_average_precision': 0.29706911150222315, 'validation/accuracy': 0.9860489964485168, 'validation/loss': 0.04668343812227249, 'validation/mean_average_precision': 0.2174827924270316, 'validation/num_examples': 43793, 'test/accuracy': 0.9852122664451599, 'test/loss': 0.04923328384757042, 'test/mean_average_precision': 0.22008655169617167, 'test/num_examples': 43793, 'score': 2414.03213095665, 'total_duration': 3556.8294620513916, 'accumulated_submission_time': 2414.03213095665, 'accumulated_eval_time': 1142.0342810153961, 'accumulated_logging_time': 0.5527384281158447}
I0206 06:52:36.672598 140229879785216 logging_writer.py:48] [7506] accumulated_eval_time=1142.034281, accumulated_logging_time=0.552738, accumulated_submission_time=2414.032131, global_step=7506, preemption_count=0, score=2414.032131, test/accuracy=0.985212, test/loss=0.049233, test/mean_average_precision=0.220087, test/num_examples=43793, total_duration=3556.829462, train/accuracy=0.989624, train/loss=0.035051, train/mean_average_precision=0.297069, validation/accuracy=0.986049, validation/loss=0.046683, validation/mean_average_precision=0.217483, validation/num_examples=43793
I0206 06:53:08.222905 140248415348480 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.016864826902747154, loss=0.03717541694641113
I0206 06:53:40.521528 140229879785216 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.016946442425251007, loss=0.03806644305586815
I0206 06:54:13.147317 140248415348480 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.01833508536219597, loss=0.03690772503614426
I0206 06:54:45.597911 140229879785216 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.015273083001375198, loss=0.031697601079940796
I0206 06:55:18.554051 140248415348480 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.013154289685189724, loss=0.03275378420948982
I0206 06:55:51.207931 140229879785216 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.021462060511112213, loss=0.038500770926475525
I0206 06:56:23.877496 140248415348480 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.022286659106612206, loss=0.035352811217308044
I0206 06:56:36.827717 140451058161472 spec.py:321] Evaluating on the training split.
I0206 06:58:17.018023 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 06:58:20.113964 140451058161472 spec.py:349] Evaluating on the test split.
I0206 06:58:23.134687 140451058161472 submission_runner.py:408] Time since start: 3903.31s, 	Step: 8241, 	{'train/accuracy': 0.9899398684501648, 'train/loss': 0.03389454632997513, 'train/mean_average_precision': 0.32697509784170464, 'validation/accuracy': 0.9864467978477478, 'validation/loss': 0.04521428048610687, 'validation/mean_average_precision': 0.23893564450384508, 'validation/num_examples': 43793, 'test/accuracy': 0.9855904579162598, 'test/loss': 0.04803219437599182, 'test/mean_average_precision': 0.235439831915132, 'test/num_examples': 43793, 'score': 2654.1512384414673, 'total_duration': 3903.309905767441, 'accumulated_submission_time': 2654.1512384414673, 'accumulated_eval_time': 1248.3411090373993, 'accumulated_logging_time': 0.5830700397491455}
I0206 06:58:23.152406 140229871392512 logging_writer.py:48] [8241] accumulated_eval_time=1248.341109, accumulated_logging_time=0.583070, accumulated_submission_time=2654.151238, global_step=8241, preemption_count=0, score=2654.151238, test/accuracy=0.985590, test/loss=0.048032, test/mean_average_precision=0.235440, test/num_examples=43793, total_duration=3903.309906, train/accuracy=0.989940, train/loss=0.033895, train/mean_average_precision=0.326975, validation/accuracy=0.986447, validation/loss=0.045214, validation/mean_average_precision=0.238936, validation/num_examples=43793
I0206 06:58:43.286892 140290188154624 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.015061822719871998, loss=0.032474860548973083
I0206 06:59:15.112438 140229871392512 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.01584741845726967, loss=0.035603515803813934
I0206 06:59:46.406712 140290188154624 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.021450281143188477, loss=0.03691074624657631
I0206 07:00:18.497083 140229871392512 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.013043510727584362, loss=0.033400919288396835
I0206 07:00:49.874109 140290188154624 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.017344310879707336, loss=0.03567178547382355
I0206 07:01:21.559276 140229871392512 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.01557639054954052, loss=0.034411702305078506
I0206 07:01:53.666079 140290188154624 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.018329676240682602, loss=0.038789261132478714
I0206 07:02:23.297969 140451058161472 spec.py:321] Evaluating on the training split.
I0206 07:04:08.921865 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 07:04:11.940978 140451058161472 spec.py:349] Evaluating on the test split.
I0206 07:04:14.944882 140451058161472 submission_runner.py:408] Time since start: 4255.12s, 	Step: 8992, 	{'train/accuracy': 0.9901648759841919, 'train/loss': 0.03290893882513046, 'train/mean_average_precision': 0.343574061270163, 'validation/accuracy': 0.9863510131835938, 'validation/loss': 0.04535606876015663, 'validation/mean_average_precision': 0.2322570774352511, 'validation/num_examples': 43793, 'test/accuracy': 0.9855828881263733, 'test/loss': 0.04782380908727646, 'test/mean_average_precision': 0.2394972606796361, 'test/num_examples': 43793, 'score': 2894.265217065811, 'total_duration': 4255.120206356049, 'accumulated_submission_time': 2894.265217065811, 'accumulated_eval_time': 1359.987991809845, 'accumulated_logging_time': 0.6116423606872559}
I0206 07:04:14.962147 140248415348480 logging_writer.py:48] [8992] accumulated_eval_time=1359.987992, accumulated_logging_time=0.611642, accumulated_submission_time=2894.265217, global_step=8992, preemption_count=0, score=2894.265217, test/accuracy=0.985583, test/loss=0.047824, test/mean_average_precision=0.239497, test/num_examples=43793, total_duration=4255.120206, train/accuracy=0.990165, train/loss=0.032909, train/mean_average_precision=0.343574, validation/accuracy=0.986351, validation/loss=0.045356, validation/mean_average_precision=0.232257, validation/num_examples=43793
I0206 07:04:17.876769 140290179761920 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.025471799075603485, loss=0.03815421462059021
I0206 07:04:50.002557 140248415348480 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.025449497625231743, loss=0.03463929519057274
I0206 07:05:22.297113 140290179761920 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.01603224314749241, loss=0.031108809635043144
I0206 07:05:54.269705 140248415348480 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.016856560483574867, loss=0.035401541739702225
I0206 07:06:26.062387 140290179761920 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.030608627945184708, loss=0.03535014018416405
I0206 07:06:57.835757 140248415348480 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.018593814224004745, loss=0.034251369535923004
I0206 07:07:29.792085 140290179761920 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.018987903371453285, loss=0.03922629728913307
I0206 07:08:01.859436 140248415348480 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.026807280257344246, loss=0.033179547637701035
I0206 07:08:14.972608 140451058161472 spec.py:321] Evaluating on the training split.
I0206 07:09:52.223217 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 07:09:55.306367 140451058161472 spec.py:349] Evaluating on the test split.
I0206 07:09:58.315748 140451058161472 submission_runner.py:408] Time since start: 4598.49s, 	Step: 9742, 	{'train/accuracy': 0.9900648593902588, 'train/loss': 0.03303992748260498, 'train/mean_average_precision': 0.33678358815061643, 'validation/accuracy': 0.9864760637283325, 'validation/loss': 0.04529368504881859, 'validation/mean_average_precision': 0.23703560899096426, 'validation/num_examples': 43793, 'test/accuracy': 0.9856751561164856, 'test/loss': 0.04794555902481079, 'test/mean_average_precision': 0.2425333064582841, 'test/num_examples': 43793, 'score': 3134.242573261261, 'total_duration': 4598.49095082283, 'accumulated_submission_time': 3134.242573261261, 'accumulated_eval_time': 1463.3309633731842, 'accumulated_logging_time': 0.6411969661712646}
I0206 07:09:58.332537 140229879785216 logging_writer.py:48] [9742] accumulated_eval_time=1463.330963, accumulated_logging_time=0.641197, accumulated_submission_time=3134.242573, global_step=9742, preemption_count=0, score=3134.242573, test/accuracy=0.985675, test/loss=0.047946, test/mean_average_precision=0.242533, test/num_examples=43793, total_duration=4598.490951, train/accuracy=0.990065, train/loss=0.033040, train/mean_average_precision=0.336784, validation/accuracy=0.986476, validation/loss=0.045294, validation/mean_average_precision=0.237036, validation/num_examples=43793
I0206 07:10:17.295537 140290188154624 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.01720064878463745, loss=0.034213095903396606
I0206 07:10:49.226212 140229879785216 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.03169575706124306, loss=0.037518665194511414
I0206 07:11:21.137846 140290188154624 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.01770068146288395, loss=0.034171219915151596
I0206 07:11:52.900308 140229879785216 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.01479580719023943, loss=0.030657688155770302
I0206 07:12:25.047116 140290188154624 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.018287457525730133, loss=0.0326664000749588
I0206 07:12:56.700606 140229879785216 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.020244108512997627, loss=0.035216983407735825
I0206 07:13:28.514339 140290188154624 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.02763124741613865, loss=0.03360702469944954
I0206 07:13:58.513295 140451058161472 spec.py:321] Evaluating on the training split.
I0206 07:15:36.547283 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 07:15:39.584519 140451058161472 spec.py:349] Evaluating on the test split.
I0206 07:15:42.608535 140451058161472 submission_runner.py:408] Time since start: 4942.78s, 	Step: 10495, 	{'train/accuracy': 0.9899468421936035, 'train/loss': 0.033407848328351974, 'train/mean_average_precision': 0.3359655048713717, 'validation/accuracy': 0.986441969871521, 'validation/loss': 0.04499449208378792, 'validation/mean_average_precision': 0.24853004020870537, 'validation/num_examples': 43793, 'test/accuracy': 0.9855492115020752, 'test/loss': 0.047698359936475754, 'test/mean_average_precision': 0.2441579029718845, 'test/num_examples': 43793, 'score': 3374.392449617386, 'total_duration': 4942.783852100372, 'accumulated_submission_time': 3374.392449617386, 'accumulated_eval_time': 1567.4261529445648, 'accumulated_logging_time': 0.6686229705810547}
I0206 07:15:42.625508 140248415348480 logging_writer.py:48] [10495] accumulated_eval_time=1567.426153, accumulated_logging_time=0.668623, accumulated_submission_time=3374.392450, global_step=10495, preemption_count=0, score=3374.392450, test/accuracy=0.985549, test/loss=0.047698, test/mean_average_precision=0.244158, test/num_examples=43793, total_duration=4942.783852, train/accuracy=0.989947, train/loss=0.033408, train/mean_average_precision=0.335966, validation/accuracy=0.986442, validation/loss=0.044994, validation/mean_average_precision=0.248530, validation/num_examples=43793
I0206 07:15:44.572221 140290179761920 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.02272992581129074, loss=0.03539925441145897
I0206 07:16:16.380578 140248415348480 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.017215637490153313, loss=0.031219102442264557
I0206 07:16:47.841073 140290179761920 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.026343178004026413, loss=0.03412698954343796
I0206 07:17:19.368701 140248415348480 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.021091707050800323, loss=0.03227924555540085
I0206 07:17:50.704983 140290179761920 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.028530772775411606, loss=0.036050356924533844
I0206 07:18:22.550022 140248415348480 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.0195933785289526, loss=0.03439115732908249
I0206 07:18:53.984898 140290179761920 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.024615656584501266, loss=0.034733060747385025
I0206 07:19:25.838702 140248415348480 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.02183399721980095, loss=0.03809313848614693
I0206 07:19:42.834283 140451058161472 spec.py:321] Evaluating on the training split.
I0206 07:21:18.419933 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 07:21:21.908171 140451058161472 spec.py:349] Evaluating on the test split.
I0206 07:21:25.342517 140451058161472 submission_runner.py:408] Time since start: 5285.52s, 	Step: 11255, 	{'train/accuracy': 0.9901042580604553, 'train/loss': 0.03283253312110901, 'train/mean_average_precision': 0.350322015439398, 'validation/accuracy': 0.9866904020309448, 'validation/loss': 0.044543277472257614, 'validation/mean_average_precision': 0.26153233983646823, 'validation/num_examples': 43793, 'test/accuracy': 0.9857833981513977, 'test/loss': 0.047420669347047806, 'test/mean_average_precision': 0.25411081427279725, 'test/num_examples': 43793, 'score': 3614.5693922042847, 'total_duration': 5285.517816543579, 'accumulated_submission_time': 3614.5693922042847, 'accumulated_eval_time': 1669.9343152046204, 'accumulated_logging_time': 0.6964986324310303}
I0206 07:21:25.362385 140229871392512 logging_writer.py:48] [11255] accumulated_eval_time=1669.934315, accumulated_logging_time=0.696499, accumulated_submission_time=3614.569392, global_step=11255, preemption_count=0, score=3614.569392, test/accuracy=0.985783, test/loss=0.047421, test/mean_average_precision=0.254111, test/num_examples=43793, total_duration=5285.517817, train/accuracy=0.990104, train/loss=0.032833, train/mean_average_precision=0.350322, validation/accuracy=0.986690, validation/loss=0.044543, validation/mean_average_precision=0.261532, validation/num_examples=43793
I0206 07:21:40.375771 140229879785216 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.030097421258687973, loss=0.03296125307679176
I0206 07:22:13.518202 140229871392512 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.028080962598323822, loss=0.03316427767276764
I0206 07:22:46.414699 140229879785216 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.026674872264266014, loss=0.03518969193100929
I0206 07:23:19.294530 140229871392512 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.022423790767788887, loss=0.035214539617300034
I0206 07:23:51.683341 140229879785216 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.02368149533867836, loss=0.0337393693625927
I0206 07:24:24.321801 140229871392512 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.03627676144242287, loss=0.0331672839820385
I0206 07:24:56.301169 140229879785216 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.023166945204138756, loss=0.03352572023868561
I0206 07:25:25.494502 140451058161472 spec.py:321] Evaluating on the training split.
I0206 07:27:06.874805 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 07:27:09.997305 140451058161472 spec.py:349] Evaluating on the test split.
I0206 07:27:12.980297 140451058161472 submission_runner.py:408] Time since start: 5633.16s, 	Step: 11991, 	{'train/accuracy': 0.9903876781463623, 'train/loss': 0.03184294328093529, 'train/mean_average_precision': 0.35954482673416543, 'validation/accuracy': 0.9868012070655823, 'validation/loss': 0.044386640191078186, 'validation/mean_average_precision': 0.25978610865418417, 'validation/num_examples': 43793, 'test/accuracy': 0.985854983329773, 'test/loss': 0.047328077256679535, 'test/mean_average_precision': 0.2575606978341206, 'test/num_examples': 43793, 'score': 3854.664434194565, 'total_duration': 5633.155611276627, 'accumulated_submission_time': 3854.664434194565, 'accumulated_eval_time': 1777.4200673103333, 'accumulated_logging_time': 0.7287240028381348}
I0206 07:27:12.997886 140248415348480 logging_writer.py:48] [11991] accumulated_eval_time=1777.420067, accumulated_logging_time=0.728724, accumulated_submission_time=3854.664434, global_step=11991, preemption_count=0, score=3854.664434, test/accuracy=0.985855, test/loss=0.047328, test/mean_average_precision=0.257561, test/num_examples=43793, total_duration=5633.155611, train/accuracy=0.990388, train/loss=0.031843, train/mean_average_precision=0.359545, validation/accuracy=0.986801, validation/loss=0.044387, validation/mean_average_precision=0.259786, validation/num_examples=43793
I0206 07:27:16.320986 140290188154624 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.028640778735280037, loss=0.03235441818833351
I0206 07:27:48.848166 140248415348480 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.024314187467098236, loss=0.032894767820835114
I0206 07:28:21.228609 140290188154624 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.03712740167975426, loss=0.03293944522738457
I0206 07:28:53.397058 140248415348480 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.028861617669463158, loss=0.0325043611228466
I0206 07:29:24.996477 140290188154624 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.02653096430003643, loss=0.03349506855010986
I0206 07:29:56.724167 140248415348480 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.02733541838824749, loss=0.03095291368663311
I0206 07:30:28.566293 140290188154624 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.027611717581748962, loss=0.03431431204080582
I0206 07:31:00.122274 140248415348480 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.027028603479266167, loss=0.03541785851120949
I0206 07:31:13.236832 140451058161472 spec.py:321] Evaluating on the training split.
I0206 07:32:49.756704 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 07:32:52.808843 140451058161472 spec.py:349] Evaluating on the test split.
I0206 07:32:55.826444 140451058161472 submission_runner.py:408] Time since start: 5976.00s, 	Step: 12741, 	{'train/accuracy': 0.9904323220252991, 'train/loss': 0.03173898905515671, 'train/mean_average_precision': 0.3702695672831442, 'validation/accuracy': 0.986777663230896, 'validation/loss': 0.04454327002167702, 'validation/mean_average_precision': 0.2593790380554532, 'validation/num_examples': 43793, 'test/accuracy': 0.985881507396698, 'test/loss': 0.04745806008577347, 'test/mean_average_precision': 0.25098736000289323, 'test/num_examples': 43793, 'score': 4094.8716711997986, 'total_duration': 5976.001766443253, 'accumulated_submission_time': 4094.8716711997986, 'accumulated_eval_time': 1880.0096390247345, 'accumulated_logging_time': 0.7573575973510742}
I0206 07:32:55.843771 140229879785216 logging_writer.py:48] [12741] accumulated_eval_time=1880.009639, accumulated_logging_time=0.757358, accumulated_submission_time=4094.871671, global_step=12741, preemption_count=0, score=4094.871671, test/accuracy=0.985882, test/loss=0.047458, test/mean_average_precision=0.250987, test/num_examples=43793, total_duration=5976.001766, train/accuracy=0.990432, train/loss=0.031739, train/mean_average_precision=0.370270, validation/accuracy=0.986778, validation/loss=0.044543, validation/mean_average_precision=0.259379, validation/num_examples=43793
I0206 07:33:15.178726 140290179761920 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.031986307352781296, loss=0.03574416786432266
I0206 07:33:47.098169 140229879785216 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.02695394866168499, loss=0.032539252191782
I0206 07:34:19.333207 140290179761920 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.021677205339074135, loss=0.029149679467082024
I0206 07:34:51.650074 140229879785216 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.03974218666553497, loss=0.03181628882884979
I0206 07:35:24.418639 140290179761920 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.023874614387750626, loss=0.03026844561100006
I0206 07:35:56.337980 140229879785216 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.029384959489107132, loss=0.031827155500650406
I0206 07:36:28.544085 140290179761920 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.02782401815056801, loss=0.03106146864593029
I0206 07:36:55.972723 140451058161472 spec.py:321] Evaluating on the training split.
I0206 07:38:32.505544 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 07:38:35.579450 140451058161472 spec.py:349] Evaluating on the test split.
I0206 07:38:38.654182 140451058161472 submission_runner.py:408] Time since start: 6318.83s, 	Step: 13487, 	{'train/accuracy': 0.9905805587768555, 'train/loss': 0.03104410320520401, 'train/mean_average_precision': 0.39502075970650574, 'validation/accuracy': 0.9867857694625854, 'validation/loss': 0.04424291104078293, 'validation/mean_average_precision': 0.2639529675838791, 'validation/num_examples': 43793, 'test/accuracy': 0.9858739376068115, 'test/loss': 0.04716877639293671, 'test/mean_average_precision': 0.2534053815363039, 'test/num_examples': 43793, 'score': 4334.968943119049, 'total_duration': 6318.829391956329, 'accumulated_submission_time': 4334.968943119049, 'accumulated_eval_time': 1982.690937757492, 'accumulated_logging_time': 0.785717248916626}
I0206 07:38:38.684715 140229871392512 logging_writer.py:48] [13487] accumulated_eval_time=1982.690938, accumulated_logging_time=0.785717, accumulated_submission_time=4334.968943, global_step=13487, preemption_count=0, score=4334.968943, test/accuracy=0.985874, test/loss=0.047169, test/mean_average_precision=0.253405, test/num_examples=43793, total_duration=6318.829392, train/accuracy=0.990581, train/loss=0.031044, train/mean_average_precision=0.395021, validation/accuracy=0.986786, validation/loss=0.044243, validation/mean_average_precision=0.263953, validation/num_examples=43793
I0206 07:38:43.314139 140290188154624 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.038513828068971634, loss=0.030964534729719162
I0206 07:39:15.890645 140229871392512 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.028539767488837242, loss=0.032876867800951004
I0206 07:39:47.512764 140290188154624 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.028508860617876053, loss=0.03128901124000549
I0206 07:40:19.470500 140229871392512 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.029960766434669495, loss=0.02891971915960312
I0206 07:40:50.794194 140290188154624 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.03460465371608734, loss=0.032944902777671814
I0206 07:41:22.678899 140229871392512 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.03063586913049221, loss=0.032243840396404266
I0206 07:41:54.559051 140290188154624 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.03524336591362953, loss=0.03486071527004242
I0206 07:42:26.501324 140229871392512 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.0422339029610157, loss=0.032350312918424606
I0206 07:42:38.790278 140451058161472 spec.py:321] Evaluating on the training split.
I0206 07:44:16.533066 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 07:44:19.662086 140451058161472 spec.py:349] Evaluating on the test split.
I0206 07:44:22.675061 140451058161472 submission_runner.py:408] Time since start: 6662.85s, 	Step: 14240, 	{'train/accuracy': 0.9905785918235779, 'train/loss': 0.030892036855220795, 'train/mean_average_precision': 0.3902965871620472, 'validation/accuracy': 0.9868515133857727, 'validation/loss': 0.04441400617361069, 'validation/mean_average_precision': 0.26351909600181445, 'validation/num_examples': 43793, 'test/accuracy': 0.9859964847564697, 'test/loss': 0.04715034365653992, 'test/mean_average_precision': 0.2576448284120592, 'test/num_examples': 43793, 'score': 4575.041334629059, 'total_duration': 6662.850385427475, 'accumulated_submission_time': 4575.041334629059, 'accumulated_eval_time': 2086.5756731033325, 'accumulated_logging_time': 0.8285675048828125}
I0206 07:44:22.695224 140248415348480 logging_writer.py:48] [14240] accumulated_eval_time=2086.575673, accumulated_logging_time=0.828568, accumulated_submission_time=4575.041335, global_step=14240, preemption_count=0, score=4575.041335, test/accuracy=0.985996, test/loss=0.047150, test/mean_average_precision=0.257645, test/num_examples=43793, total_duration=6662.850385, train/accuracy=0.990579, train/loss=0.030892, train/mean_average_precision=0.390297, validation/accuracy=0.986852, validation/loss=0.044414, validation/mean_average_precision=0.263519, validation/num_examples=43793
I0206 07:44:41.977574 140290179761920 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.03181939572095871, loss=0.03277015686035156
I0206 07:45:13.764569 140248415348480 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.034309953451156616, loss=0.03335871547460556
I0206 07:45:45.629725 140290179761920 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.031321633607149124, loss=0.029257753863930702
I0206 07:46:17.984965 140248415348480 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.040947139263153076, loss=0.03440793603658676
I0206 07:46:49.758516 140290179761920 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.04162300005555153, loss=0.032581180334091187
I0206 07:47:21.895607 140248415348480 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.037745147943496704, loss=0.02815229445695877
I0206 07:47:53.489785 140290179761920 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.04267400503158569, loss=0.030612925067543983
I0206 07:48:22.739257 140451058161472 spec.py:321] Evaluating on the training split.
I0206 07:49:56.137136 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 07:49:59.188479 140451058161472 spec.py:349] Evaluating on the test split.
I0206 07:50:02.399628 140451058161472 submission_runner.py:408] Time since start: 7002.57s, 	Step: 14992, 	{'train/accuracy': 0.9906534552574158, 'train/loss': 0.03046361729502678, 'train/mean_average_precision': 0.4054393168607071, 'validation/accuracy': 0.9867991805076599, 'validation/loss': 0.044427357614040375, 'validation/mean_average_precision': 0.26020088163370275, 'validation/num_examples': 43793, 'test/accuracy': 0.9859017133712769, 'test/loss': 0.04743032902479172, 'test/mean_average_precision': 0.25452994743615387, 'test/num_examples': 43793, 'score': 4815.053724527359, 'total_duration': 7002.574951410294, 'accumulated_submission_time': 4815.053724527359, 'accumulated_eval_time': 2186.236001253128, 'accumulated_logging_time': 0.8597097396850586}
I0206 07:50:02.417608 140229879785216 logging_writer.py:48] [14992] accumulated_eval_time=2186.236001, accumulated_logging_time=0.859710, accumulated_submission_time=4815.053725, global_step=14992, preemption_count=0, score=4815.053725, test/accuracy=0.985902, test/loss=0.047430, test/mean_average_precision=0.254530, test/num_examples=43793, total_duration=7002.574951, train/accuracy=0.990653, train/loss=0.030464, train/mean_average_precision=0.405439, validation/accuracy=0.986799, validation/loss=0.044427, validation/mean_average_precision=0.260201, validation/num_examples=43793
I0206 07:50:05.234382 140290188154624 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.04387349635362625, loss=0.0338180847465992
I0206 07:50:36.686368 140229879785216 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.03355113044381142, loss=0.0299260001629591
I0206 07:51:08.518732 140290188154624 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.03737669810652733, loss=0.033243414014577866
I0206 07:51:40.246752 140229879785216 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.04217708855867386, loss=0.030847053974866867
I0206 07:52:12.050715 140290188154624 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.038729116320610046, loss=0.03132544085383415
I0206 07:52:43.924027 140229879785216 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.03289176523685455, loss=0.03304212912917137
I0206 07:53:16.151922 140290188154624 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.035637445747852325, loss=0.03382054343819618
I0206 07:53:48.305256 140229879785216 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.03512035682797432, loss=0.03234638273715973
I0206 07:54:02.614349 140451058161472 spec.py:321] Evaluating on the training split.
I0206 07:55:41.213829 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 07:55:44.215500 140451058161472 spec.py:349] Evaluating on the test split.
I0206 07:55:47.184118 140451058161472 submission_runner.py:408] Time since start: 7347.36s, 	Step: 15745, 	{'train/accuracy': 0.9910573363304138, 'train/loss': 0.029347719624638557, 'train/mean_average_precision': 0.4316723557821035, 'validation/accuracy': 0.9867650866508484, 'validation/loss': 0.044589005410671234, 'validation/mean_average_precision': 0.2701452169981196, 'validation/num_examples': 43793, 'test/accuracy': 0.985859215259552, 'test/loss': 0.04725828394293785, 'test/mean_average_precision': 0.2598296965103563, 'test/num_examples': 43793, 'score': 5055.218120336533, 'total_duration': 7347.359417915344, 'accumulated_submission_time': 5055.218120336533, 'accumulated_eval_time': 2290.80570602417, 'accumulated_logging_time': 0.8886802196502686}
I0206 07:55:47.202165 140229871392512 logging_writer.py:48] [15745] accumulated_eval_time=2290.805706, accumulated_logging_time=0.888680, accumulated_submission_time=5055.218120, global_step=15745, preemption_count=0, score=5055.218120, test/accuracy=0.985859, test/loss=0.047258, test/mean_average_precision=0.259830, test/num_examples=43793, total_duration=7347.359418, train/accuracy=0.991057, train/loss=0.029348, train/mean_average_precision=0.431672, validation/accuracy=0.986765, validation/loss=0.044589, validation/mean_average_precision=0.270145, validation/num_examples=43793
I0206 07:56:05.063595 140290179761920 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.038007915019989014, loss=0.03207172453403473
I0206 07:56:36.663548 140229871392512 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.03386242315173149, loss=0.032253198325634
I0206 07:57:08.284945 140290179761920 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.03234531357884407, loss=0.031256821006536484
I0206 07:57:39.706977 140229871392512 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.03557027876377106, loss=0.03125802427530289
I0206 07:58:11.384145 140290179761920 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.03797316178679466, loss=0.03235281631350517
I0206 07:58:42.640075 140229871392512 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.044266846030950546, loss=0.033474840223789215
I0206 07:59:14.383174 140290179761920 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.040690138936042786, loss=0.031923405826091766
I0206 07:59:45.990544 140229871392512 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.04929875582456589, loss=0.035101812332868576
I0206 07:59:47.242410 140451058161472 spec.py:321] Evaluating on the training split.
I0206 08:01:22.506606 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 08:01:25.553616 140451058161472 spec.py:349] Evaluating on the test split.
I0206 08:01:28.529363 140451058161472 submission_runner.py:408] Time since start: 7688.70s, 	Step: 16505, 	{'train/accuracy': 0.9910054802894592, 'train/loss': 0.0295385904610157, 'train/mean_average_precision': 0.4293138323079307, 'validation/accuracy': 0.9866920113563538, 'validation/loss': 0.044278547167778015, 'validation/mean_average_precision': 0.2718293568793558, 'validation/num_examples': 43793, 'test/accuracy': 0.985924482345581, 'test/loss': 0.046870458871126175, 'test/mean_average_precision': 0.2595190000831163, 'test/num_examples': 43793, 'score': 5295.226839065552, 'total_duration': 7688.704651594162, 'accumulated_submission_time': 5295.226839065552, 'accumulated_eval_time': 2392.0925753116608, 'accumulated_logging_time': 0.9176356792449951}
I0206 08:01:28.547511 140248415348480 logging_writer.py:48] [16505] accumulated_eval_time=2392.092575, accumulated_logging_time=0.917636, accumulated_submission_time=5295.226839, global_step=16505, preemption_count=0, score=5295.226839, test/accuracy=0.985924, test/loss=0.046870, test/mean_average_precision=0.259519, test/num_examples=43793, total_duration=7688.704652, train/accuracy=0.991005, train/loss=0.029539, train/mean_average_precision=0.429314, validation/accuracy=0.986692, validation/loss=0.044279, validation/mean_average_precision=0.271829, validation/num_examples=43793
I0206 08:01:59.196424 140290188154624 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.03178867697715759, loss=0.03354789316654205
I0206 08:02:30.658216 140248415348480 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.051495082676410675, loss=0.0338568389415741
I0206 08:03:01.919407 140290188154624 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.03563971444964409, loss=0.031244972720742226
I0206 08:03:33.521798 140248415348480 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.0561496876180172, loss=0.030843527987599373
I0206 08:04:05.004307 140290188154624 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.045689795166254044, loss=0.032876159995794296
I0206 08:04:36.469201 140248415348480 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.03626123443245888, loss=0.02959037572145462
I0206 08:05:08.231366 140290188154624 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.03501778468489647, loss=0.032448772341012955
I0206 08:05:28.751768 140451058161472 spec.py:321] Evaluating on the training split.
I0206 08:07:00.736122 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 08:07:03.960409 140451058161472 spec.py:349] Evaluating on the test split.
I0206 08:07:06.969825 140451058161472 submission_runner.py:408] Time since start: 8027.15s, 	Step: 17266, 	{'train/accuracy': 0.9910851716995239, 'train/loss': 0.029401373118162155, 'train/mean_average_precision': 0.4287108022495856, 'validation/accuracy': 0.9867752194404602, 'validation/loss': 0.04428261145949364, 'validation/mean_average_precision': 0.2713982672671105, 'validation/num_examples': 43793, 'test/accuracy': 0.9859097599983215, 'test/loss': 0.04684152081608772, 'test/mean_average_precision': 0.25966081320256623, 'test/num_examples': 43793, 'score': 5535.3997938632965, 'total_duration': 8027.145152568817, 'accumulated_submission_time': 5535.3997938632965, 'accumulated_eval_time': 2490.3105919361115, 'accumulated_logging_time': 0.9467637538909912}
I0206 08:07:06.988941 140229871392512 logging_writer.py:48] [17266] accumulated_eval_time=2490.310592, accumulated_logging_time=0.946764, accumulated_submission_time=5535.399794, global_step=17266, preemption_count=0, score=5535.399794, test/accuracy=0.985910, test/loss=0.046842, test/mean_average_precision=0.259661, test/num_examples=43793, total_duration=8027.145153, train/accuracy=0.991085, train/loss=0.029401, train/mean_average_precision=0.428711, validation/accuracy=0.986775, validation/loss=0.044283, validation/mean_average_precision=0.271398, validation/num_examples=43793
I0206 08:07:18.222685 140290179761920 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.03379622846841812, loss=0.02946440689265728
I0206 08:07:49.541140 140229871392512 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.034058332443237305, loss=0.029810719192028046
I0206 08:08:21.065771 140290179761920 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.040125079452991486, loss=0.03217797726392746
I0206 08:08:53.235004 140229871392512 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.04244508221745491, loss=0.028115207329392433
I0206 08:09:25.049272 140290179761920 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.03752540051937103, loss=0.02915007621049881
I0206 08:09:57.150354 140229871392512 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.03443211689591408, loss=0.030213765799999237
I0206 08:10:28.997322 140290179761920 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.04893573746085167, loss=0.03155386075377464
I0206 08:11:00.774059 140229871392512 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.03425087034702301, loss=0.029024124145507812
I0206 08:11:07.242814 140451058161472 spec.py:321] Evaluating on the training split.
I0206 08:12:44.385482 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 08:12:47.532542 140451058161472 spec.py:349] Evaluating on the test split.
I0206 08:12:50.591554 140451058161472 submission_runner.py:408] Time since start: 8370.77s, 	Step: 18021, 	{'train/accuracy': 0.9907681941986084, 'train/loss': 0.030180849134922028, 'train/mean_average_precision': 0.4159931679730896, 'validation/accuracy': 0.9868023991584778, 'validation/loss': 0.044088248163461685, 'validation/mean_average_precision': 0.27192529178497843, 'validation/num_examples': 43793, 'test/accuracy': 0.9859362840652466, 'test/loss': 0.04672218859195709, 'test/mean_average_precision': 0.2607173361411814, 'test/num_examples': 43793, 'score': 5775.622433185577, 'total_duration': 8370.766879558563, 'accumulated_submission_time': 5775.622433185577, 'accumulated_eval_time': 2593.659286260605, 'accumulated_logging_time': 0.9766237735748291}
I0206 08:12:50.610273 140229879785216 logging_writer.py:48] [18021] accumulated_eval_time=2593.659286, accumulated_logging_time=0.976624, accumulated_submission_time=5775.622433, global_step=18021, preemption_count=0, score=5775.622433, test/accuracy=0.985936, test/loss=0.046722, test/mean_average_precision=0.260717, test/num_examples=43793, total_duration=8370.766880, train/accuracy=0.990768, train/loss=0.030181, train/mean_average_precision=0.415993, validation/accuracy=0.986802, validation/loss=0.044088, validation/mean_average_precision=0.271925, validation/num_examples=43793
I0206 08:13:15.808284 140290188154624 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.05183975771069527, loss=0.03506609424948692
I0206 08:13:47.038660 140229879785216 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.04020507633686066, loss=0.03048589453101158
I0206 08:14:18.724434 140290188154624 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.041116371750831604, loss=0.03126746788620949
I0206 08:14:50.128794 140229879785216 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.03842790797352791, loss=0.03109314851462841
I0206 08:15:21.625490 140290188154624 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.041108641773462296, loss=0.0308312326669693
I0206 08:15:52.734073 140229879785216 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.05161886662244797, loss=0.03179460018873215
I0206 08:16:24.396790 140290188154624 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.053321000188589096, loss=0.034115273505449295
I0206 08:16:50.813791 140451058161472 spec.py:321] Evaluating on the training split.
I0206 08:18:25.199536 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 08:18:28.385222 140451058161472 spec.py:349] Evaluating on the test split.
I0206 08:18:31.528229 140451058161472 submission_runner.py:408] Time since start: 8711.70s, 	Step: 18785, 	{'train/accuracy': 0.9908871650695801, 'train/loss': 0.029914895072579384, 'train/mean_average_precision': 0.40294541791037364, 'validation/accuracy': 0.9867849946022034, 'validation/loss': 0.04428034648299217, 'validation/mean_average_precision': 0.26940112567789487, 'validation/num_examples': 43793, 'test/accuracy': 0.9860011339187622, 'test/loss': 0.046740517020225525, 'test/mean_average_precision': 0.25958977611508466, 'test/num_examples': 43793, 'score': 6015.79421544075, 'total_duration': 8711.703551054, 'accumulated_submission_time': 6015.79421544075, 'accumulated_eval_time': 2694.3736753463745, 'accumulated_logging_time': 1.0064642429351807}
I0206 08:18:31.547661 140229871392512 logging_writer.py:48] [18785] accumulated_eval_time=2694.373675, accumulated_logging_time=1.006464, accumulated_submission_time=6015.794215, global_step=18785, preemption_count=0, score=6015.794215, test/accuracy=0.986001, test/loss=0.046741, test/mean_average_precision=0.259590, test/num_examples=43793, total_duration=8711.703551, train/accuracy=0.990887, train/loss=0.029915, train/mean_average_precision=0.402945, validation/accuracy=0.986785, validation/loss=0.044280, validation/mean_average_precision=0.269401, validation/num_examples=43793
I0206 08:18:37.091147 140290179761920 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.04518163204193115, loss=0.028453925624489784
I0206 08:19:09.252828 140229871392512 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.03832143545150757, loss=0.03137028217315674
I0206 08:19:42.213589 140290179761920 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.038928378373384476, loss=0.03383835405111313
I0206 08:20:14.931798 140229871392512 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.0375404916703701, loss=0.026178278028964996
I0206 08:20:47.123849 140290179761920 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.04605923965573311, loss=0.03268836438655853
I0206 08:21:19.678202 140229871392512 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.04213675484061241, loss=0.02923346683382988
I0206 08:21:51.694799 140290179761920 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.047944627702236176, loss=0.02952132374048233
I0206 08:22:23.966353 140229871392512 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.04705140367150307, loss=0.031275246292352676
I0206 08:22:31.776244 140451058161472 spec.py:321] Evaluating on the training split.
I0206 08:24:07.360314 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 08:24:10.550777 140451058161472 spec.py:349] Evaluating on the test split.
I0206 08:24:13.661270 140451058161472 submission_runner.py:408] Time since start: 9053.84s, 	Step: 19526, 	{'train/accuracy': 0.9911262392997742, 'train/loss': 0.029100975021719933, 'train/mean_average_precision': 0.42689901254227924, 'validation/accuracy': 0.9868763089179993, 'validation/loss': 0.04428340867161751, 'validation/mean_average_precision': 0.27571393971170777, 'validation/num_examples': 43793, 'test/accuracy': 0.9861102104187012, 'test/loss': 0.04693359136581421, 'test/mean_average_precision': 0.26879631236528245, 'test/num_examples': 43793, 'score': 6255.988671064377, 'total_duration': 9053.836593389511, 'accumulated_submission_time': 6255.988671064377, 'accumulated_eval_time': 2796.258656024933, 'accumulated_logging_time': 1.0368270874023438}
I0206 08:24:13.680202 140229879785216 logging_writer.py:48] [19526] accumulated_eval_time=2796.258656, accumulated_logging_time=1.036827, accumulated_submission_time=6255.988671, global_step=19526, preemption_count=0, score=6255.988671, test/accuracy=0.986110, test/loss=0.046934, test/mean_average_precision=0.268796, test/num_examples=43793, total_duration=9053.836593, train/accuracy=0.991126, train/loss=0.029101, train/mean_average_precision=0.426899, validation/accuracy=0.986876, validation/loss=0.044283, validation/mean_average_precision=0.275714, validation/num_examples=43793
I0206 08:24:37.716562 140248415348480 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.06393355876207352, loss=0.032913047820329666
I0206 08:25:09.667901 140229879785216 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.05118347331881523, loss=0.03506658971309662
I0206 08:25:41.387954 140248415348480 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.043519631028175354, loss=0.03155030682682991
I0206 08:26:13.191478 140229879785216 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.03799373656511307, loss=0.030155107378959656
I0206 08:26:44.865373 140248415348480 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.042260900139808655, loss=0.031143849715590477
I0206 08:27:17.091808 140229879785216 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.04777897894382477, loss=0.032847773283720016
I0206 08:27:48.729661 140248415348480 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.03918769210577011, loss=0.03393701836466789
I0206 08:28:13.877651 140451058161472 spec.py:321] Evaluating on the training split.
I0206 08:29:48.421780 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 08:29:51.452718 140451058161472 spec.py:349] Evaluating on the test split.
I0206 08:29:54.473064 140451058161472 submission_runner.py:408] Time since start: 9394.65s, 	Step: 20280, 	{'train/accuracy': 0.9910481572151184, 'train/loss': 0.029240449890494347, 'train/mean_average_precision': 0.43098628504950526, 'validation/accuracy': 0.9868617057800293, 'validation/loss': 0.044349849224090576, 'validation/mean_average_precision': 0.2763822320770312, 'validation/num_examples': 43793, 'test/accuracy': 0.986026406288147, 'test/loss': 0.04711419716477394, 'test/mean_average_precision': 0.26216126729949074, 'test/num_examples': 43793, 'score': 6496.154449701309, 'total_duration': 9394.648380041122, 'accumulated_submission_time': 6496.154449701309, 'accumulated_eval_time': 2896.8540201187134, 'accumulated_logging_time': 1.0669054985046387}
I0206 08:29:54.492395 140290179761920 logging_writer.py:48] [20280] accumulated_eval_time=2896.854020, accumulated_logging_time=1.066905, accumulated_submission_time=6496.154450, global_step=20280, preemption_count=0, score=6496.154450, test/accuracy=0.986026, test/loss=0.047114, test/mean_average_precision=0.262161, test/num_examples=43793, total_duration=9394.648380, train/accuracy=0.991048, train/loss=0.029240, train/mean_average_precision=0.430986, validation/accuracy=0.986862, validation/loss=0.044350, validation/mean_average_precision=0.276382, validation/num_examples=43793
I0206 08:30:01.629372 140290188154624 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.04011916741728783, loss=0.029355255886912346
I0206 08:30:33.597601 140290179761920 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.054263386875391006, loss=0.03088202513754368
I0206 08:31:05.310379 140290188154624 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.04663732647895813, loss=0.033477168530225754
I0206 08:31:37.332909 140290179761920 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.04167823866009712, loss=0.03257213160395622
I0206 08:32:09.218843 140290188154624 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.060283951461315155, loss=0.033416036516427994
I0206 08:32:40.989669 140290179761920 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.04754667729139328, loss=0.031710680574178696
I0206 08:33:12.761469 140290188154624 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.045617423951625824, loss=0.028515631332993507
I0206 08:33:44.474566 140290179761920 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.05125473812222481, loss=0.03728264197707176
I0206 08:33:54.636926 140451058161472 spec.py:321] Evaluating on the training split.
I0206 08:35:32.078870 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 08:35:35.245509 140451058161472 spec.py:349] Evaluating on the test split.
I0206 08:35:38.392144 140451058161472 submission_runner.py:408] Time since start: 9738.57s, 	Step: 21033, 	{'train/accuracy': 0.9910216927528381, 'train/loss': 0.029087426140904427, 'train/mean_average_precision': 0.44519078951854646, 'validation/accuracy': 0.9868708252906799, 'validation/loss': 0.044247254729270935, 'validation/mean_average_precision': 0.27284887694411497, 'validation/num_examples': 43793, 'test/accuracy': 0.9860247373580933, 'test/loss': 0.04698428511619568, 'test/mean_average_precision': 0.2614326363612846, 'test/num_examples': 43793, 'score': 6736.26745891571, 'total_duration': 9738.567348957062, 'accumulated_submission_time': 6736.26745891571, 'accumulated_eval_time': 3000.609070301056, 'accumulated_logging_time': 1.0970373153686523}
I0206 08:35:38.412006 140229871392512 logging_writer.py:48] [21033] accumulated_eval_time=3000.609070, accumulated_logging_time=1.097037, accumulated_submission_time=6736.267459, global_step=21033, preemption_count=0, score=6736.267459, test/accuracy=0.986025, test/loss=0.046984, test/mean_average_precision=0.261433, test/num_examples=43793, total_duration=9738.567349, train/accuracy=0.991022, train/loss=0.029087, train/mean_average_precision=0.445191, validation/accuracy=0.986871, validation/loss=0.044247, validation/mean_average_precision=0.272849, validation/num_examples=43793
I0206 08:36:00.034554 140248415348480 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.054335907101631165, loss=0.032807379961013794
I0206 08:36:31.624320 140229871392512 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.0437464639544487, loss=0.030655205249786377
I0206 08:37:03.302202 140248415348480 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.052522893995046616, loss=0.031185900792479515
I0206 08:37:34.531075 140229871392512 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.041867610067129135, loss=0.029775232076644897
I0206 08:38:05.540751 140248415348480 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.04529975727200508, loss=0.030282754451036453
I0206 08:38:36.624339 140229871392512 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.04464297741651535, loss=0.029487859457731247
I0206 08:39:07.895398 140248415348480 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.05127596855163574, loss=0.03026912361383438
I0206 08:39:38.603936 140451058161472 spec.py:321] Evaluating on the training split.
I0206 08:41:15.148148 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 08:41:18.645505 140451058161472 spec.py:349] Evaluating on the test split.
I0206 08:41:22.018517 140451058161472 submission_runner.py:408] Time since start: 10082.19s, 	Step: 21799, 	{'train/accuracy': 0.9912761449813843, 'train/loss': 0.02821793407201767, 'train/mean_average_precision': 0.45827813256329597, 'validation/accuracy': 0.9867464303970337, 'validation/loss': 0.04423868656158447, 'validation/mean_average_precision': 0.2674546311890728, 'validation/num_examples': 43793, 'test/accuracy': 0.9859337210655212, 'test/loss': 0.046887047588825226, 'test/mean_average_precision': 0.26471127378471704, 'test/num_examples': 43793, 'score': 6976.427305936813, 'total_duration': 10082.193821668625, 'accumulated_submission_time': 6976.427305936813, 'accumulated_eval_time': 3104.023591041565, 'accumulated_logging_time': 1.1280357837677002}
I0206 08:41:22.040207 140229879785216 logging_writer.py:48] [21799] accumulated_eval_time=3104.023591, accumulated_logging_time=1.128036, accumulated_submission_time=6976.427306, global_step=21799, preemption_count=0, score=6976.427306, test/accuracy=0.985934, test/loss=0.046887, test/mean_average_precision=0.264711, test/num_examples=43793, total_duration=10082.193822, train/accuracy=0.991276, train/loss=0.028218, train/mean_average_precision=0.458278, validation/accuracy=0.986746, validation/loss=0.044239, validation/mean_average_precision=0.267455, validation/num_examples=43793
I0206 08:41:22.759887 140290188154624 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.04550929367542267, loss=0.029263416305184364
I0206 08:41:55.547547 140229879785216 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.05047754943370819, loss=0.028854526579380035
I0206 08:42:27.999599 140290188154624 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.043773092329502106, loss=0.029313426464796066
I0206 08:43:00.170155 140229879785216 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.06243184953927994, loss=0.0318031869828701
I0206 08:43:31.687797 140290188154624 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.052417460829019547, loss=0.030210619792342186
I0206 08:44:03.758017 140229879785216 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.04603039100766182, loss=0.027240244671702385
I0206 08:44:35.909287 140290188154624 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.04272348806262016, loss=0.029145605862140656
I0206 08:45:08.269736 140229879785216 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.04348796233534813, loss=0.032315853983163834
I0206 08:45:22.056724 140451058161472 spec.py:321] Evaluating on the training split.
I0206 08:46:57.551328 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 08:47:00.633712 140451058161472 spec.py:349] Evaluating on the test split.
I0206 08:47:03.879939 140451058161472 submission_runner.py:408] Time since start: 10424.06s, 	Step: 22545, 	{'train/accuracy': 0.9913682341575623, 'train/loss': 0.028164630755782127, 'train/mean_average_precision': 0.4644050467140248, 'validation/accuracy': 0.9868109226226807, 'validation/loss': 0.044437095522880554, 'validation/mean_average_precision': 0.2704546050163061, 'validation/num_examples': 43793, 'test/accuracy': 0.9859463572502136, 'test/loss': 0.047244492918252945, 'test/mean_average_precision': 0.2622843075814916, 'test/num_examples': 43793, 'score': 7216.409387588501, 'total_duration': 10424.055264472961, 'accumulated_submission_time': 7216.409387588501, 'accumulated_eval_time': 3205.8467667102814, 'accumulated_logging_time': 1.1613752841949463}
I0206 08:47:03.899681 140229871392512 logging_writer.py:48] [22545] accumulated_eval_time=3205.846767, accumulated_logging_time=1.161375, accumulated_submission_time=7216.409388, global_step=22545, preemption_count=0, score=7216.409388, test/accuracy=0.985946, test/loss=0.047244, test/mean_average_precision=0.262284, test/num_examples=43793, total_duration=10424.055264, train/accuracy=0.991368, train/loss=0.028165, train/mean_average_precision=0.464405, validation/accuracy=0.986811, validation/loss=0.044437, validation/mean_average_precision=0.270455, validation/num_examples=43793
I0206 08:47:21.801538 140248415348480 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.06252997368574142, loss=0.03446858376264572
I0206 08:47:54.062047 140229871392512 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.04640982300043106, loss=0.028541728854179382
I0206 08:48:26.104468 140248415348480 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.04686518386006355, loss=0.02748623676598072
I0206 08:48:57.784871 140229871392512 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.04992317035794258, loss=0.02848043665289879
I0206 08:49:30.000131 140248415348480 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.055386561900377274, loss=0.03341396152973175
I0206 08:50:01.365125 140229871392512 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.06966646015644073, loss=0.029379453510046005
I0206 08:50:33.435135 140248415348480 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.05066584050655365, loss=0.032295722514390945
I0206 08:51:04.113089 140451058161472 spec.py:321] Evaluating on the training split.
I0206 08:52:40.373455 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 08:52:43.408814 140451058161472 spec.py:349] Evaluating on the test split.
I0206 08:52:46.457951 140451058161472 submission_runner.py:408] Time since start: 10766.63s, 	Step: 23296, 	{'train/accuracy': 0.9915869235992432, 'train/loss': 0.027325928211212158, 'train/mean_average_precision': 0.470983335381583, 'validation/accuracy': 0.9869566559791565, 'validation/loss': 0.0440373495221138, 'validation/mean_average_precision': 0.27909121384071794, 'validation/num_examples': 43793, 'test/accuracy': 0.9861654043197632, 'test/loss': 0.04652325436472893, 'test/mean_average_precision': 0.26892587217486474, 'test/num_examples': 43793, 'score': 7456.590592622757, 'total_duration': 10766.633274793625, 'accumulated_submission_time': 7456.590592622757, 'accumulated_eval_time': 3308.191586256027, 'accumulated_logging_time': 1.1935296058654785}
I0206 08:52:46.477710 140229879785216 logging_writer.py:48] [23296] accumulated_eval_time=3308.191586, accumulated_logging_time=1.193530, accumulated_submission_time=7456.590593, global_step=23296, preemption_count=0, score=7456.590593, test/accuracy=0.986165, test/loss=0.046523, test/mean_average_precision=0.268926, test/num_examples=43793, total_duration=10766.633275, train/accuracy=0.991587, train/loss=0.027326, train/mean_average_precision=0.470983, validation/accuracy=0.986957, validation/loss=0.044037, validation/mean_average_precision=0.279091, validation/num_examples=43793
I0206 08:52:48.119450 140290188154624 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.051462944597005844, loss=0.03419145569205284
I0206 08:53:20.206559 140229879785216 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.04658131301403046, loss=0.029476560652256012
I0206 08:53:51.594835 140290188154624 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.053248897194862366, loss=0.02927548997104168
I0206 08:54:23.281741 140229879785216 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.06049274280667305, loss=0.028407569974660873
I0206 08:54:54.679217 140290188154624 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.06169141083955765, loss=0.030382484197616577
I0206 08:55:26.402044 140229879785216 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.04665656387805939, loss=0.027667544782161713
I0206 08:55:58.069418 140290188154624 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.05753359943628311, loss=0.03059748187661171
I0206 08:56:30.289230 140229879785216 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.05308128148317337, loss=0.03394575044512749
I0206 08:56:46.464399 140451058161472 spec.py:321] Evaluating on the training split.
I0206 08:58:20.051363 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 08:58:23.148518 140451058161472 spec.py:349] Evaluating on the test split.
I0206 08:58:26.120231 140451058161472 submission_runner.py:408] Time since start: 11106.30s, 	Step: 24052, 	{'train/accuracy': 0.991644561290741, 'train/loss': 0.027014605700969696, 'train/mean_average_precision': 0.48840583600806126, 'validation/accuracy': 0.9869595170021057, 'validation/loss': 0.04417118802666664, 'validation/mean_average_precision': 0.28204949998268863, 'validation/num_examples': 43793, 'test/accuracy': 0.9862087965011597, 'test/loss': 0.04691678285598755, 'test/mean_average_precision': 0.2769371208521102, 'test/num_examples': 43793, 'score': 7696.545921325684, 'total_duration': 11106.29555439949, 'accumulated_submission_time': 7696.545921325684, 'accumulated_eval_time': 3407.847371816635, 'accumulated_logging_time': 1.2242672443389893}
I0206 08:58:26.140231 140248415348480 logging_writer.py:48] [24052] accumulated_eval_time=3407.847372, accumulated_logging_time=1.224267, accumulated_submission_time=7696.545921, global_step=24052, preemption_count=0, score=7696.545921, test/accuracy=0.986209, test/loss=0.046917, test/mean_average_precision=0.276937, test/num_examples=43793, total_duration=11106.295554, train/accuracy=0.991645, train/loss=0.027015, train/mean_average_precision=0.488406, validation/accuracy=0.986960, validation/loss=0.044171, validation/mean_average_precision=0.282049, validation/num_examples=43793
I0206 08:58:41.665682 140290179761920 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.05317090451717377, loss=0.0324123352766037
I0206 08:59:13.636689 140248415348480 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.055878669023513794, loss=0.031273189932107925
I0206 08:59:45.689241 140290179761920 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.052696555852890015, loss=0.032243430614471436
I0206 09:00:18.086143 140248415348480 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.05050293356180191, loss=0.029771752655506134
I0206 09:00:49.971402 140290179761920 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.06693551689386368, loss=0.032710712403059006
I0206 09:01:22.364140 140248415348480 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.05083014816045761, loss=0.030761253088712692
I0206 09:01:54.701570 140290179761920 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.05084523186087608, loss=0.03008059598505497
I0206 09:02:26.132667 140451058161472 spec.py:321] Evaluating on the training split.
I0206 09:04:06.539946 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 09:04:09.586236 140451058161472 spec.py:349] Evaluating on the test split.
I0206 09:04:12.542034 140451058161472 submission_runner.py:408] Time since start: 11452.72s, 	Step: 24798, 	{'train/accuracy': 0.9915668368339539, 'train/loss': 0.02746260166168213, 'train/mean_average_precision': 0.47432643847940575, 'validation/accuracy': 0.9868316650390625, 'validation/loss': 0.044266603887081146, 'validation/mean_average_precision': 0.2701833417002129, 'validation/num_examples': 43793, 'test/accuracy': 0.9860756993293762, 'test/loss': 0.046777963638305664, 'test/mean_average_precision': 0.2672637130743255, 'test/num_examples': 43793, 'score': 7936.503590583801, 'total_duration': 11452.717252254486, 'accumulated_submission_time': 7936.503590583801, 'accumulated_eval_time': 3514.256602048874, 'accumulated_logging_time': 1.25538969039917}
I0206 09:04:12.562077 140229871392512 logging_writer.py:48] [24798] accumulated_eval_time=3514.256602, accumulated_logging_time=1.255390, accumulated_submission_time=7936.503591, global_step=24798, preemption_count=0, score=7936.503591, test/accuracy=0.986076, test/loss=0.046778, test/mean_average_precision=0.267264, test/num_examples=43793, total_duration=11452.717252, train/accuracy=0.991567, train/loss=0.027463, train/mean_average_precision=0.474326, validation/accuracy=0.986832, validation/loss=0.044267, validation/mean_average_precision=0.270183, validation/num_examples=43793
I0206 09:04:13.523727 140290188154624 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.047372013330459595, loss=0.028792647644877434
I0206 09:04:45.144134 140229871392512 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.052006497979164124, loss=0.02926626242697239
I0206 09:05:16.835270 140290188154624 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.07940573245286942, loss=0.02856622077524662
I0206 09:05:48.406211 140229871392512 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.051767636090517044, loss=0.030078455805778503
I0206 09:06:20.155728 140290188154624 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.052678778767585754, loss=0.02886962890625
I0206 09:06:51.693798 140229871392512 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.05568668246269226, loss=0.03159839287400246
I0206 09:07:23.264513 140290188154624 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.05823596939444542, loss=0.032221268862485886
I0206 09:07:55.164355 140229871392512 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.04693961143493652, loss=0.026938363909721375
I0206 09:08:12.638303 140451058161472 spec.py:321] Evaluating on the training split.
I0206 09:09:47.114409 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 09:09:50.178086 140451058161472 spec.py:349] Evaluating on the test split.
I0206 09:09:53.171873 140451058161472 submission_runner.py:408] Time since start: 11793.35s, 	Step: 25556, 	{'train/accuracy': 0.9913595914840698, 'train/loss': 0.028126923367381096, 'train/mean_average_precision': 0.4622975459881457, 'validation/accuracy': 0.9869400262832642, 'validation/loss': 0.04411906376481056, 'validation/mean_average_precision': 0.2790331791350799, 'validation/num_examples': 43793, 'test/accuracy': 0.9861093759536743, 'test/loss': 0.046621087938547134, 'test/mean_average_precision': 0.27050036445854647, 'test/num_examples': 43793, 'score': 8176.548576593399, 'total_duration': 11793.347196340561, 'accumulated_submission_time': 8176.548576593399, 'accumulated_eval_time': 3614.790126800537, 'accumulated_logging_time': 1.2863295078277588}
I0206 09:09:53.192249 140229879785216 logging_writer.py:48] [25556] accumulated_eval_time=3614.790127, accumulated_logging_time=1.286330, accumulated_submission_time=8176.548577, global_step=25556, preemption_count=0, score=8176.548577, test/accuracy=0.986109, test/loss=0.046621, test/mean_average_precision=0.270500, test/num_examples=43793, total_duration=11793.347196, train/accuracy=0.991360, train/loss=0.028127, train/mean_average_precision=0.462298, validation/accuracy=0.986940, validation/loss=0.044119, validation/mean_average_precision=0.279033, validation/num_examples=43793
I0206 09:10:07.474579 140248415348480 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.06027853488922119, loss=0.02962147817015648
I0206 09:10:38.880966 140229879785216 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.04607278108596802, loss=0.02881794609129429
I0206 09:11:10.532667 140248415348480 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.054326627403497696, loss=0.029580600559711456
I0206 09:11:42.185470 140229879785216 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.05999649316072464, loss=0.029078947380185127
I0206 09:12:13.833717 140248415348480 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.06142973527312279, loss=0.028673766180872917
I0206 09:12:45.254366 140229879785216 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.05836833268404007, loss=0.02988681010901928
I0206 09:13:16.911817 140248415348480 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.05889977887272835, loss=0.03005152754485607
I0206 09:13:49.277236 140229879785216 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.05683282017707825, loss=0.03230244666337967
I0206 09:13:53.309383 140451058161472 spec.py:321] Evaluating on the training split.
I0206 09:15:25.662571 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 09:15:28.692013 140451058161472 spec.py:349] Evaluating on the test split.
I0206 09:15:31.663490 140451058161472 submission_runner.py:408] Time since start: 12131.84s, 	Step: 26314, 	{'train/accuracy': 0.9912744164466858, 'train/loss': 0.028144653886556625, 'train/mean_average_precision': 0.44791104391687353, 'validation/accuracy': 0.9868324398994446, 'validation/loss': 0.043981555849313736, 'validation/mean_average_precision': 0.28106984413767117, 'validation/num_examples': 43793, 'test/accuracy': 0.9859468340873718, 'test/loss': 0.046615514904260635, 'test/mean_average_precision': 0.26985342712303234, 'test/num_examples': 43793, 'score': 8416.634189844131, 'total_duration': 12131.838816642761, 'accumulated_submission_time': 8416.634189844131, 'accumulated_eval_time': 3713.1441898345947, 'accumulated_logging_time': 1.3176674842834473}
I0206 09:15:31.683831 140290179761920 logging_writer.py:48] [26314] accumulated_eval_time=3713.144190, accumulated_logging_time=1.317667, accumulated_submission_time=8416.634190, global_step=26314, preemption_count=0, score=8416.634190, test/accuracy=0.985947, test/loss=0.046616, test/mean_average_precision=0.269853, test/num_examples=43793, total_duration=12131.838817, train/accuracy=0.991274, train/loss=0.028145, train/mean_average_precision=0.447911, validation/accuracy=0.986832, validation/loss=0.043982, validation/mean_average_precision=0.281070, validation/num_examples=43793
I0206 09:15:59.220361 140290188154624 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.05181792378425598, loss=0.026385553181171417
I0206 09:16:31.077933 140290179761920 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.05268687754869461, loss=0.02986527793109417
I0206 09:17:03.092568 140290188154624 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.06206097826361656, loss=0.03505204617977142
I0206 09:17:35.648931 140290179761920 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.04349876195192337, loss=0.029596010223031044
I0206 09:18:07.763844 140290188154624 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.04880952462553978, loss=0.025311311706900597
I0206 09:18:39.896837 140290179761920 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.06440774351358414, loss=0.03237868472933769
I0206 09:19:11.891998 140290188154624 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.08026185631752014, loss=0.029297439381480217
I0206 09:19:31.876007 140451058161472 spec.py:321] Evaluating on the training split.
I0206 09:21:08.763705 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 09:21:11.908632 140451058161472 spec.py:349] Evaluating on the test split.
I0206 09:21:14.976653 140451058161472 submission_runner.py:408] Time since start: 12475.15s, 	Step: 27065, 	{'train/accuracy': 0.9913526177406311, 'train/loss': 0.0279363002628088, 'train/mean_average_precision': 0.46327236406281286, 'validation/accuracy': 0.986867368221283, 'validation/loss': 0.04432373866438866, 'validation/mean_average_precision': 0.2782958896685469, 'validation/num_examples': 43793, 'test/accuracy': 0.9860487580299377, 'test/loss': 0.04718391224741936, 'test/mean_average_precision': 0.2670339478514961, 'test/num_examples': 43793, 'score': 8656.794304132462, 'total_duration': 12475.151976585388, 'accumulated_submission_time': 8656.794304132462, 'accumulated_eval_time': 3816.2447905540466, 'accumulated_logging_time': 1.3499679565429688}
I0206 09:21:14.997453 140229871392512 logging_writer.py:48] [27065] accumulated_eval_time=3816.244791, accumulated_logging_time=1.349968, accumulated_submission_time=8656.794304, global_step=27065, preemption_count=0, score=8656.794304, test/accuracy=0.986049, test/loss=0.047184, test/mean_average_precision=0.267034, test/num_examples=43793, total_duration=12475.151977, train/accuracy=0.991353, train/loss=0.027936, train/mean_average_precision=0.463272, validation/accuracy=0.986867, validation/loss=0.044324, validation/mean_average_precision=0.278296, validation/num_examples=43793
I0206 09:21:26.489231 140248415348480 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.056534312665462494, loss=0.026930930092930794
I0206 09:21:58.423784 140229871392512 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.05573877692222595, loss=0.03429555520415306
I0206 09:22:30.201495 140248415348480 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.05588030070066452, loss=0.030335763469338417
I0206 09:23:01.945914 140229871392512 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.05568224936723709, loss=0.027317265048623085
I0206 09:23:33.866860 140248415348480 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.05920751392841339, loss=0.02624136209487915
I0206 09:24:05.827517 140229871392512 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.05420742928981781, loss=0.029078729450702667
I0206 09:24:37.422827 140248415348480 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.04518728330731392, loss=0.02905212715268135
I0206 09:25:09.146029 140229871392512 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.052033089101314545, loss=0.02814730629324913
I0206 09:25:15.109940 140451058161472 spec.py:321] Evaluating on the training split.
I0206 09:26:50.315532 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 09:26:53.418625 140451058161472 spec.py:349] Evaluating on the test split.
I0206 09:26:56.408138 140451058161472 submission_runner.py:408] Time since start: 12816.58s, 	Step: 27820, 	{'train/accuracy': 0.9914987683296204, 'train/loss': 0.027448926120996475, 'train/mean_average_precision': 0.4683983500487964, 'validation/accuracy': 0.9869043231010437, 'validation/loss': 0.04423655569553375, 'validation/mean_average_precision': 0.2760711966392069, 'validation/num_examples': 43793, 'test/accuracy': 0.9861077070236206, 'test/loss': 0.04697178676724434, 'test/mean_average_precision': 0.26493177180027205, 'test/num_examples': 43793, 'score': 8896.875655651093, 'total_duration': 12816.583455085754, 'accumulated_submission_time': 8896.875655651093, 'accumulated_eval_time': 3917.5429599285126, 'accumulated_logging_time': 1.381486415863037}
I0206 09:26:56.436557 140229879785216 logging_writer.py:48] [27820] accumulated_eval_time=3917.542960, accumulated_logging_time=1.381486, accumulated_submission_time=8896.875656, global_step=27820, preemption_count=0, score=8896.875656, test/accuracy=0.986108, test/loss=0.046972, test/mean_average_precision=0.264932, test/num_examples=43793, total_duration=12816.583455, train/accuracy=0.991499, train/loss=0.027449, train/mean_average_precision=0.468398, validation/accuracy=0.986904, validation/loss=0.044237, validation/mean_average_precision=0.276071, validation/num_examples=43793
I0206 09:27:22.070184 140290179761920 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.05499439686536789, loss=0.028294676914811134
I0206 09:27:53.616411 140229879785216 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.07518764585256577, loss=0.029909741133451462
I0206 09:28:25.292707 140290179761920 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.05530920252203941, loss=0.030441096052527428
I0206 09:28:56.666081 140229879785216 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.047881610691547394, loss=0.026104046031832695
I0206 09:29:28.124145 140290179761920 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.0552181676030159, loss=0.02801869995892048
I0206 09:29:59.648912 140229879785216 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.052226681262254715, loss=0.028442343696951866
I0206 09:30:32.101345 140290179761920 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.05439290404319763, loss=0.028893759474158287
I0206 09:30:56.643449 140451058161472 spec.py:321] Evaluating on the training split.
I0206 09:32:36.935463 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 09:32:40.026330 140451058161472 spec.py:349] Evaluating on the test split.
I0206 09:32:42.992907 140451058161472 submission_runner.py:408] Time since start: 13163.17s, 	Step: 28577, 	{'train/accuracy': 0.9915719032287598, 'train/loss': 0.027175720781087875, 'train/mean_average_precision': 0.4733769339022719, 'validation/accuracy': 0.9869562983512878, 'validation/loss': 0.04423242807388306, 'validation/mean_average_precision': 0.2856657416394459, 'validation/num_examples': 43793, 'test/accuracy': 0.9861435294151306, 'test/loss': 0.047050319612026215, 'test/mean_average_precision': 0.27184044362970006, 'test/num_examples': 43793, 'score': 9137.05087184906, 'total_duration': 13163.168231010437, 'accumulated_submission_time': 9137.05087184906, 'accumulated_eval_time': 4023.892383813858, 'accumulated_logging_time': 1.4207780361175537}
I0206 09:32:43.013818 140248415348480 logging_writer.py:48] [28577] accumulated_eval_time=4023.892384, accumulated_logging_time=1.420778, accumulated_submission_time=9137.050872, global_step=28577, preemption_count=0, score=9137.050872, test/accuracy=0.986144, test/loss=0.047050, test/mean_average_precision=0.271840, test/num_examples=43793, total_duration=13163.168231, train/accuracy=0.991572, train/loss=0.027176, train/mean_average_precision=0.473377, validation/accuracy=0.986956, validation/loss=0.044232, validation/mean_average_precision=0.285666, validation/num_examples=43793
I0206 09:32:50.488175 140290188154624 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.049709297716617584, loss=0.02756679430603981
I0206 09:33:22.284086 140248415348480 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.05769892781972885, loss=0.029030336067080498
I0206 09:33:54.012235 140290188154624 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.05105564370751381, loss=0.027155864983797073
I0206 09:34:25.675338 140248415348480 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.057550329715013504, loss=0.02654741145670414
I0206 09:34:56.883401 140290188154624 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.052957214415073395, loss=0.03018309362232685
I0206 09:35:28.783794 140248415348480 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.0608767606317997, loss=0.02721627987921238
I0206 09:36:00.295261 140290188154624 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.052670836448669434, loss=0.027765696868300438
I0206 09:36:32.007233 140248415348480 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.04653078690171242, loss=0.026850508525967598
I0206 09:36:43.259673 140451058161472 spec.py:321] Evaluating on the training split.
I0206 09:38:20.364057 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 09:38:23.407051 140451058161472 spec.py:349] Evaluating on the test split.
I0206 09:38:26.364583 140451058161472 submission_runner.py:408] Time since start: 13506.54s, 	Step: 29337, 	{'train/accuracy': 0.991813600063324, 'train/loss': 0.026514487341046333, 'train/mean_average_precision': 0.4933331247934246, 'validation/accuracy': 0.987015962600708, 'validation/loss': 0.0440828762948513, 'validation/mean_average_precision': 0.2803936676206833, 'validation/num_examples': 43793, 'test/accuracy': 0.9861460328102112, 'test/loss': 0.04681553319096565, 'test/mean_average_precision': 0.2694635318031406, 'test/num_examples': 43793, 'score': 9377.265213012695, 'total_duration': 13506.539906024933, 'accumulated_submission_time': 9377.265213012695, 'accumulated_eval_time': 4126.997245788574, 'accumulated_logging_time': 1.4529609680175781}
I0206 09:38:26.385520 140229879785216 logging_writer.py:48] [29337] accumulated_eval_time=4126.997246, accumulated_logging_time=1.452961, accumulated_submission_time=9377.265213, global_step=29337, preemption_count=0, score=9377.265213, test/accuracy=0.986146, test/loss=0.046816, test/mean_average_precision=0.269464, test/num_examples=43793, total_duration=13506.539906, train/accuracy=0.991814, train/loss=0.026514, train/mean_average_precision=0.493333, validation/accuracy=0.987016, validation/loss=0.044083, validation/mean_average_precision=0.280394, validation/num_examples=43793
I0206 09:38:46.377556 140290179761920 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.052980273962020874, loss=0.028960149735212326
I0206 09:39:18.360787 140229879785216 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.05484674498438835, loss=0.02760876901447773
I0206 09:39:49.658658 140290179761920 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.0727052092552185, loss=0.028705570846796036
I0206 09:40:21.363597 140229879785216 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.07429186254739761, loss=0.027773575857281685
I0206 09:40:53.166521 140290179761920 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.06986173987388611, loss=0.03301197290420532
I0206 09:41:24.735285 140229879785216 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.04890989884734154, loss=0.029424138367176056
I0206 09:41:56.320582 140290179761920 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.0757681280374527, loss=0.031251460313797
I0206 09:42:26.582137 140451058161472 spec.py:321] Evaluating on the training split.
I0206 09:44:03.680718 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 09:44:09.334798 140451058161472 spec.py:349] Evaluating on the test split.
I0206 09:44:12.355188 140451058161472 submission_runner.py:408] Time since start: 13852.53s, 	Step: 30096, 	{'train/accuracy': 0.9918859601020813, 'train/loss': 0.026113389059901237, 'train/mean_average_precision': 0.5055414899153353, 'validation/accuracy': 0.9869863390922546, 'validation/loss': 0.04378185793757439, 'validation/mean_average_precision': 0.2895730354147849, 'validation/num_examples': 43793, 'test/accuracy': 0.9861797094345093, 'test/loss': 0.04680396616458893, 'test/mean_average_precision': 0.2741908935948261, 'test/num_examples': 43793, 'score': 9617.430700778961, 'total_duration': 13852.530488491058, 'accumulated_submission_time': 9617.430700778961, 'accumulated_eval_time': 4232.770231723785, 'accumulated_logging_time': 1.485065221786499}
I0206 09:44:12.377971 140229871392512 logging_writer.py:48] [30096] accumulated_eval_time=4232.770232, accumulated_logging_time=1.485065, accumulated_submission_time=9617.430701, global_step=30096, preemption_count=0, score=9617.430701, test/accuracy=0.986180, test/loss=0.046804, test/mean_average_precision=0.274191, test/num_examples=43793, total_duration=13852.530488, train/accuracy=0.991886, train/loss=0.026113, train/mean_average_precision=0.505541, validation/accuracy=0.986986, validation/loss=0.043782, validation/mean_average_precision=0.289573, validation/num_examples=43793
I0206 09:44:13.927277 140290188154624 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.05560451000928879, loss=0.029737336561083794
I0206 09:44:45.778198 140229871392512 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.0566195584833622, loss=0.027298200875520706
I0206 09:45:17.408742 140290188154624 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.05187179520726204, loss=0.02724822610616684
I0206 09:45:49.084670 140229871392512 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.060952384024858475, loss=0.026409057900309563
I0206 09:46:21.033074 140290188154624 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.06654740124940872, loss=0.02978331968188286
I0206 09:46:52.350365 140229871392512 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.06781167536973953, loss=0.029340654611587524
I0206 09:47:24.322516 140290188154624 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.054536882787942886, loss=0.027233010157942772
I0206 09:47:55.833461 140229871392512 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.07877837866544724, loss=0.029410021379590034
I0206 09:48:12.410853 140451058161472 spec.py:321] Evaluating on the training split.
I0206 09:49:47.888567 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 09:49:51.051365 140451058161472 spec.py:349] Evaluating on the test split.
I0206 09:49:54.137360 140451058161472 submission_runner.py:408] Time since start: 14194.31s, 	Step: 30853, 	{'train/accuracy': 0.9918944835662842, 'train/loss': 0.02597794681787491, 'train/mean_average_precision': 0.5109262523099191, 'validation/accuracy': 0.986935555934906, 'validation/loss': 0.04440790042281151, 'validation/mean_average_precision': 0.2870528055093321, 'validation/num_examples': 43793, 'test/accuracy': 0.9860929846763611, 'test/loss': 0.04719549044966698, 'test/mean_average_precision': 0.27437808902752175, 'test/num_examples': 43793, 'score': 9857.432433843613, 'total_duration': 14194.312682628632, 'accumulated_submission_time': 9857.432433843613, 'accumulated_eval_time': 4334.496691703796, 'accumulated_logging_time': 1.5186164379119873}
I0206 09:49:54.158704 140229879785216 logging_writer.py:48] [30853] accumulated_eval_time=4334.496692, accumulated_logging_time=1.518616, accumulated_submission_time=9857.432434, global_step=30853, preemption_count=0, score=9857.432434, test/accuracy=0.986093, test/loss=0.047195, test/mean_average_precision=0.274378, test/num_examples=43793, total_duration=14194.312683, train/accuracy=0.991894, train/loss=0.025978, train/mean_average_precision=0.510926, validation/accuracy=0.986936, validation/loss=0.044408, validation/mean_average_precision=0.287053, validation/num_examples=43793
I0206 09:50:09.412690 140290179761920 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.05534432455897331, loss=0.02950296550989151
I0206 09:50:40.824002 140229879785216 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.0759330466389656, loss=0.0291152186691761
I0206 09:51:12.264357 140290179761920 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.0626123771071434, loss=0.027281196787953377
I0206 09:51:43.952534 140229879785216 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.07125376909971237, loss=0.030079703778028488
I0206 09:52:15.888645 140290179761920 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.070370614528656, loss=0.028632208704948425
I0206 09:52:48.003679 140229879785216 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.06096845120191574, loss=0.028916010633111
I0206 09:53:20.195516 140290179761920 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.06497661769390106, loss=0.025383656844496727
I0206 09:53:52.377804 140229879785216 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.05755780264735222, loss=0.0313919261097908
I0206 09:53:54.318388 140451058161472 spec.py:321] Evaluating on the training split.
I0206 09:55:34.199413 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 09:55:37.321137 140451058161472 spec.py:349] Evaluating on the test split.
I0206 09:55:40.388848 140451058161472 submission_runner.py:408] Time since start: 14540.56s, 	Step: 31607, 	{'train/accuracy': 0.9922711849212646, 'train/loss': 0.024978749454021454, 'train/mean_average_precision': 0.526230375162185, 'validation/accuracy': 0.9870455861091614, 'validation/loss': 0.04400182515382767, 'validation/mean_average_precision': 0.2825439937763267, 'validation/num_examples': 43793, 'test/accuracy': 0.9863023161888123, 'test/loss': 0.046554673463106155, 'test/mean_average_precision': 0.28262081972227066, 'test/num_examples': 43793, 'score': 10097.557705879211, 'total_duration': 14540.564175128937, 'accumulated_submission_time': 10097.557705879211, 'accumulated_eval_time': 4440.5671174526215, 'accumulated_logging_time': 1.551112174987793}
I0206 09:55:40.411096 140248415348480 logging_writer.py:48] [31607] accumulated_eval_time=4440.567117, accumulated_logging_time=1.551112, accumulated_submission_time=10097.557706, global_step=31607, preemption_count=0, score=10097.557706, test/accuracy=0.986302, test/loss=0.046555, test/mean_average_precision=0.282621, test/num_examples=43793, total_duration=14540.564175, train/accuracy=0.992271, train/loss=0.024979, train/mean_average_precision=0.526230, validation/accuracy=0.987046, validation/loss=0.044002, validation/mean_average_precision=0.282544, validation/num_examples=43793
I0206 09:56:10.500267 140290188154624 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.06134048104286194, loss=0.024918293580412865
I0206 09:56:42.069091 140248415348480 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.0689358189702034, loss=0.0315539687871933
I0206 09:57:13.753594 140290188154624 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.05615292862057686, loss=0.02987232245504856
I0206 09:57:45.468834 140248415348480 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.05924418568611145, loss=0.026772936806082726
I0206 09:58:17.147682 140290188154624 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.08238887041807175, loss=0.02800002507865429
I0206 09:58:48.477948 140248415348480 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.05525454506278038, loss=0.029600394889712334
I0206 09:59:20.260653 140290188154624 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.061287347227334976, loss=0.03091881237924099
I0206 09:59:40.597259 140451058161472 spec.py:321] Evaluating on the training split.
I0206 10:01:15.034401 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 10:01:18.127342 140451058161472 spec.py:349] Evaluating on the test split.
I0206 10:01:21.163048 140451058161472 submission_runner.py:408] Time since start: 14881.34s, 	Step: 32365, 	{'train/accuracy': 0.9920186400413513, 'train/loss': 0.025651760399341583, 'train/mean_average_precision': 0.5237306076253523, 'validation/accuracy': 0.9870054125785828, 'validation/loss': 0.044830068945884705, 'validation/mean_average_precision': 0.28193439674336773, 'validation/num_examples': 43793, 'test/accuracy': 0.986120343208313, 'test/loss': 0.047644782811403275, 'test/mean_average_precision': 0.2724481736220283, 'test/num_examples': 43793, 'score': 10337.713238954544, 'total_duration': 14881.338361740112, 'accumulated_submission_time': 10337.713238954544, 'accumulated_eval_time': 4541.132849693298, 'accumulated_logging_time': 1.584326982498169}
I0206 10:01:21.185353 140229871392512 logging_writer.py:48] [32365] accumulated_eval_time=4541.132850, accumulated_logging_time=1.584327, accumulated_submission_time=10337.713239, global_step=32365, preemption_count=0, score=10337.713239, test/accuracy=0.986120, test/loss=0.047645, test/mean_average_precision=0.272448, test/num_examples=43793, total_duration=14881.338362, train/accuracy=0.992019, train/loss=0.025652, train/mean_average_precision=0.523731, validation/accuracy=0.987005, validation/loss=0.044830, validation/mean_average_precision=0.281934, validation/num_examples=43793
I0206 10:01:32.700055 140290179761920 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.0760737732052803, loss=0.03371138498187065
I0206 10:02:04.379473 140229871392512 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.06883486360311508, loss=0.027533430606126785
I0206 10:02:35.840537 140290179761920 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.06597185879945755, loss=0.03059227392077446
I0206 10:03:07.547744 140229871392512 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.06649500876665115, loss=0.02919207140803337
I0206 10:03:39.055332 140290179761920 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.05696764215826988, loss=0.02912207879126072
I0206 10:04:10.601418 140229871392512 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.0653623640537262, loss=0.0307322908192873
I0206 10:04:42.484554 140290179761920 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.06927945464849472, loss=0.02669067680835724
I0206 10:05:14.854854 140229871392512 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.06396433711051941, loss=0.028427449986338615
I0206 10:05:21.220043 140451058161472 spec.py:321] Evaluating on the training split.
I0206 10:06:56.514213 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 10:06:59.671343 140451058161472 spec.py:349] Evaluating on the test split.
I0206 10:07:02.926031 140451058161472 submission_runner.py:408] Time since start: 15223.10s, 	Step: 33121, 	{'train/accuracy': 0.9920905828475952, 'train/loss': 0.025608157739043236, 'train/mean_average_precision': 0.5157125620277023, 'validation/accuracy': 0.9869648218154907, 'validation/loss': 0.044329915195703506, 'validation/mean_average_precision': 0.2834494862434448, 'validation/num_examples': 43793, 'test/accuracy': 0.9861447811126709, 'test/loss': 0.04674365743994713, 'test/mean_average_precision': 0.27867460650752457, 'test/num_examples': 43793, 'score': 10577.716572523117, 'total_duration': 15223.10135102272, 'accumulated_submission_time': 10577.716572523117, 'accumulated_eval_time': 4642.83878827095, 'accumulated_logging_time': 1.6173911094665527}
I0206 10:07:02.947689 140229879785216 logging_writer.py:48] [33121] accumulated_eval_time=4642.838788, accumulated_logging_time=1.617391, accumulated_submission_time=10577.716573, global_step=33121, preemption_count=0, score=10577.716573, test/accuracy=0.986145, test/loss=0.046744, test/mean_average_precision=0.278675, test/num_examples=43793, total_duration=15223.101351, train/accuracy=0.992091, train/loss=0.025608, train/mean_average_precision=0.515713, validation/accuracy=0.986965, validation/loss=0.044330, validation/mean_average_precision=0.283449, validation/num_examples=43793
I0206 10:07:28.403981 140248415348480 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.07825177907943726, loss=0.03078605979681015
I0206 10:07:59.948371 140229879785216 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.07383451610803604, loss=0.03048086352646351
I0206 10:08:31.917486 140248415348480 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.059459932148456573, loss=0.02718273364007473
I0206 10:09:04.256258 140229879785216 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.05401506647467613, loss=0.024431347846984863
I0206 10:09:35.969579 140248415348480 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.06135118752717972, loss=0.028870517387986183
I0206 10:10:07.889056 140229879785216 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.06921051442623138, loss=0.027652859687805176
I0206 10:10:39.485127 140248415348480 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.06796171516180038, loss=0.028421802446246147
I0206 10:11:03.172894 140451058161472 spec.py:321] Evaluating on the training split.
I0206 10:12:46.292210 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 10:12:49.320176 140451058161472 spec.py:349] Evaluating on the test split.
I0206 10:12:52.333609 140451058161472 submission_runner.py:408] Time since start: 15572.51s, 	Step: 33874, 	{'train/accuracy': 0.9917319416999817, 'train/loss': 0.026505768299102783, 'train/mean_average_precision': 0.4951007638370917, 'validation/accuracy': 0.9868974089622498, 'validation/loss': 0.04454047977924347, 'validation/mean_average_precision': 0.28459337684068103, 'validation/num_examples': 43793, 'test/accuracy': 0.9861708879470825, 'test/loss': 0.047231633216142654, 'test/mean_average_precision': 0.2744900853244321, 'test/num_examples': 43793, 'score': 10817.91072845459, 'total_duration': 15572.508923053741, 'accumulated_submission_time': 10817.91072845459, 'accumulated_eval_time': 4751.999447107315, 'accumulated_logging_time': 1.6501054763793945}
I0206 10:12:52.355949 140290179761920 logging_writer.py:48] [33874] accumulated_eval_time=4751.999447, accumulated_logging_time=1.650105, accumulated_submission_time=10817.910728, global_step=33874, preemption_count=0, score=10817.910728, test/accuracy=0.986171, test/loss=0.047232, test/mean_average_precision=0.274490, test/num_examples=43793, total_duration=15572.508923, train/accuracy=0.991732, train/loss=0.026506, train/mean_average_precision=0.495101, validation/accuracy=0.986897, validation/loss=0.044540, validation/mean_average_precision=0.284593, validation/num_examples=43793
I0206 10:13:01.017926 140290188154624 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.061072297394275665, loss=0.027418227866292
I0206 10:13:32.620580 140290179761920 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.0710335448384285, loss=0.032521218061447144
I0206 10:14:04.286503 140290188154624 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.06679286062717438, loss=0.025767281651496887
I0206 10:14:36.136159 140290179761920 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.06835618615150452, loss=0.029876625165343285
I0206 10:15:08.004342 140290188154624 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.05925215780735016, loss=0.02624739333987236
I0206 10:15:39.600173 140290179761920 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.0527971014380455, loss=0.023865723982453346
I0206 10:16:11.269855 140290188154624 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.07489056885242462, loss=0.02912043035030365
I0206 10:16:42.847014 140290179761920 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.08152088522911072, loss=0.027232004329562187
I0206 10:16:52.627473 140451058161472 spec.py:321] Evaluating on the training split.
I0206 10:18:23.980569 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 10:18:27.061754 140451058161472 spec.py:349] Evaluating on the test split.
I0206 10:18:30.077486 140451058161472 submission_runner.py:408] Time since start: 15910.25s, 	Step: 34632, 	{'train/accuracy': 0.9918310642242432, 'train/loss': 0.02616903744637966, 'train/mean_average_precision': 0.4915393588454686, 'validation/accuracy': 0.987043559551239, 'validation/loss': 0.04459802806377411, 'validation/mean_average_precision': 0.28650596275138934, 'validation/num_examples': 43793, 'test/accuracy': 0.9862428903579712, 'test/loss': 0.04726970195770264, 'test/mean_average_precision': 0.27895178237589713, 'test/num_examples': 43793, 'score': 11058.15099453926, 'total_duration': 15910.252793550491, 'accumulated_submission_time': 11058.15099453926, 'accumulated_eval_time': 4849.44939661026, 'accumulated_logging_time': 1.683269739151001}
I0206 10:18:30.100480 140229871392512 logging_writer.py:48] [34632] accumulated_eval_time=4849.449397, accumulated_logging_time=1.683270, accumulated_submission_time=11058.150995, global_step=34632, preemption_count=0, score=11058.150995, test/accuracy=0.986243, test/loss=0.047270, test/mean_average_precision=0.278952, test/num_examples=43793, total_duration=15910.252794, train/accuracy=0.991831, train/loss=0.026169, train/mean_average_precision=0.491539, validation/accuracy=0.987044, validation/loss=0.044598, validation/mean_average_precision=0.286506, validation/num_examples=43793
I0206 10:18:52.026772 140229879785216 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.06590461730957031, loss=0.02924441173672676
I0206 10:19:23.841673 140229871392512 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.06331463903188705, loss=0.02716977708041668
I0206 10:19:55.529625 140229879785216 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.06407639384269714, loss=0.02634187787771225
I0206 10:20:27.672897 140229871392512 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.0631452202796936, loss=0.027199679985642433
I0206 10:20:59.670121 140229879785216 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.07042483985424042, loss=0.027421224862337112
I0206 10:21:31.705946 140229871392512 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.06087935343384743, loss=0.02529706433415413
I0206 10:22:05.927394 140229879785216 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.06663118302822113, loss=0.028511689975857735
I0206 10:22:30.319776 140451058161472 spec.py:321] Evaluating on the training split.
I0206 10:24:02.674996 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 10:24:06.144809 140451058161472 spec.py:349] Evaluating on the test split.
I0206 10:24:09.583096 140451058161472 submission_runner.py:408] Time since start: 16249.76s, 	Step: 35378, 	{'train/accuracy': 0.9919923543930054, 'train/loss': 0.02578771486878395, 'train/mean_average_precision': 0.5099326315816797, 'validation/accuracy': 0.9868158102035522, 'validation/loss': 0.04446062073111534, 'validation/mean_average_precision': 0.2835898925780593, 'validation/num_examples': 43793, 'test/accuracy': 0.9859455227851868, 'test/loss': 0.047208115458488464, 'test/mean_average_precision': 0.2693484981572296, 'test/num_examples': 43793, 'score': 11298.339334487915, 'total_duration': 16249.758395671844, 'accumulated_submission_time': 11298.339334487915, 'accumulated_eval_time': 4948.712647199631, 'accumulated_logging_time': 1.7171683311462402}
I0206 10:24:09.609603 140248415348480 logging_writer.py:48] [35378] accumulated_eval_time=4948.712647, accumulated_logging_time=1.717168, accumulated_submission_time=11298.339334, global_step=35378, preemption_count=0, score=11298.339334, test/accuracy=0.985946, test/loss=0.047208, test/mean_average_precision=0.269348, test/num_examples=43793, total_duration=16249.758396, train/accuracy=0.991992, train/loss=0.025788, train/mean_average_precision=0.509933, validation/accuracy=0.986816, validation/loss=0.044461, validation/mean_average_precision=0.283590, validation/num_examples=43793
I0206 10:24:17.131388 140290188154624 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.06418818980455399, loss=0.02840302884578705
I0206 10:24:49.746378 140248415348480 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.06904648244380951, loss=0.031672585755586624
I0206 10:25:22.411707 140290188154624 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.08190079778432846, loss=0.026826893910765648
I0206 10:25:54.466551 140248415348480 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.06615641713142395, loss=0.028780337423086166
I0206 10:26:27.201205 140290188154624 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.06342214345932007, loss=0.025229396298527718
I0206 10:26:59.047562 140248415348480 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.08349823951721191, loss=0.028897663578391075
I0206 10:27:30.468270 140290188154624 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.06889888644218445, loss=0.026024121791124344
I0206 10:28:02.016864 140248415348480 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.07908233255147934, loss=0.02787690795958042
I0206 10:28:09.862392 140451058161472 spec.py:321] Evaluating on the training split.
I0206 10:29:44.512861 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 10:29:47.607149 140451058161472 spec.py:349] Evaluating on the test split.
I0206 10:29:50.622969 140451058161472 submission_runner.py:408] Time since start: 16590.80s, 	Step: 36126, 	{'train/accuracy': 0.9920006990432739, 'train/loss': 0.025485051795840263, 'train/mean_average_precision': 0.5258720739264008, 'validation/accuracy': 0.987106442451477, 'validation/loss': 0.044622622430324554, 'validation/mean_average_precision': 0.2882472900207319, 'validation/num_examples': 43793, 'test/accuracy': 0.986178457736969, 'test/loss': 0.047570228576660156, 'test/mean_average_precision': 0.2711337537526851, 'test/num_examples': 43793, 'score': 11538.558787345886, 'total_duration': 16590.798290252686, 'accumulated_submission_time': 11538.558787345886, 'accumulated_eval_time': 5049.473174333572, 'accumulated_logging_time': 1.7545788288116455}
I0206 10:29:50.645007 140229871392512 logging_writer.py:48] [36126] accumulated_eval_time=5049.473174, accumulated_logging_time=1.754579, accumulated_submission_time=11538.558787, global_step=36126, preemption_count=0, score=11538.558787, test/accuracy=0.986178, test/loss=0.047570, test/mean_average_precision=0.271134, test/num_examples=43793, total_duration=16590.798290, train/accuracy=0.992001, train/loss=0.025485, train/mean_average_precision=0.525872, validation/accuracy=0.987106, validation/loss=0.044623, validation/mean_average_precision=0.288247, validation/num_examples=43793
I0206 10:30:14.171765 140229879785216 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.07119403779506683, loss=0.027783306315541267
I0206 10:30:45.611356 140229871392512 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.08811391890048981, loss=0.029225366190075874
I0206 10:31:17.054509 140229879785216 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.059283915907144547, loss=0.026395099237561226
I0206 10:31:48.860265 140229871392512 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.08479071408510208, loss=0.0314139723777771
I0206 10:32:20.219733 140229879785216 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.06177201122045517, loss=0.02616998367011547
I0206 10:32:51.608784 140229871392512 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.07538556307554245, loss=0.02597915381193161
I0206 10:33:23.292032 140229879785216 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.0749182403087616, loss=0.027215054258704185
I0206 10:33:50.693008 140451058161472 spec.py:321] Evaluating on the training split.
I0206 10:35:27.824365 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 10:35:30.820696 140451058161472 spec.py:349] Evaluating on the test split.
I0206 10:35:33.806798 140451058161472 submission_runner.py:408] Time since start: 16933.98s, 	Step: 36889, 	{'train/accuracy': 0.9921327829360962, 'train/loss': 0.025163933634757996, 'train/mean_average_precision': 0.5333610794699605, 'validation/accuracy': 0.9869310855865479, 'validation/loss': 0.04440346360206604, 'validation/mean_average_precision': 0.2876226417928679, 'validation/num_examples': 43793, 'test/accuracy': 0.986159086227417, 'test/loss': 0.047014519572257996, 'test/mean_average_precision': 0.2754729484835849, 'test/num_examples': 43793, 'score': 11778.575371980667, 'total_duration': 16933.982120752335, 'accumulated_submission_time': 11778.575371980667, 'accumulated_eval_time': 5152.5869171619415, 'accumulated_logging_time': 1.7874071598052979}
I0206 10:35:33.829357 140248415348480 logging_writer.py:48] [36889] accumulated_eval_time=5152.586917, accumulated_logging_time=1.787407, accumulated_submission_time=11778.575372, global_step=36889, preemption_count=0, score=11778.575372, test/accuracy=0.986159, test/loss=0.047015, test/mean_average_precision=0.275473, test/num_examples=43793, total_duration=16933.982121, train/accuracy=0.992133, train/loss=0.025164, train/mean_average_precision=0.533361, validation/accuracy=0.986931, validation/loss=0.044403, validation/mean_average_precision=0.287623, validation/num_examples=43793
I0206 10:35:38.068954 140290188154624 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.06561386585235596, loss=0.027880946174263954
I0206 10:36:10.015890 140248415348480 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.07727767527103424, loss=0.028617309406399727
I0206 10:36:41.205637 140290188154624 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.0636616051197052, loss=0.023619884625077248
I0206 10:37:13.129398 140248415348480 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.07915020734071732, loss=0.028303159400820732
I0206 10:37:45.136810 140290188154624 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.09316481649875641, loss=0.031931132078170776
I0206 10:38:16.664479 140248415348480 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.06798577308654785, loss=0.027806883677840233
I0206 10:38:47.861855 140290188154624 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.06882187724113464, loss=0.026181969791650772
I0206 10:39:19.051064 140248415348480 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.08798067271709442, loss=0.028791215270757675
I0206 10:39:33.818048 140451058161472 spec.py:321] Evaluating on the training split.
I0206 10:41:11.953072 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 10:41:15.012742 140451058161472 spec.py:349] Evaluating on the test split.
I0206 10:41:18.001088 140451058161472 submission_runner.py:408] Time since start: 17278.18s, 	Step: 37648, 	{'train/accuracy': 0.9923455119132996, 'train/loss': 0.024487245827913284, 'train/mean_average_precision': 0.5460171638954645, 'validation/accuracy': 0.9870200157165527, 'validation/loss': 0.04464934021234512, 'validation/mean_average_precision': 0.2832415157203511, 'validation/num_examples': 43793, 'test/accuracy': 0.9862471222877502, 'test/loss': 0.047332052141427994, 'test/mean_average_precision': 0.27767251335974974, 'test/num_examples': 43793, 'score': 12018.111292123795, 'total_duration': 17278.17639899254, 'accumulated_submission_time': 12018.111292123795, 'accumulated_eval_time': 5256.769897699356, 'accumulated_logging_time': 2.2428689002990723}
I0206 10:41:18.024152 140229871392512 logging_writer.py:48] [37648] accumulated_eval_time=5256.769898, accumulated_logging_time=2.242869, accumulated_submission_time=12018.111292, global_step=37648, preemption_count=0, score=12018.111292, test/accuracy=0.986247, test/loss=0.047332, test/mean_average_precision=0.277673, test/num_examples=43793, total_duration=17278.176399, train/accuracy=0.992346, train/loss=0.024487, train/mean_average_precision=0.546017, validation/accuracy=0.987020, validation/loss=0.044649, validation/mean_average_precision=0.283242, validation/num_examples=43793
I0206 10:41:34.959449 140229879785216 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.06694173812866211, loss=0.025878529995679855
I0206 10:42:06.498875 140229871392512 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.07288295030593872, loss=0.02781166136264801
I0206 10:42:38.197731 140229879785216 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.06445939093828201, loss=0.026056934148073196
I0206 10:43:09.949409 140229871392512 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.08329812437295914, loss=0.02916487492620945
I0206 10:43:42.182379 140229879785216 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.07104017585515976, loss=0.028186576440930367
I0206 10:44:14.597410 140229871392512 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.07620974630117416, loss=0.027379777282476425
I0206 10:44:46.219959 140229879785216 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.07261551171541214, loss=0.026027332991361618
I0206 10:45:18.049476 140451058161472 spec.py:321] Evaluating on the training split.
I0206 10:46:51.487010 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 10:46:54.503605 140451058161472 spec.py:349] Evaluating on the test split.
I0206 10:46:57.481568 140451058161472 submission_runner.py:408] Time since start: 17617.66s, 	Step: 38400, 	{'train/accuracy': 0.9923316836357117, 'train/loss': 0.024133075028657913, 'train/mean_average_precision': 0.5530907366085315, 'validation/accuracy': 0.9871101379394531, 'validation/loss': 0.04488293081521988, 'validation/mean_average_precision': 0.2893611700860986, 'validation/num_examples': 43793, 'test/accuracy': 0.9862862825393677, 'test/loss': 0.04784107580780983, 'test/mean_average_precision': 0.27881595787050245, 'test/num_examples': 43793, 'score': 12258.103625297546, 'total_duration': 17617.65689277649, 'accumulated_submission_time': 12258.103625297546, 'accumulated_eval_time': 5356.201946020126, 'accumulated_logging_time': 2.277024269104004}
I0206 10:46:57.504434 140248415348480 logging_writer.py:48] [38400] accumulated_eval_time=5356.201946, accumulated_logging_time=2.277024, accumulated_submission_time=12258.103625, global_step=38400, preemption_count=0, score=12258.103625, test/accuracy=0.986286, test/loss=0.047841, test/mean_average_precision=0.278816, test/num_examples=43793, total_duration=17617.656893, train/accuracy=0.992332, train/loss=0.024133, train/mean_average_precision=0.553091, validation/accuracy=0.987110, validation/loss=0.044883, validation/mean_average_precision=0.289361, validation/num_examples=43793
I0206 10:46:57.880495 140290188154624 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.08430679887533188, loss=0.02614656835794449
I0206 10:47:29.730350 140248415348480 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.06477446854114532, loss=0.023018255829811096
I0206 10:48:01.317758 140290188154624 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.06323561817407608, loss=0.028764186426997185
I0206 10:48:33.137068 140248415348480 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.06393758207559586, loss=0.02316919155418873
I0206 10:49:05.094565 140290188154624 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.08197140693664551, loss=0.02596784010529518
I0206 10:49:36.938708 140248415348480 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.06686660647392273, loss=0.026180611923336983
I0206 10:50:09.411696 140290188154624 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.062020737677812576, loss=0.024402836337685585
I0206 10:50:41.189964 140248415348480 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.07149184495210648, loss=0.02539367973804474
I0206 10:50:57.548715 140451058161472 spec.py:321] Evaluating on the training split.
I0206 10:52:32.436335 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 10:52:35.543526 140451058161472 spec.py:349] Evaluating on the test split.
I0206 10:52:38.492367 140451058161472 submission_runner.py:408] Time since start: 17958.67s, 	Step: 39153, 	{'train/accuracy': 0.9926602840423584, 'train/loss': 0.023420441895723343, 'train/mean_average_precision': 0.5637032087035159, 'validation/accuracy': 0.9871218800544739, 'validation/loss': 0.04486692324280739, 'validation/mean_average_precision': 0.28992026956643213, 'validation/num_examples': 43793, 'test/accuracy': 0.9862328171730042, 'test/loss': 0.04774757847189903, 'test/mean_average_precision': 0.2744675293708428, 'test/num_examples': 43793, 'score': 12498.116117477417, 'total_duration': 17958.667691469193, 'accumulated_submission_time': 12498.116117477417, 'accumulated_eval_time': 5457.145552873611, 'accumulated_logging_time': 2.3104984760284424}
I0206 10:52:38.524654 140229871392512 logging_writer.py:48] [39153] accumulated_eval_time=5457.145553, accumulated_logging_time=2.310498, accumulated_submission_time=12498.116117, global_step=39153, preemption_count=0, score=12498.116117, test/accuracy=0.986233, test/loss=0.047748, test/mean_average_precision=0.274468, test/num_examples=43793, total_duration=17958.667691, train/accuracy=0.992660, train/loss=0.023420, train/mean_average_precision=0.563703, validation/accuracy=0.987122, validation/loss=0.044867, validation/mean_average_precision=0.289920, validation/num_examples=43793
I0206 10:52:53.681562 140290179761920 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.07505206763744354, loss=0.027611082419753075
I0206 10:53:25.460043 140229871392512 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.07120761275291443, loss=0.029557527974247932
I0206 10:53:57.194547 140290179761920 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.07677443325519562, loss=0.026916753500699997
I0206 10:54:29.139654 140229871392512 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.07270806282758713, loss=0.02619362249970436
I0206 10:55:00.706486 140290179761920 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.06536176800727844, loss=0.02660098858177662
I0206 10:55:32.740294 140229871392512 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.0844905897974968, loss=0.025462163612246513
I0206 10:56:04.709321 140290179761920 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.0676531195640564, loss=0.02720014564692974
I0206 10:56:36.422803 140229871392512 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.0814913809299469, loss=0.026612328365445137
I0206 10:56:38.683044 140451058161472 spec.py:321] Evaluating on the training split.
I0206 10:58:12.420071 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 10:58:15.474547 140451058161472 spec.py:349] Evaluating on the test split.
I0206 10:58:18.465641 140451058161472 submission_runner.py:408] Time since start: 18298.64s, 	Step: 39908, 	{'train/accuracy': 0.9927948713302612, 'train/loss': 0.023058081045746803, 'train/mean_average_precision': 0.5783965721104753, 'validation/accuracy': 0.9870395064353943, 'validation/loss': 0.04443762078881264, 'validation/mean_average_precision': 0.286592025599736, 'validation/num_examples': 43793, 'test/accuracy': 0.9862656593322754, 'test/loss': 0.047343429177999496, 'test/mean_average_precision': 0.2774957648688163, 'test/num_examples': 43793, 'score': 12738.242744922638, 'total_duration': 18298.640961408615, 'accumulated_submission_time': 12738.242744922638, 'accumulated_eval_time': 5556.928096294403, 'accumulated_logging_time': 2.3550827503204346}
I0206 10:58:18.499621 140229879785216 logging_writer.py:48] [39908] accumulated_eval_time=5556.928096, accumulated_logging_time=2.355083, accumulated_submission_time=12738.242745, global_step=39908, preemption_count=0, score=12738.242745, test/accuracy=0.986266, test/loss=0.047343, test/mean_average_precision=0.277496, test/num_examples=43793, total_duration=18298.640961, train/accuracy=0.992795, train/loss=0.023058, train/mean_average_precision=0.578397, validation/accuracy=0.987040, validation/loss=0.044438, validation/mean_average_precision=0.286592, validation/num_examples=43793
I0206 10:58:48.033193 140248415348480 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.0744200050830841, loss=0.027072256430983543
I0206 10:59:19.815599 140229879785216 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.06914816796779633, loss=0.02576437219977379
I0206 10:59:51.729924 140248415348480 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.10172808915376663, loss=0.029186531901359558
I0206 11:00:23.679085 140229879785216 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.0728595033288002, loss=0.025729332119226456
I0206 11:00:55.316968 140248415348480 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.05723165348172188, loss=0.02413945272564888
I0206 11:01:26.854125 140229879785216 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.070304736495018, loss=0.027966918423771858
I0206 11:01:58.303248 140248415348480 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.08156125992536545, loss=0.02939295768737793
I0206 11:02:18.649461 140451058161472 spec.py:321] Evaluating on the training split.
I0206 11:03:54.520076 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 11:03:57.575562 140451058161472 spec.py:349] Evaluating on the test split.
I0206 11:04:00.575568 140451058161472 submission_runner.py:408] Time since start: 18640.75s, 	Step: 40666, 	{'train/accuracy': 0.9926062822341919, 'train/loss': 0.02369121089577675, 'train/mean_average_precision': 0.5603386086431281, 'validation/accuracy': 0.986970067024231, 'validation/loss': 0.04487146437168121, 'validation/mean_average_precision': 0.2884174564599778, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.04765874147415161, 'test/mean_average_precision': 0.2742626805934189, 'test/num_examples': 43793, 'score': 12978.360262870789, 'total_duration': 18640.750893592834, 'accumulated_submission_time': 12978.360262870789, 'accumulated_eval_time': 5658.854161739349, 'accumulated_logging_time': 2.401482343673706}
I0206 11:04:00.598567 140229871392512 logging_writer.py:48] [40666] accumulated_eval_time=5658.854162, accumulated_logging_time=2.401482, accumulated_submission_time=12978.360263, global_step=40666, preemption_count=0, score=12978.360263, test/accuracy=0.986133, test/loss=0.047659, test/mean_average_precision=0.274263, test/num_examples=43793, total_duration=18640.750894, train/accuracy=0.992606, train/loss=0.023691, train/mean_average_precision=0.560339, validation/accuracy=0.986970, validation/loss=0.044871, validation/mean_average_precision=0.288417, validation/num_examples=43793
I0206 11:04:11.839105 140290188154624 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.060339659452438354, loss=0.024815108627080917
I0206 11:04:43.088631 140229871392512 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.06855115294456482, loss=0.026981951668858528
I0206 11:05:14.480384 140290188154624 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.08774033188819885, loss=0.0276961587369442
I0206 11:05:46.208596 140229871392512 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.06762152165174484, loss=0.02542717568576336
I0206 11:06:17.525035 140290188154624 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.07926761358976364, loss=0.02402925305068493
I0206 11:06:48.912289 140229871392512 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.08130864053964615, loss=0.023476149886846542
I0206 11:07:20.655703 140290188154624 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.07917618751525879, loss=0.027785388752818108
I0206 11:07:52.212997 140229871392512 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.06635139882564545, loss=0.027979988604784012
I0206 11:08:00.732458 140451058161472 spec.py:321] Evaluating on the training split.
I0206 11:09:42.917596 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 11:09:46.352962 140451058161472 spec.py:349] Evaluating on the test split.
I0206 11:09:49.688298 140451058161472 submission_runner.py:408] Time since start: 18989.86s, 	Step: 41428, 	{'train/accuracy': 0.9925146102905273, 'train/loss': 0.023902615532279015, 'train/mean_average_precision': 0.5628586654561019, 'validation/accuracy': 0.9869022965431213, 'validation/loss': 0.04503781348466873, 'validation/mean_average_precision': 0.2864156006446135, 'validation/num_examples': 43793, 'test/accuracy': 0.9861510992050171, 'test/loss': 0.04761037603020668, 'test/mean_average_precision': 0.27925009784739535, 'test/num_examples': 43793, 'score': 13218.463328838348, 'total_duration': 18989.863604068756, 'accumulated_submission_time': 13218.463328838348, 'accumulated_eval_time': 5767.809935808182, 'accumulated_logging_time': 2.4354753494262695}
I0206 11:09:49.714901 140229879785216 logging_writer.py:48] [41428] accumulated_eval_time=5767.809936, accumulated_logging_time=2.435475, accumulated_submission_time=13218.463329, global_step=41428, preemption_count=0, score=13218.463329, test/accuracy=0.986151, test/loss=0.047610, test/mean_average_precision=0.279250, test/num_examples=43793, total_duration=18989.863604, train/accuracy=0.992515, train/loss=0.023903, train/mean_average_precision=0.562859, validation/accuracy=0.986902, validation/loss=0.045038, validation/mean_average_precision=0.286416, validation/num_examples=43793
I0206 11:10:13.403039 140290179761920 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.09883572906255722, loss=0.027445046231150627
I0206 11:10:45.630761 140229879785216 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.06514094024896622, loss=0.025087032467126846
I0206 11:11:18.288997 140290179761920 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.06696328520774841, loss=0.024896681308746338
I0206 11:11:50.097079 140229879785216 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.07529810070991516, loss=0.02357432432472706
I0206 11:12:22.360429 140290179761920 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.07627227902412415, loss=0.027873974293470383
I0206 11:12:54.854804 140229879785216 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.08739404380321503, loss=0.027333030477166176
I0206 11:13:26.544426 140290179761920 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.07945077121257782, loss=0.026397567242383957
I0206 11:13:49.955210 140451058161472 spec.py:321] Evaluating on the training split.
I0206 11:15:21.938799 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 11:15:24.977894 140451058161472 spec.py:349] Evaluating on the test split.
I0206 11:15:28.028015 140451058161472 submission_runner.py:408] Time since start: 19328.20s, 	Step: 42176, 	{'train/accuracy': 0.9924070835113525, 'train/loss': 0.02437860332429409, 'train/mean_average_precision': 0.5376271174259066, 'validation/accuracy': 0.9868994355201721, 'validation/loss': 0.04490630701184273, 'validation/mean_average_precision': 0.2886609224145899, 'validation/num_examples': 43793, 'test/accuracy': 0.9860192537307739, 'test/loss': 0.04772264137864113, 'test/mean_average_precision': 0.27700951783763217, 'test/num_examples': 43793, 'score': 13458.669250249863, 'total_duration': 19328.203336000443, 'accumulated_submission_time': 13458.669250249863, 'accumulated_eval_time': 5865.882692337036, 'accumulated_logging_time': 2.4739890098571777}
I0206 11:15:28.052011 140229871392512 logging_writer.py:48] [42176] accumulated_eval_time=5865.882692, accumulated_logging_time=2.473989, accumulated_submission_time=13458.669250, global_step=42176, preemption_count=0, score=13458.669250, test/accuracy=0.986019, test/loss=0.047723, test/mean_average_precision=0.277010, test/num_examples=43793, total_duration=19328.203336, train/accuracy=0.992407, train/loss=0.024379, train/mean_average_precision=0.537627, validation/accuracy=0.986899, validation/loss=0.044906, validation/mean_average_precision=0.288661, validation/num_examples=43793
I0206 11:15:35.926898 140290188154624 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.07097630202770233, loss=0.02617350034415722
I0206 11:16:08.684725 140229871392512 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.07700232416391373, loss=0.027324741706252098
I0206 11:16:40.614578 140290188154624 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.06713831424713135, loss=0.025265641510486603
I0206 11:17:12.346369 140229871392512 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.0691927969455719, loss=0.022837795317173004
I0206 11:17:43.934099 140290188154624 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.08468721807003021, loss=0.027511026710271835
I0206 11:18:15.092996 140229871392512 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.0717523917555809, loss=0.026214400306344032
I0206 11:18:46.607710 140290188154624 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.09131387621164322, loss=0.026944058015942574
I0206 11:19:18.274421 140229871392512 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.09665850549936295, loss=0.02907673642039299
I0206 11:19:28.049357 140451058161472 spec.py:321] Evaluating on the training split.
I0206 11:21:04.532265 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 11:21:08.047962 140451058161472 spec.py:349] Evaluating on the test split.
I0206 11:21:11.475359 140451058161472 submission_runner.py:408] Time since start: 19671.65s, 	Step: 42932, 	{'train/accuracy': 0.9924079179763794, 'train/loss': 0.02405993454158306, 'train/mean_average_precision': 0.5503674235235126, 'validation/accuracy': 0.9870553016662598, 'validation/loss': 0.04493481293320656, 'validation/mean_average_precision': 0.2921576254088969, 'validation/num_examples': 43793, 'test/accuracy': 0.9862534403800964, 'test/loss': 0.04771368205547333, 'test/mean_average_precision': 0.2787048486685619, 'test/num_examples': 43793, 'score': 13698.63403391838, 'total_duration': 19671.65065932274, 'accumulated_submission_time': 13698.63403391838, 'accumulated_eval_time': 5969.3086223602295, 'accumulated_logging_time': 2.5101170539855957}
I0206 11:21:11.505661 140229879785216 logging_writer.py:48] [42932] accumulated_eval_time=5969.308622, accumulated_logging_time=2.510117, accumulated_submission_time=13698.634034, global_step=42932, preemption_count=0, score=13698.634034, test/accuracy=0.986253, test/loss=0.047714, test/mean_average_precision=0.278705, test/num_examples=43793, total_duration=19671.650659, train/accuracy=0.992408, train/loss=0.024060, train/mean_average_precision=0.550367, validation/accuracy=0.987055, validation/loss=0.044935, validation/mean_average_precision=0.292158, validation/num_examples=43793
I0206 11:21:33.972266 140248415348480 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.07786589860916138, loss=0.02459096908569336
I0206 11:22:06.432075 140229879785216 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.08319354057312012, loss=0.028148625046014786
I0206 11:22:38.960441 140248415348480 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.07824332267045975, loss=0.02638789638876915
I0206 11:23:11.694653 140229879785216 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.07639420032501221, loss=0.02494172379374504
I0206 11:23:44.207418 140248415348480 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.08448034524917603, loss=0.026022501289844513
I0206 11:24:16.521250 140229879785216 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.06950774788856506, loss=0.025546595454216003
I0206 11:24:48.264305 140248415348480 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.085799440741539, loss=0.026259731501340866
I0206 11:25:11.714328 140451058161472 spec.py:321] Evaluating on the training split.
I0206 11:26:48.028965 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 11:26:51.210282 140451058161472 spec.py:349] Evaluating on the test split.
I0206 11:26:54.340196 140451058161472 submission_runner.py:408] Time since start: 20014.52s, 	Step: 43675, 	{'train/accuracy': 0.9927031993865967, 'train/loss': 0.023324938490986824, 'train/mean_average_precision': 0.5674737634256353, 'validation/accuracy': 0.9871320724487305, 'validation/loss': 0.04500267282128334, 'validation/mean_average_precision': 0.2952275795871997, 'validation/num_examples': 43793, 'test/accuracy': 0.9862707257270813, 'test/loss': 0.04805930703878403, 'test/mean_average_precision': 0.275700904360155, 'test/num_examples': 43793, 'score': 13938.807181596756, 'total_duration': 20014.515516281128, 'accumulated_submission_time': 13938.807181596756, 'accumulated_eval_time': 6071.934442520142, 'accumulated_logging_time': 2.5530261993408203}
I0206 11:26:54.365359 140290179761920 logging_writer.py:48] [43675] accumulated_eval_time=6071.934443, accumulated_logging_time=2.553026, accumulated_submission_time=13938.807182, global_step=43675, preemption_count=0, score=13938.807182, test/accuracy=0.986271, test/loss=0.048059, test/mean_average_precision=0.275701, test/num_examples=43793, total_duration=20014.515516, train/accuracy=0.992703, train/loss=0.023325, train/mean_average_precision=0.567474, validation/accuracy=0.987132, validation/loss=0.045003, validation/mean_average_precision=0.295228, validation/num_examples=43793
I0206 11:27:03.022573 140290188154624 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.08646497875452042, loss=0.025734029710292816
I0206 11:27:34.617250 140290179761920 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.08781888335943222, loss=0.028335310518741608
I0206 11:28:06.420618 140290188154624 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.08937235176563263, loss=0.026709670200943947
I0206 11:28:37.958642 140290179761920 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.08464779704809189, loss=0.02698262594640255
I0206 11:29:09.869515 140290188154624 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.08388242870569229, loss=0.02542976476252079
I0206 11:29:41.006488 140290179761920 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.06623820215463638, loss=0.024269629269838333
I0206 11:30:13.082601 140290188154624 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.07993730157613754, loss=0.023754477500915527
I0206 11:30:44.582569 140290179761920 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.09236280620098114, loss=0.029389699921011925
I0206 11:30:54.646422 140451058161472 spec.py:321] Evaluating on the training split.
I0206 11:32:29.602041 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 11:32:32.745072 140451058161472 spec.py:349] Evaluating on the test split.
I0206 11:32:35.839157 140451058161472 submission_runner.py:408] Time since start: 20356.01s, 	Step: 44433, 	{'train/accuracy': 0.9927995800971985, 'train/loss': 0.022958585992455482, 'train/mean_average_precision': 0.5733450457802596, 'validation/accuracy': 0.9869570732116699, 'validation/loss': 0.044922780245542526, 'validation/mean_average_precision': 0.29425027494103423, 'validation/num_examples': 43793, 'test/accuracy': 0.9861220121383667, 'test/loss': 0.04775078222155571, 'test/mean_average_precision': 0.27613351271095554, 'test/num_examples': 43793, 'score': 14179.057677268982, 'total_duration': 20356.0144674778, 'accumulated_submission_time': 14179.057677268982, 'accumulated_eval_time': 6173.127116203308, 'accumulated_logging_time': 2.588895082473755}
I0206 11:32:35.871542 140229871392512 logging_writer.py:48] [44433] accumulated_eval_time=6173.127116, accumulated_logging_time=2.588895, accumulated_submission_time=14179.057677, global_step=44433, preemption_count=0, score=14179.057677, test/accuracy=0.986122, test/loss=0.047751, test/mean_average_precision=0.276134, test/num_examples=43793, total_duration=20356.014467, train/accuracy=0.992800, train/loss=0.022959, train/mean_average_precision=0.573345, validation/accuracy=0.986957, validation/loss=0.044923, validation/mean_average_precision=0.294250, validation/num_examples=43793
I0206 11:32:57.285429 140229879785216 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.09408794343471527, loss=0.026611512526869774
I0206 11:33:28.994905 140229871392512 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.06763537973165512, loss=0.023645902052521706
I0206 11:34:00.155435 140229879785216 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.08013109117746353, loss=0.02777807228267193
I0206 11:34:31.277822 140229871392512 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.07977600395679474, loss=0.02759292535483837
I0206 11:35:02.769695 140229879785216 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.07789339870214462, loss=0.023531757295131683
I0206 11:35:34.128672 140229871392512 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.09422465413808823, loss=0.028064122423529625
I0206 11:36:05.405496 140229879785216 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.07286275178194046, loss=0.028079908341169357
I0206 11:36:36.087237 140451058161472 spec.py:321] Evaluating on the training split.
I0206 11:38:12.934837 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 11:38:16.039435 140451058161472 spec.py:349] Evaluating on the test split.
I0206 11:38:19.140732 140451058161472 submission_runner.py:408] Time since start: 20699.32s, 	Step: 45198, 	{'train/accuracy': 0.992863118648529, 'train/loss': 0.022587724030017853, 'train/mean_average_precision': 0.5935786349699248, 'validation/accuracy': 0.9871166348457336, 'validation/loss': 0.04517955705523491, 'validation/mean_average_precision': 0.2876991151416316, 'validation/num_examples': 43793, 'test/accuracy': 0.9862256646156311, 'test/loss': 0.048146359622478485, 'test/mean_average_precision': 0.2737167666026969, 'test/num_examples': 43793, 'score': 14419.24270439148, 'total_duration': 20699.316056489944, 'accumulated_submission_time': 14419.24270439148, 'accumulated_eval_time': 6276.180570602417, 'accumulated_logging_time': 2.632185935974121}
I0206 11:38:19.164092 140248415348480 logging_writer.py:48] [45198] accumulated_eval_time=6276.180571, accumulated_logging_time=2.632186, accumulated_submission_time=14419.242704, global_step=45198, preemption_count=0, score=14419.242704, test/accuracy=0.986226, test/loss=0.048146, test/mean_average_precision=0.273717, test/num_examples=43793, total_duration=20699.316056, train/accuracy=0.992863, train/loss=0.022588, train/mean_average_precision=0.593579, validation/accuracy=0.987117, validation/loss=0.045180, validation/mean_average_precision=0.287699, validation/num_examples=43793
I0206 11:38:20.174643 140290179761920 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.07675138115882874, loss=0.024225058034062386
I0206 11:38:52.165738 140248415348480 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.10158418864011765, loss=0.02716873399913311
I0206 11:39:24.539964 140290179761920 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.07795612514019012, loss=0.02439711056649685
I0206 11:39:56.482674 140248415348480 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.0802266076207161, loss=0.0254674032330513
I0206 11:40:29.020081 140290179761920 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.07879134267568588, loss=0.024037212133407593
I0206 11:41:02.084635 140248415348480 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.09969860315322876, loss=0.026963945478200912
I0206 11:41:33.418206 140290179761920 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.07610948383808136, loss=0.024999169632792473
I0206 11:42:05.323077 140248415348480 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.08433816581964493, loss=0.02575797587633133
I0206 11:42:19.259837 140451058161472 spec.py:321] Evaluating on the training split.
I0206 11:43:52.434496 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 11:43:55.767785 140451058161472 spec.py:349] Evaluating on the test split.
I0206 11:43:59.028082 140451058161472 submission_runner.py:408] Time since start: 21039.20s, 	Step: 45945, 	{'train/accuracy': 0.9931178092956543, 'train/loss': 0.021580209955573082, 'train/mean_average_precision': 0.6017364886534111, 'validation/accuracy': 0.9870991706848145, 'validation/loss': 0.045759450644254684, 'validation/mean_average_precision': 0.2840771218195006, 'validation/num_examples': 43793, 'test/accuracy': 0.9862942695617676, 'test/loss': 0.048754580318927765, 'test/mean_average_precision': 0.27474917941695626, 'test/num_examples': 43793, 'score': 14659.305247306824, 'total_duration': 21039.20338702202, 'accumulated_submission_time': 14659.305247306824, 'accumulated_eval_time': 6375.948750257492, 'accumulated_logging_time': 2.666360378265381}
I0206 11:43:59.055580 140229871392512 logging_writer.py:48] [45945] accumulated_eval_time=6375.948750, accumulated_logging_time=2.666360, accumulated_submission_time=14659.305247, global_step=45945, preemption_count=0, score=14659.305247, test/accuracy=0.986294, test/loss=0.048755, test/mean_average_precision=0.274749, test/num_examples=43793, total_duration=21039.203387, train/accuracy=0.993118, train/loss=0.021580, train/mean_average_precision=0.601736, validation/accuracy=0.987099, validation/loss=0.045759, validation/mean_average_precision=0.284077, validation/num_examples=43793
I0206 11:44:16.695741 140290188154624 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.08822459727525711, loss=0.026227330788969994
I0206 11:44:47.993809 140229871392512 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.08072628825902939, loss=0.026096655055880547
I0206 11:45:19.242335 140290188154624 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.07367178052663803, loss=0.020617779344320297
I0206 11:45:51.036933 140229871392512 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.09141264110803604, loss=0.027032706886529922
I0206 11:46:22.526203 140290188154624 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.08522222191095352, loss=0.023790447041392326
I0206 11:46:53.840556 140229871392512 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.08490442484617233, loss=0.024431411176919937
I0206 11:47:25.554540 140290188154624 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.08505791425704956, loss=0.024517595767974854
I0206 11:47:56.774397 140229871392512 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.07879739254713058, loss=0.0242119450122118
I0206 11:47:59.295264 140451058161472 spec.py:321] Evaluating on the training split.
I0206 11:49:32.259717 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 11:49:35.365799 140451058161472 spec.py:349] Evaluating on the test split.
I0206 11:49:38.490230 140451058161472 submission_runner.py:408] Time since start: 21378.67s, 	Step: 46709, 	{'train/accuracy': 0.9932405352592468, 'train/loss': 0.021531784906983376, 'train/mean_average_precision': 0.607432035924848, 'validation/accuracy': 0.9870342016220093, 'validation/loss': 0.0452977679669857, 'validation/mean_average_precision': 0.28736797518016444, 'validation/num_examples': 43793, 'test/accuracy': 0.9862281680107117, 'test/loss': 0.04815120995044708, 'test/mean_average_precision': 0.27591957306825343, 'test/num_examples': 43793, 'score': 14899.513046503067, 'total_duration': 21378.665546417236, 'accumulated_submission_time': 14899.513046503067, 'accumulated_eval_time': 6475.143660068512, 'accumulated_logging_time': 2.705780267715454}
I0206 11:49:38.519435 140229879785216 logging_writer.py:48] [46709] accumulated_eval_time=6475.143660, accumulated_logging_time=2.705780, accumulated_submission_time=14899.513047, global_step=46709, preemption_count=0, score=14899.513047, test/accuracy=0.986228, test/loss=0.048151, test/mean_average_precision=0.275920, test/num_examples=43793, total_duration=21378.665546, train/accuracy=0.993241, train/loss=0.021532, train/mean_average_precision=0.607432, validation/accuracy=0.987034, validation/loss=0.045298, validation/mean_average_precision=0.287368, validation/num_examples=43793
I0206 11:50:07.800141 140248415348480 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.08366703987121582, loss=0.02421841211616993
I0206 11:50:39.514119 140229879785216 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.08410231024026871, loss=0.024716749787330627
I0206 11:51:11.256291 140248415348480 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.08266280591487885, loss=0.02451697550714016
I0206 11:51:42.926378 140229879785216 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.09389186650514603, loss=0.02684204652905464
I0206 11:52:14.412419 140248415348480 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.0883808359503746, loss=0.024339627474546432
I0206 11:52:45.866969 140229879785216 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.09645194560289383, loss=0.026285383850336075
I0206 11:53:17.619485 140248415348480 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.08244932442903519, loss=0.02434617094695568
I0206 11:53:38.691145 140451058161472 spec.py:321] Evaluating on the training split.
I0206 11:55:14.589794 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 11:55:17.631346 140451058161472 spec.py:349] Evaluating on the test split.
I0206 11:55:20.591173 140451058161472 submission_runner.py:408] Time since start: 21720.77s, 	Step: 47468, 	{'train/accuracy': 0.9933563470840454, 'train/loss': 0.020941488444805145, 'train/mean_average_precision': 0.6339597542179121, 'validation/accuracy': 0.9871182441711426, 'validation/loss': 0.04563377425074577, 'validation/mean_average_precision': 0.2866939484275668, 'validation/num_examples': 43793, 'test/accuracy': 0.986265242099762, 'test/loss': 0.04876919463276863, 'test/mean_average_precision': 0.2753911116802991, 'test/num_examples': 43793, 'score': 15139.652928829193, 'total_duration': 21720.766462802887, 'accumulated_submission_time': 15139.652928829193, 'accumulated_eval_time': 6577.043631315231, 'accumulated_logging_time': 2.747018337249756}
I0206 11:55:20.615804 140229871392512 logging_writer.py:48] [47468] accumulated_eval_time=6577.043631, accumulated_logging_time=2.747018, accumulated_submission_time=15139.652929, global_step=47468, preemption_count=0, score=15139.652929, test/accuracy=0.986265, test/loss=0.048769, test/mean_average_precision=0.275391, test/num_examples=43793, total_duration=21720.766463, train/accuracy=0.993356, train/loss=0.020941, train/mean_average_precision=0.633960, validation/accuracy=0.987118, validation/loss=0.045634, validation/mean_average_precision=0.286694, validation/num_examples=43793
I0206 11:55:31.031687 140290179761920 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.09803543239831924, loss=0.026043612509965897
I0206 11:56:02.607705 140229871392512 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.0984814316034317, loss=0.02312164381146431
I0206 11:56:34.791480 140290179761920 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.0923684686422348, loss=0.02409554459154606
I0206 11:57:06.832309 140229871392512 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.09262111037969589, loss=0.025248391553759575
I0206 11:57:38.554533 140290179761920 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.09710582345724106, loss=0.028835121542215347
I0206 11:58:10.274140 140229871392512 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.07859444618225098, loss=0.022640256211161613
I0206 11:58:41.750131 140290179761920 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.08675035089254379, loss=0.024556079879403114
I0206 11:59:13.183160 140229871392512 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.07644295692443848, loss=0.025356441736221313
I0206 11:59:20.677771 140451058161472 spec.py:321] Evaluating on the training split.
I0206 12:00:52.490065 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 12:00:55.562994 140451058161472 spec.py:349] Evaluating on the test split.
I0206 12:00:58.513494 140451058161472 submission_runner.py:408] Time since start: 22058.69s, 	Step: 48225, 	{'train/accuracy': 0.9933629035949707, 'train/loss': 0.021108966320753098, 'train/mean_average_precision': 0.6215161265846207, 'validation/accuracy': 0.9870553016662598, 'validation/loss': 0.04590858519077301, 'validation/mean_average_precision': 0.28292161729370696, 'validation/num_examples': 43793, 'test/accuracy': 0.9862454533576965, 'test/loss': 0.04892460256814957, 'test/mean_average_precision': 0.27515733139371834, 'test/num_examples': 43793, 'score': 15379.684081554413, 'total_duration': 22058.688819169998, 'accumulated_submission_time': 15379.684081554413, 'accumulated_eval_time': 6674.879307746887, 'accumulated_logging_time': 2.782480478286743}
I0206 12:00:58.538173 140229879785216 logging_writer.py:48] [48225] accumulated_eval_time=6674.879308, accumulated_logging_time=2.782480, accumulated_submission_time=15379.684082, global_step=48225, preemption_count=0, score=15379.684082, test/accuracy=0.986245, test/loss=0.048925, test/mean_average_precision=0.275157, test/num_examples=43793, total_duration=22058.688819, train/accuracy=0.993363, train/loss=0.021109, train/mean_average_precision=0.621516, validation/accuracy=0.987055, validation/loss=0.045909, validation/mean_average_precision=0.282922, validation/num_examples=43793
I0206 12:01:22.237736 140290188154624 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.08746413141489029, loss=0.025179585441946983
I0206 12:01:53.835927 140229879785216 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.11204299330711365, loss=0.023308776319026947
I0206 12:02:25.704360 140290188154624 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.07509385049343109, loss=0.023635471239686012
I0206 12:02:57.209249 140229879785216 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.08893544971942902, loss=0.026673534885048866
I0206 12:03:29.070016 140290188154624 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.09589292854070663, loss=0.025470925495028496
I0206 12:04:00.655828 140229879785216 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.10294751077890396, loss=0.0233162771910429
I0206 12:04:31.955672 140290188154624 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.1208229511976242, loss=0.029761096462607384
I0206 12:04:58.787169 140451058161472 spec.py:321] Evaluating on the training split.
I0206 12:06:33.684021 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 12:06:36.802415 140451058161472 spec.py:349] Evaluating on the test split.
I0206 12:06:39.766092 140451058161472 submission_runner.py:408] Time since start: 22399.94s, 	Step: 48987, 	{'train/accuracy': 0.9932666420936584, 'train/loss': 0.021226216107606888, 'train/mean_average_precision': 0.611152309972192, 'validation/accuracy': 0.9870601892471313, 'validation/loss': 0.04568899795413017, 'validation/mean_average_precision': 0.290932891249044, 'validation/num_examples': 43793, 'test/accuracy': 0.9862496256828308, 'test/loss': 0.048720210790634155, 'test/mean_average_precision': 0.2773312555186638, 'test/num_examples': 43793, 'score': 15619.90098786354, 'total_duration': 22399.941404104233, 'accumulated_submission_time': 15619.90098786354, 'accumulated_eval_time': 6775.858198404312, 'accumulated_logging_time': 2.8195629119873047}
I0206 12:06:39.791459 140248415348480 logging_writer.py:48] [48987] accumulated_eval_time=6775.858198, accumulated_logging_time=2.819563, accumulated_submission_time=15619.900988, global_step=48987, preemption_count=0, score=15619.900988, test/accuracy=0.986250, test/loss=0.048720, test/mean_average_precision=0.277331, test/num_examples=43793, total_duration=22399.941404, train/accuracy=0.993267, train/loss=0.021226, train/mean_average_precision=0.611152, validation/accuracy=0.987060, validation/loss=0.045689, validation/mean_average_precision=0.290933, validation/num_examples=43793
I0206 12:06:44.345250 140290179761920 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.09741660952568054, loss=0.024073444306850433
I0206 12:07:16.340532 140248415348480 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.10358414798974991, loss=0.025394517928361893
I0206 12:07:48.119580 140290179761920 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.09871726483106613, loss=0.025702444836497307
I0206 12:08:19.592793 140248415348480 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.08292712271213531, loss=0.025087349116802216
I0206 12:08:51.348381 140290179761920 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.09062406420707703, loss=0.02455773763358593
I0206 12:09:23.076365 140248415348480 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.07828406989574432, loss=0.02390759065747261
I0206 12:09:54.695818 140290179761920 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.08943622559309006, loss=0.02468200959265232
I0206 12:10:26.303775 140248415348480 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.08878493309020996, loss=0.022656431421637535
I0206 12:10:39.992406 140451058161472 spec.py:321] Evaluating on the training split.
I0206 12:12:13.638812 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 12:12:16.704473 140451058161472 spec.py:349] Evaluating on the test split.
I0206 12:12:19.746319 140451058161472 submission_runner.py:408] Time since start: 22739.92s, 	Step: 49745, 	{'train/accuracy': 0.9932006001472473, 'train/loss': 0.021596472710371017, 'train/mean_average_precision': 0.6055980733194052, 'validation/accuracy': 0.9869509935379028, 'validation/loss': 0.04600798338651657, 'validation/mean_average_precision': 0.2892799181159078, 'validation/num_examples': 43793, 'test/accuracy': 0.9861186742782593, 'test/loss': 0.04889053851366043, 'test/mean_average_precision': 0.27854648288360406, 'test/num_examples': 43793, 'score': 15860.069697856903, 'total_duration': 22739.921632766724, 'accumulated_submission_time': 15860.069697856903, 'accumulated_eval_time': 6875.612053394318, 'accumulated_logging_time': 2.857351303100586}
I0206 12:12:19.772114 140229871392512 logging_writer.py:48] [49745] accumulated_eval_time=6875.612053, accumulated_logging_time=2.857351, accumulated_submission_time=15860.069698, global_step=49745, preemption_count=0, score=15860.069698, test/accuracy=0.986119, test/loss=0.048891, test/mean_average_precision=0.278546, test/num_examples=43793, total_duration=22739.921633, train/accuracy=0.993201, train/loss=0.021596, train/mean_average_precision=0.605598, validation/accuracy=0.986951, validation/loss=0.046008, validation/mean_average_precision=0.289280, validation/num_examples=43793
I0206 12:12:37.630699 140290188154624 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.10673580318689346, loss=0.025511886924505234
I0206 12:13:09.955412 140229871392512 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.10105734318494797, loss=0.02521965652704239
I0206 12:13:42.634667 140290188154624 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.08656368404626846, loss=0.024599166586995125
I0206 12:14:15.500451 140229871392512 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.10168817639350891, loss=0.024301014840602875
I0206 12:14:48.012713 140290188154624 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.10058903694152832, loss=0.024407345801591873
I0206 12:15:20.332128 140229871392512 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.10184451192617416, loss=0.023133037611842155
I0206 12:15:52.553219 140290188154624 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.09300210326910019, loss=0.0223290603607893
I0206 12:16:20.033083 140451058161472 spec.py:321] Evaluating on the training split.
I0206 12:17:52.949945 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 12:17:56.020110 140451058161472 spec.py:349] Evaluating on the test split.
I0206 12:17:59.028326 140451058161472 submission_runner.py:408] Time since start: 23079.20s, 	Step: 50487, 	{'train/accuracy': 0.9930036664009094, 'train/loss': 0.022013938054442406, 'train/mean_average_precision': 0.593615504286974, 'validation/accuracy': 0.9869891405105591, 'validation/loss': 0.046247225254774094, 'validation/mean_average_precision': 0.2876874796115182, 'validation/num_examples': 43793, 'test/accuracy': 0.9861944913864136, 'test/loss': 0.04907441511750221, 'test/mean_average_precision': 0.27759458219580224, 'test/num_examples': 43793, 'score': 16100.29632639885, 'total_duration': 23079.203650951385, 'accumulated_submission_time': 16100.29632639885, 'accumulated_eval_time': 6974.607265710831, 'accumulated_logging_time': 2.895205497741699}
I0206 12:17:59.053613 140248415348480 logging_writer.py:48] [50487] accumulated_eval_time=6974.607266, accumulated_logging_time=2.895205, accumulated_submission_time=16100.296326, global_step=50487, preemption_count=0, score=16100.296326, test/accuracy=0.986194, test/loss=0.049074, test/mean_average_precision=0.277595, test/num_examples=43793, total_duration=23079.203651, train/accuracy=0.993004, train/loss=0.022014, train/mean_average_precision=0.593616, validation/accuracy=0.986989, validation/loss=0.046247, validation/mean_average_precision=0.287687, validation/num_examples=43793
I0206 12:18:03.774432 140290179761920 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.11588132381439209, loss=0.026954952627420425
I0206 12:18:35.199975 140248415348480 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.08975902944803238, loss=0.02447059378027916
I0206 12:19:06.719436 140290179761920 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.08748325705528259, loss=0.02660742588341236
I0206 12:19:38.106701 140248415348480 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.10341351479291916, loss=0.024344902485609055
I0206 12:20:09.455292 140290179761920 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.10032647848129272, loss=0.02356533333659172
I0206 12:20:40.656603 140248415348480 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.08704083412885666, loss=0.02227664552628994
I0206 12:21:12.131888 140290179761920 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.08570094406604767, loss=0.022612662985920906
I0206 12:21:43.155729 140248415348480 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.09432034939527512, loss=0.021100757643580437
I0206 12:21:59.180253 140451058161472 spec.py:321] Evaluating on the training split.
I0206 12:23:37.282897 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 12:23:40.313117 140451058161472 spec.py:349] Evaluating on the test split.
I0206 12:23:43.411712 140451058161472 submission_runner.py:408] Time since start: 23423.59s, 	Step: 51253, 	{'train/accuracy': 0.9931820631027222, 'train/loss': 0.021342569962143898, 'train/mean_average_precision': 0.6134943919388604, 'validation/accuracy': 0.9870630502700806, 'validation/loss': 0.04621974751353264, 'validation/mean_average_precision': 0.2916114859958877, 'validation/num_examples': 43793, 'test/accuracy': 0.9862766265869141, 'test/loss': 0.04924943670630455, 'test/mean_average_precision': 0.28097670900093114, 'test/num_examples': 43793, 'score': 16340.390928268433, 'total_duration': 23423.587039232254, 'accumulated_submission_time': 16340.390928268433, 'accumulated_eval_time': 7078.8386816978455, 'accumulated_logging_time': 2.932920217514038}
I0206 12:23:43.436869 140229879785216 logging_writer.py:48] [51253] accumulated_eval_time=7078.838682, accumulated_logging_time=2.932920, accumulated_submission_time=16340.390928, global_step=51253, preemption_count=0, score=16340.390928, test/accuracy=0.986277, test/loss=0.049249, test/mean_average_precision=0.280977, test/num_examples=43793, total_duration=23423.587039, train/accuracy=0.993182, train/loss=0.021343, train/mean_average_precision=0.613494, validation/accuracy=0.987063, validation/loss=0.046220, validation/mean_average_precision=0.291611, validation/num_examples=43793
I0206 12:23:58.787093 140290188154624 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.10498254001140594, loss=0.023079123347997665
I0206 12:24:29.970269 140229879785216 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.0883864089846611, loss=0.02313668094575405
I0206 12:25:01.092513 140290188154624 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.09034568816423416, loss=0.026092179119586945
I0206 12:25:32.552936 140229879785216 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.09163224697113037, loss=0.022365806624293327
I0206 12:26:04.062242 140290188154624 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.08540759980678558, loss=0.02181275002658367
I0206 12:26:35.150925 140229879785216 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.09614662081003189, loss=0.023884547874331474
I0206 12:27:07.439103 140290188154624 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.10028459876775742, loss=0.024045340716838837
I0206 12:27:38.701242 140229879785216 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.10998600721359253, loss=0.02473980374634266
I0206 12:27:43.471492 140451058161472 spec.py:321] Evaluating on the training split.
I0206 12:29:16.459929 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 12:29:19.557731 140451058161472 spec.py:349] Evaluating on the test split.
I0206 12:29:22.550931 140451058161472 submission_runner.py:408] Time since start: 23762.73s, 	Step: 52016, 	{'train/accuracy': 0.993344247341156, 'train/loss': 0.020870761945843697, 'train/mean_average_precision': 0.6242266281073867, 'validation/accuracy': 0.9870747923851013, 'validation/loss': 0.04647238552570343, 'validation/mean_average_precision': 0.2907870160289012, 'validation/num_examples': 43793, 'test/accuracy': 0.986294686794281, 'test/loss': 0.04942123219370842, 'test/mean_average_precision': 0.2847148471225769, 'test/num_examples': 43793, 'score': 16580.394956111908, 'total_duration': 23762.726235628128, 'accumulated_submission_time': 16580.394956111908, 'accumulated_eval_time': 7177.91805267334, 'accumulated_logging_time': 2.969144821166992}
I0206 12:29:22.575937 140229871392512 logging_writer.py:48] [52016] accumulated_eval_time=7177.918053, accumulated_logging_time=2.969145, accumulated_submission_time=16580.394956, global_step=52016, preemption_count=0, score=16580.394956, test/accuracy=0.986295, test/loss=0.049421, test/mean_average_precision=0.284715, test/num_examples=43793, total_duration=23762.726236, train/accuracy=0.993344, train/loss=0.020871, train/mean_average_precision=0.624227, validation/accuracy=0.987075, validation/loss=0.046472, validation/mean_average_precision=0.290787, validation/num_examples=43793
I0206 12:29:49.279480 140248415348480 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.11357633769512177, loss=0.02608194574713707
I0206 12:30:20.712564 140229871392512 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.08391620218753815, loss=0.021109696477651596
I0206 12:30:52.098993 140248415348480 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.09444642066955566, loss=0.021411869674921036
I0206 12:31:23.688672 140229871392512 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.09328994899988174, loss=0.02121264673769474
I0206 12:31:55.261584 140248415348480 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.10448678582906723, loss=0.023033497855067253
I0206 12:32:26.452990 140229871392512 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.10291333496570587, loss=0.020923251286149025
I0206 12:32:58.149769 140248415348480 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.11011100560426712, loss=0.026050472632050514
I0206 12:33:22.669527 140451058161472 spec.py:321] Evaluating on the training split.
I0206 12:35:00.925215 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 12:35:04.124513 140451058161472 spec.py:349] Evaluating on the test split.
I0206 12:35:07.124891 140451058161472 submission_runner.py:408] Time since start: 24107.30s, 	Step: 52779, 	{'train/accuracy': 0.993465006351471, 'train/loss': 0.020574089139699936, 'train/mean_average_precision': 0.6301589141153873, 'validation/accuracy': 0.9871048331260681, 'validation/loss': 0.04623021185398102, 'validation/mean_average_precision': 0.29009999259995073, 'validation/num_examples': 43793, 'test/accuracy': 0.9862024784088135, 'test/loss': 0.049345795065164566, 'test/mean_average_precision': 0.2773575915352462, 'test/num_examples': 43793, 'score': 16820.457956552505, 'total_duration': 24107.30019426346, 'accumulated_submission_time': 16820.457956552505, 'accumulated_eval_time': 7282.37335062027, 'accumulated_logging_time': 3.0050220489501953}
I0206 12:35:07.150285 140229879785216 logging_writer.py:48] [52779] accumulated_eval_time=7282.373351, accumulated_logging_time=3.005022, accumulated_submission_time=16820.457957, global_step=52779, preemption_count=0, score=16820.457957, test/accuracy=0.986202, test/loss=0.049346, test/mean_average_precision=0.277358, test/num_examples=43793, total_duration=24107.300194, train/accuracy=0.993465, train/loss=0.020574, train/mean_average_precision=0.630159, validation/accuracy=0.987105, validation/loss=0.046230, validation/mean_average_precision=0.290100, validation/num_examples=43793
I0206 12:35:14.180889 140290179761920 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.09975636750459671, loss=0.02093048021197319
I0206 12:35:45.544214 140229879785216 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.10604876279830933, loss=0.02642420120537281
I0206 12:36:17.203187 140290179761920 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.08450645953416824, loss=0.02155314013361931
I0206 12:36:48.349529 140229879785216 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.10446743667125702, loss=0.0200547743588686
I0206 12:37:20.144455 140290179761920 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.10590840131044388, loss=0.022198431193828583
I0206 12:37:51.690850 140229879785216 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.11367432028055191, loss=0.024877389892935753
I0206 12:38:23.326582 140290179761920 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.1111295074224472, loss=0.02378202974796295
I0206 12:38:55.184035 140229879785216 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.10695689916610718, loss=0.02532104216516018
I0206 12:39:07.393937 140451058161472 spec.py:321] Evaluating on the training split.
I0206 12:40:39.013693 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 12:40:42.076586 140451058161472 spec.py:349] Evaluating on the test split.
I0206 12:40:45.062703 140451058161472 submission_runner.py:408] Time since start: 24445.24s, 	Step: 53539, 	{'train/accuracy': 0.9936485290527344, 'train/loss': 0.019793720915913582, 'train/mean_average_precision': 0.6516988153813621, 'validation/accuracy': 0.9871584177017212, 'validation/loss': 0.04659149423241615, 'validation/mean_average_precision': 0.29266215846242827, 'validation/num_examples': 43793, 'test/accuracy': 0.9862854480743408, 'test/loss': 0.049793995916843414, 'test/mean_average_precision': 0.2795073140182851, 'test/num_examples': 43793, 'score': 17060.670438051224, 'total_duration': 24445.238028764725, 'accumulated_submission_time': 17060.670438051224, 'accumulated_eval_time': 7380.0420706272125, 'accumulated_logging_time': 3.041551351547241}
I0206 12:40:45.087690 140229871392512 logging_writer.py:48] [53539] accumulated_eval_time=7380.042071, accumulated_logging_time=3.041551, accumulated_submission_time=17060.670438, global_step=53539, preemption_count=0, score=17060.670438, test/accuracy=0.986285, test/loss=0.049794, test/mean_average_precision=0.279507, test/num_examples=43793, total_duration=24445.238029, train/accuracy=0.993649, train/loss=0.019794, train/mean_average_precision=0.651699, validation/accuracy=0.987158, validation/loss=0.046591, validation/mean_average_precision=0.292662, validation/num_examples=43793
I0206 12:41:04.861226 140290188154624 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.11196299642324448, loss=0.02517540566623211
I0206 12:41:37.242470 140229871392512 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.09595280140638351, loss=0.01962161809206009
I0206 12:42:09.715025 140290188154624 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.10643446445465088, loss=0.023640410974621773
I0206 12:42:42.155583 140229871392512 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.12119890749454498, loss=0.024898124858736992
I0206 12:43:14.520967 140290188154624 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.10372373461723328, loss=0.024056782945990562
I0206 12:43:45.956731 140229871392512 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.10308412462472916, loss=0.025608692318201065
I0206 12:44:17.949586 140290188154624 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.10005304217338562, loss=0.02397225797176361
I0206 12:44:45.244378 140451058161472 spec.py:321] Evaluating on the training split.
I0206 12:46:18.819470 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 12:46:25.384662 140451058161472 spec.py:349] Evaluating on the test split.
I0206 12:46:28.686905 140451058161472 submission_runner.py:408] Time since start: 24788.86s, 	Step: 54287, 	{'train/accuracy': 0.993804931640625, 'train/loss': 0.019352471455931664, 'train/mean_average_precision': 0.6605385215433868, 'validation/accuracy': 0.9869335293769836, 'validation/loss': 0.04682302474975586, 'validation/mean_average_precision': 0.29329886200359595, 'validation/num_examples': 43793, 'test/accuracy': 0.9861018061637878, 'test/loss': 0.049933452159166336, 'test/mean_average_precision': 0.2787303739599055, 'test/num_examples': 43793, 'score': 17300.793816804886, 'total_duration': 24788.86217021942, 'accumulated_submission_time': 17300.793816804886, 'accumulated_eval_time': 7483.484494686127, 'accumulated_logging_time': 3.0773909091949463}
I0206 12:46:28.717036 140229879785216 logging_writer.py:48] [54287] accumulated_eval_time=7483.484495, accumulated_logging_time=3.077391, accumulated_submission_time=17300.793817, global_step=54287, preemption_count=0, score=17300.793817, test/accuracy=0.986102, test/loss=0.049933, test/mean_average_precision=0.278730, test/num_examples=43793, total_duration=24788.862170, train/accuracy=0.993805, train/loss=0.019352, train/mean_average_precision=0.660539, validation/accuracy=0.986934, validation/loss=0.046823, validation/mean_average_precision=0.293299, validation/num_examples=43793
I0206 12:46:33.265488 140290179761920 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.08944207429885864, loss=0.020360127091407776
I0206 12:47:05.789119 140229879785216 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.11430347710847855, loss=0.02273615263402462
I0206 12:47:37.474838 140290179761920 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.09644385427236557, loss=0.022164013236761093
I0206 12:48:09.853249 140229879785216 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.10756229609251022, loss=0.0249891746789217
I0206 12:48:42.225120 140290179761920 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.10332594066858292, loss=0.024553397670388222
I0206 12:49:14.244085 140229879785216 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.09850697964429855, loss=0.023064229637384415
I0206 12:49:46.190330 140290179761920 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.11437498778104782, loss=0.02370559610426426
I0206 12:50:18.308702 140229879785216 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.10424413532018661, loss=0.022366337478160858
I0206 12:50:28.805954 140451058161472 spec.py:321] Evaluating on the training split.
I0206 12:52:06.213521 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 12:52:09.316967 140451058161472 spec.py:349] Evaluating on the test split.
I0206 12:52:12.268591 140451058161472 submission_runner.py:408] Time since start: 25132.44s, 	Step: 55034, 	{'train/accuracy': 0.9940738677978516, 'train/loss': 0.018616709858179092, 'train/mean_average_precision': 0.6824790188145755, 'validation/accuracy': 0.9870139360427856, 'validation/loss': 0.04679595306515694, 'validation/mean_average_precision': 0.291244495393672, 'validation/num_examples': 43793, 'test/accuracy': 0.9861464500427246, 'test/loss': 0.050152502954006195, 'test/mean_average_precision': 0.2725020764620905, 'test/num_examples': 43793, 'score': 17540.84943985939, 'total_duration': 25132.443913459778, 'accumulated_submission_time': 17540.84943985939, 'accumulated_eval_time': 7586.947088718414, 'accumulated_logging_time': 3.1195337772369385}
I0206 12:52:12.295332 140229871392512 logging_writer.py:48] [55034] accumulated_eval_time=7586.947089, accumulated_logging_time=3.119534, accumulated_submission_time=17540.849440, global_step=55034, preemption_count=0, score=17540.849440, test/accuracy=0.986146, test/loss=0.050153, test/mean_average_precision=0.272502, test/num_examples=43793, total_duration=25132.443913, train/accuracy=0.994074, train/loss=0.018617, train/mean_average_precision=0.682479, validation/accuracy=0.987014, validation/loss=0.046796, validation/mean_average_precision=0.291244, validation/num_examples=43793
I0206 12:52:33.451679 140248415348480 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.0961994007229805, loss=0.021715642884373665
I0206 12:53:05.481767 140229871392512 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.10367656499147415, loss=0.022269360721111298
I0206 12:53:37.869884 140248415348480 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.1247263252735138, loss=0.02447635680437088
I0206 12:54:09.662345 140229871392512 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.09216360747814178, loss=0.020323462784290314
I0206 12:54:41.614138 140248415348480 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.1063479632139206, loss=0.02572476677596569
I0206 12:55:13.602839 140229871392512 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.0983087494969368, loss=0.022933218628168106
I0206 12:55:45.379646 140248415348480 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.1104566752910614, loss=0.02180558070540428
I0206 12:56:12.369003 140451058161472 spec.py:321] Evaluating on the training split.
I0206 12:57:46.454475 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 12:57:49.562844 140451058161472 spec.py:349] Evaluating on the test split.
I0206 12:57:52.578434 140451058161472 submission_runner.py:408] Time since start: 25472.75s, 	Step: 55785, 	{'train/accuracy': 0.9942629933357239, 'train/loss': 0.018172141164541245, 'train/mean_average_precision': 0.6914053178116203, 'validation/accuracy': 0.9870870113372803, 'validation/loss': 0.04705231636762619, 'validation/mean_average_precision': 0.2891383001431945, 'validation/num_examples': 43793, 'test/accuracy': 0.9862500429153442, 'test/loss': 0.050257518887519836, 'test/mean_average_precision': 0.27844368586421997, 'test/num_examples': 43793, 'score': 17780.89176774025, 'total_duration': 25472.753759860992, 'accumulated_submission_time': 17780.89176774025, 'accumulated_eval_time': 7687.15647649765, 'accumulated_logging_time': 3.1576216220855713}
I0206 12:57:52.604229 140229879785216 logging_writer.py:48] [55785] accumulated_eval_time=7687.156476, accumulated_logging_time=3.157622, accumulated_submission_time=17780.891768, global_step=55785, preemption_count=0, score=17780.891768, test/accuracy=0.986250, test/loss=0.050258, test/mean_average_precision=0.278444, test/num_examples=43793, total_duration=25472.753760, train/accuracy=0.994263, train/loss=0.018172, train/mean_average_precision=0.691405, validation/accuracy=0.987087, validation/loss=0.047052, validation/mean_average_precision=0.289138, validation/num_examples=43793
I0206 12:57:58.009290 140290188154624 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.10487117618322372, loss=0.024929435923695564
I0206 12:58:29.962084 140229879785216 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.09968613088130951, loss=0.019887492060661316
I0206 12:59:02.280162 140290188154624 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.11198478937149048, loss=0.024009620770812035
I0206 12:59:34.210783 140229879785216 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.1025874987244606, loss=0.021562781184911728
I0206 13:00:06.505371 140290188154624 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.11691312491893768, loss=0.023270612582564354
I0206 13:00:37.974237 140229879785216 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.10633684694766998, loss=0.021055085584521294
I0206 13:01:09.493408 140290188154624 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.10632865130901337, loss=0.021695438772439957
I0206 13:01:41.287110 140229879785216 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.09895655512809753, loss=0.020049691200256348
I0206 13:01:52.585106 140451058161472 spec.py:321] Evaluating on the training split.
I0206 13:03:28.946189 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 13:03:32.162528 140451058161472 spec.py:349] Evaluating on the test split.
I0206 13:03:35.233588 140451058161472 submission_runner.py:408] Time since start: 25815.41s, 	Step: 56537, 	{'train/accuracy': 0.994239866733551, 'train/loss': 0.018327660858631134, 'train/mean_average_precision': 0.685328157042368, 'validation/accuracy': 0.9870309829711914, 'validation/loss': 0.04702512547373772, 'validation/mean_average_precision': 0.2896792143578579, 'validation/num_examples': 43793, 'test/accuracy': 0.986182689666748, 'test/loss': 0.050270892679691315, 'test/mean_average_precision': 0.2769247715519446, 'test/num_examples': 43793, 'score': 18020.839967250824, 'total_duration': 25815.408913373947, 'accumulated_submission_time': 18020.839967250824, 'accumulated_eval_time': 7789.804910898209, 'accumulated_logging_time': 3.1956868171691895}
I0206 13:03:35.259203 140229871392512 logging_writer.py:48] [56537] accumulated_eval_time=7789.804911, accumulated_logging_time=3.195687, accumulated_submission_time=18020.839967, global_step=56537, preemption_count=0, score=18020.839967, test/accuracy=0.986183, test/loss=0.050271, test/mean_average_precision=0.276925, test/num_examples=43793, total_duration=25815.408913, train/accuracy=0.994240, train/loss=0.018328, train/mean_average_precision=0.685328, validation/accuracy=0.987031, validation/loss=0.047025, validation/mean_average_precision=0.289679, validation/num_examples=43793
I0206 13:03:56.502665 140248415348480 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.12222444266080856, loss=0.021294770762324333
I0206 13:04:28.100288 140229871392512 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.112810879945755, loss=0.02145334705710411
I0206 13:04:59.434499 140248415348480 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.10954921692609787, loss=0.020088067278265953
I0206 13:05:30.700800 140229871392512 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.12624670565128326, loss=0.02225702442228794
I0206 13:06:02.488841 140248415348480 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.10057986527681351, loss=0.02057831361889839
I0206 13:06:33.851734 140229871392512 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.10964593291282654, loss=0.02243863418698311
I0206 13:07:05.560704 140248415348480 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.12722426652908325, loss=0.021555693820118904
I0206 13:07:35.367195 140451058161472 spec.py:321] Evaluating on the training split.
I0206 13:09:07.471539 140451058161472 spec.py:333] Evaluating on the validation split.
I0206 13:09:10.580357 140451058161472 spec.py:349] Evaluating on the test split.
I0206 13:09:13.607517 140451058161472 submission_runner.py:408] Time since start: 26153.78s, 	Step: 57296, 	{'train/accuracy': 0.9940085411071777, 'train/loss': 0.01863333210349083, 'train/mean_average_precision': 0.6795497812086909, 'validation/accuracy': 0.9870337843894958, 'validation/loss': 0.04753246530890465, 'validation/mean_average_precision': 0.2885810856821342, 'validation/num_examples': 43793, 'test/accuracy': 0.9863001704216003, 'test/loss': 0.05074793100357056, 'test/mean_average_precision': 0.2777486826024884, 'test/num_examples': 43793, 'score': 18260.91561436653, 'total_duration': 26153.78284072876, 'accumulated_submission_time': 18260.91561436653, 'accumulated_eval_time': 7888.045190811157, 'accumulated_logging_time': 3.2336459159851074}
I0206 13:09:13.633414 140229879785216 logging_writer.py:48] [57296] accumulated_eval_time=7888.045191, accumulated_logging_time=3.233646, accumulated_submission_time=18260.915614, global_step=57296, preemption_count=0, score=18260.915614, test/accuracy=0.986300, test/loss=0.050748, test/mean_average_precision=0.277749, test/num_examples=43793, total_duration=26153.782841, train/accuracy=0.994009, train/loss=0.018633, train/mean_average_precision=0.679550, validation/accuracy=0.987034, validation/loss=0.047532, validation/mean_average_precision=0.288581, validation/num_examples=43793
I0206 13:09:15.298001 140290179761920 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.10563655197620392, loss=0.021338263526558876
I0206 13:09:46.814312 140229879785216 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.1179981604218483, loss=0.02035694569349289
I0206 13:10:18.142369 140290179761920 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.11783178150653839, loss=0.023237252607941628
I0206 13:10:49.157672 140229879785216 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.12014279514551163, loss=0.02137639746069908
I0206 13:11:20.780255 140290179761920 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.12612247467041016, loss=0.023848051205277443
I0206 13:11:52.145020 140229879785216 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.10702262818813324, loss=0.02056477777659893
I0206 13:12:23.649778 140290179761920 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.1263371706008911, loss=0.022879352793097496
I0206 13:12:49.998064 140229879785216 logging_writer.py:48] [57983] global_step=57983, preemption_count=0, score=18477.231257
I0206 13:12:50.119601 140451058161472 checkpoints.py:490] Saving checkpoint at step: 57983
I0206 13:12:50.247363 140451058161472 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_2/ogbg_jax/trial_5/checkpoint_57983
I0206 13:12:50.248603 140451058161472 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_2/ogbg_jax/trial_5/checkpoint_57983.
I0206 13:12:50.466718 140451058161472 submission_runner.py:583] Tuning trial 5/5
I0206 13:12:50.467031 140451058161472 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0206 13:12:50.472011 140451058161472 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5288358926773071, 'train/loss': 0.7364843487739563, 'train/mean_average_precision': 0.020878767422651344, 'validation/accuracy': 0.5270814895629883, 'validation/loss': 0.737440824508667, 'validation/mean_average_precision': 0.024115497491982076, 'validation/num_examples': 43793, 'test/accuracy': 0.5256850719451904, 'test/loss': 0.7376683354377747, 'test/mean_average_precision': 0.02603311504248907, 'test/num_examples': 43793, 'score': 13.132712841033936, 'total_duration': 116.17477297782898, 'accumulated_submission_time': 13.132712841033936, 'accumulated_eval_time': 103.04201483726501, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (749, {'train/accuracy': 0.9869271516799927, 'train/loss': 0.05065993219614029, 'train/mean_average_precision': 0.05997201030197283, 'validation/accuracy': 0.984121561050415, 'validation/loss': 0.060095831751823425, 'validation/mean_average_precision': 0.056914372746511545, 'validation/num_examples': 43793, 'test/accuracy': 0.9831327795982361, 'test/loss': 0.06328555196523666, 'test/mean_average_precision': 0.058255329912699876, 'test/num_examples': 43793, 'score': 252.96980333328247, 'total_duration': 459.00523805618286, 'accumulated_submission_time': 252.96980333328247, 'accumulated_eval_time': 205.71116495132446, 'accumulated_logging_time': 0.3030984401702881, 'global_step': 749, 'preemption_count': 0}), (1503, {'train/accuracy': 0.9873338937759399, 'train/loss': 0.04714243486523628, 'train/mean_average_precision': 0.10909406662670157, 'validation/accuracy': 0.9847106337547302, 'validation/loss': 0.056974757462739944, 'validation/mean_average_precision': 0.10810284037495257, 'validation/num_examples': 43793, 'test/accuracy': 0.983751118183136, 'test/loss': 0.06052108854055405, 'test/mean_average_precision': 0.10710254149319327, 'test/num_examples': 43793, 'score': 493.0213305950165, 'total_duration': 806.6227686405182, 'accumulated_submission_time': 493.0213305950165, 'accumulated_eval_time': 313.22933530807495, 'accumulated_logging_time': 0.33067798614501953, 'global_step': 1503, 'preemption_count': 0}), (2259, {'train/accuracy': 0.9877690076828003, 'train/loss': 0.04325583949685097, 'train/mean_average_precision': 0.14656782099532822, 'validation/accuracy': 0.9849586486816406, 'validation/loss': 0.05234578624367714, 'validation/mean_average_precision': 0.13774467362244358, 'validation/num_examples': 43793, 'test/accuracy': 0.9840215444564819, 'test/loss': 0.055156830698251724, 'test/mean_average_precision': 0.14165930443343672, 'test/num_examples': 43793, 'score': 733.1634314060211, 'total_duration': 1150.4456820487976, 'accumulated_submission_time': 733.1634314060211, 'accumulated_eval_time': 416.8628304004669, 'accumulated_logging_time': 0.3571460247039795, 'global_step': 2259, 'preemption_count': 0}), (3014, {'train/accuracy': 0.9881166219711304, 'train/loss': 0.04161359742283821, 'train/mean_average_precision': 0.17624069387477692, 'validation/accuracy': 0.9850816130638123, 'validation/loss': 0.05104980617761612, 'validation/mean_average_precision': 0.15291850797555867, 'validation/num_examples': 43793, 'test/accuracy': 0.9841373562812805, 'test/loss': 0.053809214383363724, 'test/mean_average_precision': 0.15615814952137944, 'test/num_examples': 43793, 'score': 973.2205414772034, 'total_duration': 1492.9210562705994, 'accumulated_submission_time': 973.2205414772034, 'accumulated_eval_time': 519.2329788208008, 'accumulated_logging_time': 0.38398075103759766, 'global_step': 3014, 'preemption_count': 0}), (3762, {'train/accuracy': 0.9884961247444153, 'train/loss': 0.03974774479866028, 'train/mean_average_precision': 0.19839875751331684, 'validation/accuracy': 0.9855943322181702, 'validation/loss': 0.049146659672260284, 'validation/mean_average_precision': 0.17655218056886918, 'validation/num_examples': 43793, 'test/accuracy': 0.984641969203949, 'test/loss': 0.05208675563335419, 'test/mean_average_precision': 0.18236919829215972, 'test/num_examples': 43793, 'score': 1213.301376581192, 'total_duration': 1833.0843858718872, 'accumulated_submission_time': 1213.301376581192, 'accumulated_eval_time': 619.2677090167999, 'accumulated_logging_time': 0.4104807376861572, 'global_step': 3762, 'preemption_count': 0}), (4510, {'train/accuracy': 0.9887267351150513, 'train/loss': 0.03883034363389015, 'train/mean_average_precision': 0.2203952174799551, 'validation/accuracy': 0.9857790470123291, 'validation/loss': 0.04811367392539978, 'validation/mean_average_precision': 0.1902895326825564, 'validation/num_examples': 43793, 'test/accuracy': 0.9848470687866211, 'test/loss': 0.050781041383743286, 'test/mean_average_precision': 0.19348862270885217, 'test/num_examples': 43793, 'score': 1453.3801944255829, 'total_duration': 2173.8443462848663, 'accumulated_submission_time': 1453.3801944255829, 'accumulated_eval_time': 719.900552034378, 'accumulated_logging_time': 0.438244104385376, 'global_step': 4510, 'preemption_count': 0}), (5261, {'train/accuracy': 0.9889106750488281, 'train/loss': 0.03774059936404228, 'train/mean_average_precision': 0.237520259220091, 'validation/accuracy': 0.985815167427063, 'validation/loss': 0.047681551426649094, 'validation/mean_average_precision': 0.2013130685938968, 'validation/num_examples': 43793, 'test/accuracy': 0.984906017780304, 'test/loss': 0.0503714494407177, 'test/mean_average_precision': 0.20533108427856772, 'test/num_examples': 43793, 'score': 1693.5554299354553, 'total_duration': 2517.606040239334, 'accumulated_submission_time': 1693.5554299354553, 'accumulated_eval_time': 823.4393961429596, 'accumulated_logging_time': 0.4656791687011719, 'global_step': 5261, 'preemption_count': 0}), (6010, {'train/accuracy': 0.9891675114631653, 'train/loss': 0.03660612180829048, 'train/mean_average_precision': 0.25422043343603723, 'validation/accuracy': 0.9861252903938293, 'validation/loss': 0.04656600579619408, 'validation/mean_average_precision': 0.216148982664498, 'validation/num_examples': 43793, 'test/accuracy': 0.985258162021637, 'test/loss': 0.04918884485960007, 'test/mean_average_precision': 0.21554773051913081, 'test/num_examples': 43793, 'score': 1933.570939540863, 'total_duration': 2865.1703193187714, 'accumulated_submission_time': 1933.570939540863, 'accumulated_eval_time': 930.9390976428986, 'accumulated_logging_time': 0.49407386779785156, 'global_step': 6010, 'preemption_count': 0}), (6751, {'train/accuracy': 0.9891411662101746, 'train/loss': 0.036478422582149506, 'train/mean_average_precision': 0.27995736027791956, 'validation/accuracy': 0.9861935377120972, 'validation/loss': 0.04652482271194458, 'validation/mean_average_precision': 0.22070827583285108, 'validation/num_examples': 43793, 'test/accuracy': 0.9853482842445374, 'test/loss': 0.049075283110141754, 'test/mean_average_precision': 0.22745468948360154, 'test/num_examples': 43793, 'score': 2173.825298309326, 'total_duration': 3209.513146877289, 'accumulated_submission_time': 2173.825298309326, 'accumulated_eval_time': 1034.9742851257324, 'accumulated_logging_time': 0.5242447853088379, 'global_step': 6751, 'preemption_count': 0}), (7506, {'train/accuracy': 0.9896240234375, 'train/loss': 0.03505132719874382, 'train/mean_average_precision': 0.29706911150222315, 'validation/accuracy': 0.9860489964485168, 'validation/loss': 0.04668343812227249, 'validation/mean_average_precision': 0.2174827924270316, 'validation/num_examples': 43793, 'test/accuracy': 0.9852122664451599, 'test/loss': 0.04923328384757042, 'test/mean_average_precision': 0.22008655169617167, 'test/num_examples': 43793, 'score': 2414.03213095665, 'total_duration': 3556.8294620513916, 'accumulated_submission_time': 2414.03213095665, 'accumulated_eval_time': 1142.0342810153961, 'accumulated_logging_time': 0.5527384281158447, 'global_step': 7506, 'preemption_count': 0}), (8241, {'train/accuracy': 0.9899398684501648, 'train/loss': 0.03389454632997513, 'train/mean_average_precision': 0.32697509784170464, 'validation/accuracy': 0.9864467978477478, 'validation/loss': 0.04521428048610687, 'validation/mean_average_precision': 0.23893564450384508, 'validation/num_examples': 43793, 'test/accuracy': 0.9855904579162598, 'test/loss': 0.04803219437599182, 'test/mean_average_precision': 0.235439831915132, 'test/num_examples': 43793, 'score': 2654.1512384414673, 'total_duration': 3903.309905767441, 'accumulated_submission_time': 2654.1512384414673, 'accumulated_eval_time': 1248.3411090373993, 'accumulated_logging_time': 0.5830700397491455, 'global_step': 8241, 'preemption_count': 0}), (8992, {'train/accuracy': 0.9901648759841919, 'train/loss': 0.03290893882513046, 'train/mean_average_precision': 0.343574061270163, 'validation/accuracy': 0.9863510131835938, 'validation/loss': 0.04535606876015663, 'validation/mean_average_precision': 0.2322570774352511, 'validation/num_examples': 43793, 'test/accuracy': 0.9855828881263733, 'test/loss': 0.04782380908727646, 'test/mean_average_precision': 0.2394972606796361, 'test/num_examples': 43793, 'score': 2894.265217065811, 'total_duration': 4255.120206356049, 'accumulated_submission_time': 2894.265217065811, 'accumulated_eval_time': 1359.987991809845, 'accumulated_logging_time': 0.6116423606872559, 'global_step': 8992, 'preemption_count': 0}), (9742, {'train/accuracy': 0.9900648593902588, 'train/loss': 0.03303992748260498, 'train/mean_average_precision': 0.33678358815061643, 'validation/accuracy': 0.9864760637283325, 'validation/loss': 0.04529368504881859, 'validation/mean_average_precision': 0.23703560899096426, 'validation/num_examples': 43793, 'test/accuracy': 0.9856751561164856, 'test/loss': 0.04794555902481079, 'test/mean_average_precision': 0.2425333064582841, 'test/num_examples': 43793, 'score': 3134.242573261261, 'total_duration': 4598.49095082283, 'accumulated_submission_time': 3134.242573261261, 'accumulated_eval_time': 1463.3309633731842, 'accumulated_logging_time': 0.6411969661712646, 'global_step': 9742, 'preemption_count': 0}), (10495, {'train/accuracy': 0.9899468421936035, 'train/loss': 0.033407848328351974, 'train/mean_average_precision': 0.3359655048713717, 'validation/accuracy': 0.986441969871521, 'validation/loss': 0.04499449208378792, 'validation/mean_average_precision': 0.24853004020870537, 'validation/num_examples': 43793, 'test/accuracy': 0.9855492115020752, 'test/loss': 0.047698359936475754, 'test/mean_average_precision': 0.2441579029718845, 'test/num_examples': 43793, 'score': 3374.392449617386, 'total_duration': 4942.783852100372, 'accumulated_submission_time': 3374.392449617386, 'accumulated_eval_time': 1567.4261529445648, 'accumulated_logging_time': 0.6686229705810547, 'global_step': 10495, 'preemption_count': 0}), (11255, {'train/accuracy': 0.9901042580604553, 'train/loss': 0.03283253312110901, 'train/mean_average_precision': 0.350322015439398, 'validation/accuracy': 0.9866904020309448, 'validation/loss': 0.044543277472257614, 'validation/mean_average_precision': 0.26153233983646823, 'validation/num_examples': 43793, 'test/accuracy': 0.9857833981513977, 'test/loss': 0.047420669347047806, 'test/mean_average_precision': 0.25411081427279725, 'test/num_examples': 43793, 'score': 3614.5693922042847, 'total_duration': 5285.517816543579, 'accumulated_submission_time': 3614.5693922042847, 'accumulated_eval_time': 1669.9343152046204, 'accumulated_logging_time': 0.6964986324310303, 'global_step': 11255, 'preemption_count': 0}), (11991, {'train/accuracy': 0.9903876781463623, 'train/loss': 0.03184294328093529, 'train/mean_average_precision': 0.35954482673416543, 'validation/accuracy': 0.9868012070655823, 'validation/loss': 0.044386640191078186, 'validation/mean_average_precision': 0.25978610865418417, 'validation/num_examples': 43793, 'test/accuracy': 0.985854983329773, 'test/loss': 0.047328077256679535, 'test/mean_average_precision': 0.2575606978341206, 'test/num_examples': 43793, 'score': 3854.664434194565, 'total_duration': 5633.155611276627, 'accumulated_submission_time': 3854.664434194565, 'accumulated_eval_time': 1777.4200673103333, 'accumulated_logging_time': 0.7287240028381348, 'global_step': 11991, 'preemption_count': 0}), (12741, {'train/accuracy': 0.9904323220252991, 'train/loss': 0.03173898905515671, 'train/mean_average_precision': 0.3702695672831442, 'validation/accuracy': 0.986777663230896, 'validation/loss': 0.04454327002167702, 'validation/mean_average_precision': 0.2593790380554532, 'validation/num_examples': 43793, 'test/accuracy': 0.985881507396698, 'test/loss': 0.04745806008577347, 'test/mean_average_precision': 0.25098736000289323, 'test/num_examples': 43793, 'score': 4094.8716711997986, 'total_duration': 5976.001766443253, 'accumulated_submission_time': 4094.8716711997986, 'accumulated_eval_time': 1880.0096390247345, 'accumulated_logging_time': 0.7573575973510742, 'global_step': 12741, 'preemption_count': 0}), (13487, {'train/accuracy': 0.9905805587768555, 'train/loss': 0.03104410320520401, 'train/mean_average_precision': 0.39502075970650574, 'validation/accuracy': 0.9867857694625854, 'validation/loss': 0.04424291104078293, 'validation/mean_average_precision': 0.2639529675838791, 'validation/num_examples': 43793, 'test/accuracy': 0.9858739376068115, 'test/loss': 0.04716877639293671, 'test/mean_average_precision': 0.2534053815363039, 'test/num_examples': 43793, 'score': 4334.968943119049, 'total_duration': 6318.829391956329, 'accumulated_submission_time': 4334.968943119049, 'accumulated_eval_time': 1982.690937757492, 'accumulated_logging_time': 0.785717248916626, 'global_step': 13487, 'preemption_count': 0}), (14240, {'train/accuracy': 0.9905785918235779, 'train/loss': 0.030892036855220795, 'train/mean_average_precision': 0.3902965871620472, 'validation/accuracy': 0.9868515133857727, 'validation/loss': 0.04441400617361069, 'validation/mean_average_precision': 0.26351909600181445, 'validation/num_examples': 43793, 'test/accuracy': 0.9859964847564697, 'test/loss': 0.04715034365653992, 'test/mean_average_precision': 0.2576448284120592, 'test/num_examples': 43793, 'score': 4575.041334629059, 'total_duration': 6662.850385427475, 'accumulated_submission_time': 4575.041334629059, 'accumulated_eval_time': 2086.5756731033325, 'accumulated_logging_time': 0.8285675048828125, 'global_step': 14240, 'preemption_count': 0}), (14992, {'train/accuracy': 0.9906534552574158, 'train/loss': 0.03046361729502678, 'train/mean_average_precision': 0.4054393168607071, 'validation/accuracy': 0.9867991805076599, 'validation/loss': 0.044427357614040375, 'validation/mean_average_precision': 0.26020088163370275, 'validation/num_examples': 43793, 'test/accuracy': 0.9859017133712769, 'test/loss': 0.04743032902479172, 'test/mean_average_precision': 0.25452994743615387, 'test/num_examples': 43793, 'score': 4815.053724527359, 'total_duration': 7002.574951410294, 'accumulated_submission_time': 4815.053724527359, 'accumulated_eval_time': 2186.236001253128, 'accumulated_logging_time': 0.8597097396850586, 'global_step': 14992, 'preemption_count': 0}), (15745, {'train/accuracy': 0.9910573363304138, 'train/loss': 0.029347719624638557, 'train/mean_average_precision': 0.4316723557821035, 'validation/accuracy': 0.9867650866508484, 'validation/loss': 0.044589005410671234, 'validation/mean_average_precision': 0.2701452169981196, 'validation/num_examples': 43793, 'test/accuracy': 0.985859215259552, 'test/loss': 0.04725828394293785, 'test/mean_average_precision': 0.2598296965103563, 'test/num_examples': 43793, 'score': 5055.218120336533, 'total_duration': 7347.359417915344, 'accumulated_submission_time': 5055.218120336533, 'accumulated_eval_time': 2290.80570602417, 'accumulated_logging_time': 0.8886802196502686, 'global_step': 15745, 'preemption_count': 0}), (16505, {'train/accuracy': 0.9910054802894592, 'train/loss': 0.0295385904610157, 'train/mean_average_precision': 0.4293138323079307, 'validation/accuracy': 0.9866920113563538, 'validation/loss': 0.044278547167778015, 'validation/mean_average_precision': 0.2718293568793558, 'validation/num_examples': 43793, 'test/accuracy': 0.985924482345581, 'test/loss': 0.046870458871126175, 'test/mean_average_precision': 0.2595190000831163, 'test/num_examples': 43793, 'score': 5295.226839065552, 'total_duration': 7688.704651594162, 'accumulated_submission_time': 5295.226839065552, 'accumulated_eval_time': 2392.0925753116608, 'accumulated_logging_time': 0.9176356792449951, 'global_step': 16505, 'preemption_count': 0}), (17266, {'train/accuracy': 0.9910851716995239, 'train/loss': 0.029401373118162155, 'train/mean_average_precision': 0.4287108022495856, 'validation/accuracy': 0.9867752194404602, 'validation/loss': 0.04428261145949364, 'validation/mean_average_precision': 0.2713982672671105, 'validation/num_examples': 43793, 'test/accuracy': 0.9859097599983215, 'test/loss': 0.04684152081608772, 'test/mean_average_precision': 0.25966081320256623, 'test/num_examples': 43793, 'score': 5535.3997938632965, 'total_duration': 8027.145152568817, 'accumulated_submission_time': 5535.3997938632965, 'accumulated_eval_time': 2490.3105919361115, 'accumulated_logging_time': 0.9467637538909912, 'global_step': 17266, 'preemption_count': 0}), (18021, {'train/accuracy': 0.9907681941986084, 'train/loss': 0.030180849134922028, 'train/mean_average_precision': 0.4159931679730896, 'validation/accuracy': 0.9868023991584778, 'validation/loss': 0.044088248163461685, 'validation/mean_average_precision': 0.27192529178497843, 'validation/num_examples': 43793, 'test/accuracy': 0.9859362840652466, 'test/loss': 0.04672218859195709, 'test/mean_average_precision': 0.2607173361411814, 'test/num_examples': 43793, 'score': 5775.622433185577, 'total_duration': 8370.766879558563, 'accumulated_submission_time': 5775.622433185577, 'accumulated_eval_time': 2593.659286260605, 'accumulated_logging_time': 0.9766237735748291, 'global_step': 18021, 'preemption_count': 0}), (18785, {'train/accuracy': 0.9908871650695801, 'train/loss': 0.029914895072579384, 'train/mean_average_precision': 0.40294541791037364, 'validation/accuracy': 0.9867849946022034, 'validation/loss': 0.04428034648299217, 'validation/mean_average_precision': 0.26940112567789487, 'validation/num_examples': 43793, 'test/accuracy': 0.9860011339187622, 'test/loss': 0.046740517020225525, 'test/mean_average_precision': 0.25958977611508466, 'test/num_examples': 43793, 'score': 6015.79421544075, 'total_duration': 8711.703551054, 'accumulated_submission_time': 6015.79421544075, 'accumulated_eval_time': 2694.3736753463745, 'accumulated_logging_time': 1.0064642429351807, 'global_step': 18785, 'preemption_count': 0}), (19526, {'train/accuracy': 0.9911262392997742, 'train/loss': 0.029100975021719933, 'train/mean_average_precision': 0.42689901254227924, 'validation/accuracy': 0.9868763089179993, 'validation/loss': 0.04428340867161751, 'validation/mean_average_precision': 0.27571393971170777, 'validation/num_examples': 43793, 'test/accuracy': 0.9861102104187012, 'test/loss': 0.04693359136581421, 'test/mean_average_precision': 0.26879631236528245, 'test/num_examples': 43793, 'score': 6255.988671064377, 'total_duration': 9053.836593389511, 'accumulated_submission_time': 6255.988671064377, 'accumulated_eval_time': 2796.258656024933, 'accumulated_logging_time': 1.0368270874023438, 'global_step': 19526, 'preemption_count': 0}), (20280, {'train/accuracy': 0.9910481572151184, 'train/loss': 0.029240449890494347, 'train/mean_average_precision': 0.43098628504950526, 'validation/accuracy': 0.9868617057800293, 'validation/loss': 0.044349849224090576, 'validation/mean_average_precision': 0.2763822320770312, 'validation/num_examples': 43793, 'test/accuracy': 0.986026406288147, 'test/loss': 0.04711419716477394, 'test/mean_average_precision': 0.26216126729949074, 'test/num_examples': 43793, 'score': 6496.154449701309, 'total_duration': 9394.648380041122, 'accumulated_submission_time': 6496.154449701309, 'accumulated_eval_time': 2896.8540201187134, 'accumulated_logging_time': 1.0669054985046387, 'global_step': 20280, 'preemption_count': 0}), (21033, {'train/accuracy': 0.9910216927528381, 'train/loss': 0.029087426140904427, 'train/mean_average_precision': 0.44519078951854646, 'validation/accuracy': 0.9868708252906799, 'validation/loss': 0.044247254729270935, 'validation/mean_average_precision': 0.27284887694411497, 'validation/num_examples': 43793, 'test/accuracy': 0.9860247373580933, 'test/loss': 0.04698428511619568, 'test/mean_average_precision': 0.2614326363612846, 'test/num_examples': 43793, 'score': 6736.26745891571, 'total_duration': 9738.567348957062, 'accumulated_submission_time': 6736.26745891571, 'accumulated_eval_time': 3000.609070301056, 'accumulated_logging_time': 1.0970373153686523, 'global_step': 21033, 'preemption_count': 0}), (21799, {'train/accuracy': 0.9912761449813843, 'train/loss': 0.02821793407201767, 'train/mean_average_precision': 0.45827813256329597, 'validation/accuracy': 0.9867464303970337, 'validation/loss': 0.04423868656158447, 'validation/mean_average_precision': 0.2674546311890728, 'validation/num_examples': 43793, 'test/accuracy': 0.9859337210655212, 'test/loss': 0.046887047588825226, 'test/mean_average_precision': 0.26471127378471704, 'test/num_examples': 43793, 'score': 6976.427305936813, 'total_duration': 10082.193821668625, 'accumulated_submission_time': 6976.427305936813, 'accumulated_eval_time': 3104.023591041565, 'accumulated_logging_time': 1.1280357837677002, 'global_step': 21799, 'preemption_count': 0}), (22545, {'train/accuracy': 0.9913682341575623, 'train/loss': 0.028164630755782127, 'train/mean_average_precision': 0.4644050467140248, 'validation/accuracy': 0.9868109226226807, 'validation/loss': 0.044437095522880554, 'validation/mean_average_precision': 0.2704546050163061, 'validation/num_examples': 43793, 'test/accuracy': 0.9859463572502136, 'test/loss': 0.047244492918252945, 'test/mean_average_precision': 0.2622843075814916, 'test/num_examples': 43793, 'score': 7216.409387588501, 'total_duration': 10424.055264472961, 'accumulated_submission_time': 7216.409387588501, 'accumulated_eval_time': 3205.8467667102814, 'accumulated_logging_time': 1.1613752841949463, 'global_step': 22545, 'preemption_count': 0}), (23296, {'train/accuracy': 0.9915869235992432, 'train/loss': 0.027325928211212158, 'train/mean_average_precision': 0.470983335381583, 'validation/accuracy': 0.9869566559791565, 'validation/loss': 0.0440373495221138, 'validation/mean_average_precision': 0.27909121384071794, 'validation/num_examples': 43793, 'test/accuracy': 0.9861654043197632, 'test/loss': 0.04652325436472893, 'test/mean_average_precision': 0.26892587217486474, 'test/num_examples': 43793, 'score': 7456.590592622757, 'total_duration': 10766.633274793625, 'accumulated_submission_time': 7456.590592622757, 'accumulated_eval_time': 3308.191586256027, 'accumulated_logging_time': 1.1935296058654785, 'global_step': 23296, 'preemption_count': 0}), (24052, {'train/accuracy': 0.991644561290741, 'train/loss': 0.027014605700969696, 'train/mean_average_precision': 0.48840583600806126, 'validation/accuracy': 0.9869595170021057, 'validation/loss': 0.04417118802666664, 'validation/mean_average_precision': 0.28204949998268863, 'validation/num_examples': 43793, 'test/accuracy': 0.9862087965011597, 'test/loss': 0.04691678285598755, 'test/mean_average_precision': 0.2769371208521102, 'test/num_examples': 43793, 'score': 7696.545921325684, 'total_duration': 11106.29555439949, 'accumulated_submission_time': 7696.545921325684, 'accumulated_eval_time': 3407.847371816635, 'accumulated_logging_time': 1.2242672443389893, 'global_step': 24052, 'preemption_count': 0}), (24798, {'train/accuracy': 0.9915668368339539, 'train/loss': 0.02746260166168213, 'train/mean_average_precision': 0.47432643847940575, 'validation/accuracy': 0.9868316650390625, 'validation/loss': 0.044266603887081146, 'validation/mean_average_precision': 0.2701833417002129, 'validation/num_examples': 43793, 'test/accuracy': 0.9860756993293762, 'test/loss': 0.046777963638305664, 'test/mean_average_precision': 0.2672637130743255, 'test/num_examples': 43793, 'score': 7936.503590583801, 'total_duration': 11452.717252254486, 'accumulated_submission_time': 7936.503590583801, 'accumulated_eval_time': 3514.256602048874, 'accumulated_logging_time': 1.25538969039917, 'global_step': 24798, 'preemption_count': 0}), (25556, {'train/accuracy': 0.9913595914840698, 'train/loss': 0.028126923367381096, 'train/mean_average_precision': 0.4622975459881457, 'validation/accuracy': 0.9869400262832642, 'validation/loss': 0.04411906376481056, 'validation/mean_average_precision': 0.2790331791350799, 'validation/num_examples': 43793, 'test/accuracy': 0.9861093759536743, 'test/loss': 0.046621087938547134, 'test/mean_average_precision': 0.27050036445854647, 'test/num_examples': 43793, 'score': 8176.548576593399, 'total_duration': 11793.347196340561, 'accumulated_submission_time': 8176.548576593399, 'accumulated_eval_time': 3614.790126800537, 'accumulated_logging_time': 1.2863295078277588, 'global_step': 25556, 'preemption_count': 0}), (26314, {'train/accuracy': 0.9912744164466858, 'train/loss': 0.028144653886556625, 'train/mean_average_precision': 0.44791104391687353, 'validation/accuracy': 0.9868324398994446, 'validation/loss': 0.043981555849313736, 'validation/mean_average_precision': 0.28106984413767117, 'validation/num_examples': 43793, 'test/accuracy': 0.9859468340873718, 'test/loss': 0.046615514904260635, 'test/mean_average_precision': 0.26985342712303234, 'test/num_examples': 43793, 'score': 8416.634189844131, 'total_duration': 12131.838816642761, 'accumulated_submission_time': 8416.634189844131, 'accumulated_eval_time': 3713.1441898345947, 'accumulated_logging_time': 1.3176674842834473, 'global_step': 26314, 'preemption_count': 0}), (27065, {'train/accuracy': 0.9913526177406311, 'train/loss': 0.0279363002628088, 'train/mean_average_precision': 0.46327236406281286, 'validation/accuracy': 0.986867368221283, 'validation/loss': 0.04432373866438866, 'validation/mean_average_precision': 0.2782958896685469, 'validation/num_examples': 43793, 'test/accuracy': 0.9860487580299377, 'test/loss': 0.04718391224741936, 'test/mean_average_precision': 0.2670339478514961, 'test/num_examples': 43793, 'score': 8656.794304132462, 'total_duration': 12475.151976585388, 'accumulated_submission_time': 8656.794304132462, 'accumulated_eval_time': 3816.2447905540466, 'accumulated_logging_time': 1.3499679565429688, 'global_step': 27065, 'preemption_count': 0}), (27820, {'train/accuracy': 0.9914987683296204, 'train/loss': 0.027448926120996475, 'train/mean_average_precision': 0.4683983500487964, 'validation/accuracy': 0.9869043231010437, 'validation/loss': 0.04423655569553375, 'validation/mean_average_precision': 0.2760711966392069, 'validation/num_examples': 43793, 'test/accuracy': 0.9861077070236206, 'test/loss': 0.04697178676724434, 'test/mean_average_precision': 0.26493177180027205, 'test/num_examples': 43793, 'score': 8896.875655651093, 'total_duration': 12816.583455085754, 'accumulated_submission_time': 8896.875655651093, 'accumulated_eval_time': 3917.5429599285126, 'accumulated_logging_time': 1.381486415863037, 'global_step': 27820, 'preemption_count': 0}), (28577, {'train/accuracy': 0.9915719032287598, 'train/loss': 0.027175720781087875, 'train/mean_average_precision': 0.4733769339022719, 'validation/accuracy': 0.9869562983512878, 'validation/loss': 0.04423242807388306, 'validation/mean_average_precision': 0.2856657416394459, 'validation/num_examples': 43793, 'test/accuracy': 0.9861435294151306, 'test/loss': 0.047050319612026215, 'test/mean_average_precision': 0.27184044362970006, 'test/num_examples': 43793, 'score': 9137.05087184906, 'total_duration': 13163.168231010437, 'accumulated_submission_time': 9137.05087184906, 'accumulated_eval_time': 4023.892383813858, 'accumulated_logging_time': 1.4207780361175537, 'global_step': 28577, 'preemption_count': 0}), (29337, {'train/accuracy': 0.991813600063324, 'train/loss': 0.026514487341046333, 'train/mean_average_precision': 0.4933331247934246, 'validation/accuracy': 0.987015962600708, 'validation/loss': 0.0440828762948513, 'validation/mean_average_precision': 0.2803936676206833, 'validation/num_examples': 43793, 'test/accuracy': 0.9861460328102112, 'test/loss': 0.04681553319096565, 'test/mean_average_precision': 0.2694635318031406, 'test/num_examples': 43793, 'score': 9377.265213012695, 'total_duration': 13506.539906024933, 'accumulated_submission_time': 9377.265213012695, 'accumulated_eval_time': 4126.997245788574, 'accumulated_logging_time': 1.4529609680175781, 'global_step': 29337, 'preemption_count': 0}), (30096, {'train/accuracy': 0.9918859601020813, 'train/loss': 0.026113389059901237, 'train/mean_average_precision': 0.5055414899153353, 'validation/accuracy': 0.9869863390922546, 'validation/loss': 0.04378185793757439, 'validation/mean_average_precision': 0.2895730354147849, 'validation/num_examples': 43793, 'test/accuracy': 0.9861797094345093, 'test/loss': 0.04680396616458893, 'test/mean_average_precision': 0.2741908935948261, 'test/num_examples': 43793, 'score': 9617.430700778961, 'total_duration': 13852.530488491058, 'accumulated_submission_time': 9617.430700778961, 'accumulated_eval_time': 4232.770231723785, 'accumulated_logging_time': 1.485065221786499, 'global_step': 30096, 'preemption_count': 0}), (30853, {'train/accuracy': 0.9918944835662842, 'train/loss': 0.02597794681787491, 'train/mean_average_precision': 0.5109262523099191, 'validation/accuracy': 0.986935555934906, 'validation/loss': 0.04440790042281151, 'validation/mean_average_precision': 0.2870528055093321, 'validation/num_examples': 43793, 'test/accuracy': 0.9860929846763611, 'test/loss': 0.04719549044966698, 'test/mean_average_precision': 0.27437808902752175, 'test/num_examples': 43793, 'score': 9857.432433843613, 'total_duration': 14194.312682628632, 'accumulated_submission_time': 9857.432433843613, 'accumulated_eval_time': 4334.496691703796, 'accumulated_logging_time': 1.5186164379119873, 'global_step': 30853, 'preemption_count': 0}), (31607, {'train/accuracy': 0.9922711849212646, 'train/loss': 0.024978749454021454, 'train/mean_average_precision': 0.526230375162185, 'validation/accuracy': 0.9870455861091614, 'validation/loss': 0.04400182515382767, 'validation/mean_average_precision': 0.2825439937763267, 'validation/num_examples': 43793, 'test/accuracy': 0.9863023161888123, 'test/loss': 0.046554673463106155, 'test/mean_average_precision': 0.28262081972227066, 'test/num_examples': 43793, 'score': 10097.557705879211, 'total_duration': 14540.564175128937, 'accumulated_submission_time': 10097.557705879211, 'accumulated_eval_time': 4440.5671174526215, 'accumulated_logging_time': 1.551112174987793, 'global_step': 31607, 'preemption_count': 0}), (32365, {'train/accuracy': 0.9920186400413513, 'train/loss': 0.025651760399341583, 'train/mean_average_precision': 0.5237306076253523, 'validation/accuracy': 0.9870054125785828, 'validation/loss': 0.044830068945884705, 'validation/mean_average_precision': 0.28193439674336773, 'validation/num_examples': 43793, 'test/accuracy': 0.986120343208313, 'test/loss': 0.047644782811403275, 'test/mean_average_precision': 0.2724481736220283, 'test/num_examples': 43793, 'score': 10337.713238954544, 'total_duration': 14881.338361740112, 'accumulated_submission_time': 10337.713238954544, 'accumulated_eval_time': 4541.132849693298, 'accumulated_logging_time': 1.584326982498169, 'global_step': 32365, 'preemption_count': 0}), (33121, {'train/accuracy': 0.9920905828475952, 'train/loss': 0.025608157739043236, 'train/mean_average_precision': 0.5157125620277023, 'validation/accuracy': 0.9869648218154907, 'validation/loss': 0.044329915195703506, 'validation/mean_average_precision': 0.2834494862434448, 'validation/num_examples': 43793, 'test/accuracy': 0.9861447811126709, 'test/loss': 0.04674365743994713, 'test/mean_average_precision': 0.27867460650752457, 'test/num_examples': 43793, 'score': 10577.716572523117, 'total_duration': 15223.10135102272, 'accumulated_submission_time': 10577.716572523117, 'accumulated_eval_time': 4642.83878827095, 'accumulated_logging_time': 1.6173911094665527, 'global_step': 33121, 'preemption_count': 0}), (33874, {'train/accuracy': 0.9917319416999817, 'train/loss': 0.026505768299102783, 'train/mean_average_precision': 0.4951007638370917, 'validation/accuracy': 0.9868974089622498, 'validation/loss': 0.04454047977924347, 'validation/mean_average_precision': 0.28459337684068103, 'validation/num_examples': 43793, 'test/accuracy': 0.9861708879470825, 'test/loss': 0.047231633216142654, 'test/mean_average_precision': 0.2744900853244321, 'test/num_examples': 43793, 'score': 10817.91072845459, 'total_duration': 15572.508923053741, 'accumulated_submission_time': 10817.91072845459, 'accumulated_eval_time': 4751.999447107315, 'accumulated_logging_time': 1.6501054763793945, 'global_step': 33874, 'preemption_count': 0}), (34632, {'train/accuracy': 0.9918310642242432, 'train/loss': 0.02616903744637966, 'train/mean_average_precision': 0.4915393588454686, 'validation/accuracy': 0.987043559551239, 'validation/loss': 0.04459802806377411, 'validation/mean_average_precision': 0.28650596275138934, 'validation/num_examples': 43793, 'test/accuracy': 0.9862428903579712, 'test/loss': 0.04726970195770264, 'test/mean_average_precision': 0.27895178237589713, 'test/num_examples': 43793, 'score': 11058.15099453926, 'total_duration': 15910.252793550491, 'accumulated_submission_time': 11058.15099453926, 'accumulated_eval_time': 4849.44939661026, 'accumulated_logging_time': 1.683269739151001, 'global_step': 34632, 'preemption_count': 0}), (35378, {'train/accuracy': 0.9919923543930054, 'train/loss': 0.02578771486878395, 'train/mean_average_precision': 0.5099326315816797, 'validation/accuracy': 0.9868158102035522, 'validation/loss': 0.04446062073111534, 'validation/mean_average_precision': 0.2835898925780593, 'validation/num_examples': 43793, 'test/accuracy': 0.9859455227851868, 'test/loss': 0.047208115458488464, 'test/mean_average_precision': 0.2693484981572296, 'test/num_examples': 43793, 'score': 11298.339334487915, 'total_duration': 16249.758395671844, 'accumulated_submission_time': 11298.339334487915, 'accumulated_eval_time': 4948.712647199631, 'accumulated_logging_time': 1.7171683311462402, 'global_step': 35378, 'preemption_count': 0}), (36126, {'train/accuracy': 0.9920006990432739, 'train/loss': 0.025485051795840263, 'train/mean_average_precision': 0.5258720739264008, 'validation/accuracy': 0.987106442451477, 'validation/loss': 0.044622622430324554, 'validation/mean_average_precision': 0.2882472900207319, 'validation/num_examples': 43793, 'test/accuracy': 0.986178457736969, 'test/loss': 0.047570228576660156, 'test/mean_average_precision': 0.2711337537526851, 'test/num_examples': 43793, 'score': 11538.558787345886, 'total_duration': 16590.798290252686, 'accumulated_submission_time': 11538.558787345886, 'accumulated_eval_time': 5049.473174333572, 'accumulated_logging_time': 1.7545788288116455, 'global_step': 36126, 'preemption_count': 0}), (36889, {'train/accuracy': 0.9921327829360962, 'train/loss': 0.025163933634757996, 'train/mean_average_precision': 0.5333610794699605, 'validation/accuracy': 0.9869310855865479, 'validation/loss': 0.04440346360206604, 'validation/mean_average_precision': 0.2876226417928679, 'validation/num_examples': 43793, 'test/accuracy': 0.986159086227417, 'test/loss': 0.047014519572257996, 'test/mean_average_precision': 0.2754729484835849, 'test/num_examples': 43793, 'score': 11778.575371980667, 'total_duration': 16933.982120752335, 'accumulated_submission_time': 11778.575371980667, 'accumulated_eval_time': 5152.5869171619415, 'accumulated_logging_time': 1.7874071598052979, 'global_step': 36889, 'preemption_count': 0}), (37648, {'train/accuracy': 0.9923455119132996, 'train/loss': 0.024487245827913284, 'train/mean_average_precision': 0.5460171638954645, 'validation/accuracy': 0.9870200157165527, 'validation/loss': 0.04464934021234512, 'validation/mean_average_precision': 0.2832415157203511, 'validation/num_examples': 43793, 'test/accuracy': 0.9862471222877502, 'test/loss': 0.047332052141427994, 'test/mean_average_precision': 0.27767251335974974, 'test/num_examples': 43793, 'score': 12018.111292123795, 'total_duration': 17278.17639899254, 'accumulated_submission_time': 12018.111292123795, 'accumulated_eval_time': 5256.769897699356, 'accumulated_logging_time': 2.2428689002990723, 'global_step': 37648, 'preemption_count': 0}), (38400, {'train/accuracy': 0.9923316836357117, 'train/loss': 0.024133075028657913, 'train/mean_average_precision': 0.5530907366085315, 'validation/accuracy': 0.9871101379394531, 'validation/loss': 0.04488293081521988, 'validation/mean_average_precision': 0.2893611700860986, 'validation/num_examples': 43793, 'test/accuracy': 0.9862862825393677, 'test/loss': 0.04784107580780983, 'test/mean_average_precision': 0.27881595787050245, 'test/num_examples': 43793, 'score': 12258.103625297546, 'total_duration': 17617.65689277649, 'accumulated_submission_time': 12258.103625297546, 'accumulated_eval_time': 5356.201946020126, 'accumulated_logging_time': 2.277024269104004, 'global_step': 38400, 'preemption_count': 0}), (39153, {'train/accuracy': 0.9926602840423584, 'train/loss': 0.023420441895723343, 'train/mean_average_precision': 0.5637032087035159, 'validation/accuracy': 0.9871218800544739, 'validation/loss': 0.04486692324280739, 'validation/mean_average_precision': 0.28992026956643213, 'validation/num_examples': 43793, 'test/accuracy': 0.9862328171730042, 'test/loss': 0.04774757847189903, 'test/mean_average_precision': 0.2744675293708428, 'test/num_examples': 43793, 'score': 12498.116117477417, 'total_duration': 17958.667691469193, 'accumulated_submission_time': 12498.116117477417, 'accumulated_eval_time': 5457.145552873611, 'accumulated_logging_time': 2.3104984760284424, 'global_step': 39153, 'preemption_count': 0}), (39908, {'train/accuracy': 0.9927948713302612, 'train/loss': 0.023058081045746803, 'train/mean_average_precision': 0.5783965721104753, 'validation/accuracy': 0.9870395064353943, 'validation/loss': 0.04443762078881264, 'validation/mean_average_precision': 0.286592025599736, 'validation/num_examples': 43793, 'test/accuracy': 0.9862656593322754, 'test/loss': 0.047343429177999496, 'test/mean_average_precision': 0.2774957648688163, 'test/num_examples': 43793, 'score': 12738.242744922638, 'total_duration': 18298.640961408615, 'accumulated_submission_time': 12738.242744922638, 'accumulated_eval_time': 5556.928096294403, 'accumulated_logging_time': 2.3550827503204346, 'global_step': 39908, 'preemption_count': 0}), (40666, {'train/accuracy': 0.9926062822341919, 'train/loss': 0.02369121089577675, 'train/mean_average_precision': 0.5603386086431281, 'validation/accuracy': 0.986970067024231, 'validation/loss': 0.04487146437168121, 'validation/mean_average_precision': 0.2884174564599778, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.04765874147415161, 'test/mean_average_precision': 0.2742626805934189, 'test/num_examples': 43793, 'score': 12978.360262870789, 'total_duration': 18640.750893592834, 'accumulated_submission_time': 12978.360262870789, 'accumulated_eval_time': 5658.854161739349, 'accumulated_logging_time': 2.401482343673706, 'global_step': 40666, 'preemption_count': 0}), (41428, {'train/accuracy': 0.9925146102905273, 'train/loss': 0.023902615532279015, 'train/mean_average_precision': 0.5628586654561019, 'validation/accuracy': 0.9869022965431213, 'validation/loss': 0.04503781348466873, 'validation/mean_average_precision': 0.2864156006446135, 'validation/num_examples': 43793, 'test/accuracy': 0.9861510992050171, 'test/loss': 0.04761037603020668, 'test/mean_average_precision': 0.27925009784739535, 'test/num_examples': 43793, 'score': 13218.463328838348, 'total_duration': 18989.863604068756, 'accumulated_submission_time': 13218.463328838348, 'accumulated_eval_time': 5767.809935808182, 'accumulated_logging_time': 2.4354753494262695, 'global_step': 41428, 'preemption_count': 0}), (42176, {'train/accuracy': 0.9924070835113525, 'train/loss': 0.02437860332429409, 'train/mean_average_precision': 0.5376271174259066, 'validation/accuracy': 0.9868994355201721, 'validation/loss': 0.04490630701184273, 'validation/mean_average_precision': 0.2886609224145899, 'validation/num_examples': 43793, 'test/accuracy': 0.9860192537307739, 'test/loss': 0.04772264137864113, 'test/mean_average_precision': 0.27700951783763217, 'test/num_examples': 43793, 'score': 13458.669250249863, 'total_duration': 19328.203336000443, 'accumulated_submission_time': 13458.669250249863, 'accumulated_eval_time': 5865.882692337036, 'accumulated_logging_time': 2.4739890098571777, 'global_step': 42176, 'preemption_count': 0}), (42932, {'train/accuracy': 0.9924079179763794, 'train/loss': 0.02405993454158306, 'train/mean_average_precision': 0.5503674235235126, 'validation/accuracy': 0.9870553016662598, 'validation/loss': 0.04493481293320656, 'validation/mean_average_precision': 0.2921576254088969, 'validation/num_examples': 43793, 'test/accuracy': 0.9862534403800964, 'test/loss': 0.04771368205547333, 'test/mean_average_precision': 0.2787048486685619, 'test/num_examples': 43793, 'score': 13698.63403391838, 'total_duration': 19671.65065932274, 'accumulated_submission_time': 13698.63403391838, 'accumulated_eval_time': 5969.3086223602295, 'accumulated_logging_time': 2.5101170539855957, 'global_step': 42932, 'preemption_count': 0}), (43675, {'train/accuracy': 0.9927031993865967, 'train/loss': 0.023324938490986824, 'train/mean_average_precision': 0.5674737634256353, 'validation/accuracy': 0.9871320724487305, 'validation/loss': 0.04500267282128334, 'validation/mean_average_precision': 0.2952275795871997, 'validation/num_examples': 43793, 'test/accuracy': 0.9862707257270813, 'test/loss': 0.04805930703878403, 'test/mean_average_precision': 0.275700904360155, 'test/num_examples': 43793, 'score': 13938.807181596756, 'total_duration': 20014.515516281128, 'accumulated_submission_time': 13938.807181596756, 'accumulated_eval_time': 6071.934442520142, 'accumulated_logging_time': 2.5530261993408203, 'global_step': 43675, 'preemption_count': 0}), (44433, {'train/accuracy': 0.9927995800971985, 'train/loss': 0.022958585992455482, 'train/mean_average_precision': 0.5733450457802596, 'validation/accuracy': 0.9869570732116699, 'validation/loss': 0.044922780245542526, 'validation/mean_average_precision': 0.29425027494103423, 'validation/num_examples': 43793, 'test/accuracy': 0.9861220121383667, 'test/loss': 0.04775078222155571, 'test/mean_average_precision': 0.27613351271095554, 'test/num_examples': 43793, 'score': 14179.057677268982, 'total_duration': 20356.0144674778, 'accumulated_submission_time': 14179.057677268982, 'accumulated_eval_time': 6173.127116203308, 'accumulated_logging_time': 2.588895082473755, 'global_step': 44433, 'preemption_count': 0}), (45198, {'train/accuracy': 0.992863118648529, 'train/loss': 0.022587724030017853, 'train/mean_average_precision': 0.5935786349699248, 'validation/accuracy': 0.9871166348457336, 'validation/loss': 0.04517955705523491, 'validation/mean_average_precision': 0.2876991151416316, 'validation/num_examples': 43793, 'test/accuracy': 0.9862256646156311, 'test/loss': 0.048146359622478485, 'test/mean_average_precision': 0.2737167666026969, 'test/num_examples': 43793, 'score': 14419.24270439148, 'total_duration': 20699.316056489944, 'accumulated_submission_time': 14419.24270439148, 'accumulated_eval_time': 6276.180570602417, 'accumulated_logging_time': 2.632185935974121, 'global_step': 45198, 'preemption_count': 0}), (45945, {'train/accuracy': 0.9931178092956543, 'train/loss': 0.021580209955573082, 'train/mean_average_precision': 0.6017364886534111, 'validation/accuracy': 0.9870991706848145, 'validation/loss': 0.045759450644254684, 'validation/mean_average_precision': 0.2840771218195006, 'validation/num_examples': 43793, 'test/accuracy': 0.9862942695617676, 'test/loss': 0.048754580318927765, 'test/mean_average_precision': 0.27474917941695626, 'test/num_examples': 43793, 'score': 14659.305247306824, 'total_duration': 21039.20338702202, 'accumulated_submission_time': 14659.305247306824, 'accumulated_eval_time': 6375.948750257492, 'accumulated_logging_time': 2.666360378265381, 'global_step': 45945, 'preemption_count': 0}), (46709, {'train/accuracy': 0.9932405352592468, 'train/loss': 0.021531784906983376, 'train/mean_average_precision': 0.607432035924848, 'validation/accuracy': 0.9870342016220093, 'validation/loss': 0.0452977679669857, 'validation/mean_average_precision': 0.28736797518016444, 'validation/num_examples': 43793, 'test/accuracy': 0.9862281680107117, 'test/loss': 0.04815120995044708, 'test/mean_average_precision': 0.27591957306825343, 'test/num_examples': 43793, 'score': 14899.513046503067, 'total_duration': 21378.665546417236, 'accumulated_submission_time': 14899.513046503067, 'accumulated_eval_time': 6475.143660068512, 'accumulated_logging_time': 2.705780267715454, 'global_step': 46709, 'preemption_count': 0}), (47468, {'train/accuracy': 0.9933563470840454, 'train/loss': 0.020941488444805145, 'train/mean_average_precision': 0.6339597542179121, 'validation/accuracy': 0.9871182441711426, 'validation/loss': 0.04563377425074577, 'validation/mean_average_precision': 0.2866939484275668, 'validation/num_examples': 43793, 'test/accuracy': 0.986265242099762, 'test/loss': 0.04876919463276863, 'test/mean_average_precision': 0.2753911116802991, 'test/num_examples': 43793, 'score': 15139.652928829193, 'total_duration': 21720.766462802887, 'accumulated_submission_time': 15139.652928829193, 'accumulated_eval_time': 6577.043631315231, 'accumulated_logging_time': 2.747018337249756, 'global_step': 47468, 'preemption_count': 0}), (48225, {'train/accuracy': 0.9933629035949707, 'train/loss': 0.021108966320753098, 'train/mean_average_precision': 0.6215161265846207, 'validation/accuracy': 0.9870553016662598, 'validation/loss': 0.04590858519077301, 'validation/mean_average_precision': 0.28292161729370696, 'validation/num_examples': 43793, 'test/accuracy': 0.9862454533576965, 'test/loss': 0.04892460256814957, 'test/mean_average_precision': 0.27515733139371834, 'test/num_examples': 43793, 'score': 15379.684081554413, 'total_duration': 22058.688819169998, 'accumulated_submission_time': 15379.684081554413, 'accumulated_eval_time': 6674.879307746887, 'accumulated_logging_time': 2.782480478286743, 'global_step': 48225, 'preemption_count': 0}), (48987, {'train/accuracy': 0.9932666420936584, 'train/loss': 0.021226216107606888, 'train/mean_average_precision': 0.611152309972192, 'validation/accuracy': 0.9870601892471313, 'validation/loss': 0.04568899795413017, 'validation/mean_average_precision': 0.290932891249044, 'validation/num_examples': 43793, 'test/accuracy': 0.9862496256828308, 'test/loss': 0.048720210790634155, 'test/mean_average_precision': 0.2773312555186638, 'test/num_examples': 43793, 'score': 15619.90098786354, 'total_duration': 22399.941404104233, 'accumulated_submission_time': 15619.90098786354, 'accumulated_eval_time': 6775.858198404312, 'accumulated_logging_time': 2.8195629119873047, 'global_step': 48987, 'preemption_count': 0}), (49745, {'train/accuracy': 0.9932006001472473, 'train/loss': 0.021596472710371017, 'train/mean_average_precision': 0.6055980733194052, 'validation/accuracy': 0.9869509935379028, 'validation/loss': 0.04600798338651657, 'validation/mean_average_precision': 0.2892799181159078, 'validation/num_examples': 43793, 'test/accuracy': 0.9861186742782593, 'test/loss': 0.04889053851366043, 'test/mean_average_precision': 0.27854648288360406, 'test/num_examples': 43793, 'score': 15860.069697856903, 'total_duration': 22739.921632766724, 'accumulated_submission_time': 15860.069697856903, 'accumulated_eval_time': 6875.612053394318, 'accumulated_logging_time': 2.857351303100586, 'global_step': 49745, 'preemption_count': 0}), (50487, {'train/accuracy': 0.9930036664009094, 'train/loss': 0.022013938054442406, 'train/mean_average_precision': 0.593615504286974, 'validation/accuracy': 0.9869891405105591, 'validation/loss': 0.046247225254774094, 'validation/mean_average_precision': 0.2876874796115182, 'validation/num_examples': 43793, 'test/accuracy': 0.9861944913864136, 'test/loss': 0.04907441511750221, 'test/mean_average_precision': 0.27759458219580224, 'test/num_examples': 43793, 'score': 16100.29632639885, 'total_duration': 23079.203650951385, 'accumulated_submission_time': 16100.29632639885, 'accumulated_eval_time': 6974.607265710831, 'accumulated_logging_time': 2.895205497741699, 'global_step': 50487, 'preemption_count': 0}), (51253, {'train/accuracy': 0.9931820631027222, 'train/loss': 0.021342569962143898, 'train/mean_average_precision': 0.6134943919388604, 'validation/accuracy': 0.9870630502700806, 'validation/loss': 0.04621974751353264, 'validation/mean_average_precision': 0.2916114859958877, 'validation/num_examples': 43793, 'test/accuracy': 0.9862766265869141, 'test/loss': 0.04924943670630455, 'test/mean_average_precision': 0.28097670900093114, 'test/num_examples': 43793, 'score': 16340.390928268433, 'total_duration': 23423.587039232254, 'accumulated_submission_time': 16340.390928268433, 'accumulated_eval_time': 7078.8386816978455, 'accumulated_logging_time': 2.932920217514038, 'global_step': 51253, 'preemption_count': 0}), (52016, {'train/accuracy': 0.993344247341156, 'train/loss': 0.020870761945843697, 'train/mean_average_precision': 0.6242266281073867, 'validation/accuracy': 0.9870747923851013, 'validation/loss': 0.04647238552570343, 'validation/mean_average_precision': 0.2907870160289012, 'validation/num_examples': 43793, 'test/accuracy': 0.986294686794281, 'test/loss': 0.04942123219370842, 'test/mean_average_precision': 0.2847148471225769, 'test/num_examples': 43793, 'score': 16580.394956111908, 'total_duration': 23762.726235628128, 'accumulated_submission_time': 16580.394956111908, 'accumulated_eval_time': 7177.91805267334, 'accumulated_logging_time': 2.969144821166992, 'global_step': 52016, 'preemption_count': 0}), (52779, {'train/accuracy': 0.993465006351471, 'train/loss': 0.020574089139699936, 'train/mean_average_precision': 0.6301589141153873, 'validation/accuracy': 0.9871048331260681, 'validation/loss': 0.04623021185398102, 'validation/mean_average_precision': 0.29009999259995073, 'validation/num_examples': 43793, 'test/accuracy': 0.9862024784088135, 'test/loss': 0.049345795065164566, 'test/mean_average_precision': 0.2773575915352462, 'test/num_examples': 43793, 'score': 16820.457956552505, 'total_duration': 24107.30019426346, 'accumulated_submission_time': 16820.457956552505, 'accumulated_eval_time': 7282.37335062027, 'accumulated_logging_time': 3.0050220489501953, 'global_step': 52779, 'preemption_count': 0}), (53539, {'train/accuracy': 0.9936485290527344, 'train/loss': 0.019793720915913582, 'train/mean_average_precision': 0.6516988153813621, 'validation/accuracy': 0.9871584177017212, 'validation/loss': 0.04659149423241615, 'validation/mean_average_precision': 0.29266215846242827, 'validation/num_examples': 43793, 'test/accuracy': 0.9862854480743408, 'test/loss': 0.049793995916843414, 'test/mean_average_precision': 0.2795073140182851, 'test/num_examples': 43793, 'score': 17060.670438051224, 'total_duration': 24445.238028764725, 'accumulated_submission_time': 17060.670438051224, 'accumulated_eval_time': 7380.0420706272125, 'accumulated_logging_time': 3.041551351547241, 'global_step': 53539, 'preemption_count': 0}), (54287, {'train/accuracy': 0.993804931640625, 'train/loss': 0.019352471455931664, 'train/mean_average_precision': 0.6605385215433868, 'validation/accuracy': 0.9869335293769836, 'validation/loss': 0.04682302474975586, 'validation/mean_average_precision': 0.29329886200359595, 'validation/num_examples': 43793, 'test/accuracy': 0.9861018061637878, 'test/loss': 0.049933452159166336, 'test/mean_average_precision': 0.2787303739599055, 'test/num_examples': 43793, 'score': 17300.793816804886, 'total_duration': 24788.86217021942, 'accumulated_submission_time': 17300.793816804886, 'accumulated_eval_time': 7483.484494686127, 'accumulated_logging_time': 3.0773909091949463, 'global_step': 54287, 'preemption_count': 0}), (55034, {'train/accuracy': 0.9940738677978516, 'train/loss': 0.018616709858179092, 'train/mean_average_precision': 0.6824790188145755, 'validation/accuracy': 0.9870139360427856, 'validation/loss': 0.04679595306515694, 'validation/mean_average_precision': 0.291244495393672, 'validation/num_examples': 43793, 'test/accuracy': 0.9861464500427246, 'test/loss': 0.050152502954006195, 'test/mean_average_precision': 0.2725020764620905, 'test/num_examples': 43793, 'score': 17540.84943985939, 'total_duration': 25132.443913459778, 'accumulated_submission_time': 17540.84943985939, 'accumulated_eval_time': 7586.947088718414, 'accumulated_logging_time': 3.1195337772369385, 'global_step': 55034, 'preemption_count': 0}), (55785, {'train/accuracy': 0.9942629933357239, 'train/loss': 0.018172141164541245, 'train/mean_average_precision': 0.6914053178116203, 'validation/accuracy': 0.9870870113372803, 'validation/loss': 0.04705231636762619, 'validation/mean_average_precision': 0.2891383001431945, 'validation/num_examples': 43793, 'test/accuracy': 0.9862500429153442, 'test/loss': 0.050257518887519836, 'test/mean_average_precision': 0.27844368586421997, 'test/num_examples': 43793, 'score': 17780.89176774025, 'total_duration': 25472.753759860992, 'accumulated_submission_time': 17780.89176774025, 'accumulated_eval_time': 7687.15647649765, 'accumulated_logging_time': 3.1576216220855713, 'global_step': 55785, 'preemption_count': 0}), (56537, {'train/accuracy': 0.994239866733551, 'train/loss': 0.018327660858631134, 'train/mean_average_precision': 0.685328157042368, 'validation/accuracy': 0.9870309829711914, 'validation/loss': 0.04702512547373772, 'validation/mean_average_precision': 0.2896792143578579, 'validation/num_examples': 43793, 'test/accuracy': 0.986182689666748, 'test/loss': 0.050270892679691315, 'test/mean_average_precision': 0.2769247715519446, 'test/num_examples': 43793, 'score': 18020.839967250824, 'total_duration': 25815.408913373947, 'accumulated_submission_time': 18020.839967250824, 'accumulated_eval_time': 7789.804910898209, 'accumulated_logging_time': 3.1956868171691895, 'global_step': 56537, 'preemption_count': 0}), (57296, {'train/accuracy': 0.9940085411071777, 'train/loss': 0.01863333210349083, 'train/mean_average_precision': 0.6795497812086909, 'validation/accuracy': 0.9870337843894958, 'validation/loss': 0.04753246530890465, 'validation/mean_average_precision': 0.2885810856821342, 'validation/num_examples': 43793, 'test/accuracy': 0.9863001704216003, 'test/loss': 0.05074793100357056, 'test/mean_average_precision': 0.2777486826024884, 'test/num_examples': 43793, 'score': 18260.91561436653, 'total_duration': 26153.78284072876, 'accumulated_submission_time': 18260.91561436653, 'accumulated_eval_time': 7888.045190811157, 'accumulated_logging_time': 3.2336459159851074, 'global_step': 57296, 'preemption_count': 0})], 'global_step': 57983}
I0206 13:12:50.472279 140451058161472 submission_runner.py:586] Timing: 18477.23125720024
I0206 13:12:50.472344 140451058161472 submission_runner.py:588] Total number of evals: 77
I0206 13:12:50.472396 140451058161472 submission_runner.py:589] ====================
I0206 13:12:50.475115 140451058161472 submission_runner.py:673] Final ogbg score: 18477.1279194355
